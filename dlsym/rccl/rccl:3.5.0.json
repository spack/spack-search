{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-rccl-3.5.0-kxq4o4kydcko7vnbfq5bryvkgp6pjzel/spack-src/src/init.cc": "/*************************************************************************\n * Copyright (c) 2015-2020, NVIDIA CORPORATION. All rights reserved.\n * Modifications Copyright (c) 2019-2020 Advanced Micro Devices, Inc. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nccl.h\"\n#include \"channel.h\"\n#include \"nvmlwrap.h\"\n#include \"bootstrap.h\"\n#include \"transport.h\"\n#include \"group.h\"\n#include \"net.h\"\n#include \"coll_net.h\"\n#include \"enqueue.h\"\n#include \"graph.h\"\n#include \"argcheck.h\"\n#include <fcntl.h>\n#include <unistd.h>\n#include <hip/hip_runtime.h>\n#include <string.h>\n#include <errno.h>\n#include <assert.h>\n#include <dlfcn.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n#include \"graph/topo.h\"\n\n#define STR2(v) #v\n#define STR(v) STR2(v)\n\n#ifdef ENABLE_TRACE\nstd::chrono::high_resolution_clock::time_point ncclEpoch;\n#endif\n\n#if CUDART_VERSION >= 9020 || defined(__HIP_PLATFORM_HCC__) || defined(__HCC__) || defined(__HIPCC__)\n#define NCCL_GROUP_CUDA_STREAM 0 // CGMD: CUDA 9.2,10.X Don't need to use an internal CUDA stream\n#else\n#define NCCL_GROUP_CUDA_STREAM 1 // CGMD: CUDA 9.0,9.1 Need to use an internal CUDA stream\n#endif\n\nNCCL_PARAM(GroupCudaStream, \"GROUP_CUDA_STREAM\", NCCL_GROUP_CUDA_STREAM);\n\nNCCL_PARAM(CheckPointers, \"CHECK_POINTERS\", 0);\n\nncclNet_t* ncclNet = NULL;\nncclCollNet_t* ncclCollNet = NULL;\n\n// Returns ncclInternalError if anything fails, causing that network to be ignored.\nncclResult_t initNet(ncclNet_t* net) {\n  int ndev;\n  if (net->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (net->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initCollNet(ncclCollNet_t* collnet) {\n  int ndev;\n  if (collnet->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (collnet->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initNetPlugin(ncclNet_t** net, ncclCollNet_t** collnet) {\n  void* netPluginLib = dlopen(\"libnccl-net.so\", RTLD_NOW | RTLD_LOCAL);\n  if (netPluginLib == NULL) {\n    // dlopen does not guarantee to set errno, but dlerror only gives us a\n    // string, so checking errno doesn't hurt to try to provide a better\n    // error message\n    if (errno == ENOENT) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\");\n    } else {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : Plugin load returned %d : %s.\", errno, dlerror());\n    }\n    return ncclSuccess;\n  }\n  ncclNet_t* extNet = (ncclNet_t*) dlsym(netPluginLib, STR(NCCL_PLUGIN_SYMBOL));\n  if (extNet == NULL) {\n    INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_PLUGIN_SYMBOL) \" symbol.\");\n  } else if (initNet(extNet) == ncclSuccess) {\n    *net = extNet;\n    // Check for CollNet\n    ncclCollNet_t* extCollNet = (ncclCollNet_t*) dlsym(netPluginLib, STR(NCCL_COLLNET_PLUGIN_SYMBOL));\n    if (extCollNet == NULL) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_COLLNET_PLUGIN_SYMBOL) \" symbol.\");\n    } else if (initCollNet(extCollNet) == ncclSuccess) {\n      *collnet = extCollNet;\n    }\n    return ncclSuccess;\n  }\n  if (netPluginLib != NULL) dlclose(netPluginLib);\n  return ncclSuccess;\n}\n\nncclResult_t initNet() {\n  // Always initialize bootstrap network\n  NCCLCHECK(bootstrapNetInit());\n\n  NCCLCHECK(initNetPlugin(&ncclNet, &ncclCollNet));\n  if (ncclNet != NULL) return ncclSuccess;\n  if (initNet(&ncclNetIb) == ncclSuccess) {\n    ncclNet = &ncclNetIb;\n  } else {\n    NCCLCHECK(initNet(&ncclNetSocket));\n    ncclNet = &ncclNetSocket;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CollNetEnable, \"COLLNET_ENABLE\", 0);\n\npthread_mutex_t initLock = PTHREAD_MUTEX_INITIALIZER;\nstatic bool initialized = false;\nstatic ncclResult_t ncclInit() {\n  if (initialized) return ncclSuccess;\n  pthread_mutex_lock(&initLock);\n  if (!initialized) {\n    initEnv();\n    initNet();\n    INFO(NCCL_INIT, \"Using network %s\", ncclNetName());\n    initialized = true;\n  }\n  pthread_mutex_unlock(&initLock);\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetVersion, int* version);\nncclResult_t ncclGetVersion(int* version) {\n  if (version == NULL) return ncclInvalidArgument;\n  *version = NCCL_VERSION_CODE;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetUniqueId, ncclUniqueId* out);\nncclResult_t ncclGetUniqueId(ncclUniqueId* out) {\n  NCCLCHECK(ncclInit());\n  NCCLCHECK(PtrCheck(out, \"GetUniqueId\", \"out\"));\n  return bootstrapGetUniqueId(out);\n}\n\n// Prevent compiler from optimizing out these operations\n#ifdef __clang__\n#define NCCL_NO_OPTIMIZE __attribute__((optnone))\n#else\n#define NCCL_NO_OPTIMIZE __attribute__((optimize(\"O0\")))\n#endif\n\nvoid NCCL_NO_OPTIMIZE commPoison(ncclComm_t comm) {\n  comm->rank = comm->cudaDev = comm->busId = comm->nRanks = -1;\n}\n\n#ifdef ENABLE_COLLTRACE\nvoid *ncclCommThreadMain(void *arg) {\n  ncclComm_t comm = (ncclComm_t)arg;\n  do {\n    int tail = LOAD(comm->hostDevComm.collTraceTail)%COLLTRACE_NUM_ITEMS;\n    int head = comm->hostDevComm.collTraceHead;\n    int count;\n    if (head <= tail)\n      count = tail - head;\n    else\n      count = COLLTRACE_NUM_ITEMS + head - tail;\n    usleep(1000); //sleep 1ms\n    for (int i = 0; i < count; i++) {\n      char line[1024];\n      int offset = 0;\n      #define VEGA_GPU_RTC_FREQUENCY 2.5E7\n      sprintf(line, \"## [%12.6f] [%02d:%02d] %06lx\",\n        (double)(comm->hostDevComm.collTrace[head].timeStamp)/VEGA_GPU_RTC_FREQUENCY, comm->rank, comm->hostDevComm.collTrace[head].bid, comm->hostDevComm.collTrace[head].opCount);\n      offset = strlen(line);\n      switch (comm->hostDevComm.collTrace[head].type) {\n        case ncclCollTraceKernelLaunchType:\n          sprintf(line+offset, \" KL hwid %8x funcIndex %d\",\n            comm->hostDevComm.collTrace[head].data_0, comm->hostDevComm.collTrace[head].funcIndex);\n          break;\n        case ncclCollTraceCollEndType:\n          if (comm->hostDevComm.collTrace[head].funcIndex != -1)\n            sprintf(line+offset, \" CE next funcIndex %d\",\n              comm->hostDevComm.collTrace[head].funcIndex);\n          else\n            sprintf(line+offset, \" KE\");\n          break;\n        case ncclCollTraceAbortType:\n          sprintf(line+offset, \" Abort\");\n          break;\n        default:\n          sprintf(line+offset, \" unknown collective trace data type\");\n          break;\n      }\n      INFO(NCCL_COLL, \"%s\", line);\n      head ++;\n      head %= COLLTRACE_NUM_ITEMS;\n    }\n    comm->hostDevComm.collTraceHead = tail;\n  } while(!LOAD(&comm->hostDevComm.collTraceExit));\n  pthread_exit(NULL);\n}\n#endif\n\n#undef NCCL_NO_OPTIMIZE\n\nstatic ncclResult_t commFree(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n#ifdef ENABLE_PROFILING\n  struct ncclProf* prof = (struct ncclProf*)malloc(sizeof(struct ncclProf));\n  CUDACHECK(hipMemcpy(prof, comm->hostDevComm.devProf, sizeof(struct ncclProf), hipMemcpyDeviceToHost));\n  uint64_t wait_send_cycle = 0, wait_recv_cycle = 0;\n  for (int chan=0; chan<comm->nChannels; chan++) {\n    wait_send_cycle += prof->wait_send_cycle[chan];\n    wait_recv_cycle += prof->wait_recv_cycle[chan];\n  }\n  #define VEGA_GPU_RTC_FREQUENCY 2.5E7\n  if (comm->rank == 0) {\n    INFO(NCCL_INIT, \"# %4s %6s %6s %6s %6s %6s %7s %6s %6s %6s %6s %6s\", \"Rank\", \"total\", \"w_send\", \"w_recv\", \"send\", \"rcRdS\", \"dRcRdCS\", \"dRcCS\", \"dRc\", \"cS\", \"rc\", \"rcCS\");\n    INFO(NCCL_INIT, \"# %4s %6s %6s %6s %6s %6s %7s %6s %6s %6s %6s %6s\", \"\", \"(s)\", \"(s)\", \"(s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\", \"(GB/s)\");\n  }\n  INFO(NCCL_INIT, \"# %4d %6.4f %6.4f %6.4f %6.2f %6.2f %7.2f %6.2f %6.2f %6.2f %6.2f %6.2f\",\n    comm->rank, (double)prof->total_cycle/VEGA_GPU_RTC_FREQUENCY/comm->nChannels,\n    (double)wait_send_cycle/VEGA_GPU_RTC_FREQUENCY/comm->nChannels,\n    (double)wait_recv_cycle/VEGA_GPU_RTC_FREQUENCY/comm->nChannels,\n    (prof->send_cycle) ? (double)prof->send_byte*comm->nChannels/((double)prof->send_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->recvReduceSend_cycle) ? (double)prof->recvReduceSend_byte*comm->nChannels/((double)prof->recvReduceSend_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->directRecvReduceCopySend_cycle) ? (double)prof->directRecvReduceCopySend_byte*comm->nChannels/((double)prof->directRecvReduceCopySend_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->directRecvCopySend_cycle) ? (double)prof->directRecvCopySend_byte*comm->nChannels/((double)prof->directRecvCopySend_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->directRecv_cycle) ? (double)prof->directRecv_byte*comm->nChannels/((double)prof->directRecv_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->copySend_cycle) ? (double)prof->copySend_byte*comm->nChannels/((double)prof->copySend_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->recv_cycle) ? (double)prof->recv_byte*comm->nChannels/((double)prof->recv_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0,\n    (prof->recvCopySend_cycle) ? (double)prof->recvCopySend_byte*comm->nChannels/((double)prof->recvCopySend_cycle/VEGA_GPU_RTC_FREQUENCY*1.0E9) : 0);\n  free(prof);\n  CUDACHECK(hipFree(comm->hostDevComm.devProf));\n#endif\n\n#ifdef ENABLE_COLLTRACE\n  STORE(&comm->hostDevComm.collTraceExit, 1);\n  if (comm->hostDevComm.collTraceThread) pthread_join(comm->hostDevComm.collTraceThread, NULL);\n  CUDACHECK(hipHostFree((void *)comm->hostDevComm.collTrace));\n  CUDACHECK(hipHostFree((void *)comm->hostDevComm.collTraceTail));\n#endif\n\n  free(comm->peerInfo);\n  ncclTopoFree(comm->topo);\n\n  if (comm->bootstrap)\n    NCCLCHECK(bootstrapClose(comm->bootstrap));\n\n  CUDACHECK(hipFree(comm->hostDevComm.channels));\n  CUDACHECK(hipFree(comm->devComm));\n\n  for (int channel=0; channel<comm->nChannels; channel++)\n    NCCLCHECK(freeChannel(comm->channels+channel, comm->nRanks));\n\n  if (comm->doneEvent != NULL)\n    CUDACHECK(hipEventDestroy(comm->doneEvent));\n\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(hipStreamDestroy(comm->groupStream));\n  }\n\n  // Last rank frees shared resources between threads\n  int isLast;\n  NCCLCHECK(ncclCpuBarrierIn(comm, &isLast));\n  if (isLast) {\n    free(comm->intraBarrier);\n    free(comm->intraParams);\n    free(comm->intraCudaDevs);\n    free(comm->intraCGMode);\n    free(comm->intraCC);\n  }\n  CUDACHECK(hipHostFree((void *)comm->abortFlag));\n  CUDACHECK(hipHostFree((void *)comm->fatalDevError));\n\n  // Poison comm to try and catch a double free\n  commPoison(comm);\n\n  free(comm);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commAlloc(ncclComm_t* comret, int ndev, int rank) {\n  if (ndev < 1) {\n    WARN(\"invalid device count (%d) requested\", ndev);\n    return ncclInvalidArgument;\n  }\n  if (rank >= ndev || rank < 0) {\n    WARN(\"rank %d exceeds ndev=%d\", rank, ndev);\n    return ncclInvalidArgument;\n  }\n\n  // Try to create a CUDA object right away. If there is something wrong with\n  // the device we're on (failure cause #1) , better know it early.\n  hipEvent_t doneEvent;\n  CUDACHECK(hipEventCreateWithFlags(&doneEvent, hipEventDisableTiming));\n\n  struct ncclComm* comm;\n  NCCLCHECK(ncclCalloc(&comm, 1));\n\n  comm->rank = comm->hostDevComm.rank =rank;\n  comm->nRanks = comm->hostDevComm.nRanks = ndev;\n  hipGetDevice(&comm->cudaDev);\n  NCCLCHECK(getBusId(comm->cudaDev, &comm->busId));\n  TRACE(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x\", comm, rank, ndev, comm->cudaDev, comm->busId);\n\n  comm->doneEvent = doneEvent;\n  comm->checkPointers = ncclParamCheckPointers() == 1 ? true : false;\n#if CUDART_VERSION >= 9020 || defined(__HIP_PLATFORM_HCC__) || defined(__HCC__) || defined(__HIPCC__)\n  comm->groupCudaStream = ncclParamGroupCudaStream();\n#else\n  // Don't allow the user to overload the default setting in older CUDA builds\n  comm->groupCudaStream = NCCL_GROUP_CUDA_STREAM;\n#endif\n  comm->fatalError = ncclSuccess;\n\n  NCCLCHECK(ncclCudaHostAlloc((void**) &comm->fatalDevError, (void**) &comm->hostDevComm.fatalDevError, sizeof(ncclDevError_t)));\n  STORE(comm->fatalDevError, ncclDevSuccess);\n\n  NCCLCHECK(ncclCudaHostAlloc((void**) &comm->abortFlag, (void**) &comm->hostDevComm.abortFlag, sizeof(uint32_t)));\n  STORE(comm->abortFlag, 0);\n\n  comm->argsptr = &comm->args;\n#ifdef ENABLE_PROFILING\n  NCCLCHECK(ncclCudaCalloc(&comm->hostDevComm.devProf, 1));\n#endif\n\n#ifdef ENABLE_COLLTRACE\n  CUDACHECK(hipHostMalloc((void**) &comm->hostDevComm.collTraceTail, sizeof(uint32_t), hipHostMallocMapped));\n  CUDACHECK(hipHostMalloc((void**) &comm->hostDevComm.collTrace, sizeof(struct ncclCollTrace) * COLLTRACE_NUM_ITEMS, hipHostMallocMapped));\n  memset(comm->hostDevComm.collTrace, 0, sizeof(struct ncclCollTrace) * COLLTRACE_NUM_ITEMS);\n  comm->hostDevComm.collTraceExit = comm->hostDevComm.collTraceHead = *comm->hostDevComm.collTraceTail = 0;\n  if ((ncclDebugLevel >= NCCL_LOG_INFO) && (ncclDebugMask & NCCL_COLL))\n    pthread_create(&comm->hostDevComm.collTraceThread, NULL, ncclCommThreadMain, (void *)comm);\n  else\n    comm->hostDevComm.collTraceThread = 0;\n#endif\n  comm->collNetSupport = 0;\n\n  *comret = comm;\n  return ncclSuccess;\n}\n\nstatic ncclResult_t devCommSetup(ncclComm_t comm) {\n  // Duplicate the channels on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->hostDevComm.channels, comm->nChannels));\n  NCCLCHECK(ncclCudaMemcpy(comm->hostDevComm.channels, comm->channels, comm->nChannels));\n\n  // Copy userRanks and peers\n  for (int r=0; r<comm->nChannels; r++) {\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].ring.devUserRanks, comm->channels[r].ring.userRanks, comm->nRanks));\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].devPeers, comm->channels[r].peers, comm->nRanks+1));\n  }\n\n  // Duplicate the dev comm on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->devComm, 1));\n  NCCLCHECK(ncclCudaMemcpy(comm->devComm, &comm->hostDevComm, 1));\n  return ncclSuccess;\n}\n\n// Pre-process the string so that running \"strings\" on the lib can quickly reveal the version.\n#if defined(__HIP_PLATFORM_HCC__) || defined(__HCC__) || defined(__HIPCC__)\n#define VERSION_STRING \"RCCL version \" STR(NCCL_MAJOR) \".\" STR(NCCL_MINOR) \".\" STR(NCCL_PATCH) NCCL_SUFFIX \"+hip\" STR(HIP_VERSION_MAJOR) \".\" STR(HIP_VERSION_MINOR)\n#else\n#define VERSION_STRING \"NCCL version \" STR(NCCL_MAJOR) \".\" STR(NCCL_MINOR) \".\" STR(NCCL_PATCH) NCCL_SUFFIX \"+cuda\" STR(CUDA_MAJOR) \".\" STR(CUDA_MINOR)\n#endif\nstatic void showVersion() {\n  static int shown = 0;\n  if (shown == 0 && ncclDebugLevel >= NCCL_LOG_VERSION) {\n    printf(\"%s\\n\", VERSION_STRING);\n    fflush(stdout);\n    if (ncclDebugFile != stdout)\n      INFO(NCCL_ALL,\"%s\", VERSION_STRING); // Also log NCCL version in one of the files\n    shown = 1;\n  }\n}\n\nstatic ncclResult_t fillInfo(struct ncclComm* comm, struct ncclPeerInfo* info, uint64_t commHash) {\n  info->rank = comm->rank;\n  CUDACHECK(hipGetDevice(&info->cudaDev));\n  info->hostHash=getHostHash()+commHash;\n  info->pidHash=getPidHash()+commHash;\n\n  // Get the device MAJOR:MINOR of /dev/shm so we can use that\n  // information to decide whether we can use SHM for inter-process\n  // communication in a container environment\n  struct stat statbuf;\n  SYSCHECK(stat(\"/dev/shm\", &statbuf), \"stat\");\n  info->shmDev = statbuf.st_dev;\n\n  info->busId = comm->busId;\n\n  NCCLCHECK(ncclGpuGdrSupport(&info->gdrSupport));\n  return ncclSuccess;\n}\n\ntemplate <int type>\nstatic ncclResult_t selectTransport(struct ncclTopoSystem* topo, struct ncclTopoGraph* graph, struct ncclPeerInfo* myInfo, struct ncclPeerInfo* peerInfo, struct ncclConnect* connect, struct ncclConnector* connector, int buffSize, int channelId) {\n  for (int t=0; t<NTRANSPORTS; t++) {\n    struct ncclTransport *transport = ncclTransports+t;\n    struct ncclTransportComm* transportComm = type == 1 ? &transport->send : &transport->recv;\n    int ret = 0;\n    NCCLCHECK(transport->canConnect(&ret, topo, graph, myInfo, peerInfo));\n    if (ret) {\n      connector->transportComm = transportComm;\n      NCCLCHECK(transportComm->setup(topo, graph, myInfo, peerInfo, connect, connector, buffSize, channelId));\n      return ncclSuccess;\n    }\n  }\n  WARN(\"No transport found !\");\n  return ncclInternalError;\n}\n\nstatic ncclResult_t setupChannel(struct ncclComm* comm, int channelId, int rank, int nranks, int* ringRanks) {\n  TRACE(NCCL_INIT, \"rank %d nranks %d\", rank, nranks);\n  NCCLCHECK(initChannel(comm, channelId));\n\n  struct ncclRing* ring = &comm->channels[channelId].ring;\n  // Reorganize ranks to start with rank.\n  int shift;\n  for (shift = 0; shift<nranks; shift++) {\n    if (ringRanks[shift] == rank) {\n      break;\n    }\n  }\n  for (int i=0; i<nranks; i++) {\n    ring->userRanks[i] = ringRanks[(i+shift)%nranks];\n  }\n  return ncclSuccess;\n}\n\nvoid* waitForNonNullPtr(void* p) {\n  volatile void** ptr = (volatile void**) p;\n  while (LOAD(ptr) == NULL) sched_yield();\n  return (void*)(LOAD(ptr));\n}\n\nncclResult_t initParams(struct ncclComm* comm) {\n  hipLaunchParams* params = comm->myParams = comm->intraParams+comm->intraRank;\n  params->args =(void **)&comm->argsptr;\n  params->stream = NULL;\n  params->sharedMem = 0;\n  params->blockDim.x = 0; params->blockDim.y = params->blockDim.z = 1;\n  params->gridDim.x = 0; params->gridDim.y = params->gridDim.z = 1;\n  return ncclSuccess;\n}\n\n// Allocate/Set Intra Process Structures and set CG options\nncclResult_t ncclCommSetIntra(struct ncclComm* comm, int rank, int ranks, struct ncclComm* comm0) {\n  comm->intraRank = rank;\n  comm->intraRanks = ranks;\n  comm->intraPhase = 0;\n\n  // Alloc shared structures\n  if (rank == 0) {\n    assert(comm == comm0);\n    int* bar;\n    NCCLCHECK(ncclCalloc(&bar, 2));\n    bar[0] = bar[1] = 0;\n    comm->intraBarrier = bar;\n    NCCLCHECK(ncclCalloc(&comm->intraParams, comm->intraRanks));\n    NCCLCHECK(ncclCalloc(&comm->intraCudaDevs, comm->intraRanks));\n    int* CGMode;\n    NCCLCHECK(ncclCalloc(&CGMode, 1));\n    *CGMode = 0x11;\n    comm->intraCGMode = CGMode;\n    int* CC;\n    NCCLCHECK(ncclCalloc(&CC, 1));\n    *CC = ncclCudaCompCap();\n    comm->intraCC = CC;\n  } else {\n    comm->intraBarrier = (int*)waitForNonNullPtr(&comm0->intraBarrier);\n    comm->intraParams = (hipLaunchParams*)waitForNonNullPtr(&comm0->intraParams);\n    comm->intraCudaDevs = (int*)waitForNonNullPtr(&comm0->intraCudaDevs);\n    comm->intraCGMode = (int*)waitForNonNullPtr(&comm0->intraCGMode);\n    comm->intraCC = (int*)waitForNonNullPtr(&comm0->intraCC);\n  }\n  comm->intraCudaDevs[comm->intraRank] = comm->cudaDev;\n  NCCLCHECK(initParams(comm));\n\n  int cgMdLaunch = 1;\n\n  // Set CG Mode\n  comm->launchMode = ncclComm::GROUP;\n  char* str = getenv(\"NCCL_LAUNCH_MODE\");\n  if (comm->intraRanks == 1 || (str && strcmp(str, \"PARALLEL\") == 0)) {\n    comm->launchMode = ncclComm::PARALLEL;\n  }\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(hipStreamCreateWithFlags(&comm->groupStream, hipStreamNonBlocking));\n#if CUDART_VERSION >= 9000\n    if (*comm->intraCC && (ncclCudaCompCap() == *comm->intraCC)) {\n      // Check whether the GPU supports Cooperative Group Multi Device Launch\n      (void) hipDeviceGetAttribute(&cgMdLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, comm->cudaDev);\n    }\n#endif\n  }\n\n  // Disable cgMdLaunch if any rank does not support it\n  if (cgMdLaunch == 0) {\n    *comm->intraCGMode = 0x10;\n  }\n  return ncclSuccess;\n}\n\nstatic ncclResult_t p2pSetup(struct ncclComm* comm, struct ncclTopoGraph* graph, struct ncclChannel* channel, int nrecv, int* peerRecv, int nsend, int* peerSend) {\n  TRACE(NCCL_INIT, \"nsend %d nrecv %d\", nsend, nrecv);\n  uint32_t nSkippedSend = 0, nSkippedRecv = 0; /* for tracing */\n  struct ncclConnect connect;\n  struct ncclConnector* conn;\n  for (int i=0; i<nrecv; i++) {\n    int peer = peerRecv[i];\n    if (peer == -1 || peer >= comm->nRanks) continue;\n    conn = &channel->peers[peer].recv;\n    if (conn->connected) { ++nSkippedRecv; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(selectTransport<0>(comm->topo, graph, comm->peerInfo+comm->rank, comm->peerInfo+peer, &connect, conn, channel->buffSize, channel->id));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n  }\n  for (int i=0; i<nsend; i++) {\n    int peer = peerSend[i];\n    if (peer == -1 || peer >= comm->nRanks) continue;\n    conn = &channel->peers[peer].send;\n    if (conn->connected) { ++nSkippedSend; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(selectTransport<1>(comm->topo, graph, comm->peerInfo+comm->rank, comm->peerInfo+peer, &connect, conn, channel->buffSize, channel->id));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n  }\n  for (int i=0; i<nsend; i++) {\n    int peer = peerSend[i];\n    if (peer == -1 || peer >= comm->nRanks) continue;\n    conn = &channel->peers[peer].send;\n    if (conn->connected) {++nSkippedSend; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n    NCCLCHECK(conn->transportComm->connect(&connect, 1, comm->rank, conn));\n    conn->connected = 1;\n  }\n  for (int i=0; i<nrecv; i++) {\n    int peer = peerRecv[i];\n    if (peer == -1 || peer >= comm->nRanks) continue;\n    conn = &channel->peers[peer].recv;\n    if (conn->connected) {++nSkippedRecv; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n    NCCLCHECK(conn->transportComm->connect(&connect, 1, comm->rank, conn));\n    conn->connected = 1;\n  }\n  TRACE(NCCL_INIT, \"nsend %d nrecv %d nSkippedSend %u nSkippedRecv %u - DONE\", nsend, nrecv, nSkippedSend, nSkippedRecv);\n  return ncclSuccess;\n}\n\nextern struct ncclTransport collNetTransport;\n\n// All ranks must participate in collNetSetup call\n// type: 0 for send, 1 for recv\n// return: 0 - unsupported, 1 - supported\nstatic int collNetSetup(struct ncclComm* comm, struct ncclTopoGraph* collNetGraph, struct ncclChannel* channel, int collNetChannels, int rank, int nranks,  int masterRank, int masterPeer, int nMasters, int type) {\n  int rankInCollNet = -1;\n  int supported = 0;\n  int isMaster = (rank == masterRank) ? 1 : 0;\n  struct {\n    int collNetRank;\n    ncclConnect connect;\n  } sendrecvExchange;\n\n  // check if we can connect to collnet, whose root is the nranks-th rank\n  struct ncclPeerInfo *myInfo = comm->peerInfo+rank, *peerInfo = comm->peerInfo+nranks;\n  peerInfo->rank = nranks;\n  int ret = 1;\n  if (isMaster) {\n    NCCLCHECK(collNetTransport.canConnect(&ret, comm->topo, collNetGraph, myInfo, peerInfo));\n  }\n\n  // send master receives connect info from peer recv master\n  if (isMaster && type == 0) {\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, masterPeer, &sendrecvExchange, sizeof(sendrecvExchange)));\n    rankInCollNet = sendrecvExchange.collNetRank;\n    INFO(NCCL_INIT, \"CollNet [send] : rank %d collNetRank %d collNetNranks %d received connect from rank %d\", rank, rankInCollNet, nMasters, masterPeer);\n  }\n\n  // select\n  struct ncclPeer* root = channel->peers+nranks;\n  struct ncclConnector* conn = (type == 1) ? &root->recv : &root->send;\n  struct ncclTransportComm* transportComm = (type == 1) ? &(collNetTransport.recv) : &(collNetTransport.send);\n  conn->transportComm = transportComm;\n  // setup\n  struct ncclConnect myConnect;\n  if (isMaster && ret > 0) {\n    NCCLCHECK(transportComm->setup(comm->topo, collNetGraph, myInfo, peerInfo, &myConnect, conn, channel->buffSize, channel->id));\n  }\n  // prepare connect handles\n  ncclResult_t res;\n  struct {\n    int isMaster;\n    ncclConnect connect;\n  } *allConnects = NULL;\n  ncclConnect *masterConnects = NULL;\n  NCCLCHECK(ncclCalloc(&masterConnects, nMasters));\n  if (type == 1) {  // recv side: AllGather\n    // all ranks must participate\n    NCCLCHECK(ncclCalloc(&allConnects, nranks));\n    allConnects[rank].isMaster = isMaster;\n    memcpy(&(allConnects[rank].connect), &myConnect, sizeof(struct ncclConnect));\n    NCCLCHECKGOTO(bootstrapAllGather(comm->bootstrap, allConnects, sizeof(*allConnects)), res, cleanup);\n    // consolidate\n    int c = 0;\n    for (int r = 0; r < nranks; r++) {\n      if (allConnects[r].isMaster) {\n        memcpy(masterConnects+c, &(allConnects[r].connect), sizeof(struct ncclConnect));\n        if (r == rank) rankInCollNet = c;\n        c++;\n      }\n    }\n  } else { // send side : copy in connect info received from peer recv master\n    if (isMaster) memcpy(masterConnects+rankInCollNet, &(sendrecvExchange.connect), sizeof(struct ncclConnect));\n  }\n  // connect\n  if (isMaster && ret > 0) {\n    NCCLCHECKGOTO(transportComm->connect(masterConnects, nMasters, rankInCollNet, conn), res, cleanup);\n  }\n  // recv side sends connect info to send side\n  if (isMaster && type == 1) {\n    sendrecvExchange.collNetRank = rankInCollNet;\n    memcpy(&sendrecvExchange.connect, masterConnects+rankInCollNet, sizeof(struct ncclConnect));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, masterPeer, &sendrecvExchange, sizeof(sendrecvExchange)));\n    INFO(NCCL_INIT, \"CollNet [recv] : rank %d collNetRank %d collNetNranks %d sent connect to rank %d\", rank, rankInCollNet, nMasters, masterPeer);\n  }\n  if (ret > 0) {\n    supported = 1;\n  }\ncleanup:\n  if (allConnects != NULL) free(allConnects);\n  if (masterConnects != NULL) free(masterConnects);\n  return supported;\n}\n\nstatic ncclResult_t checkCollNetSetup(struct ncclComm* comm, int rank, int collNetSetupFail) {\n  int nranks = comm->nRanks;\n  // AllGather collNet setup results\n  int* allGatherFailures;\n  NCCLCHECK(ncclCalloc(&allGatherFailures, nranks));\n  allGatherFailures[rank] = collNetSetupFail;\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGatherFailures, sizeof(int)));\n  for (int i=0; i<nranks; i++) {\n    if (allGatherFailures[i] != 0) {\n      collNetSetupFail = 1;\n      break;\n    }\n  }\n  free(allGatherFailures);\n  if (collNetSetupFail) {\n    if (rank == 0) WARN(\"Cannot initialize CollNet, using %s instead\", ncclNetName());\n    // Free collNet resources\n    for (int r=0; r<comm->nChannels; r++) {\n      struct ncclChannel* channel = comm->channels+r;\n      struct ncclPeer* peer = channel->peers+nranks;\n      if (peer->send.transportResources && peer->send.transportComm) NCCLCHECK(peer->send.transportComm->free(peer->send.transportResources));\n      if (peer->recv.transportResources && peer->recv.transportComm) NCCLCHECK(peer->recv.transportComm->free(peer->recv.transportResources));\n      peer->send.transportResources = NULL; // avoid double free\n      peer->recv.transportResources = NULL; // avoid double free\n    }\n    // Set support to 0\n    comm->collNetSupport = 0;\n  } else {\n    comm->collNetSupport = 1;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CrossNic, \"CROSS_NIC\", 2);\nNCCL_PARAM(GraphDumpFileRank, \"GRAPH_DUMP_FILE_RANK\", 0);\n\nstatic ncclResult_t initTransportsRank(struct ncclComm* comm, ncclUniqueId* commId) {\n  // We use 3 AllGathers\n  // 1. { peerInfo, comm }\n  // 2. ConnectTransport[nranks], ConnectValue[nranks]\n  // 3. { nThreads, nrings, compCap, prev[MAXCHANNELS], next[MAXCHANNELS] }\n\n  int rank = comm->rank;\n  int nranks = comm->nRanks;\n  uint64_t commHash = getHash(commId->internal, NCCL_UNIQUE_ID_BYTES);\n  TRACE(NCCL_INIT, \"comm %p, commHash %lx, rank %d nranks %d - BEGIN\", comm, commHash, rank, nranks);\n  NCCLCHECK(bootstrapInit(commId, rank, nranks, &comm->bootstrap));\n\n  // AllGather1 - begin\n  struct {\n    struct ncclPeerInfo peerInfo;\n    struct ncclComm* comm;\n  } *allGather1Data;\n\n  NCCLCHECK(ncclCalloc(&allGather1Data, nranks));\n  allGather1Data[rank].comm = comm;\n  struct ncclPeerInfo* myInfo = &allGather1Data[rank].peerInfo;\n  NCCLCHECK(fillInfo(comm, myInfo, commHash));\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather1Data, sizeof(*allGather1Data)));\n\n  NCCLCHECK(ncclCalloc(&comm->peerInfo, nranks+1)); // Extra rank to represent CollNet root\n  for (int i = 0; i < nranks; i++) {\n    memcpy(comm->peerInfo+i, &allGather1Data[i].peerInfo, sizeof(struct ncclPeerInfo));\n    if ((i != rank) && (comm->peerInfo[i].hostHash == myInfo->hostHash) && (comm->peerInfo[i].busId == myInfo->busId)) {\n      WARN(\"Duplicate GPU detected : rank %d and rank %d both on CUDA device %x\", rank, i, myInfo->busId);\n      return ncclInvalidUsage;\n    }\n  }\n  // AllGather1 data is used again below\n  // AllGather1 - end\n\n  // Topo detection / System graph creation\n  NCCLCHECK(ncclTopoGetSystem(comm, &comm->topo));\n  // Compute paths between GPUs and NICs\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Remove inaccessible GPUs and unused NICs\n  NCCLCHECK(ncclTopoTrimSystem(comm->topo, comm));\n  // Recompute paths after trimming\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Init search\n  NCCLCHECK(ncclTopoSearchInit(comm->topo));\n  // Print final topology\n  NCCLCHECK(ncclTopoPrint(comm->topo));\n\n  // Get rings and trees\n  struct ncclTopoGraph ringGraph;\n  ringGraph.id = 0;\n  ringGraph.pattern = NCCL_TOPO_PATTERN_RING;\n  ringGraph.crossNic = ncclParamCrossNic();\n  ringGraph.collNet = 0;\n  ringGraph.minChannels = 1;\n  ringGraph.maxChannels = MAXCHANNELS/2;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &ringGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &ringGraph));\n\n  struct ncclTopoGraph treeGraph;\n  treeGraph.id = 1;\n  treeGraph.pattern = NCCL_TOPO_PATTERN_SPLIT_TREE;\n  treeGraph.crossNic = ncclParamCrossNic();\n  treeGraph.collNet = 0;\n  treeGraph.minChannels = comm->topo->nodes[NET].count != 0 ? 1 : ringGraph.nChannels;\n  treeGraph.maxChannels = ringGraph.nChannels;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &treeGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &treeGraph));\n\n  struct ncclTopoGraph collNetGraph;\n  collNetGraph.id = 2;\n  collNetGraph.pattern = NCCL_TOPO_PATTERN_TREE;\n  collNetGraph.collNet = 1;\n  collNetGraph.crossNic = ncclParamCrossNic();\n  collNetGraph.minChannels = collNetGraph.maxChannels = ringGraph.nChannels;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &collNetGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &collNetGraph));\n\n  if (comm->rank == ncclParamGraphDumpFileRank()) {\n    struct ncclTopoGraph* graphs[3] = { &ringGraph, &treeGraph, &collNetGraph };\n    NCCLCHECK(ncclTopoDumpGraphs(comm->topo, 3, graphs));\n  }\n\n  // AllGather3 - begin\n  struct ncclGraphInfo {\n    int sameChannels;\n    float speedIntra;\n    float speedInter;\n    int typeIntra;\n  };\n\n  struct {\n    int cudaCompCap;\n    int fullCudaCompCap;\n    int nChannels;\n    struct ncclGraphInfo tree;\n    struct ncclGraphInfo ring;\n    struct ncclGraphInfo collNet;\n    struct ncclTopoRanks topoRanks;\n  } *allGather3Data;\n\n  NCCLCHECK(ncclCalloc(&allGather3Data, nranks));\n  allGather3Data[rank].cudaCompCap = ncclCudaCompCap();\n  allGather3Data[rank].nChannels = comm->nChannels = treeGraph.nChannels = ringGraph.nChannels =\n    std::min(treeGraph.nChannels, ringGraph.nChannels);\n  allGather3Data[rank].tree.sameChannels = treeGraph.sameChannels;\n  allGather3Data[rank].tree.speedIntra = treeGraph.speedIntra;\n  allGather3Data[rank].tree.speedInter = treeGraph.speedInter;\n  allGather3Data[rank].tree.typeIntra = treeGraph.typeIntra;\n  allGather3Data[rank].ring.sameChannels = ringGraph.sameChannels;\n  allGather3Data[rank].ring.speedIntra = ringGraph.speedIntra;\n  allGather3Data[rank].ring.speedInter = ringGraph.speedInter;\n  allGather3Data[rank].ring.typeIntra = ringGraph.typeIntra;\n  allGather3Data[rank].collNet.sameChannels = collNetGraph.sameChannels;\n  allGather3Data[rank].collNet.speedIntra = collNetGraph.speedIntra;\n  allGather3Data[rank].collNet.speedInter = collNetGraph.speedInter;\n  allGather3Data[rank].collNet.typeIntra = collNetGraph.typeIntra;\n\n  NCCLCHECK(ncclTopoPreset(comm, &treeGraph, &ringGraph, &collNetGraph, &allGather3Data[rank].topoRanks));\n\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather3Data, sizeof(*allGather3Data)));\n\n  // Determine nNodes, firstRanks, ...\n  int* nodesFirstRank;\n  NCCLCHECK(ncclCalloc(&nodesFirstRank, nranks));\n  for (int i=0; i<nranks; i++) {\n    int node = -1;\n    int firstRank = allGather3Data[i].topoRanks.ringRecv[0];\n    for (int n=0; n<comm->nNodes; n++) {\n      if (nodesFirstRank[n] == firstRank) node = n;\n    }\n    if (node == -1) {\n      node = comm->nNodes++;\n      nodesFirstRank[node] = firstRank;\n    }\n    if (i == comm->rank) comm->node = node;\n  }\n\n  // Determine the minimum CUDA Compute capability of all GPUs\n  int myCompCap = allGather3Data[rank].cudaCompCap;\n  int minCompCap = myCompCap, maxCompCap = myCompCap;\n  for (int i = 0; i < nranks; i++) {\n    minCompCap = std::min(allGather3Data[i].cudaCompCap, minCompCap);\n    maxCompCap = std::max(allGather3Data[i].cudaCompCap, maxCompCap);\n  }\n\n  int nChannelsOrig = comm->nChannels;\n  struct ncclTopoRanks** allTopoRanks;\n  NCCLCHECK(ncclCalloc(&allTopoRanks, comm->nRanks));\n  for (int i=0; i<nranks; i++) {\n    allTopoRanks[i] = &allGather3Data[i].topoRanks;\n    // Make sure we align all ranks so that the tuning is consistent across ranks\n    treeGraph.nChannels = ringGraph.nChannels = comm->nChannels = std::min(allGather3Data[i].nChannels, comm->nChannels);\n    treeGraph.sameChannels = std::min(allGather3Data[i].tree.sameChannels, treeGraph.sameChannels);\n    treeGraph.speedIntra = std::min(allGather3Data[i].tree.speedIntra, treeGraph.speedIntra);\n    treeGraph.speedInter = std::min(allGather3Data[i].tree.speedInter, treeGraph.speedInter);\n    treeGraph.typeIntra = std::min(allGather3Data[i].tree.typeIntra, treeGraph.typeIntra);\n    ringGraph.sameChannels = std::min(allGather3Data[i].ring.sameChannels, ringGraph.sameChannels);\n    ringGraph.speedIntra = std::min(allGather3Data[i].ring.speedIntra, ringGraph.speedIntra);\n    ringGraph.speedInter = std::min(allGather3Data[i].ring.speedInter, ringGraph.speedInter);\n    ringGraph.typeIntra = std::min(allGather3Data[i].ring.typeIntra, ringGraph.typeIntra);\n    collNetGraph.sameChannels = std::min(allGather3Data[i].collNet.sameChannels, collNetGraph.sameChannels);\n    collNetGraph.speedIntra = std::min(allGather3Data[i].collNet.speedIntra, collNetGraph.speedIntra);\n    collNetGraph.speedInter = std::min(allGather3Data[i].collNet.speedInter, collNetGraph.speedInter);\n    collNetGraph.typeIntra = std::min(allGather3Data[i].collNet.typeIntra, collNetGraph.typeIntra);\n  }\n\n  if (comm->nChannels < nChannelsOrig) {\n    // We started duplicating channels during Preset(), so we need to move the\n    // duplicated channels since we have removed some.\n    for (int i=0; i<comm->nChannels; i++) memcpy(comm->channels+comm->nChannels+i, comm->channels+nChannelsOrig+i, sizeof(struct ncclChannel));\n  }\n\n  int *rings;\n  NCCLCHECK(ncclCalloc(&rings, nranks*MAXCHANNELS));\n\n  NCCLCHECK(ncclTopoPostset(comm, nodesFirstRank, allTopoRanks, rings));\n  if (comm->nNodes > 1 &&\n      ncclParamCollNetEnable() == 1 &&\n      collNetSupport()) {\n    NCCLCHECK(ncclTopoConnectCollNet(comm, &collNetGraph, rank));\n  }\n\n  free(allTopoRanks);\n  free(nodesFirstRank);\n  free(allGather3Data);\n\n  // AllGather3 - end\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BUILT %d TREES/RINGS\", rank, nranks, comm->nChannels);\n\n  NCCLCHECK(ncclTopoSetThresholds(comm, minCompCap, maxCompCap, &treeGraph, &ringGraph, &collNetGraph));\n\n  char line[1024];\n  line[0]='\\0';\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclTree* treeUp = &comm->channels[c].treeUp;\n    struct ncclTree* treeDn = &comm->channels[c].treeDn;\n    snprintf(line+strlen(line), 1023-strlen(line), \" [%d] %d/%d/%d->%d->%d|%d->%d->%d/%d/%d\",\n        c, treeUp->down[0], treeUp->down[1], treeUp->down[2], rank, treeUp->up,\n        treeDn->up, rank, treeDn->down[0], treeDn->down[1], treeDn->down[2]);\n  }\n  line[1023] = '\\0';\n  INFO(NCCL_INIT, \"Trees%s\", line);\n\n  // Set Affinity to a CPU local the our GPU, so that all memory we allocate\n  // on the host is local.\n  cpu_set_t affinitySave;\n  sched_getaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  NCCLCHECK(ncclTopoSetAffinity(comm->topo, comm->rank));\n  ncclResult_t ret;\n\n  // Connect with prev/next for each ring\n  struct ncclConnect *connect;\n  NCCLCHECKGOTO(ncclCalloc(&connect, 2), ret, affinity_restore);\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclChannel* channel = comm->channels+c;\n    NCCLCHECKGOTO(setupChannel(comm, c, rank, nranks, rings+c*nranks), ret, affinity_restore);\n    if (comm->nRanks == 1) continue;\n    NCCLCHECKGOTO(p2pSetup(comm, &ringGraph, channel, 1, &channel->ring.prev, 1, &channel->ring.next), ret, affinity_restore);\n    NCCLCHECKGOTO(p2pSetup(comm, &treeGraph, channel, NCCL_MAX_TREE_ARITY, channel->treeUp.down, 1, &channel->treeUp.up), ret, affinity_restore);\n    NCCLCHECKGOTO(p2pSetup(comm, &treeGraph, channel, 1, &channel->treeDn.up, NCCL_MAX_TREE_ARITY, channel->treeDn.down), ret, affinity_restore);\n  }\n\n  // Check if we can setup CollNet\n  if (comm->nNodes > 1 &&\n      ncclParamCollNetEnable() == 1 &&\n      collNetSupport()) {\n    int logicChannels = comm->nChannels/2;\n    int collNetSetupFail = 0;\n    const int recvIndex = 0;  // recv GPU index is always 0\n    const int sendIndex = collNetGraph.pattern == NCCL_TOPO_PATTERN_TREE ? 0 : 1;  // send GPU index depends on topo pattern\n    for (int c=0; c<logicChannels; c++) {\n      struct ncclChannel* channelRecv = comm->channels+logicChannels+c;\n      struct ncclChannel* channelSend = comm->channels+c;\n      NCCLCHECK(p2pSetup(comm, &collNetGraph, channelRecv, 1, &channelRecv->collTreeDn.up, 1, channelRecv->collTreeDn.down));\n      NCCLCHECK(p2pSetup(comm, &collNetGraph, channelSend, 1, channelSend->collTreeUp.down, 1, &channelSend->collTreeUp.up));\n      const int recvMaster = collNetGraph.intra[c*comm->localRanks+recvIndex];\n      const int sendMaster = collNetGraph.intra[c*comm->localRanks+sendIndex];\n      if (collNetSetup(comm, &collNetGraph, channelRecv, logicChannels, rank, nranks, recvMaster, sendMaster, comm->nNodes, 1) != 1)\n        collNetSetupFail = 1;\n      if (collNetSetup(comm, &collNetGraph, channelSend, logicChannels, rank, nranks, sendMaster, recvMaster, comm->nNodes, 0) != 1)\n        collNetSetupFail = 1;\n    }\n    // Verify CollNet setup across ranks\n    NCCLCHECK(checkCollNetSetup(comm, rank, collNetSetupFail));\n  }\n  TRACE(NCCL_INIT, \"rank %d nranks %d - CONNECTED %d RINGS AND TREES\", rank, nranks, comm->nChannels);\n  free(connect);\n  free(rings);\n\n  // We should have allocated all buffers, collective fifos, ... we can\n  // restore the affinity.\naffinity_restore:\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  if (ret != ncclSuccess) return ret;\n\n  // Compute intra ranks (using AllGather1 data)\n  int intraRank0 = -1, intraRank = -1, intraRanks = 0;\n  for (int i = 0; i < nranks; i++) {\n    if ((allGather1Data[i].peerInfo.hostHash == allGather1Data[rank].peerInfo.hostHash) &&\n        (allGather1Data[i].peerInfo.pidHash == allGather1Data[rank].peerInfo.pidHash)) {\n      if (intraRanks == 0) intraRank0 = i;\n      if (i == rank) intraRank = intraRanks;\n      intraRanks++;\n    }\n  }\n  TRACE(NCCL_INIT,\"hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n        rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n  if (intraRank == -1 || intraRank0 == -1 || allGather1Data[intraRank0].comm == NULL) {\n    WARN(\"Failed to determine intra ranks hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n         rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n    return ncclInternalError;\n  }\n  NCCLCHECK(ncclCommSetIntra(comm, intraRank, intraRanks, allGather1Data[intraRank0].comm));\n\n  // Done with AllGather1 data\n  free(allGather1Data);\n\n  if (comm->nNodes) NCCLCHECK(transportCreateProxy(comm));\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - DONE\", rank, nranks);\n  return ncclSuccess;\n}\n\nncclResult_t ncclCommInitRankSync(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n\n  CUDACHECK(hipSetDevice(cudaDev));\n  NCCLCHECKGOTO(commAlloc(newcomm, nranks, myrank), res, cleanup);\n  NCCLCHECKGOTO(initTransportsRank(*newcomm, &commId), res, cleanup);\n  NCCLCHECKGOTO(devCommSetup(*newcomm), res, cleanup);\n\n  INFO(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x - Init COMPLETE\", *newcomm, myrank, nranks, (*newcomm)->cudaDev, (*newcomm)->busId);\n\n  return ncclSuccess;\ncleanup:\n  if ((*newcomm) && (*newcomm)->bootstrap) bootstrapAbort((*newcomm)->bootstrap);\n  *newcomm = NULL;\n  return res;\n}\n\nstatic ncclResult_t ncclCommInitRankDev(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n  char* env = getenv(\"NCCL_COMM_ID\");\n  if (env && myrank == 0) {\n    NCCLCHECKGOTO(bootstrapCreateRoot(&commId, true), res, end);\n  }\n\n  NCCLCHECKGOTO(ncclInit(), res, end);\n  if (myrank == 0) showVersion();\n\n  // Make sure the CUDA runtime is initialized.\n  CUDACHECKGOTO(hipFree(NULL), res, end);\n\n  NCCLCHECKGOTO(PtrCheck(newcomm, \"CommInitRank\", \"newcomm\"), res, end);\n  if (nranks < 1 || myrank < 0 || myrank >= nranks) {\n    WARN(\"Invalid rank requested : %d/%d\", myrank, nranks);\n    res = ncclInvalidArgument;\n    goto end;\n  }\n\n  if (ncclAsyncMode()) {\n    NCCLCHECKGOTO(ncclAsyncInit(ncclCommInitRankSync, newcomm, nranks, commId, myrank, cudaDev), res, end);\n  } else {\n    NCCLCHECKGOTO(ncclCommInitRankSync(newcomm, nranks, commId, myrank, cudaDev), res, end);\n  }\nend:\n  if (ncclAsyncMode()) return ncclAsyncErrCheck(res);\n  else return res;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitRank, ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank);\nncclResult_t ncclCommInitRank(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank) {\n  int cudaDev;\n  CUDACHECK(hipGetDevice(&cudaDev));\n  NCCLCHECK(ncclCommInitRankDev(newcomm, nranks, commId, myrank, cudaDev));\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitAll, ncclComm_t* comms, int ndev, const int* devlist);\nncclResult_t ncclCommInitAll(ncclComm_t* comms, int ndev, const int* devlist) {\n  NCCLCHECK(PtrCheck(comms, \"CommInitAll\", \"comms\"));\n  if (ndev < 0) {\n    WARN(\"Invalid device count requested : %d\", ndev);\n    return ncclInvalidArgument;\n  }\n\n  ncclUniqueId uniqueId;\n  NCCLCHECK(ncclGetUniqueId(&uniqueId));\n  NCCLCHECK(ncclGroupStart());\n  for (int i=0; i<ndev; i++) {\n    // Ignore return codes .. we need to call ncclGroupEnd to clean up anyway\n    ncclCommInitRankDev(comms+i, ndev, uniqueId, i, devlist ? devlist[i] : i);\n  }\n  NCCLCHECK(ncclGroupEnd());\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commDestroy(ncclComm_t comm) {\n  int savedDevice;\n#ifdef ENABLE_TRACE\n  int rank = comm->rank;\n#endif\n  CUDACHECK(hipGetDevice(&savedDevice));\n  int commDevice = comm->cudaDev;\n\n  if (savedDevice != commDevice) {\n    CUDACHECK(hipSetDevice(commDevice));\n  }\n\n  TRACE(NCCL_INIT, \"Destroying comm %p rank %d abortFlag %d fatalError %d\", comm, rank, LOAD(comm->abortFlag), comm->fatalError);\n\n  CUDACHECK(hipStreamSynchronize(comm->groupStream));\n  NCCLCHECK(transportDestroyProxy(comm));\n  NCCLCHECK(commFree(comm));\n\n  if (savedDevice != commDevice)\n    CUDACHECK(hipSetDevice(savedDevice));\n\n  TRACE(NCCL_INIT, \"Destroyed comm %p rank %d\", comm, rank);\n\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommDestroy, ncclComm_t comm);\nncclResult_t ncclCommDestroy(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  TRACE(NCCL_INIT, \"comm %p rank %d nRanks %d cudaDev %d busId %x\", comm, comm->rank, comm->nRanks, comm->cudaDev, comm->busId);\n\n  // Try and prevent a double free of the comm struct (user error)\n  if (comm->rank == -1 || comm->nRanks <= 0 || comm->cudaDev == -1 || comm->busId == -1) {\n    WARN(\"comm %p has already been destroyed\", comm);\n    return ncclInvalidArgument;\n  }\n\n  return commDestroy(comm);\n}\n\nNCCL_API(ncclResult_t, ncclCommAbort, ncclComm_t comm);\nncclResult_t ncclCommAbort(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  // Ask anything that might still be running on the device to quit\n  STORE(comm->abortFlag, 1);\n\n  // do not destroy comm because kernel maybe still running\n  // return commDestroy(comm);\n  return ncclSuccess;\n}\n\nNCCL_API(const char*, ncclGetErrorString, ncclResult_t code);\nconst char* ncclGetErrorString(ncclResult_t code) {\n  switch (code) {\n    case ncclSuccess                : return \"no error\";\n    case ncclUnhandledCudaError     : return \"unhandled cuda error\";\n    case ncclSystemError            : return \"unhandled system error\";\n    case ncclInternalError          : return \"internal error\";\n    case ncclInvalidArgument        : return \"invalid argument\";\n    case ncclInvalidUsage           : return \"invalid usage\";\n    default                         : return \"unknown result code\";\n  }\n}\n\nNCCL_API(ncclResult_t, ncclCommGetAsyncError, ncclComm_t comm, ncclResult_t *asyncError);\nncclResult_t ncclCommGetAsyncError(ncclComm_t comm, ncclResult_t *asyncError) {\n  NCCLCHECK(PtrCheck(comm, \"ncclGetAsyncError\", \"comm\"));\n  NCCLCHECK(PtrCheck(asyncError, \"ncclGetAsyncError\", \"asyncError\"));\n\n  // Check device reported error\n  static ncclDevError_t printedDevErr = ncclDevSuccess;\n  switch(LOAD(comm->fatalDevError)) {\n    case ncclDevSuccess :\n      break;\n    case ncclDevAssertedMismatch :\n      if (printedDevErr != ncclDevAssertedMismatch) {\n        WARN(\"Mismatched collective detected, please check your collective calls at and around rank %d. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\", comm->rank);\n        printedDevErr = ncclDevAssertedMismatch;\n      }\n      if (comm->fatalError == ncclSuccess) {\n        comm->fatalError = ncclInvalidUsage;\n      }\n      break;\n    case ncclDevSuspectedMismatch :\n      if (printedDevErr != ncclDevSuspectedMismatch) {\n        WARN(\"Your program may be hanging, this may be caused by a collective mismatch around rank %d. Please check your collective calls at and around this rank. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\", comm->rank);\n        printedDevErr = ncclDevSuspectedMismatch;\n      }\n      break;\n    default:\n      WARN(\"Unknown device error %d\", *comm->fatalDevError);\n      return ncclInternalError;\n  }\n  *asyncError = comm->fatalError;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCount, const ncclComm_t comm, int* count);\nncclResult_t ncclCommCount(const ncclComm_t comm, int* count) {\n  NCCLCHECK(PtrCheck(comm, \"CommCount\", \"comm\"));\n  NCCLCHECK(PtrCheck(count, \"CommCount\", \"count\"));\n  *count = comm->nRanks;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCuDevice, const ncclComm_t comm, int* devid);\nncclResult_t ncclCommCuDevice(const ncclComm_t comm, int* devid) {\n  NCCLCHECK(PtrCheck(comm, \"CommCuDevice\", \"comm\"));\n  NCCLCHECK(PtrCheck(devid, \"CommCuDevice\", \"devid\"));\n  *devid = comm->cudaDev;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommUserRank, const ncclComm_t comm, int* rank);\nncclResult_t ncclCommUserRank(const ncclComm_t comm, int* rank) {\n  NCCLCHECK(PtrCheck(comm, \"CommUserRank\", \"comm\"));\n  NCCLCHECK(PtrCheck(rank, \"CommUserRank\", \"rank\"));\n  *rank = comm->rank;\n  return ncclSuccess;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-rccl-3.5.0-kxq4o4kydcko7vnbfq5bryvkgp6pjzel/spack-src/src/misc/nvmlwrap.cc": "/*************************************************************************\n * Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nvmlwrap.h\"\n\n#ifndef NVML_DIRECT\n#include <dlfcn.h>\n#include \"core.h\"\n\nstatic enum { nvmlUninitialized, nvmlInitializing, nvmlInitialized, nvmlError } nvmlState = nvmlUninitialized;\n\nstatic nvmlReturn_t (*nvmlInternalInit)(void);\nstatic nvmlReturn_t (*nvmlInternalShutdown)(void);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByPciBusId)(const char* pciBusId, nvmlDevice_t* device);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetIndex)(nvmlDevice_t device, unsigned* index);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByIndex)(unsigned int index, nvmlDevice_t* device);\nstatic const char* (*nvmlInternalErrorString)(nvmlReturn_t r);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkState)(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetPciInfo)(nvmlDevice_t device, nvmlPciInfo_t* pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkRemotePciInfo)(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkCapability)(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetMinorNumber)(nvmlDevice_t device, unsigned int* minorNumber);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetCudaComputeCapability)(nvmlDevice_t device, int* major, int* minor);\n\n// Used to make the NVML library calls thread safe\npthread_mutex_t nvmlLock = PTHREAD_MUTEX_INITIALIZER;\n\nncclResult_t wrapNvmlSymbols(void) {\n  if (nvmlState == nvmlInitialized)\n    return ncclSuccess;\n  if (nvmlState == nvmlError)\n    return ncclSystemError;\n\n  if (__sync_bool_compare_and_swap(&nvmlState, nvmlUninitialized, nvmlInitializing) == false) {\n    // Another thread raced in front of us. Wait for it to be done.\n    while (nvmlState == nvmlInitializing) pthread_yield();\n    return (nvmlState == nvmlInitialized) ? ncclSuccess : ncclSystemError;\n  }\n\n  static void* nvmlhandle = NULL;\n  void* tmp;\n  void** cast;\n\n  nvmlhandle=dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n  if (!nvmlhandle) {\n    WARN(\"Failed to open libnvidia-ml.so.1\");\n    goto teardown;\n  }\n\n#define LOAD_SYM(handle, symbol, funcptr) do {         \\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      WARN(\"dlsym failed on %s - %s\", symbol, dlerror());\\\n      goto teardown;                                     \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n#define LOAD_SYM_OPTIONAL(handle, symbol, funcptr) do {\\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      INFO(NCCL_INIT,\"dlsym failed on %s, ignoring\", symbol); \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n  LOAD_SYM(nvmlhandle, \"nvmlInit\", nvmlInternalInit);\n  LOAD_SYM(nvmlhandle, \"nvmlShutdown\", nvmlInternalShutdown);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByPciBusId\", nvmlInternalDeviceGetHandleByPciBusId);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetIndex\", nvmlInternalDeviceGetIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByIndex\", nvmlInternalDeviceGetHandleByIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlErrorString\", nvmlInternalErrorString);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetPciInfo\", nvmlInternalDeviceGetPciInfo);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetMinorNumber\", nvmlInternalDeviceGetMinorNumber);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkState\", nvmlInternalDeviceGetNvLinkState);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkRemotePciInfo\", nvmlInternalDeviceGetNvLinkRemotePciInfo);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkCapability\", nvmlInternalDeviceGetNvLinkCapability);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetCudaComputeCapability\", nvmlInternalDeviceGetCudaComputeCapability);\n\n  nvmlState = nvmlInitialized;\n  return ncclSuccess;\n\nteardown:\n  nvmlInternalInit = NULL;\n  nvmlInternalShutdown = NULL;\n  nvmlInternalDeviceGetHandleByPciBusId = NULL;\n  nvmlInternalDeviceGetIndex = NULL;\n  nvmlInternalDeviceGetHandleByIndex = NULL;\n  nvmlInternalDeviceGetPciInfo = NULL;\n  nvmlInternalDeviceGetMinorNumber = NULL;\n  nvmlInternalDeviceGetNvLinkState = NULL;\n  nvmlInternalDeviceGetNvLinkRemotePciInfo = NULL;\n  nvmlInternalDeviceGetNvLinkCapability = NULL;\n\n  if (nvmlhandle != NULL) dlclose(nvmlhandle);\n  nvmlState = nvmlError;\n  return ncclSystemError;\n}\n\n\nncclResult_t wrapNvmlInit(void) {\n  if (nvmlInternalInit == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalInit();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlInit() failed: %s\",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlShutdown(void) {\n  if (nvmlInternalShutdown == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalShutdown();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlShutdown() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByPciBusId(const char* pciBusId, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByPciBusId == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByPciBusId(pciBusId, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByPciBusId() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetIndex(nvmlDevice_t device, unsigned* index) {\n  if (nvmlInternalDeviceGetIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetIndex(device, index), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByIndex(unsigned int index, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByIndex(index, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetPciInfo(nvmlDevice_t device, nvmlPciInfo_t* pci) {\n  if (nvmlInternalDeviceGetPciInfo == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetPciInfo(device, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetPciInfo() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetMinorNumber(nvmlDevice_t device, unsigned int* minorNumber) {\n  if (nvmlInternalDeviceGetMinorNumber == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetMinorNumber(device, minorNumber), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetMinorNumber() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkState(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive) {\n  if (nvmlInternalDeviceGetNvLinkState == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkState(device, link, isActive), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkState() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkRemotePciInfo(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci) {\n  if (nvmlInternalDeviceGetNvLinkRemotePciInfo == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkRemotePciInfo(device, link, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkRemotePciInfo() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkCapability(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkCapability(device, link, capability, capResult), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkCapability() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetCudaComputeCapability(nvmlDevice_t device, int* major, int* minor) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetCudaComputeCapability(device, major, minor), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetCudaComputeCapability() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n#endif\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-rccl-3.5.0-kxq4o4kydcko7vnbfq5bryvkgp6pjzel/spack-src/tools/TopoVisual/4_nodes.log.png",
        "/tmp/vanessa/spack-stage/spack-stage-rccl-3.5.0-kxq4o4kydcko7vnbfq5bryvkgp6pjzel/spack-src/docs/rocm.jpg"
    ],
    "total_files": 162
}