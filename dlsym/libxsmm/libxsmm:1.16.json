{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/src/libxsmm_main.c": "/******************************************************************************\n* Copyright (c) Intel Corporation - All rights reserved.                      *\n* This file is part of the LIBXSMM library.                                   *\n*                                                                             *\n* For information on the license, see the LICENSE file.                       *\n* Further information: https://github.com/hfp/libxsmm/                        *\n* SPDX-License-Identifier: BSD-3-Clause                                       *\n******************************************************************************/\n/* Hans Pabst, Alexander Heinecke (Intel Corp.)\n******************************************************************************/\n#include \"libxsmm_trace.h\"\n#include \"libxsmm_xcopy.h\"\n#include \"libxsmm_gemm.h\"\n#include \"libxsmm_hash.h\"\n#include \"libxsmm_diff.h\"\n#include \"libxsmm_main.h\"\n#if defined(LIBXSMM_PERF)\n# include \"libxsmm_perf.h\"\n#endif\n#include \"generator_common.h\"\n\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n#endif\n#if !defined(NDEBUG)\n# include <errno.h>\n#endif\n#if defined(_WIN32)\n# include <Windows.h>\n#else\n# include <sys/types.h>\n# include <sys/mman.h>\n# include <sys/stat.h>\n# include <unistd.h>\n# include <fcntl.h>\n#endif\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(pop)\n#endif\n\n#if !defined(LIBXSMM_CODE_MAXSIZE)\n# define LIBXSMM_CODE_MAXSIZE 131072\n#endif\n#if !defined(LIBXSMM_DIFF_SIZE)\n# define LIBXSMM_DIFF_SIZE LIBXSMM_DESCRIPTOR_SIGSIZE\n#endif\n#if !defined(LIBXSMM_HASH_SIZE)\n# define LIBXSMM_HASH_SIZE 32\n#endif\n#if !defined(LIBXSMM_HASH_SEED)\n# define LIBXSMM_HASH_SEED 25071975\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_ALIGN) && 1\n# define LIBXSMM_MALLOC_HOOK_ALIGN\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_INIT) && 0\n# define LIBXSMM_MALLOC_HOOK_INIT\n#endif\n#if !defined(LIBXSMM_ENABLE_DEREG) && 0\n# define LIBXSMM_ENABLE_DEREG\n#endif\n#if !defined(LIBXSMM_REGLOCK_TRY) && 0\n# define LIBXSMM_REGLOCK_TRY\n#endif\n#if !defined(LIBXSMM_UNIFY_LOCKS) && 1\n# define LIBXSMM_UNIFY_LOCKS\n#endif\n#if !defined(LIBXSMM_DIFF_INLINE) && 1\n# define LIBXSMM_DIFF_INLINE\n#endif\n#if !defined(LIBXSMM_DESC_INLINE) && 0\n# define LIBXSMM_DESC_INLINE\n#endif\n#if !defined(LIBXSMM_DESC_PAD) && 1\n# define LIBXSMM_DESC_PAD\n#endif\n#if !defined(LIBXSMM_CACHE_PAD) && 1\n# define LIBXSMM_CACHE_PAD\n#endif\n#if !defined(LIBXSMM_AUTOPIN) && 1\n# define LIBXSMM_AUTOPIN\n#endif\n#if !defined(INTERNAL_DELIMS)\n# define INTERNAL_DELIMS \";,:\"\n#endif\n\n#if defined(LIBXSMM_AUTOPIN) && !defined(_WIN32)\nLIBXSMM_EXTERN int putenv(char*) LIBXSMM_THROW;\n#endif\n\n/* flag fused into the memory address of a code version in case of non-JIT */\n#define LIBXSMM_CODE_STATIC (1ULL << (8 * sizeof(void*) - 1))\n/* flag fused into the memory address of a code version in case of collision */\n#if 1 /* beneficial when registry approaches capacity (collisions) */\n# define LIBXSMM_HASH_COLLISION (1ULL << (8 * sizeof(void*) - 2))\n#endif\n\n/** Helper macro determining the default prefetch strategy which is used for statically generated kernels. */\n#if (0 > LIBXSMM_PREFETCH) /* auto-prefetch (frontend) */ || (defined(_WIN32) || defined(__CYGWIN__))\n# define INTERNAL_PREFETCH LIBXSMM_GEMM_PREFETCH_NONE\n#else\n# define INTERNAL_PREFETCH ((libxsmm_gemm_prefetch_type)LIBXSMM_PREFETCH)\n#endif\n\n#if (0 != LIBXSMM_SYNC)\n# if !defined(INTERNAL_REGLOCK_MAXN)\n#   if defined(_MSC_VER)\n#     define INTERNAL_REGLOCK_MAXN 0\n#   else\n#     define INTERNAL_REGLOCK_MAXN 0\n#   endif\n# endif\n# if (1 < INTERNAL_REGLOCK_MAXN)\n#   if !defined(LIBXSMM_CACHE_MAXSIZE) && (8 > INTERNAL_REGLOCK_MAXN)\n#     define LIBXSMM_CACHE_MAXSIZE LIBXSMM_CAPACITY_CACHE\n#   endif\n#   if !defined(LIBXSMM_REGLOCK)\n#     define LIBXSMM_REGLOCK LIBXSMM_LOCK_DEFAULT\n#   endif\n#   if !defined(LIBXSMM_CLEANUP_NTRY)\n#     define LIBXSMM_CLEANUP_NTRY 7\n#   endif\n#   if LIBXSMM_LOCK_TYPE_ISPOD(LIBXSMM_REGLOCK)\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_reglocktype {\n  char pad[LIBXSMM_CACHELINE];\n  LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK) state;\n} internal_reglocktype;\n#   else\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_reglocktype {\n  LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK) state;\n} internal_reglocktype;\n#   endif\nLIBXSMM_APIVAR_DEFINE(internal_reglocktype internal_reglock[INTERNAL_REGLOCK_MAXN]);\n# else /* RW-lock */\n#   if !defined(LIBXSMM_CACHE_MAXSIZE)\n#     define LIBXSMM_CACHE_MAXSIZE LIBXSMM_CAPACITY_CACHE\n#   endif\n#   if !defined(LIBXSMM_REGLOCK)\n#     if defined(LIBXSMM_UNIFY_LOCKS)\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK\n#     elif defined(_MSC_VER)\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK_MUTEX\n#     elif 0\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK_RWLOCK\n#     else\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK_DEFAULT\n#     endif\n#   endif\nLIBXSMM_APIVAR_DEFINE(LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK)* internal_reglock_ptr);\n# endif\n#elif !defined(LIBXSMM_CACHE_MAXSIZE)\n# define LIBXSMM_CACHE_MAXSIZE LIBXSMM_CAPACITY_CACHE\n#endif\n\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n# define INTERNAL_FIND_CODE_CACHE_GROW(RESULT_INDEX, CACHE_SIZE) \\\n    RESULT_INDEX = CACHE_SIZE; CACHE_SIZE = (unsigned char)(0 != (CACHE_SIZE) ? ((CACHE_SIZE) << 1) : 1)\n# define INTERNAL_FIND_CODE_CACHE_EVICT(RESULT_INDEX, CACHE_SIZE, CACHE_HIT) \\\n    RESULT_INDEX = (unsigned char)LIBXSMM_MOD2((CACHE_HIT) + ((CACHE_SIZE) - 1), CACHE_SIZE)\n#endif\n\n#if (0 == LIBXSMM_SYNC)\n# define INTERNAL_FIND_CODE_LOCK(LOCKINDEX, INDEX, DIFF, CODE) {\n# define INTERNAL_FIND_CODE_UNLOCK(LOCKINDEX) }\n#else\n# if defined(LIBXSMM_REGLOCK_TRY)\n#   define INTERNAL_REGLOCK_TRY(DIFF, CODE) \\\n    if (1 != internal_reglock_count) { /* (re-)try and get (meanwhile) generated code */ \\\n      LIBXSMM_ASSERT(NULL != internal_registry); /* engine is not shut down */ \\\n      continue; \\\n    } \\\n    else { /* exit dispatch and let client fall back */ \\\n      DIFF = 0; CODE = 0; break; \\\n    }\n# else\n#   define INTERNAL_REGLOCK_TRY(DIFF, CODE) \\\n      LIBXSMM_ASSERT(NULL != internal_registry); /* engine is not shut down */ \\\n      continue\n# endif\n# if (1 < INTERNAL_REGLOCK_MAXN)\n#   define INTERNAL_FIND_CODE_LOCK(LOCKINDEX, INDEX, DIFF, CODE) { \\\n      const unsigned int LOCKINDEX = (0 != internal_reglock_count ? LIBXSMM_MOD2(INDEX, internal_reglock_count) : 0); \\\n      if (LIBXSMM_LOCK_ACQUIRED(LIBXSMM_REGLOCK) != LIBXSMM_LOCK_TRYLOCK(LIBXSMM_REGLOCK, &internal_reglock[LOCKINDEX].state)) { \\\n        INTERNAL_REGLOCK_TRY(DIFF, CODE); \\\n      }\n#   define INTERNAL_FIND_CODE_UNLOCK(LOCKINDEX) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, &internal_reglock[LOCKINDEX].state); }\n# else /* RW-lock */\n#   define INTERNAL_FIND_CODE_LOCK(LOCKINDEX, INDEX, DIFF, CODE) { \\\n      if (LIBXSMM_LOCK_ACQUIRED(LIBXSMM_REGLOCK) != LIBXSMM_LOCK_TRYLOCK(LIBXSMM_REGLOCK, internal_reglock_ptr)) { \\\n        INTERNAL_REGLOCK_TRY(DIFF, CODE); \\\n      }\n#   define INTERNAL_FIND_CODE_UNLOCK(LOCKINDEX) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, internal_reglock_ptr); }\n# endif\n#endif\n\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_statistic_type {\n  unsigned int ntry, ncol, njit, nsta;\n} internal_statistic_type;\n\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_cache_entry_type {\n  libxsmm_descriptor keys[LIBXSMM_CACHE_MAXSIZE];\n  libxsmm_code_pointer code[LIBXSMM_CACHE_MAXSIZE];\n  unsigned int id; /* to invalidate */\n  unsigned char size, hit;\n} internal_cache_entry_type;\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_cache_type {\n# if defined(LIBXSMM_CACHE_PAD)\n  char pad[LIBXSMM_UP2(sizeof(internal_cache_entry_type),LIBXSMM_CACHELINE)];\n# endif\n  internal_cache_entry_type entry;\n} internal_cache_type;\n\n# if defined(LIBXSMM_NTHREADS_USE)\nLIBXSMM_APIVAR_DEFINE(internal_cache_type* internal_cache_buffer);\n# endif\nLIBXSMM_APIVAR_DEFINE(int internal_cache_size);\n#endif /*defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))*/\n\n/** Determines the try-lock property (1<N: disabled, N=1: enabled [N=0: disabled in case of RW-lock]). */\nLIBXSMM_APIVAR_DEFINE(int internal_reglock_count);\nLIBXSMM_APIVAR_DEFINE(size_t internal_registry_nbytes);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_registry_nleaks);\nLIBXSMM_APIVAR_DEFINE(libxsmm_descriptor* internal_registry_keys);\nLIBXSMM_APIVAR_DEFINE(libxsmm_code_pointer* internal_registry);\nLIBXSMM_APIVAR_DEFINE(internal_statistic_type internal_statistic[2/*DP/SP*/][4/*sml/med/big/xxx*/]);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_sml);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_med);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_mnk);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_gemv);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_mcopy);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_meltw);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_tcopy);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_trsm);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_trmm);\nLIBXSMM_APIVAR_DEFINE(unsigned int internal_statistic_num_user);\nLIBXSMM_APIVAR_DEFINE(int internal_gemm_auto_prefetch_locked);\nLIBXSMM_APIVAR_DEFINE(const char* internal_build_state);\n/** Time stamp (startup time of library). */\nLIBXSMM_APIVAR_DEFINE(libxsmm_timer_tickint internal_timer_start);\nLIBXSMM_APIVAR_DEFINE(libxsmm_cpuid_x86_info internal_cpuid_info);\n\n#if defined(_WIN32)\n# define INTERNAL_SINGLETON_HANDLE HANDLE\n# define INTERNAL_SINGLETON(HANDLE) (NULL != (HANDLE))\n#else\n# define INTERNAL_SINGLETON_HANDLE int\n# define INTERNAL_SINGLETON(HANDLE) (0 <= (HANDLE) && 0 != *internal_singleton_fname)\nLIBXSMM_APIVAR_DEFINE(char internal_singleton_fname[64]);\n#endif\nLIBXSMM_APIVAR_DEFINE(INTERNAL_SINGLETON_HANDLE internal_singleton_handle);\n\n/* definition of corresponding variables */\nLIBXSMM_APIVAR_PRIVATE_DEF(libxsmm_malloc_function libxsmm_default_malloc_fn);\nLIBXSMM_APIVAR_PRIVATE_DEF(libxsmm_malloc_function libxsmm_scratch_malloc_fn);\nLIBXSMM_APIVAR_PRIVATE_DEF(libxsmm_free_function libxsmm_default_free_fn);\nLIBXSMM_APIVAR_PRIVATE_DEF(libxsmm_free_function libxsmm_scratch_free_fn);\nLIBXSMM_APIVAR_PRIVATE_DEF(const void* libxsmm_default_allocator_context);\nLIBXSMM_APIVAR_PRIVATE_DEF(const void* libxsmm_scratch_allocator_context);\nLIBXSMM_APIVAR_PRIVATE_DEF(unsigned int libxsmm_scratch_pools);\nLIBXSMM_APIVAR_PRIVATE_DEF(double libxsmm_scratch_scale);\nLIBXSMM_APIVAR_PRIVATE_DEF(double libxsmm_timer_scale);\nLIBXSMM_APIVAR_PRIVATE_DEF(unsigned int libxsmm_statistic_num_spmdm);\nLIBXSMM_APIVAR_PRIVATE_DEF(unsigned int libxsmm_thread_count);\n/* definition of corresponding variables */\nLIBXSMM_APIVAR_PUBLIC_DEF(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK) libxsmm_lock_global);\nLIBXSMM_APIVAR_PUBLIC_DEF(int libxsmm_nosync);\n\n#if (0 != LIBXSMM_SYNC)\nLIBXSMM_APIVAR_PRIVATE_DEF(LIBXSMM_TLS_TYPE libxsmm_tlskey);\n#endif\n\n\nLIBXSMM_API_INTERN void* libxsmm_memalign_internal(size_t alignment, size_t size)\n{\n  void* result;\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n  result = __libc_memalign(alignment, size);\n#elif defined(_WIN32) || defined(__CYGWIN__)\n  LIBXSMM_UNUSED(alignment);\n  result = malloc(size);\n#else\n  if (0 != posix_memalign(&result, alignment, size)) result = NULL;\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_memalign(size_t alignment, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    1 < libxsmm_ninit &&\n# endif\n    NULL != libxsmm_malloc_fn.memalign.ptr)\n  {\n    result = libxsmm_malloc_fn.memalign.ptr(alignment, size);\n  }\n  else\n#endif\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n  result = __libc_memalign(alignment, size);\n#else\n  result = libxsmm_memalign_internal(alignment, size);\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_malloc(size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_ALIGN)\n  const size_t alignment = libxsmm_alignment(size, 0/*auto*/);\n  result = __real_memalign(alignment, size);\n#else\n# if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n#   if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    1 < libxsmm_ninit &&\n#   endif\n    NULL != libxsmm_malloc_fn.malloc.ptr)\n  {\n    LIBXSMM_ASSERT(malloc != libxsmm_malloc_fn.malloc.ptr);\n    result = libxsmm_malloc_fn.malloc.ptr(size);\n  }\n  else\n# endif\n# if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n  result = __libc_malloc(size);\n# else\n  result = malloc(size);\n# endif\n#endif\n  return result;\n}\n\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_calloc(size_t num, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    1 < libxsmm_ninit &&\n# endif\n    NULL != libxsmm_malloc_fn.calloc.ptr)\n  {\n    LIBXSMM_ASSERT(calloc != libxsmm_malloc_fn.calloc.ptr);\n    result = libxsmm_malloc_fn.calloc.ptr(num, size);\n  }\n  else\n#endif\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n  result = __libc_calloc(num, size);\n#else\n  result = calloc(num, size);\n#endif\n  return result;\n}\n#endif\n\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_realloc(void* ptr, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    1 < libxsmm_ninit &&\n# endif\n    NULL != libxsmm_malloc_fn.realloc.ptr)\n  {\n    LIBXSMM_ASSERT(realloc != libxsmm_malloc_fn.realloc.ptr);\n    result = libxsmm_malloc_fn.realloc.ptr(ptr, size);\n  }\n  else\n#endif\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n  result = __libc_realloc(ptr, size);\n#else\n  result = realloc(ptr, size);\n#endif\n  return result;\n}\n#endif\n\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void __real_free(void* ptr)\n{\n  if (NULL != ptr) {\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n    if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n      1 < libxsmm_ninit &&\n# endif\n      NULL != libxsmm_malloc_fn.free.ptr)\n    {\n      LIBXSMM_ASSERT(free != libxsmm_malloc_fn.free.ptr);\n      libxsmm_malloc_fn.free.ptr(ptr);\n    }\n    else\n#endif\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n    __libc_free(ptr);\n#else\n    free(ptr);\n#endif\n  }\n}\n\n\nLIBXSMM_API_INLINE void internal_update_mmstatistic(const libxsmm_gemm_descriptor* desc,\n  unsigned int ntry, unsigned int ncol, unsigned int njit, unsigned int nsta)\n{\n  LIBXSMM_ASSERT(NULL != desc);\n  if (1 < desc->m && 1 < desc->n) { /* only record matrix-matrix multiplication */\n    const unsigned long long kernel_size = LIBXSMM_MNK_SIZE(desc->m, desc->n, desc->k);\n    const int idx = (LIBXSMM_GEMM_PRECISION_F64 == LIBXSMM_GETENUM_OUT(desc->datatype) ? 0 : 1);\n    int bucket;\n    if (LIBXSMM_MNK_SIZE(internal_statistic_sml, internal_statistic_sml, internal_statistic_sml) >= kernel_size) {\n      bucket = 0;\n    }\n    else if (LIBXSMM_MNK_SIZE(internal_statistic_med, internal_statistic_med, internal_statistic_med) >= kernel_size) {\n      bucket = 1;\n    }\n    else if (LIBXSMM_MNK_SIZE(internal_statistic_mnk, internal_statistic_mnk, internal_statistic_mnk) >= kernel_size) {\n      bucket = 2;\n    }\n    else { /*huge*/\n      bucket = 3;\n    }\n    if (0 != ncol) ncol/*dummy assignment*/ = LIBXSMM_ATOMIC_ADD_FETCH(&internal_statistic[idx][bucket].ncol, ncol, LIBXSMM_ATOMIC_RELAXED);\n    if (0 != ntry) ntry/*dummy assignment*/ = LIBXSMM_ATOMIC_ADD_FETCH(&internal_statistic[idx][bucket].ntry, ntry, LIBXSMM_ATOMIC_RELAXED);\n    /* the following counters are not manipulated concurrently (no need for atomic increment) */\n    if (0 != njit) internal_statistic[idx][bucket].njit += njit;\n    if (0 != nsta) internal_statistic[idx][bucket].nsta += nsta;\n  }\n}\n\n\nLIBXSMM_API_INLINE unsigned int internal_print_number(unsigned int n, char default_unit, char* unit)\n{\n  unsigned int number = n;\n  LIBXSMM_ASSERT(NULL != unit);\n  *unit = default_unit;\n  if ((1000000) <= n) {\n    number = (n + 500000) / 1000000;\n    *unit = 'm';\n  }\n  else if (9999 < n) {\n    number = (n + 500) / 1000;\n    *unit = 'k';\n  }\n  return number;\n}\n\n\nLIBXSMM_API_INLINE unsigned int internal_print_statistic(FILE* ostream,\n  const char* target_arch, int precision, unsigned int linebreaks, unsigned int indent)\n{\n  const internal_statistic_type statistic_sml = internal_statistic[precision][0/*SML*/];\n  const internal_statistic_type statistic_med = internal_statistic[precision][1/*MED*/];\n  const internal_statistic_type statistic_big = internal_statistic[precision][2/*BIG*/];\n  const internal_statistic_type statistic_xxx = internal_statistic[precision][3/*XXX*/];\n  int printed = 0;\n  LIBXSMM_ASSERT(NULL != ostream && (0 <= precision && precision < 2));\n\n  if (/* omit to print anything if it is superfluous */\n    0 != statistic_sml.ntry || 0 != statistic_sml.njit || 0 != statistic_sml.nsta || 0 != statistic_sml.ncol ||\n    0 != statistic_med.ntry || 0 != statistic_med.njit || 0 != statistic_med.nsta || 0 != statistic_med.ncol ||\n    0 != statistic_big.ntry || 0 != statistic_big.njit || 0 != statistic_big.nsta || 0 != statistic_big.ncol ||\n    0 != statistic_xxx.ntry || 0 != statistic_xxx.njit || 0 != statistic_xxx.nsta || 0 != statistic_xxx.ncol)\n  {\n    char title[256], range[256], unit[4];\n    unsigned int counter[4];\n    {\n      unsigned int n;\n      if (NULL != target_arch && 0 != *target_arch) {\n        assert(strlen(target_arch) < sizeof(title)); /* !LIBXSMM_ASSERT */\n        for (n = 0; 0 != target_arch[n] /*avoid code-gen. issue with some clang versions: && n < sizeof(title)*/; ++n) {\n          const char c = target_arch[n];\n          title[n] = (char)(('a' <= c && c <= 'z') ? (c - 32) : c); /* toupper */\n        }\n        LIBXSMM_SNPRINTF(title + n, sizeof(title) - n, \"/%s\", 0 == precision ? \"DP\" : \"SP\");\n      }\n      else {\n        LIBXSMM_SNPRINTF(title, sizeof(title), \"%s\", 0 == precision ? \"DP\" : \"SP\");\n      }\n      for (n = 0; n < linebreaks; ++n) fprintf(ostream, \"\\n\");\n    }\n    fprintf(ostream, \"%*s%-8s %6s %6s %6s %6s\\n\", (int)indent, \"\", title, \"TRY\", \"JIT\", \"STA\", \"COL\");\n    LIBXSMM_SNPRINTF(range, sizeof(range), \"%u..%u\", 0u, internal_statistic_sml);\n    counter[0] = internal_print_number(statistic_sml.ntry, ' ', unit + 0);\n    counter[1] = internal_print_number(statistic_sml.njit, ' ', unit + 1);\n    counter[2] = internal_print_number(statistic_sml.nsta, ' ', unit + 2);\n    counter[3] = internal_print_number(statistic_sml.ncol, ' ', unit + 3);\n    fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n      counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    LIBXSMM_SNPRINTF(range, sizeof(range), \"%u..%u\", internal_statistic_sml + 1u, internal_statistic_med);\n    counter[0] = internal_print_number(statistic_med.ntry, ' ', unit + 0);\n    counter[1] = internal_print_number(statistic_med.njit, ' ', unit + 1);\n    counter[2] = internal_print_number(statistic_med.nsta, ' ', unit + 2);\n    counter[3] = internal_print_number(statistic_med.ncol, ' ', unit + 3);\n    fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n      counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    LIBXSMM_SNPRINTF(range, sizeof(range), \"%u..%u\", internal_statistic_med + 1u, internal_statistic_mnk);\n    counter[0] = internal_print_number(statistic_big.ntry, ' ', unit + 0);\n    counter[1] = internal_print_number(statistic_big.njit, ' ', unit + 1);\n    counter[2] = internal_print_number(statistic_big.nsta, ' ', unit + 2);\n    counter[3] = internal_print_number(statistic_big.ncol, ' ', unit + 3);\n    fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n      counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    if (0 != statistic_xxx.ntry || 0 != statistic_xxx.njit || 0 != statistic_xxx.nsta || 0 != statistic_xxx.ncol) {\n      LIBXSMM_SNPRINTF(range, sizeof(range), \"> %u\", internal_statistic_mnk);\n      counter[0] = internal_print_number(statistic_xxx.ntry, ' ', unit + 0);\n      counter[1] = internal_print_number(statistic_xxx.njit, ' ', unit + 1);\n      counter[2] = internal_print_number(statistic_xxx.nsta, ' ', unit + 2);\n      counter[3] = internal_print_number(statistic_xxx.ncol, ' ', unit + 3);\n      fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n        counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    }\n    printed = 1;\n  }\n\n  return printed;\n}\n\n\n#if !(defined(_WIN32) || defined(__CYGWIN__))\nLIBXSMM_API_INLINE unsigned int internal_statistic_ntry(int precision)\n{\n  return internal_statistic[precision][0/*SML*/].ntry + internal_statistic[precision][1/*MED*/].ntry\n       + internal_statistic[precision][2/*BIG*/].ntry + internal_statistic[precision][3/*XXX*/].ntry;\n}\n#endif\n\n\n#if !defined(_WIN32)\nLIBXSMM_API_INLINE void internal_register_static_code(\n  libxsmm_gemm_precision precision, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  libxsmm_xmmfunction xgemm, libxsmm_code_pointer* registry)\n{\n  const libxsmm_blasint lda = m, ldb = k, ldc = m;\n  /*const*/ int precondition = LIBXSMM_GEMM_NO_BYPASS_DIMS(m, n, k) && LIBXSMM_GEMM_NO_BYPASS_DIMS(lda, ldb, ldc);\n  if (precondition) {\n    const size_t size = (LIBXSMM_HASH_SIZE) - sizeof(libxsmm_descriptor_kind);\n    libxsmm_descriptor_blob blob;\n    const libxsmm_gemm_descriptor *const desc = libxsmm_gemm_descriptor_dinit(&blob, precision,\n      m, n, k, lda, ldb, ldc, LIBXSMM_ALPHA, LIBXSMM_BETA, LIBXSMM_FLAGS, INTERNAL_PREFETCH);\n    unsigned int i = LIBXSMM_MOD2(\n      libxsmm_crc32(LIBXSMM_HASH_SEED, desc, LIBXSMM_MIN(sizeof(libxsmm_gemm_descriptor), size)),\n      LIBXSMM_CAPACITY_REGISTRY);\n    libxsmm_code_pointer* dst_entry = registry + i;\n#if !defined(NDEBUG)\n    libxsmm_code_pointer code; code.xgemm = xgemm;\n    LIBXSMM_ASSERT(NULL != code.ptr_const && NULL != registry);\n    LIBXSMM_ASSERT(0 == (LIBXSMM_CODE_STATIC & code.uval));\n#endif\n    if (NULL != dst_entry->ptr_const) { /* collision */\n      const unsigned int i0 = i;\n      do { /* continue to linearly search for an available slot */\n        i = LIBXSMM_MOD2(i + 1, LIBXSMM_CAPACITY_REGISTRY);\n        if (NULL == registry[i].ptr_const) break;\n      } while (i != i0);\n#if defined(LIBXSMM_HASH_COLLISION) /* mark entry as a collision */\n      dst_entry->uval |= LIBXSMM_HASH_COLLISION;\n#endif\n      dst_entry = registry + i; /* update destination */\n      internal_update_mmstatistic(desc, 0, 1/*collision*/, 0, 0);\n      /* out of capacity (no registry slot available) */\n      LIBXSMM_ASSERT(NULL == dst_entry->ptr_const || i == i0);\n    }\n    if (NULL == dst_entry->ptr_const) { /* registry not exhausted */\n      internal_registry_keys[i].kind = LIBXSMM_KERNEL_KIND_MATMUL;\n      LIBXSMM_ASSIGN127(&internal_registry_keys[i].gemm.desc, desc);\n      dst_entry->xgemm = xgemm;\n      /* mark current entry as static code (non-JIT) */\n      dst_entry->uval |= LIBXSMM_CODE_STATIC;\n    }\n    internal_update_mmstatistic(desc, 1/*try*/, 0, 0, 0);\n  }\n}\n#endif\n\n\nLIBXSMM_API_INTERN void internal_release_scratch(void);\nLIBXSMM_API_INTERN void internal_release_scratch(void)\n{\n  libxsmm_xrelease_scratch(NULL/*lock*/);\n  /* release global services */\n  libxsmm_memory_finalize();\n  libxsmm_hash_finalize();\n  libxsmm_malloc_finalize();\n}\n\n\n/* Caution: cannot be used multiple time in a single expression! */\nLIBXSMM_API_INTERN size_t libxsmm_format_size(char buffer[32], int buffer_size, size_t nbytes, const char scale[], const char* unit, int base)\n{\n  const int len = (NULL != scale ? ((int)strlen(scale)) : 0);\n  const int m = LIBXSMM_INTRINSICS_BITSCANBWD64(nbytes) / base, n = LIBXSMM_MIN(m, len);\n  int i;\n  buffer[0] = 0; /* clear */\n  LIBXSMM_ASSERT(NULL != unit && 0 <= base);\n  for (i = 0; i < n; ++i) nbytes >>= base;\n  LIBXSMM_SNPRINTF(buffer, buffer_size, \"%i %c%s\",\n    (int)nbytes, 0 < n ? scale[n-1] : *unit, 0 < n ? unit : \"\");\n  return nbytes;\n}\n\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_NO_TRACE void internal_dump(FILE* ostream, int urgent);\nLIBXSMM_API_INTERN void internal_dump(FILE* ostream, int urgent)\n{\n  char *const env_dump_build = getenv(\"LIBXSMM_DUMP_BUILD\");\n  char *const env_dump_files = (NULL != getenv(\"LIBXSMM_DUMP_FILES\")\n    ? getenv(\"LIBXSMM_DUMP_FILES\")\n    : getenv(\"LIBXSMM_DUMP_FILE\"));\n  LIBXSMM_ASSERT_MSG(INTERNAL_SINGLETON(internal_singleton_handle), \"Invalid handle\");\n  /* determine whether this instance is unique or not */\n  if (NULL != env_dump_files && 0 != *env_dump_files && 0 == urgent) { /* dump per-node info */\n    const char* filename = strtok(env_dump_files, INTERNAL_DELIMS);\n    for (; NULL != filename; filename = strtok(NULL, INTERNAL_DELIMS)) {\n      FILE* const file = fopen(filename, \"r\");\n      if (NULL != file) {\n        int c = fgetc(file);\n        fprintf(ostream, \"\\n\\nLIBXSMM_DUMP_FILE: %s\\n\", filename);\n        /* coverity[tainted_data] */\n        while (EOF != c) {\n          fputc(c, stdout);\n          c = fgetc(file);\n        }\n        fputc('\\n', stdout);\n        fclose(file);\n      }\n    }\n  }\n  if  (NULL != internal_build_state /* dump build state */\n    && NULL != env_dump_build && 0 != *env_dump_build)\n  {\n    const int dump_build = atoi(env_dump_build);\n    if (0 == urgent ? (0 < dump_build) : (0 > dump_build)) {\n      fprintf(ostream, \"\\n\\nBUILD_DATE=%i\\n\", LIBXSMM_CONFIG_BUILD_DATE);\n      fprintf(ostream, \"%s\\n\", internal_build_state);\n    }\n  }\n}\n\n\nLIBXSMM_API_INTERN void internal_finalize(void);\nLIBXSMM_API_INTERN void internal_finalize(void)\n{\n  libxsmm_finalize();\n  LIBXSMM_STDIO_ACQUIRE(); /* synchronize I/O */\n  if (0 != libxsmm_verbosity) { /* print statistic on termination */\n    const char *const env_target_hidden = getenv(\"LIBXSMM_TARGET_HIDDEN\");\n    const char *const target_arch = (NULL == env_target_hidden || 0 == atoi(env_target_hidden))\n      ? libxsmm_cpuid_name(libxsmm_target_archid) : NULL/*hidden*/;\n    fprintf(stderr, \"\\nLIBXSMM_VERSION: %s%s%s (%i)\", LIBXSMM_BRANCH,\n      0 != *(LIBXSMM_BRANCH) ? \"-\" : \"\", 0 != *(LIBXSMM_VERSION) ? (LIBXSMM_VERSION) : \"unconfigured\",\n      LIBXSMM_VERSION4(LIBXSMM_VERSION_MAJOR, LIBXSMM_VERSION_MINOR, LIBXSMM_VERSION_UPDATE, LIBXSMM_VERSION_PATCH));\n    if (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) {\n      unsigned int linebreak = (0 == internal_print_statistic(stderr, target_arch, 1/*SP*/, 1, 0)) ? 1 : 0;\n      const int high_verbosity = (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity);\n      size_t size_scratch = 0, size_private = 0;\n      libxsmm_scratch_info scratch_info;\n      libxsmm_cpuid_x86_info info;\n      libxsmm_cpuid_x86(&info);\n      if ((LIBXSMM_VERBOSITY_HIGH < libxsmm_verbosity || 0 > libxsmm_verbosity) &&\n        0 == internal_cpuid_info.has_context && 0 != info.has_context)\n      {\n        fprintf(stderr, \"\\nLIBXSMM: CPU features have been promoted.\");\n      }\n      if (0 == internal_print_statistic(stderr, target_arch, 0/*DP*/, linebreak, 0) && 0 != linebreak && NULL != target_arch) {\n        fprintf(stderr, \"\\nLIBXSMM_TARGET: %s\\n\", target_arch);\n      }\n      if (EXIT_SUCCESS == libxsmm_get_scratch_info(&scratch_info)) {\n        size_private = scratch_info.internal;\n        size_scratch = scratch_info.size;\n      }\n      if (0 != size_private) { /* should be always true */\n        char size_private_buffer[32], size_code_buffer[32];\n        /* coverity[check_return] */\n        libxsmm_format_size(size_private_buffer, sizeof(size_private_buffer), size_private, \"KM\", \"B\", 10);\n        fprintf(stderr, \"Registry and code: %s\", size_private_buffer);\n        if (0 != libxsmm_format_size(size_code_buffer, sizeof(size_code_buffer), internal_registry_nbytes, \"KM\", \"B\", 10)) {\n          fprintf(stderr, \" + %s\", size_code_buffer);\n        }\n      }\n      if (0 != high_verbosity) {\n        unsigned int ngemms = 0;\n        int i; for (i = 0; i < 4; ++i) {\n          ngemms += internal_statistic[0/*DP*/][i].nsta + internal_statistic[1/*SP*/][i].nsta;\n          ngemms += internal_statistic[0/*DP*/][i].njit + internal_statistic[1/*SP*/][i].njit;\n        }\n        if (0 != ngemms || 0 != internal_statistic_num_gemv\n          || 0 != internal_statistic_num_mcopy || 0 != internal_statistic_num_tcopy\n          || 0 != libxsmm_statistic_num_spmdm\n          || 0 != internal_statistic_num_user\n          || 0 != internal_registry_nleaks)\n        {\n          const char sep[] = \" \", *s = \"\";\n          fprintf(stderr, \" (\");\n          if (0 != ngemms) { fprintf(stderr, \"gemm=%u\", ngemms); s = sep; }\n          if (0 != internal_statistic_num_gemv) { fprintf(stderr, \"%sgemv=%u\", s, internal_statistic_num_gemv); s = sep; }\n          if (0 != internal_statistic_num_mcopy) { fprintf(stderr, \"%smcopy=%u\", s, internal_statistic_num_mcopy); s = sep; }\n          if (0 != internal_statistic_num_meltw) { fprintf(stderr, \"%smeltw=%u\", s, internal_statistic_num_meltw); s = sep; }\n          if (0 != internal_statistic_num_tcopy) { fprintf(stderr, \"%stcopy=%u\", s, internal_statistic_num_tcopy); s = sep; }\n          if (0 != libxsmm_statistic_num_spmdm) { fprintf(stderr, \"%sspmdm=%u\", s, libxsmm_statistic_num_spmdm); s = sep; }\n          if (0 != internal_statistic_num_user) { fprintf(stderr, \"%suser=%u\", s, internal_statistic_num_user); s = sep; }\n          if (0 != internal_registry_nleaks) { fprintf(stderr, \"%snleaks=%u\", s, internal_registry_nleaks); s = sep; }\n          fprintf(stderr, \")\");\n        }\n      }\n      fprintf(stderr, \"\\n\");\n      if (0 != size_scratch) {\n        char size_scratch_buffer[32];\n        /* coverity[check_return] */\n        libxsmm_format_size(size_scratch_buffer, sizeof(size_scratch_buffer), size_scratch, \"KM\", \"B\", 10);\n        fprintf(stderr, \"Scratch: %s\", size_scratch_buffer);\n        if (0 != high_verbosity) {\n          fprintf(stderr, \" (mallocs=%lu, pools=%u)\\n\", (unsigned long int)scratch_info.nmallocs, scratch_info.npools);\n        }\n        else {\n          fprintf(stderr, \"\\n\");\n        }\n      }\n      if (LIBXSMM_VERBOSITY_HIGH < libxsmm_verbosity || 0 > libxsmm_verbosity) {\n        fprintf(stderr, \"Uptime: %f s\", libxsmm_timer_duration(internal_timer_start, libxsmm_timer_tick()));\n        if (1 < libxsmm_thread_count && INT_MAX == libxsmm_verbosity) {\n          fprintf(stderr, \" (nthreads=%u)\", libxsmm_thread_count);\n        }\n        fprintf(stderr, \"\\n\");\n      }\n    }\n    else {\n      fprintf(stderr, \"\\nLIBXSMM_TARGET: %s\\n\", target_arch);\n    }\n  }\n  /* release scratch memory pool */\n  if (EXIT_SUCCESS != atexit(internal_release_scratch) && 0 != libxsmm_verbosity) {\n    fprintf(stderr, \"LIBXSMM ERROR: failed to perform final cleanup!\\n\");\n  }\n  /* determine whether this instance is unique or not */\n  if (INTERNAL_SINGLETON(internal_singleton_handle)) {\n    internal_dump(stdout, 0/*urgent*/);\n    /* cleanup singleton */\n#if defined(_WIN32)\n    ReleaseMutex(internal_singleton_handle);\n    CloseHandle(internal_singleton_handle);\n#else\n    unlink(internal_singleton_fname);\n    close(internal_singleton_handle);\n#endif\n  }\n  LIBXSMM_STDIO_RELEASE(); /* synchronize I/O */\n#if (0 != LIBXSMM_SYNC)\n  { /* release locks */\n# if (1 < INTERNAL_REGLOCK_MAXN)\n    int i; for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_DESTROY(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n    LIBXSMM_LOCK_DESTROY(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n    LIBXSMM_LOCK_DESTROY(LIBXSMM_LOCK, &libxsmm_lock_global);\n  }\n#endif\n}\n\n\n#if defined(LIBXSMM_INTERCEPT_DYNAMIC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void _gfortran_stop_string(const char* /*message*/, int /*len*/, int /*quiet*/);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void _gfortran_stop_string(const char* message, int len, int quiet)\n{ /* STOP termination handler for GNU Fortran runtime */\n  static int once = 0;\n  if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n    union { const void* dlsym; void (*ptr)(const char*, int, int); } stop;\n    dlerror(); /* clear an eventual error status */\n    stop.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"_gfortran_stop_string\");\n    if (NULL != stop.dlsym) {\n      stop.ptr(message, len, quiet);\n    }\n    else exit(EXIT_SUCCESS); /* statically linked runtime */\n  }\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core(const char* /*message*/, int /*len*/);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core(const char* message, int len)\n{ /* STOP termination handler for Intel Fortran runtime */\n  static int once = 0;\n  if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n    union { const void* dlsym; void (*ptr)(const char*, int); } stop;\n    dlerror(); /* clear an eventual error status */\n    stop.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"for_stop_core\");\n    if (NULL != stop.dlsym) {\n      stop.ptr(message, len);\n    }\n    else exit(EXIT_SUCCESS); /* statically linked runtime */\n  }\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core_quiet(void);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core_quiet(void)\n{ /* STOP termination handler for Intel Fortran runtime */\n  static int once = 0;\n  if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n    union { const void* dlsym; void (*ptr)(void); } stop;\n    dlerror(); /* clear an eventual error status */\n    stop.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"for_stop_core_quiet\");\n    if (NULL != stop.dlsym) {\n      stop.ptr();\n    }\n    else exit(EXIT_SUCCESS); /* statically linked runtime */\n  }\n}\n#endif\n\n\nLIBXSMM_API_INTERN size_t internal_strlen(const char* /*cstr*/, size_t /*maxlen*/);\nLIBXSMM_API_INTERN size_t internal_strlen(const char* cstr, size_t maxlen)\n{\n  size_t result = 0;\n  if (NULL != cstr) {\n    while (0 != cstr[result] && result < maxlen) ++result;\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN size_t internal_parse_nbytes(const char* /*nbytes*/, size_t /*ndefault*/);\nLIBXSMM_API_INTERN size_t internal_parse_nbytes(const char* nbytes, size_t ndefault)\n{\n  size_t result = ndefault;\n  if (NULL != nbytes && 0 != *nbytes) {\n    size_t u = internal_strlen(nbytes, 32) - 1;\n    const char unit[] = \"kmgKMG\", *const hit = strchr(unit, nbytes[u]);\n    const long long int ibytes = atol(nbytes); /* take with increased type-width */\n    result = (size_t)ibytes;\n    if ((size_t)LIBXSMM_UNLIMITED != result) {\n      u = (0 != hit ? ((hit - unit) % 3) : 3);\n      if (u < 3) {\n        result <<= (u + 1) * 10;\n      }\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_NO_TRACE void internal_init(void);\nLIBXSMM_API_INTERN void internal_init(void)\n{\n  int i;\n#if (0 != LIBXSMM_SYNC) /* setup the locks in a thread-safe fashion */\n  LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, &libxsmm_lock_global);\n# if (1 < INTERNAL_REGLOCK_MAXN)\n  for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_ACQUIRE(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n  LIBXSMM_LOCK_ACQUIRE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n#endif\n  if (NULL == internal_registry) { /* double-check after acquiring the lock(s) */\n#if defined(LIBXSMM_INTERCEPT_DYNAMIC) && defined(LIBXSMM_AUTOPIN)\n    /* clear error status (dummy condition: it does not matter if MPI_Init or MPI_Abort) */\n    const char* const dlsymname = (NULL == dlerror() ? \"MPI_Init\" : \"MPI_Abort\");\n    const void* const dlsymbol = dlsym(LIBXSMM_RTLD_NEXT, dlsymname);\n    const void* const dlmpi = (NULL == dlerror() ? dlsymbol : NULL);\n#endif\n    const char* const env_verbose = getenv(\"LIBXSMM_VERBOSE\");\n    void* new_registry = NULL, * new_keys = NULL;\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n# if defined(LIBXSMM_NTHREADS_USE)\n    void* new_cache = NULL;\n# endif\n    const char* const env_cache = getenv(\"LIBXSMM_CACHE\");\n    if (NULL != env_cache && 0 != *env_cache) {\n      const int cache_size = atoi(env_cache), cache_size2 = LIBXSMM_UP2POT(cache_size);\n      internal_cache_size = LIBXSMM_MIN(cache_size2, LIBXSMM_CACHE_MAXSIZE);\n    }\n    else {\n      internal_cache_size = LIBXSMM_CACHE_MAXSIZE;\n    }\n#endif\n    /* setup verbosity as early as possible since below code may rely on verbose output */\n    if (NULL != env_verbose && 0 != *env_verbose) {\n      libxsmm_verbosity = atoi(env_verbose);\n    }\n#if !defined(NDEBUG)\n    else {\n      libxsmm_verbosity = INT_MAX; /* quiet -> verbose */\n    }\n#endif\n#if (0 == LIBXSMM_JIT)\n    if (2 > libxsmm_ninit && (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)) {\n      fprintf(stderr, \"LIBXSMM: JIT-code generation was disabled at compile-time.\\n\");\n    }\n#endif\n#if defined(LIBXSMM_AUTOPIN)\n# if defined(LIBXSMM_INTERCEPT_DYNAMIC)\n    /* MPI: unwanted affinity can slow-down unrelated jobs (over-subscription), e.g., CP2K regtests */\n    if (NULL == dlmpi)\n# endif\n    { /* setup some viable affinity if nothing else is present */\n      const char *const gomp_cpu_affinity = getenv(\"GOMP_CPU_AFFINITY\");\n      const char *const kmp_affinity = getenv(\"KMP_AFFINITY\");\n      const char *const omp_proc_bind = getenv(\"OMP_PROC_BIND\");\n      if  ((NULL == gomp_cpu_affinity || 0 == *gomp_cpu_affinity)\n        && (NULL == kmp_affinity || 0 == *kmp_affinity)\n        && (NULL == omp_proc_bind || 0 == *omp_proc_bind))\n      {\n        static char affinity[] = \"OMP_PROC_BIND=TRUE\";\n        LIBXSMM_EXPECT(EXIT_SUCCESS, LIBXSMM_PUTENV(affinity));\n        if (LIBXSMM_VERBOSITY_HIGH < libxsmm_verbosity || 0 > libxsmm_verbosity) { /* library code is expected to be mute */\n          fprintf(stderr, \"LIBXSMM: prepared to pin threads.\\n\");\n        }\n      }\n    }\n# if defined(LIBXSMM_INTERCEPT_DYNAMIC) && defined(LIBXSMM_MALLOC)\n    else if (NULL == getenv(\"I_MPI_SHM_HEAP\")) {\n      static char shmheap[] = \"I_MPI_SHM_HEAP=1\";\n      LIBXSMM_EXPECT(EXIT_SUCCESS, LIBXSMM_PUTENV(shmheap));\n    }\n# endif\n#endif\n#if !defined(_WIN32) && 0\n    umask(S_IRUSR | S_IWUSR); /* setup default/secure file mask */\n#endif\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    { const char *const env = getenv(\"LIBXSMM_SCRATCH_POOLS\");\n      if (NULL == env || 0 == *env) {\n        libxsmm_scratch_pools = LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS;\n      }\n      else {\n        libxsmm_scratch_pools = LIBXSMM_CLMP(atoi(env), 0, LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n        /*libxsmm_scratch_pools_locked = 1;*/\n      }\n      LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n    }\n    { const char *const env = getenv(\"LIBXSMM_SCRATCH_SCALE\");\n      if (NULL == env || 0 == *env) {\n        libxsmm_scratch_scale = LIBXSMM_MALLOC_SCRATCH_SCALE;\n      }\n      else {\n        libxsmm_scratch_scale = LIBXSMM_CLMP(atof(env), 1.0, 10.0);\n        /*libxsmm_scratch_scale_locked = 1;*/\n      }\n      LIBXSMM_ASSERT(1 <= libxsmm_scratch_scale);\n    }\n    libxsmm_set_scratch_limit(internal_parse_nbytes(getenv(\"LIBXSMM_SCRATCH_LIMIT\"), LIBXSMM_SCRATCH_DEFAULT));\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n    { /* setup malloc-interception after internal allocations */\n      const libxsmm_malloc_function null_malloc_fn = { 0 };\n      const libxsmm_free_function null_free_fn = { 0 };\n      const char *const env_k = getenv(\"LIBXSMM_MALLOC\");\n      char *const env_t = getenv(\"LIBXSMM_MALLOC_LIMIT\");\n      const char* env_i = (NULL != env_t ? strtok(env_t, INTERNAL_DELIMS) : NULL);\n      const size_t malloc_lo = internal_parse_nbytes(env_i, LIBXSMM_MALLOC_LIMIT);\n      const size_t malloc_hi = (NULL != env_i ? internal_parse_nbytes(\n        strtok(NULL, INTERNAL_DELIMS), LIBXSMM_SCRATCH_UNLIMITED) : LIBXSMM_SCRATCH_UNLIMITED);\n      const int malloc_kind = ((NULL == env_k || 0 == *env_k) ? 0/*disabled*/ : atoi(env_k));\n      libxsmm_xset_default_allocator(NULL/*lock*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n      libxsmm_xset_scratch_allocator(NULL/*lock*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n      libxsmm_set_malloc(malloc_kind, &malloc_lo, &malloc_hi); /* implies libxsmm_malloc_init */\n    }\n#if defined(LIBXSMM_MAXTARGET)\n    libxsmm_set_target_arch(LIBXSMM_STRINGIFY(LIBXSMM_MAXTARGET));\n#else /* attempt to set libxsmm_target_archid per environment variable */\n    libxsmm_set_target_arch(getenv(\"LIBXSMM_TARGET\"));\n#endif\n    { const char *const env = getenv(\"LIBXSMM_SYNC\");\n      libxsmm_nosync = (NULL == env || 0 == *env) ? 0/*default*/ : atoi(env);\n    }\n    /* clear internal counters/statistic */\n    for (i = 0; i < 4/*sml/med/big/xxx*/; ++i) {\n      LIBXSMM_MEMZERO127(&internal_statistic[0/*DP*/][i]);\n      LIBXSMM_MEMZERO127(&internal_statistic[1/*SP*/][i]);\n    }\n    internal_statistic_mnk = LIBXSMM_MAX_DIM;\n    internal_statistic_sml = 13;\n    internal_statistic_med = 23;\n    LIBXSMM_ASSERT(LIBXSMM_CAPACITY_REGISTRY == LIBXSMM_UP2POT(LIBXSMM_CAPACITY_REGISTRY));\n    libxsmm_hash_init(libxsmm_target_archid); /* used by debug memory allocation (checksum) */\n    libxsmm_memory_init(libxsmm_target_archid);\n    if (\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      (EXIT_SUCCESS == libxsmm_xmalloc(&new_cache, /* if internal_cache_size is zero, allocation must still happen (later control-flow too expensive) */\n        sizeof(internal_cache_type) * (LIBXSMM_NTHREADS_MAX), LIBXSMM_CACHELINE/*alignment*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, NULL/*extra*/, 0/*extra-size*/) && NULL != new_cache) &&\n#endif\n      (EXIT_SUCCESS == libxsmm_xmalloc(&new_keys, (LIBXSMM_CAPACITY_REGISTRY) * sizeof(libxsmm_descriptor), 0/*auto-align*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, NULL/*extra*/, 0/*extra-size*/) && NULL != new_keys) &&\n      (EXIT_SUCCESS == libxsmm_xmalloc(&new_registry, (LIBXSMM_CAPACITY_REGISTRY) * sizeof(libxsmm_code_pointer), 0/*auto-align*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, NULL/*extra*/, 0/*extra-size*/) && NULL != new_registry))\n    {\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      LIBXSMM_ASSERT(NULL != new_cache); /* SA: suppress false positive */\n      memset(new_cache, 0, (LIBXSMM_NTHREADS_MAX) * sizeof(internal_cache_type));\n#endif\n      libxsmm_xcopy_init(libxsmm_target_archid);\n      libxsmm_dnn_init(libxsmm_target_archid);\n#if defined(LIBXSMM_PERF)\n      libxsmm_perf_init();\n#endif\n      { const char *const env = getenv(\"LIBXSMM_GEMM_PREFETCH\");\n#if defined(_WIN32) || defined(__CYGWIN__)\n        libxsmm_gemm_auto_prefetch_default = INTERNAL_PREFETCH;\n#else\n        libxsmm_gemm_auto_prefetch_default = (0 == internal_statistic_ntry(0/*DP*/) && 0 == internal_statistic_ntry(1/*SP*/))\n          /* avoid special prefetch if static code is present, since such code uses INTERNAL_PREFETCH */\n          ? (((LIBXSMM_X86_AVX512 >= libxsmm_target_archid || LIBXSMM_X86_AVX512_CORE <= libxsmm_target_archid))\n            ? LIBXSMM_GEMM_PREFETCH_AL2BL2_VIA_C : LIBXSMM_GEMM_PREFETCH_BL2_VIA_C)\n          : INTERNAL_PREFETCH;\n#endif\n        libxsmm_gemm_auto_prefetch = INTERNAL_PREFETCH;\n        if (NULL != env && 0 != *env) { /* user input beyond auto-prefetch is always considered */\n          const int uid = atoi(env);\n          if (0 <= uid) {\n            libxsmm_gemm_auto_prefetch_default = libxsmm_gemm_uid2prefetch(uid);\n            libxsmm_gemm_auto_prefetch = libxsmm_gemm_auto_prefetch_default;\n            internal_gemm_auto_prefetch_locked = 1;\n          }\n        }\n      }\n      for (i = 0; i < (LIBXSMM_CAPACITY_REGISTRY); ++i) ((libxsmm_code_pointer*)new_registry)[i].ptr = NULL;\n      LIBXSMM_ASSERT(NULL == internal_registry && NULL == internal_registry_keys);\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      LIBXSMM_ASSERT(NULL == internal_cache_buffer);\n      internal_cache_buffer = (internal_cache_type*)new_cache;\n#endif\n      internal_registry_keys = (libxsmm_descriptor*)new_keys; /* prior to registering static kernels */\n#if defined(LIBXSMM_BUILD) && !defined(LIBXSMM_DEFAULT_CONFIG)\n#     include <libxsmm_dispatch.h>\n#endif\n      libxsmm_gemm_init(libxsmm_target_archid);\n#if defined(LIBXSMM_TRACE)\n      { int filter_threadid = 0/*only main-thread*/, filter_mindepth = 0, filter_maxnsyms = 0;\n        const int init_code = libxsmm_trace_init(filter_threadid, filter_mindepth, filter_maxnsyms);\n        if (EXIT_SUCCESS != init_code && 0 != libxsmm_verbosity) { /* library code is expected to be mute */\n          fprintf(stderr, \"LIBXSMM ERROR: failed to initialize TRACE (error #%i)!\\n\", init_code);\n        }\n      }\n#endif\n      { /* commit the registry buffer and enable global visibility */\n        void *const pv_registry = &internal_registry;\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)((void**)pv_registry, (void*)new_registry, LIBXSMM_ATOMIC_SEQ_CST);\n      }\n    }\n    else {\n      if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n        fprintf(stderr, \"LIBXSMM ERROR: failed to allocate internal buffers!\\n\");\n      }\n      libxsmm_xfree(new_registry, 0/*no check*/);\n      libxsmm_xfree(new_keys, 0/*no check*/);\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      libxsmm_xfree(new_cache, 0/*no check*/);\n#endif\n    }\n  }\n#if (0 != LIBXSMM_SYNC) /* release locks */\n# if (1 < INTERNAL_REGLOCK_MAXN)\n  for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n  LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n  LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, &libxsmm_lock_global);\n#endif\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_CTOR void libxsmm_init(void)\n{\n  if (0 == LIBXSMM_ATOMIC_LOAD(&internal_registry, LIBXSMM_ATOMIC_RELAXED)) {\n    static unsigned int ninit = 0, gid = 0;\n    const unsigned int tid = LIBXSMM_ATOMIC_ADD_FETCH(&ninit, 1, LIBXSMM_ATOMIC_SEQ_CST);\n    LIBXSMM_ASSERT(0 < tid);\n    /* libxsmm_ninit (1: initialization started, 2: library initialized, higher: to invalidate code-TLS) */\n    if (1 == tid) {\n      libxsmm_timer_tickint s0 = libxsmm_timer_tick_rtc(); /* warm-up */\n      libxsmm_timer_tickint t0 = libxsmm_timer_tick_tsc(); /* warm-up */\n      s0 = libxsmm_timer_tick_rtc(); t0 = libxsmm_timer_tick_tsc(); /* start timing */\n      assert(0 == LIBXSMM_ATOMIC_LOAD(&libxsmm_ninit, LIBXSMM_ATOMIC_SEQ_CST)); /* !LIBXSMM_ASSERT */\n      /* coverity[check_return] */\n      LIBXSMM_ATOMIC_ADD_FETCH(&libxsmm_ninit, 1, LIBXSMM_ATOMIC_SEQ_CST);\n      gid = tid; /* protect initialization */\n#if (0 != LIBXSMM_SYNC)\n      /* coverity[check_return] */\n      LIBXSMM_TLS_CREATE(&libxsmm_tlskey);\n      { /* construct and initialize locks */\n# if defined(LIBXSMM_REGLOCK_TRY)\n        const char *const env_trylock = getenv(\"LIBXSMM_TRYLOCK\");\n# endif\n        LIBXSMM_LOCK_ATTR_TYPE(LIBXSMM_LOCK) attr_global;\n# if (1 < INTERNAL_REGLOCK_MAXN)\n        int i;\n        LIBXSMM_LOCK_ATTR_TYPE(LIBXSMM_REGLOCK) attr;\n        LIBXSMM_LOCK_ATTR_INIT(LIBXSMM_REGLOCK, &attr);\n# elif defined(LIBXSMM_UNIFY_LOCKS)\n        internal_reglock_ptr = &libxsmm_lock_global;\n# else\n        static LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK) internal_reglock;\n        internal_reglock_ptr = &internal_reglock;\n        LIBXSMM_LOCK_ATTR_TYPE(LIBXSMM_REGLOCK) attr;\n        LIBXSMM_LOCK_ATTR_INIT(LIBXSMM_REGLOCK, &attr);\n        LIBXSMM_LOCK_INIT(LIBXSMM_REGLOCK, internal_reglock_ptr, &attr);\n        LIBXSMM_LOCK_ATTR_DESTROY(LIBXSMM_REGLOCK, &attr);\n# endif\n        LIBXSMM_LOCK_ATTR_INIT(LIBXSMM_LOCK, &attr_global);\n        LIBXSMM_LOCK_INIT(LIBXSMM_LOCK, &libxsmm_lock_global, &attr_global);\n        LIBXSMM_LOCK_ATTR_DESTROY(LIBXSMM_LOCK, &attr_global);\n        /* control number of locks needed; LIBXSMM_TRYLOCK implies only 1 lock */\n# if defined(LIBXSMM_REGLOCK_TRY)\n        if (NULL == env_trylock || 0 == *env_trylock)\n# endif\n        { /* no LIBXSMM_TRYLOCK */\n# if defined(LIBXSMM_VTUNE)\n          internal_reglock_count = 1; /* avoid duplicated kernels */\n# elif (1 < INTERNAL_REGLOCK_MAXN)\n          const char *const env_nlocks = getenv(\"LIBXSMM_NLOCKS\");\n          const int reglock_count = (NULL == env_nlocks || 0 == *env_nlocks || 1 > atoi(env_nlocks))\n            ? (INTERNAL_REGLOCK_MAXN) : LIBXSMM_MIN(atoi(env_nlocks), INTERNAL_REGLOCK_MAXN);\n          internal_reglock_count = LIBXSMM_LO2POT(reglock_count);\n# else\n          internal_reglock_count = 0;\n# endif\n        }\n# if defined(LIBXSMM_REGLOCK_TRY)\n        else { /* LIBXSMM_TRYLOCK environment variable specified */\n          internal_reglock_count = (0 != atoi(env_trylock) ? 1\n#   if (1 < INTERNAL_REGLOCK_MAXN)\n            : INTERNAL_REGLOCK_MAXN);\n#   else\n            : 0);\n#   endif\n        }\n# endif\n# if (1 < INTERNAL_REGLOCK_MAXN)\n        LIBXSMM_ASSERT(1 <= internal_reglock_count);\n        for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_INIT(LIBXSMM_REGLOCK, &internal_reglock[i].state, &attr);\n        LIBXSMM_LOCK_ATTR_DESTROY(LIBXSMM_REGLOCK, &attr);\n# endif\n      }\n#endif\n      { /* determine whether this instance is unique or not */\n#if defined(_WIN32)\n        internal_singleton_handle = CreateMutex(NULL, TRUE, \"GlobalLIBXSMM\");\n#else\n        const int result = LIBXSMM_SNPRINTF(internal_singleton_fname, sizeof(internal_singleton_fname), \"/tmp/.libxsmm.%u\",\n          /*rely on user id to avoid permission issues in case of left-over files*/(unsigned int)getuid());\n        struct flock singleton_flock;\n        int singleton_handle;\n        singleton_flock.l_start = 0;\n        singleton_flock.l_len = 0; /* entire file */\n        singleton_flock.l_type = F_WRLCK; /* exclusive across PIDs */\n        singleton_flock.l_whence = SEEK_SET;\n        singleton_handle = ((0 < result && (int)sizeof(internal_singleton_fname) > result) ? open(\n          internal_singleton_fname, O_WRONLY | O_CREAT, S_IRUSR | S_IWUSR) : -1);\n        internal_singleton_handle = fcntl(singleton_handle, F_SETLK, &singleton_flock);\n        if (0 > internal_singleton_handle && 0 <= singleton_handle) close(singleton_handle);\n#endif  /* coverity[leaked_handle] */\n      }\n      { /* calibrate timer */\n        int register_termination_proc;\n        libxsmm_timer_tickint s1, t1;\n        internal_init(); /* must be first to initialize verbosity, etc. */\n        if (INTERNAL_SINGLETON(internal_singleton_handle)) { /* after internal_init */\n          internal_dump(stdout, 1/*urgent*/);\n        }\n        s1 = libxsmm_timer_tick_rtc(); t1 = libxsmm_timer_tick_tsc(); /* mid-timing */\n        libxsmm_cpuid_x86(&internal_cpuid_info);\n        if (0 != internal_cpuid_info.constant_tsc && t0 < t1) {\n          libxsmm_timer_scale = libxsmm_timer_duration_rtc(s0, s1) / (t1 - t0);\n        }\n        register_termination_proc = atexit(internal_finalize);\n        s1 = libxsmm_timer_tick_rtc(); t1 = libxsmm_timer_tick_tsc(); /* final timing */\n        /* set timer-scale and determine start of the \"uptime\" (shown at termination) */\n        if (t0 < t1 && 0.0 < libxsmm_timer_scale) {\n          const double scale = libxsmm_timer_duration_rtc(s0, s1) / (t1 - t0);\n          const double diff = LIBXSMM_DELTA(libxsmm_timer_scale, scale) / scale;\n          if (5E-5 > diff) {\n            libxsmm_timer_scale = scale;\n            internal_timer_start = t0;\n          }\n          else {\n            libxsmm_timer_scale = 0;\n            internal_timer_start = s0;\n#if !defined(NDEBUG)\n            libxsmm_se = 1;\n#endif\n          }\n        }\n        else {\n          internal_timer_start = s0;\n          libxsmm_timer_scale = 0;\n        }\n        if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n          if (EXIT_SUCCESS != register_termination_proc) {\n            fprintf(stderr, \"LIBXSMM ERROR: failed to register termination procedure!\\n\");\n          }\n          if (0 == libxsmm_timer_scale) {\n            fprintf(stderr, \"LIBXSMM WARNING: timer is maybe not cycle-accurate!\\n\");\n          }\n        }\n      }\n      assert(1 == LIBXSMM_ATOMIC_LOAD(&libxsmm_ninit, LIBXSMM_ATOMIC_SEQ_CST)); /* !LIBXSMM_ASSERT */\n      /* coverity[check_return] */\n      LIBXSMM_ATOMIC_ADD_FETCH(&libxsmm_ninit, 1, LIBXSMM_ATOMIC_SEQ_CST);\n    }\n    else /*if (gid != tid)*/ { /* avoid recursion */\n      LIBXSMM_ASSERT(gid != tid);\n      while (2 > LIBXSMM_ATOMIC_LOAD(&libxsmm_ninit, LIBXSMM_ATOMIC_RELAXED)) LIBXSMM_SYNC_YIELD;\n      internal_init();\n    }\n  }\n  LIBXSMM_ASSERT(1 < libxsmm_ninit);\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_NO_TRACE void libxsmm_finalize(void);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_DTOR void libxsmm_finalize(void)\n{\n  void *const regaddr = &internal_registry;\n  uintptr_t regptr = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)((uintptr_t*)regaddr, LIBXSMM_ATOMIC_RELAXED);\n  libxsmm_code_pointer* registry = (libxsmm_code_pointer*)regptr;\n  if (NULL != registry) {\n    int i;\n#if (0 != LIBXSMM_SYNC)\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, &libxsmm_lock_global);\n# if (1 < INTERNAL_REGLOCK_MAXN)\n    { /* acquire locks and thereby shortcut lazy initialization later on */\n      int ntry = 0, n;\n      do {\n        for (i = 0, n = 0; i < internal_reglock_count; ++i) {\n          if (LIBXSMM_LOCK_ACQUIRED(LIBXSMM_REGLOCK) == LIBXSMM_LOCK_TRYLOCK(LIBXSMM_REGLOCK, &internal_reglock[i].state)) ++n;\n        }\n        ntry += (0 == n ? 1 : 0);\n      } while (n < internal_reglock_count && ntry < LIBXSMM_CLEANUP_NTRY);\n    }\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n#endif\n    regptr = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)((uintptr_t*)regaddr, LIBXSMM_ATOMIC_RELAXED);\n    registry = (libxsmm_code_pointer*)regptr;\n    if (NULL != registry) {\n      libxsmm_descriptor *const registry_keys = internal_registry_keys;\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      internal_cache_type *const cache_buffer = internal_cache_buffer;\n#endif\n      unsigned int rest = 0, errors = 0;\n#if defined(LIBXSMM_TRACE)\n      i = libxsmm_trace_finalize();\n      if (EXIT_SUCCESS != i && 0 != libxsmm_verbosity) { /* library code is expected to be mute */\n        fprintf(stderr, \"LIBXSMM ERROR: failed to finalize trace (error #%i)!\\n\", i);\n      }\n#endif\n#if defined(LIBXSMM_PERF)\n      libxsmm_perf_finalize();\n#endif\n      libxsmm_xcopy_finalize();\n      libxsmm_gemm_finalize();\n      libxsmm_dnn_finalize();\n      /* coverity[check_return] */\n      LIBXSMM_ATOMIC_ADD_FETCH(&libxsmm_ninit, 1, LIBXSMM_ATOMIC_RELAXED); /* invalidate code cache (TLS) */\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      internal_cache_buffer = NULL;\n#endif\n      internal_registry_keys = NULL; /* make registry keys unavailable */\n      LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE_ZERO, LIBXSMM_BITS)((uintptr_t*)regaddr, LIBXSMM_ATOMIC_SEQ_CST);\n      internal_registry_nbytes = 0; internal_registry_nleaks = 0;\n      for (i = 0; i < (LIBXSMM_CAPACITY_REGISTRY); ++i) {\n        /*const*/ libxsmm_code_pointer code = registry[i];\n        if (NULL != code.ptr_const) {\n          /* check if the registered entity is a GEMM kernel */\n          switch (registry_keys[i].kind) {\n            case LIBXSMM_KERNEL_KIND_MATMUL: {\n              const libxsmm_gemm_descriptor *const desc = &registry_keys[i].gemm.desc;\n              if (1 < desc->m && 1 < desc->n) {\n                const unsigned int njit = (0 == (LIBXSMM_CODE_STATIC & code.uval) ? 1 : 0);\n                const unsigned int nsta = (0 != (LIBXSMM_CODE_STATIC & code.uval) ? 1 : 0);\n                /* count whether kernel is static or JIT-code */\n                internal_update_mmstatistic(desc, 0, 0, njit, nsta);\n              }\n              else {\n                ++internal_statistic_num_gemv;\n              }\n              ++rest;\n            } break;\n            case LIBXSMM_KERNEL_KIND_MCOPY: {\n              ++internal_statistic_num_mcopy;\n            } break;\n            case LIBXSMM_KERNEL_KIND_MELTW: {\n              ++internal_statistic_num_meltw;\n            } break;\n            case LIBXSMM_KERNEL_KIND_TRANS: {\n              ++internal_statistic_num_tcopy;\n            } break;\n            case LIBXSMM_KERNEL_KIND_TRSM: {\n              ++internal_statistic_num_trsm;\n            } break;\n            case LIBXSMM_KERNEL_KIND_TRMM: {\n              ++internal_statistic_num_trmm;\n            } break;\n            case LIBXSMM_KERNEL_KIND_USER: {\n              ++internal_statistic_num_user;\n            } break;\n            default: if (LIBXSMM_KERNEL_UNREGISTERED <= registry_keys[i].kind) {\n              ++errors;\n            }\n            else {\n              ++rest;\n            }\n          }\n          if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n            if (0 != errors) {\n              fprintf(stderr, \"LIBXSMM ERROR: code registry is corrupted!\\n\");\n            }\n            if (LIBXSMM_CAPACITY_REGISTRY == (rest + errors + internal_statistic_num_gemv +\n              internal_statistic_num_mcopy + internal_statistic_num_meltw +\n              internal_statistic_num_tcopy + internal_statistic_num_trsm +\n              internal_statistic_num_trmm + internal_statistic_num_user))\n            {\n              fprintf(stderr, \"LIBXSMM WARNING: code registry was exhausted!\\n\");\n            }\n          }\n          if (0 == (LIBXSMM_CODE_STATIC & code.uval)) { /* check for allocated/generated JIT-code */\n            void* buffer = NULL;\n            size_t size = 0;\n#if defined(LIBXSMM_HASH_COLLISION)\n            code.uval &= ~LIBXSMM_HASH_COLLISION; /* clear collision flag */\n#endif\n            if (EXIT_SUCCESS == libxsmm_get_malloc_xinfo(code.ptr_const, &size, NULL/*flags*/, &buffer)) {\n#if !defined(NDEBUG)\n              registry[i].ptr = NULL;\n#endif\n              libxsmm_xfree(code.ptr_const, 0/*no check*/);\n              /* round-up size (it is fine to assume 4 KB pages since it is likely more accurate than not rounding up) */\n              internal_registry_nbytes += LIBXSMM_UP2(size + (((char*)code.ptr_const) - (char*)buffer), LIBXSMM_PAGE_MINSIZE);\n            }\n            else ++internal_registry_nleaks;\n          }\n        }\n      }\n      /* release buffers (registry, keys, cache) */\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      libxsmm_xfree(cache_buffer, 0/*no check*/);\n#endif\n      libxsmm_xfree(registry_keys, 0/*no check*/);\n      libxsmm_xfree(registry, 0/*no check*/);\n    }\n#if (0 != LIBXSMM_SYNC) /* LIBXSMM_LOCK_RELEASE, but no LIBXSMM_LOCK_DESTROY */\n# if (1 < INTERNAL_REGLOCK_MAXN)\n    for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, &libxsmm_lock_global);\n    /* coverity[check_return] */\n    LIBXSMM_TLS_DESTROY(libxsmm_tlskey);\n#endif\n  }\n}\n\n\nLIBXSMM_API void libxsmm_sink(LIBXSMM_VARIADIC)\n{\n  /* does nothing else but sinking given arguments */\n}\n\n\nLIBXSMM_API int libxsmm_get_target_archid(void)\n{\n  LIBXSMM_INIT\n#if !defined(__MIC__)\n  return libxsmm_target_archid;\n#else /* no JIT support */\n  return LIBXSMM_MIN(libxsmm_target_archid, LIBXSMM_X86_SSE3);\n#endif\n}\n\n\nLIBXSMM_API void libxsmm_set_target_archid(int id)\n{\n  int target_archid = LIBXSMM_TARGET_ARCH_UNKNOWN;\n  switch (id) {\n    case LIBXSMM_X86_AVX512_CPX:\n    case LIBXSMM_X86_AVX512_CLX:\n    case LIBXSMM_X86_AVX512_CORE:\n    case LIBXSMM_X86_AVX512_KNM:\n    case LIBXSMM_X86_AVX512_MIC:\n    case LIBXSMM_X86_AVX512:\n    case LIBXSMM_X86_AVX2:\n    case LIBXSMM_X86_AVX:\n    case LIBXSMM_X86_SSE4:\n    case LIBXSMM_X86_SSE3:\n    case LIBXSMM_TARGET_ARCH_GENERIC: {\n      target_archid = id;\n    } break;\n    default: if (LIBXSMM_X86_GENERIC <= id) {\n      target_archid = LIBXSMM_X86_GENERIC;\n    }\n    else {\n      target_archid = libxsmm_cpuid();\n    }\n  }\n  LIBXSMM_ATOMIC_STORE(&libxsmm_target_archid, target_archid, LIBXSMM_ATOMIC_RELAXED);\n  if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    const int cpuid = libxsmm_cpuid();\n    if (cpuid < target_archid) {\n      const char *const target_arch = libxsmm_cpuid_name(target_archid);\n      fprintf(stderr, \"LIBXSMM WARNING: \\\"%s\\\" code may fail to run on \\\"%s\\\"!\\n\",\n        target_arch, libxsmm_cpuid_name(cpuid));\n    }\n  }\n}\n\n\nLIBXSMM_API const char* libxsmm_get_target_arch(void)\n{\n  LIBXSMM_INIT\n  return libxsmm_cpuid_name(libxsmm_target_archid);\n}\n\n\n/* function serves as a helper for implementing the Fortran interface */\nLIBXSMM_API const char* libxsmmf_get_target_arch(int* length);\nLIBXSMM_API const char* libxsmmf_get_target_arch(int* length)\n{\n  const char *const arch = libxsmm_get_target_arch();\n  /* valid here since function is not in the public interface */\n  LIBXSMM_ASSERT(NULL != arch && 0 != length);\n  *length = (int)strlen(arch);\n  return arch;\n}\n\n\nLIBXSMM_API void libxsmm_set_target_arch(const char* arch)\n{\n  const int cpuid = libxsmm_cpuid();\n  int target_archid;\n  if (NULL != arch && 0 != *arch) {\n    const int jit = atoi(arch);\n    if (0 == strcmp(\"0\", arch)) {\n      target_archid = LIBXSMM_X86_SSE3;\n    }\n    else if (0 < jit) {\n      target_archid = LIBXSMM_X86_GENERIC + jit;\n    }\n    else if (0 == strcmp(\"cpx\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_CPX;\n    }\n    else if (0 == strcmp(\"clx\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_CLX;\n    }\n    else if (0 == strcmp(\"skx\", arch) || 0 == strcmp(\"skl\", arch)\n          /* \"avx3\"/\"avx512\" previously enabled LIBXSMM_X86_AVX512 */\n          || 0 == strcmp(\"avx3\", arch) || 0 == strcmp(\"avx512\", arch))\n    {\n      target_archid = LIBXSMM_X86_AVX512_CORE;\n    }\n    else if (0 == strcmp(\"knm\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_KNM;\n    }\n    else if (0 == strcmp(\"knl\", arch) || 0 == strcmp(\"mic\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_MIC;\n    }\n    else if (0 == strcmp(\"hsw\", arch) || 0 == strcmp(\"avx2\", arch)) {\n      target_archid = LIBXSMM_X86_AVX2;\n    }\n    else if (0 == strcmp(\"snb\", arch) || 0 == strcmp(\"avx\", arch)) {\n      target_archid = LIBXSMM_X86_AVX;\n    }\n    else if (0 == strcmp(\"wsm\", arch) || 0 == strcmp(\"nhm\", arch) || 0 == strcmp(\"sse4\", arch)\n       || 0 == strcmp(\"sse4_1\", arch) || 0 == strcmp(\"sse4.1\", arch)\n       || 0 == strcmp(\"sse4_2\", arch) || 0 == strcmp(\"sse4.2\", arch))\n    {\n      target_archid = LIBXSMM_X86_SSE4;\n    }\n    else if (0 == strcmp(\"sse\", arch) || 0 == strcmp(\"sse3\", arch)\n        || 0 == strcmp(\"ssse3\", arch) || 0 == strcmp(\"ssse\", arch))\n    {\n      target_archid = LIBXSMM_X86_SSE3;\n    }\n    else if (0 == strcmp(\"x86\", arch) || 0 == strcmp(\"x64\", arch) || 0 == strcmp(\"sse2\", arch)) {\n      target_archid = LIBXSMM_X86_GENERIC;\n    }\n    else if (0 == strcmp(\"generic\", arch) || 0 == strcmp(\"none\", arch)) {\n      target_archid = LIBXSMM_TARGET_ARCH_GENERIC;\n    }\n    else {\n      target_archid = cpuid;\n    }\n  }\n  else {\n    target_archid = cpuid;\n  }\n  if (cpuid < target_archid) { /* warn about code path if beyond CPUID */\n    static int error_once = 0;\n    if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      const char *const target_arch = libxsmm_cpuid_name(target_archid);\n      fprintf(stderr, \"LIBXSMM WARNING: \\\"%s\\\" code will fail to run on \\\"%s\\\"!\\n\",\n        target_arch, libxsmm_cpuid_name(cpuid));\n    }\n#if 0 /* limit code path to confirmed features */\n    target_archid = cpuid;\n#endif\n  }\n  LIBXSMM_ATOMIC_STORE(&libxsmm_target_archid, target_archid, LIBXSMM_ATOMIC_RELAXED);\n}\n\n\nLIBXSMM_API int libxsmm_get_verbosity(void)\n{\n  LIBXSMM_INIT\n  return libxsmm_verbosity;\n}\n\n\nLIBXSMM_API void libxsmm_set_verbosity(int level)\n{\n  LIBXSMM_INIT\n  LIBXSMM_ATOMIC_STORE(&libxsmm_verbosity, level, LIBXSMM_ATOMIC_RELAXED);\n}\n\n\nLIBXSMM_API libxsmm_gemm_prefetch_type libxsmm_get_gemm_auto_prefetch(void)\n{\n  return (libxsmm_gemm_prefetch_type)libxsmm_gemm_auto_prefetch;\n}\n\n\nLIBXSMM_API void libxsmm_set_gemm_auto_prefetch(libxsmm_gemm_prefetch_type strategy)\n{\n  if (0 == internal_gemm_auto_prefetch_locked) { /* LIBXSMM_GEMM_PREFETCH environment takes precedence */\n    LIBXSMM_ATOMIC_STORE(&libxsmm_gemm_auto_prefetch_default, strategy, LIBXSMM_ATOMIC_RELAXED);\n    LIBXSMM_ATOMIC_STORE(&libxsmm_gemm_auto_prefetch, strategy, LIBXSMM_ATOMIC_RELAXED);\n  }\n}\n\n\nLIBXSMM_API unsigned char libxsmm_typesize(libxsmm_datatype datatype)\n{\n  switch (datatype) {\n    case LIBXSMM_DATATYPE_F64:  return 8;\n    case LIBXSMM_DATATYPE_F32:  return 4;\n    case LIBXSMM_DATATYPE_BF16: return 2;\n    case LIBXSMM_DATATYPE_I64:  return 8;\n    case LIBXSMM_DATATYPE_I32:  return 4;\n    case LIBXSMM_DATATYPE_I16:  return 2;\n    case LIBXSMM_DATATYPE_I8:   return 1;\n    case LIBXSMM_DATATYPE_UNSUPPORTED: {\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM ERROR: unsupported data type!\\n\");\n      }\n    } break;\n  }\n  LIBXSMM_ASSERT_MSG(0, \"unsupported data type\");\n  return 1; /* avoid to return 0 to avoid div-by-zero in static analysis of depending code */\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_dvalue(libxsmm_datatype datatype, const void* value, double* dvalue)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != value && NULL != dvalue) {\n    switch (datatype) {\n      case LIBXSMM_DATATYPE_F64: *dvalue =         (*(const double*)value); break;\n      case LIBXSMM_DATATYPE_F32: *dvalue = (double)(*(const float *)value); break;\n      case LIBXSMM_DATATYPE_I32: *dvalue = (double)(*(const int   *)value); break;\n      case LIBXSMM_DATATYPE_I16: *dvalue = (double)(*(const short *)value); break;\n      case LIBXSMM_DATATYPE_I8:  *dvalue = (double)(*(const char  *)value); break;\n      default: result = EXIT_FAILURE;\n    }\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN const char* libxsmm_typename(libxsmm_datatype datatype)\n{\n  switch (datatype) {\n    case LIBXSMM_DATATYPE_F64:  return \"f64\";\n    case LIBXSMM_DATATYPE_F32:  return \"f32\";\n    case LIBXSMM_DATATYPE_BF16: return \"bf16\";\n    case LIBXSMM_DATATYPE_I64:  return \"i64\";\n    case LIBXSMM_DATATYPE_I32:  return \"i32\";\n    case LIBXSMM_DATATYPE_I16:  return \"i16\";\n    case LIBXSMM_DATATYPE_I8:   return \"i8\";\n    default: {\n      if (LIBXSMM_GEMM_PRECISION_I16 == LIBXSMM_GETENUM_INP(datatype) &&\n          LIBXSMM_GEMM_PRECISION_I32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"i16i32\";\n      }\n      else if (LIBXSMM_GEMM_PRECISION_I16 == LIBXSMM_GETENUM_INP(datatype) &&\n               LIBXSMM_GEMM_PRECISION_F32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"i16f32\";\n      }\n      else if (LIBXSMM_GEMM_PRECISION_I8 == LIBXSMM_GETENUM_INP(datatype) &&\n               LIBXSMM_GEMM_PRECISION_I32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"i8i32\";\n      }\n      else if (LIBXSMM_GEMM_PRECISION_BF16 == LIBXSMM_GETENUM_INP(datatype) &&\n               LIBXSMM_GEMM_PRECISION_F32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"bf16f32\";\n      }\n      else {\n        return \"void\";\n      }\n    }\n  }\n}\n\n\nLIBXSMM_API_INLINE void internal_get_typesize_string(char buffer[4], int buffer_size, size_t typesize)\n{\n  LIBXSMM_ASSERT(256 > typesize && 4 <= buffer_size);\n  if (10 > typesize) {\n    buffer[0] = (char)('0' + typesize);\n    buffer[1] = 0;\n  }\n  else {\n    LIBXSMM_SNPRINTF(buffer, buffer_size, \"%i\", (int)typesize);\n  }\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_build(const libxsmm_build_request* request, unsigned int regindex, libxsmm_code_pointer* code)\n{\n  int result = EXIT_SUCCESS;\n#if !defined(__MIC__)\n  const char * /*const*/ target_arch = libxsmm_cpuid_name(libxsmm_target_archid);\n  /* large enough temporary buffer for generated code */\n  char jit_buffer[LIBXSMM_CODE_MAXSIZE], jit_name[256] = { 0 };\n  libxsmm_generated_code generated_code;\n  libxsmm_kernel_xinfo extra;\n\n  LIBXSMM_MEMZERO127(&generated_code);\n  generated_code.generated_code = jit_buffer;\n  generated_code.buffer_size = sizeof(jit_buffer);\n  /* setup code generation */\n  generated_code.arch = libxsmm_target_archid;\n  generated_code.code_type = 2;\n\n# if !defined(NDEBUG) /* should not be needed (all members will be initialized below) */\n  LIBXSMM_MEMZERO127(&extra);\n# endif\n  extra.registered = regindex;\n  extra.nflops = 0;\n\n  LIBXSMM_ASSERT(NULL != generated_code.generated_code || 0 == generated_code.buffer_size);\n  LIBXSMM_ASSERT(NULL != request && 0 != libxsmm_target_archid);\n  LIBXSMM_ASSERT(NULL != code && NULL == code->ptr_const);\n\n  switch (request->kind) { /* generate kernel */\n    case LIBXSMM_BUILD_KIND_GEMM: { /* small MxM kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.gemm);\n# if 0 /* dummy kernel for an empty shape is desired */\n      if (0 < request->descriptor.gemm->m   && 0 < request->descriptor.gemm->n   && 0 < request->descriptor.gemm->k &&\n          0 < request->descriptor.gemm->lda && 0 < request->descriptor.gemm->ldb && 0 < request->descriptor.gemm->ldc)\n# endif\n      {\n        const unsigned int m = request->descriptor.gemm->m, n = request->descriptor.gemm->n, k = request->descriptor.gemm->k;\n        extra.nflops = 2 * m * n * k;\n# if !defined(LIBXSMM_DENY_RETARGET) /* disable: ECFLAGS=-DLIBXSMM_DENY_RETARGET */\n        if (LIBXSMM_X86_AVX2 < libxsmm_target_archid &&\n           (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.gemm->datatype) ||\n            LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.gemm->datatype)) &&\n           (16 >= (m * k) || 16 >= (k * n) || 16 >= (m * n)))\n        {\n          /* TODO: shall we update variable \"target_arch\" (name)? */\n          generated_code.arch = LIBXSMM_X86_AVX2;\n        }\n# endif\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_gemm_kernel, &generated_code, request->descriptor.gemm);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.gemm->datatype);\n          int typesigns = 0, br = 0;\n          /* query batch reduce variant */\n          if ( (LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS & request->descriptor.gemm->flags) > 1 ) {\n            br = 1;\n          } else if ( (LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET & request->descriptor.gemm->flags) > 1 ) {\n            br = 2;\n          } else if ( (LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE & request->descriptor.gemm->flags) > 1 ) {\n            br = 3;\n          } else {\n            br = 0;\n          }\n          /* query A/B sign combinations */\n          if ( (LIBXSMM_GEMM_FLAG_A_UNSIGNED & request->descriptor.gemm->flags) > 1 ) {\n            typesigns = 1;\n          } else if ( (LIBXSMM_GEMM_FLAG_B_UNSIGNED & request->descriptor.gemm->flags) > 1 ) {\n            typesigns = 2;\n          } else if ( (LIBXSMM_GEMM_FLAG_AB_UNSIGNED & request->descriptor.gemm->flags) > 1 ) {\n            typesigns = 3;\n          } else {\n            typesigns = 0;\n          }\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i_br%i_uh%u_si%i.mxm\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.gemm->flags) ? 'n' : 't', m, n, k,\n            request->descriptor.gemm->lda, request->descriptor.gemm->ldb, request->descriptor.gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.gemm->flags) ? 0 : 1, uid, br, (unsigned int)request->descriptor.gemm->c3, typesigns);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_SRSOA: { /* sparse SOA kernel, CSR format */\n      LIBXSMM_ASSERT(NULL != request->descriptor.srsoa && 0 != request->descriptor.srsoa->gemm);\n      LIBXSMM_ASSERT(NULL != request->descriptor.srsoa->row_ptr && 0 != request->descriptor.srsoa->column_idx && 0 != request->descriptor.srsoa->values);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.srsoa->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.srsoa->gemm->datatype))\n      {\n        const unsigned int nnz = (request->descriptor.srsoa->gemm->lda == 0) ?\n            request->descriptor.srsoa->row_ptr[request->descriptor.srsoa->gemm->m] : request->descriptor.srsoa->row_ptr[request->descriptor.srsoa->gemm->k];\n        const unsigned int simdw = (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.srsoa->gemm->datatype)) ?\n            libxsmm_cpuid_vlen32(libxsmm_target_archid)/2 : libxsmm_cpuid_vlen32(libxsmm_target_archid);\n        const unsigned int gemm_factor = (request->descriptor.srsoa->gemm->lda == 0) ? request->descriptor.srsoa->gemm->n : request->descriptor.srsoa->gemm->m;\n        extra.nflops = 2 * nnz * gemm_factor * simdw;\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_spgemm_csr_soa_kernel, &generated_code, request->descriptor.srsoa->gemm, target_arch,\n          request->descriptor.srsoa->row_ptr, request->descriptor.srsoa->column_idx, request->descriptor.srsoa->values, request->descriptor.srsoa->packed_width);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.srsoa->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.srsoa->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_w%u_a%i_b%i_p%i_nnz%u.srsoa\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.srsoa->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.srsoa->gemm->flags) ? 'n' : 't',\n            request->descriptor.srsoa->gemm->m,   request->descriptor.srsoa->gemm->n,   request->descriptor.srsoa->gemm->k,\n            request->descriptor.srsoa->gemm->lda, request->descriptor.srsoa->gemm->ldb, request->descriptor.srsoa->gemm->ldc,\n            request->descriptor.srsoa->packed_width,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.srsoa->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.srsoa->gemm->flags) ? 0 : 1,\n            uid, nnz);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_SCSOA: { /* sparse SOA kernel, CSC format */\n      LIBXSMM_ASSERT(NULL != request->descriptor.scsoa && 0 != request->descriptor.scsoa->gemm);\n      LIBXSMM_ASSERT(NULL != request->descriptor.scsoa->row_idx && 0 != request->descriptor.scsoa->column_ptr && 0 != request->descriptor.scsoa->values);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.scsoa->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.scsoa->gemm->datatype))\n      {\n        const unsigned int nnz = (request->descriptor.scsoa->gemm->lda == 0) ?\n            request->descriptor.scsoa->column_ptr[request->descriptor.scsoa->gemm->k] : request->descriptor.scsoa->column_ptr[request->descriptor.scsoa->gemm->n];\n        const unsigned int simdw = (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.scsoa->gemm->datatype)) ?\n            libxsmm_cpuid_vlen32(libxsmm_target_archid)/2 : libxsmm_cpuid_vlen32(libxsmm_target_archid);\n        const unsigned int gemm_factor = (request->descriptor.scsoa->gemm->lda == 0) ? request->descriptor.scsoa->gemm->n : request->descriptor.scsoa->gemm->m;\n        extra.nflops = 2 * nnz * gemm_factor * simdw;\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_spgemm_csc_soa_kernel, &generated_code, request->descriptor.scsoa->gemm, target_arch,\n          request->descriptor.scsoa->row_idx, request->descriptor.scsoa->column_ptr, request->descriptor.scsoa->values, request->descriptor.scsoa->packed_width);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.scsoa->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.scsoa->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_w%u_a%i_b%i_p%i_nnz%u.scsoa\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.scsoa->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.scsoa->gemm->flags) ? 'n' : 't',\n            request->descriptor.scsoa->gemm->m,   request->descriptor.scsoa->gemm->n,   request->descriptor.scsoa->gemm->k,\n            request->descriptor.scsoa->gemm->lda, request->descriptor.scsoa->gemm->ldb, request->descriptor.scsoa->gemm->ldc,\n            request->descriptor.scsoa->packed_width,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.scsoa->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.scsoa->gemm->flags) ? 0 : 1,\n            uid, nnz);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_PGEMMRMAC: { /* packed GEMM, B regular matrix, row-major */\n      LIBXSMM_ASSERT(NULL != request->descriptor.pgemmacrm && 0 != request->descriptor.pgemmacrm->gemm);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmacrm->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmacrm->gemm->datatype))\n      {\n        extra.nflops = 2 * request->descriptor.pgemmacrm->packed_width * request->descriptor.pgemmacrm->gemm->m * request->descriptor.pgemmacrm->gemm->n * request->descriptor.pgemmacrm->gemm->k;\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_packed_gemm_ac_rm, &generated_code, request->descriptor.pgemmacrm->gemm, request->descriptor.pgemmacrm->packed_width, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.pgemmacrm->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.pgemmacrm->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_w%u_a%i_b%i_p%i.pgemmacrm\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.pgemmacrm->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.pgemmacrm->gemm->flags) ? 'n' : 't',\n            request->descriptor.pgemmacrm->gemm->m,   request->descriptor.pgemmacrm->gemm->n,   request->descriptor.pgemmacrm->gemm->k,\n            request->descriptor.pgemmacrm->gemm->lda, request->descriptor.pgemmacrm->gemm->ldb, request->descriptor.pgemmacrm->gemm->ldc,\n            request->descriptor.pgemmacrm->packed_width,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.pgemmacrm->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.pgemmacrm->gemm->flags) ? 0 : 1,\n            uid);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_PGEMMRMBC: { /* packed GEMM, A regular matrix, row-major */\n      LIBXSMM_ASSERT(NULL != request->descriptor.pgemmbcrm && 0 != request->descriptor.pgemmbcrm->gemm);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmbcrm->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmbcrm->gemm->datatype))\n      {\n        extra.nflops = 2 * request->descriptor.pgemmbcrm->packed_width * request->descriptor.pgemmbcrm->gemm->m * request->descriptor.pgemmbcrm->gemm->n * request->descriptor.pgemmbcrm->gemm->k;\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_packed_gemm_bc_rm, &generated_code, request->descriptor.pgemmbcrm->gemm, request->descriptor.pgemmbcrm->packed_width, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.pgemmbcrm->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.pgemmbcrm->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_w%u_a%i_b%i_p%i.pgemmbcrm\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.pgemmbcrm->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.pgemmbcrm->gemm->flags) ? 'n' : 't',\n            request->descriptor.pgemmbcrm->gemm->m,   request->descriptor.pgemmbcrm->gemm->n,   request->descriptor.pgemmbcrm->gemm->k,\n            request->descriptor.pgemmbcrm->gemm->lda, request->descriptor.pgemmbcrm->gemm->ldb, request->descriptor.pgemmbcrm->gemm->ldc,\n            request->descriptor.pgemmbcrm->packed_width,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.pgemmbcrm->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.pgemmbcrm->gemm->flags) ? 0 : 1,\n            uid);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_SREG: { /* sparse register kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.sreg && 0 != request->descriptor.sreg->gemm);\n      LIBXSMM_ASSERT(NULL != request->descriptor.sreg->row_ptr && 0 != request->descriptor.sreg->column_idx && 0 != request->descriptor.sreg->values);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.sreg->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.sreg->gemm->datatype))\n      {\n        const unsigned int nnz = request->descriptor.sreg->row_ptr[request->descriptor.sreg->gemm->m];\n        extra.nflops = 2 * libxsmm_cpuid_vlen32(libxsmm_target_archid)/2 * request->descriptor.sreg->gemm->n * nnz;\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_spgemm_csr_reg_kernel, &generated_code, request->descriptor.sreg->gemm, target_arch,\n          request->descriptor.sreg->row_ptr, request->descriptor.sreg->column_idx,\n          (const double*)request->descriptor.sreg->values);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.sreg->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.sreg->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i.sreg\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.sreg->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.sreg->gemm->flags) ? 'n' : 't',\n            request->descriptor.sreg->gemm->m,   request->descriptor.sreg->gemm->n,   request->descriptor.sreg->gemm->k,\n            request->descriptor.sreg->gemm->lda, request->descriptor.sreg->gemm->ldb, request->descriptor.sreg->gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.sreg->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.sreg->gemm->flags) ? 0 : 1,\n            uid);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_MCOPY: { /* matcopy kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.mcopy);\n# if 0 /* TODO: backend supports typesize <= 4, but kernels for typesize < 4 are incorrect */\n      if (4 == request->descriptor.mcopy->typesize)\n# endif\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_matcopy_kernel, &generated_code, request->descriptor.mcopy, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), request->descriptor.mcopy->typesize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%ux%u_%ux%u_p%u.mcopy\", target_arch, tsizename,\n            request->descriptor.mcopy->m, request->descriptor.mcopy->n, request->descriptor.mcopy->ldi, request->descriptor.mcopy->ldo,\n            (unsigned int)request->descriptor.mcopy->prefetch);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_MELTW: { /* matcopy kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.meltw);\n# if 0 /* TODO: backend supports typesize <= 4, but kernels for typesize < 4 are incorrect */\n      if (4 == request->descriptor.meltw->typesize)\n# endif\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_mateltwise_kernel, &generated_code, request->descriptor.meltw);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), request->descriptor.meltw->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%ux%u_%ux%u_opcode%u_flags%u.meltw\", target_arch, tsizename,\n            request->descriptor.meltw->m, request->descriptor.meltw->n, request->descriptor.meltw->ldi, request->descriptor.meltw->ldo,\n            (unsigned int)request->descriptor.meltw->operation, (unsigned int)request->descriptor.meltw->flags);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_TRANS: { /* transpose kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.trans);\n      if (4 == request->descriptor.trans->typesize || 8 == request->descriptor.trans->typesize) {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_transpose_kernel, &generated_code, request->descriptor.trans, libxsmm_target_archid);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), request->descriptor.trans->typesize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%ux%u_%u.trans\", target_arch, tsizename,\n            request->descriptor.trans->m, request->descriptor.trans->n, request->descriptor.trans->ldo);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_PGEMM: { /* compact P/GEMM-kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.pgemm);\n      tsize = (unsigned int)request->descriptor.pgemm->typesize;\n      if (4 == tsize || 8 == tsize) {\n        extra.nflops = 0; /* TODO */\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_pgemm_kernel, &generated_code, request->descriptor.pgemm, libxsmm_target_archid);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c%c%c_%ux%ux%u_%u_%u_%u_%i.pgemm\", target_arch, tsizename,\n            request->descriptor.pgemm->transa, request->descriptor.pgemm->transb, request->descriptor.pgemm->layout,\n            request->descriptor.pgemm->m, request->descriptor.pgemm->n, request->descriptor.pgemm->k,\n            request->descriptor.pgemm->lda, request->descriptor.pgemm->ldb, request->descriptor.pgemm->ldc,\n            (int)request->descriptor.pgemm->alpha_val);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_GETRF: { /* compact GETRF kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.getrf);\n      tsize = (unsigned int)request->descriptor.getrf->typesize;\n      if (4 == tsize || 8 == tsize) {\n        extra.nflops = 0; /* TODO */\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_getrf_kernel, &generated_code, request->descriptor.getrf, libxsmm_target_archid);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c_%ux%u_%u.getrf\", target_arch, tsizename,\n            request->descriptor.getrf->layout, request->descriptor.getrf->m, request->descriptor.getrf->n, request->descriptor.getrf->lda);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_TRMM: { /* compact TRMM kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.trmm);\n      tsize = (unsigned int)request->descriptor.trmm->typesize;\n      if (4 == tsize || 8 == tsize) {\n        extra.nflops = 0; /* TODO */\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_trmm_kernel, &generated_code, request->descriptor.trmm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c%c%c%c_%ux%u_%u_%u.trmm\", target_arch, tsizename,\n            request->descriptor.trmm->transa, request->descriptor.trmm->layout, request->descriptor.trmm->side, request->descriptor.trmm->uplo,\n            request->descriptor.trmm->m, request->descriptor.trmm->n, request->descriptor.trmm->lda, request->descriptor.trmm->ldb); /* TODO: alpha */\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_TRSM: if (NULL != request->descriptor.trsm) { /* compact TRSM kernel (packed) */\n      const unsigned int tsize = (unsigned int)request->descriptor.trsm->typesize;\n      if (4 == tsize || 8 == tsize) {\n        extra.nflops = 0; /* TODO */\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_trsm_kernel, &generated_code, request->descriptor.trsm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          char tsizename[4];\n          internal_get_typesize_string(tsizename, sizeof(tsizename), tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c%c%c%c_%ux%u_%u_%u.trsm\", target_arch, tsizename,\n            request->descriptor.trsm->transa, request->descriptor.trsm->layout, request->descriptor.trsm->side, request->descriptor.trsm->uplo,\n            request->descriptor.trsm->m, request->descriptor.trsm->n, request->descriptor.trsm->lda, request->descriptor.trsm->ldb); /* TODO: alpha */\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_USER: break;\n# if !defined(NDEBUG) /* library code is expected to be mute */\n    default: { /* unknown kind */\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid build request discovered!\\n\");\n      }\n      /*result = EXIT_FAILURE;*/\n    }\n# endif\n  }\n\n  if  (0 == generated_code.last_error /* no error raised */\n    && 0 != generated_code.code_size /*check (tcopy issue?)*/)\n  {\n    char* code_buffer = NULL;\n    void* code_buffer_result = &code_buffer;\n    LIBXSMM_ASSERT(generated_code.code_size <= LIBXSMM_CODE_MAXSIZE);\n    LIBXSMM_ASSERT(NULL != generated_code.generated_code);\n    /* attempt to create executable buffer */\n    result = libxsmm_xmalloc((void**)code_buffer_result, generated_code.code_size, 0/*auto*/,\n      /* flag must be a superset of what's populated by libxsmm_malloc_attrib */\n      LIBXSMM_MALLOC_FLAG_RWX, &extra, sizeof(extra));\n    if (EXIT_SUCCESS == result) { /* check for success */\n      LIBXSMM_ASSERT(NULL != code_buffer);\n      /* copy temporary buffer into the prepared executable buffer */\n# if defined(NDEBUG)\n      { int i; /* precondition: jit_buffer == generated_code.generated_code */\n        for (i = 0; i < (int)generated_code.code_size; ++i) code_buffer[i] = jit_buffer[i];\n      }\n# else\n      memcpy(code_buffer, generated_code.generated_code, generated_code.code_size);\n# endif\n      /* attribute/protect buffer and revoke unnecessary flags */\n      result = libxsmm_malloc_attrib((void**)code_buffer_result, LIBXSMM_MALLOC_FLAG_X, jit_name);\n      if (EXIT_SUCCESS == result) { /* check for success */\n        code->ptr = code_buffer; /* commit buffer */\n        LIBXSMM_ASSERT(NULL != code->ptr && 0 == (LIBXSMM_CODE_STATIC & code->uval));\n      }\n      else { /* release buffer */\n        libxsmm_xfree(code_buffer, 0/*no check*/);\n      }\n    }\n  }\n  else if (request->kind == LIBXSMM_BUILD_KIND_USER && NULL != request->descriptor.ptr) { /* user-data */\n    if (0 != request->user_size) {\n      void* user_data = &code->ptr;\n      result = libxsmm_xmalloc((void**)user_data, request->user_size, 0/*auto*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, &extra, sizeof(extra));\n    }\n    else {\n      result = EXIT_SUCCESS;\n      code->ptr = NULL;\n    }\n  }\n  else {\n    result = (0 != generated_code.last_error ? generated_code.last_error : EXIT_FAILURE);\n  }\n#else /* unsupported platform */\n  LIBXSMM_UNUSED(request); LIBXSMM_UNUSED(regindex); LIBXSMM_UNUSED(code);\n  /* libxsmm_get_target_arch also serves as a runtime check whether JIT is available or not */\n  if (LIBXSMM_X86_SSE3 <= libxsmm_target_archid) result = EXIT_FAILURE;\n#endif\n  return result;\n}\n\n\n#if defined(LIBXSMM_DESC_PAD)\nLIBXSMM_API_INLINE void internal_pad_descriptor(libxsmm_descriptor* desc, size_t size)\n{\n  const signed char s = (signed char)LIBXSMM_MAX(LIBXSMM_DIFF_SIZE, LIBXSMM_HASH_SIZE); signed char i;\n  LIBXSMM_ASSERT(NULL != desc && s <= LIBXSMM_DESCRIPTOR_MAXSIZE);\n  for (i = (signed char)size; i < s; ++i) desc->data[i] = 0;\n}\n#endif\n\n\nLIBXSMM_API_INLINE libxsmm_code_pointer internal_find_code(libxsmm_descriptor* desc, size_t desc_size, size_t user_size)\n{\n  libxsmm_code_pointer flux_entry = { 0 };\n  const size_t size = LIBXSMM_MIN(sizeof(libxsmm_descriptor_kind) + desc_size, LIBXSMM_DIFF_SIZE);\n#if !defined(NDEBUG) && (0 != LIBXSMM_JIT)\n  int build = EXIT_SUCCESS;\n#endif\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n# if defined(LIBXSMM_NTHREADS_USE)\n  const unsigned int tid = libxsmm_get_tid();\n  internal_cache_type *const cache = internal_cache_buffer + tid;\n# else\n  static LIBXSMM_TLS internal_cache_type internal_cache_buffer;\n  internal_cache_type *const cache = &internal_cache_buffer;\n# endif\n  unsigned char cache_index;\n# if defined(LIBXSMM_DESC_PAD)\n#   if defined(LIBXSMM_DESC_INLINE)\n  LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n  internal_pad_descriptor(desc, size);\n  LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n  LIBXSMM_DIFF_N(unsigned char, cache_index, LIBXSMM_DIFF(LIBXSMM_DIFF_SIZE), xdesc, cache->entry.keys,\n    LIBXSMM_DIFF_SIZE, LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n#   else\n  internal_pad_descriptor(desc, size);\n  cache_index = (unsigned char)libxsmm_diff_n(desc, cache->entry.keys,\n    LIBXSMM_DIFF_SIZE, LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n#   endif\n# elif defined(LIBXSMM_DESC_INLINE)\n  LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n  LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n  LIBXSMM_DIFF_N(unsigned char, cache_index, LIBXSMM_DIFF(LIBXSMM_DIFF_SIZE), xdesc, cache->entry.keys,\n    size, LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n# else\n  LIBXSMM_ASSERT(NULL != desc);\n  cache_index = (unsigned char)libxsmm_diff_n(desc, cache->entry.keys,\n    size, LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n# endif\n  if (cache->entry.id == libxsmm_ninit && cache_index < cache->entry.size) { /* valid hit */\n    flux_entry = cache->entry.code[cache_index];\n    cache->entry.hit = cache_index;\n  }\n  else\n#else\n  LIBXSMM_ASSERT(NULL != desc);\n# if defined(LIBXSMM_DESC_PAD)\n# if defined(LIBXSMM_DESC_INLINE)\n  LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n  internal_pad_descriptor(desc, size);\n  LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n# else\n  internal_pad_descriptor(desc, size);\n# endif\n# endif\n#endif\n  {\n#if defined(LIBXSMM_DESC_PAD)\n    unsigned int i = LIBXSMM_CRC32(LIBXSMM_HASH_SIZE)(LIBXSMM_HASH_SEED, desc);\n#else\n    unsigned int i = libxsmm_crc32(LIBXSMM_HASH_SEED, desc, LIBXSMM_MIN(size, LIBXSMM_HASH_SIZE));\n#endif\n    unsigned int i0 = i = LIBXSMM_MOD2(i, LIBXSMM_CAPACITY_REGISTRY), mode = 0, diff = 1;\n    LIBXSMM_ASSERT(NULL != internal_registry);\n    LIBXSMM_ASSERT(&desc->kind == &desc->gemm.pad && desc->kind == desc->gemm.pad);\n    do { /* use calculated location and check if the requested code is already JITted */\n#if (1 < INTERNAL_REGLOCK_MAXN) || !LIBXSMM_LOCK_TYPE_ISRW(LIBXSMM_REGLOCK) /* read registered code */\n# if 1 /* omitting an atomic load is safe but avoids race-detectors to highlight this location */\n      uintptr_t *const fluxaddr = &internal_registry[i].uval;\n      flux_entry.uval = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(fluxaddr, LIBXSMM_ATOMIC_RELAXED);\n# else\n      flux_entry = internal_registry[i];\n# endif\n#else\n      LIBXSMM_LOCK_ACQREAD(LIBXSMM_REGLOCK, internal_reglock_ptr);\n      flux_entry = internal_registry[i]; /* read registered code */\n      LIBXSMM_LOCK_RELREAD(LIBXSMM_REGLOCK, internal_reglock_ptr);\n#endif\n      if ((NULL != flux_entry.ptr_const || 1 == mode) && 2 > mode) { /* check existing entry further */\n        if (NULL != flux_entry.ptr_const) {\n#if defined(LIBXSMM_DESC_PAD)\n# if defined(LIBXSMM_DIFF_INLINE)\n#   if !defined(LIBXSMM_DESC_INLINE)\n          LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n          LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n#   endif\n          diff = LIBXSMM_DIFF(LIBXSMM_DIFF_SIZE)(xdesc, internal_registry_keys + i, 0/*dummy*/);\n# else\n          diff = libxsmm_diff(desc, internal_registry_keys + i, LIBXSMM_DIFF_SIZE);\n# endif\n#else\n          diff = libxsmm_diff(desc, internal_registry_keys + i, size);\n#endif\n        }\n#if !defined(NDEBUG)\n        else LIBXSMM_ASSERT(0 != diff);\n#endif\n        if (0 != diff) { /* search for code version */\n          if (0 == mode) { /* transition to higher mode */\n            i0 = i; /* keep current position on record */\n#if defined(LIBXSMM_HASH_COLLISION)\n            /* enter code generation, and collision fix-up */\n            if (0 == (LIBXSMM_HASH_COLLISION & flux_entry.uval)) {\n              LIBXSMM_ASSERT(NULL != flux_entry.ptr_const); /* collision */\n              mode = 3;\n            }\n            else\n#endif      /* search for an existing code version */\n            mode = 1; /* else */\n          }\n          i = LIBXSMM_MOD2(i + 1, LIBXSMM_CAPACITY_REGISTRY);\n          if (i == i0) { /* search finished, no code version exists */\n#if defined(LIBXSMM_HASH_COLLISION)\n            mode = 3; /* enter code generation, and collision fix-up */\n#else\n            mode = 2; /* enter code generation */\n#endif\n            if (LIBXSMM_KERNEL_KIND_MATMUL == desc->kind) {\n              internal_update_mmstatistic(&desc->gemm.desc, 0, 1/*collision*/, 0, 0);\n            }\n          }\n          LIBXSMM_ASSERT(0 != diff); /* continue */\n        }\n      }\n      else { /* enter code generation (there is no code version yet) */\n        LIBXSMM_ASSERT(0 == mode || 1 < mode);\n#if (0 == LIBXSMM_JIT)\n        LIBXSMM_UNUSED(user_size);\n#else\n        if (LIBXSMM_X86_AVX <= libxsmm_target_archid || /* check if JIT is supported (CPUID) */\n           (LIBXSMM_X86_SSE3 <= libxsmm_target_archid && LIBXSMM_KERNEL_KIND_MATMUL == desc->kind) ||\n           (LIBXSMM_KERNEL_KIND_USER == desc->kind))\n        {\n          LIBXSMM_ASSERT(0 != mode || NULL == flux_entry.ptr_const/*code version does not exist*/);\n          INTERNAL_FIND_CODE_LOCK(lock, i, diff, flux_entry.ptr); /* lock the registry entry */\n          if (NULL == internal_registry[i].ptr_const) { /* double-check registry after acquiring the lock */\n            libxsmm_build_request request; /* setup the code build request */\n            LIBXSMM_ASSERT(desc->kind < LIBXSMM_KERNEL_UNREGISTERED);\n            request.kind = (libxsmm_build_kind)desc->kind;\n            request.descriptor.ptr = &desc->gemm.desc;\n            request.user_size = user_size;\n# if defined(NDEBUG)\n            if (EXIT_SUCCESS == libxsmm_build(&request, i, &flux_entry) && NULL != flux_entry.ptr_const)\n# else\n            build = libxsmm_build(&request, i, &flux_entry);\n            if (EXIT_SUCCESS == build && NULL != flux_entry.ptr_const)\n# endif\n            {\n              LIBXSMM_ASSIGN127(internal_registry_keys + i, desc);\n# if (1 < INTERNAL_REGLOCK_MAXN)\n              LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_registry[i].ptr, flux_entry.ptr, LIBXSMM_ATOMIC_SEQ_CST);\n# else\n              internal_registry[i] = flux_entry;\n# endif\n# if defined(LIBXSMM_HASH_COLLISION)\n              if (2 < mode) { /* arrived from collision state; now mark as collision */\n                libxsmm_code_pointer fix_entry;\n#   if (1 < INTERNAL_REGLOCK_MAXN)\n                fix_entry.ptr = LIBXSMM_ATOMIC_LOAD(&internal_registry[i0].ptr, LIBXSMM_ATOMIC_RELAXED);\n#   else\n                fix_entry = internal_registry[i0];\n#   endif\n                LIBXSMM_ASSERT(NULL != fix_entry.ptr_const);\n                if (0 == (LIBXSMM_HASH_COLLISION & fix_entry.uval)) {\n                  fix_entry.uval |= LIBXSMM_HASH_COLLISION; /* mark current entry as collision */\n#   if (1 < INTERNAL_REGLOCK_MAXN)\n                  LIBXSMM_ATOMIC_STORE(&internal_registry[i0].ptr, fix_entry.ptr, LIBXSMM_ATOMIC_RELAXED);\n#   else\n                  internal_registry[i0] = fix_entry;\n#   endif\n                }\n              }\n# endif\n            }\n            if (((int)LIBXSMM_KERNEL_KIND_MATMUL) == desc->kind) {\n              internal_update_mmstatistic(&desc->gemm.desc, 1/*try*/, 0, 0, 0);\n            }\n            /* leave here even in case of a build-error; do not use break (inside of locked region) */\n            diff = 0;\n          }\n          INTERNAL_FIND_CODE_UNLOCK(lock);\n          if (0 != diff) { /* acquire registry slot */\n            if (0 == mode) { /* initial condition */\n              mode = 2; /* continue to linearly search for an empty slot */\n              i0 = i; /* keep current position on record */\n            }\n            do { /* continue to linearly search for an available slot */\n              i = LIBXSMM_MOD2(i + 1, LIBXSMM_CAPACITY_REGISTRY);\n              if (NULL == internal_registry[i].ptr_const) break;\n            } while (i != i0);\n            if (i == i0) { /* out of capacity (no registry slot available) */\n              diff = 0; /* do not use break if inside of locked region */\n            }\n            flux_entry.ptr = NULL; /* no result */\n          }\n        }\n        else /* JIT-code generation not available */\n#endif\n        { /* leave the dispatch loop */\n          if (((int)LIBXSMM_KERNEL_KIND_MATMUL) == desc->kind) {\n            internal_update_mmstatistic(&desc->gemm.desc, 1/*try*/, 0, 0, 0);\n          }\n#if !defined(NDEBUG) && (0 != LIBXSMM_JIT)\n          build = EXIT_FAILURE;\n#endif\n          flux_entry.ptr = NULL;\n          diff = 0;\n        }\n      }\n    } while (0 != diff);\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n    if (NULL != flux_entry.ptr_const) { /* keep code version on record (cache) */\n      LIBXSMM_ASSERT(0 == diff);\n      if (cache->entry.id == libxsmm_ninit) { /* maintain cache */\n        if (cache->entry.size < internal_cache_size) { /* grow */\n          INTERNAL_FIND_CODE_CACHE_GROW(cache_index, cache->entry.size);\n          LIBXSMM_ASSERT(cache->entry.size <= internal_cache_size);\n        }\n        else { /* evict */\n          LIBXSMM_ASSERT(cache->entry.hit < cache->entry.size);\n          INTERNAL_FIND_CODE_CACHE_EVICT(cache_index, cache->entry.size, cache->entry.hit);\n        }\n      }\n      else if (0 != internal_cache_size) { /* reset cache */\n# if !defined(NDEBUG)\n        LIBXSMM_MEMZERO127(cache->entry.keys);\n# endif\n        cache->entry.id = libxsmm_ninit;\n        cache->entry.size = 1;\n        cache_index = 0;\n      }\n      LIBXSMM_ASSIGN127(cache->entry.keys + cache_index, desc);\n      cache->entry.code[cache_index] = flux_entry;\n      cache->entry.hit = cache_index;\n    }\n#endif\n  }\n#if defined(LIBXSMM_HASH_COLLISION)\n  flux_entry.uval &= ~(LIBXSMM_CODE_STATIC | LIBXSMM_HASH_COLLISION); /* clear non-JIT and collision flag */\n#else\n  flux_entry.uval &= ~LIBXSMM_CODE_STATIC; /* clear non-JIT flag */\n#endif\n#if (0 != LIBXSMM_JIT)\n  assert(LIBXSMM_KERNEL_KIND_MATMUL != desc->kind || NULL != flux_entry.ptr_const || EXIT_SUCCESS != build || 1 == internal_reglock_count); /*!LIBXSMM_ASSERT*/\n#endif\n  return flux_entry;\n}\n\n\nLIBXSMM_API_INTERN const libxsmm_kernel_xinfo* libxsmm_get_kernel_xinfo(libxsmm_code_pointer code, const libxsmm_descriptor** desc, size_t* code_size)\n{\n  libxsmm_kernel_xinfo* result = NULL;\n  void *const result_address = &result;\n  int flags = LIBXSMM_MALLOC_FLAG_X;\n  if (NULL != code.ptr_const && EXIT_SUCCESS == libxsmm_get_malloc_xinfo(code.ptr_const, code_size, &flags, (void**)result_address) && NULL != result) {\n    if (NULL != desc) {\n      if (NULL != internal_registry && NULL != internal_registry_keys && result->registered < (LIBXSMM_CAPACITY_REGISTRY)\n#if defined(LIBXSMM_HASH_COLLISION)\n        && code.uval == (~LIBXSMM_HASH_COLLISION & internal_registry[result->registered].uval)\n#else\n        && code.ptr_const == internal_registry[result->registered].ptr_const\n#endif\n        && internal_registry_keys[result->registered].kind < LIBXSMM_KERNEL_UNREGISTERED)\n      {\n        *desc = internal_registry_keys + result->registered;\n      }\n      else *desc = NULL;\n    }\n  }\n  else {\n    LIBXSMM_ASSERT(NULL == result);\n    if (NULL != code_size) *code_size = 0;\n    if (NULL != desc) *desc = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_kernel_info(const void* kernel, libxsmm_kernel_info* info)\n{\n  int result;\n  const libxsmm_kernel_xinfo* xinfo;\n  libxsmm_kernel_info result_info;\n  const libxsmm_descriptor* desc;\n  libxsmm_code_pointer code;\n  code.ptr_const = kernel;\n  LIBXSMM_MEMZERO127(&result_info);\n  xinfo = libxsmm_get_kernel_xinfo(code, &desc, &result_info.code_size);\n  if (NULL != xinfo) {\n    if (NULL != desc) {\n      const libxsmm_kernel_kind kind = (libxsmm_kernel_kind)desc->kind;\n      result_info.kind = kind;\n      if (LIBXSMM_KERNEL_KIND_USER == kind) {\n        result_info.code_size = 0; /* invalid */\n      }\n    }\n    else {\n      result_info.kind = LIBXSMM_KERNEL_UNREGISTERED;\n    }\n    result_info.nflops = xinfo->nflops;\n    LIBXSMM_ASSIGN127(info, &result_info);\n    result = EXIT_SUCCESS;\n  }\n  else {\n    LIBXSMM_ASSERT(NULL == desc);\n    if (NULL != info) {\n      LIBXSMM_ASSIGN127(info, &result_info);\n      result = EXIT_FAILURE;\n    }\n    else {\n      result = EXIT_SUCCESS;\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_mmkernel_info(libxsmm_xmmfunction kernel, libxsmm_mmkernel_info* info)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xgemm = kernel;\n  if (NULL != info) {\n    const libxsmm_descriptor* desc;\n    if (NULL != libxsmm_get_kernel_xinfo(code, &desc, NULL/*code_size*/) &&\n        NULL != desc && LIBXSMM_KERNEL_KIND_MATMUL == desc->kind)\n    {\n      info->iprecision = (libxsmm_gemm_precision)LIBXSMM_GETENUM_INP(desc->gemm.desc.datatype);\n      info->oprecision = (libxsmm_gemm_precision)LIBXSMM_GETENUM_OUT(desc->gemm.desc.datatype);\n      info->prefetch = (libxsmm_gemm_prefetch_type)desc->gemm.desc.prefetch;\n      info->flags = desc->gemm.desc.flags;\n      info->lda = desc->gemm.desc.lda;\n      info->ldb = desc->gemm.desc.ldb;\n      info->ldc = desc->gemm.desc.ldc;\n      info->m = desc->gemm.desc.m;\n      info->n = desc->gemm.desc.n;\n      info->k = desc->gemm.desc.k;\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        if (NULL == code.ptr_const) {\n          fprintf(stderr, \"LIBXSMM ERROR: NULL-kernel cannot be inspected!\\n\");\n        }\n        else {\n          fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n        }\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_transkernel_info(libxsmm_xtransfunction kernel, libxsmm_transkernel_info* info)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xtrans = kernel;\n  if (NULL != info) {\n    const libxsmm_descriptor* desc;\n    if (NULL != libxsmm_get_kernel_xinfo(code, &desc, NULL/*code_size*/) &&\n        NULL != desc && LIBXSMM_KERNEL_KIND_TRANS == desc->kind)\n    {\n      info->typesize = desc->trans.desc.typesize;\n      info->ldo = desc->trans.desc.ldo;\n      info->m = desc->trans.desc.m;\n      info->n = desc->trans.desc.n;\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_mcopykernel_info(libxsmm_xmcopyfunction kernel, libxsmm_mcopykernel_info* info)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xmatcopy = kernel;\n  if (NULL != info) {\n    const libxsmm_descriptor* desc;\n    if (NULL != libxsmm_get_kernel_xinfo(code, &desc, NULL/*code_size*/) &&\n        NULL != desc && LIBXSMM_KERNEL_KIND_MCOPY == desc->kind)\n    {\n      info->typesize = desc->mcopy.desc.typesize;\n      info->prefetch = desc->mcopy.desc.prefetch;\n      info->flags = desc->mcopy.desc.flags;\n      info->ldi = desc->mcopy.desc.ldi;\n      info->ldo = desc->mcopy.desc.ldo;\n      info->m = desc->mcopy.desc.m;\n      info->n = desc->mcopy.desc.n;\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_meltwkernel_info(libxsmm_xmeltwfunction kernel, libxsmm_meltwkernel_info* info)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xmateltw = kernel;\n  if (NULL != info) {\n    const libxsmm_descriptor* desc;\n    if (NULL != libxsmm_get_kernel_xinfo(code, &desc, NULL/*code_size*/) &&\n        NULL != desc && LIBXSMM_KERNEL_KIND_MELTW == desc->kind)\n    {\n      info->datatype = desc->meltw.desc.datatype;\n      info->operation = desc->meltw.desc.operation;\n      info->flags = desc->meltw.desc.flags;\n      info->ldi = desc->meltw.desc.ldi;\n      info->ldo = desc->meltw.desc.ldo;\n      info->m = desc->meltw.desc.m;\n      info->n = desc->meltw.desc.n;\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\nLIBXSMM_API int libxsmm_get_registry_info(libxsmm_registry_info* info)\n{\n  int result = EXIT_SUCCESS;\n  LIBXSMM_INIT /* verbosity */\n  if (0 != info && 0 != internal_registry) {\n    size_t i;\n    LIBXSMM_MEMZERO127(info); /* info->nstatic = 0; info->size = 0; */\n    info->nbytes = (LIBXSMM_CAPACITY_REGISTRY) * (sizeof(libxsmm_code_pointer) + sizeof(libxsmm_descriptor));\n    info->capacity = LIBXSMM_CAPACITY_REGISTRY;\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n    info->ncache = internal_cache_size;\n#else\n    info->ncache = 0;\n#endif\n    for (i = 0; i < (LIBXSMM_CAPACITY_REGISTRY); ++i) {\n      libxsmm_code_pointer code = internal_registry[i];\n      if (0 != code.ptr_const && EXIT_SUCCESS == result) {\n        if (0 == (LIBXSMM_CODE_STATIC & code.uval)) { /* check for allocated/generated JIT-code */\n          size_t buffer_size = 0;\n          void* buffer = 0;\n#if defined(LIBXSMM_HASH_COLLISION)\n          code.uval &= ~LIBXSMM_HASH_COLLISION; /* clear collision flag */\n#endif\n          result = libxsmm_get_malloc_xinfo(code.ptr_const, &buffer_size, NULL/*flags*/, &buffer);\n          if (EXIT_SUCCESS == result) {\n            info->nbytes += LIBXSMM_UP2(buffer_size + (((char*)code.ptr_const) - (char*)buffer), LIBXSMM_PAGE_MINSIZE);\n          }\n        }\n        else {\n          ++info->nstatic;\n        }\n        ++info->size;\n      }\n    }\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void* libxsmm_xregister(const void* key, size_t key_size, size_t value_size, const void* value_init)\n{\n  static int error_once = 0;\n  void* result;\n  LIBXSMM_INIT /* verbosity */\n  if (NULL != key && 0 < key_size && LIBXSMM_DESCRIPTOR_MAXSIZE >= key_size) {\n    libxsmm_descriptor wrap;\n    void* dst;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, key_size);\n#endif\n    LIBXSMM_MEMCPY127(wrap.user.desc, key, key_size);\n    wrap.kind = LIBXSMM_KERNEL_KIND_USER;\n    dst = internal_find_code(&wrap, key_size, value_size).ptr;\n    if (NULL != dst) {\n      size_t size;\n      if (EXIT_SUCCESS == libxsmm_get_malloc_xinfo(dst, &size, NULL/*flags*/, NULL/*extra*/)\n        && value_size <= size)\n      {\n        if (NULL != value_init) memcpy(dst, value_init, value_size);\n        result = dst;\n      }\n      else {\n        if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: value too large for previously registered key!\\n\");\n        }\n        result = NULL;\n      }\n    }\n    else result = NULL;\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      if (LIBXSMM_DESCRIPTOR_MAXSIZE >= key_size) {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xregister specified!\\n\");\n      }\n      else {\n        fprintf(stderr, \"LIBXSMM ERROR: libxsmm_xregister has maximum key-size of %i Byte!\\n\",\n          LIBXSMM_DESCRIPTOR_MAXSIZE);\n      }\n    }\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void* libxsmm_xdispatch(const void* key, size_t key_size)\n{\n  void* result;\n  LIBXSMM_INIT /* verbosity */\n#if !defined(NDEBUG)\n  if (NULL != key && 0 < key_size && LIBXSMM_DESCRIPTOR_MAXSIZE >= key_size)\n#endif\n  {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, key_size);\n#endif\n    LIBXSMM_MEMCPY127(wrap.user.desc, key, key_size);\n    wrap.kind = LIBXSMM_KERNEL_KIND_USER;\n    result = internal_find_code(&wrap, key_size, 0/*user_size*/).ptr;\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xdispatch specified!\\n\");\n    }\n    result = NULL;\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_xrelease(const void* key, size_t key_size)\n{\n  libxsmm_release_kernel(libxsmm_xdispatch(key, key_size));\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_xmmdispatch(const libxsmm_gemm_descriptor* descriptor)\n{\n  libxsmm_xmmfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.gemm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_MATMUL;\n    if (0 != (0x80 & descriptor->prefetch)) { /* \"sign\"-bit of byte-value is set */\n      wrap.gemm.desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n    }\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xgemm;\n#if defined(_DEBUG)\n    if (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity && INT_MAX != libxsmm_verbosity && NULL != result.xmm) {\n      LIBXSMM_STDIO_ACQUIRE();\n      fprintf(stderr, \"\\nLIBXSMM: \");\n      libxsmm_gemm_xprint(stderr, result, NULL/*a*/, NULL/*b*/, NULL/*c*/);\n      LIBXSMM_STDIO_RELEASE();\n    }\n#endif\n  }\n  else { /* quietly accept NULL-descriptor */\n    result.xmm = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction libxsmm_dmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.dmm;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction libxsmm_smmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.smm;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction libxsmm_bsmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bsmm;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction libxsmm_bmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bmm;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction libxsmm_wimmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.wimm;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction libxsmm_ssbimmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.ssbimm;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction libxsmm_usbimmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.usbimm;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction libxsmm_subimmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.subimm;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction libxsmm_uubimmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.uubimm;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction libxsmm_sububmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.sububmm;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch_addr libxsmm_dmmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.dmra;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch_addr libxsmm_smmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.smra;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch_addr libxsmm_bsmmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bsmra;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch_addr libxsmm_bmmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bmra;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction_reducebatch_addr libxsmm_wimmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.wimra;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction_reducebatch_addr libxsmm_ssbimmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.ssbimra;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction_reducebatch_addr libxsmm_usbimmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.usbimra;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction_reducebatch_addr libxsmm_subimmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.subimra;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction_reducebatch_addr libxsmm_uubimmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.uubimra;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction_reducebatch_addr libxsmm_sububmmdispatch_reducebatch_addr(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.sububmra;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch_addr libxsmm_dmmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.dmra;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch_addr libxsmm_smmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.smra;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch_addr libxsmm_bsmmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.bsmra;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch_addr libxsmm_bmmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.bmra;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction_reducebatch_addr libxsmm_wimmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.wimra;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction_reducebatch_addr libxsmm_ssbimmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.ssbimra;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction_reducebatch_addr libxsmm_usbimmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.usbimra;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction_reducebatch_addr libxsmm_subimmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.subimra;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction_reducebatch_addr libxsmm_uubimmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.uubimra;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction_reducebatch_addr libxsmm_sububmmdispatch_reducebatch_addr_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_ADDRESS, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.sububmra;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch_offs libxsmm_dmmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.dmro;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch_offs libxsmm_smmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.smro;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch_offs libxsmm_bsmmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bsmro;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch_offs libxsmm_bmmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bmro;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction_reducebatch_offs libxsmm_wimmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.wimro;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction_reducebatch_offs libxsmm_ssbimmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.ssbimro;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction_reducebatch_offs libxsmm_usbimmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.usbimro;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction_reducebatch_offs libxsmm_subimmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.subimro;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction_reducebatch_offs libxsmm_uubimmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.uubimro;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction_reducebatch_offs libxsmm_sububmmdispatch_reducebatch_offs(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.sububmro;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch_offs libxsmm_dmmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.dmro;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch_offs libxsmm_smmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.smro;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch_offs libxsmm_bsmmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.bsmro;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch_offs libxsmm_bmmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.bmro;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction_reducebatch_offs libxsmm_wimmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.wimro;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction_reducebatch_offs libxsmm_ssbimmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.ssbimro;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction_reducebatch_offs libxsmm_usbimmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.usbimro;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction_reducebatch_offs libxsmm_subimmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.subimro;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction_reducebatch_offs libxsmm_uubimmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.uubimro;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction_reducebatch_offs libxsmm_sububmmdispatch_reducebatch_offs_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_OFFSET, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  result = libxsmm_xmmdispatch(desc);\n  return result.sububmro;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch_strd libxsmm_dmmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.dmrs;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch_strd libxsmm_smmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.smrs;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch_strd libxsmm_bsmmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.bsmrs;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch_strd libxsmm_bmmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.bmrs;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction_reducebatch_strd libxsmm_wimmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.wimrs;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction_reducebatch_strd libxsmm_ssbimmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.ssbimrs;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction_reducebatch_strd libxsmm_usbimmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.usbimrs;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction_reducebatch_strd libxsmm_subimmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.subimrs;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction_reducebatch_strd libxsmm_uubimmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.uubimrs;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction_reducebatch_strd libxsmm_sububmmdispatch_reducebatch_strd(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.sububmrs;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch_strd libxsmm_dmmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.dmrs;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch_strd libxsmm_smmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.smrs;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch_strd libxsmm_bsmmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.bsmrs;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch_strd libxsmm_bmmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.bmrs;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction_reducebatch_strd libxsmm_wimmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.wimrs;\n}\n\n\nLIBXSMM_API libxsmm_ssbimmfunction_reducebatch_strd libxsmm_ssbimmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.ssbimrs;\n}\n\n\nLIBXSMM_API libxsmm_usbimmfunction_reducebatch_strd libxsmm_usbimmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_A_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.usbimrs;\n}\n\n\nLIBXSMM_API libxsmm_subimmfunction_reducebatch_strd libxsmm_subimmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.subimrs;\n}\n\n\nLIBXSMM_API libxsmm_uubimmfunction_reducebatch_strd libxsmm_uubimmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_AB_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.uubimrs;\n}\n\n\nLIBXSMM_API libxsmm_sububmmfunction_reducebatch_strd libxsmm_sububmmdispatch_reducebatch_strd_unroll(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k, libxsmm_blasint stride_a, libxsmm_blasint stride_b, libxsmm_blasint unroll_hint,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS | LIBXSMM_GEMM_FLAG_VNNI_A : *flags);\n  libxsmm_descriptor_blob blob;\n  /*const*/ libxsmm_gemm_descriptor *const desc = libxsmm_bbgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_B_UNSIGNED | LIBXSMM_GEMM_FLAG_C_UNSIGNED | LIBXSMM_GEMM_FLAG_BATCH_REDUCE_STRIDE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result;\n  desc->c1 = (unsigned long long)stride_a;\n  desc->c2 = (unsigned long long)stride_b;\n  desc->c3 = (unsigned char)(unroll_hint < 127 ? unroll_hint : 0);\n  if ( (stride_a < 0) || (stride_b < 0) ) {\n    return NULL;\n  }\n  result = libxsmm_xmmdispatch(desc);\n  return result.sububmrs;\n}\n\n\nLIBXSMM_API libxsmm_xmcopyfunction libxsmm_dispatch_mcopy(const libxsmm_mcopy_descriptor* descriptor)\n{\n  libxsmm_xmcopyfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.mcopy.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_MCOPY;\n#if defined(_WIN32) || defined(__CYGWIN__)\n    wrap.mcopy.desc.prefetch = 0;\n#endif\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xmatcopy;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_xmeltwfunction libxsmm_dispatch_meltw(const libxsmm_meltw_descriptor* descriptor)\n{\n  libxsmm_xmeltwfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.meltw.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_MELTW;\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xmateltw;\n  }\n  else {\n    result.xmeltw = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_copy libxsmm_dispatch_meltw_copy(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    0, LIBXSMM_MELTW_OPERATION_COPY);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_copy;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_zero libxsmm_dispatch_meltw_zero(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    0, LIBXSMM_MELTW_OPERATION_ZERO);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_zero;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_add libxsmm_dispatch_meltw_add(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    0, LIBXSMM_MELTW_OPERATION_ADD);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_add;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_mul libxsmm_dispatch_meltw_mul(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    0, LIBXSMM_MELTW_OPERATION_MUL);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_mul;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_relu libxsmm_dispatch_meltw_relu(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    0, LIBXSMM_MELTW_OPERATION_RELU);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_relu;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_cvtfp32bf16 libxsmm_dispatch_meltw_cvtfp32bf16(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    0, LIBXSMM_MELTW_OPERATION_CVTFP32BF16);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_cvtfp32bf16;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_cvtfp32bf16_act libxsmm_dispatch_meltw_cvtfp32bf16_act(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type, libxsmm_meltw_cvta_flags flags) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    libxsmm_get_meltw_comp_cvta_flags( flags ), LIBXSMM_MELTW_OPERATION_CVTFP32BF16_ACT);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_cvtfp32bf16_act;\n}\n\nLIBXSMM_API libxsmm_meltwfunction_act_cvtfp32bf16 libxsmm_dispatch_meltw_act_cvtfp32bf16(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type, libxsmm_meltw_acvt_flags flags) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    libxsmm_get_meltw_comp_acvt_flags( flags ), LIBXSMM_MELTW_OPERATION_ACT_CVTFP32BF16);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_act_cvtfp32bf16;\n}\n\nLIBXSMM_API libxsmm_meltwfunction_reduce libxsmm_dispatch_meltw_reduce(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type, libxsmm_meltw_redu_flags flags) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    libxsmm_get_meltw_comp_redu_flags( flags ), LIBXSMM_MELTW_OPERATION_REDUCE);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_reduce;\n}\n\n\nLIBXSMM_API libxsmm_meltwfunction_scale libxsmm_dispatch_meltw_scale(libxsmm_blasint m, libxsmm_blasint n, const libxsmm_blasint* ldi, const libxsmm_blasint* ldo, libxsmm_datatype in_type, libxsmm_datatype out_type, libxsmm_meltw_scal_flags flags) {\n  libxsmm_descriptor_blob blob;\n  const libxsmm_meltw_descriptor *const desc = libxsmm_meltw_descriptor_init(&blob,\n    in_type, out_type, m, n, (ldi == NULL) ? m : *ldi, (ldo == NULL) ? m : *ldo,\n    libxsmm_get_meltw_comp_scal_flags( flags ), LIBXSMM_MELTW_OPERATION_SCALE);\n\n  libxsmm_xmeltwfunction result = libxsmm_dispatch_meltw(desc);\n\n  return result.meltw_scale;\n}\n\n\nLIBXSMM_API libxsmm_xtransfunction libxsmm_dispatch_trans(const libxsmm_trans_descriptor* descriptor)\n{\n  libxsmm_xtransfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.trans.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_TRANS;\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xtrans;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_pgemm_xfunction libxsmm_dispatch_pgemm(const libxsmm_pgemm_descriptor* descriptor)\n{\n  libxsmm_trmm_xfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.pgemm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_PGEMM;\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xpgemm;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_getrf_xfunction libxsmm_dispatch_getrf(const libxsmm_getrf_descriptor* descriptor)\n{\n  libxsmm_trmm_xfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.getrf.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_GETRF;\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xgetrf;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_trmm_xfunction libxsmm_dispatch_trmm(const libxsmm_trmm_descriptor* descriptor)\n{\n  libxsmm_trmm_xfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.trmm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_TRMM;\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xtrmm;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_trsm_xfunction libxsmm_dispatch_trsm(const libxsmm_trsm_descriptor* descriptor)\n{\n  libxsmm_trsm_xfunction result;\n  LIBXSMM_INIT /* verbosity */\n  LIBXSMM_ASSERT((sizeof(*descriptor) + sizeof(libxsmm_descriptor_kind)) <= (LIBXSMM_DESCRIPTOR_MAXSIZE));\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.trsm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_TRSM;\n    result = internal_find_code(&wrap, sizeof(*descriptor), 0/*user_size*/).xtrsm;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_xcsr_soa(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* row_ptr, const unsigned int* column_idx, const void* values, unsigned int packed_width)\n{\n  libxsmm_code_pointer result = { 0 };\n  LIBXSMM_INIT\n  if (NULL != descriptor && NULL != row_ptr && NULL != column_idx && NULL != values) {\n    libxsmm_csr_soa_descriptor srsoa;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      srsoa.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      srsoa.gemm = &desc;\n    }\n    srsoa.row_ptr = row_ptr;\n    srsoa.column_idx = column_idx;\n    srsoa.values = values;\n    srsoa.packed_width = packed_width;\n    request.descriptor.srsoa = &srsoa;\n    request.kind = LIBXSMM_BUILD_KIND_SRSOA;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_xcsc_soa(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* column_ptr, const unsigned int* row_idx, const void* values, unsigned int packed_width)\n{\n  libxsmm_code_pointer result = { 0 };\n  LIBXSMM_INIT\n  if (NULL != descriptor && NULL != column_ptr && NULL != row_idx && NULL != values) {\n    libxsmm_csc_soa_descriptor scsoa;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      scsoa.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      scsoa.gemm = &desc;\n    }\n    scsoa.column_ptr = column_ptr;\n    scsoa.row_idx = row_idx;\n    scsoa.values = values;\n    scsoa.packed_width = packed_width;\n    request.descriptor.scsoa = &scsoa;\n    request.kind = LIBXSMM_BUILD_KIND_SCSOA;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_pgemm_ac_rm(const libxsmm_gemm_descriptor* descriptor, unsigned int packed_width)\n{\n  libxsmm_code_pointer result = { 0 };\n  LIBXSMM_INIT\n  if (NULL != descriptor) {\n    libxsmm_pgemm_ac_rm_descriptor pgemmacrm;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      pgemmacrm.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      pgemmacrm.gemm = &desc;\n    }\n    pgemmacrm.packed_width = packed_width;\n    request.descriptor.pgemmacrm = &pgemmacrm;\n    request.kind = LIBXSMM_BUILD_KIND_PGEMMRMAC;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_pgemm_bc_rm(const libxsmm_gemm_descriptor* descriptor, unsigned int packed_width)\n{\n  libxsmm_code_pointer result = { 0 };\n  LIBXSMM_INIT\n  if (NULL != descriptor) {\n    libxsmm_pgemm_bc_rm_descriptor pgemmbcrm;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      pgemmbcrm.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      pgemmbcrm.gemm = &desc;\n    }\n    pgemmbcrm.packed_width = packed_width;\n    request.descriptor.pgemmbcrm = &pgemmbcrm;\n    request.kind = LIBXSMM_BUILD_KIND_PGEMMRMBC;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction libxsmm_create_dcsr_reg(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* row_ptr, const unsigned int* column_idx, const double* values)\n{\n  libxsmm_code_pointer result = { 0 };\n  LIBXSMM_INIT\n  if (NULL != descriptor && NULL != row_ptr && NULL != column_idx && NULL != values) {\n    libxsmm_csr_reg_descriptor sreg;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      sreg.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      sreg.gemm = &desc;\n    }\n    sreg.row_ptr = row_ptr;\n    sreg.column_idx = column_idx;\n    sreg.values = values;\n    request.descriptor.sreg = &sreg;\n    request.kind = LIBXSMM_BUILD_KIND_SREG;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm.dmm;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction libxsmm_create_scsr_reg(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* row_ptr, const unsigned int* column_idx, const float* values)\n{\n  libxsmm_code_pointer result = { 0 };\n  LIBXSMM_INIT\n  if (NULL != descriptor && NULL != row_ptr && NULL != column_idx && NULL != values) {\n    libxsmm_csr_reg_descriptor sreg;\n    libxsmm_build_request request;\n    const unsigned int n = row_ptr[descriptor->m];\n    double *const d_values = (double*)(0 != n ? malloc(n * sizeof(double)) : NULL);\n    if (NULL != d_values) {\n      libxsmm_gemm_descriptor desc;\n      unsigned int i;\n      /* we need to copy the values into a double precision buffer */\n      for (i = 0; i < n; ++i) d_values[i] = (double)values[i];\n      if (0 == (0x80 & descriptor->prefetch)) {\n        sreg.gemm = descriptor;\n      }\n      else { /* \"sign\"-bit of byte-value is set */\n        LIBXSMM_ASSIGN127(&desc, descriptor);\n        desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n        sreg.gemm = &desc;\n      }\n      sreg.row_ptr = row_ptr;\n      sreg.column_idx = column_idx;\n      sreg.values = d_values;\n      request.descriptor.sreg = &sreg;\n      request.kind = LIBXSMM_BUILD_KIND_SREG;\n      libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n      free(d_values);\n    }\n  }\n  return result.xgemm.smm;\n}\n\n\nLIBXSMM_API void libxsmm_release_kernel(const void* kernel)\n{\n  if (NULL != kernel) {\n    static int error_once = 0;\n    libxsmm_kernel_xinfo* extra = NULL;\n    void *const extra_address = &extra;\n    LIBXSMM_INIT\n    if (EXIT_SUCCESS == libxsmm_get_malloc_xinfo(kernel, NULL/*size*/, NULL/*flags*/, (void**)extra_address) && NULL != extra) {\n      const unsigned int regindex = extra->registered;\n      if ((LIBXSMM_CAPACITY_REGISTRY) <= regindex) {\n        libxsmm_xfree(kernel, 0/*no check*/);\n      }\n      else { /* attempt to unregister kernel */\n        libxsmm_kernel_info info;\n#if !defined(LIBXSMM_ENABLE_DEREG)\n        if (EXIT_SUCCESS == libxsmm_get_kernel_info(kernel, &info)\n          && LIBXSMM_KERNEL_KIND_USER == info.kind)\n#endif\n        {\n          LIBXSMM_ASSERT(LIBXSMM_KERNEL_UNREGISTERED > info.kind);\n          /* coverity[check_return] */\n          LIBXSMM_ATOMIC_ADD_FETCH(&libxsmm_ninit, 1, LIBXSMM_ATOMIC_RELAXED); /* invalidate code cache (TLS) */\n          internal_registry[regindex].ptr = NULL;\n#if !defined(NDEBUG)\n          LIBXSMM_MEMZERO127(internal_registry_keys + regindex);\n#endif\n          libxsmm_xfree(kernel, 0/*no check*/);\n        }\n#if !defined(LIBXSMM_ENABLE_DEREG)\n        else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM WARNING: attempt to unregister JIT-kernel!\\n\");\n        }\n#endif\n      }\n    }\n    else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: failed to release kernel!\\n\");\n    }\n  }\n}\n\n\n#if defined(LIBXSMM_BUILD) && (!defined(LIBXSMM_NOFORTRAN) || defined(__clang_analyzer__))\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_init)(void);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_init)(void)\n{\n  libxsmm_init();\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_finalize)(void);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_finalize)(void)\n{\n  libxsmm_finalize();\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_release_kernel)(const void** /*kernel*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_release_kernel)(const void** kernel)\n{\n#if !defined(NDEBUG)\n  if (NULL != kernel)\n#endif\n  {\n    libxsmm_release_kernel(*kernel);\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument passed into libxsmm_release_kernel!\\n\");\n    }\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch2)(intptr_t* /*fn*/, const int* /*iprec*/, const int* /*oprec*/,\n  const libxsmm_blasint* /*m*/, const libxsmm_blasint* /*n*/, const libxsmm_blasint* /*k*/,\n  const libxsmm_blasint* /*lda*/, const libxsmm_blasint* /*ldb*/, const libxsmm_blasint* /*ldc*/,\n  const void* /*alpha*/, const void* /*beta*/, const int* /*flags*/, const int* /*prefetch*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch2)(intptr_t* fn, const int* iprec, const int* oprec,\n  const libxsmm_blasint* m, const libxsmm_blasint* n, const libxsmm_blasint* k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const void* alpha, const void* beta, const int* flags, const int* prefetch)\n{\n#if !defined(NDEBUG)\n  if (NULL != fn && NULL != m\n    && (NULL == iprec || (0 <= *iprec && *iprec < LIBXSMM_DATATYPE_UNSUPPORTED))\n    && (NULL == oprec || (0 <= *oprec && *oprec < LIBXSMM_DATATYPE_UNSUPPORTED)))\n#endif\n  {\n    const int gemm_flags = (NULL != flags ? *flags : LIBXSMM_FLAGS);\n    const libxsmm_gemm_descriptor* descriptor;\n    libxsmm_gemm_prefetch_type gemm_prefetch;\n    libxsmm_descriptor_blob blob;\n    libxsmm_code_pointer result;\n#if !defined(NDEBUG)\n    const libxsmm_gemm_precision itype = (NULL != iprec ? ((libxsmm_gemm_precision)*iprec) : LIBXSMM_GEMM_PRECISION_F64);\n    const libxsmm_gemm_precision otype = (NULL != oprec ? ((libxsmm_gemm_precision)*oprec) : itype);\n    const libxsmm_blasint kk = *(NULL != k ? k : m), nn = (NULL != n ? *n : kk);\n#else\n    const libxsmm_gemm_precision itype = (libxsmm_gemm_precision)*iprec, otype = (libxsmm_gemm_precision)*oprec;\n    const libxsmm_blasint kk = *k, nn = *n;\n#endif\n    LIBXSMM_PRAGMA_FORCEINLINE\n    gemm_prefetch = libxsmm_get_gemm_xprefetch(prefetch);\n    LIBXSMM_PRAGMA_FORCEINLINE\n    descriptor = libxsmm_gemm_descriptor_init2(&blob, itype, otype, *m, nn, kk,\n        NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? *m : kk),\n        NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? kk : nn),\n      *(NULL != ldc ? ldc : m), alpha, beta, gemm_flags, gemm_prefetch);\n#if !defined(NDEBUG)\n    if (NULL != descriptor)\n#endif\n    {\n      LIBXSMM_PRAGMA_FORCEINLINE\n      result.xgemm = libxsmm_xmmdispatch(descriptor);\n      *fn = result.ival;\n    }\n#if !defined(NDEBUG)\n    else { /* quiet */\n      *fn = 0;\n    }\n#endif\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument passed into libxsmm_xmmdispatch!\\n\");\n    }\n    if (NULL != fn) *fn = 0;\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch)(intptr_t* /*fn*/, const int* /*precision*/,\n  const libxsmm_blasint* /*m*/, const libxsmm_blasint* /*n*/, const libxsmm_blasint* /*k*/,\n  const libxsmm_blasint* /*lda*/, const libxsmm_blasint* /*ldb*/, const libxsmm_blasint* /*ldc*/,\n  const void* /*alpha*/, const void* /*beta*/, const int* /*flags*/, const int* /*prefetch*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch)(intptr_t* fn, const int* precision,\n  const libxsmm_blasint* m, const libxsmm_blasint* n, const libxsmm_blasint* k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const void* alpha, const void* beta, const int* flags, const int* prefetch)\n{\n  LIBXSMM_FSYMBOL(libxsmm_xmmdispatch2)(fn, precision, precision, m, n, k, lda, ldb, ldc, alpha, beta, flags, prefetch);\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_abc)(\n  const libxsmm_xmmfunction* /*fn*/, const void* /*a*/, const void* /*b*/, void* /*c*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_abc)(\n  const libxsmm_xmmfunction* fn, const void* a, const void* b, void* c)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != fn && NULL != a && NULL != b && NULL != c)\n#endif\n  {\n#if !defined(NDEBUG)\n    if (NULL != fn->xmm)\n#endif\n    {\n      fn->xmm(a, b, c);\n    }\n#if !defined(NDEBUG)\n    else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: NULL-function passed into libxsmm_xmmcall_abc!\\n\");\n    }\n#endif\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xmmcall_abc specified!\\n\");\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_prf)(\n  const libxsmm_xmmfunction* /*fn*/, const void* /*a*/, const void* /*b*/, void* /*c*/,\n  const void* /*pa*/, const void* /*pb*/, const void* /*pc*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_prf)(\n  const libxsmm_xmmfunction* fn, const void* a, const void* b, void* c,\n  const void* pa, const void* pb, const void* pc)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != fn && NULL != a && NULL != b && NULL != c)\n#endif\n  {\n#if !defined(NDEBUG)\n    if (NULL != fn->xmm)\n#endif\n    {\n      fn->xmm(a, b, c, pa, pb, pc);\n    }\n#if !defined(NDEBUG)\n    else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: NULL-function passed into libxsmm_xmmcall_prf!\\n\");\n    }\n#endif\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xmmcall_prf specified!\\n\");\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall)(\n  const libxsmm_xmmfunction* /*fn*/, const void* /*a*/, const void* /*b*/, void* /*c*/,\n  const void* /*pa*/, const void* /*pb*/, const void* /*pc*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall)(\n  const libxsmm_xmmfunction* fn, const void* a, const void* b, void* c,\n  const void* pa, const void* pb, const void* pc)\n{\n  LIBXSMM_FSYMBOL(libxsmm_xmmcall_prf)(fn, a, b, c, pa, pb, pc);\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xregister)(void** /*regval*/,\n  const void* /*key*/, const int* /*keysize*/, const int* /*valsize*/, const void* /*valinit*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xregister)(void** regval,\n  const void* key, const int* keysize, const int* valsize, const void* valinit)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != regval && NULL != key && NULL != keysize && NULL != valsize)\n#endif\n  {\n    *regval = libxsmm_xregister(key, *keysize, *valsize, valinit);\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xregister specified!\\n\");\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xdispatch)(void** /*regval*/, const void* /*key*/, const int* /*keysize*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xdispatch)(void** regval, const void* key, const int* keysize)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != regval && NULL != key && NULL != keysize)\n#endif\n  {\n    *regval = libxsmm_xdispatch(key, *keysize);\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xdispatch specified!\\n\");\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xrelease)(const void* /*key*/, const int* /*keysize*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xrelease)(const void* key, const int* keysize)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != key && NULL != keysize)\n#endif\n  {\n    libxsmm_xrelease(key, *keysize);\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xrelease specified!\\n\");\n  }\n#endif\n}\n\n#endif /*defined(LIBXSMM_BUILD) && (!defined(LIBXSMM_NOFORTRAN) || defined(__clang_analyzer__))*/\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/src/libxsmm_main.h": "/******************************************************************************\n* Copyright (c) Intel Corporation - All rights reserved.                      *\n* This file is part of the LIBXSMM library.                                   *\n*                                                                             *\n* For information on the license, see the LICENSE file.                       *\n* Further information: https://github.com/hfp/libxsmm/                        *\n* SPDX-License-Identifier: BSD-3-Clause                                       *\n******************************************************************************/\n/* Hans Pabst (Intel Corp.)\n******************************************************************************/\n#ifndef LIBXSMM_MAIN_H\n#define LIBXSMM_MAIN_H\n\n#include <libxsmm.h>\n/**\n * TF includes src/libxsmm_main.h and uses LIBXSMM's sync primitives\n * without including libxsmm_sync. However, libxsmm_sync.h shall be\n * an explicit include separate from including libxsmm.h.\n */\n#include \"libxsmm_sync.h\"\n\n/** Allow external definition to enable testing corner cases (exhausted registry space). */\n#if !defined(LIBXSMM_CAPACITY_REGISTRY) /* must be POT */\n# define LIBXSMM_CAPACITY_REGISTRY 131072\n#endif\n#if !defined(LIBXSMM_CAPACITY_CACHE) /* must be POT */\n# define LIBXSMM_CAPACITY_CACHE 16\n#endif\n\n#if !defined(LIBXSMM_PAGE_MINSIZE)\n# define LIBXSMM_PAGE_MINSIZE 4096 /* 4 KB */\n#endif\n\n#if !defined(LIBXSMM_NTHREADS_MAX)\n# if (0 != LIBXSMM_SYNC)\n#   define LIBXSMM_NTHREADS_MAX 1024\n# else\n#   define LIBXSMM_NTHREADS_MAX 1\n# endif\n#endif\n/* code relies on LIBXSMM_NTHREADS_MAX or v/forks */\n#if !defined(LIBXSMM_NTHREADS_USE) && 1\n# define LIBXSMM_NTHREADS_USE\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS)\n# define LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS LIBXSMM_NTHREADS_MAX\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_SCALE)\n# define LIBXSMM_MALLOC_SCRATCH_SCALE 1.0\n#endif\n#if !defined(LIBXSMM_MALLOC_LIMIT)\n# define LIBXSMM_MALLOC_LIMIT (2U << 20) /* 2 MB */\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_REALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_REALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_CALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_CALLOC\n#endif\n/* align even if interceptor is disabled at runtime */\n#if !defined(LIBXSMM_MALLOC_ALIGN_ALL) && 1\n# define LIBXSMM_MALLOC_ALIGN_ALL\n#endif\n#if !defined(LIBXSMM_MALLOC_INTERNAL_CALLER_ID)\n# define LIBXSMM_MALLOC_INTERNAL_CALLER_ID ((uintptr_t)LIBXSMM_UNLIMITED)\n#endif\n#if !defined(LIBXSMM_MALLOC_INTERNAL_CALLER)\n# define LIBXSMM_MALLOC_INTERNAL_CALLER ((const void*)(LIBXSMM_MALLOC_INTERNAL_CALLER_ID))\n#endif\n\n#if !defined(LIBXSMM_INTERCEPT_DYNAMIC) && defined(LIBXSMM_BUILD) && \\\n  (defined(__GNUC__) || defined(_CRAYC)) && !defined(_WIN32) && !defined(__CYGWIN__) && \\\n  !(defined(__APPLE__) && defined(__MACH__) && LIBXSMM_VERSION2(6, 1) >= \\\n    LIBXSMM_VERSION2(__clang_major__, __clang_minor__))\n# define LIBXSMM_INTERCEPT_DYNAMIC\n#endif\n\n#if !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) && defined(LIBXSMM_INTERCEPT_DYNAMIC) && \\\n  defined(LIBXSMM_MALLOC) && (0 != LIBXSMM_MALLOC) && \\\n  (!defined(_CRAYC) && !defined(__TRACE)) /* TODO */ && \\\n  (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n# define LIBXSMM_MALLOC_HOOK_DYNAMIC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_STATIC) && \\\n  defined(LIBXSMM_MALLOC) && (0 != LIBXSMM_MALLOC) && \\\n  (!defined(_WIN32)) /* TODO */ && \\\n  (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD))) /* GLIBC */\n# define LIBXSMM_MALLOC_HOOK_STATIC\n#endif\n#if !defined(LIBXSMM_DNN_CONVOLUTION_SETUP_USE_NTS) && \\\n     defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) && \\\n     defined(LIBXSMM_MALLOC_ALIGN_ALL)\n# define LIBXSMM_DNN_CONVOLUTION_SETUP_USE_NTS\n#endif\n\n#if defined(LIBXSMM_INTERCEPT_DYNAMIC)\n# if defined(LIBXSMM_OFFLOAD_TARGET)\n#   pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n# endif\n# include <dlfcn.h>\n# if defined(LIBXSMM_OFFLOAD_TARGET)\n#   pragma offload_attribute(pop)\n# endif\n# if !defined(RTLD_NEXT)\n#   define LIBXSMM_RTLD_NEXT ((void*)-1l)\n# else\n#   define LIBXSMM_RTLD_NEXT RTLD_NEXT\n# endif\n#endif\n\n#if !defined(LIBXSMM_VERBOSITY_HIGH)\n# define LIBXSMM_VERBOSITY_HIGH 3 /* secondary warning or info-verbosity */\n#endif\n#if !defined(LIBXSMM_VERBOSITY_WARN)\n# define LIBXSMM_VERBOSITY_WARN ((LIBXSMM_VERBOSITY_HIGH) - LIBXSMM_MIN(1, LIBXSMM_VERBOSITY_HIGH))\n#endif\n\n#if !defined(LIBXSMM_LOCK)\n# define LIBXSMM_LOCK LIBXSMM_LOCK_DEFAULT\n#endif\n\n#if !defined(LIBXSMM_EXT_MIN_NTASKS)\n# define LIBXSMM_MIN_NTASKS(NT) 1\n#endif\n#if !defined(LIBXSMM_OVERHEAD)\n# define LIBXSMM_OVERHEAD(NT) 0\n#endif\n#if !defined(LIBXSMM_NOOP_ARGS)\n# define LIBXSMM_NOOP_ARGS(...)\n#endif\n#if !defined(LIBXSMM_NOOP)\n# define LIBXSMM_NOOP\n#endif\n\n/** Check if M, N, K, or LDx fits into the descriptor. */\n#if (0 != LIBXSMM_ILP64)\n# define LIBXSMM_GEMM_NO_BYPASS_DIMS(M, N, K) (0xFFFFFFFF >= (M) && 0xFFFFFFFF >= (N) && 0xFFFFFFFF >= (K))\n#else /* always fits */\n# define LIBXSMM_GEMM_NO_BYPASS_DIMS(M, N, K) 1\n#endif\n\n#if defined(LIBXSMM_ASSERT) /* assert available */\n# define LIBXSMM_GEMM_DESCRIPTOR_DIM_CHECK(M, N, K) LIBXSMM_ASSERT(LIBXSMM_GEMM_NO_BYPASS_DIMS(M, N, K))\n#else\n# define LIBXSMM_GEMM_DESCRIPTOR_DIM_CHECK(M, N, K)\n#endif\n\n#if defined(LIBXSMM_UNPACKED)\n# define LIBXSMM_DESCRIPTOR_CLEAR_AUX(DST, SIZE) LIBXSMM_MEMSET127(DST, 0, SIZE)\n#else\n# define LIBXSMM_DESCRIPTOR_CLEAR_AUX(DST, SIZE)\n#endif\n#define LIBXSMM_DESCRIPTOR_CLEAR(BLOB) \\\n  LIBXSMM_ASSERT((LIBXSMM_DESCRIPTOR_MAXSIZE) == sizeof(*(BLOB))); \\\n  LIBXSMM_DESCRIPTOR_CLEAR_AUX(BLOB, LIBXSMM_DESCRIPTOR_MAXSIZE)\n\n/** Low-level/internal GEMM descriptor initialization. */\n#define LIBXSMM_GEMM_DESCRIPTOR(DESCRIPTOR, DATA_TYPE, FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH) \\\n  LIBXSMM_GEMM_DESCRIPTOR_DIM_CHECK(LDA, LDB, LDC); \\\n  LIBXSMM_GEMM_DESCRIPTOR_DIM_CHECK(M, N, K); \\\n  LIBXSMM_DESCRIPTOR_CLEAR_AUX(&(DESCRIPTOR), sizeof(DESCRIPTOR)); \\\n  (DESCRIPTOR).datatype = (unsigned char)(DATA_TYPE); (DESCRIPTOR).prefetch = (unsigned char)(PREFETCH); \\\n  (DESCRIPTOR).flags = (unsigned int)((FLAGS) \\\n    /*| (LIBXSMM_NEQ(0, ALPHA) ? 0 : LIBXSMM_GEMM_FLAG_ALPHA_0)*/ \\\n    | (LIBXSMM_NEQ(0, BETA) ? 0 : LIBXSMM_GEMM_FLAG_BETA_0)); \\\n  (DESCRIPTOR).m   = (unsigned int)(M);   (DESCRIPTOR).n   = (unsigned int)(N);   (DESCRIPTOR).k   = (unsigned int)(K); \\\n  (DESCRIPTOR).lda = (unsigned int)(LDA); (DESCRIPTOR).ldb = (unsigned int)(LDB); (DESCRIPTOR).ldc = (unsigned int)(LDC); \\\n  LIBXSMM_PAD((DESCRIPTOR).pad = 0) (DESCRIPTOR).c1 = 0; (DESCRIPTOR).c2 = 0; (DESCRIPTOR).c3 = 0; \\\n  (DESCRIPTOR).meltw_ldx = 0; (DESCRIPTOR).meltw_ldy = 0; (DESCRIPTOR).meltw_ldz = 0; \\\n  (DESCRIPTOR).meltw_datatype_aux = 0; (DESCRIPTOR).meltw_flags = 0; \\\n  (DESCRIPTOR).meltw_operation = 0\n\n/** Similar to LIBXSMM_GEMM_DESCRIPTOR, but separately taking the input-/output-precision. */\n#define LIBXSMM_GEMM_DESCRIPTOR2(DESCRIPTOR, IPREC, OPREC, FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH) \\\n  LIBXSMM_GEMM_DESCRIPTOR(DESCRIPTOR, LIBXSMM_GETENUM(IPREC, OPREC), FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH)\n\n/** Declare and construct a GEMM descriptor. */\n#define LIBXSMM_GEMM_DESCRIPTOR_TYPE(DESCRIPTOR, DATA_TYPE, FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH) \\\n  libxsmm_gemm_descriptor DESCRIPTOR; LIBXSMM_GEMM_DESCRIPTOR(DESCRIPTOR, DATA_TYPE, \\\n    FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH)\n\n/** Similar to LIBXSMM_GEMM_DESCRIPTOR_TYPE, but separately taking the input-/output-precision. */\n#define LIBXSMM_GEMM_DESCRIPTOR2_TYPE(DESCRIPTOR, IPREC, OPREC, FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH) \\\n  LIBXSMM_GEMM_DESCRIPTOR_TYPE(DESCRIPTOR, LIBXSMM_GETENUM(IPREC, OPREC), FLAGS, M, N, K, LDA, LDB, LDC, ALPHA, BETA, PREFETCH)\n\n#define LIBXSMM_REGDESC_DEFAULT\n#define LIBXSMM_REGDESC(START, MODIFIER) \\\n  START libxsmm_gemm_descriptor MODIFIER gemm; \\\n  START libxsmm_mcopy_descriptor MODIFIER mcopy; \\\n  START libxsmm_meltw_descriptor MODIFIER meltw; \\\n  START libxsmm_trans_descriptor MODIFIER trans; \\\n  START libxsmm_pgemm_descriptor MODIFIER pgemm; \\\n  START libxsmm_getrf_descriptor MODIFIER getrf; \\\n  START libxsmm_trmm_descriptor MODIFIER trmm; \\\n  START libxsmm_trsm_descriptor MODIFIER trsm\n\n\n/**\n* Packed structure, which stores the argument description of GEMM routines.\n* The size of the structure is padded to LIBXSMM_DESCRIPTOR_MAXSIZE.\n*/\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_gemm_descriptor {\n  /** Extents of the matrix. */\n  unsigned int m, n, k;\n  /** Leading dimensions. */\n  unsigned int lda, ldb, ldc;\n  /** Set of flags. */\n  unsigned int flags;\n  /** Prefetch strategy. */\n  unsigned char prefetch;\n  /** Denotes the data-type. */\n  unsigned char datatype;\n  /** Ignored entry. */\n  LIBXSMM_PAD(unsigned char pad)\n  /** multipurpose 64bit field, currently used for: a) stride_a in brgemm */\n  unsigned long long c1;\n  /** multipurpose 64bit field, currently used for: a) stride_b in brgemm */\n  unsigned long long c2;\n  /** multipurpose 8bit field, currently used for: a) unroll hint in brgemm */\n  unsigned char c3;\n  /** LDx, LDy, LDz,  additional meltw LDs */\n  unsigned int meltw_ldx, meltw_ldy, meltw_ldz;\n  /** Size of data element. */\n  unsigned char meltw_datatype_aux;\n  /** Set of flags */\n  unsigned char meltw_flags;\n  /** operation specifier */\n  unsigned char meltw_operation;\n};\n\n/** Packed structure storing the matcopy argument description. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_mcopy_descriptor {\n  /** LDx, M, and N. */\n  unsigned int m, n, ldi, ldo;\n  /** Size of data element. */\n  unsigned char typesize;\n  /** Level of unrolling. */\n  unsigned char unroll_level;\n  /** Boolean value (@TODO fix this). */\n  unsigned char prefetch;\n  /** Set of flags. */\n  unsigned char flags;\n};\n\n/** Packed structure storing the mateltw argument description. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_meltw_descriptor {\n  /** LDx, M, and N. */\n  unsigned int m, n, ldi, ldo, ldx, ldy;\n  /** Size of data element. */\n  unsigned char datatype;\n  unsigned char datatype2;\n  /** Set of flags */\n  unsigned char flags;\n  /** operation specifier */\n  unsigned char operation;\n};\n\n/** Packed structure storing the transpose argument description. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_trans_descriptor {\n  /** LD, M, and N. */\n  unsigned int m, n, ldo;\n  /** Size of data element. */\n  unsigned char typesize;\n};\n\n/** Packed structure storing arguments of packed GEMM. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_pgemm_descriptor {\n  unsigned int m, n, k, lda, ldb, ldc;\n  unsigned char typesize;\n  unsigned char layout;\n  char transa, transb;\n  char alpha_val;\n};\n\n/** Packed structure storing arguments of packed GETRF. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_getrf_descriptor {\n  unsigned int m, n, lda;\n  unsigned char typesize;\n  unsigned char layout;\n};\n\n/** Packed structure storing arguments of packed TRSM. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_trmm_descriptor {\n  union { double d; float s; } alpha;\n  unsigned int m, n, lda, ldb;\n  unsigned char typesize;\n  unsigned char layout;\n  char diag, side, uplo;\n  char transa;\n};\n\n/** Packed structure storing arguments of packed TRSM. */\nLIBXSMM_EXTERN_C LIBXSMM_PACKED(struct LIBXSMM_RETARGETABLE) libxsmm_trsm_descriptor {\n  union { double d; float s; } alpha;\n  unsigned int m, n, lda, ldb;\n  unsigned char typesize;\n  unsigned char layout;\n  char diag, side, uplo;\n  char transa;\n};\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE LIBXSMM_MAY_ALIAS libxsmm_csr_soa_descriptor {\n  const libxsmm_gemm_descriptor* gemm;\n  const unsigned int* row_ptr;\n  const unsigned int* column_idx;\n  const void* values;\n  unsigned int packed_width;\n} libxsmm_csr_soa_descriptor;\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE LIBXSMM_MAY_ALIAS libxsmm_csc_soa_descriptor {\n  const libxsmm_gemm_descriptor* gemm;\n  const unsigned int* column_ptr;\n  const unsigned int* row_idx;\n  const void* values;\n  unsigned int packed_width;\n} libxsmm_csc_soa_descriptor;\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE LIBXSMM_MAY_ALIAS libxsmm_pgemm_ac_rm_descriptor {\n  const libxsmm_gemm_descriptor* gemm;\n  unsigned int packed_width;\n} libxsmm_pgemm_ac_rm_descriptor;\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE LIBXSMM_MAY_ALIAS libxsmm_pgemm_bc_rm_descriptor {\n  const libxsmm_gemm_descriptor* gemm;\n  unsigned int packed_width;\n} libxsmm_pgemm_bc_rm_descriptor;\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE LIBXSMM_MAY_ALIAS libxsmm_csr_reg_descriptor {\n  const libxsmm_gemm_descriptor* gemm;\n  const unsigned int* row_ptr;\n  const unsigned int* column_idx;\n  const void* values;\n} libxsmm_csr_reg_descriptor;\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE libxsmm_code_pointer {\n  void (*ptr_fn)(LIBXSMM_VARIADIC);\n  const void* ptr_const;\n  void* ptr;\n  uintptr_t uval;\n  intptr_t ival;\n  libxsmm_xmmfunction xgemm; /* GEMM: smm, dmm, wimm, or void-function */\n  libxsmm_xmcopyfunction xmatcopy;\n  libxsmm_xmeltwfunction xmateltw;\n  libxsmm_xtransfunction xtrans;\n  libxsmm_pgemm_xfunction xpgemm;\n  libxsmm_getrf_xfunction xgetrf;\n  libxsmm_trmm_xfunction xtrmm;\n  libxsmm_trsm_xfunction xtrsm;\n} libxsmm_code_pointer;\n\n/** Structure which describes all tensors in LIBXSMM's DNN module */\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_tensor {\n  libxsmm_dnn_tensor_datalayout* layout;           /* data-layout descriptor */\n  void* data;                                      /* pointer to data */\n  unsigned char scf;                               /* fix point scaling factor for this tensor */\n};\n\n/* Structure to record segment in stream of code */\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE segment_t {\n  int segment_type;\n  int n_convs;\n  int aux_index;\n} segment_t;\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_layer {\n  libxsmm_dnn_datatype datatype_in;\n  libxsmm_dnn_datatype datatype_out;\n  libxsmm_dnn_conv_desc desc;\n  libxsmm_dnn_conv_algo algo;\n  libxsmm_dnn_tensor_format buffer_format;\n  libxsmm_dnn_tensor_format filter_format;\n  libxsmm_dnn_conv_fuse_op fuse_ops;\n  libxsmm_dnn_conv_option options;\n\n  /* additional size for internal data types */\n  int ifhp;\n  int ifwp;\n  int ofh;\n  int ofw;\n  int ofhp;\n  int ofwp;\n  int ifmblock;\n  int ofmblock;\n  int blocksifm;\n  int blocksofm;\n  int fwd_ofw_rb;\n  int fwd_ofh_rb;\n  int bwd_ofw_rb;\n  int bwd_ofh_rb;\n  int upd_ofw_rb;\n  int upd_ofh_rb;\n  int fm_lp_block; /* additional blocking for low precision datatypes of feature maps */\n  int blocksifm_blocking;\n  int blocksofm_blocking;\n  int avoid_acc_load;\n  int avoid_acc_load_bwd;\n  int pack_input;\n  int pack_input_bwd;\n  int spread_input_bwd;\n  int weight_copies;\n  int loop_order;\n  int use_ofm_parallelization;\n  int use_ifm_parallelization;\n  int avoid_fmas_in_rim;\n  int upd_use_batchreduce;\n  int upd_pack_input;\n  int upd_loop_order;\n  int upd_linearized_tasklist;\n  int upd_avoid_rim_fmas;\n  int fwd_flags;\n  int shuffle_filter_accesses;\n  int use_fallback_fwd_loops;\n  int use_fallback_bwd_loops;\n  int input_pixels;\n  int output_pixels;\n  int n_used_pixels;\n  int pixel_blocking;\n  int use_intermediate_f32_wt_tensor;\n  int upd_linearized_pixels;\n  int ifwp_extended;\n  int ofwp_extended;\n  int batchreduce_h_pixels;\n  int on_the_fly_input_packing;\n  int upd_pack_input_upfront;\n  int use_hybrid_imgofm_parallelization;\n  int compute_pixels;\n  int upd_trans_w_only;\n  int fwd_padding_copy;\n  int upd_padding_copy;\n  int block_fwd_oj;\n  int block_fwd_ifm;\n  int block_fwd_ofm;\n  int block_bwd_oj;\n  int block_bwd_ifm;\n  int block_bwd_ofm;\n  int block_upd_ifm;\n  int block_upd_ofm;\n\n  libxsmm_xtransfunction tr_kernel;\n  libxsmm_meltwfunction_cvtfp32bf16 fwd_cvtfp32bf16_kernel;\n\n  /* internal data representation */\n  libxsmm_dnn_tensor* reg_input;\n  libxsmm_dnn_tensor* reg_output;\n  libxsmm_dnn_tensor* reg_filter;\n  libxsmm_dnn_tensor* grad_input;\n  libxsmm_dnn_tensor* grad_output;\n  libxsmm_dnn_tensor* grad_filter;\n  libxsmm_dnn_tensor* reg_bias;\n  libxsmm_dnn_tensor* grad_bias;\n  /* internal data representations for copies of tensors */\n  libxsmm_dnn_tensor* reg_input_tr;\n  libxsmm_dnn_tensor* reg_filter_tr;\n  /* batchnorm stats */\n  libxsmm_dnn_tensor* batch_stats;\n  /* maxstats used in low-precision kernels */\n  libxsmm_dnn_tensor* maxstats_fwd;\n  libxsmm_dnn_tensor* maxstats_bwd;\n  libxsmm_dnn_tensor* maxstats_upd;\n\n  /* barrier */\n  libxsmm_barrier* barrier;\n\n  /* scratch */\n  size_t fwd_packing_padding_scratch_size;\n  size_t fwd_lp_output_full_scratch_size;\n  size_t fwd_lp_output_block_scratch_size;\n  size_t fwd_packing_padding_scratch_offset;\n  size_t fwd_lp_output_full_scratch_offset;\n  size_t fwd_lp_output_block_scratch_offset;\n  size_t fwd_scratch_size;\n\n  size_t bwd_filter_trans_scratch_size;\n  size_t bwd_packing_padding_scratch_size;\n  size_t bwd_lp_input_full_scratch_size;\n  size_t bwd_filter_trans_scratch_offset;\n  size_t bwd_packing_padding_scratch_offset;\n  size_t bwd_lp_input_full_scratch_offset;\n  size_t bwd_scratch_size;\n\n  size_t upd_packing_padding_scratch_size;\n  size_t upd_lp_output_full_scratch_size;\n  size_t upd_lp_input_full_scratch_size;\n  size_t upd_filter_scratch_size;\n  size_t upd_lp_filter_full_scratch_size;\n  size_t upd_packing_padding_scratch_offset;\n  size_t upd_lp_output_full_scratch_offset;\n  size_t upd_lp_input_full_scratch_offset;\n  size_t upd_lp_filter_full_scratch_offset;\n  size_t upd_filter_scratch_offset;\n  size_t upd_scratch_size;\n\n  void* scratch;\n  size_t scratch_size;\n\n  libxsmm_code_pointer gemm_fwd;     /* ability to hoist forward GEMMs */\n  libxsmm_code_pointer gemm_fwd2;    /* ability to hoist forward GEMMs */\n\n  unsigned long long *A_offsets;\n  unsigned long long *B_offsets;\n\n  /* JIT-generated convolution code */\n  libxsmm_code_pointer code_fwd[3];\n  libxsmm_code_pointer code_bwd[3];\n  libxsmm_code_pointer code_upd[2];\n\n  libxsmm_code_pointer matcopy_fwd[4];\n  libxsmm_code_pointer matcopy_bwd[4];\n  libxsmm_code_pointer matcopy_upd[3];\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_fusedbatchnorm {\n  libxsmm_dnn_fusedbatchnorm_desc desc;\n  libxsmm_dnn_tensor* reg_input;      /* input tensor */\n  libxsmm_dnn_tensor* reg_output;     /* output tensor */\n  libxsmm_dnn_tensor* grad_input;     /* grad input tensor */\n  libxsmm_dnn_tensor* grad_output;    /* grad output tensor */\n  libxsmm_dnn_tensor* reg_add;        /* elementwise tensor */\n  libxsmm_dnn_tensor* grad_add;       /* grad elementwise tensor */\n  libxsmm_dnn_tensor* reg_beta;       /* beta tensor */\n  libxsmm_dnn_tensor* reg_gamma;      /* gamma tensor */\n  libxsmm_dnn_tensor* grad_beta;      /* grad beta tensor */\n  libxsmm_dnn_tensor* grad_gamma;     /* grad gamma tensor */\n  libxsmm_dnn_tensor* expvalue;       /* expected value */\n  libxsmm_dnn_tensor* rcpstddev;      /* reciprocal of standard derivation */\n  libxsmm_dnn_tensor* variance;       /* variance */\n  libxsmm_dnn_tensor* relumask;       /* relumask */\n  libxsmm_barrier* barrier;           /* barrier */\n  int ifmblock;\n  int ofmblock;\n  int blocksifm;\n  int blocksofm;\n  size_t scratch_size;\n  void* scratch;\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_softmaxloss {\n  libxsmm_dnn_softmaxloss_desc desc;\n  libxsmm_dnn_tensor* reg_input;      /* input tensor */\n  libxsmm_dnn_tensor* reg_output;     /* output tensor */\n  libxsmm_dnn_tensor* grad_input;     /* grad input tensor */\n  libxsmm_dnn_tensor* label;          /* labels tensor */\n  libxsmm_barrier* barrier;           /* barrier */\n  int bc;\n  int Bc;\n  int bn;\n  int Bn;\n  float loss;\n  size_t scratch_size;\n  void* scratch;\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_optimizer {\n  libxsmm_dnn_optimizer_desc desc;\n  libxsmm_dnn_tensor* reg_filter;      /* filter tensor */\n  libxsmm_dnn_tensor* grad_filter;     /* grad filter tensor */\n  libxsmm_dnn_tensor* master_filter;   /* master filter tensor */\n  libxsmm_barrier* barrier;            /* barrier */\n  int bc;\n  int Bc;\n  int bk;\n  int Bk;\n  int fm_lp_block;\n  size_t scratch_size;\n  void* scratch;\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_fusedgroupnorm {\n  libxsmm_dnn_fusedgroupnorm_desc desc;\n  libxsmm_dnn_tensor* reg_input;      /* input tensor */\n  libxsmm_dnn_tensor* reg_output;     /* output tensor */\n  libxsmm_dnn_tensor* grad_input;     /* grad input tensor */\n  libxsmm_dnn_tensor* grad_output;    /* grad output tensor */\n  libxsmm_dnn_tensor* reg_add;        /* elementwise tensor */\n  libxsmm_dnn_tensor* grad_add;       /* grad elementwise tensor */\n  libxsmm_dnn_tensor* reg_beta;       /* beta tensor */\n  libxsmm_dnn_tensor* reg_gamma;      /* gamma tensor */\n  libxsmm_dnn_tensor* grad_beta;      /* grad beta tensor */\n  libxsmm_dnn_tensor* grad_gamma;     /* grad gamma tensor */\n  libxsmm_dnn_tensor* expvalue;       /* expected value */\n  libxsmm_dnn_tensor* rcpstddev;      /* reciprocal of standard derivation */\n  libxsmm_dnn_tensor* variance;       /* variance */\n  libxsmm_dnn_tensor* relumask;       /* relumask */\n  libxsmm_barrier* barrier;           /* barrier */\n  int ifmblock;\n  int ofmblock;\n  int blocksifm;\n  int blocksofm;\n  size_t scratch_size;\n  void* scratch;\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_fullyconnected {\n  libxsmm_dnn_fullyconnected_desc desc;\n  libxsmm_dnn_tensor* reg_input;      /* input tensor */\n  libxsmm_dnn_tensor* reg_output;     /* output tensor */\n  libxsmm_dnn_tensor* grad_input;     /* grad input tensor */\n  libxsmm_dnn_tensor* grad_output;    /* grad output tensor */\n  libxsmm_dnn_tensor* reg_filter;     /* filter tensor */\n  libxsmm_dnn_tensor* grad_filter;    /* grad filter tensor */\n  libxsmm_dnn_tensor* reg_bias;       /* bias tensor */\n  libxsmm_dnn_tensor* grad_bias;      /* grad bais tensor */\n  libxsmm_dnn_tensor* relumask;       /* relumask */\n  libxsmm_barrier* barrier;           /* barrier */\n  int ifmblock;\n  int ofmblock;\n  int blocksifm;\n  int blocksofm;\n  /* Parameters to tune/specialize FC algorithms */\n  int fwd_2d_blocking;\n  int bwd_2d_blocking;\n  int upd_2d_blocking;\n  int fwd_bf;\n  int bwd_bf;\n  int upd_bf;\n  int fwd_row_teams;\n  int fwd_column_teams;\n  int bwd_row_teams;\n  int bwd_column_teams;\n  int upd_row_teams;\n  int upd_column_teams;\n  int ifm_subtasks;\n  int ofm_subtasks;\n\n  int fm_lp_block;\n  int bn;\n  int bk;\n  int bc;\n  size_t scratch_size;\n  size_t doutput_scratch_mark;\n  void* scratch;\n\n  libxsmm_xtransfunction tr_kernel;\n  libxsmm_code_pointer gemm_fwd;     /* ability to hoist forward GEMMs */\n  libxsmm_code_pointer gemm_fwd2;    /* ability to hoist forward GEMMs */\n  libxsmm_code_pointer gemm_fwd3;    /* ability to hoist forward GEMMs */\n  libxsmm_code_pointer gemm_bwd;     /* ability to hoist backward GEMMs */\n  libxsmm_code_pointer gemm_bwd2;    /* ability to hoist backward GEMMs */\n  libxsmm_code_pointer gemm_upd;     /* ability to hoist update GEMMs */\n  libxsmm_code_pointer gemm_upd2;    /* ability to hoist update GEMMs */\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_pooling {\n  libxsmm_dnn_pooling_desc desc;\n  libxsmm_dnn_tensor* reg_input;      /* input tensor */\n  libxsmm_dnn_tensor* reg_output;     /* output tensor */\n  libxsmm_dnn_tensor* grad_input;     /* grad input tensor */\n  libxsmm_dnn_tensor* grad_output;    /* grad output tensor */\n  libxsmm_dnn_tensor* mask;           /* elementwise tensor */\n  libxsmm_barrier* barrier;           /* barrier */\n  int ifmblock;\n  int ofmblock;\n  int blocksifm;\n  int blocksofm;\n  int ofh;\n  int ofw;\n  size_t scratch_size;\n  void* scratch;\n};\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_dnn_rnncell {\n  libxsmm_dnn_rnncell_desc desc;\n  libxsmm_blasint T;                              /* sequence length, must be smaller than max sequence length in desc */\n  libxsmm_blasint bk;\n  libxsmm_blasint bn;\n  libxsmm_blasint bc;\n  libxsmm_blasint lpb;\n\n  /* external tensors */\n  libxsmm_dnn_tensor* xt;\n  libxsmm_dnn_tensor* csp;\n  libxsmm_dnn_tensor* hp;\n  libxsmm_dnn_tensor* w;\n  libxsmm_dnn_tensor* wt;\n  libxsmm_dnn_tensor* r;\n  libxsmm_dnn_tensor* rt;\n  libxsmm_dnn_tensor* b;\n  libxsmm_dnn_tensor* cst;\n  libxsmm_dnn_tensor* ht;\n  libxsmm_dnn_tensor* dxt;\n  libxsmm_dnn_tensor* dcsp;\n  libxsmm_dnn_tensor* dhp;\n  libxsmm_dnn_tensor* dw;\n  libxsmm_dnn_tensor* dr;\n  libxsmm_dnn_tensor* db;\n  libxsmm_dnn_tensor* dcs;\n  libxsmm_dnn_tensor* dht;\n  libxsmm_dnn_tensor* it;\n  libxsmm_dnn_tensor* ft;\n  libxsmm_dnn_tensor* ot;\n  libxsmm_dnn_tensor* cit;\n  libxsmm_dnn_tensor* cot;\n  float forget_bias;\n  /* internal  state */\n  void* internal_z;\n  /* scratch pointers */\n  void* scratch_base;\n  void* scratch_wT;\n  void* scratch_rT;\n  void* scratch_w;\n  void* scratch_r;\n  void* scratch_xT;\n  void* scratch_hT;\n  void* scratch_deltat;\n  void* scratch_di;\n  void* scratch_df;\n  void* scratch_do;\n  void* scratch_dci;\n  void* scratch_diB;\n  void* scratch_dfB;\n  void* scratch_dpB;\n  void* scratch_dciB;\n  void* scratch_dx;\n  void* scratch_dhp;\n  void* scratch_db;\n  void* scratch_t1;\n  void* scratch_t2;\n  void* csp_scratch;\n  void* cst_scratch;\n  void* ht_scratch;\n  void* it_scratch;\n  void* ft_scratch;\n  void* ot_scratch;\n  void* cit_scratch;\n  void* cot_scratch;\n\n  /* Ability to hoist GEMMs */\n  libxsmm_bsmmfunction_reducebatch_strd fwd_kernela;\n  libxsmm_bsmmfunction_reducebatch_strd fwd_kernelb;\n  libxsmm_bsmmfunction_reducebatch_strd bwdupd_kernela;\n  libxsmm_bsmmfunction_reducebatch_strd bwdupd_kernelb;\n  libxsmm_bsmmfunction_reducebatch_strd bwdupd_kernelc;\n  libxsmm_bsmmfunction_reducebatch_strd bwdupd_kerneld;\n\n  libxsmm_barrier* barrier; /* barrier */\n};\n\nstruct LIBXSMM_RETARGETABLE libxsmm_dfsspmdm {\n  int M;\n  int N;\n  int K;\n  int ldb;\n  int ldc;\n  int N_chunksize;\n  unsigned int* permute_operands;\n  double* a_dense;\n  libxsmm_dmmfunction kernel;\n};\n\nstruct LIBXSMM_RETARGETABLE libxsmm_sfsspmdm {\n  int M;\n  int N;\n  int K;\n  int ldb;\n  int ldc;\n  int N_chunksize;\n  unsigned int* permute_operands;\n  float* a_dense;\n  libxsmm_smmfunction kernel;\n};\n\ntypedef enum libxsmm_build_kind {\n  LIBXSMM_BUILD_KIND_GEMM       = LIBXSMM_KERNEL_KIND_MATMUL,\n  LIBXSMM_BUILD_KIND_MCOPY      = LIBXSMM_KERNEL_KIND_MCOPY,\n  LIBXSMM_BUILD_KIND_MELTW      = LIBXSMM_KERNEL_KIND_MELTW,\n  LIBXSMM_BUILD_KIND_TRANS      = LIBXSMM_KERNEL_KIND_TRANS,\n  LIBXSMM_BUILD_KIND_PGEMM      = LIBXSMM_KERNEL_KIND_PGEMM,\n  LIBXSMM_BUILD_KIND_GETRF      = LIBXSMM_KERNEL_KIND_GETRF,\n  LIBXSMM_BUILD_KIND_TRMM       = LIBXSMM_KERNEL_KIND_TRMM,\n  LIBXSMM_BUILD_KIND_TRSM       = LIBXSMM_KERNEL_KIND_TRSM,\n  LIBXSMM_BUILD_KIND_USER       = LIBXSMM_KERNEL_KIND_USER,\n  LIBXSMM_BUILD_KIND_PGEMMRMAC  = LIBXSMM_KERNEL_UNREGISTERED,\n  LIBXSMM_BUILD_KIND_PGEMMRMBC,\n  LIBXSMM_BUILD_KIND_SRSOA,\n  LIBXSMM_BUILD_KIND_SCSOA,\n  LIBXSMM_BUILD_KIND_SREG\n} libxsmm_build_kind;\n\n/** Integral type (libxsmm_kernel_kind, libxsmm_build_kind). */\n#if defined(LIBXSMM_UNPACKED)\ntypedef size_t libxsmm_descriptor_kind;\n#else\ntypedef unsigned char libxsmm_descriptor_kind;\n#endif\n\n/** All descriptor types, which are valid for code-registration. */\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE libxsmm_descriptor {\n  char data[LIBXSMM_DESCRIPTOR_MAXSIZE];\n  libxsmm_descriptor_kind kind; /* kind: must be the first member */\n  LIBXSMM_REGDESC(LIBXSMM_PACKED(struct) { libxsmm_descriptor_kind /*repeated kind*/ pad; , desc; });\n  LIBXSMM_PACKED(struct) { libxsmm_descriptor_kind /*repeated kind*/ pad; char desc[1]; } user;\n} libxsmm_descriptor;\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE libxsmm_build_request {\n  union {\n    const void* ptr; /* raw content */\n    LIBXSMM_REGDESC(LIBXSMM_REGDESC_DEFAULT, const*);\n    const libxsmm_csr_soa_descriptor* srsoa;\n    const libxsmm_csc_soa_descriptor* scsoa;\n    const libxsmm_pgemm_ac_rm_descriptor* pgemmacrm;\n    const libxsmm_pgemm_bc_rm_descriptor* pgemmbcrm;\n    const libxsmm_csr_reg_descriptor* sreg;\n  } descriptor;\n  libxsmm_build_kind kind;\n  /* used by user-kind */\n  size_t user_size;\n} libxsmm_build_request;\n\ntypedef enum libxsmm_malloc_flags {\n  LIBXSMM_MALLOC_FLAG_DEFAULT = 0,\n  LIBXSMM_MALLOC_FLAG_SCRATCH = 1,\n  LIBXSMM_MALLOC_FLAG_PRIVATE = 2,\n  LIBXSMM_MALLOC_FLAG_REALLOC = 4,\n  LIBXSMM_MALLOC_FLAG_PHUGE   = 8,\n  LIBXSMM_MALLOC_FLAG_PLOCK   = 16,\n  LIBXSMM_MALLOC_FLAG_MMAP    = 32,\n  LIBXSMM_MALLOC_FLAG_R       = 64,\n  LIBXSMM_MALLOC_FLAG_W       = 128,\n  LIBXSMM_MALLOC_FLAG_X       = 256,\n  LIBXSMM_MALLOC_FLAG_RW  = LIBXSMM_MALLOC_FLAG_R | LIBXSMM_MALLOC_FLAG_W,\n  LIBXSMM_MALLOC_FLAG_WX  = LIBXSMM_MALLOC_FLAG_X | LIBXSMM_MALLOC_FLAG_W,\n  LIBXSMM_MALLOC_FLAG_RWX = LIBXSMM_MALLOC_FLAG_X | LIBXSMM_MALLOC_FLAG_RW,\n  LIBXSMM_MALLOC_FLAG_VALID       = LIBXSMM_MALLOC_FLAG_SCRATCH |\n      LIBXSMM_MALLOC_FLAG_PRIVATE | LIBXSMM_MALLOC_FLAG_REALLOC |\n      LIBXSMM_MALLOC_FLAG_PHUGE   | LIBXSMM_MALLOC_FLAG_PLOCK |\n      LIBXSMM_MALLOC_FLAG_MMAP    | LIBXSMM_MALLOC_FLAG_RWX\n} libxsmm_malloc_flags;\n\nLIBXSMM_EXTERN_C typedef LIBXSMM_RETARGETABLE void* (*libxsmm_realloc_fun)(void* /*ptr*/, size_t /*size*/);\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE libxsmm_malloc_fntype {\n  union { const void* dlsym; void* (*ptr)(size_t, size_t);  } alignmem;\n  union { const void* dlsym; void* (*ptr)(size_t, size_t);  } memalign;\n  union { const void* dlsym; libxsmm_malloc_fun ptr;        } malloc;\n# if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n  union { const void* dlsym; void* (*ptr)(size_t, size_t);  } calloc;\n# endif\n# if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n  union { const void* dlsym; libxsmm_realloc_fun ptr;      } realloc;\n# endif\n  union { const void* dlsym; libxsmm_free_fun ptr;          } free;\n} libxsmm_malloc_fntype;\nLIBXSMM_APIVAR_PRIVATE(libxsmm_malloc_fntype libxsmm_malloc_fn);\n#endif\n\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n/* prototypes for GLIBC internal implementation */\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_memalign(size_t alignment, size_t size);\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_malloc(size_t size);\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_calloc(size_t num, size_t size);\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_realloc(void* ptr, size_t size);\n#endif\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void  __libc_free(void* ptr);\n#endif /*(defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))*/\n\nLIBXSMM_API_INTERN void* libxsmm_memalign_internal(size_t alignment, size_t size);\n\n/* See https://sourceware.org/binutils/docs-2.34/ld/Options.html#index-_002d_002dwrap_003dsymbol */\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_memalign(size_t alignment, size_t size);\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_malloc(size_t size);\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_calloc(size_t num, size_t size);\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_realloc(void* ptr, size_t size);\n#endif\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void __real_free(void* ptr);\n\n/** Retrieve internal information about a buffer (default memory domain). */\nLIBXSMM_API int libxsmm_get_malloc_xinfo(const void* memory, size_t* size, int* flags, void** extra);\n\n/** Initializes malloc hooks and other internals. */\nLIBXSMM_API_INTERN void libxsmm_malloc_init(void);\nLIBXSMM_API_INTERN void libxsmm_malloc_finalize(void);\n\n/** Calculates an alignment depending on supposedly allocated size; alignment can be zero (\"auto\"). */\nLIBXSMM_API_INTERN size_t libxsmm_alignment(size_t size, size_t alignment);\n\n/** Same as libxsmm_set_default_allocator, but takes a lock (can be NULL). */\nLIBXSMM_API_INTERN int libxsmm_xset_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn);\n/** Same as libxsmm_get_default_allocator, but takes a lock (can be NULL). */\nLIBXSMM_API_INTERN int libxsmm_xget_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn);\n\n/** Same as libxsmm_set_scratch_allocator, but takes a lock (can be NULL). */\nLIBXSMM_API_INTERN int libxsmm_xset_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn);\n/** Same as libxsmm_get_scratch_allocator, but takes a lock (can be NULL). */\nLIBXSMM_API_INTERN int libxsmm_xget_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn);\n\n/**\n * Attribute memory allocation and protect with only the necessary flags.\n * This procedure is expected to run only one time per buffer, and may\n * relocate the given memory.\n */\nLIBXSMM_API_INTERN int libxsmm_malloc_attrib(void** memory, int flags,\n  /** If a name is given, an executable buffer will be dumped into a file. */\n  const char* name);\n\n/** Allocate memory of the requested size, which is aligned according to the given alignment. */\nLIBXSMM_API_INTERN int libxsmm_xmalloc(void** memory, size_t size, size_t alignment, int flags,\n  /* The extra information is stored along with the allocated chunk; can be NULL/zero. */\n  const void* extra, size_t extra_size);\n/** Release memory, which was allocated using libxsmm_[*]malloc. */\nLIBXSMM_API_INTERN void libxsmm_xfree(const void* memory, int check);\n\n/** Like libxsmm_release_scratch, but takes a lock (can be NULL). */\nLIBXSMM_API_INTERN void libxsmm_xrelease_scratch(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock);\n\n/**\n * Format for instance an amount of Bytes like libxsmm_format_size(result, sizeof(result), nbytes, \"KMGT\", \"B\", 10).\n * The value returned is in requested/determined unit so that the user can decide about printing the buffer.\n */\nLIBXSMM_API_INTERN size_t libxsmm_format_size(char buffer[32], int buffer_size, size_t nbytes, const char scale[], const char* unit, int base);\n\n/** Returns the type-name of data-type (can be also libxsmm_gemm_precision). */\nLIBXSMM_API_INTERN const char* libxsmm_typename(libxsmm_datatype datatype);\n\n/** Determines the given value in double-precision based on the given type. */\nLIBXSMM_API_INTERN int libxsmm_dvalue(libxsmm_datatype datatype, const void* value, double* dvalue);\n\n/** Services a build request, and (optionally) registers the code (use regindex=LIBXSMM_CAPACITY_REGISTRY for unmanaged code). */\nLIBXSMM_API_INTERN int libxsmm_build(const libxsmm_build_request* request, unsigned int regindex, libxsmm_code_pointer* code);\n\n/** Returns the type-size of data-type (can be also libxsmm_gemm_precision). */\nLIBXSMM_API unsigned char libxsmm_typesize(libxsmm_datatype datatype);\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE libxsmm_kernel_xinfo {\n  /** Non-zero of kernel is registered. */\n  unsigned int registered;\n  /** Number of FLoating Point OPerationS (FLOPS). */\n  unsigned int nflops;\n} libxsmm_kernel_xinfo;\n\n/** Receive information about JIT-generated code. */\nLIBXSMM_API_INTERN const libxsmm_kernel_xinfo* libxsmm_get_kernel_xinfo(libxsmm_code_pointer code, const libxsmm_descriptor** desc, size_t* code_size);\n\n/** Calculates duration in seconds from given RTC ticks. */\nLIBXSMM_API_INTERN double libxsmm_timer_duration_rtc(libxsmm_timer_tickint tick0, libxsmm_timer_tickint tick1);\n/** Returns the current tick of platform-specific real-time clock. */\nLIBXSMM_API_INTERN libxsmm_timer_tickint libxsmm_timer_tick_rtc(void);\n/** Returns the current tick of a (monotonic) platform-specific counter. */\nLIBXSMM_API_INTERN libxsmm_timer_tickint libxsmm_timer_tick_tsc(void);\n\nLIBXSMM_API_INTERN void libxsmm_memory_init(int target_arch);\nLIBXSMM_API_INTERN void libxsmm_memory_finalize(void);\n\nLIBXSMM_API_INTERN void libxsmm_dnn_init(int target_arch);\nLIBXSMM_API_INTERN void libxsmm_dnn_finalize(void);\n\n/** intern function to calculate blockings, that's private API hence it's in this function */\nLIBXSMM_API_INTERN libxsmm_dnn_err_t libxsmm_dnn_get_feature_map_blocks(\n  int C, int K, int* C_block, int* K_block, int* fm_lp_block,\n  libxsmm_dnn_datatype datatype_in, libxsmm_dnn_datatype datatype_out);\n\n/** Global lock; create an own lock for an independent domain. */\nLIBXSMM_APIVAR_PUBLIC(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK) libxsmm_lock_global);\n/** Determines whether a threaded implementation is synchronized or not. */\nLIBXSMM_APIVAR_PUBLIC(int libxsmm_nosync);\n\n/** Function used to allocate default memory. */\nLIBXSMM_APIVAR_PRIVATE(libxsmm_malloc_function libxsmm_default_malloc_fn);\n/** Function used to allocate scratch memory. */\nLIBXSMM_APIVAR_PRIVATE(libxsmm_malloc_function libxsmm_scratch_malloc_fn);\n/** Function used to release default memory. */\nLIBXSMM_APIVAR_PRIVATE(libxsmm_free_function libxsmm_default_free_fn);\n/** Function used to release scratch memory. */\nLIBXSMM_APIVAR_PRIVATE(libxsmm_free_function libxsmm_scratch_free_fn);\n/** If non-NULL, this context is used by the context-form of memory allocation. */\nLIBXSMM_APIVAR_PRIVATE(const void* libxsmm_default_allocator_context);\n/** If non-NULL, this context is used by the context-form of memory allocation. */\nLIBXSMM_APIVAR_PRIVATE(const void* libxsmm_scratch_allocator_context);\n/** Number of scratch memory pools used; clamped against internal maximum. */\nLIBXSMM_APIVAR_PRIVATE(unsigned int libxsmm_scratch_pools);\n/** Growth factor used to scale the scratch memory in case of reallocation. */\nLIBXSMM_APIVAR_PRIVATE(double libxsmm_scratch_scale);\n/** Number of seconds per RDTSC-cycle (zero or negative if RDTSC invalid). */\nLIBXSMM_APIVAR_PRIVATE(double libxsmm_timer_scale);\n/** Counts the number of attempts to create an SPMDM-handle. */\nLIBXSMM_APIVAR_PRIVATE(unsigned int libxsmm_statistic_num_spmdm);\n/** Counts the maximum number of thread that have been active. */\nLIBXSMM_APIVAR_PRIVATE(unsigned int libxsmm_thread_count);\n\n#if (0 != LIBXSMM_SYNC)\nLIBXSMM_APIVAR_PRIVATE(LIBXSMM_TLS_TYPE libxsmm_tlskey);\n#endif\n\n#endif /*LIBXSMM_MAIN_H*/\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/src/libxsmm_gemm.h": "/******************************************************************************\n* Copyright (c) Intel Corporation - All rights reserved.                      *\n* This file is part of the LIBXSMM library.                                   *\n*                                                                             *\n* For information on the license, see the LICENSE file.                       *\n* Further information: https://github.com/hfp/libxsmm/                        *\n* SPDX-License-Identifier: BSD-3-Clause                                       *\n******************************************************************************/\n/* Hans Pabst (Intel Corp.)\n******************************************************************************/\n#ifndef LIBXSMM_GEMM_H\n#define LIBXSMM_GEMM_H\n\n#include \"libxsmm_main.h\"\n\n#if !defined(LIBXSMM_BLAS_WRAP_DYNAMIC) && defined(LIBXSMM_INTERCEPT_DYNAMIC) && (!defined(__BLAS) || (0 != __BLAS))\n# define LIBXSMM_BLAS_WRAP_DYNAMIC\n#endif\n\n#if !defined(LIBXSMM_GEMM_CHECK) && !defined(NDEBUG)\n# define LIBXSMM_GEMM_CHECK\n#endif\n#if !defined(LIBXSMM_GEMM_LOCK)\n# define LIBXSMM_GEMM_LOCK LIBXSMM_LOCK_DEFAULT\n#endif\n#if !defined(LIBXSMM_GEMM_MMBATCH_SCALE)\n# define LIBXSMM_GEMM_MMBATCH_SCALE 1.5\n#endif\n#if !defined(LIBXSMM_GEMM_MMBATCH_VERBOSITY)\n# define LIBXSMM_GEMM_MMBATCH_VERBOSITY ((LIBXSMM_VERBOSITY_HIGH) + 1)\n#endif\n#if !defined(LIBXSMM_GEMM_NPARGROUPS)\n# define LIBXSMM_GEMM_NPARGROUPS 128\n#endif\n\n#if !defined(LIBXSMM_WRAP) && defined(LIBXSMM_BUILD) && \\\n    (defined(LIBXSMM_CONFIG_WRAP) && 0 != (LIBXSMM_CONFIG_WRAP)) && \\\n    (defined(LIBXSMM_BLAS_WRAP_DYNAMIC) || !defined(NDEBUG) || defined(_WIN32)) /* debug */\n# define LIBXSMM_WRAP LIBXSMM_CONFIG_WRAP\n#endif\n\n/** Undefine (disarm) MKL's DIRECT_CALL macros. */\n#if (defined(MKL_DIRECT_CALL_SEQ) || defined(MKL_DIRECT_CALL))\n# if defined(sgemm_)\n#   undef sgemm_\n# endif\n# if defined(dgemm_)\n#   undef dgemm_\n# endif\n#endif\n\n#if !defined(LIBXSMM_BLAS_ERROR)\n#define LIBXSMM_BLAS_ERROR(SYMBOL, PCOUNTER) do { \\\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(PCOUNTER, 1, LIBXSMM_ATOMIC_RELAXED)) { \\\n      fprintf(stderr, \"LIBXSMM ERROR: application must be linked against LAPACK/BLAS %s!\\n\", SYMBOL); \\\n    } \\\n  } while(0)\n#endif\n\n#if defined(LIBXSMM_BUILD)\n# define LIBXSMM_BLAS_WRAPPER_STATIC1(TYPE, KIND, ORIGINAL) if (NULL == (ORIGINAL)) { \\\n    ORIGINAL = LIBXSMM_FSYMBOL(LIBXSMM_CONCATENATE(__real_, LIBXSMM_TPREFIX(TYPE, KIND))); \\\n  }\n# define LIBXSMM_BLAS_WRAPPER_STATIC0 LIBXSMM_BLAS_WRAPPER_STATIC1\n#else\n# define LIBXSMM_BLAS_WRAPPER_STATIC1(TYPE, KIND, ORIGINAL) if (NULL == (ORIGINAL)) { \\\n    ORIGINAL = (LIBXSMM_BLAS_FNTYPE(TYPE, KIND))LIBXSMM_BLAS_SYMBOL(TYPE, KIND); \\\n  }\n# define LIBXSMM_BLAS_WRAPPER_STATIC0(TYPE, KIND, ORIGINAL)\n#endif\n#define LIBXSMM_BLAS_WRAPPER_STATIC(CONDITION, TYPE, KIND, ORIGINAL) \\\n  LIBXSMM_CONCATENATE(LIBXSMM_BLAS_WRAPPER_STATIC, CONDITION)(TYPE, KIND, ORIGINAL)\n\n#if defined(LIBXSMM_BLAS_WRAP_DYNAMIC)\n# define LIBXSMM_BLAS_WRAPPER_DYNAMIC(TYPE, KIND, ORIGINAL, NEXT) { \\\n    union { const void* pfin; \\\n      LIBXSMM_BLAS_FNTYPE(TYPE, KIND) (*chain)(void); /* chain */ \\\n      LIBXSMM_BLAS_FNTYPE(TYPE, KIND) pfout; \\\n    } libxsmm_blas_wrapper_dynamic_ /*= { 0 }*/; \\\n    dlerror(); /* clear an eventual error status */ \\\n    libxsmm_blas_wrapper_dynamic_.chain = NEXT; \\\n    libxsmm_blas_wrapper_dynamic_.pfin = ((NULL == libxsmm_blas_wrapper_dynamic_.pfin) ? \\\n      dlsym(LIBXSMM_RTLD_NEXT, \"libxsmm_original_\" LIBXSMM_STRINGIFY(LIBXSMM_TPREFIX(TYPE, KIND))) : NULL); \\\n    if (NULL == libxsmm_blas_wrapper_dynamic_.pfout || NULL != dlerror() || NULL == libxsmm_blas_wrapper_dynamic_.chain()) { \\\n      libxsmm_blas_wrapper_dynamic_.pfin = dlsym(LIBXSMM_RTLD_NEXT, LIBXSMM_STRINGIFY(LIBXSMM_BLAS_SYMBOL(TYPE, KIND))); \\\n      /*LIBXSMM_ATOMIC_STORE(&(ORIGINAL), libxsmm_blas_wrapper_dynamic_.pfout, LIBXSMM_ATOMIC_RELAXED);*/ \\\n      ORIGINAL = (NULL == dlerror() ? libxsmm_blas_wrapper_dynamic_.pfout : NULL); \\\n    } \\\n  }\n#else\n# define LIBXSMM_BLAS_WRAPPER_DYNAMIC(TYPE, KIND, ORIGINAL, NEXT)\n#endif\n\n#define LIBXSMM_BLAS_WRAPPER(CONDITION, TYPE, KIND, ORIGINAL, NEXT) if (NULL == (ORIGINAL)) { \\\n  LIBXSMM_BLAS_WRAPPER_DYNAMIC(TYPE, KIND, ORIGINAL, NEXT); \\\n  LIBXSMM_BLAS_WRAPPER_STATIC(CONDITION, TYPE, KIND, ORIGINAL); \\\n}\n\n\n/** Provides GEMM functions available via BLAS; NOT thread-safe. */\nLIBXSMM_API_INTERN void libxsmm_gemm_init(int archid);\n\n/** Finalizes the GEMM facility; NOT thread-safe. */\nLIBXSMM_API_INTERN void libxsmm_gemm_finalize(void);\n\nLIBXSMM_API_INTERN int libxsmm_gemm_prefetch2uid(libxsmm_gemm_prefetch_type prefetch);\nLIBXSMM_API_INTERN libxsmm_gemm_prefetch_type libxsmm_gemm_uid2prefetch(int uid);\n\n#if defined(LIBXSMM_BUILD)\n#if defined(LIBXSMM_BUILD_EXT)\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_dgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_sgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_dgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_sgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_dgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemv));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_sgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemv));\nLIBXSMM_APIEXT void __wrap_dgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_APIEXT void __wrap_sgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\n#endif\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_dgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_sgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_dgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_sgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_dgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemv));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_sgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemv));\nLIBXSMM_API void __real_dgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_API void __real_sgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\n#endif\n\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_BLAS_CONST*, *, double, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_CDECL(LIBXSMM_BLAS_CONST*, *, double, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_BLAS_CONST*, *, float, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_CDECL(LIBXSMM_BLAS_CONST*, *, float, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_BLAS_CONST*, *, double, gemm);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_BLAS_CONST*, *, float, gemm);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_BLAS_CONST*, *, double, gemv);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_BLAS_CONST*, *, float, gemv);\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_gemm_handle {\n  libxsmm_code_pointer copy_a, copy_b, copy_i, copy_o;\n  libxsmm_xmmfunction kernel[2];\n  unsigned int m, n, k, lda, ldb, ldc;\n  /* kernel size (tile) */\n  unsigned int km, kn, kk;\n  /* tile size per task */\n  unsigned int dm, dn, dk;\n  unsigned int itypesize, otypesize;\n  /* number of tasks per direction */\n  unsigned int mt, nt, kt;\n  int gemm_flags, flags;\n};\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE libxsmm_mmbatch_item {\n  struct {\n    const void *a, *b;\n    void *c;\n  } value;\n  struct {\n    libxsmm_gemm_descriptor desc;\n    unsigned int count;\n    const char* symbol;\n  } stat;\n  /* TODO: consider padding */\n} libxsmm_mmbatch_item;\n\nLIBXSMM_API void libxsmm_gemm_internal_set_batchflag(libxsmm_gemm_descriptor* descriptor, void* c, libxsmm_blasint index_stride,\n  libxsmm_blasint batchsize, int multithreaded);\n\nLIBXSMM_API int libxsmm_mmbatch_kernel(libxsmm_xmmfunction kernel, libxsmm_blasint index_base,\n  libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  const void* a, const void* b, void* c, libxsmm_blasint batchsize, /*unsigned*/int tid, /*unsigned*/int ntasks,\n  unsigned char itypesize, unsigned char otypesize, int flags);\n\nLIBXSMM_API int libxsmm_mmbatch_blas(\n  libxsmm_gemm_precision iprec, libxsmm_gemm_precision oprec, const char* transa, const char* transb, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const void* alpha, const void* a, const libxsmm_blasint* lda, const void* b, const libxsmm_blasint* ldb, const void* beta, void* c, const libxsmm_blasint* ldc,\n  libxsmm_blasint index_base, libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  libxsmm_blasint batchsize);\n\nLIBXSMM_API_INTERN void libxsmm_dmmbatch_blas(const char* transa, const char* transb, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const double* alpha, const void* a, const libxsmm_blasint* lda, const void* b, const libxsmm_blasint* ldb, const double* beta, void* c, const libxsmm_blasint* ldc,\n  libxsmm_blasint index_base, libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  libxsmm_blasint batchsize);\n\nLIBXSMM_API_INTERN void libxsmm_smmbatch_blas(const char* transa, const char* transb, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const float* alpha, const void* a, const libxsmm_blasint* lda, const void* b, const libxsmm_blasint* ldb, const float* beta, void* c, const libxsmm_blasint* ldc,\n  libxsmm_blasint index_base, libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  libxsmm_blasint batchsize);\n\nLIBXSMM_EXTERN_C typedef void (*libxsmm_mmbatch_flush_function)(void);\n\n/** auto-batch descriptor (filter). */\nLIBXSMM_APIVAR_PUBLIC(libxsmm_gemm_descriptor libxsmm_mmbatch_desc);\n/** Records a batch of SMMs or is used for batch-reduce. */\nLIBXSMM_APIVAR_PUBLIC(void* libxsmm_mmbatch_array);\n/** Lock: libxsmm_mmbatch_begin, libxsmm_mmbatch_end, internal_mmbatch_flush. */\nLIBXSMM_APIVAR_PUBLIC(LIBXSMM_LOCK_TYPE(LIBXSMM_GEMM_LOCK) libxsmm_mmbatch_lock);\n/** Maximum size of the recorded batch. */\nLIBXSMM_APIVAR_PUBLIC(unsigned int libxsmm_mmbatch_size);\n/** Maximum number of parallelized batch-groups. */\nLIBXSMM_APIVAR_PUBLIC(unsigned int libxsmm_gemm_npargroups);\n/** Minimum batchsize per thread/task. */\nLIBXSMM_APIVAR_PUBLIC(unsigned int libxsmm_gemm_taskgrain);\n/** Determines if OpenMP tasks are used. */\nLIBXSMM_APIVAR_PUBLIC(int libxsmm_gemm_tasks);\n/**\n * Intercepted GEMM\n * - [>=1 and  odd]: sequential and non-tiled (small problem sizes only)\n * - [>=2 and even]: parallelized and tiled (all problem sizes)\n * - [>=3 and  odd]: GEMV is intercepted; small problem sizes\n * - [>=4 and even]: GEMV is intercepted; all problem sizes\n * - [0]: disabled\n */\nLIBXSMM_APIVAR_PUBLIC(int libxsmm_gemm_wrap);\n\n/** Determines the default prefetch strategy, which is used in case of LIBXSMM_PREFETCH_AUTO. */\nLIBXSMM_APIVAR_PRIVATE(libxsmm_gemm_prefetch_type libxsmm_gemm_auto_prefetch_default);\n/** Determines the prefetch strategy, which is used in case of LIBXSMM_PREFETCH_AUTO. */\nLIBXSMM_APIVAR_PRIVATE(libxsmm_gemm_prefetch_type libxsmm_gemm_auto_prefetch);\n\n#endif /*LIBXSMM_GEMM_H*/\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/src/libxsmm_malloc.c": "/******************************************************************************\n* Copyright (c) Intel Corporation - All rights reserved.                      *\n* This file is part of the LIBXSMM library.                                   *\n*                                                                             *\n* For information on the license, see the LICENSE file.                       *\n* Further information: https://github.com/hfp/libxsmm/                        *\n* SPDX-License-Identifier: BSD-3-Clause                                       *\n******************************************************************************/\n/* Hans Pabst (Intel Corp.)\n******************************************************************************/\n#include \"libxsmm_trace.h\"\n#include \"libxsmm_main.h\"\n#include \"libxsmm_hash.h\"\n\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n#endif\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n# include <features.h>\n# include <malloc.h>\n#endif\n#if !defined(LIBXSMM_MALLOC_GLIBC)\n# if defined(__GLIBC__)\n#   define LIBXSMM_MALLOC_GLIBC __GLIBC__\n# else\n#   define LIBXSMM_MALLOC_GLIBC 6\n# endif\n#endif\n#if defined(_WIN32)\n# include <windows.h>\n# include <malloc.h>\n# include <intrin.h>\n#else\n# include <sys/mman.h>\n# if defined(__linux__)\n#   include <linux/mman.h>\n#   include <sys/syscall.h>\n# endif\n# if defined(MAP_POPULATE)\n#   include <sys/utsname.h>\n# endif\n# include <sys/types.h>\n# include <unistd.h>\n# include <errno.h>\n# if defined(__MAP_ANONYMOUS)\n#   define LIBXSMM_MAP_ANONYMOUS __MAP_ANONYMOUS\n# elif defined(MAP_ANONYMOUS)\n#   define LIBXSMM_MAP_ANONYMOUS MAP_ANONYMOUS\n# elif defined(MAP_ANON)\n#   define LIBXSMM_MAP_ANONYMOUS MAP_ANON\n# else\n#  define LIBXSMM_MAP_ANONYMOUS 0x20\n# endif\n# if defined(MAP_SHARED) && 0\n#   define LIBXSMM_MAP_SHARED MAP_SHARED\n# else\n#   define LIBXSMM_MAP_SHARED 0\n# endif\nLIBXSMM_EXTERN int ftruncate(int, off_t) LIBXSMM_THROW;\nLIBXSMM_EXTERN int mkstemp(char*) LIBXSMM_NOTHROW;\n#endif\n#if !defined(LIBXSMM_MALLOC_FALLBACK)\n# define LIBXSMM_MALLOC_FINAL 3\n#endif\n#if defined(LIBXSMM_VTUNE)\n# if (2 <= LIBXSMM_VTUNE) /* no header file required */\n#   if !defined(LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JITVERSION LIBXSMM_VTUNE\n#   endif\n#   define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load_V2\n#   define LIBXSMM_VTUNE_JIT_LOAD 21\n#   define LIBXSMM_VTUNE_JIT_UNLOAD 14\n#   define iJIT_SAMPLING_ON 0x0001\nLIBXSMM_EXTERN unsigned int iJIT_GetNewMethodID(void);\nLIBXSMM_EXTERN /*iJIT_IsProfilingActiveFlags*/int iJIT_IsProfilingActive(void);\nLIBXSMM_EXTERN int iJIT_NotifyEvent(/*iJIT_JVM_EVENT*/int event_type, void *EventSpecificData);\nLIBXSMM_EXTERN_C typedef struct LineNumberInfo {\n  unsigned int Offset;\n  unsigned int LineNumber;\n} LineNumberInfo;\nLIBXSMM_EXTERN_C typedef struct iJIT_Method_Load_V2 {\n  unsigned int method_id;\n  char* method_name;\n  void* method_load_address;\n  unsigned int method_size;\n  unsigned int line_number_size;\n  LineNumberInfo* line_number_table;\n  char* class_file_name;\n  char* source_file_name;\n  char* module_name;\n} iJIT_Method_Load_V2;\n# else /* more safe due to header dependency */\n#   include <jitprofiling.h>\n#   if !defined(LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JITVERSION 2\n#   endif\n#   if (2 <= LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load_V2\n#     define LIBXSMM_VTUNE_JIT_LOAD iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED_V2\n#   else\n#     define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load\n#     define LIBXSMM_VTUNE_JIT_LOAD iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED\n#   endif\n#   define LIBXSMM_VTUNE_JIT_UNLOAD iJVM_EVENT_TYPE_METHOD_UNLOAD_START\n# endif\n# if !defined(LIBXSMM_MALLOC_FALLBACK)\n#   define LIBXSMM_MALLOC_FALLBACK LIBXSMM_MALLOC_FINAL\n# endif\n#else\n# if !defined(LIBXSMM_MALLOC_FALLBACK)\n#   define LIBXSMM_MALLOC_FALLBACK 0\n# endif\n#endif /*defined(LIBXSMM_VTUNE)*/\n#if !defined(LIBXSMM_MALLOC_XMAP_TEMPLATE)\n# define LIBXSMM_MALLOC_XMAP_TEMPLATE \".libxsmm_jit.\" LIBXSMM_MKTEMP_PATTERN\n#endif\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(pop)\n#endif\n#if defined(LIBXSMM_PERF)\n# include \"libxsmm_perf.h\"\n#endif\n\n#if !defined(LIBXSMM_MALLOC_ALIGNMAX)\n# define LIBXSMM_MALLOC_ALIGNMAX (2 << 20) /* 2 MB */\n#endif\n#if !defined(LIBXSMM_MALLOC_ALIGNFCT)\n# define LIBXSMM_MALLOC_ALIGNFCT 16\n#endif\n#if !defined(LIBXSMM_MALLOC_SEED)\n# define LIBXSMM_MALLOC_SEED 1051981\n#endif\n\n#if !defined(LIBXSMM_MALLOC_HOOK_KMP) && 0\n# define LIBXSMM_MALLOC_HOOK_KMP\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_QKMALLOC) && 0\n# define LIBXSMM_MALLOC_HOOK_QKMALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_IMALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_IMALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_CHECK) && 0\n# define LIBXSMM_MALLOC_HOOK_CHECK 1\n#endif\n\n#if !defined(LIBXSMM_MALLOC_CRC_LIGHT) && !defined(_DEBUG) && 1\n# define LIBXSMM_MALLOC_CRC_LIGHT\n#endif\n#if !defined(LIBXSMM_MALLOC_CRC_OFF)\n# if defined(NDEBUG) && !defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n#   define LIBXSMM_MALLOC_CRC_OFF\n# elif !defined(LIBXSMM_BUILD)\n#   define LIBXSMM_MALLOC_CRC_OFF\n# endif\n#endif\n\n#if !defined(LIBXSMM_MALLOC_SCRATCH_LIMIT)\n# define LIBXSMM_MALLOC_SCRATCH_LIMIT 0xFFFFFFFF /* ~4 GB */\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_PADDING)\n# define LIBXSMM_MALLOC_SCRATCH_PADDING LIBXSMM_CACHELINE\n#endif\n/* pointers are checked first if they belong to scratch */\n#if !defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST) && 1\n# define LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST\n#endif\n/* can clobber memory if allocations are not exactly scoped */\n#if !defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD) && 0\n# define LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_JOIN) && 1\n# define LIBXSMM_MALLOC_SCRATCH_JOIN\n#endif\n#if !defined(LIBXSMM_MALLOC_LOCK_ONFAULT) && 0\n# if defined(MLOCK_ONFAULT) && defined(SYS_mlock2)\n#   define LIBXSMM_MALLOC_LOCK_ONFAULT\n# endif\n#endif\n/* protected against double-delete (if possible) */\n#if !defined(LIBXSMM_MALLOC_DELETE_SAFE) && 0\n# define LIBXSMM_MALLOC_DELETE_SAFE\n#endif\n/* map memory for scratch buffers */\n#if !defined(LIBXSMM_MALLOC_MMAP_SCRATCH) && 1\n# define LIBXSMM_MALLOC_MMAP_SCRATCH\n#endif\n/* map memory for hooked allocation */\n#if !defined(LIBXSMM_MALLOC_MMAP_HOOK) && 1\n# define LIBXSMM_MALLOC_MMAP_HOOK\n#endif\n/* map memory also for non-executable buffers */\n#if !defined(LIBXSMM_MALLOC_MMAP) && 1\n# define LIBXSMM_MALLOC_MMAP\n#endif\n\n#if defined(LIBXSMM_MALLOC_ALIGN_ALL)\n# define INTERNAL_MALLOC_AUTOALIGN(SIZE, ALIGNMENT) libxsmm_alignment(SIZE, ALIGNMENT)\n#else\n# define INTERNAL_MALLOC_AUTOALIGN(SIZE, ALIGNMENT) (ALIGNMENT)\n#endif\n\n#define INTERNAL_MEMALIGN_HOOK(RESULT, FLAGS, ALIGNMENT, SIZE, CALLER) { \\\n  const int internal_memalign_hook_recursive_ = LIBXSMM_ATOMIC_ADD_FETCH( \\\n    &internal_malloc_recursive, 1, LIBXSMM_ATOMIC_RELAXED); \\\n  if ( 1 < internal_memalign_hook_recursive_ /* protect against recursion */ \\\n    || 0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    || (internal_malloc_limit[0] > (SIZE)) \\\n    || (internal_malloc_limit[1] < (SIZE) && 0 != internal_malloc_limit[1])) \\\n  { \\\n    const size_t internal_memalign_hook_alignment_ = INTERNAL_MALLOC_AUTOALIGN(SIZE, ALIGNMENT); \\\n    (RESULT) = (0 != internal_memalign_hook_alignment_ \\\n      ? __real_memalign(internal_memalign_hook_alignment_, SIZE) \\\n      : __real_malloc(SIZE)); \\\n  } \\\n  else { /* redirect */ \\\n    LIBXSMM_INIT \\\n    if (NULL == (CALLER)) { /* libxsmm_trace_caller_id may allocate memory */ \\\n      internal_scratch_malloc(&(RESULT), SIZE, ALIGNMENT, FLAGS, \\\n        libxsmm_trace_caller_id(0/*level*/)); \\\n    } \\\n    else { \\\n      internal_scratch_malloc(&(RESULT), SIZE, ALIGNMENT, FLAGS, CALLER); \\\n    } \\\n  } \\\n  LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_recursive, 1, LIBXSMM_ATOMIC_RELAXED); \\\n}\n\n#define INTERNAL_REALLOC_HOOK(RESULT, FLAGS, PTR, SIZE, CALLER) { \\\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    /*|| (0 != LIBXSMM_ATOMIC_LOAD(&internal_malloc_recursive, LIBXSMM_ATOMIC_RELAXED))*/ \\\n    || (internal_malloc_limit[0] > (SIZE)) \\\n    || (internal_malloc_limit[1] < (SIZE) && 0 != internal_malloc_limit[1])) \\\n  { \\\n    (RESULT) = __real_realloc(PTR, SIZE); \\\n  } \\\n  else { \\\n    const int nzeros = LIBXSMM_INTRINSICS_BITSCANFWD64((uintptr_t)(PTR)), alignment = 1 << nzeros; \\\n    LIBXSMM_ASSERT(0 == ((uintptr_t)(PTR) & ~(0xFFFFFFFFFFFFFFFF << nzeros))); \\\n    if (NULL == (CALLER)) { /* libxsmm_trace_caller_id may allocate memory */ \\\n      internal_scratch_malloc(&(PTR), SIZE, (size_t)alignment, FLAGS, \\\n        libxsmm_trace_caller_id(0/*level*/)); \\\n    } \\\n    else { \\\n      internal_scratch_malloc(&(PTR), SIZE, (size_t)alignment, FLAGS, CALLER); \\\n    } \\\n    (RESULT) = (PTR); \\\n  } \\\n}\n\n#define INTERNAL_FREE_HOOK(PTR, CALLER) { \\\n  LIBXSMM_UNUSED(CALLER); \\\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    /*|| (0 != LIBXSMM_ATOMIC_LOAD(&internal_malloc_recursive, LIBXSMM_ATOMIC_RELAXED))*/ \\\n  ){ \\\n    __real_free(PTR); \\\n  } \\\n  else { /* recognize pointers not issued by LIBXSMM */ \\\n    libxsmm_free(PTR); \\\n  } \\\n}\n\n#if !defined(WIN32)\n# if defined(MAP_32BIT)\n#   define IF_INTERNAL_XMALLOC_MAP32(ENV, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR) \\\n    if (0 != (MAP_32BIT & (MFLAGS))) { \\\n      (BUFFER) = internal_xmalloc_xmap(ENV, SIZE, (MFLAGS) & ~MAP_32BIT, REPTR); \\\n    } \\\n    if (MAP_FAILED != (BUFFER)) (MAPSTATE) = 0; else\n# else\n#   define IF_INTERNAL_XMALLOC_MAP32(ENV, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR)\n# endif\n\n# define INTERNAL_XMALLOC(I, FALLBACK, ENVVAR, ENVDEF, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR) \\\n  if ((I) == (FALLBACK)) { \\\n    static const char* internal_xmalloc_env_ = NULL; \\\n    if (NULL == internal_xmalloc_env_) { \\\n      internal_xmalloc_env_ = getenv(ENVVAR); \\\n      if (NULL == internal_xmalloc_env_) internal_xmalloc_env_ = ENVDEF; \\\n    } \\\n    (BUFFER) = internal_xmalloc_xmap(internal_xmalloc_env_, SIZE, MFLAGS, REPTR); \\\n    if (MAP_FAILED == (BUFFER)) { \\\n      IF_INTERNAL_XMALLOC_MAP32(internal_xmalloc_env_, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR) \\\n        (FALLBACK) = (I) + 1; \\\n    } \\\n  }\n\n# define INTERNAL_XMALLOC_WATERMARK(NAME, WATERMARK, LIMIT, SIZE) { \\\n  const size_t internal_xmalloc_watermark_ = (WATERMARK) + (SIZE) / 2; /* accept data-race */ \\\n  if (internal_xmalloc_watermark_ < (LIMIT)) { \\\n    static size_t internal_xmalloc_watermark_verbose_ = 0; \\\n    (LIMIT) = internal_xmalloc_watermark_; /* accept data-race */ \\\n    if (internal_xmalloc_watermark_verbose_ < internal_xmalloc_watermark_ && \\\n      (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity)) \\\n    { /* muted */ \\\n      char internal_xmalloc_watermark_buffer_[32]; \\\n      /* coverity[check_return] */ \\\n      libxsmm_format_size(internal_xmalloc_watermark_buffer_, sizeof(internal_xmalloc_watermark_buffer_), \\\n        internal_xmalloc_watermark_, \"KM\", \"B\", 10); \\\n      fprintf(stderr, \"LIBXSMM WARNING: \" NAME \" watermark reached at %s!\\n\", internal_xmalloc_watermark_buffer_); \\\n      internal_xmalloc_watermark_verbose_ = internal_xmalloc_watermark_; \\\n    } \\\n  } \\\n}\n\n# define INTERNAL_XMALLOC_KIND(KIND, NAME, FLAG, FLAGS, MFLAGS, WATERMARK, LIMIT, INFO, SIZE, BUFFER) \\\n  if (0 != ((KIND) & (MFLAGS))) { \\\n    if (MAP_FAILED != (BUFFER)) { \\\n      LIBXSMM_ASSERT(NULL != (BUFFER)); \\\n      LIBXSMM_ATOMIC_ADD_FETCH(&(WATERMARK), SIZE, LIBXSMM_ATOMIC_RELAXED); \\\n      (FLAGS) |= (FLAG); \\\n    } \\\n    else { /* retry */ \\\n      (BUFFER) = mmap(NULL == (INFO) ? NULL : (INFO)->pointer, SIZE, PROT_READ | PROT_WRITE, \\\n        MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | ((MFLAGS) & ~(KIND)), -1, 0/*offset*/); \\\n      if (MAP_FAILED != (BUFFER)) { /* successful retry */ \\\n        LIBXSMM_ASSERT(NULL != (BUFFER)); \\\n        INTERNAL_XMALLOC_WATERMARK(NAME, WATERMARK, LIMIT, SIZE); \\\n      } \\\n    } \\\n  }\n#endif\n\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_malloc_info_type {\n  libxsmm_free_function free;\n  void *pointer, *reloc;\n  const void* context;\n  size_t size;\n  int flags;\n#if defined(LIBXSMM_VTUNE)\n  unsigned int code_id;\n#endif\n#if !defined(LIBXSMM_MALLOC_CRC_OFF) /* hash *must* be the last entry */\n  unsigned int hash;\n#endif\n} internal_malloc_info_type;\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_malloc_pool_type {\n  char pad[LIBXSMM_MALLOC_SCRATCH_PADDING];\n  struct {\n    size_t minsize, counter, incsize;\n    char *buffer, *head;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    const void* site;\n# if (0 != LIBXSMM_SYNC)\n    unsigned int tid;\n# endif\n#endif\n  } instance;\n} internal_malloc_pool_type;\n\n/* Scratch pool, which supports up to MAX_NSCRATCH allocation sites. */\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n/* LIBXSMM_ALIGNED appears to contradict LIBXSMM_APIVAR, and causes multiple defined symbols (if below is seen in multiple translation units) */\nLIBXSMM_APIVAR_DEFINE(char internal_malloc_pool_buffer[(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS)*sizeof(internal_malloc_pool_type)+(LIBXSMM_MALLOC_SCRATCH_PADDING)-1]);\n#endif\n/* Interval of bytes that permit interception (internal_malloc_kind) */\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_limit[2]);\n/* Maximum total size of the scratch memory domain. */\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_scratch_limit);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_scratch_nmallocs);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_private_max);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_private_cur);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_public_max);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_public_cur);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_local_max);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_local_cur);\nLIBXSMM_APIVAR_DEFINE(int internal_malloc_recursive);\n/** 0: regular, 1/odd: intercept/scratch, otherwise: all/scratch */\nLIBXSMM_APIVAR_DEFINE(int internal_malloc_kind);\n#if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\nLIBXSMM_APIVAR_DEFINE(int internal_malloc_join);\n#endif\n#if !defined(_WIN32)\n# if defined(MAP_HUGETLB)\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_hugetlb);\n# endif\n# if defined(MAP_LOCKED)\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_plocked);\n# endif\n#endif\n\n\nLIBXSMM_API_INTERN size_t libxsmm_alignment(size_t size, size_t alignment)\n{\n  size_t result;\n  if ((LIBXSMM_MALLOC_ALIGNFCT * LIBXSMM_MALLOC_ALIGNMAX) <= size) {\n    result = libxsmm_lcm(0 == alignment ? (LIBXSMM_ALIGNMENT) : libxsmm_lcm(alignment, LIBXSMM_ALIGNMENT), LIBXSMM_MALLOC_ALIGNMAX);\n  }\n  else { /* small-size request */\n    if ((LIBXSMM_MALLOC_ALIGNFCT * LIBXSMM_ALIGNMENT) <= size) {\n      result = (0 == alignment ? (LIBXSMM_ALIGNMENT) : libxsmm_lcm(alignment, LIBXSMM_ALIGNMENT));\n    }\n    else if (0 != alignment) { /* custom alignment */\n      result = libxsmm_lcm(alignment, sizeof(void*));\n    }\n    else { /* tiny-size request */\n      result = sizeof(void*);\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API size_t libxsmm_offset(const size_t offset[], const size_t shape[], size_t ndims, size_t* size)\n{\n  size_t result = 0, size1 = 0;\n  if (0 != ndims && NULL != shape) {\n    size_t i;\n    result = (NULL != offset ? offset[0] : 0);\n    size1 = shape[0];\n    for (i = 1; i < ndims; ++i) {\n      result += (NULL != offset ? offset[i] : 0) * size1;\n      size1 *= shape[i];\n    }\n  }\n  if (NULL != size) *size = size1;\n  return result;\n}\n\n\nLIBXSMM_API_INLINE internal_malloc_info_type* internal_malloc_info(const void* memory, int check)\n{\n  const char *const buffer = (const char*)memory;\n  internal_malloc_info_type* result = (internal_malloc_info_type*)(NULL != memory\n    ? (buffer - sizeof(internal_malloc_info_type)) : NULL);\n#if defined(LIBXSMM_MALLOC_HOOK_CHECK)\n  if ((LIBXSMM_MALLOC_HOOK_CHECK) < check) check = (LIBXSMM_MALLOC_HOOK_CHECK);\n#endif\n  if (0 != check && NULL != result) { /* check ownership */\n#if !defined(_WIN32) /* mprotect: pass address rounded down to page/4k alignment */\n    if (1 == check || 0 == mprotect((void*)(((uintptr_t)result) & 0xFFFFFFFFFFFFF000),\n      sizeof(internal_malloc_info_type), PROT_READ | PROT_WRITE) || ENOMEM != errno)\n#endif\n    {\n      const size_t maxsize = LIBXSMM_MAX(LIBXSMM_MAX(internal_malloc_public_max, internal_malloc_local_max), internal_malloc_private_max);\n      const int flags_rs = LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_SCRATCH;\n      const int flags_mx = LIBXSMM_MALLOC_FLAG_MMAP | LIBXSMM_MALLOC_FLAG_X;\n      const char* const pointer = (const char*)result->pointer;\n      union { libxsmm_free_fun fun; const void* ptr; } convert;\n      convert.fun = result->free.function;\n      if (((flags_mx != (flags_mx & result->flags)) && NULL != result->reloc)\n        || (0 == (LIBXSMM_MALLOC_FLAG_X & result->flags) ? 0 : (0 != (flags_rs & result->flags)))\n        || (0 != (LIBXSMM_MALLOC_FLAG_X & result->flags) && NULL != result->context)\n#if defined(LIBXSMM_VTUNE)\n        || (0 == (LIBXSMM_MALLOC_FLAG_X & result->flags) && 0 != result->code_id)\n#endif\n        || (0 != (~LIBXSMM_MALLOC_FLAG_VALID & result->flags))\n        || (0 == (LIBXSMM_MALLOC_FLAG_R & result->flags))\n        || pointer == convert.ptr || pointer == result->context\n        || pointer >= buffer || NULL == pointer\n        || maxsize < result->size || 0 == result->size\n        || 2 > libxsmm_ninit /* before checksum calculation */\n#if !defined(LIBXSMM_MALLOC_CRC_OFF) /* last check: checksum over info */\n# if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n        || result->hash != LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &result)\n# else\n        || result->hash != libxsmm_crc32(LIBXSMM_MALLOC_SEED, result,\n            (const char*)&result->hash - (const char*)result)\n# endif\n#endif\n      ) { /* mismatch */\n        result = NULL;\n      }\n    }\n#if !defined(_WIN32)\n    else { /* mismatch */\n      result = NULL;\n    }\n#endif\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int internal_xfree(const void* /*memory*/, internal_malloc_info_type* /*info*/);\nLIBXSMM_API_INTERN int internal_xfree(const void* memory, internal_malloc_info_type* info)\n{\n#if !defined(LIBXSMM_BUILD) || !defined(_WIN32)\n  static int error_once = 0;\n#endif\n  int result = EXIT_SUCCESS, flags;\n  void* buffer;\n  size_t size;\n  LIBXSMM_ASSERT(NULL != memory && NULL != info);\n  buffer = info->pointer;\n  flags = info->flags;\n  size = info->size;\n#if !defined(LIBXSMM_BUILD) /* sanity check */\n  if (NULL != buffer || 0 == size)\n#endif\n  {\n    const size_t alloc_size = size + (((const char*)memory) - ((const char*)buffer));\n    LIBXSMM_ASSERT(NULL != buffer || 0 == size);\n    if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n      if (NULL != info->free.function) {\n#if defined(LIBXSMM_MALLOC_DELETE_SAFE)\n        info->pointer = NULL; info->size = 0;\n#endif\n        if (NULL == info->context) {\n#if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) && 0\n          if (free == info->free.function) {\n            __real_free(buffer);\n          }\n          else\n#endif\n          if (NULL != info->free.function) {\n            info->free.function(buffer);\n          }\n        }\n        else {\n          LIBXSMM_ASSERT(NULL != info->free.ctx_form);\n          info->free.ctx_form(buffer, info->context);\n        }\n      }\n    }\n    else {\n#if defined(LIBXSMM_VTUNE)\n      if (0 != (LIBXSMM_MALLOC_FLAG_X & flags) && 0 != info->code_id && iJIT_SAMPLING_ON == iJIT_IsProfilingActive()) {\n        iJIT_NotifyEvent(LIBXSMM_VTUNE_JIT_UNLOAD, &info->code_id);\n      }\n#endif\n#if defined(_WIN32)\n      result = (NULL == buffer || FALSE != VirtualFree(buffer, 0, MEM_RELEASE)) ? EXIT_SUCCESS : EXIT_FAILURE;\n#else /* !_WIN32 */\n      {\n        const size_t unmap_size = LIBXSMM_UP2(alloc_size, LIBXSMM_PAGE_MINSIZE);\n        void* const reloc = info->reloc;\n        if (0 != munmap(buffer, unmap_size)) {\n          if (0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          {\n            fprintf(stderr, \"LIBXSMM ERROR: %s (attempted to unmap buffer %p+%\" PRIuPTR \")!\\n\",\n              strerror(errno), buffer, (uintptr_t)unmap_size);\n          }\n          result = EXIT_FAILURE;\n        }\n        if (0 != (LIBXSMM_MALLOC_FLAG_X & flags) && EXIT_SUCCESS == result\n          && NULL != reloc && MAP_FAILED != reloc && buffer != reloc\n          && 0 != munmap(reloc, unmap_size))\n        {\n          if (0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          {\n            fprintf(stderr, \"LIBXSMM ERROR: %s (attempted to unmap code %p+%\" PRIuPTR \")!\\n\",\n              strerror(errno), reloc, (uintptr_t)unmap_size);\n          }\n          result = EXIT_FAILURE;\n        }\n      }\n#endif\n    }\n    if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* update statistics */\n#if !defined(_WIN32)\n# if defined(MAP_HUGETLB)\n      if (0 != (LIBXSMM_MALLOC_FLAG_PHUGE & flags)) { /* huge pages */\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n        LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_hugetlb, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n      }\n# endif\n# if defined(MAP_LOCKED)\n      if (0 != (LIBXSMM_MALLOC_FLAG_PLOCK & flags)) { /* page-locked */\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n        LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_plocked, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n      }\n# endif\n#endif\n      if (0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags)) { /* public */\n        if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) { /* scratch */\n#if 1\n          const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n            &internal_malloc_public_cur, LIBXSMM_ATOMIC_RELAXED);\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_public_cur,\n            alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n#else\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_SUB_FETCH, LIBXSMM_BITS)(\n            &internal_malloc_public_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n#endif\n        }\n        else { /* local */\n#if 1\n          const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n            &internal_malloc_local_cur, LIBXSMM_ATOMIC_RELAXED);\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_local_cur,\n            alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n#else\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_SUB_FETCH, LIBXSMM_BITS)(\n            &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n#endif\n        }\n      }\n      else { /* private */\n#if 1\n        const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n          &internal_malloc_private_cur, LIBXSMM_ATOMIC_RELAXED);\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_private_cur,\n          alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n#else\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_SUB_FETCH, LIBXSMM_BITS)(\n          &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n#endif\n      }\n    }\n  }\n#if !defined(LIBXSMM_BUILD)\n  else if ((LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: attempt to release memory from non-matching implementation!\\n\");\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INLINE size_t internal_get_scratch_size(const internal_malloc_pool_type* exclude)\n{\n  size_t result = 0;\n#if !defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) || (1 >= (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  LIBXSMM_UNUSED(exclude);\n#else\n  const internal_malloc_pool_type* pool = (const internal_malloc_pool_type*)LIBXSMM_UP2(\n    (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const internal_malloc_pool_type *const end = pool + libxsmm_scratch_pools;\n  LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  for (; pool != end; ++pool)\n# endif /*(1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  {\n    if (0 != pool->instance.minsize) {\n# if 1 /* memory info is not used */\n      if (pool != exclude && (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n        result += pool->instance.minsize;\n      }\n# else\n      const internal_malloc_info_type* const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n      if (NULL != info && pool != exclude && (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n        result += info->size;\n      }\n# endif\n    }\n    else break; /* early exit */\n  }\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  return result;\n}\n\n\nLIBXSMM_API_INLINE internal_malloc_pool_type* internal_scratch_malloc_pool(const void* memory)\n{\n  internal_malloc_pool_type* result = NULL;\n  internal_malloc_pool_type* pool = (internal_malloc_pool_type*)LIBXSMM_UP2(\n    (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n  const char* const buffer = (const char*)memory;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const unsigned int npools = libxsmm_scratch_pools;\n#else\n  const unsigned int npools = 1;\n#endif\n  internal_malloc_pool_type *const end = pool + npools;\n  LIBXSMM_ASSERT(npools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  LIBXSMM_ASSERT(NULL != memory);\n  for (; pool != end; ++pool) {\n    if (0 != pool->instance.minsize) {\n      if (0 != pool->instance.counter\n#if 1 /* should be implied by non-zero counter */\n        && NULL != pool->instance.buffer\n#endif\n      ){/* check if memory belongs to scratch domain or local domain */\n#if 1\n        const size_t size = pool->instance.minsize;\n#else\n        const internal_malloc_info_type* const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n        const size_t size = info->size;\n#endif\n        if (pool->instance.buffer == buffer /* fast path */ ||\n           (pool->instance.buffer < buffer && buffer < (pool->instance.buffer + size)))\n        {\n          result = pool;\n          break;\n        }\n      }\n    }\n    else break; /* early exit */\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void internal_scratch_free(const void* /*memory*/, internal_malloc_pool_type* /*pool*/);\nLIBXSMM_API_INTERN void internal_scratch_free(const void* memory, internal_malloc_pool_type* pool)\n{\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const size_t counter = LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n  char* const pool_buffer = pool->instance.buffer;\n# if !defined(NDEBUG) || defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD)\n  char *const buffer = (char*)memory; /* non-const */\n  LIBXSMM_ASSERT(pool_buffer <= buffer && buffer < pool_buffer + pool->instance.minsize);\n# endif\n  LIBXSMM_ASSERT(pool_buffer <= pool->instance.head);\n  if (0 == counter) { /* reuse or reallocate scratch domain */\n    internal_malloc_info_type *const info = internal_malloc_info(pool_buffer, 0/*no check*/);\n    const size_t scale_size = (size_t)(1 != libxsmm_scratch_scale ? (libxsmm_scratch_scale * info->size) : info->size); /* hysteresis */\n    const size_t size = pool->instance.minsize + pool->instance.incsize;\n    LIBXSMM_ASSERT(0 == (LIBXSMM_MALLOC_FLAG_X & info->flags)); /* scratch memory is not executable */\n    if (size <= scale_size) { /* reuse scratch domain */\n      pool->instance.head = pool_buffer; /* reuse scratch domain */\n    }\n    else { /* release buffer */\n# if !defined(NDEBUG)\n      static int error_once = 0;\n# endif\n      pool->instance.buffer = pool->instance.head = NULL;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      pool->instance.site = NULL; /* clear affinity */\n# endif\n# if !defined(NDEBUG)\n      if (EXIT_SUCCESS != internal_xfree(pool_buffer, info)\n        && 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n# else\n      internal_xfree(pool_buffer, info); /* !libxsmm_free */\n# endif\n    }\n  }\n# if defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD) /* TODO: document linear/scoped allocator policy */\n  else if (buffer < pool->instance.head) { /* reuse scratch domain */\n    pool->instance.head = buffer;\n  }\n# else\n  LIBXSMM_UNUSED(memory);\n# endif\n#else\n  LIBXSMM_UNUSED(memory); LIBXSMM_UNUSED(pool);\n#endif\n}\n\n\nLIBXSMM_API_INTERN void internal_scratch_malloc(void** /*memory*/, size_t /*size*/, size_t /*alignment*/, int /*flags*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void internal_scratch_malloc(void** memory, size_t size, size_t alignment, int flags, const void* caller)\n{\n  LIBXSMM_ASSERT(NULL != memory && 0 == (LIBXSMM_MALLOC_FLAG_X & flags));\n  if (0 == (LIBXSMM_MALLOC_FLAG_REALLOC & flags) || NULL == *memory) {\n    static int error_once = 0;\n    size_t local_size = 0;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    if (0 < libxsmm_scratch_pools) {\n      internal_malloc_pool_type *const pools = (internal_malloc_pool_type*)LIBXSMM_UP2(\n        (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n      internal_malloc_pool_type *const end = pools + libxsmm_scratch_pools, *pool = pools;\n      const size_t align_size = libxsmm_alignment(size, alignment), alloc_size = size + align_size - 1;\n# if (0 != LIBXSMM_SYNC)\n      const unsigned int tid = libxsmm_get_tid();\n# endif\n      unsigned int npools = 1;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      const void *const site = caller; /* no further attempt in case of NULL */\n      internal_malloc_pool_type *pool0 = end;\n      for (; pool != end; ++pool) { /* counter: memory info is not employed as pools are still manipulated */\n        if (NULL != pool->instance.buffer) {\n          if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) ++npools; /* count number of occupied pools */\n          if ( /* find matching pool and enter fast path (draw from pool-buffer) */\n#   if (0 != LIBXSMM_SYNC) && !defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            (site == pool->instance.site && tid == pool->instance.tid))\n#   elif (0 != LIBXSMM_SYNC)\n            (site == pool->instance.site && (0 != internal_malloc_join || tid == pool->instance.tid)))\n#   else\n            (site == pool->instance.site))\n#   endif\n          {\n            break;\n          }\n        }\n        else {\n          if (end == pool0) pool0 = pool; /* first available pool*/\n          if (0 == pool->instance.minsize) { /* early exit */\n            pool = pool0; break;\n          }\n        }\n      }\n# endif\n      LIBXSMM_ASSERT(NULL != pool);\n      if (end != pool && 0 <= internal_malloc_kind) {\n        const size_t counter = LIBXSMM_ATOMIC_ADD_FETCH(&pool->instance.counter, (size_t)1, LIBXSMM_ATOMIC_SEQ_CST);\n        if (NULL != pool->instance.buffer || 1 != counter) { /* attempt to (re-)use existing pool */\n          const internal_malloc_info_type *const info = internal_malloc_info(pool->instance.buffer, 1/*check*/);\n          const size_t pool_size = ((NULL != info && 0 != counter) ? info->size : 0);\n          const size_t used_size = pool->instance.head - pool->instance.buffer;\n          const size_t req_size = alloc_size + used_size;\n          if (req_size <= pool_size) { /* fast path: draw from pool-buffer */\n# if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            void *const headaddr = &pool->instance.head;\n            char *const head = (0 == internal_malloc_join\n              ? (pool->instance.head += alloc_size)\n              : ((char*)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                (uintptr_t*)headaddr, alloc_size, LIBXSMM_ATOMIC_SEQ_CST)));\n# else\n            char *const head = (char*)(pool->instance.head += alloc_size);\n# endif\n            *memory = LIBXSMM_ALIGN(head - alloc_size, align_size);\n          }\n          else { /* fall-back to local memory allocation */\n            const size_t incsize = req_size - LIBXSMM_MIN(pool_size, req_size);\n            pool->instance.incsize = LIBXSMM_MAX(pool->instance.incsize, incsize);\n# if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            if (0 == internal_malloc_join) {\n              --pool->instance.counter;\n            }\n            else {\n              LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n            }\n# else\n            --pool->instance.counter;\n# endif\n            if (\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n              (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site &&\n# endif\n              0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags))\n            {\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_local_max < watermark) internal_malloc_local_max = watermark; /* accept data-race */\n            }\n            else {\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_private_max < watermark) internal_malloc_private_max = watermark; /* accept data-race */\n            }\n            local_size = size;\n          }\n        }\n        else { /* fresh pool */\n          const size_t scratch_limit = libxsmm_get_scratch_limit();\n          const size_t scratch_size = internal_get_scratch_size(pool); /* exclude current pool */\n          const size_t limit_size = (1 < npools ? (scratch_limit - LIBXSMM_MIN(scratch_size, scratch_limit)) : LIBXSMM_SCRATCH_UNLIMITED);\n          const size_t scale_size = (size_t)(1 != libxsmm_scratch_scale ? (libxsmm_scratch_scale * alloc_size) : alloc_size); /* hysteresis */\n          const size_t incsize = (size_t)(libxsmm_scratch_scale * pool->instance.incsize);\n          const size_t maxsize = LIBXSMM_MAX(scale_size, pool->instance.minsize) + incsize;\n          const size_t limsize = LIBXSMM_MIN(maxsize, limit_size);\n          const size_t minsize = limsize;\n          LIBXSMM_ASSERT(1 <= libxsmm_scratch_scale);\n          LIBXSMM_ASSERT(1 == counter);\n          pool->instance.incsize = 0; /* reset */\n          pool->instance.minsize = minsize;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n          pool->instance.site = site;\n#   if (0 != LIBXSMM_SYNC)\n          pool->instance.tid = tid;\n#   endif\n# endif\n          if (alloc_size <= minsize && /* allocate scratch pool */\n            EXIT_SUCCESS == libxsmm_xmalloc(memory, minsize, 0/*auto-align*/,\n              (flags | LIBXSMM_MALLOC_FLAG_SCRATCH) & ~LIBXSMM_MALLOC_FLAG_REALLOC,\n              NULL/*extra*/, 0/*extra_size*/))\n          {\n            pool->instance.buffer = (char*)*memory;\n            pool->instance.head = pool->instance.buffer + alloc_size;\n            *memory = LIBXSMM_ALIGN((char*)*memory, align_size);\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n            if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site)\n# endif\n            {\n              LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_scratch_nmallocs, 1, LIBXSMM_ATOMIC_RELAXED);\n            }\n          }\n          else { /* fall-back to local allocation */\n            LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n            if (0 != libxsmm_verbosity /* library code is expected to be mute */\n              && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n            {\n              if (alloc_size <= minsize) {\n                fprintf(stderr, \"LIBXSMM ERROR: failed to allocate scratch memory!\\n\");\n              }\n              else if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != caller\n                && (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity))\n              {\n                fprintf(stderr, \"LIBXSMM WARNING: scratch memory domain exhausted!\\n\");\n              }\n            }\n            local_size = size;\n          }\n        }\n      }\n      else { /* fall-back to local memory allocation */\n        local_size = size;\n      }\n    }\n    else { /* fall-back to local memory allocation */\n      local_size = size;\n    }\n    if (0 != local_size)\n#else\n    local_size = size;\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n    { /* local memory allocation */\n      if (EXIT_SUCCESS != libxsmm_xmalloc(memory, local_size, alignment,\n          flags & ~(LIBXSMM_MALLOC_FLAG_SCRATCH | LIBXSMM_MALLOC_FLAG_REALLOC), NULL/*extra*/, 0/*extra_size*/)\n        && /* library code is expected to be mute */0 != libxsmm_verbosity\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: scratch memory fall-back failed!\\n\");\n        LIBXSMM_ASSERT(NULL == *memory);\n      }\n      if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != caller) {\n        LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_scratch_nmallocs, 1, LIBXSMM_ATOMIC_RELAXED);\n      }\n    }\n  }\n  else { /* reallocate memory */\n    const void *const preserve = *memory;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(preserve);\n    if (NULL != pool) {\n      const internal_malloc_info_type *const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n      void* buffer;\n      LIBXSMM_ASSERT(pool->instance.buffer <= pool->instance.head && NULL != info);\n      internal_scratch_malloc(&buffer, size, alignment,\n        ~LIBXSMM_MALLOC_FLAG_REALLOC & (LIBXSMM_MALLOC_FLAG_SCRATCH | flags), caller);\n      if (NULL != buffer) {\n        memcpy(buffer, preserve, LIBXSMM_MIN(size, info->size)); /* TODO: memmove? */\n        *memory = buffer;\n      }\n      internal_scratch_free(memory, pool);\n    }\n    else\n#endif\n    { /* non-pooled (potentially foreign pointer) */\n#if !defined(NDEBUG)\n      const int status =\n#endif\n      libxsmm_xmalloc(memory, size, alignment/* no need here to determine alignment of given buffer */,\n        ~LIBXSMM_MALLOC_FLAG_SCRATCH & flags, NULL/*extra*/, 0/*extra_size*/);\n      assert(EXIT_SUCCESS == status || NULL == *memory); /* !LIBXSMM_ASSERT */\n    }\n  }\n}\n\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_APIVAR_PRIVATE_DEF(libxsmm_malloc_fntype libxsmm_malloc_fn);\n\n#if defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)\nLIBXSMM_API_INTERN void* internal_memalign_malloc(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_malloc(size_t alignment, size_t size)\n{\n  LIBXSMM_UNUSED(alignment);\n  LIBXSMM_ASSERT(NULL != libxsmm_malloc_fn.malloc.dlsym);\n  return libxsmm_malloc_fn.malloc.ptr(size);\n}\n#elif defined(LIBXSMM_MALLOC_HOOK_KMP)\nLIBXSMM_API_INTERN void* internal_memalign_twiddle(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_twiddle(size_t alignment, size_t size)\n{\n  LIBXSMM_ASSERT(NULL != libxsmm_malloc_fn.alignmem.dlsym);\n  return libxsmm_malloc_fn.alignmem.ptr(size, alignment);\n}\n#endif\n#endif /*defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)*/\n\n\n#if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n\nLIBXSMM_API_INTERN void* internal_memalign_hook(size_t /*alignment*/, size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_memalign_hook(size_t alignment, size_t size, const void* caller)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, caller);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, caller);\n# endif\n  return result;\n}\n\nLIBXSMM_API void* __wrap_memalign(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_memalign(size_t alignment, size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\nLIBXSMM_API_INTERN void* internal_malloc_hook(size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_malloc_hook(size_t size, const void* caller)\n{\n  return internal_memalign_hook(0/*auto-alignment*/, size, caller);\n}\n\nLIBXSMM_API void* __wrap_malloc(size_t /*size*/);\nLIBXSMM_API void* __wrap_malloc(size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API void* __wrap_calloc(size_t /*num*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_calloc(size_t num, size_t size)\n{\n  void* result;\n  const size_t nbytes = num * size;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# endif\n  /* TODO: signal anonymous/zeroed pages */\n  if (NULL != result) memset(result, 0, nbytes);\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API_INTERN void* internal_realloc_hook(void* /*ptr*/, size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_realloc_hook(void* ptr, size_t size, const void* caller)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, caller);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, caller);\n# endif\n  return result;\n}\n\nLIBXSMM_API void* __wrap_realloc(void* /*ptr*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_realloc(void* ptr, size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, NULL/*caller*/);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, NULL/*caller*/);\n# endif\n  return result;\n}\n#endif\n\nLIBXSMM_API_INTERN void internal_free_hook(void* /*ptr*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void internal_free_hook(void* ptr, const void* caller)\n{\n  INTERNAL_FREE_HOOK(ptr, caller);\n}\n\nLIBXSMM_API void __wrap_free(void* /*ptr*/);\nLIBXSMM_API void __wrap_free(void* ptr)\n{\n  INTERNAL_FREE_HOOK(ptr, NULL/*caller*/);\n}\n\n#endif /*(defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))*/\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* memalign(size_t /*alignment*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* memalign(size_t alignment, size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* malloc(size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* malloc(size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* calloc(size_t /*num*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* calloc(size_t num, size_t size) LIBXSMM_THROW\n{\n  void* result;\n  const size_t nbytes = num * size;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# endif\n  /* TODO: signal anonymous/zeroed pages */\n  if (NULL != result) memset(result, 0, nbytes);\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void* realloc(void* /*ptr*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void* realloc(void* ptr, size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, NULL/*caller*/);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, NULL/*caller*/);\n# endif\n  return result;\n}\n#endif\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void free(void* /*ptr*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void free(void* ptr) LIBXSMM_THROW\n{\n  INTERNAL_FREE_HOOK(ptr, NULL/*caller*/);\n}\n#endif /*defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)*/\n\n\nLIBXSMM_API_INTERN void libxsmm_malloc_init(void)\n{\n#if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n  const char *const env = getenv(\"LIBXSMM_MALLOC_JOIN\");\n  if (NULL != env && 0 != *env) internal_malloc_join = atoi(env);\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n# if defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)\n  void* handle_qkmalloc = NULL;\n  dlerror(); /* clear an eventual error status */\n  handle_qkmalloc = dlopen(\"libqkmalloc.so\", RTLD_LAZY);\n  if (NULL != handle_qkmalloc) {\n    libxsmm_malloc_fn.memalign.ptr = internal_memalign_malloc;\n    libxsmm_malloc_fn.malloc.dlsym = dlsym(handle_qkmalloc, \"malloc\");\n    if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      libxsmm_malloc_fn.calloc.dlsym = dlsym(handle_qkmalloc, \"calloc\");\n      if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n#   endif\n      {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        libxsmm_malloc_fn.realloc.dlsym = dlsym(handle_qkmalloc, \"realloc\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n#   endif\n        {\n          libxsmm_malloc_fn.free.dlsym = dlsym(handle_qkmalloc, \"free\");\n        }\n      }\n    }\n    dlclose(handle_qkmalloc);\n  }\n  if (NULL == libxsmm_malloc_fn.free.ptr)\n# elif defined(LIBXSMM_MALLOC_HOOK_KMP)\n  dlerror(); /* clear an eventual error status */\n  libxsmm_malloc_fn.alignmem.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_aligned_malloc\");\n  if (NULL == dlerror() && NULL != libxsmm_malloc_fn.alignmem.dlsym) {\n    libxsmm_malloc_fn.memalign.ptr = internal_memalign_twiddle;\n    libxsmm_malloc_fn.malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_malloc\");\n    if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n# if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      libxsmm_malloc_fn.calloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_calloc\");\n      if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n# endif\n      {\n# if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        libxsmm_malloc_fn.realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_realloc\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n# endif\n        {\n          libxsmm_malloc_fn.free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_free\");\n        }\n      }\n    }\n  }\n  if (NULL == libxsmm_malloc_fn.free.ptr)\n# endif /*defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)*/\n  {\n    dlerror(); /* clear an eventual error status */\n# if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n    libxsmm_malloc_fn.memalign.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_memalign\");\n    if (NULL == dlerror() && NULL != libxsmm_malloc_fn.memalign.dlsym) {\n      libxsmm_malloc_fn.malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_malloc\");\n      if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n        libxsmm_malloc_fn.calloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_calloc\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n#   endif\n        {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n          libxsmm_malloc_fn.realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_realloc\");\n          if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n#   endif\n          {\n            libxsmm_malloc_fn.free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_free\");\n          }\n        }\n      }\n    }\n    if (NULL == libxsmm_malloc_fn.free.ptr) {\n      void* handle_libc = NULL;\n      dlerror(); /* clear an eventual error status */\n      handle_libc = dlopen(\"libc.so.\" LIBXSMM_STRINGIFY(LIBXSMM_MALLOC_GLIBC), RTLD_LAZY);\n      if (NULL != handle_libc) {\n        libxsmm_malloc_fn.memalign.dlsym = dlsym(handle_libc, \"__libc_memalign\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.memalign.dlsym) {\n          libxsmm_malloc_fn.malloc.dlsym = dlsym(handle_libc, \"__libc_malloc\");\n          if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n            libxsmm_malloc_fn.calloc.dlsym = dlsym(handle_libc, \"__libc_calloc\");\n            if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n#   endif\n            {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n              libxsmm_malloc_fn.realloc.dlsym = dlsym(handle_libc, \"__libc_realloc\");\n              if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n#   endif\n              {\n                libxsmm_malloc_fn.free.dlsym = dlsym(handle_libc, \"__libc_free\");\n              }\n            }\n          }\n        }\n        dlclose(handle_libc);\n      }\n    }\n#   if 0\n    { /* attempt to setup deprecated GLIBC hooks */\n      union { const void* dlsym; void* (**ptr)(size_t, size_t, const void*); } hook_memalign;\n      dlerror(); /* clear an eventual error status */\n      hook_memalign.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__memalign_hook\");\n      if (NULL == dlerror() && NULL != hook_memalign.dlsym) {\n        union { const void* dlsym; void* (**ptr)(size_t, const void*); } hook_malloc;\n        hook_malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__malloc_hook\");\n        if (NULL == dlerror() && NULL != hook_malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n          union { const void* dlsym; void* (**ptr)(void*, size_t, const void*); } hook_realloc;\n          hook_realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__realloc_hook\");\n          if (NULL == dlerror() && NULL != hook_realloc.dlsym)\n#   endif\n          {\n            union { const void* dlsym; void (**ptr)(void*, const void*); } hook_free;\n            hook_free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__free_hook\");\n            if (NULL == dlerror() && NULL != hook_free.dlsym) {\n              *hook_memalign.ptr = internal_memalign_hook;\n              *hook_malloc.ptr = internal_malloc_hook;\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n              *hook_realloc.ptr = internal_realloc_hook;\n#   endif\n              *hook_free.ptr = internal_free_hook;\n            }\n          }\n        }\n      }\n    }\n#   endif\n# else /* TODO */\n# endif /*(defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))*/\n  }\n  if (NULL != libxsmm_malloc_fn.free.ptr) {\n# if defined(LIBXSMM_MALLOC_HOOK_IMALLOC)\n    union { const void* dlsym; libxsmm_malloc_fun* ptr; } i_malloc;\n    i_malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_malloc\");\n    if (NULL == dlerror() && NULL != i_malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      union { const void* dlsym; void* (**ptr)(size_t, size_t); } i_calloc;\n      i_calloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_calloc\");\n      if (NULL == dlerror() && NULL != i_calloc.dlsym)\n#   endif\n      {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        union { const void* dlsym; libxsmm_realloc_fun* ptr; } i_realloc;\n        i_realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_realloc\");\n        if (NULL == dlerror() && NULL != i_realloc.dlsym)\n#   endif\n        {\n          union { const void* dlsym; libxsmm_free_fun* ptr; } i_free;\n          i_free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_free\");\n          if (NULL == dlerror() && NULL != i_free.dlsym) {\n            *i_malloc.ptr = libxsmm_malloc_fn.malloc.ptr;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n            *i_calloc.ptr = libxsmm_malloc_fn.calloc.ptr;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n            *i_realloc.ptr = libxsmm_malloc_fn.realloc.ptr;\n#   endif\n            *i_free.ptr = libxsmm_malloc_fn.free.ptr;\n          }\n        }\n      }\n    }\n# endif /*defined(LIBXSMM_MALLOC_HOOK_IMALLOC)*/\n  }\n  else { /* fall-back: potentially recursive */\n# if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n    libxsmm_malloc_fn.memalign.ptr = __libc_memalign;\n    libxsmm_malloc_fn.malloc.ptr = __libc_malloc;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n    libxsmm_malloc_fn.calloc.ptr = __libc_calloc;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n    libxsmm_malloc_fn.realloc.ptr = __libc_realloc;\n#   endif\n    libxsmm_malloc_fn.free.ptr = __libc_free;\n# else\n    libxsmm_malloc_fn.memalign.ptr = libxsmm_memalign_internal;\n    libxsmm_malloc_fn.malloc.ptr = malloc;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n    libxsmm_malloc_fn.calloc.ptr = calloc;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n    libxsmm_malloc_fn.realloc.ptr = realloc;\n#   endif\n    libxsmm_malloc_fn.free.ptr = free;\n# endif\n  }\n#endif\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_malloc_finalize(void)\n{\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xset_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != lock) {\n    LIBXSMM_INIT\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n  if (NULL != malloc_fn.function && NULL != free_fn.function) {\n    libxsmm_default_allocator_context = context;\n    libxsmm_default_malloc_fn = malloc_fn;\n    libxsmm_default_free_fn = free_fn;\n  }\n  else {\n    libxsmm_malloc_function internal_malloc_fn;\n    libxsmm_free_function internal_free_fn;\n    const void* internal_allocator = NULL;\n    internal_malloc_fn.function = __real_malloc;\n    internal_free_fn.function = __real_free;\n    /*internal_allocator = NULL;*/\n    if (NULL == malloc_fn.function && NULL == free_fn.function) {\n      libxsmm_default_allocator_context = internal_allocator;\n      libxsmm_default_malloc_fn = internal_malloc_fn;\n      libxsmm_default_free_fn = internal_free_fn;\n    }\n    else { /* invalid allocator */\n      static int error_once = 0;\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: allocator setup without malloc or free function!\\n\");\n      }\n      /* keep any valid (previously instantiated) default allocator */\n      if (NULL == libxsmm_default_malloc_fn.function || NULL == libxsmm_default_free_fn.function) {\n        libxsmm_default_allocator_context = internal_allocator;\n        libxsmm_default_malloc_fn = internal_malloc_fn;\n        libxsmm_default_free_fn = internal_free_fn;\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xget_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != context || NULL != malloc_fn || NULL != free_fn) {\n    if (NULL != lock) {\n      LIBXSMM_INIT\n      LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n    }\n    if (context) *context = libxsmm_default_allocator_context;\n    if (NULL != malloc_fn) *malloc_fn = libxsmm_default_malloc_fn;\n    if (NULL != free_fn) *free_fn = libxsmm_default_free_fn;\n    if (NULL != lock) {\n      LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n    }\n  }\n  else if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    static int error_once = 0;\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid signature used to get the default memory allocator!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xset_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  int result = EXIT_SUCCESS;\n  static int error_once = 0;\n  if (NULL != lock) {\n    LIBXSMM_INIT\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n  /* make sure the default allocator is setup before adopting it eventually */\n  if (NULL == libxsmm_default_malloc_fn.function || NULL == libxsmm_default_free_fn.function) {\n    const libxsmm_malloc_function null_malloc_fn = { NULL };\n    const libxsmm_free_function null_free_fn = { NULL };\n    libxsmm_xset_default_allocator(NULL/*already locked*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n  }\n  if (NULL == malloc_fn.function && NULL == free_fn.function) { /* adopt default allocator */\n    libxsmm_scratch_allocator_context = libxsmm_default_allocator_context;\n    libxsmm_scratch_malloc_fn = libxsmm_default_malloc_fn;\n    libxsmm_scratch_free_fn = libxsmm_default_free_fn;\n  }\n  else if (NULL != malloc_fn.function) {\n    if (NULL == free_fn.function\n      && /*warning*/(LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM WARNING: scratch allocator setup without free function!\\n\");\n    }\n    libxsmm_scratch_allocator_context = context;\n    libxsmm_scratch_malloc_fn = malloc_fn;\n    libxsmm_scratch_free_fn = free_fn; /* NULL allowed */\n  }\n  else { /* invalid scratch allocator */\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid scratch allocator (default used)!\\n\");\n    }\n    /* keep any valid (previously instantiated) scratch allocator */\n    if (NULL == libxsmm_scratch_malloc_fn.function) {\n      libxsmm_scratch_allocator_context = libxsmm_default_allocator_context;\n      libxsmm_scratch_malloc_fn = libxsmm_default_malloc_fn;\n      libxsmm_scratch_free_fn = libxsmm_default_free_fn;\n    }\n    result = EXIT_FAILURE;\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xget_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != context || NULL != malloc_fn || NULL != free_fn) {\n    if (NULL != lock) {\n      LIBXSMM_INIT\n      LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n    }\n    if (context) *context = libxsmm_scratch_allocator_context;\n    if (NULL != malloc_fn) *malloc_fn = libxsmm_scratch_malloc_fn;\n    if (NULL != free_fn) *free_fn = libxsmm_scratch_free_fn;\n    if (NULL != lock) {\n      LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n    }\n  }\n  else if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    static int error_once = 0;\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid signature used to get the scratch memory allocator!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_set_default_allocator(const void* context,\n  libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  return libxsmm_xset_default_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_default_allocator(const void** context,\n  libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  return libxsmm_xget_default_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_set_scratch_allocator(const void* context,\n  libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  return libxsmm_xset_scratch_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_scratch_allocator(const void** context,\n  libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  return libxsmm_xget_scratch_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc_xinfo(const void* memory, size_t* size, int* flags, void** extra)\n{\n  int result;\n#if !defined(NDEBUG)\n  if (NULL != size || NULL != extra)\n#endif\n  {\n    const int check = ((NULL == flags || 0 == (LIBXSMM_MALLOC_FLAG_X & *flags)) ? 2 : 1);\n    const internal_malloc_info_type *const info = internal_malloc_info(memory, check);\n    if (NULL != info) {\n      if (NULL != size) *size = info->size;\n      if (NULL != flags) *flags = info->flags;\n      if (NULL != extra) *extra = info->pointer;\n      result = EXIT_SUCCESS;\n    }\n    else { /* potentially foreign buffer */\n      result = (NULL != memory ? EXIT_FAILURE : EXIT_SUCCESS);\n      if (NULL != size) *size = 0;\n      if (NULL != flags) *flags = 0;\n      if (NULL != extra) *extra = 0;\n    }\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: attachment error for memory buffer %p!\\n\", memory);\n    }\n    LIBXSMM_ASSERT_MSG(0/*false*/, \"LIBXSMM ERROR: attachment error\");\n    result = EXIT_FAILURE;\n  }\n#endif\n  return result;\n}\n\n\n#if !defined(_WIN32)\n\nLIBXSMM_API_INLINE void internal_xmalloc_mhint(void* buffer, size_t size)\n{\n  LIBXSMM_ASSERT((MAP_FAILED != buffer && NULL != buffer) || 0 == size);\n#if defined(_DEFAULT_SOURCE) || defined(_BSD_SOURCE)\n  /* proceed after failed madvise (even in case of an error; take what we got) */\n  /* issue no warning as a failure seems to be related to the kernel version */\n  madvise(buffer, size, MADV_NORMAL/*MADV_RANDOM*/\n# if defined(MADV_NOHUGEPAGE) /* if not available, we then take what we got (THP) */\n    | ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) > size ? MADV_NOHUGEPAGE : 0)\n# endif\n# if defined(MADV_DONTDUMP)\n    | ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) > size ? 0 : MADV_DONTDUMP)\n# endif\n  );\n#else\n  LIBXSMM_UNUSED(buffer); LIBXSMM_UNUSED(size);\n#endif\n}\n\n\nLIBXSMM_API_INLINE void* internal_xmalloc_xmap(const char* dir, size_t size, int flags, void** rx)\n{\n  void* result = MAP_FAILED;\n  char filename[4096] = LIBXSMM_MALLOC_XMAP_TEMPLATE;\n  int i = 0;\n  LIBXSMM_ASSERT(NULL != rx && MAP_FAILED != *rx);\n  if (NULL != dir && 0 != *dir) {\n    i = LIBXSMM_SNPRINTF(filename, sizeof(filename), \"%s/\" LIBXSMM_MALLOC_XMAP_TEMPLATE, dir);\n  }\n  if (0 <= i && i < (int)sizeof(filename)) {\n    /* coverity[secure_temp] */\n    i = mkstemp(filename);\n    if (0 <= i) {\n      if (0 == unlink(filename) && 0 == ftruncate(i, size)) {\n        const int mflags = (flags | LIBXSMM_MAP_SHARED);\n        void *const xmap = mmap(*rx, size, PROT_READ | PROT_EXEC, mflags, i, 0/*offset*/);\n        if (MAP_FAILED != xmap) {\n          LIBXSMM_ASSERT(NULL != xmap);\n          result = mmap(NULL, size, PROT_READ | PROT_WRITE, mflags, i, 0/*offset*/);\n          if (MAP_FAILED != result) {\n            LIBXSMM_ASSERT(NULL != result);\n            internal_xmalloc_mhint(xmap, size);\n            *rx = xmap;\n          }\n          else {\n            munmap(xmap, size);\n            *rx = NULL;\n          }\n        }\n      }\n      close(i);\n    }\n  }\n  return result;\n}\n\n#endif /*!defined(_WIN32)*/\n\n\nLIBXSMM_API_INLINE void* internal_xrealloc(void** ptr, internal_malloc_info_type** info, size_t size,\n  libxsmm_realloc_fun realloc_fn, libxsmm_free_fun free_fn)\n{\n  char *const base = (char*)(NULL != *info ? (*info)->pointer : *ptr), *result;\n  LIBXSMM_ASSERT(NULL != *ptr);\n  /* may implicitly invalidate info */\n  result = (char*)realloc_fn(base, size);\n  if (result == base) { /* signal no-copy */\n    LIBXSMM_ASSERT(NULL != result);\n    *info = NULL; /* no delete */\n    *ptr = NULL; /* no copy */\n  }\n  else if (NULL != result) { /* copy */\n    const size_t offset_src = (const char*)*ptr - base;\n    *ptr = result + offset_src; /* copy */\n    *info = NULL; /* no delete */\n  }\n#if !defined(NDEBUG) && 0\n  else { /* failed */\n    if (NULL != *info) {\n      /* implicitly invalidates info */\n      internal_xfree(*ptr, *info);\n    }\n    else { /* foreign pointer */\n      free_fn(*ptr);\n    }\n    *info = NULL; /* no delete */\n    *ptr = NULL; /* no copy */\n  }\n#else\n  LIBXSMM_UNUSED(free_fn);\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void* internal_xmalloc(void** /*ptr*/, internal_malloc_info_type** /*info*/, size_t /*size*/,\n  const void* /*context*/, libxsmm_malloc_function /*malloc_fn*/, libxsmm_free_function /*free_fn*/);\nLIBXSMM_API_INTERN void* internal_xmalloc(void** ptr, internal_malloc_info_type** info, size_t size,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  void* result;\n  LIBXSMM_ASSERT(NULL != ptr && NULL != info && NULL != malloc_fn.function);\n  if (NULL == *ptr) {\n    result = (NULL == context\n      ? malloc_fn.function(size)\n      : malloc_fn.ctx_form(size, context));\n  }\n  else { /* reallocate */\n    if (NULL != free_fn.function /* prefer free_fn since it is part of pointer-info */\n      ? (__real_free == free_fn.function || free == free_fn.function)\n      : (__real_malloc == malloc_fn.function || malloc == malloc_fn.function))\n    {\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n      result = internal_xrealloc(ptr, info, size, __real_realloc, __real_free);\n#else\n      result = internal_xrealloc(ptr, info, size, realloc, __real_free);\n#endif\n    }\n    else { /* fall-back with regular allocation */\n      result = (NULL == context\n        ? malloc_fn.function(size)\n        : malloc_fn.ctx_form(size, context));\n      if (NULL == result) { /* failed */\n        if (NULL != *info) {\n          internal_xfree(*ptr, *info);\n        }\n        else { /* foreign pointer */\n          (NULL != free_fn.function ? free_fn.function : __real_free)(*ptr);\n        }\n        *ptr = NULL; /* safe delete */\n      }\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xmalloc(void** memory, size_t size, size_t alignment,\n  int flags, const void* extra, size_t extra_size)\n{\n  int result = EXIT_SUCCESS;\n#if !defined(NDEBUG)\n  if (NULL != memory)\n#endif\n  {\n    static int error_once = 0;\n    if (0 != size) {\n      size_t alloc_alignment = 0, alloc_size = 0, max_preserve = 0;\n      internal_malloc_info_type* info = NULL;\n      void* buffer = NULL, * reloc = NULL;\n      /* ATOMIC BEGIN: this region should be atomic/locked */\n      const void* context = libxsmm_default_allocator_context;\n      libxsmm_malloc_function malloc_fn = libxsmm_default_malloc_fn;\n      libxsmm_free_function free_fn = libxsmm_default_free_fn;\n      if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) {\n        context = libxsmm_scratch_allocator_context;\n        malloc_fn = libxsmm_scratch_malloc_fn;\n        free_fn = libxsmm_scratch_free_fn;\n#if defined(LIBXSMM_MALLOC_MMAP_SCRATCH)\n        flags |= LIBXSMM_MALLOC_FLAG_MMAP;\n#endif\n      }\n      if ((0 != (internal_malloc_kind & 1) && 0 < internal_malloc_kind)\n        || NULL == malloc_fn.function || NULL == free_fn.function)\n      {\n        malloc_fn.function = __real_malloc;\n        free_fn.function = __real_free;\n        context = NULL;\n      }\n      /* ATOMIC END: this region should be atomic */\n      flags |= LIBXSMM_MALLOC_FLAG_RW; /* normalize given flags since flags=0 is accepted as well */\n      if (0 != (LIBXSMM_MALLOC_FLAG_REALLOC & flags) && NULL != *memory) {\n        info = internal_malloc_info(*memory, 2/*check*/);\n        if (NULL != info) {\n          max_preserve = info->size;\n        }\n        else { /* reallocation of unknown allocation */\n          flags &= ~LIBXSMM_MALLOC_FLAG_MMAP;\n        }\n      }\n      else *memory = NULL;\n#if !defined(LIBXSMM_MALLOC_MMAP)\n      if (0 == (LIBXSMM_MALLOC_FLAG_X & flags) && 0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n        alloc_alignment = (0 == (LIBXSMM_MALLOC_FLAG_REALLOC & flags) ? libxsmm_alignment(size, alignment) : alignment);\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1;\n        buffer = internal_xmalloc(memory, &info, alloc_size, context, malloc_fn, free_fn);\n      }\n      else\n#endif\n      if (NULL == info || size != info->size) {\n#if defined(_WIN32) ||defined(__CYGWIN__)\n        const int mflags = (0 != (LIBXSMM_MALLOC_FLAG_X & flags) ? PAGE_EXECUTE_READWRITE : PAGE_READWRITE);\n        static SIZE_T alloc_alignmax = 0, alloc_pagesize = 0;\n        if (0 == alloc_alignmax) { /* first/one time */\n          SYSTEM_INFO system_info;\n          GetSystemInfo(&system_info);\n          alloc_pagesize = system_info.dwPageSize;\n          alloc_alignmax = GetLargePageMinimum();\n        }\n        if ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size) { /* attempt to use large pages */\n          HANDLE process_token;\n          alloc_alignment = (NULL == info\n            ? (0 == alignment ? alloc_alignmax : libxsmm_lcm(alignment, alloc_alignmax))\n            : libxsmm_lcm(alignment, alloc_alignmax));\n          alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_alignmax);\n          if (TRUE == OpenProcessToken(GetCurrentProcess(), TOKEN_ADJUST_PRIVILEGES | TOKEN_QUERY, &process_token)) {\n            TOKEN_PRIVILEGES tp;\n            if (TRUE == LookupPrivilegeValue(NULL, TEXT(\"SeLockMemoryPrivilege\"), &tp.Privileges[0].Luid)) {\n              tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED; tp.PrivilegeCount = 1; /* enable privilege */\n              if (TRUE == AdjustTokenPrivileges(process_token, FALSE, &tp, 0, (PTOKEN_PRIVILEGES)NULL, 0)\n                && ERROR_SUCCESS == GetLastError()/*may has failed (regardless of TRUE)*/)\n              {\n                /* VirtualAlloc cannot be used to reallocate memory */\n                buffer = VirtualAlloc(NULL, alloc_size, MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES, mflags);\n              }\n              tp.Privileges[0].Attributes = 0; /* disable privilege */\n              AdjustTokenPrivileges(process_token, FALSE, &tp, 0, (PTOKEN_PRIVILEGES)NULL, 0);\n            }\n            CloseHandle(process_token);\n          }\n        }\n        else { /* small allocation using regular page-size */\n          alloc_alignment = (NULL == info ? libxsmm_alignment(size, alignment) : alignment);\n          alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_pagesize);\n        }\n        if (NULL == buffer) { /* small allocation or retry with regular page size */\n          /* VirtualAlloc cannot be used to reallocate memory */\n          buffer = VirtualAlloc(NULL, alloc_size, MEM_RESERVE | MEM_COMMIT, mflags);\n        }\n        if (NULL != buffer) {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP; /* select the corresponding deallocation */\n        }\n        else if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) { /* fall-back allocation */\n          buffer = internal_xmalloc(memory, &info, alloc_size, context, malloc_fn, free_fn);\n        }\n#else /* !defined(_WIN32) */\n# if defined(MAP_HUGETLB)\n        static size_t limit_hugetlb = LIBXSMM_SCRATCH_UNLIMITED;\n# endif\n# if defined(MAP_LOCKED)\n        static size_t limit_plocked = LIBXSMM_SCRATCH_UNLIMITED;\n# endif\n# if defined(MAP_32BIT)\n        static int map32 = 1;\n# endif\n        int mflags = 0\n# if defined(MAP_UNINITIALIZED) && 0/*fails with WSL*/\n          | MAP_UNINITIALIZED /* unlikely available */\n# endif\n# if defined(MAP_NORESERVE)\n          | (LIBXSMM_MALLOC_ALIGNMAX < size ? 0 : MAP_NORESERVE)\n# endif\n# if defined(MAP_32BIT)\n          | ((0 != (LIBXSMM_MALLOC_FLAG_X & flags) && 0 != map32\n            && LIBXSMM_X86_AVX512_CORE > libxsmm_target_archid\n            && LIBXSMM_X86_AVX512 < libxsmm_target_archid) ? MAP_32BIT : 0)\n# endif\n# if defined(MAP_HUGETLB) /* may fail depending on system settings */\n          | ((0 == (LIBXSMM_MALLOC_FLAG_X & flags)\n            && ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size ||\n              0 != (LIBXSMM_MALLOC_FLAG_PHUGE & flags))\n            && (internal_malloc_hugetlb + size) < limit_hugetlb) ? MAP_HUGETLB : 0)\n# endif\n# if defined(MAP_LOCKED) && !defined(LIBXSMM_MALLOC_LOCK_ONFAULT)\n          | ((0 == (LIBXSMM_MALLOC_FLAG_X & flags)\n            && (internal_malloc_plocked + size) < limit_plocked) ? MAP_LOCKED : 0)\n# endif\n        ; /* mflags */\n# if defined(MAP_POPULATE)\n        { static int prefault = 0;\n          if (0 == prefault) { /* prefault only on Linux 3.10.0-327 (and later) to avoid data race in page-fault handler */\n            struct utsname osinfo; unsigned int version_major = 3, version_minor = 10, version_update = 0, version_patch = 327;\n            if (0 <= uname(&osinfo) && 0 == strcmp(\"Linux\", osinfo.sysname)\n              && 4 == sscanf(osinfo.release, \"%u.%u.%u-%u\", &version_major, &version_minor, &version_update, &version_patch)\n              && LIBXSMM_VERSION4(3, 10, 0, 327) > LIBXSMM_VERSION4(version_major, version_minor, version_update, version_patch))\n            {\n              mflags |= MAP_POPULATE; prefault = 1;\n            }\n            else prefault = -1;\n          }\n          else if (1 == prefault) mflags |= MAP_POPULATE;\n        }\n# endif\n        /* make allocated size at least a multiple of the smallest page-size to avoid split-pages (unmap!) */\n        alloc_alignment = libxsmm_lcm(0 == alignment ? libxsmm_alignment(size, alignment) : alignment, LIBXSMM_PAGE_MINSIZE);\n        alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_alignment);\n        if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* anonymous and non-executable */\n# if defined(MAP_32BIT)\n          LIBXSMM_ASSERT(0 == (MAP_32BIT & mflags));\n# endif\n# if 0\n          LIBXSMM_ASSERT(NULL != info || NULL == *memory); /* no memory mapping of foreign pointer */\n# endif\n          buffer = mmap(NULL == info ? NULL : info->pointer, alloc_size, PROT_READ | PROT_WRITE,\n            MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | mflags, -1, 0/*offset*/);\n# if defined(MAP_HUGETLB)\n          INTERNAL_XMALLOC_KIND(MAP_HUGETLB, \"huge-page\", LIBXSMM_MALLOC_FLAG_PHUGE, flags, mflags,\n            internal_malloc_hugetlb, limit_hugetlb, info, alloc_size, buffer);\n# endif\n# if defined(MAP_LOCKED)\n#   if !defined(LIBXSMM_MALLOC_LOCK_ONFAULT)\n          INTERNAL_XMALLOC_KIND(MAP_LOCKED, \"locked-page\", LIBXSMM_MALLOC_FLAG_PLOCK, flags, mflags,\n            internal_malloc_plocked, limit_plocked, info, alloc_size, buffer);\n#   else\n          if (0 != (MAP_LOCKED & mflags) && MAP_FAILED != buffer) {\n            LIBXSMM_ASSERT(NULL != buffer);\n#     if 0 /* mlock2 is potentially not exposed */\n            if (0 == mlock2(buffer, alloc_size, MLOCK_ONFAULT))\n#     else\n            if (0 == syscall(SYS_mlock2, buffer, alloc_size, MLOCK_ONFAULT))\n#     endif\n            {\n              LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_plocked, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              flags |= LIBXSMM_MALLOC_FLAG_PLOCK;\n            }\n            else { /* update watermark */\n              INTERNAL_XMALLOC_WATERMARK(\"locked-page\", internal_malloc_plocked, limit_plocked, alloc_size);\n            }\n          }\n#   endif\n# endif\n        }\n        else { /* executable buffer requested */\n          static /*LIBXSMM_TLS*/ int fallback = -1; /* fall-back allocation method */\n# if defined(MAP_HUGETLB)\n          LIBXSMM_ASSERT(0 == (MAP_HUGETLB & mflags));\n# endif\n# if defined(MAP_LOCKED)\n          LIBXSMM_ASSERT(0 == (MAP_LOCKED & mflags));\n# endif\n          if (0 > (int)LIBXSMM_ATOMIC_LOAD(&fallback, LIBXSMM_ATOMIC_RELAXED)) {\n            const char *const env = getenv(\"LIBXSMM_SE\");\n            LIBXSMM_ATOMIC_STORE(&fallback, NULL == env\n              /* libxsmm_se decides */\n              ? (0 == libxsmm_se ? LIBXSMM_MALLOC_FINAL : LIBXSMM_MALLOC_FALLBACK)\n              /* user's choice takes precedence */\n              : ('0' != *env ? LIBXSMM_MALLOC_FALLBACK : LIBXSMM_MALLOC_FINAL),\n              LIBXSMM_ATOMIC_SEQ_CST);\n            LIBXSMM_ASSERT(0 <= fallback);\n          }\n          INTERNAL_XMALLOC(0, fallback, \"TMPDIR\", \"/tmp\", map32, mflags, alloc_size, buffer, &reloc); /* 1st try */\n          if (1 <= fallback) { /* continue with fall-back */\n            INTERNAL_XMALLOC(1, fallback, \"JITDUMPDIR\", \"\", map32, mflags, alloc_size, buffer, &reloc); /* 2nd try */\n            if (2 <= fallback) { /* continue with fall-back */\n              INTERNAL_XMALLOC(2, fallback, \"HOME\", \"\", map32, mflags, alloc_size, buffer, &reloc); /* 3rd try */\n              if (3 <= fallback) { /* continue with fall-back */\n                if (3 == fallback) { /* 4th try */\n                  buffer = mmap(reloc, alloc_size, PROT_READ | PROT_WRITE | PROT_EXEC,\n# if defined(MAP_32BIT)\n                    MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | (mflags & ~MAP_32BIT),\n# else\n                    MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | mflags,\n# endif\n                    -1, 0/*offset*/);\n                  if (MAP_FAILED == buffer) fallback = 4;\n                }\n                if (4 == fallback && MAP_FAILED != buffer) { /* final */\n                  LIBXSMM_ASSERT(fallback == LIBXSMM_MALLOC_FINAL + 1);\n                  buffer = MAP_FAILED; /* trigger final fall-back */\n                }\n              }\n            }\n          }\n        }\n        if (MAP_FAILED != buffer && NULL != buffer) {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP; /* select deallocation */\n        }\n        else { /* allocation failed */\n          if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) { /* ultimate fall-back */\n            buffer = (NULL != malloc_fn.function\n              ? (NULL == context ? malloc_fn.function(alloc_size) : malloc_fn.ctx_form(alloc_size, context))\n              : (NULL));\n          }\n          reloc = NULL;\n        }\n        if (MAP_FAILED != buffer && NULL != buffer) {\n          internal_xmalloc_mhint(buffer, alloc_size);\n        }\n#endif /* !defined(_WIN32) */\n      }\n      else { /* reallocation of the same pointer and size */\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alignment - 1;\n        if (NULL != info) {\n          buffer = info->pointer;\n          flags |= info->flags;\n        }\n        else {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP;\n          buffer = *memory;\n        }\n        alloc_alignment = alignment;\n        *memory = NULL; /* signal no-copy */\n      }\n      if (\n#if !defined(_WIN32) && !defined(__clang_analyzer__)\n        MAP_FAILED != buffer &&\n#endif\n        NULL != buffer)\n      {\n        char *const cbuffer = (char*)buffer, *const aligned = LIBXSMM_ALIGN(\n          cbuffer + extra_size + sizeof(internal_malloc_info_type), alloc_alignment);\n        internal_malloc_info_type *const buffer_info = (internal_malloc_info_type*)(\n          aligned - sizeof(internal_malloc_info_type));\n        LIBXSMM_ASSERT((aligned + size) <= (cbuffer + alloc_size));\n        LIBXSMM_ASSERT(0 < alloc_alignment);\n        /* former content must be preserved prior to setup of buffer_info */\n        if (NULL != *memory) { /* preserve/copy previous content */\n#if 0\n          LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_REALLOC & flags));\n#endif\n          /* content behind foreign pointers is not explicitly preserved; buffers may overlap */\n          memmove(aligned, *memory, LIBXSMM_MIN(max_preserve, size));\n          if (NULL != info /* known allocation (non-foreign pointer) */\n            && EXIT_SUCCESS != internal_xfree(*memory, info) /* !libxsmm_free */\n            && 0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          { /* display some extra context of the failure (reallocation) */\n            fprintf(stderr, \"LIBXSMM ERROR: memory reallocation failed to release memory!\\n\");\n          }\n        }\n        if (NULL != extra || 0 == extra_size) {\n          const char *const src = (const char*)extra;\n          int i; for (i = 0; i < (int)extra_size; ++i) cbuffer[i] = src[i];\n        }\n        else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: incorrect extraneous data specification!\\n\");\n          /* no EXIT_FAILURE because valid buffer is returned */\n        }\n        if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* update statistics */\n          if (0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags)) { /* public */\n            if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) { /* scratch */\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_public_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_public_max < watermark) internal_malloc_public_max = watermark; /* accept data-race */\n            }\n            else { /* local */\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_local_max < watermark) internal_malloc_local_max = watermark; /* accept data-race */\n            }\n          }\n          else { /* private */\n            const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n              &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n            if (internal_malloc_private_max < watermark) internal_malloc_private_max = watermark; /* accept data-race */\n          }\n        }\n        /* keep allocation function on record */\n        if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n          buffer_info->context = context;\n          buffer_info->free = free_fn;\n        }\n        else {\n          buffer_info->free.function = NULL;\n          buffer_info->context = NULL;\n        }\n        buffer_info->size = size; /* record user's size rather than allocated size */\n        buffer_info->pointer = buffer;\n        buffer_info->reloc = reloc;\n        buffer_info->flags = flags;\n#if defined(LIBXSMM_VTUNE)\n        buffer_info->code_id = 0;\n#endif /* info must be initialized to calculate correct checksum */\n#if !defined(LIBXSMM_MALLOC_CRC_OFF)\n# if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n        buffer_info->hash = LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &buffer_info);\n# else\n        buffer_info->hash = libxsmm_crc32(LIBXSMM_MALLOC_SEED, buffer_info,\n          (unsigned int)(((char*)&buffer_info->hash) - ((char*)buffer_info)));\n# endif\n#endif  /* finally commit/return allocated buffer */\n        *memory = aligned;\n      }\n      else {\n        if (0 != libxsmm_verbosity /* library code is expected to be mute */\n         && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          char alloc_size_buffer[32];\n          libxsmm_format_size(alloc_size_buffer, sizeof(alloc_size_buffer), alloc_size, \"KM\", \"B\", 10);\n          fprintf(stderr, \"LIBXSMM ERROR: failed to allocate %s with flag=%i!\\n\", alloc_size_buffer, flags);\n        }\n        result = EXIT_FAILURE;\n        *memory = NULL;\n      }\n    }\n    else {\n      if ((LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM WARNING: zero-sized memory allocation detected!\\n\");\n      }\n      *memory = NULL; /* no EXIT_FAILURE */\n    }\n  }\n#if !defined(NDEBUG)\n  else if (0 != size) {\n    result = EXIT_FAILURE;\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_xfree(const void* memory, int check)\n{\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n  static int error_once = 0;\n#endif\n  /*const*/ internal_malloc_info_type *const info = internal_malloc_info(memory, check);\n  if (NULL != info) { /* !libxsmm_free */\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n    if (EXIT_SUCCESS != internal_xfree(memory, info)) {\n      if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n    }\n#else\n    internal_xfree(memory, info);\n#endif\n  }\n  else if (NULL != memory) {\n#if 1\n    union { const void* const_ptr; void* ptr; } cast;\n    cast.const_ptr = memory; /* C-cast still warns */\n    __real_free(cast.ptr);\n#endif\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n    if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: deallocation does not match allocation!\\n\");\n    }\n#endif\n  }\n}\n\n\n#if defined(LIBXSMM_VTUNE)\nLIBXSMM_API_INLINE void internal_get_vtune_jitdesc(const void* code,\n  unsigned int code_id, size_t code_size, const char* code_name,\n  LIBXSMM_VTUNE_JIT_DESC_TYPE* desc)\n{\n  LIBXSMM_ASSERT(NULL != code && 0 != code_id && 0 != code_size && NULL != desc);\n  desc->method_id = code_id;\n  /* incorrect constness (method_name) */\n  desc->method_name = (char*)code_name;\n  /* incorrect constness (method_load_address) */\n  desc->method_load_address = (void*)code;\n  desc->method_size = code_size;\n  desc->line_number_size = 0;\n  desc->line_number_table = NULL;\n  desc->class_file_name = NULL;\n  desc->source_file_name = NULL;\n# if (2 <= LIBXSMM_VTUNE_JITVERSION)\n  desc->module_name = \"libxsmm.jit\";\n# endif\n}\n#endif\n\n\nLIBXSMM_API_INTERN int libxsmm_malloc_attrib(void** memory, int flags, const char* name)\n{\n  internal_malloc_info_type *const info = (NULL != memory ? internal_malloc_info(*memory, 0/*no check*/) : NULL);\n  int result = EXIT_SUCCESS;\n  static int error_once = 0;\n  if (NULL != info) {\n    void *const buffer = info->pointer;\n    const size_t size = info->size;\n#if defined(_WIN32)\n    LIBXSMM_ASSERT(NULL != buffer || 0 == size);\n#else\n    LIBXSMM_ASSERT((NULL != buffer && MAP_FAILED != buffer) || 0 == size);\n#endif\n    flags |= (info->flags & ~LIBXSMM_MALLOC_FLAG_RWX); /* merge with current flags */\n    /* quietly keep the read permission, but eventually revoke write permissions */\n    if (0 == (LIBXSMM_MALLOC_FLAG_W & flags) || 0 != (LIBXSMM_MALLOC_FLAG_X & flags)) {\n      const size_t alignment = (size_t)(((const char*)(*memory)) - ((const char*)buffer));\n      const size_t alloc_size = size + alignment;\n      if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* data-buffer; non-executable */\n#if defined(_WIN32)\n        /* TODO: implement memory protection under Microsoft Windows */\n        LIBXSMM_UNUSED(alloc_size);\n#else\n        if (EXIT_SUCCESS != mprotect(buffer, alloc_size/*entire memory region*/, PROT_READ)\n          && (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM WARNING: read-only request for buffer failed!\\n\");\n        }\n#endif\n      }\n      else { /* executable buffer requested */\n        void *const code_ptr = NULL != info->reloc ? ((void*)(((char*)info->reloc) + alignment)) : *memory;\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_X & flags));\n        if (name && *name) { /* profiler support requested */\n          if (0 > libxsmm_verbosity) { /* avoid dump when only the profiler is enabled */\n            FILE* code_file = fopen(name, \"rb\");\n            int diff = 0;\n            if (NULL == code_file) { /* file does not exist */\n              code_file = fopen(name, \"wb\");\n              if (NULL != code_file) { /* dump byte-code into a file */\n                fwrite(code_ptr, 1, size, code_file);\n                fclose(code_file);\n              }\n            }\n            else { /* check existing file */\n              const char* check_a = (const char*)code_ptr;\n              char check_b[4096];\n              size_t rest = size;\n              do {\n                const size_t n = fread(check_b, 1, LIBXSMM_MIN(sizeof(check_b), rest), code_file);\n                diff += memcmp(check_a, check_b, LIBXSMM_MIN(sizeof(check_b), n));\n                check_a += n;\n                rest -= n;\n              } while (0 < rest && 0 == diff);\n              fclose(code_file);\n            }\n            fprintf(stderr, \"LIBXSMM-JIT-DUMP(ptr:file) %p : %s\\n\", code_ptr, name);\n            if (0 != diff) { /* override existing dump and warn about erroneous condition */\n              fprintf(stderr, \"LIBXSMM ERROR: %s is shared by different code!\\n\", name);\n              code_file = fopen(name, \"wb\");\n              if (NULL != code_file) { /* dump byte-code into a file */\n                fwrite(code_ptr, 1, size, code_file);\n                fclose(code_file);\n              }\n            }\n          }\n#if defined(LIBXSMM_VTUNE)\n          if (iJIT_SAMPLING_ON == iJIT_IsProfilingActive()) {\n            LIBXSMM_VTUNE_JIT_DESC_TYPE vtune_jit_desc;\n            const unsigned int code_id = iJIT_GetNewMethodID();\n            internal_get_vtune_jitdesc(code_ptr, code_id, size, name, &vtune_jit_desc);\n            iJIT_NotifyEvent(LIBXSMM_VTUNE_JIT_LOAD, &vtune_jit_desc);\n            info->code_id = code_id;\n          }\n          else {\n            info->code_id = 0;\n          }\n#endif\n#if defined(LIBXSMM_PERF)\n          /* If JIT is enabled and a valid name is given, emit information for profiler\n           * In jitdump case this needs to be done after mprotect as it gets overwritten\n           * otherwise. */\n          libxsmm_perf_dump_code(code_ptr, size, name);\n#endif\n        }\n        if (NULL != info->reloc && info->pointer != info->reloc) {\n#if defined(_WIN32)\n          /* TODO: implement memory protection under Microsoft Windows */\n#else\n          /* memory is already protected at this point; relocate code */\n          LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n          *memory = code_ptr; /* relocate */\n          info->pointer = info->reloc;\n          info->reloc = NULL;\n# if !defined(LIBXSMM_MALLOC_CRC_OFF) /* update checksum */\n#   if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n          { const internal_malloc_info_type *const code_info = internal_malloc_info(code_ptr, 0/*no check*/);\n            info->hash = LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &code_info);\n          }\n#   else\n          info->hash = libxsmm_crc32(LIBXSMM_MALLOC_SEED, info,\n            /* info size minus actual hash value */\n            (unsigned int)(((char*)&info->hash) - ((char*)info)));\n#   endif\n# endif   /* treat memory protection errors as soft error; ignore return value */\n          munmap(buffer, alloc_size);\n#endif\n        }\n#if !defined(_WIN32)\n        else { /* malloc-based fall-back */\n          int mprotect_result;\n# if !defined(LIBXSMM_MALLOC_CRC_OFF) && defined(LIBXSMM_VTUNE) /* check checksum */\n#   if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n          assert(info->hash == LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &info)); /* !LIBXSMM_ASSERT */\n#   else\n          assert(info->hash == libxsmm_crc32(LIBXSMM_MALLOC_SEED, info, /* !LIBXSMM_ASSERT */\n            /* info size minus actual hash value */\n            (unsigned int)(((char*)&info->hash) - ((char*)info))));\n#   endif\n# endif   /* treat memory protection errors as soft error; ignore return value */\n          mprotect_result = mprotect(buffer, alloc_size/*entire memory region*/, PROT_READ | PROT_EXEC);\n          if (EXIT_SUCCESS != mprotect_result) {\n            if (0 != libxsmm_se) { /* hard-error in case of SELinux */\n              if (0 != libxsmm_verbosity /* library code is expected to be mute */\n                && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n              {\n                fprintf(stderr, \"LIBXSMM ERROR: failed to allocate an executable buffer!\\n\");\n              }\n              result = mprotect_result;\n            }\n            else if ((LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n              && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n            {\n              fprintf(stderr, \"LIBXSMM WARNING: read-only request for JIT-buffer failed!\\n\");\n            }\n          }\n        }\n#endif\n      }\n    }\n  }\n  else if (NULL == memory || NULL == *memory) {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: libxsmm_malloc_attrib failed because NULL cannot be attributed!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  else if ((LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: %s buffer %p does not match!\\n\",\n      0 != (LIBXSMM_MALLOC_FLAG_X & flags) ? \"executable\" : \"memory\", *memory);\n  }\n  return result;\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_MALLOC void* libxsmm_aligned_malloc(size_t size, size_t alignment)\n{\n  void* result = NULL;\n  LIBXSMM_INIT\n  if (2 > internal_malloc_kind) {\n#if !defined(NDEBUG)\n    int status =\n#endif\n    libxsmm_xmalloc(&result, size, alignment, LIBXSMM_MALLOC_FLAG_DEFAULT, NULL/*extra*/, 0/*extra_size*/);\n    assert(EXIT_SUCCESS == status || NULL == result); /* !LIBXSMM_ASSERT */\n  }\n  else { /* scratch */\n    const void *const caller = libxsmm_trace_caller_id(0/*level*/);\n    internal_scratch_malloc(&result, size, alignment, LIBXSMM_MALLOC_FLAG_DEFAULT, caller);\n  }\n  return result;\n}\n\n\nLIBXSMM_API void* libxsmm_realloc(size_t size, void* ptr)\n{\n  const int nzeros = LIBXSMM_INTRINSICS_BITSCANFWD64((uintptr_t)ptr), alignment = 1 << nzeros;\n  LIBXSMM_ASSERT(0 == ((uintptr_t)ptr & ~(0xFFFFFFFFFFFFFFFF << nzeros)));\n  LIBXSMM_INIT\n  if (2 > internal_malloc_kind) {\n#if !defined(NDEBUG)\n    int status =\n#endif\n    libxsmm_xmalloc(&ptr, size, alignment, LIBXSMM_MALLOC_FLAG_REALLOC, NULL/*extra*/, 0/*extra_size*/);\n    assert(EXIT_SUCCESS == status || NULL == ptr); /* !LIBXSMM_ASSERT */\n  }\n  else { /* scratch */\n    const void *const caller = libxsmm_trace_caller_id(0/*level*/);\n    internal_scratch_malloc(&ptr, size, alignment, LIBXSMM_MALLOC_FLAG_REALLOC, caller);\n  }\n  return ptr;\n}\n\n\nLIBXSMM_API void* libxsmm_scratch_malloc(size_t size, size_t alignment, const void* caller)\n{\n  void* result;\n  LIBXSMM_INIT\n  internal_scratch_malloc(&result, size, alignment,\n    LIBXSMM_MALLOC_INTERNAL_CALLER != caller ? LIBXSMM_MALLOC_FLAG_DEFAULT : LIBXSMM_MALLOC_FLAG_PRIVATE,\n    caller);\n  return result;\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_MALLOC void* libxsmm_malloc(size_t size)\n{\n  return libxsmm_aligned_malloc(size, 0/*auto*/);\n}\n\n\nLIBXSMM_API void libxsmm_free(const void* memory)\n{\n  if (NULL != memory) {\n#if defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST) || /* prefer safe method if possible */ \\\n  (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(memory);\n    if (NULL != pool) { /* memory belongs to scratch domain */\n      internal_scratch_free(memory, pool);\n    }\n    else\n# endif\n    { /* local */\n      libxsmm_xfree(memory, 2/*check*/);\n    }\n#else /* lookup matching pool */\n    internal_malloc_info_type *const info = internal_malloc_info(memory, 2/*check*/);\n    static int error_once = 0;\n    if (NULL != info && 0 == (LIBXSMM_MALLOC_FLAG_SCRATCH & info->flags)) { /* !libxsmm_free */\n# if !defined(NDEBUG)\n      if (EXIT_SUCCESS != internal_xfree(memory, info)\n        && 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n# else\n      internal_xfree(memory, info); /* !libxsmm_free */\n# endif\n    }\n    else {\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(memory);\n      if (NULL != pool) { /* memory belongs to scratch domain */\n        internal_scratch_free(memory, pool);\n      }\n      else\n# endif\n      {\n# if defined(NDEBUG) && (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n        __real_free((void*)memory);\n# else\n#   if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n        __real_free((void*)memory);\n#   endif\n        if (0 != libxsmm_verbosity && /* library code is expected to be mute */\n            1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: deallocation does not match allocation!\\n\");\n        }\n# endif\n      }\n    }\n#endif\n  }\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_xrelease_scratch(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock)\n{\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  internal_malloc_pool_type* pools = NULL;\n  libxsmm_scratch_info scratch_info;\n  LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  if (NULL != lock) {\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n# if defined(LIBXSMM_MALLOC_DELETE_SAFE)\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind)\n# endif\n  {\n    unsigned int i;\n    pools = (internal_malloc_pool_type*)LIBXSMM_UP2(\n      (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n    for (i = 0; i < libxsmm_scratch_pools; ++i) {\n      if (0 != pools[i].instance.minsize) {\n        if (\n# if !defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST)\n          1 < pools[i].instance.counter &&\n# endif\n          NULL != pools[i].instance.buffer)\n        {\n          internal_malloc_info_type* const info = internal_malloc_info(pools[i].instance.buffer, 2/*check*/);\n          if (NULL != info) internal_xfree(info->pointer, info);\n        }\n      }\n      else break; /* early exit */\n    }\n  }\n  LIBXSMM_EXPECT(EXIT_SUCCESS, libxsmm_get_scratch_info(&scratch_info));\n  if (0 != scratch_info.npending && /* library code is expected to be mute */\n    (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity))\n  {\n    char pending_size_buffer[32];\n    libxsmm_format_size(pending_size_buffer, sizeof(pending_size_buffer),\n      internal_malloc_public_cur + internal_malloc_local_cur, \"KM\", \"B\", 10);\n    fprintf(stderr, \"LIBXSMM WARNING: %s pending scratch-memory by %\" PRIuPTR \" allocation%s!\\n\",\n      pending_size_buffer, (uintptr_t)scratch_info.npending, 1 < scratch_info.npending ? \"s\" : \"\");\n  }\n  if (NULL != pools) {\n    memset(pools, 0, (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) * sizeof(internal_malloc_pool_type));\n    /* no reset: keep private watermark (internal_malloc_private_max, internal_malloc_private_cur) */\n    internal_malloc_public_max = internal_malloc_public_cur = 0;\n    internal_malloc_local_max = internal_malloc_local_cur = 0;\n    internal_malloc_scratch_nmallocs = 0;\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n#endif\n}\n\n\nLIBXSMM_API void libxsmm_release_scratch(void)\n{\n  libxsmm_xrelease_scratch(&libxsmm_lock_global);\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc_info(const void* memory, libxsmm_malloc_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != info) {\n    size_t size;\n    result = libxsmm_get_malloc_xinfo(memory, &size, NULL/*flags*/, NULL/*extra*/);\n    LIBXSMM_MEMZERO127(info);\n    if (EXIT_SUCCESS == result) {\n      info->size = size;\n    }\n#if !defined(NDEBUG) /* library code is expected to be mute */\n    else if (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) {\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM WARNING: foreign memory buffer %p discovered!\\n\", memory);\n      }\n    }\n#endif\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_scratch_info(libxsmm_scratch_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != info) {\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    LIBXSMM_MEMZERO127(info);\n    info->nmallocs = internal_malloc_scratch_nmallocs;\n    info->internal = internal_malloc_private_max;\n    info->local = internal_malloc_local_max;\n    info->size = internal_malloc_public_max;\n    { const internal_malloc_pool_type* pool = (const internal_malloc_pool_type*)LIBXSMM_UP2(\n        (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      const internal_malloc_pool_type *const end = pool + libxsmm_scratch_pools;\n      LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n      for (; pool != end; ++pool) if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n# endif\n        if (0 != pool->instance.minsize) {\n          const size_t npending = pool->instance.counter;\n# if defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST)\n          info->npending += npending;\n# else\n          info->npending += 1 < npending ? (npending - 1) : 0;\n# endif\n          ++info->npools;\n        }\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n        else break; /* early exit */\n      }\n# endif\n    }\n#else\n    LIBXSMM_MEMZERO127(info);\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_set_scratch_limit(size_t nbytes)\n{\n  /* !LIBXSMM_INIT */\n  internal_malloc_scratch_limit = nbytes;\n}\n\n\nLIBXSMM_API size_t libxsmm_get_scratch_limit(void)\n{\n  size_t result;\n  /* !LIBXSMM_INIT */\n  if (LIBXSMM_SCRATCH_DEFAULT != internal_malloc_scratch_limit) {\n    result = internal_malloc_scratch_limit;\n  }\n  else if (0 == internal_malloc_kind) {\n    result = LIBXSMM_MALLOC_SCRATCH_LIMIT;\n  }\n  else {\n    result = LIBXSMM_SCRATCH_UNLIMITED;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_set_malloc(int enabled, const size_t* lo, const size_t* hi)\n{\n  /* !LIBXSMM_INIT */\n#if !(defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) || defined(LIBXSMM_INTERCEPT_DYNAMIC))\n  LIBXSMM_UNUSED(enabled);\n  internal_malloc_kind = 0;\n#elif defined(LIBXSMM_MALLOC) && (0 < LIBXSMM_MALLOC)\n  LIBXSMM_UNUSED(enabled);\n  internal_malloc_kind = LIBXSMM_MALLOC;\n#else\n  internal_malloc_kind = enabled;\n#endif\n  /* setup lo/hi after internal_malloc_kind! */\n  if (NULL != lo) internal_malloc_limit[0] = *lo;\n  if (NULL != hi) {\n    const size_t scratch_limit = libxsmm_get_scratch_limit();\n    const size_t malloc_upper = LIBXSMM_MIN(*hi, scratch_limit);\n    internal_malloc_limit[1] = LIBXSMM_MAX(malloc_upper, internal_malloc_limit[0]);\n  }\n  libxsmm_malloc_init();\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc(size_t* lo, size_t* hi)\n{\n  int result;\n  LIBXSMM_INIT\n  if (NULL != lo) *lo = internal_malloc_limit[0];\n  if (NULL != hi) *hi = internal_malloc_limit[1];\n#if (defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) || defined(LIBXSMM_INTERCEPT_DYNAMIC))\n  result = 0 != (internal_malloc_kind & 1) && 0 < internal_malloc_kind;\n#else\n  result = 0;\n#endif\n  return result;\n}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm_mm.docx",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm_prof-vtune.png",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/tensorflow.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm_samples.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/tests/mhd_image.raw",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/samples/magazine/magazine.docx",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/samples/utilities/mhd/mhd_in.mhd",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/samples/deeplearning/gxm/model_zoo/cifar10/mean.binaryproto"
    ],
    "total_files": 875
}