{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/src/libxsmm_main.c": "/******************************************************************************\n** Copyright (c) 2014-2019, Intel Corporation                                **\n** All rights reserved.                                                      **\n**                                                                           **\n** Redistribution and use in source and binary forms, with or without        **\n** modification, are permitted provided that the following conditions        **\n** are met:                                                                  **\n** 1. Redistributions of source code must retain the above copyright         **\n**    notice, this list of conditions and the following disclaimer.          **\n** 2. Redistributions in binary form must reproduce the above copyright      **\n**    notice, this list of conditions and the following disclaimer in the    **\n**    documentation and/or other materials provided with the distribution.   **\n** 3. Neither the name of the copyright holder nor the names of its          **\n**    contributors may be used to endorse or promote products derived        **\n**    from this software without specific prior written permission.          **\n**                                                                           **\n** THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS       **\n** \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT         **\n** LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR     **\n** A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT      **\n** HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,    **\n** SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED  **\n** TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR    **\n** PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF    **\n** LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING      **\n** NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS        **\n** SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.              **\n******************************************************************************/\n/* Hans Pabst, Alexander Heinecke (Intel Corp.)\n******************************************************************************/\n#include \"libxsmm_trace.h\"\n#include \"libxsmm_xcopy.h\"\n#include \"libxsmm_gemm.h\"\n#include \"libxsmm_hash.h\"\n#include \"libxsmm_diff.h\"\n#include \"libxsmm_main.h\"\n#if defined(LIBXSMM_PERF)\n# include \"libxsmm_perf.h\"\n#endif\n#include \"generator_common.h\"\n#include <libxsmm_intrinsics_x86.h>\n\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n#endif\n/* mute warning about target attribute; KNC/native plus JIT is disabled below! */\n#include <stdlib.h>\n#include <string.h>\n#include <stdio.h>\n#if !defined(NDEBUG)\n# include <errno.h>\n#endif\n#if defined(_WIN32)\n# include <Windows.h>\n#else\n# if defined(LIBXSMM_INTERCEPT_DYNAMIC)\n#   include <dlfcn.h>\n# endif\n# include <sys/types.h>\n# include <sys/mman.h>\n# include <sys/stat.h>\n# include <unistd.h>\n# include <fcntl.h>\n#endif\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(pop)\n#endif\n\n#if !defined(LIBXSMM_CODE_MAXSIZE)\n# define LIBXSMM_CODE_MAXSIZE 131072\n#endif\n#if !defined(LIBXSMM_DIFF_SIZE)\n# define LIBXSMM_DIFF_SIZE LIBXSMM_DESCRIPTOR_SIGSIZE\n#endif\n#if !defined(LIBXSMM_HASH_SIZE)\n# define LIBXSMM_HASH_SIZE LIBXSMM_DESCRIPTOR_SIGSIZE\n#endif\n#if !defined(LIBXSMM_HASH_SEED)\n# define LIBXSMM_HASH_SEED 25071975\n#endif\n#if !defined(LIBXSMM_UNIFY_LOCKS)\n# define LIBXSMM_UNIFY_LOCKS\n#endif\n#if !defined(LIBXSMM_CACHE_PAD) && 1\n# define LIBXSMM_CACHE_PAD\n#endif\n#if !defined(LIBXSMM_CACHE_CLEAR) && 0\n# define LIBXSMM_CACHE_CLEAR\n#endif\n#if !defined(LIBXSMM_ENABLE_DEREG) && 0\n# define LIBXSMM_ENABLE_DEREG\n#endif\n#if !defined(LIBXSMM_REGLOCK_TRY) && 0\n# define LIBXSMM_REGLOCK_TRY\n#endif\n#if !defined(LIBXSMM_DIFF_INLINE) && 0\n# define LIBXSMM_DIFF_INLINE\n#endif\n#if !defined(LIBXSMM_DESC_INLINE) && 0\n# define LIBXSMM_DESC_INLINE\n#endif\n#if !defined(LIBXSMM_DESC_PAD) && 1\n# define LIBXSMM_DESC_PAD\n#endif\n\n/* flag fused into the memory address of a code version in case of non-JIT */\n#define LIBXSMM_CODE_STATIC (1ULL << (8 * sizeof(void*) - 1))\n/* flag fused into the memory address of a code version in case of collision */\n#if 1 /* beneficial when registry approaches capacity (collisions) */\n# define LIBXSMM_HASH_COLLISION (1ULL << (8 * sizeof(void*) - 2))\n#endif\n\n/** Helper macro determining the default prefetch strategy which is used for statically generated kernels. */\n#if (0 > LIBXSMM_PREFETCH) /* auto-prefetch (frontend) */ || (defined(_WIN32) || defined(__CYGWIN__))\n# define INTERNAL_PREFETCH LIBXSMM_GEMM_PREFETCH_NONE\n#else\n# define INTERNAL_PREFETCH ((libxsmm_gemm_prefetch_type)LIBXSMM_PREFETCH)\n#endif\n\n#if (0 != LIBXSMM_SYNC)\n# if !defined(INTERNAL_REGLOCK_MAXN)\n#   if defined(_MSC_VER)\n#     define INTERNAL_REGLOCK_MAXN 0\n#   else\n#     define INTERNAL_REGLOCK_MAXN 0\n#   endif\n# endif\n# if (1 < INTERNAL_REGLOCK_MAXN)\n#   if !defined(LIBXSMM_CACHE_MAXSIZE) && (8 > INTERNAL_REGLOCK_MAXN)\n#     define LIBXSMM_CACHE_MAXSIZE LIBXSMM_CAPACITY_CACHE\n#   endif\n#   if !defined(LIBXSMM_REGLOCK)\n#     define LIBXSMM_REGLOCK LIBXSMM_LOCK_DEFAULT\n#   endif\n#   if !defined(LIBXSMM_CLEANUP_NTRY)\n#     define LIBXSMM_CLEANUP_NTRY 7\n#   endif\n#   if LIBXSMM_LOCK_TYPE_ISPOD(LIBXSMM_REGLOCK)\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_reglocktype {\n  char pad[LIBXSMM_CACHELINE];\n  LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK) state;\n} internal_reglocktype;\n#   else\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_reglocktype {\n  LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK) state;\n} internal_reglocktype;\n#   endif\nLIBXSMM_APIVAR_ARRAY(internal_reglocktype internal_reglock, INTERNAL_REGLOCK_MAXN);\n# else /* RW-lock */\n#   if !defined(LIBXSMM_CACHE_MAXSIZE)\n#     define LIBXSMM_CACHE_MAXSIZE LIBXSMM_CAPACITY_CACHE\n#   endif\n#   if !defined(LIBXSMM_REGLOCK)\n#     if defined(LIBXSMM_UNIFY_LOCKS)\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK\n#     elif defined(_MSC_VER)\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK_MUTEX\n#     elif 0\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK_RWLOCK\n#     else\n#       define LIBXSMM_REGLOCK LIBXSMM_LOCK_DEFAULT\n#     endif\n#   endif\nLIBXSMM_APIVAR(LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK)* internal_reglock_ptr);\n# endif\n#elif !defined(LIBXSMM_CACHE_MAXSIZE)\n# define LIBXSMM_CACHE_MAXSIZE LIBXSMM_CAPACITY_CACHE\n#endif\n\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n# define INTERNAL_FIND_CODE_CACHE_GROW(RESULT_INDEX, CACHE_SIZE) \\\n    RESULT_INDEX = CACHE_SIZE; CACHE_SIZE = (unsigned char)(0 != (CACHE_SIZE) ? ((CACHE_SIZE) << 1) : 1)\n# define INTERNAL_FIND_CODE_CACHE_EVICT(RESULT_INDEX, CACHE_SIZE, CACHE_HIT) \\\n    RESULT_INDEX = (unsigned char)LIBXSMM_MOD2((CACHE_HIT) + ((CACHE_SIZE) - 1), CACHE_SIZE)\n#endif\n\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_statistic_type {\n  unsigned int ntry, ncol, njit, nsta;\n} internal_statistic_type;\n\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_cache_entry_type {\n  libxsmm_descriptor keys[LIBXSMM_CACHE_MAXSIZE];\n  libxsmm_code_pointer code[LIBXSMM_CACHE_MAXSIZE];\n# if !defined(LIBXSMM_NTHREADS_USE) || defined(LIBXSMM_CACHE_CLEAR)\n  unsigned int id; /* to invalidate */\n# endif\n  unsigned char size, hit;\n} internal_cache_entry_type;\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_cache_type {\n# if defined(LIBXSMM_CACHE_PAD)\n  char pad[LIBXSMM_UP2(sizeof(internal_cache_entry_type),LIBXSMM_CACHELINE)];\n# endif\n  internal_cache_entry_type entry;\n} internal_cache_type;\n\n# if defined(LIBXSMM_NTHREADS_USE)\nLIBXSMM_APIVAR(internal_cache_type* internal_cache_buffer);\n# endif\n#endif /*defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))*/\n\n\n/** Determines the try-lock property (1<N: disabled, N=1: enabled [N=0: disabled in case of RW-lock]). */\nLIBXSMM_APIVAR(int internal_reglock_count);\nLIBXSMM_APIVAR(size_t internal_registry_nbytes);\nLIBXSMM_APIVAR(libxsmm_descriptor* internal_registry_keys);\nLIBXSMM_APIVAR(libxsmm_code_pointer* internal_registry);\nLIBXSMM_APIVAR_ARRAY(internal_statistic_type internal_statistic[2/*DP/SP*/], 4/*sml/med/big/xxx*/);\nLIBXSMM_APIVAR(unsigned int internal_statistic_sml);\nLIBXSMM_APIVAR(unsigned int internal_statistic_med);\nLIBXSMM_APIVAR(unsigned int internal_statistic_mnk);\nLIBXSMM_APIVAR(unsigned int internal_statistic_num_gemv);\nLIBXSMM_APIVAR(unsigned int internal_statistic_num_mcopy);\nLIBXSMM_APIVAR(unsigned int internal_statistic_num_tcopy);\nLIBXSMM_APIVAR(unsigned int internal_statistic_num_trsm);\nLIBXSMM_APIVAR(unsigned int internal_statistic_num_trmm);\nLIBXSMM_APIVAR(int internal_gemm_auto_prefetch_locked);\nLIBXSMM_APIVAR(const char* internal_build_state);\n\n#if !defined(INTERNAL_DELIMS)\n# define INTERNAL_DELIMS \";,:\"\n#endif\n\n#if defined(_WIN32)\nLIBXSMM_APIVAR(HANDLE internal_singleton_handle);\n#else\nLIBXSMM_APIVAR_ARRAY(char internal_singleton_fname, 64);\nLIBXSMM_APIVAR(int internal_singleton_handle);\n#endif\n\n#if (0 == LIBXSMM_SYNC)\n# define INTERNAL_FIND_CODE_LOCK(LOCKINDEX, INDEX, DIFF, CODE) {\n# define INTERNAL_FIND_CODE_UNLOCK(LOCKINDEX) }\n#else\n# if defined(LIBXSMM_REGLOCK_TRY)\n#   define INTERNAL_REGLOCK_TRY(DIFF, CODE) \\\n    if (1 != internal_reglock_count) { /* (re-)try and get (meanwhile) generated code */ \\\n      LIBXSMM_ASSERT(0 != internal_registry); /* engine is not shut down */ \\\n      continue; \\\n    } \\\n    else { /* exit dispatch and let client fall back */ \\\n      DIFF = 0; CODE = 0; break; \\\n    }\n# else\n#   define INTERNAL_REGLOCK_TRY(DIFF, CODE) \\\n      LIBXSMM_ASSERT(0 != internal_registry); /* engine is not shut down */ \\\n      continue\n# endif\n# if (1 < INTERNAL_REGLOCK_MAXN)\n#   define INTERNAL_FIND_CODE_LOCK(LOCKINDEX, INDEX, DIFF, CODE) { \\\n      const unsigned int LOCKINDEX = (0 != internal_reglock_count ? LIBXSMM_MOD2(INDEX, internal_reglock_count) : 0); \\\n      if (LIBXSMM_LOCK_ACQUIRED(LIBXSMM_REGLOCK) != LIBXSMM_LOCK_TRYLOCK(LIBXSMM_REGLOCK, &internal_reglock[LOCKINDEX].state)) { \\\n        INTERNAL_REGLOCK_TRY(DIFF, CODE); \\\n      }\n#   define INTERNAL_FIND_CODE_UNLOCK(LOCKINDEX) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, &internal_reglock[LOCKINDEX].state); }\n# else /* RW-lock */\n#   define INTERNAL_FIND_CODE_LOCK(LOCKINDEX, INDEX, DIFF, CODE) { \\\n      if (LIBXSMM_LOCK_ACQUIRED(LIBXSMM_REGLOCK) != LIBXSMM_LOCK_TRYLOCK(LIBXSMM_REGLOCK, internal_reglock_ptr)) { \\\n        INTERNAL_REGLOCK_TRY(DIFF, CODE); \\\n      }\n#   define INTERNAL_FIND_CODE_UNLOCK(LOCKINDEX) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, internal_reglock_ptr); }\n# endif\n#endif\n\n\nLIBXSMM_API_INLINE void internal_update_mmstatistic(const libxsmm_gemm_descriptor* desc,\n  unsigned int ntry, unsigned int ncol, unsigned int njit, unsigned int nsta)\n{\n  LIBXSMM_ASSERT(NULL != desc);\n  if (1 < desc->m && 1 < desc->n) { /* only record matrix-matrix multiplication */\n    const unsigned long long kernel_size = LIBXSMM_MNK_SIZE(desc->m, desc->n, desc->k);\n    const int idx = (LIBXSMM_GEMM_PRECISION_F64 == LIBXSMM_GETENUM_OUT(desc->datatype) ? 0 : 1);\n    int bucket;\n    if (LIBXSMM_MNK_SIZE(internal_statistic_sml, internal_statistic_sml, internal_statistic_sml) >= kernel_size) {\n      bucket = 0;\n    }\n    else if (LIBXSMM_MNK_SIZE(internal_statistic_med, internal_statistic_med, internal_statistic_med) >= kernel_size) {\n      bucket = 1;\n    }\n    else if (LIBXSMM_MNK_SIZE(internal_statistic_mnk, internal_statistic_mnk, internal_statistic_mnk) >= kernel_size) {\n      bucket = 2;\n    }\n    else { /*huge*/\n      bucket = 3;\n    }\n    if (0 != ncol) LIBXSMM_ATOMIC_ADD_FETCH(&internal_statistic[idx][bucket].ncol, ncol, LIBXSMM_ATOMIC_RELAXED);\n    if (0 != ntry) LIBXSMM_ATOMIC_ADD_FETCH(&internal_statistic[idx][bucket].ntry, ntry, LIBXSMM_ATOMIC_RELAXED);\n    /* the following counters are not manipulated concurrently (no need for atomic increment) */\n    if (0 != njit) internal_statistic[idx][bucket].njit += njit;\n    if (0 != nsta) internal_statistic[idx][bucket].nsta += nsta;\n  }\n}\n\n\nLIBXSMM_API_INLINE unsigned int internal_print_number(unsigned int n, char default_unit, char* unit)\n{\n  unsigned int number = n;\n  LIBXSMM_ASSERT(0 != unit);\n  *unit = default_unit;\n  if ((1000000) <= n) {\n    number = (n + 500000) / 1000000;\n    *unit = 'm';\n  }\n  else if (9999 < n) {\n    number = (n + 500) / 1000;\n    *unit = 'k';\n  }\n  return number;\n}\n\n\nLIBXSMM_API_INLINE unsigned int internal_print_statistic(FILE* ostream,\n  const char* target_arch, int precision, unsigned int linebreaks, unsigned int indent)\n{\n  const internal_statistic_type statistic_sml = internal_statistic[precision][0/*SML*/];\n  const internal_statistic_type statistic_med = internal_statistic[precision][1/*MED*/];\n  const internal_statistic_type statistic_big = internal_statistic[precision][2/*BIG*/];\n  const internal_statistic_type statistic_xxx = internal_statistic[precision][3/*XXX*/];\n  int printed = 0;\n  LIBXSMM_ASSERT(0 != ostream && (0 <= precision && precision < 2));\n\n  if (/* omit to print anything if it is superfluous */\n    0 != statistic_sml.ntry || 0 != statistic_sml.njit || 0 != statistic_sml.nsta || 0 != statistic_sml.ncol ||\n    0 != statistic_med.ntry || 0 != statistic_med.njit || 0 != statistic_med.nsta || 0 != statistic_med.ncol ||\n    0 != statistic_big.ntry || 0 != statistic_big.njit || 0 != statistic_big.nsta || 0 != statistic_big.ncol ||\n    0 != statistic_xxx.ntry || 0 != statistic_xxx.njit || 0 != statistic_xxx.nsta || 0 != statistic_xxx.ncol)\n  {\n    char title[256], range[256], unit[4];\n    unsigned int counter[4];\n    {\n      unsigned int n;\n      if (NULL != target_arch && 0 != *target_arch) {\n        assert(strlen(target_arch) < sizeof(title)); /* !LIBXSMM_ASSERT */\n        for (n = 0; 0 != target_arch[n] /*avoid code-gen. issue with some clang versions: && n < sizeof(title)*/; ++n) {\n          const char c = target_arch[n];\n          title[n] = (char)(('a' <= c && c <= 'z') ? (c - 32) : c); /* toupper */\n        }\n        LIBXSMM_SNPRINTF(title + n, sizeof(title) - n, \"/%s\", 0 == precision ? \"DP\" : \"SP\");\n      }\n      else {\n        LIBXSMM_SNPRINTF(title, sizeof(title), \"%s\", 0 == precision ? \"DP\" : \"SP\");\n      }\n      for (n = 0; n < linebreaks; ++n) fprintf(ostream, \"\\n\");\n    }\n    fprintf(ostream, \"%*s%-8s %6s %6s %6s %6s\\n\", (int)indent, \"\", title, \"TRY\", \"JIT\", \"STA\", \"COL\");\n    LIBXSMM_SNPRINTF(range, sizeof(range), \"%u..%u\", 0u, internal_statistic_sml);\n    counter[0] = internal_print_number(statistic_sml.ntry, ' ', unit + 0);\n    counter[1] = internal_print_number(statistic_sml.njit, ' ', unit + 1);\n    counter[2] = internal_print_number(statistic_sml.nsta, ' ', unit + 2);\n    counter[3] = internal_print_number(statistic_sml.ncol, ' ', unit + 3);\n    fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n      counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    LIBXSMM_SNPRINTF(range, sizeof(range), \"%u..%u\", internal_statistic_sml + 1u, internal_statistic_med);\n    counter[0] = internal_print_number(statistic_med.ntry, ' ', unit + 0);\n    counter[1] = internal_print_number(statistic_med.njit, ' ', unit + 1);\n    counter[2] = internal_print_number(statistic_med.nsta, ' ', unit + 2);\n    counter[3] = internal_print_number(statistic_med.ncol, ' ', unit + 3);\n    fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n      counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    LIBXSMM_SNPRINTF(range, sizeof(range), \"%u..%u\", internal_statistic_med + 1u, internal_statistic_mnk);\n    counter[0] = internal_print_number(statistic_big.ntry, ' ', unit + 0);\n    counter[1] = internal_print_number(statistic_big.njit, ' ', unit + 1);\n    counter[2] = internal_print_number(statistic_big.nsta, ' ', unit + 2);\n    counter[3] = internal_print_number(statistic_big.ncol, ' ', unit + 3);\n    fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n      counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    if (0 != statistic_xxx.ntry || 0 != statistic_xxx.njit || 0 != statistic_xxx.nsta || 0 != statistic_xxx.ncol) {\n      LIBXSMM_SNPRINTF(range, sizeof(range), \"> %u\", internal_statistic_mnk);\n      counter[0] = internal_print_number(statistic_xxx.ntry, ' ', unit + 0);\n      counter[1] = internal_print_number(statistic_xxx.njit, ' ', unit + 1);\n      counter[2] = internal_print_number(statistic_xxx.nsta, ' ', unit + 2);\n      counter[3] = internal_print_number(statistic_xxx.ncol, ' ', unit + 3);\n      fprintf(ostream, \"%*s%8s %6u%c %5u%c %5u%c %5u%c\\n\", (int)indent, \"\", range,\n        counter[0], unit[0], counter[1], unit[1], counter[2], unit[2], counter[3], unit[3]);\n    }\n    printed = 1;\n  }\n\n  return printed;\n}\n\n\n#if !(defined(_WIN32) || defined(__CYGWIN__))\nLIBXSMM_API_INLINE unsigned int internal_statistic_ntry(int precision)\n{\n  return internal_statistic[precision][0/*SML*/].ntry + internal_statistic[precision][1/*MED*/].ntry\n       + internal_statistic[precision][2/*BIG*/].ntry + internal_statistic[precision][3/*XXX*/].ntry;\n}\n#endif\n\n\n#if !defined(_WIN32)\nLIBXSMM_API_INLINE void internal_register_static_code(\n  libxsmm_gemm_precision precision, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  libxsmm_xmmfunction xgemm, libxsmm_code_pointer* registry)\n{\n  const libxsmm_blasint lda = m, ldb = k, ldc = m;\n  /*const*/ int precondition = LIBXSMM_GEMM_NO_BYPASS_DIMS(m, n, k) && LIBXSMM_GEMM_NO_BYPASS_DIMS(lda, ldb, ldc);\n  if (precondition) {\n    const size_t size = (LIBXSMM_HASH_SIZE)-sizeof(libxsmm_descriptor_kind);\n    libxsmm_descriptor_blob blob;\n    const libxsmm_gemm_descriptor *const desc = libxsmm_gemm_descriptor_dinit(&blob, precision,\n      m, n, k, lda, ldb, ldc, LIBXSMM_ALPHA, LIBXSMM_BETA, LIBXSMM_FLAGS, INTERNAL_PREFETCH);\n    unsigned int i = LIBXSMM_MOD2(\n      libxsmm_crc32(LIBXSMM_HASH_SEED, desc, LIBXSMM_MIN(sizeof(libxsmm_gemm_descriptor), size)),\n      LIBXSMM_CAPACITY_REGISTRY);\n    libxsmm_code_pointer* dst_entry = registry + i;\n#if !defined(NDEBUG)\n    libxsmm_code_pointer code; code.xgemm = xgemm;\n    LIBXSMM_ASSERT(NULL != code.ptr_const && NULL != registry);\n    LIBXSMM_ASSERT(0 == (LIBXSMM_CODE_STATIC & code.uval));\n#endif\n    if (NULL != dst_entry->ptr_const) { /* collision */\n      const unsigned int i0 = i;\n      do { /* continue to linearly search for an available slot */\n        i = LIBXSMM_MOD2(i + 1, LIBXSMM_CAPACITY_REGISTRY);\n        if (NULL == registry[i].ptr_const) break;\n      } while (i != i0);\n#if defined(LIBXSMM_HASH_COLLISION) /* mark entry as a collision */\n      dst_entry->uval |= LIBXSMM_HASH_COLLISION;\n#endif\n      dst_entry = registry + i; /* update destination */\n      internal_update_mmstatistic(desc, 0, 1/*collision*/, 0, 0);\n      /* out of capacity (no registry slot available) */\n      LIBXSMM_ASSERT(NULL == dst_entry->ptr_const || i == i0);\n    }\n    if (NULL == dst_entry->ptr_const) { /* registry not exhausted */\n      internal_registry_keys[i].kind = LIBXSMM_KERNEL_KIND_MATMUL;\n      LIBXSMM_ASSIGN127(&internal_registry_keys[i].gemm.desc, desc);\n      dst_entry->xgemm = xgemm;\n      /* mark current entry as static code (non-JIT) */\n      dst_entry->uval |= LIBXSMM_CODE_STATIC;\n    }\n    internal_update_mmstatistic(desc, 1/*try*/, 0, 0, 0);\n  }\n}\n#endif\n\n\nLIBXSMM_API_INTERN void internal_release_scratch(void);\nLIBXSMM_API_INTERN void internal_release_scratch(void)\n{\n  libxsmm_xrelease_scratch(NULL/*lock*/);\n  /* release global services */\n  libxsmm_hash_finalize();\n  libxsmm_malloc_finalize();\n}\n\n\nLIBXSMM_API_INTERN const char* libxsmm_format_size(size_t nbytes, const char scale[], const char* unit, int base)\n{\n  static LIBXSMM_TLS char formatted_size[32];\n  const int len = (NULL != scale ? ((int)strlen(scale)) : 0);\n  const int m = LIBXSMM_INTRINSICS_BITSCANBWD64(nbytes) / base, n = LIBXSMM_MIN(m, len);\n  int i;\n  formatted_size[0] = 0; /* clear */\n  LIBXSMM_ASSERT(NULL != unit && 0 <= base);\n  for (i = 0; i < n; ++i) nbytes >>= base;\n  LIBXSMM_SNPRINTF(formatted_size, sizeof(formatted_size), \"%i %c%s\",\n    (int)nbytes, 0 < n ? scale[n-1] : *unit, 0 < n ? unit : \"\");\n  return formatted_size;\n}\n\n\nLIBXSMM_API_INTERN void internal_finalize(void);\nLIBXSMM_API_INTERN void internal_finalize(void)\n{\n  char *const env_dump_build = getenv(\"LIBXSMM_DUMP_BUILD\");\n  char *const env_dump_files = (NULL != getenv(\"LIBXSMM_DUMP_FILES\") ? getenv(\"LIBXSMM_DUMP_FILES\") : getenv(\"LIBXSMM_DUMP_FILE\"));\n  libxsmm_finalize();\n  LIBXSMM_STDIO_ACQUIRE(); /* synchronize I/O */\n  if (0 != libxsmm_verbosity) { /* print statistic on termination */\n    const char *const env_target_hidden = getenv(\"LIBXSMM_TARGET_HIDDEN\");\n    const char *const target_arch = (NULL == env_target_hidden || 0 == atoi(env_target_hidden))\n      ? libxsmm_cpuid_name(libxsmm_target_archid) : NULL/*hidden*/;\n    fprintf(stderr, \"\\nLIBXSMM_VERSION: %s-%s (%i)\", LIBXSMM_BRANCH, LIBXSMM_VERSION, LIBXSMM_VERSION4(\n      LIBXSMM_VERSION_MAJOR, LIBXSMM_VERSION_MINOR, LIBXSMM_VERSION_UPDATE, LIBXSMM_VERSION_PATCH));\n    if (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) {\n      const int high_verbosity = (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity);\n      libxsmm_scratch_info scratch_info; size_t size_scratch = 0, size_private = 0;\n      unsigned int linebreak = (0 == internal_print_statistic(stderr, target_arch, 1/*SP*/, 1, 0)) ? 1 : 0;\n      if (0 == internal_print_statistic(stderr, target_arch, 0/*DP*/, linebreak, 0) && 0 != linebreak && NULL != target_arch) {\n        if (0 == libxsmm_se) {\n          fprintf(stderr, \"\\nLIBXSMM_TARGET: %s\\n\", target_arch);\n        }\n        else {\n          fprintf(stderr, \"\\nLIBXSMM_TARGET: %s*\\n\", target_arch);\n        }\n      }\n      if (EXIT_SUCCESS == libxsmm_get_scratch_info(&scratch_info)) {\n        size_private = scratch_info.internal;\n        size_scratch = scratch_info.size;\n      }\n      fprintf(stderr, \"Memory: %s\", libxsmm_format_size(internal_registry_nbytes + size_private, \"KM\", \"B\", 10));\n      if (0 != high_verbosity) {\n        size_t ngemms = 0;\n        int i; for (i = 0; i < 4; ++i) {\n          ngemms += (size_t)internal_statistic[0/*DP*/][i].nsta + internal_statistic[1/*SP*/][i].nsta;\n          ngemms += (size_t)internal_statistic[0/*DP*/][i].njit + internal_statistic[1/*SP*/][i].njit;\n        }\n        if (0 != ngemms || 0 != internal_statistic_num_gemv\n          || 0 != internal_statistic_num_mcopy || 0 != internal_statistic_num_tcopy\n          || 0 != libxsmm_statistic_num_spmdm)\n        {\n          const char sep[] = \" \", *s = \"\";\n          fprintf(stderr, \" (\");\n          if (0 != ngemms) { fprintf(stderr, \"gemm=%lu\", (unsigned long int)ngemms); s = sep; }\n          if (0 != internal_statistic_num_gemv) { fprintf(stderr, \"%sgemv=%u\", s, internal_statistic_num_gemv); s = sep; }\n          if (0 != internal_statistic_num_mcopy) { fprintf(stderr, \"%smcopy=%u\", s, internal_statistic_num_mcopy); s = sep; }\n          if (0 != internal_statistic_num_tcopy) { fprintf(stderr, \"%stcopy=%u\", s, internal_statistic_num_tcopy); s = sep; }\n          if (0 != libxsmm_statistic_num_spmdm) { fprintf(stderr, \"%sspmdm=%u\", s, libxsmm_statistic_num_spmdm); s = sep; }\n          fprintf(stderr, \")\");\n        }\n      }\n      fprintf(stderr, \"\\n\");\n      if (0 != size_scratch) {\n        fprintf(stderr, \"Scratch: %s\", libxsmm_format_size(size_scratch, \"KM\", \"B\", 10));\n        if (0 != high_verbosity) {\n          fprintf(stderr, \" (mallocs=%lu, pools=%u)\\n\", (unsigned long int)scratch_info.nmallocs, scratch_info.npools);\n        }\n        else {\n          fprintf(stderr, \"\\n\");\n        }\n      }\n    }\n    else {\n      fprintf(stderr, \"\\nLIBXSMM_TARGET: %s\\n\", target_arch);\n    }\n  }\n  /* release scratch memory pool */\n  if (EXIT_SUCCESS != atexit(internal_release_scratch) && 0 != libxsmm_verbosity) {\n    fprintf(stderr, \"LIBXSMM ERROR: failed to perform final cleanup!\\n\");\n  }\n#if defined(_WIN32)\n  if (NULL != internal_singleton_handle)\n#else\n  if (0 <= internal_singleton_handle && 0 != *internal_singleton_fname)\n#endif\n  { /* dump per-node info */\n    if (NULL != env_dump_build || NULL != env_dump_files) {\n      if (NULL != env_dump_files && 0 != *env_dump_files) {\n        const char *filename = strtok(env_dump_files, INTERNAL_DELIMS);\n        for (; NULL != filename; filename = strtok(NULL, INTERNAL_DELIMS)) {\n          FILE *const file = fopen(filename, \"r\");\n          if (NULL != file) {\n            int c = fgetc(file);\n            fprintf(stdout, \"\\n\\nLIBXSMM_DUMP_FILE: %s\\n\", filename);\n            while (EOF != c) {\n              fputc(c, stdout);\n              c = fgetc(file);\n            }\n            fputc('\\n', stdout);\n            fclose(file);\n          }\n        }\n      }\n      if (NULL != env_dump_build && 0 != *env_dump_build && '0' != *env_dump_build) {\n        fprintf(stdout, \"\\n\\nBUILD_DATE=%i\\n\", LIBXSMM_CONFIG_BUILD_DATE);\n        if (NULL != internal_build_state) {\n          fprintf(stdout, \"%s\\n\", internal_build_state);\n        }\n      }\n    }\n    /* cleanup singleton */\n#if defined(_WIN32)\n    ReleaseMutex(internal_singleton_handle);\n    CloseHandle(internal_singleton_handle);\n#else\n    unlink(internal_singleton_fname);\n    close(internal_singleton_handle);\n#endif\n  }\n  LIBXSMM_STDIO_RELEASE(); /* synchronize I/O */\n#if (0 != LIBXSMM_SYNC)\n  { /* release locks */\n# if (1 < INTERNAL_REGLOCK_MAXN)\n    int i; for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_DESTROY(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n    LIBXSMM_LOCK_DESTROY(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n    LIBXSMM_LOCK_DESTROY(LIBXSMM_LOCK, &libxsmm_lock_global);\n  }\n#endif\n}\n\n\n#if defined(LIBXSMM_INTERCEPT_DYNAMIC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void _gfortran_stop_string(const char* /*message*/, int /*len*/, int /*quiet*/);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void _gfortran_stop_string(const char* message, int len, int quiet)\n{ /* STOP termination handler for GNU Fortran runtime */\n  static int once = 0;\n  if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n    union { const void* dlsym; void (*ptr)(const char*, int, int); } stop;\n    dlerror(); /* clear an eventual error status */\n    stop.dlsym = dlsym(RTLD_NEXT, \"_gfortran_stop_string\");\n    if (NULL != stop.dlsym) {\n      stop.ptr(message, len, quiet);\n    }\n    else exit(EXIT_SUCCESS); /* statically linked runtime */\n  }\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core(const char* /*message*/, int /*len*/);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core(const char* message, int len)\n{ /* STOP termination handler for Intel Fortran runtime */\n  static int once = 0;\n  if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n    union { const void* dlsym; void (*ptr)(const char*, int); } stop;\n    dlerror(); /* clear an eventual error status */\n    stop.dlsym = dlsym(RTLD_NEXT, \"for_stop_core\");\n    if (NULL != stop.dlsym) {\n      stop.ptr(message, len);\n    }\n    else exit(EXIT_SUCCESS); /* statically linked runtime */\n  }\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core_quiet(void);\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void for_stop_core_quiet(void)\n{ /* STOP termination handler for Intel Fortran runtime */\n  static int once = 0;\n  if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n    union { const void* dlsym; void (*ptr)(void); } stop;\n    dlerror(); /* clear an eventual error status */\n    stop.dlsym = dlsym(RTLD_NEXT, \"for_stop_core_quiet\");\n    if (NULL != stop.dlsym) {\n      stop.ptr();\n    }\n    else exit(EXIT_SUCCESS); /* statically linked runtime */\n  }\n}\n#endif\n\n\nLIBXSMM_API_INTERN size_t internal_strlen(const char* /*cstr*/, size_t /*maxlen*/);\nLIBXSMM_API_INTERN size_t internal_strlen(const char* cstr, size_t maxlen)\n{\n  size_t result = 0;\n  if (NULL != cstr) {\n    while (0 != cstr[result] && result < maxlen) ++result;\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN size_t internal_parse_nbytes(const char* /*nbytes*/, size_t /*ndefault*/);\nLIBXSMM_API_INTERN size_t internal_parse_nbytes(const char* nbytes, size_t ndefault)\n{\n  size_t result = ndefault;\n  if (NULL != nbytes && 0 != *nbytes) {\n    size_t u = internal_strlen(nbytes, 32) - 1;\n    const char unit[] = \"kmgKMG\", *const hit = strchr(unit, nbytes[u]);\n    const long long int ibytes = atol(nbytes); /* take with increased type-width */\n    result = (size_t)ibytes;\n    if ((size_t)LIBXSMM_UNLIMITED != result) {\n      u = (0 != hit ? ((hit - unit) % 3) : 3);\n      if (u < 3) {\n        result <<= (u + 1) * 10;\n      }\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN\n#if defined(__GNUC__)\nLIBXSMM_ATTRIBUTE(no_instrument_function)\n#endif\nvoid internal_init(void);\n\nLIBXSMM_API_INTERN void internal_init(void)\n{\n  int i;\n#if (0 != LIBXSMM_SYNC) /* setup the locks in a thread-safe fashion */\n  LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, &libxsmm_lock_global);\n# if (1 < INTERNAL_REGLOCK_MAXN)\n  for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_ACQUIRE(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n  LIBXSMM_LOCK_ACQUIRE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n#endif\n  if (NULL == internal_registry) { /* double-check after acquiring the lock(s) */\n    void *new_registry = NULL, *new_keys = &internal_registry_keys;\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n    void* new_cache = &internal_cache_buffer;\n#endif\n    /* setup verbosity as early as possible since below code may rely on verbose output */\n    const char *const env_verbose = getenv(\"LIBXSMM_VERBOSE\");\n    if (NULL != env_verbose && 0 != *env_verbose) {\n      libxsmm_verbosity = atoi(env_verbose);\n    }\n#if !defined(NDEBUG)\n    else {\n      libxsmm_verbosity = INT_MAX; /* quiet -> verbose */\n    }\n#endif\n    LIBXSMM_ASSERT(NULL == internal_registry_keys); /* should never happen */\n#if !defined(_WIN32) && 0\n    umask(S_IRUSR | S_IWUSR); /* setup default/secure file mask */\n#endif\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    { const char *const env = getenv(\"LIBXSMM_SCRATCH_POOLS\");\n      if (NULL == env || 0 == *env) {\n        libxsmm_scratch_pools = LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS;\n      }\n      else {\n        libxsmm_scratch_pools = LIBXSMM_CLMP(atoi(env), 0, LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n        /*libxsmm_scratch_pools_locked = 1;*/\n      }\n      LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n    }\n    { const char *const env = getenv(\"LIBXSMM_SCRATCH_SCALE\");\n      if (NULL == env || 0 == *env) {\n        libxsmm_scratch_scale = LIBXSMM_MALLOC_SCRATCH_SCALE;\n      }\n      else {\n        libxsmm_scratch_scale = LIBXSMM_CLMP(atof(env), 1.0, 10.0);\n        /*libxsmm_scratch_scale_locked = 1;*/\n      }\n      LIBXSMM_ASSERT(1 <= libxsmm_scratch_scale);\n    }\n    libxsmm_set_scratch_limit(internal_parse_nbytes(getenv(\"LIBXSMM_SCRATCH_LIMIT\"), LIBXSMM_SCRATCH_DEFAULT));\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n    { /* setup malloc-interception after internal allocations */\n      const libxsmm_malloc_function null_malloc_fn = { 0 };\n      const libxsmm_free_function null_free_fn = { 0 };\n      const char *const env_k = getenv(\"LIBXSMM_MALLOC\");\n      char *const env_t = getenv(\"LIBXSMM_MALLOC_LIMIT\");\n      const char* env_i = (NULL != env_t ? strtok(env_t, INTERNAL_DELIMS) : NULL);\n      const size_t malloc_lo = internal_parse_nbytes(env_i, LIBXSMM_MALLOC_LIMIT);\n      const size_t malloc_hi = (NULL != env_i ? internal_parse_nbytes(\n        strtok(NULL, INTERNAL_DELIMS), LIBXSMM_SCRATCH_UNLIMITED) : LIBXSMM_SCRATCH_UNLIMITED);\n      const int malloc_kind = ((NULL == env_k || 0 == *env_k) ? 0/*disabled*/ : atoi(env_k));\n      libxsmm_xset_default_allocator(NULL/*lock*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n      libxsmm_xset_scratch_allocator(NULL/*lock*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n      libxsmm_set_malloc(malloc_kind, &malloc_lo, &malloc_hi); /* implies libxsmm_malloc_init */\n    }\n#if defined(LIBXSMM_MAXTARGET)\n    libxsmm_set_target_arch(LIBXSMM_STRINGIFY(LIBXSMM_MAXTARGET));\n#else /* attempt to set libxsmm_target_archid per environment variable */\n    libxsmm_set_target_arch(getenv(\"LIBXSMM_TARGET\"));\n#endif\n    { const char *const env = getenv(\"LIBXSMM_SYNC\");\n      libxsmm_nosync = (NULL == env || 0 == *env) ? 0/*default*/ : atoi(env);\n    }\n    /* clear internal counters/statistic */\n    for (i = 0; i < 4/*sml/med/big/xxx*/; ++i) {\n      LIBXSMM_MEMZERO127(&internal_statistic[0/*DP*/][i]);\n      LIBXSMM_MEMZERO127(&internal_statistic[1/*SP*/][i]);\n    }\n    internal_statistic_mnk = LIBXSMM_MAX_DIM;\n    internal_statistic_sml = 13;\n    internal_statistic_med = 23;\n#if !defined(NDEBUG) /* LIBXSMM_CAPACITY_REGISTRY: power of two */\n    { const unsigned int npot = LIBXSMM_UP2POT(LIBXSMM_CAPACITY_REGISTRY);\n      assert(LIBXSMM_CAPACITY_REGISTRY == npot); /* !LIBXSMM_ASSERT */\n    }\n#endif\n    libxsmm_hash_init(libxsmm_target_archid); /* used by debug memory allocation (checksum) */\n    if (\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      (EXIT_SUCCESS == libxsmm_xmalloc((void**)new_cache, (LIBXSMM_NTHREADS_MAX) * sizeof(internal_cache_type), LIBXSMM_CACHELINE/*alignment*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, NULL/*extra*/, 0/*extra-size*/)) &&\n#endif\n      (EXIT_SUCCESS == libxsmm_xmalloc(&new_registry, (LIBXSMM_CAPACITY_REGISTRY) * sizeof(libxsmm_code_pointer), 0/*auto-align*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, NULL/*extra*/, 0/*extra-size*/)) &&\n      (EXIT_SUCCESS == libxsmm_xmalloc((void**)new_keys, (LIBXSMM_CAPACITY_REGISTRY) * sizeof(libxsmm_descriptor), 0/*auto-align*/,\n        LIBXSMM_MALLOC_FLAG_PRIVATE, NULL/*extra*/, 0/*extra-size*/)))\n    {\n      LIBXSMM_ASSERT(NULL != new_registry && NULL != internal_registry_keys);\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      LIBXSMM_ASSERT(NULL != internal_cache_buffer);\n      memset(internal_cache_buffer, 0, (LIBXSMM_NTHREADS_MAX) * sizeof(internal_cache_type));\n#endif\n      libxsmm_xcopy_init(libxsmm_target_archid);\n      libxsmm_dnn_init(libxsmm_target_archid);\n#if defined(LIBXSMM_PERF)\n      libxsmm_perf_init();\n#endif\n      { const char *const env = getenv(\"LIBXSMM_GEMM_PREFETCH\");\n#if defined(_WIN32) || defined(__CYGWIN__) /* TODO: full support for Windows calling convention */\n        libxsmm_gemm_auto_prefetch_default = INTERNAL_PREFETCH;\n#else\n        libxsmm_gemm_auto_prefetch_default = (0 == internal_statistic_ntry(0/*DP*/) && 0 == internal_statistic_ntry(1/*SP*/))\n          /* avoid special prefetch if static code is present, since such code uses INTERNAL_PREFETCH */\n          ? (((LIBXSMM_X86_AVX512 >= libxsmm_target_archid || LIBXSMM_X86_AVX512_CORE <= libxsmm_target_archid))\n            ? LIBXSMM_GEMM_PREFETCH_AL2BL2_VIA_C : LIBXSMM_GEMM_PREFETCH_BL2_VIA_C)\n          : INTERNAL_PREFETCH;\n#endif\n        libxsmm_gemm_auto_prefetch = INTERNAL_PREFETCH;\n        if (NULL != env && 0 != *env) { /* user input beyond auto-prefetch is always considered */\n          const int uid = atoi(env);\n          if (0 <= uid) {\n            libxsmm_gemm_auto_prefetch_default = libxsmm_gemm_uid2prefetch(uid);\n            libxsmm_gemm_auto_prefetch = libxsmm_gemm_auto_prefetch_default;\n            internal_gemm_auto_prefetch_locked = 1;\n          }\n        }\n      }\n      for (i = 0; i < (LIBXSMM_CAPACITY_REGISTRY); ++i) ((libxsmm_code_pointer*)new_registry)[i].pmm = NULL;\n#if defined(LIBXSMM_BUILD)\n#     include <libxsmm_dispatch.h>\n#endif\n      libxsmm_gemm_init(libxsmm_target_archid);\n#if defined(LIBXSMM_TRACE)\n      { int filter_threadid = 0/*only main-thread*/, filter_mindepth = 0, filter_maxnsyms = 0;\n        const int init_code = libxsmm_trace_init(filter_threadid, filter_mindepth, filter_maxnsyms);\n        if (EXIT_SUCCESS != init_code && 0 != libxsmm_verbosity) { /* library code is expected to be mute */\n          fprintf(stderr, \"LIBXSMM ERROR: failed to initialize TRACE (error #%i)!\\n\", init_code);\n        }\n      }\n#endif\n      { /* commit the registry buffer and enable global visibility */\n        void *const pv_registry = &internal_registry;\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)((void**)pv_registry, (void*)new_registry, LIBXSMM_ATOMIC_SEQ_CST);\n      }\n    }\n    else {\n      if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n        fprintf(stderr, \"LIBXSMM ERROR: failed to allocate internal buffers!\\n\");\n      }\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      libxsmm_xfree(internal_cache_buffer, 0/*no check*/);\n#endif\n      libxsmm_xfree(internal_registry_keys, 0/*no check*/);\n      libxsmm_xfree(new_registry, 0/*no check*/);\n    }\n  }\n#if (0 != LIBXSMM_SYNC) /* release locks */\n# if (1 < INTERNAL_REGLOCK_MAXN)\n  for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n  LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n  LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, &libxsmm_lock_global);\n#endif\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_CTOR void libxsmm_init(void)\n{\n  if (0 == LIBXSMM_ATOMIC_LOAD(&internal_registry, LIBXSMM_ATOMIC_RELAXED)) {\n    /* libxsmm_ninit (1: started, 2: library initialized), invalidate code-TLS */\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&libxsmm_ninit, 1, LIBXSMM_ATOMIC_SEQ_CST)) {\n#if (0 != LIBXSMM_SYNC)\n# if defined(LIBXSMM_REGLOCK_TRY)\n      const char *const env_trylock = getenv(\"LIBXSMM_TRYLOCK\");\n# endif\n      LIBXSMM_LOCK_ATTR_TYPE(LIBXSMM_LOCK) attr_global;\n# if (1 < INTERNAL_REGLOCK_MAXN)\n      int i;\n      LIBXSMM_LOCK_ATTR_TYPE(LIBXSMM_REGLOCK) attr;\n      LIBXSMM_LOCK_ATTR_INIT(LIBXSMM_REGLOCK, &attr);\n# elif defined(LIBXSMM_UNIFY_LOCKS)\n      internal_reglock_ptr = &libxsmm_lock_global;\n# else\n      static LIBXSMM_LOCK_TYPE(LIBXSMM_REGLOCK) internal_reglock;\n      internal_reglock_ptr = &internal_reglock;\n      LIBXSMM_LOCK_ATTR_TYPE(LIBXSMM_REGLOCK) attr;\n      LIBXSMM_LOCK_ATTR_INIT(LIBXSMM_REGLOCK, &attr);\n      LIBXSMM_LOCK_INIT(LIBXSMM_REGLOCK, internal_reglock_ptr, &attr);\n      LIBXSMM_LOCK_ATTR_DESTROY(LIBXSMM_REGLOCK, &attr);\n# endif\n      LIBXSMM_LOCK_ATTR_INIT(LIBXSMM_LOCK, &attr_global);\n      LIBXSMM_LOCK_INIT(LIBXSMM_LOCK, &libxsmm_lock_global, &attr_global);\n      LIBXSMM_LOCK_ATTR_DESTROY(LIBXSMM_LOCK, &attr_global);\n      /* control number of locks needed; LIBXSMM_TRYLOCK implies only 1 lock */\n# if defined(LIBXSMM_REGLOCK_TRY)\n      if (NULL == env_trylock || 0 == *env_trylock)\n# endif\n      { /* no LIBXSMM_TRYLOCK */\n# if defined(LIBXSMM_VTUNE)\n        internal_reglock_count = 1; /* avoid duplicated kernels */\n# elif (1 < INTERNAL_REGLOCK_MAXN)\n        const char *const env_nlocks = getenv(\"LIBXSMM_NLOCKS\");\n        const int reglock_count = (NULL == env_nlocks || 0 == *env_nlocks || 1 > atoi(env_nlocks))\n          ? (INTERNAL_REGLOCK_MAXN) : LIBXSMM_MIN(atoi(env_nlocks), INTERNAL_REGLOCK_MAXN);\n        internal_reglock_count = LIBXSMM_LO2POT(reglock_count);\n# else\n        internal_reglock_count = 0;\n# endif\n      }\n# if defined(LIBXSMM_REGLOCK_TRY)\n      else { /* LIBXSMM_TRYLOCK environment variable specified */\n        internal_reglock_count = (0 != atoi(env_trylock) ? 1\n#   if (1 < INTERNAL_REGLOCK_MAXN)\n          : INTERNAL_REGLOCK_MAXN);\n#   else\n          : 0);\n#   endif\n      }\n# endif\n# if (1 < INTERNAL_REGLOCK_MAXN)\n      LIBXSMM_ASSERT(1 <= internal_reglock_count);\n      for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_INIT(LIBXSMM_REGLOCK, &internal_reglock[i].state, &attr);\n      LIBXSMM_LOCK_ATTR_DESTROY(LIBXSMM_REGLOCK, &attr);\n# endif\n#endif\n      { /* determine whether this instance is unique or not */\n#if defined(_WIN32)\n        internal_singleton_handle = CreateMutex(NULL, TRUE, \"GlobalLIBXSMM\");\n#else\n        const int result = LIBXSMM_SNPRINTF(internal_singleton_fname, sizeof(internal_singleton_fname), \"/tmp/.libxsmm.%u\",\n          /*rely on user id to avoid permission issues in case of left-over files*/(unsigned int)getuid());\n        struct flock singleton_flock;\n        int singleton_handle;\n        singleton_flock.l_start = 0;\n        singleton_flock.l_len = 0; /* entire file */\n        singleton_flock.l_type = F_WRLCK; /* exclusive across PIDs */\n        singleton_flock.l_whence = SEEK_SET;\n        singleton_handle = ((0 < result && (int)sizeof(internal_singleton_fname) > result) ? open(\n          internal_singleton_fname, O_WRONLY | O_CREAT, S_IRUSR | S_IWUSR) : -1);\n        internal_singleton_handle = fcntl(singleton_handle, F_SETLK, &singleton_flock);\n        if (0 > internal_singleton_handle && 0 <= singleton_handle) close(singleton_handle);\n#endif\n      }\n      { /* calibrate timer */\n        libxsmm_timer_tickint s0, t0, s1, t1;\n        libxsmm_timer_tick_rtc(); libxsmm_timer_tick(); /* warm-up */\n        s0 = libxsmm_timer_tick_rtc(); t0 = libxsmm_timer_tick(); /* start timing */\n        internal_init();\n        if (EXIT_SUCCESS != atexit(internal_finalize) && 0 != libxsmm_verbosity) {\n          fprintf(stderr, \"LIBXSMM ERROR: failed to perform final cleanup!\\n\");\n        }\n        s1 = libxsmm_timer_tick_rtc(); t1 = libxsmm_timer_tick(); /* final timing */\n        if (LIBXSMM_FEQ(0, libxsmm_timer_scale) && t0 != t1) {\n          const libxsmm_timer_tickint dt = LIBXSMM_DELTA(t0, t1);\n          libxsmm_timer_scale = libxsmm_timer_duration(s0, s1) / dt;\n          if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n            const libxsmm_timer_tickint ds = LIBXSMM_DELTA(s0, s1);\n            if (ds > LIBXSMM_DELTA(ds, dt)) { /* no LIBXSMM_TIMER_RDTSC/cycles */\n              fprintf(stderr, \"LIBXSMM WARNING: libxsmm_timer_ncycles may not measure in cycles!\\n\");\n            }\n          }\n        }\n      }\n      LIBXSMM_ATOMIC_ADD_FETCH(&libxsmm_ninit, 1, LIBXSMM_ATOMIC_SEQ_CST);\n    }\n#if (0 != LIBXSMM_SYNC)\n    else while (1) {\n      if (1 < LIBXSMM_ATOMIC_LOAD(&libxsmm_ninit, LIBXSMM_ATOMIC_RELAXED)) {\n        break;\n      }\n# if 1\n      else LIBXSMM_SYNC_YIELD();\n# else\n      else LIBXSMM_SYNC_PAUSE;\n# endif\n    }\n#endif /*0 != LIBXSMM_SYNC*/\n    internal_init();\n  }\n}\n\n\nLIBXSMM_API\n#if defined(__GNUC__)\nLIBXSMM_ATTRIBUTE(no_instrument_function)\n#endif\nvoid libxsmm_finalize(void);\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_DTOR void libxsmm_finalize(void)\n{\n  void *const regaddr = &internal_registry;\n  uintptr_t regptr = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)((uintptr_t*)regaddr, LIBXSMM_ATOMIC_RELAXED);\n  libxsmm_code_pointer* registry = (libxsmm_code_pointer*)regptr;\n  if (NULL != registry) {\n    int i;\n#if (0 != LIBXSMM_SYNC)\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, &libxsmm_lock_global);\n# if (1 < INTERNAL_REGLOCK_MAXN)\n    { /* acquire locks and thereby shortcut lazy initialization later on */\n      int ntry = 0, n;\n      do {\n        for (i = 0, n = 0; i < internal_reglock_count; ++i) {\n          if (LIBXSMM_LOCK_ACQUIRED(LIBXSMM_REGLOCK) == LIBXSMM_LOCK_TRYLOCK(LIBXSMM_REGLOCK, &internal_reglock[i].state)) ++n;\n        }\n        ntry += (0 == n ? 1 : 0);\n      } while (n < internal_reglock_count && ntry < LIBXSMM_CLEANUP_NTRY);\n    }\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n#endif\n    regptr = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)((uintptr_t*)regaddr, LIBXSMM_ATOMIC_RELAXED);\n    registry = (libxsmm_code_pointer*)regptr;\n\n    if (NULL != registry) {\n      libxsmm_descriptor *const registry_keys = internal_registry_keys;\n      unsigned int rest = 0, errors = 0;\n      internal_registry_nbytes = 0;\n      for (i = 0; i < (LIBXSMM_CAPACITY_REGISTRY); ++i) {\n        /*const*/ libxsmm_code_pointer code = registry[i];\n        if (NULL != code.ptr_const) {\n          /* check if the registered entity is a GEMM kernel */\n          switch (registry_keys[i].kind) {\n            case LIBXSMM_KERNEL_KIND_MATMUL: {\n              const libxsmm_gemm_descriptor *const desc = &registry_keys[i].gemm.desc;\n              if (1 < desc->m && 1 < desc->n) {\n                const unsigned int njit = (0 == (LIBXSMM_CODE_STATIC & code.uval) ? 1 : 0);\n                const unsigned int nsta = (0 != (LIBXSMM_CODE_STATIC & code.uval) ? 1 : 0);\n                /* count whether kernel is static or JIT-code */\n                internal_update_mmstatistic(desc, 0, 0, njit, nsta);\n              }\n              else {\n                ++internal_statistic_num_gemv;\n              }\n              ++rest;\n            } break;\n            case LIBXSMM_KERNEL_KIND_MCOPY: {\n              ++internal_statistic_num_mcopy;\n            } break;\n            case LIBXSMM_KERNEL_KIND_TRANS: {\n              ++internal_statistic_num_tcopy;\n            } break;\n            case LIBXSMM_KERNEL_KIND_TRSM: {\n              ++internal_statistic_num_trsm;\n            } break;\n            case LIBXSMM_KERNEL_KIND_TRMM: {\n              ++internal_statistic_num_trmm;\n            } break;\n            default: if (LIBXSMM_KERNEL_KIND_INVALID <= registry_keys[i].kind) {\n              ++errors;\n            }\n            else {\n              ++rest;\n            }\n          }\n          if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n            if (0 != errors) {\n              fprintf(stderr, \"LIBXSMM ERROR: code registry is corrupted!\\n\");\n            }\n            if (LIBXSMM_CAPACITY_REGISTRY == (rest + errors + internal_statistic_num_gemv +\n              internal_statistic_num_mcopy + internal_statistic_num_tcopy +\n              internal_statistic_num_trsm + internal_statistic_num_trmm))\n            {\n              fprintf(stderr, \"LIBXSMM WARNING: code registry was exhausted!\\n\");\n            }\n          }\n          if (0 == (LIBXSMM_CODE_STATIC & code.uval)) { /* check for allocated/generated JIT-code */\n            void* buffer = NULL;\n            size_t size = 0;\n#if defined(LIBXSMM_HASH_COLLISION)\n            code.uval &= ~LIBXSMM_HASH_COLLISION; /* clear collision flag */\n#endif\n            if (EXIT_SUCCESS == libxsmm_get_malloc_xinfo(code.ptr_const, &size, NULL/*flags*/, &buffer)) {\n              libxsmm_xfree(code.ptr_const, 0/*no check*/);\n              /* round-up size (it is fine to assume 4 KB pages since it is likely more accurate than not rounding up) */\n              internal_registry_nbytes += (unsigned int)LIBXSMM_UP2(size + (((char*)code.ptr_const) - (char*)buffer), 4096/*4KB*/);\n            }\n          }\n        }\n      }\n#if defined(LIBXSMM_TRACE)\n      i = libxsmm_trace_finalize();\n      if (EXIT_SUCCESS != i && 0 != libxsmm_verbosity) { /* library code is expected to be mute */\n        fprintf(stderr, \"LIBXSMM ERROR: failed to finalize trace (error #%i)!\\n\", i);\n      }\n#endif\n#if defined(LIBXSMM_PERF)\n      libxsmm_perf_finalize();\n#endif\n      libxsmm_gemm_finalize();\n      libxsmm_xcopy_finalize();\n      libxsmm_dnn_finalize();\n      /* make internal registry globally unavailable */\n      LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE_ZERO, LIBXSMM_BITS)((uintptr_t*)regaddr, LIBXSMM_ATOMIC_SEQ_CST);\n      internal_registry_keys = NULL;\n#if !defined(NDEBUG)\n      internal_registry = NULL;\n#endif\n      libxsmm_xfree(registry_keys, 0/*no check*/);\n      libxsmm_xfree(registry, 0/*no check*/);\n#if defined(LIBXSMM_NTHREADS_USE) && defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n      libxsmm_xfree(internal_cache_buffer, 0/*no check*/);\n# if !defined(NDEBUG)\n      internal_cache_buffer = NULL;\n# endif\n#endif\n      }\n#if (0 != LIBXSMM_SYNC) /* LIBXSMM_LOCK_RELEASE, but no LIBXSMM_LOCK_DESTROY */\n# if (1 < INTERNAL_REGLOCK_MAXN)\n    for (i = 0; i < internal_reglock_count; ++i) LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, &internal_reglock[i].state);\n# elif !defined(LIBXSMM_UNIFY_LOCKS)\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_REGLOCK, internal_reglock_ptr);\n# endif\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, &libxsmm_lock_global);\n#endif\n  }\n}\n\n\nLIBXSMM_API void libxsmm_sink(LIBXSMM_VARIADIC)\n{\n  /* does nothing else but sink the given arguments */\n}\n\n\nLIBXSMM_API int libxsmm_get_target_archid(void)\n{\n  LIBXSMM_INIT\n#if !defined(__MIC__)\n  return libxsmm_target_archid;\n#else /* no JIT support */\n  return LIBXSMM_MIN(libxsmm_target_archid, LIBXSMM_X86_SSE3);\n#endif\n}\n\n\nLIBXSMM_API void libxsmm_set_target_archid(int id)\n{\n  int target_archid = LIBXSMM_TARGET_ARCH_UNKNOWN;\n  switch (id) {\n    case LIBXSMM_X86_AVX512_CPX:\n    case LIBXSMM_X86_AVX512_CLX:\n    case LIBXSMM_X86_AVX512_CORE:\n    case LIBXSMM_X86_AVX512_KNM:\n    case LIBXSMM_X86_AVX512_MIC:\n    case LIBXSMM_X86_AVX512:\n    case LIBXSMM_X86_AVX2:\n    case LIBXSMM_X86_AVX:\n    case LIBXSMM_X86_SSE4:\n    case LIBXSMM_X86_SSE3:\n    case LIBXSMM_TARGET_ARCH_GENERIC: {\n      target_archid = id;\n    } break;\n    default: if (LIBXSMM_X86_GENERIC <= id) {\n      target_archid = LIBXSMM_X86_GENERIC;\n    }\n    else {\n      target_archid = libxsmm_cpuid();\n    }\n  }\n  LIBXSMM_ATOMIC_STORE(&libxsmm_target_archid, target_archid, LIBXSMM_ATOMIC_RELAXED);\n  if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    const int cpuid = libxsmm_cpuid();\n    if (cpuid < target_archid) {\n      const char *const target_arch = libxsmm_cpuid_name(target_archid);\n      fprintf(stderr, \"LIBXSMM WARNING: \\\"%s\\\" code may fail to run on \\\"%s\\\"!\\n\",\n        target_arch, libxsmm_cpuid_name(cpuid));\n    }\n  }\n}\n\n\nLIBXSMM_API const char* libxsmm_get_target_arch(void)\n{\n  LIBXSMM_INIT\n  return libxsmm_cpuid_name(libxsmm_target_archid);\n}\n\n\n/* function serves as a helper for implementing the Fortran interface */\nLIBXSMM_API const char* libxsmmf_get_target_arch(int* length);\nLIBXSMM_API const char* libxsmmf_get_target_arch(int* length)\n{\n  const char *const arch = libxsmm_get_target_arch();\n  /* valid here since function is not in the public interface */\n  LIBXSMM_ASSERT(NULL != arch && 0 != length);\n  *length = (int)strlen(arch);\n  return arch;\n}\n\n\nLIBXSMM_API void libxsmm_set_target_arch(const char* arch)\n{\n  const int cpuid = libxsmm_cpuid();\n  int target_archid;\n  if (NULL != arch && 0 != *arch) {\n    const int jit = atoi(arch);\n    if (0 == strcmp(\"0\", arch)) {\n      target_archid = LIBXSMM_X86_SSE3;\n    }\n    else if (0 < jit) {\n      target_archid = LIBXSMM_X86_GENERIC + jit;\n    }\n    else if (0 == strcmp(\"cpx\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_CPX;\n    }\n    else if (0 == strcmp(\"clx\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_CLX;\n    }\n    else if (0 == strcmp(\"skx\", arch) || 0 == strcmp(\"skl\", arch)\n          /* \"avx3\"/\"avx512\" previously enabled LIBXSMM_X86_AVX512 */\n          || 0 == strcmp(\"avx3\", arch) || 0 == strcmp(\"avx512\", arch))\n    {\n      target_archid = LIBXSMM_X86_AVX512_CORE;\n    }\n    else if (0 == strcmp(\"knm\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_KNM;\n    }\n    else if (0 == strcmp(\"knl\", arch) || 0 == strcmp(\"mic\", arch)) {\n      target_archid = LIBXSMM_X86_AVX512_MIC;\n    }\n    else if (0 == strcmp(\"hsw\", arch) || 0 == strcmp(\"avx2\", arch)) {\n      target_archid = LIBXSMM_X86_AVX2;\n    }\n    else if (0 == strcmp(\"snb\", arch) || 0 == strcmp(\"avx\", arch)) {\n      target_archid = LIBXSMM_X86_AVX;\n    }\n    else if (0 == strcmp(\"wsm\", arch) || 0 == strcmp(\"nhm\", arch) || 0 == strcmp(\"sse4\", arch)\n       || 0 == strcmp(\"sse4_1\", arch) || 0 == strcmp(\"sse4.1\", arch)\n       || 0 == strcmp(\"sse4_2\", arch) || 0 == strcmp(\"sse4.2\", arch))\n    {\n      target_archid = LIBXSMM_X86_SSE4;\n    }\n    else if (0 == strcmp(\"sse\", arch) || 0 == strcmp(\"sse3\", arch)\n        || 0 == strcmp(\"ssse3\", arch) || 0 == strcmp(\"ssse\", arch))\n    {\n      target_archid = LIBXSMM_X86_SSE3;\n    }\n    else if (0 == strcmp(\"x86\", arch) || 0 == strcmp(\"x64\", arch) || 0 == strcmp(\"sse2\", arch)) {\n      target_archid = LIBXSMM_X86_GENERIC;\n    }\n    else if (0 == strcmp(\"generic\", arch) || 0 == strcmp(\"none\", arch)) {\n      target_archid = LIBXSMM_TARGET_ARCH_GENERIC;\n    }\n    else {\n      target_archid = cpuid;\n    }\n  }\n  else {\n    target_archid = cpuid;\n  }\n  if (cpuid < target_archid) { /* warn about code path if beyond CPUID */\n    static int error_once = 0;\n    if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      const char *const target_arch = libxsmm_cpuid_name(target_archid);\n      fprintf(stderr, \"LIBXSMM WARNING: \\\"%s\\\" code will fail to run on \\\"%s\\\"!\\n\",\n        target_arch, libxsmm_cpuid_name(cpuid));\n    }\n#if 0 /* limit code path to confirmed features */\n    target_archid = cpuid;\n#endif\n  }\n  LIBXSMM_ATOMIC_STORE(&libxsmm_target_archid, target_archid, LIBXSMM_ATOMIC_RELAXED);\n}\n\n\nLIBXSMM_API int libxsmm_get_verbosity(void)\n{\n  LIBXSMM_INIT\n  return libxsmm_verbosity;\n}\n\n\nLIBXSMM_API void libxsmm_set_verbosity(int level)\n{\n  LIBXSMM_INIT\n  LIBXSMM_ATOMIC_STORE(&libxsmm_verbosity, level, LIBXSMM_ATOMIC_RELAXED);\n}\n\n\nLIBXSMM_API libxsmm_gemm_prefetch_type libxsmm_get_gemm_auto_prefetch(void)\n{\n  return (libxsmm_gemm_prefetch_type)libxsmm_gemm_auto_prefetch;\n}\n\n\nLIBXSMM_API void libxsmm_set_gemm_auto_prefetch(libxsmm_gemm_prefetch_type strategy)\n{\n  if (0 == internal_gemm_auto_prefetch_locked) { /* LIBXSMM_GEMM_PREFETCH environment takes precedence */\n    LIBXSMM_ATOMIC_STORE(&libxsmm_gemm_auto_prefetch_default, strategy, LIBXSMM_ATOMIC_RELAXED);\n    LIBXSMM_ATOMIC_STORE(&libxsmm_gemm_auto_prefetch, strategy, LIBXSMM_ATOMIC_RELAXED);\n  }\n}\n\n\nLIBXSMM_API unsigned char libxsmm_typesize(libxsmm_datatype datatype)\n{\n  switch (datatype) {\n    case LIBXSMM_DATATYPE_F64:  return 8;\n    case LIBXSMM_DATATYPE_F32:  return 4;\n    case LIBXSMM_DATATYPE_BF16: return 2;\n    case LIBXSMM_DATATYPE_I64:  return 8;\n    case LIBXSMM_DATATYPE_I32:  return 4;\n    case LIBXSMM_DATATYPE_I16:  return 2;\n    case LIBXSMM_DATATYPE_I8:   return 1;\n    case LIBXSMM_DATATYPE_UNSUPPORTED: {\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM ERROR: unsupported data type!\\n\");\n      }\n    } break;\n  }\n  LIBXSMM_ASSERT_MSG(0, \"unsupported data type\");\n  return 1; /* avoid to return 0 to avoid div-by-zero in static analysis of depending code */\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_dvalue(libxsmm_datatype datatype, const void* value, double* dvalue)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != value && NULL != dvalue) {\n    switch (datatype) {\n      case LIBXSMM_DATATYPE_F64: *dvalue =         (*(const double*)value); break;\n      case LIBXSMM_DATATYPE_F32: *dvalue = (double)(*(const float *)value); break;\n      case LIBXSMM_DATATYPE_I32: *dvalue = (double)(*(const int   *)value); break;\n      case LIBXSMM_DATATYPE_I16: *dvalue = (double)(*(const short *)value); break;\n      case LIBXSMM_DATATYPE_I8:  *dvalue = (double)(*(const char  *)value); break;\n      default: result = EXIT_FAILURE;\n    }\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN const char* libxsmm_typename(libxsmm_datatype datatype)\n{\n  switch (datatype) {\n    case LIBXSMM_DATATYPE_F64:  return \"f64\";\n    case LIBXSMM_DATATYPE_F32:  return \"f32\";\n    case LIBXSMM_DATATYPE_BF16: return \"bf16\";\n    case LIBXSMM_DATATYPE_I64:  return \"i64\";\n    case LIBXSMM_DATATYPE_I32:  return \"i32\";\n    case LIBXSMM_DATATYPE_I16:  return \"i16\";\n    case LIBXSMM_DATATYPE_I8:   return \"i8\";\n    default: {\n      if (LIBXSMM_GEMM_PRECISION_I16 == LIBXSMM_GETENUM_INP(datatype) &&\n          LIBXSMM_GEMM_PRECISION_I32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"i16i32\";\n      }\n      else if (LIBXSMM_GEMM_PRECISION_I16 == LIBXSMM_GETENUM_INP(datatype) &&\n               LIBXSMM_GEMM_PRECISION_F32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"i16f32\";\n      }\n      else if (LIBXSMM_GEMM_PRECISION_I8 == LIBXSMM_GETENUM_INP(datatype) &&\n               LIBXSMM_GEMM_PRECISION_I32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"i8i32\";\n      }\n      else if (LIBXSMM_GEMM_PRECISION_BF16 == LIBXSMM_GETENUM_INP(datatype) &&\n               LIBXSMM_GEMM_PRECISION_F32 == LIBXSMM_GETENUM_OUT(datatype))\n      {\n        return \"bf16f32\";\n      }\n      else {\n        return \"void\";\n      }\n    }\n  }\n}\n\n\nLIBXSMM_API_INLINE const char* internal_get_typesize_string(size_t typesize)\n{\n  static LIBXSMM_TLS char result[4];\n  LIBXSMM_ASSERT(256 > typesize);\n  if (10 > typesize) {\n    result[0] = (char)('0' + typesize);\n    result[1] = 0;\n  }\n  else {\n    LIBXSMM_SNPRINTF(result, sizeof(result), \"%i\", (int)typesize);\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_build(const libxsmm_build_request* request, unsigned int regindex, libxsmm_code_pointer* code)\n{\n  int result = EXIT_SUCCESS;\n#if !defined(__MIC__)\n  const char* target_arch = libxsmm_cpuid_name(libxsmm_target_archid);\n  libxsmm_generated_code generated_code;\n  char jit_name[256] = { 0 };\n\n  /* large enough temporary buffer for generated code */\n# if defined(NDEBUG)\n  char jit_buffer[LIBXSMM_CODE_MAXSIZE];\n  LIBXSMM_MEMZERO127(&generated_code);\n  generated_code.generated_code = jit_buffer;\n  generated_code.buffer_size = sizeof(jit_buffer);\n# else\n  LIBXSMM_MEMZERO127(&generated_code);\n  generated_code.generated_code = malloc(LIBXSMM_CODE_MAXSIZE);\n  generated_code.buffer_size = (NULL != generated_code.generated_code ? LIBXSMM_CODE_MAXSIZE : 0);\n# endif\n  /* setup code generation */\n  generated_code.code_type = 2;\n\n  LIBXSMM_ASSERT(NULL != generated_code.generated_code || 0 == generated_code.buffer_size);\n  LIBXSMM_ASSERT(NULL != request && 0 != libxsmm_target_archid);\n  LIBXSMM_ASSERT(NULL != code && NULL == code->ptr_const);\n\n  switch (request->kind) { /* generate kernel */\n    case LIBXSMM_BUILD_KIND_GEMM: { /* small MxM kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.gemm);\n# if 0 /* dummy kernel for an empty shape is desired */\n      if (0 < request->descriptor.gemm->m   && 0 < request->descriptor.gemm->n   && 0 < request->descriptor.gemm->k &&\n          0 < request->descriptor.gemm->lda && 0 < request->descriptor.gemm->ldb && 0 < request->descriptor.gemm->ldc)\n# endif\n      {\n        const unsigned int m = request->descriptor.gemm->m, n = request->descriptor.gemm->n, k = request->descriptor.gemm->k;\n# if !defined(LIBXSMM_DENY_RETARGET) /* disable: ECFLAGS=-DLIBXSMM_DENY_RETARGET */\n        if (LIBXSMM_X86_AVX2 < libxsmm_target_archid &&\n           (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.gemm->datatype) ||\n            LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.gemm->datatype)) &&\n           (16 >= (m * k) || 16 >= (k * n) || 16 >= (m * n)))\n        {\n          target_arch = \"hsw\";\n        }\n# endif\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_gemm_kernel, &generated_code, request->descriptor.gemm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i_br%i.mxm\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.gemm->flags) ? 'n' : 't', m, n, k,\n            request->descriptor.gemm->lda, request->descriptor.gemm->ldb, request->descriptor.gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.gemm->flags) ? 0 : 1, uid,\n            0 == (LIBXSMM_GEMM_FLAG_BATCH_REDUCE & request->descriptor.gemm->flags) ? 0 : 1);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_SRSOA: { /* sparse SOA kernel, CSR format */\n      LIBXSMM_ASSERT(NULL != request->descriptor.srsoa && 0 != request->descriptor.srsoa->gemm);\n      LIBXSMM_ASSERT(NULL != request->descriptor.srsoa->row_ptr && 0 != request->descriptor.srsoa->column_idx && 0 != request->descriptor.srsoa->values);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.srsoa->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.srsoa->gemm->datatype))\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_spgemm_csr_soa_kernel, &generated_code, request->descriptor.srsoa->gemm, target_arch,\n          request->descriptor.srsoa->row_ptr, request->descriptor.srsoa->column_idx, request->descriptor.srsoa->values);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.srsoa->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.srsoa->gemm->datatype);\n          const unsigned int nnz = (request->descriptor.srsoa->gemm->lda == 0) ?\n            request->descriptor.srsoa->row_ptr[request->descriptor.srsoa->gemm->m] : request->descriptor.srsoa->row_ptr[request->descriptor.srsoa->gemm->k];\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i_nnz%u.srsoa\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.srsoa->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.srsoa->gemm->flags) ? 'n' : 't',\n            request->descriptor.srsoa->gemm->m,   request->descriptor.srsoa->gemm->n,   request->descriptor.srsoa->gemm->k,\n            request->descriptor.srsoa->gemm->lda, request->descriptor.srsoa->gemm->ldb, request->descriptor.srsoa->gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.srsoa->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.srsoa->gemm->flags) ? 0 : 1,\n            uid, nnz);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_SCSOA: { /* sparse SOA kernel, CSC format */\n      LIBXSMM_ASSERT(NULL != request->descriptor.scsoa && 0 != request->descriptor.scsoa->gemm);\n      LIBXSMM_ASSERT(NULL != request->descriptor.scsoa->row_idx && 0 != request->descriptor.scsoa->column_ptr && 0 != request->descriptor.scsoa->values);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.scsoa->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.scsoa->gemm->datatype))\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_spgemm_csc_soa_kernel, &generated_code, request->descriptor.scsoa->gemm, target_arch,\n          request->descriptor.scsoa->row_idx, request->descriptor.scsoa->column_ptr, request->descriptor.scsoa->values);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.scsoa->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.scsoa->gemm->datatype);\n          const unsigned int nnz = (request->descriptor.scsoa->gemm->lda == 0) ?\n            request->descriptor.scsoa->column_ptr[request->descriptor.scsoa->gemm->k] : request->descriptor.scsoa->column_ptr[request->descriptor.scsoa->gemm->n];\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i_nnz%u.scsoa\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.scsoa->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.scsoa->gemm->flags) ? 'n' : 't',\n            request->descriptor.scsoa->gemm->m,   request->descriptor.scsoa->gemm->n,   request->descriptor.scsoa->gemm->k,\n            request->descriptor.scsoa->gemm->lda, request->descriptor.scsoa->gemm->ldb, request->descriptor.scsoa->gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.scsoa->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.scsoa->gemm->flags) ? 0 : 1,\n            uid, nnz);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_PGEMMRMAC: { /* packed GEMM, B regular matrix, row-major */\n      LIBXSMM_ASSERT(NULL != request->descriptor.pgemmacrm && 0 != request->descriptor.pgemmacrm->gemm);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmacrm->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmacrm->gemm->datatype))\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_packed_gemm_ac_rm, &generated_code, request->descriptor.pgemmacrm->gemm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.pgemmacrm->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.pgemmacrm->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i.pgemmacrm\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.pgemmacrm->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.pgemmacrm->gemm->flags) ? 'n' : 't',\n            request->descriptor.pgemmacrm->gemm->m,   request->descriptor.pgemmacrm->gemm->n,   request->descriptor.pgemmacrm->gemm->k,\n            request->descriptor.pgemmacrm->gemm->lda, request->descriptor.pgemmacrm->gemm->ldb, request->descriptor.pgemmacrm->gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.pgemmacrm->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.pgemmacrm->gemm->flags) ? 0 : 1,\n            uid);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_PGEMMRMBC: { /* packed GEMM, A regular matrix, row-major */\n      LIBXSMM_ASSERT(NULL != request->descriptor.pgemmbcrm && 0 != request->descriptor.pgemmbcrm->gemm);\n      /* only floating point */\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmbcrm->gemm->datatype) ||\n          LIBXSMM_GEMM_PRECISION_F32 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.pgemmbcrm->gemm->datatype))\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_packed_gemm_bc_rm, &generated_code, request->descriptor.pgemmbcrm->gemm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.pgemmbcrm->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.pgemmbcrm->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i.pgemmbcrm\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.pgemmbcrm->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.pgemmbcrm->gemm->flags) ? 'n' : 't',\n            request->descriptor.pgemmbcrm->gemm->m,   request->descriptor.pgemmbcrm->gemm->n,   request->descriptor.pgemmbcrm->gemm->k,\n            request->descriptor.pgemmbcrm->gemm->lda, request->descriptor.pgemmbcrm->gemm->ldb, request->descriptor.pgemmbcrm->gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.pgemmbcrm->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.pgemmbcrm->gemm->flags) ? 0 : 1,\n            uid);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_SREG: { /* sparse register kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.sreg && 0 != request->descriptor.sreg->gemm);\n      LIBXSMM_ASSERT(NULL != request->descriptor.sreg->row_ptr && 0 != request->descriptor.sreg->column_idx && 0 != request->descriptor.sreg->values);\n#if 1\n      if (LIBXSMM_GEMM_PRECISION_F64 == /*LIBXSMM_GETENUM_OUT*/(request->descriptor.sreg->gemm->datatype)) /* only double-precision */\n#endif\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_spgemm_csr_reg_kernel, &generated_code, request->descriptor.sreg->gemm, target_arch,\n          request->descriptor.sreg->row_ptr, request->descriptor.sreg->column_idx,\n          (const double*)request->descriptor.sreg->values);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const int uid = libxsmm_gemm_prefetch2uid((libxsmm_gemm_prefetch_type)request->descriptor.sreg->gemm->prefetch);\n          const char *const tname = libxsmm_typename((libxsmm_datatype)request->descriptor.sreg->gemm->datatype);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_%s_%c%c_%ux%ux%u_%u_%u_%u_a%i_b%i_p%i.sreg\", target_arch, tname,\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_A & request->descriptor.sreg->gemm->flags) ? 'n' : 't',\n            0 == (LIBXSMM_GEMM_FLAG_TRANS_B & request->descriptor.sreg->gemm->flags) ? 'n' : 't',\n            request->descriptor.sreg->gemm->m,   request->descriptor.sreg->gemm->n,   request->descriptor.sreg->gemm->k,\n            request->descriptor.sreg->gemm->lda, request->descriptor.sreg->gemm->ldb, request->descriptor.sreg->gemm->ldc,\n          /*0 != (LIBXSMM_GEMM_FLAG_ALPHA_0 & request->descriptor.sreg->gemm->flags) ? 0 : */1,\n            0 != (LIBXSMM_GEMM_FLAG_BETA_0  & request->descriptor.sreg->gemm->flags) ? 0 : 1,\n            uid);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_CFWD: { /* forward convolution */\n      LIBXSMM_ASSERT(NULL != request->descriptor.cfwd);\n      if (0 < request->descriptor.cfwd->kw && 0 < request->descriptor.cfwd->kh &&\n          0 != request->descriptor.cfwd->stride_w && 0 != request->descriptor.cfwd->stride_h)\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_convolution_forward_kernel, &generated_code, request->descriptor.cfwd, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const precision_in  = libxsmm_typename((libxsmm_datatype)request->descriptor.cfwd->datatype);\n          const char *const precision_out = libxsmm_typename((libxsmm_datatype)request->descriptor.cfwd->datatype_itm);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          if (request->descriptor.cfwd->use_fwd_generator_for_bwd == 0) {\n           LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_fwd_%s_%s_%ux%u_%ux%uu_s%ii%io_vl%ui%uo_ri%ux%u_ro%ux%u_r%ux%u_p%i_f%i.conv\",\n            target_arch/*code path name*/, precision_in, precision_out,\n            (unsigned int)request->descriptor.cfwd->kw/*kernel width*/, (unsigned int)request->descriptor.cfwd->kh/*kernel height*/,\n            (unsigned int)request->descriptor.cfwd->unroll_kw/*width*/, (unsigned int)request->descriptor.cfwd->unroll_kh/*height*/,\n            (int)request->descriptor.cfwd->stride_w/*input offset*/, (int)request->descriptor.cfwd->stride_h/*output offsets*/,\n            (unsigned int)request->descriptor.cfwd->ifm_block/*VLEN*/, (unsigned int)request->descriptor.cfwd->ofm_block/*VLEN*/,\n            (unsigned int)request->descriptor.cfwd->ifw_padded, (unsigned int)request->descriptor.cfwd->ifh_padded,\n            (unsigned int)request->descriptor.cfwd->ofw_padded/*1D and 2D register block*/,\n            (unsigned int)request->descriptor.cfwd->ofh_padded/*2D register block*/,\n            (unsigned int)request->descriptor.cfwd->ofw_rb/*register block ofw*/,\n            (unsigned int)request->descriptor.cfwd->ofh_rb/*register block ofh*/,\n            (int)request->descriptor.cfwd->prefetch/*binary OR'd prefetch flags*/,\n            (int)request->descriptor.cfwd->format/*binary OR'd format flags*/);\n          } else {\n           LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_bwd_%s_%s_%ux%u_%ux%uu_s%ii%io_vl%ui%uo_ri%ux%u_ro%ux%u_r%ux%u_p%i_f%i.conv\",\n            target_arch/*code path name*/, precision_in, precision_out,\n            (unsigned int)request->descriptor.cfwd->kw/*kernel width*/, (unsigned int)request->descriptor.cfwd->kh/*kernel height*/,\n            (unsigned int)request->descriptor.cfwd->unroll_kw/*width*/, (unsigned int)request->descriptor.cfwd->unroll_kh/*height*/,\n            (int)request->descriptor.cfwd->stride_w/*input offset*/, (int)request->descriptor.cfwd->stride_h/*output offsets*/,\n            (unsigned int)request->descriptor.cfwd->ifm_block/*VLEN*/, (unsigned int)request->descriptor.cfwd->ofm_block/*VLEN*/,\n            (unsigned int)request->descriptor.cfwd->ifw_padded, (unsigned int)request->descriptor.cfwd->ifh_padded,\n            (unsigned int)request->descriptor.cfwd->ofw_padded/*1D and 2D register block*/,\n            (unsigned int)request->descriptor.cfwd->ofh_padded/*2D register block*/,\n            (unsigned int)request->descriptor.cfwd->ofw_rb/*register block ofw*/,\n            (unsigned int)request->descriptor.cfwd->ofh_rb/*register block ofh*/,\n            (int)request->descriptor.cfwd->prefetch/*binary OR'd prefetch flags*/,\n            (int)request->descriptor.cfwd->format/*binary OR'd format flags*/);\n          }\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_CUPD: { /* convolution update weights */\n      LIBXSMM_ASSERT(NULL != request->descriptor.cupd);\n      if (0 < request->descriptor.cupd->kw &&\n          0 != request->descriptor.cupd->stride_w && 0 != request->descriptor.cupd->stride_h)\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_convolution_weight_update_kernel, &generated_code, request->descriptor.cupd, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const precision_in  = libxsmm_typename((libxsmm_datatype)request->descriptor.cupd->datatype);\n          const char *const precision_out = libxsmm_typename((libxsmm_datatype)request->descriptor.cupd->datatype_itm);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_upd_%s_%s_%ux%u_s%ii%io_vl%ui%uo_ri%ux%u_ro%ux%u_r%ux%u_of%uu%ux%uu%u_if%uu_t%u_p%i_f%i.conv\",\n            target_arch/*code path name*/, precision_in, precision_out,\n            (unsigned int)request->descriptor.cupd->kw/*kernel width*/, (unsigned int)request->descriptor.cupd->kh/*kernel height*/,\n            (int)request->descriptor.cupd->stride_w/*input offset*/, (int)request->descriptor.cupd->stride_h/*output offsets*/,\n            (unsigned int)request->descriptor.cupd->ifm_block/*VLEN*/, (unsigned int)request->descriptor.cupd->ofm_block/*VLEN*/,\n            (unsigned int)request->descriptor.cupd->ifw_padded, (unsigned int)request->descriptor.cupd->ifh_padded,\n            (unsigned int)request->descriptor.cupd->ofw_padded/*1D and 2D register block*/,\n            (unsigned int)request->descriptor.cupd->ofh_padded/*2D register block*/,\n            (unsigned int)request->descriptor.cupd->ofw_rb/*register block ofw*/,\n            (unsigned int)request->descriptor.cupd->ofh_rb/*register block ofh*/,\n            (unsigned int)request->descriptor.cupd->ofw/*ofw*/, (unsigned int)request->descriptor.cupd->ofw_unroll/*ofw_unroll*/,\n            (unsigned int)request->descriptor.cupd->ofh/*ofh*/, (unsigned int)request->descriptor.cupd->ofh_unroll/*ofh_unroll*/,\n            (unsigned int)request->descriptor.cupd->ifm_unroll/*ifm unroll*/,\n            (unsigned int)request->descriptor.cupd->transpose_ofw_ifm/*transpose_ofw_ifm*/,\n            (int)request->descriptor.cupd->prefetch/*binary OR'd prefetch flags*/,\n            (int)request->descriptor.cupd->format/*binary OR'd format flags*/);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_MCOPY: { /* matcopy kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.mcopy);\n# if 0 /* TODO: backend supports typesize <= 4, but kernels for typesize < 4 are incorrect */\n      if (4 == request->descriptor.mcopy->typesize)\n# endif\n      {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_matcopy_kernel, &generated_code, request->descriptor.mcopy, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const tsizename = internal_get_typesize_string(request->descriptor.mcopy->typesize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%ux%u_%ux%u_p%u.mcopy\", target_arch, tsizename,\n            request->descriptor.mcopy->m, request->descriptor.mcopy->n, request->descriptor.mcopy->ldi, request->descriptor.mcopy->ldo,\n            (unsigned int)request->descriptor.mcopy->prefetch);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_TRANS: { /* transpose kernel */\n      LIBXSMM_ASSERT(NULL != request->descriptor.trans);\n      if (4 == request->descriptor.trans->typesize || 8 == request->descriptor.trans->typesize) {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_transpose_kernel, &generated_code, request->descriptor.trans, libxsmm_target_archid);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const tsizename = internal_get_typesize_string(request->descriptor.trans->typesize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%ux%u_%u.trans\", target_arch, tsizename,\n            request->descriptor.trans->m, request->descriptor.trans->n, request->descriptor.trans->ldo);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_PGEMM: { /* compact P/GEMM-kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.pgemm);\n      tsize = (unsigned int)request->descriptor.pgemm->typesize;\n      if (4 == tsize || 8 == tsize) {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_pgemm_kernel, &generated_code, request->descriptor.pgemm, libxsmm_target_archid);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const tsizename = internal_get_typesize_string(tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c%c%c_%ux%ux%u_%u_%u_%u_%i.pgemm\", target_arch, tsizename,\n            request->descriptor.pgemm->transa, request->descriptor.pgemm->transb, request->descriptor.pgemm->layout,\n            request->descriptor.pgemm->m, request->descriptor.pgemm->n, request->descriptor.pgemm->k,\n            request->descriptor.pgemm->lda, request->descriptor.pgemm->ldb, request->descriptor.pgemm->ldc,\n            (int)request->descriptor.pgemm->alpha_val);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_GETRF: { /* compact GETRF kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.getrf);\n      tsize = (unsigned int)request->descriptor.getrf->typesize;\n      if (4 == tsize || 8 == tsize) {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_getrf_kernel, &generated_code, request->descriptor.getrf, libxsmm_target_archid);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const tsizename = internal_get_typesize_string(tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c_%ux%u_%u.getrf\", target_arch, tsizename,\n            request->descriptor.getrf->layout, request->descriptor.getrf->m, request->descriptor.getrf->n, request->descriptor.getrf->lda);\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_TRMM: { /* compact TRMM kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.trmm);\n      tsize = (unsigned int)request->descriptor.trmm->typesize;\n      if (4 == tsize || 8 == tsize) {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_trmm_kernel, &generated_code, request->descriptor.trmm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const tsizename = internal_get_typesize_string(tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c%c%c%c_%ux%u_%u_%u.trmm\", target_arch, tsizename,\n            request->descriptor.trmm->transa, request->descriptor.trmm->layout, request->descriptor.trmm->side, request->descriptor.trmm->uplo,\n            request->descriptor.trmm->m, request->descriptor.trmm->n, request->descriptor.trmm->lda, request->descriptor.trmm->ldb); /* TODO: alpha */\n        }\n      }\n    } break;\n    case LIBXSMM_BUILD_KIND_TRSM: { /* compact TRSM kernel (packed) */\n      unsigned int tsize;\n      LIBXSMM_ASSERT(NULL != request->descriptor.trsm);\n      tsize = (unsigned int)request->descriptor.trsm->typesize;\n      if (4 == tsize || 8 == tsize) {\n        LIBXSMM_NO_OFFLOAD(void, libxsmm_generator_trsm_kernel, &generated_code, request->descriptor.trsm, target_arch);\n# if !defined(LIBXSMM_VTUNE)\n        if (0 > libxsmm_verbosity)\n# endif\n        {\n          const char *const tsizename = internal_get_typesize_string(tsize);\n          /* adopt scheme which allows kernel names of LIBXSMM to appear in order (Intel VTune, etc.) */\n          LIBXSMM_SNPRINTF(jit_name, sizeof(jit_name), \"libxsmm_%s_tsize%s_%c%c%c%c_%ux%u_%u_%u.trsm\", target_arch, tsizename,\n            request->descriptor.trsm->transa, request->descriptor.trsm->layout, request->descriptor.trsm->side, request->descriptor.trsm->uplo,\n            request->descriptor.trsm->m, request->descriptor.trsm->n, request->descriptor.trsm->lda, request->descriptor.trsm->ldb); /* TODO: alpha */\n        }\n      }\n    } break;\n# if !defined(NDEBUG) /* library code is expected to be mute */\n    default: { /* unknown kind */\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid build request discovered!\\n\");\n      }\n      result = EXIT_FAILURE;\n    }\n# endif\n  }\n\n  if  (0 == generated_code.last_error /* no error raised */\n    && 0 != generated_code.code_size /*check (tcopy issue?)*/)\n  {\n    char* code_buffer = NULL;\n    void* code_buffer_result = &code_buffer;\n    LIBXSMM_ASSERT(generated_code.code_size <= LIBXSMM_CODE_MAXSIZE);\n    LIBXSMM_ASSERT(NULL != generated_code.generated_code);\n    /* attempt to create executable buffer */\n    result = libxsmm_xmalloc((void**)code_buffer_result, generated_code.code_size, 0/*auto*/,\n      /* flag must be a superset of what's populated by libxsmm_malloc_attrib */\n      LIBXSMM_MALLOC_FLAG_RWX, &regindex, sizeof(regindex));\n    if (EXIT_SUCCESS == result) { /* check for success */\n      LIBXSMM_ASSERT(NULL != code_buffer);\n      /* copy temporary buffer into the prepared executable buffer */\n# if defined(NDEBUG)\n      { int i; /* precondition: jit_buffer == generated_code.generated_code */\n        for (i = 0; i < (int)generated_code.code_size; ++i) code_buffer[i] = jit_buffer[i];\n      }\n# else\n      memcpy(code_buffer, generated_code.generated_code, generated_code.code_size);\n# endif\n      /* attribute/protect buffer and revoke unnecessary flags */\n      result = libxsmm_malloc_attrib((void**)code_buffer_result, LIBXSMM_MALLOC_FLAG_X, jit_name);\n      if (EXIT_SUCCESS == result) { /* check for success */\n        code->pmm = code_buffer; /* commit buffer */\n        LIBXSMM_ASSERT(NULL != code->pmm && 0 == (LIBXSMM_CODE_STATIC & code->uval));\n      }\n      else { /* release buffer */\n        libxsmm_xfree(code_buffer, 0/*no check*/);\n      }\n    }\n  }\n  else {\n    result = (0 != generated_code.last_error ? generated_code.last_error : EXIT_FAILURE);\n  }\n# if !defined(NDEBUG)\n  free(generated_code.generated_code); /* free temporary/initial code buffer */\n# endif\n#else /* unsupported platform */\n  LIBXSMM_UNUSED(request); LIBXSMM_UNUSED(regindex); LIBXSMM_UNUSED(code);\n  /* libxsmm_get_target_arch also serves as a runtime check whether JIT is available or not */\n  if (LIBXSMM_X86_SSE3 <= libxsmm_target_archid) result = EXIT_FAILURE;\n#endif\n  LIBXSMM_ASSERT(NULL != code->pmm || EXIT_FAILURE == result);\n  return result;\n}\n\n\n#if defined(LIBXSMM_DESC_PAD)\nLIBXSMM_API_INLINE void internal_pad_descriptor(libxsmm_descriptor* desc, size_t size)\n{\n  const signed char s = (signed char)LIBXSMM_MAX(LIBXSMM_DIFF_SIZE, LIBXSMM_HASH_SIZE); signed char i;\n  LIBXSMM_ASSERT(NULL != desc && s <= LIBXSMM_DESCRIPTOR_MAXSIZE);\n  for (i = (signed char)size; i < s; ++i) desc->data[i] = 0;\n}\n#endif\n\n\nLIBXSMM_API_INLINE libxsmm_code_pointer internal_find_code(libxsmm_descriptor* desc, size_t desc_size)\n{\n  libxsmm_code_pointer flux_entry = { 0 };\n  const size_t size = sizeof(libxsmm_descriptor_kind) + desc_size;\n#if !defined(NDEBUG) && (0 != LIBXSMM_JIT)\n  int build = EXIT_SUCCESS;\n#endif\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n# if defined(LIBXSMM_NTHREADS_USE)\n  const unsigned int tid = libxsmm_get_tid();\n  internal_cache_type *const cache = internal_cache_buffer + tid;\n# else\n  static LIBXSMM_TLS internal_cache_type internal_cache_buffer;\n  internal_cache_type *const cache = &internal_cache_buffer;\n# endif\n  unsigned char cache_index;\n# if defined(LIBXSMM_DESC_PAD)\n#   if defined(LIBXSMM_DESC_INLINE)\n  LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n  internal_pad_descriptor(desc, size);\n  LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n  LIBXSMM_DIFF_N(unsigned char, cache_index, LIBXSMM_DIFF(LIBXSMM_DIFF_SIZE), xdesc, cache->entry.keys,\n    LIBXSMM_DIFF_SIZE, LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n#   else\n  internal_pad_descriptor(desc, size);\n  cache_index = (unsigned char)libxsmm_diff_n(desc, cache->entry.keys,\n    LIBXSMM_DIFF_SIZE, LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n#   endif\n# elif defined(LIBXSMM_DESC_INLINE)\n  LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n  LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n  LIBXSMM_DIFF_N(unsigned char, cache_index, LIBXSMM_DIFF(LIBXSMM_DIFF_SIZE), xdesc, cache->entry.keys,\n    LIBXSMM_MIN(size, LIBXSMM_DIFF_SIZE), LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n# else\n  LIBXSMM_ASSERT(NULL != desc);\n  cache_index = (unsigned char)libxsmm_diff_n(desc, cache->entry.keys,\n    LIBXSMM_MIN(size, LIBXSMM_DIFF_SIZE), LIBXSMM_DESCRIPTOR_MAXSIZE, cache->entry.hit, cache->entry.size);\n# endif\n  if (\n# if !defined(LIBXSMM_NTHREADS_USE) || defined(LIBXSMM_CACHE_CLEAR)\n    cache->entry.id == libxsmm_ninit &&\n# endif\n    cache_index < cache->entry.size)\n  { /* valid hit */\n    flux_entry = cache->entry.code[cache_index];\n    cache->entry.hit = cache_index;\n  }\n  else\n#else\n  LIBXSMM_ASSERT(NULL != desc);\n# if defined(LIBXSMM_DESC_PAD)\n# if defined(LIBXSMM_DESC_INLINE)\n  LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n  internal_pad_descriptor(desc, size);\n  LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n# else\n  internal_pad_descriptor(desc, size);\n# endif\n# endif\n#endif\n  {\n#if defined(LIBXSMM_DESC_PAD)\n    unsigned int i = LIBXSMM_CRC32(LIBXSMM_HASH_SIZE)(LIBXSMM_HASH_SEED, desc);\n#else\n    unsigned int i = libxsmm_crc32(LIBXSMM_HASH_SEED, desc, LIBXSMM_MIN(size, LIBXSMM_HASH_SIZE));\n#endif\n    unsigned int i0 = i = LIBXSMM_MOD2(i, LIBXSMM_CAPACITY_REGISTRY), mode = 0, diff = 1;\n    LIBXSMM_ASSERT(NULL != internal_registry);\n    LIBXSMM_ASSERT(&desc->kind == &desc->gemm.pad && desc->kind == desc->gemm.pad);\n    do { /* use calculated location and check if the requested code is already JITted */\n#if (1 < INTERNAL_REGLOCK_MAXN) || !LIBXSMM_LOCK_TYPE_ISRW(LIBXSMM_REGLOCK) /* read registered code */\n# if 1 /* omitting an atomic load is safe but avoids race-detectors to highlight this location */\n      uintptr_t *const fluxaddr = &internal_registry[i].uval;\n      flux_entry.uval = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(fluxaddr, LIBXSMM_ATOMIC_RELAXED);\n# else\n      flux_entry = internal_registry[i];\n# endif\n#else\n      LIBXSMM_LOCK_ACQREAD(LIBXSMM_REGLOCK, internal_reglock_ptr);\n      flux_entry = internal_registry[i]; /* read registered code */\n      LIBXSMM_LOCK_RELREAD(LIBXSMM_REGLOCK, internal_reglock_ptr);\n#endif\n      if ((NULL != flux_entry.ptr_const || 1 == mode) && 2 > mode) { /* check existing entry further */\n        if (NULL != flux_entry.ptr_const) {\n#if defined(LIBXSMM_DESC_PAD)\n# if defined(LIBXSMM_DIFF_INLINE)\n#   if !defined(LIBXSMM_DESC_INLINE)\n          LIBXSMM_DIFF_DECL(LIBXSMM_DIFF_SIZE, xdesc);\n          LIBXSMM_DIFF_LOAD(LIBXSMM_DIFF_SIZE, xdesc, desc);\n#   endif\n          diff = LIBXSMM_DIFF(LIBXSMM_DIFF_SIZE)(xdesc, internal_registry_keys + i, 0/*dummy*/);\n# else\n          diff = libxsmm_diff(desc, internal_registry_keys + i, LIBXSMM_DIFF_SIZE);\n# endif\n#else\n          diff = libxsmm_diff(desc, internal_registry_keys + i, LIBXSMM_MIN(size, LIBXSMM_DIFF_SIZE));\n#endif\n        }\n#if !defined(NDEBUG)\n        else LIBXSMM_ASSERT(0 != diff);\n#endif\n        if (0 != diff) { /* search for code version */\n          if (0 == mode) { /* transition to higher mode */\n            i0 = i; /* keep current position on record */\n#if defined(LIBXSMM_HASH_COLLISION)\n            /* enter code generation, and collision fix-up */\n            if (0 == (LIBXSMM_HASH_COLLISION & flux_entry.uval)) {\n              LIBXSMM_ASSERT(NULL != flux_entry.ptr_const); /* collision */\n              mode = 3;\n            }\n            else\n#endif      /* search for an existing code version */\n            mode = 1; /* else */\n          }\n          i = LIBXSMM_MOD2(i + 1, LIBXSMM_CAPACITY_REGISTRY);\n          if (i == i0) { /* search finished, no code version exists */\n#if defined(LIBXSMM_HASH_COLLISION)\n            mode = 3; /* enter code generation, and collision fix-up */\n#else\n            mode = 2; /* enter code generation */\n#endif\n            if (LIBXSMM_KERNEL_KIND_MATMUL == desc->kind) {\n              internal_update_mmstatistic(&desc->gemm.desc, 0, 1/*collision*/, 0, 0);\n            }\n          }\n          LIBXSMM_ASSERT(0 != diff); /* continue */\n        }\n      }\n      else { /* enter code generation (there is no code version yet) */\n        LIBXSMM_ASSERT(0 == mode || 1 < mode);\n#if (0 != LIBXSMM_JIT)\n        if (LIBXSMM_X86_AVX <= libxsmm_target_archid || /* check if JIT is supported (CPUID) */\n           (LIBXSMM_X86_SSE3 <= libxsmm_target_archid && LIBXSMM_BUILD_KIND_GEMM == desc->kind))\n        {\n          LIBXSMM_ASSERT(0 != mode || NULL == flux_entry.ptr_const/*code version does not exist*/);\n          INTERNAL_FIND_CODE_LOCK(lock, i, diff, flux_entry.pmm); /* lock the registry entry */\n          if (NULL == internal_registry[i].ptr_const) { /* double-check registry after acquiring the lock */\n            libxsmm_build_request request; /* setup the code build request */\n            LIBXSMM_ASSERT(desc->kind < LIBXSMM_KERNEL_KIND_INVALID);\n            request.kind = (libxsmm_build_kind)desc->kind;\n            request.descriptor.ptr = &desc->gemm.desc;\n#if defined(NDEBUG)\n            if (EXIT_SUCCESS == libxsmm_build(&request, i, &flux_entry) && NULL != flux_entry.ptr_const)\n#else\n            build = libxsmm_build(&request, i, &flux_entry);\n            if (EXIT_SUCCESS == build && NULL != flux_entry.ptr_const)\n#endif\n            {\n              LIBXSMM_ASSIGN127(internal_registry_keys + i, desc);\n# if (1 < INTERNAL_REGLOCK_MAXN)\n              LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_registry[i].pmm, flux_entry.pmm, LIBXSMM_ATOMIC_SEQ_CST);\n# else\n              internal_registry[i] = flux_entry;\n# endif\n# if defined(LIBXSMM_HASH_COLLISION)\n              if (2 < mode) { /* arrived from collision state; now mark as collision */\n                libxsmm_code_pointer fix_entry;\n#   if (1 < INTERNAL_REGLOCK_MAXN)\n                fix_entry.pmm = LIBXSMM_ATOMIC_LOAD(&internal_registry[i0].pmm, LIBXSMM_ATOMIC_RELAXED);\n#   else\n                fix_entry = internal_registry[i0];\n#   endif\n                LIBXSMM_ASSERT(NULL != fix_entry.ptr_const);\n                if (0 == (LIBXSMM_HASH_COLLISION & fix_entry.uval)) {\n                  fix_entry.uval |= LIBXSMM_HASH_COLLISION; /* mark current entry as collision */\n#   if (1 < INTERNAL_REGLOCK_MAXN)\n                  LIBXSMM_ATOMIC_STORE(&internal_registry[i0].pmm, fix_entry.pmm, LIBXSMM_ATOMIC_RELAXED);\n#   else\n                  internal_registry[i0] = fix_entry;\n#   endif\n                }\n              }\n# endif\n            }\n            /* leave here even in case of a build-error; do not use break (inside of locked region) */\n            diff = 0;\n          }\n          INTERNAL_FIND_CODE_UNLOCK(lock);\n          if (0 != diff) { /* acquire registry slot */\n            if (0 == mode) { /* initial condition */\n              mode = 2; /* continue to linearly search for an empty slot */\n              i0 = i; /* keep current position on record */\n            }\n            do { /* continue to linearly search for an available slot */\n              i = LIBXSMM_MOD2(i + 1, LIBXSMM_CAPACITY_REGISTRY);\n              if (NULL == internal_registry[i].ptr_const) break;\n            } while (i != i0);\n            if (i == i0) { /* out of capacity (no registry slot available) */\n              diff = 0; /* do not use break if inside of locked region */\n            }\n            flux_entry.pmm = NULL; /* no result */\n          }\n        }\n        else /* JIT-code generation not available */\n#endif\n        { /* leave the dispatch loop */\n#if !defined(NDEBUG) && (0 != LIBXSMM_JIT)\n          build = EXIT_FAILURE;\n#endif\n          flux_entry.pmm = NULL;\n          diff = 0;\n        }\n        if (((int)LIBXSMM_KERNEL_KIND_MATMUL) == desc->kind) {\n          internal_update_mmstatistic(&desc->gemm.desc, 1/*try*/, 0, 0, 0);\n        }\n      }\n    } while (0 != diff);\n#if defined(LIBXSMM_CACHE_MAXSIZE) && (0 < (LIBXSMM_CACHE_MAXSIZE))\n    if (NULL != flux_entry.ptr_const) { /* keep code version on record (cache) */\n      LIBXSMM_ASSERT(0 == diff);\n# if !defined(LIBXSMM_NTHREADS_USE) || defined(LIBXSMM_CACHE_CLEAR)\n      if (cache->entry.id == libxsmm_ninit) /* maintain cache */\n# endif\n      {\n        if (cache->entry.size < (LIBXSMM_CACHE_MAXSIZE)) { /* grow */\n          INTERNAL_FIND_CODE_CACHE_GROW(cache_index, cache->entry.size);\n          LIBXSMM_ASSERT(cache->entry.size <= LIBXSMM_CACHE_MAXSIZE);\n        }\n        else { /* evict */\n          INTERNAL_FIND_CODE_CACHE_EVICT(cache_index, cache->entry.size, cache->entry.hit);\n        }\n      }\n# if !defined(LIBXSMM_NTHREADS_USE) || defined(LIBXSMM_CACHE_CLEAR)\n      else { /* reset cache */\n        cache->entry.id = libxsmm_ninit;\n        cache->entry.size = 1;\n        cache_index = 0;\n      }\n# endif\n      LIBXSMM_ASSIGN127(cache->entry.keys + cache_index, desc);\n      cache->entry.code[cache_index] = flux_entry;\n      cache->entry.hit = cache_index;\n    }\n#endif\n  }\n#if defined(LIBXSMM_HASH_COLLISION)\n  flux_entry.uval &= ~(LIBXSMM_CODE_STATIC | LIBXSMM_HASH_COLLISION); /* clear non-JIT and collision flag */\n#else\n  flux_entry.uval &= ~LIBXSMM_CODE_STATIC; /* clear non-JIT flag */\n#endif\n#if (0 != LIBXSMM_JIT)\n  assert(LIBXSMM_BUILD_KIND_GEMM != desc->kind || NULL != flux_entry.ptr_const || EXIT_SUCCESS != build || 1 == internal_reglock_count); /*!LIBXSMM_ASSERT*/\n#endif\n  return flux_entry;\n}\n\n\nLIBXSMM_API const libxsmm_descriptor* libxsmm_get_kernel_info(libxsmm_code_pointer code, size_t* size)\n{\n  const libxsmm_descriptor* result;\n  int flags = LIBXSMM_MALLOC_FLAG_X;\n  void* extra = NULL;\n  if (NULL != size) *size = 0;\n  if (NULL != code.ptr_const && NULL != internal_registry && NULL != internal_registry_keys\n    && EXIT_SUCCESS == libxsmm_get_malloc_xinfo(code.ptr_const, size, &flags, &extra)\n    && NULL != extra && *((const unsigned int*)extra) < (LIBXSMM_CAPACITY_REGISTRY)\n#if defined(LIBXSMM_HASH_COLLISION)\n    && code.uval == (~LIBXSMM_HASH_COLLISION & internal_registry[*((const unsigned int*)extra)].uval)\n#else\n    && code.ptr_const == internal_registry[*((const unsigned int*)extra)].ptr_const\n#endif\n    && internal_registry_keys[*((const unsigned int*)extra)].kind < LIBXSMM_KERNEL_KIND_INVALID)\n  {\n    result = internal_registry_keys + *((const unsigned int*)extra);\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_kernel_kind(const void* kernel, libxsmm_kernel_kind* kind)\n{\n  const libxsmm_descriptor* info;\n  libxsmm_code_pointer code;\n  int result;\n  code.ptr_const = kernel;\n  info = libxsmm_get_kernel_info(code, NULL/*code_size*/);\n  if (NULL != info && NULL != kind) {\n    *kind = (libxsmm_kernel_kind)info->kind;\n    result = EXIT_SUCCESS;\n  }\n  else {\n    if (NULL != kind) *kind = LIBXSMM_KERNEL_KIND_INVALID;\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_mmkernel_info(libxsmm_xmmfunction kernel, libxsmm_mmkernel_info* info, size_t* code_size)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xgemm = kernel;\n  if (NULL != info || NULL != code_size) {\n    const libxsmm_descriptor *const kernel_info = libxsmm_get_kernel_info(code, code_size);\n    if (NULL != kernel_info && LIBXSMM_KERNEL_KIND_MATMUL == kernel_info->kind) {\n      if (NULL != info) {\n        info->iprecision = (libxsmm_gemm_precision)LIBXSMM_GETENUM_INP(kernel_info->gemm.desc.datatype);\n        info->oprecision = (libxsmm_gemm_precision)LIBXSMM_GETENUM_OUT(kernel_info->gemm.desc.datatype);\n        info->prefetch = (libxsmm_gemm_prefetch_type)kernel_info->gemm.desc.prefetch;\n        info->flags = kernel_info->gemm.desc.flags;\n        info->lda = kernel_info->gemm.desc.lda;\n        info->ldb = kernel_info->gemm.desc.ldb;\n        info->ldc = kernel_info->gemm.desc.ldc;\n        info->m = kernel_info->gemm.desc.m;\n        info->n = kernel_info->gemm.desc.n;\n        info->k = kernel_info->gemm.desc.k;\n      }\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        if (NULL == code.ptr_const) {\n          fprintf(stderr, \"LIBXSMM ERROR: NULL-kernel cannot be inspected!\\n\");\n        }\n        else {\n          fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n        }\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_transkernel_info(libxsmm_xtransfunction kernel, libxsmm_transkernel_info* info, size_t* code_size)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xtrans = kernel;\n  if (NULL != info || 0 != code_size) {\n    const libxsmm_descriptor *const kernel_info = libxsmm_get_kernel_info(code, code_size);\n    if (NULL != kernel_info && LIBXSMM_KERNEL_KIND_TRANS == kernel_info->kind) {\n      if (NULL != info) {\n        info->typesize = kernel_info->trans.desc.typesize;\n        info->ldo = kernel_info->trans.desc.ldo;\n        info->m = kernel_info->trans.desc.m;\n        info->n = kernel_info->trans.desc.n;\n      }\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_mcopykernel_info(libxsmm_xmcopyfunction kernel, libxsmm_mcopykernel_info* info, size_t* code_size)\n{\n  libxsmm_code_pointer code;\n  static int error_once = 0;\n  int result;\n  code.xmatcopy = kernel;\n  if (NULL != info || 0 != code_size) {\n    const libxsmm_descriptor *const kernel_info = libxsmm_get_kernel_info(code, code_size);\n    if (NULL != kernel_info && LIBXSMM_KERNEL_KIND_MCOPY == kernel_info->kind) {\n      if (NULL != info) {\n        info->typesize = kernel_info->mcopy.desc.typesize;\n        info->prefetch = kernel_info->mcopy.desc.prefetch;\n        info->flags = kernel_info->mcopy.desc.flags;\n        info->ldi = kernel_info->mcopy.desc.ldi;\n        info->ldo = kernel_info->mcopy.desc.ldo;\n        info->m = kernel_info->mcopy.desc.m;\n        info->n = kernel_info->mcopy.desc.n;\n      }\n      result = EXIT_SUCCESS;\n    }\n    else {\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: invalid kernel cannot be inspected!\\n\");\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_registry_info(libxsmm_registry_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (0 != info) {\n    LIBXSMM_INIT\n    if (0 != internal_registry) {\n      size_t i;\n      LIBXSMM_MEMZERO127(info); /* info->nstatic = 0; info->size = 0; */\n      info->nbytes = (LIBXSMM_CAPACITY_REGISTRY) * (sizeof(libxsmm_code_pointer) + sizeof(libxsmm_descriptor));\n      info->capacity = LIBXSMM_CAPACITY_REGISTRY;\n#if defined(LIBXSMM_CACHE_MAXSIZE)\n      info->ncache = LIBXSMM_CACHE_MAXSIZE;\n#else\n      info->ncache = 0;\n#endif\n      for (i = 0; i < (LIBXSMM_CAPACITY_REGISTRY); ++i) {\n        libxsmm_code_pointer code = internal_registry[i];\n        if (0 != code.ptr_const && EXIT_SUCCESS == result) {\n          if (0 == (LIBXSMM_CODE_STATIC & code.uval)) { /* check for allocated/generated JIT-code */\n            size_t buffer_size = 0;\n            void* buffer = 0;\n#if defined(LIBXSMM_HASH_COLLISION)\n            code.uval &= ~LIBXSMM_HASH_COLLISION; /* clear collision flag */\n#endif\n            result = libxsmm_get_malloc_xinfo(code.ptr_const, &buffer_size, NULL/*flags*/, &buffer);\n            if (EXIT_SUCCESS == result) {\n              info->nbytes += (unsigned int)(buffer_size + (((char*)code.ptr_const) - (char*)buffer));\n            }\n          }\n          else {\n            ++info->nstatic;\n          }\n          ++info->size;\n        }\n      }\n    }\n    else {\n      result = EXIT_FAILURE;\n    }\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_xmmdispatch(const libxsmm_gemm_descriptor* descriptor)\n{\n  libxsmm_xmmfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.gemm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_MATMUL;\n    if (0 != (0x80 & descriptor->prefetch)) { /* \"sign\"-bit of byte-value is set */\n      wrap.gemm.desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n    }\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xgemm;\n#if defined(_DEBUG)\n    if (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity && INT_MAX != libxsmm_verbosity && NULL != result.xmm) {\n      LIBXSMM_STDIO_ACQUIRE();\n      fprintf(stderr, \"\\nLIBXSMM: \");\n      libxsmm_gemm_xprint(stderr, result, NULL/*a*/, NULL/*b*/, NULL/*c*/);\n      LIBXSMM_STDIO_RELEASE();\n    }\n#endif\n  }\n  else { /* quietly accept NULL-descriptor */\n    result.xmm = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction libxsmm_dmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.dmm;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction libxsmm_smmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.smm;\n}\n\n\nLIBXSMM_API libxsmm_wimmfunction libxsmm_wimmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const int* alpha, const int* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_wigemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.wimm;\n}\n\n\nLIBXSMM_API libxsmm_wsmmfunction libxsmm_wsmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_wsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.wsmm;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction libxsmm_bsmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bsmm;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction libxsmm_bmmdispatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob, m, n, k,\n    NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? m : k),\n    NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? k : n),\n    NULL != ldc ? *ldc : m, NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bmm;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction_reducebatch libxsmm_dmmdispatch_reducebatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc, const double* alpha, const double* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_dgemm_descriptor_init(&blob,\n    m, n, k, NULL != lda ? *lda : m, NULL != ldb ? *ldb : k, NULL != ldc ? *ldc : m,\n    NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.dmr;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction_reducebatch libxsmm_smmdispatch_reducebatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc, const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_sgemm_descriptor_init(&blob,\n    m, n, k, NULL != lda ? *lda : m, NULL != ldb ? *ldb : k, NULL != ldc ? *ldc : m,\n    NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.smr;\n}\n\n\nLIBXSMM_API libxsmm_bsmmfunction_reducebatch libxsmm_bsmmdispatch_reducebatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc, const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bsgemm_descriptor_init(&blob,\n    m, n, k, NULL != lda ? *lda : m, NULL != ldb ? *ldb : k, NULL != ldc ? *ldc : m,\n    NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bsmr;\n}\n\n\nLIBXSMM_API libxsmm_bmmfunction_reducebatch libxsmm_bmmdispatch_reducebatch(libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc, const float* alpha, const float* beta, const int* flags, const int* prefetch)\n{\n  const int gemm_flags = (NULL == flags ? LIBXSMM_FLAGS : *flags);\n  libxsmm_descriptor_blob blob;\n  const libxsmm_gemm_descriptor *const desc = libxsmm_bgemm_descriptor_init(&blob,\n    m, n, k, NULL != lda ? *lda : m, NULL != ldb ? *ldb : k, NULL != ldc ? *ldc : m,\n    NULL != alpha ? *alpha : LIBXSMM_ALPHA, NULL != beta ? *beta : LIBXSMM_BETA,\n    gemm_flags | LIBXSMM_GEMM_FLAG_BATCH_REDUCE, libxsmm_get_gemm_xprefetch(prefetch));\n  /*const*/ libxsmm_xmmfunction result = libxsmm_xmmdispatch(desc);\n  return result.bmr;\n}\n\n\nLIBXSMM_API libxsmm_xmcopyfunction libxsmm_dispatch_mcopy(const libxsmm_mcopy_descriptor* descriptor)\n{\n  libxsmm_xmcopyfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n    LIBXSMM_INIT\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.mcopy.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_MCOPY;\n#if defined(_WIN32) || defined(__CYGWIN__) /* TODO: full support for Windows calling convention */\n    wrap.mcopy.desc.prefetch = 0;\n#endif\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xmatcopy;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_xtransfunction libxsmm_dispatch_trans(const libxsmm_trans_descriptor* descriptor)\n{\n  libxsmm_xtransfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n    LIBXSMM_INIT\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.trans.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_TRANS;\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xtrans;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_pgemm_xfunction libxsmm_dispatch_pgemm(const libxsmm_pgemm_descriptor* descriptor)\n{\n  libxsmm_trmm_xfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n    LIBXSMM_INIT\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.pgemm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_PGEMM;\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xpgemm;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_getrf_xfunction libxsmm_dispatch_getrf(const libxsmm_getrf_descriptor* descriptor)\n{\n  libxsmm_trmm_xfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n    LIBXSMM_INIT\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.getrf.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_GETRF;\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xgetrf;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_trmm_xfunction libxsmm_dispatch_trmm(const libxsmm_trmm_descriptor* descriptor)\n{\n  libxsmm_trmm_xfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n    LIBXSMM_INIT\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.trmm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_TRMM;\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xtrmm;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_trsm_xfunction libxsmm_dispatch_trsm(const libxsmm_trsm_descriptor* descriptor)\n{\n  libxsmm_trsm_xfunction result;\n  if (NULL != descriptor) {\n    libxsmm_descriptor wrap;\n    LIBXSMM_INIT\n#if defined(LIBXSMM_UNPACKED) /* TODO: investigate (CCE) */\n    LIBXSMM_MEMSET127(&wrap, 0, sizeof(*descriptor));\n#endif\n    LIBXSMM_ASSIGN127(&wrap.trsm.desc, descriptor);\n    wrap.kind = LIBXSMM_KERNEL_KIND_TRSM;\n    result = internal_find_code(&wrap, sizeof(*descriptor)).xtrsm;\n  }\n  else {\n    result = NULL;\n  }\n  return result;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_xcsr_soa(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* row_ptr, const unsigned int* column_idx, const void* values)\n{\n  libxsmm_code_pointer result = { 0 };\n  if (NULL != descriptor && NULL != row_ptr && NULL != column_idx && NULL != values) {\n    libxsmm_csr_soa_descriptor srsoa;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      srsoa.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      srsoa.gemm = &desc;\n    }\n    srsoa.row_ptr = row_ptr;\n    srsoa.column_idx = column_idx;\n    srsoa.values = values;\n    request.descriptor.srsoa = &srsoa;\n    request.kind = LIBXSMM_BUILD_KIND_SRSOA;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_xcsc_soa(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* column_ptr, const unsigned int* row_idx, const void* values)\n{\n  libxsmm_code_pointer result = { 0 };\n  if (NULL != descriptor && NULL != column_ptr && NULL != row_idx && NULL != values) {\n    libxsmm_csc_soa_descriptor scsoa;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      scsoa.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      scsoa.gemm = &desc;\n    }\n    scsoa.column_ptr = column_ptr;\n    scsoa.row_idx = row_idx;\n    scsoa.values = values;\n    request.descriptor.scsoa = &scsoa;\n    request.kind = LIBXSMM_BUILD_KIND_SCSOA;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_pgemm_ac_rm(const libxsmm_gemm_descriptor* descriptor)\n{\n  libxsmm_code_pointer result = { 0 };\n  if (NULL != descriptor) {\n    libxsmm_pgemm_ac_rm_descriptor pgemmacrm;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      pgemmacrm.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      pgemmacrm.gemm = &desc;\n    }\n    request.descriptor.pgemmacrm = &pgemmacrm;\n    request.kind = LIBXSMM_BUILD_KIND_PGEMMRMAC;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_xmmfunction libxsmm_create_pgemm_bc_rm(const libxsmm_gemm_descriptor* descriptor)\n{\n  libxsmm_code_pointer result = { 0 };\n  if (NULL != descriptor) {\n    libxsmm_pgemm_bc_rm_descriptor pgemmbcrm;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      pgemmbcrm.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      pgemmbcrm.gemm = &desc;\n    }\n    request.descriptor.pgemmbcrm = &pgemmbcrm;\n    request.kind = LIBXSMM_BUILD_KIND_PGEMMRMBC;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm;\n}\n\n\nLIBXSMM_API libxsmm_dmmfunction libxsmm_create_dcsr_reg(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* row_ptr, const unsigned int* column_idx, const double* values)\n{\n  libxsmm_code_pointer result = { 0 };\n  if (NULL != descriptor && NULL != row_ptr && NULL != column_idx && NULL != values) {\n    libxsmm_csr_reg_descriptor sreg;\n    libxsmm_build_request request;\n    libxsmm_gemm_descriptor desc;\n    if (0 == (0x80 & descriptor->prefetch)) {\n      sreg.gemm = descriptor;\n    }\n    else { /* \"sign\"-bit of byte-value is set */\n      LIBXSMM_ASSIGN127(&desc, descriptor);\n      desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n      sreg.gemm = &desc;\n    }\n    sreg.row_ptr = row_ptr;\n    sreg.column_idx = column_idx;\n    sreg.values = values;\n    request.descriptor.sreg = &sreg;\n    request.kind = LIBXSMM_BUILD_KIND_SREG;\n    libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n  }\n  return result.xgemm.dmm;\n}\n\n\nLIBXSMM_API libxsmm_smmfunction libxsmm_create_scsr_reg(const libxsmm_gemm_descriptor* descriptor,\n  const unsigned int* row_ptr, const unsigned int* column_idx, const float* values)\n{\n  libxsmm_code_pointer result = { 0 };\n  if (NULL != descriptor && NULL != row_ptr && NULL != column_idx && NULL != values) {\n    libxsmm_csr_reg_descriptor sreg;\n    libxsmm_build_request request;\n    const unsigned int n = row_ptr[descriptor->m];\n    double *const d_values = (double*)(0 != n ? malloc(n * sizeof(double)) : NULL);\n    if (NULL != d_values) {\n      libxsmm_gemm_descriptor desc;\n      unsigned int i;\n      /* we need to copy the values into a double precision buffer */\n      for (i = 0; i < n; ++i) d_values[i] = (double)values[i];\n      if (0 == (0x80 & descriptor->prefetch)) {\n        sreg.gemm = descriptor;\n      }\n      else { /* \"sign\"-bit of byte-value is set */\n        LIBXSMM_ASSIGN127(&desc, descriptor);\n        desc.prefetch = (unsigned char)libxsmm_get_gemm_prefetch(LIBXSMM_PREFETCH_AUTO);\n        sreg.gemm = &desc;\n      }\n      sreg.row_ptr = row_ptr;\n      sreg.column_idx = column_idx;\n      sreg.values = d_values;\n      request.descriptor.sreg = &sreg;\n      request.kind = LIBXSMM_BUILD_KIND_SREG;\n      libxsmm_build(&request, LIBXSMM_CAPACITY_REGISTRY/*not managed*/, &result);\n      free(d_values);\n    }\n  }\n  return result.xgemm.smm;\n}\n\n\nLIBXSMM_API void libxsmm_release_kernel(const void* jit_kernel)\n{\n  if (NULL != jit_kernel) {\n    static int error_once = 0;\n    void* extra = NULL;\n    LIBXSMM_INIT\n    if (EXIT_SUCCESS == libxsmm_get_malloc_xinfo(jit_kernel, NULL/*size*/, NULL/*flags*/, &extra) && NULL != extra) {\n      const unsigned int regindex = *((const unsigned int*)extra);\n      if ((LIBXSMM_CAPACITY_REGISTRY) <= regindex) {\n        libxsmm_xfree(jit_kernel, 0/*no check*/);\n      }\n      else\n#if !defined(LIBXSMM_ENABLE_DEREG)\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n       && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM WARNING: attempt to unregister a JIT-kernel!\\n\");\n      }\n#else\n      { /* unregister kernel */\n        internal_registry[regindex].pmm = NULL;\n# if !defined(NDEBUG)\n        LIBXSMM_MEMZERO127(internal_registry_keys + regindex);\n# endif\n        libxsmm_xfree(jit_kernel, 0/*no check*/);\n      }\n#endif\n    }\n    else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: failed to release kernel!\\n\");\n    }\n  }\n}\n\n\n#if defined(LIBXSMM_BUILD) && (!defined(LIBXSMM_NOFORTRAN) || defined(__clang_analyzer__))\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_init)(void);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_init)(void)\n{\n  libxsmm_init();\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_finalize)(void);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_finalize)(void)\n{\n  libxsmm_finalize();\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_release_kernel)(const void** /*jit_kernel*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_release_kernel)(const void** jit_kernel)\n{\n#if !defined(NDEBUG)\n  if (NULL != jit_kernel)\n#endif\n  {\n    libxsmm_release_kernel(*jit_kernel);\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument passed into libxsmm_release_kernel!\\n\");\n    }\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch2)(intptr_t* /*fn*/, const int* /*iprec*/, const int* /*oprec*/,\n  const libxsmm_blasint* /*m*/, const libxsmm_blasint* /*n*/, const libxsmm_blasint* /*k*/,\n  const libxsmm_blasint* /*lda*/, const libxsmm_blasint* /*ldb*/, const libxsmm_blasint* /*ldc*/,\n  const void* /*alpha*/, const void* /*beta*/, const int* /*flags*/, const int* /*prefetch*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch2)(intptr_t* fn, const int* iprec, const int* oprec,\n  const libxsmm_blasint* m, const libxsmm_blasint* n, const libxsmm_blasint* k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const void* alpha, const void* beta, const int* flags, const int* prefetch)\n{\n#if !defined(NDEBUG)\n  if (NULL != fn && NULL != m\n    && (NULL == iprec || (0 <= *iprec && *iprec < LIBXSMM_DATATYPE_UNSUPPORTED))\n    && (NULL == oprec || (0 <= *oprec && *oprec < LIBXSMM_DATATYPE_UNSUPPORTED)))\n#endif\n  {\n    const int gemm_flags = (NULL != flags ? *flags : LIBXSMM_FLAGS);\n    const libxsmm_gemm_descriptor* descriptor;\n    libxsmm_gemm_prefetch_type gemm_prefetch;\n    libxsmm_descriptor_blob blob;\n    libxsmm_code_pointer result;\n#if !defined(NDEBUG)\n    const libxsmm_gemm_precision itype = (NULL != iprec ? ((libxsmm_gemm_precision)*iprec) : LIBXSMM_GEMM_PRECISION_F64);\n    const libxsmm_gemm_precision otype = (NULL != oprec ? ((libxsmm_gemm_precision)*oprec) : itype);\n    const libxsmm_blasint kk = *(NULL != k ? k : m), nn = (NULL != n ? *n : kk);\n#else\n    const libxsmm_gemm_precision itype = (libxsmm_gemm_precision)*iprec, otype = (libxsmm_gemm_precision)*oprec;\n    const libxsmm_blasint kk = *k, nn = *n;\n#endif\n    LIBXSMM_PRAGMA_FORCEINLINE\n    gemm_prefetch = libxsmm_get_gemm_xprefetch(prefetch);\n    LIBXSMM_PRAGMA_FORCEINLINE\n    descriptor = libxsmm_gemm_descriptor_init2(&blob, itype, otype, *m, nn, kk,\n        NULL != lda ? *lda : (0 == (LIBXSMM_GEMM_FLAG_TRANS_A & gemm_flags) ? *m : kk),\n        NULL != ldb ? *ldb : (0 == (LIBXSMM_GEMM_FLAG_TRANS_B & gemm_flags) ? kk : nn),\n      *(NULL != ldc ? ldc : m), alpha, beta, gemm_flags, gemm_prefetch);\n#if !defined(NDEBUG)\n    if (NULL != descriptor)\n#endif\n    {\n      LIBXSMM_PRAGMA_FORCEINLINE\n      result.xgemm = libxsmm_xmmdispatch(descriptor);\n      *fn = result.ival;\n    }\n#if !defined(NDEBUG)\n    else { /* quiet */\n      *fn = 0;\n    }\n#endif\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid argument passed into libxsmm_xmmdispatch!\\n\");\n    }\n    if (NULL != fn) *fn = 0;\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch)(intptr_t* /*fn*/, const int* /*precision*/,\n  const libxsmm_blasint* /*m*/, const libxsmm_blasint* /*n*/, const libxsmm_blasint* /*k*/,\n  const libxsmm_blasint* /*lda*/, const libxsmm_blasint* /*ldb*/, const libxsmm_blasint* /*ldc*/,\n  const void* /*alpha*/, const void* /*beta*/, const int* /*flags*/, const int* /*prefetch*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmdispatch)(intptr_t* fn, const int* precision,\n  const libxsmm_blasint* m, const libxsmm_blasint* n, const libxsmm_blasint* k,\n  const libxsmm_blasint* lda, const libxsmm_blasint* ldb, const libxsmm_blasint* ldc,\n  const void* alpha, const void* beta, const int* flags, const int* prefetch)\n{\n  LIBXSMM_FSYMBOL(libxsmm_xmmdispatch2)(fn, precision, precision, m, n, k, lda, ldb, ldc, alpha, beta, flags, prefetch);\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_abc)(\n  const libxsmm_xmmfunction* /*fn*/, const void* /*a*/, const void* /*b*/, void* /*c*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_abc)(\n  const libxsmm_xmmfunction* fn, const void* a, const void* b, void* c)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != fn && NULL != a && NULL != b && NULL != c)\n#endif\n  {\n#if !defined(NDEBUG)\n    if (NULL != fn->xmm)\n#endif\n    {\n      fn->xmm(a, b, c);\n    }\n#if !defined(NDEBUG)\n    else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: NULL-function passed into libxsmm_xmmcall_abc!\\n\");\n    }\n#endif\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xmmcall_abc specified!\\n\");\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_prf)(\n  const libxsmm_xmmfunction* /*fn*/, const void* /*a*/, const void* /*b*/, void* /*c*/,\n  const void* /*pa*/, const void* /*pb*/, const void* /*pc*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall_prf)(\n  const libxsmm_xmmfunction* fn, const void* a, const void* b, void* c,\n  const void* pa, const void* pb, const void* pc)\n{\n#if !defined(NDEBUG)\n  static int error_once = 0;\n  if (NULL != fn && NULL != a && NULL != b && NULL != c)\n#endif\n  {\n#if !defined(NDEBUG)\n    if (NULL != fn->xmm)\n#endif\n    {\n      fn->xmm(a, b, c, pa, pb, pc);\n    }\n#if !defined(NDEBUG)\n    else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: NULL-function passed into libxsmm_xmmcall_prf!\\n\");\n    }\n#endif\n  }\n#if !defined(NDEBUG)\n  else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM ERROR: invalid arguments for libxsmm_xmmcall_prf specified!\\n\");\n  }\n#endif\n}\n\n\n/* implementation provided for Fortran 77 compatibility */\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall)(\n  const libxsmm_xmmfunction* /*fn*/, const void* /*a*/, const void* /*b*/, void* /*c*/,\n  const void* /*pa*/, const void* /*pb*/, const void* /*pc*/);\nLIBXSMM_API void LIBXSMM_FSYMBOL(libxsmm_xmmcall)(\n  const libxsmm_xmmfunction* fn, const void* a, const void* b, void* c,\n  const void* pa, const void* pb, const void* pc)\n{\n  LIBXSMM_FSYMBOL(libxsmm_xmmcall_prf)(fn, a, b, c, pa, pb, pc);\n}\n\n#endif /*defined(LIBXSMM_BUILD) && (!defined(LIBXSMM_NOFORTRAN) || defined(__clang_analyzer__))*/\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/src/libxsmm_gemm.h": "/******************************************************************************\n** Copyright (c) 2015-2019, Intel Corporation                                **\n** All rights reserved.                                                      **\n**                                                                           **\n** Redistribution and use in source and binary forms, with or without        **\n** modification, are permitted provided that the following conditions        **\n** are met:                                                                  **\n** 1. Redistributions of source code must retain the above copyright         **\n**    notice, this list of conditions and the following disclaimer.          **\n** 2. Redistributions in binary form must reproduce the above copyright      **\n**    notice, this list of conditions and the following disclaimer in the    **\n**    documentation and/or other materials provided with the distribution.   **\n** 3. Neither the name of the copyright holder nor the names of its          **\n**    contributors may be used to endorse or promote products derived        **\n**    from this software without specific prior written permission.          **\n**                                                                           **\n** THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS       **\n** \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT         **\n** LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR     **\n** A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT      **\n** HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,    **\n** SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED  **\n** TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR    **\n** PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF    **\n** LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING      **\n** NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS        **\n** SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.              **\n******************************************************************************/\n/* Hans Pabst (Intel Corp.)\n******************************************************************************/\n#ifndef LIBXSMM_GEMM_H\n#define LIBXSMM_GEMM_H\n\n#include \"libxsmm_main.h\"\n\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n#endif\n#if !defined(LIBXSMM_BLAS_WRAP_DYNAMIC) && defined(LIBXSMM_INTERCEPT_DYNAMIC) && (!defined(__BLAS) || (0 != __BLAS))\n# define LIBXSMM_BLAS_WRAP_DYNAMIC\n#endif\n#if defined(LIBXSMM_BLAS_WRAP_DYNAMIC)\n# include <dlfcn.h>\n#endif\n#include <limits.h>\n#include <stdio.h>\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(pop)\n#endif\n\n#if !defined(LIBXSMM_GEMM_CHECK) && !defined(NDEBUG)\n# define LIBXSMM_GEMM_CHECK\n#endif\n#if !defined(LIBXSMM_GEMM_LOCK)\n# define LIBXSMM_GEMM_LOCK LIBXSMM_LOCK_DEFAULT\n#endif\n#if !defined(LIBXSMM_GEMM_MMBATCH_SCALE)\n# define LIBXSMM_GEMM_MMBATCH_SCALE 1.5\n#endif\n#if !defined(LIBXSMM_GEMM_MMBATCH_VERBOSITY)\n# define LIBXSMM_GEMM_MMBATCH_VERBOSITY ((LIBXSMM_VERBOSITY_HIGH) + 1)\n#endif\n#if !defined(LIBXSMM_GEMM_NPARGROUPS)\n# define LIBXSMM_GEMM_NPARGROUPS 128\n#endif\n\n#if !defined(LIBXSMM_WRAP) && defined(LIBXSMM_BUILD) && \\\n    (defined(LIBXSMM_CONFIG_WRAP) && 0 != (LIBXSMM_CONFIG_WRAP)) && \\\n    (defined(LIBXSMM_BLAS_WRAP_DYNAMIC) || !defined(NDEBUG) || defined(_WIN32)) /* debug */\n# define LIBXSMM_WRAP LIBXSMM_CONFIG_WRAP\n#endif\n\n/** Undefine (disarm) MKL's DIRECT_CALL macros. */\n#if (defined(MKL_DIRECT_CALL_SEQ) || defined(MKL_DIRECT_CALL))\n# if defined(sgemm_)\n#   undef sgemm_\n# endif\n# if defined(dgemm_)\n#   undef dgemm_\n# endif\n#endif\n\n#if defined(LIBXSMM_BUILD)\n# define LIBXSMM_BLAS_WRAPPER_STATIC1(TYPE, KIND, ORIGINAL) if (NULL == (ORIGINAL)) { \\\n    ORIGINAL = LIBXSMM_FSYMBOL(LIBXSMM_CONCATENATE(__real_, LIBXSMM_TPREFIX(TYPE, KIND))); \\\n  }\n# define LIBXSMM_BLAS_WRAPPER_STATIC0 LIBXSMM_BLAS_WRAPPER_STATIC1\n#else\n# define LIBXSMM_BLAS_WRAPPER_STATIC1(TYPE, KIND, ORIGINAL) if (NULL == (ORIGINAL)) { \\\n    ORIGINAL = (LIBXSMM_BLAS_FNTYPE(TYPE, KIND))LIBXSMM_BLAS_SYMBOL(TYPE, KIND); \\\n  }\n# define LIBXSMM_BLAS_WRAPPER_STATIC0(TYPE, KIND, ORIGINAL)\n#endif\n#define LIBXSMM_BLAS_WRAPPER_STATIC(CONDITION, TYPE, KIND, ORIGINAL) \\\n  LIBXSMM_CONCATENATE(LIBXSMM_BLAS_WRAPPER_STATIC, CONDITION)(TYPE, KIND, ORIGINAL)\n\n#if defined(LIBXSMM_BLAS_WRAP_DYNAMIC)\n# define LIBXSMM_BLAS_WRAPPER_DYNAMIC(TYPE, KIND, ORIGINAL, NEXT) { \\\n    union { const void* pfin; \\\n      LIBXSMM_BLAS_FNTYPE(TYPE, KIND) (*chain)(void); /* chain */ \\\n      LIBXSMM_BLAS_FNTYPE(TYPE, KIND) pfout; \\\n    } libxsmm_blas_wrapper_dynamic_ /*= { 0 }*/; \\\n    dlerror(); /* clear an eventual error status */ \\\n    libxsmm_blas_wrapper_dynamic_.chain = NEXT; \\\n    libxsmm_blas_wrapper_dynamic_.pfin = ((NULL == libxsmm_blas_wrapper_dynamic_.pfin) ? \\\n      dlsym(RTLD_NEXT, \"libxsmm_original_\" LIBXSMM_STRINGIFY(LIBXSMM_TPREFIX(TYPE, KIND))) : NULL); \\\n    if (NULL == libxsmm_blas_wrapper_dynamic_.pfout || NULL != dlerror() || NULL == libxsmm_blas_wrapper_dynamic_.chain()) { \\\n      libxsmm_blas_wrapper_dynamic_.pfin = dlsym(RTLD_NEXT, LIBXSMM_STRINGIFY(LIBXSMM_BLAS_SYMBOL(TYPE, KIND))); \\\n      /*LIBXSMM_ATOMIC_STORE(&(ORIGINAL), libxsmm_blas_wrapper_dynamic_.pfout, LIBXSMM_ATOMIC_RELAXED);*/ \\\n      ORIGINAL = (NULL == dlerror() ? libxsmm_blas_wrapper_dynamic_.pfout : NULL); \\\n    } \\\n  }\n#else\n# define LIBXSMM_BLAS_WRAPPER_DYNAMIC(TYPE, KIND, ORIGINAL, NEXT)\n#endif\n\n#define LIBXSMM_BLAS_WRAPPER(CONDITION, TYPE, KIND, ORIGINAL, NEXT) if (NULL == (ORIGINAL)) { \\\n  LIBXSMM_BLAS_WRAPPER_DYNAMIC(TYPE, KIND, ORIGINAL, NEXT); \\\n  LIBXSMM_BLAS_WRAPPER_STATIC(CONDITION, TYPE, KIND, ORIGINAL); \\\n}\n\n\n/** Provides GEMM functions available via BLAS; NOT thread-safe. */\nLIBXSMM_API_INTERN void libxsmm_gemm_init(int archid);\n\n/** Finalizes the GEMM facility; NOT thread-safe. */\nLIBXSMM_API_INTERN void libxsmm_gemm_finalize(void);\n\nLIBXSMM_API_INTERN int libxsmm_gemm_prefetch2uid(libxsmm_gemm_prefetch_type prefetch);\nLIBXSMM_API_INTERN libxsmm_gemm_prefetch_type libxsmm_gemm_uid2prefetch(int uid);\n\n#if defined(LIBXSMM_BUILD)\n#if defined(LIBXSMM_BUILD_EXT)\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_dgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_sgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_dgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_sgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_dgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemv));\nLIBXSMM_APIEXT void LIBXSMM_FSYMBOL(__wrap_sgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemv));\nLIBXSMM_APIEXT void __wrap_dgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_APIEXT void __wrap_sgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\n#endif\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_dgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_sgemm_batch)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_dgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_sgemm)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_dgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemv));\nLIBXSMM_API void LIBXSMM_FSYMBOL(__real_sgemv)(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemv));\nLIBXSMM_API void __real_dgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, double, gemm_batch));\nLIBXSMM_API void __real_sgemm_batch(LIBXSMM_BLAS_SYMBOL_SIGNATURE(const*, *, float, gemm_batch));\n#endif\n\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_GEMM_CONST*, *, double, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_CDECL(LIBXSMM_GEMM_CONST*, *, double, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_GEMM_CONST*, *, float, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_CDECL(LIBXSMM_GEMM_CONST*, *, float, gemm_batch);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_GEMM_CONST*, *, double, gemm);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_GEMM_CONST*, *, float, gemm);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_GEMM_CONST*, *, double, gemv);\nLIBXSMM_BLAS_SYMBOL_FDECL(LIBXSMM_GEMM_CONST*, *, float, gemv);\n\nLIBXSMM_EXTERN_C struct LIBXSMM_RETARGETABLE libxsmm_gemm_handle {\n  libxsmm_code_pointer copy_a, copy_b, copy_i, copy_o;\n  libxsmm_xmmfunction kernel[2];\n  unsigned int m, n, k, lda, ldb, ldc;\n  /* kernel size (tile) */\n  unsigned int km, kn, kk;\n  /* tile size per task */\n  unsigned int dm, dn, dk;\n  unsigned int itypesize, otypesize;\n  /* number of tasks per direction */\n  unsigned int mt, nt, kt;\n  int gemm_flags, flags;\n};\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE libxsmm_mmbatch_item {\n  struct {\n    const void *a, *b;\n    void *c;\n  } value;\n  struct {\n    libxsmm_gemm_descriptor desc;\n    unsigned int count;\n    const char* symbol;\n  } stat;\n  /* TODO: consider padding */\n} libxsmm_mmbatch_item;\n\nLIBXSMM_API void libxsmm_gemm_internal_set_batchflag(libxsmm_gemm_descriptor* descriptor, void* c, libxsmm_blasint index_stride,\n  libxsmm_blasint batchsize, int multithreaded);\n\nLIBXSMM_API int libxsmm_mmbatch_kernel(libxsmm_xmmfunction kernel, libxsmm_blasint index_base,\n  libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  const void* a, const void* b, void* c, libxsmm_blasint batchsize, /*unsigned*/int tid, /*unsigned*/int ntasks,\n  unsigned char itypesize, unsigned char otypesize, int flags);\n\nLIBXSMM_API int libxsmm_mmbatch_blas(\n  libxsmm_gemm_precision iprec, libxsmm_gemm_precision oprec, const char* transa, const char* transb, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const void* alpha, const void* a, const libxsmm_blasint* lda, const void* b, const libxsmm_blasint* ldb, const void* beta, void* c, const libxsmm_blasint* ldc,\n  libxsmm_blasint index_base, libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  libxsmm_blasint batchsize);\n\nLIBXSMM_API_INTERN void libxsmm_dmmbatch_blas(const char* transa, const char* transb, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const double* alpha, const void* a, const libxsmm_blasint* lda, const void* b, const libxsmm_blasint* ldb, const double* beta, void* c, const libxsmm_blasint* ldc,\n  libxsmm_blasint index_base, libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  libxsmm_blasint batchsize);\n\nLIBXSMM_API_INTERN void libxsmm_smmbatch_blas(const char* transa, const char* transb, libxsmm_blasint m, libxsmm_blasint n, libxsmm_blasint k,\n  const float* alpha, const void* a, const libxsmm_blasint* lda, const void* b, const libxsmm_blasint* ldb, const float* beta, void* c, const libxsmm_blasint* ldc,\n  libxsmm_blasint index_base, libxsmm_blasint index_stride, const libxsmm_blasint stride_a[], const libxsmm_blasint stride_b[], const libxsmm_blasint stride_c[],\n  libxsmm_blasint batchsize);\n\nLIBXSMM_EXTERN_C typedef void (*libxsmm_mmbatch_flush_function)(void);\n\n/** auto-batch descriptor (filter). */\nLIBXSMM_APIVAR_ALIGNED(libxsmm_gemm_descriptor libxsmm_mmbatch_desc);\n/** Records a batch of SMMs or is used for batch-reduce. */\nLIBXSMM_APIVAR_ALIGNED(void* libxsmm_mmbatch_array);\n/** Lock: libxsmm_mmbatch_begin, libxsmm_mmbatch_end, internal_mmbatch_flush. */\nLIBXSMM_APIVAR_ALIGNED(LIBXSMM_LOCK_TYPE(LIBXSMM_GEMM_LOCK) libxsmm_mmbatch_lock);\n/** Maximum size of the recorded batch. */\nLIBXSMM_APIVAR_ALIGNED(unsigned int libxsmm_mmbatch_size);\n/** Maximum number of parallelized batch-groups. */\nLIBXSMM_APIVAR_ALIGNED(unsigned int libxsmm_gemm_npargroups);\n/** Minimum batchsize per thread/task. */\nLIBXSMM_APIVAR_ALIGNED(unsigned int libxsmm_gemm_taskgrain);\n/** Determines if OpenMP tasks are used. */\nLIBXSMM_APIVAR_ALIGNED(int libxsmm_gemm_tasks);\n\n/** Determines the default prefetch strategy, which is used in case of LIBXSMM_PREFETCH_AUTO. */\nLIBXSMM_APIVAR(libxsmm_gemm_prefetch_type libxsmm_gemm_auto_prefetch_default);\n/** Determines the prefetch strategy, which is used in case of LIBXSMM_PREFETCH_AUTO. */\nLIBXSMM_APIVAR(libxsmm_gemm_prefetch_type libxsmm_gemm_auto_prefetch);\n\n/**\n * Intercepted GEMM\n * - [>=1 and  odd]: sequential and non-tiled (small problem sizes only)\n * - [>=2 and even]: parallelized and tiled (all problem sizes)\n * - [>=3 and  odd]: GEMV is intercepted; small problem sizes\n * - [>=4 and even]: GEMV is intercepted; all problem sizes\n * - [0]: disabled\n */\nLIBXSMM_APIVAR_ALIGNED(int libxsmm_gemm_wrap);\n\n#endif /*LIBXSMM_GEMM_H*/\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/src/libxsmm_malloc.c": "/******************************************************************************\n** Copyright (c) 2014-2019, Intel Corporation                                **\n** All rights reserved.                                                      **\n**                                                                           **\n** Redistribution and use in source and binary forms, with or without        **\n** modification, are permitted provided that the following conditions        **\n** are met:                                                                  **\n** 1. Redistributions of source code must retain the above copyright         **\n**    notice, this list of conditions and the following disclaimer.          **\n** 2. Redistributions in binary form must reproduce the above copyright      **\n**    notice, this list of conditions and the following disclaimer in the    **\n**    documentation and/or other materials provided with the distribution.   **\n** 3. Neither the name of the copyright holder nor the names of its          **\n**    contributors may be used to endorse or promote products derived        **\n**    from this software without specific prior written permission.          **\n**                                                                           **\n** THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS       **\n** \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT         **\n** LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR     **\n** A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT      **\n** HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,    **\n** SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED  **\n** TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR    **\n** PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF    **\n** LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING      **\n** NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS        **\n** SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.              **\n******************************************************************************/\n/* Hans Pabst (Intel Corp.)\n******************************************************************************/\n#include \"libxsmm_trace.h\"\n#include \"libxsmm_main.h\"\n#include \"libxsmm_hash.h\"\n\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n#endif\n#include <inttypes.h>\n#include <stdlib.h>\n#include <string.h>\n#include <stdio.h>\n#if defined(LIBXSMM_GLIBC)\n# include <features.h>\n# include <malloc.h>\n#endif\n#if !defined(LIBXSMM_MALLOC_GLIBC)\n# if defined(__GLIBC__)\n#   define LIBXSMM_MALLOC_GLIBC __GLIBC__\n# else\n#   define LIBXSMM_MALLOC_GLIBC 6\n# endif\n#endif\n#if defined(__THROW) && defined(__cplusplus)\n# define LIBXSMM_THROW __THROW\n#else\n# define LIBXSMM_THROW\n#endif\n#if defined(_WIN32)\n# include <windows.h>\n# include <malloc.h>\n# include <intrin.h>\n#else\n# include <sys/mman.h>\n# if defined(MAP_HUGETLB) && defined(MAP_POPULATE)\n#   include <sys/utsname.h>\n#   include <string.h>\n# endif\n# include <sys/types.h>\n# include <unistd.h>\n# include <errno.h>\n# if defined(MAP_ANONYMOUS)\n#   define LIBXSMM_MAP_ANONYMOUS MAP_ANONYMOUS\n# else\n#   define LIBXSMM_MAP_ANONYMOUS MAP_ANON\n# endif\n#endif\n#if !defined(LIBXSMM_MALLOC_FALLBACK)\n# define LIBXSMM_MALLOC_FINAL 3\n#endif\n#if defined(LIBXSMM_VTUNE)\n# if (2 <= LIBXSMM_VTUNE) /* no header file required */\n#   if !defined(LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JITVERSION LIBXSMM_VTUNE\n#   endif\n#   define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load_V2\n#   define LIBXSMM_VTUNE_JIT_LOAD 21\n#   define LIBXSMM_VTUNE_JIT_UNLOAD 14\n#   define iJIT_SAMPLING_ON 0x0001\nLIBXSMM_EXTERN unsigned int iJIT_GetNewMethodID(void);\nLIBXSMM_EXTERN /*iJIT_IsProfilingActiveFlags*/int iJIT_IsProfilingActive(void);\nLIBXSMM_EXTERN int iJIT_NotifyEvent(/*iJIT_JVM_EVENT*/int event_type, void *EventSpecificData);\nLIBXSMM_EXTERN_C typedef struct LineNumberInfo {\n  unsigned int Offset;\n  unsigned int LineNumber;\n} LineNumberInfo;\nLIBXSMM_EXTERN_C typedef struct iJIT_Method_Load_V2 {\n  unsigned int method_id;\n  char* method_name;\n  void* method_load_address;\n  unsigned int method_size;\n  unsigned int line_number_size;\n  LineNumberInfo* line_number_table;\n  char* class_file_name;\n  char* source_file_name;\n  char* module_name;\n} iJIT_Method_Load_V2;\n# else /* more safe due to header dependency */\n#   include <jitprofiling.h>\n#   if !defined(LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JITVERSION 2\n#   endif\n#   if (2 <= LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load_V2\n#     define LIBXSMM_VTUNE_JIT_LOAD iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED_V2\n#   else\n#     define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load\n#     define LIBXSMM_VTUNE_JIT_LOAD iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED\n#   endif\n#   define LIBXSMM_VTUNE_JIT_UNLOAD iJVM_EVENT_TYPE_METHOD_UNLOAD_START\n# endif\n# if !defined(LIBXSMM_MALLOC_FALLBACK)\n#   define LIBXSMM_MALLOC_FALLBACK LIBXSMM_MALLOC_FINAL\n# endif\n#else\n# if !defined(LIBXSMM_MALLOC_FALLBACK)\n#   define LIBXSMM_MALLOC_FALLBACK 0\n# endif\n#endif /*defined(LIBXSMM_VTUNE)*/\n#if !defined(LIBXSMM_MALLOC_XMAP_TEMPLATE)\n# define LIBXSMM_MALLOC_XMAP_TEMPLATE \".libxsmm_jit.\" LIBXSMM_MKTEMP_PATTERN\n#endif\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(pop)\n#endif\n#if defined(LIBXSMM_PERF)\n# include \"libxsmm_perf.h\"\n#endif\n\n#if !defined(LIBXSMM_MALLOC_ALIGNMAX)\n# define LIBXSMM_MALLOC_ALIGNMAX (2 << 20) /* 2 MB */\n#endif\n#if !defined(LIBXSMM_MALLOC_ALIGNFCT)\n# define LIBXSMM_MALLOC_ALIGNFCT 16\n#endif\n#if !defined(LIBXSMM_MALLOC_SEED)\n# define LIBXSMM_MALLOC_SEED 1051981\n#endif\n\n#if !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) && \\\n  defined(LIBXSMM_MALLOC) && (0 != LIBXSMM_MALLOC) && defined(LIBXSMM_INTERCEPT_DYNAMIC) && \\\n  defined(LIBXSMM_GLIBC) && !defined(_CRAYC) && !defined(__TRACE) /* TODO */\n# define LIBXSMM_MALLOC_HOOK_DYNAMIC\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n# if defined(LIBXSMM_OFFLOAD_TARGET)\n#   pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n# endif\n# include <dlfcn.h>\n# if defined(LIBXSMM_OFFLOAD_TARGET)\n#   pragma offload_attribute(pop)\n# endif\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_STATIC) && \\\n  defined(LIBXSMM_MALLOC) && (0 != LIBXSMM_MALLOC) && \\\n  !defined(_WIN32) && defined(LIBXSMM_GLIBC) /* TODO */\n# define LIBXSMM_MALLOC_HOOK_STATIC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_KMP) && 0\n# define LIBXSMM_MALLOC_HOOK_KMP\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_QKMALLOC) && 0\n# define LIBXSMM_MALLOC_HOOK_QKMALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_IMALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_IMALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_REALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_REALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_CALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_CALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_ALIGN) && 1\n# define LIBXSMM_MALLOC_HOOK_ALIGN\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_CHECK) && 0\n# define LIBXSMM_MALLOC_HOOK_CHECK 1\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_INIT) && 0\n# define LIBXSMM_MALLOC_HOOK_INIT\n#endif\n\n#if !defined(LIBXSMM_MALLOC_CRC_LIGHT) && !defined(_DEBUG) && 1\n# define LIBXSMM_MALLOC_CRC_LIGHT\n#endif\n#if !defined(LIBXSMM_MALLOC_CRC_OFF)\n# if defined(NDEBUG) && !defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n#   define LIBXSMM_MALLOC_CRC_OFF\n# elif !defined(LIBXSMM_BUILD)\n#   define LIBXSMM_MALLOC_CRC_OFF\n# endif\n#endif\n\n#if !defined(LIBXSMM_MALLOC_SCRATCH_LIMIT)\n# define LIBXSMM_MALLOC_SCRATCH_LIMIT 0xFFFFFFFF /* ~4 GB */\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_PADDING)\n# define LIBXSMM_MALLOC_SCRATCH_PADDING LIBXSMM_CACHELINE\n#endif\n/* pointers are checked first if they belong to scratch */\n#if !defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST) && 1\n# define LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST\n#endif\n/* can clobber memory if allocations are not exactly scoped */\n#if !defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD) && 0\n# define LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_JOIN) && 1\n# define LIBXSMM_MALLOC_SCRATCH_JOIN\n#endif\n/* protected against double-delete (if possible) */\n#if !defined(LIBXSMM_MALLOC_DELETE_SAFE) && 0\n# define LIBXSMM_MALLOC_DELETE_SAFE\n#endif\n/* map memory for scratch buffers */\n#if !defined(LIBXSMM_MALLOC_MMAP_SCRATCH) && 1\n# define LIBXSMM_MALLOC_MMAP_SCRATCH\n#endif\n/* map memory for hooked allocation */\n#if !defined(LIBXSMM_MALLOC_MMAP_HOOK) && 1\n# define LIBXSMM_MALLOC_MMAP_HOOK\n#endif\n/* map memory also for non-executable buffers */\n#if !defined(LIBXSMM_MALLOC_MMAP) && 1\n# define LIBXSMM_MALLOC_MMAP\n#endif\n\n#define INTERNAL_MEMALIGN_HOOK(RESULT, FLAGS, ALIGNMENT, SIZE, CALLER) { \\\n  const int recursive = LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_recursive, 1, LIBXSMM_ATOMIC_RELAXED); \\\n  if (0 == libxsmm_ninit && 1 == recursive) libxsmm_init(); /* !LIBXSMM_INIT */ \\\n  if ( 1 < recursive /* protect against recursion */ \\\n    || 0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    || (internal_malloc_limit[0] > (SIZE)) \\\n    || (internal_malloc_limit[1] < (SIZE) && 0 != internal_malloc_limit[1])) \\\n  { \\\n    (RESULT) = (0 != (ALIGNMENT) ? __real_memalign(ALIGNMENT, SIZE) : __real_malloc(SIZE)); \\\n  } \\\n  else { \\\n    if (NULL == (CALLER)) { /* libxsmm_trace_caller_id may allocate memory */ \\\n      internal_scratch_malloc(&(RESULT), SIZE, ALIGNMENT, FLAGS, \\\n        libxsmm_trace_caller_id(0/*level*/)); \\\n    } \\\n    else { \\\n      internal_scratch_malloc(&(RESULT), SIZE, ALIGNMENT, FLAGS, CALLER); \\\n    } \\\n  } \\\n  LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_recursive, 1, LIBXSMM_ATOMIC_RELAXED); \\\n}\n\n#define INTERNAL_REALLOC_HOOK(RESULT, FLAGS, PTR, SIZE, CALLER) { \\\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    /*|| (0 != LIBXSMM_ATOMIC_LOAD(&internal_malloc_recursive, LIBXSMM_ATOMIC_RELAXED))*/ \\\n    || (internal_malloc_limit[0] > (SIZE)) \\\n    || (internal_malloc_limit[1] < (SIZE) && 0 != internal_malloc_limit[1])) \\\n  { \\\n    (RESULT) = __real_realloc(PTR, SIZE); \\\n  } \\\n  else { \\\n    const int nzeros = LIBXSMM_INTRINSICS_BITSCANFWD64((uintptr_t)(PTR)), alignment = 1 << nzeros; \\\n    LIBXSMM_ASSERT(0 == ((uintptr_t)(PTR) & ~(0xFFFFFFFFFFFFFFFF << nzeros))); \\\n    if (NULL == (CALLER)) { /* libxsmm_trace_caller_id may allocate memory */ \\\n      internal_scratch_malloc(&(PTR), SIZE, (size_t)alignment, FLAGS, \\\n        libxsmm_trace_caller_id(0/*level*/)); \\\n    } \\\n    else { \\\n      internal_scratch_malloc(&(PTR), SIZE, (size_t)alignment, FLAGS, CALLER); \\\n    } \\\n    (RESULT) = (PTR); \\\n  } \\\n}\n\n#define INTERNAL_FREE_HOOK(PTR, CALLER) { \\\n  LIBXSMM_UNUSED(CALLER); \\\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    /*|| (0 != LIBXSMM_ATOMIC_LOAD(&internal_malloc_recursive, LIBXSMM_ATOMIC_RELAXED))*/ \\\n  ){ \\\n    __real_free(PTR); \\\n  } \\\n  else { /* recognize pointers not issued by LIBXSMM */ \\\n    libxsmm_free(PTR); \\\n  } \\\n}\n\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_malloc_info_type {\n  libxsmm_free_function free;\n  void *pointer, *reloc;\n  const void* context;\n  size_t size;\n  int flags;\n#if defined(LIBXSMM_VTUNE)\n  unsigned int code_id;\n#endif\n#if !defined(LIBXSMM_MALLOC_CRC_OFF) /* hash *must* be the last entry */\n  unsigned int hash;\n#endif\n} internal_malloc_info_type;\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_malloc_pool_type {\n  char pad[LIBXSMM_MALLOC_SCRATCH_PADDING];\n  struct {\n    size_t minsize, counter, incsize;\n    char *buffer, *head;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    const void* site;\n# if (0 != LIBXSMM_SYNC)\n    unsigned int tid;\n# endif\n#endif\n  } instance;\n} internal_malloc_pool_type;\n\nLIBXSMM_EXTERN_C typedef LIBXSMM_RETARGETABLE void* (*internal_realloc_fun)(void* /*ptr*/, size_t /*size*/);\n\n/* Scratch pool, which supports up to MAX_NSCRATCH allocation sites. */\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n/* LIBXSMM_ALIGNED appears to contradict LIBXSMM_APIVAR, and causes multiple defined symbols (if below is seen in multiple translation units) */\nLIBXSMM_APIVAR_ARRAY(char internal_malloc_pool_buffer, (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) * sizeof(internal_malloc_pool_type) + (LIBXSMM_MALLOC_SCRATCH_PADDING) - 1);\n#endif\n/* Interval of bytes that permit interception (internal_malloc_kind) */\nLIBXSMM_APIVAR_ARRAY(size_t internal_malloc_limit, 2);\n/* Maximum total size of the scratch memory domain. */\nLIBXSMM_APIVAR(size_t internal_malloc_scratch_limit);\nLIBXSMM_APIVAR(size_t internal_malloc_scratch_nmallocs);\nLIBXSMM_APIVAR(size_t internal_malloc_private_max);\nLIBXSMM_APIVAR(size_t internal_malloc_private_cur);\nLIBXSMM_APIVAR(size_t internal_malloc_public_max);\nLIBXSMM_APIVAR(size_t internal_malloc_public_cur);\nLIBXSMM_APIVAR(size_t internal_malloc_local_max);\nLIBXSMM_APIVAR(size_t internal_malloc_local_cur);\nLIBXSMM_APIVAR(int internal_malloc_recursive);\n/** 0: regular, 1/odd: intercept/scratch, otherwise: all/scratch */\nLIBXSMM_APIVAR(int internal_malloc_kind);\n#if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\nLIBXSMM_APIVAR(int internal_malloc_join);\n#endif\n\n\nLIBXSMM_API_INTERN size_t libxsmm_alignment(size_t size, size_t alignment)\n{\n  size_t result = sizeof(void*);\n  if ((LIBXSMM_MALLOC_ALIGNFCT * LIBXSMM_MALLOC_ALIGNMAX) <= size) {\n    result = libxsmm_lcm(0 == alignment ? (LIBXSMM_ALIGNMENT) : libxsmm_lcm(alignment, LIBXSMM_ALIGNMENT), LIBXSMM_MALLOC_ALIGNMAX);\n  }\n  else {\n    if ((LIBXSMM_MALLOC_ALIGNFCT * LIBXSMM_ALIGNMENT) <= size) {\n      result = (0 == alignment ? (LIBXSMM_ALIGNMENT) : libxsmm_lcm(alignment, LIBXSMM_ALIGNMENT));\n    }\n    else if (0 != alignment) {\n      result = libxsmm_lcm(alignment, result);\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API size_t libxsmm_offset(const size_t offset[], const size_t shape[], size_t ndims, size_t* size)\n{\n  size_t result = 0, size1 = 0;\n  if (0 != ndims && NULL != shape) {\n    size_t i;\n    result = (NULL != offset ? offset[0] : 0);\n    size1 = shape[0];\n    for (i = 1; i < ndims; ++i) {\n      result += (NULL != offset ? offset[i] : 0) * size1;\n      size1 *= shape[i];\n    }\n  }\n  if (NULL != size) *size = size1;\n  return result;\n}\n\n\nLIBXSMM_API_INLINE internal_malloc_info_type* internal_malloc_info(const void* memory, int check)\n{\n  const char *const buffer = (const char*)memory;\n  internal_malloc_info_type* result = (internal_malloc_info_type*)(NULL != memory\n    ? (buffer - sizeof(internal_malloc_info_type)) : NULL);\n#if defined(LIBXSMM_MALLOC_HOOK_CHECK)\n  if ((LIBXSMM_MALLOC_HOOK_CHECK) < check) check = (LIBXSMM_MALLOC_HOOK_CHECK);\n#endif\n  if (0 != check && NULL != result) { /* check ownership */\n#if !defined(_WIN32) /* mprotect: pass address rounded down to page/4k alignment */\n    if (1 == check || 0 == mprotect((void*)(((uintptr_t)result) & 0xFFFFFFFFFFFFF000),\n      sizeof(internal_malloc_info_type), PROT_READ | PROT_WRITE) || ENOMEM != errno)\n#endif\n    {\n      const size_t maxsize = LIBXSMM_MAX(LIBXSMM_MAX(internal_malloc_public_max, internal_malloc_local_max), internal_malloc_private_max);\n      const int flags_rs = LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_SCRATCH;\n      const int flags_mx = LIBXSMM_MALLOC_FLAG_MMAP | LIBXSMM_MALLOC_FLAG_X;\n      const char* const pointer = (const char*)result->pointer;\n      union { libxsmm_free_fun fun; const void* ptr; } convert;\n      convert.fun = result->free.function;\n      if (((flags_mx != (flags_mx & result->flags)) && NULL != result->reloc)\n        || (0 == (LIBXSMM_MALLOC_FLAG_X & result->flags) ? 0 : (0 != (flags_rs & result->flags)))\n        || (0 != (LIBXSMM_MALLOC_FLAG_X & result->flags) && NULL != result->context)\n#if defined(LIBXSMM_VTUNE)\n        || (0 == (LIBXSMM_MALLOC_FLAG_X & result->flags) && 0 != result->code_id)\n#endif\n        || (0 != (~LIBXSMM_MALLOC_FLAG_VALID & result->flags))\n        || (0 == (LIBXSMM_MALLOC_FLAG_R & result->flags))\n        || pointer == convert.ptr || pointer == result->context\n        || pointer >= buffer || NULL == pointer\n        || maxsize < result->size || 0 == result->size\n        || 1 >= libxsmm_ninit /* before checksum calculation */\n#if !defined(LIBXSMM_MALLOC_CRC_OFF) /* last check: checksum over info */\n# if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n        || result->hash != LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &result)\n# else\n        || result->hash != libxsmm_crc32(LIBXSMM_MALLOC_SEED, result,\n            (const char*)&result->hash - (const char*)result)\n# endif\n#endif\n      ) { /* mismatch */\n        result = NULL;\n      }\n    }\n#if !defined(_WIN32)\n    else { /* mismatch */\n      result = NULL;\n    }\n#endif\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int internal_xfree(const void* /*memory*/, internal_malloc_info_type* /*info*/);\nLIBXSMM_API_INTERN int internal_xfree(const void* memory, internal_malloc_info_type* info)\n{\n#if !defined(LIBXSMM_BUILD) || !defined(_WIN32)\n  static int error_once = 0;\n#endif\n  int result = EXIT_SUCCESS, flags;\n  void* buffer;\n  size_t size;\n  LIBXSMM_ASSERT(NULL != memory && NULL != info);\n  buffer = info->pointer;\n  flags = info->flags;\n  size = info->size;\n#if !defined(LIBXSMM_BUILD) /* sanity check */\n  if (NULL != buffer || 0 == size)\n#endif\n  {\n    const size_t alloc_size = size + (((const char*)memory) - ((const char*)buffer));\n    LIBXSMM_ASSERT(NULL != buffer || 0 == size);\n    if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n      if (NULL != info->free.function) {\n#if defined(LIBXSMM_MALLOC_DELETE_SAFE)\n        info->pointer = NULL; info->size = 0;\n#endif\n        if (NULL == info->context) {\n#if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) && 0\n          if (free == info->free.function) {\n            __real_free(buffer);\n          }\n          else\n#endif\n          if (NULL != info->free.function) {\n            info->free.function(buffer);\n          }\n        }\n        else {\n          LIBXSMM_ASSERT(NULL != info->free.ctx_form);\n          info->free.ctx_form(buffer, info->context);\n        }\n      }\n    }\n    else {\n#if defined(LIBXSMM_VTUNE)\n      if (0 != (LIBXSMM_MALLOC_FLAG_X & flags) && 0 != info->code_id && iJIT_SAMPLING_ON == iJIT_IsProfilingActive()) {\n        iJIT_NotifyEvent(LIBXSMM_VTUNE_JIT_UNLOAD, &info->code_id);\n      }\n#endif\n#if defined(_WIN32)\n      result = (NULL == buffer || FALSE != VirtualFree(buffer, 0, MEM_RELEASE)) ? EXIT_SUCCESS : EXIT_FAILURE;\n#else /* !_WIN32 */\n      {\n        void* const reloc = info->reloc;\n        if (0 != munmap(buffer, alloc_size)) {\n          if (0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          {\n            const char* const error_message = strerror(errno);\n            fprintf(stderr, \"LIBXSMM ERROR: %s (munmap error #%i for range %p+%\" PRIuPTR \")!\\n\",\n              error_message, errno, buffer, (uintptr_t)alloc_size);\n          }\n          result = EXIT_FAILURE;\n        }\n        if (0 != (LIBXSMM_MALLOC_FLAG_X & flags) && EXIT_SUCCESS == result\n          && NULL != reloc && MAP_FAILED != reloc && buffer != reloc\n          && 0 != munmap(reloc, alloc_size))\n        {\n          if (0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          {\n            const char* const error_message = strerror(errno);\n            fprintf(stderr, \"LIBXSMM ERROR: %s (munmap error #%i for range %p+%\" PRIuPTR \")!\\n\",\n              error_message, errno, reloc, (uintptr_t)alloc_size);\n          }\n          result = EXIT_FAILURE;\n        }\n      }\n#endif\n    }\n    if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* update statistics */\n      if (0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags)) { /* public */\n        if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) { /* scratch */\n          const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n            &internal_malloc_public_cur, LIBXSMM_ATOMIC_RELAXED);\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_public_cur,\n            alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n        }\n        else { /* local */\n          const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n            &internal_malloc_local_cur, LIBXSMM_ATOMIC_RELAXED);\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_local_cur,\n            alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n        }\n      }\n      else { /* private */\n        const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n          &internal_malloc_private_cur, LIBXSMM_ATOMIC_RELAXED);\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_private_cur,\n          alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n      }\n    }\n  }\n#if !defined(LIBXSMM_BUILD)\n  else if ((LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: attempt to release memory from non-matching implementation!\\n\");\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INLINE size_t internal_get_scratch_size(const internal_malloc_pool_type* exclude)\n{\n  size_t result = 0;\n#if !defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) || (1 >= (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  LIBXSMM_UNUSED(exclude);\n#else\n  const internal_malloc_pool_type* pool = (const internal_malloc_pool_type*)LIBXSMM_UP2(internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const internal_malloc_pool_type *const end = pool + libxsmm_scratch_pools;\n  LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  for (; pool != end; ++pool)\n# endif /*(1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  {\n    if (0 != pool->instance.minsize) {\n# if 1 /* memory info is not used */\n      if (pool != exclude && (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n        result += pool->instance.minsize;\n      }\n# else\n      const internal_malloc_info_type* const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n      if (NULL != info && pool != exclude && (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n        result += info->size;\n      }\n# endif\n    }\n    else break; /* early exit */\n  }\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  return result;\n}\n\n\nLIBXSMM_API_INLINE internal_malloc_pool_type* internal_scratch_malloc_pool(const void* memory)\n{\n  internal_malloc_pool_type* result = NULL;\n  internal_malloc_pool_type* pool = (internal_malloc_pool_type*)LIBXSMM_UP2(internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n  const char* const buffer = (const char*)memory;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const unsigned int npools = libxsmm_scratch_pools;\n#else\n  const unsigned int npools = 1;\n#endif\n  internal_malloc_pool_type *const end = pool + npools;\n  LIBXSMM_ASSERT(npools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  LIBXSMM_ASSERT(NULL != memory);\n  for (; pool != end; ++pool) {\n    if (0 != pool->instance.minsize) {\n      if (0 != pool->instance.counter\n#if 1 /* should be implied by non-zero counter */\n        && NULL != pool->instance.buffer\n#endif\n      ){/* check if memory belongs to scratch domain or local domain */\n#if 1\n        const size_t size = pool->instance.minsize;\n#else\n        const internal_malloc_info_type* const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n        const size_t size = info->size;\n#endif\n        if (pool->instance.buffer == buffer /* fast path */ ||\n           (pool->instance.buffer < buffer && buffer < (pool->instance.buffer + size)))\n        {\n          result = pool;\n          break;\n        }\n      }\n    }\n    else break; /* early exit */\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void internal_scratch_free(const void* /*memory*/, internal_malloc_pool_type* /*pool*/);\nLIBXSMM_API_INTERN void internal_scratch_free(const void* memory, internal_malloc_pool_type* pool)\n{\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const char *const pool_buffer = pool->instance.buffer;\n  const size_t counter = LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n  LIBXSMM_ASSERT(pool_buffer <= pool->instance.head);\n  if (0 == counter) { /* reuse or reallocate scratch domain */\n    internal_malloc_info_type *const info = internal_malloc_info(pool_buffer, 0/*no check*/);\n    const size_t scale_size = (size_t)(1 != libxsmm_scratch_scale ? (libxsmm_scratch_scale * info->size) : info->size); /* hysteresis */\n    const size_t size = pool->instance.minsize + pool->instance.incsize;\n    if (size <= scale_size) { /* reuse scratch domain */\n      pool->instance.head = pool->instance.buffer;\n    }\n    else { /* release buffer */\n# if !defined(NDEBUG)\n      static int error_once = 0;\n# endif\n      pool->instance.buffer = pool->instance.head = NULL;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      pool->instance.site = NULL; /* clear affinity */\n# endif\n# if !defined(NDEBUG)\n      if (EXIT_SUCCESS != internal_xfree(pool_buffer, info)\n        && 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n# else\n      internal_xfree(pool_buffer, info); /* !libxsmm_free */\n# endif\n    }\n  }\n# if defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD) /* TODO: document linear/scoped allocator policy */\n  else if ((char*)memory < pool->instance.head) { /* reuse scratch domain */\n    pool->instance.head = (char*)memory;\n  }\n# else\n  LIBXSMM_UNUSED(memory);\n# endif\n#else\n  LIBXSMM_UNUSED(memory); LIBXSMM_UNUSED(pool);\n#endif\n}\n\n\nLIBXSMM_API_INTERN void internal_scratch_malloc(void** /*memory*/, size_t /*size*/, size_t /*alignment*/, int /*flags*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void internal_scratch_malloc(void** memory, size_t size, size_t alignment, int flags, const void* caller)\n{\n  LIBXSMM_ASSERT(NULL != memory && 0 == (LIBXSMM_MALLOC_FLAG_X & flags));\n  if (0 == (LIBXSMM_MALLOC_FLAG_REALLOC & flags) || NULL == *memory) {\n    static int error_once = 0;\n    size_t local_size = 0;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    if (0 < libxsmm_scratch_pools) {\n      internal_malloc_pool_type *const pools = (internal_malloc_pool_type*)LIBXSMM_UP2(internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n      internal_malloc_pool_type *const end = pools + libxsmm_scratch_pools, *pool0 = end, *pool = pools;\n      const size_t align_size = libxsmm_alignment(size, alignment), alloc_size = size + align_size - 1;\n      size_t used_size = 0, pool_size = 0, req_size = 0;\n# if (0 != LIBXSMM_SYNC)\n      const unsigned int tid = libxsmm_get_tid();\n# endif\n      unsigned int npools = 1;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      const void *const site = caller; /* no further attempt in case of NULL */\n      for (; pool != end; ++pool) { /* counter: memory info is not employed as pools are still manipulated */\n        if (NULL != pool->instance.buffer) {\n          if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) ++npools; /* count number of occupied pools */\n          if ( /* find matching pool and enter fast path (draw from pool-buffer) */\n#   if (0 != LIBXSMM_SYNC) && !defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            (site == pool->instance.site && tid == pool->instance.tid))\n#   elif (0 != LIBXSMM_SYNC)\n            (site == pool->instance.site && (0 != internal_malloc_join || tid == pool->instance.tid)))\n#   else\n            (site == pool->instance.site))\n#   endif\n          {\n            pool_size = pool->instance.minsize;\n            used_size = pool->instance.head - pool->instance.buffer;\n            req_size = alloc_size + used_size;\n            if (req_size <= pool_size) break;\n          }\n        }\n        else {\n          if (end == pool0) pool0 = pool; /* first available pool*/\n          if (0 == pool->instance.minsize) break; /* early exit */\n\n        }\n      }\n# endif\n      if (end == pool) pool = pool0; /* fall-back to new pool */\n      LIBXSMM_ASSERT(NULL != pool);\n      if (end != pool && 0 <= internal_malloc_kind) {\n        const size_t counter = LIBXSMM_ATOMIC_ADD_FETCH(&pool->instance.counter, (size_t)1, LIBXSMM_ATOMIC_SEQ_CST);\n        if (NULL != pool->instance.buffer || 1 != counter) { /* attempt to (re-)use existing pool */\n          const internal_malloc_info_type *const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n          pool_size = (NULL != info ? info->size : 0);\n          used_size = pool->instance.head - pool->instance.buffer;\n          req_size = alloc_size + used_size;\n          if (req_size <= pool_size) { /* fast path: draw from pool-buffer */\n# if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            void *const headaddr = &pool->instance.head;\n            char *const head = (0 == internal_malloc_join\n              ? (pool->instance.head += alloc_size)\n              : ((char*)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                (uintptr_t*)headaddr, alloc_size, LIBXSMM_ATOMIC_SEQ_CST)));\n# else\n            char *const head = (char*)(pool->instance.head += alloc_size);\n# endif\n            *memory = LIBXSMM_ALIGN(head - alloc_size, align_size);\n          }\n          else { /* fall-back to local memory allocation */\n            const size_t incsize = req_size - LIBXSMM_MIN(pool_size, req_size);\n            pool->instance.incsize = LIBXSMM_MAX(pool->instance.incsize, incsize);\n# if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            if (0 == internal_malloc_join) {\n              --pool->instance.counter;\n            }\n            else {\n              LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n            }\n# else\n            --pool->instance.counter;\n# endif\n            if (\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n              (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site &&\n# endif\n              0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags))\n            {\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_local_max < watermark) internal_malloc_local_max = watermark; /* accept data-race */\n            }\n            else {\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_private_max < watermark) internal_malloc_private_max = watermark; /* accept data-race */\n            }\n            local_size = size;\n          }\n        }\n        else { /* fresh pool */\n          const size_t scratch_limit = libxsmm_get_scratch_limit();\n          const size_t scratch_size = internal_get_scratch_size(pool); /* exclude current pool */\n          const size_t limit_size = (1 < npools ? (scratch_limit - LIBXSMM_MIN(scratch_size, scratch_limit)) : LIBXSMM_SCRATCH_UNLIMITED);\n          const size_t scale_size = (size_t)(1 != libxsmm_scratch_scale ? (libxsmm_scratch_scale * alloc_size) : alloc_size); /* hysteresis */\n          const size_t incsize = (size_t)(libxsmm_scratch_scale * pool->instance.incsize);\n          const size_t maxsize = LIBXSMM_MAX(scale_size, pool->instance.minsize) + incsize;\n          const size_t limsize = LIBXSMM_MIN(maxsize, limit_size);\n          const size_t minsize = limsize;\n          LIBXSMM_ASSERT(1 <= libxsmm_scratch_scale);\n          LIBXSMM_ASSERT(1 == counter);\n          pool->instance.incsize = 0; /* reset */\n          pool->instance.minsize = minsize;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n          pool->instance.site = site;\n#   if (0 != LIBXSMM_SYNC)\n          pool->instance.tid = tid;\n#   endif\n# endif\n          if (alloc_size <= minsize && /* allocate scratch pool */\n            EXIT_SUCCESS == libxsmm_xmalloc(memory, minsize, 0/*auto-align*/,\n              (flags | LIBXSMM_MALLOC_FLAG_SCRATCH) & ~LIBXSMM_MALLOC_FLAG_REALLOC,\n              NULL/*extra*/, 0/*extra_size*/))\n          {\n            pool->instance.buffer = (char*)*memory;\n            pool->instance.head = pool->instance.buffer + alloc_size;\n            *memory = LIBXSMM_ALIGN((char*)*memory, align_size);\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n            if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site)\n# endif\n            {\n              LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_scratch_nmallocs, 1, LIBXSMM_ATOMIC_RELAXED);\n            }\n          }\n          else { /* fall-back to local allocation */\n            LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n            if (0 != libxsmm_verbosity /* library code is expected to be mute */\n              && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n            {\n              if (alloc_size <= minsize) {\n                fprintf(stderr, \"LIBXSMM ERROR: failed to allocate scratch memory!\\n\");\n              }\n              else if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != caller\n                && (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity))\n              {\n                fprintf(stderr, \"LIBXSMM WARNING: scratch memory domain exhausted!\\n\");\n              }\n            }\n            local_size = size;\n          }\n        }\n      }\n      else { /* fall-back to local memory allocation */\n        local_size = size;\n      }\n    }\n    else { /* fall-back to local memory allocation */\n      local_size = size;\n    }\n    if (0 != local_size)\n#else\n    local_size = size;\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n    { /* local memory allocation */\n      if (EXIT_SUCCESS != libxsmm_xmalloc(memory, local_size, alignment,\n          flags & ~(LIBXSMM_MALLOC_FLAG_SCRATCH | LIBXSMM_MALLOC_FLAG_REALLOC), NULL/*extra*/, 0/*extra_size*/)\n        && /* library code is expected to be mute */0 != libxsmm_verbosity\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: scratch memory fall-back failed!\\n\");\n        LIBXSMM_ASSERT(NULL == *memory);\n      }\n      if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != caller) {\n        LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_scratch_nmallocs, 1, LIBXSMM_ATOMIC_RELAXED);\n      }\n    }\n  }\n  else { /* reallocate memory */\n    const void *const preserve = *memory;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(preserve);\n    if (NULL != pool) {\n      const internal_malloc_info_type *const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n      void* buffer;\n      LIBXSMM_ASSERT(pool->instance.buffer <= pool->instance.head && NULL != info);\n      internal_scratch_malloc(&buffer, size, alignment,\n        ~LIBXSMM_MALLOC_FLAG_REALLOC & (LIBXSMM_MALLOC_FLAG_SCRATCH | flags), caller);\n      if (NULL != buffer) {\n        memcpy(buffer, preserve, LIBXSMM_MIN(size, info->size)); /* TODO: memmove? */\n        *memory = buffer;\n      }\n      internal_scratch_free(memory, pool);\n    }\n    else\n#endif\n    { /* non-pooled (potentially foreign pointer) */\n#if !defined(NDEBUG)\n      const int status =\n#endif\n      libxsmm_xmalloc(memory, size, alignment/* no need here to determine alignment of given buffer */,\n        ~LIBXSMM_MALLOC_FLAG_SCRATCH & flags, NULL/*extra*/, 0/*extra_size*/);\n      assert(EXIT_SUCCESS == status || NULL == *memory); /* !LIBXSMM_ASSERT */\n    }\n  }\n}\n\n\n#if defined(LIBXSMM_GLIBC)\n/* prototypes for GLIBC internal implementation */\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_memalign(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_malloc(size_t /*size*/);\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_calloc(size_t /*num*/, size_t /*size*/);\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void* __libc_realloc(void* /*ptr*/, size_t /*size*/);\n#endif\nLIBXSMM_EXTERN_C LIBXSMM_RETARGETABLE void  __libc_free(void* /*ptr*/);\n#endif /*defined(LIBXSMM_GLIBC)*/\n\n\nLIBXSMM_API_INTERN void* internal_memalign_posix(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_posix(size_t alignment, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_GLIBC)\n  result = __libc_memalign(alignment, size);\n#elif defined(_WIN32)\n  LIBXSMM_UNUSED(alignment);\n  result = malloc(size);\n#else\n  if (0 != posix_memalign(&result, alignment, size)) result = NULL;\n#endif\n  return result;\n}\n\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_malloc_struct {\n  union { const void* dlsym; void* (*ptr)(size_t, size_t);  } alignmem;\n  union { const void* dlsym; void* (*ptr)(size_t, size_t);  } memalign;\n  union { const void* dlsym; libxsmm_malloc_fun ptr;        } malloc;\n# if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n  union { const void* dlsym; void* (*ptr)(size_t, size_t);  } calloc;\n# endif\n# if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n  union { const void* dlsym; internal_realloc_fun ptr;      } realloc;\n# endif\n  union { const void* dlsym; libxsmm_free_fun ptr;          } free;\n} internal_malloc_struct;\nLIBXSMM_APIVAR(internal_malloc_struct internal_malloc);\n\n#if defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)\nLIBXSMM_API_INTERN void* internal_memalign_malloc(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_malloc(size_t alignment, size_t size)\n{\n  LIBXSMM_UNUSED(alignment);\n  LIBXSMM_ASSERT(NULL != internal_malloc.malloc.dlsym);\n  return internal_malloc.malloc.ptr(size);\n}\n#elif defined(LIBXSMM_MALLOC_HOOK_KMP)\nLIBXSMM_API_INTERN void* internal_memalign_twiddle(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_twiddle(size_t alignment, size_t size)\n{\n  LIBXSMM_ASSERT(NULL != internal_malloc.alignmem.dlsym);\n  return internal_malloc.alignmem.ptr(size, alignment);\n}\n#endif\n#endif /*defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)*/\n\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_memalign(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_memalign(size_t alignment, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    2 <= libxsmm_ninit &&\n# endif\n    NULL != internal_malloc.memalign.ptr)\n  {\n    result = internal_malloc.memalign.ptr(alignment, size);\n  }\n  else\n#endif\n#if defined(LIBXSMM_GLIBC)\n  result = __libc_memalign(alignment, size);\n#else\n  result = internal_memalign_posix(alignment, size);\n#endif\n  return result;\n}\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_malloc(size_t /*size*/);\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_malloc(size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_ALIGN)\n  const size_t alignment = libxsmm_alignment(size, 0/*auto*/);\n  result = __real_memalign(alignment, size);\n#else\n# if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n#   if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    2 <= libxsmm_ninit &&\n#   endif\n    NULL != internal_malloc.malloc.ptr)\n  {\n    LIBXSMM_ASSERT(malloc != internal_malloc.malloc.ptr);\n    result = internal_malloc.malloc.ptr(size);\n  }\n  else\n# endif\n# if defined(LIBXSMM_GLIBC)\n  result = __libc_malloc(size);\n# else\n  result = malloc(size);\n# endif\n#endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_calloc(size_t /*num*/, size_t /*size*/);\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_calloc(size_t num, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    2 <= libxsmm_ninit &&\n# endif\n    NULL != internal_malloc.calloc.ptr)\n  {\n    LIBXSMM_ASSERT(calloc != internal_malloc.calloc.ptr);\n    result = internal_malloc.calloc.ptr(num, size);\n  }\n  else\n#endif\n#if defined(LIBXSMM_GLIBC)\n  result = __libc_calloc(num, size);\n#else\n  result = calloc(num, size);\n#endif\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_realloc(void* /*ptr*/, size_t /*size*/);\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void* __real_realloc(void* ptr, size_t size)\n{\n  void* result;\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n  if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n    2 <= libxsmm_ninit &&\n# endif\n    NULL != internal_malloc.realloc.ptr)\n  {\n    LIBXSMM_ASSERT(realloc != internal_malloc.realloc.ptr);\n    result = internal_malloc.realloc.ptr(ptr, size);\n  }\n  else\n#endif\n#if defined(LIBXSMM_GLIBC)\n  result = __libc_realloc(ptr, size);\n#else\n  result = realloc(ptr, size);\n#endif\n  return result;\n}\n#endif\n\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void __real_free(void* /*ptr*/);\nLIBXSMM_API_INTERN LIBXSMM_ATTRIBUTE_WEAK void __real_free(void* ptr)\n{\n  if (NULL != ptr) {\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n    if (\n# if defined(LIBXSMM_MALLOC_HOOK_INIT)\n      2 <= libxsmm_ninit &&\n# endif\n      NULL != internal_malloc.free.ptr)\n    {\n      LIBXSMM_ASSERT(free != internal_malloc.free.ptr);\n      internal_malloc.free.ptr(ptr);\n    }\n    else\n#endif\n#if defined(LIBXSMM_GLIBC)\n    __libc_free(ptr);\n#else\n    free(ptr);\n#endif\n  }\n}\n\n#if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n\nLIBXSMM_API_INTERN void* internal_memalign_hook(size_t /*alignment*/, size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_memalign_hook(size_t alignment, size_t size, const void* caller)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, caller);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, caller);\n# endif\n  return result;\n}\n\nLIBXSMM_API void* __wrap_memalign(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_memalign(size_t alignment, size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\nLIBXSMM_API_INTERN void* internal_malloc_hook(size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_malloc_hook(size_t size, const void* caller)\n{\n  return internal_memalign_hook(0/*auto-alignment*/, size, caller);\n}\n\nLIBXSMM_API void* __wrap_malloc(size_t /*size*/);\nLIBXSMM_API void* __wrap_malloc(size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API void* __wrap_calloc(size_t /*num*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_calloc(size_t num, size_t size)\n{\n  void* result;\n  const size_t nbytes = num * size;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# endif\n  /* TODO: signal anonymous/zeroed pages */\n  if (NULL != result) memset(result, 0, nbytes);\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API_INTERN void* internal_realloc_hook(void* /*ptr*/, size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_realloc_hook(void* ptr, size_t size, const void* caller)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, caller);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, caller);\n# endif\n  return result;\n}\n\nLIBXSMM_API void* __wrap_realloc(void* /*ptr*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_realloc(void* ptr, size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, NULL/*caller*/);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, NULL/*caller*/);\n# endif\n  return result;\n}\n#endif\n\nLIBXSMM_API_INTERN void internal_free_hook(void* /*ptr*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void internal_free_hook(void* ptr, const void* caller)\n{\n  INTERNAL_FREE_HOOK(ptr, caller);\n}\n\nLIBXSMM_API void __wrap_free(void* /*ptr*/);\nLIBXSMM_API void __wrap_free(void* ptr)\n{\n  INTERNAL_FREE_HOOK(ptr, NULL/*caller*/);\n}\n\n#endif /*(defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))*/\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* memalign(size_t /*alignment*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* memalign(size_t alignment, size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* malloc(size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* malloc(size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* calloc(size_t /*num*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* calloc(size_t num, size_t size) LIBXSMM_THROW\n{\n  void* result;\n  const size_t nbytes = num * size;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# endif\n  /* TODO: signal anonymous/zeroed pages */\n  if (NULL != result) memset(result, 0, nbytes);\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void* realloc(void* /*ptr*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void* realloc(void* ptr, size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, NULL/*caller*/);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, NULL/*caller*/);\n# endif\n  return result;\n}\n#endif\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void free(void* /*ptr*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void free(void* ptr) LIBXSMM_THROW\n{\n  INTERNAL_FREE_HOOK(ptr, NULL/*caller*/);\n}\n#endif /*defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)*/\n\n\nLIBXSMM_API_INTERN void libxsmm_malloc_init(void)\n{\n#if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n  const char *const env = getenv(\"LIBXSMM_MALLOC_JOIN\");\n  if (NULL != env && 0 != *env) internal_malloc_join = atoi(env);\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n# if defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)\n  void* handle_qkmalloc = NULL;\n  dlerror(); /* clear an eventual error status */\n  handle_qkmalloc = dlopen(\"libqkmalloc.so\", RTLD_LAZY);\n  if (NULL != handle_qkmalloc) {\n    internal_malloc.memalign.ptr = internal_memalign_malloc;\n    internal_malloc.malloc.dlsym = dlsym(handle_qkmalloc, \"malloc\");\n    if (NULL == dlerror() && NULL != internal_malloc.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      internal_malloc.calloc.dlsym = dlsym(handle_qkmalloc, \"calloc\");\n      if (NULL == dlerror() && NULL != internal_malloc.calloc.dlsym)\n#   endif\n      {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        internal_malloc.realloc.dlsym = dlsym(handle_qkmalloc, \"realloc\");\n        if (NULL == dlerror() && NULL != internal_malloc.realloc.dlsym)\n#   endif\n        {\n          internal_malloc.free.dlsym = dlsym(handle_qkmalloc, \"free\");\n        }\n      }\n    }\n    dlclose(handle_qkmalloc);\n  }\n  if (NULL == internal_malloc.free.ptr)\n# elif defined(LIBXSMM_MALLOC_HOOK_KMP)\n  dlerror(); /* clear an eventual error status */\n  internal_malloc.alignmem.dlsym = dlsym(RTLD_NEXT, \"kmp_aligned_malloc\");\n  if (NULL == dlerror() && NULL != internal_malloc.alignmem.dlsym) {\n    internal_malloc.memalign.ptr = internal_memalign_twiddle;\n    internal_malloc.malloc.dlsym = dlsym(RTLD_NEXT, \"kmp_malloc\");\n    if (NULL == dlerror() && NULL != internal_malloc.malloc.dlsym) {\n# if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      internal_malloc.calloc.dlsym = dlsym(RTLD_NEXT, \"kmp_calloc\");\n      if (NULL == dlerror() && NULL != internal_malloc.calloc.dlsym)\n# endif\n      {\n# if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        internal_malloc.realloc.dlsym = dlsym(RTLD_NEXT, \"kmp_realloc\");\n        if (NULL == dlerror() && NULL != internal_malloc.realloc.dlsym)\n# endif\n        {\n          internal_malloc.free.dlsym = dlsym(RTLD_NEXT, \"kmp_free\");\n        }\n      }\n    }\n  }\n  if (NULL == internal_malloc.free.ptr)\n# endif /*defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)*/\n  {\n    dlerror(); /* clear an eventual error status */\n# if defined(LIBXSMM_GLIBC)\n    internal_malloc.memalign.dlsym = dlsym(RTLD_NEXT, \"__libc_memalign\");\n    if (NULL == dlerror() && NULL != internal_malloc.memalign.dlsym) {\n      internal_malloc.malloc.dlsym = dlsym(RTLD_NEXT, \"__libc_malloc\");\n      if (NULL == dlerror() && NULL != internal_malloc.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n        internal_malloc.calloc.dlsym = dlsym(RTLD_NEXT, \"__libc_calloc\");\n        if (NULL == dlerror() && NULL != internal_malloc.calloc.dlsym)\n#   endif\n        {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n          internal_malloc.realloc.dlsym = dlsym(RTLD_NEXT, \"__libc_realloc\");\n          if (NULL == dlerror() && NULL != internal_malloc.realloc.dlsym)\n#   endif\n          {\n            internal_malloc.free.dlsym = dlsym(RTLD_NEXT, \"__libc_free\");\n          }\n        }\n      }\n    }\n    if (NULL == internal_malloc.free.ptr) {\n      void* handle_libc = NULL;\n      dlerror(); /* clear an eventual error status */\n      handle_libc = dlopen(\"libc.so.\" LIBXSMM_STRINGIFY(LIBXSMM_MALLOC_GLIBC), RTLD_LAZY);\n      if (NULL != handle_libc) {\n        internal_malloc.memalign.dlsym = dlsym(handle_libc, \"__libc_memalign\");\n        if (NULL == dlerror() && NULL != internal_malloc.memalign.dlsym) {\n          internal_malloc.malloc.dlsym = dlsym(handle_libc, \"__libc_malloc\");\n          if (NULL == dlerror() && NULL != internal_malloc.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n            internal_malloc.calloc.dlsym = dlsym(handle_libc, \"__libc_calloc\");\n            if (NULL == dlerror() && NULL != internal_malloc.calloc.dlsym)\n#   endif\n            {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n              internal_malloc.realloc.dlsym = dlsym(handle_libc, \"__libc_realloc\");\n              if (NULL == dlerror() && NULL != internal_malloc.realloc.dlsym)\n#   endif\n              {\n                internal_malloc.free.dlsym = dlsym(handle_libc, \"__libc_free\");\n              }\n            }\n          }\n        }\n        dlclose(handle_libc);\n      }\n    }\n#   if 0\n    { /* attempt to setup deprecated GLIBC hooks */\n      union { const void* dlsym; void* (**ptr)(size_t, size_t, const void*); } hook_memalign;\n      dlerror(); /* clear an eventual error status */\n      hook_memalign.dlsym = dlsym(RTLD_NEXT, \"__memalign_hook\");\n      if (NULL == dlerror() && NULL != hook_memalign.dlsym) {\n        union { const void* dlsym; void* (**ptr)(size_t, const void*); } hook_malloc;\n        hook_malloc.dlsym = dlsym(RTLD_NEXT, \"__malloc_hook\");\n        if (NULL == dlerror() && NULL != hook_malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n          union { const void* dlsym; void* (**ptr)(void*, size_t, const void*); } hook_realloc;\n          hook_realloc.dlsym = dlsym(RTLD_NEXT, \"__realloc_hook\");\n          if (NULL == dlerror() && NULL != hook_realloc.dlsym)\n#   endif\n          {\n            union { const void* dlsym; void (**ptr)(void*, const void*); } hook_free;\n            hook_free.dlsym = dlsym(RTLD_NEXT, \"__free_hook\");\n            if (NULL == dlerror() && NULL != hook_free.dlsym) {\n              *hook_memalign.ptr = internal_memalign_hook;\n              *hook_malloc.ptr = internal_malloc_hook;\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n              *hook_realloc.ptr = internal_realloc_hook;\n#   endif\n              *hook_free.ptr = internal_free_hook;\n            }\n          }\n        }\n      }\n    }\n#   endif\n# else /* TODO */\n# endif /*defined(LIBXSMM_GLIBC)*/\n  }\n  if (NULL != internal_malloc.free.ptr) {\n# if defined(LIBXSMM_MALLOC_HOOK_IMALLOC)\n    union { const void* dlsym; libxsmm_malloc_fun* ptr; } i_malloc;\n    i_malloc.dlsym = dlsym(RTLD_NEXT, \"i_malloc\");\n    if (NULL == dlerror() && NULL != i_malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      union { const void* dlsym; void* (**ptr)(size_t, size_t); } i_calloc;\n      i_calloc.dlsym = dlsym(RTLD_NEXT, \"i_calloc\");\n      if (NULL == dlerror() && NULL != i_calloc.dlsym)\n#   endif\n      {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        union { const void* dlsym; internal_realloc_fun* ptr; } i_realloc;\n        i_realloc.dlsym = dlsym(RTLD_NEXT, \"i_realloc\");\n        if (NULL == dlerror() && NULL != i_realloc.dlsym)\n#   endif\n        {\n          union { const void* dlsym; libxsmm_free_fun* ptr; } i_free;\n          i_free.dlsym = dlsym(RTLD_NEXT, \"i_free\");\n          if (NULL == dlerror() && NULL != i_free.dlsym) {\n            *i_malloc.ptr = internal_malloc.malloc.ptr;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n            *i_calloc.ptr = internal_malloc.calloc.ptr;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n            *i_realloc.ptr = internal_malloc.realloc.ptr;\n#   endif\n            *i_free.ptr = internal_malloc.free.ptr;\n          }\n        }\n      }\n    }\n# endif /*defined(LIBXSMM_MALLOC_HOOK_IMALLOC)*/\n  }\n  else { /* fall-back: potentially recursive */\n# if defined(LIBXSMM_GLIBC)\n    internal_malloc.memalign.ptr = __libc_memalign;\n    internal_malloc.malloc.ptr = __libc_malloc;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n    internal_malloc.calloc.ptr = __libc_calloc;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n    internal_malloc.realloc.ptr = __libc_realloc;\n#   endif\n    internal_malloc.free.ptr = __libc_free;\n# else\n    internal_malloc.memalign.ptr = internal_memalign_posix;\n    internal_malloc.malloc.ptr = malloc;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n    internal_malloc.calloc.ptr = calloc;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n    internal_malloc.realloc.ptr = realloc;\n#   endif\n    internal_malloc.free.ptr = free;\n# endif\n  }\n#endif\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_malloc_finalize(void)\n{\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xset_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != lock) {\n    if (0 == libxsmm_ninit) libxsmm_init(); /* !LIBXSMM_INIT */\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n  if (NULL != malloc_fn.function && NULL != free_fn.function) {\n    libxsmm_default_allocator_context = context;\n    libxsmm_default_malloc_fn = malloc_fn;\n    libxsmm_default_free_fn = free_fn;\n  }\n  else {\n    libxsmm_malloc_function internal_malloc_fn;\n    libxsmm_free_function internal_free_fn;\n    const void* internal_allocator = NULL;\n    internal_malloc_fn.function = __real_malloc;\n    internal_free_fn.function = __real_free;\n    /*internal_allocator = NULL;*/\n    if (NULL == malloc_fn.function && NULL == free_fn.function) {\n      libxsmm_default_allocator_context = internal_allocator;\n      libxsmm_default_malloc_fn = internal_malloc_fn;\n      libxsmm_default_free_fn = internal_free_fn;\n    }\n    else { /* invalid allocator */\n      static int error_once = 0;\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: allocator setup without malloc or free function!\\n\");\n      }\n      /* keep any valid (previously instantiated) default allocator */\n      if (NULL == libxsmm_default_malloc_fn.function || NULL == libxsmm_default_free_fn.function) {\n        libxsmm_default_allocator_context = internal_allocator;\n        libxsmm_default_malloc_fn = internal_malloc_fn;\n        libxsmm_default_free_fn = internal_free_fn;\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xget_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != context || NULL != malloc_fn || NULL != free_fn) {\n    if (NULL != lock) {\n      if (0 == libxsmm_ninit) libxsmm_init(); /* !LIBXSMM_INIT */\n      LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n    }\n    if (context) *context = libxsmm_default_allocator_context;\n    if (NULL != malloc_fn) *malloc_fn = libxsmm_default_malloc_fn;\n    if (NULL != free_fn) *free_fn = libxsmm_default_free_fn;\n    if (NULL != lock) {\n      LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n    }\n  }\n  else if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    static int error_once = 0;\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid signature used to get the default memory allocator!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xset_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  int result = EXIT_SUCCESS;\n  static int error_once = 0;\n  if (NULL != lock) {\n    if (0 == libxsmm_ninit) libxsmm_init(); /* !LIBXSMM_INIT */\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n  /* make sure the default allocator is setup before adopting it eventually */\n  if (NULL == libxsmm_default_malloc_fn.function || NULL == libxsmm_default_free_fn.function) {\n    const libxsmm_malloc_function null_malloc_fn = { NULL };\n    const libxsmm_free_function null_free_fn = { NULL };\n    libxsmm_xset_default_allocator(NULL/*already locked*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n  }\n  if (NULL == malloc_fn.function && NULL == free_fn.function) { /* adopt default allocator */\n    libxsmm_scratch_allocator_context = libxsmm_default_allocator_context;\n    libxsmm_scratch_malloc_fn = libxsmm_default_malloc_fn;\n    libxsmm_scratch_free_fn = libxsmm_default_free_fn;\n  }\n  else if (NULL != malloc_fn.function) {\n    if (NULL == free_fn.function\n      && /*warning*/(LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM WARNING: scratch allocator setup without free function!\\n\");\n    }\n    libxsmm_scratch_allocator_context = context;\n    libxsmm_scratch_malloc_fn = malloc_fn;\n    libxsmm_scratch_free_fn = free_fn; /* NULL allowed */\n  }\n  else { /* invalid scratch allocator */\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid scratch allocator (default used)!\\n\");\n    }\n    /* keep any valid (previously instantiated) scratch allocator */\n    if (NULL == libxsmm_scratch_malloc_fn.function) {\n      libxsmm_scratch_allocator_context = libxsmm_default_allocator_context;\n      libxsmm_scratch_malloc_fn = libxsmm_default_malloc_fn;\n      libxsmm_scratch_free_fn = libxsmm_default_free_fn;\n    }\n    result = EXIT_FAILURE;\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xget_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != context || NULL != malloc_fn || NULL != free_fn) {\n    if (NULL != lock) {\n      if (0 == libxsmm_ninit) libxsmm_init(); /* !LIBXSMM_INIT */\n      LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n    }\n    if (context) *context = libxsmm_scratch_allocator_context;\n    if (NULL != malloc_fn) *malloc_fn = libxsmm_scratch_malloc_fn;\n    if (NULL != free_fn) *free_fn = libxsmm_scratch_free_fn;\n    if (NULL != lock) {\n      LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n    }\n  }\n  else if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    static int error_once = 0;\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid signature used to get the scratch memory allocator!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_set_default_allocator(const void* context,\n  libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  return libxsmm_xset_default_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_default_allocator(const void** context,\n  libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  return libxsmm_xget_default_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_set_scratch_allocator(const void* context,\n  libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  return libxsmm_xset_scratch_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_scratch_allocator(const void** context,\n  libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  return libxsmm_xget_scratch_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc_xinfo(const void* memory, size_t* size, int* flags, void** extra)\n{\n  int result;\n#if !defined(NDEBUG)\n  if (NULL != size || NULL != extra)\n#endif\n  {\n    const int check = ((NULL == flags || 0 == (LIBXSMM_MALLOC_FLAG_X & *flags)) ? 2 : 1);\n    const internal_malloc_info_type *const info = internal_malloc_info(memory, check);\n    if (NULL != info) {\n      if (NULL != size) *size = info->size;\n      if (NULL != flags) *flags = info->flags;\n      if (NULL != extra) *extra = info->pointer;\n      result = EXIT_SUCCESS;\n    }\n    else { /* potentially foreign buffer */\n      result = (NULL != memory ? EXIT_FAILURE : EXIT_SUCCESS);\n      if (NULL != size) *size = 0;\n      if (NULL != flags) *flags = 0;\n      if (NULL != extra) *extra = 0;\n    }\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: attachment error for memory buffer %p!\\n\", memory);\n    }\n    LIBXSMM_ASSERT_MSG(0/*false*/, \"LIBXSMM ERROR: attachment error\");\n    result = EXIT_FAILURE;\n  }\n#endif\n  return result;\n}\n\n\n#if !defined(_WIN32)\n\nLIBXSMM_API_INLINE void internal_xmalloc_mhint(void* buffer, size_t size)\n{\n  LIBXSMM_ASSERT((MAP_FAILED != buffer && NULL != buffer) || 0 == size);\n#if defined(_DEFAULT_SOURCE) || defined(_BSD_SOURCE)\n  /* proceed after failed madvise (even in case of an error; take what we got) */\n  /* issue no warning as a failure seems to be related to the kernel version */\n  madvise(buffer, size, MADV_NORMAL/*MADV_RANDOM*/\n# if defined(MADV_NOHUGEPAGE) /* if not available, we then take what we got (THP) */\n    | ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) > size ? MADV_NOHUGEPAGE : 0)\n# endif\n# if defined(MADV_DONTDUMP)\n    | ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) > size ? 0 : MADV_DONTDUMP)\n# endif\n  );\n#else\n  LIBXSMM_UNUSED(buffer); LIBXSMM_UNUSED(size);\n#endif\n}\n\n\nLIBXSMM_API_INLINE void* internal_xmalloc_xmap(const char* dir, size_t size, int flags, void** rx)\n{\n  void* result = MAP_FAILED;\n  char filename[4096] = LIBXSMM_MALLOC_XMAP_TEMPLATE;\n  int i = 0;\n  LIBXSMM_ASSERT(NULL != rx);\n  if (NULL != dir && 0 != *dir) {\n    i = LIBXSMM_SNPRINTF(filename, sizeof(filename), \"%s/\" LIBXSMM_MALLOC_XMAP_TEMPLATE, dir);\n  }\n  if (0 <= i && i < (int)sizeof(filename)) {\n    i = mkstemp(filename);\n    if (0 <= i) {\n      if (0 == unlink(filename) && 0 == ftruncate(i, size)) {\n        void *const xmap = mmap(*rx, size, PROT_READ | PROT_EXEC, flags | MAP_SHARED /*| LIBXSMM_MAP_ANONYMOUS*/, i, 0/*offset*/);\n        if (MAP_FAILED != xmap) {\n          LIBXSMM_ASSERT(NULL != xmap);\n          result = mmap(NULL, size, PROT_READ | PROT_WRITE, flags | MAP_SHARED /*| LIBXSMM_MAP_ANONYMOUS*/, i, 0/*offset*/);\n          if (MAP_FAILED != result) {\n            LIBXSMM_ASSERT(NULL != result);\n            internal_xmalloc_mhint(xmap, size);\n            *rx = xmap;\n          }\n          else {\n            munmap(xmap, size);\n          }\n        }\n      }\n      close(i);\n    }\n  }\n  return result;\n}\n\n#endif /*!defined(_WIN32)*/\n\n\nLIBXSMM_API_INLINE void* internal_xrealloc(void** ptr, internal_malloc_info_type** info, size_t size,\n  internal_realloc_fun realloc_fn, libxsmm_free_fun free_fn)\n{\n  char *const base = (char*)(NULL != *info ? (*info)->pointer : *ptr), *result;\n  LIBXSMM_ASSERT(NULL != *ptr);\n  /* may implicitly invalidate info */\n  result = (char*)realloc_fn(base, size);\n  if (result == base) { /* signal no-copy */\n    LIBXSMM_ASSERT(NULL != result);\n    *info = NULL; /* no delete */\n    *ptr = NULL; /* no copy */\n  }\n  else if (NULL != result) { /* copy */\n    const size_t offset_src = (const char*)*ptr - base;\n    *ptr = result + offset_src; /* copy */\n    *info = NULL; /* no delete */\n  }\n#if !defined(NDEBUG) && 0\n  else { /* failed */\n    if (NULL != *info) {\n      /* implicitly invalidates info */\n      internal_xfree(*ptr, *info);\n    }\n    else { /* foreign pointer */\n      free_fn(*ptr);\n    }\n    *info = NULL; /* no delete */\n    *ptr = NULL; /* no copy */\n  }\n#else\n  LIBXSMM_UNUSED(free_fn);\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void* internal_xmalloc(void** /*ptr*/, internal_malloc_info_type** /*info*/, size_t /*size*/,\n  const void* /*context*/, libxsmm_malloc_function /*malloc_fn*/, libxsmm_free_function /*free_fn*/);\nLIBXSMM_API_INTERN void* internal_xmalloc(void** ptr, internal_malloc_info_type** info, size_t size,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  void* result;\n  LIBXSMM_ASSERT(NULL != ptr && NULL != info && NULL != malloc_fn.function);\n  if (NULL == *ptr) {\n    result = (NULL == context\n      ? malloc_fn.function(size)\n      : malloc_fn.ctx_form(size, context));\n  }\n  else { /* reallocate */\n    if (NULL != free_fn.function /* prefer free_fn since it is part of pointer-info */\n      ? (__real_free == free_fn.function || free == free_fn.function)\n      : (__real_malloc == malloc_fn.function || malloc == malloc_fn.function))\n    {\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n      result = internal_xrealloc(ptr, info, size, __real_realloc, __real_free);\n#else\n      result = internal_xrealloc(ptr, info, size, realloc, __real_free);\n#endif\n    }\n    else { /* fall-back with regular allocation */\n      result = (NULL == context\n        ? malloc_fn.function(size)\n        : malloc_fn.ctx_form(size, context));\n      if (NULL == result) { /* failed */\n        if (NULL != *info) {\n          internal_xfree(*ptr, *info);\n        }\n        else { /* foreign pointer */\n          (NULL != free_fn.function ? free_fn.function : __real_free)(*ptr);\n        }\n        *ptr = NULL; /* safe delete */\n      }\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xmalloc(void** memory, size_t size, size_t alignment,\n  int flags, const void* extra, size_t extra_size)\n{\n  int result = EXIT_SUCCESS;\n#if !defined(NDEBUG)\n  if (NULL != memory)\n#endif\n  {\n    static int error_once = 0;\n    if (0 != size) {\n      void *alloc_failed = NULL, *buffer = NULL, *reloc = NULL;\n      size_t alloc_alignment = 0, alloc_size = 0, max_preserve = 0;\n      internal_malloc_info_type* info = NULL;\n      /* ATOMIC BEGIN: this region should be atomic/locked */\n      const void* context = libxsmm_default_allocator_context;\n      libxsmm_malloc_function malloc_fn = libxsmm_default_malloc_fn;\n      libxsmm_free_function free_fn = libxsmm_default_free_fn;\n      if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) {\n        context = libxsmm_scratch_allocator_context;\n        malloc_fn = libxsmm_scratch_malloc_fn;\n        free_fn = libxsmm_scratch_free_fn;\n#if defined(LIBXSMM_MALLOC_MMAP_SCRATCH)\n        flags |= LIBXSMM_MALLOC_FLAG_MMAP;\n#endif\n      }\n      if ((0 != (internal_malloc_kind & 1) && 0 < internal_malloc_kind)\n        || NULL == malloc_fn.function || NULL == free_fn.function)\n      {\n        malloc_fn.function = __real_malloc;\n        free_fn.function = __real_free;\n        context = NULL;\n      }\n      /* ATOMIC END: this region should be atomic */\n      flags |= LIBXSMM_MALLOC_FLAG_RW; /* normalize given flags since flags=0 is accepted as well */\n      if (0 != (LIBXSMM_MALLOC_FLAG_REALLOC & flags) && NULL != *memory) {\n        info = internal_malloc_info(*memory, 2/*check*/);\n        if (NULL != info) {\n          max_preserve = info->size;\n        }\n        else { /* reallocation of unknown allocation */\n          flags &= ~LIBXSMM_MALLOC_FLAG_MMAP;\n        }\n      }\n      else *memory = NULL;\n#if !defined(LIBXSMM_MALLOC_MMAP)\n      if (0 == (LIBXSMM_MALLOC_FLAG_X & flags) && 0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n        alloc_alignment = (0 == (LIBXSMM_MALLOC_FLAG_REALLOC & flags) ? libxsmm_alignment(size, alignment) : alignment);\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1;\n        buffer = internal_xmalloc(memory, &info, alloc_size, context, malloc_fn, free_fn);\n      }\n      else\n#endif\n      if (NULL == info || size != info->size) {\n#if defined(_WIN32)\n        const int xflags = (0 != (LIBXSMM_MALLOC_FLAG_X & flags) ? PAGE_EXECUTE_READWRITE : PAGE_READWRITE);\n        static SIZE_T alloc_alignmax = 0, alloc_pagesize = 0;\n        if (0 == alloc_alignmax) { /* first/one time */\n          SYSTEM_INFO system_info;\n          GetSystemInfo(&system_info);\n          alloc_pagesize = system_info.dwPageSize;\n          alloc_alignmax = GetLargePageMinimum();\n        }\n        if ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size) { /* attempt to use large pages */\n          HANDLE process_token;\n          alloc_alignment = (NULL == info\n            ? (0 == alignment ? alloc_alignmax : libxsmm_lcm(alignment, alloc_alignmax))\n            : libxsmm_lcm(alignment, alloc_alignmax));\n          alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_alignmax);\n          if (TRUE == OpenProcessToken(GetCurrentProcess(), TOKEN_ADJUST_PRIVILEGES | TOKEN_QUERY, &process_token)) {\n            TOKEN_PRIVILEGES tp;\n            if (TRUE == LookupPrivilegeValue(NULL, TEXT(\"SeLockMemoryPrivilege\"), &tp.Privileges[0].Luid)) {\n              tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED; tp.PrivilegeCount = 1; /* enable privilege */\n              if (TRUE == AdjustTokenPrivileges(process_token, FALSE, &tp, 0, (PTOKEN_PRIVILEGES)NULL, 0)\n                && ERROR_SUCCESS == GetLastError()/*may has failed (regardless of TRUE)*/)\n              {\n                /* VirtualAlloc cannot be used to reallocate memory */\n                buffer = VirtualAlloc(NULL, alloc_size, MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES, xflags);\n              }\n              tp.Privileges[0].Attributes = 0; /* disable privilege */\n              AdjustTokenPrivileges(process_token, FALSE, &tp, 0, (PTOKEN_PRIVILEGES)NULL, 0);\n            }\n            CloseHandle(process_token);\n          }\n        }\n        else { /* small allocation using regular page-size */\n          alloc_alignment = (NULL == info ? libxsmm_alignment(size, alignment) : alignment);\n          alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_pagesize);\n        }\n        if (alloc_failed == buffer) { /* small allocation or retry with regular page size */\n          /* VirtualAlloc cannot be used to reallocate memory */\n          buffer = VirtualAlloc(NULL, alloc_size, MEM_RESERVE | MEM_COMMIT, xflags);\n        }\n        if (alloc_failed != buffer) {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP; /* select the corresponding deallocation */\n        }\n        else if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) { /* fall-back allocation */\n          buffer = internal_xmalloc(memory, &info, alloc_size, context, malloc_fn, free_fn);\n        }\n#else /* !defined(_WIN32) */\n# if defined(MAP_HUGETLB)\n        static size_t hugetlb = LIBXSMM_SCRATCH_UNLIMITED;\n# endif\n# if defined(MAP_32BIT)\n        static size_t map32 = LIBXSMM_SCRATCH_UNLIMITED;\n# endif\n        int xflags = 0\n# if defined(MAP_NORESERVE)\n          | (LIBXSMM_MALLOC_ALIGNMAX < size ? 0 : MAP_NORESERVE)\n# endif\n# if defined(MAP_32BIT) /* can be quickly exhausted */\n          | (((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size && size < map32\n            && LIBXSMM_X86_AVX512_CORE > libxsmm_target_archid\n            && LIBXSMM_X86_AVX512 < libxsmm_target_archid) ? MAP_32BIT : 0)\n# endif\n# if defined(MAP_HUGETLB) /* may fail depending on system settings */\n          | (((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size && size < hugetlb) ? MAP_HUGETLB : 0)\n# endif\n# if defined(MAP_UNINITIALIZED) /* unlikely to be available */\n          | MAP_UNINITIALIZED\n# endif\n# if defined(MAP_LOCKED) && /*disadvantage*/0\n          | MAP_LOCKED\n# endif\n        ;\n        static int prefault = 0;\n# if defined(MAP_POPULATE)\n        if (0 == prefault) { /* prefault only on Linux 3.10.0-327 (and later) to avoid data race in page-fault handler */\n          struct utsname osinfo; unsigned int version_major = 3, version_minor = 10, version_update = 0, version_patch = 327;\n          if (0 <= uname(&osinfo) && 0 == strcmp(\"Linux\", osinfo.sysname)\n            && 4 == sscanf(osinfo.release, \"%u.%u.%u-%u\", &version_major, &version_minor, &version_update, &version_patch)\n            && LIBXSMM_VERSION4(3, 10, 0, 327) > LIBXSMM_VERSION4(version_major, version_minor, version_update, version_patch))\n          {\n            prefault = MAP_POPULATE;\n          }\n        }\n# endif\n        alloc_alignment = (NULL == info ? libxsmm_alignment(size, alignment) : alignment);\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1;\n        alloc_failed = MAP_FAILED;\n        if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* anonymous and non-executable */\n          LIBXSMM_ASSERT(NULL != info || NULL == *memory); /* no memory mapping of foreign pointer */\n          buffer = mmap(NULL == info ? NULL : info->pointer, alloc_size, PROT_READ | PROT_WRITE,\n            MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | prefault | xflags, -1, 0/*offset*/);\n        }\n        else { /* executable buffer requested */\n          static /*LIBXSMM_TLS*/ int fallback = -1;\n          if (0 > LIBXSMM_ATOMIC_LOAD(&fallback, LIBXSMM_ATOMIC_RELAXED)) { /* initialize fall-back allocation method */\n            FILE *const selinux = fopen(\"/sys/fs/selinux/enforce\", \"rb\");\n            const char *const env = getenv(\"LIBXSMM_SE\");\n            if (NULL != selinux) {\n              if (1 == fread(&libxsmm_se, 1/*sizeof(char)*/, 1/*count*/, selinux)) {\n                libxsmm_se = ('0' != libxsmm_se ? 1 : 0);\n              }\n              else { /* conservative assumption in case of read-error */\n                libxsmm_se = 1;\n              }\n              fclose(selinux);\n            }\n            LIBXSMM_ATOMIC_STORE(&fallback, NULL == env\n              /* libxsmm_se decides */\n              ? (0 == libxsmm_se ? LIBXSMM_MALLOC_FINAL : LIBXSMM_MALLOC_FALLBACK)\n              /* user's choice takes precedence */\n              : ('0' != *env ? LIBXSMM_MALLOC_FALLBACK : LIBXSMM_MALLOC_FINAL),\n              LIBXSMM_ATOMIC_SEQ_CST);\n            LIBXSMM_ASSERT(0 <= fallback);\n          }\n          if (0 == fallback) {\n            buffer = internal_xmalloc_xmap(\"/tmp\", alloc_size, xflags, &reloc);\n            if (alloc_failed == buffer) {\n# if defined(MAP_32BIT)\n              if (0 != (MAP_32BIT & xflags)) {\n                buffer = internal_xmalloc_xmap(\"/tmp\", alloc_size, xflags & ~MAP_32BIT, &reloc);\n              }\n              if (alloc_failed != buffer) map32 = 0; else\n# endif\n              fallback = 1;\n            }\n          }\n          if (1 <= fallback) { /* continue with fall-back */\n            if (1 == fallback) { /* 2nd try */\n              static const char* envloc = NULL;\n              if (NULL == envloc) {\n                envloc = getenv(\"JITDUMPDIR\");\n                if (NULL == envloc) envloc = \"\";\n              }\n              buffer = internal_xmalloc_xmap(envloc, alloc_size, xflags, &reloc);\n              if (alloc_failed == buffer) {\n# if defined(MAP_32BIT)\n                if (0 != (MAP_32BIT & xflags)) {\n                  buffer = internal_xmalloc_xmap(envloc, alloc_size, xflags & ~MAP_32BIT, &reloc);\n                }\n                if (alloc_failed != buffer) map32 = 0; else\n# endif\n                fallback = 2;\n              }\n            }\n            if (2 <= fallback) { /* continue with fall-back */\n              if (2 == fallback) { /* 3rd try */\n                static const char* envloc = NULL;\n                if (NULL == envloc) {\n                  envloc = getenv(\"HOME\");\n                  if (NULL == envloc) envloc = \"\";\n                }\n                buffer = internal_xmalloc_xmap(envloc, alloc_size, xflags, &reloc);\n                if (alloc_failed == buffer) {\n# if defined(MAP_32BIT)\n                  if (0 != (MAP_32BIT & xflags)) {\n                    buffer = internal_xmalloc_xmap(envloc, alloc_size, xflags & ~MAP_32BIT, &reloc);\n                  }\n                  if (alloc_failed != buffer) map32 = size; else\n# endif\n                  fallback = 3;\n                }\n              }\n              if (3 <= fallback) { /* continue with fall-back */\n                if (3 == fallback) { /* 4th try */\n                  buffer = mmap(reloc, alloc_size, PROT_READ | PROT_WRITE | PROT_EXEC,\n                    MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | xflags, -1, 0/*offset*/);\n                  if (alloc_failed == buffer) {\n# if defined(MAP_32BIT)\n                    if (0 != (MAP_32BIT & xflags)) {\n                      buffer = mmap(reloc, alloc_size, PROT_READ | PROT_WRITE | PROT_EXEC,\n                        MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | (xflags & ~MAP_32BIT), -1, 0/*offset*/);\n                    }\n                    if (alloc_failed != buffer) map32 = size; else\n# endif\n                    fallback = 4;\n                  }\n                }\n                if (4 == fallback && alloc_failed != buffer) { /* final */\n                  LIBXSMM_ASSERT(fallback == LIBXSMM_MALLOC_FINAL + 1);\n                  buffer = alloc_failed; /* trigger final fall-back */\n                }\n              }\n            }\n          }\n        }\n        if (alloc_failed != buffer && NULL != buffer) {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP; /* select deallocation */\n        }\n        else { /* allocation failed */\n# if defined(MAP_HUGETLB) /* no further attempts to rely on huge pages */\n          if (0 != (xflags & MAP_HUGETLB)) {\n            flags &= ~LIBXSMM_MALLOC_FLAG_MMAP; /* select deallocation */\n            hugetlb = size;\n          }\n# endif\n# if defined(MAP_32BIT) /* no further attempts to map to 32-bit */\n          if (0 != (xflags & MAP_32BIT)) {\n            flags &= ~LIBXSMM_MALLOC_FLAG_MMAP; /* select deallocation */\n            map32 = size;\n          }\n# endif\n          if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) { /* ultimate fall-back */\n            buffer = (NULL != malloc_fn.function\n              ? (NULL == context ? malloc_fn.function(alloc_size) : malloc_fn.ctx_form(alloc_size, context))\n              : (NULL));\n          }\n          reloc = NULL;\n        }\n        if (MAP_FAILED != buffer && NULL != buffer) {\n          internal_xmalloc_mhint(buffer, alloc_size);\n        }\n#endif\n      }\n      else { /* reallocation of the same pointer and size */\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alignment - 1;\n        if (NULL != info) {\n          buffer = info->pointer;\n          flags |= info->flags;\n        }\n        else {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP;\n          buffer = *memory;\n        }\n        alloc_alignment = alignment;\n        *memory = NULL; /* signal no-copy */\n      }\n      if (\n#if !defined(__clang_analyzer__)\n        alloc_failed != buffer &&\n#endif\n        /*fall-back*/NULL != buffer)\n      {\n        char *const cbuffer = (char*)buffer, *const aligned = LIBXSMM_ALIGN(cbuffer + extra_size + sizeof(internal_malloc_info_type), alloc_alignment);\n        internal_malloc_info_type *const buffer_info = (internal_malloc_info_type*)(aligned - sizeof(internal_malloc_info_type));\n        LIBXSMM_ASSERT((aligned + size) <= (cbuffer + alloc_size));\n        LIBXSMM_ASSERT(0 < alloc_alignment);\n        /* former content must be preserved prior to setup of buffer_info */\n        if (NULL != *memory) { /* preserve/copy previous content */\n          LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_REALLOC & flags));\n          /* content behind foreign pointers is not explicitly preserved; buffers may overlap */\n          memmove(aligned, *memory, LIBXSMM_MIN(max_preserve, size));\n          if (NULL != info /* known allocation (non-foreign pointer) */\n            && EXIT_SUCCESS != internal_xfree(*memory, info) /* !libxsmm_free */\n            && 0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          { /* display some extra context of the failure (reallocation) */\n            fprintf(stderr, \"LIBXSMM ERROR: memory reallocation failed to release memory!\\n\");\n          }\n        }\n        if (NULL != extra || 0 == extra_size) {\n          const char *const src = (const char*)extra;\n          int i; for (i = 0; i < (int)extra_size; ++i) cbuffer[i] = src[i];\n        }\n        else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: incorrect extraneous data specification!\\n\");\n          /* no EXIT_FAILURE because valid buffer is returned */\n        }\n        if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* update statistics */\n          if (0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags)) { /* public */\n            if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) { /* scratch */\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_public_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_public_max < watermark) internal_malloc_public_max = watermark; /* accept data-race */\n            }\n            else { /* local */\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_local_max < watermark) internal_malloc_local_max = watermark; /* accept data-race */\n            }\n          }\n          else { /* private */\n            const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n              &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n            if (internal_malloc_private_max < watermark) internal_malloc_private_max = watermark; /* accept data-race */\n          }\n        }\n        /* keep allocation function on record */\n        if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n          buffer_info->context = context;\n          buffer_info->free = free_fn;\n        }\n        else {\n          buffer_info->free.function = NULL;\n          buffer_info->context = NULL;\n        }\n        buffer_info->size = size; /* record user's size rather than allocated size */\n        buffer_info->pointer = buffer;\n        buffer_info->reloc = reloc;\n        buffer_info->flags = flags;\n#if defined(LIBXSMM_VTUNE)\n        buffer_info->code_id = 0;\n#endif /* info must be initialized to calculate correct checksum */\n#if !defined(LIBXSMM_MALLOC_CRC_OFF)\n# if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n        buffer_info->hash = LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &buffer_info);\n# else\n        buffer_info->hash = libxsmm_crc32(LIBXSMM_MALLOC_SEED, buffer_info,\n          (unsigned int)(((char*)&buffer_info->hash) - ((char*)buffer_info)));\n# endif\n#endif  /* finally commit/return allocated buffer */\n        *memory = aligned;\n      }\n      else {\n        if (0 != libxsmm_verbosity /* library code is expected to be mute */\n         && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: failed to allocate %s with flag=%i!\\n\",\n            libxsmm_format_size(alloc_size, \"KM\", \"B\", 10), flags);\n        }\n        result = EXIT_FAILURE;\n        *memory = NULL;\n      }\n    }\n    else {\n      if ((LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM WARNING: zero-sized memory allocation detected!\\n\");\n      }\n      *memory = NULL; /* no EXIT_FAILURE */\n    }\n  }\n#if !defined(NDEBUG)\n  else if (0 != size) {\n    result = EXIT_FAILURE;\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_xfree(const void* memory, int check)\n{\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n  static int error_once = 0;\n#endif\n  /*const*/ internal_malloc_info_type *const info = internal_malloc_info(memory, check);\n  if (NULL != info) { /* !libxsmm_free */\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n    if (EXIT_SUCCESS != internal_xfree(memory, info)) {\n      if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n    }\n#else\n    internal_xfree(memory, info);\n#endif\n  }\n  else if (NULL != memory) {\n#if 1\n    __real_free((void*)memory);\n#endif\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n    if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: deallocation does not match allocation!\\n\");\n    }\n#endif\n  }\n}\n\n\n#if defined(LIBXSMM_VTUNE)\nLIBXSMM_API_INLINE void internal_get_vtune_jitdesc(const void* code,\n  unsigned int code_id, size_t code_size, const char* code_name,\n  LIBXSMM_VTUNE_JIT_DESC_TYPE* desc)\n{\n  LIBXSMM_ASSERT(NULL != code && 0 != code_id && 0 != code_size && NULL != desc);\n  desc->method_id = code_id;\n  /* incorrect constness (method_name) */\n  desc->method_name = (char*)code_name;\n  /* incorrect constness (method_load_address) */\n  desc->method_load_address = (void*)code;\n  desc->method_size = code_size;\n  desc->line_number_size = 0;\n  desc->line_number_table = NULL;\n  desc->class_file_name = NULL;\n  desc->source_file_name = NULL;\n# if (2 <= LIBXSMM_VTUNE_JITVERSION)\n  desc->module_name = \"libxsmm.jit\";\n# endif\n}\n#endif\n\n\nLIBXSMM_API_INTERN int libxsmm_malloc_attrib(void** memory, int flags, const char* name)\n{\n  internal_malloc_info_type *const info = (NULL != memory ? internal_malloc_info(*memory, 0/*no check*/) : NULL);\n  int result = EXIT_SUCCESS;\n  static int error_once = 0;\n  if (NULL != info) {\n    void *const buffer = info->pointer;\n    const size_t size = info->size;\n#if defined(_WIN32)\n    LIBXSMM_ASSERT(NULL != buffer || 0 == size);\n#else\n    LIBXSMM_ASSERT((NULL != buffer && MAP_FAILED != buffer) || 0 == size);\n#endif\n    flags |= (info->flags & ~LIBXSMM_MALLOC_FLAG_RWX); /* merge with current flags */\n    /* quietly keep the read permission, but eventually revoke write permissions */\n    if (0 == (LIBXSMM_MALLOC_FLAG_W & flags) || 0 != (LIBXSMM_MALLOC_FLAG_X & flags)) {\n      const size_t alignment = (size_t)(((const char*)(*memory)) - ((const char*)buffer));\n      const size_t alloc_size = size + alignment;\n      if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* data-buffer; non-executable */\n#if defined(_WIN32)\n        /* TODO: implement memory protection under Microsoft Windows */\n        LIBXSMM_UNUSED(alloc_size);\n#else\n        if (EXIT_SUCCESS != mprotect(buffer, alloc_size/*entire memory region*/, PROT_READ)\n          && (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM WARNING: read-only request for buffer failed!\\n\");\n        }\n#endif\n      }\n      else { /* executable buffer requested */\n        void *const code_ptr = NULL != info->reloc ? ((void*)(((char*)info->reloc) + alignment)) : *memory;\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_X & flags));\n        if (name && *name) { /* profiler support requested */\n          if (0 > libxsmm_verbosity) { /* avoid dump when only the profiler is enabled */\n            FILE* code_file = fopen(name, \"rb\");\n            int diff = 0;\n            if (NULL == code_file) { /* file does not exist */\n              code_file = fopen(name, \"wb\");\n              if (NULL != code_file) { /* dump byte-code into a file */\n                fwrite(code_ptr, 1, size, code_file);\n                fclose(code_file);\n              }\n            }\n            else { /* check existing file */\n              const char* check_a = (const char*)code_ptr;\n              char check_b[4096];\n              size_t rest = size;\n              do {\n                const size_t n = fread(check_b, 1, LIBXSMM_MIN(sizeof(check_b), rest), code_file);\n                diff += memcmp(check_a, check_b, LIBXSMM_MIN(sizeof(check_b), n));\n                check_a += n;\n                rest -= n;\n              } while (0 < rest && 0 == diff);\n              fclose(code_file);\n            }\n            fprintf(stderr, \"LIBXSMM-JIT-DUMP(ptr:file) %p : %s\\n\", code_ptr, name);\n            if (0 != diff) { /* override existing dump and warn about erroneous condition */\n              fprintf(stderr, \"LIBXSMM ERROR: %s is shared by different code!\\n\", name);\n              code_file = fopen(name, \"wb\");\n              if (NULL != code_file) { /* dump byte-code into a file */\n                fwrite(code_ptr, 1, size, code_file);\n                fclose(code_file);\n              }\n            }\n          }\n#if defined(LIBXSMM_VTUNE)\n          if (iJIT_SAMPLING_ON == iJIT_IsProfilingActive()) {\n            LIBXSMM_VTUNE_JIT_DESC_TYPE vtune_jit_desc;\n            const unsigned int code_id = iJIT_GetNewMethodID();\n            internal_get_vtune_jitdesc(code_ptr, code_id, size, name, &vtune_jit_desc);\n            iJIT_NotifyEvent(LIBXSMM_VTUNE_JIT_LOAD, &vtune_jit_desc);\n            info->code_id = code_id;\n          }\n          else {\n            info->code_id = 0;\n          }\n#endif\n#if defined(LIBXSMM_PERF)\n          /* If JIT is enabled and a valid name is given, emit information for profiler\n           * In jitdump case this needs to be done after mprotect as it gets overwritten\n           * otherwise. */\n          libxsmm_perf_dump_code(code_ptr, size, name);\n#endif\n        }\n        if (NULL != info->reloc && info->pointer != info->reloc) {\n#if defined(_WIN32)\n          /* TODO: implement memory protection under Microsoft Windows */\n#else\n          /* memory is already protected at this point; relocate code */\n          LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n          *memory = code_ptr; /* relocate */\n          info->pointer = info->reloc;\n          info->reloc = NULL;\n# if !defined(LIBXSMM_MALLOC_CRC_OFF) /* update checksum */\n#   if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n          { const internal_malloc_info_type *const code_info = internal_malloc_info(code_ptr, 0/*no check*/);\n            info->hash = LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &code_info);\n          }\n#   else\n          info->hash = libxsmm_crc32(LIBXSMM_MALLOC_SEED, info,\n            /* info size minus actual hash value */\n            (unsigned int)(((char*)&info->hash) - ((char*)info)));\n#   endif\n# endif   /* treat memory protection errors as soft error; ignore return value */\n          munmap(buffer, alloc_size);\n#endif\n        }\n#if !defined(_WIN32)\n        else { /* malloc-based fall-back */\n          int mprotect_result;\n# if !defined(LIBXSMM_MALLOC_CRC_OFF) && defined(LIBXSMM_VTUNE) /* check checksum */\n#   if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n          LIBXSMM_ASSERT(info->hash == LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &info));\n#   else\n          LIBXSMM_ASSERT(info->hash == libxsmm_crc32(LIBXSMM_MALLOC_SEED, info,\n            /* info size minus actual hash value */\n            (unsigned int)(((char*)&info->hash) - ((char*)info))));\n#   endif\n# endif   /* treat memory protection errors as soft error; ignore return value */\n          mprotect_result = mprotect(buffer, alloc_size/*entire memory region*/, PROT_READ | PROT_EXEC);\n          if (EXIT_SUCCESS != mprotect_result) {\n            if (0 != libxsmm_se) { /* hard-error in case of SELinux */\n              if (0 != libxsmm_verbosity /* library code is expected to be mute */\n                && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n              {\n                fprintf(stderr, \"LIBXSMM ERROR: failed to allocate an executable buffer!\\n\");\n              }\n              result = mprotect_result;\n            }\n            else if ((LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n              && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n            {\n              fprintf(stderr, \"LIBXSMM WARNING: read-only request for JIT-buffer failed!\\n\");\n            }\n          }\n        }\n#endif\n      }\n    }\n  }\n  else if (NULL == memory || NULL == *memory) {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: libxsmm_malloc_attrib failed because NULL cannot be attributed!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  else if ((LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: %s buffer %p does not match!\\n\",\n      0 != (LIBXSMM_MALLOC_FLAG_X & flags) ? \"executable\" : \"memory\", *memory);\n  }\n  return result;\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_MALLOC void* libxsmm_aligned_malloc(size_t size, size_t alignment)\n{\n  void* result;\n  LIBXSMM_INIT\n  if (2 > internal_malloc_kind) {\n#if !defined(NDEBUG)\n    int status =\n#endif\n    libxsmm_xmalloc(&result, size, alignment, LIBXSMM_MALLOC_FLAG_DEFAULT, NULL/*extra*/, 0/*extra_size*/);\n    assert(EXIT_SUCCESS == status || NULL == result); /* !LIBXSMM_ASSERT */\n  }\n  else { /* scratch */\n    const void *const caller = libxsmm_trace_caller_id(0/*level*/);\n    internal_scratch_malloc(&result, size, alignment, LIBXSMM_MALLOC_FLAG_DEFAULT, caller);\n  }\n  return result;\n}\n\n\nLIBXSMM_API void* libxsmm_realloc(size_t size, void* ptr)\n{\n  const int nzeros = LIBXSMM_INTRINSICS_BITSCANFWD64((uintptr_t)ptr), alignment = 1 << nzeros;\n  LIBXSMM_ASSERT(0 == ((uintptr_t)ptr & ~(0xFFFFFFFFFFFFFFFF << nzeros)));\n  LIBXSMM_INIT\n  if (2 > internal_malloc_kind) {\n#if !defined(NDEBUG)\n    int status =\n#endif\n    libxsmm_xmalloc(&ptr, size, alignment, LIBXSMM_MALLOC_FLAG_REALLOC, NULL/*extra*/, 0/*extra_size*/);\n    assert(EXIT_SUCCESS == status || NULL == ptr); /* !LIBXSMM_ASSERT */\n  }\n  else { /* scratch */\n    const void *const caller = libxsmm_trace_caller_id(0/*level*/);\n    internal_scratch_malloc(&ptr, size, alignment, LIBXSMM_MALLOC_FLAG_REALLOC, caller);\n  }\n  return ptr;\n}\n\n\nLIBXSMM_API void* libxsmm_scratch_malloc(size_t size, size_t alignment, const void* caller)\n{\n  void* result;\n  LIBXSMM_INIT\n  internal_scratch_malloc(&result, size, alignment,\n    LIBXSMM_MALLOC_INTERNAL_CALLER != caller ? LIBXSMM_MALLOC_FLAG_DEFAULT : LIBXSMM_MALLOC_FLAG_PRIVATE,\n    caller);\n  return result;\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_MALLOC void* libxsmm_malloc(size_t size)\n{\n  return libxsmm_aligned_malloc(size, 0/*auto*/);\n}\n\n\nLIBXSMM_API void libxsmm_free(const void* memory)\n{\n  if (NULL != memory) {\n#if defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST) || /* prefer safe method if possible */ \\\n  (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(memory);\n    if (NULL != pool) { /* memory belongs to scratch domain */\n      internal_scratch_free(memory, pool);\n    }\n    else\n# endif\n    { /* local */\n      libxsmm_xfree(memory, 2/*check*/);\n    }\n#else /* lookup matching pool */\n    internal_malloc_info_type *const info = internal_malloc_info(memory, 2/*check*/);\n    static int error_once = 0;\n    if (NULL != info && 0 == (LIBXSMM_MALLOC_FLAG_SCRATCH & info->flags)) { /* !libxsmm_free */\n# if !defined(NDEBUG)\n      if (EXIT_SUCCESS != internal_xfree(memory, info)\n        && 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n# else\n      internal_xfree(memory, info); /* !libxsmm_free */\n# endif\n    }\n    else {\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(memory);\n      if (NULL != pool) { /* memory belongs to scratch domain */\n        internal_scratch_free(memory, pool);\n      }\n      else\n# endif\n      {\n# if defined(NDEBUG) && (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n        __real_free((void*)memory);\n# else\n#   if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n        __real_free((void*)memory);\n#   endif\n        if (0 != libxsmm_verbosity && /* library code is expected to be mute */\n            1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: deallocation does not match allocation!\\n\");\n        }\n# endif\n      }\n    }\n#endif\n  }\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_xrelease_scratch(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock)\n{\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  internal_malloc_pool_type* pools = NULL;\n  libxsmm_scratch_info scratch_info;\n  LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  if (NULL != lock) {\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n# if defined(LIBXSMM_MALLOC_DELETE_SAFE)\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind)\n# endif\n  {\n    unsigned int i;\n    pools = (internal_malloc_pool_type*)LIBXSMM_UP2(internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n    for (i = 0; i < libxsmm_scratch_pools; ++i) {\n      if (0 != pools[i].instance.minsize) {\n        if (\n# if !defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST)\n          1 < pools[i].instance.counter &&\n# endif\n          NULL != pools[i].instance.buffer)\n        {\n          internal_malloc_info_type* const info = internal_malloc_info(pools[i].instance.buffer, 2/*check*/);\n          if (NULL != info) internal_xfree(info->pointer, info);\n        }\n      }\n      else break; /* early exit */\n    }\n  }\n  LIBXSMM_EXPECT(EXIT_SUCCESS, libxsmm_get_scratch_info(&scratch_info));\n  if (0 != scratch_info.npending && /* library code is expected to be mute */\n    (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: %s pending scratch-memory by %\" PRIuPTR \" allocation%s!\\n\",\n      libxsmm_format_size(internal_malloc_public_cur + internal_malloc_local_cur, \"KM\", \"B\", 10),\n      (uintptr_t)scratch_info.npending, 1 < scratch_info.npending ? \"s\" : \"\");\n  }\n  if (NULL != pools) {\n    memset(pools, 0, (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) * sizeof(internal_malloc_pool_type));\n    /* no reset: keep private watermark (internal_malloc_private_max, internal_malloc_private_cur) */\n    internal_malloc_public_max = internal_malloc_public_cur = 0;\n    internal_malloc_local_max = internal_malloc_local_cur = 0;\n    internal_malloc_scratch_nmallocs = 0;\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n#endif\n}\n\n\nLIBXSMM_API void libxsmm_release_scratch(void)\n{\n  libxsmm_xrelease_scratch(&libxsmm_lock_global);\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc_info(const void* memory, libxsmm_malloc_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != info) {\n    size_t size;\n    result = libxsmm_get_malloc_xinfo(memory, &size, NULL/*flags*/, NULL/*extra*/);\n    LIBXSMM_MEMZERO127(info);\n    if (EXIT_SUCCESS == result) {\n      info->size = size;\n    }\n#if !defined(NDEBUG) /* library code is expected to be mute */\n    else if (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) {\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM WARNING: foreign memory buffer %p discovered!\\n\", memory);\n      }\n    }\n#endif\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_scratch_info(libxsmm_scratch_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != info) {\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    LIBXSMM_MEMZERO127(info);\n    info->nmallocs = internal_malloc_scratch_nmallocs;\n    info->internal = internal_malloc_private_max;\n    info->local = internal_malloc_local_max;\n    info->size = internal_malloc_public_max;\n    { const internal_malloc_pool_type* pool = (const internal_malloc_pool_type*)LIBXSMM_UP2(internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      const internal_malloc_pool_type *const end = pool + libxsmm_scratch_pools;\n      LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n      for (; pool != end; ++pool) if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n# endif\n        if (0 != pool->instance.minsize) {\n          const size_t npending = pool->instance.counter;\n# if defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST)\n          info->npending += npending;\n# else\n          info->npending += 1 < npending ? (npending - 1) : 0;\n# endif\n          ++info->npools;\n        }\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n        else break; /* early exit */\n      }\n# endif\n    }\n#else\n    LIBXSMM_MEMZERO127(info);\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_set_scratch_limit(size_t nbytes)\n{\n  LIBXSMM_INIT\n  internal_malloc_scratch_limit = nbytes;\n}\n\n\nLIBXSMM_API size_t libxsmm_get_scratch_limit(void)\n{\n  size_t result;\n  LIBXSMM_INIT\n  if (LIBXSMM_SCRATCH_DEFAULT != internal_malloc_scratch_limit) {\n    result = internal_malloc_scratch_limit;\n  }\n  else if (0 == internal_malloc_kind) {\n    result = LIBXSMM_MALLOC_SCRATCH_LIMIT;\n  }\n  else {\n    result = LIBXSMM_SCRATCH_UNLIMITED;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_set_malloc(int enabled, const size_t* lo, const size_t* hi)\n{\n  LIBXSMM_INIT\n#if !(defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) || defined(LIBXSMM_INTERCEPT_DYNAMIC))\n  LIBXSMM_UNUSED(enabled);\n  internal_malloc_kind = 0;\n#elif defined(LIBXSMM_MALLOC) && (0 < LIBXSMM_MALLOC)\n  LIBXSMM_UNUSED(enabled);\n  internal_malloc_kind = LIBXSMM_MALLOC;\n#else\n  internal_malloc_kind = enabled;\n#endif\n  /* setup lo/hi after internal_malloc_kind! */\n  if (NULL != lo) internal_malloc_limit[0] = *lo;\n  if (NULL != hi) {\n    const size_t scratch_limit = libxsmm_get_scratch_limit();\n    const size_t malloc_upper = LIBXSMM_MIN(*hi, scratch_limit);\n    internal_malloc_limit[1] = LIBXSMM_MAX(malloc_upper, internal_malloc_limit[0]);\n  }\n  libxsmm_malloc_init();\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc(size_t* lo, size_t* hi)\n{\n  int result;\n  LIBXSMM_INIT\n  if (NULL != lo) *lo = internal_malloc_limit[0];\n  if (NULL != hi) *hi = internal_malloc_limit[1];\n#if (defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) || defined(LIBXSMM_INTERCEPT_DYNAMIC))\n  result = 0 != (internal_malloc_kind & 1) && 0 < internal_malloc_kind;\n#else\n  result = 0;\n#endif\n  return result;\n}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/documentation/libxsmm.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/documentation/libxsmm_mm.docx",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/documentation/libxsmm_prof-vtune.png",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/documentation/tensorflow.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/documentation/libxsmm_samples.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/tests/mhd_image.raw",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/samples/magazine/magazine.docx",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/samples/utilities/mhd/mhd_in.mhd",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.14-prknngma7uc35aspd7dzayj4hr655y76/spack-src/samples/deeplearning/gxm/model_zoo/cifar10/mean.binaryproto"
    ],
    "total_files": 814
}