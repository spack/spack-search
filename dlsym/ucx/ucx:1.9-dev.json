{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/ucm/util/replace.h": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n\n#ifndef UCM_UTIL_REPLACE_H_\n#define UCM_UTIL_REPLACE_H_\n\n#include <ucm/bistro/bistro.h>\n#include <ucs/datastruct/list.h>\n#include <ucs/type/status.h>\n#include <pthread.h>\n\nextern pthread_mutex_t ucm_reloc_get_orig_lock;\nextern pthread_t volatile ucm_reloc_get_orig_thread;\n\n/**\n * Define a replacement function to a memory-mapping function call, which calls\n * the event handler, and if event handler returns error code - calls the original\n * function.\n */\n\n/* Due to CUDA API redifinition we have to create proxy macro to eliminate\n * redifinition of internal finction names */\n#define UCM_DEFINE_REPLACE_FUNC(_name, _rettype, _fail_val, ...) \\\n    _UCM_DEFINE_REPLACE_FUNC(ucm_override_##_name, ucm_##_name, _rettype, _fail_val, __VA_ARGS__)\n\n#define _UCM_DEFINE_REPLACE_FUNC(_over_name, _ucm_name, _rettype, _fail_val, ...) \\\n    \\\n    /* Define a symbol which goes to the replacement - in case we are loaded first */ \\\n    _rettype _over_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        _rettype res; \\\n        UCM_BISTRO_PROLOGUE; \\\n        ucm_trace(\"%s()\", __FUNCTION__); \\\n        \\\n        if (ucs_unlikely(ucm_reloc_get_orig_thread == pthread_self())) { \\\n            return (_rettype)_fail_val; \\\n        } \\\n        res = _ucm_name(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n        UCM_BISTRO_EPILOGUE; \\\n        return res; \\\n    }\n\n#define UCM_OVERRIDE_FUNC(_name, _rettype) \\\n    _rettype _name() __attribute__ ((alias (UCS_PP_QUOTE(ucm_override_##_name)))); \\\n\n#define UCM_DEFINE_DLSYM_FUNC(_name, _rettype, _fail_val, ...) \\\n    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name, ucm_override_##_name, \\\n                          _rettype, _fail_val, __VA_ARGS__)\n\n#define _UCM_DEFINE_DLSYM_FUNC(_name, _orig_name, _over_name, _rettype, _fail_val, ...) \\\n    _rettype _over_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)); \\\n    \\\n    /* Call the original function using dlsym(RTLD_NEXT) */ \\\n    _rettype _orig_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        typedef _rettype (*func_ptr_t) (__VA_ARGS__); \\\n        static func_ptr_t orig_func_ptr = NULL; \\\n        \\\n        ucm_trace(\"%s()\", __FUNCTION__); \\\n        \\\n        if (ucs_unlikely(orig_func_ptr == NULL)) { \\\n            pthread_mutex_lock(&ucm_reloc_get_orig_lock); \\\n            ucm_reloc_get_orig_thread = pthread_self(); \\\n            orig_func_ptr = (func_ptr_t)ucm_reloc_get_orig(UCS_PP_QUOTE(_name), \\\n                                                           _over_name); \\\n            ucm_reloc_get_orig_thread = (pthread_t)-1; \\\n            pthread_mutex_unlock(&ucm_reloc_get_orig_lock); \\\n        } \\\n        return orig_func_ptr(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n    }\n\n#define UCM_DEFINE_REPLACE_DLSYM_FUNC(_name, _rettype, _fail_val, ...) \\\n    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name, ucm_override_##_name, \\\n                          _rettype, _fail_val, __VA_ARGS__) \\\n    _UCM_DEFINE_REPLACE_FUNC(ucm_override_##_name, ucm_##_name, \\\n                             _rettype, _fail_val, __VA_ARGS__)\n\n#define UCM_DEFINE_SYSCALL_FUNC(_name, _rettype, _syscall_id, ...) \\\n    /* Call syscall */ \\\n    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        return (_rettype)syscall(_syscall_id, UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n    }\n\n#if UCM_BISTRO_HOOKS\n#  define UCM_DEFINE_SELECT_FUNC(_name, _rettype, _fail_val, _syscall_id, ...) \\\n    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name##_dlsym, ucm_override_##_name, \\\n                          _rettype, _fail_val, __VA_ARGS__) \\\n    UCM_DEFINE_SYSCALL_FUNC(_name##_syscall, _rettype, _syscall_id, __VA_ARGS__) \\\n    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        return (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_BISTRO) ? \\\n               ucm_orig_##_name##_syscall(UCM_FUNC_PASS_ARGS(__VA_ARGS__)) : \\\n               ucm_orig_##_name##_dlsym(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n    }\n#else\n#  define UCM_DEFINE_SELECT_FUNC(_name, _rettype, _fail_val, _syscall_id, ...) \\\n    UCM_DEFINE_DLSYM_FUNC(_name, _rettype, _fail_val, __VA_ARGS__)\n#endif\n\n/*\n * Define argument list with given types.\n */\n#define UCM_FUNC_DEFINE_ARGS(...) \\\n    UCS_PP_FOREACH_SEP(_UCM_FUNC_ARG_DEFINE, _, \\\n                       UCS_PP_ZIP((UCS_PP_SEQ(UCS_PP_NUM_ARGS(__VA_ARGS__))), \\\n                                  (__VA_ARGS__)))\n\n/*\n * Pass auto-generated arguments to a function call.\n */\n#define UCM_FUNC_PASS_ARGS(...) \\\n    UCS_PP_FOREACH_SEP(_UCM_FUNC_ARG_PASS, _, UCS_PP_SEQ(UCS_PP_NUM_ARGS(__VA_ARGS__)))\n\n\n/*\n * Helpers\n */\n#define _UCM_FUNC_ARG_DEFINE(_, _bundle) \\\n    __UCM_FUNC_ARG_DEFINE(_, UCS_PP_TUPLE_0 _bundle, UCS_PP_TUPLE_1 _bundle)\n#define __UCM_FUNC_ARG_DEFINE(_, _index, _type) \\\n    _type UCS_PP_TOKENPASTE(arg, _index)\n#define _UCM_FUNC_ARG_PASS(_, _index) \\\n    UCS_PP_TOKENPASTE(arg, _index)\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/ucm/util/reloc.h": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n\n#ifndef UCM_UTIL_RELOC_H_\n#define UCM_UTIL_RELOC_H_\n\n#include <ucs/datastruct/list.h>\n#include <ucs/type/status.h>\n#include <ucm/util/log.h>\n#include <dlfcn.h>\n\n\n/**\n * Tracks which symbols need to be patched for currently loaded libraries and\n * for libraries to be loaded in the future. We have the 'list' field so the\n * library could put those on a list without extra memory allocations.\n */\ntypedef struct ucm_reloc_patch {\n    const char       *symbol;\n    void             *value;\n    void             *prev_value;\n    ucs_list_link_t  list;\n    char             **blacklist;\n} ucm_reloc_patch_t;\n\n\n/**\n * Modify process' relocation table.\n *\n * @param [in]  patch     What and how to modify. After this call, the structure\n *                         will be owned by the library and the same patching will\n *                         happen in all objects loaded subsequently.\n */\nucs_status_t ucm_reloc_modify(ucm_reloc_patch_t* patch);\n\n\n/**\n * Get the original implementation of 'symbol', which is not equal to 'replacement'.\n *\n * This function is static to make sure the symbol search is done from the context\n * of the shared object which defines the replacement function.\n * If the replacement function is defined in a loadbale module, the symbols it\n * imports from other libraries may not be available in global scope.\n *\n * @param [in]  symbol       Symbol name.\n * @param [in]  replacement  Symbol replacement, which should be ignored.\n *\n * @return Original function pointer for 'symbol'.\n */\nstatic void* UCS_F_MAYBE_UNUSED\nucm_reloc_get_orig(const char *symbol, void *replacement)\n{\n    const char *error;\n    void *func_ptr;\n\n    func_ptr = dlsym(RTLD_NEXT, symbol);\n    if (func_ptr == NULL) {\n        (void)dlerror();\n        func_ptr = dlsym(RTLD_DEFAULT, symbol);\n        if (func_ptr == replacement) {\n            error = dlerror();\n            ucm_fatal(\"could not find address of original %s(): %s\", symbol,\n                      error ? error : \"Unknown error\");\n        }\n    }\n\n    ucm_debug(\"original %s() is at %p\", symbol, func_ptr);\n    return func_ptr;\n}\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/ucm/util/replace.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <errno.h>\n#include <unistd.h>\n#include <sys/mman.h>\n#include <sys/syscall.h>\n\n#include <ucm/event/event.h>\n#include <ucm/util/log.h>\n#include <ucm/util/reloc.h>\n#include <ucm/util/replace.h>\n#include <ucm/mmap/mmap.h>\n#include <ucs/sys/compiler.h>\n#include <ucs/sys/preprocessor.h>\n\n#ifndef MAP_FAILED\n#define MAP_FAILED ((void*)-1)\n#endif\n\n#ifdef PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP\npthread_mutex_t ucm_reloc_get_orig_lock = PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP;\n#else\npthread_mutex_t ucm_reloc_get_orig_lock;\nstatic void ucm_reloc_get_orig_lock_init(void) __attribute__((constructor(101)));\nstatic void ucm_reloc_get_orig_lock_init(void)\n{\n\tpthread_mutexattr_t attr;\n\n\tpthread_mutexattr_init(&attr);\n\tpthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);\n\tpthread_mutex_init(&ucm_reloc_get_orig_lock, &attr);\n}\n#endif\npthread_t volatile ucm_reloc_get_orig_thread = (pthread_t)-1;\n\nUCM_DEFINE_REPLACE_FUNC(mmap,    void*, MAP_FAILED, void*, size_t, int, int, int, off_t)\nUCM_DEFINE_REPLACE_FUNC(munmap,  int,   -1,         void*, size_t)\n#if HAVE_MREMAP\nUCM_DEFINE_REPLACE_FUNC(mremap,  void*, MAP_FAILED, void*, size_t, size_t, int)\n#endif\nUCM_DEFINE_REPLACE_FUNC(shmat,   void*, MAP_FAILED, int, const void*, int)\nUCM_DEFINE_REPLACE_FUNC(shmdt,   int,   -1,         const void*)\nUCM_DEFINE_REPLACE_FUNC(sbrk,    void*, MAP_FAILED, intptr_t)\nUCM_DEFINE_REPLACE_FUNC(brk,     int,   -1,         void*)\nUCM_DEFINE_REPLACE_FUNC(madvise, int,   -1,         void*, size_t, int)\n\nUCM_DEFINE_SELECT_FUNC(mmap, void*, MAP_FAILED, SYS_mmap, void*, size_t, int, int, int, off_t)\nUCM_DEFINE_SELECT_FUNC(munmap, int, -1, SYS_munmap, void*, size_t)\n#if HAVE_MREMAP\nUCM_DEFINE_SELECT_FUNC(mremap, void*, MAP_FAILED, SYS_mremap, void*, size_t, size_t, int)\n#endif\nUCM_DEFINE_SELECT_FUNC(madvise, int, -1, SYS_madvise, void*, size_t, int)\n\n#if UCM_BISTRO_HOOKS\n#if HAVE_DECL_SYS_SHMAT\n\nUCM_DEFINE_SELECT_FUNC(shmat, void*, MAP_FAILED, SYS_shmat, int, const void*, int)\n\n#elif HAVE_DECL_SYS_IPC\n#  ifndef IPCOP_shmat\n#    define IPCOP_shmat 21\n#  endif\n\n_UCM_DEFINE_DLSYM_FUNC(shmat, ucm_orig_dlsym_shmat, ucm_override_shmat,\n                       void*, MAP_FAILED, int, const void*, int)\n\nvoid *ucm_orig_shmat(int shmid, const void *shmaddr, int shmflg)\n{\n    unsigned long res;\n    void *addr;\n\n    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {\n        return ucm_orig_dlsym_shmat(shmid, shmaddr, shmflg);\n    } else {\n        /* Using IPC syscall of shmat implementation */\n        res = syscall(SYS_ipc, IPCOP_shmat, shmid, shmflg, &addr, shmaddr);\n\n        return res ? MAP_FAILED : addr;\n    }\n}\n\n#endif\n\n#if HAVE_DECL_SYS_SHMDT\n\nUCM_DEFINE_SELECT_FUNC(shmdt, int, -1, SYS_shmdt, const void*)\n\n#elif HAVE_DECL_SYS_IPC\n#  ifndef IPCOP_shmdt\n#    define IPCOP_shmdt 22\n#  endif\n\n_UCM_DEFINE_DLSYM_FUNC(shmdt, ucm_orig_dlsym_shmdt, ucm_override_shmdt,\n                       int, -1, const void*)\n\nint ucm_orig_shmdt(const void *shmaddr)\n{\n    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {\n        return ucm_orig_dlsym_shmdt(shmaddr);\n    } else {\n        /* Using IPC syscall of shmdt implementation */\n        return syscall(SYS_ipc, IPCOP_shmdt, 0, 0, 0, shmaddr);\n    }\n}\n\n#endif\n\n#if HAVE___CURBRK\nextern void *__curbrk;\n#endif\n\n_UCM_DEFINE_DLSYM_FUNC(brk, ucm_orig_dlsym_brk, ucm_override_brk, int, -1, void*)\n\nvoid *ucm_brk_syscall(void *addr)\n{\n    return (void*)syscall(SYS_brk, addr);\n}\n\nint ucm_orig_brk(void *addr)\n{\n    void *new_addr;\n\n#if HAVE___CURBRK\n    __curbrk =\n#endif\n    new_addr = ucm_brk_syscall(addr);\n\n    if (new_addr < addr) {\n        errno = ENOMEM;\n        return -1;\n    } else {\n        return 0;\n    }\n}\n\n_UCM_DEFINE_DLSYM_FUNC(sbrk, ucm_orig_dlsym_sbrk, ucm_override_sbrk,\n                       void*, MAP_FAILED, intptr_t)\n\nvoid *ucm_orig_sbrk(intptr_t increment)\n{\n    void *prev;\n\n    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {\n        return ucm_orig_dlsym_sbrk(increment);\n    } else {\n        prev = ucm_brk_syscall(0);\n        return ucm_orig_brk(UCS_PTR_BYTE_OFFSET(prev, increment)) ? (void*)-1 : prev;\n    }\n}\n\n#else /* UCM_BISTRO_HOOKS */\n\nUCM_DEFINE_DLSYM_FUNC(sbrk, void*, MAP_FAILED, intptr_t)\nUCM_DEFINE_DLSYM_FUNC(shmat, void*, MAP_FAILED, int, const void*, int)\nUCM_DEFINE_DLSYM_FUNC(shmdt, int, -1, const void*)\n\n#endif /* UCM_BISTRO_HOOKS */\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/ucm/bistro/bistro_int.h": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifndef UCM_BISTRO_BISTRO_INT_H_\n#define UCM_BISTRO_BISTRO_INT_H_\n\n#include <sys/mman.h>\n#include <dlfcn.h>\n#include <string.h>\n#include <stdlib.h>\n\n#include <ucm/bistro/bistro.h>\n#include <ucm/util/sys.h>\n#include <ucm/util/log.h>\n#include <ucs/sys/math.h>\n#include <ucs/arch/cpu.h>\n#include <ucs/debug/assert.h>\n\n#define UCM_PROT_READ_WRITE_EXEC (PROT_READ | PROT_WRITE | PROT_EXEC)\n#define UCM_PROT_READ_EXEC       (PROT_READ | PROT_EXEC)\n\n#define UCM_LOOKUP_SYMBOL(_func, _symbol) \\\n    _func = ucm_bistro_lookup(_symbol);   \\\n    if (!_func) {                         \\\n        return UCS_ERR_NO_ELEM;           \\\n    }\n\nucs_status_t ucm_bistro_apply_patch(void *dst, void *patch, size_t len);\n\nucs_status_t ucm_bistro_create_restore_point(void *addr, ucm_bistro_restore_point_t **rp);\n\nstatic inline void *ucm_bistro_lookup(const char *symbol)\n{\n    void *addr;\n\n    ucs_assert(symbol != NULL);\n\n    addr = dlsym(RTLD_NEXT, symbol);\n    if (!addr) {\n        addr = dlsym(RTLD_DEFAULT, symbol);\n    }\n    return addr;\n}\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/ucs/sys/module.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#ifndef _GNU_SOURCE\n#  define _GNU_SOURCE /* for dladdr(3) */\n#endif\n\n#include \"module.h\"\n\n#include <ucs/sys/preprocessor.h>\n#include <ucs/debug/memtrack.h>\n#include <ucs/debug/assert.h>\n#include <ucs/debug/log.h>\n#include <ucs/sys/string.h>\n#include <ucs/sys/math.h>\n#include <string.h>\n#include <limits.h>\n#include <dlfcn.h>\n#include <link.h>\n#include <libgen.h>\n\n\n#define UCS_MODULE_PATH_MEMTRACK_NAME   \"module_path\"\n#define UCS_MODULE_SRCH_PATH_MAX        2\n\n#define ucs_module_debug(_fmt, ...) \\\n    ucs_log(ucs_min(UCS_LOG_LEVEL_DEBUG, ucs_global_opts.module_log_level), \\\n            _fmt, ##  __VA_ARGS__)\n#define ucs_module_trace(_fmt, ...) \\\n    ucs_log(ucs_min(UCS_LOG_LEVEL_TRACE, ucs_global_opts.module_log_level), \\\n            _fmt, ##  __VA_ARGS__)\n\nstatic struct {\n    ucs_init_once_t  init;\n    char             module_ext[NAME_MAX];\n    unsigned         srchpath_cnt;\n    char             *srch_path[UCS_MODULE_SRCH_PATH_MAX];\n} ucs_module_loader_state = {\n    .init         = UCS_INIT_ONCE_INITIALIZER,\n    .module_ext   = \".so\", /* default extension */\n    .srchpath_cnt = 0,\n    .srch_path    = { NULL, NULL}\n};\n\n/* Should be called with lock held */\nstatic void ucs_module_loader_add_dl_dir()\n{\n    char *dlpath_dup = NULL;\n    size_t max_length;\n    Dl_info dl_info;\n    char *p, *path;\n    int ret;\n\n    (void)dlerror();\n    ret = dladdr((void*)&ucs_module_loader_state, &dl_info);\n    if (ret == 0) {\n        ucs_error(\"dladdr failed: %s\", dlerror());\n        return;\n    }\n\n    ucs_module_debug(\"ucs library path: %s\", dl_info.dli_fname);\n\n    /* copy extension */\n    dlpath_dup = ucs_strdup(dl_info.dli_fname,\n                            UCS_MODULE_PATH_MEMTRACK_NAME);\n    if (dlpath_dup == NULL) {\n        return;\n    }\n\n    p = basename(dlpath_dup);\n    p = strchr(p, '.');\n    if (p != NULL) {\n        strncpy(ucs_module_loader_state.module_ext, p,\n                sizeof(ucs_module_loader_state.module_ext) - 1);\n    }\n    ucs_free(dlpath_dup);\n\n    /* copy directory component */\n    dlpath_dup = ucs_strdup(dl_info.dli_fname,\n                            UCS_MODULE_PATH_MEMTRACK_NAME);\n    if (dlpath_dup == NULL) {\n        return;\n    }\n\n    /* construct module directory path */\n    max_length = strlen(dlpath_dup) +         /* directory */\n                 1 +                          /* '/' */\n                 strlen(UCX_MODULE_SUBDIR) +  /* sub-directory */\n                 1;                           /* '\\0' */\n    path = ucs_malloc(max_length, UCS_MODULE_PATH_MEMTRACK_NAME);\n    if (path == NULL) {\n        goto out;\n    }\n\n    snprintf(path, max_length, \"%s/%s\", dirname(dlpath_dup), UCX_MODULE_SUBDIR);\n    ucs_module_loader_state.srch_path[ucs_module_loader_state.srchpath_cnt++] = path;\n\nout:\n    ucs_free(dlpath_dup);\n}\n\n/* Should be called with lock held */\nstatic void ucs_module_loader_add_install_dir()\n{\n    ucs_module_loader_state.srch_path[ucs_module_loader_state.srchpath_cnt++] =\n                    ucs_global_opts.module_dir;\n}\n\nstatic void ucs_module_loader_init_paths()\n{\n    UCS_INIT_ONCE(&ucs_module_loader_state.init) {\n        ucs_assert(ucs_module_loader_state.srchpath_cnt == 0);\n        ucs_module_loader_add_dl_dir();\n        ucs_module_loader_add_install_dir();\n        ucs_assert(ucs_module_loader_state.srchpath_cnt <= UCS_MODULE_SRCH_PATH_MAX);\n    }\n}\n\n/* Perform shallow search for a symbol */\nstatic void *ucs_module_dlsym_shallow(const char *module_path, void *dl,\n                                      const char *symbol)\n{\n    struct link_map *lm_entry;\n    Dl_info dl_info;\n    void *addr;\n    int ret;\n\n    addr = dlsym(dl, symbol);\n    if (addr == NULL) {\n        return NULL;\n    }\n\n    (void)dlerror();\n    ret = dladdr(addr, &dl_info);\n    if (ret == 0) {\n        ucs_module_debug(\"dladdr(%p) [%s] failed: %s\", addr, symbol, dlerror());\n        return NULL;\n    }\n\n    (void)dlerror();\n    ret = dlinfo(dl, RTLD_DI_LINKMAP, &lm_entry);\n    if (ret) {\n        ucs_module_debug(\"dlinfo(%p) [%s] failed: %s\", dl, module_path, dlerror());\n        return NULL;\n    }\n\n    /* return the symbol only if it was found in the requested library, and not,\n     * for example, in one of its dependencies.\n     */\n    if (lm_entry->l_addr != (uintptr_t)dl_info.dli_fbase) {\n        ucs_module_debug(\"ignoring '%s' (%p) from %s (%p), expected in %s (%lx)\",\n                         symbol, addr, ucs_basename(dl_info.dli_fname),\n                         dl_info.dli_fbase, ucs_basename(module_path),\n                         lm_entry->l_addr);\n        return NULL;\n    }\n\n    return addr;\n}\n\nstatic void ucs_module_init(const char *module_path, void *dl)\n{\n    typedef ucs_status_t (*init_func_t)();\n\n    const char *module_init_name =\n                    UCS_PP_MAKE_STRING(UCS_MODULE_CONSTRUCTOR_NAME);\n    char *fullpath, buffer[PATH_MAX];\n    init_func_t init_func;\n    ucs_status_t status;\n\n    fullpath = realpath(module_path, buffer);\n    ucs_module_trace(\"loaded %s [%p]\", fullpath, dl);\n\n    init_func = (init_func_t)ucs_module_dlsym_shallow(module_path, dl,\n                                                      module_init_name);\n    if (init_func == NULL) {\n        ucs_module_trace(\"not calling constructor '%s' in %s\", module_init_name,\n                         module_path);\n        return;\n    }\n\n    ucs_module_trace(\"calling '%s' in '%s': [%p]\", module_init_name, fullpath,\n                     init_func);\n    status = init_func();\n    if (status != UCS_OK) {\n        ucs_module_debug(\"initializing '%s' failed: %s, unloading\", fullpath,\n                         ucs_status_string(status));\n        dlclose(dl);\n    }\n}\n\nstatic void ucs_module_load_one(const char *framework, const char *module_name,\n                                unsigned flags)\n{\n    char module_path[PATH_MAX] = {0};\n    const char *error;\n    unsigned i;\n    void *dl;\n    int mode;\n\n    mode = RTLD_LAZY;\n    if (flags & UCS_MODULE_LOAD_FLAG_NODELETE) {\n        mode |= RTLD_NODELETE;\n    }\n    if (flags & UCS_MODULE_LOAD_FLAG_GLOBAL) {\n        mode |= RTLD_GLOBAL;\n    } else {\n        mode |= RTLD_LOCAL;\n    }\n\n    for (i = 0; i < ucs_module_loader_state.srchpath_cnt; ++i) {\n        snprintf(module_path, sizeof(module_path) - 1, \"%s/lib%s_%s%s\",\n                 ucs_module_loader_state.srch_path[i], framework, module_name,\n                 ucs_module_loader_state.module_ext);\n\n        /* Clear error state */\n        (void)dlerror();\n        dl = dlopen(module_path, mode);\n        if (dl != NULL) {\n            ucs_module_init(module_path, dl);\n            break;\n        } else {\n            /* If a module fails to load, silently give up */\n            error = dlerror();\n            ucs_module_debug(\"dlopen('%s', mode=0x%x) failed: %s\", module_path,\n                             mode, error ? error : \"Unknown error\");\n        }\n    }\n\n    /* coverity[leaked_storage] : a loaded module is never unloaded */\n}\n\nvoid ucs_load_modules(const char *framework, const char *modules,\n                      ucs_init_once_t *init_once, unsigned flags)\n{\n    char *modules_str;\n    char *saveptr;\n    char *module_name;\n\n    ucs_module_loader_init_paths();\n\n    UCS_INIT_ONCE(init_once) {\n        ucs_module_debug(\"loading modules for %s\", framework);\n        modules_str = ucs_strdup(modules, \"modules_list\");\n        if (modules_str != NULL) {\n            saveptr     = NULL;\n            module_name = strtok_r(modules_str, \":\", &saveptr);\n            while (module_name != NULL) {\n                ucs_module_load_one(framework, module_name, flags);\n                module_name = strtok_r(NULL, \":\", &saveptr);\n            }\n            ucs_free(modules_str);\n        } else {\n            ucs_error(\"failed to allocate module names list\");\n        }\n    }\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/ucs/debug/debug.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2014.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include \"debug.h\"\n#include \"log.h\"\n\n#include <ucs/datastruct/khash.h>\n#include <ucs/profile/profile.h>\n#include <ucs/sys/checker.h>\n#include <ucs/sys/string.h>\n#include <ucs/sys/math.h>\n#include <ucs/sys/sys.h>\n#include <ucs/type/spinlock.h>\n#include <sys/wait.h>\n#include <execinfo.h>\n#include <dlfcn.h>\n#include <link.h>\n#include <dirent.h>\n#ifdef HAVE_DETAILED_BACKTRACE\n#  include <bfd.h>\n#endif /* HAVE_DETAILED_BACKTRACE */\n\n\nKHASH_MAP_INIT_INT64(ucs_debug_symbol, char*);\nKHASH_MAP_INIT_INT(ucs_signal_orig_action, struct sigaction*);\n\n#define UCS_GDB_MAX_ARGS         32\n#define BACKTRACE_MAX            64\n#define UCS_DEBUG_UNKNOWN_SYM    \"???\"\n\n#ifdef HAVE_DETAILED_BACKTRACE\n#    define UCS_DEBUG_BACKTRACE_LINE_FMT \"%2d 0x%016lx %s()  %s:%u\\n\"\n#    define UCS_DEBUG_BACKTRACE_LINE_ARG(_n, _line) \\\n         _n, (_line)->address, \\\n         (_line)->function ? (_line)->function : \"??\", \\\n         (_line)->file ? (_line)->file : \"??\", \\\n         (_line)->lineno\n#else\n#    define UCS_DEBUG_BACKTRACE_LINE_FMT \"%2d  %s\\n\"\n#    define UCS_DEBUG_BACKTRACE_LINE_ARG(_n, _line) _n, (_line)->symbol\n#endif\n\nstruct dl_address_search {\n    unsigned long            address;\n    const char               *filename;\n    unsigned long            base;\n};\n\n#ifdef HAVE_DETAILED_BACKTRACE\n\n#if HAVE_DECL_BFD_GET_SECTION_FLAGS\n#  define ucs_debug_bfd_section_flags(_abfd, _section) \\\n    bfd_get_section_flags(_abfd, _section)\n#elif HAVE_DECL_BFD_SECTION_FLAGS\n#  define ucs_debug_bfd_section_flags(_abfd, _section) \\\n    bfd_section_flags(_section)\n#else\n#  error \"Unsupported BFD API\"\n#endif\n\n#if HAVE_DECL_BFD_GET_SECTION_VMA\n#  define ucs_debug_bfd_section_vma(_abfd, _section) \\\n    bfd_get_section_vma(_abfd, _section)\n#elif HAVE_DECL_BFD_SECTION_VMA\n#  define ucs_debug_bfd_section_vma(_abfd, _section) \\\n    bfd_section_vma(_section)\n#else\n#  error \"Unsupported BFD API\"\n#endif\n\n#if HAVE_1_ARG_BFD_SECTION_SIZE\n#  define ucs_debug_bfd_section_size(_abfd, _section) \\\n    bfd_section_size(_section)\n#else\n#  define ucs_debug_bfd_section_size(_abfd, _section) \\\n    bfd_section_size(_abfd, _section);\n#endif\n\nstruct backtrace_line {\n    unsigned long            address;\n    char                     *file;\n    char                     *function;\n    unsigned                 lineno;\n};\n\nstruct backtrace_file {\n    struct dl_address_search dl;\n    bfd                      *abfd;\n    asymbol                  **syms;\n};\n\nstruct backtrace {\n    struct backtrace_line    lines[BACKTRACE_MAX];\n    int                      size;\n    int                      position;\n};\n\nstruct backtrace_search {\n    int                      count;\n    struct backtrace_file    *file;\n    int                      backoff; /* search the line where the function call\n                                         took place, instead of return address */\n    struct backtrace_line    *lines;\n    int                      max_lines;\n};\n\n#else /* HAVE_DETAILED_BACKTRACE */\n\nstruct backtrace_line {\n    void                     *address;\n    char                     *symbol;\n};\n\nstruct backtrace {\n    char                     **symbols;\n    void                     *addresses[BACKTRACE_MAX];\n    int                      size;\n    int                      position;\n    struct backtrace_line    line;\n};\n\n#endif /* HAVE_DETAILED_BACKTRACE */\n\n#define UCS_SYS_SIGNAME(signame) [SIG ## signame] = #signame\nconst char *ucs_signal_names[] = {\n    [0] = \"SIGNAL0\",\n    UCS_SYS_SIGNAME(HUP),\n    UCS_SYS_SIGNAME(INT),\n    UCS_SYS_SIGNAME(QUIT),\n    UCS_SYS_SIGNAME(ILL),\n    UCS_SYS_SIGNAME(TRAP),\n    UCS_SYS_SIGNAME(ABRT),\n    UCS_SYS_SIGNAME(BUS),\n    UCS_SYS_SIGNAME(FPE),\n    UCS_SYS_SIGNAME(KILL),\n    UCS_SYS_SIGNAME(USR1),\n    UCS_SYS_SIGNAME(SEGV),\n    UCS_SYS_SIGNAME(USR2),\n    UCS_SYS_SIGNAME(PIPE),\n    UCS_SYS_SIGNAME(ALRM),\n    UCS_SYS_SIGNAME(TERM),\n#ifdef SIGSTKFLT\n    UCS_SYS_SIGNAME(STKFLT),\n#endif\n    UCS_SYS_SIGNAME(CHLD),\n    UCS_SYS_SIGNAME(CONT),\n    UCS_SYS_SIGNAME(STOP),\n    UCS_SYS_SIGNAME(TSTP),\n    UCS_SYS_SIGNAME(TTIN),\n    UCS_SYS_SIGNAME(TTOU),\n    UCS_SYS_SIGNAME(URG),\n    UCS_SYS_SIGNAME(XCPU),\n    UCS_SYS_SIGNAME(XFSZ),\n    UCS_SYS_SIGNAME(VTALRM),\n    UCS_SYS_SIGNAME(PROF),\n    UCS_SYS_SIGNAME(WINCH),\n    UCS_SYS_SIGNAME(IO),\n#ifdef SIGPWR\n    UCS_SYS_SIGNAME(PWR),\n#endif\n    UCS_SYS_SIGNAME(SYS),\n#if defined __linux__\n    [SIGSYS + 1] = NULL\n#elif defined __FreeBSD__\n    [SIGRTMIN] = NULL\n#else\n#error \"Port me\"\n#endif\n};\n\n#if HAVE_SIGACTION_SA_RESTORER\nstatic void    *ucs_debug_signal_restorer = &ucs_debug_signal_restorer;\n#endif\nstatic stack_t  ucs_debug_signal_stack    = {NULL, 0, 0};\n\nstatic khash_t(ucs_debug_symbol) ucs_debug_symbols_cache;\nstatic khash_t(ucs_signal_orig_action) ucs_signal_orig_action_map;\n\nstatic ucs_recursive_spinlock_t ucs_kh_lock;\n\nstatic int ucs_debug_initialized = 0;\n\n#ifdef HAVE_CPLUS_DEMANGLE\nextern char *cplus_demangle(const char *, int);\n#endif\n\nstatic int ucs_debug_backtrace_is_excluded(void *address, const char *symbol);\n\n\nstatic char *ucs_debug_strdup(const char *str)\n{\n    size_t length;\n    char *newstr;\n\n    length = strlen(str) + 1;\n    newstr = ucs_sys_realloc(NULL, 0, length);\n    if (newstr != NULL) {\n        strncpy(newstr, str, length);\n    }\n    return newstr;\n}\n\n#ifdef HAVE_DETAILED_BACKTRACE\n\nstatic int dl_match_address(struct dl_phdr_info *info, size_t size, void *data)\n{\n    struct dl_address_search *dl = data;\n    const ElfW(Phdr) *phdr;\n    ElfW(Addr) load_base = info->dlpi_addr;\n    long n;\n\n    phdr = info->dlpi_phdr;\n    for (n = info->dlpi_phnum; --n >= 0; phdr++) {\n        if (phdr->p_type == PT_LOAD) {\n            ElfW(Addr) vbaseaddr = phdr->p_vaddr + load_base;\n            if (dl->address >= vbaseaddr && dl->address < vbaseaddr + phdr->p_memsz) {\n                dl->filename = info->dlpi_name;\n                dl->base     = info->dlpi_addr;\n            }\n        }\n    }\n    return 0;\n}\n\nstatic int dl_lookup_address(struct dl_address_search *dl)\n{\n    dl->filename = NULL;\n    dl->base     = 0;\n\n    dl_iterate_phdr(dl_match_address, dl);\n    if (dl->filename == NULL) {\n        return 0;\n    }\n\n    if (strlen(dl->filename) == 0) {\n        dl->filename = ucs_get_exe();\n    }\n    return 1;\n}\n\n/*\n * The dl member in file should be initialized\n */\nstatic int load_file(struct backtrace_file *file)\n{\n    long symcount;\n    unsigned int size;\n    char **matching;\n\n    file->syms = NULL;\n    file->abfd = bfd_openr(file->dl.filename, NULL);\n    if (!file->abfd) {\n        goto err;\n    }\n\n    if (bfd_check_format(file->abfd, bfd_archive)) {\n        goto err_close;\n    }\n\n    if (!bfd_check_format_matches(file->abfd, bfd_object, &matching)) {\n        goto err_close;\n    }\n\n    if ((bfd_get_file_flags(file->abfd) & HAS_SYMS) == 0) {\n        goto err_close;\n    }\n\n    symcount = bfd_read_minisymbols(file->abfd, 0, (PTR)&file->syms, &size);\n    if (symcount == 0) {\n        free(file->syms);\n        symcount = bfd_read_minisymbols(file->abfd, 1, (PTR)&file->syms, &size);\n    }\n    if (symcount < 0) {\n        goto err_close;\n    }\n\n    return 1;\n\nerr_close:\n    bfd_close(file->abfd);\nerr:\n    return 0;\n}\n\nstatic void unload_file(struct backtrace_file *file)\n{\n    free(file->syms);\n    bfd_close(file->abfd);\n}\n\nstatic char *ucs_debug_demangle(const char *name)\n{\n    char *demangled = NULL;\n#ifdef HAVE_CPLUS_DEMANGLE\n    demangled = cplus_demangle(name, 0);\n#endif\n    return demangled ? demangled : strdup(name);\n}\n\nstatic void find_address_in_section(bfd *abfd, asection *section, void *data)\n{\n    struct backtrace_search *search = data;\n    bfd_size_type size;\n    bfd_vma vma;\n    unsigned long address;\n    const char *filename, *function;\n    unsigned lineno;\n    int found;\n\n    if ((search->count > 0) || (search->max_lines == 0) ||\n        ((ucs_debug_bfd_section_flags(abfd, section) & SEC_ALLOC) == 0)) {\n        return;\n    }\n\n    address = search->file->dl.address - search->file->dl.base;\n    vma = ucs_debug_bfd_section_vma(abfd, section);\n    if (address < vma) {\n        return;\n    }\n\n    size = ucs_debug_bfd_section_size(abfd, section);\n    if (address >= vma + size) {\n        return;\n    }\n\n    /* Search in address-1 to get the calling line instead of return address */\n    found = bfd_find_nearest_line(abfd, section, search->file->syms,\n                                  address - vma - search->backoff,\n                                  &filename, &function, &lineno);\n    do {\n        search->lines[search->count].address  = address;\n        search->lines[search->count].file     = strdup(filename ? filename :\n                                                       UCS_DEBUG_UNKNOWN_SYM);\n        search->lines[search->count].function = function ?\n                                                ucs_debug_demangle(function) :\n                                                strdup(UCS_DEBUG_UNKNOWN_SYM);\n        search->lines[search->count].lineno   = lineno;\n        if (search->count == 0) {\n            /* To get the inliner info, search at the original address */\n            bfd_find_nearest_line(abfd, section, search->file->syms, address - vma,\n                                  &filename, &function, &lineno);\n        }\n\n        ++search->count;\n        found = bfd_find_inliner_info(abfd, &filename, &function, &lineno);\n    } while (found && (search->count < search->max_lines));\n}\n\nstatic int get_line_info(struct backtrace_file *file, int backoff,\n                         struct backtrace_line *lines, int max)\n{\n    struct backtrace_search search;\n\n    search.file      = file;\n    search.backoff   = backoff;\n    search.count     = 0;\n    search.lines     = lines;\n    search.max_lines = max;\n    bfd_map_over_sections(file->abfd, find_address_in_section, &search);\n    return search.count;\n}\n\n/**\n * Create a backtrace from the calling location.\n *\n * @param bckt          Backtrace object.\n * @param strip         How many frames to strip.\n*/\nucs_status_t ucs_debug_backtrace_create(backtrace_h *bckt, int strip)\n{\n    size_t size = sizeof(**bckt);\n    struct backtrace_file file;\n    void *addresses[BACKTRACE_MAX];\n    int i, num_addresses;\n    ucs_status_t status;\n\n    *bckt  = NULL;\n    status = ucs_mmap_alloc(&size, (void**)bckt, 0\n                            UCS_MEMTRACK_NAME(\"debug backtrace object\"));\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    num_addresses = backtrace(addresses, BACKTRACE_MAX);\n\n    (*bckt)->size     = 0;\n    (*bckt)->position = strip;\n    for (i = 0; i < num_addresses; ++i) {\n        file.dl.address = (unsigned long)addresses[i];\n        if (dl_lookup_address(&file.dl) && load_file(&file)) {\n            (*bckt)->size += get_line_info(&file, 1,\n                                           (*bckt)->lines + (*bckt)->size,\n                                           BACKTRACE_MAX - (*bckt)->size);\n            unload_file(&file);\n        }\n    }\n\n    return UCS_OK;\n}\n\n/**\n * Destroy a backtrace and free all memory.\n *\n * @param bckt          Backtrace object.\n */\nvoid ucs_debug_backtrace_destroy(backtrace_h bckt)\n{\n    int i;\n\n    for (i = 0; i < bckt->size; ++i) {\n        free(bckt->lines[i].function);\n        free(bckt->lines[i].file);\n    }\n    bckt->size = 0;\n    ucs_mmap_free(bckt, sizeof(*bckt));\n}\n\nstatic ucs_status_t\nucs_debug_get_line_info(const char *filename, unsigned long base,\n                        unsigned long address, ucs_debug_address_info_t *info)\n{\n    struct backtrace_file file;\n    struct backtrace_line line;\n    int count;\n\n    file.dl.filename = filename;\n    file.dl.base     = base;\n    file.dl.address  = address;\n\n    if (!load_file(&file)) {\n        goto err;\n    }\n\n    count = get_line_info(&file, 0, &line, 1);\n    if (count == 0) {\n        goto err_unload;\n    }\n\n    if (line.function) {\n        ucs_strncpy_zero(info->function, line.function, sizeof(info->function));\n    } else {\n        strcpy(info->function, UCS_DEBUG_UNKNOWN_SYM);\n    }\n    if (line.file) {\n        ucs_strncpy_zero(info->source_file, line.file, sizeof(info->source_file));\n    } else {\n        strcpy(info->function, UCS_DEBUG_UNKNOWN_SYM);\n    }\n    info->line_number = line.lineno;\n\n    free(line.function);\n    free(line.file);\n    unload_file(&file);\n    return UCS_OK;\n\nerr_unload:\n    unload_file(&file);\nerr:\n    strcpy(info->function,    UCS_DEBUG_UNKNOWN_SYM);\n    strcpy(info->source_file, UCS_DEBUG_UNKNOWN_SYM);\n    info->line_number = 0;\n    return UCS_ERR_NO_ELEM;\n}\n\nucs_status_t ucs_debug_lookup_address(void *address, ucs_debug_address_info_t *info)\n{\n    struct dl_address_search dl;\n\n    dl.address = (unsigned long)address;\n    if (!dl_lookup_address(&dl)) {\n        return UCS_ERR_NO_ELEM;\n    }\n\n    memset(info, 0, sizeof(*info));\n    info->file.base = dl.base;\n    ucs_expand_path(dl.filename, info->file.path, sizeof(info->file.path));\n    return ucs_debug_get_line_info(dl.filename, dl.base, dl.address, info);\n}\n\n/**\n * Walk to the next backtrace line information.\n *\n * @param bckt          Backtrace object.\n * @param line          Filled with backtrace frame info.\n *\n * NOTE: the line remains valid as long as the backtrace object is not destroyed.\n */\nint ucs_debug_backtrace_next(backtrace_h bckt, backtrace_line_h *line)\n{\n    backtrace_line_h ln;\n\n    do {\n        if (bckt->position >= bckt->size) {\n            return 0;\n        }\n\n        ln = &bckt->lines[bckt->position++];\n    } while (ucs_debug_backtrace_is_excluded((void*)ln->address, ln->function));\n\n    *line = ln;\n    return 1;\n}\n\nstatic void ucs_debug_print_source_file(const char *file, unsigned line,\n                                        const char *function, FILE *stream)\n{\n    static const int context = 3;\n    char srcline[256];\n    unsigned n;\n    FILE *f;\n\n    f = fopen(file, \"r\");\n    if (f == NULL) {\n        return;\n    }\n\n    n = 0;\n    fprintf(stream, \"\\n\");\n    fprintf(stream, \"%s: [ %s() ]\\n\", file, function);\n    if (line > context) {\n        fprintf(stream, \"      ...\\n\");\n    }\n    while (fgets(srcline, sizeof(srcline), f) != NULL) {\n        if (abs((int)line - (int)n) <= context) {\n            fprintf(stream, \"%s %5u %s\",\n                    (n == line) ? \"==>\" : \"   \", n, srcline);\n        }\n        ++n;\n    }\n    fprintf(stream, \"\\n\");\n\n    fclose(f);\n}\n\nstatic void ucs_debug_show_innermost_source_file(FILE *stream)\n{\n    backtrace_h bckt;\n    backtrace_line_h bckt_line;\n    ucs_status_t status;\n\n    status = ucs_debug_backtrace_create(&bckt, 0);\n    if (status != UCS_OK) {\n        return;\n    }\n\n    if (ucs_debug_backtrace_next(bckt, &bckt_line)) {\n        ucs_debug_print_source_file(bckt_line->file, bckt_line->lineno,\n                                    bckt_line->function, stream);\n    }\n    ucs_debug_backtrace_destroy(bckt);\n}\n\n#else /* HAVE_DETAILED_BACKTRACE */\n\nucs_status_t ucs_debug_lookup_address(void *address, ucs_debug_address_info_t *info)\n{\n    Dl_info dl_info;\n    int ret;\n\n    ret = dladdr(address, &dl_info);\n    if (!ret) {\n        return UCS_ERR_NO_ELEM;\n    }\n\n    ucs_strncpy_safe(info->file.path, dl_info.dli_fname, sizeof(info->file.path));\n    info->file.base = (uintptr_t)dl_info.dli_fbase;\n    ucs_strncpy_safe(info->function,\n                     (dl_info.dli_sname != NULL) ? dl_info.dli_sname : UCS_DEBUG_UNKNOWN_SYM,\n                     sizeof(info->function));\n    ucs_strncpy_safe(info->source_file, UCS_DEBUG_UNKNOWN_SYM, sizeof(info->source_file));\n    info->line_number = 0;\n\n    return UCS_OK;\n}\n\n/**\n * Create a backtrace from the calling location.\n */\nucs_status_t ucs_debug_backtrace_create(backtrace_h *bckt, int strip)\n{\n    size_t size = sizeof(**bckt);\n    ucs_status_t status;\n\n    *bckt  = NULL;\n    status = ucs_mmap_alloc(&size, (void**)bckt, 0\n                            UCS_MEMTRACK_NAME(\"debug backtrace object\"));\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    (*bckt)->size     = backtrace((*bckt)->addresses, BACKTRACE_MAX);\n    (*bckt)->symbols  = backtrace_symbols((*bckt)->addresses, (*bckt)->size);\n    (*bckt)->position = strip;\n\n    return UCS_OK;\n}\n\n/**\n * Destroy a backtrace and free all memory.\n *\n * @param bckt          Backtrace object.\n */\nvoid ucs_debug_backtrace_destroy(backtrace_h bckt)\n{\n    free(bckt->symbols);\n    ucs_mmap_free(bckt, sizeof(*bckt));\n}\n\n/**\n * Walk to the next backtrace line information.\n *\n * @param bckt          Backtrace object.\n * @param line          Filled with backtrace frame info.\n *\n * NOTE: the line remains valid as long as the backtrace object is not destroyed.\n */\nint ucs_debug_backtrace_next(backtrace_h bckt, backtrace_line_h *line)\n{\n    while (bckt->position < bckt->size) {\n        bckt->line.address = bckt->addresses[bckt->position];\n        bckt->line.symbol  = bckt->symbols[bckt->position];\n        bckt->position++;\n\n        if (!ucs_debug_backtrace_is_excluded(bckt->line.address,\n                                             bckt->line.symbol)) {\n            *line = &bckt->line;\n            return 1;\n        }\n    }\n\n    return 0;\n}\n\nstatic void ucs_debug_show_innermost_source_file(FILE *stream)\n{\n}\n\n#endif /* HAVE_DETAILED_BACKTRACE */\n\n/*\n * Filter specific functions from the head of the backtrace.\n */\nvoid ucs_debug_print_backtrace(FILE *stream, int strip)\n{\n    backtrace_h bckt;\n    backtrace_line_h bckt_line;\n    int i;\n\n    ucs_debug_backtrace_create(&bckt, strip);\n    fprintf(stream, \"==== backtrace (tid:%7d) ====\\n\", ucs_get_tid());\n    for (i = 0; ucs_debug_backtrace_next(bckt, &bckt_line); ++i) {\n         fprintf(stream, UCS_DEBUG_BACKTRACE_LINE_FMT,\n                 UCS_DEBUG_BACKTRACE_LINE_ARG(i, bckt_line));\n    }\n    fprintf(stream, \"=================================\\n\");\n\n    ucs_debug_backtrace_destroy(bckt);\n}\n\n/*\n * Filter specific functions from the head of the backtrace.\n */\nvoid ucs_debug_print_backtrace_line(char *buffer, size_t maxlen,\n                                    int frame_num,\n                                    backtrace_line_h line)\n{\n    snprintf(buffer, maxlen, UCS_DEBUG_BACKTRACE_LINE_FMT,\n             UCS_DEBUG_BACKTRACE_LINE_ARG(frame_num, line));\n}\n\nconst char *ucs_debug_get_symbol_name(void *address)\n{\n    static pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n    static ucs_debug_address_info_t info;\n    int hash_extra_status;\n    ucs_status_t status;\n    khiter_t hash_it;\n    size_t length;\n    char *sym;\n\n    pthread_mutex_lock(&lock);\n    hash_it = kh_put(ucs_debug_symbol, &ucs_debug_symbols_cache,\n                     (uintptr_t)address, &hash_extra_status);\n    if (hash_extra_status == 0) {\n         sym = kh_value(&ucs_debug_symbols_cache, hash_it);\n    } else {\n        status = ucs_debug_lookup_address(address, &info);\n        if (status == UCS_OK) {\n            if (hash_extra_status == -1) {\n                /* could not add to hash, return pointer to the static buffer */\n                sym = info.function;\n                goto out;\n            }\n\n            /* add new symbol to hash */\n            ucs_assert_always(hash_it != kh_end(&ucs_debug_symbols_cache));\n            length = strlen(info.function);\n            sym = ucs_malloc(length + 1, \"debug_symbol\");\n            if (sym != NULL) {\n                ucs_strncpy_safe(sym, info.function, length + 1);\n            }\n        } else {\n            /* could not resolve symbol */\n            sym = NULL;\n        }\n        kh_value(&ucs_debug_symbols_cache, hash_it) = sym;\n    }\n\nout:\n    pthread_mutex_unlock(&lock);\n    return sym ? sym : UCS_DEBUG_UNKNOWN_SYM;\n}\n\nstatic void ucs_debugger_attach()\n{\n    static const char *vg_cmds_fmt = \"file %s\\n\"\n                                     \"target remote | vgdb\\n\";\n    static const char *bt_cmds     = \"bt\\n\"\n                                     \"list\\n\";\n    static char pid_str[16];\n    char *vg_cmds;\n    char *gdb_cmdline;\n    char gdb_commands_file[256];\n    char* argv[6 + UCS_GDB_MAX_ARGS];\n    pid_t pid, debug_pid;\n    int fd, ret, narg;\n    char UCS_V_UNUSED *self_exe;\n\n    /* Fork a process which will execute gdb and attach to the current process.\n     * We must avoid trigerring calls to malloc/free, since the heap may be corrupted.\n     * Therefore all allocations are done with mmap() or use static arrays.\n     */\n\n    debug_pid = getpid();\n\n    pid = fork();\n    if (pid < 0) {\n        ucs_log_fatal_error(\"fork returned %d: %m\", pid);\n        return;\n    }\n\n    /* retrieve values from original process, before forking */\n    self_exe = ucs_debug_strdup(ucs_get_exe());\n\n    if (pid == 0) {\n        gdb_cmdline = ucs_debug_strdup(ucs_global_opts.gdb_command);\n        narg = 0;\n        argv[narg] = strtok(gdb_cmdline, \" \\t\");\n        while (argv[narg] != NULL) {\n            ++narg;\n            argv[narg] = strtok(NULL, \" \\t\");\n        }\n\n        /* Make coverity know that argv[0] will not be affected by TMPDIR */\n        if (narg == 0) {\n            return;\n        }\n\n        if (!RUNNING_ON_VALGRIND) {\n            snprintf(pid_str, sizeof(pid_str), \"%d\", debug_pid);\n            argv[narg++] = \"-p\";\n            argv[narg++] = pid_str;\n        }\n\n        /* Generate a file name for gdb commands */\n        memset(gdb_commands_file, 0, sizeof(gdb_commands_file));\n        snprintf(gdb_commands_file, sizeof(gdb_commands_file) - 1,\n                 \"%s/.gdbcommands.uid-%d\", ucs_get_tmpdir(), geteuid());\n\n        /* Write gdb commands and add the file to argv is successful */\n        fd = open(gdb_commands_file, O_WRONLY|O_TRUNC|O_CREAT, 0600);\n        if (fd >= 0) {\n            if (RUNNING_ON_VALGRIND) {\n                vg_cmds = ucs_sys_realloc(NULL, 0, strlen(vg_cmds_fmt) + strlen(self_exe));\n                sprintf(vg_cmds, vg_cmds_fmt, self_exe);\n                if (write(fd, vg_cmds, strlen(vg_cmds)) != strlen(vg_cmds)) {\n                    ucs_log_fatal_error(\"Unable to write to command file: %m\");\n                }\n            }\n\n            if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_BACKTRACE)) {\n                if (write(fd, bt_cmds, strlen(bt_cmds)) != strlen(bt_cmds)) {\n                    ucs_log_fatal_error(\"Unable to write to command file: %m\");\n                }\n            }\n            close(fd);\n\n            argv[narg++] = \"-x\";\n            argv[narg++] = gdb_commands_file;\n        } else {\n            ucs_log_fatal_error(\"Unable to open '%s' for writing: %m\",\n                                gdb_commands_file);\n        }\n\n        argv[narg++] = NULL;\n\n        /* Execute GDB */\n        /* coverity[tainted_string] */\n        ret = execvp(argv[0], argv);\n        if (ret < 0) {\n            ucs_log_fatal_error(\"Failed to execute %s: %m\", argv[0]);\n            exit(-1);\n        }\n    }\n\n    waitpid(pid, &ret, 0);\n}\n\nstatic void UCS_F_NOINLINE ucs_debug_freeze()\n{\n    static volatile int freeze = 1;\n    while (freeze) {\n        pause();\n    }\n}\n\nstatic void ucs_debug_stop_handler(int signo)\n{\n    ucs_debug_freeze();\n}\n\nstatic ucs_status_t ucs_debug_enum_threads_cb(pid_t tid, void *ctx)\n{\n    int ret;\n\n    if ((tid != 0) && (tid != ucs_get_tid())) {\n        ret = ucs_tgkill(getpid(), tid, SIGUSR1);\n        if (ret < 0) {\n            return UCS_ERR_NO_MESSAGE;\n        }\n    }\n\n    return UCS_OK;\n}\n\nstatic void ucs_debug_stop_other_threads()\n{\n    signal(SIGUSR1, ucs_debug_stop_handler);\n    ucs_sys_enum_threads(ucs_debug_enum_threads_cb, NULL);\n}\n\nstatic void ucs_debug_send_mail(const char *message)\n{\n    FILE *stream;\n\n    if (!strlen(ucs_global_opts.error_mail_to)) {\n        return;\n    }\n\n    stream = popen(\"/usr/lib/sendmail -t\", \"w\");\n    if (stream == NULL) {\n        return;\n    }\n\n    fprintf(stdout, \"Sending notification to %s\\n\", ucs_global_opts.error_mail_to);\n    fflush(stdout);\n\n    fprintf(stream, \"To:           %s\\n\", ucs_global_opts.error_mail_to);\n    fprintf(stream, \"From:         %s\\n\", \"ucx@openucx.org\");\n    fprintf(stream, \"Subject:      ucx error report on %s\\n\",\n            ucs_get_host_name());\n    fprintf(stream, \"Content-Type: text/plain\\n\");\n    fprintf(stream, \"\\n\");\n\n    fprintf(stream, \"program: %s\\n\", ucs_get_exe());\n    fprintf(stream, \"hostname: %s\\n\", ucs_get_host_name());\n    fprintf(stream, \"process id: %d\\n\", getpid());\n    fprintf(stream, \"\\n\");\n\n    fprintf(stream, \"\\n\");\n    fprintf(stream, \"%s\\n\", message);\n    fprintf(stream, \"\\n\");\n\n    ucs_debug_show_innermost_source_file(stream);\n    ucs_debug_print_backtrace(stream, 2);\n\n    if (strlen(ucs_global_opts.error_mail_footer)) {\n        fprintf(stream, \"\\n\");\n        fprintf(stream, \"%s\\n\", ucs_global_opts.error_mail_footer);\n    }\n    fprintf(stream, \"\\n\");\n\n    pclose(stream);\n}\n\nstatic void ucs_error_freeze(const char *message)\n{\n    static pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n    char response;\n    int ret;\n\n    ucs_debug_stop_other_threads();\n\n    if (pthread_mutex_trylock(&lock) == 0) {\n        if (strlen(ucs_global_opts.gdb_command) && isatty(fileno(stdout)) &&\n            isatty(fileno(stdin)))\n        {\n            ucs_log_fatal_error(\"Process frozen, press Enter to attach a debugger...\");\n            ret = read(fileno(stdin), &response, 1); /* Use low-level input to avoid deadlock */\n            if ((ret == 1) && (response == '\\n')) {\n                ucs_debugger_attach();\n            } else {\n                ucs_debug_freeze();\n            }\n        } else {\n            ucs_debug_send_mail(message);\n            ucs_log_fatal_error(\"Process frozen...\");\n            ucs_debug_freeze();\n        }\n\n        pthread_mutex_unlock(&lock);\n    } else {\n        ucs_debug_freeze();\n    }\n}\n\nstatic const char *ucs_signal_cause_common(int si_code)\n{\n    switch (si_code) {\n    case SI_USER      : return \"kill(2) or raise(3)\";\n    case SI_KERNEL    : return \"Sent by the kernel\";\n    case SI_QUEUE     : return \"sigqueue(2)\";\n    case SI_TIMER     : return \"POSIX timer expired\";\n    case SI_MESGQ     : return \"POSIX message queue state changed\";\n    case SI_ASYNCIO   : return \"AIO completed\";\n#ifdef SI_SIGIO\n    case SI_SIGIO     : return \"queued SIGIO\";\n#endif\n#ifdef SI_TKILL\n    case SI_TKILL     : return \"tkill(2) or tgkill(2)\";\n#endif\n    default           : return \"<unknown si_code>\";\n    }\n}\n\nstatic const char *ucs_signal_cause_ill(int si_code)\n{\n    switch (si_code) {\n    case ILL_ILLOPC   : return \"illegal opcode\";\n    case ILL_ILLOPN   : return \"illegal operand\";\n    case ILL_ILLADR   : return \"illegal addressing mode\";\n    case ILL_ILLTRP   : return \"illegal trap\";\n    case ILL_PRVOPC   : return \"privileged opcode\";\n    case ILL_PRVREG   : return \"privileged register\";\n    case ILL_COPROC   : return \"coprocessor error\";\n    case ILL_BADSTK   : return \"internal stack error\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_fpe(int si_code)\n{\n    switch (si_code) {\n    case FPE_INTDIV   : return \"integer divide by zero\";\n    case FPE_INTOVF   : return \"integer overflow\";\n    case FPE_FLTDIV   : return \"floating-point divide by zero\";\n    case FPE_FLTOVF   : return \"floating-point overflow\";\n    case FPE_FLTUND   : return \"floating-point underflow\";\n    case FPE_FLTRES   : return \"floating-point inexact result\";\n    case FPE_FLTINV   : return \"floating-point invalid operation\";\n    case FPE_FLTSUB   : return \"subscript out of range\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_segv(int si_code)\n{\n    switch (si_code) {\n    case SEGV_MAPERR  : return \"address not mapped to object\";\n    case SEGV_ACCERR  : return \"invalid permissions for mapped object\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_bus(int si_code)\n{\n    switch (si_code) {\n    case BUS_ADRALN   : return \"invalid address alignment\";\n    case BUS_ADRERR   : return \"nonexistent physical address\";\n    case BUS_OBJERR   : return \"object-specific hardware error\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_trap(int si_code)\n{\n    switch (si_code) {\n    case TRAP_BRKPT   : return \"process breakpoint\";\n    case TRAP_TRACE   : return \"process trace trap\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_cld(int si_code)\n{\n    switch (si_code) {\n    case CLD_EXITED   : return \"child has exited\";\n    case CLD_KILLED   : return \"child was killed\";\n    case CLD_DUMPED   : return \"child terminated abnormally\";\n    case CLD_TRAPPED  : return \"traced child has trapped\";\n    case CLD_STOPPED  : return \"child has stopped\";\n    case CLD_CONTINUED: return \"stopped child has continued\";\n    default           : return NULL;\n    }\n}\n\nstatic void ucs_debug_handle_error_signal(int signo, const char *cause,\n                                          const char *fmt, ...)\n{\n    char buf[256];\n    va_list ap;\n\n    va_start(ap, fmt);\n    vsnprintf(buf, sizeof(buf), fmt, ap);\n    va_end(ap);\n\n    ucs_log_flush();\n    ucs_log_fatal_error(\"Caught signal %d (%s: %s%s)\", signo,\n                        strsignal(signo), cause, buf);\n    ucs_handle_error(cause);\n}\n\nstatic void ucs_error_signal_handler(int signo, siginfo_t *info, void *context)\n{\n    ucs_debug_cleanup(1);\n    ucs_log_flush();\n\n    switch (signo) {\n    case SIGILL:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_ill(info->si_code), \"\");\n        break;\n    case SIGTRAP:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_trap(info->si_code), \"\");\n        break;\n    case SIGBUS:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_bus(info->si_code), \"\");\n        break;\n    case SIGFPE:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_fpe(info->si_code), \"\");\n        break;\n    case SIGSEGV:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_segv(info->si_code),\n                                      \" at address %p\", info->si_addr);\n        break;\n    case SIGCHLD:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_cld(info->si_code), \"\");\n        break;\n    case SIGINT:\n    case SIGTERM:\n        break;\n    default:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_common(info->si_code), \"\");\n        break;\n    }\n\n    raise(signo);\n}\n\nvoid ucs_handle_error(const char *message)\n{\n    ucs_debug_cleanup(1);\n\n    if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_DEBUG)) {\n        ucs_debugger_attach();\n    } else {\n        if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_BACKTRACE)) {\n            ucs_debug_show_innermost_source_file(stderr);\n            ucs_debug_print_backtrace(stderr, 2);\n        }\n        if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_FREEZE)) {\n            ucs_error_freeze(message);\n        }\n    }\n}\n\nstatic int ucs_debug_is_error_signal(int signum)\n{\n    khiter_t hash_it;\n    int result;\n\n    if (!ucs_global_opts.handle_errors) {\n        return 0;\n    }\n\n    /* If this signal is error, but was disabled. */\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    hash_it = kh_get(ucs_signal_orig_action, &ucs_signal_orig_action_map, signum);\n    result = (hash_it != kh_end(&ucs_signal_orig_action_map));\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n    return result;\n}\n\nstatic void* ucs_debug_get_orig_func(const char *symbol, void *replacement)\n{\n    void *func_ptr;\n\n    func_ptr = dlsym(RTLD_NEXT, symbol);\n    if (func_ptr == NULL) {\n        func_ptr = dlsym(RTLD_DEFAULT, symbol);\n    }\n    return func_ptr;\n}\n\n#if !HAVE_SIGHANDLER_T\n#if HAVE___SIGHANDLER_T\ntypedef __sighandler_t *sighandler_t;\n#else\n#error \"Port me\"\n#endif\n#endif\nsighandler_t signal(int signum, sighandler_t handler)\n{\n    typedef sighandler_t (*sighandler_func_t)(int, sighandler_t);\n\n    static sighandler_func_t orig = NULL;\n\n    if (ucs_debug_initialized && ucs_debug_is_error_signal(signum)) {\n        return SIG_DFL;\n    }\n\n    if (orig == NULL) {\n        orig = (sighandler_func_t)ucs_debug_get_orig_func(\"signal\", signal);\n    }\n\n    return orig(signum, handler);\n}\n\nstatic int orig_sigaction(int signum, const struct sigaction *act,\n                          struct sigaction *oact)\n{\n    typedef int (*sigaction_func_t)(int, const struct sigaction*, struct sigaction*);\n\n    static sigaction_func_t orig = NULL;\n\n    if (orig == NULL) {\n        orig = (sigaction_func_t)ucs_debug_get_orig_func(\"sigaction\", sigaction);\n    }\n\n    return orig(signum, act, oact);\n}\n\nint sigaction(int signum, const struct sigaction *act, struct sigaction *oact)\n{\n    if (ucs_debug_initialized && ucs_debug_is_error_signal(signum)) {\n        return orig_sigaction(signum, NULL, oact); /* Return old, do not set new */\n    }\n\n    return orig_sigaction(signum, act, oact);\n}\n\nstatic void ucs_debug_signal_handler(int signo)\n{\n    ucs_log_flush();\n    ucs_global_opts.log_component.log_level = UCS_LOG_LEVEL_TRACE_DATA;\n    ucs_profile_dump();\n}\n\nstatic void ucs_debug_set_signal_alt_stack()\n{\n    int ret;\n\n    ucs_debug_signal_stack.ss_size = SIGSTKSZ +\n                                     (2 * ucs_log_get_buffer_size()) +\n                                     (sizeof(void*) * BACKTRACE_MAX) +\n                                     (128 * UCS_KBYTE);\n    ucs_debug_signal_stack.ss_sp =\n                    ucs_sys_realloc(NULL, 0, ucs_debug_signal_stack.ss_size);\n    if (ucs_debug_signal_stack.ss_sp == NULL) {\n        return;\n    }\n\n    ucs_debug_signal_stack.ss_flags = 0;\n    ret = sigaltstack(&ucs_debug_signal_stack, NULL);\n    if (ret) {\n        ucs_warn(\"sigaltstack(ss_sp=%p, ss_size=%zu) failed: %m\",\n                 ucs_debug_signal_stack.ss_sp, ucs_debug_signal_stack.ss_size);\n        ucs_sys_free(ucs_debug_signal_stack.ss_sp,\n                     ucs_debug_signal_stack.ss_size);\n        ucs_debug_signal_stack.ss_sp = NULL;\n        return;\n    }\n\n    ucs_debug(\"using signal stack %p size %zu\", ucs_debug_signal_stack.ss_sp,\n              ucs_debug_signal_stack.ss_size);\n}\n\nstatic inline void ucs_debug_save_original_sighandler(int signum,\n                                                      const struct sigaction* orig_handler)\n{\n    struct sigaction *oact_copy;\n    khiter_t hash_it;\n    int hash_extra_status;\n\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    hash_it = kh_get(ucs_signal_orig_action, &ucs_signal_orig_action_map, signum);\n    if (hash_it != kh_end(&ucs_signal_orig_action_map)) {\n        goto out;\n    }\n\n    oact_copy = ucs_malloc(sizeof(*orig_handler), \"orig_sighandler\");\n    if (oact_copy == NULL) {\n        goto out;\n    }\n\n    *oact_copy = *orig_handler;\n    hash_it = kh_put(ucs_signal_orig_action,\n                     &ucs_signal_orig_action_map,\n                     signum, &hash_extra_status);\n    kh_value(&ucs_signal_orig_action_map, hash_it) = oact_copy;\n\nout:\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n}\n\nstatic void ucs_set_signal_handler(void (*handler)(int, siginfo_t*, void *))\n{\n    struct sigaction sigact, old_action;\n    int i;\n    int ret;\n\n    sigact.sa_sigaction = handler;\n    sigact.sa_flags     = SA_SIGINFO;\n    if (ucs_debug_signal_stack.ss_sp != NULL) {\n        sigact.sa_flags |= SA_ONSTACK;\n    }\n    sigemptyset(&sigact.sa_mask);\n\n    for (i = 0; i < ucs_global_opts.error_signals.count; ++i) {\n        ret = orig_sigaction(ucs_global_opts.error_signals.signals[i], &sigact,\n                             &old_action);\n        if (ret < 0) {\n            ucs_warn(\"failed to set signal handler for sig %d : %m\",\n                     ucs_global_opts.error_signals.signals[i]);\n        }\n#if HAVE_SIGACTION_SA_RESTORER\n        ucs_debug_signal_restorer = old_action.sa_restorer;\n#endif\n        ucs_debug_save_original_sighandler(ucs_global_opts.error_signals.signals[i], &old_action);\n    }\n}\n\nstatic int ucs_debug_backtrace_is_excluded(void *address, const char *symbol)\n{\n    return\n#if HAVE_SIGACTION_SA_RESTORER\n           address == ucs_debug_signal_restorer ||\n#endif\n           !strcmp(symbol, \"ucs_handle_error\") ||\n           !strcmp(symbol, \"ucs_fatal_error_format\") ||\n           !strcmp(symbol, \"ucs_fatal_error_message\") ||\n           !strcmp(symbol, \"ucs_error_freeze\") ||\n           !strcmp(symbol, \"ucs_error_signal_handler\") ||\n           !strcmp(symbol, \"ucs_debug_handle_error_signal\") ||\n           !strcmp(symbol, \"ucs_debug_backtrace_create\") ||\n           !strcmp(symbol, \"ucs_debug_show_innermost_source_file\") ||\n           !strcmp(symbol, \"ucs_log_default_handler\") ||\n           !strcmp(symbol, \"__ucs_abort\") ||\n           !strcmp(symbol, \"ucs_log_dispatch\") ||\n           !strcmp(symbol, \"__ucs_log\") ||\n           !strcmp(symbol, \"ucs_debug_send_mail\") ||\n           (strstr(symbol, \"_L_unlock_\") == symbol);\n}\n\nstatic ucs_status_t ucs_debug_get_lib_info(Dl_info *dl_info)\n{\n    int ret;\n\n    (void)dlerror();\n    ret = dladdr(ucs_debug_get_lib_info, dl_info);\n    if (ret == 0) {\n        return UCS_ERR_NO_MEMORY;\n    }\n\n    return UCS_OK;\n}\n\nconst char *ucs_debug_get_lib_path()\n{\n    ucs_status_t status;\n    Dl_info dl_info;\n\n    status = ucs_debug_get_lib_info(&dl_info);\n    if (status != UCS_OK) {\n        return \"<failed to resolve libucs path>\";\n    }\n\n    return dl_info.dli_fname;\n}\n\nunsigned long ucs_debug_get_lib_base_addr()\n{\n    ucs_status_t status;\n    Dl_info dl_info;\n\n    status = ucs_debug_get_lib_info(&dl_info);\n    if (status != UCS_OK) {\n        return 0;\n    }\n\n    return (uintptr_t)dl_info.dli_fbase;\n}\n\nvoid ucs_debug_init()\n{\n    ucs_recursive_spinlock_init(&ucs_kh_lock, 0);\n\n    kh_init_inplace(ucs_signal_orig_action, &ucs_signal_orig_action_map);\n    kh_init_inplace(ucs_debug_symbol, &ucs_debug_symbols_cache);\n\n    if (ucs_global_opts.handle_errors) {\n        ucs_debug_set_signal_alt_stack();\n        ucs_set_signal_handler(ucs_error_signal_handler);\n    }\n    if (ucs_global_opts.debug_signo > 0) {\n        struct sigaction sigact, old_action;\n        memset(&sigact, 0, sizeof(sigact));\n        memset(&old_action, 0, sizeof(old_action));\n        sigact.sa_handler = ucs_debug_signal_handler;\n        orig_sigaction(ucs_global_opts.debug_signo, &sigact, &old_action);\n        ucs_debug_save_original_sighandler(ucs_global_opts.debug_signo, &old_action);\n    }\n\n#ifdef HAVE_DETAILED_BACKTRACE\n    bfd_init();\n#endif\n\n    ucs_debug_initialized = 1;\n}\n\nvoid ucs_debug_cleanup(int on_error)\n{\n    char *sym;\n    int signum;\n    struct sigaction *hndl;\n    ucs_status_t status;\n\n    ucs_debug_initialized = 0;\n\n    kh_foreach_key(&ucs_signal_orig_action_map, signum,\n                   ucs_debug_disable_signal(signum));\n\n    if (!on_error) {\n        kh_foreach_value(&ucs_debug_symbols_cache, sym, ucs_free(sym));\n        kh_foreach_value(&ucs_signal_orig_action_map, hndl, ucs_free(hndl));\n        kh_destroy_inplace(ucs_debug_symbol, &ucs_debug_symbols_cache);\n        kh_destroy_inplace(ucs_signal_orig_action, &ucs_signal_orig_action_map);\n    }\n\n    status = ucs_recursive_spinlock_destroy(&ucs_kh_lock);\n    if (status != UCS_OK) {\n        ucs_warn(\"ucs_recursive_spinlock_destroy() failed (%d)\", status);\n    }\n}\n\nstatic inline void ucs_debug_disable_signal_nolock(int signum)\n{\n    khiter_t hash_it;\n    struct sigaction *original_action, ucs_action;\n    int ret;\n\n    hash_it = kh_get(ucs_signal_orig_action, &ucs_signal_orig_action_map,\n                     signum);\n    if (hash_it == kh_end(&ucs_signal_orig_action_map)) {\n        ucs_warn(\"ucs_debug_disable_signal: signal %d was not set in ucs\",\n                 signum);\n        return;\n    }\n\n    original_action = kh_val(&ucs_signal_orig_action_map, hash_it);\n    ret = orig_sigaction(signum, original_action, &ucs_action);\n    if (ret < 0) {\n        ucs_warn(\"failed to set signal handler for sig %d : %m\", signum);\n    }\n\n    kh_del(ucs_signal_orig_action, &ucs_signal_orig_action_map, hash_it);\n    ucs_free(original_action);\n}\n\nvoid ucs_debug_disable_signal(int signum)\n{\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    ucs_debug_disable_signal_nolock(signum);\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n}\n\nvoid ucs_debug_disable_signals()\n{\n    int signum;\n\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    kh_foreach_key(&ucs_signal_orig_action_map, signum,\n                   ucs_debug_disable_signal_nolock(signum));\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/src/tools/perf/lib/libperf.c": "/**\n* Copyright (C) Mellanox Technologies Ltd. 2001-2019.  ALL RIGHTS RESERVED.\n* Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.\n* Copyright (C) The University of Tennessee and The University\n*               of Tennessee Research Foundation. 2015-2016. ALL RIGHTS RESERVED.\n* Copyright (C) ARM Ltd. 2017.  ALL RIGHTS RESERVED.\n* See file LICENSE for terms.\n*/\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <ucs/debug/log.h>\n#include <ucs/arch/bitops.h>\n#include <ucs/sys/module.h>\n#include <ucs/sys/string.h>\n#include <string.h>\n#include <tools/perf/lib/libperf_int.h>\n#include <unistd.h>\n\n#if _OPENMP\n#include <omp.h>\n#endif /* _OPENMP */\n\n#define ATOMIC_OP_CONFIG(_size, _op32, _op64, _op, _msg, _params, _status) \\\n    _status = __get_atomic_flag((_size), (_op32), (_op64), (_op)); \\\n    if (_status != UCS_OK) { \\\n        ucs_error(UCT_PERF_TEST_PARAMS_FMT\" does not support atomic %s for \" \\\n                  \"message size %zu bytes\", UCT_PERF_TEST_PARAMS_ARG(_params), \\\n                  (_msg)[_op], (_size)); \\\n        return _status; \\\n    }\n\n#define ATOMIC_OP_CHECK(_size, _attr, _required, _params, _msg) \\\n    if (!ucs_test_all_flags(_attr, _required)) { \\\n        if ((_params)->flags & UCX_PERF_TEST_FLAG_VERBOSE) { \\\n            ucs_error(UCT_PERF_TEST_PARAMS_FMT\" does not support required \" \\\n                      #_size\"-bit atomic: %s\", UCT_PERF_TEST_PARAMS_ARG(_params), \\\n                      (_msg)[ucs_ffs64(~(_attr) & (_required))]); \\\n        } \\\n        return UCS_ERR_UNSUPPORTED; \\\n    }\n\ntypedef struct {\n    union {\n        struct {\n            size_t     dev_addr_len;\n            size_t     iface_addr_len;\n            size_t     ep_addr_len;\n        } uct;\n        struct {\n            size_t     worker_addr_len;\n            size_t     total_wireup_len;\n        } ucp;\n    };\n    size_t             rkey_size;\n    unsigned long      recv_buffer;\n} ucx_perf_ep_info_t;\n\n\nconst ucx_perf_allocator_t* ucx_perf_mem_type_allocators[UCS_MEMORY_TYPE_LAST];\n\nstatic const char *perf_iface_ops[] = {\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_SHORT)]         = \"am short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_BCOPY)]         = \"am bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_ZCOPY)]         = \"am zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PUT_SHORT)]        = \"put short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PUT_BCOPY)]        = \"put bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PUT_ZCOPY)]        = \"put zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_GET_SHORT)]        = \"get short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_GET_BCOPY)]        = \"get bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_GET_ZCOPY)]        = \"get zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE)] = \"peer failure handler\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_IFACE)] = \"connect to iface\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_EP)]    = \"connect to ep\",\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_DUP)]           = \"full reliability\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CB_SYNC)]          = \"sync callback\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CB_ASYNC)]         = \"async callback\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PENDING)]          = \"pending\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_SHORT)]  = \"tag eager short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_BCOPY)]  = \"tag eager bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_ZCOPY)]  = \"tag eager zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_RNDV_ZCOPY)]   = \"tag rndv zcopy\"\n};\n\nstatic const char *perf_atomic_op[] = {\n     [UCT_ATOMIC_OP_ADD]   = \"add\",\n     [UCT_ATOMIC_OP_AND]   = \"and\",\n     [UCT_ATOMIC_OP_OR]    = \"or\" ,\n     [UCT_ATOMIC_OP_XOR]   = \"xor\"\n};\n\nstatic const char *perf_atomic_fop[] = {\n     [UCT_ATOMIC_OP_ADD]   = \"fetch-add\",\n     [UCT_ATOMIC_OP_AND]   = \"fetch-and\",\n     [UCT_ATOMIC_OP_OR]    = \"fetch-or\",\n     [UCT_ATOMIC_OP_XOR]   = \"fetch-xor\",\n     [UCT_ATOMIC_OP_SWAP]  = \"swap\",\n     [UCT_ATOMIC_OP_CSWAP] = \"cswap\"\n};\n\n/*\n *  This Quickselect routine is based on the algorithm described in\n *  \"Numerical recipes in C\", Second Edition,\n *  Cambridge University Press, 1992, Section 8.5, ISBN 0-521-43108-5\n *  This code by Nicolas Devillard - 1998. Public domain.\n */\nstatic ucs_time_t __find_median_quick_select(ucs_time_t arr[], int n)\n{\n    int low, high ;\n    int median;\n    int middle, ll, hh;\n\n#define ELEM_SWAP(a,b) { register ucs_time_t t=(a);(a)=(b);(b)=t; }\n\n    low = 0 ; high = n-1 ; median = (low + high) / 2;\n    for (;;) {\n        if (high <= low) /* One element only */\n            return arr[median] ;\n\n        if (high == low + 1) {  /* Two elements only */\n            if (arr[low] > arr[high])\n                ELEM_SWAP(arr[low], arr[high]) ;\n            return arr[median] ;\n        }\n\n        /* Find median of low, middle and high items; swap into position low */\n        middle = (low + high) / 2;\n        if (arr[middle] > arr[high])    ELEM_SWAP(arr[middle], arr[high]) ;\n        if (arr[low] > arr[high])       ELEM_SWAP(arr[low], arr[high]) ;\n        if (arr[middle] > arr[low])     ELEM_SWAP(arr[middle], arr[low]) ;\n\n        /* Swap low item (now in position middle) into position (low+1) */\n        ELEM_SWAP(arr[middle], arr[low+1]) ;\n\n        /* Nibble from each end towards middle, swapping items when stuck */\n        ll = low + 1;\n        hh = high;\n        for (;;) {\n            do ll++; while (arr[low] > arr[ll]) ;\n            do hh--; while (arr[hh]  > arr[low]) ;\n\n            if (hh < ll)\n                break;\n\n            ELEM_SWAP(arr[ll], arr[hh]) ;\n        }\n\n        /* Swap middle item (in position low) back into correct position */\n        ELEM_SWAP(arr[low], arr[hh]) ;\n\n        /* Re-set active partition */\n        if (hh <= median)\n            low = ll;\n        if (hh >= median)\n            high = hh - 1;\n    }\n}\n\nstatic ucs_status_t\nuct_perf_test_alloc_host(const ucx_perf_context_t *perf, size_t length,\n                         unsigned flags, uct_allocated_memory_t *alloc_mem)\n{\n    ucs_status_t status;\n\n    status = uct_iface_mem_alloc(perf->uct.iface, length,\n                                 flags, \"perftest\", alloc_mem);\n    if (status != UCS_OK) {\n        ucs_free(alloc_mem);\n        ucs_error(\"failed to allocate memory: %s\", ucs_status_string(status));\n        return status;\n    }\n\n    ucs_assert(alloc_mem->md == perf->uct.md);\n\n    return UCS_OK;\n}\n\nstatic void uct_perf_test_free_host(const ucx_perf_context_t *perf,\n                                    uct_allocated_memory_t *alloc_mem)\n{\n    uct_iface_mem_free(alloc_mem);\n}\n\nstatic void ucx_perf_test_memcpy_host(void *dst, ucs_memory_type_t dst_mem_type,\n                                      const void *src, ucs_memory_type_t src_mem_type,\n                                      size_t count)\n{\n    if ((dst_mem_type != UCS_MEMORY_TYPE_HOST) ||\n        (src_mem_type != UCS_MEMORY_TYPE_HOST)) {\n        ucs_error(\"wrong memory type passed src - %d, dst - %d\",\n                  src_mem_type, dst_mem_type);\n    } else {\n        memcpy(dst, src, count);\n    }\n}\n\nstatic ucs_status_t uct_perf_test_alloc_mem(ucx_perf_context_t *perf)\n{\n    ucx_perf_params_t *params = &perf->params;\n    ucs_status_t status;\n    unsigned flags;\n    size_t buffer_size;\n\n    if ((UCT_PERF_DATA_LAYOUT_ZCOPY == params->uct.data_layout) && params->iov_stride) {\n        buffer_size = params->msg_size_cnt * params->iov_stride;\n    } else {\n        buffer_size = ucx_perf_get_message_size(params);\n    }\n\n    /* TODO use params->alignment  */\n\n    flags = (params->flags & UCX_PERF_TEST_FLAG_MAP_NONBLOCK) ?\n             UCT_MD_MEM_FLAG_NONBLOCK : 0;\n    flags |= UCT_MD_MEM_ACCESS_ALL;\n\n    /* Allocate send buffer memory */\n    status = perf->allocator->uct_alloc(perf, buffer_size * params->thread_count,\n                                        flags, &perf->uct.send_mem);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    perf->send_buffer = perf->uct.send_mem.address;\n\n    /* Allocate receive buffer memory */\n    status = perf->allocator->uct_alloc(perf, buffer_size * params->thread_count,\n                                        flags, &perf->uct.recv_mem);\n    if (status != UCS_OK) {\n        goto err_free_send;\n    }\n\n    perf->recv_buffer = perf->uct.recv_mem.address;\n\n    /* Allocate IOV datatype memory */\n    perf->params.msg_size_cnt = params->msg_size_cnt;\n    perf->uct.iov             = malloc(sizeof(*perf->uct.iov) *\n                                       perf->params.msg_size_cnt *\n                                       params->thread_count);\n    if (NULL == perf->uct.iov) {\n        status = UCS_ERR_NO_MEMORY;\n        ucs_error(\"Failed allocate send IOV(%lu) buffer: %s\",\n                  perf->params.msg_size_cnt, ucs_status_string(status));\n        goto err_free_recv;\n    }\n\n    ucs_debug(\"allocated memory. Send buffer %p, Recv buffer %p\",\n              perf->send_buffer, perf->recv_buffer);\n    return UCS_OK;\n\nerr_free_recv:\n    perf->allocator->uct_free(perf, &perf->uct.recv_mem);\nerr_free_send:\n    perf->allocator->uct_free(perf, &perf->uct.send_mem);\nerr:\n    return status;\n}\n\nstatic void uct_perf_test_free_mem(ucx_perf_context_t *perf)\n{\n    perf->allocator->uct_free(perf, &perf->uct.send_mem);\n    perf->allocator->uct_free(perf, &perf->uct.recv_mem);\n    free(perf->uct.iov);\n}\n\nvoid ucx_perf_test_start_clock(ucx_perf_context_t *perf)\n{\n    ucs_time_t start_time = ucs_get_time();\n\n    perf->start_time_acc   = ucs_get_accurate_time();\n    perf->end_time         = (perf->params.max_time == 0.0) ? UINT64_MAX :\n                              ucs_time_from_sec(perf->params.max_time) + start_time;\n    perf->prev_time        = start_time;\n    perf->prev.time        = start_time;\n    perf->prev.time_acc    = perf->start_time_acc;\n    perf->current.time_acc = perf->start_time_acc;\n}\n\n/* Initialize/reset all parameters that could be modified by the warm-up run */\nstatic void ucx_perf_test_prepare_new_run(ucx_perf_context_t *perf,\n                                          const ucx_perf_params_t *params)\n{\n    unsigned i;\n\n    perf->max_iter          = (perf->params.max_iter == 0) ? UINT64_MAX :\n                               perf->params.max_iter;\n    perf->report_interval   = ucs_time_from_sec(perf->params.report_interval);\n    perf->current.time      = 0;\n    perf->current.msgs      = 0;\n    perf->current.bytes     = 0;\n    perf->current.iters     = 0;\n    perf->prev.msgs         = 0;\n    perf->prev.bytes        = 0;\n    perf->prev.iters        = 0;\n    perf->timing_queue_head = 0;\n\n    for (i = 0; i < TIMING_QUEUE_SIZE; ++i) {\n        perf->timing_queue[i] = 0;\n    }\n    ucx_perf_test_start_clock(perf);\n}\n\nstatic void ucx_perf_test_init(ucx_perf_context_t *perf,\n                               const ucx_perf_params_t *params)\n{\n    unsigned group_index;\n\n    perf->params = *params;\n    group_index  = rte_call(perf, group_index);\n\n    if (0 == group_index) {\n        perf->allocator = ucx_perf_mem_type_allocators[params->send_mem_type];\n    } else {\n        perf->allocator = ucx_perf_mem_type_allocators[params->recv_mem_type];\n    }\n\n    ucx_perf_test_prepare_new_run(perf, params);\n}\n\nvoid ucx_perf_calc_result(ucx_perf_context_t *perf, ucx_perf_result_t *result)\n{\n    ucs_time_t median;\n    double factor;\n\n    if (perf->params.test_type == UCX_PERF_TEST_TYPE_PINGPONG) {\n        factor = 2.0;\n    } else {\n        factor = 1.0;\n    }\n\n    result->iters = perf->current.iters;\n    result->bytes = perf->current.bytes;\n    result->elapsed_time = perf->current.time_acc - perf->start_time_acc;\n\n    /* Latency */\n    median = __find_median_quick_select(perf->timing_queue, TIMING_QUEUE_SIZE);\n    result->latency.typical = ucs_time_to_sec(median) / factor;\n\n    result->latency.moment_average =\n        (perf->current.time_acc - perf->prev.time_acc)\n        / (perf->current.iters - perf->prev.iters)\n        / factor;\n\n    result->latency.total_average =\n        (perf->current.time_acc - perf->start_time_acc)\n        / perf->current.iters\n        / factor;\n\n\n    /* Bandwidth */\n\n    result->bandwidth.typical = 0.0; // Undefined\n\n    result->bandwidth.moment_average =\n        (perf->current.bytes - perf->prev.bytes) /\n        (perf->current.time_acc - perf->prev.time_acc) * factor;\n\n    result->bandwidth.total_average =\n        perf->current.bytes /\n        (perf->current.time_acc - perf->start_time_acc) * factor;\n\n\n    /* Packet rate */\n\n    result->msgrate.typical = 0.0; // Undefined\n\n    result->msgrate.moment_average =\n        (perf->current.msgs - perf->prev.msgs) /\n        (perf->current.time_acc - perf->prev.time_acc) * factor;\n\n    result->msgrate.total_average =\n        perf->current.msgs /\n        (perf->current.time_acc - perf->start_time_acc) * factor;\n\n}\n\nstatic ucs_status_t ucx_perf_test_check_params(ucx_perf_params_t *params)\n{\n    size_t it;\n\n    /* check if zero-size messages are requested and supported */\n    if ((/* they are not supported by: */\n         /* - UCT tests, except UCT AM Short/Bcopy */\n         (params->api == UCX_PERF_API_UCT) ||\n         (/* - UCP RMA and AMO tests */\n          (params->api == UCX_PERF_API_UCP) &&\n          (params->command != UCX_PERF_CMD_AM) &&\n          (params->command != UCX_PERF_CMD_TAG) &&\n          (params->command != UCX_PERF_CMD_TAG_SYNC) &&\n          (params->command != UCX_PERF_CMD_STREAM))) &&\n        ucx_perf_get_message_size(params) < 1) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Message size too small, need to be at least 1\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    if ((params->api == UCX_PERF_API_UCP) &&\n        ((params->send_mem_type != UCS_MEMORY_TYPE_HOST) ||\n         (params->recv_mem_type != UCS_MEMORY_TYPE_HOST)) &&\n        ((params->command == UCX_PERF_CMD_PUT) ||\n         (params->command == UCX_PERF_CMD_GET) ||\n         (params->command == UCX_PERF_CMD_ADD) ||\n         (params->command == UCX_PERF_CMD_FADD) ||\n         (params->command == UCX_PERF_CMD_SWAP) ||\n         (params->command == UCX_PERF_CMD_CSWAP))) {\n        /* TODO: remove when support for non-HOST memory types will be added */\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"UCP doesn't support RMA/AMO for \\\"%s\\\"<->\\\"%s\\\" memory types\",\n                      ucs_memory_type_names[params->send_mem_type],\n                      ucs_memory_type_names[params->recv_mem_type]);\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    if (params->max_outstanding < 1) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"max_outstanding, need to be at least 1\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    /* check if particular message size fit into stride size */\n    if (params->iov_stride) {\n        for (it = 0; it < params->msg_size_cnt; ++it) {\n            if (params->msg_size_list[it] > params->iov_stride) {\n                if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                    ucs_error(\"Buffer size %lu bigger than stride %lu\",\n                              params->msg_size_list[it], params->iov_stride);\n                }\n                return UCS_ERR_INVALID_PARAM;\n            }\n        }\n    }\n\n    return UCS_OK;\n}\n\nvoid uct_perf_ep_flush_b(ucx_perf_context_t *perf, int peer_index)\n{\n    uct_ep_h ep = perf->uct.peers[peer_index].ep;\n    uct_completion_t comp;\n    ucs_status_t status;\n    int started;\n\n    started    = 0;\n    comp.func  = NULL;\n    comp.count = 2;\n    do {\n        if (!started) {\n            status = uct_ep_flush(ep, 0, &comp);\n            if (status == UCS_OK) {\n                --comp.count;\n            } else if (status == UCS_INPROGRESS) {\n                started = 1;\n            } else if (status != UCS_ERR_NO_RESOURCE) {\n                ucs_error(\"uct_ep_flush() failed: %s\", ucs_status_string(status));\n                return;\n            }\n        }\n        uct_worker_progress(perf->uct.worker);\n    } while (comp.count > 1);\n}\n\nvoid uct_perf_iface_flush_b(ucx_perf_context_t *perf)\n{\n    ucs_status_t status;\n\n    do {\n        status = uct_iface_flush(perf->uct.iface, 0, NULL);\n        uct_worker_progress(perf->uct.worker);\n    } while (status == UCS_INPROGRESS);\n    if (status != UCS_OK) {\n        ucs_error(\"uct_iface_flush() failed: %s\", ucs_status_string(status));\n    }\n}\n\nstatic inline uint64_t __get_flag(uct_perf_data_layout_t layout, uint64_t short_f,\n                                  uint64_t bcopy_f, uint64_t zcopy_f)\n{\n    return (layout == UCT_PERF_DATA_LAYOUT_SHORT) ? short_f :\n           (layout == UCT_PERF_DATA_LAYOUT_BCOPY) ? bcopy_f :\n           (layout == UCT_PERF_DATA_LAYOUT_ZCOPY) ? zcopy_f :\n           0;\n}\n\nstatic inline ucs_status_t __get_atomic_flag(size_t size, uint64_t *op32,\n                                             uint64_t *op64, uint64_t op)\n{\n    if (size == sizeof(uint32_t)) {\n        *op32 = UCS_BIT(op);\n        return UCS_OK;\n    } else if (size == sizeof(uint64_t)) {\n        *op64 = UCS_BIT(op);\n        return UCS_OK;\n    }\n    return UCS_ERR_UNSUPPORTED;\n}\n\nstatic inline size_t __get_max_size(uct_perf_data_layout_t layout, size_t short_m,\n                                    size_t bcopy_m, uint64_t zcopy_m)\n{\n    return (layout == UCT_PERF_DATA_LAYOUT_SHORT) ? short_m :\n           (layout == UCT_PERF_DATA_LAYOUT_BCOPY) ? bcopy_m :\n           (layout == UCT_PERF_DATA_LAYOUT_ZCOPY) ? zcopy_m :\n           0;\n}\n\nstatic ucs_status_t uct_perf_test_check_md_support(ucx_perf_params_t *params,\n                                                   ucs_memory_type_t mem_type,\n                                                   uct_md_attr_t *md_attr)\n{\n    if (!(md_attr->cap.access_mem_type == mem_type) &&\n        !(md_attr->cap.reg_mem_types & UCS_BIT(mem_type))) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Unsupported memory type %s by \"UCT_PERF_TEST_PARAMS_FMT,\n                      ucs_memory_type_names[mem_type],\n                      UCT_PERF_TEST_PARAMS_ARG(params));\n            return UCS_ERR_INVALID_PARAM;\n        }\n    }\n    return UCS_OK;\n}\n\nstatic ucs_status_t uct_perf_test_check_capabilities(ucx_perf_params_t *params,\n                                                     uct_iface_h iface, uct_md_h md)\n{\n    uint64_t required_flags = 0;\n    uint64_t atomic_op32    = 0;\n    uint64_t atomic_op64    = 0;\n    uint64_t atomic_fop32   = 0;\n    uint64_t atomic_fop64   = 0;\n    uct_md_attr_t md_attr;\n    uct_iface_attr_t attr;\n    ucs_status_t status;\n    size_t min_size, max_size, max_iov, message_size;\n\n    status = uct_md_query(md, &md_attr);\n    if (status != UCS_OK) {\n        ucs_error(\"uct_md_query(%s) failed: %s\",\n                  params->uct.md_name, ucs_status_string(status));\n        return status;\n    }\n\n    status = uct_iface_query(iface, &attr);\n    if (status != UCS_OK) {\n        ucs_error(\"uct_iface_query(\"UCT_PERF_TEST_PARAMS_FMT\") failed: %s\",\n                  UCT_PERF_TEST_PARAMS_ARG(params),\n                  ucs_status_string(status));\n        return status;\n    }\n\n    min_size = 0;\n    max_iov  = 1;\n    message_size = ucx_perf_get_message_size(params);\n    switch (params->command) {\n    case UCX_PERF_CMD_AM:\n        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_AM_SHORT,\n                                    UCT_IFACE_FLAG_AM_BCOPY, UCT_IFACE_FLAG_AM_ZCOPY);\n        required_flags |= UCT_IFACE_FLAG_CB_SYNC;\n        min_size = __get_max_size(params->uct.data_layout, 0, 0,\n                                  attr.cap.am.min_zcopy);\n        max_size = __get_max_size(params->uct.data_layout, attr.cap.am.max_short,\n                                  attr.cap.am.max_bcopy, attr.cap.am.max_zcopy);\n        max_iov  = attr.cap.am.max_iov;\n        break;\n    case UCX_PERF_CMD_PUT:\n        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_PUT_SHORT,\n                                    UCT_IFACE_FLAG_PUT_BCOPY, UCT_IFACE_FLAG_PUT_ZCOPY);\n        min_size = __get_max_size(params->uct.data_layout, 0, 0,\n                                  attr.cap.put.min_zcopy);\n        max_size = __get_max_size(params->uct.data_layout, attr.cap.put.max_short,\n                                  attr.cap.put.max_bcopy, attr.cap.put.max_zcopy);\n        max_iov  = attr.cap.put.max_iov;\n        break;\n    case UCX_PERF_CMD_GET:\n        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_GET_SHORT,\n                                    UCT_IFACE_FLAG_GET_BCOPY, UCT_IFACE_FLAG_GET_ZCOPY);\n        min_size = __get_max_size(params->uct.data_layout, 0, 0,\n                                  attr.cap.get.min_zcopy);\n        max_size = __get_max_size(params->uct.data_layout, attr.cap.get.max_short,\n                                  attr.cap.get.max_bcopy, attr.cap.get.max_zcopy);\n        max_iov  = attr.cap.get.max_iov;\n        break;\n    case UCX_PERF_CMD_ADD:\n        ATOMIC_OP_CONFIG(message_size, &atomic_op32, &atomic_op64, UCT_ATOMIC_OP_ADD,\n                         perf_atomic_op, params, status);\n        max_size = 8;\n        break;\n    case UCX_PERF_CMD_FADD:\n        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_ADD,\n                         perf_atomic_fop, params, status);\n        max_size = 8;\n        break;\n    case UCX_PERF_CMD_SWAP:\n        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_SWAP,\n                         perf_atomic_fop, params, status);\n        max_size = 8;\n        break;\n    case UCX_PERF_CMD_CSWAP:\n        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_CSWAP,\n                         perf_atomic_fop, params, status);\n        max_size = 8;\n        break;\n    default:\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Invalid test command\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    status = ucx_perf_test_check_params(params);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    /* check atomics first */\n    ATOMIC_OP_CHECK(32, attr.cap.atomic32.op_flags, atomic_op32, params, perf_atomic_op);\n    ATOMIC_OP_CHECK(64, attr.cap.atomic64.op_flags, atomic_op64, params, perf_atomic_op);\n    ATOMIC_OP_CHECK(32, attr.cap.atomic32.fop_flags, atomic_fop32, params, perf_atomic_fop);\n    ATOMIC_OP_CHECK(64, attr.cap.atomic64.fop_flags, atomic_fop64, params, perf_atomic_fop);\n\n    /* check iface flags */\n    if (!(atomic_op32 | atomic_op64 | atomic_fop32 | atomic_fop64) &&\n        (!ucs_test_all_flags(attr.cap.flags, required_flags) || !required_flags)) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(UCT_PERF_TEST_PARAMS_FMT\" does not support operation %s\",\n                      UCT_PERF_TEST_PARAMS_ARG(params),\n                      perf_iface_ops[ucs_ffs64(~attr.cap.flags & required_flags)]);\n        }\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    if (message_size < min_size) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Message size (%zu) is smaller than min supported (%zu)\",\n                      message_size, min_size);\n        }\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    if (message_size > max_size) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Message size (%zu) is larger than max supported (%zu)\",\n                      message_size, max_size);\n        }\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    if (params->command == UCX_PERF_CMD_AM) {\n        if ((params->uct.data_layout == UCT_PERF_DATA_LAYOUT_SHORT) &&\n            (params->am_hdr_size != sizeof(uint64_t)))\n        {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"Short AM header size must be 8 bytes\");\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        if ((params->uct.data_layout == UCT_PERF_DATA_LAYOUT_ZCOPY) &&\n            (params->am_hdr_size > attr.cap.am.max_hdr))\n        {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"AM header size (%zu) is larger than max supported (%zu)\",\n                          params->am_hdr_size, attr.cap.am.max_hdr);\n            }\n            return UCS_ERR_UNSUPPORTED;\n        }\n\n        if (params->am_hdr_size > message_size) {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"AM header size (%zu) is larger than message size (%zu)\",\n                          params->am_hdr_size, message_size);\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        if (params->uct.fc_window > UCT_PERF_TEST_MAX_FC_WINDOW) {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"AM flow-control window (%d) too large (should be <= %d)\",\n                          params->uct.fc_window, UCT_PERF_TEST_MAX_FC_WINDOW);\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        if ((params->flags & UCX_PERF_TEST_FLAG_ONE_SIDED) &&\n            (params->flags & UCX_PERF_TEST_FLAG_VERBOSE))\n        {\n            ucs_warn(\"Running active-message test with on-sided progress\");\n        }\n    }\n\n    if (UCT_PERF_DATA_LAYOUT_ZCOPY == params->uct.data_layout) {\n        if (params->msg_size_cnt > max_iov) {\n            if ((params->flags & UCX_PERF_TEST_FLAG_VERBOSE) ||\n                !params->msg_size_cnt) {\n                ucs_error(\"Wrong number of IOV entries. Requested is %lu, \"\n                          \"should be in the range 1...%lu\", params->msg_size_cnt,\n                          max_iov);\n            }\n            return UCS_ERR_UNSUPPORTED;\n        }\n        /* if msg_size_cnt == 1 the message size checked above */\n        if ((UCX_PERF_CMD_AM == params->command) && (params->msg_size_cnt > 1)) {\n            if (params->am_hdr_size > params->msg_size_list[0]) {\n                if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                    ucs_error(\"AM header size (%lu) larger than the first IOV \"\n                              \"message size (%lu)\", params->am_hdr_size,\n                              params->msg_size_list[0]);\n                }\n                return UCS_ERR_INVALID_PARAM;\n            }\n        }\n    }\n\n    status = uct_perf_test_check_md_support(params, params->send_mem_type, &md_attr);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    status = uct_perf_test_check_md_support(params, params->recv_mem_type, &md_attr);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    return UCS_OK;\n}\n\nstatic ucs_status_t uct_perf_test_setup_endpoints(ucx_perf_context_t *perf)\n{\n    const size_t buffer_size = ADDR_BUF_SIZE;\n    ucx_perf_ep_info_t info, *remote_info;\n    unsigned group_size, i, group_index;\n    uct_device_addr_t *dev_addr;\n    uct_iface_addr_t *iface_addr;\n    uct_ep_addr_t *ep_addr;\n    uct_iface_attr_t iface_attr;\n    uct_md_attr_t md_attr;\n    uct_ep_params_t ep_params;\n    void *rkey_buffer;\n    ucs_status_t status;\n    struct iovec vec[5];\n    void *buffer;\n    void *req;\n\n    buffer = malloc(buffer_size);\n    if (buffer == NULL) {\n        ucs_error(\"Failed to allocate RTE buffer\");\n        status = UCS_ERR_NO_MEMORY;\n        goto err;\n    }\n\n    status = uct_iface_query(perf->uct.iface, &iface_attr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_iface_query: %s\", ucs_status_string(status));\n        goto err_free;\n    }\n\n    status = uct_md_query(perf->uct.md, &md_attr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_md_query: %s\", ucs_status_string(status));\n        goto err_free;\n    }\n\n    if (md_attr.cap.flags & (UCT_MD_FLAG_ALLOC|UCT_MD_FLAG_REG)) {\n        info.rkey_size      = md_attr.rkey_packed_size;\n    } else {\n        info.rkey_size      = 0;\n    }\n    info.uct.dev_addr_len   = iface_attr.device_addr_len;\n    info.uct.iface_addr_len = iface_attr.iface_addr_len;\n    info.uct.ep_addr_len    = iface_attr.ep_addr_len;\n    info.recv_buffer        = (uintptr_t)perf->recv_buffer;\n\n    rkey_buffer             = buffer;\n    dev_addr                = UCS_PTR_BYTE_OFFSET(rkey_buffer, info.rkey_size);\n    iface_addr              = UCS_PTR_BYTE_OFFSET(dev_addr, info.uct.dev_addr_len);\n    ep_addr                 = UCS_PTR_BYTE_OFFSET(iface_addr, info.uct.iface_addr_len);\n    ucs_assert_always(UCS_PTR_BYTE_OFFSET(ep_addr, info.uct.ep_addr_len) <=\n                      UCS_PTR_BYTE_OFFSET(buffer, buffer_size));\n\n    status = uct_iface_get_device_address(perf->uct.iface, dev_addr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_iface_get_device_address: %s\",\n                  ucs_status_string(status));\n        goto err_free;\n    }\n\n    status = uct_iface_get_address(perf->uct.iface, iface_addr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_iface_get_address: %s\", ucs_status_string(status));\n        goto err_free;\n    }\n\n    if (info.rkey_size > 0) {\n        memset(rkey_buffer, 0, info.rkey_size);\n        status = uct_md_mkey_pack(perf->uct.md, perf->uct.recv_mem.memh, rkey_buffer);\n        if (status != UCS_OK) {\n            ucs_error(\"Failed to uct_rkey_pack: %s\", ucs_status_string(status));\n            goto err_free;\n        }\n    }\n\n    group_size  = rte_call(perf, group_size);\n    group_index = rte_call(perf, group_index);\n\n    perf->uct.peers = calloc(group_size, sizeof(*perf->uct.peers));\n    if (perf->uct.peers == NULL) {\n        goto err_free;\n    }\n\n    ep_params.field_mask = UCT_EP_PARAM_FIELD_IFACE;\n    ep_params.iface      = perf->uct.iface;\n    if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_EP) {\n        for (i = 0; i < group_size; ++i) {\n            if (i == group_index) {\n                continue;\n            }\n\n            status = uct_ep_create(&ep_params, &perf->uct.peers[i].ep);\n            if (status != UCS_OK) {\n                ucs_error(\"Failed to uct_ep_create: %s\", ucs_status_string(status));\n                goto err_destroy_eps;\n            }\n            status = uct_ep_get_address(perf->uct.peers[i].ep, ep_addr);\n            if (status != UCS_OK) {\n                ucs_error(\"Failed to uct_ep_get_address: %s\", ucs_status_string(status));\n                goto err_destroy_eps;\n            }\n        }\n    } else if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE) {\n        ep_params.field_mask |= UCT_EP_PARAM_FIELD_DEV_ADDR |\n                                UCT_EP_PARAM_FIELD_IFACE_ADDR;\n    }\n\n    vec[0].iov_base         = &info;\n    vec[0].iov_len          = sizeof(info);\n    vec[1].iov_base         = buffer;\n    vec[1].iov_len          = info.rkey_size + info.uct.dev_addr_len +\n                              info.uct.iface_addr_len + info.uct.ep_addr_len;\n\n    rte_call(perf, post_vec, vec, 2, &req);\n    rte_call(perf, exchange_vec, req);\n\n    for (i = 0; i < group_size; ++i) {\n        if (i == group_index) {\n            continue;\n        }\n\n        rte_call(perf, recv, i, buffer, buffer_size, req);\n\n        remote_info = buffer;\n        rkey_buffer = remote_info + 1;\n        dev_addr    = UCS_PTR_BYTE_OFFSET(rkey_buffer, remote_info->rkey_size);\n        iface_addr  = UCS_PTR_BYTE_OFFSET(dev_addr, remote_info->uct.dev_addr_len);\n        ep_addr     = UCS_PTR_BYTE_OFFSET(iface_addr, remote_info->uct.iface_addr_len);\n        perf->uct.peers[i].remote_addr = remote_info->recv_buffer;\n\n        if (!uct_iface_is_reachable(perf->uct.iface, dev_addr,\n                                    remote_info->uct.iface_addr_len ?\n                                    iface_addr : NULL)) {\n            ucs_error(\"Destination is unreachable\");\n            status = UCS_ERR_UNREACHABLE;\n            goto err_destroy_eps;\n        }\n\n        if (remote_info->rkey_size > 0) {\n            status = uct_rkey_unpack(perf->uct.cmpt, rkey_buffer,\n                                     &perf->uct.peers[i].rkey);\n            if (status != UCS_OK) {\n                ucs_error(\"Failed to uct_rkey_unpack: %s\", ucs_status_string(status));\n                goto err_destroy_eps;\n            }\n        } else {\n            perf->uct.peers[i].rkey.handle = NULL;\n            perf->uct.peers[i].rkey.rkey   = UCT_INVALID_RKEY;\n        }\n\n        if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_EP) {\n            status = uct_ep_connect_to_ep(perf->uct.peers[i].ep, dev_addr, ep_addr);\n        } else if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE) {\n            ep_params.dev_addr   = dev_addr;\n            ep_params.iface_addr = iface_addr;\n            status = uct_ep_create(&ep_params, &perf->uct.peers[i].ep);\n        } else {\n            status = UCS_ERR_UNSUPPORTED;\n        }\n        if (status != UCS_OK) {\n            ucs_error(\"Failed to connect endpoint: %s\", ucs_status_string(status));\n            goto err_destroy_eps;\n        }\n    }\n    uct_perf_iface_flush_b(perf);\n\n    free(buffer);\n    uct_perf_barrier(perf);\n    return UCS_OK;\n\nerr_destroy_eps:\n    for (i = 0; i < group_size; ++i) {\n        if (perf->uct.peers[i].rkey.rkey != UCT_INVALID_RKEY) {\n            uct_rkey_release(perf->uct.cmpt, &perf->uct.peers[i].rkey);\n        }\n        if (perf->uct.peers[i].ep != NULL) {\n            uct_ep_destroy(perf->uct.peers[i].ep);\n        }\n    }\n    free(perf->uct.peers);\nerr_free:\n    free(buffer);\nerr:\n    return status;\n}\n\nstatic void uct_perf_test_cleanup_endpoints(ucx_perf_context_t *perf)\n{\n    unsigned group_size, group_index, i;\n\n    uct_perf_barrier(perf);\n\n    uct_iface_set_am_handler(perf->uct.iface, UCT_PERF_TEST_AM_ID, NULL, NULL, 0);\n\n    group_size  = rte_call(perf, group_size);\n    group_index = rte_call(perf, group_index);\n\n    for (i = 0; i < group_size; ++i) {\n        if (i != group_index) {\n            if (perf->uct.peers[i].rkey.rkey != UCT_INVALID_RKEY) {\n                uct_rkey_release(perf->uct.cmpt, &perf->uct.peers[i].rkey);\n            }\n            if (perf->uct.peers[i].ep) {\n                uct_ep_destroy(perf->uct.peers[i].ep);\n            }\n        }\n    }\n    free(perf->uct.peers);\n}\n\nstatic ucs_status_t ucp_perf_test_fill_params(ucx_perf_params_t *params,\n                                               ucp_params_t *ucp_params)\n{\n    ucs_status_t status;\n    size_t message_size;\n\n    message_size = ucx_perf_get_message_size(params);\n    switch (params->command) {\n    case UCX_PERF_CMD_PUT:\n    case UCX_PERF_CMD_GET:\n        ucp_params->features |= UCP_FEATURE_RMA;\n        break;\n    case UCX_PERF_CMD_ADD:\n    case UCX_PERF_CMD_FADD:\n    case UCX_PERF_CMD_SWAP:\n    case UCX_PERF_CMD_CSWAP:\n        if (message_size == sizeof(uint32_t)) {\n            ucp_params->features |= UCP_FEATURE_AMO32;\n        } else if (message_size == sizeof(uint64_t)) {\n            ucp_params->features |= UCP_FEATURE_AMO64;\n        } else {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"Atomic size should be either 32 or 64 bit\");\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        break;\n    case UCX_PERF_CMD_TAG:\n    case UCX_PERF_CMD_TAG_SYNC:\n        ucp_params->features |= UCP_FEATURE_TAG;\n        break;\n    case UCX_PERF_CMD_STREAM:\n        ucp_params->features |= UCP_FEATURE_STREAM;\n        break;\n    default:\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Invalid test command\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    status = ucx_perf_test_check_params(params);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    return UCS_OK;\n}\n\nstatic ucs_status_t ucp_perf_test_alloc_iov_mem(ucp_perf_datatype_t datatype,\n                                                size_t iovcnt, unsigned thread_count,\n                                                ucp_dt_iov_t **iov_p)\n{\n    ucp_dt_iov_t *iov;\n\n    if (UCP_PERF_DATATYPE_IOV == datatype) {\n        iov = malloc(sizeof(*iov) * iovcnt * thread_count);\n        if (NULL == iov) {\n            ucs_error(\"Failed allocate IOV buffer with iovcnt=%lu\", iovcnt);\n            return UCS_ERR_NO_MEMORY;\n        }\n        *iov_p = iov;\n    }\n\n    return UCS_OK;\n}\n\nstatic ucs_status_t\nucp_perf_test_alloc_host(const ucx_perf_context_t *perf, size_t length,\n                         void **address_p, ucp_mem_h *memh, int non_blk_flag)\n{\n    ucp_mem_map_params_t mem_map_params;\n    ucp_mem_attr_t mem_attr;\n    ucs_status_t status;\n\n    mem_map_params.field_mask = UCP_MEM_MAP_PARAM_FIELD_ADDRESS |\n                                UCP_MEM_MAP_PARAM_FIELD_LENGTH |\n                                UCP_MEM_MAP_PARAM_FIELD_FLAGS;\n    mem_map_params.address    = *address_p;\n    mem_map_params.length     = length;\n    mem_map_params.flags      = UCP_MEM_MAP_ALLOCATE;\n    if (perf->params.flags & UCX_PERF_TEST_FLAG_MAP_NONBLOCK) {\n        mem_map_params.flags |= non_blk_flag;\n    }\n\n    status = ucp_mem_map(perf->ucp.context, &mem_map_params, memh);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    mem_attr.field_mask = UCP_MEM_ATTR_FIELD_ADDRESS;\n    status = ucp_mem_query(*memh, &mem_attr);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    *address_p = mem_attr.address;\n    return UCS_OK;\n\nerr:\n    return status;\n}\n\nstatic void ucp_perf_test_free_host(const ucx_perf_context_t *perf,\n                                    void *address, ucp_mem_h memh)\n{\n    ucs_status_t status;\n\n    status = ucp_mem_unmap(perf->ucp.context, memh);\n    if (status != UCS_OK) {\n        ucs_warn(\"ucp_mem_unmap() failed: %s\", ucs_status_string(status));\n    }\n}\n\nstatic ucs_status_t ucp_perf_test_alloc_mem(ucx_perf_context_t *perf)\n{\n    ucx_perf_params_t *params = &perf->params;\n    ucs_status_t status;\n    size_t buffer_size;\n\n    if (params->iov_stride) {\n        buffer_size = params->msg_size_cnt * params->iov_stride;\n    } else {\n        buffer_size = ucx_perf_get_message_size(params);\n    }\n\n    /* Allocate send buffer memory */\n    perf->send_buffer = NULL;\n    status = perf->allocator->ucp_alloc(perf, buffer_size * params->thread_count,\n                                        &perf->send_buffer, &perf->ucp.send_memh,\n                                        UCP_MEM_MAP_NONBLOCK);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    /* Allocate receive buffer memory */\n    perf->recv_buffer = NULL;\n    status = perf->allocator->ucp_alloc(perf, buffer_size * params->thread_count,\n                                        &perf->recv_buffer, &perf->ucp.recv_memh,\n                                        0);\n    if (status != UCS_OK) {\n        goto err_free_send_buffer;\n    }\n\n    /* Allocate IOV datatype memory */\n    perf->ucp.send_iov = NULL;\n    status = ucp_perf_test_alloc_iov_mem(params->ucp.send_datatype,\n                                         perf->params.msg_size_cnt,\n                                         params->thread_count,\n                                         &perf->ucp.send_iov);\n    if (UCS_OK != status) {\n        goto err_free_buffers;\n    }\n\n    perf->ucp.recv_iov = NULL;\n    status = ucp_perf_test_alloc_iov_mem(params->ucp.recv_datatype,\n                                         perf->params.msg_size_cnt,\n                                         params->thread_count,\n                                         &perf->ucp.recv_iov);\n    if (UCS_OK != status) {\n        goto err_free_send_iov_buffers;\n    }\n\n    return UCS_OK;\n\nerr_free_send_iov_buffers:\n    free(perf->ucp.send_iov);\nerr_free_buffers:\n    perf->allocator->ucp_free(perf, perf->recv_buffer, perf->ucp.recv_memh);\nerr_free_send_buffer:\n    perf->allocator->ucp_free(perf, perf->send_buffer, perf->ucp.send_memh);\nerr:\n    return UCS_ERR_NO_MEMORY;\n}\n\nstatic void ucp_perf_test_free_mem(ucx_perf_context_t *perf)\n{\n    free(perf->ucp.recv_iov);\n    free(perf->ucp.send_iov);\n    perf->allocator->ucp_free(perf, perf->recv_buffer, perf->ucp.recv_memh);\n    perf->allocator->ucp_free(perf, perf->send_buffer, perf->ucp.send_memh);\n}\n\nstatic void ucp_perf_test_destroy_eps(ucx_perf_context_t* perf)\n{\n    unsigned i, thread_count = perf->params.thread_count;\n    ucs_status_ptr_t    *req;\n    ucs_status_t        status;\n\n    for (i = 0; i < thread_count; ++i) {\n        if (perf->ucp.tctx[i].perf.ucp.rkey != NULL) {\n            ucp_rkey_destroy(perf->ucp.tctx[i].perf.ucp.rkey);\n        }\n\n        if (perf->ucp.tctx[i].perf.ucp.ep != NULL) {\n            req = ucp_ep_close_nb(perf->ucp.tctx[i].perf.ucp.ep,\n                                  UCP_EP_CLOSE_MODE_FLUSH);\n\n            if (UCS_PTR_IS_PTR(req)) {\n                do {\n                    ucp_worker_progress(perf->ucp.tctx[i].perf.ucp.worker);\n                    status = ucp_request_check_status(req);\n                } while (status == UCS_INPROGRESS);\n\n                ucp_request_release(req);\n            } else if (UCS_PTR_STATUS(req) != UCS_OK) {\n                ucs_warn(\"failed to close ep %p on thread %d: %s\\n\",\n                         perf->ucp.tctx[i].perf.ucp.ep, i,\n                         ucs_status_string(UCS_PTR_STATUS(req)));\n            }\n        }\n    }\n}\n\nstatic ucs_status_t ucp_perf_test_exchange_status(ucx_perf_context_t *perf,\n                                                  ucs_status_t status)\n{\n    unsigned group_size  = rte_call(perf, group_size);\n    ucs_status_t collective_status = status;\n    struct iovec vec;\n    void *req = NULL;\n    unsigned i;\n\n    vec.iov_base = &status;\n    vec.iov_len  = sizeof(status);\n\n    rte_call(perf, post_vec, &vec, 1, &req);\n    rte_call(perf, exchange_vec, req);\n    for (i = 0; i < group_size; ++i) {\n        rte_call(perf, recv, i, &status, sizeof(status), req);\n        if (status != UCS_OK) {\n            collective_status = status;\n        }\n    }\n    return collective_status;\n}\n\nstatic ucs_status_t ucp_perf_test_receive_remote_data(ucx_perf_context_t *perf)\n{\n    unsigned thread_count = perf->params.thread_count;\n    void *rkey_buffer     = NULL;\n    void *req             = NULL;\n    unsigned group_size, group_index, i;\n    ucx_perf_ep_info_t *remote_info;\n    ucp_ep_params_t ep_params;\n    ucp_address_t *address;\n    ucs_status_t status;\n    size_t buffer_size;\n    void *buffer;\n\n    group_size  = rte_call(perf, group_size);\n    group_index = rte_call(perf, group_index);\n\n    if (group_size != 2) {\n        ucs_error(\"perftest requires group size to be exactly 2 \"\n                  \"(actual group size: %u)\", group_size);\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    buffer_size = ADDR_BUF_SIZE * thread_count;\n\n    buffer = malloc(buffer_size);\n    if (buffer == NULL) {\n        ucs_error(\"failed to allocate RTE receive buffer\");\n        status = UCS_ERR_NO_MEMORY;\n        goto err;\n    }\n\n    /* Initialize all endpoints and rkeys to NULL to handle error flow */\n    for (i = 0; i < thread_count; i++) {\n        perf->ucp.tctx[i].perf.ucp.ep   = NULL;\n        perf->ucp.tctx[i].perf.ucp.rkey = NULL;\n    }\n\n    /* receive the data from the remote peer, extract the address from it\n     * (along with additional wireup info) and create an endpoint to the peer */\n    rte_call(perf, recv, 1 - group_index, buffer, buffer_size, req);\n\n    remote_info = buffer;\n    for (i = 0; i < thread_count; i++) {\n        address                                = (ucp_address_t*)(remote_info + 1);\n        rkey_buffer                            = UCS_PTR_BYTE_OFFSET(address,\n                                                                     remote_info->ucp.worker_addr_len);\n        perf->ucp.tctx[i].perf.ucp.remote_addr = remote_info->recv_buffer;\n\n        ep_params.field_mask = UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;\n        ep_params.address    = address;\n\n        status = ucp_ep_create(perf->ucp.tctx[i].perf.ucp.worker, &ep_params,\n                               &perf->ucp.tctx[i].perf.ucp.ep);\n        if (status != UCS_OK) {\n            if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"ucp_ep_create() failed: %s\", ucs_status_string(status));\n            }\n            goto err_free_eps_buffer;\n        }\n\n        if (remote_info->rkey_size > 0) {\n            status = ucp_ep_rkey_unpack(perf->ucp.tctx[i].perf.ucp.ep, rkey_buffer,\n                                        &perf->ucp.tctx[i].perf.ucp.rkey);\n            if (status != UCS_OK) {\n                if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                    ucs_fatal(\"ucp_rkey_unpack() failed: %s\", ucs_status_string(status));\n                }\n                goto err_free_eps_buffer;\n            }\n        } else {\n            perf->ucp.tctx[i].perf.ucp.rkey = NULL;\n        }\n\n        remote_info = UCS_PTR_BYTE_OFFSET(remote_info,\n                                          remote_info->ucp.total_wireup_len);\n    }\n\n    free(buffer);\n    return UCS_OK;\n\nerr_free_eps_buffer:\n    ucp_perf_test_destroy_eps(perf);\n    free(buffer);\nerr:\n    return status;\n}\n\nstatic ucs_status_t ucp_perf_test_send_local_data(ucx_perf_context_t *perf,\n                                                  uint64_t features)\n{\n    unsigned i, j, thread_count = perf->params.thread_count;\n    size_t address_length       = 0;\n    void *rkey_buffer           = NULL;\n    void *req                   = NULL;\n    ucx_perf_ep_info_t *info;\n    ucp_address_t *address;\n    ucs_status_t status;\n    struct iovec *vec;\n    size_t rkey_size;\n\n    if (features & (UCP_FEATURE_RMA|UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {\n        status = ucp_rkey_pack(perf->ucp.context, perf->ucp.recv_memh,\n                               &rkey_buffer, &rkey_size);\n        if (status != UCS_OK) {\n            if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"ucp_rkey_pack() failed: %s\", ucs_status_string(status));\n            }\n            goto err;\n        }\n    } else {\n        rkey_size = 0;\n    }\n\n    /* each thread has an iovec with 3 entries to send to the remote peer:\n     * ep_info, worker_address and rkey buffer */\n    vec = calloc(3 * thread_count, sizeof(struct iovec));\n    if (vec == NULL) {\n        ucs_error(\"failed to allocate iovec\");\n        status = UCS_ERR_NO_MEMORY;\n        goto err_rkey_release;\n    }\n\n    /* get the worker address created for every thread and send it to the remote\n     * peer */\n    for (i = 0; i < thread_count; i++) {\n        status = ucp_worker_get_address(perf->ucp.tctx[i].perf.ucp.worker,\n                                        &address, &address_length);\n        if (status != UCS_OK) {\n            if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"ucp_worker_get_address() failed: %s\",\n                          ucs_status_string(status));\n            }\n            goto err_free_workers_vec;\n        }\n\n        vec[i * 3].iov_base = malloc(sizeof(*info));\n        if (vec[i * 3].iov_base == NULL) {\n            ucs_error(\"failed to allocate vec entry for info\");\n            status = UCS_ERR_NO_MEMORY;\n            ucp_worker_destroy(perf->ucp.tctx[i].perf.ucp.worker);\n            goto err_free_workers_vec;\n        }\n\n        info                       = vec[i * 3].iov_base;\n        info->ucp.worker_addr_len  = address_length;\n        info->ucp.total_wireup_len = sizeof(*info) + address_length + rkey_size;\n        info->rkey_size            = rkey_size;\n        info->recv_buffer          = (uintptr_t)perf->ucp.tctx[i].perf.recv_buffer;\n\n        vec[(i * 3) + 0].iov_len  = sizeof(*info);\n        vec[(i * 3) + 1].iov_base = address;\n        vec[(i * 3) + 1].iov_len  = address_length;\n        vec[(i * 3) + 2].iov_base = rkey_buffer;\n        vec[(i * 3) + 2].iov_len  = info->rkey_size;\n\n        address_length = 0;\n    }\n\n    /* send to the remote peer */\n    rte_call(perf, post_vec, vec, 3 * thread_count, &req);\n    rte_call(perf, exchange_vec, req);\n\n    if (features & (UCP_FEATURE_RMA|UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {\n        ucp_rkey_buffer_release(rkey_buffer);\n    }\n\n    for (i = 0; i < thread_count; i++) {\n        free(vec[i * 3].iov_base);\n        ucp_worker_release_address(perf->ucp.tctx[i].perf.ucp.worker,\n                                   vec[(i * 3) + 1].iov_base);\n    }\n\n    free(vec);\n\n    return UCS_OK;\n\nerr_free_workers_vec:\n    for (j = 0; j < i; j++) {\n        ucp_worker_destroy(perf->ucp.tctx[i].perf.ucp.worker);\n    }\n    free(vec);\nerr_rkey_release:\n    if (features & (UCP_FEATURE_RMA|UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {\n        ucp_rkey_buffer_release(rkey_buffer);\n    }\nerr:\n    return status;\n}\n\nstatic ucs_status_t ucp_perf_test_setup_endpoints(ucx_perf_context_t *perf,\n                                                  uint64_t features)\n{\n    ucs_status_t status;\n    unsigned i;\n\n    /* pack the local endpoints data and send to the remote peer */\n    status = ucp_perf_test_send_local_data(perf, features);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    /* receive remote peer's endpoints' data and connect to them */\n    status = ucp_perf_test_receive_remote_data(perf);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    /* sync status across all processes */\n    status = ucp_perf_test_exchange_status(perf, UCS_OK);\n    if (status != UCS_OK) {\n        goto err_destroy_eps;\n    }\n\n    /* force wireup completion */\n    for (i = 0; i < perf->params.thread_count; i++) {\n        status = ucp_worker_flush(perf->ucp.tctx[i].perf.ucp.worker);\n        if (status != UCS_OK) {\n            ucs_warn(\"ucp_worker_flush() failed on theread %d: %s\",\n                     i, ucs_status_string(status));\n        }\n    }\n\n    return status;\n\nerr_destroy_eps:\n    ucp_perf_test_destroy_eps(perf);\nerr:\n    (void)ucp_perf_test_exchange_status(perf, status);\n    return status;\n}\n\nstatic void ucp_perf_test_cleanup_endpoints(ucx_perf_context_t *perf)\n{\n    ucp_perf_barrier(perf);\n    ucp_perf_test_destroy_eps(perf);\n}\n\nstatic void ucp_perf_test_destroy_workers(ucx_perf_context_t *perf)\n{\n    unsigned i;\n\n    for (i = 0; i < perf->params.thread_count; i++) {\n        if (perf->ucp.tctx[i].perf.ucp.worker != NULL) {\n            ucp_worker_destroy(perf->ucp.tctx[i].perf.ucp.worker);\n        }\n    }\n}\n\nstatic void ucx_perf_set_warmup(ucx_perf_context_t* perf,\n                                const ucx_perf_params_t* params)\n{\n    perf->max_iter = ucs_min(params->warmup_iter,\n                             ucs_div_round_up(params->max_iter, 10));\n    perf->report_interval = ULONG_MAX;\n}\n\nstatic ucs_status_t uct_perf_create_md(ucx_perf_context_t *perf)\n{\n    uct_component_h *uct_components;\n    uct_component_attr_t component_attr;\n    uct_tl_resource_desc_t *tl_resources;\n    unsigned md_index, num_components;\n    unsigned tl_index, num_tl_resources;\n    unsigned cmpt_index;\n    ucs_status_t status;\n    uct_md_h md;\n    uct_md_config_t *md_config;\n\n\n    status = uct_query_components(&uct_components, &num_components);\n    if (status != UCS_OK) {\n        goto out;\n    }\n\n    for (cmpt_index = 0; cmpt_index < num_components; ++cmpt_index) {\n\n        component_attr.field_mask = UCT_COMPONENT_ATTR_FIELD_MD_RESOURCE_COUNT;\n        status = uct_component_query(uct_components[cmpt_index], &component_attr);\n        if (status != UCS_OK) {\n            goto out_release_components_list;\n        }\n\n        component_attr.field_mask   = UCT_COMPONENT_ATTR_FIELD_MD_RESOURCES;\n        component_attr.md_resources = alloca(sizeof(*component_attr.md_resources) *\n                                             component_attr.md_resource_count);\n        status = uct_component_query(uct_components[cmpt_index], &component_attr);\n        if (status != UCS_OK) {\n            goto out_release_components_list;\n        }\n\n        for (md_index = 0; md_index < component_attr.md_resource_count; ++md_index) {\n            status = uct_md_config_read(uct_components[cmpt_index], NULL, NULL,\n                                        &md_config);\n            if (status != UCS_OK) {\n                goto out_release_components_list;\n            }\n\n            ucs_strncpy_zero(perf->params.uct.md_name,\n                             component_attr.md_resources[md_index].md_name,\n                             UCT_MD_NAME_MAX);\n\n            status = uct_md_open(uct_components[cmpt_index],\n                                 component_attr.md_resources[md_index].md_name,\n                                 md_config, &md);\n            uct_config_release(md_config);\n            if (status != UCS_OK) {\n                goto out_release_components_list;\n            }\n\n            status = uct_md_query_tl_resources(md, &tl_resources, &num_tl_resources);\n            if (status != UCS_OK) {\n                uct_md_close(md);\n                goto out_release_components_list;\n            }\n\n            for (tl_index = 0; tl_index < num_tl_resources; ++tl_index) {\n                if (!strcmp(perf->params.uct.tl_name,  tl_resources[tl_index].tl_name) &&\n                    !strcmp(perf->params.uct.dev_name, tl_resources[tl_index].dev_name))\n                {\n                    uct_release_tl_resource_list(tl_resources);\n                    perf->uct.cmpt = uct_components[cmpt_index];\n                    perf->uct.md   = md;\n                    status         = UCS_OK;\n                    goto out_release_components_list;\n                }\n            }\n\n            uct_md_close(md);\n            uct_release_tl_resource_list(tl_resources);\n        }\n    }\n\n    ucs_error(\"Cannot use \"UCT_PERF_TEST_PARAMS_FMT,\n              UCT_PERF_TEST_PARAMS_ARG(&perf->params));\n    status = UCS_ERR_NO_DEVICE;\n\nout_release_components_list:\n    uct_release_component_list(uct_components);\nout:\n    return status;\n}\n\nvoid uct_perf_barrier(ucx_perf_context_t *perf)\n{\n    rte_call(perf, barrier, (void(*)(void*))uct_worker_progress,\n             (void*)perf->uct.worker);\n}\n\nvoid ucp_perf_barrier(ucx_perf_context_t *perf)\n{\n    rte_call(perf, barrier, (void(*)(void*))ucp_worker_progress,\n#if _OPENMP\n             (void*)perf->ucp.tctx[omp_get_thread_num()].perf.ucp.worker);\n#else\n             (void*)perf->ucp.tctx[0].perf.ucp.worker);\n#endif\n}\n\nstatic ucs_status_t uct_perf_setup(ucx_perf_context_t *perf)\n{\n    ucx_perf_params_t *params = &perf->params;\n    uct_iface_config_t *iface_config;\n    ucs_status_t status;\n    uct_iface_params_t iface_params = {\n        .field_mask           = UCT_IFACE_PARAM_FIELD_OPEN_MODE   |\n                                UCT_IFACE_PARAM_FIELD_STATS_ROOT  |\n                                UCT_IFACE_PARAM_FIELD_RX_HEADROOM |\n                                UCT_IFACE_PARAM_FIELD_CPU_MASK,\n        .open_mode            = UCT_IFACE_OPEN_MODE_DEVICE,\n        .mode.device.tl_name  = params->uct.tl_name,\n        .mode.device.dev_name = params->uct.dev_name,\n        .stats_root           = ucs_stats_get_root(),\n        .rx_headroom          = 0\n    };\n    UCS_CPU_ZERO(&iface_params.cpu_mask);\n\n    status = ucs_async_context_init(&perf->uct.async, params->async_mode);\n    if (status != UCS_OK) {\n        goto out;\n    }\n\n    status = uct_worker_create(&perf->uct.async, params->thread_mode,\n                               &perf->uct.worker);\n    if (status != UCS_OK) {\n        goto out_cleanup_async;\n    }\n\n    status = uct_perf_create_md(perf);\n    if (status != UCS_OK) {\n        goto out_destroy_worker;\n    }\n\n    status = uct_md_iface_config_read(perf->uct.md, params->uct.tl_name, NULL,\n                                      NULL, &iface_config);\n    if (status != UCS_OK) {\n        goto out_destroy_md;\n    }\n\n    status = uct_iface_open(perf->uct.md, perf->uct.worker, &iface_params,\n                            iface_config, &perf->uct.iface);\n    uct_config_release(iface_config);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to open iface: %s\", ucs_status_string(status));\n        goto out_destroy_md;\n    }\n\n    status = uct_perf_test_check_capabilities(params, perf->uct.iface,\n                                              perf->uct.md);\n    /* sync status across all processes */\n    status = ucp_perf_test_exchange_status(perf, status);\n    if (status != UCS_OK) {\n        goto out_iface_close;\n    }\n\n    status = uct_perf_test_alloc_mem(perf);\n    if (status != UCS_OK) {\n        goto out_iface_close;\n    }\n\n    /* Enable progress before `uct_iface_flush` and `uct_worker_progress` called\n     * to give a chance to finish connection for some tranports (ib/ud, tcp).\n     * They may return UCS_INPROGRESS from `uct_iface_flush` when connections are\n     * in progress */\n    uct_iface_progress_enable(perf->uct.iface,\n                              UCT_PROGRESS_SEND | UCT_PROGRESS_RECV);\n\n    status = uct_perf_test_setup_endpoints(perf);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to setup endpoints: %s\", ucs_status_string(status));\n        goto out_free_mem;\n    }\n\n    return UCS_OK;\n\nout_free_mem:\n    uct_perf_test_free_mem(perf);\nout_iface_close:\n    uct_iface_close(perf->uct.iface);\nout_destroy_md:\n    uct_md_close(perf->uct.md);\nout_destroy_worker:\n    uct_worker_destroy(perf->uct.worker);\nout_cleanup_async:\n    ucs_async_context_cleanup(&perf->uct.async);\nout:\n    return status;\n}\n\nstatic void uct_perf_cleanup(ucx_perf_context_t *perf)\n{\n    uct_perf_test_cleanup_endpoints(perf);\n    uct_perf_test_free_mem(perf);\n    uct_iface_close(perf->uct.iface);\n    uct_md_close(perf->uct.md);\n    uct_worker_destroy(perf->uct.worker);\n    ucs_async_context_cleanup(&perf->uct.async);\n}\n\nstatic void ucp_perf_request_init(void *req)\n{\n    ucp_perf_request_t *request = req;\n\n    request->context = NULL;\n}\n\nstatic ucs_status_t ucp_perf_setup(ucx_perf_context_t *perf)\n{\n    ucp_params_t ucp_params;\n    ucp_worker_params_t worker_params;\n    ucp_config_t *config;\n    ucs_status_t status;\n    unsigned i, thread_count;\n    size_t message_size;\n\n    ucp_params.field_mask   = UCP_PARAM_FIELD_FEATURES |\n                              UCP_PARAM_FIELD_REQUEST_SIZE |\n                              UCP_PARAM_FIELD_REQUEST_INIT;\n    ucp_params.features     = 0;\n    ucp_params.request_size = sizeof(ucp_perf_request_t);\n    ucp_params.request_init = ucp_perf_request_init;\n\n    if (perf->params.thread_count > 1) {\n        /* when there is more than one thread, a ucp_worker would be created for\n         * each. all of them will share the same ucp_context */\n        ucp_params.features          |= UCP_PARAM_FIELD_MT_WORKERS_SHARED;\n        ucp_params.mt_workers_shared  = 1;\n    }\n\n    status = ucp_perf_test_fill_params(&perf->params, &ucp_params);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    status = ucp_config_read(NULL, NULL, &config);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    status = ucp_init(&ucp_params, config, &perf->ucp.context);\n    ucp_config_release(config);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    thread_count = perf->params.thread_count;\n    message_size = ucx_perf_get_message_size(&perf->params);\n\n    status = ucp_perf_test_alloc_mem(perf);\n    if (status != UCS_OK) {\n        ucs_warn(\"ucp test failed to allocate memory\");\n        goto err_cleanup;\n    }\n\n    perf->ucp.tctx = calloc(thread_count, sizeof(ucx_perf_thread_context_t));\n    if (perf->ucp.tctx == NULL) {\n        ucs_warn(\"ucp test failed to allocate memory for thread contexts\");\n        goto err_free_mem;\n    }\n\n    worker_params.field_mask  = UCP_WORKER_PARAM_FIELD_THREAD_MODE;\n    worker_params.thread_mode = perf->params.thread_mode;\n\n    for (i = 0; i < thread_count; i++) {\n        perf->ucp.tctx[i].tid              = i;\n        perf->ucp.tctx[i].perf             = *perf;\n        /* Doctor the src and dst buffers to make them thread specific */\n        perf->ucp.tctx[i].perf.send_buffer =\n                        UCS_PTR_BYTE_OFFSET(perf->send_buffer, i * message_size);\n        perf->ucp.tctx[i].perf.recv_buffer =\n                        UCS_PTR_BYTE_OFFSET(perf->recv_buffer, i * message_size);\n\n        status = ucp_worker_create(perf->ucp.context, &worker_params,\n                                   &perf->ucp.tctx[i].perf.ucp.worker);\n        if (status != UCS_OK) {\n            goto err_free_tctx_destroy_workers;\n        }\n    }\n\n    status = ucp_perf_test_setup_endpoints(perf, ucp_params.features);\n    if (status != UCS_OK) {\n        if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Failed to setup endpoints: %s\", ucs_status_string(status));\n        }\n        goto err_free_tctx_destroy_workers;\n    }\n\n    return UCS_OK;\n\nerr_free_tctx_destroy_workers:\n    ucp_perf_test_destroy_workers(perf);\n    free(perf->ucp.tctx);\nerr_free_mem:\n    ucp_perf_test_free_mem(perf);\nerr_cleanup:\n    ucp_cleanup(perf->ucp.context);\nerr:\n    return status;\n}\n\nstatic void ucp_perf_cleanup(ucx_perf_context_t *perf)\n{\n    ucp_perf_test_cleanup_endpoints(perf);\n    ucp_perf_barrier(perf);\n    ucp_perf_test_free_mem(perf);\n    ucp_perf_test_destroy_workers(perf);\n    free(perf->ucp.tctx);\n    ucp_cleanup(perf->ucp.context);\n}\n\nstatic struct {\n    ucs_status_t (*setup)(ucx_perf_context_t *perf);\n    void         (*cleanup)(ucx_perf_context_t *perf);\n    ucs_status_t (*run)(ucx_perf_context_t *perf);\n    void         (*barrier)(ucx_perf_context_t *perf);\n} ucx_perf_funcs[] = {\n    [UCX_PERF_API_UCT] = {uct_perf_setup, uct_perf_cleanup,\n                          uct_perf_test_dispatch, uct_perf_barrier},\n    [UCX_PERF_API_UCP] = {ucp_perf_setup, ucp_perf_cleanup,\n                          ucp_perf_test_dispatch, ucp_perf_barrier}\n};\n\nstatic ucs_status_t ucx_perf_thread_spawn(ucx_perf_context_t *perf,\n                                          ucx_perf_result_t* result);\n\nucs_status_t ucx_perf_run(const ucx_perf_params_t *params,\n                          ucx_perf_result_t *result)\n{\n    ucx_perf_context_t *perf;\n    ucs_status_t status;\n\n    ucx_perf_global_init();\n\n    if (params->command == UCX_PERF_CMD_LAST) {\n        ucs_error(\"Test is not selected\");\n        status = UCS_ERR_INVALID_PARAM;\n        goto out;\n    }\n\n    if ((params->api != UCX_PERF_API_UCT) && (params->api != UCX_PERF_API_UCP)) {\n        ucs_error(\"Invalid test API parameter (should be UCT or UCP)\");\n        status = UCS_ERR_INVALID_PARAM;\n        goto out;\n    }\n\n    perf = malloc(sizeof(*perf));\n    if (perf == NULL) {\n        status = UCS_ERR_NO_MEMORY;\n        goto out;\n    }\n\n    ucx_perf_test_init(perf, params);\n\n    if (perf->allocator == NULL) {\n        ucs_error(\"Unsupported memory types %s<->%s\",\n                  ucs_memory_type_names[params->send_mem_type],\n                  ucs_memory_type_names[params->recv_mem_type]);\n        status = UCS_ERR_UNSUPPORTED;\n        goto out_free;\n    }\n\n    if ((params->api == UCX_PERF_API_UCT) &&\n        (perf->allocator->mem_type != UCS_MEMORY_TYPE_HOST)) {\n        ucs_warn(\"UCT tests also copy 2-byte values from %s memory to \"\n                 \"%s memory, which may impact performance results\",\n                 ucs_memory_type_names[perf->allocator->mem_type],\n                 ucs_memory_type_names[UCS_MEMORY_TYPE_HOST]);\n    }\n\n    status = perf->allocator->init(perf);\n    if (status != UCS_OK) {\n        goto out_free;\n    }\n\n    status = ucx_perf_funcs[params->api].setup(perf);\n    if (status != UCS_OK) {\n        goto out_free;\n    }\n\n    if (params->thread_count == 1) {\n        if (params->api == UCX_PERF_API_UCP) {\n            perf->ucp.worker      = perf->ucp.tctx[0].perf.ucp.worker;\n            perf->ucp.ep          = perf->ucp.tctx[0].perf.ucp.ep;\n            perf->ucp.remote_addr = perf->ucp.tctx[0].perf.ucp.remote_addr;\n            perf->ucp.rkey        = perf->ucp.tctx[0].perf.ucp.rkey;\n        }\n\n        if (params->warmup_iter > 0) {\n            ucx_perf_set_warmup(perf, params);\n            status = ucx_perf_funcs[params->api].run(perf);\n            if (status != UCS_OK) {\n                goto out_cleanup;\n            }\n\n            ucx_perf_funcs[params->api].barrier(perf);\n            ucx_perf_test_prepare_new_run(perf, params);\n        }\n\n        /* Run test */\n        status = ucx_perf_funcs[params->api].run(perf);\n        ucx_perf_funcs[params->api].barrier(perf);\n        if (status == UCS_OK) {\n            ucx_perf_calc_result(perf, result);\n            rte_call(perf, report, result, perf->params.report_arg, 1, 0);\n        }\n    } else {\n        status = ucx_perf_thread_spawn(perf, result);\n    }\n\nout_cleanup:\n    ucx_perf_funcs[params->api].cleanup(perf);\nout_free:\n    free(perf);\nout:\n    return status;\n}\n\n#if _OPENMP\n\nstatic ucs_status_t ucx_perf_thread_run_test(void* arg)\n{\n    ucx_perf_thread_context_t* tctx = (ucx_perf_thread_context_t*) arg; /* a single thread context */\n    ucx_perf_result_t* result       = &tctx->result;\n    ucx_perf_context_t* perf        = &tctx->perf;\n    ucx_perf_params_t* params       = &perf->params;\n    ucs_status_t status;\n\n    if (params->warmup_iter > 0) {\n        ucx_perf_set_warmup(perf, params);\n        status = ucx_perf_funcs[params->api].run(perf);\n        ucx_perf_funcs[params->api].barrier(perf);\n        if (UCS_OK != status) {\n            goto out;\n        }\n        ucx_perf_test_prepare_new_run(perf, params);\n    }\n\n    /* Run test */\n#pragma omp barrier\n    status = ucx_perf_funcs[params->api].run(perf);\n    ucx_perf_funcs[params->api].barrier(perf);\n    if (UCS_OK != status) {\n        goto out;\n    }\n\n    ucx_perf_calc_result(perf, result);\n\nout:\n    return status;\n}\n\nstatic void ucx_perf_thread_report_aggregated_results(ucx_perf_context_t *perf)\n{\n    ucx_perf_thread_context_t* tctx = perf->ucp.tctx;  /* all the thread contexts on perf */\n    unsigned i, thread_count        = perf->params.thread_count;\n    double lat_sum_total_avegare    = 0.0;\n    ucx_perf_result_t agg_result;\n\n    agg_result.iters        = tctx[0].result.iters;\n    agg_result.bytes        = tctx[0].result.bytes;\n    agg_result.elapsed_time = tctx[0].result.elapsed_time;\n\n    agg_result.bandwidth.total_average  = 0.0;\n    agg_result.bandwidth.typical        = 0.0; /* Undefined since used only for latency calculations */\n    agg_result.latency.total_average    = 0.0;\n    agg_result.msgrate.total_average    = 0.0;\n    agg_result.msgrate.typical          = 0.0; /* Undefined since used only for latency calculations */\n\n    /* when running with multiple threads, the moment average value is\n     * undefined since we don't capture the values of the last iteration */\n    agg_result.msgrate.moment_average   = 0.0;\n    agg_result.bandwidth.moment_average = 0.0;\n    agg_result.latency.moment_average   = 0.0;\n    agg_result.latency.typical          = 0.0;\n\n    /* in case of multiple threads, we have to aggregate the results so that the\n     * final output of the result would show the performance numbers that were\n     * collected from all the threads.\n     * BW and message rate values will be the sum of their values from all\n     * the threads, while the latency value is the average latency from the\n     * threads. */\n\n    for (i = 0; i < thread_count; i++) {\n        agg_result.bandwidth.total_average  += tctx[i].result.bandwidth.total_average;\n        agg_result.msgrate.total_average    += tctx[i].result.msgrate.total_average;\n        lat_sum_total_avegare               += tctx[i].result.latency.total_average;\n    }\n\n    agg_result.latency.total_average = lat_sum_total_avegare / thread_count;\n\n    rte_call(perf, report, &agg_result, perf->params.report_arg, 1, 1);\n}\n\nstatic ucs_status_t ucx_perf_thread_spawn(ucx_perf_context_t *perf,\n                                          ucx_perf_result_t* result)\n{\n    ucx_perf_thread_context_t* tctx = perf->ucp.tctx;   /* all the thread contexts on perf */\n    int ti, thread_count            = perf->params.thread_count;\n    ucs_status_t* statuses;\n    ucs_status_t status;\n\n    omp_set_num_threads(thread_count);\n\n    statuses = calloc(thread_count, sizeof(ucs_status_t));\n    if (statuses == NULL) {\n        status = UCS_ERR_NO_MEMORY;\n        goto out;\n    }\n\n#pragma omp parallel private(ti)\n{\n    ti              = omp_get_thread_num();\n    tctx[ti].status = ucx_perf_thread_run_test((void*)&tctx[ti]);\n}\n\n    status = UCS_OK;\n    for (ti = 0; ti < thread_count; ti++) {\n        if (UCS_OK != tctx[ti].status) {\n            ucs_error(\"Thread %d failed to run test: %s\", tctx[ti].tid,\n                      ucs_status_string(tctx[ti].status));\n            status = tctx[ti].status;\n        }\n    }\n\n    ucx_perf_thread_report_aggregated_results(perf);\n\n    free(statuses);\nout:\n    return status;\n}\n#else\nstatic ucs_status_t ucx_perf_thread_spawn(ucx_perf_context_t *perf,\n                                          ucx_perf_result_t* result) {\n    ucs_error(\"Invalid test parameter (thread mode requested without OpenMP capabilities)\");\n    return UCS_ERR_INVALID_PARAM;\n}\n#endif /* _OPENMP */\n\nvoid ucx_perf_global_init()\n{\n    static ucx_perf_allocator_t host_allocator = {\n        .mem_type  = UCS_MEMORY_TYPE_HOST,\n        .init      = ucs_empty_function_return_success,\n        .ucp_alloc = ucp_perf_test_alloc_host,\n        .ucp_free  = ucp_perf_test_free_host,\n        .uct_alloc = uct_perf_test_alloc_host,\n        .uct_free  = uct_perf_test_free_host,\n        .memcpy    = ucx_perf_test_memcpy_host,\n        .memset    = memset\n    };\n    UCS_MODULE_FRAMEWORK_DECLARE(ucx_perftest);\n\n    ucx_perf_mem_type_allocators[UCS_MEMORY_TYPE_HOST] = &host_allocator;\n\n    /* FIXME Memtype allocator modules must be loaded to global scope, otherwise\n     * alloc hooks, which are using dlsym() to get pointer to original function,\n     * do not work. Need to use bistro for memtype hooks to fix it.\n     */\n    UCS_MODULE_FRAMEWORK_LOAD(ucx_perftest, UCS_MODULE_LOAD_FLAG_GLOBAL);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/test/mpi/test_memhooks.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n#define _GNU_SOURCE /* For basename */\n#include <mpi.h>\n\n#include <ucs/sys/preprocessor.h>\n#include <ucm/api/ucm.h>\n#include <sys/mman.h>\n#include <sys/shm.h>\n#include <malloc.h>\n#include <dlfcn.h>\n#include <unistd.h>\n#include <string.h>\n#include <stdlib.h>\n\n#define CHKERR_JUMP(cond, msg, label) \\\n    do { \\\n        if (cond) { \\\n            printf(\"%s:%d: %s\\n\", basename(__FILE__), __LINE__, msg); \\\n            goto label; \\\n        } \\\n    } while (0)\n\n#define DL_FIND_FUNC(dl, func_name, func, err_action) \\\n    do { \\\n        char *error; \\\n        dlerror(); /* clear existing errors */ \\\n        func = dlsym(dl, func_name); \\\n        if (((error = dlerror()) != NULL) || (func == NULL)) { \\\n            error = error ? error : \"not found\"; \\\n            fprintf(stderr, \"Failed to resolve symbol '%s': %s\\n\", \\\n                    func_name, error); \\\n            err_action; \\\n        } \\\n    } while (0);\n\n#define SHMAT_FAILED ((void*)-1)\n\nvoid* open_dyn_lib(const char *lib_path);\nvoid* flag_no_install_init(const char *path);\nint malloc_hooks_run_all(void *dl);\nint malloc_hooks_run_unmapped(void *dl);\nint ext_event_run(void *dl);\nvoid *ext_event_init(const char *path);\n\ntypedef struct memtest_type {\n    const char *name;\n    void*      (*init)(const char *path);\n    int        (*run) (void *arg);\n} memtest_type_t;\n\nmemtest_type_t tests[] = {\n    {\"malloc_hooks\",          open_dyn_lib,         malloc_hooks_run_all},\n    {\"malloc_hooks_unmapped\", open_dyn_lib,         malloc_hooks_run_unmapped},\n    {\"external_events\",       ext_event_init,       ext_event_run},\n    {\"flag_no_install\",       flag_no_install_init, ext_event_run},\n    {NULL}\n};\n\nstatic volatile size_t total_mapped = 0;\nstatic volatile size_t total_unmapped = 0;\n\nstatic void usage() {\n    printf(\"Usage: test_memhooks [options]\\n\");\n    printf(\"Options are:\\n\");\n    printf(\"  -h         Print this info.\\n\");\n    printf(\"  -t <name>  Test name to execute (malloc_hooks)\\n\");\n    printf(\"                 malloc_hooks          : General UCM test for VM_MAPPED and VM_UNMAPPED\\n\");\n    printf(\"                 malloc_hooks_unmapped : Test VM_UNMAPPED event only\\n\");\n    printf(\"                 external_events       : Test of ucm_set_external_event() API\\n\");\n    printf(\"                 flag_no_install       : Test of UCM_EVENT_FLAG_NO_INSTALL flag\\n\");\n    printf(\"\\n\");\n}\n\nstatic void event_callback(ucm_event_type_t event_type, ucm_event_t *event,\n                           void *arg)\n{\n    if (event_type == UCM_EVENT_VM_MAPPED) {\n        total_mapped += event->vm_mapped.size;\n    } else if (event_type == UCM_EVENT_VM_UNMAPPED) {\n        total_unmapped += event->vm_unmapped.size;\n    }\n}\n\nstatic ucs_status_t set_event_handler(void *dl, int events)\n{\n    ucs_status_t (*set_handler)(int events, int priority,\n                                ucm_event_callback_t cb, void *arg);\n\n    DL_FIND_FUNC(dl, \"ucm_set_event_handler\", set_handler,\n                 return UCS_ERR_UNSUPPORTED);\n\n    return set_handler(events, 0, event_callback, NULL);\n}\n\nstatic ucs_status_t disable_memory_hooks(void *dl)\n{\n    setenv(\"UCX_MEM_MALLOC_HOOKS\", \"n\", 1);\n    setenv(\"UCX_MEM_MMAP_RELOC\",   \"n\", 1);\n    return UCS_OK;\n}\n\nvoid* open_dyn_lib(const char *lib_path)\n{\n    void *dl = dlopen(lib_path, RTLD_LAZY);\n    char *error;\n\n    if (dl == NULL) {\n        error = dlerror();\n        error = error ? error : \"unknown error\";\n        fprintf(stderr, \"Failed to load '%s': %s\\n\", lib_path, error);\n    }\n    return dl;\n}\n\n\nvoid *ext_event_init(const char *path)\n{\n    void (*set_ext_event)(int events);\n    ucs_status_t status;\n    void *dl_ucm;\n\n    dl_ucm = open_dyn_lib(path);\n    if (dl_ucm == NULL) {\n        return NULL;\n    }\n\n    status = disable_memory_hooks(dl_ucm);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to disable memory hooks\", fail);\n\n    DL_FIND_FUNC(dl_ucm, \"ucm_set_external_event\", set_ext_event, goto fail);\n    set_ext_event(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n\n    status = set_event_handler(dl_ucm, UCM_EVENT_VM_MAPPED |\n                                       UCM_EVENT_VM_UNMAPPED);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to set event handler\", fail);\n\n    return dl_ucm;\n\nfail:\n    dlclose(dl_ucm);\n    return NULL;\n}\n\nvoid* flag_no_install_init(const char *path)\n{\n    void *dl_ucm;\n    ucs_status_t status;\n\n    dl_ucm = open_dyn_lib(path);\n    if (dl_ucm == NULL) {\n        return NULL;\n    }\n\n    status = disable_memory_hooks(dl_ucm);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to disable memory hooks\", fail);\n\n    status = set_event_handler(dl_ucm, UCM_EVENT_VM_MAPPED   |\n                                       UCM_EVENT_VM_UNMAPPED |\n                                       UCM_EVENT_FLAG_NO_INSTALL);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to set event handler\", fail);\n    return dl_ucm;\n\nfail:\n    dlclose(dl_ucm);\n    return NULL;\n}\n\nint malloc_hooks_run_flags(void *dl, ucm_event_type_t events)\n{\n    ucs_status_t status;\n    void *ptr_malloc_core = NULL;\n    void *ptr_malloc_mmap = NULL;\n    void *ptr_direct_mmap = MAP_FAILED;\n    int  shmid            = -1;\n    void *ptr_shmat       = SHMAT_FAILED;\n    void *dl_test;\n    const size_t size = 1024 * 1024;\n    const char *lib_path = UCS_PP_MAKE_STRING(TEST_LIB_DIR) \"/\" \"libtest_memhooks.so\";\n    const char *cust_mmap_name  = \"memhook_test_lib_call_mmap\";\n    void * (*cust_mmap)(size_t size);\n\n    status = set_event_handler(dl, events);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to set event handler\", fail_close_ucm);\n\n    printf(\"Allocating memory\\n\");\n\n    /* Create SysV segment */\n    shmid = shmget(IPC_PRIVATE, size, IPC_CREAT|SHM_R|SHM_W);\n    CHKERR_JUMP(shmid == -1, \"Failed to create shared memory segment: %m\",\n                fail_close_ucm);\n\n    /*\n     * Test shmat/shmdt before malloc() because shmat() add entires to an internal\n     * hash of pointers->size, which makes previous pointers un-releasable\n     */\n\n    /* Attach SysV segment */\n    total_mapped = 0;\n    ptr_shmat = shmat(shmid, NULL, 0);\n    CHKERR_JUMP(ptr_shmat == SHMAT_FAILED, \"Failed to attach shared memory segment\",\n                fail_close_ucm);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped < size, \"No callback for shmat\", fail_close_ucm);\n    }\n    printf(\"After shmat: reported mapped=%zu\\n\", total_mapped);\n\n    /* Detach SysV segment */\n    total_unmapped = 0;\n    shmdt(ptr_shmat);\n    ptr_shmat = SHMAT_FAILED;\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No callback for shmdt\", fail_close_ucm);\n    }\n    printf(\"After shmdt: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Attach SysV segment at fixed address */\n    total_mapped = 0;\n    total_unmapped = 0;\n    ptr_shmat = shmat(shmid, (void*)0xff000000, SHM_REMAP);\n    CHKERR_JUMP(ptr_shmat == SHMAT_FAILED, \"Failed to attach shared memory segment\",\n                fail_close_ucm);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped < size, \"No map callback for shmat(REMAP)\", fail_close_ucm);\n    }\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No unmap callback for shmat(REMAP)\",\n                    fail_close_ucm);\n    }\n    printf(\"After shmat(REMAP): reported mapped=%zu unmapped=%zu\\n\", total_mapped,\n           total_unmapped);\n\n    /* Detach SysV segment */\n    total_unmapped = 0;\n    shmdt(ptr_shmat);\n    ptr_shmat = SHMAT_FAILED;\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No callback for shmdt\", fail_close_ucm);\n    }\n    printf(\"After shmdt: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Destroy SysV segment */\n    shmctl(shmid, IPC_RMID, NULL);\n    shmid = -1;\n\n    /* Allocate using morecore */\n    mallopt(M_MMAP_THRESHOLD, size * 2);\n    mallopt(M_TRIM_THRESHOLD, size / 2);\n    total_mapped = 0;\n    ptr_malloc_core = malloc(1024 * 1024);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped == 0, \"No callback for core malloc\",\n                    fail_close_ucm);\n    }\n    printf(\"After core malloc: reported mapped=%zu\\n\", total_mapped);\n\n    /* Allocate using mmap */\n    mallopt(M_MMAP_THRESHOLD, size / 2);\n    total_mapped = 0;\n    ptr_malloc_mmap = malloc(2 * 1024 * 1024);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped == 0, \"No callback for mmap malloc\",\n                    fail_close_ucm);\n    }\n    printf(\"After mmap malloc: reported mapped=%zu\\n\", total_mapped);\n\n    /* Allocate directly with mmap */\n    total_mapped = 0;\n    ptr_direct_mmap = mmap(NULL, size, PROT_READ|PROT_WRITE,\n                           MAP_PRIVATE|MAP_ANON, -1, 0);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped < size, \"No callback for mmap\", fail_close_ucm);\n    }\n    printf(\"After mmap: reported mapped=%zu\\n\", total_mapped);\n\n    /* Remap */\n    total_unmapped = 0;\n    ptr_direct_mmap = mmap(ptr_direct_mmap, size, PROT_READ|PROT_WRITE,\n                           MAP_PRIVATE|MAP_ANON|MAP_FIXED, -1, 0);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No unmap callback for mmap(FIXED)\",\n                    fail_close_ucm);\n    }\n    printf(\"After mmap(FIXED): reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Call munmap directly */\n    total_unmapped = 0;\n    munmap(ptr_direct_mmap, size);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped == 0, \"No callback for munmap\", fail_close_ucm);\n    }\n    printf(\"After munmap: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Release indirectly */\n    total_unmapped = 0;\n    free(ptr_malloc_mmap);\n    ptr_malloc_mmap = NULL;\n    malloc_trim(0);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped == 0, \"No callback for munmap from free\",\n                    fail_close_ucm);\n    }\n    printf(\"After mmap free + trim: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Call mmap from a library we load after hooks are installed */\n    dl_test = open_dyn_lib(lib_path);\n    CHKERR_JUMP(dl_test == NULL, \"Failed to load test lib\", fail_close_ucm);\n\n    DL_FIND_FUNC(dl_test, cust_mmap_name, cust_mmap, goto fail_close_all);\n    total_mapped = 0;\n    ptr_direct_mmap = cust_mmap(size);\n    CHKERR_JUMP(ptr_direct_mmap == MAP_FAILED, \"Failed to mmap from dynamic lib\",\n                fail_close_all);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped == 0,\"No callback for mmap from dynamic lib\",\n                    fail_close_all);\n    }\n    printf(\"After another mmap from dynamic lib: reported mapped=%zu\\n\", total_mapped);\n    munmap(ptr_direct_mmap, size);\n    ptr_direct_mmap = MAP_FAILED;\n\n    /*\n     * Test closing UCM.\n     * The library should not really be unloaded, because the memory hooks still\n     * point to functions inside it.\n     */\n    total_unmapped = 0;\n    dlclose(dl);\n    dlclose(dl_test);\n    free(ptr_malloc_core); /* This should still work */\n    ptr_malloc_core = NULL;\n    malloc_trim(0);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped == 0, \"No callback for munmap from malloc\", fail);\n    }\n    printf(\"After core malloc free: reported unmapped=%zu\\n\", total_unmapped);\n\n    return 0;\n\nfail_close_all:\n    dlclose(dl_test);\nfail_close_ucm:\n    dlclose(dl);\nfail:\n    if (ptr_shmat != SHMAT_FAILED) {\n        shmdt(ptr_shmat);\n    }\n    if (shmid != -1) {\n        shmctl(shmid, IPC_RMID, NULL);\n    }\n    free(ptr_malloc_mmap);\n    free(ptr_malloc_core);\n    if (ptr_direct_mmap != MAP_FAILED) {\n        munmap(ptr_direct_mmap, size);\n    }\n\n    return  -1;\n}\n\nint malloc_hooks_run_all(void *dl)\n{\n    return malloc_hooks_run_flags(dl, UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n}\n\nint malloc_hooks_run_unmapped(void *dl)\n{\n    return malloc_hooks_run_flags(dl, UCM_EVENT_VM_UNMAPPED);\n}\n\nint ext_event_run(void *dl)\n{\n    void *ptr_direct_mmap;\n    void (*ucm_event)(void *addr, size_t length);\n    const size_t size = 1024 * 1024;\n    int ret = -1;\n\n    /* Allocate directly with mmap */\n    total_mapped = 0;\n    ptr_direct_mmap = mmap(NULL, size, PROT_READ|PROT_WRITE,\n                           MAP_PRIVATE|MAP_ANON, -1, 0);\n    printf(\"totmapped %lu\\n\", total_mapped);\n    /* No callback should be called as we registered events to be external */\n    CHKERR_JUMP(total_mapped != 0,\n                \"Callback for mmap invoked, while hooks were not set\", fail);\n    DL_FIND_FUNC(dl, \"ucm_vm_mmap\", ucm_event, goto fail);\n    ucm_event(ptr_direct_mmap, size);\n    CHKERR_JUMP(total_mapped == 0, \"Callback for mmap is not called\", fail);\n    printf(\"After ucm_vm_mmap called: mapped=%zu\\n\", total_mapped);\n\n    /* Call munmap directly */\n    total_unmapped = 0;\n    munmap(ptr_direct_mmap, size);\n    CHKERR_JUMP(total_unmapped != 0,\n                \"Callback for munmap invoked, while hooks were not set\\n\", fail);\n\n    DL_FIND_FUNC(dl, \"ucm_vm_munmap\", ucm_event, goto fail);\n    ucm_event(ptr_direct_mmap, size);\n    CHKERR_JUMP(total_unmapped == 0, \"Callback for mmap is not called\", fail);\n    printf(\"After ucm_vm_munmap: unmapped=%zu\\n\", total_unmapped);\n\n    ret = 0;\n\nfail:\n    dlclose(dl);\n    return ret;\n}\n\nint main(int argc, char **argv)\n{\n    const char *ucm_path = UCS_PP_MAKE_STRING(UCM_LIB_DIR) \"/\" \"libucm.so\";\n    memtest_type_t *test = tests;\n    void *dl;\n    int ret;\n    int c;\n\n    while ((c = getopt(argc, argv, \"t:h\")) != -1) {\n        switch (c) {\n        case 't':\n            for (test = tests; test->name != NULL; ++test) {\n                if (!strcmp(test->name, optarg)){\n                    break;\n                }\n            }\n            if (test->name == NULL) {\n                fprintf(stderr, \"Wrong test name %s\\n\", optarg);\n                return -1;\n            }\n            break;\n        case 'h':\n        default:\n            usage();\n            return -1;\n        }\n    }\n\n    /* Some tests need to modify UCM config before to call ucp_init,\n     * which may be called by MPI_Init */\n    dl = test->init(ucm_path);\n    if (dl == NULL) {\n        return -1;\n    }\n\n    printf(\"%s: initialized\\n\", test->name);\n\n    MPI_Init(&argc, &argv);\n\n    ret = test->run(dl);\n\n    printf(\"%s: %s\\n\", test->name, ret == 0 ? \"PASS\" : \"FAIL\");\n\n    MPI_Finalize();\n    return ret;\n}\n\n\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/test/gtest/ucm/malloc_hook.cc": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#define __STDC_LIMIT_MACROS\n\n#include <ucm/api/ucm.h>\n\n#include <ucs/arch/atomic.h>\n#include <ucs/type/status.h>\n#include <common/test.h>\n#include <common/test_helpers.h>\n#include <pthread.h>\n#include <sstream>\n#include <stdint.h>\n#include <dlfcn.h>\n#include <libgen.h>\n\nextern \"C\" {\n#include <ucs/time/time.h>\n#include <ucm/malloc/malloc_hook.h>\n#include <ucm/bistro/bistro.h>\n#include <ucs/sys/sys.h>\n#include <malloc.h>\n}\n\n#if HAVE_MALLOC_SET_STATE && HAVE_MALLOC_GET_STATE\n#  define HAVE_MALLOC_STATES 1\n#endif /* HAVE_MALLOC_SET_STATE && HAVE_MALLOC_GET_STATE */\n\n#define EXPECT_INCREASED(_value, _prev, _size, _name)  \\\n    {                                                  \\\n        EXPECT_GE(_value, (_prev) + (_size)) << _name; \\\n        _prev = _value;                                \\\n    }\n\n\nclass malloc_hook_test_no_events : public ucs::test {\nprotected:\n    virtual ~malloc_hook_test_no_events()\n    {\n    }\n\n    static void empty_event_callback(ucm_event_type_t event_type,\n                                     ucm_event_t *event, void *arg)\n    {\n    }\n\n    virtual void init()\n    {\n        ucs::test::init();\n        m_enable_events = ucm_global_opts.enable_events;\n        ucm_global_opts.enable_events = 0;\n    }\n\n    virtual void cleanup()\n    {\n        ucm_global_opts.enable_events = m_enable_events;\n        ucs::test::cleanup();\n    }\n\n    int m_enable_events;\n};\n\nUCS_TEST_F(malloc_hook_test_no_events, empty_event) {\n    ucs_status_t status;\n    status = ucm_set_event_handler(0, 0, empty_event_callback, NULL);\n    ASSERT_UCS_OK(status);\n    ucm_unset_event_handler(0, empty_event_callback, NULL);\n}\n\n\ntemplate <class T>\nclass mhook_thread {\npublic:\n    mhook_thread(T *test): m_test(test)\n    {\n        pthread_create(&m_thread, NULL, thread_func, reinterpret_cast<void*>(m_test));\n    }\n\n    ~mhook_thread() {\n        join();\n        delete m_test;\n    }\n\n    void join() {\n        void *retval;\n        pthread_join(m_thread, &retval);\n    }\n\nprotected:\n    T         *m_test;\n    pthread_t m_thread;\n\n    static void *thread_func(void *arg) {\n        T *test = reinterpret_cast<T*>(arg);\n        test->test();\n        return NULL;\n    }\n};\n\ntemplate <class T>\nclass mmap_event {\npublic:\n    mmap_event(T *test): m_test(test), m_events(0), m_external_events(0)\n    {\n    }\n\n    ~mmap_event()\n    {\n        unset();\n        unset_external();\n    }\n\n    ucs_status_t set(int events)\n    {\n        ucs_status_t status;\n\n        status = ucm_set_event_handler(events, 0, mem_event_callback,\n                                       reinterpret_cast<void*>(m_test));\n        ASSERT_UCS_OK(status);\n        m_events |= events;\n        return status;\n    }\n\n    ucs_status_t set_external(int events)\n    {\n        ucm_set_external_event(events);\n        m_external_events |= events;\n        return UCS_OK;\n    }\n\n    void unset()\n    {\n        if (m_events) {\n            ucm_unset_event_handler(m_events, mem_event_callback,\n                                    reinterpret_cast<void*>(m_test));\n            m_events = 0;\n        }\n    }\n\n    void unset_external()\n    {\n        if (m_external_events) {\n            ucm_unset_external_event(m_external_events);\n            m_external_events = 0;\n        }\n    }\n\nprotected:\n    T   *m_test;\n    int m_events;\n    int m_external_events;\n\n    static void mem_event_callback(ucm_event_type_t event_type,\n                                   ucm_event_t *event,\n                                   void *arg)\n    {\n        T *test = reinterpret_cast<T*>(arg);\n        test->mem_event(event_type, event);\n    }\n};\n\n\nclass malloc_hook : public ucs::test {\n    friend class mmap_event<malloc_hook>;\nprotected:\n    /* use template argument to call/not call vm_unmap handler */\n    /* GCC 4.4.7 doesn't allow to define template static function\n     * with integer template argument. using template inner class\n     * to define static function */\n    template <int C> class bistro_hook {\n    public:\n        static int munmap(void *addr, size_t length)\n        {\n            UCM_BISTRO_PROLOGUE;\n            malloc_hook::bistro_call_counter++;\n            if (C) {\n                /* notify aggregate vm_munmap event only */\n                ucm_vm_munmap(addr, length);\n            }\n            int res = (intptr_t)syscall(SYS_munmap, addr, length);\n            UCM_BISTRO_EPILOGUE;\n            return res;\n        }\n\n        static int madvise(void *addr, size_t length, int advise)\n        {\n            UCM_BISTRO_PROLOGUE;\n            malloc_hook::bistro_call_counter++;\n            if (C) {\n                /* notify aggregate vm_munmap event only */\n                ucm_vm_munmap(addr, length);\n            }\n            int res = (intptr_t)syscall(SYS_madvise, addr, length, advise);\n            UCM_BISTRO_EPILOGUE;\n            return res;\n        }\n    };\n\n    class bistro_patch {\n    public:\n        bistro_patch(const char* symbol, void *hook)\n        {\n            ucs_status_t status;\n\n            status = ucm_bistro_patch(symbol, hook, &m_rp);\n            ASSERT_UCS_OK(status);\n            EXPECT_NE((intptr_t)m_rp, 0);\n        }\n\n        ~bistro_patch()\n        {\n            ucm_bistro_restore(m_rp);\n        }\n\n    protected:\n        ucm_bistro_restore_point_t *m_rp;\n    };\n\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n    {\n        m_got_event = 1;\n    }\n\n    virtual void init() {\n        ucs_status_t status;\n        mmap_event<malloc_hook> event(this);\n\n        ucs::skip_on_address_sanitizer();\n\n        m_got_event = 0;\n        ucm_malloc_state_reset(128 * 1024, 128 * 1024);\n        malloc_trim(0);\n        status = event.set(UCM_EVENT_VM_MAPPED);\n        ASSERT_UCS_OK(status);\n\n        for (;;) {\n            void *ptr = malloc(small_alloc_size);\n            if (m_got_event) {\n                /* If the heap grew, the minimal size is the previous one */\n                free(ptr);\n                break;\n            } else {\n                m_pts.push_back(ptr);\n            }\n        }\n        event.unset();\n    }\n\npublic:\n    static int            small_alloc_count;\n    static const size_t   small_alloc_size = 10000;\n    ucs::ptr_vector<void> m_pts;\n    int                   m_got_event;\n    static volatile int   bistro_call_counter;\n};\n\nstatic bool skip_on_bistro() {\n    return (ucm_global_opts.mmap_hook_mode == UCM_MMAP_HOOK_BISTRO);\n}\n\nstatic bool skip_on_bistro_without_valgrind() {\n    /* BISTRO is disabled under valgrind, we may run tests */\n    return (skip_on_bistro() && !RUNNING_ON_VALGRIND);\n}\n\nint malloc_hook::small_alloc_count            = 1000 / ucs::test_time_multiplier();\nvolatile int malloc_hook::bistro_call_counter = 0;\n\nclass test_thread {\npublic:\n    test_thread(const std::string& name, int num_threads, pthread_barrier_t *barrier,\n                malloc_hook *test, void (test_thread::*test_func)() = &test_thread::test) :\n        m_name(name), m_num_threads(num_threads), m_barrier(barrier),\n        m_map_size(0), m_unmap_size(0), m_test(test), m_event(this)\n    {\n        pthread_mutex_init(&m_stats_lock, NULL);\n    }\n\n    ~test_thread() {\n        pthread_mutex_destroy(&m_stats_lock);\n    }\n\n    void test();\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event);\n\nprivate:\n    typedef std::pair<void*, void*> range;\n\n    bool is_ptr_in_range(void *ptr, size_t size, const std::vector<range> &ranges) {\n        for (std::vector<range>::const_iterator iter = ranges.begin(); iter != ranges.end(); ++iter) {\n            if ((ptr >= iter->first) && ((char*)ptr < iter->second)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    static pthread_mutex_t   lock;\n    static pthread_barrier_t barrier;\n\n    std::string        m_name;\n    int                m_num_threads;\n    pthread_barrier_t  *m_barrier;\n\n    pthread_mutex_t    m_stats_lock;\n    size_t             m_map_size;\n    size_t             m_unmap_size;\n    std::vector<range> m_map_ranges;\n    std::vector<range> m_unmap_ranges;\n\n    malloc_hook        *m_test;\n    mmap_event<test_thread> m_event;\n};\n\npthread_mutex_t test_thread::lock = PTHREAD_MUTEX_INITIALIZER;\n\nvoid test_thread::mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n{\n    pthread_mutex_lock(&m_stats_lock);\n    switch (event_type) {\n    case UCM_EVENT_VM_MAPPED:\n        m_map_ranges.push_back(range(event->vm_mapped.address,\n                                     (char*)event->vm_mapped.address + event->vm_mapped.size));\n        m_map_size += event->vm_mapped.size;\n        break;\n    case UCM_EVENT_VM_UNMAPPED:\n        m_unmap_ranges.push_back(range(event->vm_unmapped.address,\n                                       (char*)event->vm_unmapped.address + event->vm_unmapped.size));\n        m_unmap_size += event->vm_unmapped.size;\n        break;\n    default:\n        break;\n    }\n    pthread_mutex_unlock(&m_stats_lock);\n}\n\nvoid test_thread::test() {\n    static const size_t large_alloc_size = 40 * 1024 * 1024;\n    ucs_status_t result;\n    ucs::ptr_vector<void> old_ptrs;\n    ucs::ptr_vector<void> new_ptrs;\n    void *ptr_r;\n    size_t small_map_size;\n    const size_t small_alloc_size = malloc_hook::small_alloc_size;\n    int num_ptrs_in_range;\n    static volatile uint32_t total_ptrs_in_range = 0;\n    char *test_str;\n\n    /* Allocate some pointers with old heap manager */\n    for (unsigned i = 0; i < 10; ++i) {\n        old_ptrs.push_back(malloc(small_alloc_size));\n    }\n\n    ptr_r = malloc(small_alloc_size);\n\n    m_map_ranges.reserve  ((m_test->small_alloc_count * 8 + 10) * m_num_threads);\n    m_unmap_ranges.reserve((m_test->small_alloc_count * 8 + 10) * m_num_threads);\n\n    total_ptrs_in_range = 0;\n\n    pthread_barrier_wait(m_barrier);\n\n    /* Install memory hooks */\n    result = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(result);\n\n    /* Allocate small pointers with new heap manager */\n    for (int i = 0; i < m_test->small_alloc_count; ++i) {\n        new_ptrs.push_back(malloc(small_alloc_size));\n    }\n    small_map_size = m_map_size;\n\n    /* If this test runs more than once, then sbrk may not really allocate new\n     * memory\n     */\n    EXPECT_GT(m_map_size, 0lu) << m_name;\n\n    num_ptrs_in_range = 0;\n    for (ucs::ptr_vector<void>::const_iterator iter = new_ptrs.begin();\n                    iter != new_ptrs.end(); ++iter)\n    {\n        if (is_ptr_in_range(*iter, small_alloc_size, m_map_ranges)) {\n            ++num_ptrs_in_range;\n        }\n    }\n\n    /* Need at least one ptr in the mapped ranges in one the threads */\n    ucs_atomic_add32(&total_ptrs_in_range, num_ptrs_in_range);\n    pthread_barrier_wait(m_barrier);\n\n    EXPECT_GT(total_ptrs_in_range, 0u);\n\n    /* Allocate large chunk */\n    void *ptr = malloc(large_alloc_size);\n    EXPECT_GE(m_map_size, large_alloc_size + small_map_size) << m_name;\n    EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_map_ranges)) << m_name;\n    EXPECT_GE(malloc_usable_size(ptr), large_alloc_size);\n\n    free(ptr);\n    EXPECT_GE(m_unmap_size, large_alloc_size) << m_name;\n    /* coverity[pass_freed_arg] */\n    EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_unmap_ranges)) << m_name;\n\n    /* Test strdup */\n    void *s = strdup(\"test\");\n    free(s);\n\n    /* Test setenv */\n    pthread_mutex_lock(&lock);\n    setenv(\"TEST\", \"VALUE\", 1);\n    test_str = getenv(\"TEST\");\n    if (test_str != NULL) {\n        EXPECT_EQ(std::string(\"VALUE\"), test_str);\n    } else {\n        UCS_TEST_ABORT(\"getenv(\\\"TEST\\\") returned NULL\");\n    }\n    pthread_mutex_unlock(&lock);\n\n    /* Test username */\n    ucs_get_user_name();\n\n    /* Test usable size */\n    EXPECT_GE(malloc_usable_size(ptr_r), small_alloc_size);\n\n    /* Test realloc */\n    ptr_r = realloc(ptr_r, small_alloc_size / 2);\n    free(ptr_r);\n\n    /* Test C++ allocations */\n    {\n        std::vector<char> vec(large_alloc_size, 0);\n        ptr = &vec[0];\n        EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_map_ranges)) << m_name;\n    }\n\n    /* coverity[use_after_free] - we don't dereference ptr, just search it*/\n    EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_unmap_ranges)) << m_name;\n\n    /* Release old pointers (should not crash) */\n    old_ptrs.clear();\n\n    m_map_ranges.clear();\n    m_unmap_ranges.clear();\n\n    /* Don't release pointers before other threads exit, so they will map new memory\n     * and not reuse memory from other threads.\n     */\n    pthread_barrier_wait(m_barrier);\n\n    /* This must be done when all other threads are inactive, otherwise we may leak */\n#if HAVE_MALLOC_STATES\n    if (!RUNNING_ON_VALGRIND) {\n        pthread_mutex_lock(&lock);\n        void *state = malloc_get_state();\n        malloc_set_state(state);\n        free(state);\n        pthread_mutex_unlock(&lock);\n    }\n#endif /* HAVE_MALLOC_STATES */\n\n    pthread_barrier_wait(m_barrier);\n\n    /* Release new pointers  */\n    new_ptrs.clear();\n\n    /* Call several malloc routines */\n    malloc_trim(0);\n\n    ptr = malloc(large_alloc_size);\n\n    free(ptr);\n\n    /* shmat/shmdt */\n    size_t shm_seg_size = ucs_get_page_size() * 2;\n    int shmid = shmget(IPC_PRIVATE, shm_seg_size, IPC_CREAT | SHM_R | SHM_W);\n    EXPECT_NE(-1, shmid) << strerror(errno);\n\n    ptr = shmat(shmid, NULL, 0);\n    EXPECT_NE(MAP_FAILED, ptr) << strerror(errno);\n\n    shmdt(ptr);\n    shmctl(shmid, IPC_RMID, NULL);\n\n    EXPECT_TRUE(is_ptr_in_range(ptr, shm_seg_size, m_unmap_ranges));\n\n    ptr = mmap(NULL, shm_seg_size, PROT_READ|PROT_WRITE,\n               MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    ASSERT_NE(MAP_FAILED, ptr) << strerror(errno);\n    madvise(ptr, shm_seg_size, MADV_DONTNEED);\n\n    EXPECT_TRUE(is_ptr_in_range(ptr, shm_seg_size, m_unmap_ranges));\n    munmap(ptr, shm_seg_size);\n\n    /* Print results */\n    pthread_mutex_lock(&lock);\n    UCS_TEST_MESSAGE << m_name\n                     << \": small mapped: \" << small_map_size\n                     <<  \", total mapped: \" << m_map_size\n                     <<  \", total unmapped: \" << m_unmap_size;\n    std::cout.flush();\n    pthread_mutex_unlock(&lock);\n\n    m_event.unset();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, single_thread,\n                     skip_on_bistro_without_valgrind()) {\n    pthread_barrier_t barrier;\n    pthread_barrier_init(&barrier, NULL, 1);\n    {\n        mhook_thread<test_thread>(new test_thread(\"single-thread\", 1, &barrier, this));\n    }\n    pthread_barrier_destroy(&barrier);\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, multi_threads,\n                     skip_on_bistro_without_valgrind()) {\n    typedef mhook_thread<test_thread> thread_t;\n\n    static const int num_threads = 8;\n    ucs::ptr_vector<thread_t> threads;\n    pthread_barrier_t barrier;\n\n    malloc_trim(0);\n\n    pthread_barrier_init(&barrier, NULL, num_threads);\n    for (int i = 0; i < num_threads; ++i) {\n        std::stringstream ss;\n        ss << \"thread \" << i << \"/\" << num_threads;\n        threads.push_back(new thread_t(new test_thread(ss.str(), num_threads, &barrier, this)));\n    }\n\n    threads.clear();\n    pthread_barrier_destroy(&barrier);\n}\n\nUCS_TEST_F(malloc_hook, asprintf) {\n    /* Install memory hooks */\n    (void)dlerror();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, fork, \"broken\") {\n    static const int num_processes = 4;\n    pthread_barrier_t barrier;\n    std::vector<pid_t> pids;\n    pid_t pid;\n\n    for (int i = 0; i < num_processes; ++i) {\n        pid = fork();\n        if (pid == 0) {\n            pthread_barrier_init(&barrier, NULL, 1);\n            {\n                std::stringstream ss;\n                ss << \"process \" << i << \"/\" << num_processes;\n                test_thread thread(ss.str(), 1, &barrier, this);\n            }\n            pthread_barrier_destroy(&barrier);\n            throw ucs::exit_exception(HasFailure());\n        }\n        pids.push_back(pid);\n    }\n\n    for (int i = 0; i < num_processes; ++i) {\n        int status;\n        waitpid(pids[i], &status, 0);\n        EXPECT_EQ(0, WEXITSTATUS(status)) << \"Process \" << i << \" failed\";\n    }\n}\n\nclass malloc_hook_cplusplus : public malloc_hook {\npublic:\n\n    malloc_hook_cplusplus() :\n        m_mapped_size(0), m_unmapped_size(0),\n        m_dynamic_mmap_config(ucm_global_opts.enable_dynamic_mmap_thresh),\n        m_event(this) {\n    }\n\n    ~malloc_hook_cplusplus() {\n        ucm_global_opts.enable_dynamic_mmap_thresh = m_dynamic_mmap_config;\n    }\n\n    void set() {\n        ucs_status_t status;\n        status = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);\n        ASSERT_UCS_OK(status);\n    }\n\n    void unset() {\n        m_event.unset();\n    }\n\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n    {\n        switch (event_type) {\n        case UCM_EVENT_VM_MAPPED:\n            m_mapped_size   += event->vm_mapped.size;\n            break;\n        case UCM_EVENT_VM_UNMAPPED:\n            m_unmapped_size += event->vm_unmapped.size;\n            break;\n        default:\n            break;\n        }\n    }\n\nprotected:\n    double measure_alloc_time(size_t size, unsigned iters)\n    {\n        ucs_time_t start_time = ucs_get_time();\n        for (unsigned i = 0; i < iters; ++i) {\n            void *ptr = malloc(size);\n            /* prevent the compiler from optimizing-out the memory allocation */\n            *(volatile char*)ptr = '5';\n            free(ptr);\n        }\n        return ucs_time_to_sec(ucs_get_time() - start_time);\n    }\n\n    void test_dynamic_mmap_thresh()\n    {\n        const size_t size = 8 * UCS_MBYTE;\n\n        set();\n\n        std::vector<std::string> strs;\n\n        m_mapped_size = 0;\n        while (m_mapped_size < size) {\n            strs.push_back(std::string(size, 't'));\n        }\n\n        m_unmapped_size = 0;\n        strs.clear();\n        EXPECT_GE(m_unmapped_size, size);\n\n        m_mapped_size = 0;\n        while (m_mapped_size < size) {\n            strs.push_back(std::string());\n            strs.back().resize(size);\n        }\n\n        m_unmapped_size = 0;\n        strs.clear();\n        if (ucm_global_opts.enable_dynamic_mmap_thresh) {\n            EXPECT_EQ(0ul, m_unmapped_size);\n        } else {\n            EXPECT_GE(m_unmapped_size, size);\n        }\n\n        unset();\n    }\n\n    size_t m_mapped_size;\n    size_t m_unmapped_size;\n    int    m_dynamic_mmap_config;\n    mmap_event<malloc_hook_cplusplus> m_event;\n};\n\n\nclass mmap_hooks {\npublic:\n    mmap_hooks(const std::string& name, int num_threads, pthread_barrier_t *barrier):\n        m_num_threads(num_threads), m_mapped_size(0), m_unmapped_size(0),\n        m_search_ptr(NULL), m_is_ptr_found(false), m_name(name),\n        m_barrier(barrier), m_event(this)\n    {\n        pthread_spin_init(&m_lock,0);\n    }\n\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n    {\n        pthread_spin_lock(&m_lock);\n        switch (event_type) {\n        case UCM_EVENT_VM_MAPPED:\n            m_mapped_size += event->vm_mapped.size;\n            break;\n        case UCM_EVENT_VM_UNMAPPED:\n            m_unmapped_size += event->vm_unmapped.size;\n            if (m_search_ptr == event->vm_unmapped.address) {\n                m_is_ptr_found = true;\n            }\n            break;\n        default:\n            break;\n        }\n        pthread_spin_unlock(&m_lock);\n    }\n\n    void test()\n    {\n        /*\n         * Test memory mapping functions which override an existing mapping\n         */\n        size_t size          = ucs_get_page_size() * 800;\n        size_t mapped_size   = 0;\n        size_t unmapped_size = 0;\n        void *buffer;\n        int shmid;\n        ucs_status_t status;\n        int num_threads;\n\n        EXPECT_EQ(0u, m_mapped_size) << m_name;\n        EXPECT_EQ(0u, m_unmapped_size) << m_name;\n\n        status = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);\n        ASSERT_UCS_OK(status);\n\n        pthread_barrier_wait(m_barrier);\n\n        /* 1. Map a large buffer */\n        {\n            buffer = mmap(NULL, size, PROT_READ|PROT_WRITE,\n                                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n            ASSERT_NE(MAP_FAILED, buffer) << strerror(errno);\n\n            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, 0, m_name);\n        }\n\n        /*\n         * 2. Map another buffer in the same place.\n         *    Expected behavior: unmap event on the old buffer\n         */\n        {\n            void *remap = mmap(buffer, size, PROT_READ|PROT_WRITE,\n                               MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED, -1, 0);\n            ASSERT_EQ(buffer, remap);\n\n            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 3. Create a shared memory segment */\n        {\n            shmid = shmget(IPC_PRIVATE, size, IPC_CREAT | SHM_R | SHM_W);\n            ASSERT_NE(-1, shmid) << strerror(errno) << m_name;\n        }\n\n        /*\n         * 4. Attach the segment at the same buffer address.\n         *    Expected behavior: unmap event on the old buffer\n         */\n        {\n            m_is_ptr_found = false;\n            m_search_ptr   = buffer;\n\n            /* Make sure every thread will have a unique value of 'buffer' - no\n             * thread will release its buffer before all others already\n             * allocated theirs */\n            pthread_barrier_wait(m_barrier);\n\n            /* adding 0x1 to 'buffer' with SHM_RND flag should still send event\n             * for 'buffer', because it aligns to SHMLBA\n             */\n            void *shmaddr = shmat(shmid, (char*)buffer + 0x1, SHM_REMAP | SHM_RND);\n            ASSERT_EQ(buffer, shmaddr) << m_name;\n\n            if (SHMLBA > 0x1) {\n                EXPECT_TRUE(m_is_ptr_found);\n            }\n            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 5. Detach the sysv segment */\n        {\n            shmdt(buffer);\n\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 6. Remove the shared memory segment */\n        {\n            int ret = shmctl(shmid, IPC_RMID, NULL);\n            ASSERT_NE(-1, ret) << strerror(errno);\n        }\n\n        /* 7. Unmap the buffer */\n        {\n            munmap(buffer, size);\n\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 8. sbrk call - single thread only */\n        if (!RUNNING_ON_VALGRIND && (m_num_threads < 2)) {\n            num_threads = 0;\n            ucs_sys_enum_threads(enum_threads_cb, &num_threads);\n            // use sbrk() only if there are 3 threads in the system:\n            //   1. main thread\n            //   2. watchdog thread\n            //   3. test thread\n            // otherwise, another thread can call use malloc/free in same time,\n            // leading to heap corruption\n\n            if (num_threads <= 3) {\n                /* valgrind failed when sbrk is called directly,\n                 * also sbrk is not thread safe */\n\n                /* sbrk call is used to extend/cut memory heap,\n                 * don't add any evaluations between calls sbrk+/sbrk- - it\n                 * may break heap */\n                sbrk(size);\n                sbrk(-size);\n\n                EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n                EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n            }\n        }\n        pthread_barrier_wait(m_barrier);\n    }\n\nprotected:\n    int                     m_num_threads;\n    size_t                  m_mapped_size;\n    size_t                  m_unmapped_size;\n    void                    *m_search_ptr;\n    bool                    m_is_ptr_found;\n    pthread_spinlock_t      m_lock;\n    std::string             m_name;\n    pthread_barrier_t       *m_barrier;\n    mmap_event<mmap_hooks>  m_event;\n\n    static ucs_status_t enum_threads_cb(pid_t tid, void *ctx)\n    {\n        (*(int*)ctx)++;\n        return UCS_OK;\n    }\n};\n\n\nUCS_TEST_F(malloc_hook_cplusplus, new_delete) {\n    const size_t size = 8 * 1000 * 1000;\n\n    set();\n\n    {\n        std::vector<char> vec1(size, 0);\n        std::vector<char> vec2(size, 0);\n        std::vector<char> vec3(size, 0);\n    }\n\n    {\n        std::vector<char> vec1(size, 0);\n        std::vector<char> vec2(size, 0);\n        std::vector<char> vec3(size, 0);\n    }\n\n    malloc_trim(0);\n\n    EXPECT_GE(m_unmapped_size, size);\n\n    unset();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, dynamic_mmap_enable,\n                     RUNNING_ON_VALGRIND || skip_on_bistro()) {\n    EXPECT_TRUE(ucm_global_opts.enable_dynamic_mmap_thresh);\n    test_dynamic_mmap_thresh();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, dynamic_mmap_disable,\n                     skip_on_bistro_without_valgrind()) {\n    ucm_global_opts.enable_dynamic_mmap_thresh = 0;\n\n    test_dynamic_mmap_thresh();\n}\n\nextern \"C\" {\n    int ucm_dlmallopt_get(int);\n};\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, mallopt,\n                     skip_on_bistro_without_valgrind()) {\n\n    int v;\n    int trim_thresh, mmap_thresh;\n    char *p;\n    size_t size;\n\n    /* This test can not be run with the other\n     * tests because it assumes that malloc hooks\n     * are not initialized\n     */\n    p = getenv(\"MALLOC_TRIM_THRESHOLD_\");\n    if (p == NULL) {\n        UCS_TEST_SKIP_R(\"MALLOC_TRIM_THRESHOLD_ is not set\");\n    }\n    trim_thresh = atoi(p);\n\n    p = getenv(\"MALLOC_MMAP_THRESHOLD_\");\n    if (p == NULL) {\n        UCS_TEST_SKIP_R(\"MALLOC_MMAP_THRESHOLD_ is not set\");\n    }\n    mmap_thresh = atoi(p);\n\n    /* make sure that rcache is explicitly disabled so\n     * that the malloc hooks are installed after the setenv()\n     */\n    p = getenv(\"UCX_IB_RCACHE\");\n    if ((p == NULL) || (p[0] != 'n')) {\n        UCS_TEST_SKIP_R(\"rcache must be disabled\");\n    }\n\n    set();\n\n    v = ucm_dlmallopt_get(M_TRIM_THRESHOLD);\n    EXPECT_EQ(trim_thresh, v);\n\n    v = ucm_dlmallopt_get(M_MMAP_THRESHOLD);\n    EXPECT_EQ(mmap_thresh, v);\n\n    /* give a lot of extra space since the same block\n     * can be also used by other allocations\n     */\n    if (trim_thresh > 0) {\n        size = trim_thresh/2;\n    } else if (mmap_thresh > 0) {\n        size = mmap_thresh/2;\n    } else {\n        size = 10 * 1024 * 1024;\n    }\n\n    UCS_TEST_MESSAGE << \"trim_thresh=\" << trim_thresh << \" mmap_thresh=\" << mmap_thresh <<\n                        \" allocating=\" << size;\n    p = new char [size];\n    ASSERT_TRUE(p != NULL);\n    delete [] p;\n\n    EXPECT_EQ(m_unmapped_size, size_t(0));\n\n    unset();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, mmap_ptrs, RUNNING_ON_VALGRIND) {\n    ucm_global_opts.enable_dynamic_mmap_thresh = 0;\n    set();\n\n    const size_t   size    = ucm_dlmallopt_get(M_MMAP_THRESHOLD) * 2;\n    const size_t   max_mem = ucs_min(ucs_get_phys_mem_size() / 4, 4 * UCS_GBYTE);\n    const unsigned count   = ucs_min(400000ul, max_mem / size);\n    const unsigned iters   = 100000;\n\n    std::vector< std::vector<char> > ptrs;\n\n    size_t large_blocks = 0;\n\n    /* Allocate until we get MMAP event\n     * Lock memory to avoid going to swap and ensure consistet test results.\n     */\n    while (m_mapped_size == 0) {\n        std::vector<char> str(size, 'r');\n        ptrs.push_back(str);\n        ++large_blocks;\n    }\n\n    /* Remove memory off the heap top, to ensure the following large allocations\n     * will use mmap()\n     */\n    malloc_trim(0);\n\n    /* Measure allocation time with \"clear\" heap state */\n    double alloc_time = measure_alloc_time(size, iters);\n    UCS_TEST_MESSAGE << \"With \" << large_blocks << \" large blocks:\"\n                     << \" allocated \" << iters << \" buffers of \" << size\n                     << \" bytes in \" << alloc_time << \" sec\";\n\n    /* Allocate many large strings to trigger mmap() based allocation. */\n    ptrs.resize(count);\n    for (unsigned i = 0; i < count; ++i) {\n        ptrs[i].resize(size, 't');\n        ++large_blocks;\n    }\n\n    /* Measure allocation time with many large blocks on the heap */\n    bool success = false;\n    unsigned attempt = 0;\n    while (!success && (attempt < 5)) {\n        double alloc_time_with_ptrs = measure_alloc_time(size, iters);\n        UCS_TEST_MESSAGE << \"With \" << large_blocks << \" large blocks:\"\n                         << \" allocated \" << iters << \" buffers of \" << size\n                         << \" bytes in \" << alloc_time_with_ptrs << \" sec\";\n\n        /* Allow up to 75% difference */\n        success = (alloc_time < 0.25) || (alloc_time_with_ptrs < (1.75 * alloc_time));\n        ++attempt;\n    }\n\n    if (!success) {\n        ADD_FAILURE() << \"Failed after \" << attempt << \" attempts\";\n    }\n\n    ptrs.clear();\n\n    unset();\n\n}\n\nUCS_TEST_F(malloc_hook_cplusplus, remap_override_single_thread) {\n    pthread_barrier_t barrier;\n    pthread_barrier_init(&barrier, NULL, 1);\n    {\n        mhook_thread<mmap_hooks>(new mmap_hooks(\"single-thread\", 1, &barrier));\n    }\n    pthread_barrier_destroy(&barrier);\n}\n\nUCS_TEST_F(malloc_hook_cplusplus, remap_override_multi_threads) {\n    typedef mhook_thread<mmap_hooks> thread_t;\n\n    static const int num_threads = 8;\n    ucs::ptr_vector<thread_t> threads;\n    pthread_barrier_t barrier;\n\n    pthread_barrier_init(&barrier, NULL, num_threads);\n    for (int i = 0; i < num_threads; ++i) {\n        std::stringstream ss;\n        ss << \"thread \" << i << \"/\" << num_threads;\n        threads.push_back(new thread_t(new mmap_hooks(ss.str(), num_threads, &barrier)));\n    }\n\n    threads.clear();\n    pthread_barrier_destroy(&barrier);\n}\n\ntypedef int (munmap_f_t)(void *addr, size_t len);\n\nUCS_TEST_SKIP_COND_F(malloc_hook, bistro_patch, RUNNING_ON_VALGRIND) {\n    const char *symbol = \"munmap\";\n    ucm_bistro_restore_point_t *rp = NULL;\n    ucs_status_t status;\n    munmap_f_t *munmap_f;\n    void *ptr;\n    int res;\n    uint64_t UCS_V_UNUSED patched;\n    uint64_t UCS_V_UNUSED origin;\n\n    /* set hook to mmap call */\n    status = ucm_bistro_patch(symbol, (void*)bistro_hook<0>::munmap, &rp);\n    ASSERT_UCS_OK(status);\n    EXPECT_NE((intptr_t)rp, 0);\n\n    munmap_f = (munmap_f_t*)ucm_bistro_restore_addr(rp);\n    EXPECT_NE((intptr_t)munmap_f, 0);\n\n    /* save partial body of patched function */\n    patched = *(uint64_t*)munmap_f;\n\n    bistro_call_counter = 0;\n    ptr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);\n    EXPECT_NE(ptr, MAP_FAILED);\n\n    /* try to call munmap, we should jump into munmap_hook instead */\n    res = munmap_f(ptr, 4096);\n    EXPECT_EQ(res, 0);\n    /* due to cache coherency issues on ARM systems could be executed\n     * original function body, so, skip counter evaluation */\n    EXPECT_GT(bistro_call_counter, 0);\n\n    /* restore original mmap body */\n    status = ucm_bistro_restore(rp);\n    ASSERT_UCS_OK(status);\n\n    bistro_call_counter = 0;\n    /* now try to call mmap, we should NOT jump into mmap_hook */\n    ptr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);\n    EXPECT_NE(ptr, MAP_FAILED);\n    res = munmap_f(ptr, 4096);\n    EXPECT_EQ(res, 0);\n    EXPECT_EQ(bistro_call_counter, 0);  /* hook is not called */\n    /* save partial body of restored function */\n    origin = *(uint64_t*)munmap_f;\n\n#if !defined (__powerpc64__)\n    EXPECT_NE(patched, origin);\n#endif\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_event, RUNNING_ON_VALGRIND) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n\n    status = event.set(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    status = ucm_test_events(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_event_failed,\n                     RUNNING_ON_VALGRIND || !skip_on_bistro()) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n    const char *symbol_munmap  = \"munmap\";\n    const char *symbol_madvise = \"madvise\";\n\n    status = event.set(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    /* set hook to munmap call */\n    {\n        bistro_patch patch(symbol_munmap, (void*)bistro_hook<0>::munmap);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_MUNMAP)      == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_ERR_UNSUPPORTED);\n    }\n    /* set hook to madvise call */\n    {\n        bistro_patch patch(symbol_madvise, (void*)bistro_hook<0>::madvise);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_MADVISE)     == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n    }\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_external_event,\n                     RUNNING_ON_VALGRIND || !skip_on_bistro()) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n    const char *symbol_munmap  = \"munmap\";\n    const char *symbol_madvise = \"madvise\";\n\n    status = event.set_external(UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    /* set hook to munmap call */\n    {\n        bistro_patch patch(symbol_munmap, (void*)bistro_hook<0>::munmap);\n        /* OK due to UCM_EVENT_MUNMAP is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP)      == UCS_OK);\n        /* should fail */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n        /* should fail */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_ERR_UNSUPPORTED);\n    }\n    /* set hook to madvise call */\n    {\n        bistro_patch patch(symbol_madvise, (void*)bistro_hook<0>::madvise);\n        /* OK due to UCM_EVENT_MADVISE is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MADVISE)     == UCS_OK);\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n    }\n    /* set hook to munmap/madvise call which notify vm_unmap */\n    {\n        bistro_patch patch_unmap(symbol_munmap, (void*)bistro_hook<1>::munmap);\n        bistro_patch patch_advise(symbol_madvise, (void*)bistro_hook<1>::madvise);\n        /* OK due to UCM_EVENT_MUNMAP is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP)      == UCS_OK);\n        /* OK due to UCM_EVENT_MADVISE is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MADVISE)     == UCS_OK);\n        /* should be OK */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_OK);\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_OK);\n    }\n    /* set hook to munmap & madvise call, but madvise does NOT notify vm_unmap */\n    {\n        bistro_patch patch_unmap(symbol_munmap, (void*)bistro_hook<1>::munmap);\n        bistro_patch patch_advise(symbol_madvise, (void*)bistro_hook<0>::madvise);\n        /* OK due to UCM_EVENT_MUNMAP is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP)      == UCS_OK);\n        /* OK due to UCM_EVENT_MADVISE is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MADVISE)     == UCS_OK);\n        /* should fail */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_ERR_UNSUPPORTED);\n    }\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_event_unmap,\n                     RUNNING_ON_VALGRIND || !skip_on_bistro()) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n    const char *symbol = \"munmap\";\n\n    status = event.set(UCM_EVENT_MMAP | UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    /* set hook to mmap call */\n    bistro_patch patch(symbol, (void*)bistro_hook<1>::munmap);\n\n    /* munmap should be broken */\n    status = ucm_test_events(UCM_EVENT_MUNMAP);\n    EXPECT_TRUE(status == UCS_ERR_UNSUPPORTED);\n\n    /* vm_unmap should be broken as well, because munmap is broken */\n    status = ucm_test_events(UCM_EVENT_MUNMAP);\n    EXPECT_TRUE(status == UCS_ERR_UNSUPPORTED);\n\n    /* mmap should still work */\n    status = ucm_test_events(UCM_EVENT_MMAP);\n    EXPECT_TRUE(status == UCS_OK);\n}\n\nclass malloc_hook_dlopen : public malloc_hook {\nprotected:\n    class library {\n    public:\n        typedef void* (*loader_t)(const char*, int);\n\n        library(loader_t loader, const std::string &name = \"\"):\n            m_loader(loader), m_name(name), m_lib(NULL)\n        {\n        }\n\n        ~library()\n        {\n            close();\n        }\n\n        void *open(const std::string name = \"\")\n        {\n            if (!name.empty()) {\n                m_name = name;\n            }\n\n            close();\n\n            return (m_lib = m_loader(m_name.empty() ? NULL : m_name.c_str(), RTLD_NOW));\n        }\n\n        void attach(void *lib)\n        {\n            close();\n            m_lib = lib;\n        }\n\n        void close()\n        {\n            if (m_lib != NULL) {\n                dlclose(m_lib);\n                m_lib = NULL;\n            }\n        }\n\n        operator bool()\n        {\n            return m_lib != NULL;\n        }\n\n        void* sym(const char *name)\n        {\n            return dlsym(m_lib, name);\n        }\n\n    protected:\n        loader_t    m_loader;\n        std::string m_name;\n        void       *m_lib;\n    };\n\npublic:\n    typedef library::loader_t loader_t;\n\n    static std::string get_lib_dir() {\n#ifndef GTEST_UCM_HOOK_LIB_DIR\n#  error \"Missing build configuration\"\n#else\n        return GTEST_UCM_HOOK_LIB_DIR;\n#endif\n    }\n\n    static std::string get_lib_path_do_load() {\n        return get_lib_dir() + \"/libdlopen_test_do_load.so\";\n    }\n\n    static std::string get_lib_path_do_mmap() {\n        return get_lib_dir() + \"/libdlopen_test_do_mmap.so\";\n    }\n\n    static std::string get_lib_path_do_load_rpath() {\n        return get_lib_dir() + \"/libdlopen_test_do_load_rpath.so\";\n    }\n\n    static std::string get_lib_path_do_load_sub_rpath() {\n        return \"libdlopen_test_rpath.so\"; // library should be located using rpath\n    }\n\n    /* test for mmap events are fired from non-direct load modules\n     * we are trying to load lib1, from lib1 load lib2, and\n     * fire mmap event from lib2 */\n    void test_indirect_dlopen(loader_t loader)\n    {\n        typedef void (*fire_mmap_f)(void);\n        typedef void* (*load_lib_f)(const char *path, void* (*func)(const char*, int));\n\n        const char *load_lib  = \"load_lib\";\n        const char *fire_mmap = \"fire_mmap\";\n\n        library lib(loader, get_lib_path_do_load());\n        library lib2(NULL); // lib2 is used for attach only\n        load_lib_f load;\n        fire_mmap_f fire;\n        ucs_status_t status;\n        mmap_event<malloc_hook> event(this);\n\n        status = event.set(UCM_EVENT_VM_MAPPED);\n        ASSERT_UCS_OK(status);\n\n        lib.open();\n        ASSERT_TRUE(lib);\n\n        load = (load_lib_f)lib.sym(load_lib);\n        ASSERT_TRUE(load != NULL);\n\n        lib2.attach(load(get_lib_path_do_mmap().c_str(), loader));\n        ASSERT_TRUE(lib2);\n\n        fire = (fire_mmap_f)lib2.sym(fire_mmap);\n        ASSERT_TRUE(fire != NULL);\n\n        m_got_event = 0;\n        fire();\n        EXPECT_GT(m_got_event, 0);\n    }\n\n    /* Test for rpath section of caller module is processes */\n    void test_rpath_dlopen(loader_t loader)\n    {\n        typedef void* (*load_lib_f)(const char *path, void* (*func)(const char*, int));\n\n        const char *load_lib = \"load_lib\";\n\n        library lib(loader);\n        library lib2(NULL); // lib2 is used for attach only\n        load_lib_f load;\n        ucs_status_t status;\n        mmap_event<malloc_hook> event(this);\n\n        /* in case if reloc mode is used - it force hook dlopen */\n        status = event.set(UCM_EVENT_VM_MAPPED);\n        ASSERT_UCS_OK(status);\n\n        /* first check that without rpath library located in subdirectory could not be loaded */\n        lib.open(get_lib_path_do_load());\n        ASSERT_TRUE(lib);\n        if (!lib) {\n            return;\n        }\n\n        load = (load_lib_f)lib.sym(load_lib);\n        ASSERT_TRUE(load != NULL);\n\n        lib2.attach(load(get_lib_path_do_load_sub_rpath().c_str(), loader));\n        ASSERT_FALSE(lib2);\n\n        /* next check that rpath helps to load library located in subdirectory */\n        /* don't care about opened libs - it will be closed automatically */\n        lib.open(get_lib_path_do_load_rpath());\n        ASSERT_TRUE(lib);\n        if (!lib) {\n            return;\n        }\n\n        load = (load_lib_f)lib.sym(load_lib);\n        ASSERT_TRUE(load != NULL);\n\n        lib2.attach(load(get_lib_path_do_load_sub_rpath().c_str(), loader));\n        ASSERT_TRUE(lib2);\n    }\n\n    void test_dlopen_null(loader_t loader)\n    {\n        library lib(loader);\n\n        lib.open();\n        ASSERT_TRUE(lib);\n    }\n};\n\nUCS_TEST_F(malloc_hook_dlopen, indirect_dlopen) {\n    test_indirect_dlopen(dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, indirect_ucm_dlopen) {\n    test_indirect_dlopen(ucm_dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, rpath_dlopen) {\n    test_rpath_dlopen(dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, rpath_ucm_dlopen) {\n    test_rpath_dlopen(ucm_dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, ucm_dlopen_null_dlopen) {\n    test_dlopen_null(dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, ucm_dlopen_null_ucm_dlopen) {\n    test_dlopen_null(ucm_dlopen);\n}\n\nUCS_MT_TEST_F(malloc_hook_dlopen, dlopen_mt_with_memtype, 2) {\n#ifndef GTEST_UCM_HOOK_LIB_DIR\n#  error \"Missing build configuration\"\n#endif\n    mmap_event<malloc_hook> event(this);\n\n    ucs_status_t status = event.set(UCM_EVENT_VM_MAPPED |\n                                    UCM_EVENT_MEM_TYPE_ALLOC |\n                                    UCM_EVENT_MEM_TYPE_FREE);\n    ASSERT_UCS_OK(status);\n\n    const std::string path = get_lib_path_do_mmap();\n    static uint32_t count = 0;\n\n    for (int i = 0; i < 100 / ucs::test_time_multiplier(); ++i) {\n        /* Tests that calling dlopen() from 2 threads does not deadlock, if for\n         * example we install memtype relocation patches and call dladdr() while\n         * iterating over loaded libraries.\n         */\n        if (ucs_atomic_fadd32(&count, 1) % 2) {\n            void *lib1 = dlopen(get_lib_path_do_mmap().c_str(), RTLD_LAZY);\n            ASSERT_TRUE(lib1 != NULL);\n            dlclose(lib1);\n        } else {\n            void *lib2 = dlopen(get_lib_path_do_load().c_str(), RTLD_LAZY);\n            ASSERT_TRUE(lib2 != NULL);\n            dlclose(lib2);\n        }\n\n        barrier();\n    }\n\n    event.unset();\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/test/gtest/ucs/test_debug.cc": "/**\n* Copyright (C) Mellanox Technologies Ltd. 2001-2012.  ALL RIGHTS RESERVED.\n*\n* See file LICENSE for terms.\n*/\n\n#include <common/test.h>\nextern \"C\" {\n#include <ucs/debug/debug.h>\n#include <ucs/sys/compiler.h>\n#include <ucs/sys/sys.h>\n}\n\n#include <dlfcn.h>\n\nextern \"C\" {\n\nvoid UCS_F_NOINLINE my_cool_function(unsigned *lineno) { *lineno = __LINE__; };\n\n}\n\nclass test_debug : public ucs::test {\n};\n\nstd::string __basename(const std::string& path) {\n    char *p = strdup(path.c_str());\n    std::string bn(::basename(p));\n    free(p);\n    return bn;\n}\n\nUCS_TEST_F(test_debug, lookup_ucs_func) {\n    const char sym[] = \"ucs_log_flush\";\n\n    ucs_debug_address_info info;\n    ucs_status_t status = ucs_debug_lookup_address(dlsym(RTLD_DEFAULT, sym), &info);\n    ASSERT_UCS_OK(status);\n\n    EXPECT_NE(std::string::npos, std::string(info.file.path).find(\"libucs.so\"));\n#ifdef HAVE_DETAILED_BACKTRACE\n    EXPECT_EQ(sym, std::string(info.function));\n#endif\n}\n\nUCS_TEST_F(test_debug, lookup_invalid) {\n    ucs_debug_address_info info;\n    ucs_status_t status = ucs_debug_lookup_address((void*)0xffffffffffff, &info);\n    EXPECT_EQ(UCS_ERR_NO_ELEM, status);\n}\n\nUCS_TEST_SKIP_COND_F(test_debug, lookup_address, BULLSEYE_ON) {\n    unsigned lineno;\n\n    my_cool_function(&lineno);\n\n    ucs_debug_address_info info;\n    ucs_status_t status = ucs_debug_lookup_address((void*)&my_cool_function,\n                                                   &info);\n    ASSERT_UCS_OK(status);\n\n    UCS_TEST_MESSAGE << info.source_file << \":\" << info.line_number <<\n                        \" \" << info.function << \"()\";\n\n    EXPECT_NE(std::string::npos, std::string(info.file.path).find(\"gtest\"));\n\n#ifdef HAVE_DETAILED_BACKTRACE\n    EXPECT_EQ(\"my_cool_function\", std::string(info.function));\n    EXPECT_EQ(lineno, info.line_number);\n    EXPECT_EQ(__basename(__FILE__), __basename(info.source_file));\n#else\n    EXPECT_EQ(0u, info.line_number);\n    EXPECT_EQ(\"???\", std::string(info.source_file));\n#endif\n}\n\nUCS_TEST_F(test_debug, print_backtrace) {\n    char *data;\n    size_t size;\n\n    FILE *f = open_memstream(&data, &size);\n    ucs_debug_print_backtrace(f, 0);\n    fclose(f);\n\n    /* Some functions that should appear */\n    EXPECT_TRUE(strstr(data, \"print_backtrace\") != NULL);\n#ifdef HAVE_DETAILED_BACKTRACE\n    EXPECT_TRUE(strstr(data, \"main\") != NULL);\n#endif\n\n    free(data);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/test/apps/test_dlopen_cfg_print.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <stdio.h>\n\n#define _QUOTE(x) #x\n#define QUOTE(x) _QUOTE(x)\n\n\nstatic void* do_dlopen_or_exit(const char *filename)\n{\n    void *handle;\n\n    (void)dlerror();\n    printf(\"opening '%s'\\n\", filename);\n    handle = dlopen(filename, RTLD_LAZY);\n    if (handle == NULL) {\n        fprintf(stderr, \"failed to open %s: %s\\n\", filename,\n                dlerror());\n        exit(1);\n    }\n\n    return handle;\n}\n\nint main(int argc, char **argv)\n{\n    typedef void (*print_all_opts_func_t)(FILE*, const char *, int);\n\n    const char *ucs_filename = QUOTE(UCS_LIB_PATH);\n    const char *uct_filename = QUOTE(UCT_LIB_PATH);\n    void *ucs_handle, *uct_handle;\n    int i;\n\n    /* unload and reload uct while ucs is loaded\n     * would fail if uct global vars are kept on global lists in ucs */\n    ucs_handle = do_dlopen_or_exit(ucs_filename);\n    for (i = 0; i < 2; ++i) {\n        uct_handle = do_dlopen_or_exit(uct_filename);\n        dlclose(uct_handle);\n    }\n\n    /* print all config table, to force going over the global list in ucs */\n    print_all_opts_func_t print_all_opts =\n        (print_all_opts_func_t)dlsym(ucs_handle, \"ucs_config_parser_print_all_opts\");\n    print_all_opts(stdout, \"TEST_\", 0);\n    dlclose(ucs_handle);\n\n    printf(\"done\\n\");\n    return 0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/test/apps/test_ucs_dlopen.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <sys/mman.h>\n#include <dlfcn.h>\n#include <stdio.h>\n#include <unistd.h>\n#include <stdlib.h>\n\n#include <ucm/api/ucm.h>\n\n#define _QUOTE(x) #x\n#define QUOTE(x) _QUOTE(x)\n\n\nstatic void vm_unmap_cb(ucm_event_type_t event_type, ucm_event_t *event,\n                        void *arg)\n{\n}\n\nint test_ucm_set_event_handler(void *handle)\n{\n    typedef ucs_status_t (*ucm_set_event_handler_func_t)(int events,\n                                                         int priority,\n                                                         ucm_event_callback_t cb,\n                                                         void *arg);\n\n    ucm_set_event_handler_func_t ucm_set_event_handler_f;\n    ucs_status_t status;\n\n    dlerror();\n    ucm_set_event_handler_f = (ucm_set_event_handler_func_t)dlsym(handle,\n                                                                  \"ucm_set_event_handler\");\n    if (ucm_set_event_handler_f == NULL) {\n        fprintf(stderr, \"failed to resolve ucm_set_event_handler(): %s\\n\",\n                dlerror());\n        return -1;\n    }\n\n    status = ucm_set_event_handler_f(UCM_EVENT_VM_UNMAPPED, 0, vm_unmap_cb,\n                                     NULL);\n    if (status != UCS_OK) {\n        fprintf(stderr, \"ucm_set_event_handler() failed\\n\");\n        return -1;\n    }\n\n    return 0;\n}\n\nint main(int argc, char **argv)\n{\n    const char *filename = QUOTE(LIB_PATH);\n    void *handle;\n    void *ptr1, *ptr2;\n    size_t alloc_size;\n    long ret;\n\n    /* get page size */\n    ret = sysconf(_SC_PAGESIZE);\n    if (ret < 0) {\n        fprintf(stderr, \"sysconf(_SC_PAGESIZE) failed: %m\\n\");\n        return -1;\n    }\n    alloc_size = ret;\n\n    /* allocate some memory */\n    ptr1 = malloc(alloc_size);\n    if (!ptr1) {\n        fprintf(stderr, \"malloc() failed\\n\");\n        return -1;\n    }\n\n    ptr2 = mmap(NULL, alloc_size, PROT_READ|PROT_WRITE,\n                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    if (ptr2 == MAP_FAILED) {\n        fprintf(stderr, \"mmmap() failed: %m\\n\");\n        ret = -1;\n        goto failed_mmap;\n    }\n\n    /* load ucm */\n    printf(\"opening '%s'\\n\", filename);\n    dlerror();\n    handle = dlopen(filename, RTLD_NOW);\n    if (handle == NULL) {\n        fprintf(stderr, \"failed to open %s: %s\\n\", filename, dlerror());\n        ret = -1;\n        goto failed_dlopen;\n    }\n\n    /* init ucm */\n    ret = test_ucm_set_event_handler(handle);\n\n    /* unload ucp */\n    dlclose(handle);\n\nfailed_dlopen:\n    /* release the memory - could break if UCM is unloaded */\n    munmap(ptr2, alloc_size);\nfailed_mmap:\n    free(ptr1);\n\n    printf(\"done\\n\");\n    return ret;\n}\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/test/apps/test_ucp_dlopen.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <sys/mman.h>\n#include <dlfcn.h>\n#include <stdio.h>\n#include <unistd.h>\n#include <stdlib.h>\n\n#include <ucp/api/ucp.h>\n\n#define _QUOTE(x) #x\n#define QUOTE(x) _QUOTE(x)\n\n\nint test_ucp_init(void *handle)\n{\n    typedef ucs_status_t (*ucp_init_version_func_t)(unsigned, unsigned,\n                                                    const ucp_params_t *,\n                                                    const ucp_config_t *,\n                                                    ucp_context_h *);\n    typedef void (*ucp_context_print_info_func_t)(const ucp_context_h, FILE*);\n    typedef void (*ucp_cleanup_func_t)(ucp_context_h);\n\n    ucp_init_version_func_t ucp_init_version_f;\n    ucp_context_print_info_func_t ucp_context_print_info_f;\n    ucp_cleanup_func_t ucp_cleanup_f;\n    ucp_params_t ucp_params;\n    ucs_status_t status;\n    ucp_context_h ucph;\n\n    ucp_init_version_f       = (ucp_init_version_func_t)dlsym(handle,\n                                                              \"ucp_init_version\");\n    ucp_cleanup_f            = (ucp_cleanup_func_t)dlsym(handle, \"ucp_cleanup\");\n    ucp_context_print_info_f = (ucp_context_print_info_func_t)dlsym(handle,\n                                                                    \"ucp_context_print_info\");\n\n    if (!ucp_init_version_f || !ucp_cleanup_f || !ucp_context_print_info_f) {\n        fprintf(stderr, \"failed to get UCP function pointers\\n\");\n        return -1;\n    }\n\n    ucp_params.field_mask = UCP_PARAM_FIELD_FEATURES;\n    ucp_params.features   = UCP_FEATURE_RMA;\n    status = ucp_init_version_f(UCP_API_MAJOR, UCP_API_MINOR, &ucp_params,\n                                NULL, &ucph);\n    if (status != UCS_OK) {\n        fprintf(stderr, \"ucp_init_version() failed\\n\");\n        return -1;\n    }\n\n    ucp_context_print_info_f(ucph, stdout);\n    ucp_cleanup_f(ucph);\n\n    return 0;\n}\n\nint main(int argc, char **argv)\n{\n    const char *filename = QUOTE(LIB_PATH);\n    void *handle;\n    void *ptr1, *ptr2;\n    size_t alloc_size;\n    long ret;\n\n    /* get page size */\n    ret = sysconf(_SC_PAGESIZE);\n    if (ret < 0) {\n        fprintf(stderr, \"sysconf(_SC_PAGESIZE) failed: %m\\n\");\n        return -1;\n    }\n    alloc_size = ret;\n\n    /* allocate some memory */\n    ptr1 = malloc(alloc_size);\n    if (!ptr1) {\n        fprintf(stderr, \"malloc() failed\\n\");\n        return -1;\n    }\n\n    ptr2 = mmap(NULL, alloc_size, PROT_READ|PROT_WRITE,\n                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    if (ptr2 == MAP_FAILED) {\n        fprintf(stderr, \"mmmap() failed: %m\\n\");\n        ret = -1;\n        goto failed_mmap;\n    }\n\n    /* load ucp */\n    printf(\"opening '%s'\\n\", filename);\n    handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n    if (handle == NULL) {\n        fprintf(stderr, \"failed to open %s: %m\\n\", filename);\n        ret = -1;\n        goto failed_dlopen;\n    }\n\n    /* init ucp */\n    ret = test_ucp_init(handle);\n\n    /* unload ucp */\n    dlclose(handle);\n\nfailed_dlopen:\n    /* relase the memory - could break if UCM is unloaded */\n    munmap(ptr2, alloc_size);\nfailed_mmap:\n    free(ptr1);\n\n    printf(\"done\\n\");\n    return ret;\n}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/.git/objects/pack/pack-0f47f55f3921e335951715271a730b95bb72bf75.pack",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/.git/objects/pack/pack-0f47f55f3921e335951715271a730b95bb72bf75.idx",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/docs/source/_static/ucxlogo.png",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/docs/source/_static/UCX_Layers.png",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/docs/doxygen/Architecture.png",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/docs/doxygen/Architecture.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/docs/doxygen/UCX_Logo_930x933.png",
        "/tmp/vanessa/spack-stage/spack-stage-ucx-1.9-dev-li4x2vqav4odyolcqylalut6u37elidt/spack-src/docs/doxygen/UCX_Logo_80x80.png"
    ],
    "total_files": 869
}