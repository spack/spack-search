{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/runtime/src/kmp_ftn_entry.h": "/*\n * kmp_ftn_entry.h -- Fortran entry linkage support for OpenMP.\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef FTN_STDCALL\n#error The support file kmp_ftn_entry.h should not be compiled by itself.\n#endif\n\n#ifdef KMP_STUB\n#include \"kmp_stub.h\"\n#endif\n\n#include \"kmp_i18n.h\"\n\n// For affinity format functions\n#include \"kmp_io.h\"\n#include \"kmp_str.h\"\n\n#if OMPT_SUPPORT\n#include \"ompt-specific.h\"\n#endif\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif // __cplusplus\n\n/* For compatibility with the Gnu/MS Open MP codegen, omp_set_num_threads(),\n * omp_set_nested(), and omp_set_dynamic() [in lowercase on MS, and w/o\n * a trailing underscore on Linux* OS] take call by value integer arguments.\n * + omp_set_max_active_levels()\n * + omp_set_schedule()\n *\n * For backward compatibility with 9.1 and previous Intel compiler, these\n * entry points take call by reference integer arguments. */\n#ifdef KMP_GOMP_COMPAT\n#if (KMP_FTN_ENTRIES == KMP_FTN_PLAIN) || (KMP_FTN_ENTRIES == KMP_FTN_UPPER)\n#define PASS_ARGS_BY_VALUE 1\n#endif\n#endif\n#if KMP_OS_WINDOWS\n#if (KMP_FTN_ENTRIES == KMP_FTN_PLAIN) || (KMP_FTN_ENTRIES == KMP_FTN_APPEND)\n#define PASS_ARGS_BY_VALUE 1\n#endif\n#endif\n\n// This macro helps to reduce code duplication.\n#ifdef PASS_ARGS_BY_VALUE\n#define KMP_DEREF\n#else\n#define KMP_DEREF *\n#endif\n\nvoid FTN_STDCALL FTN_SET_STACKSIZE(int KMP_DEREF arg) {\n#ifdef KMP_STUB\n  __kmps_set_stacksize(KMP_DEREF arg);\n#else\n  // __kmp_aux_set_stacksize initializes the library if needed\n  __kmp_aux_set_stacksize((size_t)KMP_DEREF arg);\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_STACKSIZE_S(size_t KMP_DEREF arg) {\n#ifdef KMP_STUB\n  __kmps_set_stacksize(KMP_DEREF arg);\n#else\n  // __kmp_aux_set_stacksize initializes the library if needed\n  __kmp_aux_set_stacksize(KMP_DEREF arg);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_STACKSIZE(void) {\n#ifdef KMP_STUB\n  return __kmps_get_stacksize();\n#else\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  return (int)__kmp_stksize;\n#endif\n}\n\nsize_t FTN_STDCALL FTN_GET_STACKSIZE_S(void) {\n#ifdef KMP_STUB\n  return __kmps_get_stacksize();\n#else\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  return __kmp_stksize;\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_BLOCKTIME(int KMP_DEREF arg) {\n#ifdef KMP_STUB\n  __kmps_set_blocktime(KMP_DEREF arg);\n#else\n  int gtid, tid;\n  kmp_info_t *thread;\n\n  gtid = __kmp_entry_gtid();\n  tid = __kmp_tid_from_gtid(gtid);\n  thread = __kmp_thread_from_gtid(gtid);\n\n  __kmp_aux_set_blocktime(KMP_DEREF arg, thread, tid);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_BLOCKTIME(void) {\n#ifdef KMP_STUB\n  return __kmps_get_blocktime();\n#else\n  int gtid, tid;\n  kmp_info_t *thread;\n  kmp_team_p *team;\n\n  gtid = __kmp_entry_gtid();\n  tid = __kmp_tid_from_gtid(gtid);\n  thread = __kmp_thread_from_gtid(gtid);\n  team = __kmp_threads[gtid]->th.th_team;\n\n  /* These must match the settings used in __kmp_wait_sleep() */\n  if (__kmp_dflt_blocktime == KMP_MAX_BLOCKTIME) {\n    KF_TRACE(10, (\"kmp_get_blocktime: T#%d(%d:%d), blocktime=%d\\n\", gtid,\n                  team->t.t_id, tid, KMP_MAX_BLOCKTIME));\n    return KMP_MAX_BLOCKTIME;\n  }\n#ifdef KMP_ADJUST_BLOCKTIME\n  else if (__kmp_zero_bt && !get__bt_set(team, tid)) {\n    KF_TRACE(10, (\"kmp_get_blocktime: T#%d(%d:%d), blocktime=%d\\n\", gtid,\n                  team->t.t_id, tid, 0));\n    return 0;\n  }\n#endif /* KMP_ADJUST_BLOCKTIME */\n  else {\n    KF_TRACE(10, (\"kmp_get_blocktime: T#%d(%d:%d), blocktime=%d\\n\", gtid,\n                  team->t.t_id, tid, get__blocktime(team, tid)));\n    return get__blocktime(team, tid);\n  }\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_LIBRARY_SERIAL(void) {\n#ifdef KMP_STUB\n  __kmps_set_library(library_serial);\n#else\n  // __kmp_user_set_library initializes the library if needed\n  __kmp_user_set_library(library_serial);\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_LIBRARY_TURNAROUND(void) {\n#ifdef KMP_STUB\n  __kmps_set_library(library_turnaround);\n#else\n  // __kmp_user_set_library initializes the library if needed\n  __kmp_user_set_library(library_turnaround);\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_LIBRARY_THROUGHPUT(void) {\n#ifdef KMP_STUB\n  __kmps_set_library(library_throughput);\n#else\n  // __kmp_user_set_library initializes the library if needed\n  __kmp_user_set_library(library_throughput);\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_LIBRARY(int KMP_DEREF arg) {\n#ifdef KMP_STUB\n  __kmps_set_library(KMP_DEREF arg);\n#else\n  enum library_type lib;\n  lib = (enum library_type)KMP_DEREF arg;\n  // __kmp_user_set_library initializes the library if needed\n  __kmp_user_set_library(lib);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_LIBRARY(void) {\n#ifdef KMP_STUB\n  return __kmps_get_library();\n#else\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  return ((int)__kmp_library);\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_DISP_NUM_BUFFERS(int KMP_DEREF arg) {\n#ifdef KMP_STUB\n  ; // empty routine\n#else\n  // ignore after initialization because some teams have already\n  // allocated dispatch buffers\n  if (__kmp_init_serial == 0 && (KMP_DEREF arg) > 0)\n    __kmp_dispatch_num_buffers = KMP_DEREF arg;\n#endif\n}\n\nint FTN_STDCALL FTN_SET_AFFINITY(void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_aux_set_affinity(mask);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_AFFINITY(void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_aux_get_affinity(mask);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_AFFINITY_MAX_PROC(void) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  // We really only NEED serial initialization here.\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_aux_get_affinity_max_proc();\n#endif\n}\n\nvoid FTN_STDCALL FTN_CREATE_AFFINITY_MASK(void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  *mask = NULL;\n#else\n  // We really only NEED serial initialization here.\n  kmp_affin_mask_t *mask_internals;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  mask_internals = __kmp_affinity_dispatch->allocate_mask();\n  KMP_CPU_ZERO(mask_internals);\n  *mask = mask_internals;\n#endif\n}\n\nvoid FTN_STDCALL FTN_DESTROY_AFFINITY_MASK(void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n// Nothing\n#else\n  // We really only NEED serial initialization here.\n  kmp_affin_mask_t *mask_internals;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (__kmp_env_consistency_check) {\n    if (*mask == NULL) {\n      KMP_FATAL(AffinityInvalidMask, \"kmp_destroy_affinity_mask\");\n    }\n  }\n  mask_internals = (kmp_affin_mask_t *)(*mask);\n  __kmp_affinity_dispatch->deallocate_mask(mask_internals);\n  *mask = NULL;\n#endif\n}\n\nint FTN_STDCALL FTN_SET_AFFINITY_MASK_PROC(int KMP_DEREF proc, void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_aux_set_affinity_mask_proc(KMP_DEREF proc, mask);\n#endif\n}\n\nint FTN_STDCALL FTN_UNSET_AFFINITY_MASK_PROC(int KMP_DEREF proc, void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_aux_unset_affinity_mask_proc(KMP_DEREF proc, mask);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_AFFINITY_MASK_PROC(int KMP_DEREF proc, void **mask) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_aux_get_affinity_mask_proc(KMP_DEREF proc, mask);\n#endif\n}\n\n/* ------------------------------------------------------------------------ */\n\n/* sets the requested number of threads for the next parallel region */\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_NUM_THREADS)(int KMP_DEREF arg) {\n#ifdef KMP_STUB\n// Nothing.\n#else\n  __kmp_set_num_threads(KMP_DEREF arg, __kmp_entry_gtid());\n#endif\n}\n\n/* returns the number of threads in current team */\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NUM_THREADS)(void) {\n#ifdef KMP_STUB\n  return 1;\n#else\n  // __kmpc_bound_num_threads initializes the library if needed\n  return __kmpc_bound_num_threads(NULL);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_MAX_THREADS)(void) {\n#ifdef KMP_STUB\n  return 1;\n#else\n  int gtid;\n  kmp_info_t *thread;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_threads[gtid];\n  // return thread -> th.th_team -> t.t_current_task[\n  // thread->th.th_info.ds.ds_tid ] -> icvs.nproc;\n  return thread->th.th_current_task->td_icvs.nproc;\n#endif\n}\n\nint FTN_STDCALL FTN_CONTROL_TOOL(int command, int modifier, void *arg) {\n#if defined(KMP_STUB) || !OMPT_SUPPORT\n  return -2;\n#else\n  OMPT_STORE_RETURN_ADDRESS(__kmp_entry_gtid());\n  if (!TCR_4(__kmp_init_middle)) {\n    return -2;\n  }\n  kmp_info_t *this_thr = __kmp_threads[__kmp_entry_gtid()];\n  ompt_task_info_t *parent_task_info = OMPT_CUR_TASK_INFO(this_thr);\n  parent_task_info->frame.enter_frame.ptr = OMPT_GET_FRAME_ADDRESS(0);\n  int ret = __kmp_control_tool(command, modifier, arg);\n  parent_task_info->frame.enter_frame.ptr = 0;\n  return ret;\n#endif\n}\n\n/* OpenMP 5.0 Memory Management support */\nomp_allocator_handle_t FTN_STDCALL\nFTN_INIT_ALLOCATOR(omp_memspace_handle_t KMP_DEREF m, int KMP_DEREF ntraits,\n                   omp_alloctrait_t tr[]) {\n#ifdef KMP_STUB\n  return NULL;\n#else\n  return __kmpc_init_allocator(__kmp_entry_gtid(), KMP_DEREF m,\n                               KMP_DEREF ntraits, tr);\n#endif\n}\n\nvoid FTN_STDCALL FTN_DESTROY_ALLOCATOR(omp_allocator_handle_t al) {\n#ifndef KMP_STUB\n  __kmpc_destroy_allocator(__kmp_entry_gtid(), al);\n#endif\n}\nvoid FTN_STDCALL FTN_SET_DEFAULT_ALLOCATOR(omp_allocator_handle_t al) {\n#ifndef KMP_STUB\n  __kmpc_set_default_allocator(__kmp_entry_gtid(), al);\n#endif\n}\nomp_allocator_handle_t FTN_STDCALL FTN_GET_DEFAULT_ALLOCATOR(void) {\n#ifdef KMP_STUB\n  return NULL;\n#else\n  return __kmpc_get_default_allocator(__kmp_entry_gtid());\n#endif\n}\n\n/* OpenMP 5.0 affinity format support */\n#ifndef KMP_STUB\nstatic void __kmp_fortran_strncpy_truncate(char *buffer, size_t buf_size,\n                                           char const *csrc, size_t csrc_size) {\n  size_t capped_src_size = csrc_size;\n  if (csrc_size >= buf_size) {\n    capped_src_size = buf_size - 1;\n  }\n  KMP_STRNCPY_S(buffer, buf_size, csrc, capped_src_size);\n  if (csrc_size >= buf_size) {\n    KMP_DEBUG_ASSERT(buffer[buf_size - 1] == '\\0');\n    buffer[buf_size - 1] = csrc[buf_size - 1];\n  } else {\n    for (size_t i = csrc_size; i < buf_size; ++i)\n      buffer[i] = ' ';\n  }\n}\n\n// Convert a Fortran string to a C string by adding null byte\nclass ConvertedString {\n  char *buf;\n  kmp_info_t *th;\n\npublic:\n  ConvertedString(char const *fortran_str, size_t size) {\n    th = __kmp_get_thread();\n    buf = (char *)__kmp_thread_malloc(th, size + 1);\n    KMP_STRNCPY_S(buf, size + 1, fortran_str, size);\n    buf[size] = '\\0';\n  }\n  ~ConvertedString() { __kmp_thread_free(th, buf); }\n  const char *get() const { return buf; }\n};\n#endif // KMP_STUB\n\n/*\n * Set the value of the affinity-format-var ICV on the current device to the\n * format specified in the argument.\n*/\nvoid FTN_STDCALL FTN_SET_AFFINITY_FORMAT(char const *format, size_t size) {\n#ifdef KMP_STUB\n  return;\n#else\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  ConvertedString cformat(format, size);\n  // Since the __kmp_affinity_format variable is a C string, do not\n  // use the fortran strncpy function\n  __kmp_strncpy_truncate(__kmp_affinity_format, KMP_AFFINITY_FORMAT_SIZE,\n                         cformat.get(), KMP_STRLEN(cformat.get()));\n#endif\n}\n\n/*\n * Returns the number of characters required to hold the entire affinity format\n * specification (not including null byte character) and writes the value of the\n * affinity-format-var ICV on the current device to buffer. If the return value\n * is larger than size, the affinity format specification is truncated.\n*/\nsize_t FTN_STDCALL FTN_GET_AFFINITY_FORMAT(char *buffer, size_t size) {\n#ifdef KMP_STUB\n  return 0;\n#else\n  size_t format_size;\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  format_size = KMP_STRLEN(__kmp_affinity_format);\n  if (buffer && size) {\n    __kmp_fortran_strncpy_truncate(buffer, size, __kmp_affinity_format,\n                                   format_size);\n  }\n  return format_size;\n#endif\n}\n\n/*\n * Prints the thread affinity information of the current thread in the format\n * specified by the format argument. If the format is NULL or a zero-length\n * string, the value of the affinity-format-var ICV is used.\n*/\nvoid FTN_STDCALL FTN_DISPLAY_AFFINITY(char const *format, size_t size) {\n#ifdef KMP_STUB\n  return;\n#else\n  int gtid;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  gtid = __kmp_get_gtid();\n  ConvertedString cformat(format, size);\n  __kmp_aux_display_affinity(gtid, cformat.get());\n#endif\n}\n\n/*\n * Returns the number of characters required to hold the entire affinity format\n * specification (not including null byte) and prints the thread affinity\n * information of the current thread into the character string buffer with the\n * size of size in the format specified by the format argument. If the format is\n * NULL or a zero-length string, the value of the affinity-format-var ICV is\n * used. The buffer must be allocated prior to calling the routine. If the\n * return value is larger than size, the affinity format specification is\n * truncated.\n*/\nsize_t FTN_STDCALL FTN_CAPTURE_AFFINITY(char *buffer, char const *format,\n                                        size_t buf_size, size_t for_size) {\n#if defined(KMP_STUB)\n  return 0;\n#else\n  int gtid;\n  size_t num_required;\n  kmp_str_buf_t capture_buf;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  gtid = __kmp_get_gtid();\n  __kmp_str_buf_init(&capture_buf);\n  ConvertedString cformat(format, for_size);\n  num_required = __kmp_aux_capture_affinity(gtid, cformat.get(), &capture_buf);\n  if (buffer && buf_size) {\n    __kmp_fortran_strncpy_truncate(buffer, buf_size, capture_buf.str,\n                                   capture_buf.used);\n  }\n  __kmp_str_buf_free(&capture_buf);\n  return num_required;\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_THREAD_NUM)(void) {\n#ifdef KMP_STUB\n  return 0;\n#else\n  int gtid;\n\n#if KMP_USE_ABT\n  if (!__kmp_init_parallel)\n    return 0;\n  gtid = __kmp_gtid_get_specific();\n#elif KMP_OS_DARWIN || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||  \\\n      KMP_OS_HURD|| KMP_OS_OPENBSD\n  gtid = __kmp_entry_gtid();\n#elif KMP_OS_WINDOWS\n  if (!__kmp_init_parallel ||\n      (gtid = (int)((kmp_intptr_t)TlsGetValue(__kmp_gtid_threadprivate_key))) ==\n          0) {\n    // Either library isn't initialized or thread is not registered\n    // 0 is the correct TID in this case\n    return 0;\n  }\n  --gtid; // We keep (gtid+1) in TLS\n#elif KMP_OS_LINUX\n#ifdef KMP_TDATA_GTID\n  if (__kmp_gtid_mode >= 3) {\n    if ((gtid = __kmp_gtid) == KMP_GTID_DNE) {\n      return 0;\n    }\n  } else {\n#endif\n    if (!__kmp_init_parallel ||\n        (gtid = (kmp_intptr_t)(\n             pthread_getspecific(__kmp_gtid_threadprivate_key))) == 0) {\n      return 0;\n    }\n    --gtid;\n#ifdef KMP_TDATA_GTID\n  }\n#endif\n#else\n#error Unknown or unsupported OS\n#endif\n\n  return __kmp_tid_from_gtid(gtid);\n#endif\n}\n\nint FTN_STDCALL FTN_GET_NUM_KNOWN_THREADS(void) {\n#ifdef KMP_STUB\n  return 1;\n#else\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  /* NOTE: this is not syncronized, so it can change at any moment */\n  /* NOTE: this number also includes threads preallocated in hot-teams */\n  return TCR_4(__kmp_nth);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NUM_PROCS)(void) {\n#ifdef KMP_STUB\n  return 1;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  return __kmp_avail_proc;\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_NESTED)(int KMP_DEREF flag) {\n  KMP_INFORM(APIDeprecated, \"omp_set_nested\", \"omp_set_max_active_levels\");\n#ifdef KMP_STUB\n  __kmps_set_nested(KMP_DEREF flag);\n#else\n  kmp_info_t *thread;\n  /* For the thread-private internal controls implementation */\n  thread = __kmp_entry_thread();\n  __kmp_save_internal_controls(thread);\n  // Somewhat arbitrarily decide where to get a value for max_active_levels\n  int max_active_levels = get__max_active_levels(thread);\n  if (max_active_levels == 1)\n    max_active_levels = KMP_MAX_ACTIVE_LEVELS_LIMIT;\n  set__max_active_levels(thread, (KMP_DEREF flag) ? max_active_levels : 1);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NESTED)(void) {\n  KMP_INFORM(APIDeprecated, \"omp_get_nested\", \"omp_get_max_active_levels\");\n#ifdef KMP_STUB\n  return __kmps_get_nested();\n#else\n  kmp_info_t *thread;\n  thread = __kmp_entry_thread();\n  return get__max_active_levels(thread) > 1;\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_DYNAMIC)(int KMP_DEREF flag) {\n#ifdef KMP_STUB\n  __kmps_set_dynamic(KMP_DEREF flag ? TRUE : FALSE);\n#else\n  kmp_info_t *thread;\n  /* For the thread-private implementation of the internal controls */\n  thread = __kmp_entry_thread();\n  // !!! What if foreign thread calls it?\n  __kmp_save_internal_controls(thread);\n  set__dynamic(thread, KMP_DEREF flag ? TRUE : FALSE);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_DYNAMIC)(void) {\n#ifdef KMP_STUB\n  return __kmps_get_dynamic();\n#else\n  kmp_info_t *thread;\n  thread = __kmp_entry_thread();\n  return get__dynamic(thread);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_IN_PARALLEL)(void) {\n#ifdef KMP_STUB\n  return 0;\n#else\n  kmp_info_t *th = __kmp_entry_thread();\n  if (th->th.th_teams_microtask) {\n    // AC: r_in_parallel does not work inside teams construct where real\n    // parallel is inactive, but all threads have same root, so setting it in\n    // one team affects other teams.\n    // The solution is to use per-team nesting level\n    return (th->th.th_team->t.t_active_level ? 1 : 0);\n  } else\n    return (th->th.th_root->r.r_active ? FTN_TRUE : FTN_FALSE);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_SCHEDULE)(kmp_sched_t KMP_DEREF kind,\n                                                   int KMP_DEREF modifier) {\n#ifdef KMP_STUB\n  __kmps_set_schedule(KMP_DEREF kind, KMP_DEREF modifier);\n#else\n  /* TO DO: For the per-task implementation of the internal controls */\n  __kmp_set_schedule(__kmp_entry_gtid(), KMP_DEREF kind, KMP_DEREF modifier);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_SCHEDULE)(kmp_sched_t *kind,\n                                                   int *modifier) {\n#ifdef KMP_STUB\n  __kmps_get_schedule(kind, modifier);\n#else\n  /* TO DO: For the per-task implementation of the internal controls */\n  __kmp_get_schedule(__kmp_entry_gtid(), kind, modifier);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_MAX_ACTIVE_LEVELS)(int KMP_DEREF arg) {\n#ifdef KMP_STUB\n// Nothing.\n#else\n  /* TO DO: We want per-task implementation of this internal control */\n  __kmp_set_max_active_levels(__kmp_entry_gtid(), KMP_DEREF arg);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_MAX_ACTIVE_LEVELS)(void) {\n#ifdef KMP_STUB\n  return 0;\n#else\n  /* TO DO: We want per-task implementation of this internal control */\n  return __kmp_get_max_active_levels(__kmp_entry_gtid());\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_ACTIVE_LEVEL)(void) {\n#ifdef KMP_STUB\n  return 0; // returns 0 if it is called from the sequential part of the program\n#else\n  /* TO DO: For the per-task implementation of the internal controls */\n  return __kmp_entry_thread()->th.th_team->t.t_active_level;\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_LEVEL)(void) {\n#ifdef KMP_STUB\n  return 0; // returns 0 if it is called from the sequential part of the program\n#else\n  /* TO DO: For the per-task implementation of the internal controls */\n  return __kmp_entry_thread()->th.th_team->t.t_level;\n#endif\n}\n\nint FTN_STDCALL\n    KMP_EXPAND_NAME(FTN_GET_ANCESTOR_THREAD_NUM)(int KMP_DEREF level) {\n#ifdef KMP_STUB\n  return (KMP_DEREF level) ? (-1) : (0);\n#else\n  return __kmp_get_ancestor_thread_num(__kmp_entry_gtid(), KMP_DEREF level);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_TEAM_SIZE)(int KMP_DEREF level) {\n#ifdef KMP_STUB\n  return (KMP_DEREF level) ? (-1) : (1);\n#else\n  return __kmp_get_team_size(__kmp_entry_gtid(), KMP_DEREF level);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_THREAD_LIMIT)(void) {\n#ifdef KMP_STUB\n  return 1; // TO DO: clarify whether it returns 1 or 0?\n#else\n  int gtid;\n  kmp_info_t *thread;\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_threads[gtid];\n  return thread->th.th_current_task->td_icvs.thread_limit;\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_IN_FINAL)(void) {\n#ifdef KMP_STUB\n  return 0; // TO DO: clarify whether it returns 1 or 0?\n#else\n  if (!TCR_4(__kmp_init_parallel)) {\n    return 0;\n  }\n  return __kmp_entry_thread()->th.th_current_task->td_flags.final;\n#endif\n}\n\nkmp_proc_bind_t FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_PROC_BIND)(void) {\n#ifdef KMP_STUB\n  return __kmps_get_proc_bind();\n#else\n  return get__proc_bind(__kmp_entry_thread());\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NUM_PLACES)(void) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  return __kmp_affinity_num_masks;\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_PLACE_NUM_PROCS)(int place_num) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  int i;\n  int retval = 0;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  if (place_num < 0 || place_num >= (int)__kmp_affinity_num_masks)\n    return 0;\n  kmp_affin_mask_t *mask = KMP_CPU_INDEX(__kmp_affinity_masks, place_num);\n  KMP_CPU_SET_ITERATE(i, mask) {\n    if ((!KMP_CPU_ISSET(i, __kmp_affin_fullMask)) ||\n        (!KMP_CPU_ISSET(i, mask))) {\n      continue;\n    }\n    ++retval;\n  }\n  return retval;\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_PLACE_PROC_IDS)(int place_num,\n                                                         int *ids) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n// Nothing.\n#else\n  int i, j;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (!KMP_AFFINITY_CAPABLE())\n    return;\n  if (place_num < 0 || place_num >= (int)__kmp_affinity_num_masks)\n    return;\n  kmp_affin_mask_t *mask = KMP_CPU_INDEX(__kmp_affinity_masks, place_num);\n  j = 0;\n  KMP_CPU_SET_ITERATE(i, mask) {\n    if ((!KMP_CPU_ISSET(i, __kmp_affin_fullMask)) ||\n        (!KMP_CPU_ISSET(i, mask))) {\n      continue;\n    }\n    ids[j++] = i;\n  }\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_PLACE_NUM)(void) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  int gtid;\n  kmp_info_t *thread;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (!KMP_AFFINITY_CAPABLE())\n    return -1;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread->th.th_current_place < 0)\n    return -1;\n  return thread->th.th_current_place;\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_PARTITION_NUM_PLACES)(void) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  int gtid, num_places, first_place, last_place;\n  kmp_info_t *thread;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  first_place = thread->th.th_first_place;\n  last_place = thread->th.th_last_place;\n  if (first_place < 0 || last_place < 0)\n    return 0;\n  if (first_place <= last_place)\n    num_places = last_place - first_place + 1;\n  else\n    num_places = __kmp_affinity_num_masks - first_place + last_place + 1;\n  return num_places;\n#endif\n}\n\nvoid\n    FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_PARTITION_PLACE_NUMS)(int *place_nums) {\n#if defined(KMP_STUB) || !KMP_AFFINITY_SUPPORTED\n// Nothing.\n#else\n  int i, gtid, place_num, first_place, last_place, start, end;\n  kmp_info_t *thread;\n  if (!TCR_4(__kmp_init_middle)) {\n    __kmp_middle_initialize();\n  }\n  if (!KMP_AFFINITY_CAPABLE())\n    return;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  first_place = thread->th.th_first_place;\n  last_place = thread->th.th_last_place;\n  if (first_place < 0 || last_place < 0)\n    return;\n  if (first_place <= last_place) {\n    start = first_place;\n    end = last_place;\n  } else {\n    start = last_place;\n    end = first_place;\n  }\n  for (i = 0, place_num = start; place_num <= end; ++place_num, ++i) {\n    place_nums[i] = place_num;\n  }\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NUM_TEAMS)(void) {\n#ifdef KMP_STUB\n  return 1;\n#else\n  return __kmp_aux_get_num_teams();\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_TEAM_NUM)(void) {\n#ifdef KMP_STUB\n  return 0;\n#else\n  return __kmp_aux_get_team_num();\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_DEFAULT_DEVICE)(void) {\n#if KMP_MIC || KMP_OS_DARWIN || defined(KMP_STUB)\n  return 0;\n#else\n  return __kmp_entry_thread()->th.th_current_task->td_icvs.default_device;\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_DEFAULT_DEVICE)(int KMP_DEREF arg) {\n#if KMP_MIC || KMP_OS_DARWIN || defined(KMP_STUB)\n// Nothing.\n#else\n  __kmp_entry_thread()->th.th_current_task->td_icvs.default_device =\n      KMP_DEREF arg;\n#endif\n}\n\n// Get number of NON-HOST devices.\n// libomptarget, if loaded, provides this function in api.cpp.\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NUM_DEVICES)(void) KMP_WEAK_ATTRIBUTE_EXTERNAL;\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_NUM_DEVICES)(void) {\n#if KMP_MIC || KMP_OS_DARWIN || KMP_OS_WINDOWS || defined(KMP_STUB)\n  return 0;\n#else\n  int (*fptr)();\n  if ((*(void **)(&fptr) = dlsym(RTLD_DEFAULT, \"_Offload_number_of_devices\"))) {\n    return (*fptr)();\n  } else if ((*(void **)(&fptr) = dlsym(RTLD_NEXT, \"omp_get_num_devices\"))) {\n    return (*fptr)();\n  } else { // liboffload & libomptarget don't exist\n    return 0;\n  }\n#endif // KMP_MIC || KMP_OS_DARWIN || KMP_OS_WINDOWS || defined(KMP_STUB)\n}\n\n// This function always returns true when called on host device.\n// Compiler/libomptarget should handle when it is called inside target region.\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_IS_INITIAL_DEVICE)(void) KMP_WEAK_ATTRIBUTE_EXTERNAL;\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_IS_INITIAL_DEVICE)(void) {\n  return 1; // This is the host\n}\n\n// libomptarget, if loaded, provides this function\nint FTN_STDCALL FTN_GET_INITIAL_DEVICE(void) KMP_WEAK_ATTRIBUTE_EXTERNAL;\nint FTN_STDCALL FTN_GET_INITIAL_DEVICE(void) {\n#if KMP_MIC || KMP_OS_DARWIN || KMP_OS_WINDOWS || defined(KMP_STUB)\n  return KMP_HOST_DEVICE;\n#else\n  int (*fptr)();\n  if ((*(void **)(&fptr) = dlsym(RTLD_NEXT, \"omp_get_initial_device\"))) {\n    return (*fptr)();\n  } else { // liboffload & libomptarget don't exist\n    return KMP_HOST_DEVICE;\n  }\n#endif\n}\n\n#if defined(KMP_STUB)\n// Entries for stubs library\n// As all *target* functions are C-only parameters always passed by value\nvoid *FTN_STDCALL FTN_TARGET_ALLOC(size_t size, int device_num) { return 0; }\n\nvoid FTN_STDCALL FTN_TARGET_FREE(void *device_ptr, int device_num) {}\n\nint FTN_STDCALL FTN_TARGET_IS_PRESENT(void *ptr, int device_num) { return 0; }\n\nint FTN_STDCALL FTN_TARGET_MEMCPY(void *dst, void *src, size_t length,\n                                  size_t dst_offset, size_t src_offset,\n                                  int dst_device, int src_device) {\n  return -1;\n}\n\nint FTN_STDCALL FTN_TARGET_MEMCPY_RECT(\n    void *dst, void *src, size_t element_size, int num_dims,\n    const size_t *volume, const size_t *dst_offsets, const size_t *src_offsets,\n    const size_t *dst_dimensions, const size_t *src_dimensions, int dst_device,\n    int src_device) {\n  return -1;\n}\n\nint FTN_STDCALL FTN_TARGET_ASSOCIATE_PTR(void *host_ptr, void *device_ptr,\n                                         size_t size, size_t device_offset,\n                                         int device_num) {\n  return -1;\n}\n\nint FTN_STDCALL FTN_TARGET_DISASSOCIATE_PTR(void *host_ptr, int device_num) {\n  return -1;\n}\n#endif // defined(KMP_STUB)\n\n#ifdef KMP_STUB\ntypedef enum { UNINIT = -1, UNLOCKED, LOCKED } kmp_stub_lock_t;\n#endif /* KMP_STUB */\n\n#if KMP_USE_DYNAMIC_LOCK\nvoid FTN_STDCALL FTN_INIT_LOCK_WITH_HINT(void **user_lock,\n                                         uintptr_t KMP_DEREF hint) {\n#ifdef KMP_STUB\n  *((kmp_stub_lock_t *)user_lock) = UNLOCKED;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_init_lock_with_hint(NULL, gtid, user_lock, KMP_DEREF hint);\n#endif\n}\n\nvoid FTN_STDCALL FTN_INIT_NEST_LOCK_WITH_HINT(void **user_lock,\n                                              uintptr_t KMP_DEREF hint) {\n#ifdef KMP_STUB\n  *((kmp_stub_lock_t *)user_lock) = UNLOCKED;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_init_nest_lock_with_hint(NULL, gtid, user_lock, KMP_DEREF hint);\n#endif\n}\n#endif\n\n/* initialize the lock */\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_INIT_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  *((kmp_stub_lock_t *)user_lock) = UNLOCKED;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_init_lock(NULL, gtid, user_lock);\n#endif\n}\n\n/* initialize the lock */\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_INIT_NEST_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  *((kmp_stub_lock_t *)user_lock) = UNLOCKED;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_init_nest_lock(NULL, gtid, user_lock);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_DESTROY_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  *((kmp_stub_lock_t *)user_lock) = UNINIT;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_destroy_lock(NULL, gtid, user_lock);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_DESTROY_NEST_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  *((kmp_stub_lock_t *)user_lock) = UNINIT;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_destroy_nest_lock(NULL, gtid, user_lock);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  if (*((kmp_stub_lock_t *)user_lock) == UNINIT) {\n    // TODO: Issue an error.\n  }\n  if (*((kmp_stub_lock_t *)user_lock) != UNLOCKED) {\n    // TODO: Issue an error.\n  }\n  *((kmp_stub_lock_t *)user_lock) = LOCKED;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_set_lock(NULL, gtid, user_lock);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_SET_NEST_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  if (*((kmp_stub_lock_t *)user_lock) == UNINIT) {\n    // TODO: Issue an error.\n  }\n  (*((int *)user_lock))++;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_set_nest_lock(NULL, gtid, user_lock);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_UNSET_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  if (*((kmp_stub_lock_t *)user_lock) == UNINIT) {\n    // TODO: Issue an error.\n  }\n  if (*((kmp_stub_lock_t *)user_lock) == UNLOCKED) {\n    // TODO: Issue an error.\n  }\n  *((kmp_stub_lock_t *)user_lock) = UNLOCKED;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_unset_lock(NULL, gtid, user_lock);\n#endif\n}\n\nvoid FTN_STDCALL KMP_EXPAND_NAME(FTN_UNSET_NEST_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  if (*((kmp_stub_lock_t *)user_lock) == UNINIT) {\n    // TODO: Issue an error.\n  }\n  if (*((kmp_stub_lock_t *)user_lock) == UNLOCKED) {\n    // TODO: Issue an error.\n  }\n  (*((int *)user_lock))--;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  __kmpc_unset_nest_lock(NULL, gtid, user_lock);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_TEST_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  if (*((kmp_stub_lock_t *)user_lock) == UNINIT) {\n    // TODO: Issue an error.\n  }\n  if (*((kmp_stub_lock_t *)user_lock) == LOCKED) {\n    return 0;\n  }\n  *((kmp_stub_lock_t *)user_lock) = LOCKED;\n  return 1;\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  return __kmpc_test_lock(NULL, gtid, user_lock);\n#endif\n}\n\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_TEST_NEST_LOCK)(void **user_lock) {\n#ifdef KMP_STUB\n  if (*((kmp_stub_lock_t *)user_lock) == UNINIT) {\n    // TODO: Issue an error.\n  }\n  return ++(*((int *)user_lock));\n#else\n  int gtid = __kmp_entry_gtid();\n#if OMPT_SUPPORT && OMPT_OPTIONAL\n  OMPT_STORE_RETURN_ADDRESS(gtid);\n#endif\n  return __kmpc_test_nest_lock(NULL, gtid, user_lock);\n#endif\n}\n\ndouble FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_WTIME)(void) {\n#ifdef KMP_STUB\n  return __kmps_get_wtime();\n#else\n  double data;\n#if !KMP_OS_LINUX\n  // We don't need library initialization to get the time on Linux* OS. The\n  // routine can be used to measure library initialization time on Linux* OS now\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n#endif\n  __kmp_elapsed(&data);\n  return data;\n#endif\n}\n\ndouble FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_WTICK)(void) {\n#ifdef KMP_STUB\n  return __kmps_get_wtick();\n#else\n  double data;\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  __kmp_elapsed_tick(&data);\n  return data;\n#endif\n}\n\n/* ------------------------------------------------------------------------ */\n\nvoid *FTN_STDCALL FTN_MALLOC(size_t KMP_DEREF size) {\n  // kmpc_malloc initializes the library if needed\n  return kmpc_malloc(KMP_DEREF size);\n}\n\nvoid *FTN_STDCALL FTN_ALIGNED_MALLOC(size_t KMP_DEREF size,\n                                     size_t KMP_DEREF alignment) {\n  // kmpc_aligned_malloc initializes the library if needed\n  return kmpc_aligned_malloc(KMP_DEREF size, KMP_DEREF alignment);\n}\n\nvoid *FTN_STDCALL FTN_CALLOC(size_t KMP_DEREF nelem, size_t KMP_DEREF elsize) {\n  // kmpc_calloc initializes the library if needed\n  return kmpc_calloc(KMP_DEREF nelem, KMP_DEREF elsize);\n}\n\nvoid *FTN_STDCALL FTN_REALLOC(void *KMP_DEREF ptr, size_t KMP_DEREF size) {\n  // kmpc_realloc initializes the library if needed\n  return kmpc_realloc(KMP_DEREF ptr, KMP_DEREF size);\n}\n\nvoid FTN_STDCALL FTN_KFREE(void *KMP_DEREF ptr) {\n  // does nothing if the library is not initialized\n  kmpc_free(KMP_DEREF ptr);\n}\n\nvoid FTN_STDCALL FTN_SET_WARNINGS_ON(void) {\n#ifndef KMP_STUB\n  __kmp_generate_warnings = kmp_warnings_explicit;\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_WARNINGS_OFF(void) {\n#ifndef KMP_STUB\n  __kmp_generate_warnings = FALSE;\n#endif\n}\n\nvoid FTN_STDCALL FTN_SET_DEFAULTS(char const *str\n#ifndef PASS_ARGS_BY_VALUE\n                                  ,\n                                  int len\n#endif\n                                  ) {\n#ifndef KMP_STUB\n#ifdef PASS_ARGS_BY_VALUE\n  int len = (int)KMP_STRLEN(str);\n#endif\n  __kmp_aux_set_defaults(str, len);\n#endif\n}\n\n/* ------------------------------------------------------------------------ */\n\n/* returns the status of cancellation */\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_CANCELLATION)(void) {\n#ifdef KMP_STUB\n  return 0 /* false */;\n#else\n  // initialize the library if needed\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  return __kmp_omp_cancellation;\n#endif\n}\n\nint FTN_STDCALL FTN_GET_CANCELLATION_STATUS(int cancel_kind) {\n#ifdef KMP_STUB\n  return 0 /* false */;\n#else\n  return __kmp_get_cancellation_status(cancel_kind);\n#endif\n}\n\n/* returns the maximum allowed task priority */\nint FTN_STDCALL KMP_EXPAND_NAME(FTN_GET_MAX_TASK_PRIORITY)(void) {\n#ifdef KMP_STUB\n  return 0;\n#else\n  if (!__kmp_init_serial) {\n    __kmp_serial_initialize();\n  }\n  return __kmp_max_task_priority;\n#endif\n}\n\n// This function will be defined in libomptarget. When libomptarget is not\n// loaded, we assume we are on the host and return KMP_HOST_DEVICE.\n// Compiler/libomptarget will handle this if called inside target.\nint FTN_STDCALL FTN_GET_DEVICE_NUM(void) KMP_WEAK_ATTRIBUTE_EXTERNAL;\nint FTN_STDCALL FTN_GET_DEVICE_NUM(void) { return KMP_HOST_DEVICE; }\n\n// Compiler will ensure that this is only called from host in sequential region\nint FTN_STDCALL FTN_PAUSE_RESOURCE(kmp_pause_status_t kind, int device_num) {\n#ifdef KMP_STUB\n  return 1; // just fail\n#else\n  if (device_num == KMP_HOST_DEVICE)\n    return __kmpc_pause_resource(kind);\n  else {\n#if !KMP_OS_WINDOWS\n    int (*fptr)(kmp_pause_status_t, int);\n    if ((*(void **)(&fptr) = dlsym(RTLD_DEFAULT, \"tgt_pause_resource\")))\n      return (*fptr)(kind, device_num);\n    else\n#endif\n      return 1; // just fail if there is no libomptarget\n  }\n#endif\n}\n\n// Compiler will ensure that this is only called from host in sequential region\nint FTN_STDCALL FTN_PAUSE_RESOURCE_ALL(kmp_pause_status_t kind) {\n#ifdef KMP_STUB\n  return 1; // just fail\n#else\n  int fails = 0;\n#if !KMP_OS_WINDOWS\n  int (*fptr)(kmp_pause_status_t, int);\n  if ((*(void **)(&fptr) = dlsym(RTLD_DEFAULT, \"tgt_pause_resource\")))\n    fails = (*fptr)(kind, KMP_DEVICE_ALL); // pause devices\n#endif\n  fails += __kmpc_pause_resource(kind); // pause host\n  return fails;\n#endif\n}\n\n// Returns the maximum number of nesting levels supported by implementation\nint FTN_STDCALL FTN_GET_SUPPORTED_ACTIVE_LEVELS(void) {\n#ifdef KMP_STUB\n  return 1;\n#else\n  return KMP_MAX_ACTIVE_LEVELS_LIMIT;\n#endif\n}\n\nvoid FTN_STDCALL FTN_FULFILL_EVENT(kmp_event_t *event) {\n#ifndef KMP_STUB\n  __kmp_fulfill_event(event);\n#endif\n}\n\n// display environment variables when requested\nvoid FTN_STDCALL FTN_DISPLAY_ENV(int verbose) {\n#ifndef KMP_STUB\n  __kmp_omp_display_env(verbose);\n#endif\n}\n\n// GCC compatibility (versioned symbols)\n#ifdef KMP_USE_VERSION_SYMBOLS\n\n/* These following sections create versioned symbols for the\n   omp_* routines. The KMP_VERSION_SYMBOL macro expands the API name and\n   then maps it to a versioned symbol.\n   libgomp ``versions'' its symbols (OMP_1.0, OMP_2.0, OMP_3.0, ...) while also\n   retaining the default version which libomp uses: VERSION (defined in\n   exports_so.txt). If you want to see the versioned symbols for libgomp.so.1\n   then just type:\n\n   objdump -T /path/to/libgomp.so.1 | grep omp_\n\n   Example:\n   Step 1) Create __kmp_api_omp_set_num_threads_10_alias which is alias of\n     __kmp_api_omp_set_num_threads\n   Step 2) Set __kmp_api_omp_set_num_threads_10_alias to version:\n     omp_set_num_threads@OMP_1.0\n   Step 2B) Set __kmp_api_omp_set_num_threads to default version:\n     omp_set_num_threads@@VERSION\n*/\n\n// OMP_1.0 versioned symbols\nKMP_VERSION_SYMBOL(FTN_SET_NUM_THREADS, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_GET_NUM_THREADS, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_GET_MAX_THREADS, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_GET_THREAD_NUM, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_GET_NUM_PROCS, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_IN_PARALLEL, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_SET_DYNAMIC, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_GET_DYNAMIC, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_SET_NESTED, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_GET_NESTED, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_INIT_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_INIT_NEST_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_DESTROY_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_DESTROY_NEST_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_SET_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_SET_NEST_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_UNSET_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_UNSET_NEST_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_TEST_LOCK, 10, \"OMP_1.0\");\nKMP_VERSION_SYMBOL(FTN_TEST_NEST_LOCK, 10, \"OMP_1.0\");\n\n// OMP_2.0 versioned symbols\nKMP_VERSION_SYMBOL(FTN_GET_WTICK, 20, \"OMP_2.0\");\nKMP_VERSION_SYMBOL(FTN_GET_WTIME, 20, \"OMP_2.0\");\n\n// OMP_3.0 versioned symbols\nKMP_VERSION_SYMBOL(FTN_SET_SCHEDULE, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_SCHEDULE, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_THREAD_LIMIT, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_SET_MAX_ACTIVE_LEVELS, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_MAX_ACTIVE_LEVELS, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_ANCESTOR_THREAD_NUM, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_LEVEL, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_TEAM_SIZE, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_GET_ACTIVE_LEVEL, 30, \"OMP_3.0\");\n\n// the lock routines have a 1.0 and 3.0 version\nKMP_VERSION_SYMBOL(FTN_INIT_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_INIT_NEST_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_DESTROY_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_DESTROY_NEST_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_SET_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_SET_NEST_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_UNSET_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_UNSET_NEST_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_TEST_LOCK, 30, \"OMP_3.0\");\nKMP_VERSION_SYMBOL(FTN_TEST_NEST_LOCK, 30, \"OMP_3.0\");\n\n// OMP_3.1 versioned symbol\nKMP_VERSION_SYMBOL(FTN_IN_FINAL, 31, \"OMP_3.1\");\n\n// OMP_4.0 versioned symbols\nKMP_VERSION_SYMBOL(FTN_GET_PROC_BIND, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_GET_NUM_TEAMS, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_GET_TEAM_NUM, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_GET_CANCELLATION, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_GET_DEFAULT_DEVICE, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_SET_DEFAULT_DEVICE, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_IS_INITIAL_DEVICE, 40, \"OMP_4.0\");\nKMP_VERSION_SYMBOL(FTN_GET_NUM_DEVICES, 40, \"OMP_4.0\");\n\n// OMP_4.5 versioned symbols\nKMP_VERSION_SYMBOL(FTN_GET_MAX_TASK_PRIORITY, 45, \"OMP_4.5\");\nKMP_VERSION_SYMBOL(FTN_GET_NUM_PLACES, 45, \"OMP_4.5\");\nKMP_VERSION_SYMBOL(FTN_GET_PLACE_NUM_PROCS, 45, \"OMP_4.5\");\nKMP_VERSION_SYMBOL(FTN_GET_PLACE_PROC_IDS, 45, \"OMP_4.5\");\nKMP_VERSION_SYMBOL(FTN_GET_PLACE_NUM, 45, \"OMP_4.5\");\nKMP_VERSION_SYMBOL(FTN_GET_PARTITION_NUM_PLACES, 45, \"OMP_4.5\");\nKMP_VERSION_SYMBOL(FTN_GET_PARTITION_PLACE_NUMS, 45, \"OMP_4.5\");\n// KMP_VERSION_SYMBOL(FTN_GET_INITIAL_DEVICE, 45, \"OMP_4.5\");\n\n// OMP_5.0 versioned symbols\n// KMP_VERSION_SYMBOL(FTN_GET_DEVICE_NUM, 50, \"OMP_5.0\");\n// KMP_VERSION_SYMBOL(FTN_PAUSE_RESOURCE, 50, \"OMP_5.0\");\n// KMP_VERSION_SYMBOL(FTN_PAUSE_RESOURCE_ALL, 50, \"OMP_5.0\");\n// KMP_VERSION_SYMBOL(FTN_GET_SUPPORTED_ACTIVE_LEVELS, 50, \"OMP_5.0\");\n// KMP_VERSION_SYMBOL(FTN_FULFILL_EVENT, 50, \"OMP_5.0\");\n\n#endif // KMP_USE_VERSION_SYMBOLS\n\n#ifdef __cplusplus\n} // extern \"C\"\n#endif // __cplusplus\n\n// end of file //\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/runtime/src/z_Linux_util.cpp": "/*\n * z_Linux_util.cpp -- platform specific routines.\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#include \"kmp.h\"\n#include \"kmp_affinity.h\"\n#include \"kmp_i18n.h\"\n#include \"kmp_io.h\"\n#include \"kmp_itt.h\"\n#include \"kmp_lock.h\"\n#include \"kmp_stats.h\"\n#include \"kmp_str.h\"\n#include \"kmp_wait_release.h\"\n#include \"kmp_wrapper_getpid.h\"\n\n#if KMP_USE_ABT\n#include \"kmp_taskdeps.h\"\n#endif\n\n#if !KMP_OS_DRAGONFLY && !KMP_OS_FREEBSD && !KMP_OS_NETBSD && !KMP_OS_OPENBSD\n#include <alloca.h>\n#endif\n#include <math.h> // HUGE_VAL.\n#include <sys/resource.h>\n#include <sys/syscall.h>\n#include <sys/time.h>\n#include <sys/times.h>\n#include <unistd.h>\n\n#if KMP_OS_LINUX && !KMP_OS_CNK\n#include <sys/sysinfo.h>\n#if KMP_USE_FUTEX\n// We should really include <futex.h>, but that causes compatibility problems on\n// different Linux* OS distributions that either require that you include (or\n// break when you try to include) <pci/types.h>. Since all we need is the two\n// macros below (which are part of the kernel ABI, so can't change) we just\n// define the constants here and don't include <futex.h>\n#ifndef FUTEX_WAIT\n#define FUTEX_WAIT 0\n#endif\n#ifndef FUTEX_WAKE\n#define FUTEX_WAKE 1\n#endif\n#endif\n#elif KMP_OS_DARWIN\n#include <mach/mach.h>\n#include <sys/sysctl.h>\n#elif KMP_OS_DRAGONFLY || KMP_OS_FREEBSD\n#include <sys/types.h>\n#include <sys/sysctl.h>\n#include <sys/user.h>\n#include <pthread_np.h>\n#elif KMP_OS_NETBSD || KMP_OS_OPENBSD\n#include <sys/types.h>\n#include <sys/sysctl.h>\n#endif\n\n#include <ctype.h>\n#include <dirent.h>\n#include <fcntl.h>\n\n#include \"tsan_annotations.h\"\n\nstruct kmp_sys_timer {\n  struct timespec start;\n};\n\n// Convert timespec to nanoseconds.\n#define TS2NS(timespec) (((timespec).tv_sec * 1e9) + (timespec).tv_nsec)\n\nstatic struct kmp_sys_timer __kmp_sys_timer_data;\n\n#if KMP_HANDLE_SIGNALS\ntypedef void (*sig_func_t)(int);\nSTATIC_EFI2_WORKAROUND struct sigaction __kmp_sighldrs[NSIG];\nstatic sigset_t __kmp_sigset;\n#endif\n\nstatic int __kmp_init_runtime = FALSE;\n\nstatic int __kmp_fork_count = 0;\n\n#if !KMP_USE_ABT\nstatic pthread_condattr_t __kmp_suspend_cond_attr;\nstatic pthread_mutexattr_t __kmp_suspend_mutex_attr;\n\nstatic kmp_cond_align_t __kmp_wait_cv;\nstatic kmp_mutex_align_t __kmp_wait_mx;\n#endif\n\nkmp_uint64 __kmp_ticks_per_msec = 1000000;\n\n#ifdef DEBUG_SUSPEND\nstatic void __kmp_print_cond(char *buffer, kmp_cond_align_t *cond) {\n  KMP_SNPRINTF(buffer, 128, \"(cond (lock (%ld, %d)), (descr (%p)))\",\n               cond->c_cond.__c_lock.__status, cond->c_cond.__c_lock.__spinlock,\n               cond->c_cond.__c_waiting);\n}\n#endif\n\n#if KMP_USE_ABT\nstatic inline ABT_pool __kmp_abt_get_pool_thread(int self_rank,\n                                                 int master_place_id, int tid,\n                                                 int num_threads, int level,\n                                                 kmp_proc_bind_t proc_bind,\n                                                 int *p_place_id);\nstatic inline ABT_pool __kmp_abt_get_pool_task();\nstatic int __kmp_abt_sched_init(ABT_sched sched, ABT_sched_config config);\nstatic void __kmp_abt_sched_run(ABT_sched sched);\nstatic int __kmp_abt_sched_free(ABT_sched sched);\nstatic void __kmp_abt_initialize(void);\nstatic void __kmp_abt_finalize(void);\n\n#endif\n\n#if ((KMP_OS_LINUX || KMP_OS_FREEBSD) && KMP_AFFINITY_SUPPORTED)\n\n/* Affinity support */\n\nvoid __kmp_affinity_bind_thread(int which) {\n  KMP_ASSERT2(KMP_AFFINITY_CAPABLE(),\n              \"Illegal set affinity operation when not capable\");\n\n  kmp_affin_mask_t *mask;\n  KMP_CPU_ALLOC_ON_STACK(mask);\n  KMP_CPU_ZERO(mask);\n  KMP_CPU_SET(which, mask);\n  __kmp_set_system_affinity(mask, TRUE);\n  KMP_CPU_FREE_FROM_STACK(mask);\n}\n\n/* Determine if we can access affinity functionality on this version of\n * Linux* OS by checking __NR_sched_{get,set}affinity system calls, and set\n * __kmp_affin_mask_size to the appropriate value (0 means not capable). */\nvoid __kmp_affinity_determine_capable(const char *env_var) {\n// Check and see if the OS supports thread affinity.\n\n#if KMP_OS_LINUX\n#define KMP_CPU_SET_SIZE_LIMIT (1024 * 1024)\n#elif KMP_OS_FREEBSD\n#define KMP_CPU_SET_SIZE_LIMIT (sizeof(cpuset_t))\n#endif\n\n\n#if KMP_OS_LINUX\n  // If Linux* OS:\n  // If the syscall fails or returns a suggestion for the size,\n  // then we don't have to search for an appropriate size.\n  int gCode;\n  int sCode;\n  unsigned char *buf;\n  buf = (unsigned char *)KMP_INTERNAL_MALLOC(KMP_CPU_SET_SIZE_LIMIT);\n  gCode = syscall(__NR_sched_getaffinity, 0, KMP_CPU_SET_SIZE_LIMIT, buf);\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"initial getaffinity call returned %d errno = %d\\n\",\n                gCode, errno));\n\n  // if ((gCode < 0) && (errno == ENOSYS))\n  if (gCode < 0) {\n    // System call not supported\n    if (__kmp_affinity_verbose ||\n        (__kmp_affinity_warnings && (__kmp_affinity_type != affinity_none) &&\n         (__kmp_affinity_type != affinity_default) &&\n         (__kmp_affinity_type != affinity_disabled))) {\n      int error = errno;\n      kmp_msg_t err_code = KMP_ERR(error);\n      __kmp_msg(kmp_ms_warning, KMP_MSG(GetAffSysCallNotSupported, env_var),\n                err_code, __kmp_msg_null);\n      if (__kmp_generate_warnings == kmp_warnings_off) {\n        __kmp_str_free(&err_code.str);\n      }\n    }\n    KMP_AFFINITY_DISABLE();\n    KMP_INTERNAL_FREE(buf);\n    return;\n  }\n  if (gCode > 0) { // Linux* OS only\n    // The optimal situation: the OS returns the size of the buffer it expects.\n    //\n    // A verification of correct behavior is that setaffinity on a NULL\n    // buffer with the same size fails with errno set to EFAULT.\n    sCode = syscall(__NR_sched_setaffinity, 0, gCode, NULL);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"setaffinity for mask size %d returned %d errno = %d\\n\",\n                  gCode, sCode, errno));\n    if (sCode < 0) {\n      if (errno == ENOSYS) {\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(SetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n      }\n      if (errno == EFAULT) {\n        KMP_AFFINITY_ENABLE(gCode);\n        KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                      \"affinity supported (mask size %d)\\n\",\n                      (int)__kmp_affin_mask_size));\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n    }\n  }\n\n  // Call the getaffinity system call repeatedly with increasing set sizes\n  // until we succeed, or reach an upper bound on the search.\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"searching for proper set size\\n\"));\n  int size;\n  for (size = 1; size <= KMP_CPU_SET_SIZE_LIMIT; size *= 2) {\n    gCode = syscall(__NR_sched_getaffinity, 0, size, buf);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"getaffinity for mask size %d returned %d errno = %d\\n\",\n                  size, gCode, errno));\n\n    if (gCode < 0) {\n      if (errno == ENOSYS) {\n        // We shouldn't get here\n        KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                      \"inconsistent OS call behavior: errno == ENOSYS for mask \"\n                      \"size %d\\n\",\n                      size));\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(GetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n      continue;\n    }\n\n    sCode = syscall(__NR_sched_setaffinity, 0, gCode, NULL);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"setaffinity for mask size %d returned %d errno = %d\\n\",\n                  gCode, sCode, errno));\n    if (sCode < 0) {\n      if (errno == ENOSYS) { // Linux* OS only\n        // We shouldn't get here\n        KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                      \"inconsistent OS call behavior: errno == ENOSYS for mask \"\n                      \"size %d\\n\",\n                      size));\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(SetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n      if (errno == EFAULT) {\n        KMP_AFFINITY_ENABLE(gCode);\n        KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                      \"affinity supported (mask size %d)\\n\",\n                      (int)__kmp_affin_mask_size));\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n    }\n  }\n#elif KMP_OS_FREEBSD\n  int gCode;\n  unsigned char *buf;\n  buf = (unsigned char *)KMP_INTERNAL_MALLOC(KMP_CPU_SET_SIZE_LIMIT);\n  gCode = pthread_getaffinity_np(pthread_self(), KMP_CPU_SET_SIZE_LIMIT, reinterpret_cast<cpuset_t *>(buf));\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"initial getaffinity call returned %d errno = %d\\n\",\n                gCode, errno));\n  if (gCode == 0) {\n    KMP_AFFINITY_ENABLE(KMP_CPU_SET_SIZE_LIMIT);\n    KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                  \"affinity supported (mask size %d)\\n\",\n\t\t  (int)__kmp_affin_mask_size));\n    KMP_INTERNAL_FREE(buf);\n    return;\n  }\n#endif\n  // save uncaught error code\n  // int error = errno;\n  KMP_INTERNAL_FREE(buf);\n  // restore uncaught error code, will be printed at the next KMP_WARNING below\n  // errno = error;\n\n  // Affinity is not supported\n  KMP_AFFINITY_DISABLE();\n  KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                \"cannot determine mask size - affinity not supported\\n\"));\n  if (__kmp_affinity_verbose ||\n      (__kmp_affinity_warnings && (__kmp_affinity_type != affinity_none) &&\n       (__kmp_affinity_type != affinity_default) &&\n       (__kmp_affinity_type != affinity_disabled))) {\n    KMP_WARNING(AffCantGetMaskSize, env_var);\n  }\n}\n\n#endif // KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED\n\n#if KMP_USE_FUTEX\n\nint __kmp_futex_determine_capable() {\n#if KMP_USE_ABT\n  return 0; // Not supported.\n#else\n  int loc = 0;\n  int rc = syscall(__NR_futex, &loc, FUTEX_WAKE, 1, NULL, NULL, 0);\n  int retval = (rc == 0) || (errno != ENOSYS);\n\n  KA_TRACE(10,\n           (\"__kmp_futex_determine_capable: rc = %d errno = %d\\n\", rc, errno));\n  KA_TRACE(10, (\"__kmp_futex_determine_capable: futex syscall%s supported\\n\",\n                retval ? \"\" : \" not\"));\n\n  return retval;\n#endif\n}\n\n#endif // KMP_USE_FUTEX\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (!KMP_ASM_INTRINS)\n/* Only 32-bit \"add-exchange\" instruction on IA-32 architecture causes us to\n   use compare_and_store for these routines */\n\nkmp_int8 __kmp_test_then_or8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value | d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_int8 __kmp_test_then_and8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value & d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\nkmp_uint32 __kmp_test_then_or32(volatile kmp_uint32 *p, kmp_uint32 d) {\n  kmp_uint32 old_value, new_value;\n\n  old_value = TCR_4(*p);\n  new_value = old_value | d;\n\n  while (!KMP_COMPARE_AND_STORE_REL32(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_4(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_uint32 __kmp_test_then_and32(volatile kmp_uint32 *p, kmp_uint32 d) {\n  kmp_uint32 old_value, new_value;\n\n  old_value = TCR_4(*p);\n  new_value = old_value & d;\n\n  while (!KMP_COMPARE_AND_STORE_REL32(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_4(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\n#if KMP_ARCH_X86\nkmp_int8 __kmp_test_then_add8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value + d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value + d;\n  }\n  return old_value;\n}\n\nkmp_int64 __kmp_test_then_add64(volatile kmp_int64 *p, kmp_int64 d) {\n  kmp_int64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value + d;\n\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value + d;\n  }\n  return old_value;\n}\n#endif /* KMP_ARCH_X86 */\n\nkmp_uint64 __kmp_test_then_or64(volatile kmp_uint64 *p, kmp_uint64 d) {\n  kmp_uint64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value | d;\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_uint64 __kmp_test_then_and64(volatile kmp_uint64 *p, kmp_uint64 d) {\n  kmp_uint64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value & d;\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\n#endif /* (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (! KMP_ASM_INTRINS) */\n\nvoid __kmp_terminate_thread(int gtid) {\n  int status;\n  kmp_info_t *th = __kmp_threads[gtid];\n\n  if (!th)\n    return;\n\n#ifdef KMP_CANCEL_THREADS\n  KA_TRACE(10, (\"__kmp_terminate_thread: kill (%d)\\n\", gtid));\n#if KMP_USE_ABT\n  status = ABT_thread_cancel(th->th.th_info.ds.ds_thread);\n  if (status != ABT_SUCCESS) {\n    __kmp_fatal(KMP_MSG(CantTerminateWorkerThread), KMP_ERR(status),\n                __kmp_msg_null);\n  }\n#else // KMP_USE_ABT\n  status = pthread_cancel(th->th.th_info.ds.ds_thread);\n  if (status != 0 && status != ESRCH) {\n    __kmp_fatal(KMP_MSG(CantTerminateWorkerThread), KMP_ERR(status),\n                __kmp_msg_null);\n  }\n#endif // !KMP_USE_ABT\n#endif\n  KMP_YIELD(TRUE);\n} //\n\n#if !KMP_USE_ABT\n\n/* Set thread stack info according to values returned by pthread_getattr_np().\n   If values are unreasonable, assume call failed and use incremental stack\n   refinement method instead. Returns TRUE if the stack parameters could be\n   determined exactly, FALSE if incremental refinement is necessary. */\nstatic kmp_int32 __kmp_set_stack_info(int gtid, kmp_info_t *th) {\n  int stack_data;\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_HURD\n  pthread_attr_t attr;\n  int status;\n  size_t size = 0;\n  void *addr = 0;\n\n  /* Always do incremental stack refinement for ubermaster threads since the\n     initial thread stack range can be reduced by sibling thread creation so\n     pthread_attr_getstack may cause thread gtid aliasing */\n  if (!KMP_UBER_GTID(gtid)) {\n\n    /* Fetch the real thread attributes */\n    status = pthread_attr_init(&attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_init\", status);\n#if KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD\n    status = pthread_attr_get_np(pthread_self(), &attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_get_np\", status);\n#else\n    status = pthread_getattr_np(pthread_self(), &attr);\n    KMP_CHECK_SYSFAIL(\"pthread_getattr_np\", status);\n#endif\n    status = pthread_attr_getstack(&attr, &addr, &size);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_getstack\", status);\n    KA_TRACE(60,\n             (\"__kmp_set_stack_info: T#%d pthread_attr_getstack returned size:\"\n              \" %lu, low addr: %p\\n\",\n              gtid, size, addr));\n    status = pthread_attr_destroy(&attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_destroy\", status);\n  }\n\n  if (size != 0 && addr != 0) { // was stack parameter determination successful?\n    /* Store the correct base and size */\n    TCW_PTR(th->th.th_info.ds.ds_stackbase, (((char *)addr) + size));\n    TCW_PTR(th->th.th_info.ds.ds_stacksize, size);\n    TCW_4(th->th.th_info.ds.ds_stackgrow, FALSE);\n    return TRUE;\n  }\n#endif /* KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||\n              KMP_OS_HURD */\n  /* Use incremental refinement starting from initial conservative estimate */\n  TCW_PTR(th->th.th_info.ds.ds_stacksize, 0);\n  TCW_PTR(th->th.th_info.ds.ds_stackbase, &stack_data);\n  TCW_4(th->th.th_info.ds.ds_stackgrow, TRUE);\n  return FALSE;\n}\n\nstatic void *__kmp_launch_worker(void *thr) {\n  int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n  sigset_t new_set, old_set;\n#endif /* KMP_BLOCK_SIGNALS */\n  void *exit_val;\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_OPENBSD || KMP_OS_HURD\n  void *volatile padding = 0;\n#endif\n  int gtid;\n\n  gtid = ((kmp_info_t *)thr)->th.th_info.ds.ds_gtid;\n  __kmp_gtid_set_specific(gtid);\n#ifdef KMP_TDATA_GTID\n  __kmp_gtid = gtid;\n#endif\n#if KMP_STATS_ENABLED\n  // set thread local index to point to thread-specific stats\n  __kmp_stats_thread_ptr = ((kmp_info_t *)thr)->th.th_stats;\n  __kmp_stats_thread_ptr->startLife();\n  KMP_SET_THREAD_STATE(IDLE);\n  KMP_INIT_PARTITIONED_TIMERS(OMP_idle);\n#endif\n\n#if USE_ITT_BUILD\n  __kmp_itt_thread_name(gtid);\n#endif /* USE_ITT_BUILD */\n\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_set_init_mask(gtid, FALSE);\n#endif\n\n#ifdef KMP_CANCEL_THREADS\n  status = pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, &old_type);\n  KMP_CHECK_SYSFAIL(\"pthread_setcanceltype\", status);\n  // josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads?\n  status = pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n  // Set FP control regs to be a copy of the parallel initialization thread's.\n  __kmp_clear_x87_fpu_status_word();\n  __kmp_load_x87_fpu_control_word(&__kmp_init_x87_fpu_control_word);\n  __kmp_load_mxcsr(&__kmp_init_mxcsr);\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = sigfillset(&new_set);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigfillset\", status);\n  status = pthread_sigmask(SIG_BLOCK, &new_set, &old_set);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_OPENBSD\n  if (__kmp_stkoffset > 0 && gtid > 0) {\n    padding = KMP_ALLOCA(gtid * __kmp_stkoffset);\n  }\n#endif\n\n  KMP_MB();\n  __kmp_set_stack_info(gtid, (kmp_info_t *)thr);\n\n  __kmp_check_stack_overlap((kmp_info_t *)thr);\n\n  exit_val = __kmp_launch_thread((kmp_info_t *)thr);\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = pthread_sigmask(SIG_SETMASK, &old_set, NULL);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n  return exit_val;\n}\n\n#else // !KMP_USE_ABT\n\nstatic void __kmp_abt_create_workers_recursive(kmp_team_t *team, int start_tid,\n                                               int end_tid);\nstatic void __kmp_abt_join_workers_recursive(kmp_team_t *team, int start_tid,\n                                             int end_tid);\n\nstatic void __kmp_abt_launch_worker(void *thr) {\n  int gtid;\n  kmp_info_t *this_thr = (kmp_info_t *)thr;\n  kmp_team_t *team = this_thr->th.th_team;\n\n  gtid = this_thr->th.th_info.ds.ds_gtid;\n  KMP_DEBUG_ASSERT(this_thr == __kmp_threads[gtid]);\n\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_set_init_mask(gtid, FALSE);\n#endif\n\n  KMP_MB();\n\n  const int start_tid = __kmp_tid_from_gtid(gtid);\n  const int end_tid = this_thr->th.th_creation_group_end_tid;\n\n  if (end_tid - start_tid > 1)\n    __kmp_abt_create_workers_recursive(team, start_tid, end_tid);\n\n  if (__kmp_tasking_mode != tskm_immediate_exec) {\n    /* It is originally set up in task_team_sync() */\n    this_thr->th.th_task_team = team->t.t_task_team[this_thr->th.th_task_state];\n  }\n  if (team && !TCR_4(__kmp_global.g.g_done)) {\n    /* run our new task */\n    if ((team->t.t_pkfn) != NULL) {\n      int rc;\n      KA_TRACE(20, (\"__kmp_abt_launch_worker: T#%d(%d:%d) \"\n                    \"invoke microtask = %p\\n\",\n                    gtid, team->t.t_id, __kmp_tid_from_gtid(gtid),\n                    team->t.t_pkfn));\n      rc = team->t.t_invoke(gtid);\n      KMP_ASSERT(rc);\n      KMP_MB();\n      KA_TRACE(20, (\"__kmp_abt_launch_worker: T#%d(%d:%d) \"\n                    \"done microtask = %p\\n\",\n                    gtid, team->t.t_id, __kmp_tid_from_gtid(gtid),\n                    team->t.t_pkfn));\n    }\n  }\n\n  KA_TRACE(10, (\"__kmp_abt_launch_worker: T#%d done\\n\", gtid));\n\n  __kmp_abt_wait_child_tasks(this_thr, true, FALSE);\n  this_thr->th.th_task_team = NULL;\n\n  /* Below is for the implicit task */\n  kmp_taskdata_t *td = this_thr->th.th_current_task;\n  if (td->td_task_queue) {\n    KMP_DEBUG_ASSERT(td->td_tq_cur_size == 0);\n    KMP_INTERNAL_FREE(td->td_task_queue);\n    td->td_task_queue = NULL;\n    td->td_tq_max_size = 0;\n  }\n\n  /* This thread has been finished. Any task can use this as a parent. */\n  __kmp_abt_release_info(this_thr);\n\n  if (end_tid - start_tid > 1)\n    __kmp_abt_join_workers_recursive(team, start_tid, end_tid);\n\n  KA_TRACE(10, (\"__kmp_abt_launch_worker: T#%d finish\\n\", gtid));\n}\n\n#endif // KMP_USE_ABT\n\n#if KMP_USE_MONITOR\n/* The monitor thread controls all of the threads in the complex */\n\nstatic void *__kmp_launch_monitor(void *thr) {\n  int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n  sigset_t new_set;\n#endif /* KMP_BLOCK_SIGNALS */\n  struct timespec interval;\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #1 launched\\n\"));\n\n  /* register us as the monitor thread */\n  __kmp_gtid_set_specific(KMP_GTID_MONITOR);\n#ifdef KMP_TDATA_GTID\n  __kmp_gtid = KMP_GTID_MONITOR;\n#endif\n\n  KMP_MB();\n\n#if USE_ITT_BUILD\n  // Instruct Intel(R) Threading Tools to ignore monitor thread.\n  __kmp_itt_thread_ignore();\n#endif /* USE_ITT_BUILD */\n\n  __kmp_set_stack_info(((kmp_info_t *)thr)->th.th_info.ds.ds_gtid,\n                       (kmp_info_t *)thr);\n\n  __kmp_check_stack_overlap((kmp_info_t *)thr);\n\n#ifdef KMP_CANCEL_THREADS\n  status = pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, &old_type);\n  KMP_CHECK_SYSFAIL(\"pthread_setcanceltype\", status);\n  // josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads?\n  status = pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n\n#if KMP_REAL_TIME_FIX\n  // This is a potential fix which allows application with real-time scheduling\n  // policy work. However, decision about the fix is not made yet, so it is\n  // disabled by default.\n  { // Are program started with real-time scheduling policy?\n    int sched = sched_getscheduler(0);\n    if (sched == SCHED_FIFO || sched == SCHED_RR) {\n      // Yes, we are a part of real-time application. Try to increase the\n      // priority of the monitor.\n      struct sched_param param;\n      int max_priority = sched_get_priority_max(sched);\n      int rc;\n      KMP_WARNING(RealTimeSchedNotSupported);\n      sched_getparam(0, &param);\n      if (param.sched_priority < max_priority) {\n        param.sched_priority += 1;\n        rc = sched_setscheduler(0, sched, &param);\n        if (rc != 0) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(CantChangeMonitorPriority),\n                    err_code, KMP_MSG(MonitorWillStarve), __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n      } else {\n        // We cannot abort here, because number of CPUs may be enough for all\n        // the threads, including the monitor thread, so application could\n        // potentially work...\n        __kmp_msg(kmp_ms_warning, KMP_MSG(RunningAtMaxPriority),\n                  KMP_MSG(MonitorWillStarve), KMP_HNT(RunningAtMaxPriority),\n                  __kmp_msg_null);\n      }\n    }\n    // AC: free thread that waits for monitor started\n    TCW_4(__kmp_global.g.g_time.dt.t_value, 0);\n  }\n#endif // KMP_REAL_TIME_FIX\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  if (__kmp_monitor_wakeups == 1) {\n    interval.tv_sec = 1;\n    interval.tv_nsec = 0;\n  } else {\n    interval.tv_sec = 0;\n    interval.tv_nsec = (KMP_NSEC_PER_SEC / __kmp_monitor_wakeups);\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #2 monitor\\n\"));\n\n  while (!TCR_4(__kmp_global.g.g_done)) {\n    struct timespec now;\n    struct timeval tval;\n\n    /*  This thread monitors the state of the system */\n\n    KA_TRACE(15, (\"__kmp_launch_monitor: update\\n\"));\n\n    status = gettimeofday(&tval, NULL);\n    KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n    TIMEVAL_TO_TIMESPEC(&tval, &now);\n\n    now.tv_sec += interval.tv_sec;\n    now.tv_nsec += interval.tv_nsec;\n\n    if (now.tv_nsec >= KMP_NSEC_PER_SEC) {\n      now.tv_sec += 1;\n      now.tv_nsec -= KMP_NSEC_PER_SEC;\n    }\n\n    status = pthread_mutex_lock(&__kmp_wait_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n    // AC: the monitor should not fall asleep if g_done has been set\n    if (!TCR_4(__kmp_global.g.g_done)) { // check once more under mutex\n      status = pthread_cond_timedwait(&__kmp_wait_cv.c_cond,\n                                      &__kmp_wait_mx.m_mutex, &now);\n      if (status != 0) {\n        if (status != ETIMEDOUT && status != EINTR) {\n          KMP_SYSFAIL(\"pthread_cond_timedwait\", status);\n        }\n      }\n    }\n    status = pthread_mutex_unlock(&__kmp_wait_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n\n    TCW_4(__kmp_global.g.g_time.dt.t_value,\n          TCR_4(__kmp_global.g.g_time.dt.t_value) + 1);\n\n    KMP_MB(); /* Flush all pending memory write invalidates.  */\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #3 cleanup\\n\"));\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = sigfillset(&new_set);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigfillset\", status);\n  status = pthread_sigmask(SIG_UNBLOCK, &new_set, NULL);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #4 finished\\n\"));\n\n  if (__kmp_global.g.g_abort != 0) {\n    /* now we need to terminate the worker threads  */\n    /* the value of t_abort is the signal we caught */\n\n    int gtid;\n\n    KA_TRACE(10, (\"__kmp_launch_monitor: #5 terminate sig=%d\\n\",\n                  __kmp_global.g.g_abort));\n\n    /* terminate the OpenMP worker threads */\n    /* TODO this is not valid for sibling threads!!\n     * the uber master might not be 0 anymore.. */\n    for (gtid = 1; gtid < __kmp_threads_capacity; ++gtid)\n      __kmp_terminate_thread(gtid);\n\n    __kmp_cleanup();\n\n    KA_TRACE(10, (\"__kmp_launch_monitor: #6 raise sig=%d\\n\",\n                  __kmp_global.g.g_abort));\n\n    if (__kmp_global.g.g_abort > 0)\n      raise(__kmp_global.g.g_abort);\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #7 exit\\n\"));\n\n  return thr;\n}\n#endif // KMP_USE_MONITOR\n\n#if !KMP_USE_ABT\n\nvoid __kmp_create_worker(int gtid, kmp_info_t *th, size_t stack_size) {\n  pthread_t handle;\n  pthread_attr_t thread_attr;\n  int status;\n\n  th->th.th_info.ds.ds_gtid = gtid;\n\n#if KMP_STATS_ENABLED\n  // sets up worker thread stats\n  __kmp_acquire_tas_lock(&__kmp_stats_lock, gtid);\n\n  // th->th.th_stats is used to transfer thread-specific stats-pointer to\n  // __kmp_launch_worker. So when thread is created (goes into\n  // __kmp_launch_worker) it will set its thread local pointer to\n  // th->th.th_stats\n  if (!KMP_UBER_GTID(gtid)) {\n    th->th.th_stats = __kmp_stats_list->push_back(gtid);\n  } else {\n    // For root threads, __kmp_stats_thread_ptr is set in __kmp_register_root(),\n    // so set the th->th.th_stats field to it.\n    th->th.th_stats = __kmp_stats_thread_ptr;\n  }\n  __kmp_release_tas_lock(&__kmp_stats_lock, gtid);\n\n#endif // KMP_STATS_ENABLED\n\n  if (KMP_UBER_GTID(gtid)) {\n    KA_TRACE(10, (\"__kmp_create_worker: uber thread (%d)\\n\", gtid));\n    th->th.th_info.ds.ds_thread = pthread_self();\n    __kmp_set_stack_info(gtid, th);\n    __kmp_check_stack_overlap(th);\n    return;\n  }\n\n  KA_TRACE(10, (\"__kmp_create_worker: try to create thread (%d)\\n\", gtid));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n#ifdef KMP_THREAD_ATTR\n\n  status = pthread_attr_init(&thread_attr);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantInitThreadAttrs), KMP_ERR(status), __kmp_msg_null);\n  }\n  status = pthread_attr_setdetachstate(&thread_attr, PTHREAD_CREATE_JOINABLE);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetWorkerState), KMP_ERR(status), __kmp_msg_null);\n  }\n\n  /* Set stack size for this thread now.\n     The multiple of 2 is there because on some machines, requesting an unusual\n     stacksize causes the thread to have an offset before the dummy alloca()\n     takes place to create the offset.  Since we want the user to have a\n     sufficient stacksize AND support a stack offset, we alloca() twice the\n     offset so that the upcoming alloca() does not eliminate any premade offset,\n     and also gives the user the stack space they requested for all threads */\n  stack_size += gtid * __kmp_stkoffset * 2;\n\n#if defined(__ANDROID__) && __ANDROID_API__ < 19\n    // Round the stack size to a multiple of the page size. Older versions of\n    // Android (until KitKat) would fail pthread_attr_setstacksize with EINVAL\n    // if the stack size was not a multiple of the page size.\n    stack_size = (stack_size + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);\n#endif\n\n  KA_TRACE(10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                \"__kmp_stksize = %lu bytes, final stacksize = %lu bytes\\n\",\n                gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size));\n\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  status = pthread_attr_setstacksize(&thread_attr, stack_size);\n#ifdef KMP_BACKUP_STKSIZE\n  if (status != 0) {\n    if (!__kmp_env_stksize) {\n      stack_size = KMP_BACKUP_STKSIZE + gtid * __kmp_stkoffset;\n      __kmp_stksize = KMP_BACKUP_STKSIZE;\n      KA_TRACE(10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                    \"__kmp_stksize = %lu bytes, (backup) final stacksize = %lu \"\n                    \"bytes\\n\",\n                    gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size));\n      status = pthread_attr_setstacksize(&thread_attr, stack_size);\n    }\n  }\n#endif /* KMP_BACKUP_STKSIZE */\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                KMP_HNT(ChangeWorkerStackSize), __kmp_msg_null);\n  }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n#endif /* KMP_THREAD_ATTR */\n\n  status =\n      pthread_create(&handle, &thread_attr, __kmp_launch_worker, (void *)th);\n  if (status != 0 || !handle) { // ??? Why do we check handle??\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n    if (status == EINVAL) {\n      __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                  KMP_HNT(IncreaseWorkerStackSize), __kmp_msg_null);\n    }\n    if (status == ENOMEM) {\n      __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                  KMP_HNT(DecreaseWorkerStackSize), __kmp_msg_null);\n    }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    if (status == EAGAIN) {\n      __kmp_fatal(KMP_MSG(NoResourcesForWorkerThread), KMP_ERR(status),\n                  KMP_HNT(Decrease_NUM_THREADS), __kmp_msg_null);\n    }\n    KMP_SYSFAIL(\"pthread_create\", status);\n  }\n\n  th->th.th_info.ds.ds_thread = handle;\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_destroy(&thread_attr);\n  if (status) {\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, KMP_MSG(CantDestroyThreadAttrs), err_code,\n              __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif /* KMP_THREAD_ATTR */\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_create_worker: done creating thread (%d)\\n\", gtid));\n\n} // __kmp_create_worker\n\n#else // KMP_USE_ABT\n\nstatic inline void __kmp_abt_create_workers_impl(kmp_team_t *team,\n                                                 const int self_rank,\n                                                 int start_tid, int end_tid) {\n  // tid must be start_tid.\n\n#ifdef KMP_THREAD_ATTR\n  ABT_thread_attr thread_attr = ABT_THREAD_ATTR_NULL;\n#endif\n\n  const kmp_proc_bind_t proc_bind = team->t.t_proc_bind_applied;\n  const int master_place_id = team->t.t_master_place_id;\n  const int team_level = team->t.t_level;\n  const int num_threads = team->t.t_nproc;\n\n  const int num_ways = __kmp_abt_global.fork_num_ways;\n  const int cutoff = __kmp_abt_global.fork_cutoff;\n  const int inc = ((end_tid - start_tid) < cutoff) ? 1\n                  : ((end_tid - start_tid + num_ways - 1) / num_ways);\n  KMP_DEBUG_ASSERT(self_rank != -1);\n  KMP_DEBUG_ASSERT(master_place_id != -1);\n  KMP_DEBUG_ASSERT(inc > 0);\n\n  // create / revive workers.\n  for (int f = start_tid + inc; f < end_tid; f += inc) {\n    kmp_info_t *th = team->t.t_threads[f];\n\n    // set up recursive division policy.\n    int new_creation_group_end_tid = f + inc;\n    if (f + inc > end_tid)\n      new_creation_group_end_tid = end_tid;\n\n#if KMP_BARRIER_ICV_PUSH\n    // If we create a thread, the master thread eagerly pushes it.\n    // If it has been run, the slave thread reads it from its master.\n    __kmp_init_implicit_task(team->t.t_ident, th, team, f, FALSE);\n    copy_icvs(&team->t.t_implicit_task_taskdata[f].td_icvs,\n              &team->t.t_master_icvs);\n#endif\n\n    // [SM] th->th.th_info.ds.ds_gtid is setup in __kmp_allocate_thread\n    KMP_DEBUG_ASSERT(th->th.th_info.ds.ds_gtid == __kmp_gtid_from_tid(f, team));\n    // uber thread is created in __kmp_abt_create_uber().\n    KMP_DEBUG_ASSERT(!KMP_UBER_GTID(__kmp_gtid_from_tid(f, team)));\n\n#if KMP_STATS_ENABLED\n    int gtid = __kmp_gtid_from_tid(f, team);\n\n    // sets up worker thread stats\n    __kmp_acquire_tas_lock(&__kmp_stats_lock, gtid);\n\n    // th->th.th_stats is used to transfer thread-specific stats-pointer to\n    // __kmp_launch_worker. So when thread is created (goes into\n    // __kmp_launch_worker) it will set its thread local pointer to\n    // th->th.th_stats\n    if (!KMP_UBER_GTID(gtid)) {\n      th->th.th_stats = __kmp_stats_list->push_back(gtid);\n    } else {\n      // For root threads, __kmp_stats_thread_ptr is set in\n      // __kmp_register_root(), so set the th->th.th_stats field to it.\n      th->th.th_stats = __kmp_stats_thread_ptr;\n    }\n    __kmp_release_tas_lock(&__kmp_stats_lock, gtid);\n#endif // KMP_STATS_ENABLED\n\n    ABT_pool target;\n    int place_id = 0;\n    target = __kmp_abt_get_pool_thread(self_rank, master_place_id, f,\n                                       num_threads, team_level, proc_bind,\n                                       &place_id);\n    th->th.th_current_place_id = place_id;\n    th->th.th_creation_group_end_tid = new_creation_group_end_tid;\n\n    if (th->th.th_info.ds.ds_thread == ABT_THREAD_NULL) {\n      int status;\n      // Create threads.\n#ifdef KMP_THREAD_ATTR\n      if (thread_attr == ABT_THREAD_ATTR_NULL) {\n        status = ABT_thread_attr_create(&thread_attr);\n        KMP_ASSERT(status == ABT_SUCCESS);\n        status = ABT_thread_attr_set_stacksize(thread_attr, __kmp_stksize);\n        KMP_ASSERT(status == ABT_SUCCESS);\n      }\n#endif\n      status = ABT_thread_create(target, __kmp_abt_launch_worker, (void *)th,\n                                 thread_attr, &th->th.th_info.ds.ds_thread);\n      KMP_ASSERT(status == ABT_SUCCESS);\n    } else {\n      // Revive thread.\n      int status = ABT_thread_revive(target, __kmp_abt_launch_worker,\n                                     (void *)th, &th->th.th_info.ds.ds_thread);\n      KMP_ASSERT(status == ABT_SUCCESS);\n    }\n  }\n\n#ifdef KMP_THREAD_ATTR\n  if (thread_attr != ABT_THREAD_ATTR_NULL) {\n      int status = ABT_thread_attr_free(&thread_attr);\n      KMP_ASSERT(status == ABT_SUCCESS);\n  }\n#endif /* KMP_THREAD_ATTR */\n\n  if (inc != 1) {\n    // Create threads in a sub group.\n    int rec_start_tid = start_tid;\n    int rec_end_tid = start_tid + inc;\n    if (rec_end_tid > end_tid)\n      rec_end_tid = end_tid;\n    __kmp_abt_create_workers_impl(team, self_rank, rec_start_tid, rec_end_tid);\n  }\n}\n\nstatic void __kmp_abt_create_workers_recursive(kmp_team_t *team, int start_tid,\n                                               int end_tid) {\n  int self_rank;\n  ABT_xstream_self_rank(&self_rank);\n  __kmp_abt_create_workers_impl(team, self_rank, start_tid, end_tid);\n}\n\nvoid __kmp_abt_create_workers(kmp_team_t *team) {\n  const int team_level = team->t.t_level;\n  const int num_threads = team->t.t_nproc;\n#if KMP_BARRIER_ICV_PUSH\n  // set up the master icvs.\n  copy_icvs(&team->t.t_master_icvs,\n            &team->t.t_implicit_task_taskdata[0].td_icvs);\n#endif\n\n  // Get self_rank\n  int self_rank;\n  ABT_xstream_self_rank(&self_rank);\n\n  // Set up proc bind.\n  kmp_proc_bind_t proc_bind = proc_bind_false;\n  // Set up the affinity of the master thread.\n  kmp_proc_bind_t team_proc_bind = team->t.t_proc_bind;\n  if (team_proc_bind == proc_bind_default) {\n    // Use global setting.\n    int size = __kmp_nested_proc_bind.size;\n    if (size > (team_level - 1))\n      proc_bind = __kmp_nested_proc_bind.bind_types[team_level - 1];\n  } else if (team_proc_bind != proc_bind_intel) {\n    proc_bind = team_proc_bind;\n  }\n  team->t.t_proc_bind_applied = proc_bind;\n\n  // Obtain master place id.\n  int master_tid = team->t.t_master_tid;\n  int master_place_id;\n  if (team_level <= 1) {\n    master_place_id = 0; // master place is set to 0.\n  } else {\n    kmp_team_t *parent_team = team->t.t_parent;\n    master_place_id\n        = parent_team->t.t_threads[master_tid]->th.th_current_place_id;\n    if (master_place_id == -1) {\n      // master thread is not bound to any place.\n      // Use the current place.\n      master_place_id = __kmp_abt_global.locals[self_rank].place_id;\n    }\n  }\n  team->t.t_master_place_id = master_place_id;\n\n  int place_id;\n  __kmp_abt_get_pool_thread(self_rank, master_place_id, master_tid, num_threads,\n                            team_level, proc_bind, &place_id);\n  team->t.t_threads[0]->th.th_current_place_id = place_id;\n\n  // core.\n  __kmp_abt_create_workers_impl(team, self_rank, 0, num_threads);\n} // __kmp_abt_create_workers\n\nstatic inline void __kmp_abt_join_workers_impl(kmp_team_t *team, int start_tid,\n                                               int end_tid) {\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  const int num_ways = __kmp_abt_global.fork_num_ways;\n  const int cutoff = __kmp_abt_global.fork_cutoff;\n  const int inc = ((end_tid - start_tid) < cutoff) ? 1\n                  : ((end_tid - start_tid + num_ways - 1) / num_ways);\n\n  if (inc != 1) {\n    // Join threads in a sub group first.\n    int rec_start_tid = start_tid;\n    int rec_end_tid = start_tid + inc;\n    if (rec_end_tid > end_tid)\n      rec_end_tid = end_tid;\n    __kmp_abt_join_workers_recursive(team, rec_start_tid, rec_end_tid);\n  }\n\n  kmp_info_t **threads = team->t.t_threads;\n\n  /* Join Argobots ULTs here */\n  for (int f = start_tid + inc; f < end_tid; f += inc) {\n    // t_threads[0] is not joined.\n    ABT_thread ds_thread = threads[f]->th.th_info.ds.ds_thread;\n    int status = ABT_thread_join(ds_thread);\n    KMP_DEBUG_ASSERT(status == ABT_SUCCESS);\n    (void)status;\n  }\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n} // __kmp_abt_join_workers_impl\n\nstatic void __kmp_abt_join_workers_recursive(kmp_team_t *team, int start_tid,\n                                             int end_tid) {\n  __kmp_abt_join_workers_impl(team, start_tid, end_tid);\n}\n\nvoid __kmp_abt_join_workers(kmp_team_t *team) {\n  const int num_threads = team->t.t_nproc;\n  __kmp_abt_join_workers_impl(team, 0, num_threads);\n  for (int tid = 0; tid < num_threads; tid++) {\n    kmp_info_t *th = team->t.t_threads[tid];\n    // Reset th_current_task; th_current_task must be consistent when the team\n    // is reused in the future. BOLT cannot run tasks on top of implicit tasks,\n    // so such an inconsistency problem occurs.\n    th->th.th_current_task = &team->t.t_implicit_task_taskdata[tid];\n    // Reset threads so that tasks cannot use these threads.\n    KMP_DEBUG_ASSERT(th->th.th_active == FALSE);\n    if (tid != 0) {\n      th->th.th_active = TRUE;\n    }\n  }\n} // __kmp_abt_join_workers\n\n#endif /* KMP_USE_ABT */\n\n#if KMP_USE_MONITOR\nvoid __kmp_create_monitor(kmp_info_t *th) {\n#if !KMP_USE_ABT\n  pthread_t handle;\n  pthread_attr_t thread_attr;\n  size_t size;\n  int status;\n  int auto_adj_size = FALSE;\n\n  if (__kmp_dflt_blocktime == KMP_MAX_BLOCKTIME) {\n    // We don't need monitor thread in case of MAX_BLOCKTIME\n    KA_TRACE(10, (\"__kmp_create_monitor: skipping monitor thread because of \"\n                  \"MAX blocktime\\n\"));\n    th->th.th_info.ds.ds_tid = 0; // this makes reap_monitor no-op\n    th->th.th_info.ds.ds_gtid = 0;\n    return;\n  }\n  KA_TRACE(10, (\"__kmp_create_monitor: try to create monitor\\n\"));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  th->th.th_info.ds.ds_tid = KMP_GTID_MONITOR;\n  th->th.th_info.ds.ds_gtid = KMP_GTID_MONITOR;\n#if KMP_REAL_TIME_FIX\n  TCW_4(__kmp_global.g.g_time.dt.t_value,\n        -1); // Will use it for synchronization a bit later.\n#else\n  TCW_4(__kmp_global.g.g_time.dt.t_value, 0);\n#endif // KMP_REAL_TIME_FIX\n\n#ifdef KMP_THREAD_ATTR\n  if (__kmp_monitor_stksize == 0) {\n    __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n    auto_adj_size = TRUE;\n  }\n  status = pthread_attr_init(&thread_attr);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantInitThreadAttrs), KMP_ERR(status), __kmp_msg_null);\n  }\n  status = pthread_attr_setdetachstate(&thread_attr, PTHREAD_CREATE_JOINABLE);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetMonitorState), KMP_ERR(status), __kmp_msg_null);\n  }\n\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  status = pthread_attr_getstacksize(&thread_attr, &size);\n  KMP_CHECK_SYSFAIL(\"pthread_attr_getstacksize\", status);\n#else\n  size = __kmp_sys_min_stksize;\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n#endif /* KMP_THREAD_ATTR */\n\n  if (__kmp_monitor_stksize == 0) {\n    __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n  }\n  if (__kmp_monitor_stksize < __kmp_sys_min_stksize) {\n    __kmp_monitor_stksize = __kmp_sys_min_stksize;\n  }\n\n  KA_TRACE(10, (\"__kmp_create_monitor: default stacksize = %lu bytes,\"\n                \"requested stacksize = %lu bytes\\n\",\n                size, __kmp_monitor_stksize));\n\nretry:\n\n/* Set stack size for this thread now. */\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  KA_TRACE(10, (\"__kmp_create_monitor: setting stacksize = %lu bytes,\",\n                __kmp_monitor_stksize));\n  status = pthread_attr_setstacksize(&thread_attr, __kmp_monitor_stksize);\n  if (status != 0) {\n    if (auto_adj_size) {\n      __kmp_monitor_stksize *= 2;\n      goto retry;\n    }\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, // should this be fatal?  BB\n              KMP_MSG(CantSetMonitorStackSize, (long int)__kmp_monitor_stksize),\n              err_code, KMP_HNT(ChangeMonitorStackSize), __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n  status =\n      pthread_create(&handle, &thread_attr, __kmp_launch_monitor, (void *)th);\n\n  if (status != 0) {\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n    if (status == EINVAL) {\n      if (auto_adj_size && (__kmp_monitor_stksize < (size_t)0x40000000)) {\n        __kmp_monitor_stksize *= 2;\n        goto retry;\n      }\n      __kmp_fatal(KMP_MSG(CantSetMonitorStackSize, __kmp_monitor_stksize),\n                  KMP_ERR(status), KMP_HNT(IncreaseMonitorStackSize),\n                  __kmp_msg_null);\n    }\n    if (status == ENOMEM) {\n      __kmp_fatal(KMP_MSG(CantSetMonitorStackSize, __kmp_monitor_stksize),\n                  KMP_ERR(status), KMP_HNT(DecreaseMonitorStackSize),\n                  __kmp_msg_null);\n    }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    if (status == EAGAIN) {\n      __kmp_fatal(KMP_MSG(NoResourcesForMonitorThread), KMP_ERR(status),\n                  KMP_HNT(DecreaseNumberOfThreadsInUse), __kmp_msg_null);\n    }\n    KMP_SYSFAIL(\"pthread_create\", status);\n  }\n\n  th->th.th_info.ds.ds_thread = handle;\n\n#if KMP_REAL_TIME_FIX\n  // Wait for the monitor thread is really started and set its *priority*.\n  KMP_DEBUG_ASSERT(sizeof(kmp_uint32) ==\n                   sizeof(__kmp_global.g.g_time.dt.t_value));\n  __kmp_wait_4((kmp_uint32 volatile *)&__kmp_global.g.g_time.dt.t_value, -1,\n               &__kmp_neq_4, NULL);\n#endif // KMP_REAL_TIME_FIX\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_destroy(&thread_attr);\n  if (status != 0) {\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, KMP_MSG(CantDestroyThreadAttrs), err_code,\n              __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_create_monitor: monitor created %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n#else // !KMP_USE_ABT\n\n  return; // Nothing to do\n\n#endif // KMP_USE_ABT\n} // __kmp_create_monitor\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_exit_thread(int exit_status) {\n#if KMP_USE_ABT\n  ABT_thread_exit();\n#else\n  pthread_exit((void *)(intptr_t)exit_status);\n#endif\n} // __kmp_exit_thread\n\n#if KMP_USE_MONITOR\nvoid __kmp_resume_monitor();\n\nvoid __kmp_reap_monitor(kmp_info_t *th) {\n#if !KMP_USE_ABT\n\n  int status;\n  void *exit_val;\n\n  KA_TRACE(10, (\"__kmp_reap_monitor: try to reap monitor thread with handle\"\n                \" %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n  // If monitor has been created, its tid and gtid should be KMP_GTID_MONITOR.\n  // If both tid and gtid are 0, it means the monitor did not ever start.\n  // If both tid and gtid are KMP_GTID_DNE, the monitor has been shut down.\n  KMP_DEBUG_ASSERT(th->th.th_info.ds.ds_tid == th->th.th_info.ds.ds_gtid);\n  if (th->th.th_info.ds.ds_gtid != KMP_GTID_MONITOR) {\n    KA_TRACE(10, (\"__kmp_reap_monitor: monitor did not start, returning\\n\"));\n    return;\n  }\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  /* First, check to see whether the monitor thread exists to wake it up. This\n     is to avoid performance problem when the monitor sleeps during\n     blocktime-size interval */\n\n  status = pthread_kill(th->th.th_info.ds.ds_thread, 0);\n  if (status != ESRCH) {\n    __kmp_resume_monitor(); // Wake up the monitor thread\n  }\n  KA_TRACE(10, (\"__kmp_reap_monitor: try to join with monitor\\n\"));\n  status = pthread_join(th->th.th_info.ds.ds_thread, &exit_val);\n  if (exit_val != th) {\n    __kmp_fatal(KMP_MSG(ReapMonitorError), KMP_ERR(status), __kmp_msg_null);\n  }\n\n  th->th.th_info.ds.ds_tid = KMP_GTID_DNE;\n  th->th.th_info.ds.ds_gtid = KMP_GTID_DNE;\n\n  KA_TRACE(10, (\"__kmp_reap_monitor: done reaping monitor thread with handle\"\n                \" %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n#else // !KMP_USE_ABT\n\n  return; // Nothing to do.\n\n#endif // KMP_USE_ABT\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_reap_worker(kmp_info_t *th) {\n  int status;\n#if !KMP_USE_ABT\n  void *exit_val;\n#endif\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(\n      10, (\"__kmp_reap_worker: try to reap T#%d\\n\", th->th.th_info.ds.ds_gtid));\n\n#if KMP_USE_ABT\n\n  ABT_thread ds_thread = th->th.th_info.ds.ds_thread;\n  if (ds_thread != ABT_THREAD_NULL) {\n    status = ABT_thread_free(&ds_thread);\n    KMP_ASSERT(status == ABT_SUCCESS);\n  }\n\n#else // KMP_USE_ABT\n\n  status = pthread_join(th->th.th_info.ds.ds_thread, &exit_val);\n#ifdef KMP_DEBUG\n  /* Don't expose these to the user until we understand when they trigger */\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(ReapWorkerError), KMP_ERR(status), __kmp_msg_null);\n  }\n  if (exit_val != th) {\n    KA_TRACE(10, (\"__kmp_reap_worker: worker T#%d did not reap properly, \"\n                  \"exit_val = %p\\n\",\n                  th->th.th_info.ds.ds_gtid, exit_val));\n  }\n#endif /* KMP_DEBUG */\n\n  KA_TRACE(10, (\"__kmp_reap_worker: done reaping T#%d\\n\",\n                th->th.th_info.ds.ds_gtid));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n#endif // !KMP_USE_ABT\n}\n\n#if KMP_HANDLE_SIGNALS\n\nstatic void __kmp_null_handler(int signo) {\n  //  Do nothing, for doing SIG_IGN-type actions.\n} // __kmp_null_handler\n\nstatic void __kmp_team_handler(int signo) {\n  if (__kmp_global.g.g_abort == 0) {\n/* Stage 1 signal handler, let's shut down all of the threads */\n#ifdef KMP_DEBUG\n    __kmp_debug_printf(\"__kmp_team_handler: caught signal = %d\\n\", signo);\n#endif\n    switch (signo) {\n    case SIGHUP:\n    case SIGINT:\n    case SIGQUIT:\n    case SIGILL:\n    case SIGABRT:\n    case SIGFPE:\n    case SIGBUS:\n    case SIGSEGV:\n#ifdef SIGSYS\n    case SIGSYS:\n#endif\n    case SIGTERM:\n      if (__kmp_debug_buf) {\n        __kmp_dump_debug_buffer();\n      }\n      KMP_MB(); // Flush all pending memory write invalidates.\n      TCW_4(__kmp_global.g.g_abort, signo);\n      KMP_MB(); // Flush all pending memory write invalidates.\n      TCW_4(__kmp_global.g.g_done, TRUE);\n      KMP_MB(); // Flush all pending memory write invalidates.\n      break;\n    default:\n#ifdef KMP_DEBUG\n      __kmp_debug_printf(\"__kmp_team_handler: unknown signal type\");\n#endif\n      break;\n    }\n  }\n} // __kmp_team_handler\n\nstatic void __kmp_sigaction(int signum, const struct sigaction *act,\n                            struct sigaction *oldact) {\n  int rc = sigaction(signum, act, oldact);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigaction\", rc);\n}\n\nstatic void __kmp_install_one_handler(int sig, sig_func_t handler_func,\n                                      int parallel_init) {\n  KMP_MB(); // Flush all pending memory write invalidates.\n  KB_TRACE(60,\n           (\"__kmp_install_one_handler( %d, ..., %d )\\n\", sig, parallel_init));\n  if (parallel_init) {\n    struct sigaction new_action;\n    struct sigaction old_action;\n    new_action.sa_handler = handler_func;\n    new_action.sa_flags = 0;\n    sigfillset(&new_action.sa_mask);\n    __kmp_sigaction(sig, &new_action, &old_action);\n    if (old_action.sa_handler == __kmp_sighldrs[sig].sa_handler) {\n      sigaddset(&__kmp_sigset, sig);\n    } else {\n      // Restore/keep user's handler if one previously installed.\n      __kmp_sigaction(sig, &old_action, NULL);\n    }\n  } else {\n    // Save initial/system signal handlers to see if user handlers installed.\n    __kmp_sigaction(sig, NULL, &__kmp_sighldrs[sig]);\n  }\n  KMP_MB(); // Flush all pending memory write invalidates.\n} // __kmp_install_one_handler\n\nstatic void __kmp_remove_one_handler(int sig) {\n  KB_TRACE(60, (\"__kmp_remove_one_handler( %d )\\n\", sig));\n  if (sigismember(&__kmp_sigset, sig)) {\n    struct sigaction old;\n    KMP_MB(); // Flush all pending memory write invalidates.\n    __kmp_sigaction(sig, &__kmp_sighldrs[sig], &old);\n    if ((old.sa_handler != __kmp_team_handler) &&\n        (old.sa_handler != __kmp_null_handler)) {\n      // Restore the users signal handler.\n      KB_TRACE(10, (\"__kmp_remove_one_handler: oops, not our handler, \"\n                    \"restoring: sig=%d\\n\",\n                    sig));\n      __kmp_sigaction(sig, &old, NULL);\n    }\n    sigdelset(&__kmp_sigset, sig);\n    KMP_MB(); // Flush all pending memory write invalidates.\n  }\n} // __kmp_remove_one_handler\n\nvoid __kmp_install_signals(int parallel_init) {\n  KB_TRACE(10, (\"__kmp_install_signals( %d )\\n\", parallel_init));\n  if (__kmp_handle_signals || !parallel_init) {\n    // If ! parallel_init, we do not install handlers, just save original\n    // handlers. Let us do it even __handle_signals is 0.\n    sigemptyset(&__kmp_sigset);\n    __kmp_install_one_handler(SIGHUP, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGINT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGQUIT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGILL, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGABRT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGFPE, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGBUS, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGSEGV, __kmp_team_handler, parallel_init);\n#ifdef SIGSYS\n    __kmp_install_one_handler(SIGSYS, __kmp_team_handler, parallel_init);\n#endif // SIGSYS\n    __kmp_install_one_handler(SIGTERM, __kmp_team_handler, parallel_init);\n#ifdef SIGPIPE\n    __kmp_install_one_handler(SIGPIPE, __kmp_team_handler, parallel_init);\n#endif // SIGPIPE\n  }\n} // __kmp_install_signals\n\nvoid __kmp_remove_signals(void) {\n  int sig;\n  KB_TRACE(10, (\"__kmp_remove_signals()\\n\"));\n  for (sig = 1; sig < NSIG; ++sig) {\n    __kmp_remove_one_handler(sig);\n  }\n} // __kmp_remove_signals\n\n#endif // KMP_HANDLE_SIGNALS\n\nvoid __kmp_enable(int new_state) {\n#ifdef KMP_CANCEL_THREADS\n  int status, old_state;\n  status = pthread_setcancelstate(new_state, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n  KMP_DEBUG_ASSERT(old_state == PTHREAD_CANCEL_DISABLE);\n#endif\n}\n\nvoid __kmp_disable(int *old_state) {\n#ifdef KMP_CANCEL_THREADS\n  int status;\n  status = pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n}\n\nstatic void __kmp_atfork_prepare(void) {\n  __kmp_acquire_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_acquire_bootstrap_lock(&__kmp_forkjoin_lock);\n}\n\nstatic void __kmp_atfork_parent(void) {\n  __kmp_release_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_release_bootstrap_lock(&__kmp_forkjoin_lock);\n}\n\n/* Reset the library so execution in the child starts \"all over again\" with\n   clean data structures in initial states.  Don't worry about freeing memory\n   allocated by parent, just abandon it to be safe. */\nstatic void __kmp_atfork_child(void) {\n  __kmp_release_bootstrap_lock(&__kmp_forkjoin_lock);\n  /* TODO make sure this is done right for nested/sibling */\n  // ATT:  Memory leaks are here? TODO: Check it and fix.\n  /* KMP_ASSERT( 0 ); */\n\n  ++__kmp_fork_count;\n\n#if KMP_AFFINITY_SUPPORTED\n#if KMP_OS_LINUX || KMP_OS_FREEBSD\n  // reset the affinity in the child to the initial thread\n  // affinity in the parent\n  kmp_set_thread_affinity_mask_initial();\n#endif\n  // Set default not to bind threads tightly in the child (we\u2019re expecting\n  // over-subscription after the fork and this can improve things for\n  // scripting languages that use OpenMP inside process-parallel code).\n  __kmp_affinity_type = affinity_none;\n  if (__kmp_nested_proc_bind.bind_types != NULL) {\n    __kmp_nested_proc_bind.bind_types[0] = proc_bind_false;\n  }\n#endif // KMP_AFFINITY_SUPPORTED\n\n  __kmp_init_runtime = FALSE;\n#if KMP_USE_MONITOR\n  __kmp_init_monitor = 0;\n#endif\n  __kmp_init_parallel = FALSE;\n  __kmp_init_middle = FALSE;\n  __kmp_init_serial = FALSE;\n  TCW_4(__kmp_init_gtid, FALSE);\n  __kmp_init_common = FALSE;\n\n  TCW_4(__kmp_init_user_locks, FALSE);\n#if !KMP_USE_DYNAMIC_LOCK\n  __kmp_user_lock_table.used = 1;\n  __kmp_user_lock_table.allocated = 0;\n  __kmp_user_lock_table.table = NULL;\n  __kmp_lock_blocks = NULL;\n#endif\n\n  __kmp_all_nth = 0;\n  TCW_4(__kmp_nth, 0);\n\n  __kmp_thread_pool = NULL;\n  __kmp_thread_pool_insert_pt = NULL;\n  __kmp_team_pool = NULL;\n\n  /* Must actually zero all the *cache arguments passed to __kmpc_threadprivate\n     here so threadprivate doesn't use stale data */\n  KA_TRACE(10, (\"__kmp_atfork_child: checking cache address list %p\\n\",\n                __kmp_threadpriv_cache_list));\n\n  while (__kmp_threadpriv_cache_list != NULL) {\n\n    if (*__kmp_threadpriv_cache_list->addr != NULL) {\n      KC_TRACE(50, (\"__kmp_atfork_child: zeroing cache at address %p\\n\",\n                    &(*__kmp_threadpriv_cache_list->addr)));\n\n      *__kmp_threadpriv_cache_list->addr = NULL;\n    }\n    __kmp_threadpriv_cache_list = __kmp_threadpriv_cache_list->next;\n  }\n\n  __kmp_init_runtime = FALSE;\n\n  /* reset statically initialized locks */\n  __kmp_init_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_init_bootstrap_lock(&__kmp_stdio_lock);\n  __kmp_init_bootstrap_lock(&__kmp_console_lock);\n  __kmp_init_bootstrap_lock(&__kmp_task_team_lock);\n\n#if USE_ITT_BUILD\n  __kmp_itt_reset(); // reset ITT's global state\n#endif /* USE_ITT_BUILD */\n\n  /* This is necessary to make sure no stale data is left around */\n  /* AC: customers complain that we use unsafe routines in the atfork\n     handler. Mathworks: dlsym() is unsafe. We call dlsym and dlopen\n     in dynamic_link when check the presence of shared tbbmalloc library.\n     Suggestion is to make the library initialization lazier, similar\n     to what done for __kmpc_begin(). */\n  // TODO: synchronize all static initializations with regular library\n  //       startup; look at kmp_global.cpp and etc.\n  //__kmp_internal_begin ();\n}\n\nvoid __kmp_register_atfork(void) {\n  if (__kmp_need_register_atfork) {\n    int status = pthread_atfork(__kmp_atfork_prepare, __kmp_atfork_parent,\n                                __kmp_atfork_child);\n    KMP_CHECK_SYSFAIL(\"pthread_atfork\", status);\n    __kmp_need_register_atfork = FALSE;\n  }\n}\n\nvoid __kmp_suspend_initialize(void) {\n#if KMP_USE_ABT\n  /* BOLT does not need to initialize them. */\n#else\n  int status;\n  status = pthread_mutexattr_init(&__kmp_suspend_mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutexattr_init\", status);\n  status = pthread_condattr_init(&__kmp_suspend_cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_condattr_init\", status);\n#endif\n}\n\nvoid __kmp_suspend_initialize_thread(kmp_info_t *th) {\n  ANNOTATE_HAPPENS_AFTER(&th->th.th_suspend_init_count);\n#if KMP_USE_ABT\n  /* BOLT does not need to initialize them. */\n#else\n  int old_value = KMP_ATOMIC_LD_RLX(&th->th.th_suspend_init_count);\n  int new_value = __kmp_fork_count + 1;\n  // Return if already initialized\n  if (old_value == new_value)\n    return;\n  // Wait, then return if being initialized\n  if (old_value == -1 ||\n      !__kmp_atomic_compare_store(&th->th.th_suspend_init_count, old_value,\n                                  -1)) {\n    while (KMP_ATOMIC_LD_ACQ(&th->th.th_suspend_init_count) != new_value) {\n      KMP_CPU_PAUSE();\n    }\n  } else {\n    // Claim to be the initializer and do initializations\n    int status;\n    status = pthread_cond_init(&th->th.th_suspend_cv.c_cond,\n                               &__kmp_suspend_cond_attr);\n    KMP_CHECK_SYSFAIL(\"pthread_cond_init\", status);\n    status = pthread_mutex_init(&th->th.th_suspend_mx.m_mutex,\n                                &__kmp_suspend_mutex_attr);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_init\", status);\n    KMP_ATOMIC_ST_REL(&th->th.th_suspend_init_count, new_value);\n    ANNOTATE_HAPPENS_BEFORE(&th->th.th_suspend_init_count);\n  }\n#endif\n}\n\nvoid __kmp_suspend_uninitialize_thread(kmp_info_t *th) {\n#if KMP_USE_ABT\n  /* BOLT does not need to initialize them. */\n#else\n  if (th->th.th_suspend_init_count > __kmp_fork_count) {\n    /* this means we have initialize the suspension pthread objects for this\n       thread in this instance of the process */\n    int status;\n\n    status = pthread_cond_destroy(&th->th.th_suspend_cv.c_cond);\n    if (status != 0 && status != EBUSY) {\n      KMP_SYSFAIL(\"pthread_cond_destroy\", status);\n    }\n    status = pthread_mutex_destroy(&th->th.th_suspend_mx.m_mutex);\n    if (status != 0 && status != EBUSY) {\n      KMP_SYSFAIL(\"pthread_mutex_destroy\", status);\n    }\n    --th->th.th_suspend_init_count;\n    KMP_DEBUG_ASSERT(KMP_ATOMIC_LD_RLX(&th->th.th_suspend_init_count) ==\n                     __kmp_fork_count);\n  }\n#endif\n}\n\n// return true if lock obtained, false otherwise\nint __kmp_try_suspend_mx(kmp_info_t *th) {\n#if KMP_USE_ABT\n  return 1;\n#else\n  return (pthread_mutex_trylock(&th->th.th_suspend_mx.m_mutex) == 0);\n#endif\n}\n\nvoid __kmp_lock_suspend_mx(kmp_info_t *th) {\n#if !KMP_USE_ABT\n  int status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n#endif\n}\n\nvoid __kmp_unlock_suspend_mx(kmp_info_t *th) {\n#if !KMP_USE_ABT\n  int status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n#endif\n}\n\n#if !KMP_USE_ABT\n/* This routine puts the calling thread to sleep after setting the\n   sleep bit for the indicated flag variable to true. */\ntemplate <class C>\nstatic inline void __kmp_suspend_template(int th_gtid, C *flag) {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_suspend);\n  kmp_info_t *th = __kmp_threads[th_gtid];\n  int status;\n  typename C::flag_t old_spin;\n\n  KF_TRACE(30, (\"__kmp_suspend_template: T#%d enter for flag = %p\\n\", th_gtid,\n                flag->get()));\n\n  __kmp_suspend_initialize_thread(th);\n\n  status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n\n  KF_TRACE(10, (\"__kmp_suspend_template: T#%d setting sleep bit for spin(%p)\\n\",\n                th_gtid, flag->get()));\n\n  /* TODO: shouldn't this use release semantics to ensure that\n     __kmp_suspend_initialize_thread gets called first? */\n  old_spin = flag->set_sleeping();\n  if (__kmp_dflt_blocktime == KMP_MAX_BLOCKTIME &&\n      __kmp_pause_status != kmp_soft_paused) {\n    flag->unset_sleeping();\n    status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n    return;\n  }\n  KF_TRACE(5, (\"__kmp_suspend_template: T#%d set sleep bit for spin(%p)==%x,\"\n               \" was %x\\n\",\n               th_gtid, flag->get(), flag->load(), old_spin));\n\n  if (flag->done_check_val(old_spin)) {\n    old_spin = flag->unset_sleeping();\n    KF_TRACE(5, (\"__kmp_suspend_template: T#%d false alarm, reset sleep bit \"\n                 \"for spin(%p)\\n\",\n                 th_gtid, flag->get()));\n  } else {\n    /* Encapsulate in a loop as the documentation states that this may\n       \"with low probability\" return when the condition variable has\n       not been signaled or broadcast */\n    int deactivated = FALSE;\n    TCW_PTR(th->th.th_sleep_loc, (void *)flag);\n\n    while (flag->is_sleeping()) {\n#ifdef DEBUG_SUSPEND\n      char buffer[128];\n      __kmp_suspend_count++;\n      __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n      __kmp_printf(\"__kmp_suspend_template: suspending T#%d: %s\\n\", th_gtid,\n                   buffer);\n#endif\n      // Mark the thread as no longer active (only in the first iteration of the\n      // loop).\n      if (!deactivated) {\n        th->th.th_active = FALSE;\n        if (th->th.th_active_in_pool) {\n          th->th.th_active_in_pool = FALSE;\n          KMP_ATOMIC_DEC(&__kmp_thread_pool_active_nth);\n          KMP_DEBUG_ASSERT(TCR_4(__kmp_thread_pool_active_nth) >= 0);\n        }\n        deactivated = TRUE;\n      }\n\n#if USE_SUSPEND_TIMEOUT\n      struct timespec now;\n      struct timeval tval;\n      int msecs;\n\n      status = gettimeofday(&tval, NULL);\n      KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n      TIMEVAL_TO_TIMESPEC(&tval, &now);\n\n      msecs = (4 * __kmp_dflt_blocktime) + 200;\n      now.tv_sec += msecs / 1000;\n      now.tv_nsec += (msecs % 1000) * 1000;\n\n      KF_TRACE(15, (\"__kmp_suspend_template: T#%d about to perform \"\n                    \"pthread_cond_timedwait\\n\",\n                    th_gtid));\n      status = pthread_cond_timedwait(&th->th.th_suspend_cv.c_cond,\n                                      &th->th.th_suspend_mx.m_mutex, &now);\n#else\n      KF_TRACE(15, (\"__kmp_suspend_template: T#%d about to perform\"\n                    \" pthread_cond_wait\\n\",\n                    th_gtid));\n      status = pthread_cond_wait(&th->th.th_suspend_cv.c_cond,\n                                 &th->th.th_suspend_mx.m_mutex);\n#endif\n\n      if ((status != 0) && (status != EINTR) && (status != ETIMEDOUT)) {\n        KMP_SYSFAIL(\"pthread_cond_wait\", status);\n      }\n#ifdef KMP_DEBUG\n      if (status == ETIMEDOUT) {\n        if (flag->is_sleeping()) {\n          KF_TRACE(100,\n                   (\"__kmp_suspend_template: T#%d timeout wakeup\\n\", th_gtid));\n        } else {\n          KF_TRACE(2, (\"__kmp_suspend_template: T#%d timeout wakeup, sleep bit \"\n                       \"not set!\\n\",\n                       th_gtid));\n        }\n      } else if (flag->is_sleeping()) {\n        KF_TRACE(100,\n                 (\"__kmp_suspend_template: T#%d spurious wakeup\\n\", th_gtid));\n      }\n#endif\n    } // while\n\n    // Mark the thread as active again (if it was previous marked as inactive)\n    if (deactivated) {\n      th->th.th_active = TRUE;\n      if (TCR_4(th->th.th_in_pool)) {\n        KMP_ATOMIC_INC(&__kmp_thread_pool_active_nth);\n        th->th.th_active_in_pool = TRUE;\n      }\n    }\n  }\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n    __kmp_printf(\"__kmp_suspend_template: T#%d has awakened: %s\\n\", th_gtid,\n                 buffer);\n  }\n#endif\n\n  status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_suspend_template: T#%d exit\\n\", th_gtid));\n}\n\nvoid __kmp_suspend_32(int th_gtid, kmp_flag_32 *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_64(int th_gtid, kmp_flag_64 *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_oncore(int th_gtid, kmp_flag_oncore *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\n\n/* This routine signals the thread specified by target_gtid to wake up\n   after setting the sleep bit indicated by the flag argument to FALSE.\n   The target thread must already have called __kmp_suspend_template() */\ntemplate <class C>\nstatic inline void __kmp_resume_template(int target_gtid, C *flag) {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_resume);\n  kmp_info_t *th = __kmp_threads[target_gtid];\n  int status;\n\n#ifdef KMP_DEBUG\n  int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n#endif\n\n  KF_TRACE(30, (\"__kmp_resume_template: T#%d wants to wakeup T#%d enter\\n\",\n                gtid, target_gtid));\n  KMP_DEBUG_ASSERT(gtid != target_gtid);\n\n  __kmp_suspend_initialize_thread(th);\n\n  status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n\n  if (!flag) { // coming from __kmp_null_resume_wrapper\n    flag = (C *)CCAST(void *, th->th.th_sleep_loc);\n  }\n\n  // First, check if the flag is null or its type has changed. If so, someone\n  // else woke it up.\n  if (!flag || flag->get_type() != flag->get_ptr_type()) { // get_ptr_type\n    // simply shows what\n    // flag was cast to\n    KF_TRACE(5, (\"__kmp_resume_template: T#%d exiting, thread T#%d already \"\n                 \"awake: flag(%p)\\n\",\n                 gtid, target_gtid, NULL));\n    status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n    return;\n  } else { // if multiple threads are sleeping, flag should be internally\n    // referring to a specific thread here\n    typename C::flag_t old_spin = flag->unset_sleeping();\n    if (!flag->is_sleeping_val(old_spin)) {\n      KF_TRACE(5, (\"__kmp_resume_template: T#%d exiting, thread T#%d already \"\n                   \"awake: flag(%p): \"\n                   \"%u => %u\\n\",\n                   gtid, target_gtid, flag->get(), old_spin, flag->load()));\n      status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n      KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n      return;\n    }\n    KF_TRACE(5, (\"__kmp_resume_template: T#%d about to wakeup T#%d, reset \"\n                 \"sleep bit for flag's loc(%p): \"\n                 \"%u => %u\\n\",\n                 gtid, target_gtid, flag->get(), old_spin, flag->load()));\n  }\n  TCW_PTR(th->th.th_sleep_loc, NULL);\n\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n    __kmp_printf(\"__kmp_resume_template: T#%d resuming T#%d: %s\\n\", gtid,\n                 target_gtid, buffer);\n  }\n#endif\n  status = pthread_cond_signal(&th->th.th_suspend_cv.c_cond);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_signal\", status);\n  status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_resume_template: T#%d exiting after signaling wake up\"\n                \" for T#%d\\n\",\n                gtid, target_gtid));\n}\n\nvoid __kmp_resume_32(int target_gtid, kmp_flag_32 *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_64(int target_gtid, kmp_flag_64 *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_oncore(int target_gtid, kmp_flag_oncore *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\n\n#if KMP_USE_MONITOR\nvoid __kmp_resume_monitor() {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_resume);\n  int status;\n#ifdef KMP_DEBUG\n  int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n  KF_TRACE(30, (\"__kmp_resume_monitor: T#%d wants to wakeup T#%d enter\\n\", gtid,\n                KMP_GTID_MONITOR));\n  KMP_DEBUG_ASSERT(gtid != KMP_GTID_MONITOR);\n#endif\n  status = pthread_mutex_lock(&__kmp_wait_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &__kmp_wait_cv.c_cond);\n    __kmp_printf(\"__kmp_resume_monitor: T#%d resuming T#%d: %s\\n\", gtid,\n                 KMP_GTID_MONITOR, buffer);\n  }\n#endif\n  status = pthread_cond_signal(&__kmp_wait_cv.c_cond);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_signal\", status);\n  status = pthread_mutex_unlock(&__kmp_wait_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_resume_monitor: T#%d exiting after signaling wake up\"\n                \" for T#%d\\n\",\n                gtid, KMP_GTID_MONITOR));\n}\n#endif // KMP_USE_MONITOR\n\n#endif // !KMP_USE_ABT\n\nvoid __kmp_yield() {\n#if KMP_USE_ABT\n  ABT_thread_yield();\n#else\n  sched_yield();\n#endif\n}\n\nvoid __kmp_gtid_set_specific(int gtid) {\n#if KMP_USE_ABT\n  ABT_thread self;\n  kmp_info_t *th;\n  KMP_ASSERT(__kmp_init_runtime);\n  ABT_thread_self(&self);\n\n  if (self != ABT_THREAD_NULL) {\n    ABT_thread_get_arg(self, (void **)&th);\n    KMP_ASSERT(th != NULL);\n    th->th.th_info.ds.ds_gtid = gtid;\n    KMP_ASSERT(__kmp_init_gtid);\n    return;\n  }\n#endif // KMP_USE_ABT\n  if (__kmp_init_gtid) {\n    int status;\n    status = pthread_setspecific(__kmp_gtid_threadprivate_key,\n                                 (void *)(intptr_t)(gtid + 1));\n    KMP_CHECK_SYSFAIL(\"pthread_setspecific\", status);\n  } else {\n    KA_TRACE(50, (\"__kmp_gtid_set_specific: runtime shutdown, returning\\n\"));\n  }\n}\n\nint __kmp_gtid_get_specific() {\n  int gtid;\n#if KMP_USE_ABT\n\n  ABT_thread self;\n  ABT_thread_self(&self);\n  if (self == ABT_THREAD_NULL) {\n    KMP_ASSERT(__kmp_init_gtid);\n    /* External threads might call OpenMP functions. */\n    gtid = (int)(size_t)pthread_getspecific(__kmp_gtid_threadprivate_key);\n    KA_TRACE(50, (\"__kmp_gtid_get_specific: key:%d gtid:%d\\n\",\n                  __kmp_gtid_threadprivate_key, gtid));\n  } else {\n    kmp_info_t *th;\n    ABT_thread_get_arg(self, (void **)&th);\n    if (th == NULL) {\n      gtid = KMP_GTID_DNE;\n    } else {\n      gtid = th->th.th_info.ds.ds_gtid;\n    }\n    KA_TRACE(50, (\"__kmp_gtid_get_specific: ULT:%p gtid:%d\\n\", self, gtid));\n  }\n\n#else // KMP_USE_ABT\n\n  if (!__kmp_init_gtid) {\n    KA_TRACE(50, (\"__kmp_gtid_get_specific: runtime shutdown, returning \"\n                  \"KMP_GTID_SHUTDOWN\\n\"));\n    return KMP_GTID_SHUTDOWN;\n  }\n  gtid = (int)(size_t)pthread_getspecific(__kmp_gtid_threadprivate_key);\n  if (gtid == 0) {\n    gtid = KMP_GTID_DNE;\n  } else {\n    gtid--;\n  }\n  KA_TRACE(50, (\"__kmp_gtid_get_specific: key:%d gtid:%d\\n\",\n                __kmp_gtid_threadprivate_key, gtid));\n\n#endif // !KMP_USE_ABT\n  return gtid;\n}\n\ndouble __kmp_read_cpu_time(void) {\n  /*clock_t   t;*/\n  struct tms buffer;\n\n  /*t =*/times(&buffer);\n\n  return (buffer.tms_utime + buffer.tms_cutime) / (double)CLOCKS_PER_SEC;\n}\n\nint __kmp_read_system_info(struct kmp_sys_info *info) {\n  int status;\n  struct rusage r_usage;\n\n  memset(info, 0, sizeof(*info));\n\n  status = getrusage(RUSAGE_SELF, &r_usage);\n  KMP_CHECK_SYSFAIL_ERRNO(\"getrusage\", status);\n\n  // The maximum resident set size utilized (in kilobytes)\n  info->maxrss = r_usage.ru_maxrss;\n  // The number of page faults serviced without any I/O\n  info->minflt = r_usage.ru_minflt;\n  // The number of page faults serviced that required I/O\n  info->majflt = r_usage.ru_majflt;\n  // The number of times a process was \"swapped\" out of memory\n  info->nswap = r_usage.ru_nswap;\n  // The number of times the file system had to perform input\n  info->inblock = r_usage.ru_inblock;\n  // The number of times the file system had to perform output\n  info->oublock = r_usage.ru_oublock;\n  // The number of times a context switch was voluntarily\n  info->nvcsw = r_usage.ru_nvcsw;\n  // The number of times a context switch was forced\n  info->nivcsw = r_usage.ru_nivcsw;\n\n  return (status != 0);\n}\n\nvoid __kmp_read_system_time(double *delta) {\n  double t_ns;\n  struct timeval tval;\n  struct timespec stop;\n  int status;\n\n  status = gettimeofday(&tval, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  TIMEVAL_TO_TIMESPEC(&tval, &stop);\n  t_ns = TS2NS(stop) - TS2NS(__kmp_sys_timer_data.start);\n  *delta = (t_ns * 1e-9);\n}\n\nvoid __kmp_clear_system_time(void) {\n  struct timeval tval;\n  int status;\n  status = gettimeofday(&tval, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  TIMEVAL_TO_TIMESPEC(&tval, &__kmp_sys_timer_data.start);\n}\n\nstatic int __kmp_get_xproc(void) {\n\n  int r = 0;\n\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_OPENBSD || KMP_OS_HURD\n\n  r = sysconf(_SC_NPROCESSORS_ONLN);\n\n#elif KMP_OS_DARWIN\n\n  // Bug C77011 High \"OpenMP Threads and number of active cores\".\n\n  // Find the number of available CPUs.\n  kern_return_t rc;\n  host_basic_info_data_t info;\n  mach_msg_type_number_t num = HOST_BASIC_INFO_COUNT;\n  rc = host_info(mach_host_self(), HOST_BASIC_INFO, (host_info_t)&info, &num);\n  if (rc == 0 && num == HOST_BASIC_INFO_COUNT) {\n    // Cannot use KA_TRACE() here because this code works before trace support\n    // is initialized.\n    r = info.avail_cpus;\n  } else {\n    KMP_WARNING(CantGetNumAvailCPU);\n    KMP_INFORM(AssumedNumCPU);\n  }\n\n#else\n\n#error \"Unknown or unsupported OS.\"\n\n#endif\n\n  return r > 0 ? r : 2; /* guess value of 2 if OS told us 0 */\n\n} // __kmp_get_xproc\n\nint __kmp_read_from_file(char const *path, char const *format, ...) {\n  int result;\n  va_list args;\n\n  va_start(args, format);\n  FILE *f = fopen(path, \"rb\");\n  if (f == NULL)\n    return 0;\n  result = vfscanf(f, format, args);\n  fclose(f);\n\n  return result;\n}\n\nvoid __kmp_runtime_initialize(void) {\n  int status;\n#if !KMP_USE_ABT\n  pthread_mutexattr_t mutex_attr;\n  pthread_condattr_t cond_attr;\n#endif\n\n  if (__kmp_init_runtime) {\n    return;\n  }\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64)\n  if (!__kmp_cpuinfo.initialized) {\n    __kmp_query_cpuid(&__kmp_cpuinfo);\n  }\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n  __kmp_xproc = __kmp_get_xproc();\n\n#if ! KMP_32_BIT_ARCH\n  struct rlimit rlim;\n  // read stack size of calling thread, save it as default for worker threads;\n  // this should be done before reading environment variables\n  status = getrlimit(RLIMIT_STACK, &rlim);\n  if (status == 0) { // success?\n    __kmp_stksize = rlim.rlim_cur;\n    __kmp_check_stksize(&__kmp_stksize); // check value and adjust if needed\n  }\n#endif /* KMP_32_BIT_ARCH */\n\n  if (sysconf(_SC_THREADS)) {\n\n    /* Query the maximum number of threads */\n    __kmp_sys_max_nth = sysconf(_SC_THREAD_THREADS_MAX);\n    if (__kmp_sys_max_nth == -1) {\n      /* Unlimited threads for NPTL */\n      __kmp_sys_max_nth = INT_MAX;\n    } else if (__kmp_sys_max_nth <= 1) {\n      /* Can't tell, just use PTHREAD_THREADS_MAX */\n      __kmp_sys_max_nth = KMP_MAX_NTH;\n    }\n\n    /* Query the minimum stack size */\n    __kmp_sys_min_stksize = sysconf(_SC_THREAD_STACK_MIN);\n    if (__kmp_sys_min_stksize <= 1) {\n      __kmp_sys_min_stksize = KMP_MIN_STKSIZE;\n    }\n  }\n\n  /* Set up minimum number of threads to switch to TLS gtid */\n  __kmp_tls_gtid_min = KMP_TLS_GTID_MIN;\n\n  status = pthread_key_create(&__kmp_gtid_threadprivate_key,\n                              __kmp_internal_end_dest);\n  KMP_CHECK_SYSFAIL(\"pthread_key_create\", status);\n#if KMP_USE_ABT\n  __kmp_abt_initialize();\n#else\n  status = pthread_mutexattr_init(&mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutexattr_init\", status);\n  status = pthread_mutex_init(&__kmp_wait_mx.m_mutex, &mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_init\", status);\n  status = pthread_condattr_init(&cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_condattr_init\", status);\n  status = pthread_cond_init(&__kmp_wait_cv.c_cond, &cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_init\", status);\n#endif\n#if USE_ITT_BUILD\n  __kmp_itt_initialize();\n#endif /* USE_ITT_BUILD */\n\n  __kmp_init_runtime = TRUE;\n}\n\nvoid __kmp_runtime_destroy(void) {\n  int status;\n\n  if (!__kmp_init_runtime) {\n    return; // Nothing to do.\n  }\n\n#if USE_ITT_BUILD\n  __kmp_itt_destroy();\n#endif /* USE_ITT_BUILD */\n\n  status = pthread_key_delete(__kmp_gtid_threadprivate_key);\n  KMP_CHECK_SYSFAIL(\"pthread_key_delete\", status);\n\n#if KMP_USE_ABT\n  __kmp_abt_finalize();\n#else\n  status = pthread_mutex_destroy(&__kmp_wait_mx.m_mutex);\n  if (status != 0 && status != EBUSY) {\n    KMP_SYSFAIL(\"pthread_mutex_destroy\", status);\n  }\n  status = pthread_cond_destroy(&__kmp_wait_cv.c_cond);\n  if (status != 0 && status != EBUSY) {\n    KMP_SYSFAIL(\"pthread_cond_destroy\", status);\n  }\n#endif\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_uninitialize();\n#endif\n\n  __kmp_init_runtime = FALSE;\n}\n\n/* Put the thread to sleep for a time period */\n/* NOTE: not currently used anywhere */\nvoid __kmp_thread_sleep(int millis) { sleep((millis + 500) / 1000); }\n\n/* Calculate the elapsed wall clock time for the user */\nvoid __kmp_elapsed(double *t) {\n  int status;\n#ifdef FIX_SGI_CLOCK\n  struct timespec ts;\n\n  status = clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ts);\n  KMP_CHECK_SYSFAIL_ERRNO(\"clock_gettime\", status);\n  *t =\n      (double)ts.tv_nsec * (1.0 / (double)KMP_NSEC_PER_SEC) + (double)ts.tv_sec;\n#else\n  struct timeval tv;\n\n  status = gettimeofday(&tv, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  *t =\n      (double)tv.tv_usec * (1.0 / (double)KMP_USEC_PER_SEC) + (double)tv.tv_sec;\n#endif\n}\n\n/* Calculate the elapsed wall clock tick for the user */\nvoid __kmp_elapsed_tick(double *t) { *t = 1 / (double)CLOCKS_PER_SEC; }\n\n/* Return the current time stamp in nsec */\nkmp_uint64 __kmp_now_nsec() {\n  struct timeval t;\n  gettimeofday(&t, NULL);\n  kmp_uint64 nsec = (kmp_uint64)KMP_NSEC_PER_SEC * (kmp_uint64)t.tv_sec +\n                    (kmp_uint64)1000 * (kmp_uint64)t.tv_usec;\n  return nsec;\n}\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n/* Measure clock ticks per millisecond */\nvoid __kmp_initialize_system_tick() {\n  kmp_uint64 now, nsec2, diff;\n  kmp_uint64 delay = 100000; // 50~100 usec on most machines.\n  kmp_uint64 nsec = __kmp_now_nsec();\n  kmp_uint64 goal = __kmp_hardware_timestamp() + delay;\n  while ((now = __kmp_hardware_timestamp()) < goal)\n    ;\n  nsec2 = __kmp_now_nsec();\n  diff = nsec2 - nsec;\n  if (diff > 0) {\n    kmp_uint64 tpms = (kmp_uint64)(1e6 * (delay + (now - goal)) / diff);\n    if (tpms > 0)\n      __kmp_ticks_per_msec = tpms;\n  }\n}\n#endif\n\n/* Determine whether the given address is mapped into the current address\n   space. */\n\nint __kmp_is_address_mapped(void *addr) {\n\n  int found = 0;\n  int rc;\n\n#if KMP_OS_LINUX || KMP_OS_HURD\n\n  /* On GNUish OSes, read the /proc/<pid>/maps pseudo-file to get all the address\n     ranges mapped into the address space. */\n\n  char *name = __kmp_str_format(\"/proc/%d/maps\", getpid());\n  FILE *file = NULL;\n\n  file = fopen(name, \"r\");\n  KMP_ASSERT(file != NULL);\n\n  for (;;) {\n\n    void *beginning = NULL;\n    void *ending = NULL;\n    char perms[5];\n\n    rc = fscanf(file, \"%p-%p %4s %*[^\\n]\\n\", &beginning, &ending, perms);\n    if (rc == EOF) {\n      break;\n    }\n    KMP_ASSERT(rc == 3 &&\n               KMP_STRLEN(perms) == 4); // Make sure all fields are read.\n\n    // Ending address is not included in the region, but beginning is.\n    if ((addr >= beginning) && (addr < ending)) {\n      perms[2] = 0; // 3th and 4th character does not matter.\n      if (strcmp(perms, \"rw\") == 0) {\n        // Memory we are looking for should be readable and writable.\n        found = 1;\n      }\n      break;\n    }\n  }\n\n  // Free resources.\n  fclose(file);\n  KMP_INTERNAL_FREE(name);\n#elif KMP_OS_FREEBSD\n  char *buf;\n  size_t lstsz;\n  int mib[] = {CTL_KERN, KERN_PROC, KERN_PROC_VMMAP, getpid()};\n  rc = sysctl(mib, 4, NULL, &lstsz, NULL, 0);\n  if (rc < 0)\n     return 0;\n  // We pass from number of vm entry's semantic\n  // to size of whole entry map list.\n  lstsz = lstsz * 4 / 3;\n  buf = reinterpret_cast<char *>(kmpc_malloc(lstsz));\n  rc = sysctl(mib, 4, buf, &lstsz, NULL, 0);\n  if (rc < 0) {\n     kmpc_free(buf);\n     return 0;\n  }\n\n  char *lw = buf;\n  char *up = buf + lstsz;\n\n  while (lw < up) {\n      struct kinfo_vmentry *cur = reinterpret_cast<struct kinfo_vmentry *>(lw);\n      size_t cursz = cur->kve_structsize;\n      if (cursz == 0)\n          break;\n      void *start = reinterpret_cast<void *>(cur->kve_start);\n      void *end = reinterpret_cast<void *>(cur->kve_end);\n      // Readable/Writable addresses within current map entry\n      if ((addr >= start) && (addr < end)) {\n          if ((cur->kve_protection & KVME_PROT_READ) != 0 &&\n              (cur->kve_protection & KVME_PROT_WRITE) != 0) {\n              found = 1;\n              break;\n          }\n      }\n      lw += cursz;\n  }\n  kmpc_free(buf);\n\n#elif KMP_OS_DARWIN\n\n  /* On OS X*, /proc pseudo filesystem is not available. Try to read memory\n     using vm interface. */\n\n  int buffer;\n  vm_size_t count;\n  rc = vm_read_overwrite(\n      mach_task_self(), // Task to read memory of.\n      (vm_address_t)(addr), // Address to read from.\n      1, // Number of bytes to be read.\n      (vm_address_t)(&buffer), // Address of buffer to save read bytes in.\n      &count // Address of var to save number of read bytes in.\n      );\n  if (rc == 0) {\n    // Memory successfully read.\n    found = 1;\n  }\n\n#elif KMP_OS_NETBSD\n\n  int mib[5];\n  mib[0] = CTL_VM;\n  mib[1] = VM_PROC;\n  mib[2] = VM_PROC_MAP;\n  mib[3] = getpid();\n  mib[4] = sizeof(struct kinfo_vmentry);\n\n  size_t size;\n  rc = sysctl(mib, __arraycount(mib), NULL, &size, NULL, 0);\n  KMP_ASSERT(!rc);\n  KMP_ASSERT(size);\n\n  size = size * 4 / 3;\n  struct kinfo_vmentry *kiv = (struct kinfo_vmentry *)KMP_INTERNAL_MALLOC(size);\n  KMP_ASSERT(kiv);\n\n  rc = sysctl(mib, __arraycount(mib), kiv, &size, NULL, 0);\n  KMP_ASSERT(!rc);\n  KMP_ASSERT(size);\n\n  for (size_t i = 0; i < size; i++) {\n    if (kiv[i].kve_start >= (uint64_t)addr &&\n        kiv[i].kve_end <= (uint64_t)addr) {\n      found = 1;\n      break;\n    }\n  }\n  KMP_INTERNAL_FREE(kiv);\n#elif KMP_OS_OPENBSD\n\n  int mib[3];\n  mib[0] = CTL_KERN;\n  mib[1] = KERN_PROC_VMMAP;\n  mib[2] = getpid();\n\n  size_t size;\n  uint64_t end;\n  rc = sysctl(mib, 3, NULL, &size, NULL, 0);\n  KMP_ASSERT(!rc);\n  KMP_ASSERT(size);\n  end = size;\n\n  struct kinfo_vmentry kiv = {.kve_start = 0};\n\n  while ((rc = sysctl(mib, 3, &kiv, &size, NULL, 0)) == 0) {\n    KMP_ASSERT(size);\n    if (kiv.kve_end == end)\n      break;\n\n    if (kiv.kve_start >= (uint64_t)addr && kiv.kve_end <= (uint64_t)addr) {\n      found = 1;\n      break;\n    }\n    kiv.kve_start += 1;\n  }\n#elif KMP_OS_DRAGONFLY\n\n  // FIXME(DragonFly): Implement this\n  found = 1;\n\n#else\n\n#error \"Unknown or unsupported OS\"\n\n#endif\n\n  return found;\n\n} // __kmp_is_address_mapped\n\n#ifdef USE_LOAD_BALANCE\n\n#if KMP_OS_DARWIN || KMP_OS_NETBSD\n\n// The function returns the rounded value of the system load average\n// during given time interval which depends on the value of\n// __kmp_load_balance_interval variable (default is 60 sec, other values\n// may be 300 sec or 900 sec).\n// It returns -1 in case of error.\nint __kmp_get_load_balance(int max) {\n  double averages[3];\n  int ret_avg = 0;\n\n  int res = getloadavg(averages, 3);\n\n  // Check __kmp_load_balance_interval to determine which of averages to use.\n  // getloadavg() may return the number of samples less than requested that is\n  // less than 3.\n  if (__kmp_load_balance_interval < 180 && (res >= 1)) {\n    ret_avg = averages[0]; // 1 min\n  } else if ((__kmp_load_balance_interval >= 180 &&\n              __kmp_load_balance_interval < 600) &&\n             (res >= 2)) {\n    ret_avg = averages[1]; // 5 min\n  } else if ((__kmp_load_balance_interval >= 600) && (res == 3)) {\n    ret_avg = averages[2]; // 15 min\n  } else { // Error occurred\n    return -1;\n  }\n\n  return ret_avg;\n}\n\n#else // Linux* OS\n\n// The function returns number of running (not sleeping) threads, or -1 in case\n// of error. Error could be reported if Linux* OS kernel too old (without\n// \"/proc\" support). Counting running threads stops if max running threads\n// encountered.\nint __kmp_get_load_balance(int max) {\n  static int permanent_error = 0;\n  static int glb_running_threads = 0; // Saved count of the running threads for\n  // the thread balance algorithm\n  static double glb_call_time = 0; /* Thread balance algorithm call time */\n\n  int running_threads = 0; // Number of running threads in the system.\n\n  DIR *proc_dir = NULL; // Handle of \"/proc/\" directory.\n  struct dirent *proc_entry = NULL;\n\n  kmp_str_buf_t task_path; // \"/proc/<pid>/task/<tid>/\" path.\n  DIR *task_dir = NULL; // Handle of \"/proc/<pid>/task/<tid>/\" directory.\n  struct dirent *task_entry = NULL;\n  int task_path_fixed_len;\n\n  kmp_str_buf_t stat_path; // \"/proc/<pid>/task/<tid>/stat\" path.\n  int stat_file = -1;\n  int stat_path_fixed_len;\n\n  int total_processes = 0; // Total number of processes in system.\n  int total_threads = 0; // Total number of threads in system.\n\n  double call_time = 0.0;\n\n  __kmp_str_buf_init(&task_path);\n  __kmp_str_buf_init(&stat_path);\n\n  __kmp_elapsed(&call_time);\n\n  if (glb_call_time &&\n      (call_time - glb_call_time < __kmp_load_balance_interval)) {\n    running_threads = glb_running_threads;\n    goto finish;\n  }\n\n  glb_call_time = call_time;\n\n  // Do not spend time on scanning \"/proc/\" if we have a permanent error.\n  if (permanent_error) {\n    running_threads = -1;\n    goto finish;\n  }\n\n  if (max <= 0) {\n    max = INT_MAX;\n  }\n\n  // Open \"/proc/\" directory.\n  proc_dir = opendir(\"/proc\");\n  if (proc_dir == NULL) {\n    // Cannot open \"/prroc/\". Probably the kernel does not support it. Return an\n    // error now and in subsequent calls.\n    running_threads = -1;\n    permanent_error = 1;\n    goto finish;\n  }\n\n  // Initialize fixed part of task_path. This part will not change.\n  __kmp_str_buf_cat(&task_path, \"/proc/\", 6);\n  task_path_fixed_len = task_path.used; // Remember number of used characters.\n\n  proc_entry = readdir(proc_dir);\n  while (proc_entry != NULL) {\n    // Proc entry is a directory and name starts with a digit. Assume it is a\n    // process' directory.\n    if (proc_entry->d_type == DT_DIR && isdigit(proc_entry->d_name[0])) {\n\n      ++total_processes;\n      // Make sure init process is the very first in \"/proc\", so we can replace\n      // strcmp( proc_entry->d_name, \"1\" ) == 0 with simpler total_processes ==\n      // 1. We are going to check that total_processes == 1 => d_name == \"1\" is\n      // true (where \"=>\" is implication). Since C++ does not have => operator,\n      // let us replace it with its equivalent: a => b == ! a || b.\n      KMP_DEBUG_ASSERT(total_processes != 1 ||\n                       strcmp(proc_entry->d_name, \"1\") == 0);\n\n      // Construct task_path.\n      task_path.used = task_path_fixed_len; // Reset task_path to \"/proc/\".\n      __kmp_str_buf_cat(&task_path, proc_entry->d_name,\n                        KMP_STRLEN(proc_entry->d_name));\n      __kmp_str_buf_cat(&task_path, \"/task\", 5);\n\n      task_dir = opendir(task_path.str);\n      if (task_dir == NULL) {\n        // Process can finish between reading \"/proc/\" directory entry and\n        // opening process' \"task/\" directory. So, in general case we should not\n        // complain, but have to skip this process and read the next one. But on\n        // systems with no \"task/\" support we will spend lot of time to scan\n        // \"/proc/\" tree again and again without any benefit. \"init\" process\n        // (its pid is 1) should exist always, so, if we cannot open\n        // \"/proc/1/task/\" directory, it means \"task/\" is not supported by\n        // kernel. Report an error now and in the future.\n        if (strcmp(proc_entry->d_name, \"1\") == 0) {\n          running_threads = -1;\n          permanent_error = 1;\n          goto finish;\n        }\n      } else {\n        // Construct fixed part of stat file path.\n        __kmp_str_buf_clear(&stat_path);\n        __kmp_str_buf_cat(&stat_path, task_path.str, task_path.used);\n        __kmp_str_buf_cat(&stat_path, \"/\", 1);\n        stat_path_fixed_len = stat_path.used;\n\n        task_entry = readdir(task_dir);\n        while (task_entry != NULL) {\n          // It is a directory and name starts with a digit.\n          if (proc_entry->d_type == DT_DIR && isdigit(task_entry->d_name[0])) {\n            ++total_threads;\n\n            // Construct complete stat file path. Easiest way would be:\n            //  __kmp_str_buf_print( & stat_path, \"%s/%s/stat\", task_path.str,\n            //  task_entry->d_name );\n            // but seriae of __kmp_str_buf_cat works a bit faster.\n            stat_path.used =\n                stat_path_fixed_len; // Reset stat path to its fixed part.\n            __kmp_str_buf_cat(&stat_path, task_entry->d_name,\n                              KMP_STRLEN(task_entry->d_name));\n            __kmp_str_buf_cat(&stat_path, \"/stat\", 5);\n\n            // Note: Low-level API (open/read/close) is used. High-level API\n            // (fopen/fclose)  works ~ 30 % slower.\n            stat_file = open(stat_path.str, O_RDONLY);\n            if (stat_file == -1) {\n              // We cannot report an error because task (thread) can terminate\n              // just before reading this file.\n            } else {\n              /* Content of \"stat\" file looks like:\n                 24285 (program) S ...\n\n                 It is a single line (if program name does not include funny\n                 symbols). First number is a thread id, then name of executable\n                 file name in paretheses, then state of the thread. We need just\n                 thread state.\n\n                 Good news: Length of program name is 15 characters max. Longer\n                 names are truncated.\n\n                 Thus, we need rather short buffer: 15 chars for program name +\n                 2 parenthesis, + 3 spaces + ~7 digits of pid = 37.\n\n                 Bad news: Program name may contain special symbols like space,\n                 closing parenthesis, or even new line. This makes parsing\n                 \"stat\" file not 100 % reliable. In case of fanny program names\n                 parsing may fail (report incorrect thread state).\n\n                 Parsing \"status\" file looks more promissing (due to different\n                 file structure and escaping special symbols) but reading and\n                 parsing of \"status\" file works slower.\n                  -- ln\n              */\n              char buffer[65];\n              int len;\n              len = read(stat_file, buffer, sizeof(buffer) - 1);\n              if (len >= 0) {\n                buffer[len] = 0;\n                // Using scanf:\n                //     sscanf( buffer, \"%*d (%*s) %c \", & state );\n                // looks very nice, but searching for a closing parenthesis\n                // works a bit faster.\n                char *close_parent = strstr(buffer, \") \");\n                if (close_parent != NULL) {\n                  char state = *(close_parent + 2);\n                  if (state == 'R') {\n                    ++running_threads;\n                    if (running_threads >= max) {\n                      goto finish;\n                    }\n                  }\n                }\n              }\n              close(stat_file);\n              stat_file = -1;\n            }\n          }\n          task_entry = readdir(task_dir);\n        }\n        closedir(task_dir);\n        task_dir = NULL;\n      }\n    }\n    proc_entry = readdir(proc_dir);\n  }\n\n  // There _might_ be a timing hole where the thread executing this\n  // code get skipped in the load balance, and running_threads is 0.\n  // Assert in the debug builds only!!!\n  KMP_DEBUG_ASSERT(running_threads > 0);\n  if (running_threads <= 0) {\n    running_threads = 1;\n  }\n\nfinish: // Clean up and exit.\n  if (proc_dir != NULL) {\n    closedir(proc_dir);\n  }\n  __kmp_str_buf_free(&task_path);\n  if (task_dir != NULL) {\n    closedir(task_dir);\n  }\n  __kmp_str_buf_free(&stat_path);\n  if (stat_file != -1) {\n    close(stat_file);\n  }\n\n  glb_running_threads = running_threads;\n\n  return running_threads;\n\n} // __kmp_get_load_balance\n\n#endif // KMP_OS_DARWIN\n\n#endif // USE_LOAD_BALANCE\n\n#if KMP_USE_ABT || !(KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_MIC ||             \\\n      ((KMP_OS_LINUX || KMP_OS_DARWIN) && KMP_ARCH_AARCH64) ||                 \\\n      KMP_ARCH_PPC64 || KMP_ARCH_RISCV64)\n\n// we really only need the case with 1 argument, because CLANG always build\n// a struct of pointers to shared variables referenced in the outlined function\nint __kmp_invoke_microtask(microtask_t pkfn, int gtid, int tid, int argc,\n                           void *p_argv[]\n#if OMPT_SUPPORT\n                           ,\n                           void **exit_frame_ptr\n#endif\n                           ) {\n#if OMPT_SUPPORT\n  *exit_frame_ptr = OMPT_GET_FRAME_ADDRESS(0);\n#endif\n\n  switch (argc) {\n  default:\n    fprintf(stderr, \"Too many args to microtask: %d!\\n\", argc);\n    fflush(stderr);\n    exit(-1);\n  case 0:\n    (*pkfn)(&gtid, &tid);\n    break;\n  case 1:\n    (*pkfn)(&gtid, &tid, p_argv[0]);\n    break;\n  case 2:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1]);\n    break;\n  case 3:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2]);\n    break;\n  case 4:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3]);\n    break;\n  case 5:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4]);\n    break;\n  case 6:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5]);\n    break;\n  case 7:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6]);\n    break;\n  case 8:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7]);\n    break;\n  case 9:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8]);\n    break;\n  case 10:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9]);\n    break;\n  case 11:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10]);\n    break;\n  case 12:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11]);\n    break;\n  case 13:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12]);\n    break;\n  case 14:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13]);\n    break;\n  case 15:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14]);\n    break;\n  case 16:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15]);\n    break;\n  case 17:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16]);\n    break;\n  case 18:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17]);\n    break;\n  case 19:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18]);\n    break;\n  case 20:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19]);\n    break;\n  case 21:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20]);\n    break;\n  case 22:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21]);\n    break;\n  case 23:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22]);\n    break;\n  case 24:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23]);\n    break;\n  case 25:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24]);\n    break;\n  case 26:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25]);\n    break;\n  case 27:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26]);\n    break;\n  case 28:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27]);\n    break;\n  case 29:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28]);\n    break;\n  case 30:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29]);\n    break;\n  case 31:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30]);\n    break;\n  case 32:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31]);\n    break;\n  case 33:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32]);\n    break;\n  case 34:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33]);\n    break;\n  case 35:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34]);\n    break;\n  case 36:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35]);\n    break;\n  case 37:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36]);\n    break;\n  case 38:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37]);\n    break;\n  case 39:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38]);\n    break;\n  case 40:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39]);\n    break;\n  case 41:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40]);\n    break;\n  case 42:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41]);\n    break;\n  case 43:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42]);\n    break;\n  case 44:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43]);\n    break;\n  case 45:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44]);\n    break;\n  case 46:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45]);\n    break;\n  case 47:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46]);\n    break;\n  case 48:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47]);\n    break;\n  case 49:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48]);\n    break;\n  case 50:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49]);\n    break;\n  case 51:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50]);\n    break;\n  case 52:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51]);\n    break;\n  case 53:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52]);\n    break;\n  case 54:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53]);\n    break;\n  case 55:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54]);\n    break;\n  case 56:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55]);\n    break;\n  case 57:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56]);\n    break;\n  case 58:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57]);\n    break;\n  case 59:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57], p_argv[58]);\n    break;\n  case 60:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57], p_argv[58], p_argv[59]);\n    break;\n  case 61:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57], p_argv[58], p_argv[59], p_argv[60]);\n    break;\n  case 62:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57], p_argv[58], p_argv[59], p_argv[60],\n            p_argv[61]);\n    break;\n  case 63:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57], p_argv[58], p_argv[59], p_argv[60],\n            p_argv[61], p_argv[62]);\n    break;\n  case 64:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14], p_argv[15],\n            p_argv[16], p_argv[17], p_argv[18], p_argv[19], p_argv[20],\n            p_argv[21], p_argv[22], p_argv[23], p_argv[24], p_argv[25],\n            p_argv[26], p_argv[27], p_argv[28], p_argv[29], p_argv[30],\n            p_argv[31], p_argv[32], p_argv[33], p_argv[34], p_argv[35],\n            p_argv[36], p_argv[37], p_argv[38], p_argv[39], p_argv[40],\n            p_argv[41], p_argv[42], p_argv[43], p_argv[44], p_argv[45],\n            p_argv[46], p_argv[47], p_argv[48], p_argv[49], p_argv[50],\n            p_argv[51], p_argv[52], p_argv[53], p_argv[54], p_argv[55],\n            p_argv[56], p_argv[57], p_argv[58], p_argv[59], p_argv[60],\n            p_argv[61], p_argv[62], p_argv[63]);\n  }\n\n  return 1;\n}\n\n#endif\n\n#if KMP_USE_ABT\n\n// self_rank and master_place_id must be specified.\nstatic inline ABT_pool __kmp_abt_get_pool_thread(int self_rank,\n                                                 int master_place_id, int tid,\n                                                 int num_threads, int level,\n                                                 kmp_proc_bind_t proc_bind,\n                                                 int *p_place_id) {\n  KMP_DEBUG_ASSERT(self_rank >= 0);\n  KMP_DEBUG_ASSERT(master_place_id >= 0);\n  KMP_DEBUG_ASSERT(level >= 0);\n  KMP_DEBUG_ASSERT(tid >= 0);\n  if (level == 0) {\n    // The initial thread must be bound to the first place unless proc_bind is\n    // proc_bind_false\n    if (proc_bind == proc_bind_false || proc_bind == proc_bind_unset) {\n      // Push to a shared pool.\n      *p_place_id = -1;\n      return __kmp_abt_global.locals[self_rank].shared_pool;\n    } else {\n      // Push to the first place pool.\n      *p_place_id = 0;\n      return __kmp_abt_global.place_pools[0];\n    }\n  } else {\n    switch (proc_bind) {\n      case proc_bind_unset:\n        // Push to a shared pool.\n        *p_place_id = -1;\n        return __kmp_abt_global.locals[self_rank].shared_pool;\n      case proc_bind_close: {\n        const int num_places = __kmp_abt_global.num_places;\n        int push_place_id;\n        KMP_DEBUG_ASSERT(master_place_id != -1);\n        if (num_threads <= num_places) {\n          push_place_id = master_place_id + tid;\n        } else {\n          push_place_id = master_place_id + (tid * num_places) / num_threads;\n        }\n        push_place_id = (push_place_id >= num_places)\n                        ? (push_place_id - num_places) : push_place_id;\n        *p_place_id = push_place_id;\n        return __kmp_abt_global.place_pools[push_place_id];\n      }\n      case proc_bind_master: {\n        // Use master pool.\n        int place_id = __kmp_abt_global.locals[self_rank].place_id;\n        ABT_pool place_pool = __kmp_abt_global.locals[self_rank].place_pool;\n        *p_place_id = place_id;\n        return place_pool;\n      }\n      case proc_bind_spread:\n      case proc_bind_true: {\n        // Push to a place pool.\n        const int num_places = __kmp_abt_global.num_places;\n        int push_place_id = master_place_id + (tid * num_places) / num_threads;\n        push_place_id = (push_place_id >= num_places)\n                        ? (push_place_id - num_places) : push_place_id;\n        *p_place_id = push_place_id;\n        return __kmp_abt_global.place_pools[push_place_id];\n      }\n      case proc_bind_false:\n      default: {\n        // Push to a shared pool.\n        *p_place_id = -1;\n        return __kmp_abt_global.locals[self_rank].shared_pool;\n      }\n    }\n  }\n}\n\nstatic inline ABT_pool __kmp_abt_get_pool_task() {\n  int self_rank;\n  ABT_xstream_self_rank(&self_rank);\n  return __kmp_abt_global.locals[self_rank].shared_pool;\n}\n\nvoid __kmp_abt_release_info(kmp_info_t *th) {\n  KMP_DEBUG_ASSERT(th->th.th_active == TRUE);\n  TCW_4(th->th.th_active, FALSE);\n}\n\nvoid __kmp_abt_acquire_info_for_task(kmp_info_t *th, kmp_taskdata_t *taskdata,\n                                     const kmp_team_t *match_team, int atomic) {\n  if (atomic) {\n    while (1) {\n      // task must be executed by an inactive thread belonging to the same team;\n      // if not, yield to a scheduler.\n\n      // Quick check.\n      if (th->th.th_team != match_team)\n        goto END_WHILE;\n      // Take a lock.\n      if (KMP_COMPARE_AND_STORE_RET32(&th->th.th_active, FALSE, TRUE) != FALSE)\n        goto END_WHILE;\n      // th->th.th_team might have been updated while taking a lock; if th_team\n      // is not matched, yield to a scheduler.\n      if (th->th.th_team != match_team) {\n        __kmp_abt_release_info(th);\n        goto END_WHILE;\n      }\n      break;\n  END_WHILE:\n      ABT_thread_yield();\n    }\n  } else {\n    KMP_DEBUG_ASSERT(th->th.th_active == FALSE);\n    th->th.th_active = TRUE;\n  }\n  th->th.th_current_task = taskdata;\n}\n\nvoid __kmp_abt_set_self_info(kmp_info_t *th) {\n  ABT_thread self;\n\n  KMP_ASSERT(__kmp_init_runtime);\n  ABT_thread_self(&self);\n  if (self == ABT_THREAD_NULL) {\n    KMP_ASSERT(__kmp_init_gtid);\n    /* External threads might call OpenMP functions. */\n    int gtid = (size_t)pthread_getspecific(__kmp_gtid_threadprivate_key);\n    KA_TRACE(50, (\"__kmp_abt_set_self_info: key:%d gtid:%d\\n\",\n                  __kmp_gtid_threadprivate_key, gtid));\n    __kmp_threads[gtid] = th;\n  } else {\n    int ret = ABT_thread_set_arg(self, (void *)th);\n    KMP_ASSERT(ret == ABT_SUCCESS);\n  }\n}\n\nkmp_info_t *__kmp_abt_get_self_info(void) {\n  ABT_thread self;\n\n  KMP_ASSERT(__kmp_init_runtime);\n  ABT_thread_self(&self);\n  if (self == ABT_THREAD_NULL) {\n    KMP_ASSERT(__kmp_init_gtid);\n    /* External threads might call OpenMP functions. */\n    int gtid = (size_t)pthread_getspecific(__kmp_gtid_threadprivate_key);\n    KA_TRACE(50, (\"__kmp_abt_get_self_info: key:%d gtid:%d\\n\",\n                  __kmp_gtid_threadprivate_key, gtid));\n    return __kmp_threads[gtid];\n  } else {\n    kmp_info_t *th;\n    int ret = ABT_thread_get_arg(self, (void **)&th);\n    KMP_ASSERT(th != NULL);\n    KMP_ASSERT(ret == ABT_SUCCESS);\n    return th;\n  }\n}\n\nstatic void __kmp_abt_initialize(void) {\n  int status;\n  int num_xstreams;\n  int i, k;\n  kmp_abt_affinity_places_t *p_affinity_places = NULL;\n\n  {\n    int verbose = 0;\n    const char *env = getenv(\"KMP_ABT_VERBOSE\");\n    if (env && atoi(env) != 0) {\n      verbose = 1;\n      printf(\"=== BOLT info (KMP_ABT_VERBOSE) ===\\n\");\n    }\n\n    env = getenv(\"KMP_ABT_NUM_ESS\");\n    if (env) {\n      num_xstreams = atoi(env);\n      if (num_xstreams < __kmp_xproc)\n        __kmp_xproc = num_xstreams;\n    } else {\n      num_xstreams = __kmp_xproc;\n    }\n    if (verbose)\n      printf(\"KMP_ABT_NUM_ESS = %d\\n\", num_xstreams);\n\n    env = getenv(\"OMP_PLACES\");\n    if (!env) {\n      env = getenv(\"KMP_AFFINITY\");\n      if (!env) {\n        env = \"threads\";\n      } else {\n        if (verbose)\n          printf(\"[warning] BOLT does not support KMP_AFFINITY; \"\n                 \"parse it as OMP_PLACES.\\n\");\n      }\n    }\n    p_affinity_places = __kmp_abt_parse_affinity(num_xstreams, env, strlen(env),\n                                                 verbose);\n    {\n      bool failure = false;\n      for (int rank = 0; rank < num_xstreams; rank++) {\n        int num_assoc_places = 0;\n        int num_places = p_affinity_places->num_places;\n        for (int place_id = 0; place_id < num_places; place_id++) {\n          kmp_abt_affinity_place_t *p_place =\n              p_affinity_places->p_places[place_id];\n          for (int i = 0, num_ranks = p_place->num_ranks; i < num_ranks; i++) {\n            if (p_place->ranks[i] == rank)\n              num_assoc_places++;\n          }\n        }\n        if (num_assoc_places > 1) {\n          failure = true;\n          break;\n        }\n      }\n      if (failure) {\n        printf(\"[warning] More than one place are associated with the same \"\n               \"processor; fall back to a default affinity.\\n\");\n        __kmp_abt_affinity_places_free(p_affinity_places);\n        p_affinity_places = __kmp_abt_parse_affinity(num_xstreams, \"threads\",\n                                                     strlen(\"threads\"),\n                                                     verbose);\n      }\n    }\n\n    env = getenv(\"KMP_ABT_FORK_CUTOFF\");\n    if (env) {\n      __kmp_abt_global.fork_cutoff = atoi(env);\n      if (__kmp_abt_global.fork_cutoff <= 0)\n        __kmp_abt_global.fork_cutoff = 1;\n    } else {\n      __kmp_abt_global.fork_cutoff = KMP_ABT_FORK_CUTOFF_DEFAULT;\n    }\n    if (verbose)\n      printf(\"KMP_ABT_FORK_CUTOFF = %d\\n\", __kmp_abt_global.fork_cutoff);\n\n    env = getenv(\"KMP_ABT_FORK_NUM_WAYS\");\n    if (env) {\n      __kmp_abt_global.fork_num_ways = atoi(env);\n      if (__kmp_abt_global.fork_num_ways <= 1)\n        __kmp_abt_global.fork_num_ways = 2;\n    } else {\n      __kmp_abt_global.fork_num_ways = KMP_ABT_FORK_NUM_WAYS_DEFAULT;\n    }\n    if (verbose)\n      printf(\"KMP_ABT_FORK_NUM_WAYS = %d\\n\", __kmp_abt_global.fork_num_ways);\n\n    env = getenv(\"KMP_ABT_SCHED_SLEEP\");\n    if (env) {\n      __kmp_abt_global.is_sched_sleep = atoi(env);\n    } else {\n      __kmp_abt_global.is_sched_sleep = KMP_ABT_SCHED_SLEEP_DEFAULT;\n    }\n    if (verbose)\n      printf(\"KMP_ABT_SCHED_SLEEP = %d\\n\", __kmp_abt_global.is_sched_sleep);\n\n    env = getenv(\"KMP_ABT_SCHED_MIN_SLEEP_NSEC\");\n    if (env) {\n      __kmp_abt_global.sched_sleep_min_nsec = atoi(env);\n      if (__kmp_abt_global.sched_sleep_min_nsec <= 0)\n        __kmp_abt_global.sched_sleep_min_nsec = 0;\n    } else {\n      __kmp_abt_global.sched_sleep_min_nsec\n          = KMP_ABT_SCHED_MIN_SLEEP_NSEC_DEFAULT;\n    }\n    if (verbose)\n      printf(\"KMP_ABT_SCHED_MIN_SLEEP_NSEC = %d\\n\",\n             __kmp_abt_global.sched_sleep_min_nsec);\n\n    env = getenv(\"KMP_ABT_SCHED_MAX_SLEEP_NSEC\");\n    if (env) {\n      __kmp_abt_global.sched_sleep_max_nsec = atoi(env);\n      if (__kmp_abt_global.sched_sleep_max_nsec\n          < __kmp_abt_global.sched_sleep_min_nsec)\n        __kmp_abt_global.sched_sleep_max_nsec\n            = __kmp_abt_global.sched_sleep_min_nsec;\n    } else {\n      __kmp_abt_global.sched_sleep_max_nsec\n          = KMP_ABT_SCHED_MAX_SLEEP_NSEC_DEFAULT;\n    }\n    if (verbose)\n      printf(\"KMP_ABT_SCHED_MAX_SLEEP_NSEC = %d\\n\",\n             __kmp_abt_global.sched_sleep_max_nsec);\n\n    env = getenv(\"KMP_ABT_SCHED_EVENT_FREQ\");\n    if (env) {\n      __kmp_abt_global.sched_event_freq = atoi(env);\n      if (__kmp_abt_global.sched_event_freq <= 1)\n        __kmp_abt_global.sched_event_freq = 1;\n      if (__kmp_abt_global.sched_event_freq > KMP_ABT_SCHED_EVENT_FREQ_MAX)\n        __kmp_abt_global.sched_event_freq = KMP_ABT_SCHED_EVENT_FREQ_MAX;\n    } else {\n      __kmp_abt_global.sched_event_freq = KMP_ABT_SCHED_EVENT_FREQ_DEFAULT;\n    }\n    // Must be 2^N\n    for (int digit = 0;; digit++) {\n      if ((1 << digit) >= __kmp_abt_global.sched_event_freq) {\n        __kmp_abt_global.sched_event_freq = 1 << digit;\n        break;\n       }\n    }\n    if (verbose)\n      printf(\"KMP_ABT_SCHED_EVENT_FREQ = %d\\n\",\n             __kmp_abt_global.sched_event_freq);\n\n    env = getenv(\"KMP_ABT_WORK_STEAL_FREQ\");\n    if (env) {\n      __kmp_abt_global.work_steal_freq = atoi(env);\n      if (__kmp_abt_global.work_steal_freq <= 0)\n        __kmp_abt_global.work_steal_freq = 0;\n    } else {\n      __kmp_abt_global.work_steal_freq = KMP_ABT_WORK_STEAL_FREQ_DEFAULT;\n    }\n    // Must be 2^N\n    if (__kmp_abt_global.work_steal_freq != 0) {\n      for (uint32_t digit = 0;; digit++) {\n        if ((1u << digit) >= __kmp_abt_global.work_steal_freq) {\n          __kmp_abt_global.work_steal_freq = 1u << digit;\n          break;\n         }\n      }\n    }\n    if (verbose)\n      printf(\"KMP_ABT_WORK_STEAL_FREQ = %ud\\n\",\n             (unsigned int)__kmp_abt_global.work_steal_freq);\n  }\n\n  KA_TRACE(10, (\"__kmp_abt_initialize: # of ESs = %d\\n\", num_xstreams));\n\n  __kmp_abt_global.locals = (kmp_abt_local *)__kmp_allocate\n      (sizeof(kmp_abt_local) * num_xstreams);\n  __kmp_abt_global.num_xstreams = num_xstreams;\n  for (int rank = 0; rank < num_xstreams; rank++) {\n    __kmp_abt_global.locals[rank].place_id = -1;\n    __kmp_abt_global.locals[rank].place_pool = ABT_POOL_NULL;\n  }\n\n  /* Create place pools. */\n  const int num_places = p_affinity_places->num_places;\n  KMP_ASSERT(num_places != 0);\n  ABT_pool *place_pools = (ABT_pool *)__kmp_allocate(sizeof(ABT_pool)\n                                                     * num_places);\n  __kmp_abt_global.num_places = num_places;\n  __kmp_abt_global.place_pools = place_pools;\n  for (int place_id = 0; place_id < num_places; place_id++) {\n    const int num_ranks = p_affinity_places->p_places[place_id]->num_ranks;\n    ABT_pool_access access = (num_ranks == 1) ? ABT_POOL_ACCESS_MPSC\n                                              : ABT_POOL_ACCESS_MPMC;\n    status = ABT_pool_create_basic(ABT_POOL_FIFO, access, ABT_TRUE,\n                                   &place_pools[place_id]);\n    KMP_CHECK_SYSFAIL(\"ABT_pool_create_basic\", status);\n    for (int i = 0; i < num_ranks; i++) {\n      int rank = p_affinity_places->p_places[place_id]->ranks[i];\n      __kmp_abt_global.locals[rank].place_id = place_id;\n      __kmp_abt_global.locals[rank].place_pool = place_pools[place_id];\n    }\n  }\n  __kmp_abt_affinity_places_free(p_affinity_places);\n\n  /* Create shared/private pools */\n  for (i = 0; i < num_xstreams; i++) {\n    status = ABT_pool_create_basic(ABT_POOL_FIFO, ABT_POOL_ACCESS_MPMC,\n                                   ABT_TRUE,\n                                   &__kmp_abt_global.locals[i].shared_pool);\n    KMP_CHECK_SYSFAIL(\"ABT_pool_create_basic\", status);\n  }\n\n  /* Create schedulers */\n  ABT_sched_def sched_def = {\n    .type = ABT_SCHED_TYPE_ULT,\n    .init = __kmp_abt_sched_init,\n    .run = __kmp_abt_sched_run,\n    .free = __kmp_abt_sched_free,\n    .get_migr_pool = NULL\n  };\n\n  ABT_pool *my_pools;\n  my_pools = (ABT_pool *)malloc((num_xstreams + 1) * sizeof(ABT_pool));\n\n  for (i = 0; i < num_xstreams; i++) {\n    for (k = 0; k < num_xstreams; k++) {\n      my_pools[k] =\n          __kmp_abt_global.locals[(i + k) % num_xstreams].shared_pool;\n    }\n    int num_pools = num_xstreams;\n    if (__kmp_abt_global.locals[i].place_id != -1) {\n      my_pools[num_pools++] = __kmp_abt_global.locals[i].place_pool;\n    }\n    status = ABT_sched_create(&sched_def, num_pools, my_pools,\n                              ABT_SCHED_CONFIG_NULL,\n                              &__kmp_abt_global.locals[i].sched);\n    KMP_CHECK_SYSFAIL(\"ABT_sched_create\", status);\n  }\n\n  free(my_pools);\n\n  /* Create ESs */\n  status = ABT_xstream_self(&__kmp_abt_global.locals[0].xstream);\n  KMP_CHECK_SYSFAIL(\"ABT_xstream_self\", status);\n  status = ABT_xstream_set_main_sched(__kmp_abt_global.locals[0].xstream,\n                                      __kmp_abt_global.locals[0].sched);\n  KMP_CHECK_SYSFAIL(\"ABT_xstream_set_main_sched\", status);\n  for (i = 1; i < num_xstreams; i++) {\n    status = ABT_xstream_create(__kmp_abt_global.locals[i].sched,\n                                &__kmp_abt_global.locals[i].xstream);\n    KMP_CHECK_SYSFAIL(\"ABT_xstream_create\", status);\n  }\n}\n\nstatic void __kmp_abt_finalize(void) {\n  int status;\n  int i;\n\n  for (i = 1; i < __kmp_abt_global.num_xstreams; i++) {\n    status = ABT_xstream_join(__kmp_abt_global.locals[i].xstream);\n    KMP_CHECK_SYSFAIL(\"ABT_xstream_join\", status);\n    status = ABT_xstream_free(&__kmp_abt_global.locals[i].xstream);\n    KMP_CHECK_SYSFAIL(\"ABT_xstream_free\", status);\n  }\n\n  /* Free schedulers */\n  for (i = 1; i < __kmp_abt_global.num_xstreams; i++) {\n    status = ABT_sched_free(&__kmp_abt_global.locals[i].sched);\n    KMP_CHECK_SYSFAIL(\"ABT_sched_free\", status);\n  }\n\n  __kmp_free(__kmp_abt_global.locals);\n  __kmp_free(__kmp_abt_global.place_pools);\n  __kmp_abt_global.num_xstreams = 0;\n  __kmp_abt_global.locals = NULL;\n  __kmp_abt_global.place_pools = NULL;\n}\n\nvolatile int __kmp_abt_init_global = FALSE;\nvoid __kmp_abt_global_initialize() {\n  int status;\n  // Initialize Argobots before other initializations.\n  status = ABT_init(0, NULL);\n  KMP_CHECK_SYSFAIL(\"ABT_init\", status);\n  __kmp_abt_init_global = TRUE;\n}\n\nvoid __kmp_abt_global_destroy() {\n  ABT_finalize();\n  __kmp_abt_init_global = FALSE;\n}\n\nstatic int __kmp_abt_sched_init(ABT_sched sched, ABT_sched_config config) {\n  return ABT_SUCCESS;\n}\n\nstatic void __kmp_abt_sched_run(ABT_sched sched) {\n  uint32_t work_count = 0;\n  int num_pools, num_shared_pools = __kmp_abt_global.num_xstreams;\n  int rank;\n  ABT_xstream_self_rank(&rank);\n  ABT_pool *shared_pools;\n  ABT_pool place_pool;\n  uint32_t seed;\n  const int sched_event_freq = __kmp_abt_global.sched_event_freq;\n  const int sched_sleep_min_nsec = __kmp_abt_global.sched_sleep_min_nsec;\n  const int sched_sleep_max_nsec = __kmp_abt_global.sched_sleep_max_nsec;\n  int sched_sleep_nsec = __kmp_abt_global.is_sched_sleep ? sched_sleep_min_nsec\n                                                         : -1;\n  const uint32_t work_steal_freq = __kmp_abt_global.work_steal_freq;\n  do {\n    seed = (uint32_t)time(NULL) + 64 + rank;\n  } while (seed == 0);\n  KMP_DEBUG_ASSERT(!(sched_event_freq & (sched_event_freq - 1))); // must be 2^N\n  const uint32_t sched_event_freq_mask = sched_event_freq - 1;\n  KMP_DEBUG_ASSERT(!(work_steal_freq & (work_steal_freq - 1))); // must be 2^N\n  const uint32_t work_steal_freq_mask = work_steal_freq - 1;\n\n  ABT_sched_get_num_pools(sched, &num_pools);\n  shared_pools = (ABT_pool *)alloca(num_pools * sizeof(ABT_pool));\n  ABT_sched_get_pools(sched, num_pools, 0, shared_pools);\n  place_pool = __kmp_abt_global.locals[rank].place_pool;\n\n  while (1) {\n    ABT_unit unit;\n    int run_cnt = 0;\n\n    /* From the place pool */\n    if (place_pool != ABT_POOL_NULL) {\n      ABT_pool_pop(place_pool, &unit);\n      if (unit != ABT_UNIT_NULL) {\n        ABT_xstream_run_unit(unit, place_pool);\n        run_cnt++;\n      }\n    }\n\n    /* From the shared pool */\n    ABT_pool_pop(shared_pools[0], &unit);\n    if (unit != ABT_UNIT_NULL) {\n      ABT_xstream_run_unit(unit, shared_pools[0]);\n      run_cnt++;\n    }\n\n    /* Steal a work unit from other pools */\n    if (num_shared_pools >= 2\n        && (run_cnt == 0 || !(work_count & work_steal_freq_mask))) {\n      int target = __kmp_abt_fast_rand32(&seed) %\n                   ((uint32_t)(num_shared_pools - 1)) + 1;\n      ABT_pool_pop(shared_pools[target], &unit);\n      if (unit != ABT_UNIT_NULL) {\n        ABT_unit_set_associated_pool(unit, shared_pools[0]);\n        ABT_xstream_run_unit(unit, shared_pools[0]);\n        run_cnt++;\n      }\n    }\n\n    if (!(++work_count & sched_event_freq_mask)) {\n      ABT_bool stop;\n      ABT_xstream_check_events(sched);\n      ABT_sched_has_to_stop(sched, &stop);\n      if (stop == ABT_TRUE)\n        break;\n      if (sched_sleep_nsec >= 0) {\n        if (run_cnt == 0) {\n          struct timespec sleep_time;\n          sleep_time.tv_sec = 0;\n          sleep_time.tv_nsec = sched_sleep_nsec;\n          nanosleep(&sleep_time, NULL);\n          sched_sleep_nsec = (sched_sleep_nsec == 0) ? 1\n                              : (sched_sleep_nsec << 1);\n          if (sched_sleep_nsec > sched_sleep_max_nsec) {\n            sched_sleep_nsec = sched_sleep_max_nsec;\n          }\n        } else {\n          sched_sleep_nsec = sched_sleep_min_nsec;\n        }\n      }\n    }\n  }\n}\n\nstatic int __kmp_abt_sched_free(ABT_sched sched) {\n    return ABT_SUCCESS;\n}\n\nstatic inline void __kmp_abt_free_task(kmp_info_t *th, kmp_taskdata_t *taskdata)\n{\n  int gtid = __kmp_gtid_from_thread(th);\n\n  KA_TRACE(30, (\"__kmp_abt_free_task: (enter) T#%d - task %p\\n\",\n                gtid, taskdata));\n\n  /* [AC] we need those steps to mark the task as finished so the dependencies\n   *  can be completed */\n  taskdata->td_flags.complete = 1; // mark the task as completed\n  __kmp_release_deps(gtid, taskdata);\n  taskdata->td_flags.executing = 0; // suspend the finishing task\n\n  // Wait for all tasks after releasing (=pushing) dependent tasks\n  __kmp_abt_wait_child_tasks(th, true, FALSE);\n\n  taskdata->td_flags.freed = 1;\n\n  /* Free the task queue if it was allocated. */\n  if (taskdata->td_task_queue) {\n    KMP_DEBUG_ASSERT(taskdata->td_tq_cur_size == 0);\n    KMP_INTERNAL_FREE(taskdata->td_task_queue);\n  }\n\n  // deallocate the taskdata and shared variable blocks associated with this\n  // task\n#if USE_FAST_MEMORY\n  __kmp_fast_free(th, taskdata);\n#else\n  __kmp_thread_free(th, taskdata);\n#endif\n\n  KA_TRACE(30, (\"__kmp_abt_free_task: (exit) T#%d - task %p\\n\",\n                gtid, taskdata));\n}\n\nstatic void __kmp_abt_execute_task(void *arg) {\n  // It is corresponding to __kmp_execute_tasks_.\n\n  kmp_task_t *task = (kmp_task_t *)arg;\n  kmp_taskdata_t *taskdata = KMP_TASK_TO_TASKDATA(task);\n  kmp_info_t *th;\n\n  th = __kmp_abt_bind_task_to_thread(taskdata->td_team, taskdata);\n\n  KA_TRACE(20, (\"__kmp_abt_execute_task: T#%d before executing task %p.\\n\",\n                __kmp_gtid_from_thread(th), task));\n\n  // See __kmp_task_start\n  taskdata->td_flags.started = 1;\n  taskdata->td_flags.executing = 1;\n  KMP_DEBUG_ASSERT(taskdata->td_flags.complete == 0);\n  KMP_DEBUG_ASSERT(taskdata->td_flags.freed == 0);\n\n  while (1) {\n    // Run __kmp_invoke_task to handle internal counters correctly.\n#ifdef KMP_GOMP_COMPAT\n    if (taskdata->td_flags.native) {\n      ((void (*)(void *))(*(task->routine)))(task->shareds);\n    } else\n#endif /* KMP_GOMP_COMPAT */\n    {\n      (*(task->routine))(__kmp_gtid_from_thread(th), task);\n    }\n\n    if (!taskdata->td_flags.tiedness) {\n      // If this task is an untied one, we need to retrieve kmp_info because it\n      // may have been changed.\n      th = __kmp_abt_get_self_info();\n    }\n    // See __kmp_task_finish (untied)\n    if (taskdata->td_flags.tiedness == TASK_UNTIED) {\n      // Check if we can finish this task.\n      kmp_int32 counter = KMP_ATOMIC_DEC(&taskdata->td_untied_count) - 1;\n      if (counter > 0) {\n        // We should keep this ULT.\n        continue;\n      }\n    }\n    // tied or finished untied.\n    break;\n  }\n\n  // See __kmp_task_finish (tied/finished untied)\n  // KMP_DEBUG_ASSERT(taskdata->td_flags.executing == 0);\n  taskdata->td_flags.executing = 0;\n  KMP_DEBUG_ASSERT(taskdata->td_flags.complete == 0);\n  taskdata->td_flags.complete = 1; // mark the task as completed\n  // KMP_DEBUG_ASSERT(taskdata->td_flags.started == 1);\n  // KMP_DEBUG_ASSERT(taskdata->td_flags.freed == 0);\n\n  // Free this task.\n  __kmp_abt_free_task(th, taskdata);\n\n  // Reset th's ownership.\n  __kmp_abt_release_info(th);\n\n  KA_TRACE(20, (\"__kmp_abt_execute_task: T#%d after executing task %p.\\n\",\n                __kmp_gtid_from_thread(th), task));\n}\n\nint __kmp_abt_create_task(kmp_info_t *th, kmp_task_t *task) {\n  int status;\n  ABT_pool dest = __kmp_abt_get_pool_task();\n\n  KA_TRACE(20, (\"__kmp_abt_create_task: T#%d before creating task %p into the \"\n                \"pool %p.\\n\", __kmp_gtid_from_thread(th), task, dest));\n\n  /* Check if the task queue has an empty slot. */\n  kmp_taskdata_t *td = th->th.th_current_task;\n  if (td->td_tq_cur_size == td->td_tq_max_size) {\n    size_t new_max_size;\n    if (td->td_tq_max_size == 0) {\n      /* Empty queue. We allocate 32 slots by default. */\n      new_max_size = 32;\n    } else {\n      /* The task queue is full. Expand it. */\n      new_max_size = td->td_tq_max_size * 2;\n    }\n\n    void *queue = (void *)td->td_task_queue;\n    size_t size = sizeof(kmp_abt_task_t) * new_max_size;\n    td->td_task_queue = (kmp_abt_task_t *)KMP_INTERNAL_REALLOC(queue, size);\n    td->td_tq_max_size = new_max_size;\n  }\n\n  status = ABT_thread_create(dest, __kmp_abt_execute_task, (void *)task,\n                             ABT_THREAD_ATTR_NULL,\n                             &td->td_task_queue[td->td_tq_cur_size++]);\n  KMP_ASSERT(status == ABT_SUCCESS);\n\n  KA_TRACE(20, (\"__kmp_abt_create_task: T#%d after creating task %p into the \"\n                \"pool %p.\\n\", __kmp_gtid_from_thread(th), task, dest));\n\n  return TRUE;\n}\n\nkmp_info_t *__kmp_abt_wait_child_tasks(kmp_info_t *th, bool thread_bind,\n                                       int yield) {\n  KA_TRACE(20, (\"__kmp_abt_wait_child_tasks: T#%d enter\\n\",\n                __kmp_gtid_from_thread(th)));\n\n  int i, status;\n  kmp_taskdata_t *taskdata = th->th.th_current_task;\n  // Get the associated team before releasing the ownership of th.\n  kmp_team_t *team = th->th.th_team;\n  kmp_info_t *new_th = th;\n\n  if (taskdata->td_tq_cur_size == 0) {\n    /* leaf task case */\n    if (yield) {\n      __kmp_abt_release_info(th);\n\n      ABT_thread_yield();\n\n      if (thread_bind || taskdata->td_flags.tiedness) {\n        __kmp_abt_acquire_info_for_task(th, taskdata, team);\n      } else {\n        new_th = __kmp_abt_bind_task_to_thread(team, taskdata);\n      }\n    }\n    KA_TRACE(20, (\"__kmp_abt_wait_child_tasks: T#%d done\\n\",\n                  __kmp_gtid_from_thread(new_th)));\n    return new_th;\n  }\n\n  /* Let others, e.g., tasks, can use this kmp_info */\n  __kmp_abt_release_info(th);\n\n  /* Give other tasks a chance for execution */\n  if (yield)\n    ABT_thread_yield();\n\n  /* Wait until all child tasks are complete. */\n  for (i = 0; i < taskdata->td_tq_cur_size; i++) {\n    status = ABT_thread_free(&taskdata->td_task_queue[i]);\n    KMP_ASSERT(status == ABT_SUCCESS);\n  }\n  taskdata->td_tq_cur_size = 0;\n\n  if (thread_bind || taskdata->td_flags.tiedness) {\n    /* Obtain kmp_info to continue the original task. */\n    __kmp_abt_acquire_info_for_task(th, taskdata, team);\n  } else {\n    new_th = __kmp_abt_bind_task_to_thread(team, taskdata);\n  }\n\n  KA_TRACE(20, (\"__kmp_abt_wait_child_tasks: T#%d done\\n\",\n                __kmp_gtid_from_thread(new_th)));\n  return new_th;\n}\n\nkmp_info_t *__kmp_abt_bind_task_to_thread(kmp_team_t *team,\n                                          kmp_taskdata_t *taskdata) {\n  int i, i_start, i_end;\n  kmp_info_t *th = NULL;\n\n  KA_TRACE(20, (\"__kmp_abt_bind_task_to_thread: (enter) task %p\\n\", taskdata));\n\n  /* To handle gtid in the task code, we look for a suspended (blocked)\n   * thread in the team and use its info to execute this task. */\n  while (1) {\n    if (team->t.t_level <= 1) {\n      /* outermost team - we try to assign the thread that was executed on\n       * the same ES first and then check other threads in the team.  */\n      int rank;\n      ABT_xstream_self_rank(&rank);\n      if (rank < team->t.t_nproc) {\n        /* [SM] I think this condition should always be true, but just in\n         * case I miss something we check this condition. */\n        i_start = rank;\n        i_end = team->t.t_nproc + rank;\n      } else {\n        i_start = 0;\n        i_end = team->t.t_nproc;\n      }\n    } else {\n      /* nested team - we ignore the ES info since threads in the nested team\n       * may be executed by any ES. */\n      i_start = 0;\n      i_end = team->t.t_nproc;\n    }\n    /* TODO: This is a linear search. Can we do better? */\n    for (i = i_start; i < i_end; i++) {\n      int idx = (i < team->t.t_nproc) ? i : i % team->t.t_nproc;\n      th = team->t.t_threads[idx];\n      ABT_thread ult = th->th.th_info.ds.ds_thread;\n\n      if (th->th.th_active == FALSE && ult != ABT_THREAD_NULL) {\n        /* Try to take the ownership of kmp_info 'th' */\n        if (th->th.th_team != team)\n          continue;\n        if (KMP_COMPARE_AND_STORE_RET32(&th->th.th_active, FALSE, TRUE)\n            == FALSE) {\n          if (th->th.th_team != team) {\n            __kmp_abt_release_info(th);\n            continue;\n          }\n          /* Bind this task as if it is executed by 'th'. */\n          th->th.th_current_task = taskdata;\n          th->th.th_task_team = taskdata->td_task_team;\n          __kmp_abt_set_self_info(th);\n          KA_TRACE(20, (\"__kmp_abt_bind_task_to_thread: (exit) task %p\"\n                        \"bound to T#%d\\n\",\n                        taskdata, __kmp_gtid_from_thread(th)));\n          return th;\n        }\n      }\n    }\n    /* We could not find an available kmp_info. Thus, this task yields\n     * control to other work units and will try to find one later. */\n    ABT_thread_yield();\n  }\n  return NULL;\n}\n\nvoid __kmp_abt_create_uber(int gtid, kmp_info_t *th, size_t stack_size) {\n  KMP_DEBUG_ASSERT(KMP_UBER_GTID(gtid));\n  KA_TRACE(10, (\"__kmp_abt_create_uber: T#%d\\n\", gtid));\n  ABT_thread handle;\n  ABT_thread_self(&handle);\n  if (handle == ABT_THREAD_NULL) {\n    // External threads might call this function.  In this case, we do not need\n    // to set `th` since external threads use pthread_setspecific,\n    __kmp_gtid_set_specific(gtid);\n  } else {\n    ABT_thread_set_arg(handle, (void *)th);\n  }\n  th->th.th_info.ds.ds_thread = handle;\n}\n\n#endif // KMP_USE_ABT\n\n// end of file //\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/runtime/src/ompt-general.cpp": "/*\n * ompt-general.cpp -- OMPT implementation of interface functions\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n/*****************************************************************************\n * system include files\n ****************************************************************************/\n\n#include <assert.h>\n\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#if KMP_OS_UNIX\n#include <dlfcn.h>\n#endif\n\n/*****************************************************************************\n * ompt include files\n ****************************************************************************/\n\n#include \"ompt-specific.cpp\"\n\n/*****************************************************************************\n * macros\n ****************************************************************************/\n\n#define ompt_get_callback_success 1\n#define ompt_get_callback_failure 0\n\n#define no_tool_present 0\n\n#define OMPT_API_ROUTINE static\n\n#ifndef OMPT_STR_MATCH\n#define OMPT_STR_MATCH(haystack, needle) (!strcasecmp(haystack, needle))\n#endif\n\n/*****************************************************************************\n * types\n ****************************************************************************/\n\ntypedef struct {\n  const char *state_name;\n  ompt_state_t state_id;\n} ompt_state_info_t;\n\ntypedef struct {\n  const char *name;\n  kmp_mutex_impl_t id;\n} kmp_mutex_impl_info_t;\n\nenum tool_setting_e {\n  omp_tool_error,\n  omp_tool_unset,\n  omp_tool_disabled,\n  omp_tool_enabled\n};\n\n/*****************************************************************************\n * global variables\n ****************************************************************************/\n\nompt_callbacks_active_t ompt_enabled;\n\nompt_state_info_t ompt_state_info[] = {\n#define ompt_state_macro(state, code) {#state, state},\n    FOREACH_OMPT_STATE(ompt_state_macro)\n#undef ompt_state_macro\n};\n\nkmp_mutex_impl_info_t kmp_mutex_impl_info[] = {\n#define kmp_mutex_impl_macro(name, id) {#name, name},\n    FOREACH_KMP_MUTEX_IMPL(kmp_mutex_impl_macro)\n#undef kmp_mutex_impl_macro\n};\n\nompt_callbacks_internal_t ompt_callbacks;\n\nstatic ompt_start_tool_result_t *ompt_start_tool_result = NULL;\n\n/*****************************************************************************\n * forward declarations\n ****************************************************************************/\n\nstatic ompt_interface_fn_t ompt_fn_lookup(const char *s);\n\nOMPT_API_ROUTINE ompt_data_t *ompt_get_thread_data(void);\n\n/*****************************************************************************\n * initialization and finalization (private operations)\n ****************************************************************************/\n\ntypedef ompt_start_tool_result_t *(*ompt_start_tool_t)(unsigned int,\n                                                       const char *);\n\n#if KMP_OS_DARWIN\n\n// While Darwin supports weak symbols, the library that wishes to provide a new\n// implementation has to link against this runtime which defeats the purpose\n// of having tools that are agnostic of the underlying runtime implementation.\n//\n// Fortunately, the linker includes all symbols of an executable in the global\n// symbol table by default so dlsym() even finds static implementations of\n// ompt_start_tool. For this to work on Linux, -Wl,--export-dynamic needs to be\n// passed when building the application which we don't want to rely on.\n\nstatic ompt_start_tool_result_t *ompt_tool_darwin(unsigned int omp_version,\n                                                  const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  // Search symbol in the current address space.\n  ompt_start_tool_t start_tool =\n      (ompt_start_tool_t)dlsym(RTLD_DEFAULT, \"ompt_start_tool\");\n  if (start_tool) {\n    ret = start_tool(omp_version, runtime_version);\n  }\n  return ret;\n}\n\n#elif OMPT_HAVE_WEAK_ATTRIBUTE\n\n// On Unix-like systems that support weak symbols the following implementation\n// of ompt_start_tool() will be used in case no tool-supplied implementation of\n// this function is present in the address space of a process.\n\n_OMP_EXTERN OMPT_WEAK_ATTRIBUTE ompt_start_tool_result_t *\nompt_start_tool(unsigned int omp_version, const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  // Search next symbol in the current address space. This can happen if the\n  // runtime library is linked before the tool. Since glibc 2.2 strong symbols\n  // don't override weak symbols that have been found before unless the user\n  // sets the environment variable LD_DYNAMIC_WEAK.\n  ompt_start_tool_t next_tool =\n      (ompt_start_tool_t)dlsym(RTLD_NEXT, \"ompt_start_tool\");\n  if (next_tool) {\n    ret = next_tool(omp_version, runtime_version);\n  }\n  return ret;\n}\n\n#elif OMPT_HAVE_PSAPI\n\n// On Windows, the ompt_tool_windows function is used to find the\n// ompt_start_tool symbol across all modules loaded by a process. If\n// ompt_start_tool is found, ompt_start_tool's return value is used to\n// initialize the tool. Otherwise, NULL is returned and OMPT won't be enabled.\n\n#include <psapi.h>\n#pragma comment(lib, \"psapi.lib\")\n\n// The number of loaded modules to start enumeration with EnumProcessModules()\n#define NUM_MODULES 128\n\nstatic ompt_start_tool_result_t *\nompt_tool_windows(unsigned int omp_version, const char *runtime_version) {\n  int i;\n  DWORD needed, new_size;\n  HMODULE *modules;\n  HANDLE process = GetCurrentProcess();\n  modules = (HMODULE *)malloc(NUM_MODULES * sizeof(HMODULE));\n  ompt_start_tool_t ompt_tool_p = NULL;\n\n#if OMPT_DEBUG\n  printf(\"ompt_tool_windows(): looking for ompt_start_tool\\n\");\n#endif\n  if (!EnumProcessModules(process, modules, NUM_MODULES * sizeof(HMODULE),\n                          &needed)) {\n    // Regardless of the error reason use the stub initialization function\n    free(modules);\n    return NULL;\n  }\n  // Check if NUM_MODULES is enough to list all modules\n  new_size = needed / sizeof(HMODULE);\n  if (new_size > NUM_MODULES) {\n#if OMPT_DEBUG\n    printf(\"ompt_tool_windows(): resize buffer to %d bytes\\n\", needed);\n#endif\n    modules = (HMODULE *)realloc(modules, needed);\n    // If resizing failed use the stub function.\n    if (!EnumProcessModules(process, modules, needed, &needed)) {\n      free(modules);\n      return NULL;\n    }\n  }\n  for (i = 0; i < new_size; ++i) {\n    (FARPROC &)ompt_tool_p = GetProcAddress(modules[i], \"ompt_start_tool\");\n    if (ompt_tool_p) {\n#if OMPT_DEBUG\n      TCHAR modName[MAX_PATH];\n      if (GetModuleFileName(modules[i], modName, MAX_PATH))\n        printf(\"ompt_tool_windows(): ompt_start_tool found in module %s\\n\",\n               modName);\n#endif\n      free(modules);\n      return (*ompt_tool_p)(omp_version, runtime_version);\n    }\n#if OMPT_DEBUG\n    else {\n      TCHAR modName[MAX_PATH];\n      if (GetModuleFileName(modules[i], modName, MAX_PATH))\n        printf(\"ompt_tool_windows(): ompt_start_tool not found in module %s\\n\",\n               modName);\n    }\n#endif\n  }\n  free(modules);\n  return NULL;\n}\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n\nstatic ompt_start_tool_result_t *\nompt_try_start_tool(unsigned int omp_version, const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  ompt_start_tool_t start_tool = NULL;\n#if KMP_OS_WINDOWS\n  // Cannot use colon to describe a list of absolute paths on Windows\n  const char *sep = \";\";\n#else\n  const char *sep = \":\";\n#endif\n\n#if KMP_OS_DARWIN\n  // Try in the current address space\n  ret = ompt_tool_darwin(omp_version, runtime_version);\n#elif OMPT_HAVE_WEAK_ATTRIBUTE\n  ret = ompt_start_tool(omp_version, runtime_version);\n#elif OMPT_HAVE_PSAPI\n  ret = ompt_tool_windows(omp_version, runtime_version);\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n  if (ret)\n    return ret;\n\n  // Try tool-libraries-var ICV\n  const char *tool_libs = getenv(\"OMP_TOOL_LIBRARIES\");\n  if (tool_libs) {\n    char *libs = __kmp_str_format(\"%s\", tool_libs);\n    char *buf;\n    char *fname = __kmp_str_token(libs, sep, &buf);\n    while (fname) {\n#if KMP_OS_UNIX\n      void *h = dlopen(fname, RTLD_LAZY);\n      if (h) {\n        start_tool = (ompt_start_tool_t)dlsym(h, \"ompt_start_tool\");\n#elif KMP_OS_WINDOWS\n      HMODULE h = LoadLibrary(fname);\n      if (h) {\n        start_tool = (ompt_start_tool_t)GetProcAddress(h, \"ompt_start_tool\");\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n        if (start_tool && (ret = (*start_tool)(omp_version, runtime_version)))\n          break;\n      }\n      fname = __kmp_str_token(NULL, sep, &buf);\n    }\n    __kmp_str_free(&libs);\n  }\n  if (ret)\n    return ret;\n\n#if KMP_OS_UNIX\n  { // Non-standard: load archer tool if application is built with TSan\n    const char *fname = \"libarcher.so\";\n    void *h = dlopen(fname, RTLD_LAZY);\n    if (h) {\n      start_tool = (ompt_start_tool_t)dlsym(h, \"ompt_start_tool\");\n      if (start_tool)\n        ret = (*start_tool)(omp_version, runtime_version);\n      if (ret)\n        return ret;\n    }\n  }\n#endif\n  return ret;\n}\n\nvoid ompt_pre_init() {\n  //--------------------------------------------------\n  // Execute the pre-initialization logic only once.\n  //--------------------------------------------------\n  static int ompt_pre_initialized = 0;\n\n  if (ompt_pre_initialized)\n    return;\n\n  ompt_pre_initialized = 1;\n\n  //--------------------------------------------------\n  // Use a tool iff a tool is enabled and available.\n  //--------------------------------------------------\n  const char *ompt_env_var = getenv(\"OMP_TOOL\");\n  tool_setting_e tool_setting = omp_tool_error;\n\n  if (!ompt_env_var || !strcmp(ompt_env_var, \"\"))\n    tool_setting = omp_tool_unset;\n  else if (OMPT_STR_MATCH(ompt_env_var, \"disabled\"))\n    tool_setting = omp_tool_disabled;\n  else if (OMPT_STR_MATCH(ompt_env_var, \"enabled\"))\n    tool_setting = omp_tool_enabled;\n\n#if OMPT_DEBUG\n  printf(\"ompt_pre_init(): tool_setting = %d\\n\", tool_setting);\n#endif\n  switch (tool_setting) {\n  case omp_tool_disabled:\n    break;\n\n  case omp_tool_unset:\n  case omp_tool_enabled:\n\n    //--------------------------------------------------\n    // Load tool iff specified in environment variable\n    //--------------------------------------------------\n    ompt_start_tool_result =\n        ompt_try_start_tool(__kmp_openmp_version, ompt_get_runtime_version());\n\n    memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n    break;\n\n  case omp_tool_error:\n    fprintf(stderr, \"Warning: OMP_TOOL has invalid value \\\"%s\\\".\\n\"\n                    \"  legal values are (NULL,\\\"\\\",\\\"disabled\\\",\"\n                    \"\\\"enabled\\\").\\n\",\n            ompt_env_var);\n    break;\n  }\n#if OMPT_DEBUG\n  printf(\"ompt_pre_init(): ompt_enabled = %d\\n\", ompt_enabled);\n#endif\n}\n\nextern \"C\" int omp_get_initial_device(void);\n\nvoid ompt_post_init() {\n  //--------------------------------------------------\n  // Execute the post-initialization logic only once.\n  //--------------------------------------------------\n  static int ompt_post_initialized = 0;\n\n  if (ompt_post_initialized)\n    return;\n\n  ompt_post_initialized = 1;\n\n  //--------------------------------------------------\n  // Initialize the tool if so indicated.\n  //--------------------------------------------------\n  if (ompt_start_tool_result) {\n    ompt_enabled.enabled = !!ompt_start_tool_result->initialize(\n        ompt_fn_lookup, omp_get_initial_device(), &(ompt_start_tool_result->tool_data));\n\n    if (!ompt_enabled.enabled) {\n      // tool not enabled, zero out the bitmap, and done\n      memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n      return;\n    }\n\n    kmp_info_t *root_thread = ompt_get_thread();\n\n    ompt_set_thread_state(root_thread, ompt_state_overhead);\n\n    if (ompt_enabled.ompt_callback_thread_begin) {\n      ompt_callbacks.ompt_callback(ompt_callback_thread_begin)(\n          ompt_thread_initial, __ompt_get_thread_data_internal());\n    }\n    ompt_data_t *task_data;\n    ompt_data_t *parallel_data;\n    __ompt_get_task_info_internal(0, NULL, &task_data, NULL, &parallel_data, NULL);\n    if (ompt_enabled.ompt_callback_implicit_task) {\n      ompt_callbacks.ompt_callback(ompt_callback_implicit_task)(\n          ompt_scope_begin, parallel_data, task_data, 1, 1, ompt_task_initial);\n    }\n\n    ompt_set_thread_state(root_thread, ompt_state_work_serial);\n  }\n}\n\nvoid ompt_fini() {\n  if (ompt_enabled.enabled) {\n    ompt_start_tool_result->finalize(&(ompt_start_tool_result->tool_data));\n  }\n\n  memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n}\n\n/*****************************************************************************\n * interface operations\n ****************************************************************************/\n\n/*****************************************************************************\n * state\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_enumerate_states(int current_state, int *next_state,\n                                           const char **next_state_name) {\n  const static int len = sizeof(ompt_state_info) / sizeof(ompt_state_info_t);\n  int i = 0;\n\n  for (i = 0; i < len - 1; i++) {\n    if (ompt_state_info[i].state_id == current_state) {\n      *next_state = ompt_state_info[i + 1].state_id;\n      *next_state_name = ompt_state_info[i + 1].state_name;\n      return 1;\n    }\n  }\n\n  return 0;\n}\n\nOMPT_API_ROUTINE int ompt_enumerate_mutex_impls(int current_impl,\n                                                int *next_impl,\n                                                const char **next_impl_name) {\n  const static int len =\n      sizeof(kmp_mutex_impl_info) / sizeof(kmp_mutex_impl_info_t);\n  int i = 0;\n  for (i = 0; i < len - 1; i++) {\n    if (kmp_mutex_impl_info[i].id != current_impl)\n      continue;\n    *next_impl = kmp_mutex_impl_info[i + 1].id;\n    *next_impl_name = kmp_mutex_impl_info[i + 1].name;\n    return 1;\n  }\n  return 0;\n}\n\n/*****************************************************************************\n * callbacks\n ****************************************************************************/\n\nOMPT_API_ROUTINE ompt_set_result_t ompt_set_callback(ompt_callbacks_t which,\n                                       ompt_callback_t callback) {\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case event_name:                                                             \\\n    ompt_callbacks.ompt_callback(event_name) = (callback_type)callback;        \\\n    ompt_enabled.event_name = (callback != 0);                                 \\\n    if (callback)                                                              \\\n      return ompt_event_implementation_status(event_name);                     \\\n    else                                                                       \\\n      return ompt_set_always;\n\n    FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_set_error;\n  }\n}\n\nOMPT_API_ROUTINE int ompt_get_callback(ompt_callbacks_t which,\n                                       ompt_callback_t *callback) {\n  if (!ompt_enabled.enabled)\n    return ompt_get_callback_failure;\n\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case event_name: {                                                           \\\n    ompt_callback_t mycb =                                                     \\\n        (ompt_callback_t)ompt_callbacks.ompt_callback(event_name);             \\\n    if (ompt_enabled.event_name && mycb) {                                     \\\n      *callback = mycb;                                                        \\\n      return ompt_get_callback_success;                                        \\\n    }                                                                          \\\n    return ompt_get_callback_failure;                                          \\\n  }\n\n    FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_get_callback_failure;\n  }\n}\n\n/*****************************************************************************\n * parallel regions\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_parallel_info(int ancestor_level,\n                                            ompt_data_t **parallel_data,\n                                            int *team_size) {\n  if (!ompt_enabled.enabled)\n    return 0;\n  return __ompt_get_parallel_info_internal(ancestor_level, parallel_data,\n                                           team_size);\n}\n\nOMPT_API_ROUTINE int ompt_get_state(ompt_wait_id_t *wait_id) {\n  if (!ompt_enabled.enabled)\n    return ompt_state_work_serial;\n  int thread_state = __ompt_get_state_internal(wait_id);\n\n  if (thread_state == ompt_state_undefined) {\n    thread_state = ompt_state_work_serial;\n  }\n\n  return thread_state;\n}\n\n/*****************************************************************************\n * tasks\n ****************************************************************************/\n\nOMPT_API_ROUTINE ompt_data_t *ompt_get_thread_data(void) {\n  if (!ompt_enabled.enabled)\n    return NULL;\n  return __ompt_get_thread_data_internal();\n}\n\nOMPT_API_ROUTINE int ompt_get_task_info(int ancestor_level, int *type,\n                                        ompt_data_t **task_data,\n                                        ompt_frame_t **task_frame,\n                                        ompt_data_t **parallel_data,\n                                        int *thread_num) {\n  if (!ompt_enabled.enabled)\n    return 0;\n  return __ompt_get_task_info_internal(ancestor_level, type, task_data,\n                                       task_frame, parallel_data, thread_num);\n}\n\nOMPT_API_ROUTINE int ompt_get_task_memory(void **addr, size_t *size,\n                                          int block) {\n  return __ompt_get_task_memory_internal(addr, size, block);\n}\n\n/*****************************************************************************\n * num_procs\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_num_procs(void) {\n  // copied from kmp_ftn_entry.h (but modified: OMPT can only be called when\n  // runtime is initialized)\n  return __kmp_avail_proc;\n}\n\n/*****************************************************************************\n * places\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_num_places(void) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  return __kmp_affinity_num_masks;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_place_proc_ids(int place_num, int ids_size,\n                                             int *ids) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  int i, count;\n  int tmp_ids[ids_size];\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  if (place_num < 0 || place_num >= (int)__kmp_affinity_num_masks)\n    return 0;\n  /* TODO: Is this safe for asynchronous call from signal handler during runtime\n   * shutdown? */\n  kmp_affin_mask_t *mask = KMP_CPU_INDEX(__kmp_affinity_masks, place_num);\n  count = 0;\n  KMP_CPU_SET_ITERATE(i, mask) {\n    if ((!KMP_CPU_ISSET(i, __kmp_affin_fullMask)) ||\n        (!KMP_CPU_ISSET(i, mask))) {\n      continue;\n    }\n    if (count < ids_size)\n      tmp_ids[count] = i;\n    count++;\n  }\n  if (ids_size >= count) {\n    for (i = 0; i < count; i++) {\n      ids[i] = tmp_ids[i];\n    }\n  }\n  return count;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_place_num(void) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!ompt_enabled.enabled || __kmp_get_gtid() < 0)\n    return -1;\n\n  int gtid;\n  kmp_info_t *thread;\n  if (!KMP_AFFINITY_CAPABLE())\n    return -1;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread == NULL || thread->th.th_current_place < 0)\n    return -1;\n  return thread->th.th_current_place;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_partition_place_nums(int place_nums_size,\n                                                   int *place_nums) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (!ompt_enabled.enabled || __kmp_get_gtid() < 0)\n    return 0;\n\n  int i, gtid, place_num, first_place, last_place, start, end;\n  kmp_info_t *thread;\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread == NULL)\n    return 0;\n  first_place = thread->th.th_first_place;\n  last_place = thread->th.th_last_place;\n  if (first_place < 0 || last_place < 0)\n    return 0;\n  if (first_place <= last_place) {\n    start = first_place;\n    end = last_place;\n  } else {\n    start = last_place;\n    end = first_place;\n  }\n  if (end - start <= place_nums_size)\n    for (i = 0, place_num = start; place_num <= end; ++place_num, ++i) {\n      place_nums[i] = place_num;\n    }\n  return end - start + 1;\n#endif\n}\n\n/*****************************************************************************\n * places\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_proc_id(void) {\n  if (!ompt_enabled.enabled || __kmp_get_gtid() < 0)\n    return -1;\n#if KMP_OS_LINUX\n  return sched_getcpu();\n#elif KMP_OS_WINDOWS\n  PROCESSOR_NUMBER pn;\n  GetCurrentProcessorNumberEx(&pn);\n  return 64 * pn.Group + pn.Number;\n#else\n  return -1;\n#endif\n}\n\n/*****************************************************************************\n * compatability\n ****************************************************************************/\n\n/*\n * Currently unused function\nOMPT_API_ROUTINE int ompt_get_ompt_version() { return OMPT_VERSION; }\n*/\n\n/*****************************************************************************\n* application-facing API\n ****************************************************************************/\n\n/*----------------------------------------------------------------------------\n | control\n ---------------------------------------------------------------------------*/\n\nint __kmp_control_tool(uint64_t command, uint64_t modifier, void *arg) {\n\n  if (ompt_enabled.enabled) {\n    if (ompt_enabled.ompt_callback_control_tool) {\n      return ompt_callbacks.ompt_callback(ompt_callback_control_tool)(\n          command, modifier, arg, OMPT_LOAD_RETURN_ADDRESS(__kmp_entry_gtid()));\n    } else {\n      return -1;\n    }\n  } else {\n    return -2;\n  }\n}\n\n/*****************************************************************************\n * misc\n ****************************************************************************/\n\nOMPT_API_ROUTINE uint64_t ompt_get_unique_id(void) {\n  return __ompt_get_unique_id_internal();\n}\n\nOMPT_API_ROUTINE void ompt_finalize_tool(void) { __kmp_internal_end_atexit(); }\n\n/*****************************************************************************\n * Target\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_target_info(uint64_t *device_num,\n                                          ompt_id_t *target_id,\n                                          ompt_id_t *host_op_id) {\n  return 0; // thread is not in a target region\n}\n\nOMPT_API_ROUTINE int ompt_get_num_devices(void) {\n  return 1; // only one device (the current device) is available\n}\n\n/*****************************************************************************\n * API inquiry for tool\n ****************************************************************************/\n\nstatic ompt_interface_fn_t ompt_fn_lookup(const char *s) {\n\n#define ompt_interface_fn(fn)                                                  \\\n  fn##_t fn##_f = fn;                                                          \\\n  if (strcmp(s, #fn) == 0)                                                     \\\n    return (ompt_interface_fn_t)fn##_f;\n\n  FOREACH_OMPT_INQUIRY_FN(ompt_interface_fn)\n\n  return (ompt_interface_fn_t)0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/runtime/src/kmp_alloc.cpp": "/*\n * kmp_alloc.cpp -- private/shared dynamic memory allocation and management\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#include \"kmp.h\"\n#include \"kmp_io.h\"\n#include \"kmp_wrapper_malloc.h\"\n\n// Disable bget when it is not used\n#if KMP_USE_BGET\n\n/* Thread private buffer management code */\n\ntypedef int (*bget_compact_t)(size_t, int);\ntypedef void *(*bget_acquire_t)(size_t);\ntypedef void (*bget_release_t)(void *);\n\n/* NOTE: bufsize must be a signed datatype */\n\n#if KMP_OS_WINDOWS\n#if KMP_ARCH_X86 || KMP_ARCH_ARM\ntypedef kmp_int32 bufsize;\n#else\ntypedef kmp_int64 bufsize;\n#endif\n#else\ntypedef ssize_t bufsize;\n#endif // KMP_OS_WINDOWS\n\n/* The three modes of operation are, fifo search, lifo search, and best-fit */\n\ntypedef enum bget_mode {\n  bget_mode_fifo = 0,\n  bget_mode_lifo = 1,\n  bget_mode_best = 2\n} bget_mode_t;\n\nstatic void bpool(kmp_info_t *th, void *buffer, bufsize len);\nstatic void *bget(kmp_info_t *th, bufsize size);\nstatic void *bgetz(kmp_info_t *th, bufsize size);\nstatic void *bgetr(kmp_info_t *th, void *buffer, bufsize newsize);\nstatic void brel(kmp_info_t *th, void *buf);\nstatic void bectl(kmp_info_t *th, bget_compact_t compact,\n                  bget_acquire_t acquire, bget_release_t release,\n                  bufsize pool_incr);\n\n/* BGET CONFIGURATION */\n/* Buffer allocation size quantum: all buffers allocated are a\n   multiple of this size.  This MUST be a power of two. */\n\n/* On IA-32 architecture with  Linux* OS, malloc() does not\n   ensure 16 byte alignment */\n\n#if KMP_ARCH_X86 || !KMP_HAVE_QUAD\n\n#define SizeQuant 8\n#define AlignType double\n\n#else\n\n#define SizeQuant 16\n#define AlignType _Quad\n\n#endif\n\n// Define this symbol to enable the bstats() function which calculates the\n// total free space in the buffer pool, the largest available buffer, and the\n// total space currently allocated.\n#define BufStats 1\n\n#ifdef KMP_DEBUG\n\n// Define this symbol to enable the bpoold() function which dumps the buffers\n// in a buffer pool.\n#define BufDump 1\n\n// Define this symbol to enable the bpoolv() function for validating a buffer\n// pool.\n#define BufValid 1\n\n// Define this symbol to enable the bufdump() function which allows dumping the\n// contents of an allocated or free buffer.\n#define DumpData 1\n\n#ifdef NOT_USED_NOW\n\n// Wipe free buffers to a guaranteed pattern of garbage to trip up miscreants\n// who attempt to use pointers into released buffers.\n#define FreeWipe 1\n\n// Use a best fit algorithm when searching for space for an allocation request.\n// This uses memory more efficiently, but allocation will be much slower.\n#define BestFit 1\n\n#endif /* NOT_USED_NOW */\n#endif /* KMP_DEBUG */\n\nstatic bufsize bget_bin_size[] = {\n    0,\n    //    1 << 6,    /* .5 Cache line */\n    1 << 7, /* 1 Cache line, new */\n    1 << 8, /* 2 Cache lines */\n    1 << 9, /* 4 Cache lines, new */\n    1 << 10, /* 8 Cache lines */\n    1 << 11, /* 16 Cache lines, new */\n    1 << 12, 1 << 13, /* new */\n    1 << 14, 1 << 15, /* new */\n    1 << 16, 1 << 17, 1 << 18, 1 << 19, 1 << 20, /*  1MB */\n    1 << 21, /*  2MB */\n    1 << 22, /*  4MB */\n    1 << 23, /*  8MB */\n    1 << 24, /* 16MB */\n    1 << 25, /* 32MB */\n};\n\n#define MAX_BGET_BINS (int)(sizeof(bget_bin_size) / sizeof(bufsize))\n\nstruct bfhead;\n\n//  Declare the interface, including the requested buffer size type, bufsize.\n\n/* Queue links */\ntypedef struct qlinks {\n  struct bfhead *flink; /* Forward link */\n  struct bfhead *blink; /* Backward link */\n} qlinks_t;\n\n/* Header in allocated and free buffers */\ntypedef struct bhead2 {\n  kmp_info_t *bthr; /* The thread which owns the buffer pool */\n  bufsize prevfree; /* Relative link back to previous free buffer in memory or\n                       0 if previous buffer is allocated.  */\n  bufsize bsize; /* Buffer size: positive if free, negative if allocated. */\n} bhead2_t;\n\n/* Make sure the bhead structure is a multiple of SizeQuant in size. */\ntypedef union bhead {\n  KMP_ALIGN(SizeQuant)\n  AlignType b_align;\n  char b_pad[sizeof(bhead2_t) + (SizeQuant - (sizeof(bhead2_t) % SizeQuant))];\n  bhead2_t bb;\n} bhead_t;\n#define BH(p) ((bhead_t *)(p))\n\n/*  Header in directly allocated buffers (by acqfcn) */\ntypedef struct bdhead {\n  bufsize tsize; /* Total size, including overhead */\n  bhead_t bh; /* Common header */\n} bdhead_t;\n#define BDH(p) ((bdhead_t *)(p))\n\n/* Header in free buffers */\ntypedef struct bfhead {\n  bhead_t bh; /* Common allocated/free header */\n  qlinks_t ql; /* Links on free list */\n} bfhead_t;\n#define BFH(p) ((bfhead_t *)(p))\n\ntypedef struct thr_data {\n  bfhead_t freelist[MAX_BGET_BINS];\n#if BufStats\n  size_t totalloc; /* Total space currently allocated */\n  long numget, numrel; /* Number of bget() and brel() calls */\n  long numpblk; /* Number of pool blocks */\n  long numpget, numprel; /* Number of block gets and rels */\n  long numdget, numdrel; /* Number of direct gets and rels */\n#endif /* BufStats */\n\n  /* Automatic expansion block management functions */\n  bget_compact_t compfcn;\n  bget_acquire_t acqfcn;\n  bget_release_t relfcn;\n\n  bget_mode_t mode; /* what allocation mode to use? */\n\n  bufsize exp_incr; /* Expansion block size */\n  bufsize pool_len; /* 0: no bpool calls have been made\n                       -1: not all pool blocks are the same size\n                       >0: (common) block size for all bpool calls made so far\n                    */\n  bfhead_t *last_pool; /* Last pool owned by this thread (delay deallocation) */\n} thr_data_t;\n\n/*  Minimum allocation quantum: */\n#define QLSize (sizeof(qlinks_t))\n#define SizeQ ((SizeQuant > QLSize) ? SizeQuant : QLSize)\n#define MaxSize                                                                \\\n  (bufsize)(                                                                   \\\n      ~(((bufsize)(1) << (sizeof(bufsize) * CHAR_BIT - 1)) | (SizeQuant - 1)))\n// Maximum for the requested size.\n\n/* End sentinel: value placed in bsize field of dummy block delimiting\n   end of pool block.  The most negative number which will  fit  in  a\n   bufsize, defined in a way that the compiler will accept. */\n\n#define ESent                                                                  \\\n  ((bufsize)(-(((((bufsize)1) << ((int)sizeof(bufsize) * 8 - 2)) - 1) * 2) - 2))\n\n/* Thread Data management routines */\nstatic int bget_get_bin(bufsize size) {\n  // binary chop bins\n  int lo = 0, hi = MAX_BGET_BINS - 1;\n\n  KMP_DEBUG_ASSERT(size > 0);\n\n  while ((hi - lo) > 1) {\n    int mid = (lo + hi) >> 1;\n    if (size < bget_bin_size[mid])\n      hi = mid - 1;\n    else\n      lo = mid;\n  }\n\n  KMP_DEBUG_ASSERT((lo >= 0) && (lo < MAX_BGET_BINS));\n\n  return lo;\n}\n\nstatic void set_thr_data(kmp_info_t *th) {\n  int i;\n  thr_data_t *data;\n\n  data = (thr_data_t *)((!th->th.th_local.bget_data)\n                            ? __kmp_allocate(sizeof(*data))\n                            : th->th.th_local.bget_data);\n\n  memset(data, '\\0', sizeof(*data));\n\n  for (i = 0; i < MAX_BGET_BINS; ++i) {\n    data->freelist[i].ql.flink = &data->freelist[i];\n    data->freelist[i].ql.blink = &data->freelist[i];\n  }\n\n  th->th.th_local.bget_data = data;\n  th->th.th_local.bget_list = 0;\n#if !USE_CMP_XCHG_FOR_BGET\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n  __kmp_init_lock(&th->th.th_local.bget_lock);\n#else\n  __kmp_init_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif /* USE_LOCK_FOR_BGET */\n#endif /* ! USE_CMP_XCHG_FOR_BGET */\n}\n\nstatic thr_data_t *get_thr_data(kmp_info_t *th) {\n  thr_data_t *data;\n\n  data = (thr_data_t *)th->th.th_local.bget_data;\n\n  KMP_DEBUG_ASSERT(data != 0);\n\n  return data;\n}\n\n/* Walk the free list and release the enqueued buffers */\nstatic void __kmp_bget_dequeue(kmp_info_t *th) {\n  void *p = TCR_SYNC_PTR(th->th.th_local.bget_list);\n\n  if (p != 0) {\n#if USE_CMP_XCHG_FOR_BGET\n    {\n      volatile void *old_value = TCR_SYNC_PTR(th->th.th_local.bget_list);\n      while (!KMP_COMPARE_AND_STORE_PTR(&th->th.th_local.bget_list,\n                                        CCAST(void *, old_value), nullptr)) {\n        KMP_CPU_PAUSE();\n        old_value = TCR_SYNC_PTR(th->th.th_local.bget_list);\n      }\n      p = CCAST(void *, old_value);\n    }\n#else /* ! USE_CMP_XCHG_FOR_BGET */\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n    __kmp_acquire_lock(&th->th.th_local.bget_lock, __kmp_gtid_from_thread(th));\n#else\n    __kmp_acquire_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif /* USE_QUEUING_LOCK_FOR_BGET */\n\n    p = (void *)th->th.th_local.bget_list;\n    th->th.th_local.bget_list = 0;\n\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n    __kmp_release_lock(&th->th.th_local.bget_lock, __kmp_gtid_from_thread(th));\n#else\n    __kmp_release_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif\n#endif /* USE_CMP_XCHG_FOR_BGET */\n\n    /* Check again to make sure the list is not empty */\n    while (p != 0) {\n      void *buf = p;\n      bfhead_t *b = BFH(((char *)p) - sizeof(bhead_t));\n\n      KMP_DEBUG_ASSERT(b->bh.bb.bsize != 0);\n      KMP_DEBUG_ASSERT(((kmp_uintptr_t)TCR_PTR(b->bh.bb.bthr) & ~1) ==\n                       (kmp_uintptr_t)th); // clear possible mark\n      KMP_DEBUG_ASSERT(b->ql.blink == 0);\n\n      p = (void *)b->ql.flink;\n\n      brel(th, buf);\n    }\n  }\n}\n\n/* Chain together the free buffers by using the thread owner field */\nstatic void __kmp_bget_enqueue(kmp_info_t *th, void *buf\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n                               ,\n                               kmp_int32 rel_gtid\n#endif\n                               ) {\n  bfhead_t *b = BFH(((char *)buf) - sizeof(bhead_t));\n\n  KMP_DEBUG_ASSERT(b->bh.bb.bsize != 0);\n  KMP_DEBUG_ASSERT(((kmp_uintptr_t)TCR_PTR(b->bh.bb.bthr) & ~1) ==\n                   (kmp_uintptr_t)th); // clear possible mark\n\n  b->ql.blink = 0;\n\n  KC_TRACE(10, (\"__kmp_bget_enqueue: moving buffer to T#%d list\\n\",\n                __kmp_gtid_from_thread(th)));\n\n#if USE_CMP_XCHG_FOR_BGET\n  {\n    volatile void *old_value = TCR_PTR(th->th.th_local.bget_list);\n    /* the next pointer must be set before setting bget_list to buf to avoid\n       exposing a broken list to other threads, even for an instant. */\n    b->ql.flink = BFH(CCAST(void *, old_value));\n\n    while (!KMP_COMPARE_AND_STORE_PTR(&th->th.th_local.bget_list,\n                                      CCAST(void *, old_value), buf)) {\n      KMP_CPU_PAUSE();\n      old_value = TCR_PTR(th->th.th_local.bget_list);\n      /* the next pointer must be set before setting bget_list to buf to avoid\n         exposing a broken list to other threads, even for an instant. */\n      b->ql.flink = BFH(CCAST(void *, old_value));\n    }\n  }\n#else /* ! USE_CMP_XCHG_FOR_BGET */\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n  __kmp_acquire_lock(&th->th.th_local.bget_lock, rel_gtid);\n#else\n  __kmp_acquire_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif\n\n  b->ql.flink = BFH(th->th.th_local.bget_list);\n  th->th.th_local.bget_list = (void *)buf;\n\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n  __kmp_release_lock(&th->th.th_local.bget_lock, rel_gtid);\n#else\n  __kmp_release_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif\n#endif /* USE_CMP_XCHG_FOR_BGET */\n}\n\n/* insert buffer back onto a new freelist */\nstatic void __kmp_bget_insert_into_freelist(thr_data_t *thr, bfhead_t *b) {\n  int bin;\n\n  KMP_DEBUG_ASSERT(((size_t)b) % SizeQuant == 0);\n  KMP_DEBUG_ASSERT(b->bh.bb.bsize % SizeQuant == 0);\n\n  bin = bget_get_bin(b->bh.bb.bsize);\n\n  KMP_DEBUG_ASSERT(thr->freelist[bin].ql.blink->ql.flink ==\n                   &thr->freelist[bin]);\n  KMP_DEBUG_ASSERT(thr->freelist[bin].ql.flink->ql.blink ==\n                   &thr->freelist[bin]);\n\n  b->ql.flink = &thr->freelist[bin];\n  b->ql.blink = thr->freelist[bin].ql.blink;\n\n  thr->freelist[bin].ql.blink = b;\n  b->ql.blink->ql.flink = b;\n}\n\n/* unlink the buffer from the old freelist */\nstatic void __kmp_bget_remove_from_freelist(bfhead_t *b) {\n  KMP_DEBUG_ASSERT(b->ql.blink->ql.flink == b);\n  KMP_DEBUG_ASSERT(b->ql.flink->ql.blink == b);\n\n  b->ql.blink->ql.flink = b->ql.flink;\n  b->ql.flink->ql.blink = b->ql.blink;\n}\n\n/*  GET STATS -- check info on free list */\nstatic void bcheck(kmp_info_t *th, bufsize *max_free, bufsize *total_free) {\n  thr_data_t *thr = get_thr_data(th);\n  int bin;\n\n  *total_free = *max_free = 0;\n\n  for (bin = 0; bin < MAX_BGET_BINS; ++bin) {\n    bfhead_t *b, *best;\n\n    best = &thr->freelist[bin];\n    b = best->ql.flink;\n\n    while (b != &thr->freelist[bin]) {\n      *total_free += (b->bh.bb.bsize - sizeof(bhead_t));\n      if ((best == &thr->freelist[bin]) || (b->bh.bb.bsize < best->bh.bb.bsize))\n        best = b;\n\n      /* Link to next buffer */\n      b = b->ql.flink;\n    }\n\n    if (*max_free < best->bh.bb.bsize)\n      *max_free = best->bh.bb.bsize;\n  }\n\n  if (*max_free > (bufsize)sizeof(bhead_t))\n    *max_free -= sizeof(bhead_t);\n}\n\n/*  BGET  --  Allocate a buffer.  */\nstatic void *bget(kmp_info_t *th, bufsize requested_size) {\n  thr_data_t *thr = get_thr_data(th);\n  bufsize size = requested_size;\n  bfhead_t *b;\n  void *buf;\n  int compactseq = 0;\n  int use_blink = 0;\n  /* For BestFit */\n  bfhead_t *best;\n\n  if (size < 0 || size + sizeof(bhead_t) > MaxSize) {\n    return NULL;\n  }\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n  if (size < (bufsize)SizeQ) { // Need at least room for the queue links.\n    size = SizeQ;\n  }\n#if defined(SizeQuant) && (SizeQuant > 1)\n  size = (size + (SizeQuant - 1)) & (~(SizeQuant - 1));\n#endif\n\n  size += sizeof(bhead_t); // Add overhead in allocated buffer to size required.\n  KMP_DEBUG_ASSERT(size >= 0);\n  KMP_DEBUG_ASSERT(size % SizeQuant == 0);\n\n  use_blink = (thr->mode == bget_mode_lifo);\n\n  /* If a compact function was provided in the call to bectl(), wrap\n     a loop around the allocation process  to  allow  compaction  to\n     intervene in case we don't find a suitable buffer in the chain. */\n\n  for (;;) {\n    int bin;\n\n    for (bin = bget_get_bin(size); bin < MAX_BGET_BINS; ++bin) {\n      /* Link to next buffer */\n      b = (use_blink ? thr->freelist[bin].ql.blink\n                     : thr->freelist[bin].ql.flink);\n\n      if (thr->mode == bget_mode_best) {\n        best = &thr->freelist[bin];\n\n        /* Scan the free list searching for the first buffer big enough\n           to hold the requested size buffer. */\n        while (b != &thr->freelist[bin]) {\n          if (b->bh.bb.bsize >= (bufsize)size) {\n            if ((best == &thr->freelist[bin]) ||\n                (b->bh.bb.bsize < best->bh.bb.bsize)) {\n              best = b;\n            }\n          }\n\n          /* Link to next buffer */\n          b = (use_blink ? b->ql.blink : b->ql.flink);\n        }\n        b = best;\n      }\n\n      while (b != &thr->freelist[bin]) {\n        if ((bufsize)b->bh.bb.bsize >= (bufsize)size) {\n\n          // Buffer is big enough to satisfy the request. Allocate it to the\n          // caller. We must decide whether the buffer is large enough to split\n          // into the part given to the caller and a free buffer that remains\n          // on the free list, or whether the entire buffer should be removed\n          // from the free list and given to the caller in its entirety. We\n          // only split the buffer if enough room remains for a header plus the\n          // minimum quantum of allocation.\n          if ((b->bh.bb.bsize - (bufsize)size) >\n              (bufsize)(SizeQ + (sizeof(bhead_t)))) {\n            bhead_t *ba, *bn;\n\n            ba = BH(((char *)b) + (b->bh.bb.bsize - (bufsize)size));\n            bn = BH(((char *)ba) + size);\n\n            KMP_DEBUG_ASSERT(bn->bb.prevfree == b->bh.bb.bsize);\n\n            /* Subtract size from length of free block. */\n            b->bh.bb.bsize -= (bufsize)size;\n\n            /* Link allocated buffer to the previous free buffer. */\n            ba->bb.prevfree = b->bh.bb.bsize;\n\n            /* Plug negative size into user buffer. */\n            ba->bb.bsize = -size;\n\n            /* Mark this buffer as owned by this thread. */\n            TCW_PTR(ba->bb.bthr,\n                    th); // not an allocated address (do not mark it)\n            /* Mark buffer after this one not preceded by free block. */\n            bn->bb.prevfree = 0;\n\n            // unlink buffer from old freelist, and reinsert into new freelist\n            __kmp_bget_remove_from_freelist(b);\n            __kmp_bget_insert_into_freelist(thr, b);\n#if BufStats\n            thr->totalloc += (size_t)size;\n            thr->numget++; /* Increment number of bget() calls */\n#endif\n            buf = (void *)((((char *)ba) + sizeof(bhead_t)));\n            KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n            return buf;\n          } else {\n            bhead_t *ba;\n\n            ba = BH(((char *)b) + b->bh.bb.bsize);\n\n            KMP_DEBUG_ASSERT(ba->bb.prevfree == b->bh.bb.bsize);\n\n            /* The buffer isn't big enough to split.  Give  the  whole\n               shebang to the caller and remove it from the free list. */\n\n            __kmp_bget_remove_from_freelist(b);\n#if BufStats\n            thr->totalloc += (size_t)b->bh.bb.bsize;\n            thr->numget++; /* Increment number of bget() calls */\n#endif\n            /* Negate size to mark buffer allocated. */\n            b->bh.bb.bsize = -(b->bh.bb.bsize);\n\n            /* Mark this buffer as owned by this thread. */\n            TCW_PTR(ba->bb.bthr, th); // not an allocated address (do not mark)\n            /* Zero the back pointer in the next buffer in memory\n               to indicate that this buffer is allocated. */\n            ba->bb.prevfree = 0;\n\n            /* Give user buffer starting at queue links. */\n            buf = (void *)&(b->ql);\n            KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n            return buf;\n          }\n        }\n\n        /* Link to next buffer */\n        b = (use_blink ? b->ql.blink : b->ql.flink);\n      }\n    }\n\n    /* We failed to find a buffer. If there's a compact function defined,\n       notify it of the size requested. If it returns TRUE, try the allocation\n       again. */\n\n    if ((thr->compfcn == 0) || (!(*thr->compfcn)(size, ++compactseq))) {\n      break;\n    }\n  }\n\n  /* No buffer available with requested size free. */\n\n  /* Don't give up yet -- look in the reserve supply. */\n  if (thr->acqfcn != 0) {\n    if (size > (bufsize)(thr->exp_incr - sizeof(bhead_t))) {\n      /* Request is too large to fit in a single expansion block.\n         Try to satisfy it by a direct buffer acquisition. */\n      bdhead_t *bdh;\n\n      size += sizeof(bdhead_t) - sizeof(bhead_t);\n\n      KE_TRACE(10, (\"%%%%%% MALLOC( %d )\\n\", (int)size));\n\n      /* richryan */\n      bdh = BDH((*thr->acqfcn)((bufsize)size));\n      if (bdh != NULL) {\n\n        // Mark the buffer special by setting size field of its header to zero.\n        bdh->bh.bb.bsize = 0;\n\n        /* Mark this buffer as owned by this thread. */\n        TCW_PTR(bdh->bh.bb.bthr, th); // don't mark buffer as allocated,\n        // because direct buffer never goes to free list\n        bdh->bh.bb.prevfree = 0;\n        bdh->tsize = size;\n#if BufStats\n        thr->totalloc += (size_t)size;\n        thr->numget++; /* Increment number of bget() calls */\n        thr->numdget++; /* Direct bget() call count */\n#endif\n        buf = (void *)(bdh + 1);\n        KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n        return buf;\n      }\n\n    } else {\n\n      /*  Try to obtain a new expansion block */\n      void *newpool;\n\n      KE_TRACE(10, (\"%%%%%% MALLOCB( %d )\\n\", (int)thr->exp_incr));\n\n      /* richryan */\n      newpool = (*thr->acqfcn)((bufsize)thr->exp_incr);\n      KMP_DEBUG_ASSERT(((size_t)newpool) % SizeQuant == 0);\n      if (newpool != NULL) {\n        bpool(th, newpool, thr->exp_incr);\n        buf = bget(\n            th, requested_size); /* This can't, I say, can't get into a loop. */\n        return buf;\n      }\n    }\n  }\n\n  /*  Still no buffer available */\n\n  return NULL;\n}\n\n/*  BGETZ  --  Allocate a buffer and clear its contents to zero.  We clear\n               the  entire  contents  of  the buffer to zero, not just the\n               region requested by the caller. */\n\nstatic void *bgetz(kmp_info_t *th, bufsize size) {\n  char *buf = (char *)bget(th, size);\n\n  if (buf != NULL) {\n    bhead_t *b;\n    bufsize rsize;\n\n    b = BH(buf - sizeof(bhead_t));\n    rsize = -(b->bb.bsize);\n    if (rsize == 0) {\n      bdhead_t *bd;\n\n      bd = BDH(buf - sizeof(bdhead_t));\n      rsize = bd->tsize - (bufsize)sizeof(bdhead_t);\n    } else {\n      rsize -= sizeof(bhead_t);\n    }\n\n    KMP_DEBUG_ASSERT(rsize >= size);\n\n    (void)memset(buf, 0, (bufsize)rsize);\n  }\n  return ((void *)buf);\n}\n\n/*  BGETR  --  Reallocate a buffer.  This is a minimal implementation,\n               simply in terms of brel()  and  bget().   It  could  be\n               enhanced to allow the buffer to grow into adjacent free\n               blocks and to avoid moving data unnecessarily.  */\n\nstatic void *bgetr(kmp_info_t *th, void *buf, bufsize size) {\n  void *nbuf;\n  bufsize osize; /* Old size of buffer */\n  bhead_t *b;\n\n  nbuf = bget(th, size);\n  if (nbuf == NULL) { /* Acquire new buffer */\n    return NULL;\n  }\n  if (buf == NULL) {\n    return nbuf;\n  }\n  b = BH(((char *)buf) - sizeof(bhead_t));\n  osize = -b->bb.bsize;\n  if (osize == 0) {\n    /*  Buffer acquired directly through acqfcn. */\n    bdhead_t *bd;\n\n    bd = BDH(((char *)buf) - sizeof(bdhead_t));\n    osize = bd->tsize - (bufsize)sizeof(bdhead_t);\n  } else {\n    osize -= sizeof(bhead_t);\n  }\n\n  KMP_DEBUG_ASSERT(osize > 0);\n\n  (void)KMP_MEMCPY((char *)nbuf, (char *)buf, /* Copy the data */\n                   (size_t)((size < osize) ? size : osize));\n  brel(th, buf);\n\n  return nbuf;\n}\n\n/*  BREL  --  Release a buffer.  */\nstatic void brel(kmp_info_t *th, void *buf) {\n  thr_data_t *thr = get_thr_data(th);\n  bfhead_t *b, *bn;\n  kmp_info_t *bth;\n\n  KMP_DEBUG_ASSERT(buf != NULL);\n  KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n\n  b = BFH(((char *)buf) - sizeof(bhead_t));\n\n  if (b->bh.bb.bsize == 0) { /* Directly-acquired buffer? */\n    bdhead_t *bdh;\n\n    bdh = BDH(((char *)buf) - sizeof(bdhead_t));\n    KMP_DEBUG_ASSERT(b->bh.bb.prevfree == 0);\n#if BufStats\n    thr->totalloc -= (size_t)bdh->tsize;\n    thr->numdrel++; /* Number of direct releases */\n    thr->numrel++; /* Increment number of brel() calls */\n#endif /* BufStats */\n#ifdef FreeWipe\n    (void)memset((char *)buf, 0x55, (size_t)(bdh->tsize - sizeof(bdhead_t)));\n#endif /* FreeWipe */\n\n    KE_TRACE(10, (\"%%%%%% FREE( %p )\\n\", (void *)bdh));\n\n    KMP_DEBUG_ASSERT(thr->relfcn != 0);\n    (*thr->relfcn)((void *)bdh); /* Release it directly. */\n    return;\n  }\n\n  bth = (kmp_info_t *)((kmp_uintptr_t)TCR_PTR(b->bh.bb.bthr) &\n                       ~1); // clear possible mark before comparison\n  if (bth != th) {\n    /* Add this buffer to be released by the owning thread later */\n    __kmp_bget_enqueue(bth, buf\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n                       ,\n                       __kmp_gtid_from_thread(th)\n#endif\n                           );\n    return;\n  }\n\n  /* Buffer size must be negative, indicating that the buffer is allocated. */\n  if (b->bh.bb.bsize >= 0) {\n    bn = NULL;\n  }\n  KMP_DEBUG_ASSERT(b->bh.bb.bsize < 0);\n\n  /*  Back pointer in next buffer must be zero, indicating the same thing: */\n\n  KMP_DEBUG_ASSERT(BH((char *)b - b->bh.bb.bsize)->bb.prevfree == 0);\n\n#if BufStats\n  thr->numrel++; /* Increment number of brel() calls */\n  thr->totalloc += (size_t)b->bh.bb.bsize;\n#endif\n\n  /* If the back link is nonzero, the previous buffer is free.  */\n\n  if (b->bh.bb.prevfree != 0) {\n    /* The previous buffer is free. Consolidate this buffer with it by adding\n       the length of this buffer to the previous free buffer. Note that we\n       subtract the size in the buffer being released, since it's negative to\n       indicate that the buffer is allocated. */\n    bufsize size = b->bh.bb.bsize;\n\n    /* Make the previous buffer the one we're working on. */\n    KMP_DEBUG_ASSERT(BH((char *)b - b->bh.bb.prevfree)->bb.bsize ==\n                     b->bh.bb.prevfree);\n    b = BFH(((char *)b) - b->bh.bb.prevfree);\n    b->bh.bb.bsize -= size;\n\n    /* unlink the buffer from the old freelist */\n    __kmp_bget_remove_from_freelist(b);\n  } else {\n    /* The previous buffer isn't allocated. Mark this buffer size as positive\n       (i.e. free) and fall through to place the buffer on the free list as an\n       isolated free block. */\n    b->bh.bb.bsize = -b->bh.bb.bsize;\n  }\n\n  /* insert buffer back onto a new freelist */\n  __kmp_bget_insert_into_freelist(thr, b);\n\n  /* Now we look at the next buffer in memory, located by advancing from\n     the  start  of  this  buffer  by its size, to see if that buffer is\n     free.  If it is, we combine  this  buffer  with  the  next  one  in\n     memory, dechaining the second buffer from the free list. */\n  bn = BFH(((char *)b) + b->bh.bb.bsize);\n  if (bn->bh.bb.bsize > 0) {\n\n    /* The buffer is free.  Remove it from the free list and add\n       its size to that of our buffer. */\n    KMP_DEBUG_ASSERT(BH((char *)bn + bn->bh.bb.bsize)->bb.prevfree ==\n                     bn->bh.bb.bsize);\n\n    __kmp_bget_remove_from_freelist(bn);\n\n    b->bh.bb.bsize += bn->bh.bb.bsize;\n\n    /* unlink the buffer from the old freelist, and reinsert it into the new\n     * freelist */\n    __kmp_bget_remove_from_freelist(b);\n    __kmp_bget_insert_into_freelist(thr, b);\n\n    /* Finally,  advance  to   the  buffer  that   follows  the  newly\n       consolidated free block.  We must set its  backpointer  to  the\n       head  of  the  consolidated free block.  We know the next block\n       must be an allocated block because the process of recombination\n       guarantees  that  two  free  blocks will never be contiguous in\n       memory.  */\n    bn = BFH(((char *)b) + b->bh.bb.bsize);\n  }\n#ifdef FreeWipe\n  (void)memset(((char *)b) + sizeof(bfhead_t), 0x55,\n               (size_t)(b->bh.bb.bsize - sizeof(bfhead_t)));\n#endif\n  KMP_DEBUG_ASSERT(bn->bh.bb.bsize < 0);\n\n  /* The next buffer is allocated.  Set the backpointer in it  to  point\n     to this buffer; the previous free buffer in memory. */\n\n  bn->bh.bb.prevfree = b->bh.bb.bsize;\n\n  /*  If  a  block-release function is defined, and this free buffer\n      constitutes the entire block, release it.  Note that  pool_len\n      is  defined  in  such a way that the test will fail unless all\n      pool blocks are the same size.  */\n  if (thr->relfcn != 0 &&\n      b->bh.bb.bsize == (bufsize)(thr->pool_len - sizeof(bhead_t))) {\n#if BufStats\n    if (thr->numpblk !=\n        1) { /* Do not release the last buffer until finalization time */\n#endif\n\n      KMP_DEBUG_ASSERT(b->bh.bb.prevfree == 0);\n      KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.bsize == ESent);\n      KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.prevfree ==\n                       b->bh.bb.bsize);\n\n      /*  Unlink the buffer from the free list  */\n      __kmp_bget_remove_from_freelist(b);\n\n      KE_TRACE(10, (\"%%%%%% FREE( %p )\\n\", (void *)b));\n\n      (*thr->relfcn)(b);\n#if BufStats\n      thr->numprel++; /* Nr of expansion block releases */\n      thr->numpblk--; /* Total number of blocks */\n      KMP_DEBUG_ASSERT(thr->numpblk == thr->numpget - thr->numprel);\n\n      // avoid leaving stale last_pool pointer around if it is being dealloced\n      if (thr->last_pool == b)\n        thr->last_pool = 0;\n    } else {\n      thr->last_pool = b;\n    }\n#endif /* BufStats */\n  }\n}\n\n/*  BECTL  --  Establish automatic pool expansion control  */\nstatic void bectl(kmp_info_t *th, bget_compact_t compact,\n                  bget_acquire_t acquire, bget_release_t release,\n                  bufsize pool_incr) {\n  thr_data_t *thr = get_thr_data(th);\n\n  thr->compfcn = compact;\n  thr->acqfcn = acquire;\n  thr->relfcn = release;\n  thr->exp_incr = pool_incr;\n}\n\n/*  BPOOL  --  Add a region of memory to the buffer pool.  */\nstatic void bpool(kmp_info_t *th, void *buf, bufsize len) {\n  /*    int bin = 0; */\n  thr_data_t *thr = get_thr_data(th);\n  bfhead_t *b = BFH(buf);\n  bhead_t *bn;\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n#ifdef SizeQuant\n  len &= ~(SizeQuant - 1);\n#endif\n  if (thr->pool_len == 0) {\n    thr->pool_len = len;\n  } else if (len != thr->pool_len) {\n    thr->pool_len = -1;\n  }\n#if BufStats\n  thr->numpget++; /* Number of block acquisitions */\n  thr->numpblk++; /* Number of blocks total */\n  KMP_DEBUG_ASSERT(thr->numpblk == thr->numpget - thr->numprel);\n#endif /* BufStats */\n\n  /* Since the block is initially occupied by a single free  buffer,\n     it  had  better  not  be  (much) larger than the largest buffer\n     whose size we can store in bhead.bb.bsize. */\n  KMP_DEBUG_ASSERT(len - sizeof(bhead_t) <= -((bufsize)ESent + 1));\n\n  /* Clear  the  backpointer at  the start of the block to indicate that\n     there  is  no  free  block  prior  to  this   one.    That   blocks\n     recombination when the first block in memory is released. */\n  b->bh.bb.prevfree = 0;\n\n  /* Create a dummy allocated buffer at the end of the pool.  This dummy\n     buffer is seen when a buffer at the end of the pool is released and\n     blocks  recombination  of  the last buffer with the dummy buffer at\n     the end.  The length in the dummy buffer  is  set  to  the  largest\n     negative  number  to  denote  the  end  of  the pool for diagnostic\n     routines (this specific value is  not  counted  on  by  the  actual\n     allocation and release functions). */\n  len -= sizeof(bhead_t);\n  b->bh.bb.bsize = (bufsize)len;\n  /* Set the owner of this buffer */\n  TCW_PTR(b->bh.bb.bthr,\n          (kmp_info_t *)((kmp_uintptr_t)th |\n                         1)); // mark the buffer as allocated address\n\n  /* Chain the new block to the free list. */\n  __kmp_bget_insert_into_freelist(thr, b);\n\n#ifdef FreeWipe\n  (void)memset(((char *)b) + sizeof(bfhead_t), 0x55,\n               (size_t)(len - sizeof(bfhead_t)));\n#endif\n  bn = BH(((char *)b) + len);\n  bn->bb.prevfree = (bufsize)len;\n  /* Definition of ESent assumes two's complement! */\n  KMP_DEBUG_ASSERT((~0) == -1 && (bn != 0));\n\n  bn->bb.bsize = ESent;\n}\n\n/*  BFREED  --  Dump the free lists for this thread. */\nstatic void bfreed(kmp_info_t *th) {\n  int bin = 0, count = 0;\n  int gtid = __kmp_gtid_from_thread(th);\n  thr_data_t *thr = get_thr_data(th);\n\n#if BufStats\n  __kmp_printf_no_lock(\"__kmp_printpool: T#%d total=%\" KMP_UINT64_SPEC\n                       \" get=%\" KMP_INT64_SPEC \" rel=%\" KMP_INT64_SPEC\n                       \" pblk=%\" KMP_INT64_SPEC \" pget=%\" KMP_INT64_SPEC\n                       \" prel=%\" KMP_INT64_SPEC \" dget=%\" KMP_INT64_SPEC\n                       \" drel=%\" KMP_INT64_SPEC \"\\n\",\n                       gtid, (kmp_uint64)thr->totalloc, (kmp_int64)thr->numget,\n                       (kmp_int64)thr->numrel, (kmp_int64)thr->numpblk,\n                       (kmp_int64)thr->numpget, (kmp_int64)thr->numprel,\n                       (kmp_int64)thr->numdget, (kmp_int64)thr->numdrel);\n#endif\n\n  for (bin = 0; bin < MAX_BGET_BINS; ++bin) {\n    bfhead_t *b;\n\n    for (b = thr->freelist[bin].ql.flink; b != &thr->freelist[bin];\n         b = b->ql.flink) {\n      bufsize bs = b->bh.bb.bsize;\n\n      KMP_DEBUG_ASSERT(b->ql.blink->ql.flink == b);\n      KMP_DEBUG_ASSERT(b->ql.flink->ql.blink == b);\n      KMP_DEBUG_ASSERT(bs > 0);\n\n      count += 1;\n\n      __kmp_printf_no_lock(\n          \"__kmp_printpool: T#%d Free block: 0x%p size %6ld bytes.\\n\", gtid, b,\n          (long)bs);\n#ifdef FreeWipe\n      {\n        char *lerr = ((char *)b) + sizeof(bfhead_t);\n        if ((bs > sizeof(bfhead_t)) &&\n            ((*lerr != 0x55) ||\n             (memcmp(lerr, lerr + 1, (size_t)(bs - (sizeof(bfhead_t) + 1))) !=\n              0))) {\n          __kmp_printf_no_lock(\"__kmp_printpool: T#%d     (Contents of above \"\n                               \"free block have been overstored.)\\n\",\n                               gtid);\n        }\n      }\n#endif\n    }\n  }\n\n  if (count == 0)\n    __kmp_printf_no_lock(\"__kmp_printpool: T#%d No free blocks\\n\", gtid);\n}\n\nvoid __kmp_initialize_bget(kmp_info_t *th) {\n  KMP_DEBUG_ASSERT(SizeQuant >= sizeof(void *) && (th != 0));\n\n  set_thr_data(th);\n\n  bectl(th, (bget_compact_t)0, (bget_acquire_t)malloc, (bget_release_t)free,\n        (bufsize)__kmp_malloc_pool_incr);\n}\n\nvoid __kmp_finalize_bget(kmp_info_t *th) {\n  thr_data_t *thr;\n  bfhead_t *b;\n\n  KMP_DEBUG_ASSERT(th != 0);\n\n#if BufStats\n  thr = (thr_data_t *)th->th.th_local.bget_data;\n  KMP_DEBUG_ASSERT(thr != NULL);\n  b = thr->last_pool;\n\n  /*  If a block-release function is defined, and this free buffer constitutes\n      the entire block, release it. Note that pool_len is defined in such a way\n      that the test will fail unless all pool blocks are the same size.  */\n\n  // Deallocate the last pool if one exists because we no longer do it in brel()\n  if (thr->relfcn != 0 && b != 0 && thr->numpblk != 0 &&\n      b->bh.bb.bsize == (bufsize)(thr->pool_len - sizeof(bhead_t))) {\n    KMP_DEBUG_ASSERT(b->bh.bb.prevfree == 0);\n    KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.bsize == ESent);\n    KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.prevfree ==\n                     b->bh.bb.bsize);\n\n    /*  Unlink the buffer from the free list  */\n    __kmp_bget_remove_from_freelist(b);\n\n    KE_TRACE(10, (\"%%%%%% FREE( %p )\\n\", (void *)b));\n\n    (*thr->relfcn)(b);\n    thr->numprel++; /* Nr of expansion block releases */\n    thr->numpblk--; /* Total number of blocks */\n    KMP_DEBUG_ASSERT(thr->numpblk == thr->numpget - thr->numprel);\n  }\n#endif /* BufStats */\n\n  /* Deallocate bget_data */\n  if (th->th.th_local.bget_data != NULL) {\n    __kmp_free(th->th.th_local.bget_data);\n    th->th.th_local.bget_data = NULL;\n  }\n}\n\nvoid kmpc_set_poolsize(size_t size) {\n  bectl(__kmp_get_thread(), (bget_compact_t)0, (bget_acquire_t)malloc,\n        (bget_release_t)free, (bufsize)size);\n}\n\nsize_t kmpc_get_poolsize(void) {\n  thr_data_t *p;\n\n  p = get_thr_data(__kmp_get_thread());\n\n  return p->exp_incr;\n}\n\nvoid kmpc_set_poolmode(int mode) {\n  thr_data_t *p;\n\n  if (mode == bget_mode_fifo || mode == bget_mode_lifo ||\n      mode == bget_mode_best) {\n    p = get_thr_data(__kmp_get_thread());\n    p->mode = (bget_mode_t)mode;\n  }\n}\n\nint kmpc_get_poolmode(void) {\n  thr_data_t *p;\n\n  p = get_thr_data(__kmp_get_thread());\n\n  return p->mode;\n}\n\nvoid kmpc_get_poolstat(size_t *maxmem, size_t *allmem) {\n  kmp_info_t *th = __kmp_get_thread();\n  bufsize a, b;\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n  bcheck(th, &a, &b);\n\n  *maxmem = a;\n  *allmem = b;\n}\n\nvoid kmpc_poolprint(void) {\n  kmp_info_t *th = __kmp_get_thread();\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n  bfreed(th);\n}\n\n#endif // #if KMP_USE_BGET\n\nvoid *kmpc_malloc(size_t size) {\n  void *ptr;\n  ptr = bget(__kmp_entry_thread(), (bufsize)(size + sizeof(ptr)));\n  if (ptr != NULL) {\n    // save allocated pointer just before one returned to user\n    *(void **)ptr = ptr;\n    ptr = (void **)ptr + 1;\n  }\n  return ptr;\n}\n\n#define IS_POWER_OF_TWO(n) (((n) & ((n)-1)) == 0)\n\nvoid *kmpc_aligned_malloc(size_t size, size_t alignment) {\n  void *ptr;\n  void *ptr_allocated;\n  KMP_DEBUG_ASSERT(alignment < 32 * 1024); // Alignment should not be too big\n  if (!IS_POWER_OF_TWO(alignment)) {\n    // AC: do we need to issue a warning here?\n    errno = EINVAL;\n    return NULL;\n  }\n  size = size + sizeof(void *) + alignment;\n  ptr_allocated = bget(__kmp_entry_thread(), (bufsize)size);\n  if (ptr_allocated != NULL) {\n    // save allocated pointer just before one returned to user\n    ptr = (void *)(((kmp_uintptr_t)ptr_allocated + sizeof(void *) + alignment) &\n                   ~(alignment - 1));\n    *((void **)ptr - 1) = ptr_allocated;\n  } else {\n    ptr = NULL;\n  }\n  return ptr;\n}\n\nvoid *kmpc_calloc(size_t nelem, size_t elsize) {\n  void *ptr;\n  ptr = bgetz(__kmp_entry_thread(), (bufsize)(nelem * elsize + sizeof(ptr)));\n  if (ptr != NULL) {\n    // save allocated pointer just before one returned to user\n    *(void **)ptr = ptr;\n    ptr = (void **)ptr + 1;\n  }\n  return ptr;\n}\n\nvoid *kmpc_realloc(void *ptr, size_t size) {\n  void *result = NULL;\n  if (ptr == NULL) {\n    // If pointer is NULL, realloc behaves like malloc.\n    result = bget(__kmp_entry_thread(), (bufsize)(size + sizeof(ptr)));\n    // save allocated pointer just before one returned to user\n    if (result != NULL) {\n      *(void **)result = result;\n      result = (void **)result + 1;\n    }\n  } else if (size == 0) {\n    // If size is 0, realloc behaves like free.\n    // The thread must be registered by the call to kmpc_malloc() or\n    // kmpc_calloc() before.\n    // So it should be safe to call __kmp_get_thread(), not\n    // __kmp_entry_thread().\n    KMP_ASSERT(*((void **)ptr - 1));\n    brel(__kmp_get_thread(), *((void **)ptr - 1));\n  } else {\n    result = bgetr(__kmp_entry_thread(), *((void **)ptr - 1),\n                   (bufsize)(size + sizeof(ptr)));\n    if (result != NULL) {\n      *(void **)result = result;\n      result = (void **)result + 1;\n    }\n  }\n  return result;\n}\n\n// NOTE: the library must have already been initialized by a previous allocate\nvoid kmpc_free(void *ptr) {\n  if (!__kmp_init_serial) {\n    return;\n  }\n  if (ptr != NULL) {\n    kmp_info_t *th = __kmp_get_thread();\n    __kmp_bget_dequeue(th); /* Release any queued buffers */\n    // extract allocated pointer and free it\n    KMP_ASSERT(*((void **)ptr - 1));\n    brel(th, *((void **)ptr - 1));\n  }\n}\n\nvoid *___kmp_thread_malloc(kmp_info_t *th, size_t size KMP_SRC_LOC_DECL) {\n  void *ptr;\n  KE_TRACE(30, (\"-> __kmp_thread_malloc( %p, %d ) called from %s:%d\\n\", th,\n                (int)size KMP_SRC_LOC_PARM));\n  ptr = bget(th, (bufsize)size);\n  KE_TRACE(30, (\"<- __kmp_thread_malloc() returns %p\\n\", ptr));\n  return ptr;\n}\n\nvoid *___kmp_thread_calloc(kmp_info_t *th, size_t nelem,\n                           size_t elsize KMP_SRC_LOC_DECL) {\n  void *ptr;\n  KE_TRACE(30, (\"-> __kmp_thread_calloc( %p, %d, %d ) called from %s:%d\\n\", th,\n                (int)nelem, (int)elsize KMP_SRC_LOC_PARM));\n  ptr = bgetz(th, (bufsize)(nelem * elsize));\n  KE_TRACE(30, (\"<- __kmp_thread_calloc() returns %p\\n\", ptr));\n  return ptr;\n}\n\nvoid *___kmp_thread_realloc(kmp_info_t *th, void *ptr,\n                            size_t size KMP_SRC_LOC_DECL) {\n  KE_TRACE(30, (\"-> __kmp_thread_realloc( %p, %p, %d ) called from %s:%d\\n\", th,\n                ptr, (int)size KMP_SRC_LOC_PARM));\n  ptr = bgetr(th, ptr, (bufsize)size);\n  KE_TRACE(30, (\"<- __kmp_thread_realloc() returns %p\\n\", ptr));\n  return ptr;\n}\n\nvoid ___kmp_thread_free(kmp_info_t *th, void *ptr KMP_SRC_LOC_DECL) {\n  KE_TRACE(30, (\"-> __kmp_thread_free( %p, %p ) called from %s:%d\\n\", th,\n                ptr KMP_SRC_LOC_PARM));\n  if (ptr != NULL) {\n    __kmp_bget_dequeue(th); /* Release any queued buffers */\n    brel(th, ptr);\n  }\n  KE_TRACE(30, (\"<- __kmp_thread_free()\\n\"));\n}\n\n/* OMP 5.0 Memory Management support */\nstatic const char *kmp_mk_lib_name;\nstatic void *h_memkind;\n/* memkind experimental API: */\n// memkind_alloc\nstatic void *(*kmp_mk_alloc)(void *k, size_t sz);\n// memkind_free\nstatic void (*kmp_mk_free)(void *kind, void *ptr);\n// memkind_check_available\nstatic int (*kmp_mk_check)(void *kind);\n// kinds we are going to use\nstatic void **mk_default;\nstatic void **mk_interleave;\nstatic void **mk_hbw;\nstatic void **mk_hbw_interleave;\nstatic void **mk_hbw_preferred;\nstatic void **mk_hugetlb;\nstatic void **mk_hbw_hugetlb;\nstatic void **mk_hbw_preferred_hugetlb;\n\n#if KMP_OS_UNIX && KMP_DYNAMIC_LIB\nstatic inline void chk_kind(void ***pkind) {\n  KMP_DEBUG_ASSERT(pkind);\n  if (*pkind) // symbol found\n    if (kmp_mk_check(**pkind)) // kind not available or error\n      *pkind = NULL;\n}\n#endif\n\nvoid __kmp_init_memkind() {\n// as of 2018-07-31 memkind does not support Windows*, exclude it for now\n#if KMP_OS_UNIX && KMP_DYNAMIC_LIB\n  // use of statically linked memkind is problematic, as it depends on libnuma\n  kmp_mk_lib_name = \"libmemkind.so\";\n  h_memkind = dlopen(kmp_mk_lib_name, RTLD_LAZY);\n  if (h_memkind) {\n    kmp_mk_check = (int (*)(void *))dlsym(h_memkind, \"memkind_check_available\");\n    kmp_mk_alloc =\n        (void *(*)(void *, size_t))dlsym(h_memkind, \"memkind_malloc\");\n    kmp_mk_free = (void (*)(void *, void *))dlsym(h_memkind, \"memkind_free\");\n    mk_default = (void **)dlsym(h_memkind, \"MEMKIND_DEFAULT\");\n    if (kmp_mk_check && kmp_mk_alloc && kmp_mk_free && mk_default &&\n        !kmp_mk_check(*mk_default)) {\n      __kmp_memkind_available = 1;\n      mk_interleave = (void **)dlsym(h_memkind, \"MEMKIND_INTERLEAVE\");\n      chk_kind(&mk_interleave);\n      mk_hbw = (void **)dlsym(h_memkind, \"MEMKIND_HBW\");\n      chk_kind(&mk_hbw);\n      mk_hbw_interleave = (void **)dlsym(h_memkind, \"MEMKIND_HBW_INTERLEAVE\");\n      chk_kind(&mk_hbw_interleave);\n      mk_hbw_preferred = (void **)dlsym(h_memkind, \"MEMKIND_HBW_PREFERRED\");\n      chk_kind(&mk_hbw_preferred);\n      mk_hugetlb = (void **)dlsym(h_memkind, \"MEMKIND_HUGETLB\");\n      chk_kind(&mk_hugetlb);\n      mk_hbw_hugetlb = (void **)dlsym(h_memkind, \"MEMKIND_HBW_HUGETLB\");\n      chk_kind(&mk_hbw_hugetlb);\n      mk_hbw_preferred_hugetlb =\n          (void **)dlsym(h_memkind, \"MEMKIND_HBW_PREFERRED_HUGETLB\");\n      chk_kind(&mk_hbw_preferred_hugetlb);\n      KE_TRACE(25, (\"__kmp_init_memkind: memkind library initialized\\n\"));\n      return; // success\n    }\n    dlclose(h_memkind); // failure\n    h_memkind = NULL;\n  }\n  kmp_mk_check = NULL;\n  kmp_mk_alloc = NULL;\n  kmp_mk_free = NULL;\n  mk_default = NULL;\n  mk_interleave = NULL;\n  mk_hbw = NULL;\n  mk_hbw_interleave = NULL;\n  mk_hbw_preferred = NULL;\n  mk_hugetlb = NULL;\n  mk_hbw_hugetlb = NULL;\n  mk_hbw_preferred_hugetlb = NULL;\n#else\n  kmp_mk_lib_name = \"\";\n  h_memkind = NULL;\n  kmp_mk_check = NULL;\n  kmp_mk_alloc = NULL;\n  kmp_mk_free = NULL;\n  mk_default = NULL;\n  mk_interleave = NULL;\n  mk_hbw = NULL;\n  mk_hbw_interleave = NULL;\n  mk_hbw_preferred = NULL;\n  mk_hugetlb = NULL;\n  mk_hbw_hugetlb = NULL;\n  mk_hbw_preferred_hugetlb = NULL;\n#endif\n}\n\nvoid __kmp_fini_memkind() {\n#if KMP_OS_UNIX && KMP_DYNAMIC_LIB\n  if (__kmp_memkind_available)\n    KE_TRACE(25, (\"__kmp_fini_memkind: finalize memkind library\\n\"));\n  if (h_memkind) {\n    dlclose(h_memkind);\n    h_memkind = NULL;\n  }\n  kmp_mk_check = NULL;\n  kmp_mk_alloc = NULL;\n  kmp_mk_free = NULL;\n  mk_default = NULL;\n  mk_interleave = NULL;\n  mk_hbw = NULL;\n  mk_hbw_interleave = NULL;\n  mk_hbw_preferred = NULL;\n  mk_hugetlb = NULL;\n  mk_hbw_hugetlb = NULL;\n  mk_hbw_preferred_hugetlb = NULL;\n#endif\n}\n\nomp_allocator_handle_t __kmpc_init_allocator(int gtid, omp_memspace_handle_t ms,\n                                             int ntraits,\n                                             omp_alloctrait_t traits[]) {\n  // OpenMP 5.0 only allows predefined memspaces\n  KMP_DEBUG_ASSERT(ms == omp_default_mem_space || ms == omp_low_lat_mem_space ||\n                   ms == omp_large_cap_mem_space || ms == omp_const_mem_space ||\n                   ms == omp_high_bw_mem_space);\n  kmp_allocator_t *al;\n  int i;\n  al = (kmp_allocator_t *)__kmp_allocate(sizeof(kmp_allocator_t)); // zeroed\n  al->memspace = ms; // not used currently\n  for (i = 0; i < ntraits; ++i) {\n    switch (traits[i].key) {\n    case omp_atk_threadmodel:\n    case omp_atk_access:\n    case omp_atk_pinned:\n      break;\n    case omp_atk_alignment:\n      al->alignment = traits[i].value;\n      KMP_ASSERT(IS_POWER_OF_TWO(al->alignment));\n      break;\n    case omp_atk_pool_size:\n      al->pool_size = traits[i].value;\n      break;\n    case omp_atk_fallback:\n      al->fb = (omp_alloctrait_value_t)traits[i].value;\n      KMP_DEBUG_ASSERT(\n          al->fb == omp_atv_default_mem_fb || al->fb == omp_atv_null_fb ||\n          al->fb == omp_atv_abort_fb || al->fb == omp_atv_allocator_fb);\n      break;\n    case omp_atk_fb_data:\n      al->fb_data = RCAST(kmp_allocator_t *, traits[i].value);\n      break;\n    case omp_atk_partition:\n      al->memkind = RCAST(void **, traits[i].value);\n      break;\n    default:\n      KMP_ASSERT2(0, \"Unexpected allocator trait\");\n    }\n  }\n  if (al->fb == 0) {\n    // set default allocator\n    al->fb = omp_atv_default_mem_fb;\n    al->fb_data = (kmp_allocator_t *)omp_default_mem_alloc;\n  } else if (al->fb == omp_atv_allocator_fb) {\n    KMP_ASSERT(al->fb_data != NULL);\n  } else if (al->fb == omp_atv_default_mem_fb) {\n    al->fb_data = (kmp_allocator_t *)omp_default_mem_alloc;\n  }\n  if (__kmp_memkind_available) {\n    // Let's use memkind library if available\n    if (ms == omp_high_bw_mem_space) {\n      if (al->memkind == (void *)omp_atv_interleaved && mk_hbw_interleave) {\n        al->memkind = mk_hbw_interleave;\n      } else if (mk_hbw_preferred) {\n        // AC: do not try to use MEMKIND_HBW for now, because memkind library\n        // cannot reliably detect exhaustion of HBW memory.\n        // It could be possible using hbw_verify_memory_region() but memkind\n        // manual says: \"Using this function in production code may result in\n        // serious performance penalty\".\n        al->memkind = mk_hbw_preferred;\n      } else {\n        // HBW is requested but not available --> return NULL allocator\n        __kmp_free(al);\n        return omp_null_allocator;\n      }\n    } else {\n      if (al->memkind == (void *)omp_atv_interleaved && mk_interleave) {\n        al->memkind = mk_interleave;\n      } else {\n        al->memkind = mk_default;\n      }\n    }\n  } else {\n    if (ms == omp_high_bw_mem_space) {\n      // cannot detect HBW memory presence without memkind library\n      __kmp_free(al);\n      return omp_null_allocator;\n    }\n  }\n  return (omp_allocator_handle_t)al;\n}\n\nvoid __kmpc_destroy_allocator(int gtid, omp_allocator_handle_t allocator) {\n  if (allocator > kmp_max_mem_alloc)\n    __kmp_free(allocator);\n}\n\nvoid __kmpc_set_default_allocator(int gtid, omp_allocator_handle_t allocator) {\n  if (allocator == omp_null_allocator)\n    allocator = omp_default_mem_alloc;\n  __kmp_threads[gtid]->th.th_def_allocator = allocator;\n}\n\nomp_allocator_handle_t __kmpc_get_default_allocator(int gtid) {\n  return __kmp_threads[gtid]->th.th_def_allocator;\n}\n\ntypedef struct kmp_mem_desc { // Memory block descriptor\n  void *ptr_alloc; // Pointer returned by allocator\n  size_t size_a; // Size of allocated memory block (initial+descriptor+align)\n  void *ptr_align; // Pointer to aligned memory, returned\n  kmp_allocator_t *allocator; // allocator\n} kmp_mem_desc_t;\nstatic int alignment = sizeof(void *); // let's align to pointer size\n\nvoid *__kmpc_alloc(int gtid, size_t size, omp_allocator_handle_t allocator) {\n  void *ptr = NULL;\n  kmp_allocator_t *al;\n  KMP_DEBUG_ASSERT(__kmp_init_serial);\n  if (allocator == omp_null_allocator)\n    allocator = __kmp_threads[gtid]->th.th_def_allocator;\n\n  KE_TRACE(25, (\"__kmpc_alloc: T#%d (%d, %p)\\n\", gtid, (int)size, allocator));\n  al = RCAST(kmp_allocator_t *, CCAST(omp_allocator_handle_t, allocator));\n\n  int sz_desc = sizeof(kmp_mem_desc_t);\n  kmp_mem_desc_t desc;\n  kmp_uintptr_t addr; // address returned by allocator\n  kmp_uintptr_t addr_align; // address to return to caller\n  kmp_uintptr_t addr_descr; // address of memory block descriptor\n  int align = alignment; // default alignment\n  if (allocator > kmp_max_mem_alloc && al->alignment > 0) {\n    align = al->alignment; // alignment requested by user\n  }\n  desc.size_a = size + sz_desc + align;\n\n  if (__kmp_memkind_available) {\n    if (allocator < kmp_max_mem_alloc) {\n      // pre-defined allocator\n      if (allocator == omp_high_bw_mem_alloc && mk_hbw_preferred) {\n        ptr = kmp_mk_alloc(*mk_hbw_preferred, desc.size_a);\n      } else {\n        ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n      }\n    } else if (al->pool_size > 0) {\n      // custom allocator with pool size requested\n      kmp_uint64 used =\n          KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, desc.size_a);\n      if (used + desc.size_a > al->pool_size) {\n        // not enough space, need to go fallback path\n        KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n        if (al->fb == omp_atv_default_mem_fb) {\n          al = (kmp_allocator_t *)omp_default_mem_alloc;\n          ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n        } else if (al->fb == omp_atv_abort_fb) {\n          KMP_ASSERT(0); // abort fallback requested\n        } else if (al->fb == omp_atv_allocator_fb) {\n          KMP_ASSERT(al != al->fb_data);\n          al = al->fb_data;\n          return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n        } // else ptr == NULL;\n      } else {\n        // pool has enough space\n        ptr = kmp_mk_alloc(*al->memkind, desc.size_a);\n        if (ptr == NULL) {\n          if (al->fb == omp_atv_default_mem_fb) {\n            al = (kmp_allocator_t *)omp_default_mem_alloc;\n            ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n          } else if (al->fb == omp_atv_abort_fb) {\n            KMP_ASSERT(0); // abort fallback requested\n          } else if (al->fb == omp_atv_allocator_fb) {\n            KMP_ASSERT(al != al->fb_data);\n            al = al->fb_data;\n            return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n          }\n        }\n      }\n    } else {\n      // custom allocator, pool size not requested\n      ptr = kmp_mk_alloc(*al->memkind, desc.size_a);\n      if (ptr == NULL) {\n        if (al->fb == omp_atv_default_mem_fb) {\n          al = (kmp_allocator_t *)omp_default_mem_alloc;\n          ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n        } else if (al->fb == omp_atv_abort_fb) {\n          KMP_ASSERT(0); // abort fallback requested\n        } else if (al->fb == omp_atv_allocator_fb) {\n          KMP_ASSERT(al != al->fb_data);\n          al = al->fb_data;\n          return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n        }\n      }\n    }\n  } else if (allocator < kmp_max_mem_alloc) {\n    // pre-defined allocator\n    if (allocator == omp_high_bw_mem_alloc) {\n      // ptr = NULL;\n    } else {\n      ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n    }\n  } else if (al->pool_size > 0) {\n    // custom allocator with pool size requested\n    kmp_uint64 used =\n        KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, desc.size_a);\n    if (used + desc.size_a > al->pool_size) {\n      // not enough space, need to go fallback path\n      KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n      if (al->fb == omp_atv_default_mem_fb) {\n        al = (kmp_allocator_t *)omp_default_mem_alloc;\n        ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n      } else if (al->fb == omp_atv_abort_fb) {\n        KMP_ASSERT(0); // abort fallback requested\n      } else if (al->fb == omp_atv_allocator_fb) {\n        KMP_ASSERT(al != al->fb_data);\n        al = al->fb_data;\n        return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n      } // else ptr == NULL;\n    } else {\n      // pool has enough space\n      ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n      if (ptr == NULL && al->fb == omp_atv_abort_fb) {\n        KMP_ASSERT(0); // abort fallback requested\n      } // no sense to look for another fallback because of same internal alloc\n    }\n  } else {\n    // custom allocator, pool size not requested\n    ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n    if (ptr == NULL && al->fb == omp_atv_abort_fb) {\n      KMP_ASSERT(0); // abort fallback requested\n    } // no sense to look for another fallback because of same internal alloc\n  }\n  KE_TRACE(10, (\"__kmpc_alloc: T#%d %p=alloc(%d)\\n\", gtid, ptr, desc.size_a));\n  if (ptr == NULL)\n    return NULL;\n\n  addr = (kmp_uintptr_t)ptr;\n  addr_align = (addr + sz_desc + align - 1) & ~(align - 1);\n  addr_descr = addr_align - sz_desc;\n\n  desc.ptr_alloc = ptr;\n  desc.ptr_align = (void *)addr_align;\n  desc.allocator = al;\n  *((kmp_mem_desc_t *)addr_descr) = desc; // save descriptor contents\n  KMP_MB();\n\n  KE_TRACE(25, (\"__kmpc_alloc returns %p, T#%d\\n\", desc.ptr_align, gtid));\n  return desc.ptr_align;\n}\n\nvoid __kmpc_free(int gtid, void *ptr, const omp_allocator_handle_t allocator) {\n  KE_TRACE(25, (\"__kmpc_free: T#%d free(%p,%p)\\n\", gtid, ptr, allocator));\n  if (ptr == NULL)\n    return;\n\n  kmp_allocator_t *al;\n  omp_allocator_handle_t oal;\n  al = RCAST(kmp_allocator_t *, CCAST(omp_allocator_handle_t, allocator));\n  kmp_mem_desc_t desc;\n  kmp_uintptr_t addr_align; // address to return to caller\n  kmp_uintptr_t addr_descr; // address of memory block descriptor\n\n  addr_align = (kmp_uintptr_t)ptr;\n  addr_descr = addr_align - sizeof(kmp_mem_desc_t);\n  desc = *((kmp_mem_desc_t *)addr_descr); // read descriptor\n\n  KMP_DEBUG_ASSERT(desc.ptr_align == ptr);\n  if (allocator) {\n    KMP_DEBUG_ASSERT(desc.allocator == al || desc.allocator == al->fb_data);\n  }\n  al = desc.allocator;\n  oal = (omp_allocator_handle_t)al; // cast to void* for comparisons\n  KMP_DEBUG_ASSERT(al);\n\n  if (__kmp_memkind_available) {\n    if (oal < kmp_max_mem_alloc) {\n      // pre-defined allocator\n      if (oal == omp_high_bw_mem_alloc && mk_hbw_preferred) {\n        kmp_mk_free(*mk_hbw_preferred, desc.ptr_alloc);\n      } else {\n        kmp_mk_free(*mk_default, desc.ptr_alloc);\n      }\n    } else {\n      if (al->pool_size > 0) { // custom allocator with pool size requested\n        kmp_uint64 used =\n            KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n        (void)used; // to suppress compiler warning\n        KMP_DEBUG_ASSERT(used >= desc.size_a);\n      }\n      kmp_mk_free(*al->memkind, desc.ptr_alloc);\n    }\n  } else {\n    if (oal > kmp_max_mem_alloc && al->pool_size > 0) {\n      kmp_uint64 used =\n          KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n      (void)used; // to suppress compiler warning\n      KMP_DEBUG_ASSERT(used >= desc.size_a);\n    }\n    __kmp_thread_free(__kmp_thread_from_gtid(gtid), desc.ptr_alloc);\n  }\n  KE_TRACE(10, (\"__kmpc_free: T#%d freed %p (%p)\\n\", gtid, desc.ptr_alloc,\n                allocator));\n}\n\n/* If LEAK_MEMORY is defined, __kmp_free() will *not* free memory. It causes\n   memory leaks, but it may be useful for debugging memory corruptions, used\n   freed pointers, etc. */\n/* #define LEAK_MEMORY */\nstruct kmp_mem_descr { // Memory block descriptor.\n  void *ptr_allocated; // Pointer returned by malloc(), subject for free().\n  size_t size_allocated; // Size of allocated memory block.\n  void *ptr_aligned; // Pointer to aligned memory, to be used by client code.\n  size_t size_aligned; // Size of aligned memory block.\n};\ntypedef struct kmp_mem_descr kmp_mem_descr_t;\n\n/* Allocate memory on requested boundary, fill allocated memory with 0x00.\n   NULL is NEVER returned, __kmp_abort() is called in case of memory allocation\n   error. Must use __kmp_free when freeing memory allocated by this routine! */\nstatic void *___kmp_allocate_align(size_t size,\n                                   size_t alignment KMP_SRC_LOC_DECL) {\n  /* __kmp_allocate() allocates (by call to malloc()) bigger memory block than\n     requested to return properly aligned pointer. Original pointer returned\n     by malloc() and size of allocated block is saved in descriptor just\n     before the aligned pointer. This information used by __kmp_free() -- it\n     has to pass to free() original pointer, not aligned one.\n\n          +---------+------------+-----------------------------------+---------+\n          | padding | descriptor |           aligned block           | padding |\n          +---------+------------+-----------------------------------+---------+\n          ^                      ^\n          |                      |\n          |                      +- Aligned pointer returned to caller\n          +- Pointer returned by malloc()\n\n      Aligned block is filled with zeros, paddings are filled with 0xEF. */\n\n  kmp_mem_descr_t descr;\n  kmp_uintptr_t addr_allocated; // Address returned by malloc().\n  kmp_uintptr_t addr_aligned; // Aligned address to return to caller.\n  kmp_uintptr_t addr_descr; // Address of memory block descriptor.\n\n  KE_TRACE(25, (\"-> ___kmp_allocate_align( %d, %d ) called from %s:%d\\n\",\n                (int)size, (int)alignment KMP_SRC_LOC_PARM));\n\n  KMP_DEBUG_ASSERT(alignment < 32 * 1024); // Alignment should not be too\n  KMP_DEBUG_ASSERT(sizeof(void *) <= sizeof(kmp_uintptr_t));\n  // Make sure kmp_uintptr_t is enough to store addresses.\n\n  descr.size_aligned = size;\n  descr.size_allocated =\n      descr.size_aligned + sizeof(kmp_mem_descr_t) + alignment;\n\n#if KMP_DEBUG\n  descr.ptr_allocated = _malloc_src_loc(descr.size_allocated, _file_, _line_);\n#else\n  descr.ptr_allocated = malloc_src_loc(descr.size_allocated KMP_SRC_LOC_PARM);\n#endif\n  KE_TRACE(10, (\"   malloc( %d ) returned %p\\n\", (int)descr.size_allocated,\n                descr.ptr_allocated));\n  if (descr.ptr_allocated == NULL) {\n    KMP_FATAL(OutOfHeapMemory);\n  }\n\n  addr_allocated = (kmp_uintptr_t)descr.ptr_allocated;\n  addr_aligned =\n      (addr_allocated + sizeof(kmp_mem_descr_t) + alignment) & ~(alignment - 1);\n  addr_descr = addr_aligned - sizeof(kmp_mem_descr_t);\n\n  descr.ptr_aligned = (void *)addr_aligned;\n\n  KE_TRACE(26, (\"   ___kmp_allocate_align: \"\n                \"ptr_allocated=%p, size_allocated=%d, \"\n                \"ptr_aligned=%p, size_aligned=%d\\n\",\n                descr.ptr_allocated, (int)descr.size_allocated,\n                descr.ptr_aligned, (int)descr.size_aligned));\n\n  KMP_DEBUG_ASSERT(addr_allocated <= addr_descr);\n  KMP_DEBUG_ASSERT(addr_descr + sizeof(kmp_mem_descr_t) == addr_aligned);\n  KMP_DEBUG_ASSERT(addr_aligned + descr.size_aligned <=\n                   addr_allocated + descr.size_allocated);\n  KMP_DEBUG_ASSERT(addr_aligned % alignment == 0);\n#ifdef KMP_DEBUG\n  memset(descr.ptr_allocated, 0xEF, descr.size_allocated);\n// Fill allocated memory block with 0xEF.\n#endif\n  memset(descr.ptr_aligned, 0x00, descr.size_aligned);\n  // Fill the aligned memory block (which is intended for using by caller) with\n  // 0x00. Do not\n  // put this filling under KMP_DEBUG condition! Many callers expect zeroed\n  // memory. (Padding\n  // bytes remain filled with 0xEF in debugging library.)\n  *((kmp_mem_descr_t *)addr_descr) = descr;\n\n  KMP_MB();\n\n  KE_TRACE(25, (\"<- ___kmp_allocate_align() returns %p\\n\", descr.ptr_aligned));\n  return descr.ptr_aligned;\n} // func ___kmp_allocate_align\n\n/* Allocate memory on cache line boundary, fill allocated memory with 0x00.\n   Do not call this func directly! Use __kmp_allocate macro instead.\n   NULL is NEVER returned, __kmp_abort() is called in case of memory allocation\n   error. Must use __kmp_free when freeing memory allocated by this routine! */\nvoid *___kmp_allocate(size_t size KMP_SRC_LOC_DECL) {\n  void *ptr;\n  KE_TRACE(25, (\"-> __kmp_allocate( %d ) called from %s:%d\\n\",\n                (int)size KMP_SRC_LOC_PARM));\n  ptr = ___kmp_allocate_align(size, __kmp_align_alloc KMP_SRC_LOC_PARM);\n  KE_TRACE(25, (\"<- __kmp_allocate() returns %p\\n\", ptr));\n  return ptr;\n} // func ___kmp_allocate\n\n/* Allocate memory on page boundary, fill allocated memory with 0x00.\n   Does not call this func directly! Use __kmp_page_allocate macro instead.\n   NULL is NEVER returned, __kmp_abort() is called in case of memory allocation\n   error. Must use __kmp_free when freeing memory allocated by this routine! */\nvoid *___kmp_page_allocate(size_t size KMP_SRC_LOC_DECL) {\n  int page_size = 8 * 1024;\n  void *ptr;\n\n  KE_TRACE(25, (\"-> __kmp_page_allocate( %d ) called from %s:%d\\n\",\n                (int)size KMP_SRC_LOC_PARM));\n  ptr = ___kmp_allocate_align(size, page_size KMP_SRC_LOC_PARM);\n  KE_TRACE(25, (\"<- __kmp_page_allocate( %d ) returns %p\\n\", (int)size, ptr));\n  return ptr;\n} // ___kmp_page_allocate\n\n/* Free memory allocated by __kmp_allocate() and __kmp_page_allocate().\n   In debug mode, fill the memory block with 0xEF before call to free(). */\nvoid ___kmp_free(void *ptr KMP_SRC_LOC_DECL) {\n  kmp_mem_descr_t descr;\n  kmp_uintptr_t addr_allocated; // Address returned by malloc().\n  kmp_uintptr_t addr_aligned; // Aligned address passed by caller.\n\n  KE_TRACE(25,\n           (\"-> __kmp_free( %p ) called from %s:%d\\n\", ptr KMP_SRC_LOC_PARM));\n  KMP_ASSERT(ptr != NULL);\n\n  descr = *(kmp_mem_descr_t *)((kmp_uintptr_t)ptr - sizeof(kmp_mem_descr_t));\n\n  KE_TRACE(26, (\"   __kmp_free:     \"\n                \"ptr_allocated=%p, size_allocated=%d, \"\n                \"ptr_aligned=%p, size_aligned=%d\\n\",\n                descr.ptr_allocated, (int)descr.size_allocated,\n                descr.ptr_aligned, (int)descr.size_aligned));\n\n  addr_allocated = (kmp_uintptr_t)descr.ptr_allocated;\n  addr_aligned = (kmp_uintptr_t)descr.ptr_aligned;\n\n  KMP_DEBUG_ASSERT(addr_aligned % CACHE_LINE == 0);\n  KMP_DEBUG_ASSERT(descr.ptr_aligned == ptr);\n  KMP_DEBUG_ASSERT(addr_allocated + sizeof(kmp_mem_descr_t) <= addr_aligned);\n  KMP_DEBUG_ASSERT(descr.size_aligned < descr.size_allocated);\n  KMP_DEBUG_ASSERT(addr_aligned + descr.size_aligned <=\n                   addr_allocated + descr.size_allocated);\n\n#ifdef KMP_DEBUG\n  memset(descr.ptr_allocated, 0xEF, descr.size_allocated);\n// Fill memory block with 0xEF, it helps catch using freed memory.\n#endif\n\n#ifndef LEAK_MEMORY\n  KE_TRACE(10, (\"   free( %p )\\n\", descr.ptr_allocated));\n#ifdef KMP_DEBUG\n  _free_src_loc(descr.ptr_allocated, _file_, _line_);\n#else\n  free_src_loc(descr.ptr_allocated KMP_SRC_LOC_PARM);\n#endif\n#endif\n  KMP_MB();\n  KE_TRACE(25, (\"<- __kmp_free() returns\\n\"));\n} // func ___kmp_free\n\n#if USE_FAST_MEMORY == 3\n// Allocate fast memory by first scanning the thread's free lists\n// If a chunk the right size exists, grab it off the free list.\n// Otherwise allocate normally using kmp_thread_malloc.\n\n// AC: How to choose the limit? Just get 16 for now...\n#define KMP_FREE_LIST_LIMIT 16\n\n// Always use 128 bytes for determining buckets for caching memory blocks\n#define DCACHE_LINE 128\n\nvoid *___kmp_fast_allocate(kmp_info_t *this_thr, size_t size KMP_SRC_LOC_DECL) {\n  void *ptr;\n  int num_lines;\n  int idx;\n  int index;\n  void *alloc_ptr;\n  size_t alloc_size;\n  kmp_mem_descr_t *descr;\n\n  KE_TRACE(25, (\"-> __kmp_fast_allocate( T#%d, %d ) called from %s:%d\\n\",\n                __kmp_gtid_from_thread(this_thr), (int)size KMP_SRC_LOC_PARM));\n\n  num_lines = (size + DCACHE_LINE - 1) / DCACHE_LINE;\n  idx = num_lines - 1;\n  KMP_DEBUG_ASSERT(idx >= 0);\n  if (idx < 2) {\n    index = 0; // idx is [ 0, 1 ], use first free list\n    num_lines = 2; // 1, 2 cache lines or less than cache line\n  } else if ((idx >>= 2) == 0) {\n    index = 1; // idx is [ 2, 3 ], use second free list\n    num_lines = 4; // 3, 4 cache lines\n  } else if ((idx >>= 2) == 0) {\n    index = 2; // idx is [ 4, 15 ], use third free list\n    num_lines = 16; // 5, 6, ..., 16 cache lines\n  } else if ((idx >>= 2) == 0) {\n    index = 3; // idx is [ 16, 63 ], use fourth free list\n    num_lines = 64; // 17, 18, ..., 64 cache lines\n  } else {\n    goto alloc_call; // 65 or more cache lines ( > 8KB ), don't use free lists\n  }\n\n  ptr = this_thr->th.th_free_lists[index].th_free_list_self;\n  if (ptr != NULL) {\n    // pop the head of no-sync free list\n    this_thr->th.th_free_lists[index].th_free_list_self = *((void **)ptr);\n    KMP_DEBUG_ASSERT(\n        this_thr ==\n        ((kmp_mem_descr_t *)((kmp_uintptr_t)ptr - sizeof(kmp_mem_descr_t)))\n            ->ptr_aligned);\n    goto end;\n  }\n  ptr = TCR_SYNC_PTR(this_thr->th.th_free_lists[index].th_free_list_sync);\n  if (ptr != NULL) {\n    // no-sync free list is empty, use sync free list (filled in by other\n    // threads only)\n    // pop the head of the sync free list, push NULL instead\n    while (!KMP_COMPARE_AND_STORE_PTR(\n        &this_thr->th.th_free_lists[index].th_free_list_sync, ptr, nullptr)) {\n      KMP_CPU_PAUSE();\n      ptr = TCR_SYNC_PTR(this_thr->th.th_free_lists[index].th_free_list_sync);\n    }\n    // push the rest of chain into no-sync free list (can be NULL if there was\n    // the only block)\n    this_thr->th.th_free_lists[index].th_free_list_self = *((void **)ptr);\n    KMP_DEBUG_ASSERT(\n        this_thr ==\n        ((kmp_mem_descr_t *)((kmp_uintptr_t)ptr - sizeof(kmp_mem_descr_t)))\n            ->ptr_aligned);\n    goto end;\n  }\n\nalloc_call:\n  // haven't found block in the free lists, thus allocate it\n  size = num_lines * DCACHE_LINE;\n\n  alloc_size = size + sizeof(kmp_mem_descr_t) + DCACHE_LINE;\n  KE_TRACE(25, (\"__kmp_fast_allocate: T#%d Calling __kmp_thread_malloc with \"\n                \"alloc_size %d\\n\",\n                __kmp_gtid_from_thread(this_thr), alloc_size));\n  alloc_ptr = bget(this_thr, (bufsize)alloc_size);\n\n  // align ptr to DCACHE_LINE\n  ptr = (void *)((((kmp_uintptr_t)alloc_ptr) + sizeof(kmp_mem_descr_t) +\n                  DCACHE_LINE) &\n                 ~(DCACHE_LINE - 1));\n  descr = (kmp_mem_descr_t *)(((kmp_uintptr_t)ptr) - sizeof(kmp_mem_descr_t));\n\n  descr->ptr_allocated = alloc_ptr; // remember allocated pointer\n  // we don't need size_allocated\n  descr->ptr_aligned = (void *)this_thr; // remember allocating thread\n  // (it is already saved in bget buffer,\n  // but we may want to use another allocator in future)\n  descr->size_aligned = size;\n\nend:\n  KE_TRACE(25, (\"<- __kmp_fast_allocate( T#%d ) returns %p\\n\",\n                __kmp_gtid_from_thread(this_thr), ptr));\n  return ptr;\n} // func __kmp_fast_allocate\n\n// Free fast memory and place it on the thread's free list if it is of\n// the correct size.\nvoid ___kmp_fast_free(kmp_info_t *this_thr, void *ptr KMP_SRC_LOC_DECL) {\n  kmp_mem_descr_t *descr;\n  kmp_info_t *alloc_thr;\n  size_t size;\n  size_t idx;\n  int index;\n\n  KE_TRACE(25, (\"-> __kmp_fast_free( T#%d, %p ) called from %s:%d\\n\",\n                __kmp_gtid_from_thread(this_thr), ptr KMP_SRC_LOC_PARM));\n  KMP_ASSERT(ptr != NULL);\n\n  descr = (kmp_mem_descr_t *)(((kmp_uintptr_t)ptr) - sizeof(kmp_mem_descr_t));\n\n  KE_TRACE(26, (\"   __kmp_fast_free:     size_aligned=%d\\n\",\n                (int)descr->size_aligned));\n\n  size = descr->size_aligned; // 2, 4, 16, 64, 65, 66, ... cache lines\n\n  idx = DCACHE_LINE * 2; // 2 cache lines is minimal size of block\n  if (idx == size) {\n    index = 0; // 2 cache lines\n  } else if ((idx <<= 1) == size) {\n    index = 1; // 4 cache lines\n  } else if ((idx <<= 2) == size) {\n    index = 2; // 16 cache lines\n  } else if ((idx <<= 2) == size) {\n    index = 3; // 64 cache lines\n  } else {\n    KMP_DEBUG_ASSERT(size > DCACHE_LINE * 64);\n    goto free_call; // 65 or more cache lines ( > 8KB )\n  }\n\n  alloc_thr = (kmp_info_t *)descr->ptr_aligned; // get thread owning the block\n  if (alloc_thr == this_thr) {\n    // push block to self no-sync free list, linking previous head (LIFO)\n    *((void **)ptr) = this_thr->th.th_free_lists[index].th_free_list_self;\n    this_thr->th.th_free_lists[index].th_free_list_self = ptr;\n  } else {\n    void *head = this_thr->th.th_free_lists[index].th_free_list_other;\n    if (head == NULL) {\n      // Create new free list\n      this_thr->th.th_free_lists[index].th_free_list_other = ptr;\n      *((void **)ptr) = NULL; // mark the tail of the list\n      descr->size_allocated = (size_t)1; // head of the list keeps its length\n    } else {\n      // need to check existed \"other\" list's owner thread and size of queue\n      kmp_mem_descr_t *dsc =\n          (kmp_mem_descr_t *)((char *)head - sizeof(kmp_mem_descr_t));\n      // allocating thread, same for all queue nodes\n      kmp_info_t *q_th = (kmp_info_t *)(dsc->ptr_aligned);\n      size_t q_sz =\n          dsc->size_allocated + 1; // new size in case we add current task\n      if (q_th == alloc_thr && q_sz <= KMP_FREE_LIST_LIMIT) {\n        // we can add current task to \"other\" list, no sync needed\n        *((void **)ptr) = head;\n        descr->size_allocated = q_sz;\n        this_thr->th.th_free_lists[index].th_free_list_other = ptr;\n      } else {\n        // either queue blocks owner is changing or size limit exceeded\n        // return old queue to allocating thread (q_th) synchronously,\n        // and start new list for alloc_thr's tasks\n        void *old_ptr;\n        void *tail = head;\n        void *next = *((void **)head);\n        while (next != NULL) {\n          KMP_DEBUG_ASSERT(\n              // queue size should decrease by 1 each step through the list\n              ((kmp_mem_descr_t *)((char *)next - sizeof(kmp_mem_descr_t)))\n                      ->size_allocated +\n                  1 ==\n              ((kmp_mem_descr_t *)((char *)tail - sizeof(kmp_mem_descr_t)))\n                  ->size_allocated);\n          tail = next; // remember tail node\n          next = *((void **)next);\n        }\n        KMP_DEBUG_ASSERT(q_th != NULL);\n        // push block to owner's sync free list\n        old_ptr = TCR_PTR(q_th->th.th_free_lists[index].th_free_list_sync);\n        /* the next pointer must be set before setting free_list to ptr to avoid\n           exposing a broken list to other threads, even for an instant. */\n        *((void **)tail) = old_ptr;\n\n        while (!KMP_COMPARE_AND_STORE_PTR(\n            &q_th->th.th_free_lists[index].th_free_list_sync, old_ptr, head)) {\n          KMP_CPU_PAUSE();\n          old_ptr = TCR_PTR(q_th->th.th_free_lists[index].th_free_list_sync);\n          *((void **)tail) = old_ptr;\n        }\n\n        // start new list of not-selt tasks\n        this_thr->th.th_free_lists[index].th_free_list_other = ptr;\n        *((void **)ptr) = NULL;\n        descr->size_allocated = (size_t)1; // head of queue keeps its length\n      }\n    }\n  }\n  goto end;\n\nfree_call:\n  KE_TRACE(25, (\"__kmp_fast_free: T#%d Calling __kmp_thread_free for size %d\\n\",\n                __kmp_gtid_from_thread(this_thr), size));\n  __kmp_bget_dequeue(this_thr); /* Release any queued buffers */\n  brel(this_thr, descr->ptr_allocated);\n\nend:\n  KE_TRACE(25, (\"<- __kmp_fast_free() returns\\n\"));\n\n} // func __kmp_fast_free\n\n// Initialize the thread free lists related to fast memory\n// Only do this when a thread is initially created.\nvoid __kmp_initialize_fast_memory(kmp_info_t *this_thr) {\n  KE_TRACE(10, (\"__kmp_initialize_fast_memory: Called from th %p\\n\", this_thr));\n\n  memset(this_thr->th.th_free_lists, 0, NUM_LISTS * sizeof(kmp_free_list_t));\n}\n\n// Free the memory in the thread free lists related to fast memory\n// Only do this when a thread is being reaped (destroyed).\nvoid __kmp_free_fast_memory(kmp_info_t *th) {\n  // Suppose we use BGET underlying allocator, walk through its structures...\n  int bin;\n  thr_data_t *thr = get_thr_data(th);\n  void **lst = NULL;\n\n  KE_TRACE(\n      5, (\"__kmp_free_fast_memory: Called T#%d\\n\", __kmp_gtid_from_thread(th)));\n\n  __kmp_bget_dequeue(th); // Release any queued buffers\n\n  // Dig through free lists and extract all allocated blocks\n  for (bin = 0; bin < MAX_BGET_BINS; ++bin) {\n    bfhead_t *b = thr->freelist[bin].ql.flink;\n    while (b != &thr->freelist[bin]) {\n      if ((kmp_uintptr_t)b->bh.bb.bthr & 1) { // the buffer is allocated address\n        *((void **)b) =\n            lst; // link the list (override bthr, but keep flink yet)\n        lst = (void **)b; // push b into lst\n      }\n      b = b->ql.flink; // get next buffer\n    }\n  }\n  while (lst != NULL) {\n    void *next = *lst;\n    KE_TRACE(10, (\"__kmp_free_fast_memory: freeing %p, next=%p th %p (%d)\\n\",\n                  lst, next, th, __kmp_gtid_from_thread(th)));\n    (*thr->relfcn)(lst);\n#if BufStats\n    // count blocks to prevent problems in __kmp_finalize_bget()\n    thr->numprel++; /* Nr of expansion block releases */\n    thr->numpblk--; /* Total number of blocks */\n#endif\n    lst = (void **)next;\n  }\n\n  KE_TRACE(\n      5, (\"__kmp_free_fast_memory: Freed T#%d\\n\", __kmp_gtid_from_thread(th)));\n}\n\n#endif // USE_FAST_MEMORY\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/runtime/src/thirdparty/ittnotify/ittnotify_config.h": "\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _ITTNOTIFY_CONFIG_H_\n#define _ITTNOTIFY_CONFIG_H_\n\n/** @cond exclude_from_documentation */\n#ifndef ITT_OS_WIN\n#  define ITT_OS_WIN   1\n#endif /* ITT_OS_WIN */\n\n#ifndef ITT_OS_LINUX\n#  define ITT_OS_LINUX 2\n#endif /* ITT_OS_LINUX */\n\n#ifndef ITT_OS_MAC\n#  define ITT_OS_MAC   3\n#endif /* ITT_OS_MAC */\n\n#ifndef ITT_OS_FREEBSD\n#  define ITT_OS_FREEBSD   4\n#endif /* ITT_OS_FREEBSD */\n\n#ifndef ITT_OS\n#  if defined WIN32 || defined _WIN32\n#    define ITT_OS ITT_OS_WIN\n#  elif defined( __APPLE__ ) && defined( __MACH__ )\n#    define ITT_OS ITT_OS_MAC\n#  elif defined( __FreeBSD__ )\n#    define ITT_OS ITT_OS_FREEBSD\n#  else\n#    define ITT_OS ITT_OS_LINUX\n#  endif\n#endif /* ITT_OS */\n\n#ifndef ITT_PLATFORM_WIN\n#  define ITT_PLATFORM_WIN 1\n#endif /* ITT_PLATFORM_WIN */\n\n#ifndef ITT_PLATFORM_POSIX\n#  define ITT_PLATFORM_POSIX 2\n#endif /* ITT_PLATFORM_POSIX */\n\n#ifndef ITT_PLATFORM_MAC\n#  define ITT_PLATFORM_MAC 3\n#endif /* ITT_PLATFORM_MAC */\n\n#ifndef ITT_PLATFORM_FREEBSD\n#  define ITT_PLATFORM_FREEBSD 4\n#endif /* ITT_PLATFORM_FREEBSD */\n\n#ifndef ITT_PLATFORM\n#  if ITT_OS==ITT_OS_WIN\n#    define ITT_PLATFORM ITT_PLATFORM_WIN\n#  elif ITT_OS==ITT_OS_MAC\n#    define ITT_PLATFORM ITT_PLATFORM_MAC\n#  elif ITT_OS==ITT_OS_FREEBSD\n#    define ITT_PLATFORM ITT_PLATFORM_FREEBSD\n#  else\n#    define ITT_PLATFORM ITT_PLATFORM_POSIX\n#  endif\n#endif /* ITT_PLATFORM */\n\n#if defined(_UNICODE) && !defined(UNICODE)\n#define UNICODE\n#endif\n\n#include <stddef.h>\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <tchar.h>\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <stdint.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE || _UNICODE */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#ifndef ITTAPI_CDECL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define ITTAPI_CDECL __cdecl\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define ITTAPI_CDECL __attribute__ ((cdecl))\n#    else  /* _M_IX86 || __i386__ */\n#      define ITTAPI_CDECL /* actual only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* ITTAPI_CDECL */\n\n#ifndef STDCALL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define STDCALL __stdcall\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define STDCALL __attribute__ ((stdcall))\n#    else  /* _M_IX86 || __i386__ */\n#      define STDCALL /* supported only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* STDCALL */\n\n#define ITTAPI    ITTAPI_CDECL\n#define LIBITTAPI ITTAPI_CDECL\n\n/* TODO: Temporary for compatibility! */\n#define ITTAPI_CALL    ITTAPI_CDECL\n#define LIBITTAPI_CALL ITTAPI_CDECL\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n/* use __forceinline (VC++ specific) */\n#define ITT_INLINE           __forceinline\n#define ITT_INLINE_ATTRIBUTE /* nothing */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/*\n * Generally, functions are not inlined unless optimization is specified.\n * For functions declared inline, this attribute inlines the function even\n * if no optimization level was specified.\n */\n#ifdef __STRICT_ANSI__\n#define ITT_INLINE           static\n#define ITT_INLINE_ATTRIBUTE __attribute__((unused))\n#else  /* __STRICT_ANSI__ */\n#define ITT_INLINE           static inline\n#define ITT_INLINE_ATTRIBUTE __attribute__((always_inline, unused))\n#endif /* __STRICT_ANSI__ */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/** @endcond */\n\n#ifndef ITT_ARCH_IA32\n#  define ITT_ARCH_IA32  1\n#endif /* ITT_ARCH_IA32 */\n\n#ifndef ITT_ARCH_IA32E\n#  define ITT_ARCH_IA32E 2\n#endif /* ITT_ARCH_IA32E */\n\n/* Was there a magical reason we didn't have 3 here before? */\n#ifndef ITT_ARCH_AARCH64\n#  define ITT_ARCH_AARCH64  3\n#endif /* ITT_ARCH_AARCH64 */\n\n#ifndef ITT_ARCH_ARM\n#  define ITT_ARCH_ARM  4\n#endif /* ITT_ARCH_ARM */\n\n#ifndef ITT_ARCH_PPC64\n#  define ITT_ARCH_PPC64  5\n#endif /* ITT_ARCH_PPC64 */\n\n#ifndef ITT_ARCH_MIPS\n#  define ITT_ARCH_MIPS  6\n#endif /* ITT_ARCH_MIPS */\n\n#ifndef ITT_ARCH_MIPS64\n#  define ITT_ARCH_MIPS64  6\n#endif /* ITT_ARCH_MIPS64 */\n\n#ifndef ITT_ARCH_RISCV64\n#  define ITT_ARCH_RISCV64  7\n#endif /* ITT_ARCH_RISCV64 */\n\n#ifndef ITT_ARCH\n#  if defined _M_IX86 || defined __i386__\n#    define ITT_ARCH ITT_ARCH_IA32\n#  elif defined _M_X64 || defined _M_AMD64 || defined __x86_64__\n#    define ITT_ARCH ITT_ARCH_IA32E\n#  elif defined _M_IA64 || defined __ia64__\n#    define ITT_ARCH ITT_ARCH_IA64\n#  elif defined _M_ARM || defined __arm__\n#    define ITT_ARCH ITT_ARCH_ARM\n#  elif defined __powerpc64__\n#    define ITT_ARCH ITT_ARCH_PPC64\n#  elif defined __aarch64__\n#    define ITT_ARCH ITT_ARCH_AARCH64\n#  elif defined __mips__ && !defined __mips64\n#    define ITT_ARCH ITT_ARCH_MIPS\n#  elif defined __mips__ && defined __mips64\n#    define ITT_ARCH ITT_ARCH_MIPS64\n#  elif defined __riscv && __riscv_xlen == 64\n#    define ITT_ARCH ITT_ARCH_RISCV64\n#  endif\n#endif\n\n#ifdef __cplusplus\n#  define ITT_EXTERN_C extern \"C\"\n#  define ITT_EXTERN_C_BEGIN extern \"C\" {\n#  define ITT_EXTERN_C_END }\n#else\n#  define ITT_EXTERN_C /* nothing */\n#  define ITT_EXTERN_C_BEGIN /* nothing */\n#  define ITT_EXTERN_C_END /* nothing */\n#endif /* __cplusplus */\n\n#define ITT_TO_STR_AUX(x) #x\n#define ITT_TO_STR(x)     ITT_TO_STR_AUX(x)\n\n#define __ITT_BUILD_ASSERT(expr, suffix) do { \\\n    static char __itt_build_check_##suffix[(expr) ? 1 : -1]; \\\n    __itt_build_check_##suffix[0] = 0; \\\n} while(0)\n#define _ITT_BUILD_ASSERT(expr, suffix)  __ITT_BUILD_ASSERT((expr), suffix)\n#define ITT_BUILD_ASSERT(expr)           _ITT_BUILD_ASSERT((expr), __LINE__)\n\n#define ITT_MAGIC { 0xED, 0xAB, 0xAB, 0xEC, 0x0D, 0xEE, 0xDA, 0x30 }\n\n/* Replace with snapshot date YYYYMMDD for promotion build. */\n#define API_VERSION_BUILD    20151119\n\n#ifndef API_VERSION_NUM\n#define API_VERSION_NUM 0.0.0\n#endif /* API_VERSION_NUM */\n\n#define API_VERSION \"ITT-API-Version \" ITT_TO_STR(API_VERSION_NUM) \\\n                                \" (\" ITT_TO_STR(API_VERSION_BUILD) \")\"\n\n/* OS communication functions */\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <windows.h>\ntypedef HMODULE           lib_t;\ntypedef DWORD             TIDT;\ntypedef CRITICAL_SECTION  mutex_t;\n#define MUTEX_INITIALIZER { 0 }\n#define strong_alias(name, aliasname) /* empty for Windows */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <dlfcn.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE */\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1 /* need for PTHREAD_MUTEX_RECURSIVE */\n#endif /* _GNU_SOURCE */\n#ifndef __USE_UNIX98\n#define __USE_UNIX98 1 /* need for PTHREAD_MUTEX_RECURSIVE, on SLES11.1 with gcc 4.3.4 wherein pthread.h missing dependency on __USE_XOPEN2K8 */\n#endif /*__USE_UNIX98*/\n#include <pthread.h>\ntypedef void*             lib_t;\ntypedef pthread_t         TIDT;\ntypedef pthread_mutex_t   mutex_t;\n#define MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n#define _strong_alias(name, aliasname) \\\n            extern __typeof (name) aliasname __attribute__ ((alias (#name)));\n#define strong_alias(name, aliasname) _strong_alias(name, aliasname)\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#define __itt_get_proc(lib, name) GetProcAddress(lib, name)\n#define __itt_mutex_init(mutex)   InitializeCriticalSection(mutex)\n#define __itt_mutex_lock(mutex)   EnterCriticalSection(mutex)\n#define __itt_mutex_unlock(mutex) LeaveCriticalSection(mutex)\n#define __itt_load_lib(name)      LoadLibraryA(name)\n#define __itt_unload_lib(handle)  FreeLibrary(handle)\n#define __itt_system_error()      (int)GetLastError()\n#define __itt_fstrcmp(s1, s2)     lstrcmpA(s1, s2)\n#define __itt_fstrnlen(s, l)      strnlen_s(s, l)\n#define __itt_fstrcpyn(s1, b, s2, l) strncpy_s(s1, b, s2, l)\n#define __itt_fstrdup(s)          _strdup(s)\n#define __itt_thread_id()         GetCurrentThreadId()\n#define __itt_thread_yield()      SwitchToThread()\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return InterlockedIncrement(ptr);\n}\n#endif /* ITT_SIMPLE_INIT */\n\n#define DL_SYMBOLS (1)\n#define PTHREAD_SYMBOLS (1)\n\n#else /* ITT_PLATFORM!=ITT_PLATFORM_WIN */\n#define __itt_get_proc(lib, name) dlsym(lib, name)\n#define __itt_mutex_init(mutex)   {\\\n    pthread_mutexattr_t mutex_attr;                                         \\\n    int error_code = pthread_mutexattr_init(&mutex_attr);                   \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_init\",    \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_settype(&mutex_attr,                     \\\n                                           PTHREAD_MUTEX_RECURSIVE);        \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_settype\", \\\n                           error_code);                                     \\\n    error_code = pthread_mutex_init(mutex, &mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutex_init\",        \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_destroy(&mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_destroy\", \\\n                           error_code);                                     \\\n}\n#define __itt_mutex_lock(mutex)   pthread_mutex_lock(mutex)\n#define __itt_mutex_unlock(mutex) pthread_mutex_unlock(mutex)\n#define __itt_load_lib(name)      dlopen(name, RTLD_LAZY)\n#define __itt_unload_lib(handle)  dlclose(handle)\n#define __itt_system_error()      errno\n#define __itt_fstrcmp(s1, s2)     strcmp(s1, s2)\n\n/* makes customer code define safe APIs for SDL_STRNLEN_S and SDL_STRNCPY_S */\n#ifdef SDL_STRNLEN_S\n#define __itt_fstrnlen(s, l)      SDL_STRNLEN_S(s, l)\n#else\n#define __itt_fstrnlen(s, l)      strlen(s)\n#endif /* SDL_STRNLEN_S */\n#ifdef SDL_STRNCPY_S\n#define __itt_fstrcpyn(s1, b, s2, l) SDL_STRNCPY_S(s1, b, s2, l)\n#else\n#define __itt_fstrcpyn(s1, b, s2, l) strncpy(s1, s2, l)\n#endif /* SDL_STRNCPY_S */\n\n#define __itt_fstrdup(s)          strdup(s)\n#define __itt_thread_id()         pthread_self()\n#define __itt_thread_yield()      sched_yield()\n#if ITT_ARCH==ITT_ARCH_IA64\n#ifdef __INTEL_COMPILER\n#define __TBB_machine_fetchadd4(addr, val) __fetchadd4_acq((void *)addr, val)\n#else  /* __INTEL_COMPILER */\n/* TODO: Add Support for not Intel compilers for IA-64 architecture */\n#endif /* __INTEL_COMPILER */\n#elif ITT_ARCH==ITT_ARCH_IA32 || ITT_ARCH==ITT_ARCH_IA32E /* ITT_ARCH!=ITT_ARCH_IA64 */\nITT_INLINE long\n__TBB_machine_fetchadd4(volatile void* ptr, long addend) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __TBB_machine_fetchadd4(volatile void* ptr, long addend)\n{\n    long result;\n    __asm__ __volatile__(\"lock\\nxadd %0,%1\"\n                          : \"=r\"(result),\"=m\"(*(volatile int*)ptr)\n                          : \"0\"(addend), \"m\"(*(volatile int*)ptr)\n                          : \"memory\");\n    return result;\n}\n#elif ITT_ARCH == ITT_ARCH_ARM || ITT_ARCH == ITT_ARCH_PPC64 ||                \\\n    ITT_ARCH == ITT_ARCH_AARCH64 || ITT_ARCH == ITT_ARCH_MIPS ||               \\\n    ITT_ARCH == ITT_ARCH_MIPS64 || ITT_ARCH == ITT_ARCH_RISCV64\n#define __TBB_machine_fetchadd4(addr, val) __sync_fetch_and_add(addr, val)\n#endif /* ITT_ARCH==ITT_ARCH_IA64 */\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return __TBB_machine_fetchadd4(ptr, 1) + 1L;\n}\n#endif /* ITT_SIMPLE_INIT */\n\nvoid* dlopen(const char*, int) __attribute__((weak));\nvoid* dlsym(void*, const char*) __attribute__((weak));\nint dlclose(void*) __attribute__((weak));\n#define DL_SYMBOLS (dlopen && dlsym && dlclose)\n\nint pthread_mutex_init(pthread_mutex_t*, const pthread_mutexattr_t*) __attribute__((weak));\nint pthread_mutex_lock(pthread_mutex_t*) __attribute__((weak));\nint pthread_mutex_unlock(pthread_mutex_t*) __attribute__((weak));\nint pthread_mutex_destroy(pthread_mutex_t*) __attribute__((weak));\nint pthread_mutexattr_init(pthread_mutexattr_t*) __attribute__((weak));\nint pthread_mutexattr_settype(pthread_mutexattr_t*, int) __attribute__((weak));\nint pthread_mutexattr_destroy(pthread_mutexattr_t*) __attribute__((weak));\npthread_t pthread_self(void) __attribute__((weak));\n#define PTHREAD_SYMBOLS (pthread_mutex_init && pthread_mutex_lock && pthread_mutex_unlock && pthread_mutex_destroy && pthread_mutexattr_init && pthread_mutexattr_settype && pthread_mutexattr_destroy && pthread_self)\n\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\ntypedef enum {\n    __itt_collection_normal = 0,\n    __itt_collection_paused = 1\n} __itt_collection_state;\n\ntypedef enum {\n    __itt_thread_normal  = 0,\n    __itt_thread_ignored = 1\n} __itt_thread_state;\n\n#pragma pack(push, 8)\n\ntypedef struct ___itt_thread_info\n{\n    const char* nameA; /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    TIDT               tid;\n    __itt_thread_state state;   /*!< Thread state (paused or normal) */\n    int                extra1;  /*!< Reserved to the runtime */\n    void*              extra2;  /*!< Reserved to the runtime */\n    struct ___itt_thread_info* next;\n} __itt_thread_info;\n\n#include \"ittnotify_types.h\" /* For __itt_group_id definition */\n\ntypedef struct ___itt_api_info_20101001\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    __itt_group_id group;\n}  __itt_api_info_20101001;\n\ntypedef struct ___itt_api_info\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    void*          null_func;\n    __itt_group_id group;\n}  __itt_api_info;\n\ntypedef struct __itt_counter_info\n{\n    const char* nameA;  /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    const char* domainA;  /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* domainW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* domainW;\n#endif /* UNICODE || _UNICODE */\n    int type;\n    long index;\n    int   extra1; /*!< Reserved to the runtime */\n    void* extra2; /*!< Reserved to the runtime */\n    struct __itt_counter_info* next;\n}  __itt_counter_info_t;\n\nstruct ___itt_domain;\nstruct ___itt_string_handle;\n\ntypedef struct ___itt_global\n{\n    unsigned char          magic[8];\n    unsigned long          version_major;\n    unsigned long          version_minor;\n    unsigned long          version_build;\n    volatile long          api_initialized;\n    volatile long          mutex_initialized;\n    volatile long          atomic_counter;\n    mutex_t                mutex;\n    lib_t                  lib;\n    void*                  error_handler;\n    const char**           dll_path_ptr;\n    __itt_api_info*        api_list_ptr;\n    struct ___itt_global*  next;\n    /* Joinable structures below */\n    __itt_thread_info*     thread_list;\n    struct ___itt_domain*  domain_list;\n    struct ___itt_string_handle* string_list;\n    __itt_collection_state state;\n    __itt_counter_info_t* counter_list;\n} __itt_global;\n\n#pragma pack(pop)\n\n#define NEW_THREAD_INFO_W(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = NULL; \\\n        h->nameW  = n ? _wcsdup(n) : NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_THREAD_INFO_A(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = n ? __itt_fstrdup(n) : NULL; \\\n        h->nameW  = NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_W(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 1;    /* domain is enabled by default */ \\\n        h->nameA  = NULL; \\\n        h->nameW  = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_A(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 1;    /* domain is enabled by default */ \\\n        h->nameA  = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW  = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_W(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = NULL; \\\n        h->strW   = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_A(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->strW   = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_COUNTER_W(gptr,h,h_tail,name,domain,type) { \\\n    h = (__itt_counter_info_t*)malloc(sizeof(__itt_counter_info_t)); \\\n    if (h != NULL) { \\\n        h->nameA   = NULL; \\\n        h->nameW   = name ? _wcsdup(name) : NULL; \\\n        h->domainA   = NULL; \\\n        h->domainW   = name ? _wcsdup(domain) : NULL; \\\n        h->type = type; \\\n        h->index = 0; \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->counter_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_COUNTER_A(gptr,h,h_tail,name,domain,type) { \\\n    h = (__itt_counter_info_t*)malloc(sizeof(__itt_counter_info_t)); \\\n    if (h != NULL) { \\\n        h->nameA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW   = NULL; \\\n        h->domainA   = domain ? __itt_fstrdup(domain) : NULL; \\\n        h->domainW   = NULL; \\\n        h->type = type; \\\n        h->index = 0; \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->counter_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#endif /* _ITTNOTIFY_CONFIG_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/libomptarget/src/rtl.cpp": "//===----------- rtl.cpp - Target independent OpenMP target RTL -----------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Functionality for handling RTL plugins.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"device.h\"\n#include \"private.h\"\n#include \"rtl.h\"\n\n#include <cassert>\n#include <cstdlib>\n#include <cstring>\n#include <dlfcn.h>\n#include <mutex>\n#include <string>\n\n// List of all plugins that can support offloading.\nstatic const char *RTLNames[] = {\n    /* SX-Aurora VE target  */ \"libomptarget.rtl.ve.so\",\n    /* PowerPC target */ \"libomptarget.rtl.ppc64.so\",\n    /* x86_64 target  */ \"libomptarget.rtl.x86_64.so\",\n    /* CUDA target    */ \"libomptarget.rtl.cuda.so\",\n    /* AArch64 target */ \"libomptarget.rtl.aarch64.so\"};\n\nRTLsTy *RTLs;\nstd::mutex *RTLsMtx;\n\nHostEntriesBeginToTransTableTy *HostEntriesBeginToTransTable;\nstd::mutex *TrlTblMtx;\n\nHostPtrToTableMapTy *HostPtrToTableMap;\nstd::mutex *TblMapMtx;\n\n__attribute__((constructor(101))) void init() {\n  DP(\"Init target library!\\n\");\n  RTLs = new RTLsTy();\n  RTLsMtx = new std::mutex();\n  HostEntriesBeginToTransTable = new HostEntriesBeginToTransTableTy();\n  TrlTblMtx = new std::mutex();\n  HostPtrToTableMap = new HostPtrToTableMapTy();\n  TblMapMtx = new std::mutex();\n}\n\n__attribute__((destructor(101))) void deinit() {\n  DP(\"Deinit target library!\\n\");\n  delete RTLs;\n  delete RTLsMtx;\n  delete HostEntriesBeginToTransTable;\n  delete TrlTblMtx;\n  delete HostPtrToTableMap;\n  delete TblMapMtx;\n}\n\nvoid RTLsTy::LoadRTLs() {\n#ifdef OMPTARGET_DEBUG\n  if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n    DebugLevel = std::stoi(envStr);\n  }\n#endif // OMPTARGET_DEBUG\n\n  // Parse environment variable OMP_TARGET_OFFLOAD (if set)\n  TargetOffloadPolicy = (kmp_target_offload_kind_t) __kmpc_get_target_offload();\n  if (TargetOffloadPolicy == tgt_disabled) {\n    return;\n  }\n\n  DP(\"Loading RTLs...\\n\");\n\n  // Attempt to open all the plugins and, if they exist, check if the interface\n  // is correct and if they are supporting any devices.\n  for (auto *Name : RTLNames) {\n    DP(\"Loading library '%s'...\\n\", Name);\n    void *dynlib_handle = dlopen(Name, RTLD_NOW);\n\n    if (!dynlib_handle) {\n      // Library does not exist or cannot be found.\n      DP(\"Unable to load library '%s': %s!\\n\", Name, dlerror());\n      continue;\n    }\n\n    DP(\"Successfully loaded library '%s'!\\n\", Name);\n\n    // Retrieve the RTL information from the runtime library.\n    RTLInfoTy R;\n\n    R.LibraryHandler = dynlib_handle;\n    R.isUsed = false;\n\n#ifdef OMPTARGET_DEBUG\n    R.RTLName = Name;\n#endif\n\n    if (!(*((void **)&R.is_valid_binary) =\n              dlsym(dynlib_handle, \"__tgt_rtl_is_valid_binary\")))\n      continue;\n    if (!(*((void **)&R.number_of_devices) =\n              dlsym(dynlib_handle, \"__tgt_rtl_number_of_devices\")))\n      continue;\n    if (!(*((void **)&R.init_device) =\n              dlsym(dynlib_handle, \"__tgt_rtl_init_device\")))\n      continue;\n    if (!(*((void **)&R.load_binary) =\n              dlsym(dynlib_handle, \"__tgt_rtl_load_binary\")))\n      continue;\n    if (!(*((void **)&R.data_alloc) =\n              dlsym(dynlib_handle, \"__tgt_rtl_data_alloc\")))\n      continue;\n    if (!(*((void **)&R.data_submit) =\n              dlsym(dynlib_handle, \"__tgt_rtl_data_submit\")))\n      continue;\n    if (!(*((void **)&R.data_retrieve) =\n              dlsym(dynlib_handle, \"__tgt_rtl_data_retrieve\")))\n      continue;\n    if (!(*((void **)&R.data_delete) =\n              dlsym(dynlib_handle, \"__tgt_rtl_data_delete\")))\n      continue;\n    if (!(*((void **)&R.run_region) =\n              dlsym(dynlib_handle, \"__tgt_rtl_run_target_region\")))\n      continue;\n    if (!(*((void **)&R.run_team_region) =\n              dlsym(dynlib_handle, \"__tgt_rtl_run_target_team_region\")))\n      continue;\n\n    // Optional functions\n    *((void **)&R.init_requires) =\n        dlsym(dynlib_handle, \"__tgt_rtl_init_requires\");\n    *((void **)&R.data_submit_async) =\n        dlsym(dynlib_handle, \"__tgt_rtl_data_submit_async\");\n    *((void **)&R.data_retrieve_async) =\n        dlsym(dynlib_handle, \"__tgt_rtl_data_retrieve_async\");\n    *((void **)&R.run_region_async) =\n        dlsym(dynlib_handle, \"__tgt_rtl_run_target_region_async\");\n    *((void **)&R.run_team_region_async) =\n        dlsym(dynlib_handle, \"__tgt_rtl_run_target_team_region_async\");\n    *((void **)&R.synchronize) = dlsym(dynlib_handle, \"__tgt_rtl_synchronize\");\n    *((void **)&R.data_exchange) =\n        dlsym(dynlib_handle, \"__tgt_rtl_data_exchange\");\n    *((void **)&R.data_exchange_async) =\n        dlsym(dynlib_handle, \"__tgt_rtl_data_exchange_async\");\n    *((void **)&R.is_data_exchangable) =\n        dlsym(dynlib_handle, \"__tgt_rtl_is_data_exchangable\");\n\n    // No devices are supported by this RTL?\n    if (!(R.NumberOfDevices = R.number_of_devices())) {\n      DP(\"No devices supported in this RTL\\n\");\n      continue;\n    }\n\n    DP(\"Registering RTL %s supporting %d devices!\\n\", R.RTLName.c_str(),\n       R.NumberOfDevices);\n\n    // The RTL is valid! Will save the information in the RTLs list.\n    AllRTLs.push_back(R);\n  }\n\n  DP(\"RTLs loaded!\\n\");\n\n  return;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Functionality for registering libs\n\nstatic void RegisterImageIntoTranslationTable(TranslationTable &TT,\n    RTLInfoTy &RTL, __tgt_device_image *image) {\n\n  // same size, as when we increase one, we also increase the other.\n  assert(TT.TargetsTable.size() == TT.TargetsImages.size() &&\n         \"We should have as many images as we have tables!\");\n\n  // Resize the Targets Table and Images to accommodate the new targets if\n  // required\n  unsigned TargetsTableMinimumSize = RTL.Idx + RTL.NumberOfDevices;\n\n  if (TT.TargetsTable.size() < TargetsTableMinimumSize) {\n    TT.TargetsImages.resize(TargetsTableMinimumSize, 0);\n    TT.TargetsTable.resize(TargetsTableMinimumSize, 0);\n  }\n\n  // Register the image in all devices for this target type.\n  for (int32_t i = 0; i < RTL.NumberOfDevices; ++i) {\n    // If we are changing the image we are also invalidating the target table.\n    if (TT.TargetsImages[RTL.Idx + i] != image) {\n      TT.TargetsImages[RTL.Idx + i] = image;\n      TT.TargetsTable[RTL.Idx + i] = 0; // lazy initialization of target table.\n    }\n  }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Functionality for registering Ctors/Dtors\n\nstatic void RegisterGlobalCtorsDtorsForImage(__tgt_bin_desc *desc,\n    __tgt_device_image *img, RTLInfoTy *RTL) {\n\n  for (int32_t i = 0; i < RTL->NumberOfDevices; ++i) {\n    DeviceTy &Device = Devices[RTL->Idx + i];\n    Device.PendingGlobalsMtx.lock();\n    Device.HasPendingGlobals = true;\n    for (__tgt_offload_entry *entry = img->EntriesBegin;\n        entry != img->EntriesEnd; ++entry) {\n      if (entry->flags & OMP_DECLARE_TARGET_CTOR) {\n        DP(\"Adding ctor \" DPxMOD \" to the pending list.\\n\",\n            DPxPTR(entry->addr));\n        Device.PendingCtorsDtors[desc].PendingCtors.push_back(entry->addr);\n      } else if (entry->flags & OMP_DECLARE_TARGET_DTOR) {\n        // Dtors are pushed in reverse order so they are executed from end\n        // to beginning when unregistering the library!\n        DP(\"Adding dtor \" DPxMOD \" to the pending list.\\n\",\n            DPxPTR(entry->addr));\n        Device.PendingCtorsDtors[desc].PendingDtors.push_front(entry->addr);\n      }\n\n      if (entry->flags & OMP_DECLARE_TARGET_LINK) {\n        DP(\"The \\\"link\\\" attribute is not yet supported!\\n\");\n      }\n    }\n    Device.PendingGlobalsMtx.unlock();\n  }\n}\n\nvoid RTLsTy::RegisterRequires(int64_t flags) {\n  // TODO: add more elaborate check.\n  // Minimal check: only set requires flags if previous value\n  // is undefined. This ensures that only the first call to this\n  // function will set the requires flags. All subsequent calls\n  // will be checked for compatibility.\n  assert(flags != OMP_REQ_UNDEFINED &&\n         \"illegal undefined flag for requires directive!\");\n  if (RequiresFlags == OMP_REQ_UNDEFINED) {\n    RequiresFlags = flags;\n    return;\n  }\n\n  // If multiple compilation units are present enforce\n  // consistency across all of them for require clauses:\n  //  - reverse_offload\n  //  - unified_address\n  //  - unified_shared_memory\n  if ((RequiresFlags & OMP_REQ_REVERSE_OFFLOAD) !=\n      (flags & OMP_REQ_REVERSE_OFFLOAD)) {\n    FATAL_MESSAGE0(1,\n        \"'#pragma omp requires reverse_offload' not used consistently!\");\n  }\n  if ((RequiresFlags & OMP_REQ_UNIFIED_ADDRESS) !=\n          (flags & OMP_REQ_UNIFIED_ADDRESS)) {\n    FATAL_MESSAGE0(1,\n        \"'#pragma omp requires unified_address' not used consistently!\");\n  }\n  if ((RequiresFlags & OMP_REQ_UNIFIED_SHARED_MEMORY) !=\n          (flags & OMP_REQ_UNIFIED_SHARED_MEMORY)) {\n    FATAL_MESSAGE0(1,\n        \"'#pragma omp requires unified_shared_memory' not used consistently!\");\n  }\n\n  // TODO: insert any other missing checks\n\n  DP(\"New requires flags %ld compatible with existing %ld!\\n\",\n     flags, RequiresFlags);\n}\n\nvoid RTLsTy::RegisterLib(__tgt_bin_desc *desc) {\n  // Attempt to load all plugins available in the system.\n  std::call_once(initFlag, &RTLsTy::LoadRTLs, this);\n\n  RTLsMtx->lock();\n  // Register the images with the RTLs that understand them, if any.\n  for (int32_t i = 0; i < desc->NumDeviceImages; ++i) {\n    // Obtain the image.\n    __tgt_device_image *img = &desc->DeviceImages[i];\n\n    RTLInfoTy *FoundRTL = NULL;\n\n    // Scan the RTLs that have associated images until we find one that supports\n    // the current image.\n    for (auto &R : AllRTLs) {\n      if (!R.is_valid_binary(img)) {\n        DP(\"Image \" DPxMOD \" is NOT compatible with RTL %s!\\n\",\n            DPxPTR(img->ImageStart), R.RTLName.c_str());\n        continue;\n      }\n\n      DP(\"Image \" DPxMOD \" is compatible with RTL %s!\\n\",\n          DPxPTR(img->ImageStart), R.RTLName.c_str());\n\n      // If this RTL is not already in use, initialize it.\n      if (!R.isUsed) {\n        // Initialize the device information for the RTL we are about to use.\n        DeviceTy device(&R);\n        size_t start = Devices.size();\n        Devices.resize(start + R.NumberOfDevices, device);\n        for (int32_t device_id = 0; device_id < R.NumberOfDevices;\n            device_id++) {\n          // global device ID\n          Devices[start + device_id].DeviceID = start + device_id;\n          // RTL local device ID\n          Devices[start + device_id].RTLDeviceID = device_id;\n        }\n\n        // Initialize the index of this RTL and save it in the used RTLs.\n        R.Idx = (UsedRTLs.empty())\n                    ? 0\n                    : UsedRTLs.back()->Idx + UsedRTLs.back()->NumberOfDevices;\n        assert((size_t) R.Idx == start &&\n            \"RTL index should equal the number of devices used so far.\");\n        R.isUsed = true;\n        UsedRTLs.push_back(&R);\n\n        DP(\"RTL \" DPxMOD \" has index %d!\\n\", DPxPTR(R.LibraryHandler), R.Idx);\n      }\n\n      // Initialize (if necessary) translation table for this library.\n      TrlTblMtx->lock();\n      if(!HostEntriesBeginToTransTable->count(desc->HostEntriesBegin)){\n        TranslationTable &tt =\n            (*HostEntriesBeginToTransTable)[desc->HostEntriesBegin];\n        tt.HostTable.EntriesBegin = desc->HostEntriesBegin;\n        tt.HostTable.EntriesEnd = desc->HostEntriesEnd;\n      }\n\n      // Retrieve translation table for this library.\n      TranslationTable &TransTable =\n          (*HostEntriesBeginToTransTable)[desc->HostEntriesBegin];\n\n      DP(\"Registering image \" DPxMOD \" with RTL %s!\\n\",\n          DPxPTR(img->ImageStart), R.RTLName.c_str());\n      RegisterImageIntoTranslationTable(TransTable, R, img);\n      TrlTblMtx->unlock();\n      FoundRTL = &R;\n\n      // Load ctors/dtors for static objects\n      RegisterGlobalCtorsDtorsForImage(desc, img, FoundRTL);\n\n      // if an RTL was found we are done - proceed to register the next image\n      break;\n    }\n\n    if (!FoundRTL) {\n      DP(\"No RTL found for image \" DPxMOD \"!\\n\", DPxPTR(img->ImageStart));\n    }\n  }\n  RTLsMtx->unlock();\n\n\n  DP(\"Done registering entries!\\n\");\n}\n\nvoid RTLsTy::UnregisterLib(__tgt_bin_desc *desc) {\n  DP(\"Unloading target library!\\n\");\n\n  RTLsMtx->lock();\n  // Find which RTL understands each image, if any.\n  for (int32_t i = 0; i < desc->NumDeviceImages; ++i) {\n    // Obtain the image.\n    __tgt_device_image *img = &desc->DeviceImages[i];\n\n    RTLInfoTy *FoundRTL = NULL;\n\n    // Scan the RTLs that have associated images until we find one that supports\n    // the current image. We only need to scan RTLs that are already being used.\n    for (auto *R : UsedRTLs) {\n\n      assert(R->isUsed && \"Expecting used RTLs.\");\n\n      if (!R->is_valid_binary(img)) {\n        DP(\"Image \" DPxMOD \" is NOT compatible with RTL \" DPxMOD \"!\\n\",\n            DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n        continue;\n      }\n\n      DP(\"Image \" DPxMOD \" is compatible with RTL \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n\n      FoundRTL = R;\n\n      // Execute dtors for static objects if the device has been used, i.e.\n      // if its PendingCtors list has been emptied.\n      for (int32_t i = 0; i < FoundRTL->NumberOfDevices; ++i) {\n        DeviceTy &Device = Devices[FoundRTL->Idx + i];\n        Device.PendingGlobalsMtx.lock();\n        if (Device.PendingCtorsDtors[desc].PendingCtors.empty()) {\n          for (auto &dtor : Device.PendingCtorsDtors[desc].PendingDtors) {\n            int rc = target(Device.DeviceID, dtor, 0, NULL, NULL, NULL, NULL, 1,\n                1, true /*team*/);\n            if (rc != OFFLOAD_SUCCESS) {\n              DP(\"Running destructor \" DPxMOD \" failed.\\n\", DPxPTR(dtor));\n            }\n          }\n          // Remove this library's entry from PendingCtorsDtors\n          Device.PendingCtorsDtors.erase(desc);\n        }\n        Device.PendingGlobalsMtx.unlock();\n      }\n\n      DP(\"Unregistered image \" DPxMOD \" from RTL \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n\n      break;\n    }\n\n    // if no RTL was found proceed to unregister the next image\n    if (!FoundRTL){\n      DP(\"No RTLs in use support the image \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart));\n    }\n  }\n  RTLsMtx->unlock();\n  DP(\"Done unregistering images!\\n\");\n\n  // Remove entries from HostPtrToTableMap\n  TblMapMtx->lock();\n  for (__tgt_offload_entry *cur = desc->HostEntriesBegin;\n      cur < desc->HostEntriesEnd; ++cur) {\n    HostPtrToTableMap->erase(cur->addr);\n  }\n\n  // Remove translation table for this descriptor.\n  auto tt = HostEntriesBeginToTransTable->find(desc->HostEntriesBegin);\n  if (tt != HostEntriesBeginToTransTable->end()) {\n    DP(\"Removing translation table for descriptor \" DPxMOD \"\\n\",\n        DPxPTR(desc->HostEntriesBegin));\n    HostEntriesBeginToTransTable->erase(tt);\n  } else {\n    DP(\"Translation table for descriptor \" DPxMOD \" cannot be found, probably \"\n        \"it has been already removed.\\n\", DPxPTR(desc->HostEntriesBegin));\n  }\n\n  TblMapMtx->unlock();\n\n  // TODO: Remove RTL and the devices it manages if it's not used anymore?\n  // TODO: Write some RTL->unload_image(...) function?\n\n  DP(\"Done unregistering library!\\n\");\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/libomptarget/plugins/ve/src/rtl.cpp": "//===-RTLs/nec-aurora/src/rtl.cpp - Target RTLs Implementation - C++ -*-======//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n//\n// RTL for NEC Aurora TSUBASA machines\n//\n//===----------------------------------------------------------------------===//\n\n#include \"omptargetplugin.h\"\n\n#include <algorithm>\n#include <cassert>\n#include <cerrno>\n#include <cstring>\n#include <list>\n#include <stdlib.h>\n#include <string>\n#include <sys/stat.h>\n#include <ve_offload.h>\n#include <vector>\n#include <veosinfo/veosinfo.h>\n\n#ifndef TARGET_ELF_ID\n#define TARGET_ELF_ID 0\n#endif\n\n#ifdef OMPTARGET_DEBUG\nstatic int DebugLevel = 0;\n\n#define GETNAME2(name) #name\n#define GETNAME(name) GETNAME2(name)\n#define DP(...)                                                                \\\n  do {                                                                         \\\n    if (DebugLevel > 0) {                                                      \\\n      DEBUGP(\"Target \" GETNAME(TARGET_NAME) \" RTL\", __VA_ARGS__);              \\\n    }                                                                          \\\n  } while (false)\n#else // OMPTARGET_DEBUG\n#define DP(...)                                                                \\\n  {}\n#endif // OMPTARGET_DEBUG\n\n#include \"../../common/elf_common.c\"\n\nstruct DynLibTy {\n  char *FileName;\n  uint64_t VeoLibHandle;\n};\n\n/// Keep entries table per device.\nstruct FuncOrGblEntryTy {\n  __tgt_target_table Table;\n  std::vector<__tgt_offload_entry> Entries;\n};\n\nclass RTLDeviceInfoTy {\n  std::vector<std::list<FuncOrGblEntryTy>> FuncOrGblEntry;\n\npublic:\n  std::vector<struct veo_proc_handle *> ProcHandles;\n  std::vector<struct veo_thr_ctxt *> Contexts;\n  std::vector<uint64_t> LibraryHandles;\n  std::list<DynLibTy> DynLibs;\n  // Maps OpenMP device Ids to Ve nodeids\n  std::vector<int> NodeIds;\n\n  void buildOffloadTableFromHost(int32_t device_id, uint64_t VeoLibHandle,\n                                 __tgt_offload_entry *HostBegin,\n                                 __tgt_offload_entry *HostEnd) {\n    FuncOrGblEntry[device_id].emplace_back();\n    std::vector<__tgt_offload_entry> &T =\n        FuncOrGblEntry[device_id].back().Entries;\n    T.clear();\n    for (__tgt_offload_entry *i = HostBegin; i != HostEnd; ++i) {\n      char *SymbolName = i->name;\n      // we have not enough access to the target memory to conveniently parse\n      // the offload table there so we need to lookup every symbol with the host\n      // table\n      DP(\"Looking up symbol: %s\\n\", SymbolName);\n      uint64_t SymbolTargetAddr =\n          veo_get_sym(ProcHandles[device_id], VeoLibHandle, SymbolName);\n      __tgt_offload_entry Entry;\n\n      if (!SymbolTargetAddr) {\n        DP(\"Symbol %s not found in target image\\n\", SymbolName);\n        Entry = {NULL, NULL, 0, 0, 0};\n      } else {\n        DP(\"Found symbol %s successfully in target image (addr: %p)\\n\",\n           SymbolName, reinterpret_cast<void *>(SymbolTargetAddr));\n        Entry = { reinterpret_cast<void *>(SymbolTargetAddr),\n                  i->name,\n                  i->size,\n                  i->flags,\n                  0 };\n      }\n\n      T.push_back(Entry);\n    }\n\n    FuncOrGblEntry[device_id].back().Table.EntriesBegin = &T.front();\n    FuncOrGblEntry[device_id].back().Table.EntriesEnd = &T.back() + 1;\n  }\n\n  __tgt_target_table *getOffloadTable(int32_t device_id) {\n    return &FuncOrGblEntry[device_id].back().Table;\n  }\n\n  RTLDeviceInfoTy() {\n#ifdef OMPTARGET_DEBUG\n    if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n      DebugLevel = std::stoi(envStr);\n    }\n#endif // OMPTARGET_DEBUG\n\n    struct ve_nodeinfo node_info;\n    ve_node_info(&node_info);\n\n    // Build a predictable mapping between VE node ids and OpenMP device ids.\n    // This is necessary, because nodes can be missing or offline and (active)\n    // node ids are thus not consecutive. The entries in ve_nodeinfo may also\n    // not be in the order of their node ids.\n    for (int i = 0; i < node_info.total_node_count; ++i) {\n      if (node_info.status[i] == 0) {\n        NodeIds.push_back(node_info.nodeid[i]);\n      }\n    }\n\n    // Because the entries in ve_nodeinfo may not be in the order of their node\n    // ids, we sort NodeIds to get a predictable mapping.\n    std::sort(NodeIds.begin(), NodeIds.end());\n\n    int NumDevices = NodeIds.size();\n    DP(\"Found %i VE devices\\n\", NumDevices);\n    ProcHandles.resize(NumDevices, NULL);\n    Contexts.resize(NumDevices, NULL);\n    FuncOrGblEntry.resize(NumDevices);\n    LibraryHandles.resize(NumDevices);\n  }\n\n  ~RTLDeviceInfoTy() {\n    for (auto &ctx : Contexts) {\n      if (ctx != NULL) {\n        if (veo_context_close(ctx) != 0) {\n          DP(\"Failed to close VEO context.\\n\");\n        }\n      }\n    }\n\n    for (auto &hdl : ProcHandles) {\n      if (hdl != NULL) {\n        veo_proc_destroy(hdl);\n      }\n    }\n\n    for (auto &lib : DynLibs) {\n      if (lib.FileName) {\n        remove(lib.FileName);\n      }\n    }\n  }\n};\n\nstatic RTLDeviceInfoTy DeviceInfo;\n\nstatic int target_run_function_wait(uint32_t DeviceID, uint64_t FuncAddr,\n                                    struct veo_args *args, uint64_t *RetVal) {\n  DP(\"Running function with entry point %p\\n\",\n     reinterpret_cast<void *>(FuncAddr));\n  uint64_t RequestHandle =\n      veo_call_async(DeviceInfo.Contexts[DeviceID], FuncAddr, args);\n  if (RequestHandle == VEO_REQUEST_ID_INVALID) {\n    DP(\"Execution of entry point %p failed\\n\",\n       reinterpret_cast<void *>(FuncAddr));\n    return OFFLOAD_FAIL;\n  }\n\n  DP(\"Function at address %p called (VEO request ID: %\" PRIu64 \")\\n\",\n     reinterpret_cast<void *>(FuncAddr), RequestHandle);\n\n  int ret = veo_call_wait_result(DeviceInfo.Contexts[DeviceID], RequestHandle,\n                                 RetVal);\n  if (ret != 0) {\n    DP(\"Waiting for entry point %p failed (Error code %d)\\n\",\n       reinterpret_cast<void *>(FuncAddr), ret);\n    return OFFLOAD_FAIL;\n  }\n  return OFFLOAD_SUCCESS;\n}\n\n\n// Return the number of available devices of the type supported by the\n// target RTL.\nint32_t __tgt_rtl_number_of_devices(void) { return DeviceInfo.NodeIds.size(); }\n\n// Return an integer different from zero if the provided device image can be\n// supported by the runtime. The functionality is similar to comparing the\n// result of __tgt__rtl__load__binary to NULL. However, this is meant to be a\n// lightweight query to determine if the RTL is suitable for an image without\n// having to load the library, which can be expensive.\nint32_t __tgt_rtl_is_valid_binary(__tgt_device_image *Image) {\n#if TARGET_ELF_ID < 1\n  return 0;\n#else\n  return elf_check_machine(Image, TARGET_ELF_ID);\n#endif\n}\n\n// Initialize the specified device. In case of success return 0; otherwise\n// return an error code.\nint32_t __tgt_rtl_init_device(int32_t ID) {\n  DP(\"Available VEO version: %i\\n\", veo_api_version());\n\n  // At the moment we do not really initialize (i.e. create a process or\n  // context on) the device here, but in \"__tgt_rtl_load_binary\".\n  // The reason for this is, that, when we create a process for a statically\n  // linked binary, the VEO api needs us to already supply the binary (but we\n  // can load a dynamically linked binary later, after we create the process).\n  // At this stage, we cannot check if we have a dynamically or statically\n  // linked binary so we defer process creation until we know.\n  return OFFLOAD_SUCCESS;\n}\n\n// Pass an executable image section described by image to the specified\n// device and prepare an address table of target entities. In case of error,\n// return NULL. Otherwise, return a pointer to the built address table.\n// Individual entries in the table may also be NULL, when the corresponding\n// offload region is not supported on the target device.\n__tgt_target_table *__tgt_rtl_load_binary(int32_t ID,\n                                          __tgt_device_image *Image) {\n  DP(\"Dev %d: load binary from \" DPxMOD \" image\\n\", ID,\n     DPxPTR(Image->ImageStart));\n\n  assert(ID >= 0 && \"bad dev id\");\n\n  size_t ImageSize = (size_t)Image->ImageEnd - (size_t)Image->ImageStart;\n  size_t NumEntries = (size_t)(Image->EntriesEnd - Image->EntriesBegin);\n  DP(\"Expecting to have %zd entries defined.\\n\", NumEntries);\n\n  // load dynamic library and get the entry points. We use the dl library\n  // to do the loading of the library, but we could do it directly to avoid the\n  // dump to the temporary file.\n  //\n  // 1) Create tmp file with the library contents.\n  // 2) Use dlopen to load the file and dlsym to retrieve the symbols.\n  char tmp_name[] = \"/tmp/tmpfile_XXXXXX\";\n  int tmp_fd = mkstemp(tmp_name);\n\n  if (tmp_fd == -1) {\n    return NULL;\n  }\n\n  FILE *ftmp = fdopen(tmp_fd, \"wb\");\n\n  if (!ftmp) {\n    DP(\"fdopen() for %s failed. Could not write target image\\n\", tmp_name);\n    return NULL;\n  }\n\n  fwrite(Image->ImageStart, ImageSize, 1, ftmp);\n\n  // at least for the static case we need to change the permissions\n  chmod(tmp_name, 0700);\n\n  DP(\"Wrote target image to %s. ImageSize=%zu\\n\", tmp_name, ImageSize);\n\n  fclose(ftmp);\n\n  // See comment in \"__tgt_rtl_init_device\"\n  bool is_dyn = true;\n  if (DeviceInfo.ProcHandles[ID] == NULL) {\n    struct veo_proc_handle *proc_handle;\n    is_dyn = elf_is_dynamic(Image);\n    // If we have a dynamically linked image, we create the process handle, then\n    // the thread, and then load the image.\n    // If we have a statically linked image, we need to create the process\n    // handle and load the image at the same time with veo_proc_create_static().\n    if (is_dyn) {\n      proc_handle = veo_proc_create(DeviceInfo.NodeIds[ID]);\n      if (!proc_handle) {\n        DP(\"veo_proc_create() failed for device %d\\n\", ID);\n        return NULL;\n      }\n    } else {\n      proc_handle = veo_proc_create_static(DeviceInfo.NodeIds[ID], tmp_name);\n      if (!proc_handle) {\n        DP(\"veo_proc_create_static() failed for device %d, image=%s\\n\", ID,\n           tmp_name);\n        return NULL;\n      }\n    }\n    DeviceInfo.ProcHandles[ID] = proc_handle;\n  }\n\n  if (DeviceInfo.Contexts[ID] == NULL) {\n    struct veo_thr_ctxt *ctx = veo_context_open(DeviceInfo.ProcHandles[ID]);\n\n    if (!ctx) {\n      DP(\"veo_context_open() failed: %s\\n\", std::strerror(errno));\n      return NULL;\n    }\n\n    DeviceInfo.Contexts[ID] = ctx;\n  }\n\n  DP(\"Aurora device successfully initialized with loaded binary: \"\n     \"proc_handle=%p, ctx=%p\\n\",\n     DeviceInfo.ProcHandles[ID], DeviceInfo.Contexts[ID]);\n\n  uint64_t LibHandle = 0UL;\n  if (is_dyn) {\n    LibHandle = veo_load_library(DeviceInfo.ProcHandles[ID], tmp_name);\n\n    if (!LibHandle) {\n      DP(\"veo_load_library() failed: LibHandle=%\" PRIu64\n         \" Name=%s. Set env VEORUN_BIN for static linked target code.\\n\",\n         LibHandle, tmp_name);\n      return NULL;\n    }\n\n    DP(\"Successfully loaded library dynamically\\n\");\n  } else {\n    DP(\"Symbol table is expected to have been created by \"\n       \"veo_create_proc_static()\\n\");\n  }\n\n  DynLibTy Lib = {tmp_name, LibHandle};\n  DeviceInfo.DynLibs.push_back(Lib);\n  DeviceInfo.LibraryHandles[ID] = LibHandle;\n\n  DeviceInfo.buildOffloadTableFromHost(ID, LibHandle, Image->EntriesBegin,\n                                       Image->EntriesEnd);\n\n  return DeviceInfo.getOffloadTable(ID);\n}\n\n// Allocate data on the particular target device, of the specified size.\n// HostPtr is a address of the host data the allocated target data\n// will be associated with (HostPtr may be NULL if it is not known at\n// allocation time, like for example it would be for target data that\n// is allocated by omp_target_alloc() API). Return address of the\n// allocated data on the target that will be used by libomptarget.so to\n// initialize the target data mapping structures. These addresses are\n// used to generate a table of target variables to pass to\n// __tgt_rtl_run_region(). The __tgt_rtl_data_alloc() returns NULL in\n// case an error occurred on the target device.\nvoid *__tgt_rtl_data_alloc(int32_t ID, int64_t Size, void *HostPtr) {\n  int ret;\n  uint64_t addr;\n\n  if (DeviceInfo.ProcHandles[ID] == NULL) {\n    struct veo_proc_handle *proc_handle;\n    proc_handle = veo_proc_create(DeviceInfo.NodeIds[ID]);\n    if (!proc_handle) {\n      DP(\"veo_proc_create() failed for device %d\\n\", ID);\n      return NULL;\n    }\n    DeviceInfo.ProcHandles[ID] = proc_handle;\n    DP(\"Aurora device successfully initialized: proc_handle=%p\", proc_handle);\n  }\n\n  ret = veo_alloc_mem(DeviceInfo.ProcHandles[ID], &addr, Size);\n  DP(\"Allocate target memory: device=%d, target addr=%p, size=%\" PRIu64 \"\\n\",\n     ID, reinterpret_cast<void *>(addr), Size);\n  if (ret != 0) {\n    DP(\"veo_alloc_mem(%d, %p, %\" PRIu64 \") failed with error code %d\\n\",\n       ID, reinterpret_cast<void *>(addr), Size, ret);\n    return NULL;\n  }\n\n  return reinterpret_cast<void *>(addr);\n}\n\n// Pass the data content to the target device using the target address.\n// In case of success, return zero. Otherwise, return an error code.\nint32_t __tgt_rtl_data_submit(int32_t ID, void *TargetPtr, void *HostPtr,\n                              int64_t Size) {\n  int ret = veo_write_mem(DeviceInfo.ProcHandles[ID], (uint64_t)TargetPtr,\n                          HostPtr, (size_t)Size);\n  if (ret != 0) {\n    DP(\"veo_write_mem() failed with error code %d\\n\", ret);\n    return OFFLOAD_FAIL;\n  }\n  return OFFLOAD_SUCCESS;\n}\n\n// Retrieve the data content from the target device using its address.\n// In case of success, return zero. Otherwise, return an error code.\nint32_t __tgt_rtl_data_retrieve(int32_t ID, void *HostPtr, void *TargetPtr,\n                                int64_t Size) {\n  int ret = veo_read_mem(DeviceInfo.ProcHandles[ID], HostPtr,\n                         (uint64_t)TargetPtr, Size);\n  if (ret != 0) {\n    DP(\"veo_read_mem() failed with error code %d\\n\", ret);\n    return OFFLOAD_FAIL;\n  }\n  return OFFLOAD_SUCCESS;\n}\n\n// De-allocate the data referenced by target ptr on the device. In case of\n// success, return zero. Otherwise, return an error code.\nint32_t __tgt_rtl_data_delete(int32_t ID, void *TargetPtr) {\n  int ret =  veo_free_mem(DeviceInfo.ProcHandles[ID], (uint64_t)TargetPtr);\n\n  if (ret != 0) {\n    DP(\"veo_free_mem() failed with error code %d\\n\", ret);\n    return OFFLOAD_FAIL;\n  }\n  return OFFLOAD_SUCCESS;\n}\n\n// Similar to __tgt_rtl_run_target_region, but additionally specify the\n// number of teams to be created and a number of threads in each team.\nint32_t __tgt_rtl_run_target_team_region(int32_t ID, void *Entry, void **Args,\n                                         ptrdiff_t *Offsets, int32_t NumArgs,\n                                         int32_t NumTeams, int32_t ThreadLimit,\n                                         uint64_t loop_tripcount) {\n  int ret;\n\n  // ignore team num and thread limit.\n  std::vector<void *> ptrs(NumArgs);\n\n  struct veo_args *TargetArgs;\n  TargetArgs = veo_args_alloc();\n\n  if (TargetArgs == NULL) {\n    DP(\"Could not allocate VEO args\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  for (int i = 0; i < NumArgs; ++i) {\n    ret = veo_args_set_u64(TargetArgs, i, (intptr_t)Args[i]);\n\n    if (ret != 0) {\n      DP(\"veo_args_set_u64() has returned %d for argnum=%d and value %p\\n\",\n         ret, i, Args[i]);\n      return OFFLOAD_FAIL;\n    }\n  }\n\n  uint64_t RetVal;\n  if (target_run_function_wait(ID, reinterpret_cast<uint64_t>(Entry),\n                               TargetArgs, &RetVal) != OFFLOAD_SUCCESS) {\n    veo_args_free(TargetArgs);\n    return OFFLOAD_FAIL;\n  }\n  veo_args_free(TargetArgs);\n  return OFFLOAD_SUCCESS;\n}\n\n// Transfer control to the offloaded entry Entry on the target device.\n// Args and Offsets are arrays of NumArgs size of target addresses and\n// offsets. An offset should be added to the target address before passing it\n// to the outlined function on device side. In case of success, return zero.\n// Otherwise, return an error code.\nint32_t __tgt_rtl_run_target_region(int32_t ID, void *Entry, void **Args,\n                                    ptrdiff_t *Offsets, int32_t NumArgs) {\n  return __tgt_rtl_run_target_team_region(ID, Entry, Args, Offsets, NumArgs, 1,\n                                          1, 0);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/libomptarget/plugins/generic-elf-64bit/src/rtl.cpp": "//===-RTLs/generic-64bit/src/rtl.cpp - Target RTLs Implementation - C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// RTL for generic 64-bit machine\n//\n//===----------------------------------------------------------------------===//\n\n#include <cassert>\n#include <cstdio>\n#include <cstring>\n#include <cstdlib>\n#include <dlfcn.h>\n#include <ffi.h>\n#include <gelf.h>\n#include <link.h>\n#include <list>\n#include <string>\n#include <vector>\n\n#include \"omptargetplugin.h\"\n\n#ifndef TARGET_NAME\n#define TARGET_NAME Generic ELF - 64bit\n#endif\n\n#ifndef TARGET_ELF_ID\n#define TARGET_ELF_ID 0\n#endif\n\n#ifdef OMPTARGET_DEBUG\nstatic int DebugLevel = 0;\n\n#define GETNAME2(name) #name\n#define GETNAME(name) GETNAME2(name)\n#define DP(...) \\\n  do { \\\n    if (DebugLevel > 0) { \\\n      DEBUGP(\"Target \" GETNAME(TARGET_NAME) \" RTL\", __VA_ARGS__); \\\n    } \\\n  } while (false)\n#else // OMPTARGET_DEBUG\n#define DP(...) {}\n#endif // OMPTARGET_DEBUG\n\n#include \"../../common/elf_common.c\"\n\n#define NUMBER_OF_DEVICES 4\n#define OFFLOADSECTIONNAME \"omp_offloading_entries\"\n\n/// Array of Dynamic libraries loaded for this target.\nstruct DynLibTy {\n  char *FileName;\n  void *Handle;\n};\n\n/// Keep entries table per device.\nstruct FuncOrGblEntryTy {\n  __tgt_target_table Table;\n};\n\n/// Class containing all the device information.\nclass RTLDeviceInfoTy {\n  std::vector<std::list<FuncOrGblEntryTy>> FuncGblEntries;\n\npublic:\n  std::list<DynLibTy> DynLibs;\n\n  // Record entry point associated with device.\n  void createOffloadTable(int32_t device_id, __tgt_offload_entry *begin,\n                          __tgt_offload_entry *end) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncGblEntries[device_id].emplace_back();\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id].back();\n\n    E.Table.EntriesBegin = begin;\n    E.Table.EntriesEnd = end;\n  }\n\n  // Return true if the entry is associated with device.\n  bool findOffloadEntry(int32_t device_id, void *addr) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id].back();\n\n    for (__tgt_offload_entry *i = E.Table.EntriesBegin, *e = E.Table.EntriesEnd;\n         i < e; ++i) {\n      if (i->addr == addr)\n        return true;\n    }\n\n    return false;\n  }\n\n  // Return the pointer to the target entries table.\n  __tgt_target_table *getOffloadEntriesTable(int32_t device_id) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id].back();\n\n    return &E.Table;\n  }\n\n  RTLDeviceInfoTy(int32_t num_devices) {\n#ifdef OMPTARGET_DEBUG\n    if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n      DebugLevel = std::stoi(envStr);\n    }\n#endif // OMPTARGET_DEBUG\n\n    FuncGblEntries.resize(num_devices);\n  }\n\n  ~RTLDeviceInfoTy() {\n    // Close dynamic libraries\n    for (auto &lib : DynLibs) {\n      if (lib.Handle) {\n        dlclose(lib.Handle);\n        remove(lib.FileName);\n      }\n    }\n  }\n};\n\nstatic RTLDeviceInfoTy DeviceInfo(NUMBER_OF_DEVICES);\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nint32_t __tgt_rtl_is_valid_binary(__tgt_device_image *image) {\n// If we don't have a valid ELF ID we can just fail.\n#if TARGET_ELF_ID < 1\n  return 0;\n#else\n  return elf_check_machine(image, TARGET_ELF_ID);\n#endif\n}\n\nint32_t __tgt_rtl_number_of_devices() { return NUMBER_OF_DEVICES; }\n\nint32_t __tgt_rtl_init_device(int32_t device_id) { return OFFLOAD_SUCCESS; }\n\n__tgt_target_table *__tgt_rtl_load_binary(int32_t device_id,\n                                          __tgt_device_image *image) {\n\n  DP(\"Dev %d: load binary from \" DPxMOD \" image\\n\", device_id,\n     DPxPTR(image->ImageStart));\n\n  assert(device_id >= 0 && device_id < NUMBER_OF_DEVICES && \"bad dev id\");\n\n  size_t ImageSize = (size_t)image->ImageEnd - (size_t)image->ImageStart;\n  size_t NumEntries = (size_t)(image->EntriesEnd - image->EntriesBegin);\n  DP(\"Expecting to have %zd entries defined.\\n\", NumEntries);\n\n  // Is the library version incompatible with the header file?\n  if (elf_version(EV_CURRENT) == EV_NONE) {\n    DP(\"Incompatible ELF library!\\n\");\n    return NULL;\n  }\n\n  // Obtain elf handler\n  Elf *e = elf_memory((char *)image->ImageStart, ImageSize);\n  if (!e) {\n    DP(\"Unable to get ELF handle: %s!\\n\", elf_errmsg(-1));\n    return NULL;\n  }\n\n  if (elf_kind(e) != ELF_K_ELF) {\n    DP(\"Invalid Elf kind!\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  // Find the entries section offset\n  Elf_Scn *section = 0;\n  Elf64_Off entries_offset = 0;\n\n  size_t shstrndx;\n\n  if (elf_getshdrstrndx(e, &shstrndx)) {\n    DP(\"Unable to get ELF strings index!\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  while ((section = elf_nextscn(e, section))) {\n    GElf_Shdr hdr;\n    gelf_getshdr(section, &hdr);\n\n    if (!strcmp(elf_strptr(e, shstrndx, hdr.sh_name), OFFLOADSECTIONNAME)) {\n      entries_offset = hdr.sh_addr;\n      break;\n    }\n  }\n\n  if (!entries_offset) {\n    DP(\"Entries Section Offset Not Found\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  DP(\"Offset of entries section is (\" DPxMOD \").\\n\", DPxPTR(entries_offset));\n\n  // load dynamic library and get the entry points. We use the dl library\n  // to do the loading of the library, but we could do it directly to avoid the\n  // dump to the temporary file.\n  //\n  // 1) Create tmp file with the library contents.\n  // 2) Use dlopen to load the file and dlsym to retrieve the symbols.\n  char tmp_name[] = \"/tmp/tmpfile_XXXXXX\";\n  int tmp_fd = mkstemp(tmp_name);\n\n  if (tmp_fd == -1) {\n    elf_end(e);\n    return NULL;\n  }\n\n  FILE *ftmp = fdopen(tmp_fd, \"wb\");\n\n  if (!ftmp) {\n    elf_end(e);\n    return NULL;\n  }\n\n  fwrite(image->ImageStart, ImageSize, 1, ftmp);\n  fclose(ftmp);\n\n  DynLibTy Lib = {tmp_name, dlopen(tmp_name, RTLD_LAZY)};\n\n  if (!Lib.Handle) {\n    DP(\"Target library loading error: %s\\n\", dlerror());\n    elf_end(e);\n    return NULL;\n  }\n\n  DeviceInfo.DynLibs.push_back(Lib);\n\n  struct link_map *libInfo = (struct link_map *)Lib.Handle;\n\n  // The place where the entries info is loaded is the library base address\n  // plus the offset determined from the ELF file.\n  Elf64_Addr entries_addr = libInfo->l_addr + entries_offset;\n\n  DP(\"Pointer to first entry to be loaded is (\" DPxMOD \").\\n\",\n      DPxPTR(entries_addr));\n\n  // Table of pointers to all the entries in the target.\n  __tgt_offload_entry *entries_table = (__tgt_offload_entry *)entries_addr;\n\n  __tgt_offload_entry *entries_begin = &entries_table[0];\n  __tgt_offload_entry *entries_end = entries_begin + NumEntries;\n\n  if (!entries_begin) {\n    DP(\"Can't obtain entries begin\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  DP(\"Entries table range is (\" DPxMOD \")->(\" DPxMOD \")\\n\",\n      DPxPTR(entries_begin), DPxPTR(entries_end));\n  DeviceInfo.createOffloadTable(device_id, entries_begin, entries_end);\n\n  elf_end(e);\n\n  return DeviceInfo.getOffloadEntriesTable(device_id);\n}\n\nvoid *__tgt_rtl_data_alloc(int32_t device_id, int64_t size, void *hst_ptr) {\n  void *ptr = malloc(size);\n  return ptr;\n}\n\nint32_t __tgt_rtl_data_submit(int32_t device_id, void *tgt_ptr, void *hst_ptr,\n                              int64_t size) {\n  memcpy(tgt_ptr, hst_ptr, size);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_data_retrieve(int32_t device_id, void *hst_ptr, void *tgt_ptr,\n                                int64_t size) {\n  memcpy(hst_ptr, tgt_ptr, size);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_data_delete(int32_t device_id, void *tgt_ptr) {\n  free(tgt_ptr);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_run_target_team_region(int32_t device_id, void *tgt_entry_ptr,\n                                         void **tgt_args,\n                                         ptrdiff_t *tgt_offsets,\n                                         int32_t arg_num, int32_t team_num,\n                                         int32_t thread_limit,\n                                         uint64_t loop_tripcount /*not used*/) {\n  // ignore team num and thread limit.\n\n  // Use libffi to launch execution.\n  ffi_cif cif;\n\n  // All args are references.\n  std::vector<ffi_type *> args_types(arg_num, &ffi_type_pointer);\n  std::vector<void *> args(arg_num);\n  std::vector<void *> ptrs(arg_num);\n\n  for (int32_t i = 0; i < arg_num; ++i) {\n    ptrs[i] = (void *)((intptr_t)tgt_args[i] + tgt_offsets[i]);\n    args[i] = &ptrs[i];\n  }\n\n  ffi_status status = ffi_prep_cif(&cif, FFI_DEFAULT_ABI, arg_num,\n                                   &ffi_type_void, &args_types[0]);\n\n  assert(status == FFI_OK && \"Unable to prepare target launch!\");\n\n  if (status != FFI_OK)\n    return OFFLOAD_FAIL;\n\n  DP(\"Running entry point at \" DPxMOD \"...\\n\", DPxPTR(tgt_entry_ptr));\n\n  void (*entry)(void);\n  *((void**) &entry) = tgt_entry_ptr;\n  ffi_call(&cif, entry, NULL, &args[0]);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_run_target_region(int32_t device_id, void *tgt_entry_ptr,\n                                    void **tgt_args, ptrdiff_t *tgt_offsets,\n                                    int32_t arg_num) {\n  // use one team and one thread.\n  return __tgt_rtl_run_target_team_region(device_id, tgt_entry_ptr, tgt_args,\n                                          tgt_offsets, arg_num, 1, 1, 0);\n}\n\n#ifdef __cplusplus\n}\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/libomptarget/test/offloading/dynamic_module_load.c": "// RUN: %libomptarget-compile-aarch64-unknown-linux-gnu -DSHARED -fPIC -shared -o %t.so && %clang %flags %s -o %t-aarch64-unknown-linux-gnu -ldl && %libomptarget-run-aarch64-unknown-linux-gnu %t.so 2>&1 | %fcheck-aarch64-unknown-linux-gnu\n// RUN: %libomptarget-compile-powerpc64-ibm-linux-gnu -DSHARED -fPIC -shared -o %t.so && %clang %flags %s -o %t-powerpc64-ibm-linux-gnu -ldl && %libomptarget-run-powerpc64-ibm-linux-gnu %t.so 2>&1 | %fcheck-powerpc64-ibm-linux-gnu\n// RUN: %libomptarget-compile-powerpc64le-ibm-linux-gnu -DSHARED -fPIC -shared -o %t.so && %clang %flags %s -o %t-powerpc64le-ibm-linux-gnu -ldl && %libomptarget-run-powerpc64le-ibm-linux-gnu %t.so 2>&1 | %fcheck-powerpc64le-ibm-linux-gnu\n// RUN: %libomptarget-compile-x86_64-pc-linux-gnu -DSHARED -fPIC -shared -o %t.so && %clang %flags %s -o %t-x86_64-pc-linux-gnu -ldl && %libomptarget-run-x86_64-pc-linux-gnu %t.so 2>&1 | %fcheck-x86_64-pc-linux-gnu\n\n// [BOLT] It takes too long time.  Let's remove this from tests.\n// UNSUPPORTED: clang-11\n\n#ifdef SHARED\n#include <stdio.h>\nint foo() {\n#pragma omp target\n  ;\n  printf(\"%s\\n\", \"DONE.\");\n  return 0;\n}\n#else\n#include <dlfcn.h>\n#include <stdio.h>\nint main(int argc, char **argv) {\n  void *Handle = dlopen(argv[1], RTLD_NOW);\n  int (*Foo)(void);\n\n  if (Handle == NULL) {\n    printf(\"dlopen() failed: %s\\n\", dlerror());\n    return 1;\n  }\n  Foo = (int (*)(void)) dlsym(Handle, \"foo\");\n  if (Handle == NULL) {\n    printf(\"dlsym() failed: %s\\n\", dlerror());\n    return 1;\n  }\n  // CHECK: DONE.\n  // CHECK-NOT: {{abort|fault}}\n  return Foo();\n}\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/tools/multiplex/ompt-multiplex.h": "//===--- ompt-multiplex.h - header-only multiplexing of OMPT tools -- C -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// This header file enables an OMPT tool to load another OMPT tool and\n// automatically forwards OMPT event-callbacks to the nested tool.\n//\n// For details see openmp/tools/multiplex/README.md\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef OMPT_MULTIPLEX_H\n#define OMPT_MULTIPLEX_H\n\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n#include <dlfcn.h>\n#include <execinfo.h>\n#include <inttypes.h>\n#include <omp-tools.h>\n#include <omp.h>\n#include <stdio.h>\n#include <string.h>\n#include <errno.h>\n\nstatic ompt_set_callback_t ompt_multiplex_set_callback;\nstatic ompt_get_task_info_t ompt_multiplex_get_task_info;\nstatic ompt_get_thread_data_t ompt_multiplex_get_thread_data;\nstatic ompt_get_parallel_info_t ompt_multiplex_get_parallel_info;\n\n// contains name of the environment var in which the tool path is specified\n#ifndef CLIENT_TOOL_LIBRARIES_VAR\n#error CLIENT_TOOL_LIBRARIES_VAR should be defined before including of ompt-multiplex.h\n#endif\n\n#if defined(CUSTOM_DELETE_DATA) && !defined(CUSTOM_GET_CLIENT_DATA)\n#error CUSTOM_GET_CLIENT_DATA must be set if CUSTOM_DELETE_DATA is set\n#endif\n\n#define OMPT_API_ROUTINE static\n\n#define OMPT_LOAD_CLIENT_FOREACH_OMPT_EVENT(macro)                             \\\n  macro(callback_thread_begin, ompt_callback_thread_begin_t, 1);               \\\n  macro(callback_thread_end, ompt_callback_thread_end_t, 2);                   \\\n  macro(callback_parallel_begin, ompt_callback_parallel_begin_t, 3);           \\\n  macro(callback_parallel_end, ompt_callback_parallel_end_t, 4);               \\\n  macro(callback_task_create, ompt_callback_task_create_t, 5);                 \\\n  macro(callback_task_schedule, ompt_callback_task_schedule_t, 6);             \\\n  macro(callback_implicit_task, ompt_callback_implicit_task_t, 7);             \\\n  macro(callback_target, ompt_callback_target_t, 8);                           \\\n  macro(callback_target_data_op, ompt_callback_target_data_op_t, 9);           \\\n  macro(callback_target_submit, ompt_callback_target_submit_t, 10);            \\\n  macro(callback_control_tool, ompt_callback_control_tool_t, 11);              \\\n  macro(callback_device_initialize, ompt_callback_device_initialize_t, 12);    \\\n  macro(callback_device_finalize, ompt_callback_device_finalize_t, 13);        \\\n  macro(callback_device_load, ompt_callback_device_load_t, 14);                \\\n  macro(callback_device_unload, ompt_callback_device_unload_t, 15);            \\\n  macro(callback_sync_region_wait, ompt_callback_sync_region_t, 16);           \\\n  macro(callback_mutex_released, ompt_callback_mutex_t, 17);                   \\\n  macro(callback_dependences, ompt_callback_dependences_t, 18);                \\\n  macro(callback_task_dependence, ompt_callback_task_dependence_t, 19);        \\\n  macro(callback_work, ompt_callback_work_t, 20);                              \\\n  macro(callback_master, ompt_callback_master_t, 21);                          \\\n  macro(callback_target_map, ompt_callback_target_map_t, 22);                  \\\n  macro(callback_sync_region, ompt_callback_sync_region_t, 23);                \\\n  macro(callback_lock_init, ompt_callback_mutex_acquire_t, 24);                \\\n  macro(callback_lock_destroy, ompt_callback_mutex_t, 25);                     \\\n  macro(callback_mutex_acquire, ompt_callback_mutex_acquire_t, 26);            \\\n  macro(callback_mutex_acquired, ompt_callback_mutex_t, 27);                   \\\n  macro(callback_nest_lock, ompt_callback_nest_lock_t, 28);                    \\\n  macro(callback_flush, ompt_callback_flush_t, 29);                            \\\n  macro(callback_cancel, ompt_callback_cancel_t, 30);                          \\\n  macro(callback_reduction, ompt_callback_sync_region_t, 31);                  \\\n  macro(callback_dispatch, ompt_callback_dispatch_t, 32);\n\ntypedef struct ompt_multiplex_callbacks_s {\n#define ompt_event_macro(event, callback, eventid) callback ompt_##event\n\n  OMPT_LOAD_CLIENT_FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n} ompt_multiplex_callbacks_t;\n\ntypedef struct ompt_multiplex_callback_implementation_status_s {\n#define ompt_event_macro(event, callback, eventid) int ompt_##event\n\n  OMPT_LOAD_CLIENT_FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n} ompt_multiplex_callback_implementation_status_t;\n\nompt_start_tool_result_t *ompt_multiplex_own_fns;\nompt_start_tool_result_t *ompt_multiplex_client_fns;\nompt_function_lookup_t ompt_multiplex_lookup_function;\nompt_multiplex_callbacks_t ompt_multiplex_own_callbacks,\n    ompt_multiplex_client_callbacks;\nompt_multiplex_callback_implementation_status_t\n    ompt_multiplex_implementation_status;\n\ntypedef struct ompt_multiplex_data_pair_s {\n  ompt_data_t own_data;\n  ompt_data_t client_data;\n} ompt_multiplex_data_pair_t;\n\n#if !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA) ||                  \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA) ||                \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA)\nstatic ompt_multiplex_data_pair_t *\nompt_multiplex_allocate_data_pair(ompt_data_t *data_pointer) {\n  data_pointer->ptr = malloc(sizeof(ompt_multiplex_data_pair_t));\n  if (!data_pointer->ptr) {\n    printf(\"Malloc ERROR\\n\");\n    exit(-1);\n  }\n  ompt_multiplex_data_pair_t *data_pair =\n      (ompt_multiplex_data_pair_t *)data_pointer->ptr;\n  data_pair->own_data.ptr = NULL;\n  data_pair->client_data.ptr = NULL;\n  return data_pair;\n}\n\nstatic void ompt_multiplex_free_data_pair(ompt_data_t *data_pointer) {\n  free((*data_pointer).ptr);\n}\n\nstatic ompt_data_t *ompt_multiplex_get_own_ompt_data(ompt_data_t *data) {\n  if (!data)\n    return NULL;\n  ompt_multiplex_data_pair_t *data_pair =\n      (ompt_multiplex_data_pair_t *)data->ptr;\n  return &(data_pair->own_data);\n}\n\nstatic ompt_data_t *ompt_multiplex_get_client_ompt_data(ompt_data_t *data) {\n  if (!data)\n    return NULL;\n  ompt_multiplex_data_pair_t *data_pair =\n      (ompt_multiplex_data_pair_t *)data->ptr;\n  return &(data_pair->client_data);\n}\n#endif //! defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA) ||\n       //! !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA) ||\n       //! !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA)\n\nstatic ompt_data_t *ompt_multiplex_get_own_thread_data(ompt_data_t *data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA\n  return ompt_multiplex_get_own_ompt_data(data);\n#else\n  return data;\n#endif\n}\n\nstatic ompt_data_t *ompt_multiplex_get_own_parallel_data(ompt_data_t *data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n  return ompt_multiplex_get_own_ompt_data(data);\n#else\n  return data;\n#endif\n}\n\nstatic ompt_data_t *ompt_multiplex_get_own_task_data(ompt_data_t *data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n  return ompt_multiplex_get_own_ompt_data(data);\n#else\n  return data;\n#endif\n}\n\nstatic ompt_data_t *ompt_multiplex_get_client_thread_data(ompt_data_t *data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA\n  return ompt_multiplex_get_client_ompt_data(data);\n#else\n  return OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA(data);\n#endif\n}\n\nstatic ompt_data_t *ompt_multiplex_get_client_parallel_data(ompt_data_t *data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n  return ompt_multiplex_get_client_ompt_data(data);\n#else\n  return OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA(data);\n#endif\n}\n\nstatic ompt_data_t *ompt_multiplex_get_client_task_data(ompt_data_t *data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n  return ompt_multiplex_get_client_ompt_data(data);\n#else\n  return OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA(data);\n#endif\n}\n\nstatic void ompt_multiplex_callback_mutex_acquire(ompt_mutex_t kind,\n                                                  unsigned int hint,\n                                                  unsigned int impl,\n                                                  ompt_wait_id_t wait_id,\n                                                  const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_mutex_acquire) {\n    ompt_multiplex_own_callbacks.ompt_callback_mutex_acquire(\n        kind, hint, impl, wait_id, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_mutex_acquire) {\n    ompt_multiplex_client_callbacks.ompt_callback_mutex_acquire(\n        kind, hint, impl, wait_id, codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_mutex_acquired(ompt_mutex_t kind,\n                                                   ompt_wait_id_t wait_id,\n                                                   const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_mutex_acquired) {\n    ompt_multiplex_own_callbacks.ompt_callback_mutex_acquired(kind, wait_id,\n                                                              codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_mutex_acquired) {\n    ompt_multiplex_client_callbacks.ompt_callback_mutex_acquired(kind, wait_id,\n                                                                 codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_mutex_released(ompt_mutex_t kind,\n                                                   ompt_wait_id_t wait_id,\n                                                   const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_mutex_released) {\n    ompt_multiplex_own_callbacks.ompt_callback_mutex_released(kind, wait_id,\n                                                              codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_mutex_released) {\n    ompt_multiplex_client_callbacks.ompt_callback_mutex_released(kind, wait_id,\n                                                                 codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_nest_lock(ompt_scope_endpoint_t endpoint,\n                                              ompt_wait_id_t wait_id,\n                                              const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_nest_lock) {\n    ompt_multiplex_own_callbacks.ompt_callback_nest_lock(endpoint, wait_id,\n                                                         codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_nest_lock) {\n    ompt_multiplex_client_callbacks.ompt_callback_nest_lock(endpoint, wait_id,\n                                                            codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_sync_region(ompt_sync_region_t kind,\n                                                ompt_scope_endpoint_t endpoint,\n                                                ompt_data_t *parallel_data,\n                                                ompt_data_t *task_data,\n                                                const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_sync_region) {\n    ompt_multiplex_own_callbacks.ompt_callback_sync_region(\n        kind, endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_sync_region) {\n    ompt_multiplex_client_callbacks.ompt_callback_sync_region(\n        kind, endpoint, ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_sync_region_wait(\n    ompt_sync_region_t kind, ompt_scope_endpoint_t endpoint,\n    ompt_data_t *parallel_data, ompt_data_t *task_data,\n    const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_sync_region_wait) {\n    ompt_multiplex_own_callbacks.ompt_callback_sync_region_wait(\n        kind, endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_sync_region_wait) {\n    ompt_multiplex_client_callbacks.ompt_callback_sync_region_wait(\n        kind, endpoint, ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_flush(ompt_data_t *thread_data,\n                                          const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_flush) {\n    ompt_multiplex_own_callbacks.ompt_callback_flush(\n        ompt_multiplex_get_own_thread_data(thread_data), codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_flush) {\n    ompt_multiplex_client_callbacks.ompt_callback_flush(\n        ompt_multiplex_get_client_thread_data(thread_data), codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_cancel(ompt_data_t *task_data, int flags,\n                                           const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_cancel) {\n    ompt_multiplex_own_callbacks.ompt_callback_cancel(\n        ompt_multiplex_get_own_task_data(task_data), flags, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_cancel) {\n    ompt_multiplex_client_callbacks.ompt_callback_cancel(\n        ompt_multiplex_get_client_task_data(task_data), flags, codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_implicit_task(\n    ompt_scope_endpoint_t endpoint, ompt_data_t *parallel_data,\n    ompt_data_t *task_data, unsigned int team_size, unsigned int thread_num,\n    int flags) {\n  if (endpoint == ompt_scope_begin) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n    ompt_multiplex_allocate_data_pair(task_data);\n#endif\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n    if (flags & ompt_task_initial)\n      ompt_multiplex_allocate_data_pair(parallel_data);\n#endif\n    if (ompt_multiplex_own_callbacks.ompt_callback_implicit_task) {\n      ompt_multiplex_own_callbacks.ompt_callback_implicit_task(\n          endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n          ompt_multiplex_get_own_task_data(task_data), team_size, thread_num,\n          flags);\n    }\n    if (ompt_multiplex_client_callbacks.ompt_callback_implicit_task) {\n      ompt_multiplex_client_callbacks.ompt_callback_implicit_task(\n          endpoint, ompt_multiplex_get_client_parallel_data(parallel_data),\n          ompt_multiplex_get_client_task_data(task_data), team_size, thread_num,\n          flags);\n    }\n  } else {\n// defines to make sure, callbacks are called in correct order depending on\n// defines set by the user\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA) ||                         \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA)\n    if (ompt_multiplex_own_callbacks.ompt_callback_implicit_task) {\n      ompt_multiplex_own_callbacks.ompt_callback_implicit_task(\n          endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n          ompt_multiplex_get_own_task_data(task_data), team_size, thread_num,\n          flags);\n    }\n#endif\n\n    if (ompt_multiplex_client_callbacks.ompt_callback_implicit_task) {\n      ompt_multiplex_client_callbacks.ompt_callback_implicit_task(\n          endpoint, ompt_multiplex_get_client_parallel_data(parallel_data),\n          ompt_multiplex_get_client_task_data(task_data), team_size, thread_num,\n          flags);\n    }\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA) &&                     \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA)\n    if (ompt_multiplex_own_callbacks.ompt_callback_implicit_task) {\n      ompt_multiplex_own_callbacks.ompt_callback_implicit_task(\n          endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n          ompt_multiplex_get_own_task_data(task_data), team_size, thread_num,\n          flags);\n    }\n#endif\n\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n    ompt_multiplex_free_data_pair(task_data);\n#endif\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_PARALLEL_DATA)\n    if (flags & ompt_task_initial)\n      OMPT_MULTIPLEX_CUSTOM_DELETE_PARALLEL_DATA(parallel_data);\n#endif\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA)\n    OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA(task_data);\n#endif\n  }\n}\n\nstatic void ompt_multiplex_callback_lock_init(ompt_mutex_t kind,\n                                              unsigned int hint,\n                                              unsigned int impl,\n                                              ompt_wait_id_t wait_id,\n                                              const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_lock_init) {\n    ompt_multiplex_own_callbacks.ompt_callback_lock_init(kind, hint, impl,\n                                                         wait_id, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_lock_init) {\n    ompt_multiplex_client_callbacks.ompt_callback_lock_init(\n        kind, hint, impl, wait_id, codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_lock_destroy(ompt_mutex_t kind,\n                                                 ompt_wait_id_t wait_id,\n                                                 const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_lock_destroy) {\n    ompt_multiplex_own_callbacks.ompt_callback_lock_destroy(kind, wait_id,\n                                                            codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_lock_destroy) {\n    ompt_multiplex_client_callbacks.ompt_callback_lock_destroy(kind, wait_id,\n                                                               codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_work(ompt_work_t wstype,\n                                         ompt_scope_endpoint_t endpoint,\n                                         ompt_data_t *parallel_data,\n                                         ompt_data_t *task_data, uint64_t count,\n                                         const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_work) {\n    ompt_multiplex_own_callbacks.ompt_callback_work(\n        wstype, endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), count, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_work) {\n    ompt_multiplex_client_callbacks.ompt_callback_work(\n        wstype, endpoint,\n        ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), count, codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_master(ompt_scope_endpoint_t endpoint,\n                                           ompt_data_t *parallel_data,\n                                           ompt_data_t *task_data,\n                                           const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_master) {\n    ompt_multiplex_own_callbacks.ompt_callback_master(\n        endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_master) {\n    ompt_multiplex_client_callbacks.ompt_callback_master(\n        endpoint, ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_parallel_begin(\n    ompt_data_t *parent_task_data, const ompt_frame_t *parent_task_frame,\n    ompt_data_t *parallel_data, uint32_t requested_team_size, int flag,\n    const void *codeptr_ra) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n  ompt_multiplex_allocate_data_pair(parallel_data);\n#endif\n  if (ompt_multiplex_own_callbacks.ompt_callback_parallel_begin) {\n    ompt_multiplex_own_callbacks.ompt_callback_parallel_begin(\n        ompt_multiplex_get_own_task_data(parent_task_data), parent_task_frame,\n        ompt_multiplex_get_own_parallel_data(parallel_data),\n        requested_team_size, flag, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_parallel_begin) {\n    ompt_multiplex_client_callbacks.ompt_callback_parallel_begin(\n        ompt_multiplex_get_client_task_data(parent_task_data),\n        parent_task_frame,\n        ompt_multiplex_get_client_parallel_data(parallel_data),\n        requested_team_size, flag, codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_parallel_end(ompt_data_t *parallel_data,\n                                                 ompt_data_t *task_data,\n                                                 int flag,\n                                                 const void *codeptr_ra) {\n// defines to make sure, callbacks are called in correct order depending on\n// defines set by the user\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_PARALLEL_DATA) ||                     \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA)\n  if (ompt_multiplex_own_callbacks.ompt_callback_parallel_end) {\n    ompt_multiplex_own_callbacks.ompt_callback_parallel_end(\n        ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), flag, codeptr_ra);\n  }\n#endif\n\n  if (ompt_multiplex_client_callbacks.ompt_callback_parallel_end) {\n    ompt_multiplex_client_callbacks.ompt_callback_parallel_end(\n        ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), flag, codeptr_ra);\n  }\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA) &&                 \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_DELETE_PARALLEL_DATA)\n  if (ompt_multiplex_own_callbacks.ompt_callback_parallel_end) {\n    ompt_multiplex_own_callbacks.ompt_callback_parallel_end(\n        ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), flag, codeptr_ra);\n  }\n#endif\n\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n  ompt_multiplex_free_data_pair(parallel_data);\n#endif\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_PARALLEL_DATA)\n  OMPT_MULTIPLEX_CUSTOM_DELETE_PARALLEL_DATA(parallel_data);\n#endif\n}\n\nstatic void ompt_multiplex_callback_task_create(\n    ompt_data_t *parent_task_data, const ompt_frame_t *parent_frame,\n    ompt_data_t *new_task_data, int type, int has_dependences,\n    const void *codeptr_ra) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n  ompt_multiplex_allocate_data_pair(new_task_data);\n#endif\n\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n  if (type & ompt_task_initial) {\n    ompt_data_t *parallel_data;\n    ompt_multiplex_get_parallel_info(0, &parallel_data, NULL);\n    ompt_multiplex_allocate_data_pair(parallel_data);\n  }\n#endif\n\n  if (ompt_multiplex_own_callbacks.ompt_callback_task_create) {\n    ompt_multiplex_own_callbacks.ompt_callback_task_create(\n        ompt_multiplex_get_own_task_data(parent_task_data), parent_frame,\n        ompt_multiplex_get_own_task_data(new_task_data), type, has_dependences,\n        codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_task_create) {\n    ompt_multiplex_client_callbacks.ompt_callback_task_create(\n        ompt_multiplex_get_client_task_data(parent_task_data), parent_frame,\n        ompt_multiplex_get_client_task_data(new_task_data), type,\n        has_dependences, codeptr_ra);\n  }\n}\n\nstatic void\nompt_multiplex_callback_task_schedule(ompt_data_t *first_task_data,\n                                      ompt_task_status_t prior_task_status,\n                                      ompt_data_t *second_task_data) {\n  if (prior_task_status != ompt_task_complete) {\n    if (ompt_multiplex_own_callbacks.ompt_callback_task_schedule) {\n      ompt_multiplex_own_callbacks.ompt_callback_task_schedule(\n          ompt_multiplex_get_own_task_data(first_task_data), prior_task_status,\n          ompt_multiplex_get_own_task_data(second_task_data));\n    }\n    if (ompt_multiplex_client_callbacks.ompt_callback_task_schedule) {\n      ompt_multiplex_client_callbacks.ompt_callback_task_schedule(\n          ompt_multiplex_get_client_task_data(first_task_data),\n          prior_task_status,\n          ompt_multiplex_get_client_task_data(second_task_data));\n    }\n  } else {\n// defines to make sure, callbacks are called in correct order depending on\n// defines set by the user\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA) ||                         \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA)\n    if (ompt_multiplex_own_callbacks.ompt_callback_task_schedule) {\n      ompt_multiplex_own_callbacks.ompt_callback_task_schedule(\n          ompt_multiplex_get_own_task_data(first_task_data), prior_task_status,\n          ompt_multiplex_get_own_task_data(second_task_data));\n    }\n#endif\n\n    if (ompt_multiplex_client_callbacks.ompt_callback_task_schedule) {\n      ompt_multiplex_client_callbacks.ompt_callback_task_schedule(\n          ompt_multiplex_get_client_task_data(first_task_data),\n          prior_task_status,\n          ompt_multiplex_get_client_task_data(second_task_data));\n    }\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA) &&                     \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA)\n    if (ompt_multiplex_own_callbacks.ompt_callback_task_schedule) {\n      ompt_multiplex_own_callbacks.ompt_callback_task_schedule(\n          ompt_multiplex_get_own_task_data(first_task_data), prior_task_status,\n          ompt_multiplex_get_own_task_data(second_task_data));\n    }\n#endif\n\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n    ompt_multiplex_free_data_pair(first_task_data);\n#endif\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA)\n    OMPT_MULTIPLEX_CUSTOM_DELETE_TASK_DATA(first_task_data);\n#endif\n  }\n}\n\nstatic void ompt_multiplex_callback_dependences(ompt_data_t *task_data,\n                                                const ompt_dependence_t *deps,\n                                                int ndeps) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_dependences) {\n    ompt_multiplex_own_callbacks.ompt_callback_dependences(\n        ompt_multiplex_get_own_task_data(task_data), deps, ndeps);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_dependences) {\n    ompt_multiplex_client_callbacks.ompt_callback_dependences(\n        ompt_multiplex_get_client_task_data(task_data), deps, ndeps);\n  }\n}\n\nstatic void\nompt_multiplex_callback_task_dependence(ompt_data_t *first_task_data,\n                                        ompt_data_t *second_task_data) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_task_dependence) {\n    ompt_multiplex_own_callbacks.ompt_callback_task_dependence(\n        ompt_multiplex_get_own_task_data(first_task_data),\n        ompt_multiplex_get_own_task_data(second_task_data));\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_task_dependence) {\n    ompt_multiplex_client_callbacks.ompt_callback_task_dependence(\n        ompt_multiplex_get_client_task_data(first_task_data),\n        ompt_multiplex_get_client_task_data(second_task_data));\n  }\n}\n\nstatic void ompt_multiplex_callback_thread_begin(ompt_thread_t thread_type,\n                                                 ompt_data_t *thread_data) {\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA\n  ompt_multiplex_allocate_data_pair(thread_data);\n#endif\n  if (ompt_multiplex_own_callbacks.ompt_callback_thread_begin) {\n    ompt_multiplex_own_callbacks.ompt_callback_thread_begin(\n        thread_type, ompt_multiplex_get_own_thread_data(thread_data));\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_thread_begin) {\n    ompt_multiplex_client_callbacks.ompt_callback_thread_begin(\n        thread_type, ompt_multiplex_get_client_thread_data(thread_data));\n  }\n}\n\nstatic void ompt_multiplex_callback_thread_end(ompt_data_t *thread_data) {\n// defines to make sure, callbacks are called in correct order depending on\n// defines set by the user\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_THREAD_DATA) ||                       \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA)\n  if (ompt_multiplex_own_callbacks.ompt_callback_thread_end) {\n    ompt_multiplex_own_callbacks.ompt_callback_thread_end(\n        ompt_multiplex_get_own_thread_data(thread_data));\n  }\n#endif\n\n  if (ompt_multiplex_client_callbacks.ompt_callback_thread_end) {\n    ompt_multiplex_client_callbacks.ompt_callback_thread_end(\n        ompt_multiplex_get_client_thread_data(thread_data));\n  }\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA) &&                   \\\n    !defined(OMPT_MULTIPLEX_CUSTOM_DELETE_THREAD_DATA)\n  if (ompt_multiplex_own_callbacks.ompt_callback_thread_end) {\n    ompt_multiplex_own_callbacks.ompt_callback_thread_end(\n        ompt_multiplex_get_own_thread_data(thread_data));\n  }\n#endif\n\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA\n  ompt_multiplex_free_data_pair(thread_data);\n#endif\n\n#if defined(OMPT_MULTIPLEX_CUSTOM_DELETE_THREAD_DATA)\n  OMPT_MULTIPLEX_CUSTOM_DELETE_THREAD_DATA(thread_data);\n#endif\n}\n\nstatic int ompt_multiplex_callback_control_tool(uint64_t command,\n                                                uint64_t modifier, void *arg,\n                                                const void *codeptr_ra) {\n  int ownRet = 0, clientRet = 0;\n  if (ompt_multiplex_own_callbacks.ompt_callback_control_tool) {\n    ownRet = ompt_multiplex_own_callbacks.ompt_callback_control_tool(\n        command, modifier, arg, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_control_tool) {\n    clientRet = ompt_multiplex_client_callbacks.ompt_callback_control_tool(\n        command, modifier, arg, codeptr_ra);\n  }\n  return ownRet < clientRet ? ownRet : clientRet;\n}\n\nstatic void ompt_multiplex_callback_target(\n    ompt_target_t kind, ompt_scope_endpoint_t endpoint, int device_num,\n    ompt_data_t *task_data, ompt_id_t target_id, const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_target) {\n    ompt_multiplex_own_callbacks.ompt_callback_target(\n        kind, endpoint, device_num, ompt_multiplex_get_own_task_data(task_data),\n        target_id, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_target) {\n    ompt_multiplex_client_callbacks.ompt_callback_target(\n        kind, endpoint, device_num,\n        ompt_multiplex_get_client_task_data(task_data), target_id, codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_target_data_op(\n    ompt_id_t target_id, ompt_id_t host_op_id, ompt_target_data_op_t optype,\n    void *src_addr, int src_device_num, void *dest_addr, int dest_device_num,\n    size_t bytes, const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_target_data_op) {\n    ompt_multiplex_own_callbacks.ompt_callback_target_data_op(\n        target_id, host_op_id, optype, src_addr, src_device_num, dest_addr,\n        dest_device_num, bytes, codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_target_data_op) {\n    ompt_multiplex_client_callbacks.ompt_callback_target_data_op(\n        target_id, host_op_id, optype, src_addr, src_device_num, dest_addr,\n        dest_device_num, bytes, codeptr_ra);\n  }\n}\n\nstatic void\nompt_multiplex_callback_target_submit(ompt_id_t target_id, ompt_id_t host_op_id,\n                                      unsigned int requested_num_teams) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_target_submit) {\n    ompt_multiplex_own_callbacks.ompt_callback_target_submit(\n        target_id, host_op_id, requested_num_teams);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_target_submit) {\n    ompt_multiplex_client_callbacks.ompt_callback_target_submit(\n        target_id, host_op_id, requested_num_teams);\n  }\n}\n\nstatic void ompt_multiplex_callback_device_initialize(\n    int device_num, const char *type, ompt_device_t *device,\n    ompt_function_lookup_t lookup, const char *documentation) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_device_initialize) {\n    ompt_multiplex_own_callbacks.ompt_callback_device_initialize(\n        device_num, type, device, lookup, documentation);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_device_initialize) {\n    ompt_multiplex_client_callbacks.ompt_callback_device_initialize(\n        device_num, type, device, lookup, documentation);\n  }\n}\n\nstatic void ompt_multiplex_callback_device_finalize(int device_num) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_device_finalize) {\n    ompt_multiplex_own_callbacks.ompt_callback_device_finalize(device_num);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_device_finalize) {\n    ompt_multiplex_client_callbacks.ompt_callback_device_finalize(device_num);\n  }\n}\n\nstatic void\nompt_multiplex_callback_device_load(int device_num, const char *filename,\n                                    int64_t offset_in_file, void *vma_in_file,\n                                    size_t bytes, void *host_addr,\n                                    void *device_addr, uint64_t module_id) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_device_load) {\n    ompt_multiplex_own_callbacks.ompt_callback_device_load(\n        device_num, filename, offset_in_file, vma_in_file, bytes, host_addr,\n        device_addr, module_id);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_device_load) {\n    ompt_multiplex_client_callbacks.ompt_callback_device_load(\n        device_num, filename, offset_in_file, vma_in_file, bytes, host_addr,\n        device_addr, module_id);\n  }\n}\n\nstatic void ompt_multiplex_callback_device_unload(int device_num,\n                                                  uint64_t module_id) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_device_unload) {\n    ompt_multiplex_own_callbacks.ompt_callback_device_unload(device_num,\n                                                             module_id);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_device_unload) {\n    ompt_multiplex_client_callbacks.ompt_callback_device_unload(device_num,\n                                                                module_id);\n  }\n}\n\nstatic void\nompt_multiplex_callback_target_map(ompt_id_t target_id, unsigned int nitems,\n                                   void **host_addr, void **device_addr,\n                                   size_t *bytes, unsigned int *mapping_flags,\n                                   const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_target_map) {\n    ompt_multiplex_own_callbacks.ompt_callback_target_map(\n        target_id, nitems, host_addr, device_addr, bytes, mapping_flags,\n        codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_target_map) {\n    ompt_multiplex_client_callbacks.ompt_callback_target_map(\n        target_id, nitems, host_addr, device_addr, bytes, mapping_flags,\n        codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_reduction(ompt_sync_region_t kind,\n                                              ompt_scope_endpoint_t endpoint,\n                                              ompt_data_t *parallel_data,\n                                              ompt_data_t *task_data,\n                                              const void *codeptr_ra) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_reduction) {\n    ompt_multiplex_own_callbacks.ompt_callback_reduction(\n        kind, endpoint, ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), codeptr_ra);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_reduction) {\n    ompt_multiplex_client_callbacks.ompt_callback_reduction(\n        kind, endpoint, ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), codeptr_ra);\n  }\n}\n\nstatic void ompt_multiplex_callback_dispatch(ompt_data_t *parallel_data,\n                                             ompt_data_t *task_data,\n                                             ompt_dispatch_t kind,\n                                             ompt_data_t instance) {\n  if (ompt_multiplex_own_callbacks.ompt_callback_dispatch) {\n    ompt_multiplex_own_callbacks.ompt_callback_dispatch(\n        ompt_multiplex_get_own_parallel_data(parallel_data),\n        ompt_multiplex_get_own_task_data(task_data), kind, instance);\n  }\n  if (ompt_multiplex_client_callbacks.ompt_callback_dispatch) {\n    ompt_multiplex_client_callbacks.ompt_callback_dispatch(\n        ompt_multiplex_get_client_parallel_data(parallel_data),\n        ompt_multiplex_get_client_task_data(task_data), kind, instance);\n  }\n}\n\n// runtime entry functions\n\nint ompt_multiplex_own_get_task_info(int ancestor_level, int *type,\n                                     ompt_data_t **task_data,\n                                     ompt_frame_t **task_frame,\n                                     ompt_data_t **parallel_data,\n                                     int *thread_num) {\n  int ret = ompt_multiplex_get_task_info(ancestor_level, type, task_data,\n                                         task_frame, parallel_data, thread_num);\n\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n  if (task_data)\n    *task_data = ompt_multiplex_get_own_ompt_data(*task_data);\n#endif\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n  if (parallel_data)\n    *parallel_data = ompt_multiplex_get_own_ompt_data(*parallel_data);\n#endif\n  return ret;\n}\n\nint ompt_multiplex_client_get_task_info(int ancestor_level, int *type,\n                                        ompt_data_t **task_data,\n                                        ompt_frame_t **task_frame,\n                                        ompt_data_t **parallel_data,\n                                        int *thread_num) {\n  int ret = ompt_multiplex_get_task_info(ancestor_level, type, task_data,\n                                         task_frame, parallel_data, thread_num);\n\n  if (task_data)\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA\n    *task_data = ompt_multiplex_get_client_ompt_data(*task_data);\n#else\n    *task_data = OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_TASK_DATA(*task_data);\n#endif\n\n  if (parallel_data)\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n    *parallel_data = ompt_multiplex_get_client_ompt_data(*parallel_data);\n#else\n    *parallel_data =\n        OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA(*parallel_data);\n#endif\n  return ret;\n}\n\nompt_data_t *ompt_multiplex_own_get_thread_data() {\n  ompt_data_t *ret;\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA\n  ret = ompt_multiplex_get_own_ompt_data(ompt_multiplex_get_thread_data());\n#else\n  ret = ompt_multiplex_get_thread_data();\n#endif\n  return ret;\n}\n\nompt_data_t *ompt_multiplex_client_get_thread_data() {\n  ompt_data_t *ret;\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA\n  ret = ompt_multiplex_get_client_ompt_data(ompt_multiplex_get_thread_data());\n#else\n  ret = OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_THREAD_DATA(\n      ompt_multiplex_get_thread_data());\n#endif\n  return ret;\n}\n\nint ompt_multiplex_own_get_parallel_info(int ancestor_level,\n                                         ompt_data_t **parallel_data,\n                                         int *team_size) {\n  int ret = ompt_multiplex_get_parallel_info(ancestor_level, parallel_data,\n                                             team_size);\n  if (parallel_data)\n    *parallel_data = ompt_multiplex_get_own_parallel_data(*parallel_data);\n  return ret;\n}\n\nint ompt_multiplex_client_get_parallel_info(int ancestor_level,\n                                            ompt_data_t **parallel_data,\n                                            int *team_size) {\n  int ret = ompt_multiplex_get_parallel_info(ancestor_level, parallel_data,\n                                             team_size);\n  if (parallel_data)\n#ifndef OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA\n    *parallel_data = ompt_multiplex_get_client_ompt_data(*parallel_data);\n#else\n    *parallel_data =\n        OMPT_MULTIPLEX_CUSTOM_GET_CLIENT_PARALLEL_DATA(*parallel_data);\n#endif\n  return ret;\n}\n\nOMPT_API_ROUTINE int ompt_multiplex_own_set_callback(ompt_callbacks_t which,\n                                                     ompt_callback_t callback) {\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case ompt_##event_name:                                                      \\\n    ompt_multiplex_own_callbacks.ompt_##event_name = (callback_type)callback;  \\\n    if (ompt_multiplex_implementation_status.ompt_##event_name == -1)          \\\n      return ompt_multiplex_implementation_status.ompt_##event_name =          \\\n                 ompt_multiplex_set_callback(                                  \\\n                     ompt_##event_name,                                        \\\n                     (ompt_callback_t)&ompt_multiplex_##event_name);           \\\n    else                                                                       \\\n      return ompt_multiplex_implementation_status.ompt_##event_name\n\n    OMPT_LOAD_CLIENT_FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_set_error;\n  }\n}\n\nOMPT_API_ROUTINE int\nompt_multiplex_client_set_callback(ompt_callbacks_t which,\n                                   ompt_callback_t callback) {\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case ompt_##event_name:                                                      \\\n    ompt_multiplex_client_callbacks.ompt_##event_name =                        \\\n        (callback_type)callback;                                               \\\n    if (ompt_multiplex_implementation_status.ompt_##event_name == -1)          \\\n      return ompt_multiplex_implementation_status.ompt_##event_name =          \\\n                 ompt_multiplex_set_callback(                                  \\\n                     ompt_##event_name,                                        \\\n                     (ompt_callback_t)&ompt_multiplex_##event_name);           \\\n    else                                                                       \\\n      return ompt_multiplex_implementation_status.ompt_##event_name\n\n    OMPT_LOAD_CLIENT_FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_set_error;\n  }\n}\n\nompt_interface_fn_t ompt_multiplex_own_lookup(const char *name) {\n  if (!strcmp(name, \"ompt_set_callback\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_own_set_callback;\n  else if (!strcmp(name, \"ompt_get_task_info\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_own_get_task_info;\n  else if (!strcmp(name, \"ompt_get_thread_data\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_own_get_thread_data;\n  else if (!strcmp(name, \"ompt_get_parallel_info\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_own_get_parallel_info;\n  else\n    return ompt_multiplex_lookup_function(name);\n}\n\nompt_interface_fn_t ompt_multiplex_client_lookup(const char *name) {\n  if (!strcmp(name, \"ompt_set_callback\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_client_set_callback;\n  else if (!strcmp(name, \"ompt_get_task_info\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_client_get_task_info;\n  else if (!strcmp(name, \"ompt_get_thread_data\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_client_get_thread_data;\n  else if (!strcmp(name, \"ompt_get_parallel_info\"))\n    return (ompt_interface_fn_t)&ompt_multiplex_client_get_parallel_info;\n  else\n    return ompt_multiplex_lookup_function(name);\n}\n\nint ompt_multiplex_initialize(ompt_function_lookup_t lookup,\n                              int initial_device_num, ompt_data_t *data) {\n  ompt_multiplex_lookup_function = lookup;\n  ompt_multiplex_set_callback =\n      (ompt_set_callback_t)lookup(\"ompt_set_callback\");\n  ompt_multiplex_get_task_info =\n      (ompt_get_task_info_t)lookup(\"ompt_get_task_info\");\n  ompt_multiplex_get_thread_data =\n      (ompt_get_thread_data_t)lookup(\"ompt_get_thread_data\");\n  ompt_multiplex_get_parallel_info =\n      (ompt_get_parallel_info_t)lookup(\"ompt_get_parallel_info\");\n\n  // initialize ompt_multiplex_implementation_status\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  ompt_multiplex_implementation_status.ompt_##event_name = -1\n\n  OMPT_LOAD_CLIENT_FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  int ownRet = ompt_multiplex_own_fns->initialize(\n      ompt_multiplex_own_lookup, initial_device_num,\n      &(ompt_multiplex_own_fns->tool_data));\n  int clientRet = 0;\n  if (ompt_multiplex_client_fns)\n    clientRet = ompt_multiplex_client_fns->initialize(\n        ompt_multiplex_client_lookup, initial_device_num,\n        &(ompt_multiplex_client_fns->tool_data));\n\n  return ownRet > clientRet ? ownRet : clientRet;\n}\n\nvoid ompt_multiplex_finalize(ompt_data_t *fns) {\n  if (ompt_multiplex_client_fns)\n    ompt_multiplex_client_fns->finalize(\n        &(ompt_multiplex_client_fns->tool_data));\n  ompt_multiplex_own_fns->finalize(&(ompt_multiplex_own_fns->tool_data));\n}\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\nompt_start_tool_result_t *\nompt_multiplex_own_start_tool(unsigned int omp_version,\n                              const char *runtime_version);\n\nompt_start_tool_result_t *ompt_start_tool(unsigned int omp_version,\n                                          const char *runtime_version) {\n  // try loading client tool\n  ompt_multiplex_client_fns = NULL;\n  ompt_start_tool_result_t *(*client_start_tool)(unsigned int, const char *) =\n      NULL;\n\n  const char *tool_libs = getenv(CLIENT_TOOL_LIBRARIES_VAR);\n  if (tool_libs) {\n    // copy environement variable\n    char *tool_libs_buffer = strdup(tool_libs);\n    if (!tool_libs_buffer) {\n      printf(\"strdup Error (%i)\\n\", errno);\n      exit(-1);\n    }\n\n    int progress = 0;\n    while (progress < strlen(tool_libs)) {\n      int tmp_progress = progress;\n      while (tmp_progress < strlen(tool_libs) &&\n             tool_libs_buffer[tmp_progress] != ':')\n        tmp_progress++;\n      if (tmp_progress < strlen(tool_libs))\n        tool_libs_buffer[tmp_progress] = 0;\n      void *h = dlopen(tool_libs_buffer + progress, RTLD_LAZY);\n      if (h) {\n        client_start_tool =\n            (ompt_start_tool_result_t * (*)(unsigned int, const char *))\n                dlsym(h, \"ompt_start_tool\");\n        if (client_start_tool &&\n            (ompt_multiplex_client_fns =\n                 (*client_start_tool)(omp_version, runtime_version))) {\n          break;\n        }\n      } else {\n        printf(\"Loading %s from %s failed with: %s\\n\",\n               tool_libs_buffer + progress, CLIENT_TOOL_LIBRARIES_VAR,\n               dlerror());\n      }\n      progress = tmp_progress + 1;\n    }\n    free(tool_libs_buffer);\n  }\n  // load own tool\n  ompt_multiplex_own_fns =\n      ompt_multiplex_own_start_tool(omp_version, runtime_version);\n\n  // return multiplexed versions\n  static ompt_start_tool_result_t ompt_start_tool_result = {\n      &ompt_multiplex_initialize, &ompt_multiplex_finalize, {0}};\n  return &ompt_start_tool_result;\n}\n#ifdef __cplusplus\n}\n#endif\n\n// We rename the ompt_start_tool function of the OMPT tool and call the\n// renamed function from the ompt_start_tool function defined above.\n#define ompt_start_tool ompt_multiplex_own_start_tool\n\n#endif /* OMPT_MULTIPLEX_H */\n",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/tools/archer/ompt-tsan.cpp": "/*\n * ompt-tsan.cpp -- Archer runtime library, TSan annotations for Archer\n */\n  \n  //===----------------------------------------------------------------------===//\n  //\n  // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n  // See https://llvm.org/LICENSE.txt for details.\n  // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n  //\n  //===----------------------------------------------------------------------===//\n\n\n#ifndef __STDC_FORMAT_MACROS\n#define __STDC_FORMAT_MACROS\n#endif\n\n#include <atomic>\n#include <cassert>\n#include <cstdlib>\n#include <cstring>\n#include <inttypes.h>\n#include <iostream>\n#include <mutex>\n#include <sstream>\n#include <stack>\n#include <list>\n#include <string>\n#include <iostream>\n#include <unordered_map>\n#include <vector>\n\n#if (defined __APPLE__ && defined __MACH__)\n#include <dlfcn.h>\n#endif\n\n#include <sys/resource.h>\n#include \"omp-tools.h\"\n\nstatic int runOnTsan;\nstatic int hasReductionCallback;\n\nclass ArcherFlags {\npublic:\n#if (LLVM_VERSION) >= 40\n  int flush_shadow;\n#endif\n  int print_max_rss;\n  int verbose;\n  int enabled;\n\n  ArcherFlags(const char *env)\n      :\n#if (LLVM_VERSION) >= 40\n        flush_shadow(0),\n#endif\n        print_max_rss(0), verbose(0), enabled(1) {\n    if (env) {\n      std::vector<std::string> tokens;\n      std::string token;\n      std::string str(env);\n      std::istringstream iss(str);\n      while (std::getline(iss, token, ' '))\n        tokens.push_back(token);\n\n      for (std::vector<std::string>::iterator it = tokens.begin();\n           it != tokens.end(); ++it) {\n#if (LLVM_VERSION) >= 40\n        if (sscanf(it->c_str(), \"flush_shadow=%d\", &flush_shadow))\n          continue;\n#endif\n        if (sscanf(it->c_str(), \"print_max_rss=%d\", &print_max_rss))\n          continue;\n        if (sscanf(it->c_str(), \"verbose=%d\", &verbose))\n          continue;\n        if (sscanf(it->c_str(), \"enable=%d\", &enabled))\n          continue;\n        std::cerr << \"Illegal values for ARCHER_OPTIONS variable: \" << token\n                  << std::endl;\n      }\n    }\n  }\n};\n\nclass TsanFlags {\npublic:\n  int ignore_noninstrumented_modules;\n\n  TsanFlags(const char *env) : ignore_noninstrumented_modules(0) {\n    if (env) {\n      std::vector<std::string> tokens;\n      std::string token;\n      std::string str(env);\n      std::istringstream iss(str);\n      while (std::getline(iss, token, ' '))\n        tokens.push_back(token);\n\n      for (std::vector<std::string>::iterator it = tokens.begin();\n           it != tokens.end(); ++it) {\n        // we are interested in ignore_noninstrumented_modules to print a\n        // warning\n        if (sscanf(it->c_str(), \"ignore_noninstrumented_modules=%d\",\n                   &ignore_noninstrumented_modules))\n          continue;\n      }\n    }\n  }\n};\n\n#if (LLVM_VERSION) >= 40\nextern \"C\" {\nint __attribute__((weak)) __archer_get_omp_status();\nvoid __attribute__((weak)) __tsan_flush_memory() {}\n}\n#endif\nArcherFlags *archer_flags;\n\n// The following definitions are pasted from \"llvm/Support/Compiler.h\" to allow\n// the code\n// to be compiled with other compilers like gcc:\n\n#ifndef TsanHappensBefore\n// Thread Sanitizer is a tool that finds races in code.\n// See http://code.google.com/p/data-race-test/wiki/DynamicAnnotations .\n// tsan detects these exact functions by name.\nextern \"C\" {\n#if (defined __APPLE__ && defined __MACH__)\nstatic void AnnotateHappensAfter(const char *file, int line,\n                                 const volatile void *cv) {\n  void (*fptr)(const char *, int, const volatile void *);\n\n  fptr = (void (*)(const char *, int, const volatile void *))dlsym(\n      RTLD_DEFAULT, \"AnnotateHappensAfter\");\n  (*fptr)(file, line, cv);\n}\nstatic void AnnotateHappensBefore(const char *file, int line,\n                                  const volatile void *cv) {\n  void (*fptr)(const char *, int, const volatile void *);\n\n  fptr = (void (*)(const char *, int, const volatile void *))dlsym(\n      RTLD_DEFAULT, \"AnnotateHappensBefore\");\n  (*fptr)(file, line, cv);\n}\nstatic void AnnotateIgnoreWritesBegin(const char *file, int line) {\n  void (*fptr)(const char *, int);\n\n  fptr = (void (*)(const char *, int))dlsym(RTLD_DEFAULT,\n                                            \"AnnotateIgnoreWritesBegin\");\n  (*fptr)(file, line);\n}\nstatic void AnnotateIgnoreWritesEnd(const char *file, int line) {\n  void (*fptr)(const char *, int);\n\n  fptr = (void (*)(const char *, int))dlsym(RTLD_DEFAULT,\n                                            \"AnnotateIgnoreWritesEnd\");\n  (*fptr)(file, line);\n}\nstatic void AnnotateNewMemory(const char *file, int line,\n                              const volatile void *cv, size_t size) {\n  void (*fptr)(const char *, int, const volatile void *, size_t);\n\n  fptr = (void (*)(const char *, int, const volatile void *, size_t))dlsym(\n      RTLD_DEFAULT, \"AnnotateNewMemory\");\n  (*fptr)(file, line, cv, size);\n}\nstatic int RunningOnValgrind() {\n  int (*fptr)();\n\n  fptr = (int (*)())dlsym(RTLD_DEFAULT, \"RunningOnValgrind\");\n  if (fptr && fptr != RunningOnValgrind)\n    runOnTsan = 0;\n  return 0;\n}\n#else\nvoid __attribute__((weak))\nAnnotateHappensAfter(const char *file, int line, const volatile void *cv) {}\nvoid __attribute__((weak))\nAnnotateHappensBefore(const char *file, int line, const volatile void *cv) {}\nvoid __attribute__((weak))\nAnnotateIgnoreWritesBegin(const char *file, int line) {}\nvoid __attribute__((weak)) AnnotateIgnoreWritesEnd(const char *file, int line) {\n}\nvoid __attribute__((weak))\nAnnotateNewMemory(const char *file, int line, const volatile void *cv,\n                  size_t size) {}\nint __attribute__((weak)) RunningOnValgrind() {\n  runOnTsan = 0;\n  return 0;\n}\nvoid __attribute__((weak)) __tsan_func_entry(const void *call_pc) {}\nvoid __attribute__((weak)) __tsan_func_exit(void) {}\n#endif\n}\n\n// This marker is used to define a happens-before arc. The race detector will\n// infer an arc from the begin to the end when they share the same pointer\n// argument.\n#define TsanHappensBefore(cv) AnnotateHappensBefore(__FILE__, __LINE__, cv)\n\n// This marker defines the destination of a happens-before arc.\n#define TsanHappensAfter(cv) AnnotateHappensAfter(__FILE__, __LINE__, cv)\n\n// Ignore any races on writes between here and the next TsanIgnoreWritesEnd.\n#define TsanIgnoreWritesBegin() AnnotateIgnoreWritesBegin(__FILE__, __LINE__)\n\n// Resume checking for racy writes.\n#define TsanIgnoreWritesEnd() AnnotateIgnoreWritesEnd(__FILE__, __LINE__)\n\n// We don't really delete the clock for now\n#define TsanDeleteClock(cv)\n\n// newMemory\n#define TsanNewMemory(addr, size)                                              \\\n  AnnotateNewMemory(__FILE__, __LINE__, addr, size)\n#define TsanFreeMemory(addr, size)                                             \\\n  AnnotateNewMemory(__FILE__, __LINE__, addr, size)\n#endif\n\n// Function entry/exit\n#define TsanFuncEntry(pc) __tsan_func_entry(pc)\n#define TsanFuncExit() __tsan_func_exit()\n\n/// Required OMPT inquiry functions.\nstatic ompt_get_parallel_info_t ompt_get_parallel_info;\nstatic ompt_get_thread_data_t ompt_get_thread_data;\n\ntypedef uint64_t ompt_tsan_clockid;\n\nstatic uint64_t my_next_id() {\n  static uint64_t ID = 0;\n  uint64_t ret = __sync_fetch_and_add(&ID, 1);\n  return ret;\n}\n\n// Data structure to provide a threadsafe pool of reusable objects.\n// DataPool<Type of objects, Size of blockalloc>\ntemplate <typename T, int N> struct DataPool {\n  std::mutex DPMutex;\n  std::stack<T *> DataPointer;\n  std::list<void *> memory;\n  int total;\n\n  void newDatas() {\n    // prefix the Data with a pointer to 'this', allows to return memory to\n    // 'this',\n    // without explicitly knowing the source.\n    //\n    // To reduce lock contention, we use thread local DataPools, but Data\n    // objects move to other threads.\n    // The strategy is to get objects from local pool. Only if the object moved\n    // to another\n    // thread, we might see a penalty on release (returnData).\n    // For \"single producer\" pattern, a single thread creates tasks, these are\n    // executed by other threads.\n    // The master will have a high demand on TaskData, so return after use.\n    struct pooldata {\n      DataPool<T, N> *dp;\n      T data;\n    };\n    // We alloc without initialize the memory. We cannot call constructors.\n    // Therefore use malloc!\n    pooldata *datas = (pooldata *)malloc(sizeof(pooldata) * N);\n    memory.push_back(datas);\n    for (int i = 0; i < N; i++) {\n      datas[i].dp = this;\n      DataPointer.push(&(datas[i].data));\n    }\n    total += N;\n  }\n\n  T *getData() {\n    T *ret;\n    DPMutex.lock();\n    if (DataPointer.empty())\n      newDatas();\n    ret = DataPointer.top();\n    DataPointer.pop();\n    DPMutex.unlock();\n    return ret;\n  }\n\n  void returnData(T *data) {\n    DPMutex.lock();\n    DataPointer.push(data);\n    DPMutex.unlock();\n  }\n\n  void getDatas(int n, T **datas) {\n    DPMutex.lock();\n    for (int i = 0; i < n; i++) {\n      if (DataPointer.empty())\n        newDatas();\n      datas[i] = DataPointer.top();\n      DataPointer.pop();\n    }\n    DPMutex.unlock();\n  }\n\n  void returnDatas(int n, T **datas) {\n    DPMutex.lock();\n    for (int i = 0; i < n; i++) {\n      DataPointer.push(datas[i]);\n    }\n    DPMutex.unlock();\n  }\n\n  DataPool() : DPMutex(), DataPointer(), total(0) {}\n\n  ~DataPool() {\n    // we assume all memory is returned when the thread finished / destructor is\n    // called\n    for (auto i : memory)\n      if (i)\n        free(i);\n  }\n};\n\n// This function takes care to return the data to the originating DataPool\n// A pointer to the originating DataPool is stored just before the actual data.\ntemplate <typename T, int N> static void retData(void *data) {\n  ((DataPool<T, N> **)data)[-1]->returnData((T *)data);\n}\n\nstruct ParallelData;\n__thread DataPool<ParallelData, 4> *pdp;\n\n/// Data structure to store additional information for parallel regions.\nstruct ParallelData {\n\n  // Parallel fork is just another barrier, use Barrier[1]\n\n  /// Two addresses for relationships with barriers.\n  ompt_tsan_clockid Barrier[2];\n\n  const void *codePtr;\n\n  void *GetParallelPtr() { return &(Barrier[1]); }\n\n  void *GetBarrierPtr(unsigned Index) { return &(Barrier[Index]); }\n\n  ParallelData(const void *codeptr) : codePtr(codeptr) {}\n  ~ParallelData() {\n    TsanDeleteClock(&(Barrier[0]));\n    TsanDeleteClock(&(Barrier[1]));\n  }\n  // overload new/delete to use DataPool for memory management.\n  void *operator new(size_t size) { return pdp->getData(); }\n  void operator delete(void *p, size_t) { retData<ParallelData, 4>(p); }\n};\n\nstatic inline ParallelData *ToParallelData(ompt_data_t *parallel_data) {\n  return reinterpret_cast<ParallelData *>(parallel_data->ptr);\n}\n\nstruct Taskgroup;\n__thread DataPool<Taskgroup, 4> *tgp;\n\n/// Data structure to support stacking of taskgroups and allow synchronization.\nstruct Taskgroup {\n  /// Its address is used for relationships of the taskgroup's task set.\n  ompt_tsan_clockid Ptr;\n\n  /// Reference to the parent taskgroup.\n  Taskgroup *Parent;\n\n  Taskgroup(Taskgroup *Parent) : Parent(Parent) {}\n  ~Taskgroup() { TsanDeleteClock(&Ptr); }\n\n  void *GetPtr() { return &Ptr; }\n  // overload new/delete to use DataPool for memory management.\n  void *operator new(size_t size) { return tgp->getData(); }\n  void operator delete(void *p, size_t) { retData<Taskgroup, 4>(p); }\n};\n\nstruct TaskData;\n__thread DataPool<TaskData, 4> *tdp;\n\n/// Data structure to store additional information for tasks.\nstruct TaskData {\n  /// Its address is used for relationships of this task.\n  ompt_tsan_clockid Task;\n\n  /// Child tasks use its address to declare a relationship to a taskwait in\n  /// this task.\n  ompt_tsan_clockid Taskwait;\n\n  /// Whether this task is currently executing a barrier.\n  bool InBarrier;\n\n  /// Whether this task is an included task.\n  bool Included;\n\n  /// Index of which barrier to use next.\n  char BarrierIndex;\n\n  /// Count how often this structure has been put into child tasks + 1.\n  std::atomic_int RefCount;\n\n  /// Reference to the parent that created this task.\n  TaskData *Parent;\n\n  /// Reference to the implicit task in the stack above this task.\n  TaskData *ImplicitTask;\n\n  /// Reference to the team of this task.\n  ParallelData *Team;\n\n  /// Reference to the current taskgroup that this task either belongs to or\n  /// that it just created.\n  Taskgroup *TaskGroup;\n\n  /// Dependency information for this task.\n  ompt_dependence_t *Dependencies;\n\n  /// Number of dependency entries.\n  unsigned DependencyCount;\n\n  void *PrivateData;\n  size_t PrivateDataSize;\n\n  int execution;\n  int freed;\n\n  TaskData(TaskData *Parent)\n      : InBarrier(false), Included(false), BarrierIndex(0), RefCount(1),\n        Parent(Parent), ImplicitTask(nullptr), Team(Parent->Team),\n        TaskGroup(nullptr), DependencyCount(0), execution(0), freed(0) {\n    if (Parent != nullptr) {\n      Parent->RefCount++;\n      // Copy over pointer to taskgroup. This task may set up its own stack\n      // but for now belongs to its parent's taskgroup.\n      TaskGroup = Parent->TaskGroup;\n    }\n  }\n\n  TaskData(ParallelData *Team = nullptr)\n      : InBarrier(false), Included(false), BarrierIndex(0), RefCount(1),\n        Parent(nullptr), ImplicitTask(this), Team(Team), TaskGroup(nullptr),\n        DependencyCount(0), execution(1), freed(0) {}\n\n  ~TaskData() {\n    TsanDeleteClock(&Task);\n    TsanDeleteClock(&Taskwait);\n  }\n\n  void *GetTaskPtr() { return &Task; }\n\n  void *GetTaskwaitPtr() { return &Taskwait; }\n  // overload new/delete to use DataPool for memory management.\n  void *operator new(size_t size) { return tdp->getData(); }\n  void operator delete(void *p, size_t) { retData<TaskData, 4>(p); }\n};\n\nstatic inline TaskData *ToTaskData(ompt_data_t *task_data) {\n  return reinterpret_cast<TaskData *>(task_data->ptr);\n}\n\nstatic inline void *ToInAddr(void *OutAddr) {\n  // FIXME: This will give false negatives when a second variable lays directly\n  //        behind a variable that only has a width of 1 byte.\n  //        Another approach would be to \"negate\" the address or to flip the\n  //        first bit...\n  return reinterpret_cast<char *>(OutAddr) + 1;\n}\n\n/// Store a mutex for each wait_id to resolve race condition with callbacks.\nstd::unordered_map<ompt_wait_id_t, std::mutex> Locks;\nstd::mutex LocksMutex;\n\nstatic void ompt_tsan_thread_begin(ompt_thread_t thread_type,\n                                   ompt_data_t *thread_data) {\n  pdp = new DataPool<ParallelData, 4>;\n  TsanNewMemory(pdp, sizeof(pdp));\n  tgp = new DataPool<Taskgroup, 4>;\n  TsanNewMemory(tgp, sizeof(tgp));\n  tdp = new DataPool<TaskData, 4>;\n  TsanNewMemory(tdp, sizeof(tdp));\n  thread_data->value = my_next_id();\n}\n\nstatic void ompt_tsan_thread_end(ompt_data_t *thread_data) {\n  delete pdp;\n  delete tgp;\n  delete tdp;\n}\n\n/// OMPT event callbacks for handling parallel regions.\n\nstatic void ompt_tsan_parallel_begin(ompt_data_t *parent_task_data,\n                                     const ompt_frame_t *parent_task_frame,\n                                     ompt_data_t *parallel_data,\n                                     uint32_t requested_team_size,\n                                     int flag,\n                                     const void *codeptr_ra) {\n  ParallelData *Data = new ParallelData(codeptr_ra);\n  parallel_data->ptr = Data;\n\n  TsanHappensBefore(Data->GetParallelPtr());\n}\n\nstatic void ompt_tsan_parallel_end(ompt_data_t *parallel_data,\n                                   ompt_data_t *task_data,\n                                   int flag,\n                                   const void *codeptr_ra) {\n  ParallelData *Data = ToParallelData(parallel_data);\n  TsanHappensAfter(Data->GetBarrierPtr(0));\n  TsanHappensAfter(Data->GetBarrierPtr(1));\n\n  delete Data;\n\n#if (LLVM_VERSION >= 40)\n  if (&__archer_get_omp_status) {\n    if (__archer_get_omp_status() == 0 && archer_flags->flush_shadow)\n      __tsan_flush_memory();\n  }\n#endif\n\n}\n\nstatic void ompt_tsan_implicit_task(ompt_scope_endpoint_t endpoint,\n                                    ompt_data_t *parallel_data,\n                                    ompt_data_t *task_data,\n                                    unsigned int team_size,\n                                    unsigned int thread_num,\n                                    int type) {\n  switch (endpoint) {\n  case ompt_scope_begin:\n    if (type & ompt_task_initial) {\n      parallel_data->ptr = new ParallelData(nullptr);\n    }\n    task_data->ptr = new TaskData(ToParallelData(parallel_data));\n    TsanHappensAfter(ToParallelData(parallel_data)->GetParallelPtr());\n    TsanFuncEntry(ToParallelData(parallel_data)->codePtr);\n    break;\n  case ompt_scope_end:\n    TaskData *Data = ToTaskData(task_data);\n    assert(Data->freed == 0 && \"Implicit task end should only be called once!\");\n    Data->freed = 1;\n    assert(Data->RefCount == 1 &&\n           \"All tasks should have finished at the implicit barrier!\");\n    delete Data;\n    TsanFuncExit();\n    break;\n  }\n}\n\nstatic void ompt_tsan_sync_region(ompt_sync_region_t kind,\n                                  ompt_scope_endpoint_t endpoint,\n                                  ompt_data_t *parallel_data,\n                                  ompt_data_t *task_data,\n                                  const void *codeptr_ra) {\n  TaskData *Data = ToTaskData(task_data);\n  switch (endpoint) {\n  case ompt_scope_begin:\n    TsanFuncEntry(codeptr_ra);\n    switch (kind) {\n      case ompt_sync_region_barrier_implementation:\n      case ompt_sync_region_barrier_implicit:\n      case ompt_sync_region_barrier_explicit:\n      case ompt_sync_region_barrier: {\n        char BarrierIndex = Data->BarrierIndex;\n        TsanHappensBefore(Data->Team->GetBarrierPtr(BarrierIndex));\n\n        if (hasReductionCallback < ompt_set_always) {\n          // We ignore writes inside the barrier. These would either occur during\n          // 1. reductions performed by the runtime which are guaranteed to be\n          // race-free.\n          // 2. execution of another task.\n          // For the latter case we will re-enable tracking in task_switch.\n          Data->InBarrier = true;\n          TsanIgnoreWritesBegin();\n        }\n\n        break;\n      }\n\n      case ompt_sync_region_taskwait:\n        break;\n\n      case ompt_sync_region_taskgroup:\n        Data->TaskGroup = new Taskgroup(Data->TaskGroup);\n        break;\n\n      default:\n        break;\n    }\n    break;\n  case ompt_scope_end:\n    TsanFuncExit();\n    switch (kind) {\n      case ompt_sync_region_barrier_implementation:\n      case ompt_sync_region_barrier_implicit:\n      case ompt_sync_region_barrier_explicit:\n      case ompt_sync_region_barrier: {\n        if (hasReductionCallback < ompt_set_always) {\n          // We want to track writes after the barrier again.\n          Data->InBarrier = false;\n          TsanIgnoreWritesEnd();\n        }\n\n        char BarrierIndex = Data->BarrierIndex;\n        // Barrier will end after it has been entered by all threads.\n        if (parallel_data)\n          TsanHappensAfter(Data->Team->GetBarrierPtr(BarrierIndex));\n\n        // It is not guaranteed that all threads have exited this barrier before\n        // we enter the next one. So we will use a different address.\n        // We are however guaranteed that this current barrier is finished\n        // by the time we exit the next one. So we can then reuse the first\n        // address.\n        Data->BarrierIndex = (BarrierIndex + 1) % 2;\n        break;\n      }\n\n      case ompt_sync_region_taskwait: {\n        if (Data->execution > 1)\n          TsanHappensAfter(Data->GetTaskwaitPtr());\n        break;\n      }\n\n      case ompt_sync_region_taskgroup: {\n        assert(Data->TaskGroup != nullptr &&\n               \"Should have at least one taskgroup!\");\n\n        TsanHappensAfter(Data->TaskGroup->GetPtr());\n\n        // Delete this allocated taskgroup, all descendent task are finished by\n        // now.\n        Taskgroup *Parent = Data->TaskGroup->Parent;\n        delete Data->TaskGroup;\n        Data->TaskGroup = Parent;\n        break;\n      }\n\n      default:\n        break;\n    }\n    break;\n  }\n}\n\nstatic void ompt_tsan_reduction(ompt_sync_region_t kind,\n                                ompt_scope_endpoint_t endpoint,\n                                ompt_data_t *parallel_data,\n                                ompt_data_t *task_data,\n                                const void *codeptr_ra) {\n  switch (endpoint) {\n  case ompt_scope_begin:\n    switch (kind) {\n      case ompt_sync_region_reduction:\n        TsanIgnoreWritesBegin();\n        break;\n      default:\n        break;\n    }\n    break;\n  case ompt_scope_end:\n    switch (kind) {\n      case ompt_sync_region_reduction:\n        TsanIgnoreWritesEnd();\n        break;\n      default:\n        break;\n    }\n    break;\n  }\n}\n\n/// OMPT event callbacks for handling tasks.\n\nstatic void ompt_tsan_task_create(\n    ompt_data_t *parent_task_data, /* id of parent task            */\n    const ompt_frame_t *parent_frame, /* frame data for parent task   */\n    ompt_data_t *new_task_data, /* id of created task           */\n    int type, int has_dependences,\n    const void *codeptr_ra) /* pointer to outlined function */\n{\n  TaskData *Data;\n  assert(new_task_data->ptr == NULL &&\n         \"Task data should be initialized to NULL\");\n  if (type & ompt_task_initial) {\n    ompt_data_t *parallel_data;\n    int team_size = 1;\n    ompt_get_parallel_info(0, &parallel_data, &team_size);\n    ParallelData *PData = new ParallelData(nullptr);\n    parallel_data->ptr = PData;\n\n    Data = new TaskData(PData);\n    new_task_data->ptr = Data;\n  } else if (type & ompt_task_undeferred) {\n    Data = new TaskData(ToTaskData(parent_task_data));\n    new_task_data->ptr = Data;\n    Data->Included = true;\n  } else if (type & ompt_task_explicit || type & ompt_task_target) {\n    Data = new TaskData(ToTaskData(parent_task_data));\n    new_task_data->ptr = Data;\n\n    // Use the newly created address. We cannot use a single address from the\n    // parent because that would declare wrong relationships with other\n    // sibling tasks that may be created before this task is started!\n    TsanHappensBefore(Data->GetTaskPtr());\n    ToTaskData(parent_task_data)->execution++;\n  }\n}\n\nstatic void ompt_tsan_task_schedule(ompt_data_t *first_task_data,\n                                    ompt_task_status_t prior_task_status,\n                                    ompt_data_t *second_task_data) {\n  TaskData *FromTask = ToTaskData(first_task_data);\n  TaskData *ToTask = ToTaskData(second_task_data);\n\n  if (ToTask->Included && prior_task_status != ompt_task_complete)\n    return; // No further synchronization for begin included tasks\n  if (FromTask->Included && prior_task_status == ompt_task_complete) {\n    // Just delete the task:\n    while (FromTask != nullptr && --FromTask->RefCount == 0) {\n      TaskData *Parent = FromTask->Parent;\n      if (FromTask->DependencyCount > 0) {\n        delete[] FromTask->Dependencies;\n      }\n      delete FromTask;\n      FromTask = Parent;\n    }\n    return;\n  }\n\n  if (ToTask->execution == 0) {\n    ToTask->execution++;\n    // 1. Task will begin execution after it has been created.\n    TsanHappensAfter(ToTask->GetTaskPtr());\n    for (unsigned i = 0; i < ToTask->DependencyCount; i++) {\n      ompt_dependence_t *Dependency = &ToTask->Dependencies[i];\n\n      TsanHappensAfter(Dependency->variable.ptr);\n      // in and inout dependencies are also blocked by prior in dependencies!\n      if (Dependency->dependence_type == ompt_dependence_type_out || Dependency->dependence_type == ompt_dependence_type_inout) {\n        TsanHappensAfter(ToInAddr(Dependency->variable.ptr));\n      }\n    }\n  } else {\n    // 2. Task will resume after it has been switched away.\n    TsanHappensAfter(ToTask->GetTaskPtr());\n  }\n\n  if (prior_task_status != ompt_task_complete) {\n    ToTask->ImplicitTask = FromTask->ImplicitTask;\n    assert(ToTask->ImplicitTask != NULL &&\n           \"A task belongs to a team and has an implicit task on the stack\");\n  }\n\n  // Task may be resumed at a later point in time.\n  TsanHappensBefore(FromTask->GetTaskPtr());\n\n  if (hasReductionCallback < ompt_set_always && FromTask->InBarrier) {\n    // We want to ignore writes in the runtime code during barriers,\n    // but not when executing tasks with user code!\n    TsanIgnoreWritesEnd();\n  }\n\n  if (prior_task_status == ompt_task_complete) { // task finished\n\n    // Task will finish before a barrier in the surrounding parallel region ...\n    ParallelData *PData = FromTask->Team;\n    TsanHappensBefore(\n        PData->GetBarrierPtr(FromTask->ImplicitTask->BarrierIndex));\n\n    // ... and before an eventual taskwait by the parent thread.\n    TsanHappensBefore(FromTask->Parent->GetTaskwaitPtr());\n\n    if (FromTask->TaskGroup != nullptr) {\n      // This task is part of a taskgroup, so it will finish before the\n      // corresponding taskgroup_end.\n      TsanHappensBefore(FromTask->TaskGroup->GetPtr());\n    }\n    for (unsigned i = 0; i < FromTask->DependencyCount; i++) {\n      ompt_dependence_t *Dependency = &FromTask->Dependencies[i];\n\n      // in dependencies block following inout and out dependencies!\n      TsanHappensBefore(ToInAddr(Dependency->variable.ptr));\n      if (Dependency->dependence_type == ompt_dependence_type_out || Dependency->dependence_type == ompt_dependence_type_inout) {\n        TsanHappensBefore(Dependency->variable.ptr);\n      }\n    }\n    while (FromTask != nullptr && --FromTask->RefCount == 0) {\n      TaskData *Parent = FromTask->Parent;\n      if (FromTask->DependencyCount > 0) {\n        delete[] FromTask->Dependencies;\n      }\n      delete FromTask;\n      FromTask = Parent;\n    }\n  }\n  if (hasReductionCallback < ompt_set_always && ToTask->InBarrier) {\n    // We re-enter runtime code which currently performs a barrier.\n    TsanIgnoreWritesBegin();\n  }\n}\n\nstatic void ompt_tsan_dependences(ompt_data_t *task_data,\n                                  const ompt_dependence_t *deps,\n                                  int ndeps) {\n  if (ndeps > 0) {\n    // Copy the data to use it in task_switch and task_end.\n    TaskData *Data = ToTaskData(task_data);\n    Data->Dependencies = new ompt_dependence_t[ndeps];\n    std::memcpy(Data->Dependencies, deps,\n                sizeof(ompt_dependence_t) * ndeps);\n    Data->DependencyCount = ndeps;\n\n    // This callback is executed before this task is first started.\n    TsanHappensBefore(Data->GetTaskPtr());\n  }\n}\n\n/// OMPT event callbacks for handling locking.\nstatic void ompt_tsan_mutex_acquired(ompt_mutex_t kind,\n                                     ompt_wait_id_t wait_id,\n                                     const void *codeptr_ra) {\n\n  // Acquire our own lock to make sure that\n  // 1. the previous release has finished.\n  // 2. the next acquire doesn't start before we have finished our release.\n    LocksMutex.lock();\n    std::mutex &Lock = Locks[wait_id];\n    LocksMutex.unlock();\n\n    Lock.lock();\n    TsanHappensAfter(&Lock);\n}\n\nstatic void ompt_tsan_mutex_released(ompt_mutex_t kind,\n                                     ompt_wait_id_t wait_id,\n                                     const void *codeptr_ra) {\n    LocksMutex.lock();\n    std::mutex &Lock = Locks[wait_id];\n    LocksMutex.unlock();\n    TsanHappensBefore(&Lock);\n\n    Lock.unlock();\n}\n\n// callback , signature , variable to store result , required support level\n#define SET_OPTIONAL_CALLBACK_T(event, type, result, level)                             \\\n  do {                                                                                  \\\n    ompt_callback_##type##_t tsan_##event = &ompt_tsan_##event;                         \\\n    result = ompt_set_callback(ompt_callback_##event,                                   \\\n                                (ompt_callback_t)tsan_##event);                         \\\n    if (result < level)                                                                 \\\n      printf(\"Registered callback '\" #event \"' is not supported at \" #level \" (%i)\\n\",  \\\n             result);                                                                   \\\n  } while (0)\n\n#define SET_CALLBACK_T(event, type)                              \\\n  do {                                                           \\\n    int res;                                                     \\\n    SET_OPTIONAL_CALLBACK_T(event, type, res, ompt_set_always);  \\\n  } while (0)\n\n#define SET_CALLBACK(event) SET_CALLBACK_T(event, event)\n\nstatic int ompt_tsan_initialize(ompt_function_lookup_t lookup,\n                                int device_num,\n                                ompt_data_t *tool_data) {\n  const char *options = getenv(\"TSAN_OPTIONS\");\n  TsanFlags tsan_flags(options);\n\n  ompt_set_callback_t ompt_set_callback =\n      (ompt_set_callback_t)lookup(\"ompt_set_callback\");\n  if (ompt_set_callback == NULL) {\n    std::cerr << \"Could not set callback, exiting...\" << std::endl;\n    std::exit(1);\n  }\n  ompt_get_parallel_info =\n      (ompt_get_parallel_info_t)lookup(\"ompt_get_parallel_info\");\n  ompt_get_thread_data = (ompt_get_thread_data_t)lookup(\"ompt_get_thread_data\");\n\n  if (ompt_get_parallel_info == NULL) {\n    fprintf(stderr, \"Could not get inquiry function 'ompt_get_parallel_info', \"\n                    \"exiting...\\n\");\n    exit(1);\n  }\n\n  SET_CALLBACK(thread_begin);\n  SET_CALLBACK(thread_end);\n  SET_CALLBACK(parallel_begin);\n  SET_CALLBACK(implicit_task);\n  SET_CALLBACK(sync_region);\n  SET_CALLBACK(parallel_end);\n\n  SET_CALLBACK(task_create);\n  SET_CALLBACK(task_schedule);\n  SET_CALLBACK(dependences);\n\n  SET_CALLBACK_T(mutex_acquired, mutex);\n  SET_CALLBACK_T(mutex_released, mutex);\n  SET_OPTIONAL_CALLBACK_T(reduction, sync_region, hasReductionCallback, ompt_set_never);\n\n  if (!tsan_flags.ignore_noninstrumented_modules)\n    fprintf(\n        stderr,\n        \"Warning: please export TSAN_OPTIONS='ignore_noninstrumented_modules=1' \"\n        \"to avoid false positive reports from the OpenMP runtime!\\n\");\n  return 1; // success\n}\n\nstatic void ompt_tsan_finalize(ompt_data_t *tool_data) {\n  if (archer_flags->print_max_rss) {\n    struct rusage end;\n    getrusage(RUSAGE_SELF, &end);\n    printf(\"MAX RSS[KBytes] during execution: %ld\\n\", end.ru_maxrss);\n  }\n\n  if (archer_flags)\n    delete archer_flags;\n}\n\nextern \"C\"\nompt_start_tool_result_t *ompt_start_tool(unsigned int omp_version,\n                                          const char *runtime_version) {\n  const char *options = getenv(\"ARCHER_OPTIONS\");\n  archer_flags = new ArcherFlags(options);\n  if (!archer_flags->enabled)\n  {\n    if (archer_flags->verbose)\n      std::cout << \"Archer disabled, stopping operation\"\n                << std::endl;\n    delete archer_flags;\n    return NULL;\n  }\n  \n  static ompt_start_tool_result_t ompt_start_tool_result = {\n      &ompt_tsan_initialize, &ompt_tsan_finalize, {0}};\n  runOnTsan=1;\n  RunningOnValgrind();\n  if (!runOnTsan) // if we are not running on TSAN, give a different tool the\n    // chance to be loaded\n  {\n    if (archer_flags->verbose)\n      std::cout << \"Archer detected OpenMP application without TSan \"\n                   \"stopping operation\"\n                << std::endl;\n    delete archer_flags;\n    return NULL;\n  }\n\n  if (archer_flags->verbose)\n    std::cout << \"Archer detected OpenMP application with TSan, supplying \"\n                 \"OpenMP synchronization semantics\"\n              << std::endl;\n  return &ompt_start_tool_result;\n}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/runtime/doc/Reference.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/external/argobots/doc/img/ult_states.png",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/external/argobots/doc/img/tasklet_states.png",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/external/argobots/doc/img/es_states.png",
        "/tmp/vanessa/spack-stage/spack-stage-bolt-2.0-qzvng4ikinbupdrskjzx3jinrzop54wg/spack-src/www/Reference.pdf"
    ],
    "total_files": 850
}