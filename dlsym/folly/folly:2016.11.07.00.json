{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/ClockGettimeWrappers.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/ClockGettimeWrappers.h>\n#include <folly/Likely.h>\n#include <folly/portability/Time.h>\n\n#include <chrono>\n\n#include <time.h>\n\n#ifndef _WIN32\n#define _GNU_SOURCE 1\n#include <dlfcn.h>\n#endif\n\nnamespace folly {\nnamespace chrono {\n\nstatic int64_t clock_gettime_ns_fallback(clockid_t clock) {\n  struct timespec ts;\n  int r = clock_gettime(clock, &ts);\n  if (UNLIKELY(r != 0)) {\n    // Mimic what __clock_gettime_ns does (even though this can be a legit\n    // value).\n    return -1;\n  }\n  std::chrono::nanoseconds result =\n      std::chrono::seconds(ts.tv_sec) + std::chrono::nanoseconds(ts.tv_nsec);\n  return result.count();\n}\n\n// Initialize with default behavior, which we might override on Linux hosts\n// with VDSO support.\nint (*clock_gettime)(clockid_t, timespec* ts) = &::clock_gettime;\nint64_t (*clock_gettime_ns)(clockid_t) = &clock_gettime_ns_fallback;\n\n#ifdef __linux__\n\nnamespace {\n\nstruct VdsoInitializer {\n  VdsoInitializer() {\n    m_handle = dlopen(\"linux-vdso.so.1\", RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);\n    if (!m_handle) {\n      return;\n    }\n\n    void* p = dlsym(m_handle, \"__vdso_clock_gettime\");\n    if (p) {\n      folly::chrono::clock_gettime = (int (*)(clockid_t, timespec*))p;\n    }\n    p = dlsym(m_handle, \"__vdso_clock_gettime_ns\");\n    if (p) {\n      folly::chrono::clock_gettime_ns = (int64_t(*)(clockid_t))p;\n    }\n  }\n\n  ~VdsoInitializer() {\n    if (m_handle) {\n      clock_gettime = &::clock_gettime;\n      clock_gettime_ns = &clock_gettime_ns_fallback;\n      dlclose(m_handle);\n    }\n  }\n\n private:\n  void* m_handle;\n};\n\nstatic const VdsoInitializer vdso_initializer;\n}\n\n#endif\n}\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/experimental/exception_tracer/ExceptionTracerLib.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/experimental/exception_tracer/ExceptionTracerLib.h>\n\n#include <dlfcn.h>\n\n#include <vector>\n\n#include <folly/Indestructible.h>\n#include <folly/Portability.h>\n#include <folly/SharedMutex.h>\n#include <folly/Synchronized.h>\n\nnamespace __cxxabiv1 {\n\nextern \"C\" {\nvoid __cxa_throw(\n    void* thrownException,\n    std::type_info* type,\n    void (*destructor)(void*)) __attribute__((__noreturn__));\nvoid* __cxa_begin_catch(void* excObj) throw();\nvoid __cxa_rethrow(void) __attribute__((__noreturn__));\nvoid __cxa_rethrow(void);\nvoid __cxa_end_catch(void);\n}\n\n} // namespace __cxxabiv1\n\nusing namespace folly::exception_tracer;\n\nnamespace {\n\ntemplate <typename Function>\nclass CallbackHolder {\n public:\n  void registerCallback(Function f) {\n    SYNCHRONIZED(callbacks_) { callbacks_.push_back(std::move(f)); }\n  }\n\n  // always inline to enforce kInternalFramesNumber\n  template <typename... Args>\n  FOLLY_ALWAYS_INLINE void invoke(Args... args) {\n    SYNCHRONIZED_CONST(callbacks_) {\n      for (auto& cb : callbacks_) {\n        cb(args...);\n      }\n    }\n  }\n\n private:\n  folly::Synchronized<std::vector<Function>> callbacks_;\n};\n\n} // namespace\n\nnamespace folly {\nnamespace exception_tracer {\n\n#define DECLARE_CALLBACK(NAME)                                   \\\n  CallbackHolder<NAME##Type>& get##NAME##Callbacks() {           \\\n    static Indestructible<CallbackHolder<NAME##Type>> Callbacks; \\\n    return *Callbacks;                                           \\\n  }                                                              \\\n  void register##NAME##Callback(NAME##Type callback) {           \\\n    get##NAME##Callbacks().registerCallback(callback);           \\\n  }\n\nDECLARE_CALLBACK(CxaThrow);\nDECLARE_CALLBACK(CxaBeginCatch);\nDECLARE_CALLBACK(CxaRethrow);\nDECLARE_CALLBACK(CxaEndCatch);\nDECLARE_CALLBACK(RethrowException);\n\n} // exception_tracer\n} // folly\n\nnamespace __cxxabiv1 {\n\nvoid __cxa_throw(void* thrownException,\n                 std::type_info* type,\n                 void (*destructor)(void*)) {\n  static auto orig_cxa_throw =\n      reinterpret_cast<decltype(&__cxa_throw)>(dlsym(RTLD_NEXT, \"__cxa_throw\"));\n  getCxaThrowCallbacks().invoke(thrownException, type, destructor);\n  orig_cxa_throw(thrownException, type, destructor);\n  __builtin_unreachable(); // orig_cxa_throw never returns\n}\n\nvoid __cxa_rethrow() {\n  // __cxa_rethrow leaves the current exception on the caught stack,\n  // and __cxa_begin_catch recognizes that case.  We could do the same, but\n  // we'll implement something simpler (and slower): we pop the exception from\n  // the caught stack, and push it back onto the active stack; this way, our\n  // implementation of __cxa_begin_catch doesn't have to do anything special.\n  static auto orig_cxa_rethrow = reinterpret_cast<decltype(&__cxa_rethrow)>(\n      dlsym(RTLD_NEXT, \"__cxa_rethrow\"));\n  getCxaRethrowCallbacks().invoke();\n  orig_cxa_rethrow();\n  __builtin_unreachable(); // orig_cxa_rethrow never returns\n}\n\nvoid* __cxa_begin_catch(void* excObj) throw() {\n  // excObj is a pointer to the unwindHeader in __cxa_exception\n  static auto orig_cxa_begin_catch =\n      reinterpret_cast<decltype(&__cxa_begin_catch)>(\n          dlsym(RTLD_NEXT, \"__cxa_begin_catch\"));\n  getCxaBeginCatchCallbacks().invoke(excObj);\n  return orig_cxa_begin_catch(excObj);\n}\n\nvoid __cxa_end_catch() {\n  static auto orig_cxa_end_catch = reinterpret_cast<decltype(&__cxa_end_catch)>(\n      dlsym(RTLD_NEXT, \"__cxa_end_catch\"));\n  getCxaEndCatchCallbacks().invoke();\n  orig_cxa_end_catch();\n}\n\n} // namespace __cxxabiv1\n\nnamespace std {\n\nvoid rethrow_exception(std::exception_ptr ep) {\n  // Mangled name for std::rethrow_exception\n  // TODO(tudorb): Dicey, as it relies on the fact that std::exception_ptr\n  // is typedef'ed to a type in namespace __exception_ptr\n  static auto orig_rethrow_exception =\n      reinterpret_cast<decltype(&rethrow_exception)>(\n          dlsym(RTLD_NEXT,\n                \"_ZSt17rethrow_exceptionNSt15__exception_ptr13exception_ptrE\"));\n  getRethrowExceptionCallbacks().invoke(ep);\n  orig_rethrow_exception(ep);\n  __builtin_unreachable(); // orig_rethrow_exception never returns\n}\n\n} // namespace std\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/experimental/exception_tracer/ExceptionTracer.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/experimental/exception_tracer/ExceptionTracer.h>\n\n#include <dlfcn.h>\n#include <exception>\n#include <iostream>\n#include <glog/logging.h>\n\n#include <folly/experimental/exception_tracer/ExceptionAbi.h>\n#include <folly/experimental/exception_tracer/StackTrace.h>\n#include <folly/experimental/symbolizer/Symbolizer.h>\n#include <folly/String.h>\n\nnamespace {\n\nusing namespace ::folly::exception_tracer;\nusing namespace ::folly::symbolizer;\nusing namespace __cxxabiv1;\n\nextern \"C\" {\nStackTraceStack* getExceptionStackTraceStack(void) __attribute__((__weak__));\ntypedef StackTraceStack* (*GetExceptionStackTraceStackType)(void);\nGetExceptionStackTraceStackType getExceptionStackTraceStackFn;\n}\n\n}  // namespace\n\nnamespace folly {\nnamespace exception_tracer {\n\nstd::ostream& operator<<(std::ostream& out, const ExceptionInfo& info) {\n  printExceptionInfo(out, info, SymbolizePrinter::COLOR_IF_TTY);\n  return out;\n}\n\nvoid printExceptionInfo(\n    std::ostream& out,\n    const ExceptionInfo& info,\n    int options) {\n  out << \"Exception type: \";\n  if (info.type) {\n    out << folly::demangle(*info.type);\n  } else {\n    out << \"(unknown type)\";\n  }\n  out << \" (\" << info.frames.size()\n      << (info.frames.size() == 1 ? \" frame\" : \" frames\")\n      << \")\\n\";\n  try {\n    size_t frameCount = info.frames.size();\n\n    // Skip our own internal frames\n    static constexpr size_t kInternalFramesNumber = 3;\n    if (frameCount > kInternalFramesNumber) {\n      auto addresses = info.frames.data() + kInternalFramesNumber;\n      frameCount -= kInternalFramesNumber;\n\n      std::vector<SymbolizedFrame> frames;\n      frames.resize(frameCount);\n\n      Symbolizer symbolizer(\n          (options & SymbolizePrinter::NO_FILE_AND_LINE)\n              ? Dwarf::LocationInfoMode::DISABLED\n              : Symbolizer::kDefaultLocationInfoMode);\n      symbolizer.symbolize(addresses, frames.data(), frameCount);\n\n      OStreamSymbolizePrinter osp(out, options);\n      osp.println(addresses, frames.data(), frameCount);\n    }\n  } catch (const std::exception& e) {\n    out << \"\\n !! caught \" << folly::exceptionStr(e) << \"\\n\";\n  } catch (...) {\n    out << \"\\n !!! caught unexpected exception\\n\";\n  }\n}\n\nnamespace {\n\n/**\n * Is this a standard C++ ABI exception?\n *\n * Dependent exceptions (thrown via std::rethrow_exception) aren't --\n * exc doesn't actually point to a __cxa_exception structure, but\n * the offset of unwindHeader is correct, so exc->unwindHeader actually\n * returns a _Unwind_Exception object.  Yeah, it's ugly like that.\n */\nbool isAbiCppException(const __cxa_exception* exc) {\n  // The least significant four bytes must be \"C++\\0\"\n  static const uint64_t cppClass =\n    ((uint64_t)'C' << 24) |\n    ((uint64_t)'+' << 16) |\n    ((uint64_t)'+' << 8);\n  return (exc->unwindHeader.exception_class & 0xffffffff) == cppClass;\n}\n\n}  // namespace\n\nstd::vector<ExceptionInfo> getCurrentExceptions() {\n  struct Once {\n    Once() {\n      // See if linked in with us (getExceptionStackTraceStack is weak)\n      getExceptionStackTraceStackFn = getExceptionStackTraceStack;\n\n      if (!getExceptionStackTraceStackFn) {\n        // Nope, see if it's in a shared library\n        getExceptionStackTraceStackFn =\n          (GetExceptionStackTraceStackType)dlsym(\n              RTLD_NEXT, \"getExceptionStackTraceStack\");\n      }\n    }\n  };\n  static Once once;\n\n  std::vector<ExceptionInfo> exceptions;\n  auto currentException = __cxa_get_globals()->caughtExceptions;\n  if (!currentException) {\n    return exceptions;\n  }\n\n  StackTraceStack* traceStack = nullptr;\n  if (!getExceptionStackTraceStackFn) {\n    static bool logged = false;\n    if (!logged) {\n      LOG(WARNING)\n        << \"Exception tracer library not linked, stack traces not available\";\n      logged = true;\n    }\n  } else if ((traceStack = getExceptionStackTraceStackFn()) == nullptr) {\n    static bool logged = false;\n    if (!logged) {\n      LOG(WARNING)\n        << \"Exception stack trace invalid, stack traces not available\";\n      logged = true;\n    }\n  }\n\n  StackTrace* trace = traceStack ? traceStack->top() : nullptr;\n  while (currentException) {\n    ExceptionInfo info;\n    // Dependent exceptions (thrown via std::rethrow_exception) aren't\n    // standard ABI __cxa_exception objects, and are correctly labeled as\n    // such in the exception_class field.  We could try to extract the\n    // primary exception type in horribly hacky ways, but, for now, NULL.\n    info.type =\n      isAbiCppException(currentException) ?\n      currentException->exceptionType :\n      nullptr;\n    if (traceStack) {\n      CHECK(trace) << \"Invalid trace stack!\";\n      info.frames.assign(trace->addresses,\n                         trace->addresses + trace->frameCount);\n      trace = traceStack->next(trace);\n    }\n    currentException = currentException->nextException;\n    exceptions.push_back(std::move(info));\n  }\n  CHECK(!trace) << \"Invalid trace stack!\";\n\n  return exceptions;\n}\n\nnamespace {\n\nstd::terminate_handler origTerminate = abort;\nstd::unexpected_handler origUnexpected = abort;\n\nvoid dumpExceptionStack(const char* prefix) {\n  auto exceptions = getCurrentExceptions();\n  if (exceptions.empty()) {\n    return;\n  }\n  LOG(ERROR) << prefix << \", exception stack follows\";\n  for (auto& exc : exceptions) {\n    LOG(ERROR) << exc << \"\\n\";\n  }\n  LOG(ERROR) << \"exception stack complete\";\n}\n\nvoid terminateHandler() {\n  dumpExceptionStack(\"terminate() called\");\n  origTerminate();\n}\n\nvoid unexpectedHandler() {\n  dumpExceptionStack(\"Unexpected exception\");\n  origUnexpected();\n}\n\n}  // namespace\n\nvoid installHandlers() {\n  struct Once {\n    Once() {\n      origTerminate = std::set_terminate(terminateHandler);\n      origUnexpected = std::set_unexpected(unexpectedHandler);\n    }\n  };\n  static Once once;\n}\n\n}  // namespace exception_tracer\n}  // namespace folly\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/experimental/exception_tracer/ExceptionStackTraceLib.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <exception>\n\n#include <folly/experimental/exception_tracer/ExceptionAbi.h>\n#include <folly/experimental/exception_tracer/StackTrace.h>\n#include <folly/experimental/exception_tracer/ExceptionTracer.h>\n#include <folly/experimental/exception_tracer/ExceptionTracerLib.h>\n#include <folly/experimental/symbolizer/Symbolizer.h>\n\nusing namespace folly::exception_tracer;\n\nnamespace {\n\n// If we somehow ended up in an invalid state, we don't want to print any stack\n// trace at all because in could be bogus\nFOLLY_TLS bool invalid;\n\nFOLLY_TLS StackTraceStack activeExceptions;\nFOLLY_TLS StackTraceStack caughtExceptions;\n\n} // namespace\n\n// This function is exported and may be found via dlsym(RTLD_NEXT, ...)\nextern \"C\" StackTraceStack* getExceptionStackTraceStack() {\n  return invalid ? nullptr : &caughtExceptions;\n}\n\nnamespace {\n\nvoid addActiveException() {\n  // Capture stack trace\n  if (!invalid) {\n    if (!activeExceptions.pushCurrent()) {\n      activeExceptions.clear();\n      caughtExceptions.clear();\n      invalid = true;\n    }\n  }\n}\n\nvoid moveTopException(StackTraceStack& from, StackTraceStack& to) {\n  if (invalid) {\n    return;\n  }\n  if (!to.moveTopFrom(from)) {\n    from.clear();\n    to.clear();\n    invalid = true;\n  }\n}\n\nstruct Initializer {\n  Initializer() {\n    registerCxaThrowCallback(\n        [](void*, std::type_info*, void (*)(void*)) { addActiveException(); });\n\n    registerCxaBeginCatchCallback(\n        [](void*) { moveTopException(activeExceptions, caughtExceptions); });\n\n    registerCxaRethrowCallback(\n        []() { moveTopException(caughtExceptions, activeExceptions); });\n\n    registerCxaEndCatchCallback([]() {\n      if (invalid) {\n        return;\n      }\n\n      __cxxabiv1::__cxa_exception* top =\n          __cxxabiv1::__cxa_get_globals_fast()->caughtExceptions;\n      // This is gcc specific and not specified in the ABI:\n      // abs(handlerCount) is the number of active handlers, it's negative\n      // for rethrown exceptions and positive (always 1) for regular\n      // exceptions.\n      // In the rethrow case, we've already popped the exception off the\n      // caught stack, so we don't do anything here.\n      if (top->handlerCount == 1) {\n        if (!caughtExceptions.pop()) {\n          activeExceptions.clear();\n          invalid = true;\n        }\n      }\n    });\n\n    registerRethrowExceptionCallback(\n        [](std::exception_ptr) { addActiveException(); });\n\n    try {\n      ::folly::exception_tracer::installHandlers();\n    } catch (...) {\n    }\n  }\n};\n\nInitializer initializer;\n\n} // namespace\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/fibers/GuardPageAllocator.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#include \"GuardPageAllocator.h\"\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#endif\n#include <signal.h>\n\n#include <iostream>\n#include <mutex>\n\n#include <folly/Singleton.h>\n#include <folly/SpinLock.h>\n#include <folly/Synchronized.h>\n#include <folly/portability/SysMman.h>\n#include <folly/portability/Unistd.h>\n\n#include <glog/logging.h>\n\nnamespace folly {\nnamespace fibers {\n\n/**\n * Each stack with a guard page creates two memory mappings.\n * Since this is a limited resource, we don't want to create too many of these.\n *\n * The upper bound on total number of mappings created\n * is kNumGuarded * kMaxInUse.\n */\n\n/**\n * Number of guarded stacks per allocator instance\n */\nconstexpr size_t kNumGuarded = 100;\n\n/**\n * Maximum number of allocator instances with guarded stacks enabled\n */\nconstexpr size_t kMaxInUse = 100;\n\n/**\n * A cache for kNumGuarded stacks of a given size\n *\n * Thread safe.\n */\nclass StackCache {\n public:\n  explicit StackCache(size_t stackSize) : allocSize_(allocSize(stackSize)) {\n    auto p = ::mmap(\n        nullptr,\n        allocSize_ * kNumGuarded,\n        PROT_READ | PROT_WRITE,\n        MAP_PRIVATE | MAP_ANONYMOUS,\n        -1,\n        0);\n    PCHECK(p != (void*)(-1));\n    storage_ = reinterpret_cast<unsigned char*>(p);\n\n    /* Protect the bottommost page of every stack allocation */\n    for (size_t i = 0; i < kNumGuarded; ++i) {\n      auto allocBegin = storage_ + allocSize_ * i;\n      freeList_.emplace_back(allocBegin, /* protected= */ false);\n    }\n  }\n\n  unsigned char* borrow(size_t size) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n\n    assert(storage_);\n\n    auto as = allocSize(size);\n    if (as != allocSize_ || freeList_.empty()) {\n      return nullptr;\n    }\n\n    auto p = freeList_.back().first;\n    if (!freeList_.back().second) {\n      PCHECK(0 == ::mprotect(p, pagesize(), PROT_NONE));\n      SYNCHRONIZED(pages, protectedPages()) {\n        pages.insert(reinterpret_cast<intptr_t>(p));\n      }\n    }\n    freeList_.pop_back();\n\n    /* We allocate minimum number of pages required, plus a guard page.\n       Since we use this for stack storage, requested allocation is aligned\n       at the top of the allocated pages, while the guard page is at the bottom.\n\n               -- increasing addresses -->\n             Guard page     Normal pages\n            |xxxxxxxxxx|..........|..........|\n            <- allocSize_ ------------------->\n         p -^                <- size -------->\n                      limit -^\n    */\n    auto limit = p + allocSize_ - size;\n    assert(limit >= p + pagesize());\n    return limit;\n  }\n\n  bool giveBack(unsigned char* limit, size_t size) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n\n    assert(storage_);\n\n    auto as = allocSize(size);\n    auto p = limit + size - as;\n    if (p < storage_ || p >= storage_ + allocSize_ * kNumGuarded) {\n      /* not mine */\n      return false;\n    }\n\n    assert(as == allocSize_);\n    assert((p - storage_) % allocSize_ == 0);\n    freeList_.emplace_back(p, /* protected= */ true);\n    return true;\n  }\n\n  ~StackCache() {\n    assert(storage_);\n    SYNCHRONIZED(pages, protectedPages()) {\n      for (const auto& item : freeList_) {\n        pages.erase(reinterpret_cast<intptr_t>(item.first));\n      }\n    }\n    PCHECK(0 == ::munmap(storage_, allocSize_ * kNumGuarded));\n  }\n\n  static bool isProtected(intptr_t addr) {\n    // Use a read lock for reading.\n    SYNCHRONIZED_CONST(pages, protectedPages()) {\n      for (const auto& page : pages) {\n        intptr_t pageEnd = page + pagesize();\n        if (page <= addr && addr < pageEnd) {\n          return true;\n        }\n      }\n    }\n    return false;\n  }\n\n private:\n  folly::SpinLock lock_;\n  unsigned char* storage_{nullptr};\n  size_t allocSize_{0};\n\n  /**\n   * LIFO free list. Each pair contains stack pointer and protected flag.\n   */\n  std::vector<std::pair<unsigned char*, bool>> freeList_;\n\n  static size_t pagesize() {\n    static const size_t pagesize = sysconf(_SC_PAGESIZE);\n    return pagesize;\n  }\n\n  /* Returns a multiple of pagesize() enough to store size + one guard page */\n  static size_t allocSize(size_t size) {\n    return pagesize() * ((size + pagesize() - 1) / pagesize() + 1);\n  }\n\n  static folly::Synchronized<std::unordered_set<intptr_t>>& protectedPages() {\n    static auto instance =\n        new folly::Synchronized<std::unordered_set<intptr_t>>();\n    return *instance;\n  }\n};\n\n#ifndef _WIN32\n\nnamespace {\n\nstruct sigaction oldSigsegvAction;\n\nvoid sigsegvSignalHandler(int signum, siginfo_t* info, void*) {\n  if (signum != SIGSEGV) {\n    std::cerr << \"GuardPageAllocator signal handler called for signal: \"\n              << signum;\n    return;\n  }\n\n  if (info &&\n      StackCache::isProtected(reinterpret_cast<intptr_t>(info->si_addr))) {\n    std::cerr << \"folly::fibers Fiber stack overflow detected.\" << std::endl;\n  }\n\n  // Restore old signal handler and let it handle the signal.\n  sigaction(signum, &oldSigsegvAction, nullptr);\n  raise(signum);\n}\n\nbool isInJVM() {\n  auto getCreated = dlsym(RTLD_DEFAULT, \"JNI_GetCreatedJavaVMs\");\n  return getCreated;\n}\n\nvoid installSignalHandler() {\n  static std::once_flag onceFlag;\n  std::call_once(onceFlag, []() {\n    if (isInJVM()) {\n      // Don't install signal handler, since JVM internal signal handler doesn't\n      // work with SA_ONSTACK\n      return;\n    }\n\n    struct sigaction sa;\n    memset(&sa, 0, sizeof(sa));\n    sigemptyset(&sa.sa_mask);\n    // By default signal handlers are run on the signaled thread's stack.\n    // In case of stack overflow running the SIGSEGV signal handler on\n    // the same stack leads to another SIGSEGV and crashes the program.\n    // Use SA_ONSTACK, so alternate stack is used (only if configured via\n    // sigaltstack).\n    sa.sa_flags |= SA_SIGINFO | SA_ONSTACK;\n    sa.sa_sigaction = &sigsegvSignalHandler;\n    sigaction(SIGSEGV, &sa, &oldSigsegvAction);\n  });\n}\n}\n\n#endif\n\nclass CacheManager {\n public:\n  static CacheManager& instance() {\n    static auto inst = new CacheManager();\n    return *inst;\n  }\n\n  std::unique_ptr<StackCacheEntry> getStackCache(size_t stackSize) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n    if (inUse_ < kMaxInUse) {\n      ++inUse_;\n      return folly::make_unique<StackCacheEntry>(stackSize);\n    }\n\n    return nullptr;\n  }\n\n private:\n  folly::SpinLock lock_;\n  size_t inUse_{0};\n\n  friend class StackCacheEntry;\n\n  void giveBack(std::unique_ptr<StackCache> /* stackCache_ */) {\n    assert(inUse_ > 0);\n    --inUse_;\n    /* Note: we can add a free list for each size bucket\n       if stack re-use is important.\n       In this case this needs to be a folly::Singleton\n       to make sure the free list is cleaned up on fork.\n\n       TODO(t7351705): fix Singleton destruction order\n    */\n  }\n};\n\n/*\n * RAII Wrapper around a StackCache that calls\n * CacheManager::giveBack() on destruction.\n */\nclass StackCacheEntry {\n public:\n  explicit StackCacheEntry(size_t stackSize)\n      : stackCache_(folly::make_unique<StackCache>(stackSize)) {}\n\n  StackCache& cache() const noexcept {\n    return *stackCache_;\n  }\n\n  ~StackCacheEntry() {\n    CacheManager::instance().giveBack(std::move(stackCache_));\n  }\n\n private:\n  std::unique_ptr<StackCache> stackCache_;\n};\n\nGuardPageAllocator::GuardPageAllocator(bool useGuardPages)\n    : useGuardPages_(useGuardPages) {\n#ifndef _WIN32\n  installSignalHandler();\n#endif\n}\n\nGuardPageAllocator::~GuardPageAllocator() = default;\n\nunsigned char* GuardPageAllocator::allocate(size_t size) {\n  if (useGuardPages_ && !stackCache_) {\n    stackCache_ = CacheManager::instance().getStackCache(size);\n  }\n\n  if (stackCache_) {\n    auto p = stackCache_->cache().borrow(size);\n    if (p != nullptr) {\n      return p;\n    }\n  }\n  return fallbackAllocator_.allocate(size);\n}\n\nvoid GuardPageAllocator::deallocate(unsigned char* limit, size_t size) {\n  if (!(stackCache_ && stackCache_->cache().giveBack(limit, size))) {\n    fallbackAllocator_.deallocate(limit, size);\n  }\n}\n}\n} // folly::fibers\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/fibers/FiberManager.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#include \"FiberManagerInternal.h\"\n\n#include <signal.h>\n\n#include <cassert>\n#include <stdexcept>\n\n#include <glog/logging.h>\n\n#include <folly/fibers/Fiber.h>\n#include <folly/fibers/LoopController.h>\n\n#include <folly/SingletonThreadLocal.h>\n#include <folly/portability/SysSyscall.h>\n#include <folly/portability/Unistd.h>\n\n#ifdef FOLLY_SANITIZE_ADDRESS\n\n#include <dlfcn.h>\n\nstatic void __sanitizer_start_switch_fiber_weak(\n    void** fake_stack_save,\n    void const* fiber_stack_base,\n    size_t fiber_stack_extent)\n    __attribute__((__weakref__(\"__sanitizer_start_switch_fiber\")));\nstatic void __sanitizer_finish_switch_fiber_weak(\n    void* fake_stack_save,\n    void const** old_stack_base,\n    size_t* old_stack_extent)\n    __attribute__((__weakref__(\"__sanitizer_finish_switch_fiber\")));\nstatic void __asan_unpoison_memory_region_weak(\n    void const /* nolint */ volatile* addr,\n    size_t size) __attribute__((__weakref__(\"__asan_unpoison_memory_region\")));\n\ntypedef void (*AsanStartSwitchStackFuncPtr)(void**, void const*, size_t);\ntypedef void (*AsanFinishSwitchStackFuncPtr)(void*, void const**, size_t*);\ntypedef void (*AsanUnpoisonMemoryRegionFuncPtr)(\n    void const /* nolint */ volatile*,\n    size_t);\n\nnamespace folly {\nnamespace fibers {\n\nstatic AsanStartSwitchStackFuncPtr getStartSwitchStackFunc();\nstatic AsanFinishSwitchStackFuncPtr getFinishSwitchStackFunc();\nstatic AsanUnpoisonMemoryRegionFuncPtr getUnpoisonMemoryRegionFunc();\n}\n}\n\n#endif\n\nnamespace folly {\nnamespace fibers {\n\nFOLLY_TLS FiberManager* FiberManager::currentFiberManager_ = nullptr;\n\nFiberManager::FiberManager(\n    std::unique_ptr<LoopController> loopController,\n    Options options)\n    : FiberManager(\n          LocalType<void>(),\n          std::move(loopController),\n          std::move(options)) {}\n\nFiberManager::~FiberManager() {\n  if (isLoopScheduled_) {\n    loopController_->cancel();\n  }\n\n  while (!fibersPool_.empty()) {\n    fibersPool_.pop_front_and_dispose([](Fiber* fiber) { delete fiber; });\n  }\n  assert(readyFibers_.empty());\n  assert(fibersActive_ == 0);\n}\n\nLoopController& FiberManager::loopController() {\n  return *loopController_;\n}\n\nconst LoopController& FiberManager::loopController() const {\n  return *loopController_;\n}\n\nbool FiberManager::hasTasks() const {\n  return fibersActive_ > 0 || !remoteReadyQueue_.empty() ||\n      !remoteTaskQueue_.empty();\n}\n\nFiber* FiberManager::getFiber() {\n  Fiber* fiber = nullptr;\n\n  if (options_.fibersPoolResizePeriodMs > 0 && !fibersPoolResizerScheduled_) {\n    fibersPoolResizer_();\n    fibersPoolResizerScheduled_ = true;\n  }\n\n  if (fibersPool_.empty()) {\n    fiber = new Fiber(*this);\n    ++fibersAllocated_;\n  } else {\n    fiber = &fibersPool_.front();\n    fibersPool_.pop_front();\n    assert(fibersPoolSize_ > 0);\n    --fibersPoolSize_;\n  }\n  assert(fiber);\n  if (++fibersActive_ > maxFibersActiveLastPeriod_) {\n    maxFibersActiveLastPeriod_ = fibersActive_;\n  }\n  ++fiberId_;\n  bool recordStack = (options_.recordStackEvery != 0) &&\n      (fiberId_ % options_.recordStackEvery == 0);\n  fiber->init(recordStack);\n  return fiber;\n}\n\nvoid FiberManager::setExceptionCallback(FiberManager::ExceptionCallback ec) {\n  assert(ec);\n  exceptionCallback_ = std::move(ec);\n}\n\nsize_t FiberManager::fibersAllocated() const {\n  return fibersAllocated_;\n}\n\nsize_t FiberManager::fibersPoolSize() const {\n  return fibersPoolSize_;\n}\n\nsize_t FiberManager::stackHighWatermark() const {\n  return stackHighWatermark_;\n}\n\nvoid FiberManager::remoteReadyInsert(Fiber* fiber) {\n  if (observer_) {\n    observer_->runnable(reinterpret_cast<uintptr_t>(fiber));\n  }\n  auto insertHead = [&]() { return remoteReadyQueue_.insertHead(fiber); };\n  loopController_->scheduleThreadSafe(std::ref(insertHead));\n}\n\nvoid FiberManager::setObserver(ExecutionObserver* observer) {\n  observer_ = observer;\n}\n\nExecutionObserver* FiberManager::getObserver() {\n  return observer_;\n}\n\nvoid FiberManager::setPreemptRunner(InlineFunctionRunner* preemptRunner) {\n  preemptRunner_ = preemptRunner;\n}\n\nvoid FiberManager::doFibersPoolResizing() {\n  while (fibersAllocated_ > maxFibersActiveLastPeriod_ &&\n         fibersPoolSize_ > options_.maxFibersPoolSize) {\n    auto fiber = &fibersPool_.front();\n    assert(fiber != nullptr);\n    fibersPool_.pop_front();\n    delete fiber;\n    --fibersPoolSize_;\n    --fibersAllocated_;\n  }\n\n  maxFibersActiveLastPeriod_ = fibersActive_;\n}\n\nvoid FiberManager::FibersPoolResizer::operator()() {\n  fiberManager_.doFibersPoolResizing();\n  fiberManager_.timeoutManager_->registerTimeout(\n      *this,\n      std::chrono::milliseconds(\n          fiberManager_.options_.fibersPoolResizePeriodMs));\n}\n\n#ifdef FOLLY_SANITIZE_ADDRESS\n\nvoid FiberManager::registerStartSwitchStackWithAsan(\n    void** saveFakeStack,\n    const void* stackBottom,\n    size_t stackSize) {\n  // Check if we can find a fiber enter function and call it if we find one\n  static AsanStartSwitchStackFuncPtr fn = getStartSwitchStackFunc();\n  if (fn == nullptr) {\n    LOG(FATAL) << \"The version of ASAN in use doesn't support fibers\";\n  } else {\n    fn(saveFakeStack, stackBottom, stackSize);\n  }\n}\n\nvoid FiberManager::registerFinishSwitchStackWithAsan(\n    void* saveFakeStack,\n    const void** saveStackBottom,\n    size_t* saveStackSize) {\n  // Check if we can find a fiber exit function and call it if we find one\n  static AsanFinishSwitchStackFuncPtr fn = getFinishSwitchStackFunc();\n  if (fn == nullptr) {\n    LOG(FATAL) << \"The version of ASAN in use doesn't support fibers\";\n  } else {\n    fn(saveFakeStack, saveStackBottom, saveStackSize);\n  }\n}\n\nvoid FiberManager::unpoisonFiberStack(const Fiber* fiber) {\n  auto stack = fiber->getStack();\n\n  // Check if we can find a fiber enter function and call it if we find one\n  static AsanUnpoisonMemoryRegionFuncPtr fn = getUnpoisonMemoryRegionFunc();\n  if (fn == nullptr) {\n    LOG(FATAL) << \"This version of ASAN doesn't support memory unpoisoning\";\n  } else {\n    fn(stack.first, stack.second);\n  }\n}\n\nstatic AsanStartSwitchStackFuncPtr getStartSwitchStackFunc() {\n  AsanStartSwitchStackFuncPtr fn{nullptr};\n\n  // Check whether weak reference points to statically linked enter function\n  if (nullptr != (fn = &::__sanitizer_start_switch_fiber_weak)) {\n    return fn;\n  }\n\n  // Check whether we can find a dynamically linked enter function\n  if (nullptr != (fn = (AsanStartSwitchStackFuncPtr)dlsym(\n                      RTLD_DEFAULT, \"__sanitizer_start_switch_fiber\"))) {\n    return fn;\n  }\n\n  // Couldn't find the function at all\n  return nullptr;\n}\n\nstatic AsanFinishSwitchStackFuncPtr getFinishSwitchStackFunc() {\n  AsanFinishSwitchStackFuncPtr fn{nullptr};\n\n  // Check whether weak reference points to statically linked exit function\n  if (nullptr != (fn = &::__sanitizer_finish_switch_fiber_weak)) {\n    return fn;\n  }\n\n  // Check whether we can find a dynamically linked exit function\n  if (nullptr != (fn = (AsanFinishSwitchStackFuncPtr)dlsym(\n                      RTLD_DEFAULT, \"__sanitizer_finish_switch_fiber\"))) {\n    return fn;\n  }\n\n  // Couldn't find the function at all\n  return nullptr;\n}\n\nstatic AsanUnpoisonMemoryRegionFuncPtr getUnpoisonMemoryRegionFunc() {\n  AsanUnpoisonMemoryRegionFuncPtr fn{nullptr};\n\n  // Check whether weak reference points to statically linked unpoison function\n  if (nullptr != (fn = &::__asan_unpoison_memory_region_weak)) {\n    return fn;\n  }\n\n  // Check whether we can find a dynamically linked unpoison function\n  if (nullptr != (fn = (AsanUnpoisonMemoryRegionFuncPtr)dlsym(\n                      RTLD_DEFAULT, \"__asan_unpoison_memory_region\"))) {\n    return fn;\n  }\n\n  // Couldn't find the function at all\n  return nullptr;\n}\n\n#endif // FOLLY_SANITIZE_ADDRESS\n\n#ifndef _WIN32\nnamespace {\n\n// SIGSTKSZ (8 kB on our architectures) isn't always enough for\n// folly::symbolizer, so allocate 32 kB.\nconstexpr size_t kAltStackSize = folly::constexpr_max(SIGSTKSZ, 32 * 1024);\n\nbool hasAlternateStack() {\n  stack_t ss;\n  sigaltstack(nullptr, &ss);\n  return !(ss.ss_flags & SS_DISABLE);\n}\n\nint setAlternateStack(char* sp, size_t size) {\n  CHECK(sp);\n  stack_t ss{};\n  ss.ss_sp = sp;\n  ss.ss_size = size;\n  return sigaltstack(&ss, nullptr);\n}\n\nint unsetAlternateStack() {\n  stack_t ss{};\n  ss.ss_flags = SS_DISABLE;\n  return sigaltstack(&ss, nullptr);\n}\n\nclass ScopedAlternateSignalStack {\n public:\n  ScopedAlternateSignalStack() {\n    if (hasAlternateStack()) {\n      return;\n    }\n\n    stack_ = folly::make_unique<AltStackBuffer>();\n\n    setAlternateStack(stack_->data(), stack_->size());\n  }\n\n  ~ScopedAlternateSignalStack() {\n    if (stack_) {\n      unsetAlternateStack();\n    }\n  }\n\n private:\n  using AltStackBuffer = std::array<char, kAltStackSize>;\n  std::unique_ptr<AltStackBuffer> stack_;\n};\n}\n\nvoid FiberManager::registerAlternateSignalStack() {\n  static folly::SingletonThreadLocal<ScopedAlternateSignalStack> singleton;\n  singleton.get();\n\n  alternateSignalStackRegistered_ = true;\n}\n#endif\n}\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/detail/CacheLocality.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/detail/CacheLocality.h>\n\n#ifndef _MSC_VER\n#define _GNU_SOURCE 1 // for RTLD_NOLOAD\n#include <dlfcn.h>\n#endif\n#include <fstream>\n\n#include <folly/Conv.h>\n#include <folly/Exception.h>\n#include <folly/FileUtil.h>\n#include <folly/Format.h>\n#include <folly/ScopeGuard.h>\n\nnamespace folly {\nnamespace detail {\n\n///////////// CacheLocality\n\n/// Returns the best real CacheLocality information available\nstatic CacheLocality getSystemLocalityInfo() {\n#ifdef __linux__\n  try {\n    return CacheLocality::readFromSysfs();\n  } catch (...) {\n    // keep trying\n  }\n#endif\n\n  long numCpus = sysconf(_SC_NPROCESSORS_CONF);\n  if (numCpus <= 0) {\n    // This shouldn't happen, but if it does we should try to keep\n    // going.  We are probably not going to be able to parse /sys on\n    // this box either (although we will try), which means we are going\n    // to fall back to the SequentialThreadId splitter.  On my 16 core\n    // (x hyperthreading) dev box 16 stripes is enough to get pretty good\n    // contention avoidance with SequentialThreadId, and there is little\n    // improvement from going from 32 to 64.  This default gives us some\n    // wiggle room\n    numCpus = 32;\n  }\n  return CacheLocality::uniform(numCpus);\n}\n\ntemplate <>\nconst CacheLocality& CacheLocality::system<std::atomic>() {\n  static auto* cache = new CacheLocality(getSystemLocalityInfo());\n  return *cache;\n}\n\n// Each level of cache has sharing sets, which are the set of cpus\n// that share a common cache at that level.  These are available in a\n// hex bitset form (/sys/devices/system/cpu/cpu0/index0/shared_cpu_map,\n// for example).  They are also available in a human-readable list form,\n// as in /sys/devices/system/cpu/cpu0/index0/shared_cpu_list.  The list\n// is a comma-separated list of numbers and ranges, where the ranges are\n// a pair of decimal numbers separated by a '-'.\n//\n// To sort the cpus for optimum locality we don't really need to parse\n// the sharing sets, we just need a unique representative from the\n// equivalence class.  The smallest value works fine, and happens to be\n// the first decimal number in the file.  We load all of the equivalence\n// class information from all of the cpu*/index* directories, order the\n// cpus first by increasing last-level cache equivalence class, then by\n// the smaller caches.  Finally, we break ties with the cpu number itself.\n\n/// Returns the first decimal number in the string, or throws an exception\n/// if the string does not start with a number terminated by ',', '-',\n/// '\\n', or eos.\nstatic size_t parseLeadingNumber(const std::string& line) {\n  auto raw = line.c_str();\n  char* end;\n  unsigned long val = strtoul(raw, &end, 10);\n  if (end == raw || (*end != ',' && *end != '-' && *end != '\\n' && *end != 0)) {\n    throw std::runtime_error(\n        to<std::string>(\"error parsing list '\", line, \"'\").c_str());\n  }\n  return val;\n}\n\nCacheLocality CacheLocality::readFromSysfsTree(\n    const std::function<std::string(std::string)>& mapping) {\n  // number of equivalence classes per level\n  std::vector<size_t> numCachesByLevel;\n\n  // the list of cache equivalence classes, where equivalance classes\n  // are named by the smallest cpu in the class\n  std::vector<std::vector<size_t>> equivClassesByCpu;\n\n  std::vector<size_t> cpus;\n\n  while (true) {\n    auto cpu = cpus.size();\n    std::vector<size_t> levels;\n    for (size_t index = 0;; ++index) {\n      auto dir =\n          sformat(\"/sys/devices/system/cpu/cpu{}/cache/index{}/\", cpu, index);\n      auto cacheType = mapping(dir + \"type\");\n      auto equivStr = mapping(dir + \"shared_cpu_list\");\n      if (cacheType.size() == 0 || equivStr.size() == 0) {\n        // no more caches\n        break;\n      }\n      if (cacheType[0] == 'I') {\n        // cacheType in { \"Data\", \"Instruction\", \"Unified\" }. skip icache\n        continue;\n      }\n      auto equiv = parseLeadingNumber(equivStr);\n      auto level = levels.size();\n      levels.push_back(equiv);\n\n      if (equiv == cpu) {\n        // we only want to count the equiv classes once, so we do it when\n        // we first encounter them\n        while (numCachesByLevel.size() <= level) {\n          numCachesByLevel.push_back(0);\n        }\n        numCachesByLevel[level]++;\n      }\n    }\n\n    if (levels.size() == 0) {\n      // no levels at all for this cpu, we must be done\n      break;\n    }\n    equivClassesByCpu.emplace_back(std::move(levels));\n    cpus.push_back(cpu);\n  }\n\n  if (cpus.size() == 0) {\n    throw std::runtime_error(\"unable to load cache sharing info\");\n  }\n\n  std::sort(cpus.begin(),\n            cpus.end(),\n            [&](size_t lhs, size_t rhs) -> bool {\n              // sort first by equiv class of cache with highest index,\n              // direction doesn't matter.  If different cpus have\n              // different numbers of caches then this code might produce\n              // a sub-optimal ordering, but it won't crash\n              auto& lhsEquiv = equivClassesByCpu[lhs];\n              auto& rhsEquiv = equivClassesByCpu[rhs];\n              for (int i = std::min(lhsEquiv.size(), rhsEquiv.size()) - 1;\n                   i >= 0;\n                   --i) {\n                if (lhsEquiv[i] != rhsEquiv[i]) {\n                  return lhsEquiv[i] < rhsEquiv[i];\n                }\n              }\n\n              // break ties deterministically by cpu\n              return lhs < rhs;\n            });\n\n  // the cpus are now sorted by locality, with neighboring entries closer\n  // to each other than entries that are far away.  For striping we want\n  // the inverse map, since we are starting with the cpu\n  std::vector<size_t> indexes(cpus.size());\n  for (size_t i = 0; i < cpus.size(); ++i) {\n    indexes[cpus[i]] = i;\n  }\n\n  return CacheLocality{\n      cpus.size(), std::move(numCachesByLevel), std::move(indexes)};\n}\n\nCacheLocality CacheLocality::readFromSysfs() {\n  return readFromSysfsTree([](std::string name) {\n    std::ifstream xi(name.c_str());\n    std::string rv;\n    std::getline(xi, rv);\n    return rv;\n  });\n}\n\nCacheLocality CacheLocality::uniform(size_t numCpus) {\n  CacheLocality rv;\n\n  rv.numCpus = numCpus;\n\n  // one cache shared by all cpus\n  rv.numCachesByLevel.push_back(numCpus);\n\n  // no permutations in locality index mapping\n  for (size_t cpu = 0; cpu < numCpus; ++cpu) {\n    rv.localityIndexByCpu.push_back(cpu);\n  }\n\n  return rv;\n}\n\n////////////// Getcpu\n\nGetcpu::Func Getcpu::resolveVdsoFunc() {\n#if !FOLLY_HAVE_LINUX_VDSO\n  return nullptr;\n#else\n  void* h = dlopen(\"linux-vdso.so.1\", RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);\n  if (h == nullptr) {\n    return nullptr;\n  }\n\n  auto func = Getcpu::Func(dlsym(h, \"__vdso_getcpu\"));\n  if (func == nullptr) {\n    // technically a null result could either be a failure or a successful\n    // lookup of a symbol with the null value, but the second can't actually\n    // happen for this symbol.  No point holding the handle forever if\n    // we don't need the code\n    dlclose(h);\n  }\n\n  return func;\n#endif\n}\n\n#ifdef FOLLY_TLS\n/////////////// SequentialThreadId\ntemplate struct SequentialThreadId<std::atomic>;\n#endif\n\n/////////////// AccessSpreader\ntemplate struct AccessSpreader<std::atomic>;\n\n} // namespace detail\n} // namespace folly\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/test/ThreadLocalTest.cpp": "/*\n * Copyright 2016 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/ThreadLocal.h>\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#include <sys/wait.h>\n#endif\n\n#include <sys/types.h>\n\n#include <array>\n#include <atomic>\n#include <chrono>\n#include <condition_variable>\n#include <limits.h>\n#include <map>\n#include <mutex>\n#include <set>\n#include <thread>\n#include <unordered_map>\n\n#include <glog/logging.h>\n\n#include <folly/Baton.h>\n#include <folly/Memory.h>\n#include <folly/experimental/io/FsUtil.h>\n#include <folly/portability/GTest.h>\n#include <folly/portability/Unistd.h>\n\nusing namespace folly;\n\nstruct Widget {\n  static int totalVal_;\n  int val_;\n  ~Widget() {\n    totalVal_ += val_;\n  }\n\n  static void customDeleter(Widget* w, TLPDestructionMode mode) {\n    totalVal_ += (mode == TLPDestructionMode::ALL_THREADS) ? 1000 : 1;\n    delete w;\n  }\n};\nint Widget::totalVal_ = 0;\n\nTEST(ThreadLocalPtr, BasicDestructor) {\n  Widget::totalVal_ = 0;\n  ThreadLocalPtr<Widget> w;\n  std::thread([&w]() {\n      w.reset(new Widget());\n      w.get()->val_ += 10;\n    }).join();\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, CustomDeleter1) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocalPtr<Widget> w;\n    std::thread([&w]() {\n        w.reset(new Widget(), Widget::customDeleter);\n        w.get()->val_ += 10;\n      }).join();\n    EXPECT_EQ(11, Widget::totalVal_);\n  }\n  EXPECT_EQ(11, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, CustomDeleterOwnershipTransfer) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocalPtr<Widget> w;\n    auto deleter = [](Widget* ptr) {\n      Widget::customDeleter(ptr, TLPDestructionMode::THIS_THREAD);\n    };\n    std::unique_ptr<Widget, decltype(deleter)> source(new Widget(), deleter);\n    std::thread([&w, &source]() {\n      w.reset(std::move(source));\n      w.get()->val_ += 10;\n    }).join();\n    EXPECT_EQ(11, Widget::totalVal_);\n  }\n  EXPECT_EQ(11, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, DefaultDeleterOwnershipTransfer) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocalPtr<Widget> w;\n    auto source = folly::make_unique<Widget>();\n    std::thread([&w, &source]() {\n      w.reset(std::move(source));\n      w.get()->val_ += 10;\n    }).join();\n    EXPECT_EQ(10, Widget::totalVal_);\n  }\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, resetNull) {\n  ThreadLocalPtr<int> tl;\n  EXPECT_FALSE(tl);\n  tl.reset(new int(4));\n  EXPECT_TRUE(static_cast<bool>(tl));\n  EXPECT_EQ(*tl.get(), 4);\n  tl.reset();\n  EXPECT_FALSE(tl);\n}\n\nTEST(ThreadLocalPtr, TestRelease) {\n  Widget::totalVal_ = 0;\n  ThreadLocalPtr<Widget> w;\n  std::unique_ptr<Widget> wPtr;\n  std::thread([&w, &wPtr]() {\n      w.reset(new Widget());\n      w.get()->val_ += 10;\n\n      wPtr.reset(w.release());\n    }).join();\n  EXPECT_EQ(0, Widget::totalVal_);\n  wPtr.reset();\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, CreateOnThreadExit) {\n  Widget::totalVal_ = 0;\n  ThreadLocal<Widget> w;\n  ThreadLocalPtr<int> tl;\n\n  std::thread([&] {\n    tl.reset(new int(1),\n             [&](int* ptr, TLPDestructionMode /* mode */) {\n               delete ptr;\n               // This test ensures Widgets allocated here are not leaked.\n               ++w.get()->val_;\n               ThreadLocal<Widget> wl;\n               ++wl.get()->val_;\n             });\n  }).join();\n  EXPECT_EQ(2, Widget::totalVal_);\n}\n\n// Test deleting the ThreadLocalPtr object\nTEST(ThreadLocalPtr, CustomDeleter2) {\n  Widget::totalVal_ = 0;\n  std::thread t;\n  std::mutex mutex;\n  std::condition_variable cv;\n  enum class State {\n    START,\n    DONE,\n    EXIT\n  };\n  State state = State::START;\n  {\n    ThreadLocalPtr<Widget> w;\n    t = std::thread([&]() {\n        w.reset(new Widget(), Widget::customDeleter);\n        w.get()->val_ += 10;\n\n        // Notify main thread that we're done\n        {\n          std::unique_lock<std::mutex> lock(mutex);\n          state = State::DONE;\n          cv.notify_all();\n        }\n\n        // Wait for main thread to allow us to exit\n        {\n          std::unique_lock<std::mutex> lock(mutex);\n          while (state != State::EXIT) {\n            cv.wait(lock);\n          }\n        }\n    });\n\n    // Wait for main thread to start (and set w.get()->val_)\n    {\n      std::unique_lock<std::mutex> lock(mutex);\n      while (state != State::DONE) {\n        cv.wait(lock);\n      }\n    }\n\n    // Thread started but hasn't exited yet\n    EXPECT_EQ(0, Widget::totalVal_);\n\n    // Destroy ThreadLocalPtr<Widget> (by letting it go out of scope)\n  }\n\n  EXPECT_EQ(1010, Widget::totalVal_);\n\n  // Allow thread to exit\n  {\n    std::unique_lock<std::mutex> lock(mutex);\n    state = State::EXIT;\n    cv.notify_all();\n  }\n  t.join();\n\n  EXPECT_EQ(1010, Widget::totalVal_);\n}\n\nTEST(ThreadLocal, BasicDestructor) {\n  Widget::totalVal_ = 0;\n  ThreadLocal<Widget> w;\n  std::thread([&w]() { w->val_ += 10; }).join();\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocal, SimpleRepeatDestructor) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocal<Widget> w;\n    w->val_ += 10;\n  }\n  {\n    ThreadLocal<Widget> w;\n    w->val_ += 10;\n  }\n  EXPECT_EQ(20, Widget::totalVal_);\n}\n\nTEST(ThreadLocal, InterleavedDestructors) {\n  Widget::totalVal_ = 0;\n  std::unique_ptr<ThreadLocal<Widget>> w;\n  int wVersion = 0;\n  const int wVersionMax = 2;\n  int thIter = 0;\n  std::mutex lock;\n  auto th = std::thread([&]() {\n    int wVersionPrev = 0;\n    while (true) {\n      while (true) {\n        std::lock_guard<std::mutex> g(lock);\n        if (wVersion > wVersionMax) {\n          return;\n        }\n        if (wVersion > wVersionPrev) {\n          // We have a new version of w, so it should be initialized to zero\n          EXPECT_EQ((*w)->val_, 0);\n          break;\n        }\n      }\n      std::lock_guard<std::mutex> g(lock);\n      wVersionPrev = wVersion;\n      (*w)->val_ += 10;\n      ++thIter;\n    }\n  });\n  FOR_EACH_RANGE(i, 0, wVersionMax) {\n    int thIterPrev = 0;\n    {\n      std::lock_guard<std::mutex> g(lock);\n      thIterPrev = thIter;\n      w.reset(new ThreadLocal<Widget>());\n      ++wVersion;\n    }\n    while (true) {\n      std::lock_guard<std::mutex> g(lock);\n      if (thIter > thIterPrev) {\n        break;\n      }\n    }\n  }\n  {\n    std::lock_guard<std::mutex> g(lock);\n    wVersion = wVersionMax + 1;\n  }\n  th.join();\n  EXPECT_EQ(wVersionMax * 10, Widget::totalVal_);\n}\n\nclass SimpleThreadCachedInt {\n\n  class NewTag;\n  ThreadLocal<int,NewTag> val_;\n\n public:\n  void add(int val) {\n    *val_ += val;\n  }\n\n  int read() {\n    int ret = 0;\n    for (const auto& i : val_.accessAllThreads()) {\n      ret += i;\n    }\n    return ret;\n  }\n};\n\nTEST(ThreadLocalPtr, AccessAllThreadsCounter) {\n  const int kNumThreads = 10;\n  SimpleThreadCachedInt stci;\n  std::atomic<bool> run(true);\n  std::atomic<int> totalAtomic(0);\n  std::vector<std::thread> threads;\n  for (int i = 0; i < kNumThreads; ++i) {\n    threads.push_back(std::thread([&,i]() {\n      stci.add(1);\n      totalAtomic.fetch_add(1);\n      while (run.load()) { usleep(100); }\n    }));\n  }\n  while (totalAtomic.load() != kNumThreads) { usleep(100); }\n  EXPECT_EQ(kNumThreads, stci.read());\n  run.store(false);\n  for (auto& t : threads) {\n    t.join();\n  }\n}\n\nTEST(ThreadLocal, resetNull) {\n  ThreadLocal<int> tl;\n  tl.reset(new int(4));\n  EXPECT_EQ(*tl.get(), 4);\n  tl.reset();\n  EXPECT_EQ(*tl.get(), 0);\n  tl.reset(new int(5));\n  EXPECT_EQ(*tl.get(), 5);\n}\n\nnamespace {\nstruct Tag {};\n\nstruct Foo {\n  folly::ThreadLocal<int, Tag> tl;\n};\n}  // namespace\n\nTEST(ThreadLocal, Movable1) {\n  Foo a;\n  Foo b;\n  EXPECT_TRUE(a.tl.get() != b.tl.get());\n\n  a = Foo();\n  b = Foo();\n  EXPECT_TRUE(a.tl.get() != b.tl.get());\n}\n\nTEST(ThreadLocal, Movable2) {\n  std::map<int, Foo> map;\n\n  map[42];\n  map[10];\n  map[23];\n  map[100];\n\n  std::set<void*> tls;\n  for (auto& m : map) {\n    tls.insert(m.second.tl.get());\n  }\n\n  // Make sure that we have 4 different instances of *tl\n  EXPECT_EQ(4, tls.size());\n}\n\nnamespace {\n\nconstexpr size_t kFillObjectSize = 300;\n\nstd::atomic<uint64_t> gDestroyed;\n\n/**\n * Fill a chunk of memory with a unique-ish pattern that includes the thread id\n * (so deleting one of these from another thread would cause a failure)\n *\n * Verify it explicitly and on destruction.\n */\nclass FillObject {\n public:\n  explicit FillObject(uint64_t idx) : idx_(idx) {\n    uint64_t v = val();\n    for (size_t i = 0; i < kFillObjectSize; ++i) {\n      data_[i] = v;\n    }\n  }\n\n  void check() {\n    uint64_t v = val();\n    for (size_t i = 0; i < kFillObjectSize; ++i) {\n      CHECK_EQ(v, data_[i]);\n    }\n  }\n\n  ~FillObject() {\n    ++gDestroyed;\n  }\n\n private:\n  uint64_t val() const {\n    return (idx_ << 40) | uint64_t(pthread_self());\n  }\n\n  uint64_t idx_;\n  uint64_t data_[kFillObjectSize];\n};\n\n}  // namespace\n\n#if FOLLY_HAVE_STD_THIS_THREAD_SLEEP_FOR\nTEST(ThreadLocal, Stress) {\n  constexpr size_t numFillObjects = 250;\n  std::array<ThreadLocalPtr<FillObject>, numFillObjects> objects;\n\n  constexpr size_t numThreads = 32;\n  constexpr size_t numReps = 20;\n\n  std::vector<std::thread> threads;\n  threads.reserve(numThreads);\n\n  for (size_t k = 0; k < numThreads; ++k) {\n    threads.emplace_back([&objects] {\n      for (size_t rep = 0; rep < numReps; ++rep) {\n        for (size_t i = 0; i < objects.size(); ++i) {\n          objects[i].reset(new FillObject(rep * objects.size() + i));\n          std::this_thread::sleep_for(std::chrono::microseconds(100));\n        }\n        for (size_t i = 0; i < objects.size(); ++i) {\n          objects[i]->check();\n        }\n      }\n    });\n  }\n\n  for (auto& t : threads) {\n    t.join();\n  }\n\n  EXPECT_EQ(numFillObjects * numThreads * numReps, gDestroyed);\n}\n#endif\n\n// Yes, threads and fork don't mix\n// (http://cppwisdom.quora.com/Why-threads-and-fork-dont-mix) but if you're\n// stupid or desperate enough to try, we shouldn't stand in your way.\nnamespace {\nclass HoldsOne {\n public:\n  HoldsOne() : value_(1) { }\n  // Do an actual access to catch the buggy case where this == nullptr\n  int value() const { return value_; }\n private:\n  int value_;\n};\n\nstruct HoldsOneTag {};\n\nThreadLocal<HoldsOne, HoldsOneTag> ptr;\n\nint totalValue() {\n  int value = 0;\n  for (auto& p : ptr.accessAllThreads()) {\n    value += p.value();\n  }\n  return value;\n}\n\n}  // namespace\n\n#ifdef FOLLY_HAVE_PTHREAD_ATFORK\nTEST(ThreadLocal, Fork) {\n  EXPECT_EQ(1, ptr->value());  // ensure created\n  EXPECT_EQ(1, totalValue());\n  // Spawn a new thread\n\n  std::mutex mutex;\n  bool started = false;\n  std::condition_variable startedCond;\n  bool stopped = false;\n  std::condition_variable stoppedCond;\n\n  std::thread t([&] () {\n    EXPECT_EQ(1, ptr->value());  // ensure created\n    {\n      std::unique_lock<std::mutex> lock(mutex);\n      started = true;\n      startedCond.notify_all();\n    }\n    {\n      std::unique_lock<std::mutex> lock(mutex);\n      while (!stopped) {\n        stoppedCond.wait(lock);\n      }\n    }\n  });\n\n  {\n    std::unique_lock<std::mutex> lock(mutex);\n    while (!started) {\n      startedCond.wait(lock);\n    }\n  }\n\n  EXPECT_EQ(2, totalValue());\n\n  pid_t pid = fork();\n  if (pid == 0) {\n    // in child\n    int v = totalValue();\n\n    // exit successfully if v == 1 (one thread)\n    // diagnostic error code otherwise :)\n    switch (v) {\n    case 1: _exit(0);\n    case 0: _exit(1);\n    }\n    _exit(2);\n  } else if (pid > 0) {\n    // in parent\n    int status;\n    EXPECT_EQ(pid, waitpid(pid, &status, 0));\n    EXPECT_TRUE(WIFEXITED(status));\n    EXPECT_EQ(0, WEXITSTATUS(status));\n  } else {\n    EXPECT_TRUE(false) << \"fork failed\";\n  }\n\n  EXPECT_EQ(2, totalValue());\n\n  {\n    std::unique_lock<std::mutex> lock(mutex);\n    stopped = true;\n    stoppedCond.notify_all();\n  }\n\n  t.join();\n\n  EXPECT_EQ(1, totalValue());\n}\n#endif\n\n#ifndef _WIN32\nstruct HoldsOneTag2 {};\n\nTEST(ThreadLocal, Fork2) {\n  // A thread-local tag that was used in the parent from a *different* thread\n  // (but not the forking thread) would cause the child to hang in a\n  // ThreadLocalPtr's object destructor. Yeah.\n  ThreadLocal<HoldsOne, HoldsOneTag2> p;\n  {\n    // use tag in different thread\n    std::thread t([&p] { p.get(); });\n    t.join();\n  }\n  pid_t pid = fork();\n  if (pid == 0) {\n    {\n      ThreadLocal<HoldsOne, HoldsOneTag2> q;\n      q.get();\n    }\n    _exit(0);\n  } else if (pid > 0) {\n    int status;\n    EXPECT_EQ(pid, waitpid(pid, &status, 0));\n    EXPECT_TRUE(WIFEXITED(status));\n    EXPECT_EQ(0, WEXITSTATUS(status));\n  } else {\n    EXPECT_TRUE(false) << \"fork failed\";\n  }\n}\n\n// Elide this test when using any sanitizer. Otherwise, the dlopen'ed code\n// would end up running without e.g., ASAN-initialized data structures and\n// failing right away.\n#if !defined FOLLY_SANITIZE_ADDRESS && !defined UNDEFINED_SANITIZER && \\\n    !defined FOLLY_SANITIZE_THREAD\n\nTEST(ThreadLocal, SharedLibrary) {\n  auto exe = fs::executable_path();\n  auto lib = exe.parent_path() / \"lib_thread_local_test.so\";\n  auto handle = dlopen(lib.string().c_str(), RTLD_LAZY);\n  EXPECT_NE(nullptr, handle);\n\n  typedef void (*useA_t)();\n  dlerror();\n  useA_t useA = (useA_t) dlsym(handle, \"useA\");\n\n  const char *dlsym_error = dlerror();\n  EXPECT_EQ(nullptr, dlsym_error);\n\n  useA();\n\n  folly::Baton<> b11, b12, b21, b22;\n\n  std::thread t1([&]() {\n      useA();\n      b11.post();\n      b12.wait();\n    });\n\n  std::thread t2([&]() {\n      useA();\n      b21.post();\n      b22.wait();\n    });\n\n  b11.wait();\n  b21.wait();\n\n  dlclose(handle);\n\n  b12.post();\n  b22.post();\n\n  t1.join();\n  t2.join();\n}\n\n#endif\n#endif\n\nnamespace folly { namespace threadlocal_detail {\nstruct PthreadKeyUnregisterTester {\n  PthreadKeyUnregister p;\n  constexpr PthreadKeyUnregisterTester() = default;\n};\n}}\n\nTEST(ThreadLocal, UnregisterClassHasConstExprCtor) {\n  folly::threadlocal_detail::PthreadKeyUnregisterTester x;\n  // yep!\n  SUCCEED();\n}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-folly-2016.11.07.00-zj2h2mpp6yey55d2unbd6u5b6j2urpe7/spack-src/folly/docs/Fbvector--graphical_solutions.png"
    ],
    "total_files": 908
}