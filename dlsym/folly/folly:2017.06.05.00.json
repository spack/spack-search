{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/Singleton.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/Singleton.h>\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#endif\n\n#include <atomic>\n#include <cstdio>\n#include <cstdlib>\n#include <iostream>\n#include <string>\n\n#include <folly/ScopeGuard.h>\n\n#if !defined(_WIN32) && !defined(__APPLE__) && !defined(__ANDROID__)\nstatic void hs_init_weak(int* argc, char** argv[])\n    __attribute__((__weakref__(\"hs_init\")));\n#endif\n\nnamespace folly {\n\nSingletonVault::Type SingletonVault::defaultVaultType() {\n#if !defined(_WIN32) && !defined(__APPLE__) && !defined(__ANDROID__)\n  bool isPython = dlsym(RTLD_DEFAULT, \"Py_Main\");\n  bool isHaskel = &::hs_init_weak || dlsym(RTLD_DEFAULT, \"hs_init\");\n  bool isJVM = dlsym(RTLD_DEFAULT, \"JNI_GetCreatedJavaVMs\");\n  bool isD = dlsym(RTLD_DEFAULT, \"_d_run_main\");\n\n  return isPython || isHaskel || isJVM || isD ? Type::Relaxed : Type::Strict;\n#else\n  return Type::Relaxed;\n#endif\n}\n\nnamespace detail {\n\n[[noreturn]] void singletonWarnDoubleRegistrationAndAbort(\n    const TypeDescriptor& type) {\n  // Ensure the availability of std::cerr\n  std::ios_base::Init ioInit;\n  std::cerr << \"Double registration of singletons of the same \"\n               \"underlying type; check for multiple definitions \"\n               \"of type folly::Singleton<\"\n            << type.name() << \">\\n\";\n  std::abort();\n}\n}\n\nnamespace {\n\nstruct FatalHelper {\n  ~FatalHelper() {\n    if (!leakedSingletons_.empty()) {\n      std::string leakedTypes;\n      for (const auto& singleton : leakedSingletons_) {\n        leakedTypes += \"\\t\" + singleton.name() + \"\\n\";\n      }\n      LOG(DFATAL) << \"Singletons of the following types had living references \"\n                  << \"after destroyInstances was finished:\\n\" << leakedTypes\n                  << \"beware! It is very likely that those singleton instances \"\n                  << \"are leaked.\";\n    }\n  }\n\n  std::vector<detail::TypeDescriptor> leakedSingletons_;\n};\n\n#if defined(__APPLE__) || defined(_MSC_VER)\n// OS X doesn't support constructor priorities.\nFatalHelper fatalHelper;\n#else\nFatalHelper __attribute__ ((__init_priority__ (101))) fatalHelper;\n#endif\n\n}\n\nSingletonVault::~SingletonVault() { destroyInstances(); }\n\nvoid SingletonVault::registerSingleton(detail::SingletonHolderBase* entry) {\n  auto state = state_.rlock();\n  stateCheck(SingletonVaultState::Running, *state);\n\n  if (UNLIKELY(state->registrationComplete)) {\n    LOG(ERROR) << \"Registering singleton after registrationComplete().\";\n  }\n\n  auto singletons = singletons_.wlock();\n  CHECK_THROW(\n      singletons->emplace(entry->type(), entry).second, std::logic_error);\n}\n\nvoid SingletonVault::addEagerInitSingleton(detail::SingletonHolderBase* entry) {\n  auto state = state_.rlock();\n  stateCheck(SingletonVaultState::Running, *state);\n\n  if (UNLIKELY(state->registrationComplete)) {\n    LOG(ERROR) << \"Registering for eager-load after registrationComplete().\";\n  }\n\n  CHECK_THROW(singletons_.rlock()->count(entry->type()), std::logic_error);\n\n  auto eagerInitSingletons = eagerInitSingletons_.wlock();\n  eagerInitSingletons->insert(entry);\n}\n\nvoid SingletonVault::registrationComplete() {\n  std::atexit([](){ SingletonVault::singleton()->destroyInstances(); });\n\n  auto state = state_.wlock();\n  stateCheck(SingletonVaultState::Running, *state);\n\n  if (state->registrationComplete) {\n    return;\n  }\n\n  auto singletons = singletons_.rlock();\n  if (type_ == Type::Strict) {\n    for (const auto& p : *singletons) {\n      if (p.second->hasLiveInstance()) {\n        throw std::runtime_error(\n            \"Singleton \" + p.first.name() +\n            \" created before registration was complete.\");\n      }\n    }\n  }\n\n  state->registrationComplete = true;\n}\n\nvoid SingletonVault::doEagerInit() {\n  {\n    auto state = state_.rlock();\n    stateCheck(SingletonVaultState::Running, *state);\n    if (UNLIKELY(!state->registrationComplete)) {\n      throw std::logic_error(\"registrationComplete() not yet called\");\n    }\n  }\n\n  auto eagerInitSingletons = eagerInitSingletons_.rlock();\n  for (auto* single : *eagerInitSingletons) {\n    single->createInstance();\n  }\n}\n\nvoid SingletonVault::doEagerInitVia(Executor& exe, folly::Baton<>* done) {\n  {\n    auto state = state_.rlock();\n    stateCheck(SingletonVaultState::Running, *state);\n    if (UNLIKELY(!state->registrationComplete)) {\n      throw std::logic_error(\"registrationComplete() not yet called\");\n    }\n  }\n\n  auto eagerInitSingletons = eagerInitSingletons_.rlock();\n  auto countdown =\n      std::make_shared<std::atomic<size_t>>(eagerInitSingletons->size());\n  for (auto* single : *eagerInitSingletons) {\n    // countdown is retained by shared_ptr, and will be alive until last lambda\n    // is done.  notifyBaton is provided by the caller, and expected to remain\n    // present (if it's non-nullptr).  singletonSet can go out of scope but\n    // its values, which are SingletonHolderBase pointers, are alive as long as\n    // SingletonVault is not being destroyed.\n    exe.add([=] {\n      // decrement counter and notify if requested, whether initialization\n      // was successful, was skipped (already initialized), or exception thrown.\n      SCOPE_EXIT {\n        if (--(*countdown) == 0) {\n          if (done != nullptr) {\n            done->post();\n          }\n        }\n      };\n      // if initialization is in progress in another thread, don't try to init\n      // here.  Otherwise the current thread will block on 'createInstance'.\n      if (!single->creationStarted()) {\n        single->createInstance();\n      }\n    });\n  }\n}\n\nvoid SingletonVault::destroyInstances() {\n  auto stateW = state_.wlock();\n  if (stateW->state == SingletonVaultState::Quiescing) {\n    return;\n  }\n  stateW->state = SingletonVaultState::Quiescing;\n\n  auto stateR = stateW.moveFromWriteToRead();\n  {\n    auto singletons = singletons_.rlock();\n    auto creationOrder = creationOrder_.rlock();\n\n    CHECK_GE(singletons->size(), creationOrder->size());\n\n    // Release all ReadMostlyMainPtrs at once\n    {\n      ReadMostlyMainPtrDeleter<> deleter;\n      for (auto& singleton_type : *creationOrder) {\n        singletons->at(singleton_type)->preDestroyInstance(deleter);\n      }\n    }\n\n    for (auto type_iter = creationOrder->rbegin();\n         type_iter != creationOrder->rend();\n         ++type_iter) {\n      singletons->at(*type_iter)->destroyInstance();\n    }\n\n    for (auto& singleton_type : *creationOrder) {\n      auto singleton = singletons->at(singleton_type);\n      if (!singleton->hasLiveInstance()) {\n        continue;\n      }\n\n      fatalHelper.leakedSingletons_.push_back(singleton->type());\n    }\n  }\n\n  {\n    auto creationOrder = creationOrder_.wlock();\n    creationOrder->clear();\n  }\n}\n\nvoid SingletonVault::reenableInstances() {\n  auto state = state_.wlock();\n\n  stateCheck(SingletonVaultState::Quiescing, *state);\n\n  state->state = SingletonVaultState::Running;\n}\n\nvoid SingletonVault::scheduleDestroyInstances() {\n  // Add a dependency on folly::ThreadLocal to make sure all its static\n  // singletons are initalized first.\n  threadlocal_detail::StaticMeta<void, void>::instance();\n\n  class SingletonVaultDestructor {\n   public:\n    ~SingletonVaultDestructor() {\n      SingletonVault::singleton()->destroyInstances();\n    }\n  };\n\n  // Here we intialize a singleton, which calls destroyInstances in its\n  // destructor. Because of singleton destruction order - it will be destroyed\n  // before all the singletons, which were initialized before it and after all\n  // the singletons initialized after it.\n  static SingletonVaultDestructor singletonVaultDestructor;\n}\n\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/ClockGettimeWrappers.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/ClockGettimeWrappers.h>\n#include <folly/Likely.h>\n#include <folly/portability/Time.h>\n\n#include <chrono>\n\n#include <time.h>\n\n#ifndef _WIN32\n#define _GNU_SOURCE 1\n#include <dlfcn.h>\n#endif\n\nnamespace folly {\nnamespace chrono {\n\nstatic int64_t clock_gettime_ns_fallback(clockid_t clock) {\n  struct timespec ts;\n  int r = clock_gettime(clock, &ts);\n  if (UNLIKELY(r != 0)) {\n    // Mimic what __clock_gettime_ns does (even though this can be a legit\n    // value).\n    return -1;\n  }\n  std::chrono::nanoseconds result =\n      std::chrono::seconds(ts.tv_sec) + std::chrono::nanoseconds(ts.tv_nsec);\n  return result.count();\n}\n\n// Initialize with default behavior, which we might override on Linux hosts\n// with VDSO support.\nint (*clock_gettime)(clockid_t, timespec* ts) = &::clock_gettime;\nint64_t (*clock_gettime_ns)(clockid_t) = &clock_gettime_ns_fallback;\n\n#ifdef FOLLY_HAVE_LINUX_VDSO\n\nnamespace {\n\nstruct VdsoInitializer {\n  VdsoInitializer() {\n    m_handle = dlopen(\"linux-vdso.so.1\", RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);\n    if (!m_handle) {\n      return;\n    }\n\n    void* p = dlsym(m_handle, \"__vdso_clock_gettime\");\n    if (p) {\n      folly::chrono::clock_gettime = (int (*)(clockid_t, timespec*))p;\n    }\n    p = dlsym(m_handle, \"__vdso_clock_gettime_ns\");\n    if (p) {\n      folly::chrono::clock_gettime_ns = (int64_t(*)(clockid_t))p;\n    }\n  }\n\n  ~VdsoInitializer() {\n    if (m_handle) {\n      clock_gettime = &::clock_gettime;\n      clock_gettime_ns = &clock_gettime_ns_fallback;\n      dlclose(m_handle);\n    }\n  }\n\n private:\n  void* m_handle;\n};\n\nstatic const VdsoInitializer vdso_initializer;\n}\n\n#endif\n}\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/experimental/exception_tracer/ExceptionTracerLib.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/experimental/exception_tracer/ExceptionTracerLib.h>\n\n#include <dlfcn.h>\n\n#include <vector>\n\n#include <folly/Indestructible.h>\n#include <folly/Portability.h>\n#include <folly/SharedMutex.h>\n#include <folly/Synchronized.h>\n\nnamespace __cxxabiv1 {\n\nextern \"C\" {\nvoid __cxa_throw(\n    void* thrownException,\n    std::type_info* type,\n    void (*destructor)(void*)) __attribute__((__noreturn__));\nvoid* __cxa_begin_catch(void* excObj) throw();\nvoid __cxa_rethrow(void) __attribute__((__noreturn__));\nvoid __cxa_end_catch(void);\n}\n\n} // namespace __cxxabiv1\n\nusing namespace folly::exception_tracer;\n\nnamespace {\n\ntemplate <typename Function>\nclass CallbackHolder {\n public:\n  void registerCallback(Function f) {\n    SYNCHRONIZED(callbacks_) { callbacks_.push_back(std::move(f)); }\n  }\n\n  // always inline to enforce kInternalFramesNumber\n  template <typename... Args>\n  FOLLY_ALWAYS_INLINE void invoke(Args... args) {\n    SYNCHRONIZED_CONST(callbacks_) {\n      for (auto& cb : callbacks_) {\n        cb(args...);\n      }\n    }\n  }\n\n private:\n  folly::Synchronized<std::vector<Function>> callbacks_;\n};\n\n} // namespace\n\nnamespace folly {\nnamespace exception_tracer {\n\n#define DECLARE_CALLBACK(NAME)                                   \\\n  CallbackHolder<NAME##Type>& get##NAME##Callbacks() {           \\\n    static Indestructible<CallbackHolder<NAME##Type>> Callbacks; \\\n    return *Callbacks;                                           \\\n  }                                                              \\\n  void register##NAME##Callback(NAME##Type callback) {           \\\n    get##NAME##Callbacks().registerCallback(callback);           \\\n  }\n\nDECLARE_CALLBACK(CxaThrow);\nDECLARE_CALLBACK(CxaBeginCatch);\nDECLARE_CALLBACK(CxaRethrow);\nDECLARE_CALLBACK(CxaEndCatch);\nDECLARE_CALLBACK(RethrowException);\n\n} // exception_tracer\n} // folly\n\n// Clang is smart enough to understand that the symbols we're loading\n// are [[noreturn]], but GCC is not. In order to be able to build with\n// -Wunreachable-code enable for Clang, these __builtin_unreachable()\n// calls need to go away. Everything else is messy though, so just\n// #define it to an empty macro under Clang and be done with it.\n#ifdef __clang__\n# define __builtin_unreachable()\n#endif\n\nnamespace __cxxabiv1 {\n\nvoid __cxa_throw(void* thrownException,\n                 std::type_info* type,\n                 void (*destructor)(void*)) {\n  static auto orig_cxa_throw =\n      reinterpret_cast<decltype(&__cxa_throw)>(dlsym(RTLD_NEXT, \"__cxa_throw\"));\n  getCxaThrowCallbacks().invoke(thrownException, type, destructor);\n  orig_cxa_throw(thrownException, type, destructor);\n  __builtin_unreachable(); // orig_cxa_throw never returns\n}\n\nvoid __cxa_rethrow() {\n  // __cxa_rethrow leaves the current exception on the caught stack,\n  // and __cxa_begin_catch recognizes that case.  We could do the same, but\n  // we'll implement something simpler (and slower): we pop the exception from\n  // the caught stack, and push it back onto the active stack; this way, our\n  // implementation of __cxa_begin_catch doesn't have to do anything special.\n  static auto orig_cxa_rethrow = reinterpret_cast<decltype(&__cxa_rethrow)>(\n      dlsym(RTLD_NEXT, \"__cxa_rethrow\"));\n  getCxaRethrowCallbacks().invoke();\n  orig_cxa_rethrow();\n  __builtin_unreachable(); // orig_cxa_rethrow never returns\n}\n\nvoid* __cxa_begin_catch(void* excObj) throw() {\n  // excObj is a pointer to the unwindHeader in __cxa_exception\n  static auto orig_cxa_begin_catch =\n      reinterpret_cast<decltype(&__cxa_begin_catch)>(\n          dlsym(RTLD_NEXT, \"__cxa_begin_catch\"));\n  getCxaBeginCatchCallbacks().invoke(excObj);\n  return orig_cxa_begin_catch(excObj);\n}\n\nvoid __cxa_end_catch() {\n  static auto orig_cxa_end_catch = reinterpret_cast<decltype(&__cxa_end_catch)>(\n      dlsym(RTLD_NEXT, \"__cxa_end_catch\"));\n  getCxaEndCatchCallbacks().invoke();\n  orig_cxa_end_catch();\n}\n\n} // namespace __cxxabiv1\n\nnamespace std {\n\nvoid rethrow_exception(std::exception_ptr ep) {\n  // Mangled name for std::rethrow_exception\n  // TODO(tudorb): Dicey, as it relies on the fact that std::exception_ptr\n  // is typedef'ed to a type in namespace __exception_ptr\n  static auto orig_rethrow_exception =\n      reinterpret_cast<decltype(&rethrow_exception)>(\n          dlsym(RTLD_NEXT,\n                \"_ZSt17rethrow_exceptionNSt15__exception_ptr13exception_ptrE\"));\n  getRethrowExceptionCallbacks().invoke(ep);\n  orig_rethrow_exception(ep);\n  __builtin_unreachable(); // orig_rethrow_exception never returns\n}\n\n} // namespace std\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/experimental/exception_tracer/ExceptionTracer.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/experimental/exception_tracer/ExceptionTracer.h>\n\n#include <dlfcn.h>\n#include <exception>\n#include <iostream>\n#include <glog/logging.h>\n\n#include <folly/experimental/exception_tracer/ExceptionAbi.h>\n#include <folly/experimental/exception_tracer/StackTrace.h>\n#include <folly/experimental/symbolizer/Symbolizer.h>\n#include <folly/String.h>\n\nnamespace {\n\nusing namespace ::folly::exception_tracer;\nusing namespace ::folly::symbolizer;\nusing namespace __cxxabiv1;\n\nextern \"C\" {\nStackTraceStack* getExceptionStackTraceStack(void) __attribute__((__weak__));\ntypedef StackTraceStack* (*GetExceptionStackTraceStackType)(void);\nGetExceptionStackTraceStackType getExceptionStackTraceStackFn;\n}\n\n}  // namespace\n\nnamespace folly {\nnamespace exception_tracer {\n\nstd::ostream& operator<<(std::ostream& out, const ExceptionInfo& info) {\n  printExceptionInfo(out, info, SymbolizePrinter::COLOR_IF_TTY);\n  return out;\n}\n\nvoid printExceptionInfo(\n    std::ostream& out,\n    const ExceptionInfo& info,\n    int options) {\n  out << \"Exception type: \";\n  if (info.type) {\n    out << folly::demangle(*info.type);\n  } else {\n    out << \"(unknown type)\";\n  }\n  out << \" (\" << info.frames.size()\n      << (info.frames.size() == 1 ? \" frame\" : \" frames\")\n      << \")\\n\";\n  try {\n    size_t frameCount = info.frames.size();\n\n    // Skip our own internal frames\n    static constexpr size_t kInternalFramesNumber = 3;\n    if (frameCount > kInternalFramesNumber) {\n      auto addresses = info.frames.data() + kInternalFramesNumber;\n      frameCount -= kInternalFramesNumber;\n\n      std::vector<SymbolizedFrame> frames;\n      frames.resize(frameCount);\n\n      Symbolizer symbolizer(\n          (options & SymbolizePrinter::NO_FILE_AND_LINE)\n              ? Dwarf::LocationInfoMode::DISABLED\n              : Symbolizer::kDefaultLocationInfoMode);\n      symbolizer.symbolize(addresses, frames.data(), frameCount);\n\n      OStreamSymbolizePrinter osp(out, options);\n      osp.println(addresses, frames.data(), frameCount);\n    }\n  } catch (const std::exception& e) {\n    out << \"\\n !! caught \" << folly::exceptionStr(e) << \"\\n\";\n  } catch (...) {\n    out << \"\\n !!! caught unexpected exception\\n\";\n  }\n}\n\nnamespace {\n\n/**\n * Is this a standard C++ ABI exception?\n *\n * Dependent exceptions (thrown via std::rethrow_exception) aren't --\n * exc doesn't actually point to a __cxa_exception structure, but\n * the offset of unwindHeader is correct, so exc->unwindHeader actually\n * returns a _Unwind_Exception object.  Yeah, it's ugly like that.\n */\nbool isAbiCppException(const __cxa_exception* exc) {\n  // The least significant four bytes must be \"C++\\0\"\n  static const uint64_t cppClass =\n    ((uint64_t)'C' << 24) |\n    ((uint64_t)'+' << 16) |\n    ((uint64_t)'+' << 8);\n  return (exc->unwindHeader.exception_class & 0xffffffff) == cppClass;\n}\n\n}  // namespace\n\nstd::vector<ExceptionInfo> getCurrentExceptions() {\n  struct Once {\n    Once() {\n      // See if linked in with us (getExceptionStackTraceStack is weak)\n      getExceptionStackTraceStackFn = getExceptionStackTraceStack;\n\n      if (!getExceptionStackTraceStackFn) {\n        // Nope, see if it's in a shared library\n        getExceptionStackTraceStackFn =\n          (GetExceptionStackTraceStackType)dlsym(\n              RTLD_NEXT, \"getExceptionStackTraceStack\");\n      }\n    }\n  };\n  static Once once;\n\n  std::vector<ExceptionInfo> exceptions;\n  auto currentException = __cxa_get_globals()->caughtExceptions;\n  if (!currentException) {\n    return exceptions;\n  }\n\n  StackTraceStack* traceStack = nullptr;\n  if (!getExceptionStackTraceStackFn) {\n    static bool logged = false;\n    if (!logged) {\n      LOG(WARNING)\n        << \"Exception tracer library not linked, stack traces not available\";\n      logged = true;\n    }\n  } else if ((traceStack = getExceptionStackTraceStackFn()) == nullptr) {\n    static bool logged = false;\n    if (!logged) {\n      LOG(WARNING)\n        << \"Exception stack trace invalid, stack traces not available\";\n      logged = true;\n    }\n  }\n\n  StackTrace* trace = traceStack ? traceStack->top() : nullptr;\n  while (currentException) {\n    ExceptionInfo info;\n    // Dependent exceptions (thrown via std::rethrow_exception) aren't\n    // standard ABI __cxa_exception objects, and are correctly labeled as\n    // such in the exception_class field.  We could try to extract the\n    // primary exception type in horribly hacky ways, but, for now, NULL.\n    info.type =\n      isAbiCppException(currentException) ?\n      currentException->exceptionType :\n      nullptr;\n    if (traceStack) {\n      CHECK(trace) << \"Invalid trace stack!\";\n      info.frames.assign(trace->addresses,\n                         trace->addresses + trace->frameCount);\n      trace = traceStack->next(trace);\n    }\n    currentException = currentException->nextException;\n    exceptions.push_back(std::move(info));\n  }\n  CHECK(!trace) << \"Invalid trace stack!\";\n\n  return exceptions;\n}\n\nnamespace {\n\nstd::terminate_handler origTerminate = abort;\nstd::unexpected_handler origUnexpected = abort;\n\nvoid dumpExceptionStack(const char* prefix) {\n  auto exceptions = getCurrentExceptions();\n  if (exceptions.empty()) {\n    return;\n  }\n  LOG(ERROR) << prefix << \", exception stack follows\";\n  for (auto& exc : exceptions) {\n    LOG(ERROR) << exc << \"\\n\";\n  }\n  LOG(ERROR) << \"exception stack complete\";\n}\n\nvoid terminateHandler() {\n  dumpExceptionStack(\"terminate() called\");\n  origTerminate();\n}\n\nvoid unexpectedHandler() {\n  dumpExceptionStack(\"Unexpected exception\");\n  origUnexpected();\n}\n\n}  // namespace\n\nvoid installHandlers() {\n  struct Once {\n    Once() {\n      origTerminate = std::set_terminate(terminateHandler);\n      origUnexpected = std::set_unexpected(unexpectedHandler);\n    }\n  };\n  static Once once;\n}\n\n}  // namespace exception_tracer\n}  // namespace folly\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/experimental/exception_tracer/ExceptionStackTraceLib.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <exception>\n\n#include <folly/experimental/exception_tracer/ExceptionAbi.h>\n#include <folly/experimental/exception_tracer/StackTrace.h>\n#include <folly/experimental/exception_tracer/ExceptionTracer.h>\n#include <folly/experimental/exception_tracer/ExceptionTracerLib.h>\n#include <folly/experimental/symbolizer/Symbolizer.h>\n\nusing namespace folly::exception_tracer;\n\nnamespace {\n\n// If we somehow ended up in an invalid state, we don't want to print any stack\n// trace at all because in could be bogus\nFOLLY_TLS bool invalid;\n\nFOLLY_TLS StackTraceStack activeExceptions;\nFOLLY_TLS StackTraceStack caughtExceptions;\n\n} // namespace\n\n// This function is exported and may be found via dlsym(RTLD_NEXT, ...)\nextern \"C\" StackTraceStack* getExceptionStackTraceStack() {\n  return invalid ? nullptr : &caughtExceptions;\n}\n\nnamespace {\n\nvoid addActiveException() {\n  // Capture stack trace\n  if (!invalid) {\n    if (!activeExceptions.pushCurrent()) {\n      activeExceptions.clear();\n      caughtExceptions.clear();\n      invalid = true;\n    }\n  }\n}\n\nvoid moveTopException(StackTraceStack& from, StackTraceStack& to) {\n  if (invalid) {\n    return;\n  }\n  if (!to.moveTopFrom(from)) {\n    from.clear();\n    to.clear();\n    invalid = true;\n  }\n}\n\nstruct Initializer {\n  Initializer() {\n    registerCxaThrowCallback(\n        [](void*, std::type_info*, void (*)(void*)) { addActiveException(); });\n\n    registerCxaBeginCatchCallback(\n        [](void*) { moveTopException(activeExceptions, caughtExceptions); });\n\n    registerCxaRethrowCallback(\n        []() { moveTopException(caughtExceptions, activeExceptions); });\n\n    registerCxaEndCatchCallback([]() {\n      if (invalid) {\n        return;\n      }\n\n      __cxxabiv1::__cxa_exception* top =\n          __cxxabiv1::__cxa_get_globals_fast()->caughtExceptions;\n      // This is gcc specific and not specified in the ABI:\n      // abs(handlerCount) is the number of active handlers, it's negative\n      // for rethrown exceptions and positive (always 1) for regular\n      // exceptions.\n      // In the rethrow case, we've already popped the exception off the\n      // caught stack, so we don't do anything here.\n      if (top->handlerCount == 1) {\n        if (!caughtExceptions.pop()) {\n          activeExceptions.clear();\n          invalid = true;\n        }\n      }\n    });\n\n    registerRethrowExceptionCallback(\n        [](std::exception_ptr) { addActiveException(); });\n\n    try {\n      ::folly::exception_tracer::installHandlers();\n    } catch (...) {\n    }\n  }\n};\n\nInitializer initializer;\n\n} // namespace\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/experimental/test/FunctionSchedulerTest.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#include <algorithm>\n#include <atomic>\n#include <cassert>\n#include <random>\n\n#include <folly/Random.h>\n#include <folly/experimental/FunctionScheduler.h>\n#include <folly/portability/GTest.h>\n\n#if defined(__linux__)\n#include <dlfcn.h>\n#endif\n\nusing namespace folly;\nusing std::chrono::milliseconds;\n\nnamespace {\n\n/*\n * Helper functions for controlling how long this test takes.\n *\n * Using larger intervals here will make the tests less flaky when run on\n * heavily loaded systems.  However, this will also make the tests take longer\n * to run.\n */\nstatic const auto timeFactor = std::chrono::milliseconds(100);\nstd::chrono::milliseconds testInterval(int n) { return n * timeFactor; }\nint getTicksWithinRange(int n, int min, int max) {\n  assert(min <= max);\n  n = std::max(min, n);\n  n = std::min(max, n);\n  return n;\n}\nvoid delay(int n) {\n  std::chrono::microseconds usec(n * timeFactor);\n  usleep(usec.count());\n}\n\n} // unnamed namespace\n\nTEST(FunctionScheduler, StartAndShutdown) {\n  FunctionScheduler fs;\n  EXPECT_TRUE(fs.start());\n  EXPECT_FALSE(fs.start());\n  EXPECT_TRUE(fs.shutdown());\n  EXPECT_FALSE(fs.shutdown());\n  // start again\n  EXPECT_TRUE(fs.start());\n  EXPECT_FALSE(fs.start());\n  EXPECT_TRUE(fs.shutdown());\n  EXPECT_FALSE(fs.shutdown());\n}\n\nTEST(FunctionScheduler, SimpleAdd) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  fs.start();\n  delay(1);\n  EXPECT_EQ(2, total);\n  fs.shutdown();\n  delay(2);\n  EXPECT_EQ(2, total);\n}\n\nTEST(FunctionScheduler, AddCancel) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  fs.start();\n  delay(1);\n  EXPECT_EQ(2, total);\n  delay(2);\n  EXPECT_EQ(4, total);\n  EXPECT_TRUE(fs.cancelFunction(\"add2\"));\n  EXPECT_FALSE(fs.cancelFunction(\"NO SUCH FUNC\"));\n  delay(2);\n  EXPECT_EQ(4, total);\n  fs.addFunction([&] { total += 1; }, testInterval(2), \"add2\");\n  delay(1);\n  EXPECT_EQ(5, total);\n  delay(2);\n  EXPECT_EQ(6, total);\n  fs.shutdown();\n}\n\nTEST(FunctionScheduler, AddCancel2) {\n  int total = 0;\n  FunctionScheduler fs;\n\n  // Test adds and cancels while the scheduler is stopped\n  EXPECT_FALSE(fs.cancelFunction(\"add2\"));\n  fs.addFunction([&] { total += 1; }, testInterval(2), \"add2\");\n  EXPECT_TRUE(fs.cancelFunction(\"add2\"));\n  EXPECT_FALSE(fs.cancelFunction(\"add2\"));\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  fs.addFunction([&] { total += 3; }, testInterval(3), \"add3\");\n\n  EXPECT_EQ(0, total);\n  fs.start();\n  delay(1);\n  EXPECT_EQ(5, total);\n\n  // Cancel add2 while the scheduler is running\n  EXPECT_TRUE(fs.cancelFunction(\"add2\"));\n  EXPECT_FALSE(fs.cancelFunction(\"add2\"));\n  EXPECT_FALSE(fs.cancelFunction(\"bogus\"));\n\n  delay(3);\n  EXPECT_EQ(8, total);\n  EXPECT_TRUE(fs.cancelFunction(\"add3\"));\n\n  // Test a function that cancels itself\n  int selfCancelCount = 0;\n  fs.addFunction(\n      [&] {\n        ++selfCancelCount;\n        if (selfCancelCount > 2) {\n          fs.cancelFunction(\"selfCancel\");\n        }\n      },\n      testInterval(1), \"selfCancel\", testInterval(1));\n  delay(4);\n  EXPECT_EQ(3, selfCancelCount);\n  EXPECT_FALSE(fs.cancelFunction(\"selfCancel\"));\n\n  // Test a function that schedules another function\n  int adderCount = 0;\n  int fn2Count = 0;\n  auto fn2 = [&] { ++fn2Count; };\n  auto fnAdder = [&] {\n    ++adderCount;\n    if (adderCount == 2) {\n      fs.addFunction(fn2, testInterval(3), \"fn2\", testInterval(2));\n    }\n  };\n  fs.addFunction(fnAdder, testInterval(4), \"adder\");\n  // t0: adder fires\n  delay(1); // t1\n  EXPECT_EQ(1, adderCount);\n  EXPECT_EQ(0, fn2Count);\n  // t4: adder fires, schedules fn2\n  delay(4); // t5\n  EXPECT_EQ(2, adderCount);\n  EXPECT_EQ(0, fn2Count);\n  // t6: fn2 fires\n  delay(2); // t7\n  EXPECT_EQ(2, adderCount);\n  EXPECT_EQ(1, fn2Count);\n  // t8: adder fires\n  // t9: fn2 fires\n  delay(3); // t10\n  EXPECT_EQ(3, adderCount);\n  EXPECT_EQ(2, fn2Count);\n  EXPECT_TRUE(fs.cancelFunction(\"fn2\"));\n  EXPECT_TRUE(fs.cancelFunction(\"adder\"));\n  delay(5); // t10\n  EXPECT_EQ(3, adderCount);\n  EXPECT_EQ(2, fn2Count);\n\n  EXPECT_EQ(8, total);\n  EXPECT_EQ(3, selfCancelCount);\n}\n\nTEST(FunctionScheduler, AddMultiple) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  fs.addFunction([&] { total += 3; }, testInterval(3), \"add3\");\n  EXPECT_THROW(fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\"),\n               std::invalid_argument); // function name already exists\n\n  fs.start();\n  delay(1);\n  EXPECT_EQ(5, total);\n  delay(4);\n  EXPECT_EQ(12, total);\n  EXPECT_TRUE(fs.cancelFunction(\"add2\"));\n  delay(2);\n  EXPECT_EQ(15, total);\n  fs.shutdown();\n  delay(3);\n  EXPECT_EQ(15, total);\n  fs.shutdown();\n}\n\nTEST(FunctionScheduler, AddAfterStart) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  fs.addFunction([&] { total += 3; }, testInterval(2), \"add3\");\n  fs.start();\n  delay(3);\n  EXPECT_EQ(10, total);\n  fs.addFunction([&] { total += 2; }, testInterval(3), \"add22\");\n  delay(2);\n  EXPECT_EQ(17, total);\n}\n\nTEST(FunctionScheduler, ShutdownStart) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  fs.start();\n  delay(1);\n  fs.shutdown();\n  fs.start();\n  delay(1);\n  EXPECT_EQ(4, total);\n  EXPECT_FALSE(fs.cancelFunction(\"add3\")); // non existing\n  delay(2);\n  EXPECT_EQ(6, total);\n}\n\nTEST(FunctionScheduler, ResetFunc) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(3), \"add2\");\n  fs.addFunction([&] { total += 3; }, testInterval(3), \"add3\");\n  fs.start();\n  delay(1);\n  EXPECT_EQ(5, total);\n  EXPECT_FALSE(fs.resetFunctionTimer(\"NON_EXISTING\"));\n  EXPECT_TRUE(fs.resetFunctionTimer(\"add2\"));\n  delay(1);\n  // t2: after the reset, add2 should have been invoked immediately\n  EXPECT_EQ(7, total);\n  usleep(150000);\n  // t3.5: add3 should have been invoked. add2 should not\n  EXPECT_EQ(10, total);\n  delay(1);\n  // t4.5: add2 should have been invoked once more (it was reset at t1)\n  EXPECT_EQ(12, total);\n}\n\nTEST(FunctionScheduler, AddInvalid) {\n  int total = 0;\n  FunctionScheduler fs;\n  // interval may not be negative\n  EXPECT_THROW(fs.addFunction([&] { total += 2; }, testInterval(-1), \"add2\"),\n               std::invalid_argument);\n\n  EXPECT_FALSE(fs.cancelFunction(\"addNoFunc\"));\n}\n\nTEST(FunctionScheduler, NoFunctions) {\n  FunctionScheduler fs;\n  EXPECT_TRUE(fs.start());\n  fs.shutdown();\n  FunctionScheduler fs2;\n  fs2.shutdown();\n}\n\nTEST(FunctionScheduler, AddWhileRunning) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.start();\n  delay(1);\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\");\n  // The function should be invoked nearly immediately when we add it\n  // and the FunctionScheduler is already running\n  usleep(50000);\n  EXPECT_EQ(2, total);\n  delay(2);\n  EXPECT_EQ(4, total);\n}\n\nTEST(FunctionScheduler, NoShutdown) {\n  int total = 0;\n  {\n    FunctionScheduler fs;\n    fs.addFunction([&] { total += 2; }, testInterval(1), \"add2\");\n    fs.start();\n    usleep(50000);\n    EXPECT_EQ(2, total);\n  }\n  // Destroyed the FunctionScheduler without calling shutdown.\n  // Everything should have been cleaned up, and the function will no longer\n  // get called.\n  delay(2);\n  EXPECT_EQ(2, total);\n}\n\nTEST(FunctionScheduler, StartDelay) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction([&] { total += 2; }, testInterval(2), \"add2\",\n                 testInterval(2));\n  fs.addFunction([&] { total += 3; }, testInterval(3), \"add3\",\n                 testInterval(2));\n  EXPECT_THROW(fs.addFunction([&] { total += 2; }, testInterval(3),\n                              \"addX\", testInterval(-1)),\n               std::invalid_argument);\n  fs.start();\n  delay(1); // t1\n  EXPECT_EQ(0, total);\n  // t2 : add2 total=2\n  // t2 : add3 total=5\n  delay(2); // t3\n  EXPECT_EQ(5, total);\n  // t4 : add2: total=7\n  // t5 : add3: total=10\n  // t6 : add2: total=12\n  delay(4); // t7\n  EXPECT_EQ(12, total);\n  fs.cancelFunction(\"add2\");\n  // t8 : add3: total=15\n  delay(2); // t9\n  EXPECT_EQ(15, total);\n  fs.shutdown();\n  delay(3);\n  EXPECT_EQ(15, total);\n  fs.shutdown();\n}\n\nTEST(FunctionScheduler, NoSteadyCatchup) {\n  std::atomic<int> ticks(0);\n  FunctionScheduler fs;\n  // fs.setSteady(false); is the default\n  fs.addFunction([&ticks] {\n                   if (++ticks == 2) {\n                     std::this_thread::sleep_for(\n                         std::chrono::milliseconds(200));\n                   }\n                 },\n                 milliseconds(5));\n  fs.start();\n  std::this_thread::sleep_for(std::chrono::milliseconds(500));\n\n  // no steady catch up means we'd tick once for 200ms, then remaining\n  // 300ms / 5 = 60 times\n  EXPECT_LE(ticks.load(), 61);\n}\n\nTEST(FunctionScheduler, SteadyCatchup) {\n  std::atomic<int> ticks(0);\n  FunctionScheduler fs;\n  fs.setSteady(true);\n  fs.addFunction([&ticks] {\n                   if (++ticks == 2) {\n                     std::this_thread::sleep_for(\n                         std::chrono::milliseconds(200));\n                   }\n                 },\n                 milliseconds(5));\n  fs.start();\n\n  std::this_thread::sleep_for(std::chrono::milliseconds(500));\n\n  // tick every 5ms. Despite tick == 2 is slow, later ticks should be fast\n  // enough to catch back up to schedule\n  EXPECT_NEAR(100, ticks.load(), 10);\n}\n\nTEST(FunctionScheduler, UniformDistribution) {\n  int total = 0;\n  const int kTicks = 2;\n  std::chrono::milliseconds minInterval =\n      testInterval(kTicks) - (timeFactor / 5);\n  std::chrono::milliseconds maxInterval =\n      testInterval(kTicks) + (timeFactor / 5);\n  FunctionScheduler fs;\n  fs.addFunctionUniformDistribution([&] { total += 2; },\n                                    minInterval,\n                                    maxInterval,\n                                    \"UniformDistribution\",\n                                    std::chrono::milliseconds(0));\n  fs.start();\n  delay(1);\n  EXPECT_EQ(2, total);\n  delay(kTicks);\n  EXPECT_EQ(4, total);\n  delay(kTicks);\n  EXPECT_EQ(6, total);\n  fs.shutdown();\n  delay(2);\n  EXPECT_EQ(6, total);\n}\n\nTEST(FunctionScheduler, ExponentialBackoff) {\n  int total = 0;\n  int expectedInterval = 0;\n  int nextInterval = 2;\n  FunctionScheduler fs;\n  fs.addFunctionGenericDistribution(\n      [&] { total += 2; },\n      [&expectedInterval, nextInterval]() mutable {\n        expectedInterval = nextInterval;\n        nextInterval *= nextInterval;\n        return testInterval(expectedInterval);\n      },\n      \"ExponentialBackoff\",\n      \"2^n * 100ms\",\n      std::chrono::milliseconds(0));\n  fs.start();\n  delay(1);\n  EXPECT_EQ(2, total);\n  delay(expectedInterval);\n  EXPECT_EQ(4, total);\n  delay(expectedInterval);\n  EXPECT_EQ(6, total);\n  fs.shutdown();\n  delay(2);\n  EXPECT_EQ(6, total);\n}\n\nTEST(FunctionScheduler, GammaIntervalDistribution) {\n  int total = 0;\n  int expectedInterval = 0;\n  FunctionScheduler fs;\n  std::default_random_engine generator(folly::Random::rand32());\n  // The alpha and beta arguments are selected, somewhat randomly, to be 2.0.\n  // These values do not matter much in this test, as we are not testing the\n  // std::gamma_distribution itself...\n  std::gamma_distribution<double> gamma(2.0, 2.0);\n  fs.addFunctionGenericDistribution(\n      [&] { total += 2; },\n      [&expectedInterval, generator, gamma]() mutable {\n        expectedInterval =\n            getTicksWithinRange(static_cast<int>(gamma(generator)), 2, 10);\n        return testInterval(expectedInterval);\n      },\n      \"GammaDistribution\",\n      \"gamma(2.0,2.0)*100ms\",\n      std::chrono::milliseconds(0));\n  fs.start();\n  delay(1);\n  EXPECT_EQ(2, total);\n  delay(expectedInterval);\n  EXPECT_EQ(4, total);\n  delay(expectedInterval);\n  EXPECT_EQ(6, total);\n  fs.shutdown();\n  delay(2);\n  EXPECT_EQ(6, total);\n}\n\nTEST(FunctionScheduler, AddWithRunOnce) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunctionOnce([&] { total += 2; }, \"add2\");\n  fs.start();\n  delay(1);\n  EXPECT_EQ(2, total);\n  delay(2);\n  EXPECT_EQ(2, total);\n\n  fs.addFunctionOnce([&] { total += 2; }, \"add2\");\n  delay(1);\n  EXPECT_EQ(4, total);\n  delay(2);\n  EXPECT_EQ(4, total);\n\n  fs.shutdown();\n}\n\nTEST(FunctionScheduler, cancelFunctionAndWait) {\n  int total = 0;\n  FunctionScheduler fs;\n  fs.addFunction(\n      [&] {\n        delay(5);\n        total += 2;\n      },\n      testInterval(100),\n      \"add2\");\n\n  fs.start();\n  delay(1);\n  EXPECT_EQ(0, total); // add2 is still sleeping\n\n  EXPECT_TRUE(fs.cancelFunctionAndWait(\"add2\"));\n  EXPECT_EQ(2, total); // add2 should have completed\n\n  EXPECT_FALSE(fs.cancelFunction(\"add2\")); // add2 has been canceled\n  fs.shutdown();\n}\n\n#if defined(__linux__)\nnamespace {\n/**\n * A helper class that forces our pthread_create() wrapper to fail when\n * an PThreadCreateFailure object exists.\n */\nclass PThreadCreateFailure {\n public:\n  PThreadCreateFailure() {\n    ++forceFailure_;\n  }\n  ~PThreadCreateFailure() {\n    --forceFailure_;\n  }\n\n  static bool shouldFail() {\n    return forceFailure_ > 0;\n  }\n\n private:\n  static std::atomic<int> forceFailure_;\n};\n\nstd::atomic<int> PThreadCreateFailure::forceFailure_{0};\n} // unnamed namespce\n\n// Replace the system pthread_create() function with our own stub, so we can\n// trigger failures in the StartThrows() test.\nextern \"C\" int pthread_create(\n    pthread_t* thread,\n    const pthread_attr_t* attr,\n    void* (*start_routine)(void*),\n    void* arg) {\n  static const auto realFunction = reinterpret_cast<decltype(&pthread_create)>(\n      dlsym(RTLD_NEXT, \"pthread_create\"));\n  // For sanity, make sure we didn't find ourself,\n  // since that would cause infinite recursion.\n  CHECK_NE(realFunction, pthread_create);\n\n  if (PThreadCreateFailure::shouldFail()) {\n    errno = EINVAL;\n    return -1;\n  }\n  return realFunction(thread, attr, start_routine, arg);\n}\n\nTEST(FunctionScheduler, StartThrows) {\n  FunctionScheduler fs;\n  PThreadCreateFailure fail;\n  EXPECT_ANY_THROW(fs.start());\n  EXPECT_NO_THROW(fs.shutdown());\n}\n#endif\n\nTEST(FunctionScheduler, cancelAllFunctionsAndWait) {\n  int total = 0;\n  FunctionScheduler fs;\n\n  fs.addFunction(\n      [&] {\n        delay(5);\n        total += 2;\n      },\n      testInterval(100),\n      \"add2\");\n\n  fs.start();\n  delay(1);\n  EXPECT_EQ(0, total); // add2 is still sleeping\n\n  fs.cancelAllFunctionsAndWait();\n  EXPECT_EQ(2, total);\n\n  EXPECT_FALSE(fs.cancelFunction(\"add2\")); // add2 has been canceled\n  fs.shutdown();\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/fibers/GuardPageAllocator.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#include \"GuardPageAllocator.h\"\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#endif\n#include <signal.h>\n\n#include <iostream>\n#include <mutex>\n\n#include <folly/Singleton.h>\n#include <folly/SpinLock.h>\n#include <folly/Synchronized.h>\n#include <folly/portability/SysMman.h>\n#include <folly/portability/Unistd.h>\n\n#include <glog/logging.h>\n\nnamespace folly {\nnamespace fibers {\n\n/**\n * Each stack with a guard page creates two memory mappings.\n * Since this is a limited resource, we don't want to create too many of these.\n *\n * The upper bound on total number of mappings created\n * is kNumGuarded * kMaxInUse.\n */\n\n/**\n * Number of guarded stacks per allocator instance\n */\nconstexpr size_t kNumGuarded = 100;\n\n/**\n * Maximum number of allocator instances with guarded stacks enabled\n */\nconstexpr size_t kMaxInUse = 100;\n\n/**\n * A cache for kNumGuarded stacks of a given size\n *\n * Thread safe.\n */\nclass StackCache {\n public:\n  explicit StackCache(size_t stackSize) : allocSize_(allocSize(stackSize)) {\n    auto p = ::mmap(\n        nullptr,\n        allocSize_ * kNumGuarded,\n        PROT_READ | PROT_WRITE,\n        MAP_PRIVATE | MAP_ANONYMOUS,\n        -1,\n        0);\n    PCHECK(p != (void*)(-1));\n    storage_ = reinterpret_cast<unsigned char*>(p);\n\n    /* Protect the bottommost page of every stack allocation */\n    for (size_t i = 0; i < kNumGuarded; ++i) {\n      auto allocBegin = storage_ + allocSize_ * i;\n      freeList_.emplace_back(allocBegin, /* protected= */ false);\n    }\n  }\n\n  unsigned char* borrow(size_t size) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n\n    assert(storage_);\n\n    auto as = allocSize(size);\n    if (as != allocSize_ || freeList_.empty()) {\n      return nullptr;\n    }\n\n    auto p = freeList_.back().first;\n    if (!freeList_.back().second) {\n      PCHECK(0 == ::mprotect(p, pagesize(), PROT_NONE));\n      SYNCHRONIZED(pages, protectedPages()) {\n        pages.insert(reinterpret_cast<intptr_t>(p));\n      }\n    }\n    freeList_.pop_back();\n\n    /* We allocate minimum number of pages required, plus a guard page.\n       Since we use this for stack storage, requested allocation is aligned\n       at the top of the allocated pages, while the guard page is at the bottom.\n\n               -- increasing addresses -->\n             Guard page     Normal pages\n            |xxxxxxxxxx|..........|..........|\n            <- allocSize_ ------------------->\n         p -^                <- size -------->\n                      limit -^\n    */\n    auto limit = p + allocSize_ - size;\n    assert(limit >= p + pagesize());\n    return limit;\n  }\n\n  bool giveBack(unsigned char* limit, size_t size) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n\n    assert(storage_);\n\n    auto as = allocSize(size);\n    auto p = limit + size - as;\n    if (p < storage_ || p >= storage_ + allocSize_ * kNumGuarded) {\n      /* not mine */\n      return false;\n    }\n\n    assert(as == allocSize_);\n    assert((p - storage_) % allocSize_ == 0);\n    freeList_.emplace_back(p, /* protected= */ true);\n    return true;\n  }\n\n  ~StackCache() {\n    assert(storage_);\n    SYNCHRONIZED(pages, protectedPages()) {\n      for (const auto& item : freeList_) {\n        pages.erase(reinterpret_cast<intptr_t>(item.first));\n      }\n    }\n    PCHECK(0 == ::munmap(storage_, allocSize_ * kNumGuarded));\n  }\n\n  static bool isProtected(intptr_t addr) {\n    // Use a read lock for reading.\n    SYNCHRONIZED_CONST(pages, protectedPages()) {\n      for (const auto& page : pages) {\n        intptr_t pageEnd = intptr_t(page + pagesize());\n        if (page <= addr && addr < pageEnd) {\n          return true;\n        }\n      }\n    }\n    return false;\n  }\n\n private:\n  folly::SpinLock lock_;\n  unsigned char* storage_{nullptr};\n  size_t allocSize_{0};\n\n  /**\n   * LIFO free list. Each pair contains stack pointer and protected flag.\n   */\n  std::vector<std::pair<unsigned char*, bool>> freeList_;\n\n  static size_t pagesize() {\n    static const size_t pagesize = size_t(sysconf(_SC_PAGESIZE));\n    return pagesize;\n  }\n\n  /* Returns a multiple of pagesize() enough to store size + one guard page */\n  static size_t allocSize(size_t size) {\n    return pagesize() * ((size + pagesize() - 1) / pagesize() + 1);\n  }\n\n  static folly::Synchronized<std::unordered_set<intptr_t>>& protectedPages() {\n    static auto instance =\n        new folly::Synchronized<std::unordered_set<intptr_t>>();\n    return *instance;\n  }\n};\n\n#ifndef _WIN32\n\nnamespace {\n\nstruct sigaction oldSigsegvAction;\n\nvoid sigsegvSignalHandler(int signum, siginfo_t* info, void*) {\n  if (signum != SIGSEGV) {\n    std::cerr << \"GuardPageAllocator signal handler called for signal: \"\n              << signum;\n    return;\n  }\n\n  if (info &&\n      StackCache::isProtected(reinterpret_cast<intptr_t>(info->si_addr))) {\n    std::cerr << \"folly::fibers Fiber stack overflow detected.\" << std::endl;\n  }\n\n  // Restore old signal handler and let it handle the signal.\n  sigaction(signum, &oldSigsegvAction, nullptr);\n  raise(signum);\n}\n\nbool isInJVM() {\n  auto getCreated = dlsym(RTLD_DEFAULT, \"JNI_GetCreatedJavaVMs\");\n  return getCreated;\n}\n\nvoid installSignalHandler() {\n  static std::once_flag onceFlag;\n  std::call_once(onceFlag, []() {\n    if (isInJVM()) {\n      // Don't install signal handler, since JVM internal signal handler doesn't\n      // work with SA_ONSTACK\n      return;\n    }\n\n    struct sigaction sa;\n    memset(&sa, 0, sizeof(sa));\n    sigemptyset(&sa.sa_mask);\n    // By default signal handlers are run on the signaled thread's stack.\n    // In case of stack overflow running the SIGSEGV signal handler on\n    // the same stack leads to another SIGSEGV and crashes the program.\n    // Use SA_ONSTACK, so alternate stack is used (only if configured via\n    // sigaltstack).\n    sa.sa_flags |= SA_SIGINFO | SA_ONSTACK;\n    sa.sa_sigaction = &sigsegvSignalHandler;\n    sigaction(SIGSEGV, &sa, &oldSigsegvAction);\n  });\n}\n}\n\n#endif\n\nclass CacheManager {\n public:\n  static CacheManager& instance() {\n    static auto inst = new CacheManager();\n    return *inst;\n  }\n\n  std::unique_ptr<StackCacheEntry> getStackCache(size_t stackSize) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n    if (inUse_ < kMaxInUse) {\n      ++inUse_;\n      return std::make_unique<StackCacheEntry>(stackSize);\n    }\n\n    return nullptr;\n  }\n\n private:\n  folly::SpinLock lock_;\n  size_t inUse_{0};\n\n  friend class StackCacheEntry;\n\n  void giveBack(std::unique_ptr<StackCache> /* stackCache_ */) {\n    assert(inUse_ > 0);\n    --inUse_;\n    /* Note: we can add a free list for each size bucket\n       if stack re-use is important.\n       In this case this needs to be a folly::Singleton\n       to make sure the free list is cleaned up on fork.\n\n       TODO(t7351705): fix Singleton destruction order\n    */\n  }\n};\n\n/*\n * RAII Wrapper around a StackCache that calls\n * CacheManager::giveBack() on destruction.\n */\nclass StackCacheEntry {\n public:\n  explicit StackCacheEntry(size_t stackSize)\n      : stackCache_(std::make_unique<StackCache>(stackSize)) {}\n\n  StackCache& cache() const noexcept {\n    return *stackCache_;\n  }\n\n  ~StackCacheEntry() {\n    CacheManager::instance().giveBack(std::move(stackCache_));\n  }\n\n private:\n  std::unique_ptr<StackCache> stackCache_;\n};\n\nGuardPageAllocator::GuardPageAllocator(bool useGuardPages)\n    : useGuardPages_(useGuardPages) {\n#ifndef _WIN32\n  installSignalHandler();\n#endif\n}\n\nGuardPageAllocator::~GuardPageAllocator() = default;\n\nunsigned char* GuardPageAllocator::allocate(size_t size) {\n  if (useGuardPages_ && !stackCache_) {\n    stackCache_ = CacheManager::instance().getStackCache(size);\n  }\n\n  if (stackCache_) {\n    auto p = stackCache_->cache().borrow(size);\n    if (p != nullptr) {\n      return p;\n    }\n  }\n  return fallbackAllocator_.allocate(size);\n}\n\nvoid GuardPageAllocator::deallocate(unsigned char* limit, size_t size) {\n  if (!(stackCache_ && stackCache_->cache().giveBack(limit, size))) {\n    fallbackAllocator_.deallocate(limit, size);\n  }\n}\n}\n} // folly::fibers\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/fibers/FiberManager.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#include \"FiberManagerInternal.h\"\n\n#include <signal.h>\n\n#include <cassert>\n#include <stdexcept>\n\n#include <glog/logging.h>\n\n#include <folly/fibers/Fiber.h>\n#include <folly/fibers/LoopController.h>\n\n#include <folly/SingletonThreadLocal.h>\n#include <folly/portability/SysSyscall.h>\n#include <folly/portability/Unistd.h>\n\n#ifdef FOLLY_SANITIZE_ADDRESS\n\n#include <dlfcn.h>\n\nstatic void __sanitizer_start_switch_fiber_weak(\n    void** fake_stack_save,\n    void const* fiber_stack_base,\n    size_t fiber_stack_extent)\n    __attribute__((__weakref__(\"__sanitizer_start_switch_fiber\")));\nstatic void __sanitizer_finish_switch_fiber_weak(\n    void* fake_stack_save,\n    void const** old_stack_base,\n    size_t* old_stack_extent)\n    __attribute__((__weakref__(\"__sanitizer_finish_switch_fiber\")));\nstatic void __asan_unpoison_memory_region_weak(\n    void const /* nolint */ volatile* addr,\n    size_t size) __attribute__((__weakref__(\"__asan_unpoison_memory_region\")));\n\ntypedef void (*AsanStartSwitchStackFuncPtr)(void**, void const*, size_t);\ntypedef void (*AsanFinishSwitchStackFuncPtr)(void*, void const**, size_t*);\ntypedef void (*AsanUnpoisonMemoryRegionFuncPtr)(\n    void const /* nolint */ volatile*,\n    size_t);\n\nnamespace folly {\nnamespace fibers {\n\nstatic AsanStartSwitchStackFuncPtr getStartSwitchStackFunc();\nstatic AsanFinishSwitchStackFuncPtr getFinishSwitchStackFunc();\nstatic AsanUnpoisonMemoryRegionFuncPtr getUnpoisonMemoryRegionFunc();\n}\n}\n\n#endif\n\nnamespace folly {\nnamespace fibers {\n\nFOLLY_TLS FiberManager* FiberManager::currentFiberManager_ = nullptr;\n\nFiberManager::FiberManager(\n    std::unique_ptr<LoopController> loopController,\n    Options options)\n    : FiberManager(\n          LocalType<void>(),\n          std::move(loopController),\n          std::move(options)) {}\n\nFiberManager::~FiberManager() {\n  if (isLoopScheduled_) {\n    loopController_->cancel();\n  }\n\n  while (!fibersPool_.empty()) {\n    fibersPool_.pop_front_and_dispose([](Fiber* fiber) { delete fiber; });\n  }\n  assert(readyFibers_.empty());\n  assert(fibersActive_ == 0);\n}\n\nLoopController& FiberManager::loopController() {\n  return *loopController_;\n}\n\nconst LoopController& FiberManager::loopController() const {\n  return *loopController_;\n}\n\nbool FiberManager::hasTasks() const {\n  return fibersActive_ > 0 || !remoteReadyQueue_.empty() ||\n      !remoteTaskQueue_.empty();\n}\n\nFiber* FiberManager::getFiber() {\n  Fiber* fiber = nullptr;\n\n  if (options_.fibersPoolResizePeriodMs > 0 && !fibersPoolResizerScheduled_) {\n    fibersPoolResizer_();\n    fibersPoolResizerScheduled_ = true;\n  }\n\n  if (fibersPool_.empty()) {\n    fiber = new Fiber(*this);\n    ++fibersAllocated_;\n  } else {\n    fiber = &fibersPool_.front();\n    fibersPool_.pop_front();\n    assert(fibersPoolSize_ > 0);\n    --fibersPoolSize_;\n  }\n  assert(fiber);\n  if (++fibersActive_ > maxFibersActiveLastPeriod_) {\n    maxFibersActiveLastPeriod_ = fibersActive_;\n  }\n  ++fiberId_;\n  bool recordStack = (options_.recordStackEvery != 0) &&\n      (fiberId_ % options_.recordStackEvery == 0);\n  fiber->init(recordStack);\n  return fiber;\n}\n\nvoid FiberManager::setExceptionCallback(FiberManager::ExceptionCallback ec) {\n  assert(ec);\n  exceptionCallback_ = std::move(ec);\n}\n\nsize_t FiberManager::fibersAllocated() const {\n  return fibersAllocated_;\n}\n\nsize_t FiberManager::fibersPoolSize() const {\n  return fibersPoolSize_;\n}\n\nsize_t FiberManager::stackHighWatermark() const {\n  return stackHighWatermark_;\n}\n\nvoid FiberManager::remoteReadyInsert(Fiber* fiber) {\n  if (observer_) {\n    observer_->runnable(reinterpret_cast<uintptr_t>(fiber));\n  }\n  auto insertHead = [&]() { return remoteReadyQueue_.insertHead(fiber); };\n  loopController_->scheduleThreadSafe(std::ref(insertHead));\n}\n\nvoid FiberManager::setObserver(ExecutionObserver* observer) {\n  observer_ = observer;\n}\n\nExecutionObserver* FiberManager::getObserver() {\n  return observer_;\n}\n\nvoid FiberManager::setPreemptRunner(InlineFunctionRunner* preemptRunner) {\n  preemptRunner_ = preemptRunner;\n}\n\nvoid FiberManager::doFibersPoolResizing() {\n  while (fibersAllocated_ > maxFibersActiveLastPeriod_ &&\n         fibersPoolSize_ > options_.maxFibersPoolSize) {\n    auto fiber = &fibersPool_.front();\n    assert(fiber != nullptr);\n    fibersPool_.pop_front();\n    delete fiber;\n    --fibersPoolSize_;\n    --fibersAllocated_;\n  }\n\n  maxFibersActiveLastPeriod_ = fibersActive_;\n}\n\nvoid FiberManager::FibersPoolResizer::operator()() {\n  fiberManager_.doFibersPoolResizing();\n  fiberManager_.timeoutManager_->registerTimeout(\n      *this,\n      std::chrono::milliseconds(\n          fiberManager_.options_.fibersPoolResizePeriodMs));\n}\n\n#ifdef FOLLY_SANITIZE_ADDRESS\n\nvoid FiberManager::registerStartSwitchStackWithAsan(\n    void** saveFakeStack,\n    const void* stackBottom,\n    size_t stackSize) {\n  // Check if we can find a fiber enter function and call it if we find one\n  static AsanStartSwitchStackFuncPtr fn = getStartSwitchStackFunc();\n  if (fn == nullptr) {\n    LOG(FATAL) << \"The version of ASAN in use doesn't support fibers\";\n  } else {\n    fn(saveFakeStack, stackBottom, stackSize);\n  }\n}\n\nvoid FiberManager::registerFinishSwitchStackWithAsan(\n    void* saveFakeStack,\n    const void** saveStackBottom,\n    size_t* saveStackSize) {\n  // Check if we can find a fiber exit function and call it if we find one\n  static AsanFinishSwitchStackFuncPtr fn = getFinishSwitchStackFunc();\n  if (fn == nullptr) {\n    LOG(FATAL) << \"The version of ASAN in use doesn't support fibers\";\n  } else {\n    fn(saveFakeStack, saveStackBottom, saveStackSize);\n  }\n}\n\nvoid FiberManager::freeFakeStack(void* fakeStack) {\n  static AsanStartSwitchStackFuncPtr fnStart = getStartSwitchStackFunc();\n  static AsanFinishSwitchStackFuncPtr fnFinish = getFinishSwitchStackFunc();\n  if (fnStart == nullptr || fnFinish == nullptr) {\n    LOG(FATAL) << \"The version of ASAN in use doesn't support fibers\";\n  }\n\n  void* saveFakeStack;\n  const void* stackBottom;\n  size_t stackSize;\n  fnStart(&saveFakeStack, nullptr, 0);\n  fnFinish(fakeStack, &stackBottom, &stackSize);\n  fnStart(nullptr, stackBottom, stackSize);\n  fnFinish(saveFakeStack, nullptr, nullptr);\n}\n\nvoid FiberManager::unpoisonFiberStack(const Fiber* fiber) {\n  auto stack = fiber->getStack();\n\n  // Check if we can find a fiber enter function and call it if we find one\n  static AsanUnpoisonMemoryRegionFuncPtr fn = getUnpoisonMemoryRegionFunc();\n  if (fn == nullptr) {\n    LOG(FATAL) << \"This version of ASAN doesn't support memory unpoisoning\";\n  } else {\n    fn(stack.first, stack.second);\n  }\n}\n\nstatic AsanStartSwitchStackFuncPtr getStartSwitchStackFunc() {\n  AsanStartSwitchStackFuncPtr fn{nullptr};\n\n  // Check whether weak reference points to statically linked enter function\n  if (nullptr != (fn = &::__sanitizer_start_switch_fiber_weak)) {\n    return fn;\n  }\n\n  // Check whether we can find a dynamically linked enter function\n  if (nullptr != (fn = (AsanStartSwitchStackFuncPtr)dlsym(\n                      RTLD_DEFAULT, \"__sanitizer_start_switch_fiber\"))) {\n    return fn;\n  }\n\n  // Couldn't find the function at all\n  return nullptr;\n}\n\nstatic AsanFinishSwitchStackFuncPtr getFinishSwitchStackFunc() {\n  AsanFinishSwitchStackFuncPtr fn{nullptr};\n\n  // Check whether weak reference points to statically linked exit function\n  if (nullptr != (fn = &::__sanitizer_finish_switch_fiber_weak)) {\n    return fn;\n  }\n\n  // Check whether we can find a dynamically linked exit function\n  if (nullptr != (fn = (AsanFinishSwitchStackFuncPtr)dlsym(\n                      RTLD_DEFAULT, \"__sanitizer_finish_switch_fiber\"))) {\n    return fn;\n  }\n\n  // Couldn't find the function at all\n  return nullptr;\n}\n\nstatic AsanUnpoisonMemoryRegionFuncPtr getUnpoisonMemoryRegionFunc() {\n  AsanUnpoisonMemoryRegionFuncPtr fn{nullptr};\n\n  // Check whether weak reference points to statically linked unpoison function\n  if (nullptr != (fn = &::__asan_unpoison_memory_region_weak)) {\n    return fn;\n  }\n\n  // Check whether we can find a dynamically linked unpoison function\n  if (nullptr != (fn = (AsanUnpoisonMemoryRegionFuncPtr)dlsym(\n                      RTLD_DEFAULT, \"__asan_unpoison_memory_region\"))) {\n    return fn;\n  }\n\n  // Couldn't find the function at all\n  return nullptr;\n}\n\n#endif // FOLLY_SANITIZE_ADDRESS\n\n#ifndef _WIN32\nnamespace {\n\n// SIGSTKSZ (8 kB on our architectures) isn't always enough for\n// folly::symbolizer, so allocate 32 kB.\nconstexpr size_t kAltStackSize = folly::constexpr_max(SIGSTKSZ, 32 * 1024);\n\nbool hasAlternateStack() {\n  stack_t ss;\n  sigaltstack(nullptr, &ss);\n  return !(ss.ss_flags & SS_DISABLE);\n}\n\nint setAlternateStack(char* sp, size_t size) {\n  CHECK(sp);\n  stack_t ss{};\n  ss.ss_sp = sp;\n  ss.ss_size = size;\n  return sigaltstack(&ss, nullptr);\n}\n\nint unsetAlternateStack() {\n  stack_t ss{};\n  ss.ss_flags = SS_DISABLE;\n  return sigaltstack(&ss, nullptr);\n}\n\nclass ScopedAlternateSignalStack {\n public:\n  ScopedAlternateSignalStack() {\n    if (hasAlternateStack()) {\n      return;\n    }\n\n    stack_ = std::make_unique<AltStackBuffer>();\n\n    setAlternateStack(stack_->data(), stack_->size());\n  }\n\n  ~ScopedAlternateSignalStack() {\n    if (stack_) {\n      unsetAlternateStack();\n    }\n  }\n\n private:\n  using AltStackBuffer = std::array<char, kAltStackSize>;\n  std::unique_ptr<AltStackBuffer> stack_;\n};\n}\n\nvoid FiberManager::registerAlternateSignalStack() {\n  static folly::SingletonThreadLocal<ScopedAlternateSignalStack> singleton;\n  singleton.get();\n\n  alternateSignalStackRegistered_ = true;\n}\n#endif\n}\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/detail/CacheLocality.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/detail/CacheLocality.h>\n\n#ifndef _MSC_VER\n#define _GNU_SOURCE 1 // for RTLD_NOLOAD\n#include <dlfcn.h>\n#endif\n#include <fstream>\n\n#include <folly/Conv.h>\n#include <folly/Exception.h>\n#include <folly/FileUtil.h>\n#include <folly/Format.h>\n#include <folly/ScopeGuard.h>\n\nnamespace folly {\nnamespace detail {\n\n///////////// CacheLocality\n\n/// Returns the best real CacheLocality information available\nstatic CacheLocality getSystemLocalityInfo() {\n#ifdef __linux__\n  try {\n    return CacheLocality::readFromSysfs();\n  } catch (...) {\n    // keep trying\n  }\n#endif\n\n  long numCpus = sysconf(_SC_NPROCESSORS_CONF);\n  if (numCpus <= 0) {\n    // This shouldn't happen, but if it does we should try to keep\n    // going.  We are probably not going to be able to parse /sys on\n    // this box either (although we will try), which means we are going\n    // to fall back to the SequentialThreadId splitter.  On my 16 core\n    // (x hyperthreading) dev box 16 stripes is enough to get pretty good\n    // contention avoidance with SequentialThreadId, and there is little\n    // improvement from going from 32 to 64.  This default gives us some\n    // wiggle room\n    numCpus = 32;\n  }\n  return CacheLocality::uniform(size_t(numCpus));\n}\n\ntemplate <>\nconst CacheLocality& CacheLocality::system<std::atomic>() {\n  static auto* cache = new CacheLocality(getSystemLocalityInfo());\n  return *cache;\n}\n\n// Each level of cache has sharing sets, which are the set of cpus\n// that share a common cache at that level.  These are available in a\n// hex bitset form (/sys/devices/system/cpu/cpu0/index0/shared_cpu_map,\n// for example).  They are also available in a human-readable list form,\n// as in /sys/devices/system/cpu/cpu0/index0/shared_cpu_list.  The list\n// is a comma-separated list of numbers and ranges, where the ranges are\n// a pair of decimal numbers separated by a '-'.\n//\n// To sort the cpus for optimum locality we don't really need to parse\n// the sharing sets, we just need a unique representative from the\n// equivalence class.  The smallest value works fine, and happens to be\n// the first decimal number in the file.  We load all of the equivalence\n// class information from all of the cpu*/index* directories, order the\n// cpus first by increasing last-level cache equivalence class, then by\n// the smaller caches.  Finally, we break ties with the cpu number itself.\n\n/// Returns the first decimal number in the string, or throws an exception\n/// if the string does not start with a number terminated by ',', '-',\n/// '\\n', or eos.\nstatic size_t parseLeadingNumber(const std::string& line) {\n  auto raw = line.c_str();\n  char* end;\n  unsigned long val = strtoul(raw, &end, 10);\n  if (end == raw || (*end != ',' && *end != '-' && *end != '\\n' && *end != 0)) {\n    throw std::runtime_error(\n        to<std::string>(\"error parsing list '\", line, \"'\").c_str());\n  }\n  return val;\n}\n\nCacheLocality CacheLocality::readFromSysfsTree(\n    const std::function<std::string(std::string)>& mapping) {\n  // number of equivalence classes per level\n  std::vector<size_t> numCachesByLevel;\n\n  // the list of cache equivalence classes, where equivalance classes\n  // are named by the smallest cpu in the class\n  std::vector<std::vector<size_t>> equivClassesByCpu;\n\n  std::vector<size_t> cpus;\n\n  while (true) {\n    auto cpu = cpus.size();\n    std::vector<size_t> levels;\n    for (size_t index = 0;; ++index) {\n      auto dir =\n          sformat(\"/sys/devices/system/cpu/cpu{}/cache/index{}/\", cpu, index);\n      auto cacheType = mapping(dir + \"type\");\n      auto equivStr = mapping(dir + \"shared_cpu_list\");\n      if (cacheType.size() == 0 || equivStr.size() == 0) {\n        // no more caches\n        break;\n      }\n      if (cacheType[0] == 'I') {\n        // cacheType in { \"Data\", \"Instruction\", \"Unified\" }. skip icache\n        continue;\n      }\n      auto equiv = parseLeadingNumber(equivStr);\n      auto level = levels.size();\n      levels.push_back(equiv);\n\n      if (equiv == cpu) {\n        // we only want to count the equiv classes once, so we do it when\n        // we first encounter them\n        while (numCachesByLevel.size() <= level) {\n          numCachesByLevel.push_back(0);\n        }\n        numCachesByLevel[level]++;\n      }\n    }\n\n    if (levels.size() == 0) {\n      // no levels at all for this cpu, we must be done\n      break;\n    }\n    equivClassesByCpu.emplace_back(std::move(levels));\n    cpus.push_back(cpu);\n  }\n\n  if (cpus.size() == 0) {\n    throw std::runtime_error(\"unable to load cache sharing info\");\n  }\n\n  std::sort(cpus.begin(),\n            cpus.end(),\n            [&](size_t lhs, size_t rhs) -> bool {\n              // sort first by equiv class of cache with highest index,\n              // direction doesn't matter.  If different cpus have\n              // different numbers of caches then this code might produce\n              // a sub-optimal ordering, but it won't crash\n              auto& lhsEquiv = equivClassesByCpu[lhs];\n              auto& rhsEquiv = equivClassesByCpu[rhs];\n              for (ssize_t i = ssize_t(std::min(lhsEquiv.size(), rhsEquiv.size())) - 1;\n                   i >= 0;\n                   --i) {\n                auto idx = size_t(i);\n                if (lhsEquiv[idx] != rhsEquiv[idx]) {\n                  return lhsEquiv[idx] < rhsEquiv[idx];\n                }\n              }\n\n              // break ties deterministically by cpu\n              return lhs < rhs;\n            });\n\n  // the cpus are now sorted by locality, with neighboring entries closer\n  // to each other than entries that are far away.  For striping we want\n  // the inverse map, since we are starting with the cpu\n  std::vector<size_t> indexes(cpus.size());\n  for (size_t i = 0; i < cpus.size(); ++i) {\n    indexes[cpus[i]] = i;\n  }\n\n  return CacheLocality{\n      cpus.size(), std::move(numCachesByLevel), std::move(indexes)};\n}\n\nCacheLocality CacheLocality::readFromSysfs() {\n  return readFromSysfsTree([](std::string name) {\n    std::ifstream xi(name.c_str());\n    std::string rv;\n    std::getline(xi, rv);\n    return rv;\n  });\n}\n\nCacheLocality CacheLocality::uniform(size_t numCpus) {\n  CacheLocality rv;\n\n  rv.numCpus = numCpus;\n\n  // one cache shared by all cpus\n  rv.numCachesByLevel.push_back(numCpus);\n\n  // no permutations in locality index mapping\n  for (size_t cpu = 0; cpu < numCpus; ++cpu) {\n    rv.localityIndexByCpu.push_back(cpu);\n  }\n\n  return rv;\n}\n\n////////////// Getcpu\n\nGetcpu::Func Getcpu::resolveVdsoFunc() {\n#if !FOLLY_HAVE_LINUX_VDSO\n  return nullptr;\n#else\n  void* h = dlopen(\"linux-vdso.so.1\", RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);\n  if (h == nullptr) {\n    return nullptr;\n  }\n\n  auto func = Getcpu::Func(dlsym(h, \"__vdso_getcpu\"));\n  if (func == nullptr) {\n    // technically a null result could either be a failure or a successful\n    // lookup of a symbol with the null value, but the second can't actually\n    // happen for this symbol.  No point holding the handle forever if\n    // we don't need the code\n    dlclose(h);\n  }\n\n  return func;\n#endif\n}\n\n#ifdef FOLLY_TLS\n/////////////// SequentialThreadId\ntemplate struct SequentialThreadId<std::atomic>;\n#endif\n\n/////////////// AccessSpreader\ntemplate struct AccessSpreader<std::atomic>;\n\n} // namespace detail\n} // namespace folly\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/test/ThreadLocalTest.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/ThreadLocal.h>\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#include <sys/wait.h>\n#endif\n\n#include <sys/types.h>\n\n#include <array>\n#include <atomic>\n#include <chrono>\n#include <condition_variable>\n#include <limits.h>\n#include <map>\n#include <mutex>\n#include <set>\n#include <thread>\n#include <unordered_map>\n\n#include <glog/logging.h>\n\n#include <folly/Baton.h>\n#include <folly/Memory.h>\n#include <folly/ThreadId.h>\n#include <folly/experimental/io/FsUtil.h>\n#include <folly/portability/GTest.h>\n#include <folly/portability/Unistd.h>\n\nusing namespace folly;\n\nstruct Widget {\n  static int totalVal_;\n  int val_;\n  ~Widget() {\n    totalVal_ += val_;\n  }\n\n  static void customDeleter(Widget* w, TLPDestructionMode mode) {\n    totalVal_ += (mode == TLPDestructionMode::ALL_THREADS) ? 1000 : 1;\n    delete w;\n  }\n};\nint Widget::totalVal_ = 0;\n\nTEST(ThreadLocalPtr, BasicDestructor) {\n  Widget::totalVal_ = 0;\n  ThreadLocalPtr<Widget> w;\n  std::thread([&w]() {\n      w.reset(new Widget());\n      w.get()->val_ += 10;\n    }).join();\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, CustomDeleter1) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocalPtr<Widget> w;\n    std::thread([&w]() {\n        w.reset(new Widget(), Widget::customDeleter);\n        w.get()->val_ += 10;\n      }).join();\n    EXPECT_EQ(11, Widget::totalVal_);\n  }\n  EXPECT_EQ(11, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, CustomDeleterOwnershipTransfer) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocalPtr<Widget> w;\n    auto deleter = [](Widget* ptr) {\n      Widget::customDeleter(ptr, TLPDestructionMode::THIS_THREAD);\n    };\n    std::unique_ptr<Widget, decltype(deleter)> source(new Widget(), deleter);\n    std::thread([&w, &source]() {\n      w.reset(std::move(source));\n      w.get()->val_ += 10;\n    }).join();\n    EXPECT_EQ(11, Widget::totalVal_);\n  }\n  EXPECT_EQ(11, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, DefaultDeleterOwnershipTransfer) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocalPtr<Widget> w;\n    auto source = std::make_unique<Widget>();\n    std::thread([&w, &source]() {\n      w.reset(std::move(source));\n      w.get()->val_ += 10;\n    }).join();\n    EXPECT_EQ(10, Widget::totalVal_);\n  }\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, resetNull) {\n  ThreadLocalPtr<int> tl;\n  EXPECT_FALSE(tl);\n  tl.reset(new int(4));\n  EXPECT_TRUE(static_cast<bool>(tl));\n  EXPECT_EQ(*tl.get(), 4);\n  tl.reset();\n  EXPECT_FALSE(tl);\n}\n\nTEST(ThreadLocalPtr, TestRelease) {\n  Widget::totalVal_ = 0;\n  ThreadLocalPtr<Widget> w;\n  std::unique_ptr<Widget> wPtr;\n  std::thread([&w, &wPtr]() {\n      w.reset(new Widget());\n      w.get()->val_ += 10;\n\n      wPtr.reset(w.release());\n    }).join();\n  EXPECT_EQ(0, Widget::totalVal_);\n  wPtr.reset();\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocalPtr, CreateOnThreadExit) {\n  Widget::totalVal_ = 0;\n  ThreadLocal<Widget> w;\n  ThreadLocalPtr<int> tl;\n\n  std::thread([&] {\n    tl.reset(new int(1),\n             [&](int* ptr, TLPDestructionMode /* mode */) {\n               delete ptr;\n               // This test ensures Widgets allocated here are not leaked.\n               ++w.get()->val_;\n               ThreadLocal<Widget> wl;\n               ++wl.get()->val_;\n             });\n  }).join();\n  EXPECT_EQ(2, Widget::totalVal_);\n}\n\n// Test deleting the ThreadLocalPtr object\nTEST(ThreadLocalPtr, CustomDeleter2) {\n  Widget::totalVal_ = 0;\n  std::thread t;\n  std::mutex mutex;\n  std::condition_variable cv;\n  enum class State {\n    START,\n    DONE,\n    EXIT\n  };\n  State state = State::START;\n  {\n    ThreadLocalPtr<Widget> w;\n    t = std::thread([&]() {\n        w.reset(new Widget(), Widget::customDeleter);\n        w.get()->val_ += 10;\n\n        // Notify main thread that we're done\n        {\n          std::unique_lock<std::mutex> lock(mutex);\n          state = State::DONE;\n          cv.notify_all();\n        }\n\n        // Wait for main thread to allow us to exit\n        {\n          std::unique_lock<std::mutex> lock(mutex);\n          while (state != State::EXIT) {\n            cv.wait(lock);\n          }\n        }\n    });\n\n    // Wait for main thread to start (and set w.get()->val_)\n    {\n      std::unique_lock<std::mutex> lock(mutex);\n      while (state != State::DONE) {\n        cv.wait(lock);\n      }\n    }\n\n    // Thread started but hasn't exited yet\n    EXPECT_EQ(0, Widget::totalVal_);\n\n    // Destroy ThreadLocalPtr<Widget> (by letting it go out of scope)\n  }\n\n  EXPECT_EQ(1010, Widget::totalVal_);\n\n  // Allow thread to exit\n  {\n    std::unique_lock<std::mutex> lock(mutex);\n    state = State::EXIT;\n    cv.notify_all();\n  }\n  t.join();\n\n  EXPECT_EQ(1010, Widget::totalVal_);\n}\n\nTEST(ThreadLocal, BasicDestructor) {\n  Widget::totalVal_ = 0;\n  ThreadLocal<Widget> w;\n  std::thread([&w]() { w->val_ += 10; }).join();\n  EXPECT_EQ(10, Widget::totalVal_);\n}\n\nTEST(ThreadLocal, SimpleRepeatDestructor) {\n  Widget::totalVal_ = 0;\n  {\n    ThreadLocal<Widget> w;\n    w->val_ += 10;\n  }\n  {\n    ThreadLocal<Widget> w;\n    w->val_ += 10;\n  }\n  EXPECT_EQ(20, Widget::totalVal_);\n}\n\nTEST(ThreadLocal, InterleavedDestructors) {\n  Widget::totalVal_ = 0;\n  std::unique_ptr<ThreadLocal<Widget>> w;\n  int wVersion = 0;\n  const int wVersionMax = 2;\n  int thIter = 0;\n  std::mutex lock;\n  auto th = std::thread([&]() {\n    int wVersionPrev = 0;\n    while (true) {\n      while (true) {\n        std::lock_guard<std::mutex> g(lock);\n        if (wVersion > wVersionMax) {\n          return;\n        }\n        if (wVersion > wVersionPrev) {\n          // We have a new version of w, so it should be initialized to zero\n          EXPECT_EQ((*w)->val_, 0);\n          break;\n        }\n      }\n      std::lock_guard<std::mutex> g(lock);\n      wVersionPrev = wVersion;\n      (*w)->val_ += 10;\n      ++thIter;\n    }\n  });\n  FOR_EACH_RANGE(i, 0, wVersionMax) {\n    int thIterPrev = 0;\n    {\n      std::lock_guard<std::mutex> g(lock);\n      thIterPrev = thIter;\n      w.reset(new ThreadLocal<Widget>());\n      ++wVersion;\n    }\n    while (true) {\n      std::lock_guard<std::mutex> g(lock);\n      if (thIter > thIterPrev) {\n        break;\n      }\n    }\n  }\n  {\n    std::lock_guard<std::mutex> g(lock);\n    wVersion = wVersionMax + 1;\n  }\n  th.join();\n  EXPECT_EQ(wVersionMax * 10, Widget::totalVal_);\n}\n\nclass SimpleThreadCachedInt {\n\n  class NewTag;\n  ThreadLocal<int,NewTag> val_;\n\n public:\n  void add(int val) {\n    *val_ += val;\n  }\n\n  int read() {\n    int ret = 0;\n    for (const auto& i : val_.accessAllThreads()) {\n      ret += i;\n    }\n    return ret;\n  }\n};\n\nTEST(ThreadLocalPtr, AccessAllThreadsCounter) {\n  const int kNumThreads = 10;\n  SimpleThreadCachedInt stci;\n  std::atomic<bool> run(true);\n  std::atomic<int> totalAtomic(0);\n  std::vector<std::thread> threads;\n  for (int i = 0; i < kNumThreads; ++i) {\n    threads.push_back(std::thread([&]() {\n      stci.add(1);\n      totalAtomic.fetch_add(1);\n      while (run.load()) {\n        usleep(100);\n      }\n    }));\n  }\n  while (totalAtomic.load() != kNumThreads) { usleep(100); }\n  EXPECT_EQ(kNumThreads, stci.read());\n  run.store(false);\n  for (auto& t : threads) {\n    t.join();\n  }\n}\n\nTEST(ThreadLocal, resetNull) {\n  ThreadLocal<int> tl;\n  tl.reset(new int(4));\n  EXPECT_EQ(*tl.get(), 4);\n  tl.reset();\n  EXPECT_EQ(*tl.get(), 0);\n  tl.reset(new int(5));\n  EXPECT_EQ(*tl.get(), 5);\n}\n\nnamespace {\nstruct Tag {};\n\nstruct Foo {\n  folly::ThreadLocal<int, Tag> tl;\n};\n}  // namespace\n\nTEST(ThreadLocal, Movable1) {\n  Foo a;\n  Foo b;\n  EXPECT_TRUE(a.tl.get() != b.tl.get());\n\n  a = Foo();\n  b = Foo();\n  EXPECT_TRUE(a.tl.get() != b.tl.get());\n}\n\nTEST(ThreadLocal, Movable2) {\n  std::map<int, Foo> map;\n\n  map[42];\n  map[10];\n  map[23];\n  map[100];\n\n  std::set<void*> tls;\n  for (auto& m : map) {\n    tls.insert(m.second.tl.get());\n  }\n\n  // Make sure that we have 4 different instances of *tl\n  EXPECT_EQ(4, tls.size());\n}\n\nnamespace {\n\nconstexpr size_t kFillObjectSize = 300;\n\nstd::atomic<uint64_t> gDestroyed;\n\n/**\n * Fill a chunk of memory with a unique-ish pattern that includes the thread id\n * (so deleting one of these from another thread would cause a failure)\n *\n * Verify it explicitly and on destruction.\n */\nclass FillObject {\n public:\n  explicit FillObject(uint64_t idx) : idx_(idx) {\n    uint64_t v = val();\n    for (size_t i = 0; i < kFillObjectSize; ++i) {\n      data_[i] = v;\n    }\n  }\n\n  void check() {\n    uint64_t v = val();\n    for (size_t i = 0; i < kFillObjectSize; ++i) {\n      CHECK_EQ(v, data_[i]);\n    }\n  }\n\n  ~FillObject() {\n    ++gDestroyed;\n  }\n\n private:\n  uint64_t val() const {\n    return (idx_ << 40) | folly::getCurrentThreadID();\n  }\n\n  uint64_t idx_;\n  uint64_t data_[kFillObjectSize];\n};\n\n}  // namespace\n\nTEST(ThreadLocal, Stress) {\n  static constexpr size_t numFillObjects = 250;\n  std::array<ThreadLocalPtr<FillObject>, numFillObjects> objects;\n\n  static constexpr size_t numThreads = 32;\n  static constexpr size_t numReps = 20;\n\n  std::vector<std::thread> threads;\n  threads.reserve(numThreads);\n\n  for (size_t k = 0; k < numThreads; ++k) {\n    threads.emplace_back([&objects] {\n      for (size_t rep = 0; rep < numReps; ++rep) {\n        for (size_t i = 0; i < objects.size(); ++i) {\n          objects[i].reset(new FillObject(rep * objects.size() + i));\n          std::this_thread::sleep_for(std::chrono::microseconds(100));\n        }\n        for (size_t i = 0; i < objects.size(); ++i) {\n          objects[i]->check();\n        }\n      }\n    });\n  }\n\n  for (auto& t : threads) {\n    t.join();\n  }\n\n  EXPECT_EQ(numFillObjects * numThreads * numReps, gDestroyed);\n}\n\n// Yes, threads and fork don't mix\n// (http://cppwisdom.quora.com/Why-threads-and-fork-dont-mix) but if you're\n// stupid or desperate enough to try, we shouldn't stand in your way.\nnamespace {\nclass HoldsOne {\n public:\n  HoldsOne() : value_(1) { }\n  // Do an actual access to catch the buggy case where this == nullptr\n  int value() const { return value_; }\n private:\n  int value_;\n};\n\nstruct HoldsOneTag {};\n\nThreadLocal<HoldsOne, HoldsOneTag> ptr;\n\nint totalValue() {\n  int value = 0;\n  for (auto& p : ptr.accessAllThreads()) {\n    value += p.value();\n  }\n  return value;\n}\n\n}  // namespace\n\n#ifdef FOLLY_HAVE_PTHREAD_ATFORK\nTEST(ThreadLocal, Fork) {\n  EXPECT_EQ(1, ptr->value());  // ensure created\n  EXPECT_EQ(1, totalValue());\n  // Spawn a new thread\n\n  std::mutex mutex;\n  bool started = false;\n  std::condition_variable startedCond;\n  bool stopped = false;\n  std::condition_variable stoppedCond;\n\n  std::thread t([&] () {\n    EXPECT_EQ(1, ptr->value());  // ensure created\n    {\n      std::unique_lock<std::mutex> lock(mutex);\n      started = true;\n      startedCond.notify_all();\n    }\n    {\n      std::unique_lock<std::mutex> lock(mutex);\n      while (!stopped) {\n        stoppedCond.wait(lock);\n      }\n    }\n  });\n\n  {\n    std::unique_lock<std::mutex> lock(mutex);\n    while (!started) {\n      startedCond.wait(lock);\n    }\n  }\n\n  EXPECT_EQ(2, totalValue());\n\n  pid_t pid = fork();\n  if (pid == 0) {\n    // in child\n    int v = totalValue();\n\n    // exit successfully if v == 1 (one thread)\n    // diagnostic error code otherwise :)\n    switch (v) {\n    case 1: _exit(0);\n    case 0: _exit(1);\n    }\n    _exit(2);\n  } else if (pid > 0) {\n    // in parent\n    int status;\n    EXPECT_EQ(pid, waitpid(pid, &status, 0));\n    EXPECT_TRUE(WIFEXITED(status));\n    EXPECT_EQ(0, WEXITSTATUS(status));\n  } else {\n    EXPECT_TRUE(false) << \"fork failed\";\n  }\n\n  EXPECT_EQ(2, totalValue());\n\n  {\n    std::unique_lock<std::mutex> lock(mutex);\n    stopped = true;\n    stoppedCond.notify_all();\n  }\n\n  t.join();\n\n  EXPECT_EQ(1, totalValue());\n}\n#endif\n\n#ifndef _WIN32\nstruct HoldsOneTag2 {};\n\nTEST(ThreadLocal, Fork2) {\n  // A thread-local tag that was used in the parent from a *different* thread\n  // (but not the forking thread) would cause the child to hang in a\n  // ThreadLocalPtr's object destructor. Yeah.\n  ThreadLocal<HoldsOne, HoldsOneTag2> p;\n  {\n    // use tag in different thread\n    std::thread t([&p] { p.get(); });\n    t.join();\n  }\n  pid_t pid = fork();\n  if (pid == 0) {\n    {\n      ThreadLocal<HoldsOne, HoldsOneTag2> q;\n      q.get();\n    }\n    _exit(0);\n  } else if (pid > 0) {\n    int status;\n    EXPECT_EQ(pid, waitpid(pid, &status, 0));\n    EXPECT_TRUE(WIFEXITED(status));\n    EXPECT_EQ(0, WEXITSTATUS(status));\n  } else {\n    EXPECT_TRUE(false) << \"fork failed\";\n  }\n}\n\n// Elide this test when using any sanitizer. Otherwise, the dlopen'ed code\n// would end up running without e.g., ASAN-initialized data structures and\n// failing right away.\n#if !defined FOLLY_SANITIZE_ADDRESS && !defined UNDEFINED_SANITIZER && \\\n    !defined FOLLY_SANITIZE_THREAD\n\nTEST(ThreadLocal, SharedLibrary) {\n  auto exe = fs::executable_path();\n  auto lib = exe.parent_path() / \"thread_local_test_lib.so\";\n  auto handle = dlopen(lib.string().c_str(), RTLD_LAZY);\n  EXPECT_NE(nullptr, handle);\n\n  typedef void (*useA_t)();\n  dlerror();\n  useA_t useA = (useA_t) dlsym(handle, \"useA\");\n\n  const char *dlsym_error = dlerror();\n  EXPECT_EQ(nullptr, dlsym_error);\n\n  useA();\n\n  folly::Baton<> b11, b12, b21, b22;\n\n  std::thread t1([&]() {\n      useA();\n      b11.post();\n      b12.wait();\n    });\n\n  std::thread t2([&]() {\n      useA();\n      b21.post();\n      b22.wait();\n    });\n\n  b11.wait();\n  b21.wait();\n\n  dlclose(handle);\n\n  b12.post();\n  b22.post();\n\n  t1.join();\n  t2.join();\n}\n\n#endif\n#endif\n\nnamespace folly { namespace threadlocal_detail {\nstruct PthreadKeyUnregisterTester {\n  PthreadKeyUnregister p;\n  constexpr PthreadKeyUnregisterTester() = default;\n};\n}}\n\nTEST(ThreadLocal, UnregisterClassHasConstExprCtor) {\n  folly::threadlocal_detail::PthreadKeyUnregisterTester x;\n  // yep!\n  SUCCEED();\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/test/FileUtilTest.cpp": "/*\n * Copyright 2017 Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/FileUtil.h>\n#include <folly/detail/FileUtilDetail.h>\n#include <folly/experimental/TestUtil.h>\n\n#include <deque>\n#if defined(__linux__)\n#include <dlfcn.h>\n#endif\n\n#include <glog/logging.h>\n\n#include <folly/Exception.h>\n#include <folly/File.h>\n#include <folly/Range.h>\n#include <folly/String.h>\n#include <folly/portability/GTest.h>\n\nnamespace folly { namespace test {\n\nusing namespace fileutil_detail;\nusing namespace std;\n\nnamespace {\n\nclass Reader {\n public:\n  Reader(off_t offset, StringPiece data, std::deque<ssize_t> spec);\n\n  // write-like\n  ssize_t operator()(int fd, void* buf, size_t count);\n\n  // pwrite-like\n  ssize_t operator()(int fd, void* buf, size_t count, off_t offset);\n\n  // writev-like\n  ssize_t operator()(int fd, const iovec* iov, int count);\n\n  // pwritev-like\n  ssize_t operator()(int fd, const iovec* iov, int count, off_t offset);\n\n  const std::deque<ssize_t> spec() const { return spec_; }\n\n private:\n  ssize_t nextSize();\n\n  off_t offset_;\n  StringPiece data_;\n  std::deque<ssize_t> spec_;\n};\n\nReader::Reader(off_t offset, StringPiece data, std::deque<ssize_t> spec)\n  : offset_(offset),\n    data_(data),\n    spec_(std::move(spec)) {\n}\n\nssize_t Reader::nextSize() {\n  if (spec_.empty()) {\n    throw std::runtime_error(\"spec empty\");\n  }\n  ssize_t n = spec_.front();\n  spec_.pop_front();\n  if (n <= 0) {\n    if (n == -1) {\n      errno = EIO;\n    }\n    spec_.clear();  // so we fail if called again\n  } else {\n    offset_ += n;\n  }\n  return n;\n}\n\nssize_t Reader::operator()(int /* fd */, void* buf, size_t count) {\n  ssize_t n = nextSize();\n  if (n <= 0) {\n    return n;\n  }\n  if (size_t(n) > count) {\n    throw std::runtime_error(\"requested count too small\");\n  }\n  memcpy(buf, data_.data(), n);\n  data_.advance(n);\n  return n;\n}\n\nssize_t Reader::operator()(int fd, void* buf, size_t count, off_t offset) {\n  EXPECT_EQ(offset_, offset);\n  return operator()(fd, buf, count);\n}\n\nssize_t Reader::operator()(int /* fd */, const iovec* iov, int count) {\n  ssize_t n = nextSize();\n  if (n <= 0) {\n    return n;\n  }\n  ssize_t remaining = n;\n  for (; count != 0 && remaining != 0; ++iov, --count) {\n    ssize_t len = std::min(remaining, ssize_t(iov->iov_len));\n    memcpy(iov->iov_base, data_.data(), len);\n    data_.advance(len);\n    remaining -= len;\n  }\n  if (remaining != 0) {\n    throw std::runtime_error(\"requested total size too small\");\n  }\n  return n;\n}\n\nssize_t Reader::operator()(int fd, const iovec* iov, int count, off_t offset) {\n  EXPECT_EQ(offset_, offset);\n  return operator()(fd, iov, count);\n}\n\n}  // namespace\n\nclass FileUtilTest : public ::testing::Test {\n protected:\n  FileUtilTest();\n\n  Reader reader(std::deque<ssize_t> spec);\n\n  std::string in_;\n  std::vector<std::pair<size_t, Reader>> readers_;\n};\n\nFileUtilTest::FileUtilTest()\n  : in_(\"1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\") {\n  CHECK_EQ(62, in_.size());\n\n  readers_.emplace_back(0, reader({0}));\n  readers_.emplace_back(62, reader({62}));\n  readers_.emplace_back(62, reader({62, -1}));  // error after end (not called)\n  readers_.emplace_back(61, reader({61, 0}));\n  readers_.emplace_back(-1, reader({61, -1}));  // error before end\n  readers_.emplace_back(62, reader({31, 31}));\n  readers_.emplace_back(62, reader({1, 10, 20, 10, 1, 20}));\n  readers_.emplace_back(61, reader({1, 10, 20, 10, 20, 0}));\n  readers_.emplace_back(41, reader({1, 10, 20, 10, 0}));\n  readers_.emplace_back(-1, reader({1, 10, 20, 10, 20, -1}));\n}\n\nReader FileUtilTest::reader(std::deque<ssize_t> spec) {\n  return Reader(42, in_, std::move(spec));\n}\n\nTEST_F(FileUtilTest, read) {\n  for (auto& p : readers_) {\n    std::string out(in_.size(), '\\0');\n    EXPECT_EQ(p.first, wrapFull(p.second, 0, &out[0], out.size()));\n    if (p.first != (decltype(p.first))(-1)) {\n      EXPECT_EQ(in_.substr(0, p.first), out.substr(0, p.first));\n    }\n  }\n}\n\nTEST_F(FileUtilTest, pread) {\n  for (auto& p : readers_) {\n    std::string out(in_.size(), '\\0');\n    EXPECT_EQ(p.first, wrapFull(p.second, 0, &out[0], out.size(), off_t(42)));\n    if (p.first != (decltype(p.first))(-1)) {\n      EXPECT_EQ(in_.substr(0, p.first), out.substr(0, p.first));\n    }\n  }\n}\n\nclass IovecBuffers {\n public:\n  explicit IovecBuffers(std::initializer_list<size_t> sizes);\n  explicit IovecBuffers(std::vector<size_t> sizes);\n\n  std::vector<iovec> iov() const { return iov_; }  // yes, make a copy\n  std::string join() const { return folly::join(\"\", buffers_); }\n  size_t size() const;\n\n private:\n  std::vector<std::string> buffers_;\n  std::vector<iovec> iov_;\n};\n\nIovecBuffers::IovecBuffers(std::initializer_list<size_t> sizes) {\n  iov_.reserve(sizes.size());\n  for (auto& s : sizes) {\n    buffers_.push_back(std::string(s, '\\0'));\n  }\n  for (auto& b : buffers_) {\n    iovec iov;\n    iov.iov_base = &b[0];\n    iov.iov_len = b.size();\n    iov_.push_back(iov);\n  }\n}\n\nIovecBuffers::IovecBuffers(std::vector<size_t> sizes) {\n  iov_.reserve(sizes.size());\n  for (auto s : sizes) {\n    buffers_.push_back(std::string(s, '\\0'));\n  }\n  for (auto& b : buffers_) {\n    iovec iov;\n    iov.iov_base = &b[0];\n    iov.iov_len = b.size();\n    iov_.push_back(iov);\n  }\n}\n\nsize_t IovecBuffers::size() const {\n  size_t s = 0;\n  for (auto& b : buffers_) {\n    s += b.size();\n  }\n  return s;\n}\n\nTEST_F(FileUtilTest, readv) {\n  for (auto& p : readers_) {\n    IovecBuffers buf({12, 19, 31});\n    ASSERT_EQ(62, buf.size());\n\n    auto iov = buf.iov();\n    EXPECT_EQ(p.first, wrapvFull(p.second, 0, iov.data(), iov.size()));\n    if (p.first != (decltype(p.first))(-1)) {\n      EXPECT_EQ(in_.substr(0, p.first), buf.join().substr(0, p.first));\n    }\n  }\n}\n\nTEST(FileUtilTest2, wrapv) {\n  TemporaryFile tempFile(\"file-util-test\");\n  std::vector<size_t> sizes;\n  size_t sum = 0;\n  for (int32_t i = 0; i < 1500; ++i) {\n    sizes.push_back(i % 3 + 1);\n    sum += sizes.back();\n  }\n  IovecBuffers buf(sizes);\n  ASSERT_EQ(sum, buf.size());\n  auto iov = buf.iov();\n  EXPECT_EQ(sum, wrapvFull(writev, tempFile.fd(), iov.data(), iov.size()));\n}\n\nTEST_F(FileUtilTest, preadv) {\n  for (auto& p : readers_) {\n    IovecBuffers buf({12, 19, 31});\n    ASSERT_EQ(62, buf.size());\n\n    auto iov = buf.iov();\n    EXPECT_EQ(p.first,\n              wrapvFull(p.second, 0, iov.data(), iov.size(), off_t(42)));\n    if (p.first != (decltype(p.first))(-1)) {\n      EXPECT_EQ(in_.substr(0, p.first), buf.join().substr(0, p.first));\n    }\n  }\n}\n\nTEST(String, readFile) {\n  const TemporaryFile afileTemp, emptyFileTemp;\n  auto afile = afileTemp.path().string();\n  auto emptyFile = emptyFileTemp.path().string();\n\n  EXPECT_TRUE(writeFile(string(), emptyFile.c_str()));\n  EXPECT_TRUE(writeFile(StringPiece(\"bar\"), afile.c_str()));\n\n  {\n    string contents;\n    EXPECT_TRUE(readFile(emptyFile.c_str(), contents));\n    EXPECT_EQ(contents, \"\");\n    EXPECT_TRUE(readFile(afile.c_str(), contents, 0));\n    EXPECT_EQ(\"\", contents);\n    EXPECT_TRUE(readFile(afile.c_str(), contents, 2));\n    EXPECT_EQ(\"ba\", contents);\n    EXPECT_TRUE(readFile(afile.c_str(), contents));\n    EXPECT_EQ(\"bar\", contents);\n  }\n  {\n    vector<unsigned char> contents;\n    EXPECT_TRUE(readFile(emptyFile.c_str(), contents));\n    EXPECT_EQ(vector<unsigned char>(), contents);\n    EXPECT_TRUE(readFile(afile.c_str(), contents, 0));\n    EXPECT_EQ(vector<unsigned char>(), contents);\n    EXPECT_TRUE(readFile(afile.c_str(), contents, 2));\n    EXPECT_EQ(vector<unsigned char>({'b', 'a'}), contents);\n    EXPECT_TRUE(readFile(afile.c_str(), contents));\n    EXPECT_EQ(vector<unsigned char>({'b', 'a', 'r'}), contents);\n  }\n}\n\nclass ReadFileFd : public ::testing::Test {\n protected:\n  void SetUp() override {\n    ASSERT_TRUE(writeFile(StringPiece(\"bar\"), aFile.path().string().c_str()));\n  }\n\n  TemporaryFile aFile;\n};\n\nTEST_F(ReadFileFd, ReadZeroBytes) {\n  std::string contents;\n  EXPECT_TRUE(readFile(aFile.fd(), contents, 0));\n  EXPECT_EQ(\"\", contents);\n}\n\nTEST_F(ReadFileFd, ReadPartial) {\n  std::string contents;\n  EXPECT_TRUE(readFile(aFile.fd(), contents, 2));\n  EXPECT_EQ(\"ba\", contents);\n}\n\nTEST_F(ReadFileFd, ReadFull) {\n  std::string contents;\n  EXPECT_TRUE(readFile(aFile.fd(), contents));\n  EXPECT_EQ(\"bar\", contents);\n}\n\nTEST_F(ReadFileFd, WriteOnlyFd) {\n  File f(aFile.path().string(), O_WRONLY);\n  std::string contents;\n  EXPECT_FALSE(readFile(f.fd(), contents));\n  PLOG(INFO);\n}\n\nTEST_F(ReadFileFd, InvalidFd) {\n  File f(aFile.path().string());\n  f.close();\n  std::string contents;\n  msvcSuppressAbortOnInvalidParams([&] {\n    EXPECT_FALSE(readFile(f.fd(), contents));\n  });\n  PLOG(INFO);\n}\n\nclass WriteFileAtomic : public ::testing::Test {\n protected:\n  WriteFileAtomic() {}\n\n  std::set<std::string> listTmpDir() const {\n    std::set<std::string> entries;\n    for (auto& entry : fs::directory_iterator(tmpDir_.path())) {\n      entries.insert(entry.path().filename().string());\n    }\n    return entries;\n  }\n\n  std::string readData(const string& path) const {\n    string data;\n    if (!readFile(path.c_str(), data)) {\n      throwSystemError(\"failed to read \", path);\n    }\n    return data;\n  }\n\n  struct stat statFile(const string& path) const {\n    struct stat s;\n    auto rc = stat(path.c_str(), &s);\n    checkUnixError(rc, \"failed to stat() \", path);\n    return s;\n  }\n\n  mode_t getPerms(const string& path) {\n    return (statFile(path).st_mode & 0777);\n  }\n\n  string tmpPath(StringPiece name) {\n    return tmpDir_.path().string() + \"/\" + name.str();\n  }\n\n  void setDirPerms(mode_t mode) {\n    auto rc = chmod(tmpDir_.path().string().c_str(), mode);\n    checkUnixError(rc, \"failed to set permissions on tmp dir\");\n  }\n\n  TemporaryDirectory tmpDir_{\"folly_file_test\"};\n};\n\nTEST_F(WriteFileAtomic, writeNew) {\n  // Call writeFileAtomic() to create a new file\n  auto path = tmpPath(\"foo\");\n  auto contents = StringPiece{\"contents\\n\"};\n  writeFileAtomic(path, contents);\n\n  // The directory should contain exactly 1 file now, with the correct contents\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n  EXPECT_EQ(contents, readData(path));\n  EXPECT_EQ(0644, getPerms(path));\n}\n\nTEST_F(WriteFileAtomic, overwrite) {\n  // Call writeFileAtomic() to create a new file\n  auto path = tmpPath(\"foo\");\n  auto contents1 = StringPiece{\"contents\\n\"};\n  writeFileAtomic(path, contents1);\n\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n  EXPECT_EQ(contents1, readData(path));\n  EXPECT_EQ(0644, getPerms(path));\n\n  // Now overwrite the file with different contents\n  auto contents2 = StringPiece{\"testing\"};\n  writeFileAtomic(path, contents2);\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n  EXPECT_EQ(contents2, readData(path));\n  EXPECT_EQ(0644, getPerms(path));\n\n  // Test overwriting with relatively large contents, and different permissions\n  auto contents3 =\n      \"asdf\" + string(10240, '\\n') + \"foobar\\n\" + string(10240, 'b') + \"\\n\";\n  writeFileAtomic(path, contents3, 0444);\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n  EXPECT_EQ(contents3, readData(path));\n  EXPECT_EQ(0444, getPerms(path));\n\n  // Test overwriting with empty contents\n  //\n  // Note that the file's permissions are 0444 at this point (read-only),\n  // but we writeFileAtomic() should still replace it successfully.  Since we\n  // update it with a rename we need write permissions on the parent directory,\n  // but not the destination file.\n  auto contents4 = StringPiece(\"\");\n  writeFileAtomic(path, contents4, 0400);\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n  EXPECT_EQ(contents4, readData(path));\n  EXPECT_EQ(0400, getPerms(path));\n}\n\nTEST_F(WriteFileAtomic, directoryPermissions) {\n  // Test writeFileAtomic() when we do not have write permission in the target\n  // directory.\n  //\n  // Make the test directory read-only\n  setDirPerms(0555);\n  SCOPE_EXIT {\n    // Restore directory permissions before we exit, just to ensure the code\n    // will be able to clean up the directory.\n    try {\n      setDirPerms(0755);\n    } catch (const std::exception&) {\n      // Intentionally ignore errors here, in case an exception is already\n      // being thrown.\n    }\n  };\n\n  // writeFileAtomic() should fail, and the directory should still be empty\n  auto path1 = tmpPath(\"foo\");\n  auto contents = StringPiece(\"testing\");\n  EXPECT_THROW(writeFileAtomic(path1, contents), std::system_error);\n  EXPECT_EQ(set<string>{}, listTmpDir());\n\n  // Make the directory writable again, then create the file\n  setDirPerms(0755);\n  writeFileAtomic(path1, contents, 0400);\n  EXPECT_EQ(contents, readData(path1));\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n\n  // Make the directory read-only again\n  // Creating another file now should fail and we should still have only the\n  // first file.\n  setDirPerms(0555);\n  EXPECT_THROW(\n      writeFileAtomic(tmpPath(\"another_file.txt\"), \"x\\n\"), std::system_error);\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n}\n\nTEST_F(WriteFileAtomic, multipleFiles) {\n  // Test creating multiple files in the same directory\n  writeFileAtomic(tmpPath(\"foo.txt\"), \"foo\");\n  writeFileAtomic(tmpPath(\"bar.txt\"), \"bar\", 0400);\n  writeFileAtomic(tmpPath(\"foo_txt\"), \"underscore\", 0440);\n  writeFileAtomic(tmpPath(\"foo.txt2\"), \"foo2\", 0444);\n\n  auto expectedPaths = set<string>{\"foo.txt\", \"bar.txt\", \"foo_txt\", \"foo.txt2\"};\n  EXPECT_EQ(expectedPaths, listTmpDir());\n  EXPECT_EQ(\"foo\", readData(tmpPath(\"foo.txt\")));\n  EXPECT_EQ(\"bar\", readData(tmpPath(\"bar.txt\")));\n  EXPECT_EQ(\"underscore\", readData(tmpPath(\"foo_txt\")));\n  EXPECT_EQ(\"foo2\", readData(tmpPath(\"foo.txt2\")));\n  EXPECT_EQ(0644, getPerms(tmpPath(\"foo.txt\")));\n  EXPECT_EQ(0400, getPerms(tmpPath(\"bar.txt\")));\n  EXPECT_EQ(0440, getPerms(tmpPath(\"foo_txt\")));\n  EXPECT_EQ(0444, getPerms(tmpPath(\"foo.txt2\")));\n}\n}}  // namespaces\n\n#if defined(__linux__)\nnamespace {\n/**\n * A helper class that forces our fchmod() wrapper to fail when\n * an FChmodFailure object exists.\n */\nclass FChmodFailure {\n public:\n  FChmodFailure() {\n    ++forceFailure_;\n  }\n  ~FChmodFailure() {\n    --forceFailure_;\n  }\n\n  static bool shouldFail() {\n    return forceFailure_.load() > 0;\n  }\n\n private:\n  static std::atomic<int> forceFailure_;\n};\n\nstd::atomic<int> FChmodFailure::forceFailure_{0};\n}\n\n// Replace the system fchmod() function with our own stub, so we can\n// trigger failures in the writeFileAtomic() tests.\nint fchmod(int fd, mode_t mode) {\n  static const auto realFunction =\n      reinterpret_cast<int (*)(int, mode_t)>(dlsym(RTLD_NEXT, \"fchmod\"));\n  // For sanity, make sure we didn't find ourself,\n  // since that would cause infinite recursion.\n  CHECK_NE(realFunction, fchmod);\n\n  if (FChmodFailure::shouldFail()) {\n    errno = EINVAL;\n    return -1;\n  }\n  return realFunction(fd, mode);\n}\n\nnamespace folly {\nnamespace test {\nTEST_F(WriteFileAtomic, chmodFailure) {\n  auto path = tmpPath(\"foo\");\n\n  // Use our stubbed out fchmod() function to force a failure when setting up\n  // the temporary file.\n  //\n  // First try when creating the file for the first time.\n  {\n    FChmodFailure fail;\n    EXPECT_THROW(writeFileAtomic(path, \"foobar\"), std::system_error);\n  }\n  EXPECT_EQ(set<string>{}, listTmpDir());\n\n  // Now create a file normally so we can overwrite it\n  auto contents = StringPiece(\"regular perms\");\n  writeFileAtomic(path, contents, 0600);\n  EXPECT_EQ(contents, readData(path));\n  EXPECT_EQ(0600, getPerms(path));\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n\n  // Now try overwriting the file when forcing fchmod to fail\n  {\n    FChmodFailure fail;\n    EXPECT_THROW(writeFileAtomic(path, \"overwrite\"), std::system_error);\n  }\n  // The file should be unchanged\n  EXPECT_EQ(contents, readData(path));\n  EXPECT_EQ(0600, getPerms(path));\n  EXPECT_EQ(set<string>{\"foo\"}, listTmpDir());\n}\n}\n}\n#endif\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-folly-2017.06.05.00-ursbfg4eq25y3oacejjbxzt5g7k6tye4/spack-src/folly/docs/Fbvector--graphical_solutions.png"
    ],
    "total_files": 1010
}