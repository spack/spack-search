{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/src/hmem_cuda.c": "/*\n * (C) Copyright 2020 Hewlett Packard Enterprise Development LP\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#if HAVE_CONFIG_H\n#include <config.h>\n#endif\n\n#include \"ofi_hmem.h\"\n#include \"ofi.h\"\n\n#if HAVE_LIBCUDA\n\n#include <cuda.h>\n#include <cuda_runtime.h>\n\nstruct cuda_ops {\n\tcudaError_t (*cudaMemcpy)(void *dst, const void *src, size_t count,\n\t\t\t\t  enum cudaMemcpyKind kind);\n\tconst char *(*cudaGetErrorName)(cudaError_t error);\n\tconst char *(*cudaGetErrorString)(cudaError_t error);\n\tCUresult (*cuPointerGetAttribute)(void *data,\n\t\t\t\t\t  CUpointer_attribute attribute,\n\t\t\t\t\t  CUdeviceptr ptr);\n\tcudaError_t (*cudaHostRegister)(void *ptr, size_t size,\n\t\t\t\t\tunsigned int flags);\n\tcudaError_t (*cudaHostUnregister)(void *ptr);\n\tcudaError_t (*cudaGetDeviceCount)(int *count);\n};\n\nstatic int hmem_cuda_use_gdrcopy;\n\n#ifdef ENABLE_CUDA_DLOPEN\n\n#include <dlfcn.h>\n\nstatic void *cudart_handle;\nstatic void *cuda_handle;\nstatic struct cuda_ops cuda_ops;\n\n#else\n\nstatic struct cuda_ops cuda_ops = {\n\t.cudaMemcpy = cudaMemcpy,\n\t.cudaGetErrorName = cudaGetErrorName,\n\t.cudaGetErrorString = cudaGetErrorString,\n\t.cuPointerGetAttribute = cuPointerGetAttribute,\n\t.cudaHostRegister = cudaHostRegister,\n\t.cudaHostUnregister = cudaHostUnregister,\n\t.cudaGetDeviceCount = cudaGetDeviceCount,\n};\n\n#endif /* ENABLE_CUDA_DLOPEN */\n\ncudaError_t ofi_cudaMemcpy(void *dst, const void *src, size_t count,\n\t\t\t   enum cudaMemcpyKind kind)\n{\n\treturn cuda_ops.cudaMemcpy(dst, src, count, kind);\n}\n\nconst char *ofi_cudaGetErrorName(cudaError_t error)\n{\n\treturn cuda_ops.cudaGetErrorName(error);\n}\n\nconst char *ofi_cudaGetErrorString(cudaError_t error)\n{\n\treturn cuda_ops.cudaGetErrorString(error);\n}\n\nCUresult ofi_cuPointerGetAttribute(void *data, CUpointer_attribute attribute,\n\t\t\t\t   CUdeviceptr ptr)\n{\n\treturn cuda_ops.cuPointerGetAttribute(data, attribute, ptr);\n}\n\ncudaError_t ofi_cudaHostRegister(void *ptr, size_t size, unsigned int flags)\n{\n\treturn cuda_ops.cudaHostRegister(ptr, size, flags);\n}\n\ncudaError_t ofi_cudaHostUnregister(void *ptr)\n{\n\treturn cuda_ops.cudaHostUnregister(ptr);\n}\n\nstatic cudaError_t ofi_cudaGetDeviceCount(int *count)\n{\n\treturn cuda_ops.cudaGetDeviceCount(count);\n}\n\nint cuda_copy_to_dev(uint64_t device, void *dev, const void *host, size_t size)\n{\n\tif (hmem_cuda_use_gdrcopy) {\n\t\tcuda_gdrcopy_to_dev(device, dev, host, size);\n\t\treturn FI_SUCCESS;\n\t}\n\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = ofi_cudaMemcpy(dev, host, size, cudaMemcpyHostToDevice);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn 0;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform cudaMemcpy: %s:%s\\n\",\n\t\tofi_cudaGetErrorName(cuda_ret),\n\t\tofi_cudaGetErrorString(cuda_ret));\n\n\treturn -FI_EIO;\n}\n\nint cuda_copy_from_dev(uint64_t device, void *host, const void *dev, size_t size)\n{\n\tif (hmem_cuda_use_gdrcopy) {\n\t\tcuda_gdrcopy_from_dev(device, host, dev, size);\n\t\treturn FI_SUCCESS;\n\t}\n\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = ofi_cudaMemcpy(host, dev, size, cudaMemcpyDeviceToHost);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn 0;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform cudaMemcpy: %s:%s\\n\",\n\t\tofi_cudaGetErrorName(cuda_ret),\n\t\tofi_cudaGetErrorString(cuda_ret));\n\n\treturn -FI_EIO;\n}\n\nint cuda_dev_register(struct fi_mr_attr *mr_attr, uint64_t *handle)\n{\n\tif (hmem_cuda_use_gdrcopy)\n\t\treturn cuda_gdrcopy_dev_register(mr_attr, handle);\n\n\t*handle = mr_attr->device.cuda;\n\treturn FI_SUCCESS;\n}\n\nint cuda_dev_unregister(uint64_t handle)\n{\n\tif (hmem_cuda_use_gdrcopy)\n\t\treturn cuda_gdrcopy_dev_unregister(handle);\n\n\treturn FI_SUCCESS;\n}\n\nstatic int cuda_hmem_dl_init(void)\n{\n#ifdef ENABLE_CUDA_DLOPEN\n\t/* Assume failure to dlopen CUDA runtime is caused by the library not\n\t * being found. Thus, CUDA is not supported.\n\t */\n\tcudart_handle = dlopen(\"libcudart.so\", RTLD_NOW);\n\tif (!cudart_handle) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to dlopen libcudart.so\\n\");\n\t\treturn -FI_ENOSYS;\n\t}\n\n\tcuda_handle = dlopen(\"libcuda.so\", RTLD_NOW);\n\tif (!cuda_handle) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to dlopen libcuda.so\\n\");\n\t\tgoto err_dlclose_cudart;\n\t}\n\n\tcuda_ops.cudaMemcpy = dlsym(cudart_handle, \"cudaMemcpy\");\n\tif (!cuda_ops.cudaMemcpy) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"Failed to find cudaMemcpy\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaGetErrorName = dlsym(cudart_handle, \"cudaGetErrorName\");\n\tif (!cuda_ops.cudaGetErrorName) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find cudaGetErrorName\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaGetErrorString = dlsym(cudart_handle,\n\t\t\t\t\t    \"cudaGetErrorString\");\n\tif (!cuda_ops.cudaGetErrorString) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find cudaGetErrorString\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cuPointerGetAttribute = dlsym(cuda_handle,\n\t\t\t\t\t       \"cuPointerGetAttribute\");\n\tif (!cuda_ops.cuPointerGetAttribute) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find cuPointerGetAttribute\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaHostRegister = dlsym(cudart_handle, \"cudaHostRegister\");\n\tif (!cuda_ops.cudaHostRegister) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find cudaHostRegister\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaHostUnregister = dlsym(cudart_handle,\n\t\t\t\t\t    \"cudaHostUnregister\");\n\tif (!cuda_ops.cudaHostUnregister) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find cudaHostUnregister\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaGetDeviceCount = dlsym(cudart_handle,\n\t\t\t\t\t    \"cudaGetDeviceCount\");\n\tif (!cuda_ops.cudaGetDeviceCount) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find cudaGetDeviceCount\\n\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\treturn FI_SUCCESS;\n\nerr_dlclose_cuda:\n\tdlclose(cuda_handle);\nerr_dlclose_cudart:\n\tdlclose(cudart_handle);\n\n\treturn -FI_ENODATA;\n#else\n\treturn FI_SUCCESS;\n#endif /* ENABLE_CUDA_DLOPEN */\n}\n\nstatic void cuda_hmem_dl_cleanup(void)\n{\n#ifdef ENABLE_CUDA_DLOPEN\n\tdlclose(cuda_handle);\n\tdlclose(cudart_handle);\n#endif\n}\n\nstatic int cuda_hmem_verify_devices(void)\n{\n\tint device_count;\n\tcudaError_t cuda_ret;\n\n\t/* Verify CUDA compute-capable devices are present on the host. */\n\tcuda_ret = ofi_cudaGetDeviceCount(&device_count);\n\tswitch (cuda_ret) {\n\tcase cudaSuccess:\n\t\tbreak;\n\n\tcase cudaErrorNoDevice:\n\t\treturn -FI_ENOSYS;\n\n\tdefault:\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to perform cudaGetDeviceCount: %s:%s\\n\",\n\t\t\tofi_cudaGetErrorName(cuda_ret),\n\t\t\tofi_cudaGetErrorString(cuda_ret));\n\t\treturn -FI_EIO;\n\t}\n\n\tif (device_count == 0)\n\t\treturn -FI_ENOSYS;\n\n\treturn FI_SUCCESS;\n}\n\nint cuda_hmem_init(void)\n{\n\tint ret;\n\n\tret = cuda_hmem_dl_init();\n\tif (ret != FI_SUCCESS)\n\t\treturn ret;\n\n\tret = cuda_hmem_verify_devices();\n\tif (ret != FI_SUCCESS)\n\t\tgoto dl_cleanup;\n\n\tret = cuda_gdrcopy_hmem_init();\n\tif (ret == FI_SUCCESS) {\n\t\thmem_cuda_use_gdrcopy = 1;\n\t\tfi_param_define(NULL, \"hmem_cuda_use_gdrcopy\", FI_PARAM_BOOL,\n\t\t\t\t\"Use gdrcopy to copy data to/from GPU memory\");\n\t\tfi_param_get_bool(NULL, \"hmem_cuda_use_gdrcopy\",\n\t\t\t\t  &hmem_cuda_use_gdrcopy);\n\t} else {\n\t\thmem_cuda_use_gdrcopy = 0;\n\t\tif (ret != -FI_ENOSYS)\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"gdrcopy initialization failed! gdrcopy will not be used.\\n\");\n\t}\n\n\treturn ret;\n\ndl_cleanup:\n\tcuda_hmem_dl_cleanup();\n\n\treturn ret;\n}\n\nint cuda_hmem_cleanup(void)\n{\n\tcuda_hmem_dl_cleanup();\n\tcuda_gdrcopy_hmem_cleanup();\n\treturn FI_SUCCESS;\n}\n\nbool cuda_is_addr_valid(const void *addr)\n{\n\tCUresult cuda_ret;\n\tunsigned int data;\n\n\tcuda_ret = ofi_cuPointerGetAttribute(&data,\n\t\t\t\t\t     CU_POINTER_ATTRIBUTE_MEMORY_TYPE,\n\t\t\t\t\t     (CUdeviceptr)addr);\n\tswitch (cuda_ret) {\n\tcase CUDA_SUCCESS:\n\t\tif (data == CU_MEMORYTYPE_DEVICE)\n\t\t\treturn true;\n\t\tbreak;\n\n\t/* Returned if the buffer is not associated with the CUcontext support\n\t * unified virtual addressing. Since host buffers may fall into this\n\t * category, this is not treated as an error.\n\t */\n\tcase CUDA_ERROR_INVALID_VALUE:\n\t\tbreak;\n\n\t/* Returned if cuInit() has not been called. This can happen if support\n\t * for CUDA is enabled but the user has not made a CUDA call. This is\n\t * not treated as an error.\n\t */\n\tcase CUDA_ERROR_NOT_INITIALIZED:\n\t\tbreak;\n\n\t/* Returned if the CUcontext does not support unified virtual\n\t * addressing.\n\t */\n\tcase CUDA_ERROR_INVALID_CONTEXT:\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"CUcontext does not support unified virtual addressining\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Unhandle cuPointerGetAttribute return code: ret=%d\\n\",\n\t\t\tcuda_ret);\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nint cuda_host_register(void *ptr, size_t size)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = ofi_cudaHostRegister(ptr, size, cudaHostRegisterDefault);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform cudaMemcpy: %s:%s\\n\",\n\t\tofi_cudaGetErrorName(cuda_ret),\n\t\tofi_cudaGetErrorString(cuda_ret));\n\n\treturn -FI_EIO;\n}\n\nint cuda_host_unregister(void *ptr)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = ofi_cudaHostUnregister(ptr);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform cudaMemcpy: %s:%s\\n\",\n\t\tofi_cudaGetErrorName(cuda_ret),\n\t\tofi_cudaGetErrorString(cuda_ret));\n\n\treturn -FI_EIO;\n}\n\n#else\n\nint cuda_copy_to_dev(uint64_t device, void *dev, const void *host, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint cuda_copy_from_dev(uint64_t device, void *host, const void *dev, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint cuda_hmem_init(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint cuda_hmem_cleanup(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nbool cuda_is_addr_valid(const void *addr)\n{\n\treturn false;\n}\n\nint cuda_host_register(void *ptr, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint cuda_host_unregister(void *ptr)\n{\n\treturn -FI_ENOSYS;\n}\n\nint cuda_dev_register(struct fi_mr_attr *mr_attr, uint64_t *handle)\n{\n\treturn FI_SUCCESS;\n}\n\nint cuda_dev_unregister(uint64_t handle)\n{\n\treturn FI_SUCCESS;\n}\n\n#endif /* HAVE_LIBCUDA */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/src/hmem_cuda_gdrcopy.c": "/*\n * Copyright (c) 2020 Amazon.com, Inc. or its affiliates.\n * All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#if HAVE_CONFIG_H\n#include <config.h>\n#endif\n\n#include \"ofi_hmem.h\"\n#include \"ofi.h\"\n\n#ifdef HAVE_GDRCOPY\n\n#include <pthread.h>\n#include <gdrapi.h>\n\nstruct gdrcopy_handle {\n\tgdr_mh_t mh; /* memory handler */\n\tvoid *cuda_ptr; /* page aligned gpu pointer */\n\tvoid *user_ptr; /* user space ptr mapped to GPU memory */\n\tsize_t length; /* page aligned length */\n};\n\nstruct gdrcopy_ops {\n\tgdr_t (*gdr_open)();\n\tint (*gdr_close)(gdr_t g);\n\tint (*gdr_pin_buffer)(gdr_t g, unsigned long addr, size_t size,\n\t\t\t      uint64_t p2p_token, uint32_t va_space,\n\t\t\t      gdr_mh_t *handle);\n\tint (*gdr_unpin_buffer)(gdr_t g, gdr_mh_t handle);\n\tint (*gdr_map)(gdr_t g, gdr_mh_t handle, void **va, size_t size);\n\tint (*gdr_unmap)(gdr_t g, gdr_mh_t handle, void *va, size_t size);\n\tint (*gdr_copy_to_mapping)(gdr_mh_t handle, void *map_d_ptr,\n\t\t\t\t   const void *h_ptr, size_t size);\n\tint (*gdr_copy_from_mapping)(gdr_mh_t handle, void *map_d_ptr,\n\t\t\t\t     const void *h_ptr, size_t size);\n};\n\nenum gdrcopy_dir {\n\tGDRCOPY_TO_DEVICE,\n\tGDRCOPY_FROM_DEVICE,\n};\n\nstatic gdr_t global_gdr;\nstatic pthread_spinlock_t global_gdr_lock;\n\n#ifdef ENABLE_GDRCOPY_DLOPEN\n\n#include <dlfcn.h>\n\nstatic void *gdrapi_handle;\nstatic struct gdrcopy_ops global_gdrcopy_ops;\n\nstatic int cuda_gdrcopy_dl_hmem_init(void)\n{\n\tgdrapi_handle = dlopen(\"libgdrapi.so\", RTLD_NOW);\n\tif (!gdrapi_handle) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to dlopen libgdrapi.so\\n\");\n\t\treturn -FI_ENOSYS;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_open = dlsym(gdrapi_handle, \"gdr_open\");\n\tif (!global_gdrcopy_ops.gdr_open) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"Failed to find gdr_open\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_close = dlsym(gdrapi_handle, \"gdr_close\");\n\tif (!global_gdrcopy_ops.gdr_close) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"Failed to find gdr_close\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_pin_buffer = dlsym(gdrapi_handle, \"gdr_pin_buffer\");\n\tif (!global_gdrcopy_ops.gdr_pin_buffer) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find gdr_pin_buffer\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_unpin_buffer = dlsym(gdrapi_handle, \"gdr_unpin_buffer\");\n\tif (!global_gdrcopy_ops.gdr_unpin_buffer) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find gdr_unpin_buffer\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_map = dlsym(gdrapi_handle, \"gdr_map\");\n\tif (!global_gdrcopy_ops.gdr_map) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find gdr_map\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_unmap = dlsym(gdrapi_handle, \"gdr_unmap\");\n\tif (!global_gdrcopy_ops.gdr_unmap) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find gdr_unmap\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_copy_to_mapping = dlsym(gdrapi_handle, \"gdr_copy_to_mapping\");\n\tif (!global_gdrcopy_ops.gdr_copy_to_mapping) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find gdr_copy_to_mapping\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\tglobal_gdrcopy_ops.gdr_copy_from_mapping = dlsym(gdrapi_handle, \"gdr_copy_from_mapping\");\n\tif (!global_gdrcopy_ops.gdr_copy_from_mapping) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find gdr_copy_from_mapping\\n\");\n\t\tgoto err_dlclose_gdrapi;\n\t}\n\n\treturn FI_SUCCESS;\n\nerr_dlclose_gdrapi:\n\tmemset(&global_gdrcopy_ops, 0, sizeof(global_gdrcopy_ops));\n\tdlclose(gdrapi_handle);\n\treturn -FI_ENODATA;\n}\n\nstatic int cuda_gdrcopy_dl_hmem_cleanup(void)\n{\n\tdlclose(gdrapi_handle);\n\treturn FI_SUCCESS;\n}\n\n#else\n\nstatic struct gdrcopy_ops global_gdrcopy_ops = {\n\t.gdr_open = gdr_open,\n\t.gdr_close = gdr_close,\n\t.gdr_pin_buffer = gdr_pin_buffer,\n\t.gdr_unpin_buffer = gdr_unpin_buffer,\n\t.gdr_map = gdr_map,\n\t.gdr_unmap = gdr_unmap,\n\t.gdr_copy_to_mapping = gdr_copy_to_mapping,\n\t.gdr_copy_from_mapping = gdr_copy_from_mapping\n};\n\nstatic int cuda_gdrcopy_dl_hmem_init(void)\n{\n\treturn FI_SUCCESS;\n}\n\nstatic int cuda_gdrcopy_dl_hmem_cleanup(void)\n{\n\treturn FI_SUCCESS;\n}\n\n#endif /* ENABLE_CUDA_DLOPEN */\n\nint cuda_gdrcopy_hmem_init(void)\n{\n\tint err, ret = 0;\n\n\terr = cuda_gdrcopy_dl_hmem_init();\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"gdrcopy_dl_hmem_init failed!\\n\");\n\t\treturn -FI_ENOSYS;\n\t}\n\n\tassert(global_gdrcopy_ops.gdr_open);\n\n\tglobal_gdr = global_gdrcopy_ops.gdr_open();\n\tif (!global_gdr) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"gdr_open failed!\\n\");\n\t\tret = -FI_ENOMEM;\n\t\tgoto exit;\n\t}\n\n\terr = pthread_spin_init(&global_gdr_lock, 0);\n\tif (err) {\n\t\tassert(global_gdrcopy_ops.gdr_close);\n\t\tglobal_gdrcopy_ops.gdr_close(global_gdr);\n\t\tret = -err;\n\t}\n\nexit:\n\tcuda_gdrcopy_dl_hmem_cleanup();\n\treturn ret;\n}\n\nint cuda_gdrcopy_hmem_cleanup(void)\n{\n\tint err, ret = 0;\n\n\terr = pthread_spin_destroy(&global_gdr_lock);\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"destroy global_gdr_lock failed! err: %s\\n\",\n\t\t\tstrerror(err));\n\t\tret = err;\n\t}\n\n\tassert(global_gdrcopy_ops.gdr_close);\n\terr = global_gdrcopy_ops.gdr_close(global_gdr);\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"close global_gdr failed! err: %s\\n\",\n\t\t\tstrerror(err));\n\t\tret = err;\n\t}\n\n\terr = cuda_gdrcopy_dl_hmem_cleanup();\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"cuda_gdrcopy_dl_hmem_cleaup() failed! err: %s\\n\",\n\t\t\tstrerror(err));\n\t\tret = err;\n\t}\n\n\treturn ret;\n}\n\nvoid cuda_gdrcopy_impl(uint64_t handle, void *devptr,\n\t\t       void *hostptr, size_t len,\n\t\t       enum gdrcopy_dir dir)\n{\n\tssize_t off;\n\tstruct gdrcopy_handle *gdrcopy;\n\tvoid *gdrcopy_user_ptr;\n\n\tassert(global_gdrcopy_ops.gdr_copy_to_mapping);\n\tassert(handle);\n\n\tgdrcopy = (struct gdrcopy_handle *)handle;\n\toff = (char *)devptr - (char *)gdrcopy->cuda_ptr;\n\tassert(off >= 0 && off + len <= gdrcopy->length);\n\tgdrcopy_user_ptr = (char *)gdrcopy->user_ptr + off;\n\tif (dir == GDRCOPY_TO_DEVICE) {\n\t\tglobal_gdrcopy_ops.gdr_copy_to_mapping(gdrcopy->mh,\n\t\t\t\t\t\t       gdrcopy_user_ptr,\n\t\t\t\t\t\t       hostptr, len);\n\t} else {\n\t\tassert(dir == GDRCOPY_FROM_DEVICE);\n\t\tglobal_gdrcopy_ops.gdr_copy_from_mapping(gdrcopy->mh,\n\t\t\t\t\t\t\t gdrcopy_user_ptr,\n\t\t\t\t\t\t\t hostptr, len);\n\t}\n}\n\nvoid cuda_gdrcopy_to_dev(uint64_t handle, void *devptr,\n\t\t\t const void *hostptr, size_t len)\n{\n\tcuda_gdrcopy_impl(handle, devptr, (void *)hostptr, len,\n\t\t\t  GDRCOPY_TO_DEVICE);\n}\n\nvoid cuda_gdrcopy_from_dev(uint64_t handle, void *hostptr,\n\t\t\t   const void *devptr, size_t len)\n{\n\tcuda_gdrcopy_impl(handle, (void *)devptr, hostptr, len,\n\t\t\t  GDRCOPY_FROM_DEVICE);\n}\n\nint cuda_gdrcopy_dev_register(struct fi_mr_attr *mr_attr, uint64_t *handle)\n{\n\tint err;\n\tuintptr_t regbgn, regend;\n\tsize_t reglen;\n\tstruct gdrcopy_handle *gdrcopy;\n\n\tassert(global_gdr);\n\tassert(global_gdrcopy_ops.gdr_pin_buffer);\n\tassert(global_gdrcopy_ops.gdr_map);\n\n\tregbgn = (uintptr_t)mr_attr->mr_iov->iov_base;\n\tregend = (uintptr_t)mr_attr->mr_iov->iov_base + mr_attr->mr_iov->iov_len;\n\tregbgn = regbgn & GPU_PAGE_MASK;\n\tregend = (regend & GPU_PAGE_MASK) + GPU_PAGE_SIZE;\n\treglen = regend - regbgn;\n\n\tgdrcopy = malloc(sizeof(struct gdrcopy_handle));\n\tif (!gdrcopy)\n\t\treturn -FI_ENOMEM;\n\n\tassert(global_gdr);\n\tpthread_spin_lock(&global_gdr_lock);\n\terr = global_gdrcopy_ops.gdr_pin_buffer(global_gdr, regbgn,\n\t\t\t\t\t reglen, 0, 0, &gdrcopy->mh);\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"gdr_pin_buffer failed! error: %s\",\n\t\t\tstrerror(err));\n\t\tfree(gdrcopy);\n\t\tgoto exit;\n\t}\n\n\tgdrcopy->cuda_ptr = (void *)regbgn;\n\tgdrcopy->length = reglen;\n\n\terr = global_gdrcopy_ops.gdr_map(global_gdr, gdrcopy->mh,\n\t\t\t\t\t &gdrcopy->user_ptr, gdrcopy->length);\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"gdr_map failed! error: %s\\n\",\n\t\t\tstrerror(err));\n\t\tglobal_gdrcopy_ops.gdr_unpin_buffer(global_gdr, gdrcopy->mh);\n\t\tfree(gdrcopy);\n\t\tgoto exit;\n\t}\n\n\t*handle = (uint64_t)gdrcopy;\nexit:\n\tpthread_spin_unlock(&global_gdr_lock);\n\treturn err;\n}\n\nint cuda_gdrcopy_dev_unregister(uint64_t handle)\n{\n\tint err;\n\tstruct gdrcopy_handle *gdrcopy;\n\n\tassert(global_gdr);\n\tassert(global_gdrcopy_ops.gdr_unmap);\n\tassert(global_gdrcopy_ops.gdr_unpin_buffer);\n\n\tgdrcopy = (struct gdrcopy_handle *)handle;\n\tassert(gdrcopy);\n\n\tpthread_spin_lock(&global_gdr_lock);\n\terr = global_gdrcopy_ops.gdr_unmap(global_gdr, gdrcopy->mh,\n\t\t\t\t\t   gdrcopy->user_ptr, gdrcopy->length);\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"gdr_unmap failed! error: %s\\n\",\n\t\t\tstrerror(err));\n\t\tgoto exit;\n\t}\n\n\terr = global_gdrcopy_ops.gdr_unpin_buffer(global_gdr, gdrcopy->mh);\n\tif (err) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t\t\"gdr_unmap failed! error: %s\\n\",\n\t\t\tstrerror(err));\n\t\tgoto exit;\n\t}\n\nexit:\n\tpthread_spin_unlock(&global_gdr_lock);\n\tfree(gdrcopy);\n\treturn err;\n}\n\n#else\n\nint cuda_gdrcopy_hmem_init(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint cuda_gdrcopy_hmem_cleanup(void)\n{\n\treturn FI_SUCCESS;\n}\n\nvoid cuda_gdrcopy_to_dev(uint64_t devhandle, void *devptr,\n\t\t\t const void *hostptr, size_t len)\n{\n}\n\nvoid cuda_gdrcopy_from_dev(uint64_t devhandle, void *hostptr,\n\t\t\t   const void *devptr, size_t len)\n{\n}\n\nint cuda_gdrcopy_dev_register(struct fi_mr_attr *mr_attr, uint64_t *handle)\n{\n\treturn FI_SUCCESS;\n}\n\nint cuda_gdrcopy_dev_unregister(uint64_t handle)\n{\n\treturn FI_SUCCESS;\n}\n\n#endif /* HAVE_GDRCOPY */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/src/fabric.c": "/*\n * Copyright (c) 2004, 2005 Topspin Communications.  All rights reserved.\n * Copyright (c) 2006-2016 Cisco Systems, Inc.  All rights reserved.\n * Copyright (c) 2013-2017 Intel Corp., Inc.  All rights reserved.\n * (C) Copyright 2020 Hewlett Packard Enterprise Development LP\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include \"config.h\"\n\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <dirent.h>\n#include <ctype.h>\n\n#include <rdma/fi_errno.h>\n#include \"ofi_util.h\"\n#include \"ofi.h\"\n#include \"shared/ofi_str.h\"\n#include \"ofi_prov.h\"\n#include \"ofi_perf.h\"\n#include \"ofi_hmem.h\"\n\n#ifdef HAVE_LIBDL\n#include <dlfcn.h>\n#endif\n\nstruct ofi_prov {\n\tstruct ofi_prov\t\t*next;\n\tchar\t\t\t*prov_name;\n\tstruct fi_provider\t*provider;\n\tvoid\t\t\t*dlhandle;\n\tbool\t\t\thidden;\n};\n\nstatic struct ofi_prov *prov_head, *prov_tail;\nint ofi_init = 0;\nextern struct ofi_common_locks common_locks;\n\nstatic struct fi_filter prov_filter;\n\nstatic int ofi_find_name(char **names, const char *name)\n{\n\tint i;\n\n\tfor (i = 0; names[i]; i++) {\n\t\tif (!strcasecmp(name, names[i]))\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\n\n/* matches if names[i] == \"xxx;yyy\" and name == \"xxx\" */\nstatic int ofi_find_layered_name(char **names, const char *name)\n{\n\tint i, len;\n\n\tlen = strlen(name);\n\tfor (i = 0; names[i]; i++) {\n\t\tif (!strncasecmp(name, names[i], len) && names[i][len] == ';' )\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\n\n/* matches if names[i] == \"xxx\" and name == \"xxx;yyy\" */\nstatic int ofi_find_core_name(char **names, const char *name)\n{\n\tint i, len;\n\n\tfor (i = 0; names[i]; i++) {\n\t\tlen = strlen(names[i]);\n\t\tif (!strncasecmp(name, names[i], len) && name[len] == ';' )\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\n\nstatic void ofi_closest_prov_names(char *prov_name, char* miss_prov_name, int n)\n{\n\tif (strncasecmp( prov_name, miss_prov_name, n ) == 0 ) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Instead misspelled provider: %s, you may want: %s?\\n\",\n\t\t\tmiss_prov_name, prov_name);\n\t}\n}\n\nstatic void ofi_suggest_prov_names(char *name_to_match)\n{\n\tstruct ofi_prov *prov;\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif (strlen(prov->prov_name) != strlen(name_to_match)\n\t\t    && !strncasecmp(prov->prov_name, name_to_match,\n\t\t\t\t    strlen(name_to_match))) {\n\t\t\tif (strlen(name_to_match) > 5)\n\t\t\t\tofi_closest_prov_names(prov->prov_name,\n\t\t\t\t\t\t       name_to_match, 5);\n\t\t\telse\n\t\t\t\tofi_closest_prov_names(prov->prov_name,\n\t\t\t\t\t\t       name_to_match, 2);\n\t\t}\n\t}\n}\n\nstatic enum ofi_prov_type ofi_prov_type(const struct fi_provider *provider)\n{\n\tconst struct fi_prov_context *ctx;\n\tctx = (const struct fi_prov_context *) &provider->context;\n\treturn ctx->type;\n}\n\nstatic int ofi_disable_util_layering(const struct fi_provider *provider) {\n\tconst struct fi_prov_context *ctx;\n\n\tctx = (const struct fi_prov_context *) &provider->context;\n\treturn ctx->disable_layering;\n}\n\nstatic int ofi_is_util_prov(const struct fi_provider *provider)\n{\n\treturn ofi_prov_type(provider) == OFI_PROV_UTIL;\n}\n\nstatic int ofi_is_core_prov(const struct fi_provider *provider)\n{\n\treturn ofi_prov_type(provider) == OFI_PROV_CORE;\n}\n\nstatic int ofi_is_hook_prov(const struct fi_provider *provider)\n{\n\treturn ofi_prov_type(provider) == OFI_PROV_HOOK;\n}\n\nint ofi_apply_filter(struct fi_filter *filter, const char *name)\n{\n\tif (!filter->names)\n\t\treturn 0;\n\n\tif (ofi_find_name(filter->names, name) >= 0)\n\t\treturn filter->negated ? 1 : 0;\n\n\treturn filter->negated ? 0 : 1;\n}\n\n/*\n * The provider init filter is used to filter out unnecessary core providers\n * at the initialization time. Utility providers are not concerned.\n *\n * Special handling is needed for layered provider names:\n *\n * If the filter is not negated, a name \"xxx;yyy\" in the filter should match\n * input \"xxx\" to ensure that the core provider \"xxx\" is included.\n *\n * If the filter is negated, a name \"xxx;yyy\" in the filter should not match\n * input \"xxx\" otherwise the core provider \"xxx\" may be incorrectly filtered\n * out.\n */\nint ofi_apply_prov_init_filter(struct fi_filter *filter, const char *name)\n{\n\tif (!filter->names)\n\t\treturn 0;\n\n\tif (ofi_find_name(filter->names, name) >= 0)\n\t\treturn filter->negated ? 1 : 0;\n\n\tif (filter->negated)\n\t\treturn 0;\n\n\tif (ofi_find_layered_name(filter->names, name) >= 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * The provider post filter is used to remove unwanted entries from the fi_info\n * list before returning from fi_getinfo().\n *\n * Layered provider names are handled in the same way as non-layered provider\n * names -- requiring full match.\n *\n * In addition, a name \"xxx\" in the filter should be able to match an input\n * \"xxx;yyy\" to allow extra layering on top of what is requested by the user.\n */\nint ofi_apply_prov_post_filter(struct fi_filter *filter, const char *name)\n{\n\tif (!filter->names)\n\t\treturn 0;\n\n\tif (ofi_find_name(filter->names, name) >= 0 ||\n\t    ofi_find_core_name(filter->names, name) >= 0)\n\t\treturn filter->negated ? 1 : 0;\n\n\treturn filter->negated ? 0 : 1;\n}\n\nstatic int ofi_getinfo_filter(const struct fi_provider *provider)\n{\n\t/* Positive filters only apply to core providers.  They must be\n\t * explicitly enabled by the filter.  Other providers (i.e. utility)\n\t * are automatically enabled in this case, so that they can work\n\t * over any enabled core filter.  Negative filters may be used\n\t * to disable any provider.\n\t */\n\tif (!prov_filter.negated && !ofi_is_core_prov(provider))\n\t\treturn 0;\n\n\treturn ofi_apply_prov_init_filter(&prov_filter, provider->name);\n}\n\nstatic void ofi_filter_info(struct fi_info **info)\n{\n\tstruct fi_info *cur, *prev, *tmp;\n\n\tif (!prov_filter.names)\n\t\treturn;\n\n\tprev = NULL;\n\tcur = *info;\n\twhile (cur) {\n\t\tassert(cur->fabric_attr && cur->fabric_attr->prov_name);\n\n\t\tif (ofi_apply_prov_post_filter(&prov_filter, cur->fabric_attr->prov_name)) {\n\t\t\ttmp = cur;\n\t\t\tcur = cur->next;\n\t\t\tif (prev)\n\t\t\t\tprev->next = cur;\n\t\t\telse\n\t\t\t\t*info = cur;\n\t\t\ttmp->next = NULL;\n\t\t\tfi_freeinfo(tmp);\n\t\t} else {\n\t\t\tprev = cur;\n\t\t\tcur = cur->next;\n\t\t}\n\t}\n}\n\nstatic struct ofi_prov *ofi_getprov(const char *prov_name, size_t len)\n{\n\tstruct ofi_prov *prov;\n\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif ((strlen(prov->prov_name) == len) &&\n\t\t    !strncasecmp(prov->prov_name, prov_name, len))\n\t\t\treturn prov;\n\t}\n\n\treturn NULL;\n}\n\nstruct fi_provider *ofi_get_hook(const char *name)\n{\n\tstruct ofi_prov *prov;\n\tstruct fi_provider *provider = NULL;\n\tchar *try_name = NULL;\n\tint ret;\n\n\tprov = ofi_getprov(name, strlen(name));\n\tif (!prov) {\n\t\tret = asprintf(&try_name, \"ofi_hook_%s\", name);\n\t\tif (ret > 0)\n\t\t\tprov = ofi_getprov(try_name, ret);\n\t\telse\n\t\t\ttry_name = NULL;\n\t}\n\n\tif (prov) {\n\t\tif (prov->provider && ofi_is_hook_prov(prov->provider)) {\n\t\t\tprovider = prov->provider;\n\t\t} else {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Specified provider is not a hook: %s\\n\", name);\n\t\t}\n\t} else {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"No hook found for: %s\\n\", name);\n\t}\n\n\tfree(try_name);\n\treturn provider;\n}\n\nstatic void cleanup_provider(struct fi_provider *provider, void *dlhandle)\n{\n\tOFI_UNUSED(dlhandle);\n\n\tif (provider) {\n\t\tfi_param_undefine(provider);\n\n\t\tif (provider->cleanup)\n\t\t\tprovider->cleanup();\n\t}\n\n#ifdef HAVE_LIBDL\n\tif (dlhandle)\n\t\tdlclose(dlhandle);\n#endif\n}\n\nstatic struct ofi_prov *ofi_create_prov_entry(const char *prov_name)\n{\n\tstruct ofi_prov *prov = NULL;\n\tprov = calloc(sizeof *prov, 1);\n\tif (!prov) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Not enough memory to allocate provider registry\\n\");\n\t\treturn NULL;\n\t}\n\n\tprov->prov_name = strdup(prov_name);\n\tif (!prov->prov_name) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to init pre-registered provider name\\n\");\n\t\tfree(prov);\n\t\treturn NULL;\n\t}\n\tif (prov_tail)\n\t\tprov_tail->next = prov;\n\telse\n\t\tprov_head = prov;\n\tprov_tail = prov;\n\n\tprov->hidden = false;\n\n\treturn prov;\n}\n\n/* This is the default order that providers will be reported when a provider\n * is available.  Initialize the socket(s) provider last.  This will result in\n * it being the least preferred provider.\n */\nstatic void ofi_ordered_provs_init(void)\n{\n\tchar *ordered_prov_names[] = {\n\t\t\"psm3\", \"psm2\", \"psm\", \"efa\", \"usnic\", \"gni\", \"bgq\", \"verbs\",\n\t\t\"netdir\", \"ofi_rxm\", \"ofi_rxd\", \"shm\",\n\t\t/* Initialize the socket based providers last of the\n\t\t * standard providers.  This will result in them being\n\t\t * the least preferred providers.\n\t\t */\n\n\t\t/* Before you add ANYTHING here, read the comment above!!! */\n\t\t\"udp\", \"tcp\", \"sockets\", /* NOTHING GOES HERE! */\n\t\t/* Seriously, read it! */\n\n\t\t/* These are hooking providers only.  Their order\n\t\t * doesn't matter\n\t\t */\n\t\t\"ofi_hook_perf\", \"ofi_hook_debug\", \"ofi_hook_noop\",\n\t};\n\tint num_provs = sizeof(ordered_prov_names)/sizeof(ordered_prov_names[0]), i;\n\n\tfor (i = 0; i < num_provs; i++)\n\t\tofi_create_prov_entry(ordered_prov_names[i]);\n}\n\nstatic void ofi_set_prov_type(struct fi_prov_context *ctx,\n\t\t\t      struct fi_provider *provider)\n{\n\tif (!provider->getinfo)\n\t\tctx->type = OFI_PROV_HOOK;\n\telse if (ofi_has_util_prefix(provider->name))\n\t\tctx->type = OFI_PROV_UTIL;\n\telse\n\t\tctx->type = OFI_PROV_CORE;\n}\n\nstatic void ofi_register_provider(struct fi_provider *provider, void *dlhandle)\n{\n\tstruct fi_prov_context *ctx;\n\tstruct ofi_prov *prov = NULL;\n\tbool hidden = false;\n\n\tif (!provider || !provider->name) {\n\t\tFI_DBG(&core_prov, FI_LOG_CORE,\n\t\t       \"no provider structure or name\\n\");\n\t\tgoto cleanup;\n\t}\n\n\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t       \"registering provider: %s (%d.%d)\\n\", provider->name,\n\t       FI_MAJOR(provider->version), FI_MINOR(provider->version));\n\n\tif (!provider->fabric) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"provider missing mandatory entry points\\n\");\n\t\tgoto cleanup;\n\t}\n\n\t/* The current core implementation is not backward compatible\n\t * with providers that support a release earlier than v1.3.\n\t * See commit 0f4b6651.\n\t */\n\tif (provider->fi_version < FI_VERSION(1, 3)) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"provider has unsupported FI version \"\n\t\t\t\"(provider %d.%d != libfabric %d.%d); ignoring\\n\",\n\t\t\tFI_MAJOR(provider->fi_version),\n\t\t\tFI_MINOR(provider->fi_version), FI_MAJOR_VERSION,\n\t\t\tFI_MINOR_VERSION);\n\t\tgoto cleanup;\n\t}\n\n\tctx = (struct fi_prov_context *) &provider->context;\n\tofi_set_prov_type(ctx, provider);\n\n\tif (ofi_getinfo_filter(provider)) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"\\\"%s\\\" filtered by provider include/exclude \"\n\t\t\t\"list, skipping\\n\", provider->name);\n\t\thidden = true;\n\t}\n\n\tif (ofi_apply_filter(&prov_log_filter, provider->name))\n\t\tctx->disable_logging = 1;\n\n\t/*\n\t * Prevent utility providers from layering on these core providers\n\t * unless explicitly requested.\n\t */\n\tif (!strcasecmp(provider->name, \"sockets\") ||\n\t    !strcasecmp(provider->name, \"shm\") ||\n\t    !strcasecmp(provider->name, \"efa\") ||\n\t    !strcasecmp(provider->name, \"psm3\") || ofi_is_util_prov(provider))\n\t\tctx->disable_layering = 1;\n\n\tprov = ofi_getprov(provider->name, strlen(provider->name));\n\tif (prov) {\n\t\t/* If this provider has not been init yet, then we add the\n\t\t * provider and dlhandle to the struct and exit.\n\t\t */\n\t\tif (prov->provider == NULL)\n\t\t\tgoto update_prov_registry;\n\n\t\t/* If this provider is older than an already-loaded\n\t\t * provider of the same name, then discard this one.\n\t\t */\n\t\tif (FI_VERSION_GE(prov->provider->version, provider->version)) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"a newer %s provider was already loaded; \"\n\t\t\t\t\"ignoring this one\\n\", provider->name);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t/* This provider is newer than an already-loaded\n\t\t * provider of the same name, so discard the\n\t\t * already-loaded one.\n\t\t */\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"an older %s provider was already loaded; \"\n\t\t\t\"keeping this one and ignoring the older one\\n\",\n\t\t\tprovider->name);\n\t\tcleanup_provider(prov->provider, prov->dlhandle);\n\t} else {\n\t\tprov = ofi_create_prov_entry(provider->name);\n\t\tif (!prov)\n\t\t\tgoto cleanup;\n\t}\n\n\tif (hidden)\n\t\tprov->hidden = true;\n\nupdate_prov_registry:\n\tprov->dlhandle = dlhandle;\n\tprov->provider = provider;\n\treturn;\n\ncleanup:\n\tcleanup_provider(provider, dlhandle);\n}\n\n#ifdef HAVE_LIBDL\nstatic int lib_filter(const struct dirent *entry)\n{\n\tsize_t l = strlen(entry->d_name);\n\tsize_t sfx = sizeof (FI_LIB_SUFFIX) - 1;\n\n\tif (l > sfx)\n\t\treturn !strcmp(&(entry->d_name[l-sfx]), FI_LIB_SUFFIX);\n\telse\n\t\treturn 0;\n}\n#endif\n\nstatic int verify_filter_names(char **names)\n{\n\tint i, j;\n\tchar** split_names;\n\tfor (i = 0; names[i]; i++) {\n\t\tsplit_names = ofi_split_and_alloc(names[i], \";\", NULL);\n\t\tif (!split_names) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"unable to parse given filter string\\n\");\n\t\t\treturn -FI_ENODATA;\n\t\t}\n\n\t\tfor(j = 0; split_names[j]; j++) {\n\t\t\tif(!ofi_getprov(split_names[j], strlen(split_names[j]))) {\n\t\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\t\"provider %s is unknown, misspelled\"\n\t\t\t\t\t\" or DL provider?\\n\", split_names[j]);\n\t\t\t\tofi_suggest_prov_names(split_names[j]);\n\t\t\t}\n\t\t}\n\t\tofi_free_string_array(split_names);\n\t}\n\n\treturn FI_SUCCESS;\n}\n\nvoid ofi_free_filter(struct fi_filter *filter)\n{\n\tofi_free_string_array(filter->names);\n}\n\nvoid ofi_create_filter(struct fi_filter *filter, const char *raw_filter)\n{\n\tmemset(filter, 0, sizeof *filter);\n\tif (raw_filter == NULL)\n\t\treturn;\n\n\tif (*raw_filter == '^') {\n\t\tfilter->negated = 1;\n\t\t++raw_filter;\n\t}\n\n\tfilter->names = ofi_split_and_alloc(raw_filter, \",\", NULL);\n\tif (!filter->names) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"unable to parse filter from: %s\\n\", raw_filter);\n\t\treturn;\n\t}\n\n\tif (verify_filter_names(filter->names))\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t        \"unable to verify filter name\\n\");\n}\n\n#ifdef HAVE_LIBDL\nstatic void ofi_reg_dl_prov(const char *lib)\n{\n\tvoid *dlhandle;\n\tstruct fi_provider* (*inif)(void);\n\n\tFI_DBG(&core_prov, FI_LOG_CORE, \"opening provider lib %s\\n\", lib);\n\n\tdlhandle = dlopen(lib, RTLD_NOW);\n\tif (dlhandle == NULL) {\n\t\tFI_DBG(&core_prov, FI_LOG_CORE,\n\t\t\t\"dlopen(%s): %s\\n\", lib, dlerror());\n\t\treturn;\n\t}\n\n\tinif = dlsym(dlhandle, \"fi_prov_ini\");\n\tif (inif == NULL) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"dlsym: %s\\n\", dlerror());\n\t\tdlclose(dlhandle);\n\t} else {\n\t\tofi_register_provider((inif)(), dlhandle);\n\t}\n}\n\nstatic void ofi_ini_dir(const char *dir)\n{\n\tint n = 0;\n\tchar *lib;\n\tstruct dirent **liblist = NULL;\n\n\tn = scandir(dir, &liblist, lib_filter, NULL);\n\tif (n < 0)\n\t\tgoto libdl_done;\n\n\twhile (n--) {\n\t\tif (asprintf(&lib, \"%s/%s\", dir, liblist[n]->d_name) < 0) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t       \"asprintf failed to allocate memory\\n\");\n\t\t\tgoto libdl_done;\n\t\t}\n\t\tofi_reg_dl_prov(lib);\n\n\t\tfree(liblist[n]);\n\t\tfree(lib);\n\t}\n\nlibdl_done:\n\twhile (n-- > 0)\n\t\tfree(liblist[n]);\n\tfree(liblist);\n}\n\n/* Search standard system library paths (i.e. LD_LIBRARY_PATH) for known DL provider\n * libraries.\n */\nstatic void ofi_find_prov_libs(void)\n{\n\tconst char* lib_prefix = \"lib\";\n\tstruct ofi_prov *prov;\n\tchar* lib;\n\tchar* short_prov_name;\n\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\n\t\tif (!prov->prov_name)\n\t\t\tcontinue;\n\n\t\tif (ofi_has_util_prefix(prov->prov_name)) {\n\t\t\tshort_prov_name = prov->prov_name + strlen(OFI_UTIL_PREFIX);\n\t\t} else {\n\t\t\tshort_prov_name = prov->prov_name;\n\t\t}\n\n\t\tif (asprintf(&lib, \"%s%s%s%s\", lib_prefix,\n\t\t\tshort_prov_name, \"-\", FI_LIB_SUFFIX) < 0) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"asprintf failed to allocate memory\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tofi_reg_dl_prov(lib);\n\t\tfree(lib);\n\t}\n}\n#endif\n\nvoid fi_ini(void)\n{\n\tchar *param_val = NULL;\n\n\tpthread_mutex_lock(&common_locks.ini_lock);\n\n\tif (ofi_init)\n\t\tgoto unlock;\n\n\tofi_ordered_provs_init();\n\tfi_param_init();\n\tfi_log_init();\n\tofi_osd_init();\n\tofi_mem_init();\n\tofi_pmem_init();\n\tofi_perf_init();\n\tofi_hook_init();\n\tofi_hmem_init();\n\tofi_monitors_init();\n\n\tfi_param_define(NULL, \"provider\", FI_PARAM_STRING,\n\t\t\t\"Only use specified provider (default: all available)\");\n\tfi_param_define(NULL, \"fork_unsafe\", FI_PARAM_BOOL,\n\t\t\t\"Whether use of fork() may be unsafe for some providers\"\n\t\t\t\" (default: no). Setting this to yes could improve\"\n\t\t\t\" performance at the expense of making fork() potentially\"\n\t\t\t\" unsafe\");\n\tfi_param_define(NULL, \"universe_size\", FI_PARAM_SIZE_T,\n\t\t\t\"Defines the maximum number of processes that will be\"\n\t\t\t\" used by distribute OFI application. The provider uses\"\n\t\t\t\" this to optimize resource allocations\"\n\t\t\t\" (default: provider specific)\");\n\tfi_param_get_size_t(NULL, \"universe_size\", &ofi_universe_size);\n\tfi_param_get_str(NULL, \"provider\", &param_val);\n\tofi_create_filter(&prov_filter, param_val);\n\n#ifdef HAVE_LIBDL\n\tint n = 0;\n\tchar **dirs;\n\tchar *provdir = NULL;\n\tvoid *dlhandle;\n\n\t/* If dlopen fails, assume static linking and just return\n\t   without error */\n\tdlhandle = dlopen(NULL, RTLD_NOW);\n\tif (dlhandle == NULL) {\n\t\tgoto libdl_done;\n\t}\n\tdlclose(dlhandle);\n\n\tfi_param_define(NULL, \"provider_path\", FI_PARAM_STRING,\n\t\t\t\"Search for providers in specific path (default: \"\n\t\t\tPROVDLDIR \")\");\n\tfi_param_get_str(NULL, \"provider_path\", &provdir);\n\tif (!provdir) {\n\t\tprovdir = PROVDLDIR;\n\t\tofi_find_prov_libs();\n\t}\n\tdirs = ofi_split_and_alloc(provdir, \":\", NULL);\n\tif (dirs) {\n\t\tfor (n = 0; dirs[n]; ++n) {\n\t\t\tofi_ini_dir(dirs[n]);\n\t\t}\n\t\tofi_free_string_array(dirs);\n\t}\nlibdl_done:\n#endif\n\n\tofi_register_provider(PSM3_INIT, NULL);\n\tofi_register_provider(PSM2_INIT, NULL);\n\tofi_register_provider(PSM_INIT, NULL);\n\tofi_register_provider(USNIC_INIT, NULL);\n\tofi_register_provider(GNI_INIT, NULL);\n\tofi_register_provider(BGQ_INIT, NULL);\n\tofi_register_provider(NETDIR_INIT, NULL);\n\tofi_register_provider(SHM_INIT, NULL);\n\tofi_register_provider(RXM_INIT, NULL);\n\tofi_register_provider(VERBS_INIT, NULL);\n\t/* ofi_register_provider(RSTREAM_INIT, NULL); - no support */\n\tofi_register_provider(MRAIL_INIT, NULL);\n\tofi_register_provider(RXD_INIT, NULL);\n\tofi_register_provider(EFA_INIT, NULL);\n\tofi_register_provider(UDP_INIT, NULL);\n\tofi_register_provider(SOCKETS_INIT, NULL);\n\tofi_register_provider(TCP_INIT, NULL);\n\n\tofi_register_provider(HOOK_PERF_INIT, NULL);\n\tofi_register_provider(HOOK_DEBUG_INIT, NULL);\n\tofi_register_provider(HOOK_NOOP_INIT, NULL);\n\n\tofi_init = 1;\n\nunlock:\n\tpthread_mutex_unlock(&common_locks.ini_lock);\n}\n\nFI_DESTRUCTOR(fi_fini(void))\n{\n\tstruct ofi_prov *prov;\n\n\tif (!ofi_init)\n\t\treturn;\n\n\twhile (prov_head) {\n\t\tprov = prov_head;\n\t\tprov_head = prov->next;\n\t\tcleanup_provider(prov->provider, prov->dlhandle);\n\t\tfree(prov->prov_name);\n\t\tfree(prov);\n\t}\n\n\tofi_free_filter(&prov_filter);\n\tofi_monitors_cleanup();\n\tofi_hmem_cleanup();\n\tofi_mem_fini();\n\tfi_log_fini();\n\tfi_param_fini();\n\tofi_osd_fini();\n}\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nvoid DEFAULT_SYMVER_PRE(fi_freeinfo)(struct fi_info *info)\n{\n\tstruct fi_info *next;\n\n\tfor (; info; info = next) {\n\t\tnext = info->next;\n\n\t\tfree(info->src_addr);\n\t\tfree(info->dest_addr);\n\t\tfree(info->tx_attr);\n\t\tfree(info->rx_attr);\n\t\tif (info->ep_attr) {\n\t\t\tfree(info->ep_attr->auth_key);\n\t\t\tfree(info->ep_attr);\n\t\t}\n\t\tif (info->domain_attr) {\n\t\t\tfree(info->domain_attr->auth_key);\n\t\t\tfree(info->domain_attr->name);\n\t\t\tfree(info->domain_attr);\n\t\t}\n\t\tif (info->fabric_attr) {\n\t\t\tfree(info->fabric_attr->name);\n\t\t\tfree(info->fabric_attr->prov_name);\n\t\t\tfree(info->fabric_attr);\n\t\t}\n\t\tif (info->nic &&\n\t\t    FI_CHECK_OP(info->nic->fid.ops, struct fi_ops, close)) {\n\t\t\tfi_close(&info->nic->fid);\n\t\t}\n\t\tfree(info);\n\t}\n}\nDEFAULT_SYMVER(fi_freeinfo_, fi_freeinfo, FABRIC_1.3);\n\n/*\n * Make a dummy info object for each provider, and copy in the\n * provider name and version.  We report utility providers directly\n * to export their version.\n */\nstatic int ofi_getprovinfo(struct fi_info **info)\n{\n\tstruct ofi_prov *prov;\n\tstruct fi_info *tail, *cur;\n\tint ret = -FI_ENODATA;\n\n\t*info = tail = NULL;\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif (!prov->provider)\n\t\t\tcontinue;\n\n\t\tcur = fi_allocinfo();\n\t\tif (!cur) {\n\t\t\tret = -FI_ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tcur->fabric_attr->prov_name = strdup(prov->provider->name);\n\t\tcur->fabric_attr->prov_version = prov->provider->version;\n\n\t\tif (!*info) {\n\t\t\t*info = tail = cur;\n\t\t} else {\n\t\t\ttail->next = cur;\n\t\t}\n\t\ttail = cur;\n\n\t\tret = 0;\n\t}\n\n\treturn ret;\n\nerr:\n\twhile (tail) {\n\t\tcur = tail->next;\n\t\tfi_freeinfo(tail);\n\t\ttail = cur;\n\t}\n\treturn ret;\n}\n\nstatic void ofi_set_prov_attr(struct fi_fabric_attr *attr,\n\t\t\t      struct fi_provider *prov)\n{\n\tchar *core_name;\n\n\tcore_name = attr->prov_name;\n\tif (core_name) {\n\t\tassert(ofi_is_util_prov(prov));\n\t\tattr->prov_name = ofi_strdup_append(core_name, prov->name);\n\t\tfree(core_name);\n\t} else {\n\t\tassert(ofi_is_core_prov(prov));\n\t\tattr->prov_name = strdup(prov->name);\n\t}\n\tattr->prov_version = prov->version;\n}\n\n/*\n * The layering of utility providers over core providers follows these rules.\n * 0. Provider names are delimited by \";\"\n * 1. Rules when # of providers <= 2:\n *    1a. If both are specified, then only return that layering\n *    1b. If a utility provider is specified, return it over any* core provider.\n *    1c. If a core provider is specified, return any utility provider that can\n *        layer over it, plus the core provider itself, if possible.\n *    1d. A utility provider will not layer over a provider that has disabled\n *        utility provider layering unless the user explicitly requests that\n *        combination.\n *    1e. OFI_CORE_PROV_ONLY flag prevents utility providers layering over other\n *        utility providers.\n * 2. If both the providers are utility providers or if more than two providers\n *    are specified, the rightmost provider would be compared.\n * 3. If any provider has a caret symbol \"^\" is prefixed before any provider\n *    name it would be excluded (internal use only). These excluded providers\n *    should be listed only at the end.\n */\nstatic int ofi_layering_ok(const struct fi_provider *provider,\n\t\t\t   char **prov_vec, size_t count,\n\t\t\t   uint64_t flags)\n{\n\tchar *prov_name;\n\tstruct ofi_prov *core_ofi_prov;\n\tint i;\n\n\t/* Excluded providers must be at the end */\n\tfor (i = count - 1; i >= 0; i--) {\n\t\tif (prov_vec[i][0] != '^')\n\t\t    break;\n\n\t\tif (!strcasecmp(&prov_vec[i][1], provider->name))\n\t\t\treturn 0;\n\t}\n\tcount = i + 1;\n\n\tif (flags & OFI_CORE_PROV_ONLY) {\n\t\tassert((count == 1) || (count == 0));\n\t\tif (!ofi_is_core_prov(provider)) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Need core provider, skipping %s\\n\",\n\t\t\t\tprovider->name);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif ((count == 0) && ofi_disable_util_layering(provider)) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Skipping util;%s layering\\n\", provider->name);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (!count)\n\t\treturn 1;\n\n\t/* To maintain backward compatibility with the previous behavior of\n\t * ofi_layering_ok we need to check if the # of providers is two or\n\t * fewer. In such a case, we have to be agnostic to the ordering of\n\t * core and utility providers */\n\n\tif ((count == 1) && ofi_is_util_prov(provider) &&\n\t    !ofi_has_util_prefix(prov_vec[0])) {\n\t\tcore_ofi_prov = ofi_getprov(prov_vec[0], strlen(prov_vec[0]));\n\t\tif (core_ofi_prov && core_ofi_prov->provider &&\n\t\t    ofi_disable_util_layering(core_ofi_prov->provider)) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Skipping %s;%s layering\\n\", prov_vec[0],\n\t\t\t\tprovider->name);\n\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tif ((count == 2) && ofi_has_util_prefix(prov_vec[0]) &&\n\t    !ofi_has_util_prefix(prov_vec[1]))\n\t\tprov_name = prov_vec[0];\n\telse\n\t\tprov_name = prov_vec[count - 1];\n\n\treturn !strcasecmp(provider->name, prov_name);\n}\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nint DEFAULT_SYMVER_PRE(fi_getinfo)(uint32_t version, const char *node,\n\t\tconst char *service, uint64_t flags,\n\t\tconst struct fi_info *hints, struct fi_info **info)\n{\n\tstruct ofi_prov *prov;\n\tstruct fi_info *tail, *cur;\n\tchar **prov_vec = NULL;\n\tsize_t count = 0;\n\tenum fi_log_level level;\n\tint ret;\n\n\tif (!ofi_init)\n\t\tfi_ini();\n\n\tif (FI_VERSION_LT(fi_version(), version)) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Requested version is newer than library\\n\");\n\t\treturn -FI_ENOSYS;\n\t}\n\n\tif (flags == FI_PROV_ATTR_ONLY) {\n\t\treturn ofi_getprovinfo(info);\n\t}\n\n\tif (hints && hints->fabric_attr && hints->fabric_attr->prov_name) {\n\t\tprov_vec = ofi_split_and_alloc(hints->fabric_attr->prov_name,\n\t\t\t\t\t       \";\", &count);\n\t\tif (!prov_vec)\n\t\t\treturn -FI_ENOMEM;\n\t\tFI_DBG(&core_prov, FI_LOG_CORE, \"hints prov_name: %s\\n\",\n\t\t       hints->fabric_attr->prov_name);\n\t}\n\n\t*info = tail = NULL;\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif (!prov->provider || !prov->provider->getinfo)\n\t\t\tcontinue;\n\n\t\tif (prov->hidden && !(flags & OFI_GETINFO_HIDDEN))\n\t\t\tcontinue;\n\n\t\tif (!ofi_layering_ok(prov->provider, prov_vec, count, flags))\n\t\t\tcontinue;\n\n\t\tif (FI_VERSION_LT(prov->provider->fi_version, version)) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Provider %s fi_version %d.%d < requested %d.%d\\n\",\n\t\t\t\tprov->provider->name,\n\t\t\t\tFI_MAJOR(prov->provider->fi_version),\n\t\t\t\tFI_MINOR(prov->provider->fi_version),\n\t\t\t\tFI_MAJOR(version), FI_MINOR(version));\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur = NULL;\n\t\tret = prov->provider->getinfo(version, node, service, flags,\n\t\t\t\t\t      hints, &cur);\n\t\tif (ret) {\n\t\t\tlevel = ((hints && hints->fabric_attr &&\n\t\t\t\t  hints->fabric_attr->prov_name) ?\n\t\t\t\t FI_LOG_WARN : FI_LOG_INFO);\n\n\t\t\tFI_LOG(&core_prov, level, FI_LOG_CORE,\n\t\t\t       \"fi_getinfo: provider %s returned -%d (%s)\\n\",\n\t\t\t       prov->provider->name, -ret, fi_strerror(-ret));\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!cur) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"fi_getinfo: provider %s output empty list\\n\",\n\t\t\t\tprov->provider->name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tFI_DBG(&core_prov, FI_LOG_CORE, \"fi_getinfo: provider %s \"\n\t\t       \"returned success\\n\", prov->provider->name);\n\n\t\tif (!*info)\n\t\t\t*info = cur;\n\t\telse\n\t\t\ttail->next = cur;\n\n\t\tfor (tail = cur; tail->next; tail = tail->next) {\n\t\t\tofi_set_prov_attr(tail->fabric_attr, prov->provider);\n\t\t\ttail->fabric_attr->api_version = version;\n\t\t}\n\t\tofi_set_prov_attr(tail->fabric_attr, prov->provider);\n\t\ttail->fabric_attr->api_version = version;\n\t}\n\tofi_free_string_array(prov_vec);\n\n\tif (!(flags & (OFI_CORE_PROV_ONLY | OFI_GETINFO_INTERNAL |\n\t               OFI_GETINFO_HIDDEN)))\n\t\tofi_filter_info(info);\n\n\treturn *info ? 0 : -FI_ENODATA;\n}\nDEFAULT_SYMVER(fi_getinfo_, fi_getinfo, FABRIC_1.3);\n\nstruct fi_info *ofi_allocinfo_internal(void)\n{\n\tstruct fi_info *info;\n\n\tinfo = calloc(1, sizeof(*info));\n\tif (!info)\n\t\treturn NULL;\n\n\tinfo->tx_attr = calloc(1, sizeof(*info->tx_attr));\n\tinfo->rx_attr = calloc(1, sizeof(*info->rx_attr));\n\tinfo->ep_attr = calloc(1, sizeof(*info->ep_attr));\n\tinfo->domain_attr = calloc(1, sizeof(*info->domain_attr));\n\tinfo->fabric_attr = calloc(1, sizeof(*info->fabric_attr));\n\tif (!info->tx_attr|| !info->rx_attr || !info->ep_attr ||\n\t    !info->domain_attr || !info->fabric_attr)\n\t\tgoto err;\n\n\treturn info;\nerr:\n\tfi_freeinfo(info);\n\treturn NULL;\n}\n\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nstruct fi_info *DEFAULT_SYMVER_PRE(fi_dupinfo)(const struct fi_info *info)\n{\n\tstruct fi_info *dup;\n\tint ret;\n\n\tif (!info)\n\t\treturn ofi_allocinfo_internal();\n\n\tdup = mem_dup(info, sizeof(*dup));\n\tif (dup == NULL) {\n\t\treturn NULL;\n\t}\n\tdup->src_addr = NULL;\n\tdup->dest_addr = NULL;\n\tdup->tx_attr = NULL;\n\tdup->rx_attr = NULL;\n\tdup->ep_attr = NULL;\n\tdup->domain_attr = NULL;\n\tdup->fabric_attr = NULL;\n\tdup->next = NULL;\n\n\tif (info->src_addr != NULL) {\n\t\tdup->src_addr = mem_dup(info->src_addr, info->src_addrlen);\n\t\tif (dup->src_addr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->dest_addr != NULL) {\n\t\tdup->dest_addr = mem_dup(info->dest_addr, info->dest_addrlen);\n\t\tif (dup->dest_addr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->tx_attr != NULL) {\n\t\tdup->tx_attr = mem_dup(info->tx_attr, sizeof(*info->tx_attr));\n\t\tif (dup->tx_attr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->rx_attr != NULL) {\n\t\tdup->rx_attr = mem_dup(info->rx_attr, sizeof(*info->rx_attr));\n\t\tif (dup->rx_attr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->ep_attr != NULL) {\n\t\tdup->ep_attr = mem_dup(info->ep_attr, sizeof(*info->ep_attr));\n\t\tif (dup->ep_attr == NULL)\n\t\t\tgoto fail;\n\t\tif (info->ep_attr->auth_key != NULL) {\n\t\t\tdup->ep_attr->auth_key =\n\t\t\t\tmem_dup(info->ep_attr->auth_key,\n\t\t\t\t\tinfo->ep_attr->auth_key_size);\n\t\t\tif (dup->ep_attr->auth_key == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\tif (info->domain_attr) {\n\t\tdup->domain_attr = mem_dup(info->domain_attr,\n\t\t\t\t\t   sizeof(*info->domain_attr));\n\t\tif (dup->domain_attr == NULL)\n\t\t\tgoto fail;\n\t\tdup->domain_attr->name = NULL;\n\t\tdup->domain_attr->auth_key = NULL;\n\t\tif (info->domain_attr->name != NULL) {\n\t\t\tdup->domain_attr->name = strdup(info->domain_attr->name);\n\t\t\tif (dup->domain_attr->name == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tif (info->domain_attr->auth_key != NULL) {\n\t\t\tdup->domain_attr->auth_key =\n\t\t\t\tmem_dup(info->domain_attr->auth_key,\n\t\t\t\t\tinfo->domain_attr->auth_key_size);\n\t\t\tif (dup->domain_attr->auth_key == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\tif (info->fabric_attr) {\n\t\tdup->fabric_attr = mem_dup(info->fabric_attr,\n\t\t\t\t\t   sizeof(*info->fabric_attr));\n\t\tif (dup->fabric_attr == NULL)\n\t\t\tgoto fail;\n\t\tdup->fabric_attr->name = NULL;\n\t\tdup->fabric_attr->prov_name = NULL;\n\t\tif (info->fabric_attr->name != NULL) {\n\t\t\tdup->fabric_attr->name = strdup(info->fabric_attr->name);\n\t\t\tif (dup->fabric_attr->name == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tif (info->fabric_attr->prov_name != NULL) {\n\t\t\tdup->fabric_attr->prov_name = strdup(info->fabric_attr->prov_name);\n\t\t\tif (dup->fabric_attr->prov_name == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (info->nic) {\n\t\tret = fi_control(&info->nic->fid, FI_DUP, &dup->nic);\n\t\tif (ret && ret != -FI_ENOSYS)\n\t\t\tgoto fail;\n\t}\n\n\treturn dup;\n\nfail:\n\tfi_freeinfo(dup);\n\treturn NULL;\n}\nDEFAULT_SYMVER(fi_dupinfo_, fi_dupinfo, FABRIC_1.3);\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nint DEFAULT_SYMVER_PRE(fi_fabric)(struct fi_fabric_attr *attr,\n\t\tstruct fid_fabric **fabric, void *context)\n{\n\tstruct ofi_prov *prov;\n\tconst char *top_name;\n\tint ret;\n\n\tif (!attr || !attr->prov_name || !attr->name)\n\t\treturn -FI_EINVAL;\n\n\tif (!ofi_init)\n\t\tfi_ini();\n\n\ttop_name = strrchr(attr->prov_name, OFI_NAME_DELIM);\n\tif (top_name)\n\t\ttop_name++;\n\telse\n\t\ttop_name = attr->prov_name;\n\n\tif (!top_name)\n\t\treturn -FI_EINVAL;\n\n\tprov = ofi_getprov(top_name, strlen(top_name));\n\tif (!prov || !prov->provider || !prov->provider->fabric)\n\t\treturn -FI_ENODEV;\n\n\tret = prov->provider->fabric(attr, fabric, context);\n\tif (!ret) {\n\t\tif (FI_VERSION_GE(prov->provider->fi_version, FI_VERSION(1, 5)))\n\t\t\t(*fabric)->api_version = attr->api_version;\n\t\tFI_INFO(&core_prov, FI_LOG_CORE, \"Opened fabric: %s\\n\",\n\t\t\tattr->name);\n\n\t\tofi_hook_install(*fabric, fabric, prov->provider);\n\t}\n\n\treturn ret;\n}\nDEFAULT_SYMVER(fi_fabric_, fi_fabric, FABRIC_1.1);\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nuint32_t DEFAULT_SYMVER_PRE(fi_version)(void)\n{\n\treturn FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION);\n}\nDEFAULT_SYMVER(fi_version_, fi_version, FABRIC_1.0);\n\nstatic const char *const errstr[] = {\n\t[FI_EOTHER - FI_ERRNO_OFFSET] = \"Unspecified error\",\n\t[FI_ETOOSMALL - FI_ERRNO_OFFSET] = \"Provided buffer is too small\",\n\t[FI_EOPBADSTATE - FI_ERRNO_OFFSET] = \"Operation not permitted in current state\",\n\t[FI_EAVAIL - FI_ERRNO_OFFSET]  = \"Error available\",\n\t[FI_EBADFLAGS - FI_ERRNO_OFFSET] = \"Flags not supported\",\n\t[FI_ENOEQ - FI_ERRNO_OFFSET] = \"Missing or unavailable event queue\",\n\t[FI_EDOMAIN - FI_ERRNO_OFFSET] = \"Invalid resource domain\",\n\t[FI_ENOCQ - FI_ERRNO_OFFSET] = \"Missing or unavailable completion queue\",\n\t[FI_ECRC - FI_ERRNO_OFFSET] = \"CRC error\",\n\t[FI_ETRUNC - FI_ERRNO_OFFSET] = \"Truncation error\",\n\t[FI_ENOKEY - FI_ERRNO_OFFSET] = \"Required key not available\",\n\t[FI_ENOAV - FI_ERRNO_OFFSET] = \"Missing or unavailable address vector\",\n\t[FI_EOVERRUN - FI_ERRNO_OFFSET] = \"Queue has been overrun\",\n};\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nconst char *DEFAULT_SYMVER_PRE(fi_strerror)(int errnum)\n{\n\tif (errnum < FI_ERRNO_OFFSET)\n\t\treturn strerror(errnum);\n\telse if (errnum < FI_ERRNO_MAX)\n\t\treturn errstr[errnum - FI_ERRNO_OFFSET];\n\telse\n\t\treturn errstr[FI_EOTHER - FI_ERRNO_OFFSET];\n}\nDEFAULT_SYMVER(fi_strerror_, fi_strerror, FABRIC_1.0);\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/src/hmem_rocr.c": "/*\n * (C) Copyright 2020 Hewlett Packard Enterprise Development LP\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#if HAVE_CONFIG_H\n#include <config.h>\n#endif\n\n#include \"ofi_hmem.h\"\n#include \"ofi.h\"\n\n#ifdef HAVE_ROCR\n\n#include <hsa/hsa_ext_amd.h>\n\nstruct rocr_ops {\n\thsa_status_t (*hsa_memory_copy)(void *dst, const void *src,\n\t\t\t\t\tsize_t size);\n\thsa_status_t (*hsa_amd_pointer_info)(void *ptr,\n\t\t\t\t\t     hsa_amd_pointer_info_t *info,\n\t\t\t\t\t     void *(*alloc)(size_t),\n\t\t\t\t\t     uint32_t *num_agents_accessible,\n\t\t\t\t\t     hsa_agent_t **accessible);\n\thsa_status_t (*hsa_init)(void);\n\thsa_status_t (*hsa_shut_down)(void);\n\thsa_status_t (*hsa_status_string)(hsa_status_t status,\n\t\t\t\t\t  const char **status_string);\n\thsa_status_t (*hsa_amd_dereg_dealloc_cb)(void *ptr,\n\t\t\t\t\t\t hsa_amd_deallocation_callback_t cb);\n\thsa_status_t (*hsa_amd_reg_dealloc_cb)(void *ptr,\n\t\t\t\t\t       hsa_amd_deallocation_callback_t cb,\n\t\t\t\t\t       void *user_data);\n\thsa_status_t (*hsa_amd_memory_lock)(void *host_ptr, size_t size,\n\t\t\t\t\t    hsa_agent_t *agents, int num_agents,\n\t\t\t\t\t    void **agent_ptr);\n\thsa_status_t (*hsa_amd_memory_unlock)(void *host_ptr);\n\thsa_status_t (*hsa_agent_get_info)(hsa_agent_t agent,\n\t\t\t\t\t   hsa_agent_info_t attribute,\n\t\t\t\t\t   void *value);\n};\n\n#ifdef ENABLE_ROCR_DLOPEN\n\n#include <dlfcn.h>\n\nstatic void *rocr_handle;\nstatic struct rocr_ops rocr_ops;\n\n#else\n\nstatic struct rocr_ops rocr_ops = {\n\t.hsa_memory_copy = hsa_memory_copy,\n\t.hsa_amd_pointer_info = hsa_amd_pointer_info,\n\t.hsa_init = hsa_init,\n\t.hsa_shut_down = hsa_shut_down,\n\t.hsa_status_string = hsa_status_string,\n\t.hsa_amd_dereg_dealloc_cb =\n\t\thsa_amd_deregister_deallocation_callback,\n\t.hsa_amd_reg_dealloc_cb =\n\t\thsa_amd_register_deallocation_callback,\n\t.hsa_amd_memory_lock = hsa_amd_memory_lock,\n\t.hsa_amd_memory_unlock = hsa_amd_memory_unlock,\n\t.hsa_agent_get_info = hsa_agent_get_info,\n};\n\n#endif /* ENABLE_ROCR_DLOPEN */\n\nhsa_status_t ofi_hsa_amd_memory_lock(void *host_ptr, size_t size,\n\t\t\t\t     hsa_agent_t *agents, int num_agents,\n\t\t\t\t     void **agent_ptr)\n{\n\treturn rocr_ops.hsa_amd_memory_lock(host_ptr, size, agents, num_agents,\n\t\t\t\t\t    agent_ptr);\n}\n\nhsa_status_t ofi_hsa_amd_memory_unlock(void *host_ptr)\n{\n\treturn rocr_ops.hsa_amd_memory_unlock(host_ptr);\n}\n\nhsa_status_t ofi_hsa_memory_copy(void *dst, const void *src, size_t size)\n{\n\treturn rocr_ops.hsa_memory_copy(dst, src, size);\n}\n\nhsa_status_t ofi_hsa_amd_pointer_info(void *ptr, hsa_amd_pointer_info_t *info,\n\t\t\t\t      void *(*alloc)(size_t),\n\t\t\t\t      uint32_t *num_agents_accessible,\n\t\t\t\t      hsa_agent_t **accessible)\n{\n\treturn rocr_ops.hsa_amd_pointer_info(ptr, info, alloc,\n\t\t\t\t\t     num_agents_accessible, accessible);\n}\n\nhsa_status_t ofi_hsa_init(void)\n{\n\treturn rocr_ops.hsa_init();\n}\n\nhsa_status_t ofi_hsa_shut_down(void)\n{\n\treturn rocr_ops.hsa_shut_down();\n}\n\nhsa_status_t ofi_hsa_status_string(hsa_status_t status,\n\t\t\t\t   const char **status_string)\n{\n\treturn rocr_ops.hsa_status_string(status, status_string);\n}\n\nconst char *ofi_hsa_status_to_string(hsa_status_t status)\n{\n\tconst char *str;\n\thsa_status_t hsa_ret;\n\n\thsa_ret = ofi_hsa_status_string(status, &str);\n\tif (hsa_ret != HSA_STATUS_SUCCESS)\n\t\treturn \"unknown error\";\n\n\treturn str;\n}\n\nhsa_status_t ofi_hsa_amd_dereg_dealloc_cb(void *ptr,\n\t\t\t\t\t  hsa_amd_deallocation_callback_t cb)\n{\n\treturn rocr_ops.hsa_amd_dereg_dealloc_cb(ptr, cb);\n}\n\nhsa_status_t ofi_hsa_amd_reg_dealloc_cb(void *ptr,\n\t\t\t\t\thsa_amd_deallocation_callback_t cb,\n\t\t\t\t\tvoid *user_data)\n{\n\treturn rocr_ops.hsa_amd_reg_dealloc_cb(ptr, cb, user_data);\n}\n\nstatic hsa_status_t ofi_hsa_agent_get_info(hsa_agent_t agent,\n\t\t\t\t\t   hsa_agent_info_t attribute,\n\t\t\t\t\t   void *value)\n{\n\treturn rocr_ops.hsa_agent_get_info(agent, attribute, value);\n}\n\nstatic int rocr_memcpy(void *dest, const void *src, size_t size)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = ofi_hsa_memory_copy(dest, src, size);\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn 0;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform hsa_memory_copy: %s\\n\",\n\t\tofi_hsa_status_to_string(hsa_ret));\n\n\treturn -FI_EIO;\n}\n\nstatic int rocr_host_memory_ptr(void *host_ptr, void **ptr)\n{\n\thsa_amd_pointer_info_t info = {\n\t\t.size = sizeof(info),\n\t};\n\thsa_status_t hsa_ret;\n\n\thsa_ret = ofi_hsa_amd_pointer_info((void *)host_ptr, &info, NULL, NULL,\n\t\t\t\t\t   NULL);\n\tif (hsa_ret != HSA_STATUS_SUCCESS) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to perform hsa_amd_pointer_info: %s\\n\",\n\t\t\tofi_hsa_status_to_string(hsa_ret));\n\n\t\treturn -FI_EIO;\n\t}\n\n\tif (info.type != HSA_EXT_POINTER_TYPE_LOCKED)\n\t\t*ptr = host_ptr;\n\telse\n\t\t*ptr = (void *) ((uintptr_t) info.agentBaseAddress +\n\t\t\t\t (uintptr_t) host_ptr -\n\t\t\t\t (uintptr_t) info.hostBaseAddress);\n\n\treturn FI_SUCCESS;\n}\n\nint rocr_copy_from_dev(uint64_t device, void *dest, const void *src,\n\t\t       size_t size)\n{\n\tint ret;\n\tvoid *dest_memcpy_ptr;\n\n\tret = rocr_host_memory_ptr(dest, &dest_memcpy_ptr);\n\tif (ret != FI_SUCCESS)\n\t\treturn ret;\n\n\tret = rocr_memcpy(dest_memcpy_ptr, src, size);\n\n\treturn ret;\n}\n\nint rocr_copy_to_dev(uint64_t device, void *dest, const void *src,\n\t\t     size_t size)\n{\n\tint ret;\n\tvoid *src_memcpy_ptr;\n\n\tret = rocr_host_memory_ptr((void *) src, &src_memcpy_ptr);\n\tif (ret != FI_SUCCESS)\n\t\treturn ret;\n\n\tret = rocr_memcpy(dest, src_memcpy_ptr, size);\n\n\treturn ret;\n}\n\nbool rocr_is_addr_valid(const void *addr)\n{\n\thsa_amd_pointer_info_t hsa_info = {\n\t\t.size = sizeof(hsa_info),\n\t};\n\thsa_device_type_t hsa_dev_type;\n\thsa_status_t hsa_ret;\n\n\thsa_ret = ofi_hsa_amd_pointer_info((void *)addr, &hsa_info, NULL, NULL,\n\t\t\t\t\t   NULL);\n\tif (hsa_ret == HSA_STATUS_SUCCESS) {\n\t\thsa_ret = ofi_hsa_agent_get_info(hsa_info.agentOwner,\n\t\t\t\t\t\t HSA_AGENT_INFO_DEVICE,\n\t\t\t\t\t\t (void *) &hsa_dev_type);\n\t\tif (hsa_ret == HSA_STATUS_SUCCESS) {\n\t\t\tif (hsa_dev_type == HSA_DEVICE_TYPE_GPU)\n\t\t\t\treturn true;\n\t\t} else {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Failed to perform hsa_agent_get_info: %s\\n\",\n\t\t\t\tofi_hsa_status_to_string(hsa_ret));\n\t\t}\n\t} else {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to perform hsa_amd_pointer_info: %s\\n\",\n\t\t\tofi_hsa_status_to_string(hsa_ret));\n\t}\n\n\treturn false;\n}\n\nstatic int rocr_hmem_dl_init(void)\n{\n#ifdef ENABLE_ROCR_DLOPEN\n\t/* Assume if dlopen fails, the ROCR library could not be found. Do not\n\t * treat this as an error.\n\t */\n\trocr_handle = dlopen(\"libhsa-runtime64.so\", RTLD_NOW);\n\tif (!rocr_handle) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"Unable to dlopen libhsa-runtime64.so\\n\");\n\t\treturn -FI_ENOSYS;\n\t}\n\n\trocr_ops.hsa_memory_copy = dlsym(rocr_handle, \"hsa_memory_copy\");\n\tif (!rocr_ops.hsa_memory_copy) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_memory_copy\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_amd_pointer_info = dlsym(rocr_handle,\n\t\t\t\t\t      \"hsa_amd_pointer_info\");\n\tif (!rocr_ops.hsa_amd_pointer_info) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_amd_pointer_info\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_init = dlsym(rocr_handle, \"hsa_init\");\n\tif (!rocr_ops.hsa_init) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"Failed to find hsa_init\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_shut_down = dlsym(rocr_handle, \"hsa_shut_down\");\n\tif (!rocr_ops.hsa_shut_down) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_shut_down\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_status_string = dlsym(rocr_handle, \"hsa_status_string\");\n\tif (!rocr_ops.hsa_status_string) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_status_string\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_amd_dereg_dealloc_cb =\n\t\tdlsym(rocr_handle, \"hsa_amd_deregister_deallocation_callback\");\n\tif (!rocr_ops.hsa_amd_dereg_dealloc_cb) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_amd_deregister_deallocation_callback\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_amd_reg_dealloc_cb =\n\t\tdlsym(rocr_handle, \"hsa_amd_register_deallocation_callback\");\n\tif (!rocr_ops.hsa_amd_reg_dealloc_cb) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_amd_register_deallocation_callback\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_amd_memory_lock = dlsym(rocr_handle,\n\t\t\t\t\t     \"hsa_amd_memory_lock\");\n\tif (!rocr_ops.hsa_amd_memory_lock) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_amd_memory_lock\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_amd_memory_unlock = dlsym(rocr_handle,\n\t\t\t\t\t       \"hsa_amd_memory_unlock\");\n\tif (!rocr_ops.hsa_amd_memory_lock) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_amd_memory_unlock\\n\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_agent_get_info = dlsym(rocr_handle, \"hsa_agent_get_info\");\n\tif (!rocr_ops.hsa_agent_get_info) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to find hsa_agent_get_info\\n\");\n\t\tgoto err;\n\t}\n\n\treturn FI_SUCCESS;\n\nerr:\n\tdlclose(rocr_handle);\n\n\treturn -FI_ENODATA;\n#else\n\treturn FI_SUCCESS;\n#endif /* ENABLE_ROCR_DLOPEN */\n}\n\nstatic void rocr_hmem_dl_cleanup(void)\n{\n#ifdef ENABLE_ROCR_DLOPEN\n\tdlclose(rocr_handle);\n#endif\n}\n\nint rocr_hmem_init(void)\n{\n\thsa_status_t hsa_ret;\n\tint ret;\n\tint log_level;\n\n\tret = rocr_hmem_dl_init();\n\tif (ret != FI_SUCCESS)\n\t\treturn ret;\n\n\thsa_ret = ofi_hsa_init();\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn FI_SUCCESS;\n\n\t/* Treat HSA_STATUS_ERROR_OUT_OF_RESOURCES as ROCR not being supported\n\t * instead of an error. This ROCR error is typically returned if no\n\t * devices are supported.\n\t */\n\tif (hsa_ret == HSA_STATUS_ERROR_OUT_OF_RESOURCES) {\n\t\tlog_level = FI_LOG_INFO;\n\t\tret = -FI_ENOSYS;\n\t} else {\n\t\tlog_level = FI_LOG_WARN;\n\t\tret = -FI_EIO;\n\t}\n\n\tFI_LOG(&core_prov, log_level, FI_LOG_CORE,\n\t       \"Failed to perform hsa_init: %s\\n\",\n\t       ofi_hsa_status_to_string(hsa_ret));\n\n\trocr_hmem_dl_cleanup();\n\n\treturn ret;\n}\n\nint rocr_hmem_cleanup(void)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = ofi_hsa_shut_down();\n\tif (hsa_ret != HSA_STATUS_SUCCESS) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to perform hsa_shut_down: %s\\n\",\n\t\t\tofi_hsa_status_to_string(hsa_ret));\n\t\treturn -FI_ENODATA;\n\t}\n\n\trocr_hmem_dl_cleanup();\n\n\treturn FI_SUCCESS;\n}\n\nint rocr_host_register(void *ptr, size_t size)\n{\n\thsa_status_t hsa_ret;\n\tvoid *tmp;\n\n\thsa_ret = ofi_hsa_amd_memory_lock(ptr, size, NULL, 0, &tmp);\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn FI_SUCCESS;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform hsa_amd_memory_lock: %s\\n\",\n\t\tofi_hsa_status_to_string(hsa_ret));\n\n\treturn -FI_EIO;\n}\n\nint rocr_host_unregister(void *ptr)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = ofi_hsa_amd_memory_unlock(ptr);\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn FI_SUCCESS;\n\n\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\"Failed to perform hsa_amd_memory_unlock: %s\\n\",\n\t\tofi_hsa_status_to_string(hsa_ret));\n\n\treturn -FI_EIO;\n}\n\n#else\n\nint rocr_copy_from_dev(uint64_t device, void *dest, const void *src,\n\t\t       size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint rocr_copy_to_dev(uint64_t device, void *dest, const void *src,\n\t\t     size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint rocr_hmem_init(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint rocr_hmem_cleanup(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nbool rocr_is_addr_valid(const void *addr)\n{\n\treturn false;\n}\n\nint rocr_host_register(void *ptr, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint rocr_host_unregister(void *ptr)\n{\n\treturn -FI_ENOSYS;\n}\n\n#endif /* HAVE_ROCR */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/fabtests/common/hmem_cuda.c": "/*\n * (C) Copyright 2020 Hewlett Packard Enterprise Development LP\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include \"hmem.h\"\n#include \"shared.h\"\n\n#ifdef HAVE_CUDA_RUNTIME_H\n\n#include <dlfcn.h>\n#include <stdio.h>\n#include <cuda_runtime.h>\n\nstruct cuda_ops {\n\tcudaError_t (*cudaMemcpy)(void *dst, const void *src, size_t count,\n\t\t\t\t  enum cudaMemcpyKind kind);\n\tcudaError_t (*cudaMalloc)(void **ptr, size_t size);\n\tcudaError_t (*cudaFree)(void *ptr);\n\tcudaError_t (*cudaMemset)(void *ptr, int value, size_t count);\n\tconst char *(*cudaGetErrorName)(cudaError_t error);\n\tconst char *(*cudaGetErrorString)(cudaError_t error);\n};\n\nstatic struct cuda_ops cuda_ops;\nstatic void *cudart_handle;\n\n#define CUDA_ERR(err, fmt, ...) \\\n\tFT_ERR(fmt \": %s %s\", ##__VA_ARGS__, cuda_ops.cudaGetErrorName(err), \\\n\t       cuda_ops.cudaGetErrorString(err))\n\nint ft_cuda_init(void)\n{\n\tcudart_handle = dlopen(\"libcudart.so\", RTLD_NOW);\n\tif (!cudart_handle) {\n\t\tFT_ERR(\"Failed to dlopen libcudart.so\");\n\t\tgoto err;\n\t}\n\n\tcuda_ops.cudaMemcpy = dlsym(cudart_handle, \"cudaMemcpy\");\n\tif (!cuda_ops.cudaMemcpy) {\n\t\tFT_ERR(\"Failed to find cudaMemcpy\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaMalloc = dlsym(cudart_handle, \"cudaMalloc\");\n\tif (!cuda_ops.cudaMalloc) {\n\t\tFT_ERR(\"Failed to find cudaMalloc\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaFree = dlsym(cudart_handle, \"cudaFree\");\n\tif (!cuda_ops.cudaFree) {\n\t\tFT_ERR(\"Failed to find cudaFree\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaMemset = dlsym(cudart_handle, \"cudaMemset\");\n\tif (!cuda_ops.cudaMemset) {\n\t\tFT_ERR(\"Failed to find cudaMemset\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaGetErrorName = dlsym(cudart_handle, \"cudaGetErrorName\");\n\tif (!cuda_ops.cudaGetErrorName) {\n\t\tFT_ERR(\"Failed to find cudaGetErrorName\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\tcuda_ops.cudaGetErrorString = dlsym(cudart_handle,\n\t\t\t\t\t    \"cudaGetErrorString\");\n\tif (!cuda_ops.cudaGetErrorString) {\n\t\tFT_ERR(\"Failed to find cudaGetErrorString\");\n\t\tgoto err_dlclose_cuda;\n\t}\n\n\treturn FI_SUCCESS;\n\nerr_dlclose_cuda:\n\tdlclose(cudart_handle);\nerr:\n\treturn -FI_ENODATA;\n}\n\nint ft_cuda_cleanup(void)\n{\n\tdlclose(cudart_handle);\n\treturn FI_SUCCESS;\n}\n\nint ft_cuda_alloc(uint64_t device, void **buf, size_t size)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = cuda_ops.cudaMalloc(buf, size);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tCUDA_ERR(cuda_ret, \"cudaMalloc failed\");\n\n\treturn -FI_ENOMEM;\n}\n\nint ft_cuda_free(void *buf)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = cuda_ops.cudaFree(buf);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tCUDA_ERR(cuda_ret, \"cudaFree failed\");\n\n\treturn -FI_EIO;\n}\n\nint ft_cuda_memset(uint64_t device, void *buf, int value, size_t size)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = cuda_ops.cudaMemset(buf, value, size);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tCUDA_ERR(cuda_ret, \"cudaMemset failed\");\n\n\treturn -FI_EIO;\n}\n\nint ft_cuda_copy_to_hmem(uint64_t device, void *dst, const void *src,\n\t\t\t size_t size)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = cuda_ops.cudaMemcpy(dst, src, size, cudaMemcpyHostToDevice);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tCUDA_ERR(cuda_ret, \"cudaMemcpy failed\");\n\n\treturn -FI_EIO;\n}\n\nint ft_cuda_copy_from_hmem(uint64_t device, void *dst, const void *src,\n\t\t\t   size_t size)\n{\n\tcudaError_t cuda_ret;\n\n\tcuda_ret = cuda_ops.cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost);\n\tif (cuda_ret == cudaSuccess)\n\t\treturn FI_SUCCESS;\n\n\tCUDA_ERR(cuda_ret, \"cudaMemcpy failed\");\n\n\treturn -FI_EIO;\n}\n\n#else\n\nint ft_cuda_init(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_cuda_cleanup(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_cuda_alloc(uint64_t device, void **buf, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_cuda_free(void *buf)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_cuda_memset(uint64_t device, void *buf, int value, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_cuda_copy_to_hmem(uint64_t device, void *dst, const void *src,\n\t\t\t size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_cuda_copy_from_hmem(uint64_t device, void *dst, const void *src,\n\t\t\t   size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\n#endif /* HAVE_CUDA_RUNTIME_H */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/fabtests/common/hmem_rocr.c": "/*\n * (C) Copyright 2020 Hewlett Packard Enterprise Development LP\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include \"hmem.h\"\n#include \"shared.h\"\n\n#ifdef HAVE_ROCR_RUNTIME_H\n\n#include <dlfcn.h>\n#include <stdio.h>\n#include <hsa/hsa.h>\n\nstruct rocr_ops {\n\thsa_status_t (*hsa_memory_copy)(void *dst, const void *src,\n\t\t\t\t\tsize_t size);\n\thsa_status_t (*hsa_init)(void);\n\thsa_status_t (*hsa_shut_down)(void);\n\thsa_status_t (*hsa_status_string)(hsa_status_t status,\n\t\t\t\t\t  const char **status_string);\n\thsa_status_t (*hsa_agent_get_info)(hsa_agent_t agent,\n\t\t\t\t\t   hsa_agent_info_t attribute,\n\t\t\t\t\t   void *value);\n\thsa_status_t (*hsa_region_get_info)(hsa_region_t region,\n\t\t\t\t\t    hsa_region_info_t attribute,\n\t\t\t\t\t    void *value);\n\thsa_status_t (*hsa_iterate_agents)\n\t\t(hsa_status_t (*cb)(hsa_agent_t agent, void* data), void *data);\n\thsa_status_t (*hsa_agent_iterate_regions)\n\t\t(hsa_agent_t agent,\n\t\t hsa_status_t (*cb)(hsa_region_t region, void* data),\n\t\t void *data);\n\thsa_status_t (*hsa_memory_allocate)(hsa_region_t region, size_t size,\n\t\t\t\t\t    void **ptr);\n\thsa_status_t (*hsa_memory_free)(void *ptr);\n\thsa_status_t (*hsa_amd_memory_fill)(void* ptr, uint32_t value,\n\t\t\t\t\t    size_t count);\n};\n\nstatic struct rocr_ops rocr_ops;\nstatic void *rocr_handle;\n\nstatic const char *hsa_status_to_string(hsa_status_t status)\n{\n\tconst char *str;\n\thsa_status_t hsa_ret;\n\n\thsa_ret = rocr_ops.hsa_status_string(status, &str);\n\tif (hsa_ret != HSA_STATUS_SUCCESS)\n\t\treturn \"unknown error\";\n\n\treturn str;\n}\n\n#define ROCR_ERR(err, fmt, ...) \\\n\tFT_ERR(fmt \": %s\", ##__VA_ARGS__, hsa_status_to_string(err))\n\nstatic hsa_agent_t gpu_agent;\nstatic hsa_region_t gpu_region;\n\nstatic hsa_status_t agent_cb(hsa_agent_t agent, void *data)\n{\n\thsa_status_t hsa_ret;\n\thsa_device_type_t hsa_dev_type;\n\n\thsa_ret = rocr_ops.hsa_agent_get_info(agent, HSA_AGENT_INFO_DEVICE,\n\t\t\t\t\t      (void *) &hsa_dev_type);\n\n\tif (hsa_ret == HSA_STATUS_SUCCESS &&\n\t    hsa_dev_type == HSA_DEVICE_TYPE_GPU) {\n\t\tgpu_agent = agent;\n\t\treturn HSA_STATUS_INFO_BREAK;\n\t}\n\n\treturn hsa_ret;\n}\n\nstatic hsa_status_t region_cb(hsa_region_t region, void *data)\n{\n\thsa_status_t hsa_ret;\n\thsa_region_segment_t hsa_segment;\n\n\thsa_ret = rocr_ops.hsa_region_get_info(region, HSA_REGION_INFO_SEGMENT,\n\t\t\t\t\t       &hsa_segment);\n\n\tif (hsa_ret == HSA_STATUS_SUCCESS &&\n\t    hsa_segment == HSA_REGION_SEGMENT_GLOBAL) {\n\t\tgpu_region = region;\n\t\treturn HSA_STATUS_INFO_BREAK;\n\t}\n\n\treturn hsa_ret;\n}\n\nint ft_rocr_init(void)\n{\n\thsa_status_t hsa_ret;\n\n\trocr_handle = dlopen(\"libhsa-runtime64.so\", RTLD_NOW);\n\tif (!rocr_handle) {\n\t\tFT_ERR(\"Failed to dlopen libhsa-runtime64.so\");\n\t\tgoto err;\n\t}\n\n\trocr_ops.hsa_memory_copy = dlsym(rocr_handle, \"hsa_memory_copy\");\n\tif (!rocr_ops.hsa_memory_copy) {\n\t\tFT_ERR(\"Failed to find hsa_memory_copy\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_init = dlsym(rocr_handle, \"hsa_init\");\n\tif (!rocr_ops.hsa_init) {\n\t\tFT_ERR(\"Failed to find hsa_init\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_shut_down = dlsym(rocr_handle, \"hsa_shut_down\");\n\tif (!rocr_ops.hsa_shut_down) {\n\t\tFT_ERR(\"Failed to find hsa_shut_down\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_status_string = dlsym(rocr_handle, \"hsa_status_string\");\n\tif (!rocr_ops.hsa_status_string) {\n\t\tFT_ERR(\"Failed to find hsa_status_string\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_agent_get_info = dlsym(rocr_handle, \"hsa_agent_get_info\");\n\tif (!rocr_ops.hsa_agent_get_info) {\n\t\tFT_ERR(\"Failed to find hsa_agent_get_info\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_region_get_info = dlsym(rocr_handle,\n\t\t\t\t\t     \"hsa_region_get_info\");\n\tif (!rocr_ops.hsa_region_get_info) {\n\t\tFT_ERR(\"Failed to find hsa_region_get_info\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_iterate_agents = dlsym(rocr_handle, \"hsa_iterate_agents\");\n\tif (!rocr_ops.hsa_iterate_agents) {\n\t\tFT_ERR(\"Failed to find hsa_iterate_agents\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_agent_iterate_regions =\n\t\tdlsym(rocr_handle, \"hsa_agent_iterate_regions\");\n\tif (!rocr_ops.hsa_agent_iterate_regions) {\n\t\tFT_ERR(\"Failed to find hsa_agent_iterate_regions\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_memory_allocate =\n\t\tdlsym(rocr_handle, \"hsa_memory_allocate\");\n\tif (!rocr_ops.hsa_memory_allocate) {\n\t\tFT_ERR(\"Failed to find hsa_memory_allocate\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_memory_free = dlsym(rocr_handle, \"hsa_memory_free\");\n\tif (!rocr_ops.hsa_memory_free) {\n\t\tFT_ERR(\"Failed to find hsa_memory_free\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\trocr_ops.hsa_amd_memory_fill = dlsym(rocr_handle,\n\t\t\t\t\t     \"hsa_amd_memory_fill\");\n\tif (!rocr_ops.hsa_amd_memory_fill) {\n\t\tFT_ERR(\"Failed to find hsa_amd_memory_fill\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\thsa_ret = rocr_ops.hsa_init();\n\tif (hsa_ret != HSA_STATUS_SUCCESS) {\n\t\tROCR_ERR(hsa_ret, \"hsa_init failed\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\thsa_ret = rocr_ops.hsa_iterate_agents(agent_cb, NULL);\n\tif (hsa_ret != HSA_STATUS_INFO_BREAK) {\n\t\tFT_ERR(\"Failed to find GPU agent\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\thsa_ret = rocr_ops.hsa_agent_iterate_regions(gpu_agent, region_cb,\n\t\t\t\t\t\t     NULL);\n\tif (hsa_ret != HSA_STATUS_INFO_BREAK) {\n\t\tFT_ERR(\"Failed to find GPU region\");\n\t\tgoto err_dlclose_rocr;\n\t}\n\n\treturn FI_SUCCESS;\n\nerr_dlclose_rocr:\n\tdlclose(rocr_handle);\nerr:\n\treturn -FI_ENODATA;\n}\n\nint ft_rocr_cleanup(void)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = rocr_ops.hsa_shut_down();\n\tif (hsa_ret != HSA_STATUS_SUCCESS) {\n\t\tROCR_ERR(hsa_ret, \"hsa_init failed\");\n\t\treturn -FI_ENODATA;\n\t}\n\n\tdlclose(rocr_handle);\n\n\treturn FI_SUCCESS;\n}\n\nint ft_rocr_alloc(uint64_t device, void **buf, size_t size)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = rocr_ops.hsa_memory_allocate(gpu_region, size, buf);\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn FI_SUCCESS;\n\n\tROCR_ERR(hsa_ret, \"hsa_memory_allocate failed\");\n\n\treturn -FI_ENOMEM;\n}\n\nint ft_rocr_free(void *buf)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = rocr_ops.hsa_memory_free(buf);\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn FI_SUCCESS;\n\n\tROCR_ERR(hsa_ret, \"hsa_memory_free failed\");\n\n\treturn -FI_EIO;\n}\n\n#define ROCR_MEM_FILL_BYTE_ALIGNMENT 4U\n\nint ft_rocr_memset(uint64_t device, void *buf, int value, size_t size)\n{\n\tunsigned char set_value = value;\n\tvoid *mem_fill_ptr;\n\tsize_t mem_fill_size;\n\tuint32_t mem_fill_value;\n\thsa_status_t hsa_ret;\n\tunsigned char *ptr = buf;\n\tint ret;\n\n\t/* Determine if ROCR memory fill can be used to set device memory. ROCR\n\t * memory fill requires 4-byte alignment.\n\t */\n\tmem_fill_ptr = (void *) ALIGN((uintptr_t) buf,\n\t\t\t\t      ROCR_MEM_FILL_BYTE_ALIGNMENT);\n\n\t/* Use ROCR memory copy to fill the start of the buffer until the buffer\n\t * is correctly aligned.\n\t */\n\twhile (ptr != mem_fill_ptr && size > 0) {\n\t\tret = ft_rocr_memcpy(device, ptr, &set_value, sizeof(*ptr));\n\t\tif (ret != FI_SUCCESS)\n\t\t\treturn ret;\n\n\t\tsize--;\n\t\tptr++;\n\t}\n\n\t/* Use ROCR memory fill to fill the middle of the buffer. */\n\tif (size >= ROCR_MEM_FILL_BYTE_ALIGNMENT) {\n\t\tmem_fill_size = ALIGN_DOWN(size, ROCR_MEM_FILL_BYTE_ALIGNMENT);\n\n\t\tmemset(&mem_fill_value, set_value, sizeof(mem_fill_value));\n\n\t\thsa_ret = rocr_ops.hsa_amd_memory_fill(mem_fill_ptr,\n\t\t\t\t\t\t       mem_fill_value,\n\t\t\t\t\t\t       mem_fill_size /\n\t\t\t\t\t\t       ROCR_MEM_FILL_BYTE_ALIGNMENT);\n\t\tif (hsa_ret != HSA_STATUS_SUCCESS) {\n\t\t\tROCR_ERR(hsa_ret, \"hsa_amd_memory_fill failed\");\n\t\t\treturn -FI_EIO;\n\t\t}\n\n\t\tsize -= mem_fill_size;\n\t\tptr += mem_fill_size;\n\t}\n\n\t/* Use ROCR memory copy to fill the end of the buffer. */\n\twhile (size > 0) {\n\t\tret = ft_rocr_memcpy(device, ptr, &set_value, sizeof(*ptr));\n\t\tif (ret != FI_SUCCESS)\n\t\t\treturn ret;\n\n\t\tsize--;\n\t\tptr++;\n\t}\n\n\treturn FI_SUCCESS;\n}\n\nint ft_rocr_memcpy(uint64_t device, void *dst, const void *src, size_t size)\n{\n\thsa_status_t hsa_ret;\n\n\thsa_ret = rocr_ops.hsa_memory_copy(dst, src, size);\n\tif (hsa_ret == HSA_STATUS_SUCCESS)\n\t\treturn FI_SUCCESS;\n\n\tROCR_ERR(hsa_ret, \"hsa_memory_copy failed\");\n\n\treturn -FI_EIO;\n}\n\n#else\n\nint ft_rocr_init(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_rocr_cleanup(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_rocr_alloc(uint64_t device, void **buf, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_rocr_free(void *buf)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_rocr_memset(uint64_t device, void *buf, int value, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\nint ft_rocr_memcpy(uint64_t device, void *dst, const void *src, size_t size)\n{\n\treturn -FI_ENOSYS;\n}\n\n#endif /* HAVE_ROCR_RUNTIME_H */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/prov/psm/src/psmx_am.c": "/*\n * Copyright (c) 2013-2017 Intel Corporation. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include \"psmx.h\"\n\nstruct psm_am_parameters psmx_am_param;\n\nstatic psm_am_handler_fn_t psmx_am_handlers[3] = {\n\tpsmx_am_rma_handler,\n\tpsmx_am_msg_handler,\n\tpsmx_am_atomic_handler,\n};\n\nstatic int psmx_am_handlers_idx[3];\nstatic int psmx_am_handlers_initialized = 0;\n\n/* The AM handler signature is different between PSM1 and PSM2. The compat\n * handlers are used when compiled with PSM1 headers and run over the\n * psm2-compat library.\n */\n\nint psmx_am_compat_mode = 0;\nstatic int (*psmx_am_get_source)(psm_am_token_t token, psm_epaddr_t *epaddr);\n\nstatic int psmx_am_compat_rma_handler(psm_am_token_t token,\n\t\t\t\t      psm_amarg_t *args, int nargs,\n\t\t\t\t      void *src, uint32_t len)\n{\n\tpsm_epaddr_t epaddr;\n\t(*psmx_am_get_source)(token, &epaddr);\n\treturn psmx_am_rma_handler(token, epaddr, args, nargs, src, len);\n}\n\nstatic int psmx_am_compat_msg_handler(psm_am_token_t token,\n\t\t\t\t      psm_amarg_t *args, int nargs,\n\t\t\t\t      void *src, uint32_t len)\n{\n\tpsm_epaddr_t epaddr;\n\t(*psmx_am_get_source)(token, &epaddr);\n\treturn psmx_am_msg_handler(token, epaddr, args, nargs, src, len);\n}\n\nstatic int psmx_am_compat_atomic_handler(psm_am_token_t token,\n\t\t\t\t\t psm_amarg_t *args, int nargs,\n\t\t\t\t\t void *src, uint32_t len)\n{\n\tpsm_epaddr_t epaddr;\n\t(*psmx_am_get_source)(token, &epaddr);\n\treturn psmx_am_atomic_handler(token, epaddr, args, nargs, src, len);\n}\n\nint psmx_am_progress(struct psmx_fid_domain *domain)\n{\n\tstruct slist_entry *item;\n\tstruct psmx_am_request *req;\n\tstruct psmx_trigger *trigger;\n\n\tif (psmx_env.am_msg) {\n\t\tfastlock_acquire(&domain->send_queue.lock);\n\t\twhile (!slist_empty(&domain->send_queue.list)) {\n\t\t\titem = slist_remove_head(&domain->send_queue.list);\n\t\t\treq = container_of(item, struct psmx_am_request, list_entry);\n\t\t\tfastlock_release(&domain->send_queue.lock);\n\t\t\tpsmx_am_process_send(domain, req);\n\t\t\tfastlock_acquire(&domain->send_queue.lock);\n\t\t}\n\t\tfastlock_release(&domain->send_queue.lock);\n\t}\n\n\tif (psmx_env.tagged_rma) {\n\t\tfastlock_acquire(&domain->rma_queue.lock);\n\t\twhile (!slist_empty(&domain->rma_queue.list)) {\n\t\t\titem = slist_remove_head(&domain->rma_queue.list);\n\t\t\treq = container_of(item, struct psmx_am_request, list_entry);\n\t\t\tfastlock_release(&domain->rma_queue.lock);\n\t\t\tpsmx_am_process_rma(domain, req);\n\t\t\tfastlock_acquire(&domain->rma_queue.lock);\n\t\t}\n\t\tfastlock_release(&domain->rma_queue.lock);\n\t}\n\n\tfastlock_acquire(&domain->trigger_queue.lock);\n\twhile (!slist_empty(&domain->trigger_queue.list)) {\n\t\titem = slist_remove_head(&domain->trigger_queue.list);\n\t\ttrigger = container_of(item, struct psmx_trigger, list_entry);\n\t\tfastlock_release(&domain->trigger_queue.lock);\n\t\tpsmx_process_trigger(domain, trigger);\n\t\tfastlock_acquire(&domain->trigger_queue.lock);\n\t}\n\tfastlock_release(&domain->trigger_queue.lock);\n\n\treturn 0;\n}\n\nint psmx_am_init(struct psmx_fid_domain *domain)\n{\n\tpsm_ep_t psm_ep = domain->psm_ep;\n\tsize_t size;\n\tint err = 0;\n\n\tFI_INFO(&psmx_prov, FI_LOG_CORE, \"\\n\");\n\n\tpsmx_atomic_init();\n\n\tif (!psmx_am_handlers_initialized) {\n\t\terr = psm_am_get_parameters(psm_ep, &psmx_am_param,\n\t\t\t\t\t\tsizeof(psmx_am_param), &size);\n\t\tif (err)\n\t\t\treturn psmx_errno(err);\n\n\t\tif (psmx_am_compat_mode) {\n\t\t\tvoid *dlsym(void*, const char *);\n\t\t\tpsmx_am_get_source = dlsym(NULL, \"psm2_am_get_source\");\n\t\t\tif (!psmx_am_get_source) {\n\t\t\t\tFI_WARN(&psmx_prov, FI_LOG_CORE,\n\t\t\t\t\t\"failed to load function psm2_am_get_source\\n\");\n\t\t\t\treturn -FI_EOTHER;\n\t\t\t}\n\n\t\t\tpsmx_am_handlers[0] = (void *)psmx_am_compat_rma_handler;\n\t\t\tpsmx_am_handlers[1] = (void *)psmx_am_compat_msg_handler;\n\t\t\tpsmx_am_handlers[2] = (void *)psmx_am_compat_atomic_handler;\n\t\t}\n\n\t\terr = psm_am_register_handlers(psm_ep, psmx_am_handlers, 3,\n\t\t\t\t\t\tpsmx_am_handlers_idx);\n\t\tif (err)\n\t\t\treturn psmx_errno(err);\n\n\t\tif ((psmx_am_handlers_idx[0] != PSMX_AM_RMA_HANDLER) ||\n\t\t    (psmx_am_handlers_idx[1] != PSMX_AM_MSG_HANDLER) ||\n\t\t    (psmx_am_handlers_idx[2] != PSMX_AM_ATOMIC_HANDLER)) {\n\t\t\tFI_WARN(&psmx_prov, FI_LOG_CORE,\n\t\t\t\t\"failed to register one or more AM handlers \"\n\t\t\t\t\"at indecies %d, %d, %d\\n\", PSMX_AM_RMA_HANDLER,\n\t\t\t\tPSMX_AM_MSG_HANDLER, PSMX_AM_ATOMIC_HANDLER);\n\t\t\treturn -FI_EBUSY;\n\t\t}\n\n\t\tpsmx_am_handlers_initialized = 1;\n\t}\n\n\tslist_init(&domain->rma_queue.list);\n\tslist_init(&domain->recv_queue.list);\n\tslist_init(&domain->unexp_queue.list);\n\tslist_init(&domain->trigger_queue.list);\n\tslist_init(&domain->send_queue.list);\n\tfastlock_init(&domain->rma_queue.lock);\n\tfastlock_init(&domain->recv_queue.lock);\n\tfastlock_init(&domain->unexp_queue.lock);\n\tfastlock_init(&domain->trigger_queue.lock);\n\tfastlock_init(&domain->send_queue.lock);\n\n\treturn err;\n}\n\nint psmx_am_fini(struct psmx_fid_domain *domain)\n{\n\tfastlock_destroy(&domain->rma_queue.lock);\n\tfastlock_destroy(&domain->recv_queue.lock);\n\tfastlock_destroy(&domain->unexp_queue.lock);\n\tfastlock_destroy(&domain->trigger_queue.lock);\n\tfastlock_destroy(&domain->send_queue.lock);\n\n\tpsmx_atomic_fini();\n\n\treturn 0;\n}\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/prov/util/src/util_mem_hooks.c": "/*\n * Copyright (c) 2004-2007 The Trustees of Indiana University and Indiana\n *                         University Research and Technology\n *                         Corporation.  All rights reserved.\n * Copyright (c) 2004-2005 The University of Tennessee and The University\n *                         of Tennessee Research Foundation.  All rights\n *                         reserved.\n * Copyright (c) 2004-2005 High Performance Computing Center Stuttgart,\n *                         University of Stuttgart.  All rights reserved.\n * Copyright (c) 2004-2005 The Regents of the University of California.\n *                         All rights reserved.\n * Copyright (c) 2009-2017 Cisco Systems, Inc.  All rights reserved\n * Copyright (c) 2013-2018 Los Alamos National Security, LLC. All rights\n *                         reserved.\n * Copyright (c) 2016-2017 Research Organization for Information Science\n *                         and Technology (RIST). All rights reserved.\n * Copyright (c) 2016-2020 IBM Corporation.  All rights reserved.\n * Copyright (c) 2019 Intel Corporation, Inc.  All rights reserved.\n * Copyright (c) 2020 Amazon.com, Inc. or its affiliates. All rights reserved.\n *\n * License text from Open-MPI (www.open-mpi.org/community/license.php)\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are\n * met:\n *\n * - Redistributions of source code must retain the above copyright\n * notice, this list of conditions and the following disclaimer.\n *\n * - Redistributions in binary form must reproduce the above copyright\n * notice, this list of conditions and the following disclaimer listed\n * in this license in the documentation and/or other materials\n * provided with the distribution.\n *\n * - Neither the name of the copyright holders nor the names of its\n * contributors may be used to endorse or promote products derived from\n * this software without specific prior written permission.\n *\n * The copyright holders provide no reassurances that the source code\n * provided does not infringe any patent, copyright, or any other\n * intellectual property rights of third parties.  The copyright holders\n * disclaim any liability to any recipient for claims brought against\n * recipient by any third party for infringement of that parties\n * intellectual property rights.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <ofi_mr.h>\n#include <ofi_mem.h>\n\nstatic int ofi_memhooks_start(struct ofi_mem_monitor *monitor);\nstatic void ofi_memhooks_stop(struct ofi_mem_monitor *monitor);\n\nstruct ofi_memhooks memhooks = {\n\t.monitor.iface = FI_HMEM_SYSTEM,\n\t.monitor.init = ofi_monitor_init,\n\t.monitor.cleanup = ofi_monitor_cleanup,\n\t.monitor.start = ofi_memhooks_start,\n\t.monitor.stop = ofi_memhooks_stop,\n};\nstruct ofi_mem_monitor *memhooks_monitor = &memhooks.monitor;\n\n\n/* memhook support checks */\n#if HAVE_MEMHOOKS_MONITOR\n\n#include <sys/mman.h>\n#include <sys/syscall.h>\n#include <sys/types.h>\n#include <sys/shm.h>\n#include <sys/ipc.h>\n#include <unistd.h>\n#include <dlfcn.h>\n#include <fcntl.h>\n#include <link.h>\n\n#if HAVE_DECL___SYSCALL && defined(HAVE___SYSCALL)\n/* calling __syscall is preferred on some systems when some arguments may be 64-bit. it also\n * has the benefit of having an off_t return type */\n#define ofi_memhooks_syscall __syscall\n#else\n#define ofi_memhooks_syscall syscall\n#endif\n\n// These op codes used to be in bits/ipc.h but were removed in glibc in 2015\n// with a comment saying they should be defined in internal headers:\n// https://sourceware.org/bugzilla/show_bug.cgi?id=18560\n// and when glibc uses that syscall it seems to do so from its own definitions:\n// https://github.com/bminor/glibc/search?q=IPCOP_shmat&unscoped_q=IPCOP_shmat\n#if (!defined(SYS_shmat) && !defined(IPCOP_shmat))\n#define IPCOP_shmat                21\n#endif\n#if (!defined(SYS_shmdt) && !defined(IPCOP_shmdt))\n#define IPCOP_shmdt                22\n#endif\n\n#define OFI_INTERCEPT_MAX_PATCH 32\n\nstruct ofi_intercept {\n\tstruct dlist_entry \t\tentry;\n\tconst char\t\t\t*symbol;\n\tvoid\t\t\t\t*our_func;\n\tvoid\t\t\t\t*orig_func;\n\tunsigned char\t\t\tpatch_data[OFI_INTERCEPT_MAX_PATCH];\n\tunsigned char\t\t\tpatch_orig_data[OFI_INTERCEPT_MAX_PATCH];\n\tunsigned\t\t\tpatch_data_size;\n\tstruct dlist_entry\t\tdl_intercept_list;\n};\n\nenum {\n\tOFI_INTERCEPT_MMAP,\n\tOFI_INTERCEPT_MUNMAP,\n\tOFI_INTERCEPT_MREMAP,\n\tOFI_INTERCEPT_MADVISE,\n\tOFI_INTERCEPT_SHMAT,\n\tOFI_INTERCEPT_SHMDT,\n\tOFI_INTERCEPT_BRK,\n\tOFI_INTERCEPT_MAX\n};\n\nstatic void *ofi_intercept_mmap(void *start, size_t length,\n\t\t\t\tint prot, int flags, int fd, off_t offset);\nstatic int ofi_intercept_munmap(void *start, size_t length);\nstatic void *ofi_intercept_mremap(void *old_address, size_t old_size,\n\t\tsize_t new_size, int flags, void *new_address);\nstatic int ofi_intercept_madvise(void *addr, size_t length, int advice);\nstatic void *ofi_intercept_shmat(int shmid, const void *shmaddr, int shmflg);\nstatic int ofi_intercept_shmdt(const void *shmaddr);\nstatic int ofi_intercept_brk(const void *brkaddr);\n\nstatic struct ofi_intercept intercepts[] = {\n\t[OFI_INTERCEPT_MMAP] = { .symbol = \"mmap\",\n\t\t\t\t.our_func = ofi_intercept_mmap},\n\t[OFI_INTERCEPT_MUNMAP] = { .symbol = \"munmap\",\n\t\t\t\t.our_func = ofi_intercept_munmap},\n\t[OFI_INTERCEPT_MREMAP] = { .symbol = \"mremap\",\n\t\t\t\t.our_func = ofi_intercept_mremap},\n\t[OFI_INTERCEPT_MADVISE] = { .symbol = \"madvise\",\n\t\t\t\t.our_func = ofi_intercept_madvise},\n\t[OFI_INTERCEPT_SHMAT] = { .symbol = \"shmat\",\n\t\t\t\t.our_func = ofi_intercept_shmat},\n\t[OFI_INTERCEPT_SHMDT] = { .symbol = \"shmdt\",\n\t\t\t\t.our_func = ofi_intercept_shmdt},\n\t[OFI_INTERCEPT_BRK] = { .symbol = \"brk\",\n\t\t\t\t.our_func = ofi_intercept_brk},\n};\n\n#ifdef HAVE___CURBRK\nextern void *__curbrk; /* in libc */\n#endif\n\n#if HAVE___CLEAR_CACHE\n/*\n * Used on ARM64 platforms, see https://github.com/open-mpi/ompi/issues/5631\n */\nstatic inline void ofi_clear_instruction_cache(uintptr_t address, size_t data_size)\n{\n\t/* do not allow global declaration of compiler intrinsic */\n\tvoid __clear_cache(void* beg, void* end);\n\n\t__clear_cache ((void *) address, (void *) (address + data_size));\n}\n#else\nstatic inline void ofi_clear_instruction_cache(uintptr_t address, size_t data_size)\n{\n\tsize_t i;\n\tsize_t offset_jump = 16;\n#if defined(__aarch64__)\n\toffset_jump = 32;\n#endif\n\t/* align the address */\n\taddress &= ~(offset_jump - 1);\n\n\tfor (i = 0 ; i < data_size ; i += offset_jump) {\n#if (defined(__x86_64__) || defined(__amd64__))\n\t\t__asm__ volatile(\"mfence;clflush %0;mfence\"::\n\t\t\t\t \"m\" (*((char*) address + i)));\n#elif defined(__aarch64__)\n\t\t__asm__ volatile (\"dc cvau, %0\\n\\t\"\n\t\t\t  \"dsb ish\\n\\t\"\n\t\t\t  \"ic ivau, %0\\n\\t\"\n\t\t\t  \"dsb ish\\n\\t\"\n\t\t\t  \"isb\":: \"r\" (address + i));\n#endif\n\t}\n}\n#endif\n\nstatic inline int ofi_write_patch(unsigned char *patch_data, void *address,\n\t\t\t\t  size_t data_size)\n{\n\tlong page_size;\n\tvoid *base;\n\tvoid *bound;\n\tsize_t length;\n\n\tpage_size = ofi_get_page_size();\n\tif (page_size < 0) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t\t\"failed to get page size: %s\\n\", fi_strerror(-page_size));\n\t\treturn page_size;\n\t}\n\n\tbase = ofi_get_page_start(address, page_size);\n\tbound = ofi_get_page_end(address, page_size);\n\tlength = (uintptr_t) bound - (uintptr_t) base;\n\n\tif (mprotect(base, length, PROT_EXEC|PROT_READ|PROT_WRITE)) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t\t\"mprotect to set PROT_WRITE on %p len %lu failed: %s\\n\",\n\t\t\t(void *) base, length, strerror(errno));\n\t\treturn -errno;\n\t}\n\n\tmemcpy(address, patch_data, data_size);\n\n\tofi_clear_instruction_cache((uintptr_t) address, data_size);\n\n\t/*\n\t * Nothing we can do here if this fails so ignore the return code. It\n\t * shouldn't due to alignment since the parameters are the same as\n\t * before.\n\t */\n\tif (mprotect(base, length, PROT_EXEC|PROT_READ))\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t\t\"mprotect to drop PROT_WRITE on %p len %lu failed: %s\\n\",\n\t\t\t base, length, strerror(errno));\n\n\treturn 0;\n}\n\nstatic int ofi_apply_patch(struct ofi_intercept *intercept)\n{\n\tmemcpy(intercept->patch_orig_data, intercept->orig_func,\n\t       intercept->patch_data_size);\n\treturn ofi_write_patch(intercept->patch_data, intercept->orig_func,\n\t\t\t       intercept->patch_data_size);\n}\n\nstatic int ofi_remove_patch(struct ofi_intercept *intercept)\n{\n\treturn ofi_write_patch(intercept->patch_orig_data, intercept->orig_func,\n\t\t\t       intercept->patch_data_size);\n}\n\nstatic void ofi_restore_intercepts(void)\n{\n\tstruct ofi_intercept *intercept;\n\n\tdlist_foreach_container(&memhooks.intercept_list, struct ofi_intercept,\n\t\tintercept, entry)\n\t\tofi_remove_patch(intercept);\n}\n\n#if (defined(__x86_64___) || defined(__amd64__))\nstatic int ofi_patch_function(struct ofi_intercept *intercept)\n{\n\tintercept->patch_data_size = 13;\n\t*(unsigned short*)(intercept->patch_data + 0) = 0xbb49;\n\t*(unsigned long* )(intercept->patch_data + 2) =\n\t\t(unsigned long) intercept->our_func;\n\t*(unsigned char*) (intercept->patch_data +10) = 0x41;\n\t*(unsigned char*) (intercept->patch_data +11) = 0xff;\n\t*(unsigned char*) (intercept->patch_data +12) = 0xe3;\n\n\treturn ofi_apply_patch(intercept);\n}\n#elif defined(__aarch64__)\n/**\n * @brief Generate a mov immediate instruction\n *\n * @param[in] reg   register number (0-31)\n * @param[in] shift shift amount (0-3) * 16-bits\n * @param[in] value immediate value\n */\nstatic uint32_t mov(unsigned int reg, uint16_t shift, uint16_t value)\n{\n\treturn (0x1a5 << 23) + ((uint32_t) shift << 21) + ((uint32_t) value << 5) + reg;\n}\n\n/**\n * @brief Generate a mov immediate with keep instruction\n *\n * @param[in] reg   register number (0-31)\n * @param[in] shift shift amount (0-3) * 16-bits\n * @param[in] value immediate value\n */\nstatic uint32_t movk(unsigned int reg, uint16_t shift, uint16_t value)\n{\n\treturn (0x1e5 << 23) + ((uint32_t) shift << 21) + ((uint32_t) value << 5) + reg;\n}\n\n/**\n * @brief Generate a branch to register instruction\n *\n * @param[in] reg   register number (0-31)\n */\nstatic uint32_t br(unsigned int reg)\n{\n\treturn (0xd61f << 16) + (reg << 5);\n}\n\nstatic int ofi_patch_function(struct ofi_intercept *intercept)\n{\n\t/*\n\t * r15 is the highest numbered temporary register. I am\n\t * assuming this one is safe to use.\n\t */\n\tconst unsigned int gr = 15;\n\tuintptr_t addr = (uintptr_t) intercept->patch_data;\n\tuintptr_t value = (uintptr_t) intercept->our_func;\n\n\t*(uint32_t *) (addr +  0) = mov(gr, 3, value >> 48);\n\t*(uint32_t *) (addr +  4) = movk(gr, 2, value >> 32);\n\t*(uint32_t *) (addr +  8) = movk(gr, 1, value >> 16);\n\t*(uint32_t *) (addr + 12) = movk(gr, 0, value);\n\tintercept->patch_data_size = 16;\n\n\t*(uint32_t *) ((uintptr_t) intercept->patch_data +\n\t\t       intercept->patch_data_size) = br(gr);\n\tintercept->patch_data_size = intercept->patch_data_size + 4;\n\n\treturn ofi_apply_patch(intercept);\n}\n#endif\n\n/*\n * This implementation intercepts syscalls by overwriting the beginning of\n * glibc's functions with a jump to our intercept function. After notifying the\n * cache we will make the syscall directly. We store the original instructions\n * and restore them when memhooks is unloaded.\n */\nstatic int ofi_intercept_symbol(struct ofi_intercept *intercept)\n{\n\tvoid *func_addr;\n\tint ret;\n\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"overwriting function %s\\n\", intercept->symbol);\n\n\tfunc_addr = dlsym(RTLD_NEXT, intercept->symbol);\n\tif (!func_addr) {\n\t\tfunc_addr = dlsym(RTLD_DEFAULT, intercept->symbol);\n\t\tif (!func_addr) {\n\t\t\tFI_DBG(&core_prov, FI_LOG_MR,\n\t\t\t       \"could not find symbol %s\\n\", intercept->symbol);\n\t\t\tret = -FI_ENOMEM;\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tintercept->orig_func = func_addr;\n\n\tret = ofi_patch_function(intercept);\n\n\tif (!ret)\n\t\tdlist_insert_tail(&intercept->entry, &memhooks.intercept_list);\n\n\treturn ret;\n}\n\nvoid ofi_intercept_handler(const void *addr, size_t len)\n{\n\tpthread_rwlock_rdlock(&mm_list_rwlock);\n\tpthread_mutex_lock(&mm_lock);\n\tofi_monitor_notify(memhooks_monitor, addr, len);\n\tpthread_mutex_unlock(&mm_lock);\n\tpthread_rwlock_unlock(&mm_list_rwlock);\n}\n\nstatic void *ofi_intercept_mmap(void *start, size_t length,\n                            int prot, int flags, int fd, off_t offset)\n{\n\tif ((flags & MAP_FIXED) && start)\n\t\tofi_intercept_handler(start, length);\n\n\treturn (void *)(intptr_t) ofi_memhooks_syscall(SYS_mmap, start, length,\n\t\t\t\t\t\t       prot, flags, fd, offset);\n}\n\nstatic int ofi_intercept_munmap(void *start, size_t length)\n{\n\tofi_intercept_handler(start, length);\n\n\treturn ofi_memhooks_syscall(SYS_munmap, start, length);\n}\n\nstatic void *ofi_intercept_mremap(void *old_address, size_t old_size,\n\t\tsize_t new_size, int flags, void *new_address)\n{\n\tofi_intercept_handler(old_address, old_size);\n\n#ifdef MREMAP_FIXED\n\t/*\n\t * new_address is an optional argument. Explicitly set it to NULL\n\t * if it is not applicable.\n\t */\n\tif (!(flags & MREMAP_FIXED))\n\t\tnew_address = NULL;\n#endif\n\n\treturn (void *)(intptr_t) ofi_memhooks_syscall(SYS_mremap, old_address,\n\t\t\t\t\t\t       old_size, new_size,\n\t\t\t\t\t\t       flags, new_address);\n}\n\nstatic int ofi_intercept_madvise(void *addr, size_t length, int advice)\n{\n\tif (advice == MADV_DONTNEED ||\n#ifdef MADV_FREE\n\t    advice == MADV_FREE ||\n#endif\n#ifdef MADV_REMOVE\n\t    advice == MADV_REMOVE ||\n#endif\n\t    advice == POSIX_MADV_DONTNEED) {\n\t\tofi_intercept_handler(addr, length);\n\t}\n\n\treturn ofi_memhooks_syscall(SYS_madvise, addr, length, advice);\n}\n\nstatic void *ofi_intercept_shmat(int shmid, const void *shmaddr, int shmflg)\n{\n\tstruct shmid_ds ds;\n\tconst void *start;\n\tvoid *result;\n\tsize_t len;\n\tint ret;\n\n\tif (shmaddr && (shmflg & SHM_REMAP)) {\n\t\tret = shmctl(shmid, IPC_STAT, &ds);\n\t\tlen = (ret < 0) ? 0 : ds.shm_segsz;\n\n\t\tif (shmflg & SHM_RND) {\n\t\t\tstart = (char *) shmaddr - ((uintptr_t) shmaddr) % SHMLBA;\n\t\t\tlen += ((uintptr_t) shmaddr) % SHMLBA;\n\t\t} else {\n\t\t\tstart = shmaddr;\n\t\t}\n\n\t\tofi_intercept_handler(start, len);\n\t}\n\n#ifdef SYS_shmat\n\tresult = (void *) ofi_memhooks_syscall(SYS_shmat, shmid, shmaddr, shmflg);\n#else // IPCOP_shmat\n\tunsigned long sysret;\n\tsysret = ofi_memhooks_syscall(SYS_ipc, IPCOP_shmat,\n\t\t\t\t      shmid, shmflg, &shmaddr, shmaddr);\n\tresult = (sysret > -(unsigned long)SHMLBA) ? (void *)sysret :\n\t\t\t\t\t\t     (void *)shmaddr;\n#endif\n\treturn result;\n}\n\nstatic int ofi_intercept_shmdt(const void *shmaddr)\n{\n\tint ret;\n\n\t/*\n\t * Overly aggressive, but simple.  Invalidate everything after shmaddr.\n\t * We could choose to find the shared memory segment size in /proc but\n\t * that seems like a great way to deadlock ourselves.\n\t */\n\tofi_intercept_handler(shmaddr, SIZE_MAX - (uintptr_t) shmaddr);\n\n#ifdef SYS_shmdt\n\tret = ofi_memhooks_syscall(SYS_shmdt, shmaddr);\n#else // IPCOP_shmdt\n\tret = ofi_memhooks_syscall(SYS_ipc, IPCOP_shmdt, 0, 0, 0, shmaddr);\n#endif\n\treturn ret;\n}\n\nstatic int ofi_intercept_brk(const void *brkaddr)\n{\n\tvoid *old_addr, *new_addr;\n\n#ifdef HAVE___CURBRK\n\told_addr = __curbrk;\n#else\n\told_addr = sbrk(0);\n#endif\n\tnew_addr = (void *) (intptr_t) ofi_memhooks_syscall(SYS_brk, brkaddr);\n\n#ifdef HAVE___CURBRK\n\t/*\n\t * Note: if we were using glibc brk/sbrk, their __curbrk would get\n\t * updated, but since we're going straight to the syscall, we have\n\t * to update __curbrk or else glibc won't see it.\n\t */\n\t__curbrk = new_addr;\n#endif\n\n\tif (new_addr < brkaddr) {\n\t\terrno = ENOMEM;\n\t\treturn -1;\n\t} else if (new_addr < old_addr) {\n\t\tofi_intercept_handler(new_addr, (intptr_t) old_addr -\n\t\t\t\t      (intptr_t) new_addr);\n\t}\n\n\treturn 0;\n}\n\nstatic int ofi_memhooks_subscribe(struct ofi_mem_monitor *monitor,\n\t\t\t\t  const void *addr, size_t len,\n\t\t\t\t  union ofi_mr_hmem_info *hmem_info)\n{\n\t/* no-op */\n\treturn FI_SUCCESS;\n}\n\nstatic void ofi_memhooks_unsubscribe(struct ofi_mem_monitor *monitor,\n\t\t\t\t     const void *addr, size_t len,\n\t\t\t\t     union ofi_mr_hmem_info *hmem_info)\n{\n\t/* no-op */\n}\n\nstatic bool ofi_memhooks_valid(struct ofi_mem_monitor *monitor,\n\t\t\t       const void *addr, size_t len,\n\t\t\t       union ofi_mr_hmem_info *hmem_info)\n{\n\t/* no-op */\n\treturn true;\n}\n\nstatic int ofi_memhooks_start(struct ofi_mem_monitor *monitor)\n{\n\tint i, ret;\n\n\tif (memhooks_monitor->subscribe == ofi_memhooks_subscribe)\n\t\treturn 0;\n\n\tmemhooks_monitor->subscribe = ofi_memhooks_subscribe;\n\tmemhooks_monitor->unsubscribe = ofi_memhooks_unsubscribe;\n\tmemhooks_monitor->valid = ofi_memhooks_valid;\n\tdlist_init(&memhooks.intercept_list);\n\n\tfor (i = 0; i < OFI_INTERCEPT_MAX; ++i)\n\t\tdlist_init(&intercepts[i].dl_intercept_list);\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MMAP]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept mmap failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MUNMAP]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept munmap failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MREMAP]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept mremap failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MADVISE]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept madvise failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_SHMAT]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept shmat failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_SHMDT]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept shmdt failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_BRK]);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept brk failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void ofi_memhooks_stop(struct ofi_mem_monitor *monitor)\n{\n\tofi_restore_intercepts();\n\tmemhooks_monitor->subscribe = NULL;\n\tmemhooks_monitor->unsubscribe = NULL;\n}\n\n#else\n\nstatic int ofi_memhooks_start(struct ofi_mem_monitor *monitor)\n{\n\treturn -FI_ENOSYS;\n}\n\nstatic void ofi_memhooks_stop(struct ofi_mem_monitor *monitor)\n{\n}\n\n#endif /* memhook support checks */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/prov/psm3/psm3/psm_user.h": "/*\n\n  This file is provided under a dual BSD/GPLv2 license.  When using or\n  redistributing this file, you may do so under either license.\n\n  GPL LICENSE SUMMARY\n\n  Copyright(c) 2016 Intel Corporation.\n\n  This program is free software; you can redistribute it and/or modify\n  it under the terms of version 2 of the GNU General Public License as\n  published by the Free Software Foundation.\n\n  This program is distributed in the hope that it will be useful, but\n  WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n  General Public License for more details.\n\n  Contact Information:\n  Intel Corporation, www.intel.com\n\n  BSD LICENSE\n\n  Copyright(c) 2016 Intel Corporation.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions\n  are met:\n\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in\n      the documentation and/or other materials provided with the\n      distribution.\n    * Neither the name of Intel Corporation nor the names of its\n      contributors may be used to endorse or promote products derived\n      from this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n*/\n\n/* Copyright (c) 2003-2016 Intel Corporation. All rights reserved. */\n\n#ifndef _PSMI_USER_H\n#define _PSMI_USER_H\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#include \"psm_config.h\"\n#include <inttypes.h>\n#include <pthread.h>\n\n#include <sched.h>\n#include <numa.h>\n#include <semaphore.h>\n#include <fcntl.h>\n#include <stdbool.h>\n\n#include \"psm2.h\"\n#include \"psm2_mq.h\"\n\n#include \"ptl.h\"\n\n#include \"opa_user.h\"\n#include \"opa_queue.h\"\n\n#include \"psm_log.h\"\n#include \"psm_perf.h\"\n\n#define PSMI_LOCK_NO_OWNER\t((pthread_t)(-1))\n\n#define _PSMI_IN_USER_H\n\n/* Opaque hw context pointer used in HAL,\n   and defined by each HAL instance. */\ntypedef void *psmi_hal_hw_context;\n\n#include \"psm_help.h\"\n#include \"psm_error.h\"\n#include \"psm_context.h\"\n#include \"psm_utils.h\"\n#include \"psm_timer.h\"\n#include \"psm_mpool.h\"\n#include \"psm_ep.h\"\n#include \"psm_lock.h\"\n#include \"psm_stats.h\"\n#include \"psm2_mock_testing.h\"\n\n#undef _PSMI_IN_USER_H\n\n#define PSMI_VERNO_MAKE(major, minor) ((((major)&0xff)<<8)|((minor)&0xff))\n#define PSMI_VERNO  PSMI_VERNO_MAKE(PSM2_VERNO_MAJOR, PSM2_VERNO_MINOR)\n#define PSMI_VERNO_GET_MAJOR(verno) (((verno)>>8) & 0xff)\n#define PSMI_VERNO_GET_MINOR(verno) (((verno)>>0) & 0xff)\n\nint psmi_verno_client();\nint psmi_verno_isinteroperable(uint16_t verno);\nint MOCKABLE(psmi_isinitialized)();\nMOCK_DCL_EPILOGUE(psmi_isinitialized);\n\npsm2_error_t psmi_poll_internal(psm2_ep_t ep, int poll_amsh);\npsm2_error_t psmi_mq_wait_internal(psm2_mq_req_t *ireq);\n\nint psmi_get_current_proc_location();\n\nextern int psmi_epid_ver;\nextern int psmi_allow_routers;\nextern uint32_t non_dw_mul_sdma;\nextern psmi_lock_t psmi_creation_lock;\nextern psm2_ep_t psmi_opened_endpoint;\n\nextern int psmi_affinity_shared_file_opened;\nextern uint64_t *shared_affinity_ptr;\nextern char *affinity_shm_name;\n\nextern sem_t *sem_affinity_shm_rw;\nextern int psmi_affinity_semaphore_open;\nextern char *sem_affinity_shm_rw_name;\n\nPSMI_ALWAYS_INLINE(\nint\n_psmi_get_epid_version()) {\n\treturn psmi_epid_ver;\n}\n\n#define PSMI_EPID_VERSION_SHM \t\t\t\t0\n#define PSMI_EPID_SHM_ONLY\t\t\t\t1\n#define PSMI_EPID_IPS_SHM\t\t\t\t0\n#define PSMI_EPID_VERSION \t\t\t\t_psmi_get_epid_version()\n#define PSMI_MAX_EPID_VERNO_SUPPORTED\t\t\t4\n#define PSMI_MIN_EPID_VERNO_SUPPORTED\t\t\t3\n#define PSMI_EPID_VERNO_DEFAULT\t\t\t\t3\t// allows 3 or 4 based on NIC\n#define PSMI_EPID_V3\t\t\t\t\t3\t// IB UD\n#define PSMI_EPID_V4\t\t\t\t\t4\t// Eth UD\n\n#define PSMI_EPID_GET_LID(epid) ((PSMI_EPID_GET_EPID_VERSION(epid) == PSMI_EPID_V3) ? \\\n\t\t\t\t\t\t\t\t (int)PSMI_EPID_GET_LID_V3(epid)      \\\n\t\t\t\t\t\t\t   : (int)PSMI_EPID_GET_LID_V4(epid))\n// for V3 we use low 16 and next 16 should be zero\n// for V4 we have network in low 32 bits\n#define PSMI_GET_SUBNET_ID(gid_hi) (gid_hi & 0xffffffff)\n\n\n/*\n * Following is the definition of various lock implementations. The choice is\n * made by defining specific lock type in relevant section of psm_config.h\n */\n#ifdef PSMI_LOCK_IS_SPINLOCK\n#define _PSMI_LOCK_INIT(pl)\tpsmi_spin_init(&((pl).lock))\n#define _PSMI_LOCK_TRY(pl)\tpsmi_spin_trylock(&((pl).lock))\n#define _PSMI_LOCK(pl)\t\tpsmi_spin_lock(&((pl).lock))\n#define _PSMI_UNLOCK(pl)\tpsmi_spin_unlock(&((pl).lock))\n#define _PSMI_LOCK_ASSERT(pl)\n#define _PSMI_UNLOCK_ASSERT(pl)\n#define PSMI_LOCK_DISABLED\t0\n\n#elif defined(PSMI_LOCK_IS_MUTEXLOCK_DEBUG)\n\nPSMI_ALWAYS_INLINE(\nint\n_psmi_mutex_trylock_inner(pthread_mutex_t *mutex,\n\t\t\t  const char *curloc, pthread_t *lock_owner))\n{\n\tpsmi_assert_always_loc(*lock_owner != pthread_self(),\n\t\t\t       curloc);\n\tint ret = pthread_mutex_trylock(mutex);\n\tif (ret == 0)\n\t\t*lock_owner = pthread_self();\n\treturn ret;\n}\n\nPSMI_ALWAYS_INLINE(\nint\n_psmi_mutex_lock_inner(pthread_mutex_t *mutex,\n\t\t       const char *curloc, pthread_t *lock_owner))\n{\n\tpsmi_assert_always_loc(*lock_owner != pthread_self(),\n\t\t\t       curloc);\n\tint ret = pthread_mutex_lock(mutex);\n\tpsmi_assert_always_loc(ret != EDEADLK, curloc);\n\t*lock_owner = pthread_self();\n\treturn ret;\n}\n\nPSMI_ALWAYS_INLINE(\nvoid\n_psmi_mutex_unlock_inner(pthread_mutex_t *mutex,\n\t\t\t const char *curloc, pthread_t *lock_owner))\n{\n\tpsmi_assert_always_loc(*lock_owner == pthread_self(),\n\t\t\t       curloc);\n\t*lock_owner = PSMI_LOCK_NO_OWNER;\n\tpsmi_assert_always_loc(pthread_mutex_unlock(mutex) !=\n\t\t\t       EPERM, curloc);\n\treturn;\n}\n\n#define _PSMI_LOCK_INIT(pl)\t/* static initialization */\n#define _PSMI_LOCK_TRY(pl)\t\t\t\t\t\t\t\\\n\t    _psmi_mutex_trylock_inner(&((pl).lock), PSMI_CURLOC,\t\t\\\n\t\t\t\t\t&((pl).lock_owner))\n#define _PSMI_LOCK(pl)\t\t\t\t\t\t\t\t\\\n\t    _psmi_mutex_lock_inner(&((pl).lock), PSMI_CURLOC,\t\t\t\\\n                                        &((pl).lock_owner))\n#define _PSMI_UNLOCK(pl)\t\t\t\t\t\t\t\\\n\t    _psmi_mutex_unlock_inner(&((pl).lock), PSMI_CURLOC,\t\t\t\\\n                                        &((pl).lock_owner))\n#define _PSMI_LOCK_ASSERT(pl)\t\t\t\t\t\t\t\\\n\tpsmi_assert_always((pl).lock_owner == pthread_self());\n#define _PSMI_UNLOCK_ASSERT(pl)\t\t\t\t\t\t\t\\\n\tpsmi_assert_always((pl).lock_owner != pthread_self());\n#define PSMI_LOCK_DISABLED\t0\n\n#elif defined(PSMI_LOCK_IS_MUTEXLOCK)\n#define _PSMI_LOCK_INIT(pl)\t/* static initialization */\n#define _PSMI_LOCK_TRY(pl)\tpthread_mutex_trylock(&((pl).lock))\n#define _PSMI_LOCK(pl)\t\tpthread_mutex_lock(&((pl).lock))\n#define _PSMI_UNLOCK(pl)\tpthread_mutex_unlock(&((pl).lock))\n#define PSMI_LOCK_DISABLED\t0\n#define _PSMI_LOCK_ASSERT(pl)\n#define _PSMI_UNLOCK_ASSERT(pl)\n\n#elif defined(PSMI_PLOCK_IS_NOLOCK)\n#define _PSMI_LOCK_TRY(pl)\t0\t/* 0 *only* so progress thread never succeeds */\n#define _PSMI_LOCK(pl)\n#define _PSMI_UNLOCK(pl)\n#define PSMI_LOCK_DISABLED\t1\n#define _PSMI_LOCK_ASSERT(pl)\n#define _PSMI_UNLOCK_ASSERT(pl)\n#else\n#error No LOCK lock type declared\n#endif\n\n#define PSMI_YIELD(pl)\t\t\t\t\t\t\t\\\n\tdo { _PSMI_UNLOCK((pl)); sched_yield(); _PSMI_LOCK((pl)); } while (0)\n\n#ifdef PSM2_MOCK_TESTING\n/* If this is a mocking tests build, all the operations on the locks\n * are routed through functions which may be mocked, if necessary.  */\nvoid MOCKABLE(psmi_mockable_lock_init)(psmi_lock_t *pl);\nMOCK_DCL_EPILOGUE(psmi_mockable_lock_init);\n\nint MOCKABLE(psmi_mockable_lock_try)(psmi_lock_t *pl);\nMOCK_DCL_EPILOGUE(psmi_mockable_lock_try);\n\nvoid MOCKABLE(psmi_mockable_lock)(psmi_lock_t *pl);\nMOCK_DCL_EPILOGUE(psmi_mockable_lock);\n\nvoid MOCKABLE(psmi_mockable_unlock)(psmi_lock_t *pl);\nMOCK_DCL_EPILOGUE(psmi_mockable_unlock);\n\nvoid MOCKABLE(psmi_mockable_lock_assert)(psmi_lock_t *pl);\nMOCK_DCL_EPILOGUE(psmi_mockable_lock_assert);\n\nvoid MOCKABLE(psmi_mockable_unlock_assert)(psmi_lock_t *pl);\nMOCK_DCL_EPILOGUE(psmi_mockable_unlock_assert);\n\n#define PSMI_LOCK_INIT(pl)\tpsmi_mockable_lock_init(&(pl))\n#define PSMI_LOCK_TRY(pl)\tpsmi_mockable_lock_try(&(pl))\n#define PSMI_LOCK(pl)\t\tpsmi_mockable_lock(&(pl))\n#define PSMI_UNLOCK(pl)\t\tpsmi_mockable_unlock(&(pl))\n#define PSMI_LOCK_ASSERT(pl)\tpsmi_mockable_lock_assert(&(pl))\n#define PSMI_UNLOCK_ASSERT(pl)\tpsmi_mockable_unlock_assert(&(pl))\n#else\n#define PSMI_LOCK_INIT(pl)\t_PSMI_LOCK_INIT(pl)\n#define PSMI_LOCK_TRY(pl)\t_PSMI_LOCK_TRY(pl)\n#define PSMI_LOCK(pl)\t\t_PSMI_LOCK(pl)\n#define PSMI_UNLOCK(pl)\t\t_PSMI_UNLOCK(pl)\n#define PSMI_LOCK_ASSERT(pl)\t_PSMI_LOCK_ASSERT(pl)\n#define PSMI_UNLOCK_ASSERT(pl)\t_PSMI_UNLOCK_ASSERT(pl)\n#endif\n\n#ifdef PSM_PROFILE\nvoid psmi_profile_block() __attribute__ ((weak));\nvoid psmi_profile_unblock() __attribute__ ((weak));\nvoid psmi_profile_reblock(int did_no_progress) __attribute__ ((weak));\n\n#define PSMI_PROFILE_BLOCK()\t\tpsmi_profile_block()\n#define PSMI_PROFILE_UNBLOCK()\t\tpsmi_profile_unblock()\n#define PSMI_PROFILE_REBLOCK(noprog)\tpsmi_profile_reblock(noprog)\n#else\n#define PSMI_PROFILE_BLOCK()\n#define PSMI_PROFILE_UNBLOCK()\n#define PSMI_PROFILE_REBLOCK(noprog)\n#endif\n\n#ifdef PSM_CUDA\n\n#ifndef PSM_CUDA_MOCK\n#include <cuda.h>\n#include <driver_types.h>\n\n#if CUDA_VERSION < 7000\n#error Please update CUDA driver, required minimum version is 7.0\n#endif\n#else\n// included in stand-alone unit test that does not use real CUDA functions\n#include \"psmi_cuda_mock.h\"\n#endif /* PSM_CUDA_MOCK */\n\nextern int is_cuda_enabled;\nextern int is_gdr_copy_enabled;\nextern int device_support_gpudirect;\nextern int gpu_p2p_supported;\nextern int my_gpu_device;\nextern int cuda_lib_version;\n\nextern CUcontext ctxt;\nextern void *psmi_cuda_lib;\n\nextern CUresult (*psmi_cuInit)(unsigned int  Flags );\nextern CUresult (*psmi_cuCtxDetach)(CUcontext c);\nextern CUresult (*psmi_cuCtxGetCurrent)(CUcontext *c);\nextern CUresult (*psmi_cuCtxSetCurrent)(CUcontext c);\nextern CUresult (*psmi_cuPointerGetAttribute)(void *data, CUpointer_attribute pa, CUdeviceptr p);\nextern CUresult (*psmi_cuPointerSetAttribute)(void *data, CUpointer_attribute pa, CUdeviceptr p);\nextern CUresult (*psmi_cuDeviceCanAccessPeer)(int *canAccessPeer, CUdevice dev, CUdevice peerDev);\nextern CUresult (*psmi_cuDeviceGet)(CUdevice* device, int  ordinal);\nextern CUresult (*psmi_cuDeviceGetAttribute)(int* pi, CUdevice_attribute attrib, CUdevice dev);\nextern CUresult (*psmi_cuDriverGetVersion)(int* driverVersion);\nextern CUresult (*psmi_cuDeviceGetCount)(int* count);\nextern CUresult (*psmi_cuStreamCreate)(CUstream* phStream, unsigned int Flags);\nextern CUresult (*psmi_cuStreamDestroy)(CUstream phStream);\nextern CUresult (*psmi_cuEventCreate)(CUevent* phEvent, unsigned int Flags);\nextern CUresult (*psmi_cuEventDestroy)(CUevent hEvent);\nextern CUresult (*psmi_cuEventQuery)(CUevent hEvent);\nextern CUresult (*psmi_cuEventRecord)(CUevent hEvent, CUstream hStream);\nextern CUresult (*psmi_cuEventSynchronize)(CUevent hEvent);\nextern CUresult (*psmi_cuMemHostAlloc)(void** pp, size_t bytesize, unsigned int Flags);\nextern CUresult (*psmi_cuMemFreeHost)(void* p);\nextern CUresult (*psmi_cuMemcpy)(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);\nextern CUresult (*psmi_cuMemcpyDtoD)(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount);\nextern CUresult (*psmi_cuMemcpyDtoH)(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount);\nextern CUresult (*psmi_cuMemcpyHtoD)(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount);\nextern CUresult (*psmi_cuMemcpyDtoHAsync)(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);\nextern CUresult (*psmi_cuMemcpyHtoDAsync)(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount, CUstream hStream);\nextern CUresult (*psmi_cuIpcGetMemHandle)(CUipcMemHandle* pHandle, CUdeviceptr dptr);\nextern CUresult (*psmi_cuIpcOpenMemHandle)(CUdeviceptr* pdptr, CUipcMemHandle handle, unsigned int Flags);\nextern CUresult (*psmi_cuIpcCloseMemHandle)(CUdeviceptr dptr);\nextern CUresult (*psmi_cuMemGetAddressRange)(CUdeviceptr* pbase, size_t* psize, CUdeviceptr dptr);\nextern CUresult (*psmi_cuDevicePrimaryCtxGetState)(CUdevice dev, unsigned int* flags, int* active);\nextern CUresult (*psmi_cuDevicePrimaryCtxRetain)(CUcontext* pctx, CUdevice dev);\nextern CUresult (*psmi_cuCtxGetDevice)(CUdevice* device);\nextern CUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);\n\n#define PSMI_CUDA_CALL(func, args...) do {\t\t\t\t\\\n\t\tCUresult cudaerr;\t\t\t\t\t\\\n\t\tcudaerr = psmi_##func(args);\t\t\t\t\\\n\t\tif (cudaerr != CUDA_SUCCESS) {\t\t\t\t\\\n\t\t\tif (ctxt == NULL)\t\t\t\t\\\n\t\t\t\t_HFI_ERROR(\t\t\t\t\\\n\t\t\t\t\"Check if CUDA is initialized\"\t\\\n\t\t\t\t\"before psm2_ep_open call \\n\");\t\t\\\n\t\t\t_HFI_ERROR(\t\t\t\t\t\\\n\t\t\t\t\"CUDA failure: %s() (at %s:%d)\"\t\t\\\n\t\t\t\t\"returned %d\\n\",\t\t\t\\\n\t\t\t\t#func, __FILE__, __LINE__, cudaerr);\t\\\n\t\t\tpsmi_handle_error(\t\t\t\t\\\n\t\t\t\tPSMI_EP_NORETURN, PSM2_INTERNAL_ERR,\t\\\n\t\t\t\t\"Error returned from CUDA function.\\n\");\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n/**\n * Similar to PSMI_CUDA_CALL() except does not error out\n * if func(args) returns CUDA_SUCCESS or except_err\n *\n * Invoker must provide 'CUresult cudaerr' in invoked scope\n * so invoker can inspect whether cudaerr == CUDA_SUCCESS or\n * cudaerr == except_err after expanded code is executed.\n *\n * As except_err is an allowed value, message is printed at\n * DBG level.\n */\n#define PSMI_CUDA_CALL_EXCEPT(except_err, func, args...) do { \\\n\t\tcudaerr = psmi_##func(args);\t\t\t\t\\\n\t\tif (cudaerr != CUDA_SUCCESS && cudaerr != except_err) {\t\\\n\t\t\tif (ctxt == NULL)\t\t\t\t\\\n\t\t\t\t_HFI_ERROR(\t\t\t\t\\\n\t\t\t\t\"Check if CUDA is initialized\"\t\\\n\t\t\t\t\"before psm2_ep_open call \\n\");\t\t\\\n\t\t\t_HFI_ERROR(\t\t\t\t\t\\\n\t\t\t\t\"CUDA failure: %s() (at %s:%d)\"\t\t\\\n\t\t\t\t\"returned %d\\n\",\t\t\t\\\n\t\t\t\t#func, __FILE__, __LINE__, cudaerr);\t\\\n\t\t\tpsmi_handle_error(\t\t\t\t\\\n\t\t\t\tPSMI_EP_NORETURN, PSM2_INTERNAL_ERR,\t\\\n\t\t\t\t\"Error returned from CUDA function.\\n\");\\\n\t\t} else if (cudaerr == except_err) { \\\n\t\t\t_HFI_DBG( \\\n\t\t\t\t\"CUDA non-zero return value: %s() (at %s:%d)\"\t\t\\\n\t\t\t\t\"returned %d\\n\",\t\t\t\\\n\t\t\t\t#func, __FILE__, __LINE__, cudaerr);\t\\\n\t\t} \\\n\t} while (0)\n\n#define PSMI_CUDA_CHECK_EVENT(event, cudaerr) do {\t\t\t\\\n\t\tcudaerr = psmi_cuEventQuery(event);\t\t\t\\\n\t\tif ((cudaerr != CUDA_SUCCESS) &&\t\t\t\\\n\t\t    (cudaerr != CUDA_ERROR_NOT_READY)) {\t\t\\\n\t\t\t_HFI_ERROR(\t\t\t\t\t\\\n\t\t\t\t\"CUDA failure: %s() returned %d\\n\",\t\\\n\t\t\t\t\"cuEventQuery\", cudaerr);\t\t\\\n\t\t\tpsmi_handle_error(\t\t\t\t\\\n\t\t\t\tPSMI_EP_NORETURN, PSM2_INTERNAL_ERR,\t\\\n\t\t\t\t\"Error returned from CUDA function.\\n\");\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define PSMI_CUDA_DLSYM(psmi_cuda_lib,func) do {                        \\\n\tpsmi_##func = dlsym(psmi_cuda_lib, STRINGIFY(func));            \\\n\tif (!psmi_##func) {               \t\t\t\t\\\n\t\tpsmi_handle_error(PSMI_EP_NORETURN,                     \\\n\t\t\t       PSM2_INTERNAL_ERR,                       \\\n\t\t\t       \" Unable to resolve %s symbol\"\t\t\\\n\t\t\t       \" in CUDA libraries.\\n\",STRINGIFY(func));\\\n\t}                                                               \\\n} while (0)\n\nPSMI_ALWAYS_INLINE(\nint\n_psmi_is_cuda_mem(const void *ptr))\n{\n\tCUresult cres;\n\tCUmemorytype mt;\n\tunsigned uvm = 0;\n\tcres = psmi_cuPointerGetAttribute(\n\t\t&mt, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, (CUdeviceptr) ptr);\n\tif ((cres == CUDA_SUCCESS) && (mt == CU_MEMORYTYPE_DEVICE)) {\n\t\tcres = psmi_cuPointerGetAttribute(\n\t\t\t&uvm, CU_POINTER_ATTRIBUTE_IS_MANAGED, (CUdeviceptr) ptr);\n\t\tif ((cres == CUDA_SUCCESS) && (uvm == 0))\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn 0;\n\t} else\n\t\treturn 0;\n}\n\n#define PSMI_IS_CUDA_ENABLED  likely(is_cuda_enabled)\n#define PSMI_IS_CUDA_DISABLED unlikely(!is_cuda_enabled)\n\nPSMI_ALWAYS_INLINE(\nint\n_psmi_is_gdr_copy_enabled())\n{\n        return is_gdr_copy_enabled;\n}\n\n#define PSMI_IS_GDR_COPY_ENABLED _psmi_is_gdr_copy_enabled()\n\n#define PSMI_IS_CUDA_MEM(p) _psmi_is_cuda_mem(p)\n\nstruct ips_cuda_hostbuf {\n\tSTAILQ_ENTRY(ips_cuda_hostbuf) req_next;\n\tSTAILQ_ENTRY(ips_cuda_hostbuf) next;\n\tuint32_t size, offset, bytes_read;\n\t/* This flag indicates whether a chb is\n\t * pulled from a mpool or dynamically\n\t * allocated using calloc. */\n\tuint8_t is_tempbuf;\n\tCUevent copy_status;\n\tpsm2_mq_req_t req;\n\tvoid *host_buf;\n\tCUdeviceptr gpu_buf;\n};\n\nstruct ips_cuda_hostbuf_mpool_cb_context {\n\tunsigned bufsz;\n};\nvoid psmi_cuda_hostbuf_alloc_func(int is_alloc, void *context, void *obj);\n\n#define CUDA_HOSTBUFFER_LIMITS {\t\t\t\t\\\n\t    .env = \"PSM3_CUDA_BOUNCEBUFFERS_MAX\",\t\t\\\n\t    .descr = \"Max CUDA bounce buffers (in MB)\",\t\t\\\n\t    .env_level = PSMI_ENVVAR_LEVEL_HIDDEN,\t\t\\\n\t    .minval = 1,\t\t\t\t\t\\\n\t    .maxval = 1<<30,\t\t\t\t\t\\\n\t    .mode[PSMI_MEMMODE_NORMAL]  = {  16, 256 },\t\t\\\n\t    .mode[PSMI_MEMMODE_MINIMAL] = {   1,   1 },\t\t\\\n\t    .mode[PSMI_MEMMODE_LARGE]   = {  32, 512 }\t\t\\\n\t}\n\nextern uint32_t gpudirect_send_threshold;\nextern uint32_t gpudirect_recv_threshold;\nextern uint32_t cuda_thresh_rndv;\n/* This threshold dictates when the sender turns off\n * GDR Copy. The threshold needs to be less than\n * CUDA RNDV threshold.\n */\nextern uint32_t gdr_copy_threshold_send;\n/* This threshold dictates when the reciever turns off\n * GDR Copy. The threshold needs to be less than\n * CUDA RNDV threshold.\n */\nextern uint32_t gdr_copy_threshold_recv;\n\n#define PSMI_USE_GDR_COPY(req, len) req->is_buf_gpu_mem &&       \\\n\t\t\t\t    PSMI_IS_GDR_COPY_ENABLED  && \\\n\t\t\t\t    len >=1 && len <= gdr_copy_threshold_recv\n\nenum psm2_chb_match_type {\n\t/* Complete data found in a single chb */\n\tPSMI_CUDA_FULL_MATCH_FOUND = 0,\n\t/* Data is spread across two chb's */\n\tPSMI_CUDA_SPLIT_MATCH_FOUND = 1,\n\t/* Data is only partially prefetched */\n\tPSMI_CUDA_PARTIAL_MATCH_FOUND = 2,\n\tPSMI_CUDA_CONTINUE = 3\n};\ntypedef enum psm2_chb_match_type psm2_chb_match_type_t;\n\n/*\n * CUDA documentation dictates the use of SYNC_MEMOPS attribute\n * when the buffer pointer received into PSM has been allocated\n * by the application. This guarantees that all memory operations\n * to this region of memory (used by multiple layers of the stack)\n * always synchronize.\n */\nstatic inline\nvoid psmi_cuda_set_attr_sync_memops(const void *ubuf)\n{\n\tint true_flag = 1;\n\n\tPSMI_CUDA_CALL(cuPointerSetAttribute, &true_flag,\n\t\t       CU_POINTER_ATTRIBUTE_SYNC_MEMOPS, (CUdeviceptr) ubuf);\n}\n\n#endif /* PSM_CUDA */\n\n#define COMPILE_TIME_ASSERT(NAME,COND) extern char NAME[1/ COND]\n\n#ifdef __cplusplus\n} /* extern \"C\" */\n#endif\n\n#endif /* _PSMI_USER_H */\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/prov/psm3/psm3/psm.c": "/*\n\n  This file is provided under a dual BSD/GPLv2 license.  When using or\n  redistributing this file, you may do so under either license.\n\n  GPL LICENSE SUMMARY\n\n  Copyright(c) 2016 Intel Corporation.\n\n  This program is free software; you can redistribute it and/or modify\n  it under the terms of version 2 of the GNU General Public License as\n  published by the Free Software Foundation.\n\n  This program is distributed in the hope that it will be useful, but\n  WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n  General Public License for more details.\n\n  Contact Information:\n  Intel Corporation, www.intel.com\n\n  BSD LICENSE\n\n  Copyright(c) 2016 Intel Corporation.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions\n  are met:\n\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in\n      the documentation and/or other materials provided with the\n      distribution.\n    * Neither the name of Intel Corporation nor the names of its\n      contributors may be used to endorse or promote products derived\n      from this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n*/\n\n/* Copyright (c) 2003-2016 Intel Corporation. All rights reserved. */\n\n#include <dlfcn.h>\n#include \"psm_user.h\"\n#include \"psm2_hal.h\"\n#include \"opa_revision.h\"\n#include \"psm_mq_internal.h\"\n\nstatic int psmi_verno_major = PSM2_VERNO_MAJOR;\nstatic int psmi_verno_minor = PSM2_VERNO_MINOR;\nstatic int psmi_verno = PSMI_VERNO_MAKE(PSM2_VERNO_MAJOR, PSM2_VERNO_MINOR);\nstatic int psmi_verno_client_val;\nint psmi_epid_ver;\nint psmi_allow_routers;\n\n// Special psmi_refcount values\n#define PSMI_NOT_INITIALIZED    0\n#define PSMI_FINALIZED         -1\n\n// PSM2 doesn't support transitioning out of the PSMI_FINALIZED state\n// once psmi_refcount is set to PSMI_FINALIZED, any further attempts to change\n// psmi_refcount should be treated as an error\nstatic int psmi_refcount = PSMI_NOT_INITIALIZED;\n\n/* Global lock used for endpoint creation and destroy\n * (in functions psm2_ep_open and psm2_ep_close) and also\n * for synchronization with recv_thread (so that recv_thread\n * will not work on an endpoint which is in a middle of closing). */\npsmi_lock_t psmi_creation_lock;\n\nsem_t *sem_affinity_shm_rw = NULL;\nint psmi_affinity_shared_file_opened = 0;\nint psmi_affinity_semaphore_open = 0;\nuint64_t *shared_affinity_ptr;\nchar *sem_affinity_shm_rw_name;\nchar *affinity_shm_name;\n\nuint32_t psmi_cpu_model;\n\n#ifdef PSM_CUDA\nint is_cuda_enabled;\nint is_gdr_copy_enabled;\nint device_support_gpudirect;\nint gpu_p2p_supported = 0;\nint my_gpu_device = 0;\nint cuda_lib_version;\nint is_driver_gpudirect_enabled;\nint is_cuda_primary_context_retain = 0;\nuint32_t cuda_thresh_rndv;\nuint32_t gdr_copy_threshold_send;\nuint32_t gdr_copy_threshold_recv;\n\nvoid *psmi_cuda_lib;\nCUresult (*psmi_cuInit)(unsigned int  Flags );\nCUresult (*psmi_cuCtxDetach)(CUcontext c);\nCUresult (*psmi_cuCtxGetCurrent)(CUcontext *c);\nCUresult (*psmi_cuCtxSetCurrent)(CUcontext c);\nCUresult (*psmi_cuPointerGetAttribute)(void *data, CUpointer_attribute pa, CUdeviceptr p);\nCUresult (*psmi_cuPointerSetAttribute)(void *data, CUpointer_attribute pa, CUdeviceptr p);\nCUresult (*psmi_cuDeviceCanAccessPeer)(int *canAccessPeer, CUdevice dev, CUdevice peerDev);\nCUresult (*psmi_cuDeviceGet)(CUdevice* device, int  ordinal);\nCUresult (*psmi_cuDeviceGetAttribute)(int* pi, CUdevice_attribute attrib, CUdevice dev);\nCUresult (*psmi_cuDriverGetVersion)(int* driverVersion);\nCUresult (*psmi_cuDeviceGetCount)(int* count);\nCUresult (*psmi_cuStreamCreate)(CUstream* phStream, unsigned int Flags);\nCUresult (*psmi_cuStreamDestroy)(CUstream phStream);\nCUresult (*psmi_cuEventCreate)(CUevent* phEvent, unsigned int Flags);\nCUresult (*psmi_cuEventDestroy)(CUevent hEvent);\nCUresult (*psmi_cuEventQuery)(CUevent hEvent);\nCUresult (*psmi_cuEventRecord)(CUevent hEvent, CUstream hStream);\nCUresult (*psmi_cuEventSynchronize)(CUevent hEvent);\nCUresult (*psmi_cuMemHostAlloc)(void** pp, size_t bytesize, unsigned int Flags);\nCUresult (*psmi_cuMemFreeHost)(void* p);\nCUresult (*psmi_cuMemcpy)(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);\nCUresult (*psmi_cuMemcpyDtoD)(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount);\nCUresult (*psmi_cuMemcpyDtoH)(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount);\nCUresult (*psmi_cuMemcpyHtoD)(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount);\nCUresult (*psmi_cuMemcpyDtoHAsync)(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);\nCUresult (*psmi_cuMemcpyHtoDAsync)(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount, CUstream hStream);\nCUresult (*psmi_cuIpcGetMemHandle)(CUipcMemHandle* pHandle, CUdeviceptr dptr);\nCUresult (*psmi_cuIpcOpenMemHandle)(CUdeviceptr* pdptr, CUipcMemHandle handle, unsigned int Flags);\nCUresult (*psmi_cuIpcCloseMemHandle)(CUdeviceptr dptr);\nCUresult (*psmi_cuMemGetAddressRange)(CUdeviceptr* pbase, size_t* psize, CUdeviceptr dptr);\nCUresult (*psmi_cuDevicePrimaryCtxGetState)(CUdevice dev, unsigned int* flags, int* active);\nCUresult (*psmi_cuDevicePrimaryCtxRetain)(CUcontext* pctx, CUdevice dev);\nCUresult (*psmi_cuCtxGetDevice)(CUdevice* device);\nCUresult (*psmi_cuDevicePrimaryCtxRelease)(CUdevice device);\n#endif\n\n/*\n * Bit field that contains capability set.\n * Each bit represents different capability.\n * It is supposed to be filled with logical OR\n * on conditional compilation basis\n * along with future features/capabilities.\n */\nuint64_t psm2_capabilities_bitset = PSM2_MULTI_EP_CAP | PSM2_LIB_REFCOUNT_CAP;\n\nint psmi_verno_client()\n{\n\treturn psmi_verno_client_val;\n}\n\n/* This function is used to determine whether the current library build can\n * successfully communicate with another library that claims to be version\n * 'verno'.\n *\n * PSM 2.x is always ABI compatible, but this checks to see if two different\n * versions of the library can coexist.\n */\nint psmi_verno_isinteroperable(uint16_t verno)\n{\n\tif (PSMI_VERNO_GET_MAJOR(verno) != PSM2_VERNO_MAJOR)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nint MOCKABLE(psmi_isinitialized)()\n{\n\treturn (psmi_refcount > 0);\n}\nMOCK_DEF_EPILOGUE(psmi_isinitialized);\n\n#ifdef PSM_CUDA\nint psmi_cuda_lib_load()\n{\n\tpsm2_error_t err = PSM2_OK;\n\tchar *dlerr;\n\n\tPSM2_LOG_MSG(\"entering\");\n\t_HFI_VDBG(\"Loading CUDA library.\\n\");\n\n\tpsmi_cuda_lib = dlopen(\"libcuda.so.1\", RTLD_LAZY);\n\tif (!psmi_cuda_lib) {\n\t\tdlerr = dlerror();\n\t\t_HFI_ERROR(\"Unable to open libcuda.so.  Error %s\\n\",\n\t\t\t\tdlerr ? dlerr : \"no dlerror()\");\n\t\tgoto fail;\n\t}\n\n\tpsmi_cuDriverGetVersion = dlsym(psmi_cuda_lib, \"cuDriverGetVersion\");\n\n\tif (!psmi_cuDriverGetVersion) {\n\t\t_HFI_ERROR\n\t\t\t(\"Unable to resolve symbols in CUDA libraries.\\n\");\n\t\tgoto fail;\n\t}\n\n\tPSMI_CUDA_CALL(cuDriverGetVersion, &cuda_lib_version);\n\tif (cuda_lib_version < 7000) {\n\t\t_HFI_ERROR(\"Please update CUDA driver, required minimum version is 7.0\\n\");\n\t\tgoto fail;\n\t}\n\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuInit);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuCtxGetCurrent);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuCtxDetach);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuCtxSetCurrent);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuPointerGetAttribute);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuPointerSetAttribute);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDeviceCanAccessPeer);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDeviceGetAttribute);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDeviceGet);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDeviceGetCount);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuStreamCreate);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuStreamDestroy);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventCreate);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventDestroy);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventQuery);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventRecord);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuEventSynchronize);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemHostAlloc);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemFreeHost);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemcpy);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemcpyDtoD);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemcpyDtoH);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemcpyHtoD);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemcpyDtoHAsync);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemcpyHtoDAsync);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuIpcGetMemHandle);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuIpcOpenMemHandle);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuIpcCloseMemHandle);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuMemGetAddressRange);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDevicePrimaryCtxGetState);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDevicePrimaryCtxRetain);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuDevicePrimaryCtxRelease);\n\tPSMI_CUDA_DLSYM(psmi_cuda_lib, cuCtxGetDevice);\n\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn err;\nfail:\n\tif (psmi_cuda_lib)\n\t\tdlclose(psmi_cuda_lib);\n\terr = psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR, \"Unable to load CUDA library.\\n\");\n\treturn err;\n}\n\nint psmi_cuda_initialize()\n{\n\tpsm2_error_t err = PSM2_OK;\n\tint num_devices, dev;\n\n\tPSM2_LOG_MSG(\"entering\");\n\t_HFI_VDBG(\"Enabling CUDA support.\\n\");\n\n\terr = psmi_cuda_lib_load();\n\tif (err != PSM2_OK)\n\t\tgoto fail;\n\n\tPSMI_CUDA_CALL(cuInit, 0);\n\n\t/* Check if CUDA context is available. If not, we are not allowed to\n\t * launch any CUDA API calls */\n\tPSMI_CUDA_CALL(cuCtxGetCurrent, &ctxt);\n\tif (ctxt == NULL) {\n\t\t_HFI_INFO(\"Unable to find active CUDA context\\n\");\n\t\tis_cuda_enabled = 0;\n\t\terr = PSM2_OK;\n\t\treturn err;\n\t}\n\n\tCUdevice current_device;\n\tCUcontext primary_ctx;\n\tPSMI_CUDA_CALL(cuCtxGetDevice, &current_device);\n\tint is_ctx_active;\n\tunsigned ctx_flags;\n\tPSMI_CUDA_CALL(cuDevicePrimaryCtxGetState, current_device, &ctx_flags,\n\t\t\t&is_ctx_active);\n\tif (!is_ctx_active) {\n\t\t/* There is an issue where certain CUDA API calls create\n\t\t * contexts but does not make it active which cause the\n\t\t * driver API call to fail with error 709 */\n\t\tPSMI_CUDA_CALL(cuDevicePrimaryCtxRetain, &primary_ctx,\n\t\t\t\tcurrent_device);\n\t\tis_cuda_primary_context_retain = 1;\n\t}\n\n\t/* Check if all devices support Unified Virtual Addressing. */\n\tPSMI_CUDA_CALL(cuDeviceGetCount, &num_devices);\n\n\tdevice_support_gpudirect = 1;\n\n\tfor (dev = 0; dev < num_devices; dev++) {\n\t\tCUdevice device;\n\t\tPSMI_CUDA_CALL(cuDeviceGet, &device, dev);\n\t\tint unifiedAddressing;\n\t\tPSMI_CUDA_CALL(cuDeviceGetAttribute,\n\t\t\t\t&unifiedAddressing,\n\t\t\t\tCU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING,\n\t\t\t\tdevice);\n\n\t\tif (unifiedAddressing !=1) {\n\t\t\t_HFI_ERROR(\"CUDA device %d does not support Unified Virtual Addressing.\\n\", dev);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tint major;\n\t\tPSMI_CUDA_CALL(cuDeviceGetAttribute,\n\t\t\t\t&major,\n\t\t\t\tCU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR,\n\t\t\t\tdevice);\n\t\tif (major < 3) {\n\t\t\tdevice_support_gpudirect = 0;\n\t\t\t_HFI_INFO(\"CUDA device %d does not support GPUDirect RDMA (Non-fatal error)\\n\", dev);\n\t\t}\n\n\t\tif (device != current_device) {\n\t\t\tint canAccessPeer = 0;\n\t\t\tPSMI_CUDA_CALL(cuDeviceCanAccessPeer, &canAccessPeer,\n\t\t\t\t\tcurrent_device, device);\n\n\t\t\tif (canAccessPeer != 1)\n\t\t\t\t_HFI_DBG(\"CUDA device %d does not support P2P from current device (Non-fatal error)\\n\", dev);\n\t\t\telse\n\t\t\t\tgpu_p2p_supported |= (1 << device);\n\t\t} else {\n\t\t\t/* Always support p2p on the same GPU */\n\t\t\tmy_gpu_device = device;\n\t\t\tgpu_p2p_supported |= (1 << device);\n\t\t}\n\t}\n\n\tunion psmi_envvar_val env_enable_gdr_copy;\n\tpsmi_getenv(\"PSM3_GDRCOPY\",\n\t\t\t\t\"Enable (set envvar to 1) for gdr copy support in PSM (Enabled by default)\",\n\t\t\t\tPSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,\n\t\t\t\t(union psmi_envvar_val)1, &env_enable_gdr_copy);\n\tis_gdr_copy_enabled = env_enable_gdr_copy.e_int;\n\n\tunion psmi_envvar_val env_cuda_thresh_rndv;\n\tpsmi_getenv(\"PSM3_CUDA_THRESH_RNDV\",\n\t\t\t\t\"RNDV protocol is used for message sizes greater than the threshold \\n\",\n\t\t\t\tPSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,\n\t\t\t\t(union psmi_envvar_val)CUDA_THRESH_RNDV, &env_cuda_thresh_rndv);\n\tcuda_thresh_rndv = env_cuda_thresh_rndv.e_int;\n\n\tif (cuda_thresh_rndv < 0 || cuda_thresh_rndv > CUDA_THRESH_RNDV)\n\t    cuda_thresh_rndv = CUDA_THRESH_RNDV;\n\n\tunion psmi_envvar_val env_gdr_copy_thresh_send;\n\tpsmi_getenv(\"PSM3_GDRCOPY_THRESH_SEND\",\n\t\t\t\t\"GDR Copy is turned off on the send side\"\n\t\t\t\t\" for message sizes greater than the threshold \\n\",\n\t\t\t\tPSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,\n\t\t\t\t(union psmi_envvar_val)GDR_COPY_THRESH_SEND, &env_gdr_copy_thresh_send);\n\tgdr_copy_threshold_send = env_gdr_copy_thresh_send.e_int;\n\n\tif (gdr_copy_threshold_send < 8 || gdr_copy_threshold_send > cuda_thresh_rndv)\n\t\tgdr_copy_threshold_send = GDR_COPY_THRESH_SEND;\n\n\tunion psmi_envvar_val env_gdr_copy_thresh_recv;\n\tpsmi_getenv(\"PSM3_GDRCOPY_THRESH_RECV\",\n\t\t\t\t\"GDR Copy is turned off on the recv side\"\n\t\t\t\t\" for message sizes greater than the threshold \\n\",\n\t\t\t\tPSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,\n\t\t\t\t(union psmi_envvar_val)GDR_COPY_THRESH_RECV, &env_gdr_copy_thresh_recv);\n\tgdr_copy_threshold_recv = env_gdr_copy_thresh_recv.e_int;\n\n\tif (gdr_copy_threshold_recv < 8)\n\t\tgdr_copy_threshold_recv = GDR_COPY_THRESH_RECV;\n\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn err;\nfail:\n\terr = psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR, \"Unable to initialize PSM3 CUDA support.\\n\");\n\treturn err;\n}\n#endif\n\npsm2_error_t __psm2_init(int *major, int *minor)\n{\n\tpsm2_error_t err = PSM2_OK;\n\tunion psmi_envvar_val env_tmask;\n\n\tpsmi_stats_initialize();\n\n\tpsmi_mem_stats_register();\n\n\tpsmi_log_initialize();\n\n\tPSM2_LOG_MSG(\"entering\");\n\n\t/* When PSM_PERF is enabled, the following code causes the\n\t   PMU to be programmed to measure instruction cycles of the\n\t   TX/RX speedpaths of PSM. */\n\tGENERIC_PERF_INIT();\n\tGENERIC_PERF_SET_SLOT_NAME(PSM_TX_SPEEDPATH_CTR, \"TX\");\n\tGENERIC_PERF_SET_SLOT_NAME(PSM_RX_SPEEDPATH_CTR, \"RX\");\n\n\tif (psmi_refcount > 0) {\n\t\tpsmi_refcount++;\n\t\tgoto update;\n\t}\n\n\tif (psmi_refcount == PSMI_FINALIZED) {\n\t\terr = PSM2_IS_FINALIZED;\n\t\tgoto fail;\n\t}\n\n\tif (major == NULL || minor == NULL) {\n\t\terr = PSM2_PARAM_ERR;\n\t\tgoto fail;\n\t}\n\n\tpsmi_init_lock(&psmi_creation_lock);\n\n#ifdef PSM_DEBUG\n\tif (!getenv(\"PSM3_NO_WARN\"))\n\t\tfprintf(stderr,\n\t\t\t\"!!! WARNING !!! YOU ARE RUNNING AN INTERNAL-ONLY PSM *DEBUG* BUILD.\\n\");\n#endif\n\n#ifdef PSM_PROFILE\n\tif (!getenv(\"PSM3_NO_WARN\"))\n\t\tfprintf(stderr,\n\t\t\t\"!!! WARNING !!! YOU ARE RUNNING AN INTERNAL-ONLY PSM *PROFILE* BUILD.\\n\");\n#endif\n\n#ifdef PSM_FI\n\t/* Make sure we complain if fault injection is enabled */\n\tif (getenv(\"PSM3_FI\") && !getenv(\"PSM3_NO_WARN\"))\n\t\tfprintf(stderr,\n\t\t\t\"!!! WARNING !!! YOU ARE RUNNING WITH FAULT INJECTION ENABLED!\\n\");\n#endif /* #ifdef PSM_FI */\n\n\t/* Make sure, as an internal check, that this version knows how to detect\n\t * compatibility with other library versions it may communicate with */\n\tif (psmi_verno_isinteroperable(psmi_verno) != 1) {\n\t\terr = psmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,\n\t\t\t\t\t\"psmi_verno_isinteroperable() not updated for current version!\");\n\t\tgoto fail;\n\t}\n\n\t/* The only way to not support a client is if the major number doesn't\n\t * match */\n\tif (*major != PSM2_VERNO_MAJOR && *major != PSM2_VERNO_COMPAT_MAJOR) {\n\t\terr = psmi_handle_error(NULL, PSM2_INIT_BAD_API_VERSION,\n\t\t\t\t\t\"This library does not implement version %d.%d\",\n\t\t\t\t\t*major, *minor);\n\t\tgoto fail;\n\t}\n\n\t/* Make sure we don't keep track of a client that claims a higher version\n\t * number than we are */\n\tpsmi_verno_client_val =\n\t    min(PSMI_VERNO_MAKE(*major, *minor), psmi_verno);\n\n\t/* Check to see if we need to set Architecture flags to something\n\t * besides big core Xeons */\n\tcpuid_t id;\n\tpsmi_cpu_model = CPUID_MODEL_UNDEFINED;\n\n\t/* First check to ensure Genuine Intel */\n\tget_cpuid(0x0, 0, &id);\n\tif(id.ebx == CPUID_GENUINE_INTEL_EBX\n\t\t&& id.ecx == CPUID_GENUINE_INTEL_ECX\n\t\t&& id.edx == CPUID_GENUINE_INTEL_EDX)\n\t{\n\t\t/* Use cpuid with EAX=1 to get processor info */\n\t\tget_cpuid(0x1, 0, &id);\n\t\tpsmi_cpu_model = CPUID_GENUINE_INTEL;\n\t}\n\n\tif( (psmi_cpu_model == CPUID_GENUINE_INTEL) &&\n\t\t(id.eax & CPUID_FAMILY_MASK) == CPUID_FAMILY_XEON)\n\t{\n\t\tpsmi_cpu_model = ((id.eax & CPUID_MODEL_MASK) >> 4) |\n\t\t\t\t((id.eax & CPUID_EXMODEL_MASK) >> 12);\n\t}\n\n\tpsmi_refcount++;\n\t/* hfi_debug lives in libhfi.so */\n\tpsmi_getenv(\"PSM3_TRACEMASK\",\n\t\t    \"Mask flags for tracing\",\n\t\t    PSMI_ENVVAR_LEVEL_USER,\n\t\t    PSMI_ENVVAR_TYPE_ULONG_FLAGS,\n\t\t    (union psmi_envvar_val)hfi_debug, &env_tmask);\n\thfi_debug = (long)env_tmask.e_ulong;\n\n\t/* The \"real thing\" is done in hfi_proto.c as a constructor function, but\n\t * we getenv it here to report what we're doing with the setting */\n\t{\n\t\textern int __hfi_malloc_no_mmap;\n\t\tunion psmi_envvar_val env_mmap;\n\t\tchar *env = getenv(\"PSM3_DISABLE_MMAP_MALLOC\");\n\t\tint broken = (env && *env && !__hfi_malloc_no_mmap);\n\t\tpsmi_getenv(\"PSM3_DISABLE_MMAP_MALLOC\",\n\t\t\t    broken ? \"Skipping mmap disable for malloc()\" :\n\t\t\t    \"Disable mmap for malloc()\",\n\t\t\t    PSMI_ENVVAR_LEVEL_USER,\n\t\t\t    PSMI_ENVVAR_TYPE_YESNO,\n\t\t\t    (union psmi_envvar_val)0, &env_mmap);\n\t\tif (broken)\n\t\t\t_HFI_ERROR\n\t\t\t    (\"Couldn't successfully disable mmap in mallocs \"\n\t\t\t     \"with mallopt()\\n\");\n\t}\n\n\t{\n\t\tunion psmi_envvar_val env_epid_ver;\n\t\tpsmi_getenv(\"PSM3_ADDR_FMT\",\n\t\t\t\t\t\"Used to force PSM3 to use a particular version of EPID\",\n\t\t\t\t\tPSMI_ENVVAR_LEVEL_HIDDEN, PSMI_ENVVAR_TYPE_INT,\n\t\t\t\t\t(union psmi_envvar_val)PSMI_EPID_VERNO_DEFAULT, &env_epid_ver);\n\t\tpsmi_epid_ver = env_epid_ver.e_int;\n\t\tif (psmi_epid_ver > PSMI_MAX_EPID_VERNO_SUPPORTED) {\n\t\t\tpsmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,\n\t\t\t\t\t  \" The max epid version supported in this version of PSM3 is %d \\n\"\n\t\t\t\t\t  \"Please upgrade PSM3 \\n\",\n\t\t\t\t\t  PSMI_MAX_EPID_VERNO_SUPPORTED);\n\t\t\tgoto fail;\n\t\t} else if (psmi_epid_ver < PSMI_MIN_EPID_VERNO_SUPPORTED) {\n\t\t\tpsmi_handle_error(PSMI_EP_NORETURN, PSM2_INTERNAL_ERR,\n\t\t\t\t\t  \" Invalid value provided through PSM3_ADDR_FMT \\n\");\n\t\t\tgoto fail;\n\t\t}\n\t}\n\t{\n\t\tunion psmi_envvar_val env_allow_routers;\n\t\tpsmi_getenv(\"PSM3_ALLOW_ROUTERS\",\n\t\t\t\t\t\"Disable check for Ethernet subnet equality between nodes\\n\"\n\t\t\t\t\t\" allows routers between nodes and assumes single network plane for multi-rail\\n\",\n\t\t\t\t\tPSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_INT,\n\t\t\t\t\t(union psmi_envvar_val)0, &env_allow_routers);\n\t\tpsmi_allow_routers = env_allow_routers.e_int;\n\t}\n\n\tif (getenv(\"PSM3_DIAGS\")) {\n\t\t_HFI_INFO(\"Running diags...\\n\");\n\t\tpsmi_diags();\n\t}\n\n\tpsmi_multi_ep_init();\n\n#ifdef PSM_FI\n\tpsmi_faultinj_init();\n#endif /* #ifdef PSM_FI */\n\n\tpsmi_epid_init();\n\n\tint rc = psmi_hal_initialize();\n\n\tif (rc)\n\t{\n\t\terr = PSM2_INTERNAL_ERR;\n\t\tgoto fail;\n\t}\n\n#ifdef PSM_CUDA\n\tunion psmi_envvar_val env_enable_cuda;\n\tpsmi_getenv(\"PSM3_CUDA\",\n\t\t\t\"Enable (set envvar to 1) for cuda support in PSM (Disabled by default)\",\n\t\t\tPSMI_ENVVAR_LEVEL_USER, PSMI_ENVVAR_TYPE_INT,\n\t\t\t(union psmi_envvar_val)0, &env_enable_cuda);\n\tis_cuda_enabled = env_enable_cuda.e_int;\n\n\tif (PSMI_IS_CUDA_ENABLED) {\n\t\terr = psmi_cuda_initialize();\n\t\tif (err != PSM2_OK)\n\t\t\tgoto fail;\n\t}\n#endif\n\nupdate:\n\tif (psmi_parse_identify()) {\n                Dl_info info_psm;\n\t\tchar ofed_delta[100] = \"\";\n\t\tstrcat(strcat(ofed_delta,\" built for IFS OFA DELTA \"),psmi_hfi_IFS_version);\n                printf(\"%s %s PSM3 v%d.%d%s\\n\"\n\t\t       \"%s %s location %s\\n\"\n\t\t       \"%s %s build date %s\\n\"\n\t\t       \"%s %s src checksum %s\\n\"\n                       \"%s %s git checksum %s\\n\"\n#ifdef RNDV_MOD_MR\n                       \"%s %s built against rv interface v%d.%d\\n\"\n#endif\n                       \"%s %s Global Rank %d (%d total) Local Rank %d (%d total)\\n\"\n\t\t       , hfi_get_mylabel(), hfi_ident_tag,\n\t\t       PSM2_VERNO_MAJOR,PSM2_VERNO_MINOR,\n\t\t       (strcmp(psmi_hfi_IFS_version,\"\") != 0) ? ofed_delta\n#ifdef PSM_CUDA\n\t\t       : \"-cuda\",\n#else\n\t\t       : \"\",\n#endif\n\t\t       hfi_get_mylabel(), hfi_ident_tag, dladdr(psm2_init, &info_psm) ?\n\t\t       info_psm.dli_fname : \"PSM3 path not available\",\n\t\t       hfi_get_mylabel(), hfi_ident_tag, psmi_hfi_build_timestamp,\n\t\t       hfi_get_mylabel(), hfi_ident_tag, psmi_hfi_sources_checksum,\n\t\t       hfi_get_mylabel(), hfi_ident_tag,\n\t\t       (strcmp(psmi_hfi_git_checksum,\"\") != 0) ?\n\t\t       psmi_hfi_git_checksum : \"<not available>\",\n#ifdef RNDV_MOD_MR\n\t\t       hfi_get_mylabel(), hfi_ident_tag,\n\t\t\t\tpsm2_rv_get_user_major_bldtime_version(),\n\t\t\t\tpsm2_rv_get_user_minor_bldtime_version(),\n#endif\n\t\t       hfi_get_mylabel(), hfi_ident_tag,\n\t\t\t\thfi_get_myrank(), hfi_get_myrank_count(),\n\t\t\t\thfi_get_mylocalrank(),\n\t\t\t\thfi_get_mylocalrank_count()\n\t\t       );\n\t}\n\n\t*major = (int)psmi_verno_major;\n\t*minor = (int)psmi_verno_minor;\nfail:\n\t_HFI_DBG(\"psmi_refcount=%d,err=%u\\n\", psmi_refcount, err);\n\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn err;\n}\nPSMI_API_DECL(psm2_init)\n\nstatic\npsm2_error_t psmi_get_psm2_config(psm2_mq_t     mq,\n\t\t\t\t  psm2_epaddr_t epaddr,\n\t\t\t\t  uint32_t *out)\n{\n\tpsm2_error_t rv = PSM2_INTERNAL_ERR;\n\n\t*out = 0;\n\tif (&mq->ep->ptl_ips == epaddr->ptlctl)\n\t{\n\t\trv = PSM2_OK;\n\t\t*out |= PSM2_INFO_QUERY_CONFIG_IPS;\n#ifdef PSM_CUDA\n\t\tif (PSMI_IS_CUDA_ENABLED)\n\t\t{\n\t\t\t*out |= PSM2_INFO_QUERY_CONFIG_CUDA;\n\t\t\tif (PSMI_IS_GDR_COPY_ENABLED)\n\t\t\t\t*out |= PSM2_INFO_QUERY_CONFIG_GDR_COPY;\n\t\t}\n#endif\n\t\t*out |= PSM2_INFO_QUERY_CONFIG_PIO;\n\t}\n\telse if (&mq->ep->ptl_amsh == epaddr->ptlctl)\n\t{\n\t\t*out |= PSM2_INFO_QUERY_CONFIG_AMSH;\n\t\trv = PSM2_OK;\n\t}\n\telse if (&mq->ep->ptl_self == epaddr->ptlctl)\n\t{\n\t\t*out |= PSM2_INFO_QUERY_CONFIG_SELF;\n\t\trv = PSM2_OK;\n\t}\n\treturn rv;\n}\n\npsm2_error_t __psm2_info_query(psm2_info_query_t q, void *out,\n\t\t\t       size_t nargs, psm2_info_query_arg_t args[])\n{\n\tstatic const size_t expected_arg_cnt[PSM2_INFO_QUERY_LAST] =\n\t{\n\t\t0, /* PSM2_INFO_QUERY_NUM_UNITS         */\n\t\t0, /* PSM2_INFO_QUERY_NUM_PORTS         */\n\t\t1, /* PSM2_INFO_QUERY_UNIT_STATUS       */\n\t\t2, /* PSM2_INFO_QUERY_UNIT_PORT_STATUS  */\n\t\t1, /* PSM2_INFO_QUERY_NUM_FREE_CONTEXTS */\n\t\t1, /* PSM2_INFO_QUERY_NUM_CONTEXTS      */\n\t\t2, /* PSM2_INFO_QUERY_CONFIG            */\n\t\t3, /* PSM2_INFO_QUERY_THRESH            */\n\t\t3, /* PSM2_INFO_QUERY_DEVICE_NAME       */\n\t\t2, /* PSM2_INFO_QUERY_MTU               */\n\t\t2, /* PSM2_INFO_QUERY_LINK_SPEED        */\n\t\t1, /* PSM2_INFO_QUERY_NETWORK_TYPE      */\n\t\t0, /* PSM2_INFO_QUERY_FEATURE_MASK      */\n\t\t2, /* PSM2_INFO_QUERY_UNIT_NAME         */\n\t\t2, /* PSM2_INFO_QUERY_UNIT_SYS_PATH     */\n\t};\n\tpsm2_error_t rv = PSM2_INTERNAL_ERR;\n\n\tif ((q < 0) ||\n\t    (q >= PSM2_INFO_QUERY_LAST))\n\t\treturn \tPSM2_IQ_INVALID_QUERY;\n\n\tif (nargs != expected_arg_cnt[q])\n\t\treturn PSM2_PARAM_ERR;\n\n\tswitch (q)\n\t{\n\tcase PSM2_INFO_QUERY_NUM_UNITS:\n\t\t*((uint32_t*)out) = psmi_hal_get_num_units_();\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_NUM_PORTS:\n\t\t*((uint32_t*)out) = psmi_hal_get_num_ports_();\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_UNIT_STATUS:\n\t\t*((uint32_t*)out) = psmi_hal_get_unit_active(args[0].unit);\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_UNIT_PORT_STATUS:\n\t\t*((uint32_t*)out) = psmi_hal_get_port_active(args[0].unit,\n\t\t\t\t\t\t\t\targs[1].port);\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_NUM_FREE_CONTEXTS:\n\t\t*((uint32_t*)out) = psmi_hal_get_num_free_contexts(args[0].unit);\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_NUM_CONTEXTS:\n\t\t*((uint32_t*)out) = psmi_hal_get_num_contexts(args[0].unit);\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_CONFIG:\n\t\t{\n\t\t\tpsm2_mq_t     mq     = args[0].mq;\n\t\t\tpsm2_epaddr_t epaddr = args[1].epaddr;\n\t\t\trv = psmi_get_psm2_config(mq, epaddr, (uint32_t*)out);\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_THRESH:\n\t\t{\n\t\t\tpsm2_mq_t                      mq     = args[0].mq;\n\t\t\tpsm2_epaddr_t                  epaddr = args[1].epaddr;\n\t\t\tenum psm2_info_query_thresh_et iqt    = args[2].mstq;\n\n\t\t\tuint32_t                       config;\n\t\t\trv = psmi_get_psm2_config(mq, epaddr, &config);\n\t\t\tif (rv == PSM2_OK)\n\t\t\t{\n\t\t\t\t*((uint32_t*)out) = 0;\n\t\t\t\t/* Delegate the call to the ptl member function: */\n\t\t\t\trv = epaddr->ptlctl->msg_size_thresh_query(iqt, (uint32_t*)out, mq, epaddr);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_DEVICE_NAME:\n\t\t{\n\t\t\tchar         *hfiName       = (char*)out;\n\t\t\tpsm2_mq_t     mq            = args[0].mq;\n\t\t\tpsm2_epaddr_t epaddr        = args[1].epaddr;\n\t\t\tsize_t        hfiNameLength = args[2].length;\n\t\t\tuint32_t      config;\n\n\t\t\trv = psmi_get_psm2_config(mq, epaddr, &config);\n\t\t\tif (rv == PSM2_OK)\n\t\t\t{\n\t\t\t\tif (snprintf(hfiName, hfiNameLength, \"%s_%d\",\n\t\t\t\t\t     psmi_hal_get_hfi_name(),\n\t\t\t\t\t     mq->ep->unit_id)\n\t\t\t\t    < hfiNameLength)\n\t\t\t\t\trv = PSM2_OK;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_MTU:\n\t\t{\n\t\t\tpsm2_mq_t     mq     = args[0].mq;\n\t\t\tpsm2_epaddr_t epaddr = args[1].epaddr;\n\t\t\tuint32_t      config;\n\n\t\t\trv = psmi_get_psm2_config(mq, epaddr, &config);\n\t\t\tif (rv == PSM2_OK)\n\t\t\t{\n\t\t\t\t// TBD - should get ipsaddr to find pr_mtu negotiated\n\t\t\t\t*((uint32_t*)out) = mq->ep->mtu;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_LINK_SPEED:\n\t\t{\n\t\t\tpsm2_mq_t     mq     = args[0].mq;\n\t\t\tpsm2_epaddr_t epaddr = args[1].epaddr;\n\t\t\tuint32_t      config;\n\n\t\t\trv = psmi_get_psm2_config(mq, epaddr, &config);\n\t\t\tif (rv == PSM2_OK)\n\t\t\t{\n\t\t\t\t*((uint32_t*)out) = psmi_hal_get_port_rate(mq->ep->unit_id,\n\t\t\t\t\t\t\t\t       mq->ep->portnum);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_NETWORK_TYPE:\n\t\t{\n\t\t\tchar              *networkType      = (char*)out;\n\t\t\tsize_t            networkTypeLength = args[0].length;\n\t\t\tconst char *const intelopa          = \"Intel(R) OPA\";\n\t\t\tif (networkTypeLength >= strlen(intelopa)+1)\n\t\t\t{\n\t\t\t\tstrcpy(networkType,intelopa);\n\t\t\t\trv = PSM2_OK;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_FEATURE_MASK:\n\t\t{\n#ifdef PSM_CUDA\n\t\t*((uint32_t*)out) = PSM2_INFO_QUERY_FEATURE_CUDA;\n#else\n\t\t*((uint32_t*)out) = 0;\n#endif /* #ifdef PSM_CUDA */\n\t\t}\n\t\trv = PSM2_OK;\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_UNIT_NAME:\n\t\t{\n\t\t\tchar         *hfiName       = (char*)out;\n\t\t\tuint32_t      unit          = args[0].unit;\n\t\t\tsize_t        hfiNameLength = args[1].length;\n\t\t\tconst char   *pathName      = sysfs_unit_path(unit);\n\t\t\tchar         *unitName      = NULL;\n\n\t\t\tif (!pathName) break;\n\n\t\t\tunitName = strrchr(sysfs_unit_path(unit),'/');\n\t\t\tif (!unitName) break;\n\n\t\t\tstrncpy(hfiName, ++unitName, hfiNameLength);\n\t\t\thfiName[hfiNameLength-1] = '\\0';\n\t\t\trv = PSM2_OK;\n\t\t}\n\t\tbreak;\n\tcase PSM2_INFO_QUERY_UNIT_SYS_PATH:\n\t\t{\n\t\t\tchar         *hfiName       = (char*)out;\n\t\t\tuint32_t      unit          = args[0].unit;\n\t\t\tsize_t        hfiNameLength = args[1].length;\n\t\t\tconst char   *pathName      = sysfs_unit_path(unit);\n\t\t\t//char         *unitName      = NULL;\n\n\t\t\tif (!pathName) break;\n\n\t\t\tstrncpy(hfiName, pathName, hfiNameLength);\n\t\t\thfiName[hfiNameLength-1] = '\\0';\n\t\t\trv = PSM2_OK;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn rv;\n}\nPSMI_API_DECL(psm2_info_query)\n\nuint64_t __psm2_get_capability_mask(uint64_t req_cap_mask)\n{\n\treturn (psm2_capabilities_bitset & req_cap_mask);\n}\nPSMI_API_DECL(psm2_get_capability_mask)\n\npsm2_error_t __psm2_finalize(void)\n{\n\tstruct psmi_eptab_iterator itor;\n\tchar *hostname;\n\tpsm2_ep_t ep;\n\n\tPSM2_LOG_MSG(\"entering\");\n\n\t_HFI_DBG(\"psmi_refcount=%d\\n\", psmi_refcount);\n\tPSMI_ERR_UNLESS_INITIALIZED(NULL);\n\tpsmi_assert(psmi_refcount > 0);\n\tpsmi_refcount--;\n\n\tif (psmi_refcount > 0) {\n\t\treturn PSM2_OK;\n\t}\n\n\t/* When PSM_PERF is enabled, the following line causes the\n\t   instruction cycles gathered in the current run to be dumped\n\t   to stderr. */\n\tGENERIC_PERF_DUMP(stderr);\n\tep = psmi_opened_endpoint;\n\twhile (ep != NULL) {\n\t\tpsm2_ep_t saved_ep = ep->user_ep_next;\n\t\tpsm2_ep_close(ep, PSM2_EP_CLOSE_GRACEFUL,\n\t\t\t     2 * PSMI_MIN_EP_CLOSE_TIMEOUT);\n\t\tpsmi_opened_endpoint = ep = saved_ep;\n\t}\n\n#ifdef PSM_FI\n\tpsmi_faultinj_fini();\n#endif /* #ifdef PSM_FI */\n\n\t/* De-allocate memory for any allocated space to store hostnames */\n\tpsmi_epid_itor_init(&itor, PSMI_EP_HOSTNAME);\n\twhile ((hostname = psmi_epid_itor_next(&itor)))\n\t\tpsmi_free(hostname);\n\tpsmi_epid_itor_fini(&itor);\n\n\tpsmi_epid_fini();\n\n\t/* unmap shared mem object for affinity */\n\tif (psmi_affinity_shared_file_opened) {\n\t\t/*\n\t\t * Start critical section to decrement ref count and unlink\n\t\t * affinity shm file.\n\t\t */\n\t\tpsmi_sem_timedwait(sem_affinity_shm_rw, sem_affinity_shm_rw_name);\n\n\t\tshared_affinity_ptr[AFFINITY_SHM_REF_COUNT_LOCATION] -= 1;\n\t\tif (shared_affinity_ptr[AFFINITY_SHM_REF_COUNT_LOCATION] <= 0) {\n\t\t\t_HFI_VDBG(\"Unlink shm file for NIC affinity as there are no more users\\n\");\n\t\t\tshm_unlink(affinity_shm_name);\n\t\t} else {\n\t\t\t_HFI_VDBG(\"Number of affinity shared memory users left=%ld\\n\",\n\t\t\t\t  shared_affinity_ptr[AFFINITY_SHM_REF_COUNT_LOCATION]);\n\t\t}\n\n\t\tmsync(shared_affinity_ptr, AFFINITY_SHMEMSIZE, MS_SYNC);\n\n\t\t/* End critical section */\n\t\tpsmi_sem_post(sem_affinity_shm_rw, sem_affinity_shm_rw_name);\n\n\t\tmunmap(shared_affinity_ptr, AFFINITY_SHMEMSIZE);\n\t\tpsmi_free(affinity_shm_name);\n\t\taffinity_shm_name = NULL;\n\t\tpsmi_affinity_shared_file_opened = 0;\n\t}\n\n\tif (psmi_affinity_semaphore_open) {\n\t\t_HFI_VDBG(\"Closing and Unlinking Semaphore: %s.\\n\", sem_affinity_shm_rw_name);\n\t\tsem_close(sem_affinity_shm_rw);\n\t\tsem_unlink(sem_affinity_shm_rw_name);\n\t\tpsmi_free(sem_affinity_shm_rw_name);\n\t\tsem_affinity_shm_rw_name = NULL;\n\t\tpsmi_affinity_semaphore_open = 0;\n\t}\n\n\tpsmi_hal_finalize();\n#ifdef PSM_CUDA\n\tif (is_cuda_primary_context_retain) {\n\t\t/*\n\t\t * This code will be called during deinitialization, and if\n\t\t * CUDA is deinitialized before PSM, then\n\t\t * CUDA_ERROR_DEINITIALIZED will happen here\n\t\t */\n\t\tCUdevice device;\n\t\tif (psmi_cuCtxGetDevice(&device) == CUDA_SUCCESS)\n\t\t\tPSMI_CUDA_CALL(cuDevicePrimaryCtxRelease, device);\n\t}\n#endif\n\n\tpsmi_refcount = PSMI_FINALIZED;\n\tPSM2_LOG_MSG(\"leaving\");\n\tpsmi_log_fini();\n\n\tpsmi_stats_finalize();\n\n\tpsmi_heapdebug_finalize();\n\n\treturn PSM2_OK;\n}\nPSMI_API_DECL(psm2_finalize)\n\n/*\n * Function exposed in >= 1.05\n */\npsm2_error_t\n__psm2_map_nid_hostname(int num, const uint64_t *nids, const char **hostnames)\n{\n\tint i;\n\tpsm2_error_t err = PSM2_OK;\n\n\tPSM2_LOG_MSG(\"entering\");\n\n\tPSMI_ERR_UNLESS_INITIALIZED(NULL);\n\n\tif (nids == NULL || hostnames == NULL) {\n\t\terr = PSM2_PARAM_ERR;\n\t\tgoto fail;\n\t}\n\n\tfor (i = 0; i < num; i++) {\n\t\tif ((err = psmi_epid_set_hostname(nids[i], hostnames[i], 1)))\n\t\t\tbreak;\n\t}\n\nfail:\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn err;\n}\nPSMI_API_DECL(psm2_map_nid_hostname)\n\nvoid __psm2_epaddr_setlabel(psm2_epaddr_t epaddr, char const *epaddr_label)\n{\n\tPSM2_LOG_MSG(\"entering\");\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn;\t\t\t/* ignore this function */\n}\nPSMI_API_DECL(psm2_epaddr_setlabel)\n\nvoid __psm2_epaddr_setctxt(psm2_epaddr_t epaddr, void *ctxt)\n{\n\n\t/* Eventually deprecate this API to use set/get opt as this is unsafe. */\n\tPSM2_LOG_MSG(\"entering\");\n\tpsm2_setopt(PSM2_COMPONENT_CORE, (const void *)epaddr,\n\t\t   PSM2_CORE_OPT_EP_CTXT, (const void *)ctxt, sizeof(void *));\n\tPSM2_LOG_MSG(\"leaving\");\n}\nPSMI_API_DECL(psm2_epaddr_setctxt)\n\nvoid *__psm2_epaddr_getctxt(psm2_epaddr_t epaddr)\n{\n\tpsm2_error_t err;\n\tuint64_t optlen = sizeof(void *);\n\tvoid *result = NULL;\n\n\tPSM2_LOG_MSG(\"entering\");\n\t/* Eventually deprecate this API to use set/get opt as this is unsafe. */\n\terr = psm2_getopt(PSM2_COMPONENT_CORE, (const void *)epaddr,\n\t\t\t PSM2_CORE_OPT_EP_CTXT, (void *)&result, &optlen);\n\n\tPSM2_LOG_MSG(\"leaving\");\n\n\tif (err == PSM2_OK)\n\t\treturn result;\n\telse\n\t\treturn NULL;\n}\nPSMI_API_DECL(psm2_epaddr_getctxt)\n\npsm2_error_t\n__psm2_setopt(psm2_component_t component, const void *component_obj,\n\t     int optname, const void *optval, uint64_t optlen)\n{\n\tpsm2_error_t rv;\n\tPSM2_LOG_MSG(\"entering\");\n\tswitch (component) {\n\tcase PSM2_COMPONENT_CORE:\n\t\trv = psmi_core_setopt(component_obj, optname, optval, optlen);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\tcase PSM2_COMPONENT_MQ:\n\t\t/* Use the deprecated MQ set/get opt for now which does not use optlen */\n\t\trv = psm2_mq_setopt((psm2_mq_t) component_obj, optname, optval);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\tcase PSM2_COMPONENT_AM:\n\t\t/* Hand off to active messages */\n\t\trv = psmi_am_setopt(component_obj, optname, optval, optlen);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\tcase PSM2_COMPONENT_IB:\n\t\t/* Hand off to IPS ptl to set option */\n\t\trv = psmi_ptl_ips.setopt(component_obj, optname, optval,\n\t\t\t\t\t   optlen);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\t}\n\n\t/* Unrecognized/unknown component */\n\trv = psmi_handle_error(NULL, PSM2_PARAM_ERR, \"Unknown component %u\",\n\t\t\t\t component);\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn rv;\n}\nPSMI_API_DECL(psm2_setopt);\n\npsm2_error_t\n__psm2_getopt(psm2_component_t component, const void *component_obj,\n\t     int optname, void *optval, uint64_t *optlen)\n{\n\tpsm2_error_t rv;\n\n\tPSM2_LOG_MSG(\"entering\");\n\tswitch (component) {\n\tcase PSM2_COMPONENT_CORE:\n\t\trv = psmi_core_getopt(component_obj, optname, optval, optlen);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\tcase PSM2_COMPONENT_MQ:\n\t\t/* Use the deprecated MQ set/get opt for now which does not use optlen */\n\t\trv = psm2_mq_getopt((psm2_mq_t) component_obj, optname, optval);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\tcase PSM2_COMPONENT_AM:\n\t\t/* Hand off to active messages */\n\t\trv = psmi_am_getopt(component_obj, optname, optval, optlen);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\tcase PSM2_COMPONENT_IB:\n\t\t/* Hand off to IPS ptl to set option */\n\t\trv = psmi_ptl_ips.getopt(component_obj, optname, optval,\n\t\t\t\t\t   optlen);\n\t\tPSM2_LOG_MSG(\"leaving\");\n\t\treturn rv;\n\t\tbreak;\n\t}\n\n\t/* Unrecognized/unknown component */\n\trv = psmi_handle_error(NULL, PSM2_PARAM_ERR, \"Unknown component %u\",\n\t\t\t\t component);\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn rv;\n}\nPSMI_API_DECL(psm2_getopt);\n\npsm2_error_t __psmi_poll_noop(ptl_t *ptl, int replyonly)\n{\n\tPSM2_LOG_MSG(\"entering\");\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn PSM2_OK_NO_PROGRESS;\n}\nPSMI_API_DECL(psmi_poll_noop)\n\npsm2_error_t __psm2_poll(psm2_ep_t ep)\n{\n\tpsm2_error_t err1 = PSM2_OK, err2 = PSM2_OK;\n\tpsm2_ep_t tmp;\n\n\tPSM2_LOG_MSG(\"entering\");\n\n\tPSMI_ASSERT_INITIALIZED();\n\n\tPSMI_LOCK(ep->mq->progress_lock);\n\n\ttmp = ep;\n\tdo {\n\t\terr1 = ep->ptl_amsh.ep_poll(ep->ptl_amsh.ptl, 0);\t/* poll reqs & reps */\n\t\tif (err1 > PSM2_OK_NO_PROGRESS) {\t/* some error unrelated to polling */\n\t\t\tPSMI_UNLOCK(ep->mq->progress_lock);\n\t\t\tPSM2_LOG_MSG(\"leaving\");\n\t\t\treturn err1;\n\t\t}\n\n\t\terr2 = ep->ptl_ips.ep_poll(ep->ptl_ips.ptl, 0);\t/* get into ips_do_work */\n\t\tif (err2 > PSM2_OK_NO_PROGRESS) {\t/* some error unrelated to polling */\n\t\t\tPSMI_UNLOCK(ep->mq->progress_lock);\n\t\t\tPSM2_LOG_MSG(\"leaving\");\n\t\t\treturn err2;\n\t\t}\n\t\tep = ep->mctxt_next;\n\t} while (ep != tmp);\n\n\t/* This is valid because..\n\t * PSM2_OK & PSM2_OK_NO_PROGRESS => PSM2_OK\n\t * PSM2_OK & PSM2_OK => PSM2_OK\n\t * PSM2_OK_NO_PROGRESS & PSM2_OK => PSM2_OK\n\t * PSM2_OK_NO_PROGRESS & PSM2_OK_NO_PROGRESS => PSM2_OK_NO_PROGRESS */\n\tPSMI_UNLOCK(ep->mq->progress_lock);\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn (err1 & err2);\n}\nPSMI_API_DECL(psm2_poll)\n\npsm2_error_t __psmi_poll_internal(psm2_ep_t ep, int poll_amsh)\n{\n\tpsm2_error_t err1 = PSM2_OK_NO_PROGRESS;\n\tpsm2_error_t err2;\n\tpsm2_ep_t tmp;\n\n\tPSM2_LOG_MSG(\"entering\");\n\tPSMI_LOCK_ASSERT(ep->mq->progress_lock);\n\n\ttmp = ep;\n\tdo {\n\t\tif (poll_amsh) {\n\t\t\terr1 = ep->ptl_amsh.ep_poll(ep->ptl_amsh.ptl, 0);\t/* poll reqs & reps */\n\t\t\tif (err1 > PSM2_OK_NO_PROGRESS) { /* some error unrelated to polling */\n\t\t\t\tPSM2_LOG_MSG(\"leaving\");\n\t\t\t\treturn err1;\n\t\t\t}\n\t\t}\n\n\t\terr2 = ep->ptl_ips.ep_poll(ep->ptl_ips.ptl, 0);\t/* get into ips_do_work */\n\t\tif (err2 > PSM2_OK_NO_PROGRESS) { /* some error unrelated to polling */\n\t\t\tPSM2_LOG_MSG(\"leaving\");\n\t\t\treturn err2;\n\t\t}\n\n\t\tep = ep->mctxt_next;\n\t} while (ep != tmp);\n\tPSM2_LOG_MSG(\"leaving\");\n\treturn (err1 & err2);\n}\nPSMI_API_DECL(psmi_poll_internal)\n#ifdef PSM_PROFILE\n/* These functions each have weak symbols */\nvoid psmi_profile_block()\n{\n\t;\t\t\t/* empty for profiler */\n}\n\nvoid psmi_profile_unblock()\n{\n\t;\t\t\t/* empty for profiler */\n}\n\nvoid psmi_profile_reblock(int did_no_progress)\n{\n\t;\t\t\t/* empty for profiler */\n}\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/prov/psm3/psm3/ptl_ips/ips_opp_path_rec.c": "/*\n\n  This file is provided under a dual BSD/GPLv2 license.  When using or\n  redistributing this file, you may do so under either license.\n\n  GPL LICENSE SUMMARY\n\n  Copyright(c) 2015 Intel Corporation.\n\n  This program is free software; you can redistribute it and/or modify\n  it under the terms of version 2 of the GNU General Public License as\n  published by the Free Software Foundation.\n\n  This program is distributed in the hope that it will be useful, but\n  WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n  General Public License for more details.\n\n  Contact Information:\n  Intel Corporation, www.intel.com\n\n  BSD LICENSE\n\n  Copyright(c) 2015 Intel Corporation.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions\n  are met:\n\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in\n      the documentation and/or other materials provided with the\n      distribution.\n    * Neither the name of Intel Corporation nor the names of its\n      contributors may be used to endorse or promote products derived\n      from this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n*/\n\n/* Copyright (c) 2003-2014 Intel Corporation. All rights reserved. */\n\n#include \"psm_user.h\"\n#include \"psm2_hal.h\"\n#include \"ips_proto.h\"\n#include <dlfcn.h>\n\n/* SLID and DLID are in network byte order */\nstatic psm2_error_t\nips_opp_get_path_rec(ips_path_type_t type, struct ips_proto *proto,\n\t\t     uint16_t slid, uint16_t dlid,\n\t\t     ips_path_rec_t **ppath_rec)\n{\n\tpsm2_error_t err = PSM2_OK;\n\tibta_path_rec_t query, opp_response;\n#ifdef _HFI_DEBUGGING\n\tint opp_response_set = 0;\n#endif\n\tips_path_rec_t *path_rec;\n\tint opp_err;\n\tENTRY elid, *epath = NULL;\n\tchar eplid[128];\n\tuint64_t timeout_ack_ms;\n\n\t/* Query path record query cache first */\n\tbzero(&query, sizeof(query));\n\tbzero(eplid, sizeof(eplid));\n\n\t/* Bulk service ID is control service id + 1 */\n\tswitch (type) {\n\tcase IPS_PATH_LOW_PRIORITY:\n\t\tquery.service_id =\n\t\t    __cpu_to_be64(proto->ep->service_id + DATA_VFABRIC_OFFSET);\n\t\tbreak;\n\tcase IPS_PATH_NORMAL_PRIORITY:\n\tcase IPS_PATH_HIGH_PRIORITY:\n\tdefault:\n\t\tquery.service_id = __cpu_to_be64(proto->ep->service_id);\n\t}\n\n\tquery.slid = slid;\n\tquery.dlid = dlid;\n\n\tsnprintf(eplid, sizeof(eplid), \"%s_%x_%x\",\n\t\t (type == IPS_PATH_LOW_PRIORITY) ? \"LOW\" : \"HIGH\",\n\t\t query.slid, query.dlid);\n\telid.key = eplid;\n\thsearch_r(elid, FIND, &epath, &proto->ips_path_rec_hash);\n\n\tif (!epath) {\t\t/* Unable to find path record in cache */\n\t\telid.key =\n\t\t    psmi_calloc(proto->ep, UNDEFINED, 1, strlen(eplid) + 1);\n\t\tpath_rec = (ips_path_rec_t *)\n\t\t    psmi_calloc(proto->ep, UNDEFINED, 1,\n\t\t\t\tsizeof(ips_path_rec_t));\n\t\tif (!elid.key || !path_rec) {\n\t\t\tif (elid.key)\n\t\t\t\tpsmi_free(elid.key);\n\t\t\tif (path_rec)\n\t\t\t\tpsmi_free(path_rec);\n\t\t\terr = PSM2_NO_MEMORY;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/* Get path record between local LID and remote */\n\t\topp_err =\n\t\t    proto->opp_fn.op_path_get_path_by_rec(proto->opp_ctxt,\n\t\t\t\t\t\t\t  &query,\n\t\t\t\t\t\t\t  &opp_response);\n\t\tif (opp_err) {\n\t\t\tpsmi_free(path_rec);\n\t\t\tpsmi_free(elid.key);\n\t\t\terr = PSM2_EPID_PATH_RESOLUTION;\n\t\t\tgoto fail;\n\t\t}\n#ifdef _HFI_DEBUGGING\n\t\topp_response_set = 1;\n#endif\n\t\t// this should not happen since we are using a LID to LID query\n\t\t// but at some point we need to figure out how to deal with\n\t\t// virtualized IB environments where a GRH may be needed\n\t\t// HOP Limit >1 indicates a global route with a GRH\n\t\tif ((__be32_to_cpu(opp_response.hop_flow_raw) & 0xFF) > 1) {\n\t\t\t_HFI_ERROR\n\t\t    \t(\"Global Routed Path Record not supported SLID 0x%d DLID 0x%x\\n\",\n\t\t\t\t__be16_to_cpu(slid), __be16_to_cpu(dlid));\n\t\t\terr = PSM2_EPID_PATH_RESOLUTION;\n\t\t\tgoto fail;\n\t\t}\n\t\t/* Create path record */\n\t\tpath_rec->pr_slid = opp_response.slid;\n\t\tpath_rec->pr_dlid = opp_response.dlid;\n\t\tpath_rec->pr_mtu =\n\t\t    min(opa_mtu_enum_to_int(opp_response.mtu & 0x3f)\n\t\t\t\t- MAX_PSM_HEADER\n\t\t\t, proto->epinfo.ep_mtu);\n\t\tpath_rec->pr_pkey = ntohs(opp_response.pkey);\n\t\tpath_rec->pr_sl = ntohs(opp_response.qos_class_sl);\n\t\tpath_rec->pr_static_rate = opp_response.rate & 0x3f;\n\n\t\t/* Setup CCA parameters for path */\n\t\tif (path_rec->pr_sl > PSMI_SL_MAX) {\n\t\t\tpsmi_free(path_rec);\n\t\t\tpsmi_free(elid.key);\n\t\t\terr = PSM2_INTERNAL_ERR;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/* Compute max timeout based on pkt life time for path */\n\t\ttimeout_ack_ms =\n\t\t    ((4096UL * (1UL << (opp_response.pkt_life & 0x3f))) /\n\t\t     1000000UL);\n\t\ttimeout_ack_ms =\n\t\t    ms_2_cycles(IPS_PROTO_ERRCHK_MS_MIN_DEFAULT +\n\t\t\t\ttimeout_ack_ms);\n\t\tif (proto->epinfo.ep_timeout_ack_max < timeout_ack_ms)\n\t\t\tproto->epinfo.ep_timeout_ack_max = timeout_ack_ms;\n\t\terr = ips_make_ah(proto->ep, path_rec);\n\t\tif (err != PSM2_OK) {\n\t\t\tpsmi_free(elid.key);\n\t\t\tpsmi_free(path_rec);\n\t\t\treturn err;\n\t\t}\n\n\t\t/* Add path record into cache */\n\t\tstrcpy(elid.key, eplid);\n\t\telid.data = (void *)path_rec;\n\t\thsearch_r(elid, ENTER, &epath, &proto->ips_path_rec_hash);\n\t} else\t\t\t/* Path record found in cache */\n\t\tpath_rec = (ips_path_rec_t *) epath->data;\n\n#ifdef _HFI_DEBUGGING\n\t/* Dump path record stats */\n\t_HFI_PRDBG(\"Path Record ServiceID: %\" PRIx64 \" %x -----> %x\\n\",\n\t\t   (uint64_t) __be64_to_cpu(query.service_id),\n\t\t   __be16_to_cpu(slid), __be16_to_cpu(dlid));\n\tif (opp_response_set)\n\t{\n\t\t_HFI_PRDBG(\"MTU: %x, %x\\n\", (opp_response.mtu & 0x3f),\n\t\t\t   path_rec->pr_mtu);\n\t\t_HFI_PRDBG(\"PKEY: 0x%04x\\n\", ntohs(opp_response.pkey));\n\t\t_HFI_PRDBG(\"SL: 0x%04x\\n\", ntohs(opp_response.qos_class_sl));\n\t\t_HFI_PRDBG(\"Rate: %x\\n\", (opp_response.rate & 0x3f));\n\t}\n\t_HFI_PRDBG(\"Timeout Init.: 0x%\" PRIx64 \" Max: 0x%\" PRIx64 \"\\n\",\n\t\t   proto->epinfo.ep_timeout_ack,\n\t\t   proto->epinfo.ep_timeout_ack_max);\n#endif\n\t/* Return the IPS path record */\n\t*ppath_rec = path_rec;\n\nfail:\n\treturn err;\n}\n\nstatic psm2_error_t\nips_opp_path_rec(struct ips_proto *proto,\n\t\t uint16_t slid, uint16_t dlid,\n\t\t uint16_t ip_hi,\t// unused here, but must match API signature\n\t\t unsigned long timeout, ips_path_grp_t **ppathgrp)\n{\n\tpsm2_error_t err = PSM2_OK;\n\tuint16_t pidx, cpath, num_path = (1 << proto->epinfo.ep_lmc);\n\tips_path_type_t path_type = IPS_PATH_NORMAL_PRIORITY;\n\tips_path_rec_t *path;\n\tips_path_grp_t *pathgrp;\n\tuint16_t path_slid, path_dlid;\n\tENTRY elid, *epath = NULL;\n\tchar eplid[128];\n\n\t/*\n\t * High Priority Path\n\t * ------------------\n\t *\n\t * Uses the \"base\" Service ID. For now there exists only 1 high priority\n\t * path between nodes even for non zero LMC fabrics.\n\t *\n\t * Normal/Low Priority Paths\n\t * -------------------------\n\t *\n\t * Currently these paths are the same i.e. they are queried for the same\n\t * Service ID/vFabric which is the Base Service ID for High Priority + 1.\n\t *\n\t * Use case Scenarios\n\t * ------------------\n\t *\n\t * Since with vFabrics we have the capability to define different QoS\n\t * parameters per vFabric it is envisioned that the IPS_PATH_HIGH_PRIORITY is\n\t * setup in a separate vFabric for high priority traffic. The NORMAL paths\n\t * are setup in a separate vFabric optimized for high bandwidth. This allows\n\t * us to potentially have control traffic (RTS, CTS etc.) not be bottlenecked\n\t * by bulk transfer data. All control messages (ACKs,NAKs, TID_GRANT etc.)\n\t * also use the high priority control vFabric.\n\t *\n\t * NOTE: In order to distinguish between the different vFabrics the user\n\t * specifies the service ID to use via mpirun (or environment variable).\n\t * This is the service ID for the high priority control traffic. The bulk\n\t * data vFabric is identified by service ID + 1. So for each MPI application\n\t * one should specify two service IDs for the high priority and bulk data.\n\t * Both these service IDs can be placed in the same vFabric which can be\n\t * configured for high priority or bandwidth traffic giving us the default\n\t * behavior upto Infinhfi 2.5 release.\n\t *\n\t * NOTE: All of the above would have really helped if the S20 silicon could\n\t * correctly support IBTA QoS features. Due to S20 design we can only have\n\t * high priority VLarb table (low priority VLarb table results in round\n\t * robin arbitration ignoring the weights!). But if this is fixed in a\n\t * subsequent chip respin then this may potentially help our scalability\n\t * on large fabrics.\n\t *\n\t * Mesh/Torus and DOR routed networks\n\t * ----------------------------------\n\t *\n\t * In a mesh/torus fabric we always have a non zero LMC (at least 1 can be\n\t * more). We would like to take advantage of dispersive routing on these\n\t * fabrics as well to obtain better \"worst case/congested\" bandwidth. For\n\t * these networks currently the base LIDs are used for UPDN routing which\n\t * is suboptimal on these networks. Higher order LIDs (+1 .. +N) use DOR\n\t * routing (Dimension Ordered Routing) to avoid deadlocks and provide\n\t * higher performance. If a fabric is disrupted then only the base UPDN\n\t * routing is available. PSM should continue to operate in this environment\n\t * albeit with degraded performance. In disrupted fabric the OPP path\n\t * record queries may fail for some DOR routed LIDs i.e. no path exists\n\t * PSM should hence ignore path record failures as they indicate a disrupted\n\t * fabric and only use valid paths that are returned from the replica. This\n\t * will degenerate to only using the UPDN paths on disrupted fabrics and DOR\n\t * routes only for fully configured fabrics. Note: For a clean fabric the\n\t * base LIDs that are configured for UPDN route will not exist in the replica\n\t * as DOR routes are preferred. Hence we will only dispersively route across\n\t * the DOR routes only using the UPDN route for disrupted fabrics.\n\t *\n\t * AS LONG AS ONE PATH EXISTS (for each of the priorities) COMMUNICATION CAN\n\t * TAKE PLACE.\n\t */\n\n\t/* Check if this path grp is already in hash table */\n\tsnprintf(eplid, sizeof(eplid), \"%x_%x\", slid, dlid);\n\telid.key = eplid;\n\thsearch_r(elid, FIND, &epath, &proto->ips_path_grp_hash);\n\n\tif (epath) {\t\t/* Find path group in cache */\n\t\t*ppathgrp = (ips_path_grp_t *) epath->data;\n\t\treturn err;\n\t}\n\n\t/* If base lids are only used then reset num_path to 1 */\n\tif (proto->flags & IPS_PROTO_FLAG_PPOLICY_STATIC_BASE)\n\t\tnum_path = 1;\n\n\t/* Allocate a new pathgroup */\n\telid.key = psmi_calloc(proto->ep, UNDEFINED, 1, strlen(eplid) + 1);\n\tpathgrp = (ips_path_grp_t *)\n\t    psmi_calloc(proto->ep, UNDEFINED, 1, sizeof(ips_path_grp_t) +\n\t\t\tnum_path * IPS_PATH_MAX_PRIORITY *\n\t\t\tsizeof(ips_path_rec_t *));\n\tif (!elid.key || !pathgrp) {\n\t\tif (elid.key)\n\t\t\tpsmi_free(elid.key);\n\t\tif (pathgrp)\n\t\t\tpsmi_free(pathgrp);\n\t\terr = PSM2_NO_MEMORY;\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * dlid is the peer base lid.\n\t * slid is the base lid for the local end point.\n\t * Store here in network byte order.\n\t */\n\tpathgrp->pg_base_dlid = dlid;\n\tpathgrp->pg_base_slid = slid;\n\n\tpathgrp->pg_num_paths[IPS_PATH_HIGH_PRIORITY] =\n\t    pathgrp->pg_num_paths[IPS_PATH_NORMAL_PRIORITY] =\n\t    pathgrp->pg_num_paths[IPS_PATH_LOW_PRIORITY] = 0;\n\n\t/* For now there is always only one high priority path between nodes. */\n\tfor (pidx = 0, cpath = 0; pidx < num_path && cpath == 0; pidx++) {\n\t\tpath_slid = __cpu_to_be16(__be16_to_cpu(slid) + pidx);\n\t\tpath_dlid = __cpu_to_be16(__be16_to_cpu(dlid) + pidx);\n\n\t\terr = ips_opp_get_path_rec(IPS_PATH_HIGH_PRIORITY, proto,\n\t\t\t\t\t   path_slid, path_dlid,\n\t\t\t\t\t   &path);\n\n\t\tif (err == PSM2_OK) {\t/* Valid high priority path found */\n\t\t\t/* Resolved high priority path successfully */\n\t\t\tpathgrp->pg_num_paths[IPS_PATH_HIGH_PRIORITY]++;\n\t\t\tpathgrp->pg_path[cpath][IPS_PATH_HIGH_PRIORITY] = path;\n\n\t\t\t/* Increment current path index */\n\t\t\tcpath++;\n\t\t}\n\n\t\tPSM2_LOG_MSG(\"path %p slid %hu dlid %hu\\n\",\n\t\t\t      path,\n\t\t\t      __be16_to_cpu(path->pr_slid),\n\t\t\t      __be16_to_cpu(path->pr_dlid));\n\t}\n\n\t/* Make sure we have atleast 1 high priority path */\n\tif (pathgrp->pg_num_paths[IPS_PATH_HIGH_PRIORITY] == 0) {\n\t\tpsmi_free(elid.key);\n\t\tpsmi_free(pathgrp);\n\t\terr = psmi_handle_error(NULL, PSM2_EPID_PATH_RESOLUTION,\n\t\t\t\t\t\"OFED Plus path lookup failed. Unable to resolve high priority network path for LID 0x%x <---> 0x%x. Is the SM running or service ID %\"\n\t\t\t\t\tPRIx64 \" defined?\", ntohs(slid),\n\t\t\t\t\tntohs(dlid),\n\t\t\t\t\t(uint64_t) proto->ep->service_id);\n\t\tgoto fail;\n\t}\n\n\n\n\t/* Next setup the bulk paths. If the subnet administrator has misconfigured\n\t * or rather not configured two separate service IDs we place the bulk\n\t * paths in the same vFabric as the control paths.\n\t */\n\n\tpath_type = IPS_PATH_NORMAL_PRIORITY;\n\tfor (pidx = 0, cpath = 0; pidx < num_path; pidx++) {\n\t\tpath_slid = __cpu_to_be16(__be16_to_cpu(slid) + pidx);\n\t\tpath_dlid = __cpu_to_be16(__be16_to_cpu(dlid) + pidx);\n\nretry_normal_path_res:\n\t\terr = ips_opp_get_path_rec(path_type, proto,\n\t\t\t\t\t   path_slid, path_dlid,\n\t\t\t\t\t   &path);\n\t\tif (err != PSM2_OK) {\n\t\t\tif (path_type == IPS_PATH_NORMAL_PRIORITY) {\n\t\t\t\t/* Subnet may only be configured for one service ID/vFabric. Default\n\t\t\t\t * to using the control vFabric/service ID for bulk data as well.\n\t\t\t\t */\n\t\t\t\tpath_type = IPS_PATH_HIGH_PRIORITY;\n\t\t\t\tgoto retry_normal_path_res;\n\t\t\t}\n\n\t\t\t/* Unable to resolve path for <path_slid, path_dline>. This is possible\n\t\t\t * for disrupted fabrics using DOR routing so continue to acquire paths\n\t\t\t */\n\t\t\terr = PSM2_OK;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Valid path. */\n\t\tpathgrp->pg_path[cpath][IPS_PATH_NORMAL_PRIORITY] = path;\n\t\tpathgrp->pg_num_paths[IPS_PATH_NORMAL_PRIORITY]++;\n\t\tcpath++;\n\t}\n\n\t/* Make sure we have at least have a single bulk data transfer path */\n\tif (pathgrp->pg_num_paths[IPS_PATH_NORMAL_PRIORITY] == 0) {\n\t\tpsmi_free(elid.key);\n\t\tpsmi_free(pathgrp);\n\t\terr = psmi_handle_error(NULL, PSM2_EPID_PATH_RESOLUTION,\n\t\t\t\t\t\"OFED Plus path lookup failed. Unable to resolve normal priority network path for LID 0x%x <---> 0x%x. Is the SM running or service ID %\"\n\t\t\t\t\tPRIx64 \" defined?\", ntohs(slid),\n\t\t\t\t\tntohs(dlid),\n\t\t\t\t\t(uint64_t) proto->ep->service_id);\n\t\tgoto fail;\n\t}\n\n\tpath_type = IPS_PATH_LOW_PRIORITY;\n\tfor (pidx = 0, cpath = 0; pidx < num_path; pidx++) {\n\t\tpath_slid = __cpu_to_be16(__be16_to_cpu(slid) + pidx);\n\t\tpath_dlid = __cpu_to_be16(__be16_to_cpu(dlid) + pidx);\n\nretry_low_path_res:\n\t\terr = ips_opp_get_path_rec(path_type, proto,\n\t\t\t\t\t   path_slid, path_dlid,\n\t\t\t\t\t   &path);\n\t\tif (err != PSM2_OK) {\n\t\t\tif (path_type == IPS_PATH_LOW_PRIORITY) {\n\t\t\t\t/* Subnet may only be configured for one service ID/vFabric. Default\n\t\t\t\t * to using the control vFabric/service ID for bulk data as well.\n\t\t\t\t */\n\t\t\t\tpath_type = IPS_PATH_HIGH_PRIORITY;\n\t\t\t\tgoto retry_low_path_res;\n\t\t\t}\n\n\t\t\t/* Unable to resolve path for <path_slid, path_dline>. This is possible\n\t\t\t * for disrupted fabrics using DOR routing so continue to acquire paths\n\t\t\t */\n\t\t\terr = PSM2_OK;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Valid path. */\n\t\tpathgrp->pg_path[cpath][IPS_PATH_LOW_PRIORITY] = path;\n\t\tpathgrp->pg_num_paths[IPS_PATH_LOW_PRIORITY]++;\n\t\tcpath++;\n\t}\n\n\t/* Make sure we have at least have a single bulk data transfer path */\n\tif (pathgrp->pg_num_paths[IPS_PATH_LOW_PRIORITY] == 0) {\n\t\tpsmi_free(elid.key);\n\t\tpsmi_free(pathgrp);\n\t\terr = psmi_handle_error(NULL, PSM2_EPID_PATH_RESOLUTION,\n\t\t\t\t\t\"OFED Plus path lookup failed. Unable to resolve low priority network path for LID 0x%x <---> 0x%x. Is the SM running or service ID %\"\n\t\t\t\t\tPRIx64 \" defined?\", ntohs(slid),\n\t\t\t\t\tntohs(dlid),\n\t\t\t\t\t(uint64_t) proto->ep->service_id);\n\t\tgoto fail;\n\t}\n\n\tif (proto->flags & IPS_PROTO_FLAG_PPOLICY_ADAPTIVE) {\n\t\tpathgrp->pg_next_path[IPS_PATH_NORMAL_PRIORITY] =\n\t\t    proto->epinfo.EP_HASH %\n\t\t    pathgrp->pg_num_paths[IPS_PATH_NORMAL_PRIORITY];\n\t\tpathgrp->pg_next_path[IPS_PATH_LOW_PRIORITY] =\n\t\t    proto->epinfo.EP_HASH %\n\t\t    pathgrp->pg_num_paths[IPS_PATH_LOW_PRIORITY];\n\t}\n\n\t/* Add path group into cache */\n\tstrcpy(elid.key, eplid);\n\telid.data = (void *)pathgrp;\n\thsearch_r(elid, ENTER, &epath, &proto->ips_path_grp_hash);\n\n\t*ppathgrp = pathgrp;\n\nfail:\n\tif (err != PSM2_OK)\n\t\t_HFI_PRDBG\n\t\t    (\"Unable to get path record for LID 0x%x <---> DLID 0x%x.\\n\",\n\t\t     slid, dlid);\n\treturn err;\n}\n\nstatic psm2_error_t ips_opp_fini(struct ips_proto *proto)\n{\n\tpsm2_error_t err = PSM2_OK;\n\n\tif (proto->opp_lib)\n\t\tdlclose(proto->opp_lib);\n\n\treturn err;\n}\n\npsm2_error_t ips_opp_init(struct ips_proto *proto)\n{\n\tpsm2_error_t err = PSM2_OK;\n\tchar hfiName[32];\n\n\tproto->opp_lib = dlopen(DF_OPP_LIBRARY, RTLD_NOW);\n\tif (!proto->opp_lib) {\n\t\tchar *err = dlerror();\n\t\t_HFI_ERROR\n\t\t    (\"Unable to open OFED Plus Plus library %s. Error: %s\\n\",\n\t\t     DF_OPP_LIBRARY, err ? err : \"no dlerror()\");\n\t\tgoto fail;\n\t}\n\n\t/* Resolve symbols that we require within opp library */\n\tproto->opp_fn.op_path_find_hca =\n\t    dlsym(proto->opp_lib, \"op_path_find_hfi\");\n\tproto->opp_fn.op_path_open = dlsym(proto->opp_lib, \"op_path_open\");\n\tproto->opp_fn.op_path_close = dlsym(proto->opp_lib, \"op_path_close\");\n\tproto->opp_fn.op_path_get_path_by_rec =\n\t    dlsym(proto->opp_lib, \"op_path_get_path_by_rec\");\n\n\t/* If we can't resovle any symbol then fail to load opp module */\n\tif (!proto->opp_fn.op_path_find_hca || !proto->opp_fn.op_path_open ||\n\t    !proto->opp_fn.op_path_close\n\t    || !proto->opp_fn.op_path_get_path_by_rec) {\n\t\t_HFI_ERROR\n\t\t    (\"Unable to resolve symbols in OPP library. Unloading.\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* If PSM3_IDENTIFY is set display the OPP library location being used. */\n\tif (psmi_parse_identify()) {\n\t\tDl_info info_opp;\n\t\tprintf\n\t\t    (\"PSM3 path record queries using OFED Plus Plus (%s) from %s\\n\",\n\t\t     DF_OPP_LIBRARY, dladdr(proto->opp_fn.op_path_open,\n\t\t\t\t\t    &info_opp) ? info_opp.\n\t\t     dli_fname :\n\t\t     \"Unknown/unsupported version of OPP library found!\");\n\t}\n\n\t/* Obtain handle to hfi (requires verbs on node) */\n\tsnprintf(hfiName, sizeof(hfiName), \"%s_%d\",\n\t\t psmi_hal_get_hfi_name(),\n\t\t proto->ep->unit_id);\n\tproto->hndl = proto->opp_fn.op_path_find_hca(hfiName, &proto->device);\n\tif (!proto->hndl) {\n\t\t_HFI_ERROR\n\t\t    (\"OPP: Unable to find NIC %s. Disabling OPP interface for path record queries.\\n\",\n\t\t     hfiName);\n\t\tgoto fail;\n\t}\n\n\t/* Get OPP context */\n\tproto->opp_ctxt = proto->opp_fn.op_path_open(proto->device, 1);\n\tif (!proto->opp_ctxt) {\n\t\t_HFI_ERROR\n\t\t    (\"OPP: Unable to obtain OPP context. Disabling OPP interface for path record queries.\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* Setup default errorcheck timeout. OPP may change it later. */\n\tproto->epinfo.ep_timeout_ack =\n\t    ms_2_cycles(IPS_PROTO_ERRCHK_MS_MIN_DEFAULT);\n\tproto->epinfo.ep_timeout_ack_max =\n\t    ms_2_cycles(IPS_PROTO_ERRCHK_MS_MIN_DEFAULT);\n\tproto->epinfo.ep_timeout_ack_factor = IPS_PROTO_ERRCHK_FACTOR_DEFAULT;\n\n\t/* OPP initialized successfully */\n\tproto->ibta.get_path_rec = ips_opp_path_rec;\n\tproto->ibta.fini = ips_opp_fini;\n\tproto->flags |= IPS_PROTO_FLAG_QUERY_PATH_REC;\n\n\treturn err;\n\nfail:\n\t_HFI_ERROR(\"Make sure SM is running...\\n\");\n\t_HFI_ERROR(\"Make sure service ibacm is running...\\n\");\n\t_HFI_ERROR(\"to start ibacm: service ibacm start\\n\");\n\t_HFI_ERROR(\"or enable it at boot time: iefsconfig -E ibacm\\n\\n\");\n\n\terr = psmi_handle_error(NULL, PSM2_EPID_PATH_RESOLUTION,\n\t\t\t\t\"Unable to initialize OFED Plus library successfully.\\n\");\n\n\tif (proto->opp_lib)\n\t\tdlclose(proto->opp_lib);\n\n\treturn err;\n}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/.git/objects/pack/pack-1323eda25e0d2ad77126dfda9bbbbb541a0e18ce.pack",
        "/tmp/vanessa/spack-stage/spack-stage-libfabric-master-7qyp67ncottp7it3zyzdyxcucafartnj/spack-src/.git/objects/pack/pack-1323eda25e0d2ad77126dfda9bbbbb541a0e18ce.idx"
    ],
    "total_files": 1395
}