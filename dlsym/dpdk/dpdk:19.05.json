{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/mlx4/mlx4.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright 2012 6WIND S.A.\n * Copyright 2012 Mellanox Technologies, Ltd\n */\n\n/**\n * @file\n * mlx4 driver initialization.\n */\n\n#include <assert.h>\n#include <dlfcn.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <stddef.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/mman.h>\n#include <unistd.h>\n\n/* Verbs headers do not support -pedantic. */\n#ifdef PEDANTIC\n#pragma GCC diagnostic ignored \"-Wpedantic\"\n#endif\n#include <infiniband/verbs.h>\n#ifdef PEDANTIC\n#pragma GCC diagnostic error \"-Wpedantic\"\n#endif\n\n#include <rte_common.h>\n#include <rte_config.h>\n#include <rte_dev.h>\n#include <rte_errno.h>\n#include <rte_ethdev_driver.h>\n#include <rte_ethdev_pci.h>\n#include <rte_ether.h>\n#include <rte_flow.h>\n#include <rte_interrupts.h>\n#include <rte_kvargs.h>\n#include <rte_malloc.h>\n#include <rte_mbuf.h>\n\n#include \"mlx4.h\"\n#include \"mlx4_glue.h\"\n#include \"mlx4_flow.h\"\n#include \"mlx4_mr.h\"\n#include \"mlx4_rxtx.h\"\n#include \"mlx4_utils.h\"\n\nstatic const char *MZ_MLX4_PMD_SHARED_DATA = \"mlx4_pmd_shared_data\";\n\n/* Shared memory between primary and secondary processes. */\nstruct mlx4_shared_data *mlx4_shared_data;\n\n/* Spinlock for mlx4_shared_data allocation. */\nstatic rte_spinlock_t mlx4_shared_data_lock = RTE_SPINLOCK_INITIALIZER;\n\n/* Process local data for secondary processes. */\nstatic struct mlx4_local_data mlx4_local_data;\n\n/** Configuration structure for device arguments. */\nstruct mlx4_conf {\n\tstruct {\n\t\tuint32_t present; /**< Bit-field for existing ports. */\n\t\tuint32_t enabled; /**< Bit-field for user-enabled ports. */\n\t} ports;\n\tint mr_ext_memseg_en;\n\t/** Whether memseg should be extended for MR creation. */\n};\n\n/* Available parameters list. */\nconst char *pmd_mlx4_init_params[] = {\n\tMLX4_PMD_PORT_KVARG,\n\tMLX4_MR_EXT_MEMSEG_EN_KVARG,\n\tNULL,\n};\n\nstatic void mlx4_dev_stop(struct rte_eth_dev *dev);\n\n/**\n * Initialize shared data between primary and secondary process.\n *\n * A memzone is reserved by primary process and secondary processes attach to\n * the memzone.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_init_shared_data(void)\n{\n\tconst struct rte_memzone *mz;\n\tint ret = 0;\n\n\trte_spinlock_lock(&mlx4_shared_data_lock);\n\tif (mlx4_shared_data == NULL) {\n\t\tif (rte_eal_process_type() == RTE_PROC_PRIMARY) {\n\t\t\t/* Allocate shared memory. */\n\t\t\tmz = rte_memzone_reserve(MZ_MLX4_PMD_SHARED_DATA,\n\t\t\t\t\t\t sizeof(*mlx4_shared_data),\n\t\t\t\t\t\t SOCKET_ID_ANY, 0);\n\t\t\tif (mz == NULL) {\n\t\t\t\tERROR(\"Cannot allocate mlx4 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx4_shared_data = mz->addr;\n\t\t\tmemset(mlx4_shared_data, 0, sizeof(*mlx4_shared_data));\n\t\t\trte_spinlock_init(&mlx4_shared_data->lock);\n\t\t} else {\n\t\t\t/* Lookup allocated shared memory. */\n\t\t\tmz = rte_memzone_lookup(MZ_MLX4_PMD_SHARED_DATA);\n\t\t\tif (mz == NULL) {\n\t\t\t\tERROR(\"Cannot attach mlx4 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx4_shared_data = mz->addr;\n\t\t\tmemset(&mlx4_local_data, 0, sizeof(mlx4_local_data));\n\t\t}\n\t}\nerror:\n\trte_spinlock_unlock(&mlx4_shared_data_lock);\n\treturn ret;\n}\n\n#ifdef HAVE_IBV_MLX4_BUF_ALLOCATORS\n/**\n * Verbs callback to allocate a memory. This function should allocate the space\n * according to the size provided residing inside a huge page.\n * Please note that all allocation must respect the alignment from libmlx4\n * (i.e. currently sysconf(_SC_PAGESIZE)).\n *\n * @param[in] size\n *   The size in bytes of the memory to allocate.\n * @param[in] data\n *   A pointer to the callback data.\n *\n * @return\n *   Allocated buffer, NULL otherwise and rte_errno is set.\n */\nstatic void *\nmlx4_alloc_verbs_buf(size_t size, void *data)\n{\n\tstruct mlx4_priv *priv = data;\n\tvoid *ret;\n\tsize_t alignment = sysconf(_SC_PAGESIZE);\n\tunsigned int socket = SOCKET_ID_ANY;\n\n\tif (priv->verbs_alloc_ctx.type == MLX4_VERBS_ALLOC_TYPE_TX_QUEUE) {\n\t\tconst struct txq *txq = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = txq->socket;\n\t} else if (priv->verbs_alloc_ctx.type ==\n\t\t   MLX4_VERBS_ALLOC_TYPE_RX_QUEUE) {\n\t\tconst struct rxq *rxq = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = rxq->socket;\n\t}\n\tassert(data != NULL);\n\tret = rte_malloc_socket(__func__, size, alignment, socket);\n\tif (!ret && size)\n\t\trte_errno = ENOMEM;\n\treturn ret;\n}\n\n/**\n * Verbs callback to free a memory.\n *\n * @param[in] ptr\n *   A pointer to the memory to free.\n * @param[in] data\n *   A pointer to the callback data.\n */\nstatic void\nmlx4_free_verbs_buf(void *ptr, void *data __rte_unused)\n{\n\tassert(data != NULL);\n\trte_free(ptr);\n}\n#endif\n\n/**\n * Initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_proc_priv_init(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_proc_priv *ppriv;\n\tsize_t ppriv_size;\n\n\t/*\n\t * UAR register table follows the process private structure. BlueFlame\n\t * registers for Tx queues are stored in the table.\n\t */\n\tppriv_size = sizeof(struct mlx4_proc_priv) +\n\t\t     dev->data->nb_tx_queues * sizeof(void *);\n\tppriv = rte_malloc_socket(\"mlx4_proc_priv\", ppriv_size,\n\t\t\t\t  RTE_CACHE_LINE_SIZE, dev->device->numa_node);\n\tif (!ppriv) {\n\t\trte_errno = ENOMEM;\n\t\treturn -rte_errno;\n\t}\n\tppriv->uar_table_sz = ppriv_size;\n\tdev->process_private = ppriv;\n\treturn 0;\n}\n\n/**\n * Un-initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_proc_priv_uninit(struct rte_eth_dev *dev)\n{\n\tif (!dev->process_private)\n\t\treturn;\n\trte_free(dev->process_private);\n\tdev->process_private = NULL;\n}\n\n/**\n * DPDK callback for Ethernet device configuration.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_dev_configure(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tstruct rte_flow_error error;\n\tint ret;\n\n\t/* Prepare internal flow rules. */\n\tret = mlx4_flow_sync(priv, &error);\n\tif (ret) {\n\t\tERROR(\"cannot set up internal flow rules (code %d, \\\"%s\\\"),\"\n\t\t      \" flow error type %d, cause %p, message: %s\",\n\t\t      -ret, strerror(-ret), error.type, error.cause,\n\t\t      error.message ? error.message : \"(unspecified)\");\n\t\tgoto exit;\n\t}\n\tret = mlx4_intr_install(priv);\n\tif (ret) {\n\t\tERROR(\"%p: interrupt handler installation failed\",\n\t\t      (void *)dev);\n\t\tgoto exit;\n\t}\n\tret = mlx4_proc_priv_init(dev);\n\tif (ret) {\n\t\tERROR(\"%p: process private data allocation failed\",\n\t\t      (void *)dev);\n\t\tgoto exit;\n\t}\nexit:\n\treturn ret;\n}\n\n/**\n * DPDK callback to start the device.\n *\n * Simulate device start by initializing common RSS resources and attaching\n * all configured flows.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_dev_start(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tstruct rte_flow_error error;\n\tint ret;\n\n\tif (priv->started)\n\t\treturn 0;\n\tDEBUG(\"%p: attaching configured flows to all RX queues\", (void *)dev);\n\tpriv->started = 1;\n\tret = mlx4_rss_init(priv);\n\tif (ret) {\n\t\tERROR(\"%p: cannot initialize RSS resources: %s\",\n\t\t      (void *)dev, strerror(-ret));\n\t\tgoto err;\n\t}\n#ifndef NDEBUG\n\tmlx4_mr_dump_dev(dev);\n#endif\n\tret = mlx4_rxq_intr_enable(priv);\n\tif (ret) {\n\t\tERROR(\"%p: interrupt handler installation failed\",\n\t\t     (void *)dev);\n\t\tgoto err;\n\t}\n\tret = mlx4_flow_sync(priv, &error);\n\tif (ret) {\n\t\tERROR(\"%p: cannot attach flow rules (code %d, \\\"%s\\\"),\"\n\t\t      \" flow error type %d, cause %p, message: %s\",\n\t\t      (void *)dev,\n\t\t      -ret, strerror(-ret), error.type, error.cause,\n\t\t      error.message ? error.message : \"(unspecified)\");\n\t\tgoto err;\n\t}\n\trte_wmb();\n\tdev->tx_pkt_burst = mlx4_tx_burst;\n\tdev->rx_pkt_burst = mlx4_rx_burst;\n\t/* Enable datapath on secondary process. */\n\tmlx4_mp_req_start_rxtx(dev);\n\treturn 0;\nerr:\n\tmlx4_dev_stop(dev);\n\treturn ret;\n}\n\n/**\n * DPDK callback to stop the device.\n *\n * Simulate device stop by detaching all configured flows.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_dev_stop(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\n\tif (!priv->started)\n\t\treturn;\n\tDEBUG(\"%p: detaching flows from all RX queues\", (void *)dev);\n\tpriv->started = 0;\n\tdev->tx_pkt_burst = mlx4_tx_burst_removed;\n\tdev->rx_pkt_burst = mlx4_rx_burst_removed;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx4_mp_req_stop_rxtx(dev);\n\tmlx4_flow_sync(priv, NULL);\n\tmlx4_rxq_intr_disable(priv);\n\tmlx4_rss_deinit(priv);\n}\n\n/**\n * DPDK callback to close the device.\n *\n * Destroy all queues and objects, free memory.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tunsigned int i;\n\n\tDEBUG(\"%p: closing device \\\"%s\\\"\",\n\t      (void *)dev,\n\t      ((priv->ctx != NULL) ? priv->ctx->device->name : \"\"));\n\tdev->rx_pkt_burst = mlx4_rx_burst_removed;\n\tdev->tx_pkt_burst = mlx4_tx_burst_removed;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx4_mp_req_stop_rxtx(dev);\n\tmlx4_flow_clean(priv);\n\tmlx4_rss_deinit(priv);\n\tfor (i = 0; i != dev->data->nb_rx_queues; ++i)\n\t\tmlx4_rx_queue_release(dev->data->rx_queues[i]);\n\tfor (i = 0; i != dev->data->nb_tx_queues; ++i)\n\t\tmlx4_tx_queue_release(dev->data->tx_queues[i]);\n\tmlx4_proc_priv_uninit(dev);\n\tmlx4_mr_release(dev);\n\tif (priv->pd != NULL) {\n\t\tassert(priv->ctx != NULL);\n\t\tclaim_zero(mlx4_glue->dealloc_pd(priv->pd));\n\t\tclaim_zero(mlx4_glue->close_device(priv->ctx));\n\t} else\n\t\tassert(priv->ctx == NULL);\n\tmlx4_intr_uninstall(priv);\n\tmemset(priv, 0, sizeof(*priv));\n}\n\nstatic const struct eth_dev_ops mlx4_dev_ops = {\n\t.dev_configure = mlx4_dev_configure,\n\t.dev_start = mlx4_dev_start,\n\t.dev_stop = mlx4_dev_stop,\n\t.dev_set_link_down = mlx4_dev_set_link_down,\n\t.dev_set_link_up = mlx4_dev_set_link_up,\n\t.dev_close = mlx4_dev_close,\n\t.link_update = mlx4_link_update,\n\t.promiscuous_enable = mlx4_promiscuous_enable,\n\t.promiscuous_disable = mlx4_promiscuous_disable,\n\t.allmulticast_enable = mlx4_allmulticast_enable,\n\t.allmulticast_disable = mlx4_allmulticast_disable,\n\t.mac_addr_remove = mlx4_mac_addr_remove,\n\t.mac_addr_add = mlx4_mac_addr_add,\n\t.mac_addr_set = mlx4_mac_addr_set,\n\t.set_mc_addr_list = mlx4_set_mc_addr_list,\n\t.stats_get = mlx4_stats_get,\n\t.stats_reset = mlx4_stats_reset,\n\t.fw_version_get = mlx4_fw_version_get,\n\t.dev_infos_get = mlx4_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx4_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx4_vlan_filter_set,\n\t.rx_queue_setup = mlx4_rx_queue_setup,\n\t.tx_queue_setup = mlx4_tx_queue_setup,\n\t.rx_queue_release = mlx4_rx_queue_release,\n\t.tx_queue_release = mlx4_tx_queue_release,\n\t.flow_ctrl_get = mlx4_flow_ctrl_get,\n\t.flow_ctrl_set = mlx4_flow_ctrl_set,\n\t.mtu_set = mlx4_mtu_set,\n\t.filter_ctrl = mlx4_filter_ctrl,\n\t.rx_queue_intr_enable = mlx4_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx4_rx_intr_disable,\n\t.is_removed = mlx4_is_removed,\n};\n\n/* Available operations from secondary process. */\nstatic const struct eth_dev_ops mlx4_dev_sec_ops = {\n\t.stats_get = mlx4_stats_get,\n\t.stats_reset = mlx4_stats_reset,\n\t.fw_version_get = mlx4_fw_version_get,\n\t.dev_infos_get = mlx4_dev_infos_get,\n};\n\n/**\n * Get PCI information from struct ibv_device.\n *\n * @param device\n *   Pointer to Ethernet device structure.\n * @param[out] pci_addr\n *   PCI bus address output buffer.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_ibv_device_to_pci_addr(const struct ibv_device *device,\n\t\t\t    struct rte_pci_addr *pci_addr)\n{\n\tFILE *file;\n\tchar line[32];\n\tMKSTR(path, \"%s/device/uevent\", device->ibdev_path);\n\n\tfile = fopen(path, \"rb\");\n\tif (file == NULL) {\n\t\trte_errno = errno;\n\t\treturn -rte_errno;\n\t}\n\twhile (fgets(line, sizeof(line), file) == line) {\n\t\tsize_t len = strlen(line);\n\t\tint ret;\n\n\t\t/* Truncate long lines. */\n\t\tif (len == (sizeof(line) - 1))\n\t\t\twhile (line[(len - 1)] != '\\n') {\n\t\t\t\tret = fgetc(file);\n\t\t\t\tif (ret == EOF)\n\t\t\t\t\tbreak;\n\t\t\t\tline[(len - 1)] = ret;\n\t\t\t}\n\t\t/* Extract information. */\n\t\tif (sscanf(line,\n\t\t\t   \"PCI_SLOT_NAME=\"\n\t\t\t   \"%\" SCNx32 \":%\" SCNx8 \":%\" SCNx8 \".%\" SCNx8 \"\\n\",\n\t\t\t   &pci_addr->domain,\n\t\t\t   &pci_addr->bus,\n\t\t\t   &pci_addr->devid,\n\t\t\t   &pci_addr->function) == 4) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfclose(file);\n\treturn 0;\n}\n\n/**\n * Verify and store value for device argument.\n *\n * @param[in] key\n *   Key argument to verify.\n * @param[in] val\n *   Value associated with key.\n * @param[in, out] conf\n *   Shared configuration data.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_arg_parse(const char *key, const char *val, struct mlx4_conf *conf)\n{\n\tunsigned long tmp;\n\n\terrno = 0;\n\ttmp = strtoul(val, NULL, 0);\n\tif (errno) {\n\t\trte_errno = errno;\n\t\tWARN(\"%s: \\\"%s\\\" is not a valid integer\", key, val);\n\t\treturn -rte_errno;\n\t}\n\tif (strcmp(MLX4_PMD_PORT_KVARG, key) == 0) {\n\t\tuint32_t ports = rte_log2_u32(conf->ports.present + 1);\n\n\t\tif (tmp >= ports) {\n\t\t\tERROR(\"port index %lu outside range [0,%\" PRIu32 \")\",\n\t\t\t      tmp, ports);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(conf->ports.present & (1 << tmp))) {\n\t\t\trte_errno = EINVAL;\n\t\t\tERROR(\"invalid port index %lu\", tmp);\n\t\t\treturn -rte_errno;\n\t\t}\n\t\tconf->ports.enabled |= 1 << tmp;\n\t} else if (strcmp(MLX4_MR_EXT_MEMSEG_EN_KVARG, key) == 0) {\n\t\tconf->mr_ext_memseg_en = !!tmp;\n\t} else {\n\t\trte_errno = EINVAL;\n\t\tWARN(\"%s: unknown parameter\", key);\n\t\treturn -rte_errno;\n\t}\n\treturn 0;\n}\n\n/**\n * Parse device parameters.\n *\n * @param devargs\n *   Device arguments structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_args(struct rte_devargs *devargs, struct mlx4_conf *conf)\n{\n\tstruct rte_kvargs *kvlist;\n\tunsigned int arg_count;\n\tint ret = 0;\n\tint i;\n\n\tif (devargs == NULL)\n\t\treturn 0;\n\tkvlist = rte_kvargs_parse(devargs->args, pmd_mlx4_init_params);\n\tif (kvlist == NULL) {\n\t\trte_errno = EINVAL;\n\t\tERROR(\"failed to parse kvargs\");\n\t\treturn -rte_errno;\n\t}\n\t/* Process parameters. */\n\tfor (i = 0; pmd_mlx4_init_params[i]; ++i) {\n\t\targ_count = rte_kvargs_count(kvlist, pmd_mlx4_init_params[i]);\n\t\twhile (arg_count-- > 0) {\n\t\t\tret = rte_kvargs_process(kvlist,\n\t\t\t\t\t\t pmd_mlx4_init_params[i],\n\t\t\t\t\t\t (int (*)(const char *,\n\t\t\t\t\t\t\t  const char *,\n\t\t\t\t\t\t\t  void *))\n\t\t\t\t\t\t mlx4_arg_parse,\n\t\t\t\t\t\t conf);\n\t\t\tif (ret != 0)\n\t\t\t\tgoto free_kvlist;\n\t\t}\n\t}\nfree_kvlist:\n\trte_kvargs_free(kvlist);\n\treturn ret;\n}\n\n/**\n * Interpret RSS capabilities reported by device.\n *\n * This function returns the set of usable Verbs RSS hash fields, kernel\n * quirks taken into account.\n *\n * @param ctx\n *   Verbs context.\n * @param pd\n *   Verbs protection domain.\n * @param device_attr_ex\n *   Extended device attributes to interpret.\n *\n * @return\n *   Usable RSS hash fields mask in Verbs format.\n */\nstatic uint64_t\nmlx4_hw_rss_sup(struct ibv_context *ctx, struct ibv_pd *pd,\n\t\tstruct ibv_device_attr_ex *device_attr_ex)\n{\n\tuint64_t hw_rss_sup = device_attr_ex->rss_caps.rx_hash_fields_mask;\n\tstruct ibv_cq *cq = NULL;\n\tstruct ibv_wq *wq = NULL;\n\tstruct ibv_rwq_ind_table *ind = NULL;\n\tstruct ibv_qp *qp = NULL;\n\n\tif (!hw_rss_sup) {\n\t\tWARN(\"no RSS capabilities reported; disabling support for UDP\"\n\t\t     \" RSS and inner VXLAN RSS\");\n\t\treturn IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 |\n\t\t\tIBV_RX_HASH_SRC_IPV6 | IBV_RX_HASH_DST_IPV6 |\n\t\t\tIBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP;\n\t}\n\tif (!(hw_rss_sup & IBV_RX_HASH_INNER))\n\t\treturn hw_rss_sup;\n\t/*\n\t * Although reported as supported, missing code in some Linux\n\t * versions (v4.15, v4.16) prevents the creation of hash QPs with\n\t * inner capability.\n\t *\n\t * There is no choice but to attempt to instantiate a temporary RSS\n\t * context in order to confirm its support.\n\t */\n\tcq = mlx4_glue->create_cq(ctx, 1, NULL, NULL, 0);\n\twq = cq ? mlx4_glue->create_wq\n\t\t(ctx,\n\t\t &(struct ibv_wq_init_attr){\n\t\t\t.wq_type = IBV_WQT_RQ,\n\t\t\t.max_wr = 1,\n\t\t\t.max_sge = 1,\n\t\t\t.pd = pd,\n\t\t\t.cq = cq,\n\t\t }) : NULL;\n\tind = wq ? mlx4_glue->create_rwq_ind_table\n\t\t(ctx,\n\t\t &(struct ibv_rwq_ind_table_init_attr){\n\t\t\t.log_ind_tbl_size = 0,\n\t\t\t.ind_tbl = &wq,\n\t\t\t.comp_mask = 0,\n\t\t }) : NULL;\n\tqp = ind ? mlx4_glue->create_qp_ex\n\t\t(ctx,\n\t\t &(struct ibv_qp_init_attr_ex){\n\t\t\t.comp_mask =\n\t\t\t\t(IBV_QP_INIT_ATTR_PD |\n\t\t\t\t IBV_QP_INIT_ATTR_RX_HASH |\n\t\t\t\t IBV_QP_INIT_ATTR_IND_TABLE),\n\t\t\t.qp_type = IBV_QPT_RAW_PACKET,\n\t\t\t.pd = pd,\n\t\t\t.rwq_ind_tbl = ind,\n\t\t\t.rx_hash_conf = {\n\t\t\t\t.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,\n\t\t\t\t.rx_hash_key_len = MLX4_RSS_HASH_KEY_SIZE,\n\t\t\t\t.rx_hash_key = mlx4_rss_hash_key_default,\n\t\t\t\t.rx_hash_fields_mask = hw_rss_sup,\n\t\t\t},\n\t\t }) : NULL;\n\tif (!qp) {\n\t\tWARN(\"disabling unusable inner RSS capability due to kernel\"\n\t\t     \" quirk\");\n\t\thw_rss_sup &= ~IBV_RX_HASH_INNER;\n\t} else {\n\t\tclaim_zero(mlx4_glue->destroy_qp(qp));\n\t}\n\tif (ind)\n\t\tclaim_zero(mlx4_glue->destroy_rwq_ind_table(ind));\n\tif (wq)\n\t\tclaim_zero(mlx4_glue->destroy_wq(wq));\n\tif (cq)\n\t\tclaim_zero(mlx4_glue->destroy_cq(cq));\n\treturn hw_rss_sup;\n}\n\nstatic struct rte_pci_driver mlx4_driver;\n\n/**\n * PMD global initialization.\n *\n * Independent from individual device, this function initializes global\n * per-PMD data structures distinguishing primary and secondary processes.\n * Hence, each initialization is called once per a process.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_init_once(void)\n{\n\tstruct mlx4_shared_data *sd;\n\tstruct mlx4_local_data *ld = &mlx4_local_data;\n\n\tif (mlx4_init_shared_data())\n\t\treturn -rte_errno;\n\tsd = mlx4_shared_data;\n\tassert(sd);\n\trte_spinlock_lock(&sd->lock);\n\tswitch (rte_eal_process_type()) {\n\tcase RTE_PROC_PRIMARY:\n\t\tif (sd->init_done)\n\t\t\tbreak;\n\t\tLIST_INIT(&sd->mem_event_cb_list);\n\t\trte_rwlock_init(&sd->mem_event_rwlock);\n\t\trte_mem_event_callback_register(\"MLX4_MEM_EVENT_CB\",\n\t\t\t\t\t\tmlx4_mr_mem_event_cb, NULL);\n\t\tmlx4_mp_init_primary();\n\t\tsd->init_done = true;\n\t\tbreak;\n\tcase RTE_PROC_SECONDARY:\n\t\tif (ld->init_done)\n\t\t\tbreak;\n\t\tmlx4_mp_init_secondary();\n\t\t++sd->secondary_cnt;\n\t\tld->init_done = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\trte_spinlock_unlock(&sd->lock);\n\treturn 0;\n}\n\n/**\n * DPDK callback to register a PCI device.\n *\n * This function creates an Ethernet device for each port of a given\n * PCI device.\n *\n * @param[in] pci_drv\n *   PCI driver structure (mlx4_driver).\n * @param[in] pci_dev\n *   PCI device information.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_pci_probe(struct rte_pci_driver *pci_drv, struct rte_pci_device *pci_dev)\n{\n\tstruct ibv_device **list;\n\tstruct ibv_device *ibv_dev;\n\tint err = 0;\n\tstruct ibv_context *attr_ctx = NULL;\n\tstruct ibv_device_attr device_attr;\n\tstruct ibv_device_attr_ex device_attr_ex;\n\tstruct mlx4_conf conf = {\n\t\t.ports.present = 0,\n\t\t.mr_ext_memseg_en = 1,\n\t};\n\tunsigned int vf;\n\tint i;\n\n\t(void)pci_drv;\n\terr = mlx4_init_once();\n\tif (err) {\n\t\tERROR(\"unable to init PMD global data: %s\",\n\t\t      strerror(rte_errno));\n\t\treturn -rte_errno;\n\t}\n\tassert(pci_drv == &mlx4_driver);\n\tlist = mlx4_glue->get_device_list(&i);\n\tif (list == NULL) {\n\t\trte_errno = errno;\n\t\tassert(rte_errno);\n\t\tif (rte_errno == ENOSYS)\n\t\t\tERROR(\"cannot list devices, is ib_uverbs loaded?\");\n\t\treturn -rte_errno;\n\t}\n\tassert(i >= 0);\n\t/*\n\t * For each listed device, check related sysfs entry against\n\t * the provided PCI ID.\n\t */\n\twhile (i != 0) {\n\t\tstruct rte_pci_addr pci_addr;\n\n\t\t--i;\n\t\tDEBUG(\"checking device \\\"%s\\\"\", list[i]->name);\n\t\tif (mlx4_ibv_device_to_pci_addr(list[i], &pci_addr))\n\t\t\tcontinue;\n\t\tif ((pci_dev->addr.domain != pci_addr.domain) ||\n\t\t    (pci_dev->addr.bus != pci_addr.bus) ||\n\t\t    (pci_dev->addr.devid != pci_addr.devid) ||\n\t\t    (pci_dev->addr.function != pci_addr.function))\n\t\t\tcontinue;\n\t\tvf = (pci_dev->id.device_id ==\n\t\t      PCI_DEVICE_ID_MELLANOX_CONNECTX3VF);\n\t\tINFO(\"PCI information matches, using device \\\"%s\\\" (VF: %s)\",\n\t\t     list[i]->name, (vf ? \"true\" : \"false\"));\n\t\tattr_ctx = mlx4_glue->open_device(list[i]);\n\t\terr = errno;\n\t\tbreak;\n\t}\n\tif (attr_ctx == NULL) {\n\t\tmlx4_glue->free_device_list(list);\n\t\tswitch (err) {\n\t\tcase 0:\n\t\t\trte_errno = ENODEV;\n\t\t\tERROR(\"cannot access device, is mlx4_ib loaded?\");\n\t\t\treturn -rte_errno;\n\t\tcase EINVAL:\n\t\t\trte_errno = EINVAL;\n\t\t\tERROR(\"cannot use device, are drivers up to date?\");\n\t\t\treturn -rte_errno;\n\t\t}\n\t\tassert(err > 0);\n\t\trte_errno = err;\n\t\treturn -rte_errno;\n\t}\n\tibv_dev = list[i];\n\tDEBUG(\"device opened\");\n\tif (mlx4_glue->query_device(attr_ctx, &device_attr)) {\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tINFO(\"%u port(s) detected\", device_attr.phys_port_cnt);\n\tconf.ports.present |= (UINT64_C(1) << device_attr.phys_port_cnt) - 1;\n\tif (mlx4_args(pci_dev->device.devargs, &conf)) {\n\t\tERROR(\"failed to process device arguments\");\n\t\terr = EINVAL;\n\t\tgoto error;\n\t}\n\t/* Use all ports when none are defined */\n\tif (!conf.ports.enabled)\n\t\tconf.ports.enabled = conf.ports.present;\n\t/* Retrieve extended device attributes. */\n\tif (mlx4_glue->query_device_ex(attr_ctx, NULL, &device_attr_ex)) {\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tassert(device_attr.max_sge >= MLX4_MAX_SGE);\n\tfor (i = 0; i < device_attr.phys_port_cnt; i++) {\n\t\tuint32_t port = i + 1; /* ports are indexed from one */\n\t\tstruct ibv_context *ctx = NULL;\n\t\tstruct ibv_port_attr port_attr;\n\t\tstruct ibv_pd *pd = NULL;\n\t\tstruct mlx4_priv *priv = NULL;\n\t\tstruct rte_eth_dev *eth_dev = NULL;\n\t\tstruct ether_addr mac;\n\t\tchar name[RTE_ETH_NAME_MAX_LEN];\n\n\t\t/* If port is not enabled, skip. */\n\t\tif (!(conf.ports.enabled & (1 << i)))\n\t\t\tcontinue;\n\t\tDEBUG(\"using port %u\", port);\n\t\tctx = mlx4_glue->open_device(ibv_dev);\n\t\tif (ctx == NULL) {\n\t\t\terr = ENODEV;\n\t\t\tgoto port_error;\n\t\t}\n\t\tsnprintf(name, sizeof(name), \"%s port %u\",\n\t\t\t mlx4_glue->get_device_name(ibv_dev), port);\n\t\tif (rte_eal_process_type() == RTE_PROC_SECONDARY) {\n\t\t\teth_dev = rte_eth_dev_attach_secondary(name);\n\t\t\tif (eth_dev == NULL) {\n\t\t\t\tERROR(\"can not attach rte ethdev\");\n\t\t\t\trte_errno = ENOMEM;\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tpriv = eth_dev->data->dev_private;\n\t\t\tif (!priv->verbs_alloc_ctx.enabled) {\n\t\t\t\tERROR(\"secondary process is not supported\"\n\t\t\t\t      \" due to lack of external allocator\"\n\t\t\t\t      \" from Verbs\");\n\t\t\t\trte_errno = ENOTSUP;\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\teth_dev->device = &pci_dev->device;\n\t\t\teth_dev->dev_ops = &mlx4_dev_sec_ops;\n\t\t\terr = mlx4_proc_priv_init(eth_dev);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t\t/* Receive command fd from primary process. */\n\t\t\terr = mlx4_mp_req_verbs_cmd_fd(eth_dev);\n\t\t\tif (err < 0) {\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/* Remap UAR for Tx queues. */\n\t\t\terr = mlx4_tx_uar_init_secondary(eth_dev, err);\n\t\t\tif (err) {\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Ethdev pointer is still required as input since\n\t\t\t * the primary device is not accessible from the\n\t\t\t * secondary process.\n\t\t\t */\n\t\t\teth_dev->tx_pkt_burst = mlx4_tx_burst;\n\t\t\teth_dev->rx_pkt_burst = mlx4_rx_burst;\n\t\t\tclaim_zero(mlx4_glue->close_device(ctx));\n\t\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\t\t\trte_eth_dev_probing_finish(eth_dev);\n\t\t\tcontinue;\n\t\t}\n\t\t/* Check port status. */\n\t\terr = mlx4_glue->query_port(ctx, port, &port_attr);\n\t\tif (err) {\n\t\t\terr = ENODEV;\n\t\t\tERROR(\"port query failed: %s\", strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\tif (port_attr.link_layer != IBV_LINK_LAYER_ETHERNET) {\n\t\t\terr = ENOTSUP;\n\t\t\tERROR(\"port %d is not configured in Ethernet mode\",\n\t\t\t      port);\n\t\t\tgoto port_error;\n\t\t}\n\t\tif (port_attr.state != IBV_PORT_ACTIVE)\n\t\t\tDEBUG(\"port %d is not active: \\\"%s\\\" (%d)\",\n\t\t\t      port, mlx4_glue->port_state_str(port_attr.state),\n\t\t\t      port_attr.state);\n\t\t/* Make asynchronous FD non-blocking to handle interrupts. */\n\t\terr = mlx4_fd_set_non_blocking(ctx->async_fd);\n\t\tif (err) {\n\t\t\tERROR(\"cannot make asynchronous FD non-blocking: %s\",\n\t\t\t      strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* Allocate protection domain. */\n\t\tpd = mlx4_glue->alloc_pd(ctx);\n\t\tif (pd == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"PD allocation failure\");\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* from rte_ethdev.c */\n\t\tpriv = rte_zmalloc(\"ethdev private structure\",\n\t\t\t\t   sizeof(*priv),\n\t\t\t\t   RTE_CACHE_LINE_SIZE);\n\t\tif (priv == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"priv allocation failure\");\n\t\t\tgoto port_error;\n\t\t}\n\t\tpriv->ctx = ctx;\n\t\tpriv->device_attr = device_attr;\n\t\tpriv->port = port;\n\t\tpriv->pd = pd;\n\t\tpriv->mtu = ETHER_MTU;\n\t\tpriv->vf = vf;\n\t\tpriv->hw_csum =\t!!(device_attr.device_cap_flags &\n\t\t\t\t   IBV_DEVICE_RAW_IP_CSUM);\n\t\tDEBUG(\"checksum offloading is %ssupported\",\n\t\t      (priv->hw_csum ? \"\" : \"not \"));\n\t\t/* Only ConnectX-3 Pro supports tunneling. */\n\t\tpriv->hw_csum_l2tun =\n\t\t\tpriv->hw_csum &&\n\t\t\t(device_attr.vendor_part_id ==\n\t\t\t PCI_DEVICE_ID_MELLANOX_CONNECTX3PRO);\n\t\tDEBUG(\"L2 tunnel checksum offloads are %ssupported\",\n\t\t      priv->hw_csum_l2tun ? \"\" : \"not \");\n\t\tpriv->hw_rss_sup = mlx4_hw_rss_sup(priv->ctx, priv->pd,\n\t\t\t\t\t\t   &device_attr_ex);\n\t\tDEBUG(\"supported RSS hash fields mask: %016\" PRIx64,\n\t\t      priv->hw_rss_sup);\n\t\tpriv->hw_rss_max_qps =\n\t\t\tdevice_attr_ex.rss_caps.max_rwq_indirection_table_size;\n\t\tDEBUG(\"MAX RSS queues %d\", priv->hw_rss_max_qps);\n\t\tpriv->hw_fcs_strip = !!(device_attr_ex.raw_packet_caps &\n\t\t\t\t\tIBV_RAW_PACKET_CAP_SCATTER_FCS);\n\t\tDEBUG(\"FCS stripping toggling is %ssupported\",\n\t\t      priv->hw_fcs_strip ? \"\" : \"not \");\n\t\tpriv->tso =\n\t\t\t((device_attr_ex.tso_caps.max_tso > 0) &&\n\t\t\t (device_attr_ex.tso_caps.supported_qpts &\n\t\t\t  (1 << IBV_QPT_RAW_PACKET)));\n\t\tif (priv->tso)\n\t\t\tpriv->tso_max_payload_sz =\n\t\t\t\t\tdevice_attr_ex.tso_caps.max_tso;\n\t\tDEBUG(\"TSO is %ssupported\",\n\t\t      priv->tso ? \"\" : \"not \");\n\t\tpriv->mr_ext_memseg_en = conf.mr_ext_memseg_en;\n\t\t/* Configure the first MAC address by default. */\n\t\terr = mlx4_get_mac(priv, &mac.addr_bytes);\n\t\tif (err) {\n\t\t\tERROR(\"cannot get MAC address, is mlx4_en loaded?\"\n\t\t\t      \" (error: %s)\", strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\tINFO(\"port %u MAC address is %02x:%02x:%02x:%02x:%02x:%02x\",\n\t\t     priv->port,\n\t\t     mac.addr_bytes[0], mac.addr_bytes[1],\n\t\t     mac.addr_bytes[2], mac.addr_bytes[3],\n\t\t     mac.addr_bytes[4], mac.addr_bytes[5]);\n\t\t/* Register MAC address. */\n\t\tpriv->mac[0] = mac;\n#ifndef NDEBUG\n\t\t{\n\t\t\tchar ifname[IF_NAMESIZE];\n\n\t\t\tif (mlx4_get_ifname(priv, &ifname) == 0)\n\t\t\t\tDEBUG(\"port %u ifname is \\\"%s\\\"\",\n\t\t\t\t      priv->port, ifname);\n\t\t\telse\n\t\t\t\tDEBUG(\"port %u ifname is unknown\", priv->port);\n\t\t}\n#endif\n\t\t/* Get actual MTU if possible. */\n\t\tmlx4_mtu_get(priv, &priv->mtu);\n\t\tDEBUG(\"port %u MTU is %u\", priv->port, priv->mtu);\n\t\teth_dev = rte_eth_dev_allocate(name);\n\t\tif (eth_dev == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"can not allocate rte ethdev\");\n\t\t\tgoto port_error;\n\t\t}\n\t\teth_dev->data->dev_private = priv;\n\t\teth_dev->data->mac_addrs = priv->mac;\n\t\teth_dev->device = &pci_dev->device;\n\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\t\t/* Initialize local interrupt handle for current port. */\n\t\tpriv->intr_handle = (struct rte_intr_handle){\n\t\t\t.fd = -1,\n\t\t\t.type = RTE_INTR_HANDLE_EXT,\n\t\t};\n\t\t/*\n\t\t * Override ethdev interrupt handle pointer with private\n\t\t * handle instead of that of the parent PCI device used by\n\t\t * default. This prevents it from being shared between all\n\t\t * ports of the same PCI device since each of them is\n\t\t * associated its own Verbs context.\n\t\t *\n\t\t * Rx interrupts in particular require this as the PMD has\n\t\t * no control over the registration of queue interrupts\n\t\t * besides setting up eth_dev->intr_handle, the rest is\n\t\t * handled by rte_intr_rx_ctl().\n\t\t */\n\t\teth_dev->intr_handle = &priv->intr_handle;\n\t\tpriv->dev_data = eth_dev->data;\n\t\teth_dev->dev_ops = &mlx4_dev_ops;\n#ifdef HAVE_IBV_MLX4_BUF_ALLOCATORS\n\t\t/* Hint libmlx4 to use PMD allocator for data plane resources */\n\t\tstruct mlx4dv_ctx_allocators alctr = {\n\t\t\t.alloc = &mlx4_alloc_verbs_buf,\n\t\t\t.free = &mlx4_free_verbs_buf,\n\t\t\t.data = priv,\n\t\t};\n\t\terr = mlx4_glue->dv_set_context_attr\n\t\t\t(ctx, MLX4DV_SET_CTX_ATTR_BUF_ALLOCATORS,\n\t\t\t (void *)((uintptr_t)&alctr));\n\t\tif (err)\n\t\t\tWARN(\"Verbs external allocator is not supported\");\n\t\telse\n\t\t\tpriv->verbs_alloc_ctx.enabled = 1;\n#endif\n\t\t/* Bring Ethernet device up. */\n\t\tDEBUG(\"forcing Ethernet interface up\");\n\t\tmlx4_dev_set_link_up(eth_dev);\n\t\t/* Update link status once if waiting for LSC. */\n\t\tif (eth_dev->data->dev_flags & RTE_ETH_DEV_INTR_LSC)\n\t\t\tmlx4_link_update(eth_dev, 0);\n\t\t/*\n\t\t * Once the device is added to the list of memory event\n\t\t * callback, its global MR cache table cannot be expanded\n\t\t * on the fly because of deadlock. If it overflows, lookup\n\t\t * should be done by searching MR list linearly, which is slow.\n\t\t */\n\t\terr = mlx4_mr_btree_init(&priv->mr.cache,\n\t\t\t\t\t MLX4_MR_BTREE_CACHE_N * 2,\n\t\t\t\t\t eth_dev->device->numa_node);\n\t\tif (err) {\n\t\t\t/* rte_errno is already set. */\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* Add device to memory callback list. */\n\t\trte_rwlock_write_lock(&mlx4_shared_data->mem_event_rwlock);\n\t\tLIST_INSERT_HEAD(&mlx4_shared_data->mem_event_cb_list,\n\t\t\t\t priv, mem_event_cb);\n\t\trte_rwlock_write_unlock(&mlx4_shared_data->mem_event_rwlock);\n\t\trte_eth_dev_probing_finish(eth_dev);\n\t\tcontinue;\nport_error:\n\t\trte_free(priv);\n\t\tif (eth_dev != NULL)\n\t\t\teth_dev->data->dev_private = NULL;\n\t\tif (pd)\n\t\t\tclaim_zero(mlx4_glue->dealloc_pd(pd));\n\t\tif (ctx)\n\t\t\tclaim_zero(mlx4_glue->close_device(ctx));\n\t\tif (eth_dev != NULL) {\n\t\t\t/* mac_addrs must not be freed because part of dev_private */\n\t\t\teth_dev->data->mac_addrs = NULL;\n\t\t\trte_eth_dev_release_port(eth_dev);\n\t\t}\n\t\tbreak;\n\t}\n\t/*\n\t * XXX if something went wrong in the loop above, there is a resource\n\t * leak (ctx, pd, priv, dpdk ethdev) but we can do nothing about it as\n\t * long as the dpdk does not provide a way to deallocate a ethdev and a\n\t * way to enumerate the registered ethdevs to free the previous ones.\n\t */\nerror:\n\tif (attr_ctx)\n\t\tclaim_zero(mlx4_glue->close_device(attr_ctx));\n\tif (list)\n\t\tmlx4_glue->free_device_list(list);\n\tif (err)\n\t\trte_errno = err;\n\treturn -err;\n}\n\nstatic const struct rte_pci_id mlx4_pci_id_map[] = {\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3PRO)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3VF)\n\t},\n\t{\n\t\t.vendor_id = 0\n\t}\n};\n\nstatic struct rte_pci_driver mlx4_driver = {\n\t.driver = {\n\t\t.name = MLX4_DRIVER_NAME\n\t},\n\t.id_table = mlx4_pci_id_map,\n\t.probe = mlx4_pci_probe,\n\t.drv_flags = RTE_PCI_DRV_INTR_LSC |\n\t\t     RTE_PCI_DRV_INTR_RMV,\n};\n\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\n/**\n * Suffix RTE_EAL_PMD_PATH with \"-glue\".\n *\n * This function performs a sanity check on RTE_EAL_PMD_PATH before\n * suffixing its last component.\n *\n * @param buf[out]\n *   Output buffer, should be large enough otherwise NULL is returned.\n * @param size\n *   Size of @p out.\n *\n * @return\n *   Pointer to @p buf or @p NULL in case suffix cannot be appended.\n */\nstatic char *\nmlx4_glue_path(char *buf, size_t size)\n{\n\tstatic const char *const bad[] = { \"/\", \".\", \"..\", NULL };\n\tconst char *path = RTE_EAL_PMD_PATH;\n\tsize_t len = strlen(path);\n\tsize_t off;\n\tint i;\n\n\twhile (len && path[len - 1] == '/')\n\t\t--len;\n\tfor (off = len; off && path[off - 1] != '/'; --off)\n\t\t;\n\tfor (i = 0; bad[i]; ++i)\n\t\tif (!strncmp(path + off, bad[i], (int)(len - off)))\n\t\t\tgoto error;\n\ti = snprintf(buf, size, \"%.*s-glue\", (int)len, path);\n\tif (i == -1 || (size_t)i >= size)\n\t\tgoto error;\n\treturn buf;\nerror:\n\tERROR(\"unable to append \\\"-glue\\\" to last component of\"\n\t      \" RTE_EAL_PMD_PATH (\\\"\" RTE_EAL_PMD_PATH \"\\\"),\"\n\t      \" please re-configure DPDK\");\n\treturn NULL;\n}\n\n/**\n * Initialization routine for run-time dependency on rdma-core.\n */\nstatic int\nmlx4_glue_init(void)\n{\n\tchar glue_path[sizeof(RTE_EAL_PMD_PATH) - 1 + sizeof(\"-glue\")];\n\tconst char *path[] = {\n\t\t/*\n\t\t * A basic security check is necessary before trusting\n\t\t * MLX4_GLUE_PATH, which may override RTE_EAL_PMD_PATH.\n\t\t */\n\t\t(geteuid() == getuid() && getegid() == getgid() ?\n\t\t getenv(\"MLX4_GLUE_PATH\") : NULL),\n\t\t/*\n\t\t * When RTE_EAL_PMD_PATH is set, use its glue-suffixed\n\t\t * variant, otherwise let dlopen() look up libraries on its\n\t\t * own.\n\t\t */\n\t\t(*RTE_EAL_PMD_PATH ?\n\t\t mlx4_glue_path(glue_path, sizeof(glue_path)) : \"\"),\n\t};\n\tunsigned int i = 0;\n\tvoid *handle = NULL;\n\tvoid **sym;\n\tconst char *dlmsg;\n\n\twhile (!handle && i != RTE_DIM(path)) {\n\t\tconst char *end;\n\t\tsize_t len;\n\t\tint ret;\n\n\t\tif (!path[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\tend = strpbrk(path[i], \":;\");\n\t\tif (!end)\n\t\t\tend = path[i] + strlen(path[i]);\n\t\tlen = end - path[i];\n\t\tret = 0;\n\t\tdo {\n\t\t\tchar name[ret + 1];\n\n\t\t\tret = snprintf(name, sizeof(name), \"%.*s%s\" MLX4_GLUE,\n\t\t\t\t       (int)len, path[i],\n\t\t\t\t       (!len || *(end - 1) == '/') ? \"\" : \"/\");\n\t\t\tif (ret == -1)\n\t\t\t\tbreak;\n\t\t\tif (sizeof(name) != (size_t)ret + 1)\n\t\t\t\tcontinue;\n\t\t\tDEBUG(\"looking for rdma-core glue as \\\"%s\\\"\", name);\n\t\t\thandle = dlopen(name, RTLD_LAZY);\n\t\t\tbreak;\n\t\t} while (1);\n\t\tpath[i] = end + 1;\n\t\tif (!*end)\n\t\t\t++i;\n\t}\n\tif (!handle) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tWARN(\"cannot load glue library: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tsym = dlsym(handle, \"mlx4_glue\");\n\tif (!sym || !*sym) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tERROR(\"cannot resolve glue symbol: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tmlx4_glue = *sym;\n\treturn 0;\nglue_error:\n\tif (handle)\n\t\tdlclose(handle);\n\tWARN(\"cannot initialize PMD due to missing run-time\"\n\t     \" dependency on rdma-core libraries (libibverbs,\"\n\t     \" libmlx4)\");\n\treturn -rte_errno;\n}\n\n#endif\n\n/**\n * Driver initialization routine.\n */\nRTE_INIT(rte_mlx4_pmd_init)\n{\n\t/*\n\t * MLX4_DEVICE_FATAL_CLEANUP tells ibv_destroy functions we\n\t * want to get success errno value in case of calling them\n\t * when the device was removed.\n\t */\n\tsetenv(\"MLX4_DEVICE_FATAL_CLEANUP\", \"1\", 1);\n\t/*\n\t * RDMAV_HUGEPAGES_SAFE tells ibv_fork_init() we intend to use\n\t * huge pages. Calling ibv_fork_init() during init allows\n\t * applications to use fork() safely for purposes other than\n\t * using this PMD, which is not supported in forked processes.\n\t */\n\tsetenv(\"RDMAV_HUGEPAGES_SAFE\", \"1\", 1);\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\tif (mlx4_glue_init())\n\t\treturn;\n\tassert(mlx4_glue);\n#endif\n#ifndef NDEBUG\n\t/* Glue structure must not contain any NULL pointers. */\n\t{\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i != sizeof(*mlx4_glue) / sizeof(void *); ++i)\n\t\t\tassert(((const void *const *)mlx4_glue)[i]);\n\t}\n#endif\n\tif (strcmp(mlx4_glue->version, MLX4_GLUE_VERSION)) {\n\t\tERROR(\"rdma-core glue \\\"%s\\\" mismatch: \\\"%s\\\" is required\",\n\t\t      mlx4_glue->version, MLX4_GLUE_VERSION);\n\t\treturn;\n\t}\n\tmlx4_glue->fork_init();\n\trte_pci_register(&mlx4_driver);\n}\n\nRTE_PMD_EXPORT_NAME(net_mlx4, __COUNTER__);\nRTE_PMD_REGISTER_PCI_TABLE(net_mlx4, mlx4_pci_id_map);\nRTE_PMD_REGISTER_KMOD_DEP(net_mlx4,\n\t\"* ib_uverbs & mlx4_en & mlx4_core & mlx4_ib\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/ark/ark_ethdev.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright (c) 2015-2018 Atomic Rules LLC\n */\n\n#include <unistd.h>\n#include <sys/stat.h>\n#include <dlfcn.h>\n\n#include <rte_bus_pci.h>\n#include <rte_ethdev_pci.h>\n#include <rte_kvargs.h>\n\n#include \"ark_global.h\"\n#include \"ark_logs.h\"\n#include \"ark_ethdev_tx.h\"\n#include \"ark_ethdev_rx.h\"\n#include \"ark_mpu.h\"\n#include \"ark_ddm.h\"\n#include \"ark_udm.h\"\n#include \"ark_rqp.h\"\n#include \"ark_pktdir.h\"\n#include \"ark_pktgen.h\"\n#include \"ark_pktchkr.h\"\n\n/*  Internal prototypes */\nstatic int eth_ark_check_args(struct ark_adapter *ark, const char *params);\nstatic int eth_ark_dev_init(struct rte_eth_dev *dev);\nstatic int ark_config_device(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_uninit(struct rte_eth_dev *eth_dev);\nstatic int eth_ark_dev_configure(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_start(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_stop(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_close(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_info_get(struct rte_eth_dev *dev,\n\t\t\t\t struct rte_eth_dev_info *dev_info);\nstatic int eth_ark_dev_link_update(struct rte_eth_dev *dev,\n\t\t\t\t   int wait_to_complete);\nstatic int eth_ark_dev_set_link_up(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_set_link_down(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_stats_get(struct rte_eth_dev *dev,\n\t\t\t\t  struct rte_eth_stats *stats);\nstatic void eth_ark_dev_stats_reset(struct rte_eth_dev *dev);\nstatic int eth_ark_set_default_mac_addr(struct rte_eth_dev *dev,\n\t\t\t\t\t struct ether_addr *mac_addr);\nstatic int eth_ark_macaddr_add(struct rte_eth_dev *dev,\n\t\t\t       struct ether_addr *mac_addr,\n\t\t\t       uint32_t index,\n\t\t\t       uint32_t pool);\nstatic void eth_ark_macaddr_remove(struct rte_eth_dev *dev,\n\t\t\t\t   uint32_t index);\nstatic int  eth_ark_set_mtu(struct rte_eth_dev *dev, uint16_t size);\n\n/*\n * The packet generator is a functional block used to generate packet\n * patterns for testing.  It is not intended for nominal use.\n */\n#define ARK_PKTGEN_ARG \"Pkt_gen\"\n\n/*\n * The packet checker is a functional block used to verify packet\n * patterns for testing.  It is not intended for nominal use.\n */\n#define ARK_PKTCHKR_ARG \"Pkt_chkr\"\n\n/*\n * The packet director is used to select the internal ingress and\n * egress packets paths during testing.  It is not intended for\n * nominal use.\n */\n#define ARK_PKTDIR_ARG \"Pkt_dir\"\n\n/* Devinfo configurations */\n#define ARK_RX_MAX_QUEUE (4096 * 4)\n#define ARK_RX_MIN_QUEUE (512)\n#define ARK_RX_MAX_PKT_LEN ((16 * 1024) - 128)\n#define ARK_RX_MIN_BUFSIZE (1024)\n\n#define ARK_TX_MAX_QUEUE (4096 * 4)\n#define ARK_TX_MIN_QUEUE (256)\n\nstatic const char * const valid_arguments[] = {\n\tARK_PKTGEN_ARG,\n\tARK_PKTCHKR_ARG,\n\tARK_PKTDIR_ARG,\n\tNULL\n};\n\nstatic const struct rte_pci_id pci_id_ark_map[] = {\n\t{RTE_PCI_DEVICE(0x1d6c, 0x100d)},\n\t{RTE_PCI_DEVICE(0x1d6c, 0x100e)},\n\t{.vendor_id = 0, /* sentinel */ },\n};\n\nstatic int\neth_ark_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,\n\t\tstruct rte_pci_device *pci_dev)\n{\n\tstruct rte_eth_dev *eth_dev;\n\tint ret;\n\n\teth_dev = rte_eth_dev_pci_allocate(pci_dev, sizeof(struct ark_adapter));\n\n\tif (eth_dev == NULL)\n\t\treturn -ENOMEM;\n\n\tret = eth_ark_dev_init(eth_dev);\n\tif (ret)\n\t\trte_eth_dev_pci_release(eth_dev);\n\n\treturn ret;\n}\n\nstatic int\neth_ark_pci_remove(struct rte_pci_device *pci_dev)\n{\n\treturn rte_eth_dev_pci_generic_remove(pci_dev, eth_ark_dev_uninit);\n}\n\nstatic struct rte_pci_driver rte_ark_pmd = {\n\t.id_table = pci_id_ark_map,\n\t.drv_flags = RTE_PCI_DRV_NEED_MAPPING | RTE_PCI_DRV_INTR_LSC,\n\t.probe = eth_ark_pci_probe,\n\t.remove = eth_ark_pci_remove,\n};\n\nstatic const struct eth_dev_ops ark_eth_dev_ops = {\n\t.dev_configure = eth_ark_dev_configure,\n\t.dev_start = eth_ark_dev_start,\n\t.dev_stop = eth_ark_dev_stop,\n\t.dev_close = eth_ark_dev_close,\n\n\t.dev_infos_get = eth_ark_dev_info_get,\n\n\t.rx_queue_setup = eth_ark_dev_rx_queue_setup,\n\t.rx_queue_count = eth_ark_dev_rx_queue_count,\n\t.tx_queue_setup = eth_ark_tx_queue_setup,\n\n\t.link_update = eth_ark_dev_link_update,\n\t.dev_set_link_up = eth_ark_dev_set_link_up,\n\t.dev_set_link_down = eth_ark_dev_set_link_down,\n\n\t.rx_queue_start = eth_ark_rx_start_queue,\n\t.rx_queue_stop = eth_ark_rx_stop_queue,\n\n\t.tx_queue_start = eth_ark_tx_queue_start,\n\t.tx_queue_stop = eth_ark_tx_queue_stop,\n\n\t.stats_get = eth_ark_dev_stats_get,\n\t.stats_reset = eth_ark_dev_stats_reset,\n\n\t.mac_addr_add = eth_ark_macaddr_add,\n\t.mac_addr_remove = eth_ark_macaddr_remove,\n\t.mac_addr_set = eth_ark_set_default_mac_addr,\n\n\t.mtu_set = eth_ark_set_mtu,\n};\n\nstatic int\ncheck_for_ext(struct ark_adapter *ark)\n{\n\tint found = 0;\n\n\t/* Get the env */\n\tconst char *dllpath = getenv(\"ARK_EXT_PATH\");\n\n\tif (dllpath == NULL) {\n\t\tPMD_DEBUG_LOG(DEBUG, \"ARK EXT NO dll path specified\\n\");\n\t\treturn 0;\n\t}\n\tPMD_DRV_LOG(INFO, \"ARK EXT found dll path at %s\\n\", dllpath);\n\n\t/* Open and load the .so */\n\tark->d_handle = dlopen(dllpath, RTLD_LOCAL | RTLD_LAZY);\n\tif (ark->d_handle == NULL) {\n\t\tPMD_DRV_LOG(ERR, \"Could not load user extension %s\\n\",\n\t\t\t    dllpath);\n\t\treturn -1;\n\t}\n\tPMD_DRV_LOG(INFO, \"SUCCESS: loaded user extension %s\\n\",\n\t\t\t    dllpath);\n\n\t/* Get the entry points */\n\tark->user_ext.dev_init =\n\t\t(void *(*)(struct rte_eth_dev *, void *, int))\n\t\tdlsym(ark->d_handle, \"dev_init\");\n\tPMD_DEBUG_LOG(DEBUG, \"device ext init pointer = %p\\n\",\n\t\t      ark->user_ext.dev_init);\n\tark->user_ext.dev_get_port_count =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_get_port_count\");\n\tark->user_ext.dev_uninit =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_uninit\");\n\tark->user_ext.dev_configure =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_configure\");\n\tark->user_ext.dev_start =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_start\");\n\tark->user_ext.dev_stop =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_stop\");\n\tark->user_ext.dev_close =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_close\");\n\tark->user_ext.link_update =\n\t\t(int (*)(struct rte_eth_dev *, int, void *))\n\t\tdlsym(ark->d_handle, \"link_update\");\n\tark->user_ext.dev_set_link_up =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_set_link_up\");\n\tark->user_ext.dev_set_link_down =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_set_link_down\");\n\tark->user_ext.stats_get =\n\t\t(int (*)(struct rte_eth_dev *, struct rte_eth_stats *,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"stats_get\");\n\tark->user_ext.stats_reset =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"stats_reset\");\n\tark->user_ext.mac_addr_add =\n\t\t(void (*)(struct rte_eth_dev *, struct ether_addr *, uint32_t,\n\t\t\t  uint32_t, void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_add\");\n\tark->user_ext.mac_addr_remove =\n\t\t(void (*)(struct rte_eth_dev *, uint32_t, void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_remove\");\n\tark->user_ext.mac_addr_set =\n\t\t(void (*)(struct rte_eth_dev *, struct ether_addr *,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_set\");\n\tark->user_ext.set_mtu =\n\t\t(int (*)(struct rte_eth_dev *, uint16_t,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"set_mtu\");\n\n\treturn found;\n}\n\nstatic int\neth_ark_dev_init(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tstruct rte_pci_device *pci_dev;\n\tint ret;\n\tint port_count = 1;\n\tint p;\n\n\tark->eth_dev = dev;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\t/* Check to see if there is an extension that we need to load */\n\tret = check_for_ext(ark);\n\tif (ret)\n\t\treturn ret;\n\tpci_dev = RTE_ETH_DEV_TO_PCI(dev);\n\trte_eth_copy_pci_info(dev, pci_dev);\n\n\t/* Use dummy function until setup */\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts_noop;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts_noop;\n\n\tark->bar0 = (uint8_t *)pci_dev->mem_resource[0].addr;\n\tark->a_bar = (uint8_t *)pci_dev->mem_resource[2].addr;\n\n\tark->sysctrl.v  = (void *)&ark->bar0[ARK_SYSCTRL_BASE];\n\tark->mpurx.v  = (void *)&ark->bar0[ARK_MPU_RX_BASE];\n\tark->udm.v  = (void *)&ark->bar0[ARK_UDM_BASE];\n\tark->mputx.v  = (void *)&ark->bar0[ARK_MPU_TX_BASE];\n\tark->ddm.v  = (void *)&ark->bar0[ARK_DDM_BASE];\n\tark->cmac.v  = (void *)&ark->bar0[ARK_CMAC_BASE];\n\tark->external.v  = (void *)&ark->bar0[ARK_EXTERNAL_BASE];\n\tark->pktdir.v  = (void *)&ark->bar0[ARK_PKTDIR_BASE];\n\tark->pktgen.v  = (void *)&ark->bar0[ARK_PKTGEN_BASE];\n\tark->pktchkr.v  = (void *)&ark->bar0[ARK_PKTCHKR_BASE];\n\n\tark->rqpacing =\n\t\t(struct ark_rqpace_t *)(ark->bar0 + ARK_RCPACING_BASE);\n\tark->started = 0;\n\n\tPMD_DEBUG_LOG(INFO, \"Sys Ctrl Const = 0x%x  HW Commit_ID: %08x\\n\",\n\t\t      ark->sysctrl.t32[4],\n\t\t      rte_be_to_cpu_32(ark->sysctrl.t32[0x20 / 4]));\n\tPMD_DRV_LOG(INFO, \"Arkville HW Commit_ID: %08x\\n\",\n\t\t    rte_be_to_cpu_32(ark->sysctrl.t32[0x20 / 4]));\n\n\t/* If HW sanity test fails, return an error */\n\tif (ark->sysctrl.t32[4] != 0xcafef00d) {\n\t\tPMD_DRV_LOG(ERR,\n\t\t\t    \"HW Sanity test has failed, expected constant\"\n\t\t\t    \" 0x%x, read 0x%x (%s)\\n\",\n\t\t\t    0xcafef00d,\n\t\t\t    ark->sysctrl.t32[4], __func__);\n\t\treturn -1;\n\t}\n\tif (ark->sysctrl.t32[3] != 0) {\n\t\tif (ark_rqp_lasped(ark->rqpacing)) {\n\t\t\tPMD_DRV_LOG(ERR, \"Arkville Evaluation System - \"\n\t\t\t\t    \"Timer has Expired\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tPMD_DRV_LOG(WARNING, \"Arkville Evaluation System - \"\n\t\t\t    \"Timer is Running\\n\");\n\t}\n\n\tPMD_DRV_LOG(INFO,\n\t\t    \"HW Sanity test has PASSED, expected constant\"\n\t\t    \" 0x%x, read 0x%x (%s)\\n\",\n\t\t    0xcafef00d, ark->sysctrl.t32[4], __func__);\n\n\t/* We are a single function multi-port device. */\n\tret = ark_config_device(dev);\n\tif (ret)\n\t\treturn -1;\n\n\tdev->dev_ops = &ark_eth_dev_ops;\n\n\tdev->data->mac_addrs = rte_zmalloc(\"ark\", ETHER_ADDR_LEN, 0);\n\tif (!dev->data->mac_addrs) {\n\t\tPMD_DRV_LOG(ERR,\n\t\t\t    \"Failed to allocated memory for storing mac address\"\n\t\t\t    );\n\t}\n\n\tif (ark->user_ext.dev_init) {\n\t\tark->user_data[dev->data->port_id] =\n\t\t\tark->user_ext.dev_init(dev, ark->a_bar, 0);\n\t\tif (!ark->user_data[dev->data->port_id]) {\n\t\t\tPMD_DRV_LOG(INFO,\n\t\t\t\t    \"Failed to initialize PMD extension!\"\n\t\t\t\t    \" continuing without it\\n\");\n\t\t\tmemset(&ark->user_ext, 0, sizeof(struct ark_user_ext));\n\t\t\tdlclose(ark->d_handle);\n\t\t}\n\t}\n\n\tif (pci_dev->device.devargs)\n\t\tret = eth_ark_check_args(ark, pci_dev->device.devargs->args);\n\telse\n\t\tPMD_DRV_LOG(INFO, \"No Device args found\\n\");\n\n\tif (ret)\n\t\tgoto error;\n\t/*\n\t * We will create additional devices based on the number of requested\n\t * ports\n\t */\n\tif (ark->user_ext.dev_get_port_count)\n\t\tport_count =\n\t\t\tark->user_ext.dev_get_port_count(dev,\n\t\t\t\t ark->user_data[dev->data->port_id]);\n\tark->num_ports = port_count;\n\n\tfor (p = 0; p < port_count; p++) {\n\t\tstruct rte_eth_dev *eth_dev;\n\t\tchar name[RTE_ETH_NAME_MAX_LEN];\n\n\t\tsnprintf(name, sizeof(name), \"arketh%d\",\n\t\t\t dev->data->port_id + p);\n\n\t\tif (p == 0) {\n\t\t\t/* First port is already allocated by DPDK */\n\t\t\teth_dev = ark->eth_dev;\n\t\t\trte_eth_dev_probing_finish(eth_dev);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* reserve an ethdev entry */\n\t\teth_dev = rte_eth_dev_allocate(name);\n\t\tif (!eth_dev) {\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"Could not allocate eth_dev for port %d\\n\",\n\t\t\t\t    p);\n\t\t\tgoto error;\n\t\t}\n\n\t\teth_dev->device = &pci_dev->device;\n\t\teth_dev->data->dev_private = ark;\n\t\teth_dev->dev_ops = ark->eth_dev->dev_ops;\n\t\teth_dev->tx_pkt_burst = ark->eth_dev->tx_pkt_burst;\n\t\teth_dev->rx_pkt_burst = ark->eth_dev->rx_pkt_burst;\n\n\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\n\t\teth_dev->data->mac_addrs = rte_zmalloc(name, ETHER_ADDR_LEN, 0);\n\t\tif (!eth_dev->data->mac_addrs) {\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"Memory allocation for MAC failed!\"\n\t\t\t\t    \" Exiting.\\n\");\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (ark->user_ext.dev_init) {\n\t\t\tark->user_data[eth_dev->data->port_id] =\n\t\t\t\tark->user_ext.dev_init(dev, ark->a_bar, p);\n\t\t}\n\n\t\trte_eth_dev_probing_finish(eth_dev);\n\t}\n\n\treturn ret;\n\n error:\n\tif (dev->data->mac_addrs)\n\t\trte_free(dev->data->mac_addrs);\n\treturn -1;\n}\n\n/*\n *Initial device configuration when device is opened\n * setup the DDM, and UDM\n * Called once per PCIE device\n */\nstatic int\nark_config_device(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tuint16_t num_q, i;\n\tstruct ark_mpu_t *mpu;\n\n\t/*\n\t * Make sure that the packet director, generator and checker are in a\n\t * known state\n\t */\n\tark->start_pg = 0;\n\tark->pg = ark_pktgen_init(ark->pktgen.v, 0, 1);\n\tif (ark->pg == NULL)\n\t\treturn -1;\n\tark_pktgen_reset(ark->pg);\n\tark->pc = ark_pktchkr_init(ark->pktchkr.v, 0, 1);\n\tif (ark->pc == NULL)\n\t\treturn -1;\n\tark_pktchkr_stop(ark->pc);\n\tark->pd = ark_pktdir_init(ark->pktdir.v);\n\tif (ark->pd == NULL)\n\t\treturn -1;\n\n\t/* Verify HW */\n\tif (ark_udm_verify(ark->udm.v))\n\t\treturn -1;\n\tif (ark_ddm_verify(ark->ddm.v))\n\t\treturn -1;\n\n\t/* UDM */\n\tif (ark_udm_reset(ark->udm.v)) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to stop and reset UDM\\n\");\n\t\treturn -1;\n\t}\n\t/* Keep in reset until the MPU are cleared */\n\n\t/* MPU reset */\n\tmpu = ark->mpurx.v;\n\tnum_q = ark_api_num_queues(mpu);\n\tark->rx_queues = num_q;\n\tfor (i = 0; i < num_q; i++) {\n\t\tark_mpu_reset(mpu);\n\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t}\n\n\tark_udm_stop(ark->udm.v, 0);\n\tark_udm_configure(ark->udm.v,\n\t\t\t  RTE_PKTMBUF_HEADROOM,\n\t\t\t  RTE_MBUF_DEFAULT_DATAROOM,\n\t\t\t  ARK_RX_WRITE_TIME_NS);\n\tark_udm_stats_reset(ark->udm.v);\n\tark_udm_stop(ark->udm.v, 0);\n\n\t/* TX -- DDM */\n\tif (ark_ddm_stop(ark->ddm.v, 1))\n\t\tPMD_DRV_LOG(ERR, \"Unable to stop DDM\\n\");\n\n\tmpu = ark->mputx.v;\n\tnum_q = ark_api_num_queues(mpu);\n\tark->tx_queues = num_q;\n\tfor (i = 0; i < num_q; i++) {\n\t\tark_mpu_reset(mpu);\n\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t}\n\n\tark_ddm_reset(ark->ddm.v);\n\tark_ddm_stats_reset(ark->ddm.v);\n\n\tark_ddm_stop(ark->ddm.v, 0);\n\tark_rqp_stats_reset(ark->rqpacing);\n\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_uninit(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (rte_eal_process_type() != RTE_PROC_PRIMARY)\n\t\treturn 0;\n\n\tif (ark->user_ext.dev_uninit)\n\t\tark->user_ext.dev_uninit(dev,\n\t\t\t ark->user_data[dev->data->port_id]);\n\n\tark_pktgen_uninit(ark->pg);\n\tark_pktchkr_uninit(ark->pc);\n\n\tdev->dev_ops = NULL;\n\tdev->rx_pkt_burst = NULL;\n\tdev->tx_pkt_burst = NULL;\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_configure(struct rte_eth_dev *dev)\n{\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\teth_ark_dev_set_link_up(dev);\n\tif (ark->user_ext.dev_configure)\n\t\treturn ark->user_ext.dev_configure(dev,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic void *\ndelay_pg_start(void *arg)\n{\n\tstruct ark_adapter *ark = (struct ark_adapter *)arg;\n\n\t/* This function is used exclusively for regression testing, We\n\t * perform a blind sleep here to ensure that the external test\n\t * application has time to setup the test before we generate packets\n\t */\n\tusleep(100000);\n\tark_pktgen_run(ark->pg);\n\treturn NULL;\n}\n\nstatic int\neth_ark_dev_start(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tint i;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\t/* RX Side */\n\t/* start UDM */\n\tark_udm_start(ark->udm.v);\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_ark_rx_start_queue(dev, i);\n\n\t/* TX Side */\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_ark_tx_queue_start(dev, i);\n\n\t/* start DDM */\n\tark_ddm_start(ark->ddm.v);\n\n\tark->started = 1;\n\t/* set xmit and receive function */\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts;\n\n\tif (ark->start_pg)\n\t\tark_pktchkr_run(ark->pc);\n\n\tif (ark->start_pg && (dev->data->port_id == 0)) {\n\t\tpthread_t thread;\n\n\t\t/* Delay packet generatpr start allow the hardware to be ready\n\t\t * This is only used for sanity checking with internal generator\n\t\t */\n\t\tif (pthread_create(&thread, NULL, delay_pg_start, ark)) {\n\t\t\tPMD_DRV_LOG(ERR, \"Could not create pktgen \"\n\t\t\t\t    \"starter thread\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (ark->user_ext.dev_start)\n\t\tark->user_ext.dev_start(dev,\n\t\t\tark->user_data[dev->data->port_id]);\n\n\treturn 0;\n}\n\nstatic void\neth_ark_dev_stop(struct rte_eth_dev *dev)\n{\n\tuint16_t i;\n\tint status;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tstruct ark_mpu_t *mpu;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\tif (ark->started == 0)\n\t\treturn;\n\tark->started = 0;\n\n\t/* Stop the extension first */\n\tif (ark->user_ext.dev_stop)\n\t\tark->user_ext.dev_stop(dev,\n\t\t       ark->user_data[dev->data->port_id]);\n\n\t/* Stop the packet generator */\n\tif (ark->start_pg)\n\t\tark_pktgen_pause(ark->pg);\n\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts_noop;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts_noop;\n\n\t/* STOP TX Side */\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++) {\n\t\tstatus = eth_ark_tx_queue_stop(dev, i);\n\t\tif (status != 0) {\n\t\t\tuint16_t port = dev->data->port_id;\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"tx_queue stop anomaly\"\n\t\t\t\t    \" port %u, queue %u\\n\",\n\t\t\t\t    port, i);\n\t\t}\n\t}\n\n\t/* Stop DDM */\n\t/* Wait up to 0.1 second.  each stop is up to 1000 * 10 useconds */\n\tfor (i = 0; i < 10; i++) {\n\t\tstatus = ark_ddm_stop(ark->ddm.v, 1);\n\t\tif (status == 0)\n\t\t\tbreak;\n\t}\n\tif (status || i != 0) {\n\t\tPMD_DRV_LOG(ERR, \"DDM stop anomaly. status:\"\n\t\t\t    \" %d iter: %u. (%s)\\n\",\n\t\t\t    status,\n\t\t\t    i,\n\t\t\t    __func__);\n\t\tark_ddm_dump(ark->ddm.v, \"Stop anomaly\");\n\n\t\tmpu = ark->mputx.v;\n\t\tfor (i = 0; i < ark->tx_queues; i++) {\n\t\t\tark_mpu_dump(mpu, \"DDM failure dump\", i);\n\t\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t\t}\n\t}\n\n\t/* STOP RX Side */\n\t/* Stop UDM  multiple tries attempted */\n\tfor (i = 0; i < 10; i++) {\n\t\tstatus = ark_udm_stop(ark->udm.v, 1);\n\t\tif (status == 0)\n\t\t\tbreak;\n\t}\n\tif (status || i != 0) {\n\t\tPMD_DRV_LOG(ERR, \"UDM stop anomaly. status %d iter: %u. (%s)\\n\",\n\t\t\t    status, i, __func__);\n\t\tark_udm_dump(ark->udm.v, \"Stop anomaly\");\n\n\t\tmpu = ark->mpurx.v;\n\t\tfor (i = 0; i < ark->rx_queues; i++) {\n\t\t\tark_mpu_dump(mpu, \"UDM Stop anomaly\", i);\n\t\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t\t}\n\t}\n\n\tark_udm_dump_stats(ark->udm.v, \"Post stop\");\n\tark_udm_dump_perf(ark->udm.v, \"Post stop\");\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_ark_rx_dump_queue(dev, i, __func__);\n\n\t/* Stop the packet checker if it is running */\n\tif (ark->start_pg) {\n\t\tark_pktchkr_dump_stats(ark->pc);\n\t\tark_pktchkr_stop(ark->pc);\n\t}\n}\n\nstatic void\neth_ark_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tuint16_t i;\n\n\tif (ark->user_ext.dev_close)\n\t\tark->user_ext.dev_close(dev,\n\t\t ark->user_data[dev->data->port_id]);\n\n\teth_ark_dev_stop(dev);\n\teth_ark_udm_force_close(dev);\n\n\t/*\n\t * TODO This should only be called once for the device during shutdown\n\t */\n\tark_rqp_dump(ark->rqpacing);\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++) {\n\t\teth_ark_tx_queue_release(dev->data->tx_queues[i]);\n\t\tdev->data->tx_queues[i] = 0;\n\t}\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++) {\n\t\teth_ark_dev_rx_queue_release(dev->data->rx_queues[i]);\n\t\tdev->data->rx_queues[i] = 0;\n\t}\n}\n\nstatic void\neth_ark_dev_info_get(struct rte_eth_dev *dev,\n\t\t     struct rte_eth_dev_info *dev_info)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tstruct ark_mpu_t *tx_mpu = RTE_PTR_ADD(ark->bar0, ARK_MPU_TX_BASE);\n\tstruct ark_mpu_t *rx_mpu = RTE_PTR_ADD(ark->bar0, ARK_MPU_RX_BASE);\n\tuint16_t ports = ark->num_ports;\n\n\tdev_info->max_rx_pktlen = ARK_RX_MAX_PKT_LEN;\n\tdev_info->min_rx_bufsize = ARK_RX_MIN_BUFSIZE;\n\n\tdev_info->max_rx_queues = ark_api_num_queues_per_port(rx_mpu, ports);\n\tdev_info->max_tx_queues = ark_api_num_queues_per_port(tx_mpu, ports);\n\n\tdev_info->rx_desc_lim = (struct rte_eth_desc_lim) {\n\t\t.nb_max = ARK_RX_MAX_QUEUE,\n\t\t.nb_min = ARK_RX_MIN_QUEUE,\n\t\t.nb_align = ARK_RX_MIN_QUEUE}; /* power of 2 */\n\n\tdev_info->tx_desc_lim = (struct rte_eth_desc_lim) {\n\t\t.nb_max = ARK_TX_MAX_QUEUE,\n\t\t.nb_min = ARK_TX_MIN_QUEUE,\n\t\t.nb_align = ARK_TX_MIN_QUEUE}; /* power of 2 */\n\n\t/* ARK PMD supports all line rates, how do we indicate that here ?? */\n\tdev_info->speed_capa = (ETH_LINK_SPEED_1G |\n\t\t\t\tETH_LINK_SPEED_10G |\n\t\t\t\tETH_LINK_SPEED_25G |\n\t\t\t\tETH_LINK_SPEED_40G |\n\t\t\t\tETH_LINK_SPEED_50G |\n\t\t\t\tETH_LINK_SPEED_100G);\n}\n\nstatic int\neth_ark_dev_link_update(struct rte_eth_dev *dev, int wait_to_complete)\n{\n\tPMD_DEBUG_LOG(DEBUG, \"link status = %d\\n\",\n\t\t\tdev->data->dev_link.link_status);\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.link_update) {\n\t\treturn ark->user_ext.link_update\n\t\t\t(dev, wait_to_complete,\n\t\t\t ark->user_data[dev->data->port_id]);\n\t}\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_set_link_up(struct rte_eth_dev *dev)\n{\n\tdev->data->dev_link.link_status = 1;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.dev_set_link_up)\n\t\treturn ark->user_ext.dev_set_link_up(dev,\n\t\t\t     ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_set_link_down(struct rte_eth_dev *dev)\n{\n\tdev->data->dev_link.link_status = 0;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.dev_set_link_down)\n\t\treturn ark->user_ext.dev_set_link_down(dev,\n\t\t       ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_stats_get(struct rte_eth_dev *dev, struct rte_eth_stats *stats)\n{\n\tuint16_t i;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tstats->ipackets = 0;\n\tstats->ibytes = 0;\n\tstats->opackets = 0;\n\tstats->obytes = 0;\n\tstats->imissed = 0;\n\tstats->oerrors = 0;\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_tx_queue_stats_get(dev->data->tx_queues[i], stats);\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_rx_queue_stats_get(dev->data->rx_queues[i], stats);\n\tif (ark->user_ext.stats_get)\n\t\treturn ark->user_ext.stats_get(dev, stats,\n\t\t\tark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic void\neth_ark_dev_stats_reset(struct rte_eth_dev *dev)\n{\n\tuint16_t i;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_tx_queue_stats_reset(dev->data->tx_queues[i]);\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_rx_queue_stats_reset(dev->data->rx_queues[i]);\n\tif (ark->user_ext.stats_reset)\n\t\tark->user_ext.stats_reset(dev,\n\t\t\t  ark->user_data[dev->data->port_id]);\n}\n\nstatic int\neth_ark_macaddr_add(struct rte_eth_dev *dev,\n\t\t    struct ether_addr *mac_addr,\n\t\t    uint32_t index,\n\t\t    uint32_t pool)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_add) {\n\t\tark->user_ext.mac_addr_add(dev,\n\t\t\t\t\t   mac_addr,\n\t\t\t\t\t   index,\n\t\t\t\t\t   pool,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\t\treturn 0;\n\t}\n\treturn -ENOTSUP;\n}\n\nstatic void\neth_ark_macaddr_remove(struct rte_eth_dev *dev, uint32_t index)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_remove)\n\t\tark->user_ext.mac_addr_remove(dev, index,\n\t\t\t      ark->user_data[dev->data->port_id]);\n}\n\nstatic int\neth_ark_set_default_mac_addr(struct rte_eth_dev *dev,\n\t\t\t     struct ether_addr *mac_addr)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_set) {\n\t\tark->user_ext.mac_addr_set(dev, mac_addr,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\t\treturn 0;\n\t}\n\treturn -ENOTSUP;\n}\n\nstatic int\neth_ark_set_mtu(struct rte_eth_dev *dev, uint16_t  size)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.set_mtu)\n\t\treturn ark->user_ext.set_mtu(dev, size,\n\t\t\t     ark->user_data[dev->data->port_id]);\n\n\treturn -ENOTSUP;\n}\n\nstatic inline int\nprocess_pktdir_arg(const char *key, const char *value,\n\t\t   void *extra_args)\n{\n\tPMD_FUNC_LOG(DEBUG, \"key = %s, value = %s\\n\",\n\t\t    key, value);\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)extra_args;\n\n\tark->pkt_dir_v = strtol(value, NULL, 16);\n\tPMD_FUNC_LOG(DEBUG, \"pkt_dir_v = 0x%x\\n\", ark->pkt_dir_v);\n\treturn 0;\n}\n\nstatic inline int\nprocess_file_args(const char *key, const char *value, void *extra_args)\n{\n\tPMD_FUNC_LOG(DEBUG, \"key = %s, value = %s\\n\",\n\t\t    key, value);\n\tchar *args = (char *)extra_args;\n\n\t/* Open the configuration file */\n\tFILE *file = fopen(value, \"r\");\n\tchar line[ARK_MAX_ARG_LEN];\n\tint  size = 0;\n\tint first = 1;\n\n\tif (file == NULL) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to open \"\n\t\t\t    \"config file %s\\n\", value);\n\t\treturn -1;\n\t}\n\n\twhile (fgets(line, sizeof(line), file)) {\n\t\tsize += strlen(line);\n\t\tif (size >= ARK_MAX_ARG_LEN) {\n\t\t\tPMD_DRV_LOG(ERR, \"Unable to parse file %s args, \"\n\t\t\t\t    \"parameter list is too long\\n\", value);\n\t\t\tfclose(file);\n\t\t\treturn -1;\n\t\t}\n\t\tif (first) {\n\t\t\tstrncpy(args, line, ARK_MAX_ARG_LEN);\n\t\t\tfirst = 0;\n\t\t} else {\n\t\t\tstrncat(args, line, ARK_MAX_ARG_LEN);\n\t\t}\n\t}\n\tPMD_FUNC_LOG(DEBUG, \"file = %s\\n\", args);\n\tfclose(file);\n\treturn 0;\n}\n\nstatic int\neth_ark_check_args(struct ark_adapter *ark, const char *params)\n{\n\tstruct rte_kvargs *kvlist;\n\tunsigned int k_idx;\n\tstruct rte_kvargs_pair *pair = NULL;\n\tint ret = -1;\n\n\tkvlist = rte_kvargs_parse(params, valid_arguments);\n\tif (kvlist == NULL)\n\t\treturn 0;\n\n\tark->pkt_gen_args[0] = 0;\n\tark->pkt_chkr_args[0] = 0;\n\n\tfor (k_idx = 0; k_idx < kvlist->count; k_idx++) {\n\t\tpair = &kvlist->pairs[k_idx];\n\t\tPMD_FUNC_LOG(DEBUG, \"**** Arg passed to PMD = %s:%s\\n\",\n\t\t\t     pair->key,\n\t\t\t     pair->value);\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTDIR_ARG,\n\t\t\t       &process_pktdir_arg,\n\t\t\t       ark) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTDIR_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTGEN_ARG,\n\t\t\t       &process_file_args,\n\t\t\t       ark->pkt_gen_args) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTGEN_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTCHKR_ARG,\n\t\t\t       &process_file_args,\n\t\t\t       ark->pkt_chkr_args) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTCHKR_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tPMD_DRV_LOG(INFO, \"packet director set to 0x%x\\n\", ark->pkt_dir_v);\n\t/* Setup the packet director */\n\tark_pktdir_setup(ark->pd, ark->pkt_dir_v);\n\n\t/* Setup the packet generator */\n\tif (ark->pkt_gen_args[0]) {\n\t\tPMD_DRV_LOG(INFO, \"Setting up the packet generator\\n\");\n\t\tark_pktgen_parse(ark->pkt_gen_args);\n\t\tark_pktgen_reset(ark->pg);\n\t\tark_pktgen_setup(ark->pg);\n\t\tark->start_pg = 1;\n\t}\n\n\t/* Setup the packet checker */\n\tif (ark->pkt_chkr_args[0]) {\n\t\tark_pktchkr_parse(ark->pkt_chkr_args);\n\t\tark_pktchkr_setup(ark->pc);\n\t}\n\n\tret = 0;\n\nfree_kvlist:\n\trte_kvargs_free(kvlist);\n\n\treturn ret;\n}\n\nRTE_PMD_REGISTER_PCI(net_ark, rte_ark_pmd);\nRTE_PMD_REGISTER_KMOD_DEP(net_ark, \"* igb_uio | uio_pci_generic \");\nRTE_PMD_REGISTER_PCI_TABLE(net_ark, pci_id_ark_map);\nRTE_PMD_REGISTER_PARAM_STRING(net_ark,\n\t\t\t      ARK_PKTGEN_ARG \"=<filename> \"\n\t\t\t      ARK_PKTCHKR_ARG \"=<filename> \"\n\t\t\t      ARK_PKTDIR_ARG \"=<bitmap>\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/mlx5/mlx5.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright 2015 6WIND S.A.\n * Copyright 2015 Mellanox Technologies, Ltd\n */\n\n#include <stddef.h>\n#include <unistd.h>\n#include <string.h>\n#include <assert.h>\n#include <dlfcn.h>\n#include <stdint.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <net/if.h>\n#include <sys/mman.h>\n#include <linux/rtnetlink.h>\n\n/* Verbs header. */\n/* ISO C doesn't support unnamed structs/unions, disabling -pedantic. */\n#ifdef PEDANTIC\n#pragma GCC diagnostic ignored \"-Wpedantic\"\n#endif\n#include <infiniband/verbs.h>\n#ifdef PEDANTIC\n#pragma GCC diagnostic error \"-Wpedantic\"\n#endif\n\n#include <rte_malloc.h>\n#include <rte_ethdev_driver.h>\n#include <rte_ethdev_pci.h>\n#include <rte_pci.h>\n#include <rte_bus_pci.h>\n#include <rte_common.h>\n#include <rte_config.h>\n#include <rte_eal_memconfig.h>\n#include <rte_kvargs.h>\n#include <rte_rwlock.h>\n#include <rte_spinlock.h>\n#include <rte_string_fns.h>\n\n#include \"mlx5.h\"\n#include \"mlx5_utils.h\"\n#include \"mlx5_rxtx.h\"\n#include \"mlx5_autoconf.h\"\n#include \"mlx5_defs.h\"\n#include \"mlx5_glue.h\"\n#include \"mlx5_mr.h\"\n#include \"mlx5_flow.h\"\n\n/* Device parameter to enable RX completion queue compression. */\n#define MLX5_RXQ_CQE_COMP_EN \"rxq_cqe_comp_en\"\n\n/* Device parameter to enable RX completion entry padding to 128B. */\n#define MLX5_RXQ_CQE_PAD_EN \"rxq_cqe_pad_en\"\n\n/* Device parameter to enable padding Rx packet to cacheline size. */\n#define MLX5_RXQ_PKT_PAD_EN \"rxq_pkt_pad_en\"\n\n/* Device parameter to enable Multi-Packet Rx queue. */\n#define MLX5_RX_MPRQ_EN \"mprq_en\"\n\n/* Device parameter to configure log 2 of the number of strides for MPRQ. */\n#define MLX5_RX_MPRQ_LOG_STRIDE_NUM \"mprq_log_stride_num\"\n\n/* Device parameter to limit the size of memcpy'd packet for MPRQ. */\n#define MLX5_RX_MPRQ_MAX_MEMCPY_LEN \"mprq_max_memcpy_len\"\n\n/* Device parameter to set the minimum number of Rx queues to enable MPRQ. */\n#define MLX5_RXQS_MIN_MPRQ \"rxqs_min_mprq\"\n\n/* Device parameter to configure inline send. */\n#define MLX5_TXQ_INLINE \"txq_inline\"\n\n/*\n * Device parameter to configure the number of TX queues threshold for\n * enabling inline send.\n */\n#define MLX5_TXQS_MIN_INLINE \"txqs_min_inline\"\n\n/*\n * Device parameter to configure the number of TX queues threshold for\n * enabling vectorized Tx.\n */\n#define MLX5_TXQS_MAX_VEC \"txqs_max_vec\"\n\n/* Device parameter to enable multi-packet send WQEs. */\n#define MLX5_TXQ_MPW_EN \"txq_mpw_en\"\n\n/* Device parameter to include 2 dsegs in the title WQEBB. */\n#define MLX5_TXQ_MPW_HDR_DSEG_EN \"txq_mpw_hdr_dseg_en\"\n\n/* Device parameter to limit the size of inlining packet. */\n#define MLX5_TXQ_MAX_INLINE_LEN \"txq_max_inline_len\"\n\n/* Device parameter to enable hardware Tx vector. */\n#define MLX5_TX_VEC_EN \"tx_vec_en\"\n\n/* Device parameter to enable hardware Rx vector. */\n#define MLX5_RX_VEC_EN \"rx_vec_en\"\n\n/* Allow L3 VXLAN flow creation. */\n#define MLX5_L3_VXLAN_EN \"l3_vxlan_en\"\n\n/* Activate DV E-Switch flow steering. */\n#define MLX5_DV_ESW_EN \"dv_esw_en\"\n\n/* Activate DV flow steering. */\n#define MLX5_DV_FLOW_EN \"dv_flow_en\"\n\n/* Activate Netlink support in VF mode. */\n#define MLX5_VF_NL_EN \"vf_nl_en\"\n\n/* Enable extending memsegs when creating a MR. */\n#define MLX5_MR_EXT_MEMSEG_EN \"mr_ext_memseg_en\"\n\n/* Select port representors to instantiate. */\n#define MLX5_REPRESENTOR \"representor\"\n\n#ifndef HAVE_IBV_MLX5_MOD_MPW\n#define MLX5DV_CONTEXT_FLAGS_MPW_ALLOWED (1 << 2)\n#define MLX5DV_CONTEXT_FLAGS_ENHANCED_MPW (1 << 3)\n#endif\n\n#ifndef HAVE_IBV_MLX5_MOD_CQE_128B_COMP\n#define MLX5DV_CONTEXT_FLAGS_CQE_128B_COMP (1 << 4)\n#endif\n\nstatic const char *MZ_MLX5_PMD_SHARED_DATA = \"mlx5_pmd_shared_data\";\n\n/* Shared memory between primary and secondary processes. */\nstruct mlx5_shared_data *mlx5_shared_data;\n\n/* Spinlock for mlx5_shared_data allocation. */\nstatic rte_spinlock_t mlx5_shared_data_lock = RTE_SPINLOCK_INITIALIZER;\n\n/* Process local data for secondary processes. */\nstatic struct mlx5_local_data mlx5_local_data;\n\n/** Driver-specific log messages type. */\nint mlx5_logtype;\n\n/** Data associated with devices to spawn. */\nstruct mlx5_dev_spawn_data {\n\tuint32_t ifindex; /**< Network interface index. */\n\tuint32_t max_port; /**< IB device maximal port index. */\n\tuint32_t ibv_port; /**< IB device physical port index. */\n\tstruct mlx5_switch_info info; /**< Switch information. */\n\tstruct ibv_device *ibv_dev; /**< Associated IB device. */\n\tstruct rte_eth_dev *eth_dev; /**< Associated Ethernet device. */\n\tstruct rte_pci_device *pci_dev; /**< Backend PCI device. */\n};\n\nstatic LIST_HEAD(, mlx5_ibv_shared) mlx5_ibv_list = LIST_HEAD_INITIALIZER();\nstatic pthread_mutex_t mlx5_ibv_list_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n/**\n * Allocate shared IB device context. If there is multiport device the\n * master and representors will share this context, if there is single\n * port dedicated IB device, the context will be used by only given\n * port due to unification.\n *\n * Routine first searches the context for the specified IB device name,\n * if found the shared context assumed and reference counter is incremented.\n * If no context found the new one is created and initialized with specified\n * IB device context and parameters.\n *\n * @param[in] spawn\n *   Pointer to the IB device attributes (name, port, etc).\n *\n * @return\n *   Pointer to mlx5_ibv_shared object on success,\n *   otherwise NULL and rte_errno is set.\n */\nstatic struct mlx5_ibv_shared *\nmlx5_alloc_shared_ibctx(const struct mlx5_dev_spawn_data *spawn)\n{\n\tstruct mlx5_ibv_shared *sh;\n\tint err = 0;\n\tuint32_t i;\n\n\tassert(spawn);\n\t/* Secondary process should not create the shared context. */\n\tassert(rte_eal_process_type() == RTE_PROC_PRIMARY);\n\tpthread_mutex_lock(&mlx5_ibv_list_mutex);\n\t/* Search for IB context by device name. */\n\tLIST_FOREACH(sh, &mlx5_ibv_list, next) {\n\t\tif (!strcmp(sh->ibdev_name, spawn->ibv_dev->name)) {\n\t\t\tsh->refcnt++;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\t/* No device found, we have to create new shared context. */\n\tassert(spawn->max_port);\n\tsh = rte_zmalloc(\"ethdev shared ib context\",\n\t\t\t sizeof(struct mlx5_ibv_shared) +\n\t\t\t spawn->max_port *\n\t\t\t sizeof(struct mlx5_ibv_shared_port),\n\t\t\t RTE_CACHE_LINE_SIZE);\n\tif (!sh) {\n\t\tDRV_LOG(ERR, \"shared context allocation failure\");\n\t\trte_errno  = ENOMEM;\n\t\tgoto exit;\n\t}\n\t/* Try to open IB device with DV first, then usual Verbs. */\n\terrno = 0;\n\tsh->ctx = mlx5_glue->dv_open_device(spawn->ibv_dev);\n\tif (sh->ctx) {\n\t\tsh->devx = 1;\n\t\tDRV_LOG(DEBUG, \"DevX is supported\");\n\t} else {\n\t\tsh->ctx = mlx5_glue->open_device(spawn->ibv_dev);\n\t\tif (!sh->ctx) {\n\t\t\terr = errno ? errno : ENODEV;\n\t\t\tgoto error;\n\t\t}\n\t\tDRV_LOG(DEBUG, \"DevX is NOT supported\");\n\t}\n\terr = mlx5_glue->query_device_ex(sh->ctx, NULL, &sh->device_attr);\n\tif (err) {\n\t\tDRV_LOG(DEBUG, \"ibv_query_device_ex() failed\");\n\t\tgoto error;\n\t}\n\tsh->refcnt = 1;\n\tsh->max_port = spawn->max_port;\n\tstrncpy(sh->ibdev_name, sh->ctx->device->name,\n\t\tsizeof(sh->ibdev_name));\n\tstrncpy(sh->ibdev_path, sh->ctx->device->ibdev_path,\n\t\tsizeof(sh->ibdev_path));\n\tsh->pci_dev = spawn->pci_dev;\n\tpthread_mutex_init(&sh->intr_mutex, NULL);\n\t/*\n\t * Setting port_id to max unallowed value means\n\t * there is no interrupt subhandler installed for\n\t * the given port index i.\n\t */\n\tfor (i = 0; i < sh->max_port; i++)\n\t\tsh->port[i].ih_port_id = RTE_MAX_ETHPORTS;\n\tsh->pd = mlx5_glue->alloc_pd(sh->ctx);\n\tif (sh->pd == NULL) {\n\t\tDRV_LOG(ERR, \"PD allocation failure\");\n\t\terr = ENOMEM;\n\t\tgoto error;\n\t}\n\t/*\n\t * Once the device is added to the list of memory event\n\t * callback, its global MR cache table cannot be expanded\n\t * on the fly because of deadlock. If it overflows, lookup\n\t * should be done by searching MR list linearly, which is slow.\n\t *\n\t * At this point the device is not added to the memory\n\t * event list yet, context is just being created.\n\t */\n\terr = mlx5_mr_btree_init(&sh->mr.cache,\n\t\t\t\t MLX5_MR_BTREE_CACHE_N * 2,\n\t\t\t\t sh->pci_dev->device.numa_node);\n\tif (err) {\n\t\terr = rte_errno;\n\t\tgoto error;\n\t}\n\tLIST_INSERT_HEAD(&mlx5_ibv_list, sh, next);\nexit:\n\tpthread_mutex_unlock(&mlx5_ibv_list_mutex);\n\treturn sh;\nerror:\n\tpthread_mutex_unlock(&mlx5_ibv_list_mutex);\n\tassert(sh);\n\tif (sh->pd)\n\t\tclaim_zero(mlx5_glue->dealloc_pd(sh->pd));\n\tif (sh->ctx)\n\t\tclaim_zero(mlx5_glue->close_device(sh->ctx));\n\trte_free(sh);\n\tassert(err > 0);\n\trte_errno = err;\n\treturn NULL;\n}\n\n/**\n * Free shared IB device context. Decrement counter and if zero free\n * all allocated resources and close handles.\n *\n * @param[in] sh\n *   Pointer to mlx5_ibv_shared object to free\n */\nstatic void\nmlx5_free_shared_ibctx(struct mlx5_ibv_shared *sh)\n{\n\tpthread_mutex_lock(&mlx5_ibv_list_mutex);\n#ifndef NDEBUG\n\t/* Check the object presence in the list. */\n\tstruct mlx5_ibv_shared *lctx;\n\n\tLIST_FOREACH(lctx, &mlx5_ibv_list, next)\n\t\tif (lctx == sh)\n\t\t\tbreak;\n\tassert(lctx);\n\tif (lctx != sh) {\n\t\tDRV_LOG(ERR, \"Freeing non-existing shared IB context\");\n\t\tgoto exit;\n\t}\n#endif\n\tassert(sh);\n\tassert(sh->refcnt);\n\t/* Secondary process should not free the shared context. */\n\tassert(rte_eal_process_type() == RTE_PROC_PRIMARY);\n\tif (--sh->refcnt)\n\t\tgoto exit;\n\t/* Release created Memory Regions. */\n\tmlx5_mr_release(sh);\n\tLIST_REMOVE(sh, next);\n\t/*\n\t *  Ensure there is no async event handler installed.\n\t *  Only primary process handles async device events.\n\t **/\n\tassert(!sh->intr_cnt);\n\tif (sh->intr_cnt)\n\t\trte_intr_callback_unregister\n\t\t\t(&sh->intr_handle, mlx5_dev_interrupt_handler, sh);\n\tpthread_mutex_destroy(&sh->intr_mutex);\n\tif (sh->pd)\n\t\tclaim_zero(mlx5_glue->dealloc_pd(sh->pd));\n\tif (sh->ctx)\n\t\tclaim_zero(mlx5_glue->close_device(sh->ctx));\n\trte_free(sh);\nexit:\n\tpthread_mutex_unlock(&mlx5_ibv_list_mutex);\n}\n\n/**\n * Initialize DR related data within private structure.\n * Routine checks the reference counter and does actual\n * resources creation/initialization only if counter is zero.\n *\n * @param[in] priv\n *   Pointer to the private device data structure.\n *\n * @return\n *   Zero on success, positive error code otherwise.\n */\nstatic int\nmlx5_alloc_shared_dr(struct mlx5_priv *priv)\n{\n#ifdef HAVE_MLX5DV_DR\n\tstruct mlx5_ibv_shared *sh = priv->sh;\n\tint err = 0;\n\tvoid *domain;\n\n\tassert(sh);\n\tif (sh->dv_refcnt) {\n\t\t/* Shared DV/DR structures is already initialized. */\n\t\tsh->dv_refcnt++;\n\t\tpriv->dr_shared = 1;\n\t\treturn 0;\n\t}\n\t/* Reference counter is zero, we should initialize structures. */\n\tdomain = mlx5_glue->dr_create_domain(sh->ctx,\n\t\t\t\t\t     MLX5DV_DR_DOMAIN_TYPE_NIC_RX);\n\tif (!domain) {\n\t\tDRV_LOG(ERR, \"ingress mlx5dv_dr_create_domain failed\");\n\t\terr = errno;\n\t\tgoto error;\n\t}\n\tsh->rx_domain = domain;\n\tdomain = mlx5_glue->dr_create_domain(sh->ctx,\n\t\t\t\t\t     MLX5DV_DR_DOMAIN_TYPE_NIC_TX);\n\tif (!domain) {\n\t\tDRV_LOG(ERR, \"egress mlx5dv_dr_create_domain failed\");\n\t\terr = errno;\n\t\tgoto error;\n\t}\n\tpthread_mutex_init(&sh->dv_mutex, NULL);\n\tsh->tx_domain = domain;\n#ifdef HAVE_MLX5DV_DR_ESWITCH\n\tif (priv->config.dv_esw_en) {\n\t\tdomain  = mlx5_glue->dr_create_domain\n\t\t\t(sh->ctx, MLX5DV_DR_DOMAIN_TYPE_FDB);\n\t\tif (!domain) {\n\t\t\tDRV_LOG(ERR, \"FDB mlx5dv_dr_create_domain failed\");\n\t\t\terr = errno;\n\t\t\tgoto error;\n\t\t}\n\t\tsh->fdb_domain = domain;\n\t\tsh->esw_drop_action = mlx5_glue->dr_create_flow_action_drop();\n\t}\n#endif\n\tsh->dv_refcnt++;\n\tpriv->dr_shared = 1;\n\treturn 0;\n\nerror:\n       /* Rollback the created objects. */\n\tif (sh->rx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->rx_domain);\n\t\tsh->rx_domain = NULL;\n\t}\n\tif (sh->tx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->tx_domain);\n\t\tsh->tx_domain = NULL;\n\t}\n\tif (sh->fdb_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->fdb_domain);\n\t\tsh->fdb_domain = NULL;\n\t}\n\tif (sh->esw_drop_action) {\n\t\tmlx5_glue->destroy_flow_action(sh->esw_drop_action);\n\t\tsh->esw_drop_action = NULL;\n\t}\n\treturn err;\n#else\n\t(void)priv;\n\treturn 0;\n#endif\n}\n\n/**\n * Destroy DR related data within private structure.\n *\n * @param[in] priv\n *   Pointer to the private device data structure.\n */\nstatic void\nmlx5_free_shared_dr(struct mlx5_priv *priv)\n{\n#ifdef HAVE_MLX5DV_DR\n\tstruct mlx5_ibv_shared *sh;\n\n\tif (!priv->dr_shared)\n\t\treturn;\n\tpriv->dr_shared = 0;\n\tsh = priv->sh;\n\tassert(sh);\n\tassert(sh->dv_refcnt);\n\tif (sh->dv_refcnt && --sh->dv_refcnt)\n\t\treturn;\n\tif (sh->rx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->rx_domain);\n\t\tsh->rx_domain = NULL;\n\t}\n\tif (sh->tx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->tx_domain);\n\t\tsh->tx_domain = NULL;\n\t}\n#ifdef HAVE_MLX5DV_DR_ESWITCH\n\tif (sh->fdb_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->fdb_domain);\n\t\tsh->fdb_domain = NULL;\n\t}\n\tif (sh->esw_drop_action) {\n\t\tmlx5_glue->destroy_flow_action(sh->esw_drop_action);\n\t\tsh->esw_drop_action = NULL;\n\t}\n#endif\n\tpthread_mutex_destroy(&sh->dv_mutex);\n#else\n\t(void)priv;\n#endif\n}\n\n/**\n * Initialize shared data between primary and secondary process.\n *\n * A memzone is reserved by primary process and secondary processes attach to\n * the memzone.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_init_shared_data(void)\n{\n\tconst struct rte_memzone *mz;\n\tint ret = 0;\n\n\trte_spinlock_lock(&mlx5_shared_data_lock);\n\tif (mlx5_shared_data == NULL) {\n\t\tif (rte_eal_process_type() == RTE_PROC_PRIMARY) {\n\t\t\t/* Allocate shared memory. */\n\t\t\tmz = rte_memzone_reserve(MZ_MLX5_PMD_SHARED_DATA,\n\t\t\t\t\t\t sizeof(*mlx5_shared_data),\n\t\t\t\t\t\t SOCKET_ID_ANY, 0);\n\t\t\tif (mz == NULL) {\n\t\t\t\tDRV_LOG(ERR,\n\t\t\t\t\t\"Cannot allocate mlx5 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx5_shared_data = mz->addr;\n\t\t\tmemset(mlx5_shared_data, 0, sizeof(*mlx5_shared_data));\n\t\t\trte_spinlock_init(&mlx5_shared_data->lock);\n\t\t} else {\n\t\t\t/* Lookup allocated shared memory. */\n\t\t\tmz = rte_memzone_lookup(MZ_MLX5_PMD_SHARED_DATA);\n\t\t\tif (mz == NULL) {\n\t\t\t\tDRV_LOG(ERR,\n\t\t\t\t\t\"Cannot attach mlx5 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx5_shared_data = mz->addr;\n\t\t\tmemset(&mlx5_local_data, 0, sizeof(mlx5_local_data));\n\t\t}\n\t}\nerror:\n\trte_spinlock_unlock(&mlx5_shared_data_lock);\n\treturn ret;\n}\n\n/**\n * Retrieve integer value from environment variable.\n *\n * @param[in] name\n *   Environment variable name.\n *\n * @return\n *   Integer value, 0 if the variable is not set.\n */\nint\nmlx5_getenv_int(const char *name)\n{\n\tconst char *val = getenv(name);\n\n\tif (val == NULL)\n\t\treturn 0;\n\treturn atoi(val);\n}\n\n/**\n * Verbs callback to allocate a memory. This function should allocate the space\n * according to the size provided residing inside a huge page.\n * Please note that all allocation must respect the alignment from libmlx5\n * (i.e. currently sysconf(_SC_PAGESIZE)).\n *\n * @param[in] size\n *   The size in bytes of the memory to allocate.\n * @param[in] data\n *   A pointer to the callback data.\n *\n * @return\n *   Allocated buffer, NULL otherwise and rte_errno is set.\n */\nstatic void *\nmlx5_alloc_verbs_buf(size_t size, void *data)\n{\n\tstruct mlx5_priv *priv = data;\n\tvoid *ret;\n\tsize_t alignment = sysconf(_SC_PAGESIZE);\n\tunsigned int socket = SOCKET_ID_ANY;\n\n\tif (priv->verbs_alloc_ctx.type == MLX5_VERBS_ALLOC_TYPE_TX_QUEUE) {\n\t\tconst struct mlx5_txq_ctrl *ctrl = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = ctrl->socket;\n\t} else if (priv->verbs_alloc_ctx.type ==\n\t\t   MLX5_VERBS_ALLOC_TYPE_RX_QUEUE) {\n\t\tconst struct mlx5_rxq_ctrl *ctrl = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = ctrl->socket;\n\t}\n\tassert(data != NULL);\n\tret = rte_malloc_socket(__func__, size, alignment, socket);\n\tif (!ret && size)\n\t\trte_errno = ENOMEM;\n\treturn ret;\n}\n\n/**\n * Verbs callback to free a memory.\n *\n * @param[in] ptr\n *   A pointer to the memory to free.\n * @param[in] data\n *   A pointer to the callback data.\n */\nstatic void\nmlx5_free_verbs_buf(void *ptr, void *data __rte_unused)\n{\n\tassert(data != NULL);\n\trte_free(ptr);\n}\n\n/**\n * Initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nint\nmlx5_proc_priv_init(struct rte_eth_dev *dev)\n{\n\tstruct mlx5_priv *priv = dev->data->dev_private;\n\tstruct mlx5_proc_priv *ppriv;\n\tsize_t ppriv_size;\n\n\t/*\n\t * UAR register table follows the process private structure. BlueFlame\n\t * registers for Tx queues are stored in the table.\n\t */\n\tppriv_size =\n\t\tsizeof(struct mlx5_proc_priv) + priv->txqs_n * sizeof(void *);\n\tppriv = rte_malloc_socket(\"mlx5_proc_priv\", ppriv_size,\n\t\t\t\t  RTE_CACHE_LINE_SIZE, dev->device->numa_node);\n\tif (!ppriv) {\n\t\trte_errno = ENOMEM;\n\t\treturn -rte_errno;\n\t}\n\tppriv->uar_table_sz = ppriv_size;\n\tdev->process_private = ppriv;\n\treturn 0;\n}\n\n/**\n * Un-initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx5_proc_priv_uninit(struct rte_eth_dev *dev)\n{\n\tif (!dev->process_private)\n\t\treturn;\n\trte_free(dev->process_private);\n\tdev->process_private = NULL;\n}\n\n/**\n * DPDK callback to close the device.\n *\n * Destroy all queues and objects, free memory.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx5_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct mlx5_priv *priv = dev->data->dev_private;\n\tunsigned int i;\n\tint ret;\n\n\tDRV_LOG(DEBUG, \"port %u closing device \\\"%s\\\"\",\n\t\tdev->data->port_id,\n\t\t((priv->sh->ctx != NULL) ? priv->sh->ctx->device->name : \"\"));\n\t/* In case mlx5_dev_stop() has not been called. */\n\tmlx5_dev_interrupt_handler_uninstall(dev);\n\tmlx5_traffic_disable(dev);\n\tmlx5_flow_flush(dev, NULL);\n\t/* Prevent crashes when queues are still in use. */\n\tdev->rx_pkt_burst = removed_rx_burst;\n\tdev->tx_pkt_burst = removed_tx_burst;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx5_mp_req_stop_rxtx(dev);\n\tif (priv->rxqs != NULL) {\n\t\t/* XXX race condition if mlx5_rx_burst() is still running. */\n\t\tusleep(1000);\n\t\tfor (i = 0; (i != priv->rxqs_n); ++i)\n\t\t\tmlx5_rxq_release(dev, i);\n\t\tpriv->rxqs_n = 0;\n\t\tpriv->rxqs = NULL;\n\t}\n\tif (priv->txqs != NULL) {\n\t\t/* XXX race condition if mlx5_tx_burst() is still running. */\n\t\tusleep(1000);\n\t\tfor (i = 0; (i != priv->txqs_n); ++i)\n\t\t\tmlx5_txq_release(dev, i);\n\t\tpriv->txqs_n = 0;\n\t\tpriv->txqs = NULL;\n\t}\n\tmlx5_proc_priv_uninit(dev);\n\tmlx5_mprq_free_mp(dev);\n\t/* Remove from memory callback device list. */\n\trte_rwlock_write_lock(&mlx5_shared_data->mem_event_rwlock);\n\tassert(priv->sh);\n\tLIST_REMOVE(priv->sh, mem_event_cb);\n\trte_rwlock_write_unlock(&mlx5_shared_data->mem_event_rwlock);\n\tmlx5_free_shared_dr(priv);\n\tif (priv->rss_conf.rss_key != NULL)\n\t\trte_free(priv->rss_conf.rss_key);\n\tif (priv->reta_idx != NULL)\n\t\trte_free(priv->reta_idx);\n\tif (priv->config.vf)\n\t\tmlx5_nl_mac_addr_flush(dev);\n\tif (priv->nl_socket_route >= 0)\n\t\tclose(priv->nl_socket_route);\n\tif (priv->nl_socket_rdma >= 0)\n\t\tclose(priv->nl_socket_rdma);\n\tif (priv->tcf_context)\n\t\tmlx5_flow_tcf_context_destroy(priv->tcf_context);\n\tif (priv->sh) {\n\t\t/*\n\t\t * Free the shared context in last turn, because the cleanup\n\t\t * routines above may use some shared fields, like\n\t\t * mlx5_nl_mac_addr_flush() uses ibdev_path for retrieveing\n\t\t * ifindex if Netlink fails.\n\t\t */\n\t\tmlx5_free_shared_ibctx(priv->sh);\n\t\tpriv->sh = NULL;\n\t}\n\tret = mlx5_hrxq_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some hash Rx queue still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_ind_table_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some indirection table still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_rxq_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Verbs Rx queue still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_rxq_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Rx queues still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_txq_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Verbs Tx queue still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_txq_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Tx queues still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_flow_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some flows still remain\",\n\t\t\tdev->data->port_id);\n\tif (priv->domain_id != RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID) {\n\t\tunsigned int c = 0;\n\t\tuint16_t port_id;\n\n\t\tRTE_ETH_FOREACH_DEV_OF(port_id, dev->device) {\n\t\t\tstruct mlx5_priv *opriv =\n\t\t\t\trte_eth_devices[port_id].data->dev_private;\n\n\t\t\tif (!opriv ||\n\t\t\t    opriv->domain_id != priv->domain_id ||\n\t\t\t    &rte_eth_devices[port_id] == dev)\n\t\t\t\tcontinue;\n\t\t\t++c;\n\t\t}\n\t\tif (!c)\n\t\t\tclaim_zero(rte_eth_switch_domain_free(priv->domain_id));\n\t}\n\tmemset(priv, 0, sizeof(*priv));\n\tpriv->domain_id = RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID;\n\t/*\n\t * Reset mac_addrs to NULL such that it is not freed as part of\n\t * rte_eth_dev_release_port(). mac_addrs is part of dev_private so\n\t * it is freed when dev_private is freed.\n\t */\n\tdev->data->mac_addrs = NULL;\n}\n\nconst struct eth_dev_ops mlx5_dev_ops = {\n\t.dev_configure = mlx5_dev_configure,\n\t.dev_start = mlx5_dev_start,\n\t.dev_stop = mlx5_dev_stop,\n\t.dev_set_link_down = mlx5_set_link_down,\n\t.dev_set_link_up = mlx5_set_link_up,\n\t.dev_close = mlx5_dev_close,\n\t.promiscuous_enable = mlx5_promiscuous_enable,\n\t.promiscuous_disable = mlx5_promiscuous_disable,\n\t.allmulticast_enable = mlx5_allmulticast_enable,\n\t.allmulticast_disable = mlx5_allmulticast_disable,\n\t.link_update = mlx5_link_update,\n\t.stats_get = mlx5_stats_get,\n\t.stats_reset = mlx5_stats_reset,\n\t.xstats_get = mlx5_xstats_get,\n\t.xstats_reset = mlx5_xstats_reset,\n\t.xstats_get_names = mlx5_xstats_get_names,\n\t.fw_version_get = mlx5_fw_version_get,\n\t.dev_infos_get = mlx5_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx5_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx5_vlan_filter_set,\n\t.rx_queue_setup = mlx5_rx_queue_setup,\n\t.tx_queue_setup = mlx5_tx_queue_setup,\n\t.rx_queue_release = mlx5_rx_queue_release,\n\t.tx_queue_release = mlx5_tx_queue_release,\n\t.flow_ctrl_get = mlx5_dev_get_flow_ctrl,\n\t.flow_ctrl_set = mlx5_dev_set_flow_ctrl,\n\t.mac_addr_remove = mlx5_mac_addr_remove,\n\t.mac_addr_add = mlx5_mac_addr_add,\n\t.mac_addr_set = mlx5_mac_addr_set,\n\t.set_mc_addr_list = mlx5_set_mc_addr_list,\n\t.mtu_set = mlx5_dev_set_mtu,\n\t.vlan_strip_queue_set = mlx5_vlan_strip_queue_set,\n\t.vlan_offload_set = mlx5_vlan_offload_set,\n\t.reta_update = mlx5_dev_rss_reta_update,\n\t.reta_query = mlx5_dev_rss_reta_query,\n\t.rss_hash_update = mlx5_rss_hash_update,\n\t.rss_hash_conf_get = mlx5_rss_hash_conf_get,\n\t.filter_ctrl = mlx5_dev_filter_ctrl,\n\t.rx_descriptor_status = mlx5_rx_descriptor_status,\n\t.tx_descriptor_status = mlx5_tx_descriptor_status,\n\t.rx_queue_count = mlx5_rx_queue_count,\n\t.rx_queue_intr_enable = mlx5_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx5_rx_intr_disable,\n\t.is_removed = mlx5_is_removed,\n};\n\n/* Available operations from secondary process. */\nstatic const struct eth_dev_ops mlx5_dev_sec_ops = {\n\t.stats_get = mlx5_stats_get,\n\t.stats_reset = mlx5_stats_reset,\n\t.xstats_get = mlx5_xstats_get,\n\t.xstats_reset = mlx5_xstats_reset,\n\t.xstats_get_names = mlx5_xstats_get_names,\n\t.fw_version_get = mlx5_fw_version_get,\n\t.dev_infos_get = mlx5_dev_infos_get,\n\t.rx_descriptor_status = mlx5_rx_descriptor_status,\n\t.tx_descriptor_status = mlx5_tx_descriptor_status,\n};\n\n/* Available operations in flow isolated mode. */\nconst struct eth_dev_ops mlx5_dev_ops_isolate = {\n\t.dev_configure = mlx5_dev_configure,\n\t.dev_start = mlx5_dev_start,\n\t.dev_stop = mlx5_dev_stop,\n\t.dev_set_link_down = mlx5_set_link_down,\n\t.dev_set_link_up = mlx5_set_link_up,\n\t.dev_close = mlx5_dev_close,\n\t.promiscuous_enable = mlx5_promiscuous_enable,\n\t.promiscuous_disable = mlx5_promiscuous_disable,\n\t.allmulticast_enable = mlx5_allmulticast_enable,\n\t.allmulticast_disable = mlx5_allmulticast_disable,\n\t.link_update = mlx5_link_update,\n\t.stats_get = mlx5_stats_get,\n\t.stats_reset = mlx5_stats_reset,\n\t.xstats_get = mlx5_xstats_get,\n\t.xstats_reset = mlx5_xstats_reset,\n\t.xstats_get_names = mlx5_xstats_get_names,\n\t.fw_version_get = mlx5_fw_version_get,\n\t.dev_infos_get = mlx5_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx5_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx5_vlan_filter_set,\n\t.rx_queue_setup = mlx5_rx_queue_setup,\n\t.tx_queue_setup = mlx5_tx_queue_setup,\n\t.rx_queue_release = mlx5_rx_queue_release,\n\t.tx_queue_release = mlx5_tx_queue_release,\n\t.flow_ctrl_get = mlx5_dev_get_flow_ctrl,\n\t.flow_ctrl_set = mlx5_dev_set_flow_ctrl,\n\t.mac_addr_remove = mlx5_mac_addr_remove,\n\t.mac_addr_add = mlx5_mac_addr_add,\n\t.mac_addr_set = mlx5_mac_addr_set,\n\t.set_mc_addr_list = mlx5_set_mc_addr_list,\n\t.mtu_set = mlx5_dev_set_mtu,\n\t.vlan_strip_queue_set = mlx5_vlan_strip_queue_set,\n\t.vlan_offload_set = mlx5_vlan_offload_set,\n\t.filter_ctrl = mlx5_dev_filter_ctrl,\n\t.rx_descriptor_status = mlx5_rx_descriptor_status,\n\t.tx_descriptor_status = mlx5_tx_descriptor_status,\n\t.rx_queue_intr_enable = mlx5_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx5_rx_intr_disable,\n\t.is_removed = mlx5_is_removed,\n};\n\n/**\n * Verify and store value for device argument.\n *\n * @param[in] key\n *   Key argument to verify.\n * @param[in] val\n *   Value associated with key.\n * @param opaque\n *   User data.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_args_check(const char *key, const char *val, void *opaque)\n{\n\tstruct mlx5_dev_config *config = opaque;\n\tunsigned long tmp;\n\n\t/* No-op, port representors are processed in mlx5_dev_spawn(). */\n\tif (!strcmp(MLX5_REPRESENTOR, key))\n\t\treturn 0;\n\terrno = 0;\n\ttmp = strtoul(val, NULL, 0);\n\tif (errno) {\n\t\trte_errno = errno;\n\t\tDRV_LOG(WARNING, \"%s: \\\"%s\\\" is not a valid integer\", key, val);\n\t\treturn -rte_errno;\n\t}\n\tif (strcmp(MLX5_RXQ_CQE_COMP_EN, key) == 0) {\n\t\tconfig->cqe_comp = !!tmp;\n\t} else if (strcmp(MLX5_RXQ_CQE_PAD_EN, key) == 0) {\n\t\tconfig->cqe_pad = !!tmp;\n\t} else if (strcmp(MLX5_RXQ_PKT_PAD_EN, key) == 0) {\n\t\tconfig->hw_padding = !!tmp;\n\t} else if (strcmp(MLX5_RX_MPRQ_EN, key) == 0) {\n\t\tconfig->mprq.enabled = !!tmp;\n\t} else if (strcmp(MLX5_RX_MPRQ_LOG_STRIDE_NUM, key) == 0) {\n\t\tconfig->mprq.stride_num_n = tmp;\n\t} else if (strcmp(MLX5_RX_MPRQ_MAX_MEMCPY_LEN, key) == 0) {\n\t\tconfig->mprq.max_memcpy_len = tmp;\n\t} else if (strcmp(MLX5_RXQS_MIN_MPRQ, key) == 0) {\n\t\tconfig->mprq.min_rxqs_num = tmp;\n\t} else if (strcmp(MLX5_TXQ_INLINE, key) == 0) {\n\t\tconfig->txq_inline = tmp;\n\t} else if (strcmp(MLX5_TXQS_MIN_INLINE, key) == 0) {\n\t\tconfig->txqs_inline = tmp;\n\t} else if (strcmp(MLX5_TXQS_MAX_VEC, key) == 0) {\n\t\tconfig->txqs_vec = tmp;\n\t} else if (strcmp(MLX5_TXQ_MPW_EN, key) == 0) {\n\t\tconfig->mps = !!tmp;\n\t} else if (strcmp(MLX5_TXQ_MPW_HDR_DSEG_EN, key) == 0) {\n\t\tconfig->mpw_hdr_dseg = !!tmp;\n\t} else if (strcmp(MLX5_TXQ_MAX_INLINE_LEN, key) == 0) {\n\t\tconfig->inline_max_packet_sz = tmp;\n\t} else if (strcmp(MLX5_TX_VEC_EN, key) == 0) {\n\t\tconfig->tx_vec_en = !!tmp;\n\t} else if (strcmp(MLX5_RX_VEC_EN, key) == 0) {\n\t\tconfig->rx_vec_en = !!tmp;\n\t} else if (strcmp(MLX5_L3_VXLAN_EN, key) == 0) {\n\t\tconfig->l3_vxlan_en = !!tmp;\n\t} else if (strcmp(MLX5_VF_NL_EN, key) == 0) {\n\t\tconfig->vf_nl_en = !!tmp;\n\t} else if (strcmp(MLX5_DV_ESW_EN, key) == 0) {\n\t\tconfig->dv_esw_en = !!tmp;\n\t} else if (strcmp(MLX5_DV_FLOW_EN, key) == 0) {\n\t\tconfig->dv_flow_en = !!tmp;\n\t} else if (strcmp(MLX5_MR_EXT_MEMSEG_EN, key) == 0) {\n\t\tconfig->mr_ext_memseg_en = !!tmp;\n\t} else {\n\t\tDRV_LOG(WARNING, \"%s: unknown parameter\", key);\n\t\trte_errno = EINVAL;\n\t\treturn -rte_errno;\n\t}\n\treturn 0;\n}\n\n/**\n * Parse device parameters.\n *\n * @param config\n *   Pointer to device configuration structure.\n * @param devargs\n *   Device arguments structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_args(struct mlx5_dev_config *config, struct rte_devargs *devargs)\n{\n\tconst char **params = (const char *[]){\n\t\tMLX5_RXQ_CQE_COMP_EN,\n\t\tMLX5_RXQ_CQE_PAD_EN,\n\t\tMLX5_RXQ_PKT_PAD_EN,\n\t\tMLX5_RX_MPRQ_EN,\n\t\tMLX5_RX_MPRQ_LOG_STRIDE_NUM,\n\t\tMLX5_RX_MPRQ_MAX_MEMCPY_LEN,\n\t\tMLX5_RXQS_MIN_MPRQ,\n\t\tMLX5_TXQ_INLINE,\n\t\tMLX5_TXQS_MIN_INLINE,\n\t\tMLX5_TXQS_MAX_VEC,\n\t\tMLX5_TXQ_MPW_EN,\n\t\tMLX5_TXQ_MPW_HDR_DSEG_EN,\n\t\tMLX5_TXQ_MAX_INLINE_LEN,\n\t\tMLX5_TX_VEC_EN,\n\t\tMLX5_RX_VEC_EN,\n\t\tMLX5_L3_VXLAN_EN,\n\t\tMLX5_VF_NL_EN,\n\t\tMLX5_DV_ESW_EN,\n\t\tMLX5_DV_FLOW_EN,\n\t\tMLX5_MR_EXT_MEMSEG_EN,\n\t\tMLX5_REPRESENTOR,\n\t\tNULL,\n\t};\n\tstruct rte_kvargs *kvlist;\n\tint ret = 0;\n\tint i;\n\n\tif (devargs == NULL)\n\t\treturn 0;\n\t/* Following UGLY cast is done to pass checkpatch. */\n\tkvlist = rte_kvargs_parse(devargs->args, params);\n\tif (kvlist == NULL)\n\t\treturn 0;\n\t/* Process parameters. */\n\tfor (i = 0; (params[i] != NULL); ++i) {\n\t\tif (rte_kvargs_count(kvlist, params[i])) {\n\t\t\tret = rte_kvargs_process(kvlist, params[i],\n\t\t\t\t\t\t mlx5_args_check, config);\n\t\t\tif (ret) {\n\t\t\t\trte_errno = EINVAL;\n\t\t\t\trte_kvargs_free(kvlist);\n\t\t\t\treturn -rte_errno;\n\t\t\t}\n\t\t}\n\t}\n\trte_kvargs_free(kvlist);\n\treturn 0;\n}\n\nstatic struct rte_pci_driver mlx5_driver;\n\n/**\n * PMD global initialization.\n *\n * Independent from individual device, this function initializes global\n * per-PMD data structures distinguishing primary and secondary processes.\n * Hence, each initialization is called once per a process.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_init_once(void)\n{\n\tstruct mlx5_shared_data *sd;\n\tstruct mlx5_local_data *ld = &mlx5_local_data;\n\n\tif (mlx5_init_shared_data())\n\t\treturn -rte_errno;\n\tsd = mlx5_shared_data;\n\tassert(sd);\n\trte_spinlock_lock(&sd->lock);\n\tswitch (rte_eal_process_type()) {\n\tcase RTE_PROC_PRIMARY:\n\t\tif (sd->init_done)\n\t\t\tbreak;\n\t\tLIST_INIT(&sd->mem_event_cb_list);\n\t\trte_rwlock_init(&sd->mem_event_rwlock);\n\t\trte_mem_event_callback_register(\"MLX5_MEM_EVENT_CB\",\n\t\t\t\t\t\tmlx5_mr_mem_event_cb, NULL);\n\t\tmlx5_mp_init_primary();\n\t\tsd->init_done = true;\n\t\tbreak;\n\tcase RTE_PROC_SECONDARY:\n\t\tif (ld->init_done)\n\t\t\tbreak;\n\t\tmlx5_mp_init_secondary();\n\t\t++sd->secondary_cnt;\n\t\tld->init_done = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\trte_spinlock_unlock(&sd->lock);\n\treturn 0;\n}\n\n/**\n * Spawn an Ethernet device from Verbs information.\n *\n * @param dpdk_dev\n *   Backing DPDK device.\n * @param spawn\n *   Verbs device parameters (name, port, switch_info) to spawn.\n * @param config\n *   Device configuration parameters.\n *\n * @return\n *   A valid Ethernet device object on success, NULL otherwise and rte_errno\n *   is set. The following errors are defined:\n *\n *   EBUSY: device is not supposed to be spawned.\n *   EEXIST: device is already spawned\n */\nstatic struct rte_eth_dev *\nmlx5_dev_spawn(struct rte_device *dpdk_dev,\n\t       struct mlx5_dev_spawn_data *spawn,\n\t       struct mlx5_dev_config config)\n{\n\tconst struct mlx5_switch_info *switch_info = &spawn->info;\n\tstruct mlx5_ibv_shared *sh = NULL;\n\tstruct ibv_port_attr port_attr;\n\tstruct mlx5dv_context dv_attr = { .comp_mask = 0 };\n\tstruct rte_eth_dev *eth_dev = NULL;\n\tstruct mlx5_priv *priv = NULL;\n\tint err = 0;\n\tunsigned int hw_padding = 0;\n\tunsigned int mps;\n\tunsigned int cqe_comp;\n\tunsigned int cqe_pad = 0;\n\tunsigned int tunnel_en = 0;\n\tunsigned int mpls_en = 0;\n\tunsigned int swp = 0;\n\tunsigned int mprq = 0;\n\tunsigned int mprq_min_stride_size_n = 0;\n\tunsigned int mprq_max_stride_size_n = 0;\n\tunsigned int mprq_min_stride_num_n = 0;\n\tunsigned int mprq_max_stride_num_n = 0;\n\tstruct ether_addr mac;\n\tchar name[RTE_ETH_NAME_MAX_LEN];\n\tint own_domain_id = 0;\n\tuint16_t port_id;\n\tunsigned int i;\n\n\t/* Determine if this port representor is supposed to be spawned. */\n\tif (switch_info->representor && dpdk_dev->devargs) {\n\t\tstruct rte_eth_devargs eth_da;\n\n\t\terr = rte_eth_devargs_parse(dpdk_dev->devargs->args, &eth_da);\n\t\tif (err) {\n\t\t\trte_errno = -err;\n\t\t\tDRV_LOG(ERR, \"failed to process device arguments: %s\",\n\t\t\t\tstrerror(rte_errno));\n\t\t\treturn NULL;\n\t\t}\n\t\tfor (i = 0; i < eth_da.nb_representor_ports; ++i)\n\t\t\tif (eth_da.representor_ports[i] ==\n\t\t\t    (uint16_t)switch_info->port_name)\n\t\t\t\tbreak;\n\t\tif (i == eth_da.nb_representor_ports) {\n\t\t\trte_errno = EBUSY;\n\t\t\treturn NULL;\n\t\t}\n\t}\n\t/* Build device name. */\n\tif (!switch_info->representor)\n\t\tstrlcpy(name, dpdk_dev->name, sizeof(name));\n\telse\n\t\tsnprintf(name, sizeof(name), \"%s_representor_%u\",\n\t\t\t dpdk_dev->name, switch_info->port_name);\n\t/* check if the device is already spawned */\n\tif (rte_eth_dev_get_port_by_name(name, &port_id) == 0) {\n\t\trte_errno = EEXIST;\n\t\treturn NULL;\n\t}\n\tDRV_LOG(DEBUG, \"naming Ethernet device \\\"%s\\\"\", name);\n\tif (rte_eal_process_type() == RTE_PROC_SECONDARY) {\n\t\teth_dev = rte_eth_dev_attach_secondary(name);\n\t\tif (eth_dev == NULL) {\n\t\t\tDRV_LOG(ERR, \"can not attach rte ethdev\");\n\t\t\trte_errno = ENOMEM;\n\t\t\treturn NULL;\n\t\t}\n\t\teth_dev->device = dpdk_dev;\n\t\teth_dev->dev_ops = &mlx5_dev_sec_ops;\n\t\terr = mlx5_proc_priv_init(eth_dev);\n\t\tif (err)\n\t\t\treturn NULL;\n\t\t/* Receive command fd from primary process */\n\t\terr = mlx5_mp_req_verbs_cmd_fd(eth_dev);\n\t\tif (err < 0)\n\t\t\treturn NULL;\n\t\t/* Remap UAR for Tx queues. */\n\t\terr = mlx5_tx_uar_init_secondary(eth_dev, err);\n\t\tif (err)\n\t\t\treturn NULL;\n\t\t/*\n\t\t * Ethdev pointer is still required as input since\n\t\t * the primary device is not accessible from the\n\t\t * secondary process.\n\t\t */\n\t\teth_dev->rx_pkt_burst = mlx5_select_rx_function(eth_dev);\n\t\teth_dev->tx_pkt_burst = mlx5_select_tx_function(eth_dev);\n\t\treturn eth_dev;\n\t}\n\tsh = mlx5_alloc_shared_ibctx(spawn);\n\tif (!sh)\n\t\treturn NULL;\n\tconfig.devx = sh->devx;\n#ifdef HAVE_IBV_MLX5_MOD_SWP\n\tdv_attr.comp_mask |= MLX5DV_CONTEXT_MASK_SWP;\n#endif\n\t/*\n\t * Multi-packet send is supported by ConnectX-4 Lx PF as well\n\t * as all ConnectX-5 devices.\n\t */\n#ifdef HAVE_IBV_DEVICE_TUNNEL_SUPPORT\n\tdv_attr.comp_mask |= MLX5DV_CONTEXT_MASK_TUNNEL_OFFLOADS;\n#endif\n#ifdef HAVE_IBV_DEVICE_STRIDING_RQ_SUPPORT\n\tdv_attr.comp_mask |= MLX5DV_CONTEXT_MASK_STRIDING_RQ;\n#endif\n\tmlx5_glue->dv_query_device(sh->ctx, &dv_attr);\n\tif (dv_attr.flags & MLX5DV_CONTEXT_FLAGS_MPW_ALLOWED) {\n\t\tif (dv_attr.flags & MLX5DV_CONTEXT_FLAGS_ENHANCED_MPW) {\n\t\t\tDRV_LOG(DEBUG, \"enhanced MPW is supported\");\n\t\t\tmps = MLX5_MPW_ENHANCED;\n\t\t} else {\n\t\t\tDRV_LOG(DEBUG, \"MPW is supported\");\n\t\t\tmps = MLX5_MPW;\n\t\t}\n\t} else {\n\t\tDRV_LOG(DEBUG, \"MPW isn't supported\");\n\t\tmps = MLX5_MPW_DISABLED;\n\t}\n#ifdef HAVE_IBV_MLX5_MOD_SWP\n\tif (dv_attr.comp_mask & MLX5DV_CONTEXT_MASK_SWP)\n\t\tswp = dv_attr.sw_parsing_caps.sw_parsing_offloads;\n\tDRV_LOG(DEBUG, \"SWP support: %u\", swp);\n#endif\n\tconfig.swp = !!swp;\n#ifdef HAVE_IBV_DEVICE_STRIDING_RQ_SUPPORT\n\tif (dv_attr.comp_mask & MLX5DV_CONTEXT_MASK_STRIDING_RQ) {\n\t\tstruct mlx5dv_striding_rq_caps mprq_caps =\n\t\t\tdv_attr.striding_rq_caps;\n\n\t\tDRV_LOG(DEBUG, \"\\tmin_single_stride_log_num_of_bytes: %d\",\n\t\t\tmprq_caps.min_single_stride_log_num_of_bytes);\n\t\tDRV_LOG(DEBUG, \"\\tmax_single_stride_log_num_of_bytes: %d\",\n\t\t\tmprq_caps.max_single_stride_log_num_of_bytes);\n\t\tDRV_LOG(DEBUG, \"\\tmin_single_wqe_log_num_of_strides: %d\",\n\t\t\tmprq_caps.min_single_wqe_log_num_of_strides);\n\t\tDRV_LOG(DEBUG, \"\\tmax_single_wqe_log_num_of_strides: %d\",\n\t\t\tmprq_caps.max_single_wqe_log_num_of_strides);\n\t\tDRV_LOG(DEBUG, \"\\tsupported_qpts: %d\",\n\t\t\tmprq_caps.supported_qpts);\n\t\tDRV_LOG(DEBUG, \"device supports Multi-Packet RQ\");\n\t\tmprq = 1;\n\t\tmprq_min_stride_size_n =\n\t\t\tmprq_caps.min_single_stride_log_num_of_bytes;\n\t\tmprq_max_stride_size_n =\n\t\t\tmprq_caps.max_single_stride_log_num_of_bytes;\n\t\tmprq_min_stride_num_n =\n\t\t\tmprq_caps.min_single_wqe_log_num_of_strides;\n\t\tmprq_max_stride_num_n =\n\t\t\tmprq_caps.max_single_wqe_log_num_of_strides;\n\t\tconfig.mprq.stride_num_n = RTE_MAX(MLX5_MPRQ_STRIDE_NUM_N,\n\t\t\t\t\t\t   mprq_min_stride_num_n);\n\t}\n#endif\n\tif (RTE_CACHE_LINE_SIZE == 128 &&\n\t    !(dv_attr.flags & MLX5DV_CONTEXT_FLAGS_CQE_128B_COMP))\n\t\tcqe_comp = 0;\n\telse\n\t\tcqe_comp = 1;\n\tconfig.cqe_comp = cqe_comp;\n#ifdef HAVE_IBV_MLX5_MOD_CQE_128B_PAD\n\t/* Whether device supports 128B Rx CQE padding. */\n\tcqe_pad = RTE_CACHE_LINE_SIZE == 128 &&\n\t\t  (dv_attr.flags & MLX5DV_CONTEXT_FLAGS_CQE_128B_PAD);\n#endif\n#ifdef HAVE_IBV_DEVICE_TUNNEL_SUPPORT\n\tif (dv_attr.comp_mask & MLX5DV_CONTEXT_MASK_TUNNEL_OFFLOADS) {\n\t\ttunnel_en = ((dv_attr.tunnel_offloads_caps &\n\t\t\t      MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_VXLAN) &&\n\t\t\t     (dv_attr.tunnel_offloads_caps &\n\t\t\t      MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_GRE));\n\t}\n\tDRV_LOG(DEBUG, \"tunnel offloading is %ssupported\",\n\t\ttunnel_en ? \"\" : \"not \");\n#else\n\tDRV_LOG(WARNING,\n\t\t\"tunnel offloading disabled due to old OFED/rdma-core version\");\n#endif\n\tconfig.tunnel_en = tunnel_en;\n#ifdef HAVE_IBV_DEVICE_MPLS_SUPPORT\n\tmpls_en = ((dv_attr.tunnel_offloads_caps &\n\t\t    MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_CW_MPLS_OVER_GRE) &&\n\t\t   (dv_attr.tunnel_offloads_caps &\n\t\t    MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_CW_MPLS_OVER_UDP));\n\tDRV_LOG(DEBUG, \"MPLS over GRE/UDP tunnel offloading is %ssupported\",\n\t\tmpls_en ? \"\" : \"not \");\n#else\n\tDRV_LOG(WARNING, \"MPLS over GRE/UDP tunnel offloading disabled due to\"\n\t\t\" old OFED/rdma-core version or firmware configuration\");\n#endif\n\tconfig.mpls_en = mpls_en;\n\t/* Check port status. */\n\terr = mlx5_glue->query_port(sh->ctx, spawn->ibv_port, &port_attr);\n\tif (err) {\n\t\tDRV_LOG(ERR, \"port query failed: %s\", strerror(err));\n\t\tgoto error;\n\t}\n\tif (port_attr.link_layer != IBV_LINK_LAYER_ETHERNET) {\n\t\tDRV_LOG(ERR, \"port is not configured in Ethernet mode\");\n\t\terr = EINVAL;\n\t\tgoto error;\n\t}\n\tif (port_attr.state != IBV_PORT_ACTIVE)\n\t\tDRV_LOG(DEBUG, \"port is not active: \\\"%s\\\" (%d)\",\n\t\t\tmlx5_glue->port_state_str(port_attr.state),\n\t\t\tport_attr.state);\n\t/* Allocate private eth device data. */\n\tpriv = rte_zmalloc(\"ethdev private structure\",\n\t\t\t   sizeof(*priv),\n\t\t\t   RTE_CACHE_LINE_SIZE);\n\tif (priv == NULL) {\n\t\tDRV_LOG(ERR, \"priv allocation failure\");\n\t\terr = ENOMEM;\n\t\tgoto error;\n\t}\n\tpriv->sh = sh;\n\tpriv->ibv_port = spawn->ibv_port;\n\tpriv->mtu = ETHER_MTU;\n#ifndef RTE_ARCH_64\n\t/* Initialize UAR access locks for 32bit implementations. */\n\trte_spinlock_init(&priv->uar_lock_cq);\n\tfor (i = 0; i < MLX5_UAR_PAGE_NUM_MAX; i++)\n\t\trte_spinlock_init(&priv->uar_lock[i]);\n#endif\n\t/* Some internal functions rely on Netlink sockets, open them now. */\n\tpriv->nl_socket_rdma = mlx5_nl_init(NETLINK_RDMA);\n\tpriv->nl_socket_route =\tmlx5_nl_init(NETLINK_ROUTE);\n\tpriv->nl_sn = 0;\n\tpriv->representor = !!switch_info->representor;\n\tpriv->master = !!switch_info->master;\n\tpriv->domain_id = RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID;\n\t/*\n\t * Currently we support single E-Switch per PF configurations\n\t * only and vport_id field contains the vport index for\n\t * associated VF, which is deduced from representor port name.\n\t * For example, let's have the IB device port 10, it has\n\t * attached network device eth0, which has port name attribute\n\t * pf0vf2, we can deduce the VF number as 2, and set vport index\n\t * as 3 (2+1). This assigning schema should be changed if the\n\t * multiple E-Switch instances per PF configurations or/and PCI\n\t * subfunctions are added.\n\t */\n\tpriv->vport_id = switch_info->representor ?\n\t\t\t switch_info->port_name + 1 : -1;\n\t/* representor_id field keeps the unmodified port/VF index. */\n\tpriv->representor_id = switch_info->representor ?\n\t\t\t       switch_info->port_name : -1;\n\t/*\n\t * Look for sibling devices in order to reuse their switch domain\n\t * if any, otherwise allocate one.\n\t */\n\tRTE_ETH_FOREACH_DEV_OF(port_id, dpdk_dev) {\n\t\tconst struct mlx5_priv *opriv =\n\t\t\trte_eth_devices[port_id].data->dev_private;\n\n\t\tif (!opriv ||\n\t\t\topriv->domain_id ==\n\t\t\tRTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID)\n\t\t\tcontinue;\n\t\tpriv->domain_id = opriv->domain_id;\n\t\tbreak;\n\t}\n\tif (priv->domain_id == RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID) {\n\t\terr = rte_eth_switch_domain_alloc(&priv->domain_id);\n\t\tif (err) {\n\t\t\terr = rte_errno;\n\t\t\tDRV_LOG(ERR, \"unable to allocate switch domain: %s\",\n\t\t\t\tstrerror(rte_errno));\n\t\t\tgoto error;\n\t\t}\n\t\town_domain_id = 1;\n\t}\n\terr = mlx5_args(&config, dpdk_dev->devargs);\n\tif (err) {\n\t\terr = rte_errno;\n\t\tDRV_LOG(ERR, \"failed to process device arguments: %s\",\n\t\t\tstrerror(rte_errno));\n\t\tgoto error;\n\t}\n\tconfig.hw_csum = !!(sh->device_attr.device_cap_flags_ex &\n\t\t\t    IBV_DEVICE_RAW_IP_CSUM);\n\tDRV_LOG(DEBUG, \"checksum offloading is %ssupported\",\n\t\t(config.hw_csum ? \"\" : \"not \"));\n#if !defined(HAVE_IBV_DEVICE_COUNTERS_SET_V42) && \\\n\t!defined(HAVE_IBV_DEVICE_COUNTERS_SET_V45)\n\tDRV_LOG(DEBUG, \"counters are not supported\");\n#endif\n#ifndef HAVE_IBV_FLOW_DV_SUPPORT\n\tif (config.dv_flow_en) {\n\t\tDRV_LOG(WARNING, \"DV flow is not supported\");\n\t\tconfig.dv_flow_en = 0;\n\t}\n#endif\n\tconfig.ind_table_max_size =\n\t\tsh->device_attr.rss_caps.max_rwq_indirection_table_size;\n\t/*\n\t * Remove this check once DPDK supports larger/variable\n\t * indirection tables.\n\t */\n\tif (config.ind_table_max_size > (unsigned int)ETH_RSS_RETA_SIZE_512)\n\t\tconfig.ind_table_max_size = ETH_RSS_RETA_SIZE_512;\n\tDRV_LOG(DEBUG, \"maximum Rx indirection table size is %u\",\n\t\tconfig.ind_table_max_size);\n\tconfig.hw_vlan_strip = !!(sh->device_attr.raw_packet_caps &\n\t\t\t\t  IBV_RAW_PACKET_CAP_CVLAN_STRIPPING);\n\tDRV_LOG(DEBUG, \"VLAN stripping is %ssupported\",\n\t\t(config.hw_vlan_strip ? \"\" : \"not \"));\n\tconfig.hw_fcs_strip = !!(sh->device_attr.raw_packet_caps &\n\t\t\t\t IBV_RAW_PACKET_CAP_SCATTER_FCS);\n\tDRV_LOG(DEBUG, \"FCS stripping configuration is %ssupported\",\n\t\t(config.hw_fcs_strip ? \"\" : \"not \"));\n#if defined(HAVE_IBV_WQ_FLAG_RX_END_PADDING)\n\thw_padding = !!sh->device_attr.rx_pad_end_addr_align;\n#elif defined(HAVE_IBV_WQ_FLAGS_PCI_WRITE_END_PADDING)\n\thw_padding = !!(sh->device_attr.device_cap_flags_ex &\n\t\t\tIBV_DEVICE_PCI_WRITE_END_PADDING);\n#endif\n\tif (config.hw_padding && !hw_padding) {\n\t\tDRV_LOG(DEBUG, \"Rx end alignment padding isn't supported\");\n\t\tconfig.hw_padding = 0;\n\t} else if (config.hw_padding) {\n\t\tDRV_LOG(DEBUG, \"Rx end alignment padding is enabled\");\n\t}\n\tconfig.tso = (sh->device_attr.tso_caps.max_tso > 0 &&\n\t\t      (sh->device_attr.tso_caps.supported_qpts &\n\t\t       (1 << IBV_QPT_RAW_PACKET)));\n\tif (config.tso)\n\t\tconfig.tso_max_payload_sz = sh->device_attr.tso_caps.max_tso;\n\t/*\n\t * MPW is disabled by default, while the Enhanced MPW is enabled\n\t * by default.\n\t */\n\tif (config.mps == MLX5_ARG_UNSET)\n\t\tconfig.mps = (mps == MLX5_MPW_ENHANCED) ? MLX5_MPW_ENHANCED :\n\t\t\t\t\t\t\t  MLX5_MPW_DISABLED;\n\telse\n\t\tconfig.mps = config.mps ? mps : MLX5_MPW_DISABLED;\n\tDRV_LOG(INFO, \"%sMPS is %s\",\n\t\tconfig.mps == MLX5_MPW_ENHANCED ? \"enhanced \" : \"\",\n\t\tconfig.mps != MLX5_MPW_DISABLED ? \"enabled\" : \"disabled\");\n\tif (config.cqe_comp && !cqe_comp) {\n\t\tDRV_LOG(WARNING, \"Rx CQE compression isn't supported\");\n\t\tconfig.cqe_comp = 0;\n\t}\n\tif (config.cqe_pad && !cqe_pad) {\n\t\tDRV_LOG(WARNING, \"Rx CQE padding isn't supported\");\n\t\tconfig.cqe_pad = 0;\n\t} else if (config.cqe_pad) {\n\t\tDRV_LOG(INFO, \"Rx CQE padding is enabled\");\n\t}\n\tif (config.mprq.enabled && mprq) {\n\t\tif (config.mprq.stride_num_n > mprq_max_stride_num_n ||\n\t\t    config.mprq.stride_num_n < mprq_min_stride_num_n) {\n\t\t\tconfig.mprq.stride_num_n =\n\t\t\t\tRTE_MAX(MLX5_MPRQ_STRIDE_NUM_N,\n\t\t\t\t\tmprq_min_stride_num_n);\n\t\t\tDRV_LOG(WARNING,\n\t\t\t\t\"the number of strides\"\n\t\t\t\t\" for Multi-Packet RQ is out of range,\"\n\t\t\t\t\" setting default value (%u)\",\n\t\t\t\t1 << config.mprq.stride_num_n);\n\t\t}\n\t\tconfig.mprq.min_stride_size_n = mprq_min_stride_size_n;\n\t\tconfig.mprq.max_stride_size_n = mprq_max_stride_size_n;\n\t} else if (config.mprq.enabled && !mprq) {\n\t\tDRV_LOG(WARNING, \"Multi-Packet RQ isn't supported\");\n\t\tconfig.mprq.enabled = 0;\n\t}\n\teth_dev = rte_eth_dev_allocate(name);\n\tif (eth_dev == NULL) {\n\t\tDRV_LOG(ERR, \"can not allocate rte ethdev\");\n\t\terr = ENOMEM;\n\t\tgoto error;\n\t}\n\t/* Flag to call rte_eth_dev_release_port() in rte_eth_dev_close(). */\n\teth_dev->data->dev_flags |= RTE_ETH_DEV_CLOSE_REMOVE;\n\tif (priv->representor) {\n\t\teth_dev->data->dev_flags |= RTE_ETH_DEV_REPRESENTOR;\n\t\teth_dev->data->representor_id = priv->representor_id;\n\t}\n\teth_dev->data->dev_private = priv;\n\tpriv->dev_data = eth_dev->data;\n\teth_dev->data->mac_addrs = priv->mac;\n\teth_dev->device = dpdk_dev;\n\t/* Configure the first MAC address by default. */\n\tif (mlx5_get_mac(eth_dev, &mac.addr_bytes)) {\n\t\tDRV_LOG(ERR,\n\t\t\t\"port %u cannot get MAC address, is mlx5_en\"\n\t\t\t\" loaded? (errno: %s)\",\n\t\t\teth_dev->data->port_id, strerror(rte_errno));\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tDRV_LOG(INFO,\n\t\t\"port %u MAC address is %02x:%02x:%02x:%02x:%02x:%02x\",\n\t\teth_dev->data->port_id,\n\t\tmac.addr_bytes[0], mac.addr_bytes[1],\n\t\tmac.addr_bytes[2], mac.addr_bytes[3],\n\t\tmac.addr_bytes[4], mac.addr_bytes[5]);\n#ifndef NDEBUG\n\t{\n\t\tchar ifname[IF_NAMESIZE];\n\n\t\tif (mlx5_get_ifname(eth_dev, &ifname) == 0)\n\t\t\tDRV_LOG(DEBUG, \"port %u ifname is \\\"%s\\\"\",\n\t\t\t\teth_dev->data->port_id, ifname);\n\t\telse\n\t\t\tDRV_LOG(DEBUG, \"port %u ifname is unknown\",\n\t\t\t\teth_dev->data->port_id);\n\t}\n#endif\n\t/* Get actual MTU if possible. */\n\terr = mlx5_get_mtu(eth_dev, &priv->mtu);\n\tif (err) {\n\t\terr = rte_errno;\n\t\tgoto error;\n\t}\n\tDRV_LOG(DEBUG, \"port %u MTU is %u\", eth_dev->data->port_id,\n\t\tpriv->mtu);\n\t/* Initialize burst functions to prevent crashes before link-up. */\n\teth_dev->rx_pkt_burst = removed_rx_burst;\n\teth_dev->tx_pkt_burst = removed_tx_burst;\n\teth_dev->dev_ops = &mlx5_dev_ops;\n\t/* Register MAC address. */\n\tclaim_zero(mlx5_mac_addr_add(eth_dev, &mac, 0, 0));\n\tif (config.vf && config.vf_nl_en)\n\t\tmlx5_nl_mac_addr_sync(eth_dev);\n\tpriv->tcf_context = mlx5_flow_tcf_context_create();\n\tif (!priv->tcf_context) {\n\t\terr = -rte_errno;\n\t\tDRV_LOG(WARNING,\n\t\t\t\"flow rules relying on switch offloads will not be\"\n\t\t\t\" supported: cannot open libmnl socket: %s\",\n\t\t\tstrerror(rte_errno));\n\t} else {\n\t\tstruct rte_flow_error error;\n\t\tunsigned int ifindex = mlx5_ifindex(eth_dev);\n\n\t\tif (!ifindex) {\n\t\t\terr = -rte_errno;\n\t\t\terror.message =\n\t\t\t\t\"cannot retrieve network interface index\";\n\t\t} else {\n\t\t\terr = mlx5_flow_tcf_init(priv->tcf_context,\n\t\t\t\t\t\t ifindex, &error);\n\t\t}\n\t\tif (err) {\n\t\t\tDRV_LOG(WARNING,\n\t\t\t\t\"flow rules relying on switch offloads will\"\n\t\t\t\t\" not be supported: %s: %s\",\n\t\t\t\terror.message, strerror(rte_errno));\n\t\t\tmlx5_flow_tcf_context_destroy(priv->tcf_context);\n\t\t\tpriv->tcf_context = NULL;\n\t\t}\n\t}\n\tTAILQ_INIT(&priv->flows);\n\tTAILQ_INIT(&priv->ctrl_flows);\n\t/* Hint libmlx5 to use PMD allocator for data plane resources */\n\tstruct mlx5dv_ctx_allocators alctr = {\n\t\t.alloc = &mlx5_alloc_verbs_buf,\n\t\t.free = &mlx5_free_verbs_buf,\n\t\t.data = priv,\n\t};\n\tmlx5_glue->dv_set_context_attr(sh->ctx,\n\t\t\t\t       MLX5DV_CTX_ATTR_BUF_ALLOCATORS,\n\t\t\t\t       (void *)((uintptr_t)&alctr));\n\t/* Bring Ethernet device up. */\n\tDRV_LOG(DEBUG, \"port %u forcing Ethernet interface up\",\n\t\teth_dev->data->port_id);\n\tmlx5_set_link_up(eth_dev);\n\t/*\n\t * Even though the interrupt handler is not installed yet,\n\t * interrupts will still trigger on the async_fd from\n\t * Verbs context returned by ibv_open_device().\n\t */\n\tmlx5_link_update(eth_dev, 0);\n#ifdef HAVE_IBV_DEVX_OBJ\n\tif (config.devx) {\n\t\terr = mlx5_devx_cmd_query_hca_attr(sh->ctx, &config.hca_attr);\n\t\tif (err) {\n\t\t\terr = -err;\n\t\t\tgoto error;\n\t\t}\n\t}\n#endif\n#ifdef HAVE_MLX5DV_DR_ESWITCH\n\tif (!(config.hca_attr.eswitch_manager && config.dv_flow_en &&\n\t      (switch_info->representor || switch_info->master)))\n\t\tconfig.dv_esw_en = 0;\n#else\n\tconfig.dv_esw_en = 0;\n#endif\n\t/* Store device configuration on private structure. */\n\tpriv->config = config;\n\tif (config.dv_flow_en) {\n\t\terr = mlx5_alloc_shared_dr(priv);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\t/* Supported Verbs flow priority number detection. */\n\terr = mlx5_flow_discover_priorities(eth_dev);\n\tif (err < 0) {\n\t\terr = -err;\n\t\tgoto error;\n\t}\n\tpriv->config.flow_prio = err;\n\t/* Add device to memory callback list. */\n\trte_rwlock_write_lock(&mlx5_shared_data->mem_event_rwlock);\n\tLIST_INSERT_HEAD(&mlx5_shared_data->mem_event_cb_list,\n\t\t\t sh, mem_event_cb);\n\trte_rwlock_write_unlock(&mlx5_shared_data->mem_event_rwlock);\n\treturn eth_dev;\nerror:\n\tif (priv) {\n\t\tif (priv->sh)\n\t\t\tmlx5_free_shared_dr(priv);\n\t\tif (priv->nl_socket_route >= 0)\n\t\t\tclose(priv->nl_socket_route);\n\t\tif (priv->nl_socket_rdma >= 0)\n\t\t\tclose(priv->nl_socket_rdma);\n\t\tif (priv->tcf_context)\n\t\t\tmlx5_flow_tcf_context_destroy(priv->tcf_context);\n\t\tif (own_domain_id)\n\t\t\tclaim_zero(rte_eth_switch_domain_free(priv->domain_id));\n\t\trte_free(priv);\n\t\tif (eth_dev != NULL)\n\t\t\teth_dev->data->dev_private = NULL;\n\t}\n\tif (eth_dev != NULL) {\n\t\t/* mac_addrs must not be freed alone because part of dev_private */\n\t\teth_dev->data->mac_addrs = NULL;\n\t\trte_eth_dev_release_port(eth_dev);\n\t}\n\tif (sh)\n\t\tmlx5_free_shared_ibctx(sh);\n\tassert(err > 0);\n\trte_errno = err;\n\treturn NULL;\n}\n\n/**\n * Comparison callback to sort device data.\n *\n * This is meant to be used with qsort().\n *\n * @param a[in]\n *   Pointer to pointer to first data object.\n * @param b[in]\n *   Pointer to pointer to second data object.\n *\n * @return\n *   0 if both objects are equal, less than 0 if the first argument is less\n *   than the second, greater than 0 otherwise.\n */\nstatic int\nmlx5_dev_spawn_data_cmp(const void *a, const void *b)\n{\n\tconst struct mlx5_switch_info *si_a =\n\t\t&((const struct mlx5_dev_spawn_data *)a)->info;\n\tconst struct mlx5_switch_info *si_b =\n\t\t&((const struct mlx5_dev_spawn_data *)b)->info;\n\tint ret;\n\n\t/* Master device first. */\n\tret = si_b->master - si_a->master;\n\tif (ret)\n\t\treturn ret;\n\t/* Then representor devices. */\n\tret = si_b->representor - si_a->representor;\n\tif (ret)\n\t\treturn ret;\n\t/* Unidentified devices come last in no specific order. */\n\tif (!si_a->representor)\n\t\treturn 0;\n\t/* Order representors by name. */\n\treturn si_a->port_name - si_b->port_name;\n}\n\n/**\n * DPDK callback to register a PCI device.\n *\n * This function spawns Ethernet devices out of a given PCI device.\n *\n * @param[in] pci_drv\n *   PCI driver structure (mlx5_driver).\n * @param[in] pci_dev\n *   PCI device information.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,\n\t       struct rte_pci_device *pci_dev)\n{\n\tstruct ibv_device **ibv_list;\n\t/*\n\t * Number of found IB Devices matching with requested PCI BDF.\n\t * nd != 1 means there are multiple IB devices over the same\n\t * PCI device and we have representors and master.\n\t */\n\tunsigned int nd = 0;\n\t/*\n\t * Number of found IB device Ports. nd = 1 and np = 1..n means\n\t * we have the single multiport IB device, and there may be\n\t * representors attached to some of found ports.\n\t */\n\tunsigned int np = 0;\n\t/*\n\t * Number of DPDK ethernet devices to Spawn - either over\n\t * multiple IB devices or multiple ports of single IB device.\n\t * Actually this is the number of iterations to spawn.\n\t */\n\tunsigned int ns = 0;\n\tstruct mlx5_dev_config dev_config;\n\tint ret;\n\n\tret = mlx5_init_once();\n\tif (ret) {\n\t\tDRV_LOG(ERR, \"unable to init PMD global data: %s\",\n\t\t\tstrerror(rte_errno));\n\t\treturn -rte_errno;\n\t}\n\tassert(pci_drv == &mlx5_driver);\n\terrno = 0;\n\tibv_list = mlx5_glue->get_device_list(&ret);\n\tif (!ibv_list) {\n\t\trte_errno = errno ? errno : ENOSYS;\n\t\tDRV_LOG(ERR, \"cannot list devices, is ib_uverbs loaded?\");\n\t\treturn -rte_errno;\n\t}\n\t/*\n\t * First scan the list of all Infiniband devices to find\n\t * matching ones, gathering into the list.\n\t */\n\tstruct ibv_device *ibv_match[ret + 1];\n\tint nl_route = -1;\n\tint nl_rdma = -1;\n\tunsigned int i;\n\n\twhile (ret-- > 0) {\n\t\tstruct rte_pci_addr pci_addr;\n\n\t\tDRV_LOG(DEBUG, \"checking device \\\"%s\\\"\", ibv_list[ret]->name);\n\t\tif (mlx5_ibv_device_to_pci_addr(ibv_list[ret], &pci_addr))\n\t\t\tcontinue;\n\t\tif (pci_dev->addr.domain != pci_addr.domain ||\n\t\t    pci_dev->addr.bus != pci_addr.bus ||\n\t\t    pci_dev->addr.devid != pci_addr.devid ||\n\t\t    pci_dev->addr.function != pci_addr.function)\n\t\t\tcontinue;\n\t\tDRV_LOG(INFO, \"PCI information matches for device \\\"%s\\\"\",\n\t\t\tibv_list[ret]->name);\n\t\tibv_match[nd++] = ibv_list[ret];\n\t}\n\tibv_match[nd] = NULL;\n\tif (!nd) {\n\t\t/* No device matches, just complain and bail out. */\n\t\tmlx5_glue->free_device_list(ibv_list);\n\t\tDRV_LOG(WARNING,\n\t\t\t\"no Verbs device matches PCI device \" PCI_PRI_FMT \",\"\n\t\t\t\" are kernel drivers loaded?\",\n\t\t\tpci_dev->addr.domain, pci_dev->addr.bus,\n\t\t\tpci_dev->addr.devid, pci_dev->addr.function);\n\t\trte_errno = ENOENT;\n\t\tret = -rte_errno;\n\t\treturn ret;\n\t}\n\tnl_route = mlx5_nl_init(NETLINK_ROUTE);\n\tnl_rdma = mlx5_nl_init(NETLINK_RDMA);\n\tif (nd == 1) {\n\t\t/*\n\t\t * Found single matching device may have multiple ports.\n\t\t * Each port may be representor, we have to check the port\n\t\t * number and check the representors existence.\n\t\t */\n\t\tif (nl_rdma >= 0)\n\t\t\tnp = mlx5_nl_portnum(nl_rdma, ibv_match[0]->name);\n\t\tif (!np)\n\t\t\tDRV_LOG(WARNING, \"can not get IB device \\\"%s\\\"\"\n\t\t\t\t\t \" ports number\", ibv_match[0]->name);\n\t}\n\t/*\n\t * Now we can determine the maximal\n\t * amount of devices to be spawned.\n\t */\n\tstruct mlx5_dev_spawn_data list[np ? np : nd];\n\n\tif (np > 1) {\n\t\t/*\n\t\t * Single IB device with multiple ports found,\n\t\t * it may be E-Switch master device and representors.\n\t\t * We have to perform identification trough the ports.\n\t\t */\n\t\tassert(nl_rdma >= 0);\n\t\tassert(ns == 0);\n\t\tassert(nd == 1);\n\t\tfor (i = 1; i <= np; ++i) {\n\t\t\tlist[ns].max_port = np;\n\t\t\tlist[ns].ibv_port = i;\n\t\t\tlist[ns].ibv_dev = ibv_match[0];\n\t\t\tlist[ns].eth_dev = NULL;\n\t\t\tlist[ns].pci_dev = pci_dev;\n\t\t\tlist[ns].ifindex = mlx5_nl_ifindex\n\t\t\t\t\t(nl_rdma, list[ns].ibv_dev->name, i);\n\t\t\tif (!list[ns].ifindex) {\n\t\t\t\t/*\n\t\t\t\t * No network interface index found for the\n\t\t\t\t * specified port, it means there is no\n\t\t\t\t * representor on this port. It's OK,\n\t\t\t\t * there can be disabled ports, for example\n\t\t\t\t * if sriov_numvfs < sriov_totalvfs.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tret = -1;\n\t\t\tif (nl_route >= 0)\n\t\t\t\tret = mlx5_nl_switch_info\n\t\t\t\t\t       (nl_route,\n\t\t\t\t\t\tlist[ns].ifindex,\n\t\t\t\t\t\t&list[ns].info);\n\t\t\tif (ret || (!list[ns].info.representor &&\n\t\t\t\t    !list[ns].info.master)) {\n\t\t\t\t/*\n\t\t\t\t * We failed to recognize representors with\n\t\t\t\t * Netlink, let's try to perform the task\n\t\t\t\t * with sysfs.\n\t\t\t\t */\n\t\t\t\tret =  mlx5_sysfs_switch_info\n\t\t\t\t\t\t(list[ns].ifindex,\n\t\t\t\t\t\t &list[ns].info);\n\t\t\t}\n\t\t\tif (!ret && (list[ns].info.representor ^\n\t\t\t\t     list[ns].info.master))\n\t\t\t\tns++;\n\t\t}\n\t\tif (!ns) {\n\t\t\tDRV_LOG(ERR,\n\t\t\t\t\"unable to recognize master/representors\"\n\t\t\t\t\" on the IB device with multiple ports\");\n\t\t\trte_errno = ENOENT;\n\t\t\tret = -rte_errno;\n\t\t\tgoto exit;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * The existence of several matching entries (nd > 1) means\n\t\t * port representors have been instantiated. No existing Verbs\n\t\t * call nor sysfs entries can tell them apart, this can only\n\t\t * be done through Netlink calls assuming kernel drivers are\n\t\t * recent enough to support them.\n\t\t *\n\t\t * In the event of identification failure through Netlink,\n\t\t * try again through sysfs, then:\n\t\t *\n\t\t * 1. A single IB device matches (nd == 1) with single\n\t\t *    port (np=0/1) and is not a representor, assume\n\t\t *    no switch support.\n\t\t *\n\t\t * 2. Otherwise no safe assumptions can be made;\n\t\t *    complain louder and bail out.\n\t\t */\n\t\tnp = 1;\n\t\tfor (i = 0; i != nd; ++i) {\n\t\t\tmemset(&list[ns].info, 0, sizeof(list[ns].info));\n\t\t\tlist[ns].max_port = 1;\n\t\t\tlist[ns].ibv_port = 1;\n\t\t\tlist[ns].ibv_dev = ibv_match[i];\n\t\t\tlist[ns].eth_dev = NULL;\n\t\t\tlist[ns].pci_dev = pci_dev;\n\t\t\tlist[ns].ifindex = 0;\n\t\t\tif (nl_rdma >= 0)\n\t\t\t\tlist[ns].ifindex = mlx5_nl_ifindex\n\t\t\t\t\t(nl_rdma, list[ns].ibv_dev->name, 1);\n\t\t\tif (!list[ns].ifindex) {\n\t\t\t\tchar ifname[IF_NAMESIZE];\n\n\t\t\t\t/*\n\t\t\t\t * Netlink failed, it may happen with old\n\t\t\t\t * ib_core kernel driver (before 4.16).\n\t\t\t\t * We can assume there is old driver because\n\t\t\t\t * here we are processing single ports IB\n\t\t\t\t * devices. Let's try sysfs to retrieve\n\t\t\t\t * the ifindex. The method works for\n\t\t\t\t * master device only.\n\t\t\t\t */\n\t\t\t\tif (nd > 1) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Multiple devices found, assume\n\t\t\t\t\t * representors, can not distinguish\n\t\t\t\t\t * master/representor and retrieve\n\t\t\t\t\t * ifindex via sysfs.\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tret = mlx5_get_master_ifname\n\t\t\t\t\t(ibv_match[i]->ibdev_path, &ifname);\n\t\t\t\tif (!ret)\n\t\t\t\t\tlist[ns].ifindex =\n\t\t\t\t\t\tif_nametoindex(ifname);\n\t\t\t\tif (!list[ns].ifindex) {\n\t\t\t\t\t/*\n\t\t\t\t\t * No network interface index found\n\t\t\t\t\t * for the specified device, it means\n\t\t\t\t\t * there it is neither representor\n\t\t\t\t\t * nor master.\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tret = -1;\n\t\t\tif (nl_route >= 0)\n\t\t\t\tret = mlx5_nl_switch_info\n\t\t\t\t\t       (nl_route,\n\t\t\t\t\t\tlist[ns].ifindex,\n\t\t\t\t\t\t&list[ns].info);\n\t\t\tif (ret || (!list[ns].info.representor &&\n\t\t\t\t    !list[ns].info.master)) {\n\t\t\t\t/*\n\t\t\t\t * We failed to recognize representors with\n\t\t\t\t * Netlink, let's try to perform the task\n\t\t\t\t * with sysfs.\n\t\t\t\t */\n\t\t\t\tret =  mlx5_sysfs_switch_info\n\t\t\t\t\t\t(list[ns].ifindex,\n\t\t\t\t\t\t &list[ns].info);\n\t\t\t}\n\t\t\tif (!ret && (list[ns].info.representor ^\n\t\t\t\t     list[ns].info.master)) {\n\t\t\t\tns++;\n\t\t\t} else if ((nd == 1) &&\n\t\t\t\t   !list[ns].info.representor &&\n\t\t\t\t   !list[ns].info.master) {\n\t\t\t\t/*\n\t\t\t\t * Single IB device with\n\t\t\t\t * one physical port and\n\t\t\t\t * attached network device.\n\t\t\t\t * May be SRIOV is not enabled\n\t\t\t\t * or there is no representors.\n\t\t\t\t */\n\t\t\t\tDRV_LOG(INFO, \"no E-Switch support detected\");\n\t\t\t\tns++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!ns) {\n\t\t\tDRV_LOG(ERR,\n\t\t\t\t\"unable to recognize master/representors\"\n\t\t\t\t\" on the multiple IB devices\");\n\t\t\trte_errno = ENOENT;\n\t\t\tret = -rte_errno;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\tassert(ns);\n\t/*\n\t * Sort list to probe devices in natural order for users convenience\n\t * (i.e. master first, then representors from lowest to highest ID).\n\t */\n\tqsort(list, ns, sizeof(*list), mlx5_dev_spawn_data_cmp);\n\t/* Default configuration. */\n\tdev_config = (struct mlx5_dev_config){\n\t\t.hw_padding = 0,\n\t\t.mps = MLX5_ARG_UNSET,\n\t\t.tx_vec_en = 1,\n\t\t.rx_vec_en = 1,\n\t\t.txq_inline = MLX5_ARG_UNSET,\n\t\t.txqs_inline = MLX5_ARG_UNSET,\n\t\t.txqs_vec = MLX5_ARG_UNSET,\n\t\t.inline_max_packet_sz = MLX5_ARG_UNSET,\n\t\t.vf_nl_en = 1,\n\t\t.mr_ext_memseg_en = 1,\n\t\t.mprq = {\n\t\t\t.enabled = 0, /* Disabled by default. */\n\t\t\t.stride_num_n = MLX5_MPRQ_STRIDE_NUM_N,\n\t\t\t.max_memcpy_len = MLX5_MPRQ_MEMCPY_DEFAULT_LEN,\n\t\t\t.min_rxqs_num = MLX5_MPRQ_MIN_RXQS,\n\t\t},\n\t\t.dv_esw_en = 1,\n\t};\n\t/* Device specific configuration. */\n\tswitch (pci_dev->id.device_id) {\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX5BF:\n\t\tdev_config.txqs_vec = MLX5_VPMD_MAX_TXQS_BLUEFIELD;\n\t\tbreak;\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX4VF:\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX4LXVF:\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX5VF:\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX5EXVF:\n\t\tdev_config.vf = 1;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\t/* Set architecture-dependent default value if unset. */\n\tif (dev_config.txqs_vec == MLX5_ARG_UNSET)\n\t\tdev_config.txqs_vec = MLX5_VPMD_MAX_TXQS;\n\tfor (i = 0; i != ns; ++i) {\n\t\tuint32_t restore;\n\n\t\tlist[i].eth_dev = mlx5_dev_spawn(&pci_dev->device,\n\t\t\t\t\t\t &list[i],\n\t\t\t\t\t\t dev_config);\n\t\tif (!list[i].eth_dev) {\n\t\t\tif (rte_errno != EBUSY && rte_errno != EEXIST)\n\t\t\t\tbreak;\n\t\t\t/* Device is disabled or already spawned. Ignore it. */\n\t\t\tcontinue;\n\t\t}\n\t\trestore = list[i].eth_dev->data->dev_flags;\n\t\trte_eth_copy_pci_info(list[i].eth_dev, pci_dev);\n\t\t/* Restore non-PCI flags cleared by the above call. */\n\t\tlist[i].eth_dev->data->dev_flags |= restore;\n\t\trte_eth_dev_probing_finish(list[i].eth_dev);\n\t}\n\tif (i != ns) {\n\t\tDRV_LOG(ERR,\n\t\t\t\"probe of PCI device \" PCI_PRI_FMT \" aborted after\"\n\t\t\t\" encountering an error: %s\",\n\t\t\tpci_dev->addr.domain, pci_dev->addr.bus,\n\t\t\tpci_dev->addr.devid, pci_dev->addr.function,\n\t\t\tstrerror(rte_errno));\n\t\tret = -rte_errno;\n\t\t/* Roll back. */\n\t\twhile (i--) {\n\t\t\tif (!list[i].eth_dev)\n\t\t\t\tcontinue;\n\t\t\tmlx5_dev_close(list[i].eth_dev);\n\t\t\t/* mac_addrs must not be freed because in dev_private */\n\t\t\tlist[i].eth_dev->data->mac_addrs = NULL;\n\t\t\tclaim_zero(rte_eth_dev_release_port(list[i].eth_dev));\n\t\t}\n\t\t/* Restore original error. */\n\t\trte_errno = -ret;\n\t} else {\n\t\tret = 0;\n\t}\nexit:\n\t/*\n\t * Do the routine cleanup:\n\t * - close opened Netlink sockets\n\t * - free the Infiniband device list\n\t */\n\tif (nl_rdma >= 0)\n\t\tclose(nl_rdma);\n\tif (nl_route >= 0)\n\t\tclose(nl_route);\n\tassert(ibv_list);\n\tmlx5_glue->free_device_list(ibv_list);\n\treturn ret;\n}\n\n/**\n * DPDK callback to remove a PCI device.\n *\n * This function removes all Ethernet devices belong to a given PCI device.\n *\n * @param[in] pci_dev\n *   Pointer to the PCI device.\n *\n * @return\n *   0 on success, the function cannot fail.\n */\nstatic int\nmlx5_pci_remove(struct rte_pci_device *pci_dev)\n{\n\tuint16_t port_id;\n\n\tRTE_ETH_FOREACH_DEV_OF(port_id, &pci_dev->device)\n\t\trte_eth_dev_close(port_id);\n\treturn 0;\n}\n\nstatic const struct rte_pci_id mlx5_pci_id_map[] = {\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4VF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4LX)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4LXVF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5VF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5EX)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5EXVF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5BF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5BFVF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t\tPCI_DEVICE_ID_MELLANOX_CONNECTX6)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t\tPCI_DEVICE_ID_MELLANOX_CONNECTX6VF)\n\t},\n\t{\n\t\t.vendor_id = 0\n\t}\n};\n\nstatic struct rte_pci_driver mlx5_driver = {\n\t.driver = {\n\t\t.name = MLX5_DRIVER_NAME\n\t},\n\t.id_table = mlx5_pci_id_map,\n\t.probe = mlx5_pci_probe,\n\t.remove = mlx5_pci_remove,\n\t.dma_map = mlx5_dma_map,\n\t.dma_unmap = mlx5_dma_unmap,\n\t.drv_flags = (RTE_PCI_DRV_INTR_LSC | RTE_PCI_DRV_INTR_RMV |\n\t\t      RTE_PCI_DRV_PROBE_AGAIN),\n};\n\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\n/**\n * Suffix RTE_EAL_PMD_PATH with \"-glue\".\n *\n * This function performs a sanity check on RTE_EAL_PMD_PATH before\n * suffixing its last component.\n *\n * @param buf[out]\n *   Output buffer, should be large enough otherwise NULL is returned.\n * @param size\n *   Size of @p out.\n *\n * @return\n *   Pointer to @p buf or @p NULL in case suffix cannot be appended.\n */\nstatic char *\nmlx5_glue_path(char *buf, size_t size)\n{\n\tstatic const char *const bad[] = { \"/\", \".\", \"..\", NULL };\n\tconst char *path = RTE_EAL_PMD_PATH;\n\tsize_t len = strlen(path);\n\tsize_t off;\n\tint i;\n\n\twhile (len && path[len - 1] == '/')\n\t\t--len;\n\tfor (off = len; off && path[off - 1] != '/'; --off)\n\t\t;\n\tfor (i = 0; bad[i]; ++i)\n\t\tif (!strncmp(path + off, bad[i], (int)(len - off)))\n\t\t\tgoto error;\n\ti = snprintf(buf, size, \"%.*s-glue\", (int)len, path);\n\tif (i == -1 || (size_t)i >= size)\n\t\tgoto error;\n\treturn buf;\nerror:\n\tDRV_LOG(ERR,\n\t\t\"unable to append \\\"-glue\\\" to last component of\"\n\t\t\" RTE_EAL_PMD_PATH (\\\"\" RTE_EAL_PMD_PATH \"\\\"),\"\n\t\t\" please re-configure DPDK\");\n\treturn NULL;\n}\n\n/**\n * Initialization routine for run-time dependency on rdma-core.\n */\nstatic int\nmlx5_glue_init(void)\n{\n\tchar glue_path[sizeof(RTE_EAL_PMD_PATH) - 1 + sizeof(\"-glue\")];\n\tconst char *path[] = {\n\t\t/*\n\t\t * A basic security check is necessary before trusting\n\t\t * MLX5_GLUE_PATH, which may override RTE_EAL_PMD_PATH.\n\t\t */\n\t\t(geteuid() == getuid() && getegid() == getgid() ?\n\t\t getenv(\"MLX5_GLUE_PATH\") : NULL),\n\t\t/*\n\t\t * When RTE_EAL_PMD_PATH is set, use its glue-suffixed\n\t\t * variant, otherwise let dlopen() look up libraries on its\n\t\t * own.\n\t\t */\n\t\t(*RTE_EAL_PMD_PATH ?\n\t\t mlx5_glue_path(glue_path, sizeof(glue_path)) : \"\"),\n\t};\n\tunsigned int i = 0;\n\tvoid *handle = NULL;\n\tvoid **sym;\n\tconst char *dlmsg;\n\n\twhile (!handle && i != RTE_DIM(path)) {\n\t\tconst char *end;\n\t\tsize_t len;\n\t\tint ret;\n\n\t\tif (!path[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\tend = strpbrk(path[i], \":;\");\n\t\tif (!end)\n\t\t\tend = path[i] + strlen(path[i]);\n\t\tlen = end - path[i];\n\t\tret = 0;\n\t\tdo {\n\t\t\tchar name[ret + 1];\n\n\t\t\tret = snprintf(name, sizeof(name), \"%.*s%s\" MLX5_GLUE,\n\t\t\t\t       (int)len, path[i],\n\t\t\t\t       (!len || *(end - 1) == '/') ? \"\" : \"/\");\n\t\t\tif (ret == -1)\n\t\t\t\tbreak;\n\t\t\tif (sizeof(name) != (size_t)ret + 1)\n\t\t\t\tcontinue;\n\t\t\tDRV_LOG(DEBUG, \"looking for rdma-core glue as \\\"%s\\\"\",\n\t\t\t\tname);\n\t\t\thandle = dlopen(name, RTLD_LAZY);\n\t\t\tbreak;\n\t\t} while (1);\n\t\tpath[i] = end + 1;\n\t\tif (!*end)\n\t\t\t++i;\n\t}\n\tif (!handle) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tDRV_LOG(WARNING, \"cannot load glue library: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tsym = dlsym(handle, \"mlx5_glue\");\n\tif (!sym || !*sym) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tDRV_LOG(ERR, \"cannot resolve glue symbol: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tmlx5_glue = *sym;\n\treturn 0;\nglue_error:\n\tif (handle)\n\t\tdlclose(handle);\n\tDRV_LOG(WARNING,\n\t\t\"cannot initialize PMD due to missing run-time dependency on\"\n\t\t\" rdma-core libraries (libibverbs, libmlx5)\");\n\treturn -rte_errno;\n}\n\n#endif\n\n/**\n * Driver initialization routine.\n */\nRTE_INIT(rte_mlx5_pmd_init)\n{\n\t/* Initialize driver log type. */\n\tmlx5_logtype = rte_log_register(\"pmd.net.mlx5\");\n\tif (mlx5_logtype >= 0)\n\t\trte_log_set_level(mlx5_logtype, RTE_LOG_NOTICE);\n\n\t/* Build the static tables for Verbs conversion. */\n\tmlx5_set_ptype_table();\n\tmlx5_set_cksum_table();\n\tmlx5_set_swp_types_table();\n\t/*\n\t * RDMAV_HUGEPAGES_SAFE tells ibv_fork_init() we intend to use\n\t * huge pages. Calling ibv_fork_init() during init allows\n\t * applications to use fork() safely for purposes other than\n\t * using this PMD, which is not supported in forked processes.\n\t */\n\tsetenv(\"RDMAV_HUGEPAGES_SAFE\", \"1\", 1);\n\t/* Match the size of Rx completion entry to the size of a cacheline. */\n\tif (RTE_CACHE_LINE_SIZE == 128)\n\t\tsetenv(\"MLX5_CQE_SIZE\", \"128\", 0);\n\t/*\n\t * MLX5_DEVICE_FATAL_CLEANUP tells ibv_destroy functions to\n\t * cleanup all the Verbs resources even when the device was removed.\n\t */\n\tsetenv(\"MLX5_DEVICE_FATAL_CLEANUP\", \"1\", 1);\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\tif (mlx5_glue_init())\n\t\treturn;\n\tassert(mlx5_glue);\n#endif\n#ifndef NDEBUG\n\t/* Glue structure must not contain any NULL pointers. */\n\t{\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i != sizeof(*mlx5_glue) / sizeof(void *); ++i)\n\t\t\tassert(((const void *const *)mlx5_glue)[i]);\n\t}\n#endif\n\tif (strcmp(mlx5_glue->version, MLX5_GLUE_VERSION)) {\n\t\tDRV_LOG(ERR,\n\t\t\t\"rdma-core glue \\\"%s\\\" mismatch: \\\"%s\\\" is required\",\n\t\t\tmlx5_glue->version, MLX5_GLUE_VERSION);\n\t\treturn;\n\t}\n\tmlx5_glue->fork_init();\n\trte_pci_register(&mlx5_driver);\n}\n\nRTE_PMD_EXPORT_NAME(net_mlx5, __COUNTER__);\nRTE_PMD_REGISTER_PCI_TABLE(net_mlx5, mlx5_pci_id_map);\nRTE_PMD_REGISTER_KMOD_DEP(net_mlx5, \"* ib_uverbs & mlx5_core & mlx5_ib\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/examples/performance-thread/pthread_shim/pthread_shim.h": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2015 Intel Corporation\n */\n\n#ifndef _PTHREAD_SHIM_H_\n#define _PTHREAD_SHIM_H_\n\n#include <rte_lcore.h>\n\n/*\n * This pthread shim is an example that demonstrates how legacy code\n * that makes use of POSIX pthread services can make use of lthreads\n * with reduced porting effort.\n *\n * N.B. The example is not a complete implementation, only a subset of\n * pthread APIs sufficient to demonstrate the principle of operation\n * are implemented.\n *\n * In general pthread attribute objects do not have equivalent functions\n * in lthreads, and are ignored.\n *\n * There is one exception and that is the use of attr to specify a\n * core affinity in calls to pthread_create.\n *\n * The shim operates as follows:-\n *\n * On initialisation a constructor function uses dlsym to obtain and\n * save the loaded address of the full set of pthread APIs that will\n * be overridden.\n *\n * For each function there is a stub provided that will invoke either\n * the genuine pthread library function saved saved by the constructor,\n * or else the corresponding equivalent lthread function.\n *\n * The stub functions are implemented in pthread_shim.c\n *\n * The stub will take care of adapting parameters, and any police\n * any constraints where lthread functionality differs.\n *\n * The initial thread must always be a pure lthread.\n *\n * The decision whether to invoke the real library function or the lthread\n * function is controlled by a per pthread flag that can be switched\n * on of off by the pthread_override_set() API described below. Typcially\n * this should be done as the first action of the initial lthread.\n *\n * N.B In general it would be poor practice to revert to invoke a real\n * pthread function when running as an lthread, since these may block and\n * effectively stall the lthread scheduler.\n *\n */\n\n\n/*\n * An exiting lthread must not terminate the pthread it is running in\n * since this would mean terminating the lthread scheduler.\n * We override pthread_exit() with a macro because it is typically declared with\n * __attribute__((noreturn))\n */\nvoid pthread_exit_override(void *v);\n\n#define pthread_exit(v) do { \\\n\tpthread_exit_override((v));\t\\\n\treturn NULL;\t\\\n} while (0)\n\n/*\n * Enable/Disable pthread override\n * state\n * 0 disable\n * 1 enable\n */\nvoid pthread_override_set(int state);\n\n\n/*\n * Return pthread override state\n * return\n * 0 disable\n * 1 enable\n */\nint pthread_override_get(void);\n\n\n#endif /* _PTHREAD_SHIM_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/examples/performance-thread/pthread_shim/pthread_shim.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2015 Intel Corporation\n */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <errno.h>\n#include <sched.h>\n#include <dlfcn.h>\n\n#include <rte_log.h>\n\n#include \"lthread_api.h\"\n#include \"pthread_shim.h\"\n\n#define RTE_LOGTYPE_PTHREAD_SHIM RTE_LOGTYPE_USER3\n\n#define POSIX_ERRNO(x)  (x)\n\n/* some releases of FreeBSD 10, e.g. 10.0, don't have CPU_COUNT macro */\n#ifndef CPU_COUNT\n#define CPU_COUNT(x) __cpu_count(x)\n\nstatic inline unsigned int\n__cpu_count(const rte_cpuset_t *cpuset)\n{\n\tunsigned int i, count = 0;\n\tfor (i = 0; i < RTE_MAX_LCORE; i++)\n\t\tif (CPU_ISSET(i, cpuset))\n\t\t\tcount++;\n\treturn count;\n}\n#endif\n\n/*\n * this flag determines at run time if we override pthread\n * calls and map then to equivalent lthread calls\n * or of we call the standard pthread function\n */\nstatic __thread int override;\n\n\n/*\n * this structures contains function pointers that will be\n * initialised to the loaded address of the real\n * pthread library API functions\n */\nstruct pthread_lib_funcs {\nint (*f_pthread_barrier_destroy)\n\t(pthread_barrier_t *);\nint (*f_pthread_barrier_init)\n\t(pthread_barrier_t *, const pthread_barrierattr_t *, unsigned);\nint (*f_pthread_barrier_wait)\n\t(pthread_barrier_t *);\nint (*f_pthread_cond_broadcast)\n\t(pthread_cond_t *);\nint (*f_pthread_cond_destroy)\n\t(pthread_cond_t *);\nint (*f_pthread_cond_init)\n\t(pthread_cond_t *, const pthread_condattr_t *);\nint (*f_pthread_cond_signal)\n\t(pthread_cond_t *);\nint (*f_pthread_cond_timedwait)\n\t(pthread_cond_t *, pthread_mutex_t *, const struct timespec *);\nint (*f_pthread_cond_wait)\n\t(pthread_cond_t *, pthread_mutex_t *);\nint (*f_pthread_create)\n\t(pthread_t *, const pthread_attr_t *, void *(*)(void *), void *);\nint (*f_pthread_detach)\n\t(pthread_t);\nint (*f_pthread_equal)\n\t(pthread_t, pthread_t);\nvoid (*f_pthread_exit)\n\t(void *);\nvoid * (*f_pthread_getspecific)\n\t(pthread_key_t);\nint (*f_pthread_getcpuclockid)\n\t(pthread_t, clockid_t *);\nint (*f_pthread_join)\n\t(pthread_t, void **);\nint (*f_pthread_key_create)\n\t(pthread_key_t *, void (*) (void *));\nint (*f_pthread_key_delete)\n\t(pthread_key_t);\nint (*f_pthread_mutex_destroy)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_mutex_init)\n\t(pthread_mutex_t *__mutex, const pthread_mutexattr_t *);\nint (*f_pthread_mutex_lock)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_mutex_trylock)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_mutex_timedlock)\n\t(pthread_mutex_t *__mutex, const struct timespec *);\nint (*f_pthread_mutex_unlock)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_once)\n\t(pthread_once_t *, void (*) (void));\nint (*f_pthread_rwlock_destroy)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_init)\n\t(pthread_rwlock_t *__rwlock, const pthread_rwlockattr_t *);\nint (*f_pthread_rwlock_rdlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_timedrdlock)\n\t(pthread_rwlock_t *__rwlock, const struct timespec *);\nint (*f_pthread_rwlock_timedwrlock)\n\t(pthread_rwlock_t *__rwlock, const struct timespec *);\nint (*f_pthread_rwlock_tryrdlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_trywrlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_unlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_wrlock)\n\t(pthread_rwlock_t *__rwlock);\npthread_t (*f_pthread_self)\n\t(void);\nint (*f_pthread_setspecific)\n\t(pthread_key_t, const void *);\nint (*f_pthread_spin_init)\n\t(pthread_spinlock_t *__spin, int);\nint (*f_pthread_spin_destroy)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_spin_lock)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_spin_trylock)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_spin_unlock)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_cancel)\n\t(pthread_t);\nint (*f_pthread_setcancelstate)\n\t(int, int *);\nint (*f_pthread_setcanceltype)\n\t(int, int *);\nvoid (*f_pthread_testcancel)\n\t(void);\nint (*f_pthread_getschedparam)\n\t(pthread_t pthread, int *, struct sched_param *);\nint (*f_pthread_setschedparam)\n\t(pthread_t, int, const struct sched_param *);\nint (*f_pthread_yield)\n\t(void);\nint (*f_pthread_setaffinity_np)\n\t(pthread_t thread, size_t cpusetsize, const rte_cpuset_t *cpuset);\nint (*f_nanosleep)\n\t(const struct timespec *req, struct timespec *rem);\n} _sys_pthread_funcs = {\n\t.f_pthread_barrier_destroy = NULL,\n};\n\n\n/*\n * this macro obtains the loaded address of a library function\n * and saves it.\n */\nstatic void *__libc_dl_handle = RTLD_NEXT;\n\n#define get_addr_of_loaded_symbol(name) do {\t\t\t\t\\\n\tchar *error_str;\t\t\t\t\t\t\\\n\t_sys_pthread_funcs.f_##name = dlsym(__libc_dl_handle, (#name));\t\\\n\terror_str = dlerror();\t\t\t\t\t\t\\\n\tif (error_str != NULL) {\t\t\t\t\t\\\n\t\tfprintf(stderr, \"%s\\n\", error_str);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n\n/*\n * The constructor function initialises the\n * function pointers for pthread library functions\n */\nRTE_INIT(pthread_intercept_ctor)\n{\n\toverride = 0;\n\t/*\n\t * Get the original functions\n\t */\n\tget_addr_of_loaded_symbol(pthread_barrier_destroy);\n\tget_addr_of_loaded_symbol(pthread_barrier_init);\n\tget_addr_of_loaded_symbol(pthread_barrier_wait);\n\tget_addr_of_loaded_symbol(pthread_cond_broadcast);\n\tget_addr_of_loaded_symbol(pthread_cond_destroy);\n\tget_addr_of_loaded_symbol(pthread_cond_init);\n\tget_addr_of_loaded_symbol(pthread_cond_signal);\n\tget_addr_of_loaded_symbol(pthread_cond_timedwait);\n\tget_addr_of_loaded_symbol(pthread_cond_wait);\n\tget_addr_of_loaded_symbol(pthread_create);\n\tget_addr_of_loaded_symbol(pthread_detach);\n\tget_addr_of_loaded_symbol(pthread_equal);\n\tget_addr_of_loaded_symbol(pthread_exit);\n\tget_addr_of_loaded_symbol(pthread_getspecific);\n\tget_addr_of_loaded_symbol(pthread_getcpuclockid);\n\tget_addr_of_loaded_symbol(pthread_join);\n\tget_addr_of_loaded_symbol(pthread_key_create);\n\tget_addr_of_loaded_symbol(pthread_key_delete);\n\tget_addr_of_loaded_symbol(pthread_mutex_destroy);\n\tget_addr_of_loaded_symbol(pthread_mutex_init);\n\tget_addr_of_loaded_symbol(pthread_mutex_lock);\n\tget_addr_of_loaded_symbol(pthread_mutex_trylock);\n\tget_addr_of_loaded_symbol(pthread_mutex_timedlock);\n\tget_addr_of_loaded_symbol(pthread_mutex_unlock);\n\tget_addr_of_loaded_symbol(pthread_once);\n\tget_addr_of_loaded_symbol(pthread_rwlock_destroy);\n\tget_addr_of_loaded_symbol(pthread_rwlock_init);\n\tget_addr_of_loaded_symbol(pthread_rwlock_rdlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_timedrdlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_timedwrlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_tryrdlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_trywrlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_unlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_wrlock);\n\tget_addr_of_loaded_symbol(pthread_self);\n\tget_addr_of_loaded_symbol(pthread_setspecific);\n\tget_addr_of_loaded_symbol(pthread_spin_init);\n\tget_addr_of_loaded_symbol(pthread_spin_destroy);\n\tget_addr_of_loaded_symbol(pthread_spin_lock);\n\tget_addr_of_loaded_symbol(pthread_spin_trylock);\n\tget_addr_of_loaded_symbol(pthread_spin_unlock);\n\tget_addr_of_loaded_symbol(pthread_cancel);\n\tget_addr_of_loaded_symbol(pthread_setcancelstate);\n\tget_addr_of_loaded_symbol(pthread_setcanceltype);\n\tget_addr_of_loaded_symbol(pthread_testcancel);\n\tget_addr_of_loaded_symbol(pthread_getschedparam);\n\tget_addr_of_loaded_symbol(pthread_setschedparam);\n\tget_addr_of_loaded_symbol(pthread_yield);\n\tget_addr_of_loaded_symbol(pthread_setaffinity_np);\n\tget_addr_of_loaded_symbol(nanosleep);\n}\n\n\n/*\n * Enable/Disable pthread override\n * state\n *  0 disable\n *  1 enable\n */\nvoid pthread_override_set(int state)\n{\n\toverride = state;\n}\n\n\n/*\n * Return pthread override state\n * return\n *  0 disable\n *  1 enable\n */\nint pthread_override_get(void)\n{\n\treturn override;\n}\n\n/*\n * This macro is used to catch and log\n * invocation of stubs for unimplemented pthread\n * API functions.\n */\n#define NOT_IMPLEMENTED do {\t\t\t\t\\\n\tif (override) {\t\t\t\t\t\\\n\t\tRTE_LOG(WARNING,\t\t\t\\\n\t\t\tPTHREAD_SHIM,\t\t\t\\\n\t\t\t\"WARNING %s NOT IMPLEMENTED\\n\",\t\\\n\t\t\t__func__);\t\t\t\\\n\t}\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * pthread API override functions follow\n * Note in this example code only a subset of functions are\n * implemented.\n *\n * The stub functions provided will issue a warning log\n * message if an unimplemented function is invoked\n *\n */\n\nint pthread_barrier_destroy(pthread_barrier_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_barrier_destroy(a);\n}\n\nint\npthread_barrier_init(pthread_barrier_t *a,\n\t\t     const pthread_barrierattr_t *b, unsigned c)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_barrier_init(a, b, c);\n}\n\nint pthread_barrier_wait(pthread_barrier_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_barrier_wait(a);\n}\n\nint pthread_cond_broadcast(pthread_cond_t *cond)\n{\n\tif (override) {\n\n\t\tlthread_cond_broadcast(*(struct lthread_cond **)cond);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cond_broadcast(cond);\n}\n\nint pthread_mutex_destroy(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_destroy(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_destroy(mutex);\n}\n\nint pthread_cond_destroy(pthread_cond_t *cond)\n{\n\tif (override)\n\t\treturn lthread_cond_destroy(*(struct lthread_cond **)cond);\n\treturn _sys_pthread_funcs.f_pthread_cond_destroy(cond);\n}\n\nint pthread_cond_init(pthread_cond_t *cond, const pthread_condattr_t *attr)\n{\n\tif (override)\n\t\treturn lthread_cond_init(NULL,\n\t\t\t\t(struct lthread_cond **)cond,\n\t\t\t\t(const struct lthread_condattr *) attr);\n\treturn _sys_pthread_funcs.f_pthread_cond_init(cond, attr);\n}\n\nint pthread_cond_signal(pthread_cond_t *cond)\n{\n\tif (override) {\n\t\tlthread_cond_signal(*(struct lthread_cond **)cond);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cond_signal(cond);\n}\n\nint\npthread_cond_timedwait(pthread_cond_t *__restrict cond,\n\t\t       pthread_mutex_t *__restrict mutex,\n\t\t       const struct timespec *__restrict time)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_cond_timedwait(cond, mutex, time);\n}\n\nint pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex)\n{\n\tif (override) {\n\t\tpthread_mutex_unlock(mutex);\n\t\tint rv = lthread_cond_wait(*(struct lthread_cond **)cond, 0);\n\n\t\tpthread_mutex_lock(mutex);\n\t\treturn rv;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cond_wait(cond, mutex);\n}\n\nint\npthread_create(pthread_t *__restrict tid,\n\t\tconst pthread_attr_t *__restrict attr,\n\t\tlthread_func_t func,\n\t       void *__restrict arg)\n{\n\tif (override) {\n\t\tint lcore = -1;\n\n\t\tif (attr != NULL) {\n\t\t\t/* determine CPU being requested */\n\t\t\trte_cpuset_t cpuset;\n\n\t\t\tCPU_ZERO(&cpuset);\n\t\t\tpthread_attr_getaffinity_np(attr,\n\t\t\t\t\t\tsizeof(rte_cpuset_t),\n\t\t\t\t\t\t&cpuset);\n\n\t\t\tif (CPU_COUNT(&cpuset) != 1)\n\t\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t\tfor (lcore = 0; lcore < LTHREAD_MAX_LCORES; lcore++) {\n\t\t\t\tif (!CPU_ISSET(lcore, &cpuset))\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\treturn lthread_create((struct lthread **)tid, lcore,\n\t\t\t\t      func, arg);\n\t}\n\treturn _sys_pthread_funcs.f_pthread_create(tid, attr, func, arg);\n}\n\nint pthread_detach(pthread_t tid)\n{\n\tif (override) {\n\t\tstruct lthread *lt = (struct lthread *)tid;\n\n\t\tif (lt == lthread_current()) {\n\t\t\tlthread_detach();\n\t\t\treturn 0;\n\t\t}\n\t\tNOT_IMPLEMENTED;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_detach(tid);\n}\n\nint pthread_equal(pthread_t a, pthread_t b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_equal(a, b);\n}\n\nvoid pthread_exit_override(void *v)\n{\n\tif (override) {\n\t\tlthread_exit(v);\n\t\treturn;\n\t}\n\t_sys_pthread_funcs.f_pthread_exit(v);\n}\n\nvoid\n*pthread_getspecific(pthread_key_t key)\n{\n\tif (override)\n\t\treturn lthread_getspecific((unsigned int) key);\n\treturn _sys_pthread_funcs.f_pthread_getspecific(key);\n}\n\nint pthread_getcpuclockid(pthread_t a, clockid_t *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_getcpuclockid(a, b);\n}\n\nint pthread_join(pthread_t tid, void **val)\n{\n\tif (override)\n\t\treturn lthread_join((struct lthread *)tid, val);\n\treturn _sys_pthread_funcs.f_pthread_join(tid, val);\n}\n\nint pthread_key_create(pthread_key_t *keyptr, void (*dtor) (void *))\n{\n\tif (override)\n\t\treturn lthread_key_create((unsigned int *)keyptr, dtor);\n\treturn _sys_pthread_funcs.f_pthread_key_create(keyptr, dtor);\n}\n\nint pthread_key_delete(pthread_key_t key)\n{\n\tif (override) {\n\t\tlthread_key_delete((unsigned int) key);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_key_delete(key);\n}\n\n\nint\npthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t *attr)\n{\n\tif (override)\n\t\treturn lthread_mutex_init(NULL,\n\t\t\t\t(struct lthread_mutex **)mutex,\n\t\t\t\t(const struct lthread_mutexattr *)attr);\n\treturn _sys_pthread_funcs.f_pthread_mutex_init(mutex, attr);\n}\n\nint pthread_mutex_lock(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_lock(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_lock(mutex);\n}\n\nint pthread_mutex_trylock(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_trylock(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_trylock(mutex);\n}\n\nint pthread_mutex_timedlock(pthread_mutex_t *mutex, const struct timespec *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_mutex_timedlock(mutex, b);\n}\n\nint pthread_mutex_unlock(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_unlock(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_unlock(mutex);\n}\n\nint pthread_once(pthread_once_t *a, void (b) (void))\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_once(a, b);\n}\n\nint pthread_rwlock_destroy(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_destroy(a);\n}\n\nint pthread_rwlock_init(pthread_rwlock_t *a, const pthread_rwlockattr_t *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_init(a, b);\n}\n\nint pthread_rwlock_rdlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_rdlock(a);\n}\n\nint pthread_rwlock_timedrdlock(pthread_rwlock_t *a, const struct timespec *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_timedrdlock(a, b);\n}\n\nint pthread_rwlock_timedwrlock(pthread_rwlock_t *a, const struct timespec *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_timedwrlock(a, b);\n}\n\nint pthread_rwlock_tryrdlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_tryrdlock(a);\n}\n\nint pthread_rwlock_trywrlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_trywrlock(a);\n}\n\nint pthread_rwlock_unlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_unlock(a);\n}\n\nint pthread_rwlock_wrlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_wrlock(a);\n}\n\n#ifdef RTE_EXEC_ENV_LINUX\nint\npthread_yield(void)\n{\n\tif (override) {\n\t\tlthread_yield();\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_yield();\n}\n#else\nvoid\npthread_yield(void)\n{\n\tif (override)\n\t\tlthread_yield();\n\telse\n\t\t_sys_pthread_funcs.f_pthread_yield();\n}\n#endif\n\npthread_t pthread_self(void)\n{\n\tif (override)\n\t\treturn (pthread_t) lthread_current();\n\treturn _sys_pthread_funcs.f_pthread_self();\n}\n\nint pthread_setspecific(pthread_key_t key, const void *data)\n{\n\tif (override) {\n\t\tint rv =  lthread_setspecific((unsigned int)key, data);\n\t\treturn rv;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_setspecific(key, data);\n}\n\nint pthread_spin_init(pthread_spinlock_t *a, int b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_init(a, b);\n}\n\nint pthread_spin_destroy(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_destroy(a);\n}\n\nint pthread_spin_lock(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_lock(a);\n}\n\nint pthread_spin_trylock(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_trylock(a);\n}\n\nint pthread_spin_unlock(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_unlock(a);\n}\n\nint pthread_cancel(pthread_t tid)\n{\n\tif (override) {\n\t\tlthread_cancel(*(struct lthread **)tid);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cancel(tid);\n}\n\nint pthread_setcancelstate(int a, int *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_setcancelstate(a, b);\n}\n\nint pthread_setcanceltype(int a, int *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_setcanceltype(a, b);\n}\n\nvoid pthread_testcancel(void)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_testcancel();\n}\n\n\nint pthread_getschedparam(pthread_t tid, int *a, struct sched_param *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_getschedparam(tid, a, b);\n}\n\nint pthread_setschedparam(pthread_t a, int b, const struct sched_param *c)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_setschedparam(a, b, c);\n}\n\n\nint nanosleep(const struct timespec *req, struct timespec *rem)\n{\n\tif (override) {\n\t\tuint64_t ns = req->tv_sec * 1000000000 + req->tv_nsec;\n\n\t\tlthread_sleep(ns);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_nanosleep(req, rem);\n}\n\nint\npthread_setaffinity_np(pthread_t thread, size_t cpusetsize,\n\t\t       const rte_cpuset_t *cpuset)\n{\n\tif (override) {\n\t\t/* we only allow affinity with a single CPU */\n\t\tif (CPU_COUNT(cpuset) != 1)\n\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t/* we only allow the current thread to sets its own affinity */\n\t\tstruct lthread *lt = (struct lthread *)thread;\n\n\t\tif (lthread_current() != lt)\n\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t/* determine the CPU being requested */\n\t\tint i;\n\n\t\tfor (i = 0; i < LTHREAD_MAX_LCORES; i++) {\n\t\t\tif (!CPU_ISSET(i, cpuset))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\t/* check requested core is allowed */\n\t\tif (i == LTHREAD_MAX_LCORES)\n\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t/* finally we can set affinity to the requested lcore */\n\t\tlthread_set_affinity(i);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_setaffinity_np(thread, cpusetsize,\n\t\t\t\t\t\t\t   cpuset);\n}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/forward_stats.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/console.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/host_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/fast_pkt_proc.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/perf_benchmark.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/vm_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/vswitch_vm.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/host_vm_comms_qemu.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/inter_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/vmxnet3_int.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/single_port_nic.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/load_bal_app_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/quickassist_block_diagram.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/l2_fwd_virtenv_benchmark_setup.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/pipeline_overview.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/kernel_nic.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/example_rules.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/client_svr_sym_multi_proc_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/sym_multi_proc_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/test_pipeline_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/ring_pipeline_perf_setup.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/qos_sched_app_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/ipv4_acl_rule.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/threads_pipelines.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/ex_data_flow_tru_dropper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure35.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure33.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/m_definition.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/ewma_filter_eq_1.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/kernel_nic_intf.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/prefetch_pipeline.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/drop_probability_eq3.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/packet_distributor2.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pkt_flow_kni.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/tbl24_tbl8_tbl8.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/packet_distributor1.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/hier_sched_blk.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/ewma_filter_eq_2.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/eq2_factor.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure34.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/kni_traffic_flow.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pkt_drop_probability.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pipe_prefetch_sm.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/sched_hier_per_port.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/data_struct_per_port.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/drop_probability_eq4.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/vhost_net_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/flow_tru_droppper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure39.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pkt_proc_pipeline_qos.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure32.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/tbl24_tbl8.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure38.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/blk_diag_dropper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/eq2_expression.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure37.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/drop_probability_graph.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/logo/DPDK_logo_vertical_rev_small.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/logo/DPDK_logo_horizontal_tag.png"
    ],
    "total_files": 3537
}