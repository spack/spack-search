{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/drivers/common/mlx5/mlx5_common.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright 2019 Mellanox Technologies, Ltd\n */\n\n#include <dlfcn.h>\n#include <unistd.h>\n#include <string.h>\n#include <stdio.h>\n\n#include <rte_errno.h>\n\n#include \"mlx5_common.h\"\n#include \"mlx5_common_utils.h\"\n#include \"mlx5_glue.h\"\n\n\nint mlx5_common_logtype;\n\n\n/**\n * Get PCI information by sysfs device path.\n *\n * @param dev_path\n *   Pointer to device sysfs folder name.\n * @param[out] pci_addr\n *   PCI bus address output buffer.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nint\nmlx5_dev_to_pci_addr(const char *dev_path,\n\t\t     struct rte_pci_addr *pci_addr)\n{\n\tFILE *file;\n\tchar line[32];\n\tMKSTR(path, \"%s/device/uevent\", dev_path);\n\n\tfile = fopen(path, \"rb\");\n\tif (file == NULL) {\n\t\trte_errno = errno;\n\t\treturn -rte_errno;\n\t}\n\twhile (fgets(line, sizeof(line), file) == line) {\n\t\tsize_t len = strlen(line);\n\t\tint ret;\n\n\t\t/* Truncate long lines. */\n\t\tif (len == (sizeof(line) - 1))\n\t\t\twhile (line[(len - 1)] != '\\n') {\n\t\t\t\tret = fgetc(file);\n\t\t\t\tif (ret == EOF)\n\t\t\t\t\tbreak;\n\t\t\t\tline[(len - 1)] = ret;\n\t\t\t}\n\t\t/* Extract information. */\n\t\tif (sscanf(line,\n\t\t\t   \"PCI_SLOT_NAME=\"\n\t\t\t   \"%\" SCNx32 \":%\" SCNx8 \":%\" SCNx8 \".%\" SCNx8 \"\\n\",\n\t\t\t   &pci_addr->domain,\n\t\t\t   &pci_addr->bus,\n\t\t\t   &pci_addr->devid,\n\t\t\t   &pci_addr->function) == 4) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfclose(file);\n\treturn 0;\n}\n\nstatic int\nmlx5_class_check_handler(__rte_unused const char *key, const char *value,\n\t\t\t void *opaque)\n{\n\tenum mlx5_class *ret = opaque;\n\n\tif (strcmp(value, \"vdpa\") == 0) {\n\t\t*ret = MLX5_CLASS_VDPA;\n\t} else if (strcmp(value, \"net\") == 0) {\n\t\t*ret = MLX5_CLASS_NET;\n\t} else {\n\t\tDRV_LOG(ERR, \"Invalid mlx5 class %s. Maybe typo in device\"\n\t\t\t\" class argument setting?\", value);\n\t\t*ret = MLX5_CLASS_INVALID;\n\t}\n\treturn 0;\n}\n\nenum mlx5_class\nmlx5_class_get(struct rte_devargs *devargs)\n{\n\tstruct rte_kvargs *kvlist;\n\tconst char *key = MLX5_CLASS_ARG_NAME;\n\tenum mlx5_class ret = MLX5_CLASS_NET;\n\n\tif (devargs == NULL)\n\t\treturn ret;\n\tkvlist = rte_kvargs_parse(devargs->args, NULL);\n\tif (kvlist == NULL)\n\t\treturn ret;\n\tif (rte_kvargs_count(kvlist, key))\n\t\trte_kvargs_process(kvlist, key, mlx5_class_check_handler, &ret);\n\trte_kvargs_free(kvlist);\n\treturn ret;\n}\n\n/**\n * Extract port name, as a number, from sysfs or netlink information.\n *\n * @param[in] port_name_in\n *   String representing the port name.\n * @param[out] port_info_out\n *   Port information, including port name as a number and port name\n *   type if recognized\n *\n * @return\n *   port_name field set according to recognized name format.\n */\nvoid\nmlx5_translate_port_name(const char *port_name_in,\n\t\t\t struct mlx5_switch_info *port_info_out)\n{\n\tchar pf_c1, pf_c2, vf_c1, vf_c2;\n\tchar *end;\n\tint sc_items;\n\n\t/*\n\t * Check for port-name as a string of the form pf0vf0\n\t * (support kernel ver >= 5.0 or OFED ver >= 4.6).\n\t */\n\tsc_items = sscanf(port_name_in, \"%c%c%d%c%c%d\",\n\t\t\t  &pf_c1, &pf_c2, &port_info_out->pf_num,\n\t\t\t  &vf_c1, &vf_c2, &port_info_out->port_name);\n\tif (sc_items == 6 &&\n\t    pf_c1 == 'p' && pf_c2 == 'f' &&\n\t    vf_c1 == 'v' && vf_c2 == 'f') {\n\t\tport_info_out->name_type = MLX5_PHYS_PORT_NAME_TYPE_PFVF;\n\t\treturn;\n\t}\n\t/*\n\t * Check for port-name as a string of the form p0\n\t * (support kernel ver >= 5.0, or OFED ver >= 4.6).\n\t */\n\tsc_items = sscanf(port_name_in, \"%c%d\",\n\t\t\t  &pf_c1, &port_info_out->port_name);\n\tif (sc_items == 2 && pf_c1 == 'p') {\n\t\tport_info_out->name_type = MLX5_PHYS_PORT_NAME_TYPE_UPLINK;\n\t\treturn;\n\t}\n\t/* Check for port-name as a number (support kernel ver < 5.0 */\n\terrno = 0;\n\tport_info_out->port_name = strtol(port_name_in, &end, 0);\n\tif (!errno &&\n\t    (size_t)(end - port_name_in) == strlen(port_name_in)) {\n\t\tport_info_out->name_type = MLX5_PHYS_PORT_NAME_TYPE_LEGACY;\n\t\treturn;\n\t}\n\tport_info_out->name_type = MLX5_PHYS_PORT_NAME_TYPE_UNKNOWN;\n\treturn;\n}\n\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\n/**\n * Suffix RTE_EAL_PMD_PATH with \"-glue\".\n *\n * This function performs a sanity check on RTE_EAL_PMD_PATH before\n * suffixing its last component.\n *\n * @param buf[out]\n *   Output buffer, should be large enough otherwise NULL is returned.\n * @param size\n *   Size of @p out.\n *\n * @return\n *   Pointer to @p buf or @p NULL in case suffix cannot be appended.\n */\nstatic char *\nmlx5_glue_path(char *buf, size_t size)\n{\n\tstatic const char *const bad[] = { \"/\", \".\", \"..\", NULL };\n\tconst char *path = RTE_EAL_PMD_PATH;\n\tsize_t len = strlen(path);\n\tsize_t off;\n\tint i;\n\n\twhile (len && path[len - 1] == '/')\n\t\t--len;\n\tfor (off = len; off && path[off - 1] != '/'; --off)\n\t\t;\n\tfor (i = 0; bad[i]; ++i)\n\t\tif (!strncmp(path + off, bad[i], (int)(len - off)))\n\t\t\tgoto error;\n\ti = snprintf(buf, size, \"%.*s-glue\", (int)len, path);\n\tif (i == -1 || (size_t)i >= size)\n\t\tgoto error;\n\treturn buf;\nerror:\n\tRTE_LOG(ERR, PMD, \"unable to append \\\"-glue\\\" to last component of\"\n\t\t\" RTE_EAL_PMD_PATH (\\\"\" RTE_EAL_PMD_PATH \"\\\"), please\"\n\t\t\" re-configure DPDK\");\n\treturn NULL;\n}\n#endif\n\n/**\n * Initialization routine for run-time dependency on rdma-core.\n */\nRTE_INIT_PRIO(mlx5_glue_init, CLASS)\n{\n\tvoid *handle = NULL;\n\n\t/* Initialize common log type. */\n\tmlx5_common_logtype = rte_log_register(\"pmd.common.mlx5\");\n\tif (mlx5_common_logtype >= 0)\n\t\trte_log_set_level(mlx5_common_logtype, RTE_LOG_NOTICE);\n\t/*\n\t * RDMAV_HUGEPAGES_SAFE tells ibv_fork_init() we intend to use\n\t * huge pages. Calling ibv_fork_init() during init allows\n\t * applications to use fork() safely for purposes other than\n\t * using this PMD, which is not supported in forked processes.\n\t */\n\tsetenv(\"RDMAV_HUGEPAGES_SAFE\", \"1\", 1);\n\t/* Match the size of Rx completion entry to the size of a cacheline. */\n\tif (RTE_CACHE_LINE_SIZE == 128)\n\t\tsetenv(\"MLX5_CQE_SIZE\", \"128\", 0);\n\t/*\n\t * MLX5_DEVICE_FATAL_CLEANUP tells ibv_destroy functions to\n\t * cleanup all the Verbs resources even when the device was removed.\n\t */\n\tsetenv(\"MLX5_DEVICE_FATAL_CLEANUP\", \"1\", 1);\n\t/* The glue initialization was done earlier by mlx5 common library. */\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\tchar glue_path[sizeof(RTE_EAL_PMD_PATH) - 1 + sizeof(\"-glue\")];\n\tconst char *path[] = {\n\t\t/*\n\t\t * A basic security check is necessary before trusting\n\t\t * MLX5_GLUE_PATH, which may override RTE_EAL_PMD_PATH.\n\t\t */\n\t\t(geteuid() == getuid() && getegid() == getgid() ?\n\t\t getenv(\"MLX5_GLUE_PATH\") : NULL),\n\t\t/*\n\t\t * When RTE_EAL_PMD_PATH is set, use its glue-suffixed\n\t\t * variant, otherwise let dlopen() look up libraries on its\n\t\t * own.\n\t\t */\n\t\t(*RTE_EAL_PMD_PATH ?\n\t\t mlx5_glue_path(glue_path, sizeof(glue_path)) : \"\"),\n\t};\n\tunsigned int i = 0;\n\tvoid **sym;\n\tconst char *dlmsg;\n\n\twhile (!handle && i != RTE_DIM(path)) {\n\t\tconst char *end;\n\t\tsize_t len;\n\t\tint ret;\n\n\t\tif (!path[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\tend = strpbrk(path[i], \":;\");\n\t\tif (!end)\n\t\t\tend = path[i] + strlen(path[i]);\n\t\tlen = end - path[i];\n\t\tret = 0;\n\t\tdo {\n\t\t\tchar name[ret + 1];\n\n\t\t\tret = snprintf(name, sizeof(name), \"%.*s%s\" MLX5_GLUE,\n\t\t\t\t       (int)len, path[i],\n\t\t\t\t       (!len || *(end - 1) == '/') ? \"\" : \"/\");\n\t\t\tif (ret == -1)\n\t\t\t\tbreak;\n\t\t\tif (sizeof(name) != (size_t)ret + 1)\n\t\t\t\tcontinue;\n\t\t\tDRV_LOG(DEBUG, \"Looking for rdma-core glue as \"\n\t\t\t\t\"\\\"%s\\\"\", name);\n\t\t\thandle = dlopen(name, RTLD_LAZY);\n\t\t\tbreak;\n\t\t} while (1);\n\t\tpath[i] = end + 1;\n\t\tif (!*end)\n\t\t\t++i;\n\t}\n\tif (!handle) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tDRV_LOG(WARNING, \"Cannot load glue library: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tsym = dlsym(handle, \"mlx5_glue\");\n\tif (!sym || !*sym) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tDRV_LOG(ERR, \"Cannot resolve glue symbol: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tmlx5_glue = *sym;\n#endif /* RTE_IBVERBS_LINK_DLOPEN */\n#ifdef RTE_LIBRTE_MLX5_DEBUG\n\t/* Glue structure must not contain any NULL pointers. */\n\t{\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i != sizeof(*mlx5_glue) / sizeof(void *); ++i)\n\t\t\tMLX5_ASSERT(((const void *const *)mlx5_glue)[i]);\n\t}\n#endif\n\tif (strcmp(mlx5_glue->version, MLX5_GLUE_VERSION)) {\n\t\trte_errno = EINVAL;\n\t\tDRV_LOG(ERR, \"rdma-core glue \\\"%s\\\" mismatch: \\\"%s\\\" is \"\n\t\t\t\"required\", mlx5_glue->version, MLX5_GLUE_VERSION);\n\t\tgoto glue_error;\n\t}\n\tmlx5_glue->fork_init();\n\treturn;\nglue_error:\n\tif (handle)\n\t\tdlclose(handle);\n\tDRV_LOG(WARNING, \"Cannot initialize MLX5 common due to missing\"\n\t\t\" run-time dependency on rdma-core libraries (libibverbs,\"\n\t\t\" libmlx5)\");\n\tmlx5_glue = NULL;\n\treturn;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/drivers/net/mlx4/mlx4.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright 2012 6WIND S.A.\n * Copyright 2012 Mellanox Technologies, Ltd\n */\n\n/**\n * @file\n * mlx4 driver initialization.\n */\n\n#include <dlfcn.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <stddef.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/mman.h>\n#include <unistd.h>\n\n/* Verbs headers do not support -pedantic. */\n#ifdef PEDANTIC\n#pragma GCC diagnostic ignored \"-Wpedantic\"\n#endif\n#include <infiniband/verbs.h>\n#ifdef PEDANTIC\n#pragma GCC diagnostic error \"-Wpedantic\"\n#endif\n\n#include <rte_common.h>\n#include <rte_dev.h>\n#include <rte_errno.h>\n#include <rte_ethdev_driver.h>\n#include <rte_ethdev_pci.h>\n#include <rte_ether.h>\n#include <rte_flow.h>\n#include <rte_interrupts.h>\n#include <rte_kvargs.h>\n#include <rte_malloc.h>\n#include <rte_mbuf.h>\n\n#include \"mlx4.h\"\n#include \"mlx4_glue.h\"\n#include \"mlx4_flow.h\"\n#include \"mlx4_mr.h\"\n#include \"mlx4_rxtx.h\"\n#include \"mlx4_utils.h\"\n\nstatic const char *MZ_MLX4_PMD_SHARED_DATA = \"mlx4_pmd_shared_data\";\n\n/* Shared memory between primary and secondary processes. */\nstruct mlx4_shared_data *mlx4_shared_data;\n\n/* Spinlock for mlx4_shared_data allocation. */\nstatic rte_spinlock_t mlx4_shared_data_lock = RTE_SPINLOCK_INITIALIZER;\n\n/* Process local data for secondary processes. */\nstatic struct mlx4_local_data mlx4_local_data;\n\n/** Driver-specific log messages type. */\nint mlx4_logtype;\n\n/** Configuration structure for device arguments. */\nstruct mlx4_conf {\n\tstruct {\n\t\tuint32_t present; /**< Bit-field for existing ports. */\n\t\tuint32_t enabled; /**< Bit-field for user-enabled ports. */\n\t} ports;\n\tint mr_ext_memseg_en;\n\t/** Whether memseg should be extended for MR creation. */\n};\n\n/* Available parameters list. */\nconst char *pmd_mlx4_init_params[] = {\n\tMLX4_PMD_PORT_KVARG,\n\tMLX4_MR_EXT_MEMSEG_EN_KVARG,\n\tNULL,\n};\n\nstatic void mlx4_dev_stop(struct rte_eth_dev *dev);\n\n/**\n * Initialize shared data between primary and secondary process.\n *\n * A memzone is reserved by primary process and secondary processes attach to\n * the memzone.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_init_shared_data(void)\n{\n\tconst struct rte_memzone *mz;\n\tint ret = 0;\n\n\trte_spinlock_lock(&mlx4_shared_data_lock);\n\tif (mlx4_shared_data == NULL) {\n\t\tif (rte_eal_process_type() == RTE_PROC_PRIMARY) {\n\t\t\t/* Allocate shared memory. */\n\t\t\tmz = rte_memzone_reserve(MZ_MLX4_PMD_SHARED_DATA,\n\t\t\t\t\t\t sizeof(*mlx4_shared_data),\n\t\t\t\t\t\t SOCKET_ID_ANY, 0);\n\t\t\tif (mz == NULL) {\n\t\t\t\tERROR(\"Cannot allocate mlx4 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx4_shared_data = mz->addr;\n\t\t\tmemset(mlx4_shared_data, 0, sizeof(*mlx4_shared_data));\n\t\t\trte_spinlock_init(&mlx4_shared_data->lock);\n\t\t} else {\n\t\t\t/* Lookup allocated shared memory. */\n\t\t\tmz = rte_memzone_lookup(MZ_MLX4_PMD_SHARED_DATA);\n\t\t\tif (mz == NULL) {\n\t\t\t\tERROR(\"Cannot attach mlx4 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx4_shared_data = mz->addr;\n\t\t\tmemset(&mlx4_local_data, 0, sizeof(mlx4_local_data));\n\t\t}\n\t}\nerror:\n\trte_spinlock_unlock(&mlx4_shared_data_lock);\n\treturn ret;\n}\n\n#ifdef HAVE_IBV_MLX4_BUF_ALLOCATORS\n/**\n * Verbs callback to allocate a memory. This function should allocate the space\n * according to the size provided residing inside a huge page.\n * Please note that all allocation must respect the alignment from libmlx4\n * (i.e. currently sysconf(_SC_PAGESIZE)).\n *\n * @param[in] size\n *   The size in bytes of the memory to allocate.\n * @param[in] data\n *   A pointer to the callback data.\n *\n * @return\n *   Allocated buffer, NULL otherwise and rte_errno is set.\n */\nstatic void *\nmlx4_alloc_verbs_buf(size_t size, void *data)\n{\n\tstruct mlx4_priv *priv = data;\n\tvoid *ret;\n\tsize_t alignment = sysconf(_SC_PAGESIZE);\n\tunsigned int socket = SOCKET_ID_ANY;\n\n\tif (priv->verbs_alloc_ctx.type == MLX4_VERBS_ALLOC_TYPE_TX_QUEUE) {\n\t\tconst struct txq *txq = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = txq->socket;\n\t} else if (priv->verbs_alloc_ctx.type ==\n\t\t   MLX4_VERBS_ALLOC_TYPE_RX_QUEUE) {\n\t\tconst struct rxq *rxq = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = rxq->socket;\n\t}\n\tMLX4_ASSERT(data != NULL);\n\tret = rte_malloc_socket(__func__, size, alignment, socket);\n\tif (!ret && size)\n\t\trte_errno = ENOMEM;\n\treturn ret;\n}\n\n/**\n * Verbs callback to free a memory.\n *\n * @param[in] ptr\n *   A pointer to the memory to free.\n * @param[in] data\n *   A pointer to the callback data.\n */\nstatic void\nmlx4_free_verbs_buf(void *ptr, void *data __rte_unused)\n{\n\tMLX4_ASSERT(data != NULL);\n\trte_free(ptr);\n}\n#endif\n\n/**\n * Initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_proc_priv_init(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_proc_priv *ppriv;\n\tsize_t ppriv_size;\n\n\t/*\n\t * UAR register table follows the process private structure. BlueFlame\n\t * registers for Tx queues are stored in the table.\n\t */\n\tppriv_size = sizeof(struct mlx4_proc_priv) +\n\t\t     dev->data->nb_tx_queues * sizeof(void *);\n\tppriv = rte_malloc_socket(\"mlx4_proc_priv\", ppriv_size,\n\t\t\t\t  RTE_CACHE_LINE_SIZE, dev->device->numa_node);\n\tif (!ppriv) {\n\t\trte_errno = ENOMEM;\n\t\treturn -rte_errno;\n\t}\n\tppriv->uar_table_sz = ppriv_size;\n\tdev->process_private = ppriv;\n\treturn 0;\n}\n\n/**\n * Un-initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_proc_priv_uninit(struct rte_eth_dev *dev)\n{\n\tif (!dev->process_private)\n\t\treturn;\n\trte_free(dev->process_private);\n\tdev->process_private = NULL;\n}\n\n/**\n * DPDK callback for Ethernet device configuration.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_dev_configure(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tstruct rte_flow_error error;\n\tint ret;\n\n\tif (dev->data->dev_conf.rxmode.mq_mode & ETH_MQ_RX_RSS_FLAG)\n\t\tdev->data->dev_conf.rxmode.offloads |= DEV_RX_OFFLOAD_RSS_HASH;\n\n\t/* Prepare internal flow rules. */\n\tret = mlx4_flow_sync(priv, &error);\n\tif (ret) {\n\t\tERROR(\"cannot set up internal flow rules (code %d, \\\"%s\\\"),\"\n\t\t      \" flow error type %d, cause %p, message: %s\",\n\t\t      -ret, strerror(-ret), error.type, error.cause,\n\t\t      error.message ? error.message : \"(unspecified)\");\n\t\tgoto exit;\n\t}\n\tret = mlx4_intr_install(priv);\n\tif (ret) {\n\t\tERROR(\"%p: interrupt handler installation failed\",\n\t\t      (void *)dev);\n\t\tgoto exit;\n\t}\n\tret = mlx4_proc_priv_init(dev);\n\tif (ret) {\n\t\tERROR(\"%p: process private data allocation failed\",\n\t\t      (void *)dev);\n\t\tgoto exit;\n\t}\nexit:\n\treturn ret;\n}\n\n/**\n * DPDK callback to start the device.\n *\n * Simulate device start by initializing common RSS resources and attaching\n * all configured flows.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_dev_start(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tstruct rte_flow_error error;\n\tint ret;\n\n\tif (priv->started)\n\t\treturn 0;\n\tDEBUG(\"%p: attaching configured flows to all RX queues\", (void *)dev);\n\tpriv->started = 1;\n\tret = mlx4_rss_init(priv);\n\tif (ret) {\n\t\tERROR(\"%p: cannot initialize RSS resources: %s\",\n\t\t      (void *)dev, strerror(-ret));\n\t\tgoto err;\n\t}\n#ifdef RTE_LIBRTE_MLX4_DEBUG\n\tmlx4_mr_dump_dev(dev);\n#endif\n\tret = mlx4_rxq_intr_enable(priv);\n\tif (ret) {\n\t\tERROR(\"%p: interrupt handler installation failed\",\n\t\t     (void *)dev);\n\t\tgoto err;\n\t}\n\tret = mlx4_flow_sync(priv, &error);\n\tif (ret) {\n\t\tERROR(\"%p: cannot attach flow rules (code %d, \\\"%s\\\"),\"\n\t\t      \" flow error type %d, cause %p, message: %s\",\n\t\t      (void *)dev,\n\t\t      -ret, strerror(-ret), error.type, error.cause,\n\t\t      error.message ? error.message : \"(unspecified)\");\n\t\tgoto err;\n\t}\n\trte_wmb();\n\tdev->tx_pkt_burst = mlx4_tx_burst;\n\tdev->rx_pkt_burst = mlx4_rx_burst;\n\t/* Enable datapath on secondary process. */\n\tmlx4_mp_req_start_rxtx(dev);\n\treturn 0;\nerr:\n\tmlx4_dev_stop(dev);\n\treturn ret;\n}\n\n/**\n * DPDK callback to stop the device.\n *\n * Simulate device stop by detaching all configured flows.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_dev_stop(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\n\tif (!priv->started)\n\t\treturn;\n\tDEBUG(\"%p: detaching flows from all RX queues\", (void *)dev);\n\tpriv->started = 0;\n\tdev->tx_pkt_burst = mlx4_tx_burst_removed;\n\tdev->rx_pkt_burst = mlx4_rx_burst_removed;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx4_mp_req_stop_rxtx(dev);\n\tmlx4_flow_sync(priv, NULL);\n\tmlx4_rxq_intr_disable(priv);\n\tmlx4_rss_deinit(priv);\n}\n\n/**\n * DPDK callback to close the device.\n *\n * Destroy all queues and objects, free memory.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tunsigned int i;\n\n\tDEBUG(\"%p: closing device \\\"%s\\\"\",\n\t      (void *)dev,\n\t      ((priv->ctx != NULL) ? priv->ctx->device->name : \"\"));\n\tdev->rx_pkt_burst = mlx4_rx_burst_removed;\n\tdev->tx_pkt_burst = mlx4_tx_burst_removed;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx4_mp_req_stop_rxtx(dev);\n\tmlx4_flow_clean(priv);\n\tmlx4_rss_deinit(priv);\n\tfor (i = 0; i != dev->data->nb_rx_queues; ++i)\n\t\tmlx4_rx_queue_release(dev->data->rx_queues[i]);\n\tfor (i = 0; i != dev->data->nb_tx_queues; ++i)\n\t\tmlx4_tx_queue_release(dev->data->tx_queues[i]);\n\tmlx4_proc_priv_uninit(dev);\n\tmlx4_mr_release(dev);\n\tif (priv->pd != NULL) {\n\t\tMLX4_ASSERT(priv->ctx != NULL);\n\t\tclaim_zero(mlx4_glue->dealloc_pd(priv->pd));\n\t\tclaim_zero(mlx4_glue->close_device(priv->ctx));\n\t} else\n\t\tMLX4_ASSERT(priv->ctx == NULL);\n\tmlx4_intr_uninstall(priv);\n\tmemset(priv, 0, sizeof(*priv));\n}\n\nstatic const struct eth_dev_ops mlx4_dev_ops = {\n\t.dev_configure = mlx4_dev_configure,\n\t.dev_start = mlx4_dev_start,\n\t.dev_stop = mlx4_dev_stop,\n\t.dev_set_link_down = mlx4_dev_set_link_down,\n\t.dev_set_link_up = mlx4_dev_set_link_up,\n\t.dev_close = mlx4_dev_close,\n\t.link_update = mlx4_link_update,\n\t.promiscuous_enable = mlx4_promiscuous_enable,\n\t.promiscuous_disable = mlx4_promiscuous_disable,\n\t.allmulticast_enable = mlx4_allmulticast_enable,\n\t.allmulticast_disable = mlx4_allmulticast_disable,\n\t.mac_addr_remove = mlx4_mac_addr_remove,\n\t.mac_addr_add = mlx4_mac_addr_add,\n\t.mac_addr_set = mlx4_mac_addr_set,\n\t.set_mc_addr_list = mlx4_set_mc_addr_list,\n\t.stats_get = mlx4_stats_get,\n\t.stats_reset = mlx4_stats_reset,\n\t.fw_version_get = mlx4_fw_version_get,\n\t.dev_infos_get = mlx4_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx4_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx4_vlan_filter_set,\n\t.rx_queue_setup = mlx4_rx_queue_setup,\n\t.tx_queue_setup = mlx4_tx_queue_setup,\n\t.rx_queue_release = mlx4_rx_queue_release,\n\t.tx_queue_release = mlx4_tx_queue_release,\n\t.flow_ctrl_get = mlx4_flow_ctrl_get,\n\t.flow_ctrl_set = mlx4_flow_ctrl_set,\n\t.mtu_set = mlx4_mtu_set,\n\t.filter_ctrl = mlx4_filter_ctrl,\n\t.rx_queue_intr_enable = mlx4_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx4_rx_intr_disable,\n\t.is_removed = mlx4_is_removed,\n};\n\n/* Available operations from secondary process. */\nstatic const struct eth_dev_ops mlx4_dev_sec_ops = {\n\t.stats_get = mlx4_stats_get,\n\t.stats_reset = mlx4_stats_reset,\n\t.fw_version_get = mlx4_fw_version_get,\n\t.dev_infos_get = mlx4_dev_infos_get,\n};\n\n/**\n * Get PCI information from struct ibv_device.\n *\n * @param device\n *   Pointer to Ethernet device structure.\n * @param[out] pci_addr\n *   PCI bus address output buffer.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_ibv_device_to_pci_addr(const struct ibv_device *device,\n\t\t\t    struct rte_pci_addr *pci_addr)\n{\n\tFILE *file;\n\tchar line[32];\n\tMKSTR(path, \"%s/device/uevent\", device->ibdev_path);\n\n\tfile = fopen(path, \"rb\");\n\tif (file == NULL) {\n\t\trte_errno = errno;\n\t\treturn -rte_errno;\n\t}\n\twhile (fgets(line, sizeof(line), file) == line) {\n\t\tsize_t len = strlen(line);\n\t\tint ret;\n\n\t\t/* Truncate long lines. */\n\t\tif (len == (sizeof(line) - 1))\n\t\t\twhile (line[(len - 1)] != '\\n') {\n\t\t\t\tret = fgetc(file);\n\t\t\t\tif (ret == EOF)\n\t\t\t\t\tbreak;\n\t\t\t\tline[(len - 1)] = ret;\n\t\t\t}\n\t\t/* Extract information. */\n\t\tif (sscanf(line,\n\t\t\t   \"PCI_SLOT_NAME=\"\n\t\t\t   \"%\" SCNx32 \":%\" SCNx8 \":%\" SCNx8 \".%\" SCNx8 \"\\n\",\n\t\t\t   &pci_addr->domain,\n\t\t\t   &pci_addr->bus,\n\t\t\t   &pci_addr->devid,\n\t\t\t   &pci_addr->function) == 4) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfclose(file);\n\treturn 0;\n}\n\n/**\n * Verify and store value for device argument.\n *\n * @param[in] key\n *   Key argument to verify.\n * @param[in] val\n *   Value associated with key.\n * @param[in, out] conf\n *   Shared configuration data.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_arg_parse(const char *key, const char *val, struct mlx4_conf *conf)\n{\n\tunsigned long tmp;\n\n\terrno = 0;\n\ttmp = strtoul(val, NULL, 0);\n\tif (errno) {\n\t\trte_errno = errno;\n\t\tWARN(\"%s: \\\"%s\\\" is not a valid integer\", key, val);\n\t\treturn -rte_errno;\n\t}\n\tif (strcmp(MLX4_PMD_PORT_KVARG, key) == 0) {\n\t\tuint32_t ports = rte_log2_u32(conf->ports.present + 1);\n\n\t\tif (tmp >= ports) {\n\t\t\tERROR(\"port index %lu outside range [0,%\" PRIu32 \")\",\n\t\t\t      tmp, ports);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(conf->ports.present & (1 << tmp))) {\n\t\t\trte_errno = EINVAL;\n\t\t\tERROR(\"invalid port index %lu\", tmp);\n\t\t\treturn -rte_errno;\n\t\t}\n\t\tconf->ports.enabled |= 1 << tmp;\n\t} else if (strcmp(MLX4_MR_EXT_MEMSEG_EN_KVARG, key) == 0) {\n\t\tconf->mr_ext_memseg_en = !!tmp;\n\t} else {\n\t\trte_errno = EINVAL;\n\t\tWARN(\"%s: unknown parameter\", key);\n\t\treturn -rte_errno;\n\t}\n\treturn 0;\n}\n\n/**\n * Parse device parameters.\n *\n * @param devargs\n *   Device arguments structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_args(struct rte_devargs *devargs, struct mlx4_conf *conf)\n{\n\tstruct rte_kvargs *kvlist;\n\tunsigned int arg_count;\n\tint ret = 0;\n\tint i;\n\n\tif (devargs == NULL)\n\t\treturn 0;\n\tkvlist = rte_kvargs_parse(devargs->args, pmd_mlx4_init_params);\n\tif (kvlist == NULL) {\n\t\trte_errno = EINVAL;\n\t\tERROR(\"failed to parse kvargs\");\n\t\treturn -rte_errno;\n\t}\n\t/* Process parameters. */\n\tfor (i = 0; pmd_mlx4_init_params[i]; ++i) {\n\t\targ_count = rte_kvargs_count(kvlist, pmd_mlx4_init_params[i]);\n\t\twhile (arg_count-- > 0) {\n\t\t\tret = rte_kvargs_process(kvlist,\n\t\t\t\t\t\t pmd_mlx4_init_params[i],\n\t\t\t\t\t\t (int (*)(const char *,\n\t\t\t\t\t\t\t  const char *,\n\t\t\t\t\t\t\t  void *))\n\t\t\t\t\t\t mlx4_arg_parse,\n\t\t\t\t\t\t conf);\n\t\t\tif (ret != 0)\n\t\t\t\tgoto free_kvlist;\n\t\t}\n\t}\nfree_kvlist:\n\trte_kvargs_free(kvlist);\n\treturn ret;\n}\n\n/**\n * Interpret RSS capabilities reported by device.\n *\n * This function returns the set of usable Verbs RSS hash fields, kernel\n * quirks taken into account.\n *\n * @param ctx\n *   Verbs context.\n * @param pd\n *   Verbs protection domain.\n * @param device_attr_ex\n *   Extended device attributes to interpret.\n *\n * @return\n *   Usable RSS hash fields mask in Verbs format.\n */\nstatic uint64_t\nmlx4_hw_rss_sup(struct ibv_context *ctx, struct ibv_pd *pd,\n\t\tstruct ibv_device_attr_ex *device_attr_ex)\n{\n\tuint64_t hw_rss_sup = device_attr_ex->rss_caps.rx_hash_fields_mask;\n\tstruct ibv_cq *cq = NULL;\n\tstruct ibv_wq *wq = NULL;\n\tstruct ibv_rwq_ind_table *ind = NULL;\n\tstruct ibv_qp *qp = NULL;\n\n\tif (!hw_rss_sup) {\n\t\tWARN(\"no RSS capabilities reported; disabling support for UDP\"\n\t\t     \" RSS and inner VXLAN RSS\");\n\t\treturn IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 |\n\t\t\tIBV_RX_HASH_SRC_IPV6 | IBV_RX_HASH_DST_IPV6 |\n\t\t\tIBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP;\n\t}\n\tif (!(hw_rss_sup & IBV_RX_HASH_INNER))\n\t\treturn hw_rss_sup;\n\t/*\n\t * Although reported as supported, missing code in some Linux\n\t * versions (v4.15, v4.16) prevents the creation of hash QPs with\n\t * inner capability.\n\t *\n\t * There is no choice but to attempt to instantiate a temporary RSS\n\t * context in order to confirm its support.\n\t */\n\tcq = mlx4_glue->create_cq(ctx, 1, NULL, NULL, 0);\n\twq = cq ? mlx4_glue->create_wq\n\t\t(ctx,\n\t\t &(struct ibv_wq_init_attr){\n\t\t\t.wq_type = IBV_WQT_RQ,\n\t\t\t.max_wr = 1,\n\t\t\t.max_sge = 1,\n\t\t\t.pd = pd,\n\t\t\t.cq = cq,\n\t\t }) : NULL;\n\tind = wq ? mlx4_glue->create_rwq_ind_table\n\t\t(ctx,\n\t\t &(struct ibv_rwq_ind_table_init_attr){\n\t\t\t.log_ind_tbl_size = 0,\n\t\t\t.ind_tbl = &wq,\n\t\t\t.comp_mask = 0,\n\t\t }) : NULL;\n\tqp = ind ? mlx4_glue->create_qp_ex\n\t\t(ctx,\n\t\t &(struct ibv_qp_init_attr_ex){\n\t\t\t.comp_mask =\n\t\t\t\t(IBV_QP_INIT_ATTR_PD |\n\t\t\t\t IBV_QP_INIT_ATTR_RX_HASH |\n\t\t\t\t IBV_QP_INIT_ATTR_IND_TABLE),\n\t\t\t.qp_type = IBV_QPT_RAW_PACKET,\n\t\t\t.pd = pd,\n\t\t\t.rwq_ind_tbl = ind,\n\t\t\t.rx_hash_conf = {\n\t\t\t\t.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,\n\t\t\t\t.rx_hash_key_len = MLX4_RSS_HASH_KEY_SIZE,\n\t\t\t\t.rx_hash_key = mlx4_rss_hash_key_default,\n\t\t\t\t.rx_hash_fields_mask = hw_rss_sup,\n\t\t\t},\n\t\t }) : NULL;\n\tif (!qp) {\n\t\tWARN(\"disabling unusable inner RSS capability due to kernel\"\n\t\t     \" quirk\");\n\t\thw_rss_sup &= ~IBV_RX_HASH_INNER;\n\t} else {\n\t\tclaim_zero(mlx4_glue->destroy_qp(qp));\n\t}\n\tif (ind)\n\t\tclaim_zero(mlx4_glue->destroy_rwq_ind_table(ind));\n\tif (wq)\n\t\tclaim_zero(mlx4_glue->destroy_wq(wq));\n\tif (cq)\n\t\tclaim_zero(mlx4_glue->destroy_cq(cq));\n\treturn hw_rss_sup;\n}\n\nstatic struct rte_pci_driver mlx4_driver;\n\n/**\n * PMD global initialization.\n *\n * Independent from individual device, this function initializes global\n * per-PMD data structures distinguishing primary and secondary processes.\n * Hence, each initialization is called once per a process.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_init_once(void)\n{\n\tstruct mlx4_shared_data *sd;\n\tstruct mlx4_local_data *ld = &mlx4_local_data;\n\tint ret = 0;\n\n\tif (mlx4_init_shared_data())\n\t\treturn -rte_errno;\n\tsd = mlx4_shared_data;\n\tMLX4_ASSERT(sd);\n\trte_spinlock_lock(&sd->lock);\n\tswitch (rte_eal_process_type()) {\n\tcase RTE_PROC_PRIMARY:\n\t\tif (sd->init_done)\n\t\t\tbreak;\n\t\tLIST_INIT(&sd->mem_event_cb_list);\n\t\trte_rwlock_init(&sd->mem_event_rwlock);\n\t\trte_mem_event_callback_register(\"MLX4_MEM_EVENT_CB\",\n\t\t\t\t\t\tmlx4_mr_mem_event_cb, NULL);\n\t\tret = mlx4_mp_init_primary();\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tsd->init_done = 1;\n\t\tbreak;\n\tcase RTE_PROC_SECONDARY:\n\t\tif (ld->init_done)\n\t\t\tbreak;\n\t\tret = mlx4_mp_init_secondary();\n\t\tif (ret)\n\t\t\tgoto out;\n\t\t++sd->secondary_cnt;\n\t\tld->init_done = 1;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\nout:\n\trte_spinlock_unlock(&sd->lock);\n\treturn ret;\n}\n\n/**\n * DPDK callback to register a PCI device.\n *\n * This function creates an Ethernet device for each port of a given\n * PCI device.\n *\n * @param[in] pci_drv\n *   PCI driver structure (mlx4_driver).\n * @param[in] pci_dev\n *   PCI device information.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_pci_probe(struct rte_pci_driver *pci_drv, struct rte_pci_device *pci_dev)\n{\n\tstruct ibv_device **list;\n\tstruct ibv_device *ibv_dev;\n\tint err = 0;\n\tstruct ibv_context *attr_ctx = NULL;\n\tstruct ibv_device_attr device_attr;\n\tstruct ibv_device_attr_ex device_attr_ex;\n\tstruct mlx4_conf conf = {\n\t\t.ports.present = 0,\n\t\t.mr_ext_memseg_en = 1,\n\t};\n\tunsigned int vf;\n\tint i;\n\tchar ifname[IF_NAMESIZE];\n\n\t(void)pci_drv;\n\terr = mlx4_init_once();\n\tif (err) {\n\t\tERROR(\"unable to init PMD global data: %s\",\n\t\t      strerror(rte_errno));\n\t\treturn -rte_errno;\n\t}\n\tMLX4_ASSERT(pci_drv == &mlx4_driver);\n\tlist = mlx4_glue->get_device_list(&i);\n\tif (list == NULL) {\n\t\trte_errno = errno;\n\t\tMLX4_ASSERT(rte_errno);\n\t\tif (rte_errno == ENOSYS)\n\t\t\tERROR(\"cannot list devices, is ib_uverbs loaded?\");\n\t\treturn -rte_errno;\n\t}\n\tMLX4_ASSERT(i >= 0);\n\t/*\n\t * For each listed device, check related sysfs entry against\n\t * the provided PCI ID.\n\t */\n\twhile (i != 0) {\n\t\tstruct rte_pci_addr pci_addr;\n\n\t\t--i;\n\t\tDEBUG(\"checking device \\\"%s\\\"\", list[i]->name);\n\t\tif (mlx4_ibv_device_to_pci_addr(list[i], &pci_addr))\n\t\t\tcontinue;\n\t\tif ((pci_dev->addr.domain != pci_addr.domain) ||\n\t\t    (pci_dev->addr.bus != pci_addr.bus) ||\n\t\t    (pci_dev->addr.devid != pci_addr.devid) ||\n\t\t    (pci_dev->addr.function != pci_addr.function))\n\t\t\tcontinue;\n\t\tvf = (pci_dev->id.device_id ==\n\t\t      PCI_DEVICE_ID_MELLANOX_CONNECTX3VF);\n\t\tINFO(\"PCI information matches, using device \\\"%s\\\" (VF: %s)\",\n\t\t     list[i]->name, (vf ? \"true\" : \"false\"));\n\t\tattr_ctx = mlx4_glue->open_device(list[i]);\n\t\terr = errno;\n\t\tbreak;\n\t}\n\tif (attr_ctx == NULL) {\n\t\tmlx4_glue->free_device_list(list);\n\t\tswitch (err) {\n\t\tcase 0:\n\t\t\trte_errno = ENODEV;\n\t\t\tERROR(\"cannot access device, is mlx4_ib loaded?\");\n\t\t\treturn -rte_errno;\n\t\tcase EINVAL:\n\t\t\trte_errno = EINVAL;\n\t\t\tERROR(\"cannot use device, are drivers up to date?\");\n\t\t\treturn -rte_errno;\n\t\t}\n\t\tMLX4_ASSERT(err > 0);\n\t\trte_errno = err;\n\t\treturn -rte_errno;\n\t}\n\tibv_dev = list[i];\n\tDEBUG(\"device opened\");\n\tif (mlx4_glue->query_device(attr_ctx, &device_attr)) {\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tINFO(\"%u port(s) detected\", device_attr.phys_port_cnt);\n\tconf.ports.present |= (UINT64_C(1) << device_attr.phys_port_cnt) - 1;\n\tif (mlx4_args(pci_dev->device.devargs, &conf)) {\n\t\tERROR(\"failed to process device arguments\");\n\t\terr = EINVAL;\n\t\tgoto error;\n\t}\n\t/* Use all ports when none are defined */\n\tif (!conf.ports.enabled)\n\t\tconf.ports.enabled = conf.ports.present;\n\t/* Retrieve extended device attributes. */\n\tif (mlx4_glue->query_device_ex(attr_ctx, NULL, &device_attr_ex)) {\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tMLX4_ASSERT(device_attr.max_sge >= MLX4_MAX_SGE);\n\tfor (i = 0; i < device_attr.phys_port_cnt; i++) {\n\t\tuint32_t port = i + 1; /* ports are indexed from one */\n\t\tstruct ibv_context *ctx = NULL;\n\t\tstruct ibv_port_attr port_attr;\n\t\tstruct ibv_pd *pd = NULL;\n\t\tstruct mlx4_priv *priv = NULL;\n\t\tstruct rte_eth_dev *eth_dev = NULL;\n\t\tstruct rte_ether_addr mac;\n\t\tchar name[RTE_ETH_NAME_MAX_LEN];\n\n\t\t/* If port is not enabled, skip. */\n\t\tif (!(conf.ports.enabled & (1 << i)))\n\t\t\tcontinue;\n\t\tDEBUG(\"using port %u\", port);\n\t\tctx = mlx4_glue->open_device(ibv_dev);\n\t\tif (ctx == NULL) {\n\t\t\terr = ENODEV;\n\t\t\tgoto port_error;\n\t\t}\n\t\tsnprintf(name, sizeof(name), \"%s port %u\",\n\t\t\t mlx4_glue->get_device_name(ibv_dev), port);\n\t\tif (rte_eal_process_type() == RTE_PROC_SECONDARY) {\n\t\t\teth_dev = rte_eth_dev_attach_secondary(name);\n\t\t\tif (eth_dev == NULL) {\n\t\t\t\tERROR(\"can not attach rte ethdev\");\n\t\t\t\trte_errno = ENOMEM;\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tpriv = eth_dev->data->dev_private;\n\t\t\tif (!priv->verbs_alloc_ctx.enabled) {\n\t\t\t\tERROR(\"secondary process is not supported\"\n\t\t\t\t      \" due to lack of external allocator\"\n\t\t\t\t      \" from Verbs\");\n\t\t\t\trte_errno = ENOTSUP;\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\teth_dev->device = &pci_dev->device;\n\t\t\teth_dev->dev_ops = &mlx4_dev_sec_ops;\n\t\t\terr = mlx4_proc_priv_init(eth_dev);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t\t/* Receive command fd from primary process. */\n\t\t\terr = mlx4_mp_req_verbs_cmd_fd(eth_dev);\n\t\t\tif (err < 0) {\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/* Remap UAR for Tx queues. */\n\t\t\terr = mlx4_tx_uar_init_secondary(eth_dev, err);\n\t\t\tif (err) {\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Ethdev pointer is still required as input since\n\t\t\t * the primary device is not accessible from the\n\t\t\t * secondary process.\n\t\t\t */\n\t\t\teth_dev->tx_pkt_burst = mlx4_tx_burst;\n\t\t\teth_dev->rx_pkt_burst = mlx4_rx_burst;\n\t\t\tclaim_zero(mlx4_glue->close_device(ctx));\n\t\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\t\t\trte_eth_dev_probing_finish(eth_dev);\n\t\t\tcontinue;\n\t\t}\n\t\t/* Check port status. */\n\t\terr = mlx4_glue->query_port(ctx, port, &port_attr);\n\t\tif (err) {\n\t\t\terr = ENODEV;\n\t\t\tERROR(\"port query failed: %s\", strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\tif (port_attr.link_layer != IBV_LINK_LAYER_ETHERNET) {\n\t\t\terr = ENOTSUP;\n\t\t\tERROR(\"port %d is not configured in Ethernet mode\",\n\t\t\t      port);\n\t\t\tgoto port_error;\n\t\t}\n\t\tif (port_attr.state != IBV_PORT_ACTIVE)\n\t\t\tDEBUG(\"port %d is not active: \\\"%s\\\" (%d)\",\n\t\t\t      port, mlx4_glue->port_state_str(port_attr.state),\n\t\t\t      port_attr.state);\n\t\t/* Make asynchronous FD non-blocking to handle interrupts. */\n\t\terr = mlx4_fd_set_non_blocking(ctx->async_fd);\n\t\tif (err) {\n\t\t\tERROR(\"cannot make asynchronous FD non-blocking: %s\",\n\t\t\t      strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* Allocate protection domain. */\n\t\tpd = mlx4_glue->alloc_pd(ctx);\n\t\tif (pd == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"PD allocation failure\");\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* from rte_ethdev.c */\n\t\tpriv = rte_zmalloc(\"ethdev private structure\",\n\t\t\t\t   sizeof(*priv),\n\t\t\t\t   RTE_CACHE_LINE_SIZE);\n\t\tif (priv == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"priv allocation failure\");\n\t\t\tgoto port_error;\n\t\t}\n\t\tpriv->ctx = ctx;\n\t\tpriv->device_attr = device_attr;\n\t\tpriv->port = port;\n\t\tpriv->pd = pd;\n\t\tpriv->mtu = RTE_ETHER_MTU;\n\t\tpriv->vf = vf;\n\t\tpriv->hw_csum =\t!!(device_attr.device_cap_flags &\n\t\t\t\t   IBV_DEVICE_RAW_IP_CSUM);\n\t\tDEBUG(\"checksum offloading is %ssupported\",\n\t\t      (priv->hw_csum ? \"\" : \"not \"));\n\t\t/* Only ConnectX-3 Pro supports tunneling. */\n\t\tpriv->hw_csum_l2tun =\n\t\t\tpriv->hw_csum &&\n\t\t\t(device_attr.vendor_part_id ==\n\t\t\t PCI_DEVICE_ID_MELLANOX_CONNECTX3PRO);\n\t\tDEBUG(\"L2 tunnel checksum offloads are %ssupported\",\n\t\t      priv->hw_csum_l2tun ? \"\" : \"not \");\n\t\tpriv->hw_rss_sup = mlx4_hw_rss_sup(priv->ctx, priv->pd,\n\t\t\t\t\t\t   &device_attr_ex);\n\t\tDEBUG(\"supported RSS hash fields mask: %016\" PRIx64,\n\t\t      priv->hw_rss_sup);\n\t\tpriv->hw_rss_max_qps =\n\t\t\tdevice_attr_ex.rss_caps.max_rwq_indirection_table_size;\n\t\tDEBUG(\"MAX RSS queues %d\", priv->hw_rss_max_qps);\n\t\tpriv->hw_fcs_strip = !!(device_attr_ex.raw_packet_caps &\n\t\t\t\t\tIBV_RAW_PACKET_CAP_SCATTER_FCS);\n\t\tDEBUG(\"FCS stripping toggling is %ssupported\",\n\t\t      priv->hw_fcs_strip ? \"\" : \"not \");\n\t\tpriv->tso =\n\t\t\t((device_attr_ex.tso_caps.max_tso > 0) &&\n\t\t\t (device_attr_ex.tso_caps.supported_qpts &\n\t\t\t  (1 << IBV_QPT_RAW_PACKET)));\n\t\tif (priv->tso)\n\t\t\tpriv->tso_max_payload_sz =\n\t\t\t\t\tdevice_attr_ex.tso_caps.max_tso;\n\t\tDEBUG(\"TSO is %ssupported\",\n\t\t      priv->tso ? \"\" : \"not \");\n\t\tpriv->mr_ext_memseg_en = conf.mr_ext_memseg_en;\n\t\t/* Configure the first MAC address by default. */\n\t\terr = mlx4_get_mac(priv, &mac.addr_bytes);\n\t\tif (err) {\n\t\t\tERROR(\"cannot get MAC address, is mlx4_en loaded?\"\n\t\t\t      \" (error: %s)\", strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\tINFO(\"port %u MAC address is %02x:%02x:%02x:%02x:%02x:%02x\",\n\t\t     priv->port,\n\t\t     mac.addr_bytes[0], mac.addr_bytes[1],\n\t\t     mac.addr_bytes[2], mac.addr_bytes[3],\n\t\t     mac.addr_bytes[4], mac.addr_bytes[5]);\n\t\t/* Register MAC address. */\n\t\tpriv->mac[0] = mac;\n\n\t\tif (mlx4_get_ifname(priv, &ifname) == 0) {\n\t\t\tDEBUG(\"port %u ifname is \\\"%s\\\"\",\n\t\t\t      priv->port, ifname);\n\t\t\tpriv->if_index = if_nametoindex(ifname);\n\t\t} else {\n\t\t\tDEBUG(\"port %u ifname is unknown\", priv->port);\n\t\t}\n\n\t\t/* Get actual MTU if possible. */\n\t\tmlx4_mtu_get(priv, &priv->mtu);\n\t\tDEBUG(\"port %u MTU is %u\", priv->port, priv->mtu);\n\t\teth_dev = rte_eth_dev_allocate(name);\n\t\tif (eth_dev == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"can not allocate rte ethdev\");\n\t\t\tgoto port_error;\n\t\t}\n\t\teth_dev->data->dev_private = priv;\n\t\teth_dev->data->mac_addrs = priv->mac;\n\t\teth_dev->device = &pci_dev->device;\n\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\t\t/* Initialize local interrupt handle for current port. */\n\t\tpriv->intr_handle = (struct rte_intr_handle){\n\t\t\t.fd = -1,\n\t\t\t.type = RTE_INTR_HANDLE_EXT,\n\t\t};\n\t\t/*\n\t\t * Override ethdev interrupt handle pointer with private\n\t\t * handle instead of that of the parent PCI device used by\n\t\t * default. This prevents it from being shared between all\n\t\t * ports of the same PCI device since each of them is\n\t\t * associated its own Verbs context.\n\t\t *\n\t\t * Rx interrupts in particular require this as the PMD has\n\t\t * no control over the registration of queue interrupts\n\t\t * besides setting up eth_dev->intr_handle, the rest is\n\t\t * handled by rte_intr_rx_ctl().\n\t\t */\n\t\teth_dev->intr_handle = &priv->intr_handle;\n\t\tpriv->dev_data = eth_dev->data;\n\t\teth_dev->dev_ops = &mlx4_dev_ops;\n#ifdef HAVE_IBV_MLX4_BUF_ALLOCATORS\n\t\t/* Hint libmlx4 to use PMD allocator for data plane resources */\n\t\tstruct mlx4dv_ctx_allocators alctr = {\n\t\t\t.alloc = &mlx4_alloc_verbs_buf,\n\t\t\t.free = &mlx4_free_verbs_buf,\n\t\t\t.data = priv,\n\t\t};\n\t\terr = mlx4_glue->dv_set_context_attr\n\t\t\t(ctx, MLX4DV_SET_CTX_ATTR_BUF_ALLOCATORS,\n\t\t\t (void *)((uintptr_t)&alctr));\n\t\tif (err)\n\t\t\tWARN(\"Verbs external allocator is not supported\");\n\t\telse\n\t\t\tpriv->verbs_alloc_ctx.enabled = 1;\n#endif\n\t\t/* Bring Ethernet device up. */\n\t\tDEBUG(\"forcing Ethernet interface up\");\n\t\tmlx4_dev_set_link_up(eth_dev);\n\t\t/* Update link status once if waiting for LSC. */\n\t\tif (eth_dev->data->dev_flags & RTE_ETH_DEV_INTR_LSC)\n\t\t\tmlx4_link_update(eth_dev, 0);\n\t\t/*\n\t\t * Once the device is added to the list of memory event\n\t\t * callback, its global MR cache table cannot be expanded\n\t\t * on the fly because of deadlock. If it overflows, lookup\n\t\t * should be done by searching MR list linearly, which is slow.\n\t\t */\n\t\terr = mlx4_mr_btree_init(&priv->mr.cache,\n\t\t\t\t\t MLX4_MR_BTREE_CACHE_N * 2,\n\t\t\t\t\t eth_dev->device->numa_node);\n\t\tif (err) {\n\t\t\t/* rte_errno is already set. */\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* Add device to memory callback list. */\n\t\trte_rwlock_write_lock(&mlx4_shared_data->mem_event_rwlock);\n\t\tLIST_INSERT_HEAD(&mlx4_shared_data->mem_event_cb_list,\n\t\t\t\t priv, mem_event_cb);\n\t\trte_rwlock_write_unlock(&mlx4_shared_data->mem_event_rwlock);\n\t\trte_eth_dev_probing_finish(eth_dev);\n\t\tcontinue;\nport_error:\n\t\trte_free(priv);\n\t\tif (eth_dev != NULL)\n\t\t\teth_dev->data->dev_private = NULL;\n\t\tif (pd)\n\t\t\tclaim_zero(mlx4_glue->dealloc_pd(pd));\n\t\tif (ctx)\n\t\t\tclaim_zero(mlx4_glue->close_device(ctx));\n\t\tif (eth_dev != NULL) {\n\t\t\t/* mac_addrs must not be freed because part of dev_private */\n\t\t\teth_dev->data->mac_addrs = NULL;\n\t\t\trte_eth_dev_release_port(eth_dev);\n\t\t}\n\t\tbreak;\n\t}\n\t/*\n\t * XXX if something went wrong in the loop above, there is a resource\n\t * leak (ctx, pd, priv, dpdk ethdev) but we can do nothing about it as\n\t * long as the dpdk does not provide a way to deallocate a ethdev and a\n\t * way to enumerate the registered ethdevs to free the previous ones.\n\t */\nerror:\n\tif (attr_ctx)\n\t\tclaim_zero(mlx4_glue->close_device(attr_ctx));\n\tif (list)\n\t\tmlx4_glue->free_device_list(list);\n\tif (err)\n\t\trte_errno = err;\n\treturn -err;\n}\n\nstatic const struct rte_pci_id mlx4_pci_id_map[] = {\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3PRO)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3VF)\n\t},\n\t{\n\t\t.vendor_id = 0\n\t}\n};\n\nstatic struct rte_pci_driver mlx4_driver = {\n\t.driver = {\n\t\t.name = MLX4_DRIVER_NAME\n\t},\n\t.id_table = mlx4_pci_id_map,\n\t.probe = mlx4_pci_probe,\n\t.drv_flags = RTE_PCI_DRV_INTR_LSC | RTE_PCI_DRV_INTR_RMV,\n};\n\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\n/**\n * Suffix RTE_EAL_PMD_PATH with \"-glue\".\n *\n * This function performs a sanity check on RTE_EAL_PMD_PATH before\n * suffixing its last component.\n *\n * @param buf[out]\n *   Output buffer, should be large enough otherwise NULL is returned.\n * @param size\n *   Size of @p out.\n *\n * @return\n *   Pointer to @p buf or @p NULL in case suffix cannot be appended.\n */\nstatic char *\nmlx4_glue_path(char *buf, size_t size)\n{\n\tstatic const char *const bad[] = { \"/\", \".\", \"..\", NULL };\n\tconst char *path = RTE_EAL_PMD_PATH;\n\tsize_t len = strlen(path);\n\tsize_t off;\n\tint i;\n\n\twhile (len && path[len - 1] == '/')\n\t\t--len;\n\tfor (off = len; off && path[off - 1] != '/'; --off)\n\t\t;\n\tfor (i = 0; bad[i]; ++i)\n\t\tif (!strncmp(path + off, bad[i], (int)(len - off)))\n\t\t\tgoto error;\n\ti = snprintf(buf, size, \"%.*s-glue\", (int)len, path);\n\tif (i == -1 || (size_t)i >= size)\n\t\tgoto error;\n\treturn buf;\nerror:\n\tERROR(\"unable to append \\\"-glue\\\" to last component of\"\n\t      \" RTE_EAL_PMD_PATH (\\\"\" RTE_EAL_PMD_PATH \"\\\"),\"\n\t      \" please re-configure DPDK\");\n\treturn NULL;\n}\n\n/**\n * Initialization routine for run-time dependency on rdma-core.\n */\nstatic int\nmlx4_glue_init(void)\n{\n\tchar glue_path[sizeof(RTE_EAL_PMD_PATH) - 1 + sizeof(\"-glue\")];\n\tconst char *path[] = {\n\t\t/*\n\t\t * A basic security check is necessary before trusting\n\t\t * MLX4_GLUE_PATH, which may override RTE_EAL_PMD_PATH.\n\t\t */\n\t\t(geteuid() == getuid() && getegid() == getgid() ?\n\t\t getenv(\"MLX4_GLUE_PATH\") : NULL),\n\t\t/*\n\t\t * When RTE_EAL_PMD_PATH is set, use its glue-suffixed\n\t\t * variant, otherwise let dlopen() look up libraries on its\n\t\t * own.\n\t\t */\n\t\t(*RTE_EAL_PMD_PATH ?\n\t\t mlx4_glue_path(glue_path, sizeof(glue_path)) : \"\"),\n\t};\n\tunsigned int i = 0;\n\tvoid *handle = NULL;\n\tvoid **sym;\n\tconst char *dlmsg;\n\n\twhile (!handle && i != RTE_DIM(path)) {\n\t\tconst char *end;\n\t\tsize_t len;\n\t\tint ret;\n\n\t\tif (!path[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\tend = strpbrk(path[i], \":;\");\n\t\tif (!end)\n\t\t\tend = path[i] + strlen(path[i]);\n\t\tlen = end - path[i];\n\t\tret = 0;\n\t\tdo {\n\t\t\tchar name[ret + 1];\n\n\t\t\tret = snprintf(name, sizeof(name), \"%.*s%s\" MLX4_GLUE,\n\t\t\t\t       (int)len, path[i],\n\t\t\t\t       (!len || *(end - 1) == '/') ? \"\" : \"/\");\n\t\t\tif (ret == -1)\n\t\t\t\tbreak;\n\t\t\tif (sizeof(name) != (size_t)ret + 1)\n\t\t\t\tcontinue;\n\t\t\tDEBUG(\"looking for rdma-core glue as \\\"%s\\\"\", name);\n\t\t\thandle = dlopen(name, RTLD_LAZY);\n\t\t\tbreak;\n\t\t} while (1);\n\t\tpath[i] = end + 1;\n\t\tif (!*end)\n\t\t\t++i;\n\t}\n\tif (!handle) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tWARN(\"cannot load glue library: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tsym = dlsym(handle, \"mlx4_glue\");\n\tif (!sym || !*sym) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tERROR(\"cannot resolve glue symbol: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tmlx4_glue = *sym;\n\treturn 0;\nglue_error:\n\tif (handle)\n\t\tdlclose(handle);\n\tWARN(\"cannot initialize PMD due to missing run-time\"\n\t     \" dependency on rdma-core libraries (libibverbs,\"\n\t     \" libmlx4)\");\n\treturn -rte_errno;\n}\n\n#endif\n\n/**\n * Driver initialization routine.\n */\nRTE_INIT(rte_mlx4_pmd_init)\n{\n\t/* Initialize driver log type. */\n\tmlx4_logtype = rte_log_register(\"pmd.net.mlx4\");\n\tif (mlx4_logtype >= 0)\n\t\trte_log_set_level(mlx4_logtype, RTE_LOG_NOTICE);\n\n\t/*\n\t * MLX4_DEVICE_FATAL_CLEANUP tells ibv_destroy functions we\n\t * want to get success errno value in case of calling them\n\t * when the device was removed.\n\t */\n\tsetenv(\"MLX4_DEVICE_FATAL_CLEANUP\", \"1\", 1);\n\t/*\n\t * RDMAV_HUGEPAGES_SAFE tells ibv_fork_init() we intend to use\n\t * huge pages. Calling ibv_fork_init() during init allows\n\t * applications to use fork() safely for purposes other than\n\t * using this PMD, which is not supported in forked processes.\n\t */\n\tsetenv(\"RDMAV_HUGEPAGES_SAFE\", \"1\", 1);\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\tif (mlx4_glue_init())\n\t\treturn;\n\tMLX4_ASSERT(mlx4_glue);\n#endif\n#ifdef RTE_LIBRTE_MLX4_DEBUG\n\t/* Glue structure must not contain any NULL pointers. */\n\t{\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i != sizeof(*mlx4_glue) / sizeof(void *); ++i)\n\t\t\tMLX4_ASSERT(((const void *const *)mlx4_glue)[i]);\n\t}\n#endif\n\tif (strcmp(mlx4_glue->version, MLX4_GLUE_VERSION)) {\n\t\tERROR(\"rdma-core glue \\\"%s\\\" mismatch: \\\"%s\\\" is required\",\n\t\t      mlx4_glue->version, MLX4_GLUE_VERSION);\n\t\treturn;\n\t}\n\tmlx4_glue->fork_init();\n\trte_pci_register(&mlx4_driver);\n}\n\nRTE_PMD_EXPORT_NAME(net_mlx4, __COUNTER__);\nRTE_PMD_REGISTER_PCI_TABLE(net_mlx4, mlx4_pci_id_map);\nRTE_PMD_REGISTER_KMOD_DEP(net_mlx4,\n\t\"* ib_uverbs & mlx4_en & mlx4_core & mlx4_ib\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/drivers/net/ark/ark_ethdev.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright (c) 2015-2018 Atomic Rules LLC\n */\n\n#include <unistd.h>\n#include <sys/stat.h>\n#include <dlfcn.h>\n\n#include <rte_bus_pci.h>\n#include <rte_ethdev_pci.h>\n#include <rte_kvargs.h>\n\n#include \"ark_global.h\"\n#include \"ark_logs.h\"\n#include \"ark_ethdev_tx.h\"\n#include \"ark_ethdev_rx.h\"\n#include \"ark_mpu.h\"\n#include \"ark_ddm.h\"\n#include \"ark_udm.h\"\n#include \"ark_rqp.h\"\n#include \"ark_pktdir.h\"\n#include \"ark_pktgen.h\"\n#include \"ark_pktchkr.h\"\n\n/*  Internal prototypes */\nstatic int eth_ark_check_args(struct ark_adapter *ark, const char *params);\nstatic int eth_ark_dev_init(struct rte_eth_dev *dev);\nstatic int ark_config_device(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_uninit(struct rte_eth_dev *eth_dev);\nstatic int eth_ark_dev_configure(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_start(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_stop(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_close(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_info_get(struct rte_eth_dev *dev,\n\t\t\t\tstruct rte_eth_dev_info *dev_info);\nstatic int eth_ark_dev_link_update(struct rte_eth_dev *dev,\n\t\t\t\t   int wait_to_complete);\nstatic int eth_ark_dev_set_link_up(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_set_link_down(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_stats_get(struct rte_eth_dev *dev,\n\t\t\t\t  struct rte_eth_stats *stats);\nstatic int eth_ark_dev_stats_reset(struct rte_eth_dev *dev);\nstatic int eth_ark_set_default_mac_addr(struct rte_eth_dev *dev,\n\t\t\t\t\t struct rte_ether_addr *mac_addr);\nstatic int eth_ark_macaddr_add(struct rte_eth_dev *dev,\n\t\t\t       struct rte_ether_addr *mac_addr,\n\t\t\t       uint32_t index,\n\t\t\t       uint32_t pool);\nstatic void eth_ark_macaddr_remove(struct rte_eth_dev *dev,\n\t\t\t\t   uint32_t index);\nstatic int  eth_ark_set_mtu(struct rte_eth_dev *dev, uint16_t size);\n\n/*\n * The packet generator is a functional block used to generate packet\n * patterns for testing.  It is not intended for nominal use.\n */\n#define ARK_PKTGEN_ARG \"Pkt_gen\"\n\n/*\n * The packet checker is a functional block used to verify packet\n * patterns for testing.  It is not intended for nominal use.\n */\n#define ARK_PKTCHKR_ARG \"Pkt_chkr\"\n\n/*\n * The packet director is used to select the internal ingress and\n * egress packets paths during testing.  It is not intended for\n * nominal use.\n */\n#define ARK_PKTDIR_ARG \"Pkt_dir\"\n\n/* Devinfo configurations */\n#define ARK_RX_MAX_QUEUE (4096 * 4)\n#define ARK_RX_MIN_QUEUE (512)\n#define ARK_RX_MAX_PKT_LEN ((16 * 1024) - 128)\n#define ARK_RX_MIN_BUFSIZE (1024)\n\n#define ARK_TX_MAX_QUEUE (4096 * 4)\n#define ARK_TX_MIN_QUEUE (256)\n\nint ark_logtype;\n\nstatic const char * const valid_arguments[] = {\n\tARK_PKTGEN_ARG,\n\tARK_PKTCHKR_ARG,\n\tARK_PKTDIR_ARG,\n\tNULL\n};\n\nstatic const struct rte_pci_id pci_id_ark_map[] = {\n\t{RTE_PCI_DEVICE(0x1d6c, 0x100d)},\n\t{RTE_PCI_DEVICE(0x1d6c, 0x100e)},\n\t{.vendor_id = 0, /* sentinel */ },\n};\n\nstatic int\neth_ark_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,\n\t\tstruct rte_pci_device *pci_dev)\n{\n\tstruct rte_eth_dev *eth_dev;\n\tint ret;\n\n\teth_dev = rte_eth_dev_pci_allocate(pci_dev, sizeof(struct ark_adapter));\n\n\tif (eth_dev == NULL)\n\t\treturn -ENOMEM;\n\n\tret = eth_ark_dev_init(eth_dev);\n\tif (ret)\n\t\trte_eth_dev_pci_release(eth_dev);\n\n\treturn ret;\n}\n\nstatic int\neth_ark_pci_remove(struct rte_pci_device *pci_dev)\n{\n\treturn rte_eth_dev_pci_generic_remove(pci_dev, eth_ark_dev_uninit);\n}\n\nstatic struct rte_pci_driver rte_ark_pmd = {\n\t.id_table = pci_id_ark_map,\n\t.drv_flags = RTE_PCI_DRV_NEED_MAPPING | RTE_PCI_DRV_INTR_LSC,\n\t.probe = eth_ark_pci_probe,\n\t.remove = eth_ark_pci_remove,\n};\n\nstatic const struct eth_dev_ops ark_eth_dev_ops = {\n\t.dev_configure = eth_ark_dev_configure,\n\t.dev_start = eth_ark_dev_start,\n\t.dev_stop = eth_ark_dev_stop,\n\t.dev_close = eth_ark_dev_close,\n\n\t.dev_infos_get = eth_ark_dev_info_get,\n\n\t.rx_queue_setup = eth_ark_dev_rx_queue_setup,\n\t.rx_queue_count = eth_ark_dev_rx_queue_count,\n\t.tx_queue_setup = eth_ark_tx_queue_setup,\n\n\t.link_update = eth_ark_dev_link_update,\n\t.dev_set_link_up = eth_ark_dev_set_link_up,\n\t.dev_set_link_down = eth_ark_dev_set_link_down,\n\n\t.rx_queue_start = eth_ark_rx_start_queue,\n\t.rx_queue_stop = eth_ark_rx_stop_queue,\n\n\t.tx_queue_start = eth_ark_tx_queue_start,\n\t.tx_queue_stop = eth_ark_tx_queue_stop,\n\n\t.stats_get = eth_ark_dev_stats_get,\n\t.stats_reset = eth_ark_dev_stats_reset,\n\n\t.mac_addr_add = eth_ark_macaddr_add,\n\t.mac_addr_remove = eth_ark_macaddr_remove,\n\t.mac_addr_set = eth_ark_set_default_mac_addr,\n\n\t.mtu_set = eth_ark_set_mtu,\n};\n\nstatic int\ncheck_for_ext(struct ark_adapter *ark)\n{\n\tint found = 0;\n\n\t/* Get the env */\n\tconst char *dllpath = getenv(\"ARK_EXT_PATH\");\n\n\tif (dllpath == NULL) {\n\t\tPMD_DEBUG_LOG(DEBUG, \"ARK EXT NO dll path specified\\n\");\n\t\treturn 0;\n\t}\n\tPMD_DRV_LOG(INFO, \"ARK EXT found dll path at %s\\n\", dllpath);\n\n\t/* Open and load the .so */\n\tark->d_handle = dlopen(dllpath, RTLD_LOCAL | RTLD_LAZY);\n\tif (ark->d_handle == NULL) {\n\t\tPMD_DRV_LOG(ERR, \"Could not load user extension %s\\n\",\n\t\t\t    dllpath);\n\t\treturn -1;\n\t}\n\tPMD_DRV_LOG(INFO, \"SUCCESS: loaded user extension %s\\n\",\n\t\t\t    dllpath);\n\n\t/* Get the entry points */\n\tark->user_ext.dev_init =\n\t\t(void *(*)(struct rte_eth_dev *, void *, int))\n\t\tdlsym(ark->d_handle, \"dev_init\");\n\tPMD_DEBUG_LOG(DEBUG, \"device ext init pointer = %p\\n\",\n\t\t      ark->user_ext.dev_init);\n\tark->user_ext.dev_get_port_count =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_get_port_count\");\n\tark->user_ext.dev_uninit =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_uninit\");\n\tark->user_ext.dev_configure =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_configure\");\n\tark->user_ext.dev_start =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_start\");\n\tark->user_ext.dev_stop =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_stop\");\n\tark->user_ext.dev_close =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_close\");\n\tark->user_ext.link_update =\n\t\t(int (*)(struct rte_eth_dev *, int, void *))\n\t\tdlsym(ark->d_handle, \"link_update\");\n\tark->user_ext.dev_set_link_up =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_set_link_up\");\n\tark->user_ext.dev_set_link_down =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_set_link_down\");\n\tark->user_ext.stats_get =\n\t\t(int (*)(struct rte_eth_dev *, struct rte_eth_stats *,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"stats_get\");\n\tark->user_ext.stats_reset =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"stats_reset\");\n\tark->user_ext.mac_addr_add =\n\t\t(void (*)(struct rte_eth_dev *, struct rte_ether_addr *,\n\t\t\tuint32_t, uint32_t, void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_add\");\n\tark->user_ext.mac_addr_remove =\n\t\t(void (*)(struct rte_eth_dev *, uint32_t, void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_remove\");\n\tark->user_ext.mac_addr_set =\n\t\t(void (*)(struct rte_eth_dev *, struct rte_ether_addr *,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_set\");\n\tark->user_ext.set_mtu =\n\t\t(int (*)(struct rte_eth_dev *, uint16_t,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"set_mtu\");\n\n\treturn found;\n}\n\nstatic int\neth_ark_dev_init(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\tstruct rte_pci_device *pci_dev;\n\tint ret;\n\tint port_count = 1;\n\tint p;\n\n\tark->eth_dev = dev;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\t/* Check to see if there is an extension that we need to load */\n\tret = check_for_ext(ark);\n\tif (ret)\n\t\treturn ret;\n\tpci_dev = RTE_ETH_DEV_TO_PCI(dev);\n\trte_eth_copy_pci_info(dev, pci_dev);\n\n\t/* Use dummy function until setup */\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts_noop;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts_noop;\n\t/* Let rte_eth_dev_close() release the port resources */\n\tdev->data->dev_flags |= RTE_ETH_DEV_CLOSE_REMOVE;\n\n\tark->bar0 = (uint8_t *)pci_dev->mem_resource[0].addr;\n\tark->a_bar = (uint8_t *)pci_dev->mem_resource[2].addr;\n\n\tark->sysctrl.v  = (void *)&ark->bar0[ARK_SYSCTRL_BASE];\n\tark->mpurx.v  = (void *)&ark->bar0[ARK_MPU_RX_BASE];\n\tark->udm.v  = (void *)&ark->bar0[ARK_UDM_BASE];\n\tark->mputx.v  = (void *)&ark->bar0[ARK_MPU_TX_BASE];\n\tark->ddm.v  = (void *)&ark->bar0[ARK_DDM_BASE];\n\tark->cmac.v  = (void *)&ark->bar0[ARK_CMAC_BASE];\n\tark->external.v  = (void *)&ark->bar0[ARK_EXTERNAL_BASE];\n\tark->pktdir.v  = (void *)&ark->bar0[ARK_PKTDIR_BASE];\n\tark->pktgen.v  = (void *)&ark->bar0[ARK_PKTGEN_BASE];\n\tark->pktchkr.v  = (void *)&ark->bar0[ARK_PKTCHKR_BASE];\n\n\tark->rqpacing =\n\t\t(struct ark_rqpace_t *)(ark->bar0 + ARK_RCPACING_BASE);\n\tark->started = 0;\n\n\tPMD_DEBUG_LOG(INFO, \"Sys Ctrl Const = 0x%x  HW Commit_ID: %08x\\n\",\n\t\t      ark->sysctrl.t32[4],\n\t\t      rte_be_to_cpu_32(ark->sysctrl.t32[0x20 / 4]));\n\tPMD_DRV_LOG(INFO, \"Arkville HW Commit_ID: %08x\\n\",\n\t\t    rte_be_to_cpu_32(ark->sysctrl.t32[0x20 / 4]));\n\n\t/* If HW sanity test fails, return an error */\n\tif (ark->sysctrl.t32[4] != 0xcafef00d) {\n\t\tPMD_DRV_LOG(ERR,\n\t\t\t    \"HW Sanity test has failed, expected constant\"\n\t\t\t    \" 0x%x, read 0x%x (%s)\\n\",\n\t\t\t    0xcafef00d,\n\t\t\t    ark->sysctrl.t32[4], __func__);\n\t\treturn -1;\n\t}\n\tif (ark->sysctrl.t32[3] != 0) {\n\t\tif (ark_rqp_lasped(ark->rqpacing)) {\n\t\t\tPMD_DRV_LOG(ERR, \"Arkville Evaluation System - \"\n\t\t\t\t    \"Timer has Expired\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tPMD_DRV_LOG(WARNING, \"Arkville Evaluation System - \"\n\t\t\t    \"Timer is Running\\n\");\n\t}\n\n\tPMD_DRV_LOG(INFO,\n\t\t    \"HW Sanity test has PASSED, expected constant\"\n\t\t    \" 0x%x, read 0x%x (%s)\\n\",\n\t\t    0xcafef00d, ark->sysctrl.t32[4], __func__);\n\n\t/* We are a single function multi-port device. */\n\tret = ark_config_device(dev);\n\tif (ret)\n\t\treturn -1;\n\n\tdev->dev_ops = &ark_eth_dev_ops;\n\n\tdev->data->mac_addrs = rte_zmalloc(\"ark\", RTE_ETHER_ADDR_LEN, 0);\n\tif (!dev->data->mac_addrs) {\n\t\tPMD_DRV_LOG(ERR,\n\t\t\t    \"Failed to allocated memory for storing mac address\"\n\t\t\t    );\n\t}\n\n\tif (ark->user_ext.dev_init) {\n\t\tark->user_data[dev->data->port_id] =\n\t\t\tark->user_ext.dev_init(dev, ark->a_bar, 0);\n\t\tif (!ark->user_data[dev->data->port_id]) {\n\t\t\tPMD_DRV_LOG(INFO,\n\t\t\t\t    \"Failed to initialize PMD extension!\"\n\t\t\t\t    \" continuing without it\\n\");\n\t\t\tmemset(&ark->user_ext, 0, sizeof(struct ark_user_ext));\n\t\t\tdlclose(ark->d_handle);\n\t\t}\n\t}\n\n\tif (pci_dev->device.devargs)\n\t\tret = eth_ark_check_args(ark, pci_dev->device.devargs->args);\n\telse\n\t\tPMD_DRV_LOG(INFO, \"No Device args found\\n\");\n\n\tif (ret)\n\t\tgoto error;\n\t/*\n\t * We will create additional devices based on the number of requested\n\t * ports\n\t */\n\tif (ark->user_ext.dev_get_port_count)\n\t\tport_count =\n\t\t\tark->user_ext.dev_get_port_count(dev,\n\t\t\t\t ark->user_data[dev->data->port_id]);\n\tark->num_ports = port_count;\n\n\tfor (p = 0; p < port_count; p++) {\n\t\tstruct rte_eth_dev *eth_dev;\n\t\tchar name[RTE_ETH_NAME_MAX_LEN];\n\n\t\tsnprintf(name, sizeof(name), \"arketh%d\",\n\t\t\t dev->data->port_id + p);\n\n\t\tif (p == 0) {\n\t\t\t/* First port is already allocated by DPDK */\n\t\t\teth_dev = ark->eth_dev;\n\t\t\trte_eth_dev_probing_finish(eth_dev);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* reserve an ethdev entry */\n\t\teth_dev = rte_eth_dev_allocate(name);\n\t\tif (!eth_dev) {\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"Could not allocate eth_dev for port %d\\n\",\n\t\t\t\t    p);\n\t\t\tgoto error;\n\t\t}\n\n\t\teth_dev->device = &pci_dev->device;\n\t\teth_dev->data->dev_private = ark;\n\t\teth_dev->dev_ops = ark->eth_dev->dev_ops;\n\t\teth_dev->tx_pkt_burst = ark->eth_dev->tx_pkt_burst;\n\t\teth_dev->rx_pkt_burst = ark->eth_dev->rx_pkt_burst;\n\n\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\n\t\teth_dev->data->mac_addrs = rte_zmalloc(name,\n\t\t\t\t\t\tRTE_ETHER_ADDR_LEN, 0);\n\t\tif (!eth_dev->data->mac_addrs) {\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"Memory allocation for MAC failed!\"\n\t\t\t\t    \" Exiting.\\n\");\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (ark->user_ext.dev_init) {\n\t\t\tark->user_data[eth_dev->data->port_id] =\n\t\t\t\tark->user_ext.dev_init(dev, ark->a_bar, p);\n\t\t}\n\n\t\trte_eth_dev_probing_finish(eth_dev);\n\t}\n\n\treturn ret;\n\nerror:\n\trte_free(dev->data->mac_addrs);\n\tdev->data->mac_addrs = NULL;\n\treturn -1;\n}\n\n/*\n *Initial device configuration when device is opened\n * setup the DDM, and UDM\n * Called once per PCIE device\n */\nstatic int\nark_config_device(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\tuint16_t num_q, i;\n\tstruct ark_mpu_t *mpu;\n\n\t/*\n\t * Make sure that the packet director, generator and checker are in a\n\t * known state\n\t */\n\tark->start_pg = 0;\n\tark->pg = ark_pktgen_init(ark->pktgen.v, 0, 1);\n\tif (ark->pg == NULL)\n\t\treturn -1;\n\tark_pktgen_reset(ark->pg);\n\tark->pc = ark_pktchkr_init(ark->pktchkr.v, 0, 1);\n\tif (ark->pc == NULL)\n\t\treturn -1;\n\tark_pktchkr_stop(ark->pc);\n\tark->pd = ark_pktdir_init(ark->pktdir.v);\n\tif (ark->pd == NULL)\n\t\treturn -1;\n\n\t/* Verify HW */\n\tif (ark_udm_verify(ark->udm.v))\n\t\treturn -1;\n\tif (ark_ddm_verify(ark->ddm.v))\n\t\treturn -1;\n\n\t/* UDM */\n\tif (ark_udm_reset(ark->udm.v)) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to stop and reset UDM\\n\");\n\t\treturn -1;\n\t}\n\t/* Keep in reset until the MPU are cleared */\n\n\t/* MPU reset */\n\tmpu = ark->mpurx.v;\n\tnum_q = ark_api_num_queues(mpu);\n\tark->rx_queues = num_q;\n\tfor (i = 0; i < num_q; i++) {\n\t\tark_mpu_reset(mpu);\n\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t}\n\n\tark_udm_stop(ark->udm.v, 0);\n\tark_udm_configure(ark->udm.v,\n\t\t\t  RTE_PKTMBUF_HEADROOM,\n\t\t\t  RTE_MBUF_DEFAULT_DATAROOM,\n\t\t\t  ARK_RX_WRITE_TIME_NS);\n\tark_udm_stats_reset(ark->udm.v);\n\tark_udm_stop(ark->udm.v, 0);\n\n\t/* TX -- DDM */\n\tif (ark_ddm_stop(ark->ddm.v, 1))\n\t\tPMD_DRV_LOG(ERR, \"Unable to stop DDM\\n\");\n\n\tmpu = ark->mputx.v;\n\tnum_q = ark_api_num_queues(mpu);\n\tark->tx_queues = num_q;\n\tfor (i = 0; i < num_q; i++) {\n\t\tark_mpu_reset(mpu);\n\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t}\n\n\tark_ddm_reset(ark->ddm.v);\n\tark_ddm_stats_reset(ark->ddm.v);\n\n\tark_ddm_stop(ark->ddm.v, 0);\n\tark_rqp_stats_reset(ark->rqpacing);\n\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_uninit(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (rte_eal_process_type() != RTE_PROC_PRIMARY)\n\t\treturn 0;\n\n\tif (ark->user_ext.dev_uninit)\n\t\tark->user_ext.dev_uninit(dev,\n\t\t\t ark->user_data[dev->data->port_id]);\n\n\tark_pktgen_uninit(ark->pg);\n\tark_pktchkr_uninit(ark->pc);\n\n\tdev->dev_ops = NULL;\n\tdev->rx_pkt_burst = NULL;\n\tdev->tx_pkt_burst = NULL;\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_configure(struct rte_eth_dev *dev)\n{\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\teth_ark_dev_set_link_up(dev);\n\tif (ark->user_ext.dev_configure)\n\t\treturn ark->user_ext.dev_configure(dev,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic void *\ndelay_pg_start(void *arg)\n{\n\tstruct ark_adapter *ark = (struct ark_adapter *)arg;\n\n\t/* This function is used exclusively for regression testing, We\n\t * perform a blind sleep here to ensure that the external test\n\t * application has time to setup the test before we generate packets\n\t */\n\tusleep(100000);\n\tark_pktgen_run(ark->pg);\n\treturn NULL;\n}\n\nstatic int\neth_ark_dev_start(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\tint i;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\t/* RX Side */\n\t/* start UDM */\n\tark_udm_start(ark->udm.v);\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_ark_rx_start_queue(dev, i);\n\n\t/* TX Side */\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_ark_tx_queue_start(dev, i);\n\n\t/* start DDM */\n\tark_ddm_start(ark->ddm.v);\n\n\tark->started = 1;\n\t/* set xmit and receive function */\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts;\n\n\tif (ark->start_pg)\n\t\tark_pktchkr_run(ark->pc);\n\n\tif (ark->start_pg && (dev->data->port_id == 0)) {\n\t\tpthread_t thread;\n\n\t\t/* Delay packet generatpr start allow the hardware to be ready\n\t\t * This is only used for sanity checking with internal generator\n\t\t */\n\t\tif (pthread_create(&thread, NULL, delay_pg_start, ark)) {\n\t\t\tPMD_DRV_LOG(ERR, \"Could not create pktgen \"\n\t\t\t\t    \"starter thread\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (ark->user_ext.dev_start)\n\t\tark->user_ext.dev_start(dev,\n\t\t\tark->user_data[dev->data->port_id]);\n\n\treturn 0;\n}\n\nstatic void\neth_ark_dev_stop(struct rte_eth_dev *dev)\n{\n\tuint16_t i;\n\tint status;\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\tstruct ark_mpu_t *mpu;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\tif (ark->started == 0)\n\t\treturn;\n\tark->started = 0;\n\n\t/* Stop the extension first */\n\tif (ark->user_ext.dev_stop)\n\t\tark->user_ext.dev_stop(dev,\n\t\t       ark->user_data[dev->data->port_id]);\n\n\t/* Stop the packet generator */\n\tif (ark->start_pg)\n\t\tark_pktgen_pause(ark->pg);\n\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts_noop;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts_noop;\n\n\t/* STOP TX Side */\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++) {\n\t\tstatus = eth_ark_tx_queue_stop(dev, i);\n\t\tif (status != 0) {\n\t\t\tuint16_t port = dev->data->port_id;\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"tx_queue stop anomaly\"\n\t\t\t\t    \" port %u, queue %u\\n\",\n\t\t\t\t    port, i);\n\t\t}\n\t}\n\n\t/* Stop DDM */\n\t/* Wait up to 0.1 second.  each stop is up to 1000 * 10 useconds */\n\tfor (i = 0; i < 10; i++) {\n\t\tstatus = ark_ddm_stop(ark->ddm.v, 1);\n\t\tif (status == 0)\n\t\t\tbreak;\n\t}\n\tif (status || i != 0) {\n\t\tPMD_DRV_LOG(ERR, \"DDM stop anomaly. status:\"\n\t\t\t    \" %d iter: %u. (%s)\\n\",\n\t\t\t    status,\n\t\t\t    i,\n\t\t\t    __func__);\n\t\tark_ddm_dump(ark->ddm.v, \"Stop anomaly\");\n\n\t\tmpu = ark->mputx.v;\n\t\tfor (i = 0; i < ark->tx_queues; i++) {\n\t\t\tark_mpu_dump(mpu, \"DDM failure dump\", i);\n\t\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t\t}\n\t}\n\n\t/* STOP RX Side */\n\t/* Stop UDM  multiple tries attempted */\n\tfor (i = 0; i < 10; i++) {\n\t\tstatus = ark_udm_stop(ark->udm.v, 1);\n\t\tif (status == 0)\n\t\t\tbreak;\n\t}\n\tif (status || i != 0) {\n\t\tPMD_DRV_LOG(ERR, \"UDM stop anomaly. status %d iter: %u. (%s)\\n\",\n\t\t\t    status, i, __func__);\n\t\tark_udm_dump(ark->udm.v, \"Stop anomaly\");\n\n\t\tmpu = ark->mpurx.v;\n\t\tfor (i = 0; i < ark->rx_queues; i++) {\n\t\t\tark_mpu_dump(mpu, \"UDM Stop anomaly\", i);\n\t\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t\t}\n\t}\n\n\tark_udm_dump_stats(ark->udm.v, \"Post stop\");\n\tark_udm_dump_perf(ark->udm.v, \"Post stop\");\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_ark_rx_dump_queue(dev, i, __func__);\n\n\t/* Stop the packet checker if it is running */\n\tif (ark->start_pg) {\n\t\tark_pktchkr_dump_stats(ark->pc);\n\t\tark_pktchkr_stop(ark->pc);\n\t}\n}\n\nstatic void\neth_ark_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\tuint16_t i;\n\n\tif (ark->user_ext.dev_close)\n\t\tark->user_ext.dev_close(dev,\n\t\t ark->user_data[dev->data->port_id]);\n\n\teth_ark_dev_stop(dev);\n\teth_ark_udm_force_close(dev);\n\n\t/*\n\t * TODO This should only be called once for the device during shutdown\n\t */\n\tark_rqp_dump(ark->rqpacing);\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++) {\n\t\teth_ark_tx_queue_release(dev->data->tx_queues[i]);\n\t\tdev->data->tx_queues[i] = 0;\n\t}\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++) {\n\t\teth_ark_dev_rx_queue_release(dev->data->rx_queues[i]);\n\t\tdev->data->rx_queues[i] = 0;\n\t}\n\n\trte_free(dev->data->mac_addrs);\n\tdev->data->mac_addrs = 0;\n}\n\nstatic int\neth_ark_dev_info_get(struct rte_eth_dev *dev,\n\t\t     struct rte_eth_dev_info *dev_info)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\tstruct ark_mpu_t *tx_mpu = RTE_PTR_ADD(ark->bar0, ARK_MPU_TX_BASE);\n\tstruct ark_mpu_t *rx_mpu = RTE_PTR_ADD(ark->bar0, ARK_MPU_RX_BASE);\n\tuint16_t ports = ark->num_ports;\n\n\tdev_info->max_rx_pktlen = ARK_RX_MAX_PKT_LEN;\n\tdev_info->min_rx_bufsize = ARK_RX_MIN_BUFSIZE;\n\n\tdev_info->max_rx_queues = ark_api_num_queues_per_port(rx_mpu, ports);\n\tdev_info->max_tx_queues = ark_api_num_queues_per_port(tx_mpu, ports);\n\n\tdev_info->rx_desc_lim = (struct rte_eth_desc_lim) {\n\t\t.nb_max = ARK_RX_MAX_QUEUE,\n\t\t.nb_min = ARK_RX_MIN_QUEUE,\n\t\t.nb_align = ARK_RX_MIN_QUEUE}; /* power of 2 */\n\n\tdev_info->tx_desc_lim = (struct rte_eth_desc_lim) {\n\t\t.nb_max = ARK_TX_MAX_QUEUE,\n\t\t.nb_min = ARK_TX_MIN_QUEUE,\n\t\t.nb_align = ARK_TX_MIN_QUEUE}; /* power of 2 */\n\n\t/* ARK PMD supports all line rates, how do we indicate that here ?? */\n\tdev_info->speed_capa = (ETH_LINK_SPEED_1G |\n\t\t\t\tETH_LINK_SPEED_10G |\n\t\t\t\tETH_LINK_SPEED_25G |\n\t\t\t\tETH_LINK_SPEED_40G |\n\t\t\t\tETH_LINK_SPEED_50G |\n\t\t\t\tETH_LINK_SPEED_100G);\n\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_link_update(struct rte_eth_dev *dev, int wait_to_complete)\n{\n\tPMD_DEBUG_LOG(DEBUG, \"link status = %d\\n\",\n\t\t\tdev->data->dev_link.link_status);\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.link_update) {\n\t\treturn ark->user_ext.link_update\n\t\t\t(dev, wait_to_complete,\n\t\t\t ark->user_data[dev->data->port_id]);\n\t}\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_set_link_up(struct rte_eth_dev *dev)\n{\n\tdev->data->dev_link.link_status = 1;\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.dev_set_link_up)\n\t\treturn ark->user_ext.dev_set_link_up(dev,\n\t\t\t     ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_set_link_down(struct rte_eth_dev *dev)\n{\n\tdev->data->dev_link.link_status = 0;\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.dev_set_link_down)\n\t\treturn ark->user_ext.dev_set_link_down(dev,\n\t\t       ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_stats_get(struct rte_eth_dev *dev, struct rte_eth_stats *stats)\n{\n\tuint16_t i;\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tstats->ipackets = 0;\n\tstats->ibytes = 0;\n\tstats->opackets = 0;\n\tstats->obytes = 0;\n\tstats->imissed = 0;\n\tstats->oerrors = 0;\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_tx_queue_stats_get(dev->data->tx_queues[i], stats);\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_rx_queue_stats_get(dev->data->rx_queues[i], stats);\n\tif (ark->user_ext.stats_get)\n\t\treturn ark->user_ext.stats_get(dev, stats,\n\t\t\tark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_stats_reset(struct rte_eth_dev *dev)\n{\n\tuint16_t i;\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_tx_queue_stats_reset(dev->data->tx_queues[i]);\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_rx_queue_stats_reset(dev->data->rx_queues[i]);\n\tif (ark->user_ext.stats_reset)\n\t\tark->user_ext.stats_reset(dev,\n\t\t\t  ark->user_data[dev->data->port_id]);\n\n\treturn 0;\n}\n\nstatic int\neth_ark_macaddr_add(struct rte_eth_dev *dev,\n\t\t    struct rte_ether_addr *mac_addr,\n\t\t    uint32_t index,\n\t\t    uint32_t pool)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_add) {\n\t\tark->user_ext.mac_addr_add(dev,\n\t\t\t\t\t   mac_addr,\n\t\t\t\t\t   index,\n\t\t\t\t\t   pool,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\t\treturn 0;\n\t}\n\treturn -ENOTSUP;\n}\n\nstatic void\neth_ark_macaddr_remove(struct rte_eth_dev *dev, uint32_t index)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_remove)\n\t\tark->user_ext.mac_addr_remove(dev, index,\n\t\t\t      ark->user_data[dev->data->port_id]);\n}\n\nstatic int\neth_ark_set_default_mac_addr(struct rte_eth_dev *dev,\n\t\t\t     struct rte_ether_addr *mac_addr)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_set) {\n\t\tark->user_ext.mac_addr_set(dev, mac_addr,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\t\treturn 0;\n\t}\n\treturn -ENOTSUP;\n}\n\nstatic int\neth_ark_set_mtu(struct rte_eth_dev *dev, uint16_t  size)\n{\n\tstruct ark_adapter *ark = dev->data->dev_private;\n\n\tif (ark->user_ext.set_mtu)\n\t\treturn ark->user_ext.set_mtu(dev, size,\n\t\t\t     ark->user_data[dev->data->port_id]);\n\n\treturn -ENOTSUP;\n}\n\nstatic inline int\nprocess_pktdir_arg(const char *key, const char *value,\n\t\t   void *extra_args)\n{\n\tPMD_FUNC_LOG(DEBUG, \"key = %s, value = %s\\n\",\n\t\t    key, value);\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)extra_args;\n\n\tark->pkt_dir_v = strtol(value, NULL, 16);\n\tPMD_FUNC_LOG(DEBUG, \"pkt_dir_v = 0x%x\\n\", ark->pkt_dir_v);\n\treturn 0;\n}\n\nstatic inline int\nprocess_file_args(const char *key, const char *value, void *extra_args)\n{\n\tPMD_FUNC_LOG(DEBUG, \"key = %s, value = %s\\n\",\n\t\t    key, value);\n\tchar *args = (char *)extra_args;\n\n\t/* Open the configuration file */\n\tFILE *file = fopen(value, \"r\");\n\tchar line[ARK_MAX_ARG_LEN];\n\tint  size = 0;\n\tint first = 1;\n\n\tif (file == NULL) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to open \"\n\t\t\t    \"config file %s\\n\", value);\n\t\treturn -1;\n\t}\n\n\twhile (fgets(line, sizeof(line), file)) {\n\t\tsize += strlen(line);\n\t\tif (size >= ARK_MAX_ARG_LEN) {\n\t\t\tPMD_DRV_LOG(ERR, \"Unable to parse file %s args, \"\n\t\t\t\t    \"parameter list is too long\\n\", value);\n\t\t\tfclose(file);\n\t\t\treturn -1;\n\t\t}\n\t\tif (first) {\n\t\t\tstrncpy(args, line, ARK_MAX_ARG_LEN);\n\t\t\tfirst = 0;\n\t\t} else {\n\t\t\tstrncat(args, line, ARK_MAX_ARG_LEN);\n\t\t}\n\t}\n\tPMD_FUNC_LOG(DEBUG, \"file = %s\\n\", args);\n\tfclose(file);\n\treturn 0;\n}\n\nstatic int\neth_ark_check_args(struct ark_adapter *ark, const char *params)\n{\n\tstruct rte_kvargs *kvlist;\n\tunsigned int k_idx;\n\tstruct rte_kvargs_pair *pair = NULL;\n\tint ret = -1;\n\n\tkvlist = rte_kvargs_parse(params, valid_arguments);\n\tif (kvlist == NULL)\n\t\treturn 0;\n\n\tark->pkt_gen_args[0] = 0;\n\tark->pkt_chkr_args[0] = 0;\n\n\tfor (k_idx = 0; k_idx < kvlist->count; k_idx++) {\n\t\tpair = &kvlist->pairs[k_idx];\n\t\tPMD_FUNC_LOG(DEBUG, \"**** Arg passed to PMD = %s:%s\\n\",\n\t\t\t     pair->key,\n\t\t\t     pair->value);\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTDIR_ARG,\n\t\t\t       &process_pktdir_arg,\n\t\t\t       ark) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTDIR_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTGEN_ARG,\n\t\t\t       &process_file_args,\n\t\t\t       ark->pkt_gen_args) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTGEN_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTCHKR_ARG,\n\t\t\t       &process_file_args,\n\t\t\t       ark->pkt_chkr_args) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTCHKR_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tPMD_DRV_LOG(INFO, \"packet director set to 0x%x\\n\", ark->pkt_dir_v);\n\t/* Setup the packet director */\n\tark_pktdir_setup(ark->pd, ark->pkt_dir_v);\n\n\t/* Setup the packet generator */\n\tif (ark->pkt_gen_args[0]) {\n\t\tPMD_DRV_LOG(INFO, \"Setting up the packet generator\\n\");\n\t\tark_pktgen_parse(ark->pkt_gen_args);\n\t\tark_pktgen_reset(ark->pg);\n\t\tark_pktgen_setup(ark->pg);\n\t\tark->start_pg = 1;\n\t}\n\n\t/* Setup the packet checker */\n\tif (ark->pkt_chkr_args[0]) {\n\t\tark_pktchkr_parse(ark->pkt_chkr_args);\n\t\tark_pktchkr_setup(ark->pc);\n\t}\n\n\tret = 0;\n\nfree_kvlist:\n\trte_kvargs_free(kvlist);\n\n\treturn ret;\n}\n\nRTE_PMD_REGISTER_PCI(net_ark, rte_ark_pmd);\nRTE_PMD_REGISTER_KMOD_DEP(net_ark, \"* igb_uio | uio_pci_generic \");\nRTE_PMD_REGISTER_PCI_TABLE(net_ark, pci_id_ark_map);\nRTE_PMD_REGISTER_PARAM_STRING(net_ark,\n\t\t\t      ARK_PKTGEN_ARG \"=<filename> \"\n\t\t\t      ARK_PKTCHKR_ARG \"=<filename> \"\n\t\t\t      ARK_PKTDIR_ARG \"=<bitmap>\");\n\nRTE_INIT(ark_init_log)\n{\n\tark_logtype = rte_log_register(\"pmd.net.ark\");\n\tif (ark_logtype >= 0)\n\t\trte_log_set_level(ark_logtype, RTE_LOG_NOTICE);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/examples/performance-thread/pthread_shim/pthread_shim.h": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2015 Intel Corporation\n */\n\n#ifndef _PTHREAD_SHIM_H_\n#define _PTHREAD_SHIM_H_\n\n#include <rte_lcore.h>\n\n/*\n * This pthread shim is an example that demonstrates how legacy code\n * that makes use of POSIX pthread services can make use of lthreads\n * with reduced porting effort.\n *\n * N.B. The example is not a complete implementation, only a subset of\n * pthread APIs sufficient to demonstrate the principle of operation\n * are implemented.\n *\n * In general pthread attribute objects do not have equivalent functions\n * in lthreads, and are ignored.\n *\n * There is one exception and that is the use of attr to specify a\n * core affinity in calls to pthread_create.\n *\n * The shim operates as follows:-\n *\n * On initialisation a constructor function uses dlsym to obtain and\n * save the loaded address of the full set of pthread APIs that will\n * be overridden.\n *\n * For each function there is a stub provided that will invoke either\n * the genuine pthread library function saved saved by the constructor,\n * or else the corresponding equivalent lthread function.\n *\n * The stub functions are implemented in pthread_shim.c\n *\n * The stub will take care of adapting parameters, and any police\n * any constraints where lthread functionality differs.\n *\n * The initial thread must always be a pure lthread.\n *\n * The decision whether to invoke the real library function or the lthread\n * function is controlled by a per pthread flag that can be switched\n * on of off by the pthread_override_set() API described below. Typcially\n * this should be done as the first action of the initial lthread.\n *\n * N.B In general it would be poor practice to revert to invoke a real\n * pthread function when running as an lthread, since these may block and\n * effectively stall the lthread scheduler.\n *\n */\n\n\n/*\n * An exiting lthread must not terminate the pthread it is running in\n * since this would mean terminating the lthread scheduler.\n * We override pthread_exit() with a macro because it is typically declared with\n * __attribute__((noreturn))\n */\nvoid pthread_exit_override(void *v);\n\n#define pthread_exit(v) do { \\\n\tpthread_exit_override((v));\t\\\n\treturn NULL;\t\\\n} while (0)\n\n/*\n * Enable/Disable pthread override\n * state\n * 0 disable\n * 1 enable\n */\nvoid pthread_override_set(int state);\n\n\n/*\n * Return pthread override state\n * return\n * 0 disable\n * 1 enable\n */\nint pthread_override_get(void);\n\n\n#endif /* _PTHREAD_SHIM_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/examples/performance-thread/pthread_shim/pthread_shim.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2015 Intel Corporation\n */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <errno.h>\n#include <sched.h>\n#include <dlfcn.h>\n\n#include <rte_log.h>\n\n#include \"lthread_api.h\"\n#include \"pthread_shim.h\"\n\n#define RTE_LOGTYPE_PTHREAD_SHIM RTE_LOGTYPE_USER3\n\n#define POSIX_ERRNO(x)  (x)\n\n/* some releases of FreeBSD 10, e.g. 10.0, don't have CPU_COUNT macro */\n#ifndef CPU_COUNT\n#define CPU_COUNT(x) __cpu_count(x)\n\nstatic inline unsigned int\n__cpu_count(const rte_cpuset_t *cpuset)\n{\n\tunsigned int i, count = 0;\n\tfor (i = 0; i < RTE_MAX_LCORE; i++)\n\t\tif (CPU_ISSET(i, cpuset))\n\t\t\tcount++;\n\treturn count;\n}\n#endif\n\n/*\n * this flag determines at run time if we override pthread\n * calls and map then to equivalent lthread calls\n * or of we call the standard pthread function\n */\nstatic __thread int override;\n\n\n/*\n * this structures contains function pointers that will be\n * initialised to the loaded address of the real\n * pthread library API functions\n */\nstruct pthread_lib_funcs {\nint (*f_pthread_barrier_destroy)\n\t(pthread_barrier_t *);\nint (*f_pthread_barrier_init)\n\t(pthread_barrier_t *, const pthread_barrierattr_t *, unsigned);\nint (*f_pthread_barrier_wait)\n\t(pthread_barrier_t *);\nint (*f_pthread_cond_broadcast)\n\t(pthread_cond_t *);\nint (*f_pthread_cond_destroy)\n\t(pthread_cond_t *);\nint (*f_pthread_cond_init)\n\t(pthread_cond_t *, const pthread_condattr_t *);\nint (*f_pthread_cond_signal)\n\t(pthread_cond_t *);\nint (*f_pthread_cond_timedwait)\n\t(pthread_cond_t *, pthread_mutex_t *, const struct timespec *);\nint (*f_pthread_cond_wait)\n\t(pthread_cond_t *, pthread_mutex_t *);\nint (*f_pthread_create)\n\t(pthread_t *, const pthread_attr_t *, void *(*)(void *), void *);\nint (*f_pthread_detach)\n\t(pthread_t);\nint (*f_pthread_equal)\n\t(pthread_t, pthread_t);\nvoid (*f_pthread_exit)\n\t(void *);\nvoid * (*f_pthread_getspecific)\n\t(pthread_key_t);\nint (*f_pthread_getcpuclockid)\n\t(pthread_t, clockid_t *);\nint (*f_pthread_join)\n\t(pthread_t, void **);\nint (*f_pthread_key_create)\n\t(pthread_key_t *, void (*) (void *));\nint (*f_pthread_key_delete)\n\t(pthread_key_t);\nint (*f_pthread_mutex_destroy)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_mutex_init)\n\t(pthread_mutex_t *__mutex, const pthread_mutexattr_t *);\nint (*f_pthread_mutex_lock)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_mutex_trylock)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_mutex_timedlock)\n\t(pthread_mutex_t *__mutex, const struct timespec *);\nint (*f_pthread_mutex_unlock)\n\t(pthread_mutex_t *__mutex);\nint (*f_pthread_once)\n\t(pthread_once_t *, void (*) (void));\nint (*f_pthread_rwlock_destroy)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_init)\n\t(pthread_rwlock_t *__rwlock, const pthread_rwlockattr_t *);\nint (*f_pthread_rwlock_rdlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_timedrdlock)\n\t(pthread_rwlock_t *__rwlock, const struct timespec *);\nint (*f_pthread_rwlock_timedwrlock)\n\t(pthread_rwlock_t *__rwlock, const struct timespec *);\nint (*f_pthread_rwlock_tryrdlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_trywrlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_unlock)\n\t(pthread_rwlock_t *__rwlock);\nint (*f_pthread_rwlock_wrlock)\n\t(pthread_rwlock_t *__rwlock);\npthread_t (*f_pthread_self)\n\t(void);\nint (*f_pthread_setspecific)\n\t(pthread_key_t, const void *);\nint (*f_pthread_spin_init)\n\t(pthread_spinlock_t *__spin, int);\nint (*f_pthread_spin_destroy)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_spin_lock)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_spin_trylock)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_spin_unlock)\n\t(pthread_spinlock_t *__spin);\nint (*f_pthread_cancel)\n\t(pthread_t);\nint (*f_pthread_setcancelstate)\n\t(int, int *);\nint (*f_pthread_setcanceltype)\n\t(int, int *);\nvoid (*f_pthread_testcancel)\n\t(void);\nint (*f_pthread_getschedparam)\n\t(pthread_t pthread, int *, struct sched_param *);\nint (*f_pthread_setschedparam)\n\t(pthread_t, int, const struct sched_param *);\nint (*f_pthread_yield)\n\t(void);\nint (*f_pthread_setaffinity_np)\n\t(pthread_t thread, size_t cpusetsize, const rte_cpuset_t *cpuset);\nint (*f_nanosleep)\n\t(const struct timespec *req, struct timespec *rem);\n} _sys_pthread_funcs = {\n\t.f_pthread_barrier_destroy = NULL,\n};\n\n\n/*\n * this macro obtains the loaded address of a library function\n * and saves it.\n */\nstatic void *__libc_dl_handle = RTLD_NEXT;\n\n#define get_addr_of_loaded_symbol(name) do {\t\t\t\t\\\n\tchar *error_str;\t\t\t\t\t\t\\\n\t_sys_pthread_funcs.f_##name = dlsym(__libc_dl_handle, (#name));\t\\\n\terror_str = dlerror();\t\t\t\t\t\t\\\n\tif (error_str != NULL) {\t\t\t\t\t\\\n\t\tfprintf(stderr, \"%s\\n\", error_str);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n\n/*\n * The constructor function initialises the\n * function pointers for pthread library functions\n */\nRTE_INIT(pthread_intercept_ctor)\n{\n\toverride = 0;\n\t/*\n\t * Get the original functions\n\t */\n\tget_addr_of_loaded_symbol(pthread_barrier_destroy);\n\tget_addr_of_loaded_symbol(pthread_barrier_init);\n\tget_addr_of_loaded_symbol(pthread_barrier_wait);\n\tget_addr_of_loaded_symbol(pthread_cond_broadcast);\n\tget_addr_of_loaded_symbol(pthread_cond_destroy);\n\tget_addr_of_loaded_symbol(pthread_cond_init);\n\tget_addr_of_loaded_symbol(pthread_cond_signal);\n\tget_addr_of_loaded_symbol(pthread_cond_timedwait);\n\tget_addr_of_loaded_symbol(pthread_cond_wait);\n\tget_addr_of_loaded_symbol(pthread_create);\n\tget_addr_of_loaded_symbol(pthread_detach);\n\tget_addr_of_loaded_symbol(pthread_equal);\n\tget_addr_of_loaded_symbol(pthread_exit);\n\tget_addr_of_loaded_symbol(pthread_getspecific);\n\tget_addr_of_loaded_symbol(pthread_getcpuclockid);\n\tget_addr_of_loaded_symbol(pthread_join);\n\tget_addr_of_loaded_symbol(pthread_key_create);\n\tget_addr_of_loaded_symbol(pthread_key_delete);\n\tget_addr_of_loaded_symbol(pthread_mutex_destroy);\n\tget_addr_of_loaded_symbol(pthread_mutex_init);\n\tget_addr_of_loaded_symbol(pthread_mutex_lock);\n\tget_addr_of_loaded_symbol(pthread_mutex_trylock);\n\tget_addr_of_loaded_symbol(pthread_mutex_timedlock);\n\tget_addr_of_loaded_symbol(pthread_mutex_unlock);\n\tget_addr_of_loaded_symbol(pthread_once);\n\tget_addr_of_loaded_symbol(pthread_rwlock_destroy);\n\tget_addr_of_loaded_symbol(pthread_rwlock_init);\n\tget_addr_of_loaded_symbol(pthread_rwlock_rdlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_timedrdlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_timedwrlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_tryrdlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_trywrlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_unlock);\n\tget_addr_of_loaded_symbol(pthread_rwlock_wrlock);\n\tget_addr_of_loaded_symbol(pthread_self);\n\tget_addr_of_loaded_symbol(pthread_setspecific);\n\tget_addr_of_loaded_symbol(pthread_spin_init);\n\tget_addr_of_loaded_symbol(pthread_spin_destroy);\n\tget_addr_of_loaded_symbol(pthread_spin_lock);\n\tget_addr_of_loaded_symbol(pthread_spin_trylock);\n\tget_addr_of_loaded_symbol(pthread_spin_unlock);\n\tget_addr_of_loaded_symbol(pthread_cancel);\n\tget_addr_of_loaded_symbol(pthread_setcancelstate);\n\tget_addr_of_loaded_symbol(pthread_setcanceltype);\n\tget_addr_of_loaded_symbol(pthread_testcancel);\n\tget_addr_of_loaded_symbol(pthread_getschedparam);\n\tget_addr_of_loaded_symbol(pthread_setschedparam);\n\tget_addr_of_loaded_symbol(pthread_yield);\n\tget_addr_of_loaded_symbol(pthread_setaffinity_np);\n\tget_addr_of_loaded_symbol(nanosleep);\n}\n\n\n/*\n * Enable/Disable pthread override\n * state\n *  0 disable\n *  1 enable\n */\nvoid pthread_override_set(int state)\n{\n\toverride = state;\n}\n\n\n/*\n * Return pthread override state\n * return\n *  0 disable\n *  1 enable\n */\nint pthread_override_get(void)\n{\n\treturn override;\n}\n\n/*\n * This macro is used to catch and log\n * invocation of stubs for unimplemented pthread\n * API functions.\n */\n#define NOT_IMPLEMENTED do {\t\t\t\t\\\n\tif (override) {\t\t\t\t\t\\\n\t\tRTE_LOG(WARNING,\t\t\t\\\n\t\t\tPTHREAD_SHIM,\t\t\t\\\n\t\t\t\"WARNING %s NOT IMPLEMENTED\\n\",\t\\\n\t\t\t__func__);\t\t\t\\\n\t}\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * pthread API override functions follow\n * Note in this example code only a subset of functions are\n * implemented.\n *\n * The stub functions provided will issue a warning log\n * message if an unimplemented function is invoked\n *\n */\n\nint pthread_barrier_destroy(pthread_barrier_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_barrier_destroy(a);\n}\n\nint\npthread_barrier_init(pthread_barrier_t *a,\n\t\t     const pthread_barrierattr_t *b, unsigned c)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_barrier_init(a, b, c);\n}\n\nint pthread_barrier_wait(pthread_barrier_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_barrier_wait(a);\n}\n\nint pthread_cond_broadcast(pthread_cond_t *cond)\n{\n\tif (override) {\n\n\t\tlthread_cond_broadcast(*(struct lthread_cond **)cond);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cond_broadcast(cond);\n}\n\nint pthread_mutex_destroy(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_destroy(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_destroy(mutex);\n}\n\nint pthread_cond_destroy(pthread_cond_t *cond)\n{\n\tif (override)\n\t\treturn lthread_cond_destroy(*(struct lthread_cond **)cond);\n\treturn _sys_pthread_funcs.f_pthread_cond_destroy(cond);\n}\n\nint pthread_cond_init(pthread_cond_t *cond, const pthread_condattr_t *attr)\n{\n\tif (override)\n\t\treturn lthread_cond_init(NULL,\n\t\t\t\t(struct lthread_cond **)cond,\n\t\t\t\t(const struct lthread_condattr *) attr);\n\treturn _sys_pthread_funcs.f_pthread_cond_init(cond, attr);\n}\n\nint pthread_cond_signal(pthread_cond_t *cond)\n{\n\tif (override) {\n\t\tlthread_cond_signal(*(struct lthread_cond **)cond);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cond_signal(cond);\n}\n\nint\npthread_cond_timedwait(pthread_cond_t *__restrict cond,\n\t\t       pthread_mutex_t *__restrict mutex,\n\t\t       const struct timespec *__restrict time)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_cond_timedwait(cond, mutex, time);\n}\n\nint pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex)\n{\n\tif (override) {\n\t\tpthread_mutex_unlock(mutex);\n\t\tint rv = lthread_cond_wait(*(struct lthread_cond **)cond, 0);\n\n\t\tpthread_mutex_lock(mutex);\n\t\treturn rv;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cond_wait(cond, mutex);\n}\n\nint\npthread_create(pthread_t *__restrict tid,\n\t\tconst pthread_attr_t *__restrict attr,\n\t\tlthread_func_t func,\n\t       void *__restrict arg)\n{\n\tif (override) {\n\t\tint lcore = -1;\n\n\t\tif (attr != NULL) {\n\t\t\t/* determine CPU being requested */\n\t\t\trte_cpuset_t cpuset;\n\n\t\t\tCPU_ZERO(&cpuset);\n\t\t\tpthread_attr_getaffinity_np(attr,\n\t\t\t\t\t\tsizeof(rte_cpuset_t),\n\t\t\t\t\t\t&cpuset);\n\n\t\t\tif (CPU_COUNT(&cpuset) != 1)\n\t\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t\tfor (lcore = 0; lcore < LTHREAD_MAX_LCORES; lcore++) {\n\t\t\t\tif (!CPU_ISSET(lcore, &cpuset))\n\t\t\t\t\tcontinue;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\treturn lthread_create((struct lthread **)tid, lcore,\n\t\t\t\t      func, arg);\n\t}\n\treturn _sys_pthread_funcs.f_pthread_create(tid, attr, func, arg);\n}\n\nint pthread_detach(pthread_t tid)\n{\n\tif (override) {\n\t\tstruct lthread *lt = (struct lthread *)tid;\n\n\t\tif (lt == lthread_current()) {\n\t\t\tlthread_detach();\n\t\t\treturn 0;\n\t\t}\n\t\tNOT_IMPLEMENTED;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_detach(tid);\n}\n\nint pthread_equal(pthread_t a, pthread_t b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_equal(a, b);\n}\n\nvoid pthread_exit_override(void *v)\n{\n\tif (override) {\n\t\tlthread_exit(v);\n\t\treturn;\n\t}\n\t_sys_pthread_funcs.f_pthread_exit(v);\n}\n\nvoid\n*pthread_getspecific(pthread_key_t key)\n{\n\tif (override)\n\t\treturn lthread_getspecific((unsigned int) key);\n\treturn _sys_pthread_funcs.f_pthread_getspecific(key);\n}\n\nint pthread_getcpuclockid(pthread_t a, clockid_t *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_getcpuclockid(a, b);\n}\n\nint pthread_join(pthread_t tid, void **val)\n{\n\tif (override)\n\t\treturn lthread_join((struct lthread *)tid, val);\n\treturn _sys_pthread_funcs.f_pthread_join(tid, val);\n}\n\nint pthread_key_create(pthread_key_t *keyptr, void (*dtor) (void *))\n{\n\tif (override)\n\t\treturn lthread_key_create((unsigned int *)keyptr, dtor);\n\treturn _sys_pthread_funcs.f_pthread_key_create(keyptr, dtor);\n}\n\nint pthread_key_delete(pthread_key_t key)\n{\n\tif (override) {\n\t\tlthread_key_delete((unsigned int) key);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_key_delete(key);\n}\n\n\nint\npthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t *attr)\n{\n\tif (override)\n\t\treturn lthread_mutex_init(NULL,\n\t\t\t\t(struct lthread_mutex **)mutex,\n\t\t\t\t(const struct lthread_mutexattr *)attr);\n\treturn _sys_pthread_funcs.f_pthread_mutex_init(mutex, attr);\n}\n\nint pthread_mutex_lock(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_lock(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_lock(mutex);\n}\n\nint pthread_mutex_trylock(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_trylock(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_trylock(mutex);\n}\n\nint pthread_mutex_timedlock(pthread_mutex_t *mutex, const struct timespec *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_mutex_timedlock(mutex, b);\n}\n\nint pthread_mutex_unlock(pthread_mutex_t *mutex)\n{\n\tif (override)\n\t\treturn lthread_mutex_unlock(*(struct lthread_mutex **)mutex);\n\treturn _sys_pthread_funcs.f_pthread_mutex_unlock(mutex);\n}\n\nint pthread_once(pthread_once_t *a, void (b) (void))\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_once(a, b);\n}\n\nint pthread_rwlock_destroy(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_destroy(a);\n}\n\nint pthread_rwlock_init(pthread_rwlock_t *a, const pthread_rwlockattr_t *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_init(a, b);\n}\n\nint pthread_rwlock_rdlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_rdlock(a);\n}\n\nint pthread_rwlock_timedrdlock(pthread_rwlock_t *a, const struct timespec *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_timedrdlock(a, b);\n}\n\nint pthread_rwlock_timedwrlock(pthread_rwlock_t *a, const struct timespec *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_timedwrlock(a, b);\n}\n\nint pthread_rwlock_tryrdlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_tryrdlock(a);\n}\n\nint pthread_rwlock_trywrlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_trywrlock(a);\n}\n\nint pthread_rwlock_unlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_unlock(a);\n}\n\nint pthread_rwlock_wrlock(pthread_rwlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_rwlock_wrlock(a);\n}\n\n#ifdef RTE_EXEC_ENV_LINUX\nint\npthread_yield(void)\n{\n\tif (override) {\n\t\tlthread_yield();\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_yield();\n}\n#else\nvoid\npthread_yield(void)\n{\n\tif (override)\n\t\tlthread_yield();\n\telse\n\t\t_sys_pthread_funcs.f_pthread_yield();\n}\n#endif\n\npthread_t pthread_self(void)\n{\n\tif (override)\n\t\treturn (pthread_t) lthread_current();\n\treturn _sys_pthread_funcs.f_pthread_self();\n}\n\nint pthread_setspecific(pthread_key_t key, const void *data)\n{\n\tif (override) {\n\t\tint rv =  lthread_setspecific((unsigned int)key, data);\n\t\treturn rv;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_setspecific(key, data);\n}\n\nint pthread_spin_init(pthread_spinlock_t *a, int b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_init(a, b);\n}\n\nint pthread_spin_destroy(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_destroy(a);\n}\n\nint pthread_spin_lock(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_lock(a);\n}\n\nint pthread_spin_trylock(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_trylock(a);\n}\n\nint pthread_spin_unlock(pthread_spinlock_t *a)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_spin_unlock(a);\n}\n\nint pthread_cancel(pthread_t tid)\n{\n\tif (override) {\n\t\tlthread_cancel(*(struct lthread **)tid);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_cancel(tid);\n}\n\nint pthread_setcancelstate(int a, int *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_setcancelstate(a, b);\n}\n\nint pthread_setcanceltype(int a, int *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_setcanceltype(a, b);\n}\n\nvoid pthread_testcancel(void)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_testcancel();\n}\n\n\nint pthread_getschedparam(pthread_t tid, int *a, struct sched_param *b)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_getschedparam(tid, a, b);\n}\n\nint pthread_setschedparam(pthread_t a, int b, const struct sched_param *c)\n{\n\tNOT_IMPLEMENTED;\n\treturn _sys_pthread_funcs.f_pthread_setschedparam(a, b, c);\n}\n\n\nint nanosleep(const struct timespec *req, struct timespec *rem)\n{\n\tif (override) {\n\t\tuint64_t ns = req->tv_sec * 1000000000 + req->tv_nsec;\n\n\t\tlthread_sleep(ns);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_nanosleep(req, rem);\n}\n\nint\npthread_setaffinity_np(pthread_t thread, size_t cpusetsize,\n\t\t       const rte_cpuset_t *cpuset)\n{\n\tif (override) {\n\t\t/* we only allow affinity with a single CPU */\n\t\tif (CPU_COUNT(cpuset) != 1)\n\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t/* we only allow the current thread to sets its own affinity */\n\t\tstruct lthread *lt = (struct lthread *)thread;\n\n\t\tif (lthread_current() != lt)\n\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t/* determine the CPU being requested */\n\t\tint i;\n\n\t\tfor (i = 0; i < LTHREAD_MAX_LCORES; i++) {\n\t\t\tif (!CPU_ISSET(i, cpuset))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\t/* check requested core is allowed */\n\t\tif (i == LTHREAD_MAX_LCORES)\n\t\t\treturn POSIX_ERRNO(EINVAL);\n\n\t\t/* finally we can set affinity to the requested lcore */\n\t\tlthread_set_affinity(i);\n\t\treturn 0;\n\t}\n\treturn _sys_pthread_funcs.f_pthread_setaffinity_np(thread, cpusetsize,\n\t\t\t\t\t\t\t   cpuset);\n}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/forward_stats.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/console.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/host_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/fast_pkt_proc.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/perf_benchmark.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/vm_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/vswitch_vm.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/host_vm_comms_qemu.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/inter_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/vmxnet3_int.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/nics/img/single_port_nic.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/load_bal_app_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/quickassist_block_diagram.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/l2_fwd_virtenv_benchmark_setup.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/pipeline_overview.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/kernel_nic.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/example_rules.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/client_svr_sym_multi_proc_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/sym_multi_proc_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/test_pipeline_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/ring_pipeline_perf_setup.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/qos_sched_app_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/ipv4_acl_rule.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/sample_app_ug/img/threads_pipelines.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/ex_data_flow_tru_dropper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure35.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure33.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/m_definition.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/ewma_filter_eq_1.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/kernel_nic_intf.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/prefetch_pipeline.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/drop_probability_eq3.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/packet_distributor2.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/pkt_flow_kni.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/tbl24_tbl8_tbl8.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/packet_distributor1.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/hier_sched_blk.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/ewma_filter_eq_2.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/eq2_factor.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure34.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/kni_traffic_flow.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/pkt_drop_probability.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/pipe_prefetch_sm.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/data_struct_per_port.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/drop_probability_eq4.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/vhost_net_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/flow_tru_droppper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure39.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/pkt_proc_pipeline_qos.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure32.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/tbl24_tbl8.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure38.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/blk_diag_dropper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/eq2_expression.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/figure37.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/guides/prog_guide/img/drop_probability_graph.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/logo/DPDK_logo_vertical_rev_small.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-20.02-vatcqxnchicpxv3lf547pfzhoqumu5wb/spack-src/doc/logo/DPDK_logo_horizontal_tag.png"
    ],
    "total_files": 3923
}