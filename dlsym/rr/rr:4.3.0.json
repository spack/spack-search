{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-rr-4.3.0-6iirswmpvavrsquath3ng4aepljcfxgv/spack-src/src/preload/preload.c": "/* -*- Mode: C; tab-width: 8; c-basic-offset: 2; indent-tabs-mode: nil; -*- */\n\n#define RR_IMPLEMENT_PRELOAD\n\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1\n#endif\n\n#include \"preload_interface.h\"\n\n/**\n * Buffer syscalls, so that rr can process the entire buffer with one\n * trap instead of a trap per call.\n *\n * This file is compiled into a dso that's PRELOADed in recorded\n * applications.  The dso replaces libc syscall wrappers with our own\n * implementation that saves nondetermistic outparams in a fixed-size\n * buffer.  When the buffer is full or the recorded application\n * invokes an un-buffered syscall or receives a signal, we trap to rr\n * and it records the state of the buffer.\n *\n * During replay, rr simply refills the buffer with the recorded data\n * when it reaches the \"flush-buffer\" events that were recorded.  Then\n * rr emulates each buffered syscall, and the code here restores the\n * client data from the refilled buffer.\n *\n * The crux of the implementation here is to selectively ptrace-trap\n * syscalls.  The normal (un-buffered) syscalls generate a ptrace\n * trap, and the buffered syscalls trap directly to the kernel.  This\n * is implemented with a seccomp-bpf which examines the syscall and\n * decides how to handle it (see seccomp-bpf.h and Task::spawn).\n *\n * Because this code runs in the tracee's address space and overrides\n * system calls, the code is rather delicate.  The following rules\n * must be followed\n *\n * o No rr headers (other than seccomp-bpf.h and rr.h) may be included\n * o All syscalls invoked by this code must be called directly, not\n *   through libc wrappers (which this file may itself indirectly override)\n */\n\n/**\n * We also use this preload library to disable XShm by overriding\n * XShmQueryExtension.\n */\n\n#include \"rr/rr.h\"\n#include <errno.h>\n#include <fcntl.h>\n#include <limits.h>\n#include <link.h>\n#include <linux/futex.h>\n#include <linux/net.h>\n#include <linux/perf_event.h>\n#include <poll.h>\n#include <pthread.h>\n#include <signal.h>\n#include <stdarg.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/epoll.h>\n#include <sys/file.h>\n#include <sys/ioctl.h>\n#include <sys/mman.h>\n#include <sys/resource.h>\n#include <sys/socket.h>\n#include <sys/stat.h>\n#include <sys/time.h>\n#include <sys/types.h>\n#include <sys/uio.h>\n#include <sys/un.h>\n#include <syscall.h>\n#include <sysexits.h>\n#include <time.h>\n#include <unistd.h>\n\n#ifndef BTRFS_IOCTL_MAGIC\n#define BTRFS_IOCTL_MAGIC 0x94\n#endif\n#ifndef BTRFS_IOC_CLONE_RANGE\nstruct btrfs_ioctl_clone_range_args {\n  int64_t src_fd;\n  uint64_t src_offset;\n  uint64_t src_length;\n  uint64_t dest_offset;\n};\n#define BTRFS_IOC_CLONE_RANGE                                                  \\\n  _IOW(BTRFS_IOCTL_MAGIC, 13, struct btrfs_ioctl_clone_range_args)\n#endif\n\n/* NB: don't include any other local headers here. */\n\n#ifdef memcpy\n#undef memcpy\n#endif\n#define memcpy you_must_use_local_memcpy\n\n#ifdef syscall\n#undef syscall\n#endif\n#define syscall you_must_use_traced_syscall\n\n#define RR_HIDDEN __attribute__((visibility(\"hidden\")))\n\n/**\n * Represents syscall params.  Makes it simpler to pass them around,\n * and avoids pushing/popping all the data for calls.\n */\nstruct syscall_info {\n  long no;\n  long args[6];\n};\n\n/* Nonzero when syscall buffering is enabled. */\nstatic int buffer_enabled;\n/* Nonzero after process-global state has been initialized. */\nstatic int process_inited;\n\nstatic struct preload_globals globals;\n\n/**\n * Because this library is always loaded via LD_PRELOAD, we can use the\n * initial-exec TLS model (see http://www.akkadia.org/drepper/tls.pdf) which\n * lets the compiler generate better code which, crucially, does not call\n * helper functions outside of our library.\n */\n#define TLS_STORAGE_MODEL __attribute__((tls_model(\"initial-exec\")))\n\n/* Nonzero when thread-local state like the syscallbuf has been\n * initialized.  */\nstatic __thread int thread_inited TLS_STORAGE_MODEL;\n/* When buffering is enabled, points at the thread's mapped buffer\n * segment.  At the start of the segment is an object of type |struct\n * syscallbuf_hdr|, so |buffer| is also a pointer to the buffer\n * header. */\nstatic __thread uint8_t* buffer TLS_STORAGE_MODEL;\nstatic __thread size_t buffer_size TLS_STORAGE_MODEL;\n/* This is used to support the buffering of \"may-block\" system calls.\n * The problem that needs to be addressed can be introduced with a\n * simple example; assume that we're buffering the \"read\" and \"write\"\n * syscalls.\n *\n *  o (Tasks W and R set up a synchronous-IO pipe open between them; W\n *    \"owns\" the write end of the pipe; R owns the read end; the pipe\n *    buffer is full)\n *  o Task W invokes the write syscall on the pipe\n *  o Since write is a buffered syscall, the seccomp filter traps W\n *    directly to the kernel; there's no trace event for W delivered\n *    to rr.\n *  o The pipe is full, so W is descheduled by the kernel because W\n *    can't make progress.\n *  o rr thinks W is still running and doesn't schedule R.\n *\n * At this point, progress in the recorded application can only be\n * made by scheduling R, but no one tells rr to do that.  Oops!\n *\n * Thus enter the \"desched counter\".  It's a perf_event for the \"sw t\n * switches\" event (which, more precisely, is \"sw deschedule\"; it\n * counts schedule-out, not schedule-in).  We program the counter to\n * deliver a signal to this task when there's new counter data\n * available.  And we set up the \"sample period\", how many descheds\n * are triggered before the signal is delivered, to be \"1\".  This\n * means that when the counter is armed, the next desched (i.e., the\n * next time the desched counter is bumped up) of this task will\n * deliver the signal to it.  And signal delivery always generates a\n * ptrace trap, so rr can deduce that this task was descheduled and\n * schedule another.\n *\n * The description above is sort of an idealized view; there are\n * numerous implementation details that are documented in\n * handle_signal.c, where they're dealt with. */\nstatic __thread int desched_counter_fd TLS_STORAGE_MODEL;\n\nstatic __thread int cloned_file_data_fd TLS_STORAGE_MODEL;\nstatic __thread off_t cloned_file_data_offset TLS_STORAGE_MODEL;\nstatic __thread void* scratch_buf TLS_STORAGE_MODEL;\nstatic __thread size_t scratch_size TLS_STORAGE_MODEL;\n\nstatic __thread struct msghdr* notify_control_msg TLS_STORAGE_MODEL;\n\n/* Points at the libc/pthread pthread_create().  We wrap\n * pthread_create, so need to retain this pointer to call out to the\n * libc version. There is no __pthread_create stub to call. There are\n * some explicitly-versioned stubs but let's not use those. */\nstatic int (*real_pthread_create)(pthread_t* thread, const pthread_attr_t* attr,\n                                  void* (*start_routine)(void*), void* arg);\n\nstatic int (*real_pthread_mutex_timedlock)(pthread_mutex_t* mutex,\n                                           const struct timespec* abstime);\n\n/**\n * Return a pointer to the buffer header, which happens to occupy the\n * initial bytes in the mapped region.\n */\nstatic struct syscallbuf_hdr* buffer_hdr(void) {\n  return (struct syscallbuf_hdr*)buffer;\n}\n\n/**\n * Return a pointer to the byte just after the last valid syscall record in\n * the buffer.\n */\nstatic uint8_t* buffer_last(void) {\n  return (uint8_t*)next_record(buffer_hdr());\n}\n\n/**\n * Return a pointer to the byte just after the very end of the mapped\n * region.\n */\nstatic uint8_t* buffer_end(void) { return buffer + buffer_size; }\n\n/**\n * Same as libc memcpy(), but usable within syscallbuf transaction\n * critical sections.\n */\nstatic void local_memcpy(void* dest, const void* source, int n) {\n#if defined(__i386__) || defined(__x86_64__)\n  /* On modern x86-ish CPUs rep movsb is fast, usually able to move\n   * 64 bytes at a time.\n   */\n  __asm__ __volatile__(\"rep movsb\\n\\t\"\n                       : \"+S\"(source), \"+D\"(dest), \"+c\"(n)\n                       :\n                       : \"cc\", \"memory\");\n#else\n#error Unknown architecture\n#endif\n}\n\n/* The following are wrappers for the syscalls invoked by this library\n * itself.  These syscalls will generate ptrace traps.\n * stack_param_1 and stack_param_2 are pushed onto the stack just before\n * the syscall, for SYS_rrcall_notify_syscall_hook_exit which takes stack\n * parameters as well as register parameters.\n * syscall_instruction is the actual syscall invocation instruction\n * (a function which we call with the registers set up appropriately).\n */\n\nextern RR_HIDDEN long _raw_syscall(int syscallno, long a0, long a1, long a2,\n                                   long a3, long a4, long a5,\n                                   void* syscall_instruction,\n                                   long stack_param_1, long stack_param_2);\n\nstatic int update_errno_ret(long ret) {\n  /* EHWPOISON is the last known errno as of linux 3.9.5. */\n  if (0 > ret && ret >= -EHWPOISON) {\n    errno = -ret;\n    ret = -1;\n  }\n  return ret;\n}\n\nstatic int privileged_traced_syscall(int syscallno, long a0, long a1, long a2,\n                                     long a3, long a4, long a5) {\n  long ret = _raw_syscall(syscallno, a0, a1, a2, a3, a4, a5,\n                          RR_PAGE_SYSCALL_PRIVILEGED_TRACED, 0, 0);\n  return update_errno_ret(ret);\n}\n#define privileged_traced_syscall6(no, a0, a1, a2, a3, a4, a5)                 \\\n  privileged_traced_syscall(no, (uintptr_t)a0, (uintptr_t)a1, (uintptr_t)a2,   \\\n                            (uintptr_t)a3, (uintptr_t)a4, (uintptr_t)a5)\n#define privileged_traced_syscall5(no, a0, a1, a2, a3, a4)                     \\\n  privileged_traced_syscall6(no, a0, a1, a2, a3, a4, 0)\n#define privileged_traced_syscall4(no, a0, a1, a2, a3)                         \\\n  privileged_traced_syscall5(no, a0, a1, a2, a3, 0)\n#define privileged_traced_syscall3(no, a0, a1, a2)                             \\\n  privileged_traced_syscall4(no, a0, a1, a2, 0)\n#define privileged_traced_syscall2(no, a0, a1)                                 \\\n  privileged_traced_syscall3(no, a0, a1, 0)\n#define privileged_traced_syscall1(no, a0) privileged_traced_syscall2(no, a0, 0)\n#define privileged_traced_syscall0(no) privileged_traced_syscall1(no, 0)\n\n/**\n * Make a raw traced syscall using the params in |call|.  \"Raw\" traced\n * syscalls return the raw kernel return value, and don't transform it\n * to -1/errno per POSIX semantics.\n */\nstatic long traced_raw_syscall(const struct syscall_info* call) {\n  /* FIXME: pass |call| to avoid pushing these on the stack\n   * again. */\n  return _raw_syscall(call->no, call->args[0], call->args[1], call->args[2],\n                      call->args[3], call->args[4], call->args[5],\n                      RR_PAGE_SYSCALL_TRACED, 0, 0);\n}\n\n#if defined(SYS_fcntl64)\n#define RR_FCNTL_SYSCALL SYS_fcntl64\n#else\n#define RR_FCNTL_SYSCALL SYS_fcntl\n#endif\n\nstatic int privileged_traced_fcntl(int fd, int cmd, ...) {\n  va_list ap;\n  void* arg;\n\n  va_start(ap, cmd);\n  arg = va_arg(ap, void*);\n  va_end(ap);\n\n  return privileged_traced_syscall3(RR_FCNTL_SYSCALL, fd, cmd, arg);\n}\n\nstatic pid_t privileged_traced_getpid(void) {\n  return privileged_traced_syscall0(SYS_getpid);\n}\n\nstatic pid_t privileged_traced_gettid(void) {\n  return privileged_traced_syscall0(SYS_gettid);\n}\n\nstatic int privileged_traced_perf_event_open(struct perf_event_attr* attr,\n                                             pid_t pid, int cpu, int group_fd,\n                                             unsigned long flags) {\n  return privileged_traced_syscall5(SYS_perf_event_open, attr, pid, cpu,\n                                    group_fd, flags);\n}\n\nstatic int privileged_traced_raise(int sig) {\n  return privileged_traced_syscall2(SYS_kill, privileged_traced_getpid(), sig);\n}\n\nstatic ssize_t privileged_traced_write(int fd, const void* buf, size_t count) {\n  return privileged_traced_syscall3(SYS_write, fd, buf, count);\n}\n\n/* We can't use the rr logging helpers because they rely on libc\n * syscall-invoking functions, so roll our own here.\n *\n * XXX just use these for all logging? */\n\n__attribute__((format(printf, 1, 2))) static void logmsg(const char* msg, ...) {\n  va_list args;\n  char buf[1024];\n  int len;\n\n  va_start(args, msg);\n  len = vsnprintf(buf, sizeof(buf) - 1, msg, args);\n  va_end(args);\n\n  privileged_traced_write(STDERR_FILENO, buf, len);\n}\n\n#ifndef NDEBUG\n#define assert(cond)                                                           \\\n  do {                                                                         \\\n    if (!(cond)) {                                                             \\\n      logmsg(\"%s:%d: Assertion `\" #cond \"' failed.\\n\", __FILE__, __LINE__);    \\\n      privileged_traced_raise(SIGABRT);                                        \\\n    }                                                                          \\\n  } while (0)\n#else\n#define assert(cond) ((void)0)\n#endif\n\n#define fatal(msg, ...)                                                        \\\n  do {                                                                         \\\n    logmsg(\"[FATAL] (%s:%d: errno: %s: tid: %d) \" msg \"\\n\", __FILE__,          \\\n           __LINE__, strerror(errno), privileged_traced_gettid(),              \\\n           ##__VA_ARGS__);                                                     \\\n    privileged_traced_syscall1(SYS_exit_group, EX_OSERR);                      \\\n  } while (0)\n\n#ifdef DEBUGTAG\n#define debug(msg, ...) logmsg(\"[\" DEBUGTAG \"] \" msg \"\\n\", ##__VA_ARGS__)\n#else\n#define debug(msg, ...) ((void)0)\n#endif\n\n/**\n * Unlike |traced_syscall()|, this helper is implicitly \"raw\" (returns\n * the direct kernel return value), because the syscall hooks have to\n * save that raw return value.\n * This is only called from syscall wrappers that are doing a proper\n * buffered syscall.\n */\nstatic long untraced_syscall_base(int syscallno, long a0, long a1, long a2,\n                                  long a3, long a4, long a5,\n                                  void* syscall_instruction) {\n  struct syscallbuf_record* rec = (struct syscallbuf_record*)buffer_last();\n  long ret = _raw_syscall(syscallno, a0, a1, a2, a3, a4, a5,\n                          syscall_instruction, 0, 0);\n  unsigned char tmp_in_replay = globals.in_replay;\n/* During replay, return the result that's already in the buffer, instead\n   of what our \"syscall\" returned. */\n#if defined(__i386__) || defined(__x86_64__)\n  /* On entry, during recording %eax/%rax are whatever the kernel returned\n   * but during replay they may be invalid (e.g. 0). During replay, reload\n   * %eax/%rax from |rec->ret|. At the end of this sequence all registers\n   * will match between recording and replay. We clobber the temporary\n   * in_replay register, and the condition codes, to ensure this.\n   * This all assumes the compiler doesn't create unnecessary temporaries\n   * holding values like |ret|. Inspection of generated code shows it doesn't.\n   */\n  __asm__(\"test %1,%1\\n\\t\"\n          \"cmovne %2,%0\\n\\t\"\n          \"xor %1,%1\\n\\t\"\n          : \"+a\"(ret), \"+c\"(tmp_in_replay)\n          : \"m\"(rec->ret)\n          : \"cc\");\n#else\n#error Unknown architecture\n#endif\n  return ret;\n}\n#define untraced_syscall6(no, a0, a1, a2, a3, a4, a5)                          \\\n  untraced_syscall_base(no, (uintptr_t)a0, (uintptr_t)a1, (uintptr_t)a2,       \\\n                        (uintptr_t)a3, (uintptr_t)a4, (uintptr_t)a5,           \\\n                        RR_PAGE_SYSCALL_UNTRACED_RECORDING_ONLY)\n#define untraced_syscall5(no, a0, a1, a2, a3, a4)                              \\\n  untraced_syscall6(no, a0, a1, a2, a3, a4, 0)\n#define untraced_syscall4(no, a0, a1, a2, a3)                                  \\\n  untraced_syscall5(no, a0, a1, a2, a3, 0)\n#define untraced_syscall3(no, a0, a1, a2) untraced_syscall4(no, a0, a1, a2, 0)\n#define untraced_syscall2(no, a0, a1) untraced_syscall3(no, a0, a1, 0)\n#define untraced_syscall1(no, a0) untraced_syscall2(no, a0, 0)\n#define untraced_syscall0(no) untraced_syscall1(no, 0)\n\n#define untraced_replayed_syscall6(no, a0, a1, a2, a3, a4, a5)                 \\\n  untraced_syscall_base(no, (uintptr_t)a0, (uintptr_t)a1, (uintptr_t)a2,       \\\n                        (uintptr_t)a3, (uintptr_t)a4, (uintptr_t)a5,           \\\n                        RR_PAGE_SYSCALL_UNTRACED)\n#define untraced_replayed_syscall5(no, a0, a1, a2, a3, a4)                     \\\n  untraced_replayed_syscall6(no, a0, a1, a2, a3, a4, 0)\n#define untraced_replayed_syscall4(no, a0, a1, a2, a3)                         \\\n  untraced_replayed_syscall5(no, a0, a1, a2, a3, 0)\n#define untraced_replayed_syscall3(no, a0, a1, a2)                             \\\n  untraced_replayed_syscall4(no, a0, a1, a2, 0)\n#define untraced_replayed_syscall2(no, a0, a1)                                 \\\n  untraced_replayed_syscall3(no, a0, a1, 0)\n#define untraced_replayed_syscall1(no, a0) untraced_replayed_syscall2(no, a0, 0)\n#define untraced_replayed_syscall0(no) untraced_replayed_syscall1(no, 0)\n\n#define privileged_untraced_syscall6(no, a0, a1, a2, a3, a4, a5)               \\\n  _raw_syscall(no, (uintptr_t)a0, (uintptr_t)a1, (uintptr_t)a2, (uintptr_t)a3, \\\n               (uintptr_t)a4, (uintptr_t)a5,                                   \\\n               RR_PAGE_SYSCALL_PRIVILEGED_UNTRACED_RECORDING_ONLY, 0, 0)\n#define privileged_untraced_syscall5(no, a0, a1, a2, a3, a4)                   \\\n  privileged_untraced_syscall6(no, a0, a1, a2, a3, a4, 0)\n#define privileged_untraced_syscall4(no, a0, a1, a2, a3)                       \\\n  privileged_untraced_syscall5(no, a0, a1, a2, a3, 0)\n#define privileged_untraced_syscall3(no, a0, a1, a2)                           \\\n  privileged_untraced_syscall4(no, a0, a1, a2, 0)\n#define privileged_untraced_syscall2(no, a0, a1)                               \\\n  privileged_untraced_syscall3(no, a0, a1, 0)\n#define privileged_untraced_syscall1(no, a0)                                   \\\n  privileged_untraced_syscall2(no, a0, 0)\n#define privileged_untraced_syscall0(no) privileged_untraced_syscall1(no, 0)\n\n#define replay_only_syscall6(no, a0, a1, a2, a3, a4, a5)                       \\\n  _raw_syscall(no, (uintptr_t)a0, (uintptr_t)a1, (uintptr_t)a2, (uintptr_t)a3, \\\n               (uintptr_t)a4, (uintptr_t)a5,                                   \\\n               RR_PAGE_SYSCALL_PRIVILEGED_UNTRACED_REPLAY_ONLY, 0, 0)\n#define replay_only_syscall5(no, a0, a1, a2, a3, a4)                           \\\n  replay_only_syscall6(no, a0, a1, a2, a3, a4, 0)\n#define replay_only_syscall4(no, a0, a1, a2, a3)                               \\\n  replay_only_syscall5(no, a0, a1, a2, a3, 0)\n#define replay_only_syscall3(no, a0, a1, a2)                                   \\\n  replay_only_syscall4(no, a0, a1, a2, 0)\n#define replay_only_syscall2(no, a0, a1) replay_only_syscall3(no, a0, a1, 0)\n#define replay_only_syscall1(no, a0) replay_only_syscall2(no, a0, 0)\n#define replay_only_syscall0(no) replay_only_syscall1(no, 0)\n\nstatic int privileged_untraced_close(int fd) {\n  return privileged_untraced_syscall1(SYS_close, fd);\n}\n\nstatic int privileged_untraced_fcntl(int fd, int cmd, ...) {\n  va_list ap;\n  void* arg;\n\n  va_start(ap, cmd);\n  arg = va_arg(ap, void*);\n  va_end(ap);\n\n  return privileged_untraced_syscall3(RR_FCNTL_SYSCALL, fd, cmd, arg);\n}\n\n/**\n * Do what's necessary to set up buffers for the caller.\n * |untraced_syscall_ip| lets rr know where our untraced syscalls will\n * originate from.  |addr| is the address of the control socket the\n * child expects to connect to.  |msg| is a pre-prepared IPC that can\n * be used to share fds; |fdptr| is a pointer to the control-message\n * data buffer where the fd number being shared will be stored.\n * |args_vec| provides the tracer with preallocated space to make\n * socketcall syscalls.\n *\n * Return a pointer to the syscallbuf (with an initialized header\n * including the available size), if syscallbuf is enabled.\n *\n * This is a \"magic\" syscall implemented by rr.\n */\nstatic void rrcall_init_buffers(struct rrcall_init_buffers_params* args) {\n  privileged_traced_syscall1(SYS_rrcall_init_buffers, args);\n}\n\n/**\n * Return a counter that generates a signal targeted at this task\n * every time the task is descheduled |nr_descheds| times.\n */\nstatic int open_desched_event_counter(size_t nr_descheds, pid_t tid) {\n  struct perf_event_attr attr;\n  int tmp_fd, fd;\n  struct f_owner_ex own;\n\n  memset(&attr, 0, sizeof(attr));\n  attr.size = sizeof(attr);\n  attr.type = PERF_TYPE_SOFTWARE;\n  attr.config = PERF_COUNT_SW_CONTEXT_SWITCHES;\n  attr.disabled = 1;\n  attr.sample_period = nr_descheds;\n\n  tmp_fd = privileged_traced_perf_event_open(&attr, 0 /*self*/, -1 /*any cpu*/,\n                                             -1, 0);\n  if (0 > tmp_fd) {\n    fatal(\"Failed to perf_event_open(cs, period=%zu)\", nr_descheds);\n  }\n  fd = privileged_traced_fcntl(tmp_fd, F_DUPFD_CLOEXEC,\n                               RR_DESCHED_EVENT_FLOOR_FD);\n  if (0 > fd) {\n    fatal(\"Failed to dup desched fd\");\n  }\n  if (privileged_untraced_close(tmp_fd)) {\n    fatal(\"Failed to close tmp_fd\");\n  }\n  if (privileged_untraced_fcntl(fd, F_SETFL, O_ASYNC)) {\n    fatal(\"Failed to fcntl(O_ASYNC) the desched counter\");\n  }\n  own.type = F_OWNER_TID;\n  own.pid = tid;\n  if (privileged_untraced_fcntl(fd, F_SETOWN_EX, &own)) {\n    fatal(\"Failed to fcntl(SETOWN_EX) the desched counter to this\");\n  }\n  if (privileged_untraced_fcntl(fd, F_SETSIG, SYSCALLBUF_DESCHED_SIGNAL)) {\n    fatal(\"Failed to fcntl(SETSIG, %d) the desched counter\",\n          SYSCALLBUF_DESCHED_SIGNAL);\n  }\n\n  return fd;\n}\n\n/**\n * Initialize thread-local buffering state, if enabled.\n */\nstatic void init_thread(void) {\n  struct rrcall_init_buffers_params args;\n\n  assert(process_inited);\n  assert(!thread_inited);\n\n  if (!buffer_enabled) {\n    thread_inited = 1;\n    return;\n  }\n\n  /* NB: we want this setup emulated during replay. */\n  desched_counter_fd =\n      open_desched_event_counter(1, privileged_traced_gettid());\n\n  args.desched_counter_fd = desched_counter_fd;\n\n  /* Trap to rr: let the magic begin!\n   *\n   * If the desched signal is currently blocked, then the tracer\n   * will clear our TCB guard and we won't be able to buffer\n   * syscalls.  But the tracee will set the guard when (or if)\n   * the signal is unblocked. */\n  rrcall_init_buffers(&args);\n\n  cloned_file_data_fd = args.cloned_file_data_fd;\n  /* rr initializes the buffer header. */\n  buffer = args.syscallbuf_ptr;\n  buffer_size = args.syscallbuf_size;\n  scratch_buf = args.scratch_buf;\n  scratch_size = args.scratch_size;\n\n  thread_inited = 1;\n}\n\n/**\n * After a fork(), we retain a CoW mapping of our parent's syscallbuf.\n * That's bad, because we don't want to use that buffer.  So drop the\n * parent's copy and reinstall our own.\n *\n * FIXME: this \"leaks\" the parent's old copy in our address space.\n */\nstatic void post_fork_child(void) {\n  buffer = NULL;\n  thread_inited = 0;\n  init_thread();\n}\n\nextern char _breakpoint_table_entry_start;\nextern char _breakpoint_table_entry_end;\n\n/**\n * Initialize process-global buffering state, if enabled.\n */\nstatic void __attribute__((constructor)) init_process(void) {\n  struct rrcall_init_preload_params params;\n  extern RR_HIDDEN void _syscall_hook_trampoline(void);\n  extern RR_HIDDEN void _syscall_hook_end(void);\n\n#if defined(__i386__)\n  extern RR_HIDDEN void _syscall_hook_trampoline_3d_01_f0_ff_ff(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_90_90_90(void);\n  struct syscall_patch_hook syscall_patch_hooks[] = {\n    /* pthread_cond_broadcast has 'int 80' followed by\n     * cmp $-4095,%eax (in glibc-2.18-16.fc20.i686) */\n    { 5,\n      { 0x3d, 0x01, 0xf0, 0xff, 0xff },\n      (uintptr_t)_syscall_hook_trampoline_3d_01_f0_ff_ff },\n    /* Our vdso syscall patch has 'int 80' followed by onp; nop; nop */\n    { 3, { 0x90, 0x90, 0x90 }, (uintptr_t)_syscall_hook_trampoline_90_90_90 }\n  };\n\n  /* Load GLIBC 2.1 version of pthread_create. Otherwise we may get the 2.0\n     version, which cannot handle the pthread_attr values passed by callers\n     expecting to call the glibc 2.1 version. */\n  real_pthread_create = dlvsym(RTLD_NEXT, \"pthread_create\", \"GLIBC_2.1\");\n#elif defined(__x86_64__)\n  extern RR_HIDDEN void _syscall_hook_trampoline_48_3d_01_f0_ff_ff(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_48_3d_00_f0_ff_ff(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_48_8b_3c_24(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_5a_5e_c3(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_89_c2_f7_da(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_90_90_90(void);\n  extern RR_HIDDEN void _syscall_hook_trampoline_ba_01_00_00_00(void);\n\n  struct syscall_patch_hook syscall_patch_hooks[] = {\n    /* Many glibc syscall wrappers (e.g. read) have 'syscall' followed by\n     * cmp $-4095,%rax (in glibc-2.18-16.fc20.x86_64) */\n    { 6,\n      { 0x48, 0x3d, 0x01, 0xf0, 0xff, 0xff },\n      (uintptr_t)_syscall_hook_trampoline_48_3d_01_f0_ff_ff },\n    /* Many glibc syscall wrappers (e.g. __libc_recv) have 'syscall' followed by\n     * cmp $-4096,%rax (in glibc-2.18-16.fc20.x86_64) */\n    { 6,\n      { 0x48, 0x3d, 0x00, 0xf0, 0xff, 0xff },\n      (uintptr_t)_syscall_hook_trampoline_48_3d_00_f0_ff_ff },\n    /* Many glibc syscall wrappers (e.g. read) have 'syscall' followed by\n     * mov (%rsp),%rdi (in glibc-2.18-16.fc20.x86_64) */\n    { 4,\n      { 0x48, 0x8b, 0x3c, 0x24 },\n      (uintptr_t)_syscall_hook_trampoline_48_8b_3c_24 },\n    /* __lll_unlock_wake has 'syscall' followed by\n     * pop %rdx; pop %rsi; ret */\n    { 3, { 0x5a, 0x5e, 0xc3 }, (uintptr_t)_syscall_hook_trampoline_5a_5e_c3 },\n    /* posix_fadvise64 has 'syscall' followed by\n     * mov %eax,%edx; neg %edx (in glibc-2.22-11.fc23.x86_64) */\n    { 4,\n      { 0x89, 0xc2, 0xf7, 0xda },\n      (uintptr_t)_syscall_hook_trampoline_89_c2_f7_da },\n    /* Our VDSO vsyscall patches have 'syscall' followed by \"nop; nop; nop\" */\n    { 3,\n      { 0x90, 0x90, 0x90 },\n      (uintptr_t)_syscall_hook_trampoline_ba_01_00_00_00 },\n    /* glibc-2.22-17.fc23.x86_64 has 'syscall' followed by 'mov $1,%rdx' in\n     * pthread_barrier_wait.\n     */\n    { 5,\n      { 0xba, 0x01, 0x00, 0x00, 0x00 },\n      (uintptr_t)_syscall_hook_trampoline_ba_01_00_00_00 }\n  };\n\n  real_pthread_create = dlsym(RTLD_NEXT, \"pthread_create\");\n#else\n#error Unknown architecture\n#endif\n  if (process_inited) {\n    return;\n  }\n\n  buffer_enabled = !!getenv(SYSCALLBUF_ENABLED_ENV_VAR);\n\n  pthread_atfork(NULL, NULL, post_fork_child);\n\n  params.syscallbuf_enabled = buffer_enabled;\n  params.syscall_hook_trampoline = (void*)_syscall_hook_trampoline;\n  params.syscall_hook_end = (void*)_syscall_hook_end;\n  params.syscall_patch_hook_count =\n      sizeof(syscall_patch_hooks) / sizeof(syscall_patch_hooks[0]);\n  params.syscall_patch_hooks = syscall_patch_hooks;\n  params.globals = &globals;\n  params.breakpoint_table = &_breakpoint_table_entry_start;\n  params.breakpoint_table_entry_size =\n      &_breakpoint_table_entry_end - &_breakpoint_table_entry_start;\n\n  privileged_traced_syscall1(SYS_rrcall_init_preload, &params);\n\n  process_inited = 1;\n\n  init_thread();\n}\n\n/**\n * In a thread newly created by |pthread_create()|, first initialize\n * thread-local internal rr data, then trampoline into the user's\n * thread function.\n */\nstruct thread_func_data {\n  void* (*start_routine)(void*);\n  void* arg;\n};\n\nstatic void* thread_trampoline(void* arg) {\n  struct thread_func_data data = *(struct thread_func_data*)arg;\n  free(arg);\n\n  init_thread();\n\n  return data.start_routine(data.arg);\n}\n\n/**\n * Interpose |pthread_create()| so that we can use a custom trampoline\n * function (see above) that initializes rr thread-local data for new\n * threads.\n *\n * This is a wrapper of |pthread_create()|, but not like the ones\n * below: we don't wrap |pthread_create()| in order to buffer its\n * syscalls, rather in order to initialize rr thread data.\n */\nint pthread_create(pthread_t* thread, const pthread_attr_t* attr,\n                   void* (*start_routine)(void*), void* arg) {\n  struct thread_func_data* data = malloc(sizeof(*data));\n  void* saved_buffer = buffer;\n  int ret;\n\n  /* Init syscallbuf now if we haven't yet (e.g. if pthread_create is called\n   * during library initialization before our preload library).\n   * This also fetches real_pthread_create which we'll need below. */\n  init_process();\n\n  data->start_routine = start_routine;\n  data->arg = arg;\n  /* Don't let the new thread use our TLS pointer. */\n  buffer = NULL;\n  ret = real_pthread_create(thread, attr, thread_trampoline, data);\n  buffer = saved_buffer;\n  return ret;\n}\n\n#define PTHREAD_MUTEX_TYPE_MASK 3\n#define PTHREAD_MUTEX_PRIO_INHERIT_NP 32\n\nstatic void fix_mutex_kind(pthread_mutex_t* mutex) {\n  /* Disable priority inheritance. */\n  mutex->__data.__kind &= ~PTHREAD_MUTEX_PRIO_INHERIT_NP;\n}\n\n/*\n * We bind directly to __pthread_mutex_lock and __pthread_mutex_trylock\n * because setting up indirect function pointers in init_process requires\n * calls to dlsym which itself can call pthread_mutex_lock (e.g. via\n * application code overriding malloc/calloc to use a pthreads-based\n * implementation).\n */\nextern int __pthread_mutex_lock(pthread_mutex_t* mutex);\nextern int __pthread_mutex_trylock(pthread_mutex_t* mutex);\n\n/* Prevent use of lock elision; Haswell's TSX/RTM features used by\n   lock elision increment the rbc perf counter for instructions which\n   are later rolled back if the transaction fails. */\nint pthread_mutex_lock(pthread_mutex_t* mutex) {\n  fix_mutex_kind(mutex);\n  return __pthread_mutex_lock(mutex);\n}\n\nint pthread_mutex_timedlock(pthread_mutex_t* mutex,\n                            const struct timespec* abstime) {\n  fix_mutex_kind(mutex);\n  /* No __pthread_mutex_timedlock stub exists, so we have to use the\n   * indirect call.\n   */\n  if (!real_pthread_mutex_timedlock) {\n    real_pthread_mutex_timedlock = dlsym(RTLD_NEXT, \"pthread_mutex_timedlock\");\n  }\n  return real_pthread_mutex_timedlock(mutex, abstime);\n}\n\nint pthread_mutex_trylock(pthread_mutex_t* mutex) {\n  fix_mutex_kind(mutex);\n  return __pthread_mutex_trylock(mutex);\n}\n\n/**\n * syscall hooks start here.\n *\n * !!! NBB !!!: from here on, all code that executes within the\n * critical sections of transactions *MUST KEEP $ip IN THE SYSCALLBUF\n * CODE*.  That means no calls into libc, even for innocent-looking\n * functions like |memcpy()|.\n *\n * How syscall hooks operate:\n *\n * 1. The rr tracer monkey-patches __kernel_vsyscall() to jump to\n *    _syscall_hook_trampoline() above.\n * 2. When a call is made to __kernel_vsyscall(), it jumps to\n *    _syscall_hook_trampoline(), where the syscall params are\n *    packaged up into a call to syscall_hook() below.\n * 3. syscall_hook() dispatches to a syscall processor function.\n * 4. The syscall processor prepares a new record in the buffer. See\n *    struct syscallbuf_record for record fields.  If the buffer runs\n *    out of space, the processor function aborts and makes a traced\n *    syscall, trapping to rr.  rr then flushes the buffer.  Records\n *    are directly saved to trace, and a buffer-flush event is\n *    recorded without execution info because it's a synthetic event.\n * 5. Then, the syscall processor redirects all potential output\n *    for the syscall to the record (and corrects the overall size of\n *    the record while it does so).\n * 6. The syscall is invoked through a asm helper that does *not*\n *    ptrace-trap to rr.\n * 7. The syscall output, written on the buffer, is copied to the\n *    original pointers provided by the user.  Take notice that this\n *    part saves us the injection of the data on replay, as we only\n *    need to push the data to the buffer and the wrapper code will\n *    copy it to the user address for us.\n * 8. The return value and overall size are saved to the record.\n */\n\n/**\n * Call this and save the result at the start of every system call we\n * want to buffer. The result is a pointer into the record space. You\n * can add to this pointer to allocate space in the trace record.\n * However, do not read or write through this pointer until\n * start_commit_syscall() has been called.  And you *must* call\n * start_commit_syscall() after this is called, otherwise buffering\n * state will be inconsistent between syscalls.\n *\n * See |sys_clock_gettime()| for a simple example of how this helper\n * should be used to buffer outparam data.\n */\n\nstatic void* prep_syscall(void) {\n  if (!buffer) {\n    return NULL;\n  }\n  if (buffer_hdr()->locked) {\n    /* We may be reentering via a signal handler. Return\n     * an invalid pointer. */\n    return NULL;\n  }\n  /* We don't need to worry about a race between testing\n   * |locked| and setting it here. rr recording is responsible\n   * for ensuring signals are not delivered during\n   * syscall_buffer prologue and epilogue code.\n   *\n   * XXX except for synchronous signals generated in the syscall\n   * buffer code, while reading/writing user pointers */\n  buffer_hdr()->locked = 1;\n  /* \"Allocate\" space for a new syscall record, not including\n   * syscall outparam data. */\n  return buffer_last() + sizeof(struct syscallbuf_record);\n}\n\nstatic int is_bufferable_fd(int fd) {\n  return fd < 0 || (fd < SYSCALLBUF_FDS_DISABLED_SIZE &&\n                    !globals.syscallbuf_fds_disabled[fd]);\n}\n\n/**\n * Like prep_syscall, but preps a syscall to operate on a particular fd. If\n * syscallbuf is disabled for this fd, returns NULL (in which case\n * start_commit_syscall will abort cleanly and a traced syscall will be used).\n * Allow negative fds to pass through; they'll either trigger an error or\n * receive special treatment by the kernel (e.g. AT_FDCWD).\n */\nstatic void* prep_syscall_for_fd(int fd) {\n  if (!is_bufferable_fd(fd)) {\n    return NULL;\n  }\n  return prep_syscall();\n}\n\nstatic void arm_desched_event(void) {\n  /* Don't trace the ioctl; doing so would trigger a flushing\n   * ptrace trap, which is exactly what this code is trying to\n   * avoid! :) Although we don't allocate extra space for these\n   * ioctl's, we do record that we called them; the replayer\n   * knows how to skip over them. */\n  if ((int)privileged_untraced_syscall3(SYS_ioctl, desched_counter_fd,\n                                        PERF_EVENT_IOC_ENABLE, 0)) {\n    fatal(\"Failed to ENABLE counter %d\", desched_counter_fd);\n  }\n}\n\nstatic void disarm_desched_event(void) {\n  /* See above. */\n  if ((int)privileged_untraced_syscall3(SYS_ioctl, desched_counter_fd,\n                                        PERF_EVENT_IOC_DISABLE, 0)) {\n    fatal(\"Failed to DISABLE counter %d\", desched_counter_fd);\n  }\n}\n\n/**\n * Return 1 if it's ok to proceed with buffering this system call.\n * Return 0 if we should trace the system call.\n * This must be checked before proceeding with the buffered system call.\n */\n/* (Negative numbers so as to not be valid syscall numbers, in case\n * the |int| arguments below are passed in the wrong order.) */\nenum { MAY_BLOCK = -1, WONT_BLOCK = -2 };\nstatic int start_commit_buffered_syscall(int syscallno, void* record_end,\n                                         int blockness) {\n  void* record_start;\n  void* stored_end;\n  struct syscallbuf_record* rec;\n\n  if (!buffer) {\n    return 0;\n  }\n  record_start = buffer_last();\n  stored_end = record_start + stored_record_size(record_end - record_start);\n  rec = record_start;\n\n  if (stored_end < record_start + sizeof(struct syscallbuf_record)) {\n    /* Either a catastrophic buffer overflow or\n     * we failed to lock the buffer. Just bail out. */\n    return 0;\n  }\n  if (stored_end > (void*)buffer_end() - sizeof(struct syscallbuf_record)) {\n    /* Buffer overflow.\n     * Unlock the buffer and then execute the system call\n     * with a trap to rr.  Note that we reserve enough\n     * space in the buffer for the next prep_syscall(). */\n    buffer_hdr()->locked = 0;\n    return 0;\n  }\n  /* Store this breadcrumb so that the tracer can find out what\n   * syscall we're executing if our registers are in a weird\n   * state.  If we end up aborting this syscall, no worry, this\n   * will just be overwritten later.\n   *\n   * NBB: this *MUST* be set before the desched event is\n   * armed. */\n  rec->syscallno = syscallno;\n  rec->desched = MAY_BLOCK == blockness;\n  rec->size = record_end - record_start;\n  if (rec->desched) {\n    /* NB: the ordering of the next two statements is\n     * important.\n     *\n     * We set this flag to notify rr that it should pay\n     * attention to desched signals pending for this task.\n     * We have to set it *before* we arm the notification\n     * because we can't set the flag atomically with\n     * arming the event (too bad there's no ioctl() for\n     * querying the event enabled-ness state).  That's\n     * important because if the notification is armed,\n     * then rr must be confident that when it disarms the\n     * event, the tracee is at an execution point that\n     * *must not* need the desched event.\n     *\n     * If we were to set the flag non-atomically after the\n     * event was armed, then if a desched signal was\n     * delivered right at the instruction that set the\n     * flag, rr wouldn't know that it needed to advance\n     * the tracee to the untraced syscall entry point.\n     * (And if rr didn't do /that/, then the syscall might\n     * block without rr knowing it, and the recording\n     * session would deadlock.) */\n    buffer_hdr()->desched_signal_may_be_relevant = 1;\n    arm_desched_event();\n  }\n  return 1;\n}\n\n/**\n * Commit the record for a buffered system call.  record_end can be\n * adjusted downward from what was passed to\n * start_commit_buffered_syscall, if not all of the initially\n * requested space is needed.  The result of this function should be\n * returned directly by the kernel syscall hook.\n */\nstatic long commit_raw_syscall(int syscallno, void* record_end, long ret) {\n  void* record_start = buffer_last();\n  struct syscallbuf_record* rec = record_start;\n  struct syscallbuf_hdr* hdr = buffer_hdr();\n  void (*breakpoint_function)(void) = 0;\n\n  assert(record_end >= record_start);\n  rec->size = record_end - record_start;\n\n  assert(buffer_hdr()->locked);\n\n  /* NB: the ordering of this statement with the\n   * |disarm_desched_event()| call below is important.\n   *\n   * We clear this flag to notify rr that the may-block syscall\n   * has finished, so there's no danger of blocking anymore.\n   * (And thus the desched signal is no longer relevant.)  We\n   * have to clear this *before* disarming the event, because if\n   * rr sees the flag set, it has to PTRACE_SYSCALL this task to\n   * ensure it reaches an execution point where the desched\n   * signal is no longer relevant.  We have to use the ioctl()\n   * that disarms the event as a safe \"backstop\" that can be hit\n   * by the PTRACE_SYSCALL.\n   *\n   * If we were to clear the flag *after* disarming the event,\n   * and the signal arrived at the instruction that cleared the\n   * flag, and rr issued the PTRACE_SYSCALL, then this tracee\n   * could fly off to any unknown execution point, including an\n   * iloop.  So the recording session could livelock. */\n  hdr->desched_signal_may_be_relevant = 0;\n\n  if (rec->syscallno != syscallno) {\n    fatal(\"Record is for %d but trying to commit %d\", rec->syscallno,\n          syscallno);\n  }\n\n  if (hdr->abort_commit) {\n    /* We were descheduled in the middle of a may-block\n     * syscall, and it was recorded as a normal entry/exit\n     * pair.  So don't record the syscall in the buffer or\n     * replay will go haywire. */\n    hdr->abort_commit = 0;\n    /* Clear the return value that rr puts there during replay */\n    rec->ret = 0;\n  } else {\n    int breakpoint_entry_size =\n        &_breakpoint_table_entry_end - &_breakpoint_table_entry_start;\n\n    rec->ret = ret;\n    // Finish 'rec' first before updating num_rec_bytes, since\n    // rr might read the record anytime after this update.\n    hdr->num_rec_bytes += stored_record_size(rec->size);\n\n    breakpoint_function =\n        (void*)(&_breakpoint_table_entry_start +\n                (hdr->num_rec_bytes / 8) * breakpoint_entry_size);\n  }\n\n  if (rec->desched) {\n    disarm_desched_event();\n  }\n  /* NBB: for may-block syscalls that are descheduled, the\n   * tracer uses the previous ioctl() as a stable point to reset\n   * the record counter.  Therefore nothing from here on in the\n   * current txn must touch the record counter (at least, must\n   * not assume it's unchanged). */\n\n  buffer_hdr()->locked = 0;\n\n  if (breakpoint_function) {\n    /* Call the breakpoint function corresponding to the record we just\n     * committed. This function just returns, but during replay it gives rr\n     * a chance to set a breakpoint for when a specific syscallbuf record\n     * has been processed.\n     */\n    breakpoint_function();\n  }\n\n  return ret;\n}\n\n/**\n * |ret_size| is the result of a syscall indicating how much data was returned\n * in scratch buffer |buf2|; this function copies that data to |buf| and returns\n * a pointer to the end of it. If there is no scratch buffer (|buf2| is NULL)\n * just returns |ptr|.\n */\nstatic void* copy_output_buffer(int ret_size, void* ptr, void* buf,\n                                void* buf2) {\n  if (!buf2) {\n    return ptr;\n  }\n  if (ret_size <= 0) {\n    return buf2;\n  }\n  local_memcpy(buf, buf2, ret_size);\n  return buf2 + ret_size;\n}\n\n/**\n * Copy an input parameter to the syscallbuf where the kernel needs to\n * read and write it. During replay, we do a no-op self-copy in the buffer\n * so that the buffered data is not lost.\n * This code is written in assembler to ensure that the registers that receive\n * values differing between record and replay (%0, rsi/esi, and flags)\n * are reset to values that are the same between record and replay immediately\n * afterward. This guards against diverging register values leaking into\n * later code.\n * Use local_memcpy or plain assignment instead if the kernel is not going to\n * overwrite the values.\n */\nstatic void memcpy_input_parameter(void* buf, void* src, int size) {\n#if defined(__i386__) || defined(__x86_64__)\n  unsigned char tmp_in_replay = globals.in_replay;\n  __asm__ __volatile__(\"test %0,%0\\n\\t\"\n                       \"cmovne %1,%2\\n\\t\"\n                       \"rep movsb\\n\\t\"\n                       \"xor %0,%0\\n\\t\"\n                       \"xor %2,%2\\n\\t\"\n                       : \"+a\"(tmp_in_replay), \"+D\"(buf), \"+S\"(src), \"+c\"(size)\n                       :\n                       : \"cc\", \"memory\");\n#else\n#error Unknown architecture\n#endif\n}\n\n/**\n * During recording, we copy *real to *buf.\n * During replay, we copy *buf to *real.\n * Behaves like memcpy_input_parameter in terms of hiding differences between\n * recording and replay.\n */\nstatic void copy_futex_int(uint32_t* buf, uint32_t* real) {\n#if defined(__i386__) || defined(__x86_64__)\n  uint32_t tmp_in_replay = globals.in_replay;\n  __asm__ __volatile__(\"test %0,%0\\n\\t\"\n                       \"mov %2,%0\\n\\t\"\n                       \"cmovne %1,%0\\n\\t\"\n                       \"mov %0,%1\\n\\t\"\n                       \"mov %0,%2\\n\\t\"\n                       /* This instruction is just to clear flags */\n                       \"xor %0,%0\\n\\t\"\n                       : \"+a\"(tmp_in_replay)\n                       : \"m\"(*buf), \"m\"(*real)\n                       : \"cc\", \"memory\");\n#else\n#error Unknown architecture\n#endif\n}\n\n/* Keep syscalls in alphabetical order, please. */\n\n/**\n * Call this for syscalls that have no memory effects, don't block, and\n * aren't fd-related.\n */\nstatic long sys_generic_nonblocking(const struct syscall_info* call) {\n  void* ptr = prep_syscall();\n  long ret;\n\n  if (!start_commit_buffered_syscall(call->no, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall6(call->no, call->args[0], call->args[1], call->args[2],\n                          call->args[3], call->args[4], call->args[5]);\n  return commit_raw_syscall(call->no, ptr, ret);\n}\n\n/**\n * Call this for syscalls that have no memory effects, don't block, and\n * have an fd as their first parameter.\n */\nstatic long sys_generic_nonblocking_fd(const struct syscall_info* call) {\n  int fd = call->args[0];\n  void* ptr = prep_syscall_for_fd(fd);\n  long ret;\n\n  if (!start_commit_buffered_syscall(call->no, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall6(call->no, fd, call->args[1], call->args[2],\n                          call->args[3], call->args[4], call->args[5]);\n  return commit_raw_syscall(call->no, ptr, ret);\n}\n\nstatic long sys_clock_gettime(const struct syscall_info* call) {\n  const int syscallno = SYS_clock_gettime;\n  clockid_t clk_id = (clockid_t)call->args[0];\n  struct timespec* tp = (struct timespec*)call->args[1];\n\n  void* ptr = prep_syscall();\n  struct timespec* tp2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (tp) {\n    tp2 = ptr;\n    ptr += sizeof(*tp2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall2(syscallno, clk_id, tp2);\n  if (tp) {\n    local_memcpy(tp, tp2, sizeof(*tp));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_open(const struct syscall_info* call);\nstatic long sys_creat(const struct syscall_info* call) {\n  const char* pathname = (const char*)call->args[0];\n  mode_t mode = call->args[1];\n  /* Thus sayeth the man page:\n   *\n   *   creat() is equivalent to open() with flags equal to\n   *   O_CREAT|O_WRONLY|O_TRUNC. */\n  struct syscall_info open_call;\n  open_call.no = SYS_open;\n  open_call.args[0] = (long)pathname;\n  open_call.args[1] = O_CREAT | O_TRUNC | O_WRONLY;\n  open_call.args[2] = mode;\n  return sys_open(&open_call);\n}\n\nstatic int sys_fcntl64_no_outparams(const struct syscall_info* call) {\n  const int syscallno = RR_FCNTL_SYSCALL;\n  int fd = call->args[0];\n  int cmd = call->args[1];\n  long arg = call->args[2];\n\n  /* None of the no-outparam fcntl's are known to be\n   * may-block. */\n  void* ptr = prep_syscall_for_fd(fd);\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall3(syscallno, fd, cmd, arg);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic int sys_fcntl64_own_ex(const struct syscall_info* call) {\n  const int syscallno = RR_FCNTL_SYSCALL;\n  int fd = call->args[0];\n  int cmd = call->args[1];\n  struct f_owner_ex* owner = (struct f_owner_ex*)call->args[2];\n\n  /* The OWN_EX fcntl's aren't may-block. */\n  void* ptr = prep_syscall_for_fd(fd);\n  struct f_owner_ex* owner2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (owner) {\n    owner2 = ptr;\n    ptr += sizeof(*owner2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  if (owner2) {\n    memcpy_input_parameter(owner2, owner, sizeof(*owner2));\n  }\n  ret = untraced_syscall3(syscallno, fd, cmd, owner2);\n  if (owner2) {\n    local_memcpy(owner, owner2, sizeof(*owner));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic int sys_fcntl64_setlk64(const struct syscall_info* call) {\n  const int syscallno = RR_FCNTL_SYSCALL;\n  int fd = call->args[0];\n  int cmd = call->args[1];\n  struct flock64* lock = (struct flock64*)call->args[2];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  struct flock64* lock2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (lock) {\n    lock2 = ptr;\n    ptr += sizeof(*lock2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  if (lock2) {\n    memcpy_input_parameter(lock2, lock, sizeof(*lock2));\n  }\n  ret = untraced_syscall3(syscallno, fd, cmd, lock2);\n  if (lock2) {\n    local_memcpy(lock, lock2, sizeof(*lock));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic int sys_fcntl64_setlkw64(const struct syscall_info* call) {\n  const int syscallno = RR_FCNTL_SYSCALL;\n  int fd = call->args[0];\n  int cmd = call->args[1];\n  struct flock64* lock = (struct flock64*)call->args[2];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall3(syscallno, fd, cmd, lock);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\n#if defined(SYS_fcntl64)\nstatic long sys_fcntl64(const struct syscall_info* call)\n#else\nstatic long sys_fcntl(const struct syscall_info* call)\n#endif\n{\n  switch (call->args[1]) {\n    case F_DUPFD:\n    case F_GETFD:\n    case F_GETFL:\n    case F_GETOWN:\n    case F_SETFL:\n    case F_SETFD:\n    case F_SETOWN:\n    case F_SETSIG:\n      return sys_fcntl64_no_outparams(call);\n\n    case F_GETOWN_EX:\n    case F_SETOWN_EX:\n      return sys_fcntl64_own_ex(call);\n\n#if F_SETLK != F_SETLK64\n    case F_SETLK64:\n#else\n    case F_SETLK:\n#endif\n      return sys_fcntl64_setlk64(call);\n\n#if F_SETLKW != F_SETLKW64\n    case F_SETLKW64:\n#else\n    case F_SETLKW:\n#endif\n      return sys_fcntl64_setlkw64(call);\n\n    default:\n      return traced_raw_syscall(call);\n  }\n}\n\nstatic long sys_flistxattr(const struct syscall_info* call) {\n  const int syscallno = SYS_flistxattr;\n  int fd = (int)call->args[0];\n  char* buf = (char*)call->args[1];\n  size_t size = call->args[2];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  void* buf2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (buf && size > 0) {\n    buf2 = ptr;\n    ptr += size;\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, fd, buf2, size);\n  ptr = copy_output_buffer(ret > (long)size ? (long)size : ret, ptr, buf, buf2);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_safe_nonblocking_ioctl(const struct syscall_info* call) {\n  const int syscallno = SYS_ioctl;\n  int fd = call->args[0];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  long ret;\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall3(syscallno, fd, call->args[1], call->args[2]);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_ioctl(const struct syscall_info* call) {\n  switch (call->args[1]) {\n    case BTRFS_IOC_CLONE_RANGE:\n    case FIOCLEX:\n    case FIONCLEX:\n      return sys_safe_nonblocking_ioctl(call);\n    default:\n      return traced_raw_syscall(call);\n  }\n}\n\nstatic long sys_futex(const struct syscall_info* call) {\n  enum {\n    FUTEX_USES_UADDR2 = 1 << 0,\n  };\n\n  int op = call->args[1];\n  int flags = 0;\n  switch (FUTEX_CMD_MASK & op) {\n    case FUTEX_WAKE:\n      break;\n    case FUTEX_CMP_REQUEUE:\n    case FUTEX_WAKE_OP:\n      flags |= FUTEX_USES_UADDR2;\n      break;\n\n    /* It turns out not to be worth buffering the FUTEX_WAIT*\n     * calls.  When a WAIT call is made, we know almost for sure\n     * that the tracee is going to be desched'd (otherwise the\n     * userspace CAS would have succeeded).  This is unlike\n     * read/write, f.e., where the vast majority of calls aren't\n     * desched'd and the overhead is worth it.  So all that\n     * buffering WAIT does is add the overhead of arming/disarming\n     * desched (which is a measurable perf loss).\n     *\n     * NB: don't ever try to buffer FUTEX_LOCK_PI; it requires\n     * special processing in the tracer process (in addition to\n     * not being worth doing for perf reasons). */\n    default:\n      return traced_raw_syscall(call);\n  }\n\n  const int syscallno = SYS_futex;\n  uint32_t* uaddr = (uint32_t*)call->args[0];\n  uint32_t val = call->args[2];\n  const struct timespec* timeout = (const struct timespec*)call->args[3];\n  uint32_t* uaddr2 = (uint32_t*)call->args[4];\n  uint32_t val3 = call->args[5];\n\n  void* ptr = prep_syscall();\n  uint32_t* saved_uaddr;\n  uint32_t* saved_uaddr2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  /* We have to record the value of the futex at kernel exit,\n   * but we can't substitute a scratch pointer for the uaddrs:\n   * the futex identity is the memory cell.  There are schemes\n   * that would allow us to use scratch futexes, but they get\n   * complicated quickly. */\n  saved_uaddr = ptr;\n  ptr += sizeof(*saved_uaddr);\n  if (FUTEX_USES_UADDR2 & flags) {\n    saved_uaddr2 = ptr;\n    ptr += sizeof(*saved_uaddr2);\n  }\n  /* See above; it's not worth buffering may-block futex\n   * calls. */\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall6(syscallno, uaddr, op, val, timeout, uaddr2, val3);\n  /* During recording, save the real outparams to the buffer.\n   * During replay, save the values from the buffer to the real outparams.\n   *\n   * The *ONLY* reason it's correct for us to read the outparams\n   * carelessly is that rr protects this syscallbuf\n   * transaction as as a critical section. */\n  copy_futex_int(saved_uaddr, uaddr);\n  if (saved_uaddr2) {\n    copy_futex_int(saved_uaddr2, uaddr2);\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_generic_getdents(const struct syscall_info* call) {\n  int fd = (int)call->args[0];\n  void* buf = (void*)call->args[1];\n  unsigned int count = (unsigned int)call->args[2];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  void* buf2 = NULL;\n  long ret;\n\n  if (buf && count > 0) {\n    buf2 = ptr;\n    ptr += count;\n  }\n  if (!start_commit_buffered_syscall(call->no, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(call->no, fd, buf2, count);\n  ptr = copy_output_buffer(ret, ptr, buf, buf2);\n  return commit_raw_syscall(call->no, ptr, ret);\n}\n\nstatic long sys_getdents(const struct syscall_info* call) {\n  return sys_generic_getdents(call);\n}\n\nstatic long sys_getdents64(const struct syscall_info* call) {\n  return sys_generic_getdents(call);\n}\n\nstatic long sys_gettimeofday(const struct syscall_info* call) {\n  const int syscallno = SYS_gettimeofday;\n  struct timeval* tp = (struct timeval*)call->args[0];\n  struct timezone* tzp = (struct timezone*)call->args[1];\n\n  /* XXX it seems odd that clock_gettime() is spec'd to be\n   * async-signal-safe while gettimeofday() isn't, but that's\n   * what the docs say! */\n  void* ptr = prep_syscall();\n  struct timeval* tp2 = NULL;\n  struct timezone* tzp2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (tp) {\n    tp2 = ptr;\n    ptr += sizeof(*tp2);\n  }\n  if (tzp) {\n    tzp2 = ptr;\n    ptr += sizeof(*tzp2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall2(syscallno, tp2, tzp2);\n  if (tp) {\n    local_memcpy(tp, tp2, sizeof(*tp));\n  }\n  if (tzp) {\n    local_memcpy(tzp, tzp2, sizeof(*tzp));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_generic_getxattr(const struct syscall_info* call) {\n  const char* path = (const char*)call->args[0];\n  const char* name = (const char*)call->args[1];\n  void* value = (void*)call->args[2];\n  size_t size = call->args[3];\n\n  void* ptr = prep_syscall();\n  void* value2 = NULL;\n  long ret;\n\n  if (value && size > 0) {\n    value2 = ptr;\n    ptr += size;\n  }\n  if (!start_commit_buffered_syscall(call->no, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall4(call->no, path, name, value2, size);\n  ptr = copy_output_buffer(ret > (long)size ? (long)size : ret, ptr, value,\n                           value2);\n  return commit_raw_syscall(call->no, ptr, ret);\n}\n\nstatic long sys_getxattr(const struct syscall_info* call) {\n  return sys_generic_getxattr(call);\n}\n\nstatic long sys_lgetxattr(const struct syscall_info* call) {\n  return sys_generic_getxattr(call);\n}\n\nstatic long sys_fgetxattr(const struct syscall_info* call) {\n  int fd = (int)call->args[0];\n  const char* name = (const char*)call->args[1];\n  void* value = (void*)call->args[2];\n  size_t size = call->args[3];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  void* value2 = NULL;\n  long ret;\n\n  if (value && size > 0) {\n    value2 = ptr;\n    ptr += size;\n  }\n  if (!start_commit_buffered_syscall(call->no, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall4(call->no, fd, name, value2, size);\n  ptr = copy_output_buffer(ret > (long)size ? (long)size : ret, ptr, value,\n                           value2);\n  return commit_raw_syscall(call->no, ptr, ret);\n}\n\nstatic long sys_generic_listxattr(const struct syscall_info* call) {\n  char* path = (char*)call->args[0];\n  char* buf = (char*)call->args[1];\n  size_t size = call->args[2];\n\n  void* ptr = prep_syscall();\n  void* buf2 = NULL;\n  long ret;\n\n  if (buf && size > 0) {\n    buf2 = ptr;\n    ptr += size;\n  }\n  if (!start_commit_buffered_syscall(call->no, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(call->no, path, buf2, size);\n  ptr = copy_output_buffer(ret > (long)size ? (long)size : ret, ptr, buf, buf2);\n  return commit_raw_syscall(call->no, ptr, ret);\n}\n\nstatic long sys_listxattr(const struct syscall_info* call) {\n  return sys_generic_listxattr(call);\n}\n\nstatic long sys_llistxattr(const struct syscall_info* call) {\n  return sys_generic_listxattr(call);\n}\n\n#if defined(SYS__llseek)\nstatic long sys__llseek(const struct syscall_info* call) {\n  const int syscallno = SYS__llseek;\n  int fd = call->args[0];\n  unsigned long offset_high = call->args[1];\n  unsigned long offset_low = call->args[2];\n  loff_t* result = (loff_t*)call->args[3];\n  unsigned int whence = call->args[4];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  loff_t* result2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (result) {\n    result2 = ptr;\n    ptr += sizeof(*result2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  if (result2) {\n    memcpy_input_parameter(result2, result, sizeof(*result2));\n  }\n  ret = untraced_syscall5(syscallno, fd, offset_high, offset_low, result2,\n                          whence);\n  if (result2) {\n    *result = *result2;\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n#endif\n\nstatic long sys_madvise(const struct syscall_info* call) {\n  const int syscallno = SYS_madvise;\n  void* addr = (void*)call->args[0];\n  size_t length = call->args[1];\n  int advice = call->args[2];\n\n  void* ptr;\n  long ret;\n\n  switch (advice) {\n    case MADV_DOFORK:\n    case MADV_DONTFORK:\n    case MADV_REMOVE:\n      return traced_raw_syscall(call);\n  }\n\n  ptr = prep_syscall();\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  /* Ensure this syscall happens during replay. In particular MADV_DONTNEED\n   * must be executed.\n   */\n  ret = untraced_replayed_syscall3(syscallno, addr, length, advice);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_mprotect(const struct syscall_info* call) {\n  const int syscallno = SYS_mprotect;\n  void* addr = (void*)call->args[0];\n  size_t length = call->args[1];\n  int prot = call->args[2];\n  struct mprotect_record* mrec;\n\n  void* ptr;\n  long ret;\n\n  if ((prot & ~(PROT_READ | PROT_WRITE | PROT_EXEC)) || !buffer_hdr() ||\n      buffer_hdr()->mprotect_record_count >= MPROTECT_RECORD_COUNT) {\n    return traced_raw_syscall(call);\n  }\n\n  ptr = prep_syscall();\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  mrec = &globals.mprotect_records[buffer_hdr()->mprotect_record_count++];\n  mrec->start = (uint64_t)(uintptr_t)addr;\n  mrec->size = length;\n  mrec->prot = prot;\n  ret = untraced_replayed_syscall3(syscallno, addr, length, prot);\n  buffer_hdr()->mprotect_record_count_completed++;\n\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_open(const struct syscall_info* call) {\n  const int syscallno = SYS_open;\n  const char* pathname = (const char*)call->args[0];\n  int flags = call->args[1];\n  mode_t mode = call->args[2];\n\n  /* NB: not arming the desched event is technically correct,\n   * since open() can't deadlock if it blocks.  However, not\n   * allowing descheds here may cause performance issues if the\n   * open does block for a while.  Err on the side of simplicity\n   * until we have perf data. */\n  void* ptr;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  /* The strcmp()s done here are OK because we're not in the\n   * critical section yet. */\n  if (!allow_buffered_open(pathname)) {\n    return traced_raw_syscall(call);\n  }\n\n  ptr = prep_syscall();\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, pathname, flags, mode);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\n/**\n * Make this function external so desched_ticks.py can set a breakpoint on it.\n */\nvoid __before_poll_syscall_breakpoint(void) {}\n\nstatic long sys_poll(const struct syscall_info* call) {\n  const int syscallno = SYS_poll;\n  struct pollfd* fds = (struct pollfd*)call->args[0];\n  unsigned int nfds = call->args[1];\n  int timeout = call->args[2];\n\n  void* ptr = prep_syscall();\n  struct pollfd* fds2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (fds && nfds > 0) {\n    fds2 = ptr;\n    ptr += nfds * sizeof(*fds2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  if (fds2) {\n    memcpy_input_parameter(fds2, fds, nfds * sizeof(*fds2));\n  }\n\n  __before_poll_syscall_breakpoint();\n\n  ret = untraced_syscall3(syscallno, fds2, nfds, timeout);\n\n  if (fds2 && ret >= 0) {\n    /* NB: even when poll returns 0 indicating no pending\n     * fds, it still sets each .revent outparam to 0.\n     * (Reasonably.)  So we always need to copy on return\n     * value >= 0.\n     * It's important that we not copy when there's an error.\n     * The syscallbuf commit might have been aborted, which means\n     * during replay fds2 might be non-recorded data, so we'd be\n     * incorrectly trashing 'fds'. */\n    local_memcpy(fds, fds2, nfds * sizeof(*fds));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\n#define CLONE_SIZE_THRESHOLD 0x10000\n\nstatic long sys_read(const struct syscall_info* call) {\n  const int syscallno = SYS_read;\n  int fd = call->args[0];\n  void* buf = (void*)call->args[1];\n  size_t count = call->args[2];\n\n  void* ptr;\n  void* buf2 = NULL;\n  long ret;\n\n  /* Try cloning data using CLONE_RANGE ioctl.\n   * XXX switch to FIOCLONERANGE when that's more widely available. It's the\n   * same ioctl number so it won't affect rr per se but it'd be cleaner code.\n   * 64-bit only for now, since lseek and pread64 need special handling for\n   * 32-bit.\n   * Basically we break down the read into three syscalls lseek, clone and\n   * read-from-clone, each of which is individually syscall-buffered.\n   * Crucially, the read-from-clone syscall does NOT store data in the syscall\n   * buffer; instead, we perform the syscall during replay, assuming that\n   * cloned_file_data_fd is open to the same file during replay.\n   * Reads that hit EOF are rejected by the CLONE_RANGE ioctl so we take the\n   * slow path. That's OK.\n   * There is a possible race here: between cloning the data and reading from\n   * |fd|, |fd|'s data may be overwritten, in which case the data read during\n   * replay will not match the data read during recording, causing divergence.\n   * I don't see any performant way to avoid this race; I tried reading from\n   * the cloned data instead of |fd|, but that is very slow because readahead\n   * doesn't work. (The cloned data file always ends at the current offset so\n   * there is nothing to readahead.) However, if an application triggers this\n   * race, it's almost certainly a bad bug because Linux can return any\n   * interleaving of old+new data for the read even without rr.\n   */\n  if (buf && count >= CLONE_SIZE_THRESHOLD && cloned_file_data_fd >= 0 &&\n      is_bufferable_fd(fd) && sizeof(void*) == 8 && !(count & 4095)) {\n    struct syscall_info lseek_call = { SYS_lseek,\n                                       { fd, 0, SEEK_CUR, 0, 0, 0 } };\n    off_t lseek_ret = sys_generic_nonblocking_fd(&lseek_call);\n    if (lseek_ret > 0 && !(lseek_ret & 4095)) {\n      struct btrfs_ioctl_clone_range_args ioctl_args;\n      int ioctl_ret;\n      void* ioctl_ptr = prep_syscall();\n      ioctl_args.src_fd = fd;\n      ioctl_args.src_offset = lseek_ret;\n      ioctl_args.src_length = count;\n      ioctl_args.dest_offset = cloned_file_data_offset;\n\n      /* Don't call sys_ioctl here; cloned_file_data_fd has syscall buffering\n       * disabled for it so rr can reject attempts to close/dup to it. But\n       * we want to allow syscall buffering of this ioctl on it.\n       */\n      if (!start_commit_buffered_syscall(SYS_ioctl, ioctl_ptr, WONT_BLOCK)) {\n        struct syscall_info ioctl_call = { SYS_ioctl,\n                                           { cloned_file_data_fd,\n                                             BTRFS_IOC_CLONE_RANGE,\n                                             (long)&ioctl_args, 0, 0, 0 } };\n        ioctl_ret = traced_raw_syscall(&ioctl_call);\n      } else {\n        ioctl_ret = untraced_syscall3(SYS_ioctl, cloned_file_data_fd,\n                                      BTRFS_IOC_CLONE_RANGE, &ioctl_args);\n        ioctl_ret = commit_raw_syscall(SYS_ioctl, ioctl_ptr, ioctl_ret);\n      }\n\n      if (ioctl_ret >= 0) {\n        struct syscall_info read_call = { SYS_read,\n                                          { fd, (long)buf, count, 0, 0, 0 } };\n        cloned_file_data_offset += count;\n\n        replay_only_syscall2(SYS_dup2, cloned_file_data_fd, fd);\n\n        ptr = prep_syscall();\n        if (count > scratch_size) {\n          if (!start_commit_buffered_syscall(SYS_read, ptr, WONT_BLOCK)) {\n            return traced_raw_syscall(&read_call);\n          }\n          ret = untraced_replayed_syscall3(SYS_read, fd, buf, count);\n        } else {\n          if (!start_commit_buffered_syscall(SYS_read, ptr, MAY_BLOCK)) {\n            return traced_raw_syscall(&read_call);\n          }\n          ret = untraced_replayed_syscall3(SYS_read, fd, scratch_buf, count);\n          copy_output_buffer(ret, NULL, buf, scratch_buf);\n        }\n        // Do this now before we finish processing the syscallbuf record.\n        // This means the syscall will be executed in\n        // ReplaySession::flush_syscallbuf instead of\n        // ReplaySession::enter_syscall or something similar.\n        replay_only_syscall1(SYS_close, fd);\n        ret = commit_raw_syscall(SYS_read, ptr, ret);\n        return ret;\n      }\n    }\n  }\n\n  ptr = prep_syscall_for_fd(fd);\n\n  assert(syscallno == call->no);\n\n  if (buf && count > 0) {\n    buf2 = ptr;\n    ptr += count;\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, fd, buf2, count);\n  ptr = copy_output_buffer(ret, ptr, buf, buf2);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_readlink(const struct syscall_info* call) {\n  const int syscallno = SYS_readlink;\n  const char* path = (const char*)call->args[0];\n  char* buf = (char*)call->args[1];\n  int bufsiz = call->args[2];\n\n  void* ptr = prep_syscall();\n  char* buf2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (buf && bufsiz > 0) {\n    buf2 = ptr;\n    ptr += bufsiz;\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, path, buf2, bufsiz);\n  ptr = copy_output_buffer(ret, ptr, buf, buf2);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\n#if defined(SYS_socketcall)\nstatic long sys_socketcall_recv(const struct syscall_info* call) {\n  const int syscallno = SYS_socketcall;\n  long* args = (long*)call->args[1];\n  int sockfd = args[0];\n  void* buf = (void*)args[1];\n  size_t len = args[2];\n  unsigned int flags = args[3];\n  unsigned long new_args[4];\n\n  void* ptr = prep_syscall_for_fd(sockfd);\n  void* buf2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (buf && len > 0) {\n    buf2 = ptr;\n    ptr += len;\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  new_args[0] = sockfd;\n  new_args[1] = (unsigned long)buf2;\n  new_args[2] = len;\n  new_args[3] = flags;\n  ret = untraced_syscall2(SYS_socketcall, SYS_RECV, new_args);\n  ptr = copy_output_buffer(ret, ptr, buf, buf2);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_socketcall(const struct syscall_info* call) {\n  switch (call->args[0]) {\n    case SYS_RECV:\n      return sys_socketcall_recv(call);\n    default:\n      return traced_raw_syscall(call);\n  }\n}\n#endif\n\n#ifdef SYS_recvfrom\nstatic long sys_recvfrom(const struct syscall_info* call) {\n  const int syscallno = SYS_recvfrom;\n  int sockfd = call->args[0];\n  void* buf = (void*)call->args[1];\n  size_t len = call->args[2];\n  int flags = call->args[3];\n  /* struct sockaddr isn't useful here since some sockaddrs are bigger than\n   * it. To avoid making false assumptions, treat the sockaddr parameter\n   * as an untyped buffer.\n   */\n  void* src_addr = (void*)call->args[4];\n  socklen_t* addrlen = (socklen_t*)call->args[5];\n\n  void* ptr = prep_syscall_for_fd(sockfd);\n  void* buf2 = NULL;\n  struct sockaddr* src_addr2 = NULL;\n  socklen_t* addrlen2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n  /* If addrlen is NULL then src_addr must also be null */\n  assert(addrlen || !src_addr);\n\n  if (src_addr) {\n    src_addr2 = ptr;\n    ptr += *addrlen;\n  }\n  if (addrlen) {\n    addrlen2 = ptr;\n    ptr += sizeof(*addrlen);\n  }\n  if (buf && len > 0) {\n    buf2 = ptr;\n    ptr += len;\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  if (addrlen) {\n    memcpy_input_parameter(addrlen2, addrlen, sizeof(*addrlen2));\n  }\n  ret = untraced_syscall6(syscallno, sockfd, buf2, len, flags, src_addr2,\n                          addrlen2);\n\n  if (ret >= 0) {\n    if (src_addr2) {\n      socklen_t actual_size = *addrlen2;\n      if (actual_size > *addrlen) {\n        actual_size = *addrlen;\n      }\n      local_memcpy(src_addr, src_addr2, actual_size);\n    }\n    if (addrlen2) {\n      *addrlen = *addrlen2;\n    }\n  }\n  ptr = copy_output_buffer(ret, ptr, buf, buf2);\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n#endif\n\n#ifdef SYS_recvmsg\nstatic long sys_recvmsg(const struct syscall_info* call) {\n  const int syscallno = SYS_recvmsg;\n  int sockfd = call->args[0];\n  struct msghdr* msg = (struct msghdr*)call->args[1];\n  int flags = call->args[2];\n\n  void* ptr = prep_syscall_for_fd(sockfd);\n  long ret;\n  struct msghdr* msg2;\n  void* ptr_base = ptr;\n  void* ptr_overwritten_end;\n  void* ptr_bytes_start;\n  void* ptr_end;\n  size_t i;\n\n  assert(syscallno == call->no);\n\n  /* Compute final buffer size up front, before writing syscall inputs to the\n   * buffer. Thus if we decide not to buffer this syscall, we bail out\n   * before trying to write to a buffer that won't be recorded and may be\n   * invalid (e.g. overflow).\n   */\n  ptr += sizeof(struct msghdr) + sizeof(struct iovec) * msg->msg_iovlen;\n  if (msg->msg_name) {\n    ptr += msg->msg_namelen;\n  }\n  if (msg->msg_control) {\n    ptr += msg->msg_controllen;\n  }\n  for (i = 0; i < msg->msg_iovlen; ++i) {\n    ptr += msg->msg_iov[i].iov_len;\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  /**\n   * The kernel only writes to the struct msghdr, and the iov buffers. We must\n   * not overwrite that data (except using memcpy_input_parameter) during\n   * replay. For the rest of the data, the values we write here during replay\n   * are guaranteed to match what was recorded in the buffer.\n   * We can't rely on the values we wrote here during recording also being\n   * here during replay since the syscall might have been aborted and our\n   * written data not recorded.\n   */\n  msg2 = ptr = ptr_base;\n  memcpy_input_parameter(msg2, msg, sizeof(*msg));\n  ptr += sizeof(struct msghdr);\n  msg2->msg_iov = ptr;\n  ptr += sizeof(struct iovec) * msg->msg_iovlen;\n  ptr_overwritten_end = ptr;\n  if (msg->msg_name) {\n    msg2->msg_name = ptr;\n    ptr += msg->msg_namelen;\n  }\n  if (msg->msg_control) {\n    msg2->msg_control = ptr;\n    ptr += msg->msg_controllen;\n  }\n  ptr_bytes_start = ptr;\n  for (i = 0; i < msg->msg_iovlen; ++i) {\n    msg2->msg_iov[i].iov_base = ptr;\n    ptr += msg->msg_iov[i].iov_len;\n    msg2->msg_iov[i].iov_len = msg->msg_iov[i].iov_len;\n  }\n\n  ret = untraced_syscall3(syscallno, sockfd, msg2, flags);\n\n  if (ret >= 0) {\n    size_t bytes = ret;\n    size_t i;\n    if (msg->msg_name) {\n      local_memcpy(msg->msg_name, msg2->msg_name, msg2->msg_namelen);\n    }\n    msg->msg_namelen = msg2->msg_namelen;\n    if (msg->msg_control) {\n      local_memcpy(msg->msg_control, msg2->msg_control, msg2->msg_controllen);\n    }\n    msg->msg_controllen = msg2->msg_controllen;\n    ptr_end = ptr_bytes_start + bytes;\n    for (i = 0; i < msg->msg_iovlen; ++i) {\n      long copy_bytes =\n          bytes < msg->msg_iov[i].iov_len ? bytes : msg->msg_iov[i].iov_len;\n      local_memcpy(msg->msg_iov[i].iov_base, msg2->msg_iov[i].iov_base,\n                   copy_bytes);\n      bytes -= copy_bytes;\n    }\n    msg->msg_flags = msg2->msg_flags;\n\n    if (msg->msg_controllen) {\n      /* When we reach a safe point, notify rr that the control message was\n       * received.\n       */\n      notify_control_msg = msg;\n    }\n  } else {\n    /* Allocate record space as least to cover the data we overwrote above.\n     * We don't want to start the next record overlapping that data, since then\n     * we'll corrupt it during replay.\n     */\n    ptr_end = ptr_overwritten_end;\n  }\n  return commit_raw_syscall(syscallno, ptr_end, ret);\n}\n#endif\n\n#ifdef SYS_sendmsg\nstatic long sys_sendmsg(const struct syscall_info* call) {\n  const int syscallno = SYS_sendmsg;\n  int sockfd = call->args[0];\n  struct msghdr* msg = (struct msghdr*)call->args[1];\n  int flags = call->args[2];\n\n  void* ptr = prep_syscall_for_fd(sockfd);\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, sockfd, msg, flags);\n\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n#endif\n\n#ifdef SYS_socketpair\ntypedef int two_ints[2];\nstatic long sys_socketpair(const struct syscall_info* call) {\n  const int syscallno = SYS_socketpair;\n  int domain = call->args[0];\n  int type = call->args[1];\n  int protocol = call->args[2];\n  two_ints* sv = (two_ints*)call->args[3];\n\n  void* ptr = prep_syscall();\n  struct timezone* sv2 = NULL;\n  long ret;\n\n  assert(syscallno == call->no);\n\n  sv2 = ptr;\n  ptr += sizeof(*sv2);\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall4(syscallno, domain, type, protocol, sv2);\n  local_memcpy(sv, sv2, sizeof(*sv));\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n#endif\n\nstatic long sys_time(const struct syscall_info* call) {\n  const int syscallno = SYS_time;\n  time_t* tp = (time_t*)call->args[0];\n\n  void* ptr = prep_syscall();\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall1(syscallno, NULL);\n  if (tp) {\n    /* No error is possible here. */\n    *tp = ret;\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_xstat64(const struct syscall_info* call) {\n  const int syscallno = call->no;\n  /* NB: this arg may be a string or an fd, but for the purposes\n   * of this generic helper we don't care. */\n  long what = call->args[0];\n  struct stat64* buf = (struct stat64*)call->args[1];\n\n  /* Like open(), not arming the desched event because it's not\n   * needed for correctness, and there are no data to suggest\n   * whether it's a good idea perf-wise. */\n  void* ptr = prep_syscall();\n  struct stat64* buf2 = NULL;\n  long ret;\n\n  if (buf) {\n    buf2 = ptr;\n    ptr += sizeof(*buf2);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n  ret = untraced_syscall2(syscallno, what, buf2);\n  if (buf2) {\n    local_memcpy(buf, buf2, sizeof(*buf));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_write(const struct syscall_info* call) {\n  const int syscallno = SYS_write;\n  int fd = call->args[0];\n  const void* buf = (const void*)call->args[1];\n  size_t count = call->args[2];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, fd, buf, count);\n\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_writev(const struct syscall_info* call) {\n  int syscallno = SYS_writev;\n  int fd = call->args[0];\n  const struct iovec* iov = (const struct iovec*)call->args[1];\n  unsigned long iovcnt = call->args[2];\n\n  void* ptr = prep_syscall_for_fd(fd);\n  long ret;\n\n  assert(syscallno == call->no);\n\n  if (!start_commit_buffered_syscall(syscallno, ptr, MAY_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall3(syscallno, fd, iov, iovcnt);\n\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long sys_getrusage(const struct syscall_info* call) {\n  const int syscallno = SYS_getrusage;\n  int who = (int)call->args[0];\n  struct rusage* buf = (struct rusage*)call->args[1];\n  void* ptr = prep_syscall();\n  long ret;\n  struct rusage* buf2 = NULL;\n\n  assert(syscallno == call->no);\n\n  if (buf) {\n    buf2 = ptr;\n    ptr += sizeof(struct rusage);\n  }\n  if (!start_commit_buffered_syscall(syscallno, ptr, WONT_BLOCK)) {\n    return traced_raw_syscall(call);\n  }\n\n  ret = untraced_syscall2(syscallno, who, buf2);\n  if (buf2 && ret >= 0) {\n    local_memcpy(buf, buf2, sizeof(*buf));\n  }\n  return commit_raw_syscall(syscallno, ptr, ret);\n}\n\nstatic long syscall_hook_internal(const struct syscall_info* call) {\n  switch (call->no) {\n#define CASE(syscallname)                                                      \\\n  case SYS_##syscallname:                                                      \\\n    return sys_##syscallname(call)\n#define CASE_GENERIC_NONBLOCKING(syscallname)                                  \\\n  case SYS_##syscallname:                                                      \\\n    return sys_generic_nonblocking(call)\n#define CASE_GENERIC_NONBLOCKING_FD(syscallname)                               \\\n  case SYS_##syscallname:                                                      \\\n    return sys_generic_nonblocking_fd(call)\n    CASE_GENERIC_NONBLOCKING(access);\n    CASE(clock_gettime);\n    CASE_GENERIC_NONBLOCKING_FD(close);\n    CASE(creat);\n    CASE_GENERIC_NONBLOCKING(fchmod);\n    CASE_GENERIC_NONBLOCKING_FD(fadvise64);\n#if defined(SYS_fcntl64)\n    CASE(fcntl64);\n#else\n    CASE(fcntl);\n#endif\n    CASE(fgetxattr);\n    CASE(flistxattr);\n    CASE_GENERIC_NONBLOCKING_FD(fsetxattr);\n    CASE(futex);\n    CASE(getdents);\n    CASE(getdents64);\n    CASE_GENERIC_NONBLOCKING(geteuid);\n    CASE_GENERIC_NONBLOCKING(getpid);\n    CASE(getrusage);\n    CASE_GENERIC_NONBLOCKING(gettid);\n    CASE(gettimeofday);\n    CASE(getxattr);\n    CASE(ioctl);\n    CASE_GENERIC_NONBLOCKING(lchown);\n    CASE(lgetxattr);\n    CASE(listxattr);\n    CASE(llistxattr);\n#if defined(SYS__llseek)\n    CASE(_llseek);\n#endif\n    CASE_GENERIC_NONBLOCKING_FD(lseek);\n    CASE(madvise);\n    CASE_GENERIC_NONBLOCKING(mkdir);\n    CASE_GENERIC_NONBLOCKING(mknod);\n    CASE(mprotect);\n    CASE(open);\n    CASE(poll);\n    CASE(read);\n    CASE(readlink);\n#if defined(SYS_recvfrom)\n    CASE(recvfrom);\n#endif\n#if defined(SYS_recvmsg)\n    CASE(recvmsg);\n#endif\n#if defined(SYS_sendmsg)\n    CASE(sendmsg);\n#endif\n    CASE_GENERIC_NONBLOCKING(setxattr);\n#if defined(SYS_socketcall)\n    CASE(socketcall);\n#endif\n#if defined(SYS_socketpair)\n    CASE(socketpair);\n#endif\n    CASE_GENERIC_NONBLOCKING(symlink);\n    CASE(time);\n    CASE_GENERIC_NONBLOCKING_FD(utimensat);\n    CASE(write);\n    CASE(writev);\n#undef CASE\n#if defined(SYS_fstat64)\n    case SYS_fstat64:\n#else\n    case SYS_fstat:\n#endif\n#if defined(SYS_lstat64)\n    case SYS_lstat64:\n#else\n    case SYS_lstat:\n#endif\n#if defined(SYS_stat64)\n    case SYS_stat64:\n#else\n    case SYS_stat:\n#endif\n      return sys_xstat64(call);\n    default:\n      return traced_raw_syscall(call);\n  }\n}\n\n/* Explicitly declare this as hidden so we can call it from\n * _syscall_hook_trampoline without doing all sorts of special PIC handling.\n */\nRR_HIDDEN long syscall_hook(const struct syscall_info* call) {\n  if (!globals.thread_locals_initialized) {\n    return traced_raw_syscall(call);\n  }\n  long result = syscall_hook_internal(call);\n  if (buffer_hdr() && buffer_hdr()->notify_on_syscall_hook_exit) {\n    // SYS_rrcall_notify_syscall_hook_exit will clear\n    // notify_on_syscall_hook_exit. Clearing it ourselves is tricky to get\n    // right without races.\n    //\n    // During recording, this flag is set when the recorder needs to delay\n    // delivery of a signal until we've stopped using the syscallbuf.\n    // During replay, this flag is set when the next event is entering a\n    // SYS_rrcall_notify_syscall_hook_exit.\n    //\n    // The correctness argument is as follows:\n    // Correctness requires that a) replay's setting of the flag happens before\n    // we read the flag in the call to syscall_hook that triggered the\n    // SYS_rrcall_notify_syscall_hook_exit and b) replay's setting of the flag\n    // must happen after we read the flag in the previous execution of\n    // syscall_hook.\n    // Condition a) holds as long as no events are recorded between the\n    // checking of the flag above and the execution of this syscall. This\n    // should be the case; no synchronous signals or syscalls are\n    // triggerable, all async signals other than SYSCALLBUF_DESCHED_SIGNAL\n    // are delayed, and SYSCALLBUF_DESCHED_SIGNAL shouldn't fire since we've\n    // disarmed the desched fd at this point. SYSCALLBUF_FLUSH events may be\n    // emitted when we process the SYS_rrcall_notify_syscall_hook_exit event,\n    // but replay of those events ends at the last flushed syscall, before\n    // we exit syscall_hook_internal.\n    // Condition b) failing would mean no new events were generated between\n    // testing the flag in the previous syscall_hook and the execution of this\n    // SYS_rrcall_notify_syscall_hook_exit. However, every invocation of\n    // syscall_hook_internal generates either a traced syscall or a syscallbuf\n    // record that would be flushed by SYSCALLBUF_FLUSH, so that can't\n    // happen.\n    //\n    // Another crazy thing is going on here: it's possible that a signal\n    // intended to be delivered\n    result = _raw_syscall(SYS_rrcall_notify_syscall_hook_exit, call->args[0],\n                          call->args[1], call->args[2], call->args[3],\n                          call->args[4], call->args[5],\n                          RR_PAGE_SYSCALL_PRIVILEGED_TRACED, result, call->no);\n  }\n  // Do work that can only be safely done after syscallbuf can be flushed\n  if (notify_control_msg) {\n    privileged_traced_syscall1(SYS_rrcall_notify_control_msg,\n                               notify_control_msg);\n    notify_control_msg = NULL;\n  }\n  return result;\n}\n\n/**\n * Exported glibc synonym for |sysconf()|.  We can't use |dlsym()| to\n * resolve the next \"sysconf\" symbol, because\n *  - dlysym usually calls malloc()\n *  - custom allocators like jemalloc may use sysconf()\n *  - if our sysconf wrapper is re-entered during initialization, it\n *    has nothing to fall back on to get the conf name, and chaos will\n *    likely ensue if we return something random.\n */\nlong __sysconf(int name);\n\n/**\n *  Pretend that only 1 processor is configured/online, because rr\n *  binds all tracees to one logical CPU.\n */\nlong sysconf(int name) {\n  switch (name) {\n    case _SC_NPROCESSORS_ONLN:\n    case _SC_NPROCESSORS_CONF:\n      return globals.pretend_num_cores ? globals.pretend_num_cores : 1;\n  }\n  return __sysconf(name);\n}\n\n/** Disable XShm since rr doesn't work with it */\nint XShmQueryExtension(__attribute__((unused)) void* dpy) { return 0; }\n\n/** Make sure XShmCreateImage returns null in case an application doesn't do\n    extension checks first. */\nvoid* XShmCreateImage(__attribute__((unused)) register void* dpy,\n                      __attribute__((unused)) register void* visual,\n                      __attribute__((unused)) unsigned int depth,\n                      __attribute__((unused)) int format,\n                      __attribute__((unused)) char* data,\n                      __attribute__((unused)) void* shminfo,\n                      __attribute__((unused)) unsigned int width,\n                      __attribute__((unused)) unsigned int height) {\n  return 0;\n}\n\n/**\n * This is for testing purposes only.\n */\nvoid very_slow_exit_syscall(void) {\n  int i;\n  int result = 0;\n  struct syscall_info info;\n  for (i = 0; i < 1000000; ++i) {\n    result += i * i;\n  }\n  info.no = SYS_exit;\n  info.args[0] = result;\n  syscall_hook(&info);\n}\n\n/**\n * glibc geteuid() can be compiled to instructions ending in \"syscall; ret\"\n * which can't be hooked. So override it here and call the hook directly.\n */\nuid_t geteuid(void) {\n  struct syscall_info call = { SYS_geteuid, { 0, 0, 0, 0, 0, 0 } };\n  return syscall_hook(&call);\n}\n\ntypedef void* (*fopen_ptr)(const char* filename, const char* mode);\n\nstatic void random_device_init_helper(void* this) {\n  void** file_ptr = (void**)this;\n  void* f_ptr = dlsym(RTLD_DEFAULT, \"fopen\");\n  fopen_ptr fopen = (fopen_ptr)f_ptr;\n  *file_ptr = fopen(\"/dev/urandom\", \"rb\");\n}\n\n/**\n * libstdc++3 uses RDRAND. Bypass that with this incredible hack.\n */\nvoid _ZNSt13random_device7_M_initERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE(\n    void* this, __attribute__((unused)) void* token) {\n  random_device_init_helper(this);\n}\n\n/**\n * gcc 4.8.4 in Ubuntu 14.04-32\n */\nvoid _ZNSt13random_device7_M_initERKSs(void* this,\n                                       __attribute__((unused)) void* token) {\n  random_device_init_helper(this);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-rr-4.3.0-6iirswmpvavrsquath3ng4aepljcfxgv/spack-src/src/test/exit_with_syscallbuf_signal.c": "/* -*- Mode: C; tab-width: 8; c-basic-offset: 2; indent-tabs-mode: nil; -*- */\n\n#include \"rrutil.h\"\n\ntypedef void (*NullCall)(void);\nstatic NullCall very_slow_exit_syscall;\n\nstatic void* run_child(__attribute__((unused)) void* arg) {\n  /* context-switch events will happen during our long delay in the syscallbuf.\n     These will be queued and must be processed during exit.\n     In general these could be other signals that must not be dropped\n     so we want to handle them. */\n  very_slow_exit_syscall();\n  test_assert(0 && \"Should not reach here!\");\n  return 0;\n}\n\nint main(void) {\n  pthread_t thread;\n\n  very_slow_exit_syscall = dlsym(RTLD_DEFAULT, \"very_slow_exit_syscall\");\n  if (!very_slow_exit_syscall) {\n    atomic_puts(\"syscallbuf not loaded\");\n    atomic_puts(\"EXIT-SUCCESS\");\n    return 0;\n  }\n\n  pthread_create(&thread, NULL, run_child, NULL);\n  pthread_join(thread, NULL);\n\n  atomic_puts(\"EXIT-SUCCESS\");\n  return 0;\n}\n"
    },
    "skipped": [],
    "total_files": 817
}