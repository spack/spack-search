{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/runtime/src/z_Linux_util.c": "/*\n * z_Linux_util.c -- platform specific routines.\n */\n\n\n//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n\n\n#include \"kmp.h\"\n#include \"kmp_wrapper_getpid.h\"\n#include \"kmp_itt.h\"\n#include \"kmp_str.h\"\n#include \"kmp_i18n.h\"\n#include \"kmp_io.h\"\n#include \"kmp_stats.h\"\n#include \"kmp_wait_release.h\"\n\n#if !KMP_OS_FREEBSD && !KMP_OS_NETBSD\n# include <alloca.h>\n#endif\n#include <unistd.h>\n#include <math.h>               // HUGE_VAL.\n#include <sys/time.h>\n#include <sys/times.h>\n#include <sys/resource.h>\n#include <sys/syscall.h>\n\n#if KMP_OS_LINUX && !KMP_OS_CNK\n# include <sys/sysinfo.h>\n# if KMP_OS_LINUX && (KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_ARCH_ARM || KMP_ARCH_AARCH64)\n// We should really include <futex.h>, but that causes compatibility problems on different\n// Linux* OS distributions that either require that you include (or break when you try to include)\n// <pci/types.h>.\n// Since all we need is the two macros below (which are part of the kernel ABI, so can't change)\n// we just define the constants here and don't include <futex.h>\n#  ifndef FUTEX_WAIT\n#   define FUTEX_WAIT    0\n#  endif\n#  ifndef FUTEX_WAKE\n#   define FUTEX_WAKE    1\n#  endif\n# endif\n#elif KMP_OS_DARWIN\n# include <sys/sysctl.h>\n# include <mach/mach.h>\n#elif KMP_OS_FREEBSD\n# include <pthread_np.h>\n#endif\n\n\n#include <dirent.h>\n#include <ctype.h>\n#include <fcntl.h>\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\nstruct kmp_sys_timer {\n    struct timespec     start;\n};\n\n// Convert timespec to nanoseconds.\n#define TS2NS(timespec) (((timespec).tv_sec * 1e9) + (timespec).tv_nsec)\n\nstatic struct kmp_sys_timer __kmp_sys_timer_data;\n\n#if KMP_HANDLE_SIGNALS\n    typedef void                            (* sig_func_t )( int );\n    STATIC_EFI2_WORKAROUND struct sigaction    __kmp_sighldrs[ NSIG ];\n    static sigset_t                            __kmp_sigset;\n#endif\n\nstatic int __kmp_init_runtime   = FALSE;\n\nstatic int __kmp_fork_count = 0;\n\nstatic pthread_condattr_t  __kmp_suspend_cond_attr;\nstatic pthread_mutexattr_t __kmp_suspend_mutex_attr;\n\nstatic kmp_cond_align_t    __kmp_wait_cv;\nstatic kmp_mutex_align_t   __kmp_wait_mx;\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n#ifdef DEBUG_SUSPEND\nstatic void\n__kmp_print_cond( char *buffer, kmp_cond_align_t *cond )\n{\n    KMP_SNPRINTF( buffer, 128, \"(cond (lock (%ld, %d)), (descr (%p)))\",\n                      cond->c_cond.__c_lock.__status, cond->c_cond.__c_lock.__spinlock,\n                      cond->c_cond.__c_waiting );\n}\n#endif\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n#if ( KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED)\n\n/*\n * Affinity support\n */\n\n/*\n * On some of the older OS's that we build on, these constants aren't present\n * in <asm/unistd.h> #included from <sys.syscall.h>.  They must be the same on\n * all systems of the same arch where they are defined, and they cannot change.\n * stone forever.\n */\n\n#  if KMP_ARCH_X86 || KMP_ARCH_ARM\n#   ifndef __NR_sched_setaffinity\n#    define __NR_sched_setaffinity  241\n#   elif __NR_sched_setaffinity != 241\n#    error Wrong code for setaffinity system call.\n#   endif /* __NR_sched_setaffinity */\n#   ifndef __NR_sched_getaffinity\n#    define __NR_sched_getaffinity  242\n#   elif __NR_sched_getaffinity != 242\n#    error Wrong code for getaffinity system call.\n#   endif /* __NR_sched_getaffinity */\n\n#  elif KMP_ARCH_AARCH64\n#   ifndef __NR_sched_setaffinity\n#    define __NR_sched_setaffinity  122\n#   elif __NR_sched_setaffinity != 122\n#    error Wrong code for setaffinity system call.\n#   endif /* __NR_sched_setaffinity */\n#   ifndef __NR_sched_getaffinity\n#    define __NR_sched_getaffinity  123\n#   elif __NR_sched_getaffinity != 123\n#    error Wrong code for getaffinity system call.\n#   endif /* __NR_sched_getaffinity */\n\n#  elif KMP_ARCH_X86_64\n#   ifndef __NR_sched_setaffinity\n#    define __NR_sched_setaffinity  203\n#   elif __NR_sched_setaffinity != 203\n#    error Wrong code for setaffinity system call.\n#   endif /* __NR_sched_setaffinity */\n#   ifndef __NR_sched_getaffinity\n#    define __NR_sched_getaffinity  204\n#   elif __NR_sched_getaffinity != 204\n#    error Wrong code for getaffinity system call.\n#   endif /* __NR_sched_getaffinity */\n\n#  elif KMP_ARCH_PPC64\n#   ifndef __NR_sched_setaffinity\n#    define __NR_sched_setaffinity  222\n#   elif __NR_sched_setaffinity != 222\n#    error Wrong code for setaffinity system call.\n#   endif /* __NR_sched_setaffinity */\n#   ifndef __NR_sched_getaffinity\n#    define __NR_sched_getaffinity  223\n#   elif __NR_sched_getaffinity != 223\n#    error Wrong code for getaffinity system call.\n#   endif /* __NR_sched_getaffinity */\n\n\n#  else\n#   error Unknown or unsupported architecture\n\n#  endif /* KMP_ARCH_* */\n\nint\n__kmp_set_system_affinity( kmp_affin_mask_t const *mask, int abort_on_error )\n{\n    KMP_ASSERT2(KMP_AFFINITY_CAPABLE(),\n      \"Illegal set affinity operation when not capable\");\n\n    int retval = syscall( __NR_sched_setaffinity, 0, __kmp_affin_mask_size, mask );\n    if (retval >= 0) {\n        return 0;\n    }\n    int error = errno;\n    if (abort_on_error) {\n        __kmp_msg(\n            kmp_ms_fatal,\n            KMP_MSG( FatalSysError ),\n            KMP_ERR( error ),\n            __kmp_msg_null\n        );\n    }\n    return error;\n}\n\nint\n__kmp_get_system_affinity( kmp_affin_mask_t *mask, int abort_on_error )\n{\n    KMP_ASSERT2(KMP_AFFINITY_CAPABLE(),\n      \"Illegal get affinity operation when not capable\");\n\n    int retval = syscall( __NR_sched_getaffinity, 0, __kmp_affin_mask_size, mask );\n    if (retval >= 0) {\n        return 0;\n    }\n    int error = errno;\n    if (abort_on_error) {\n        __kmp_msg(\n            kmp_ms_fatal,\n            KMP_MSG( FatalSysError ),\n            KMP_ERR( error ),\n            __kmp_msg_null\n        );\n    }\n    return error;\n}\n\nvoid\n__kmp_affinity_bind_thread( int which )\n{\n    KMP_ASSERT2(KMP_AFFINITY_CAPABLE(),\n      \"Illegal set affinity operation when not capable\");\n\n    kmp_affin_mask_t *mask = (kmp_affin_mask_t *)KMP_ALLOCA(__kmp_affin_mask_size);\n    KMP_CPU_ZERO(mask);\n    KMP_CPU_SET(which, mask);\n    __kmp_set_system_affinity(mask, TRUE);\n}\n\n/*\n * Determine if we can access affinity functionality on this version of\n * Linux* OS by checking __NR_sched_{get,set}affinity system calls, and set\n * __kmp_affin_mask_size to the appropriate value (0 means not capable).\n */\nvoid\n__kmp_affinity_determine_capable(const char *env_var)\n{\n    //\n    // Check and see if the OS supports thread affinity.\n    //\n\n# define KMP_CPU_SET_SIZE_LIMIT          (1024*1024)\n\n    int gCode;\n    int sCode;\n    kmp_affin_mask_t *buf;\n    buf = ( kmp_affin_mask_t * ) KMP_INTERNAL_MALLOC( KMP_CPU_SET_SIZE_LIMIT );\n\n    // If Linux* OS:\n    // If the syscall fails or returns a suggestion for the size,\n    // then we don't have to search for an appropriate size.\n    gCode = syscall( __NR_sched_getaffinity, 0, KMP_CPU_SET_SIZE_LIMIT, buf );\n    KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n       \"initial getaffinity call returned %d errno = %d\\n\",\n       gCode, errno));\n\n    //if ((gCode < 0) && (errno == ENOSYS))\n    if (gCode < 0) {\n        //\n        // System call not supported\n        //\n        if (__kmp_affinity_verbose || (__kmp_affinity_warnings\n          && (__kmp_affinity_type != affinity_none)\n          && (__kmp_affinity_type != affinity_default)\n          && (__kmp_affinity_type != affinity_disabled))) {\n            int error = errno;\n            __kmp_msg(\n                kmp_ms_warning,\n                KMP_MSG( GetAffSysCallNotSupported, env_var ),\n                KMP_ERR( error ),\n                __kmp_msg_null\n            );\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n    }\n    if (gCode > 0) { // Linux* OS only\n        // The optimal situation: the OS returns the size of the buffer\n        // it expects.\n        //\n        // A verification of correct behavior is that Isetaffinity on a NULL\n        // buffer with the same size fails with errno set to EFAULT.\n        sCode = syscall( __NR_sched_setaffinity, 0, gCode, NULL );\n        KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n           \"setaffinity for mask size %d returned %d errno = %d\\n\",\n           gCode, sCode, errno));\n        if (sCode < 0) {\n            if (errno == ENOSYS) {\n                if (__kmp_affinity_verbose || (__kmp_affinity_warnings\n                  && (__kmp_affinity_type != affinity_none)\n                  && (__kmp_affinity_type != affinity_default)\n                  && (__kmp_affinity_type != affinity_disabled))) {\n                    int error = errno;\n                    __kmp_msg(\n                        kmp_ms_warning,\n                        KMP_MSG( SetAffSysCallNotSupported, env_var ),\n                        KMP_ERR( error ),\n                        __kmp_msg_null\n                    );\n                }\n                KMP_AFFINITY_DISABLE();\n                KMP_INTERNAL_FREE(buf);\n            }\n            if (errno == EFAULT) {\n                KMP_AFFINITY_ENABLE(gCode);\n                KA_TRACE(10, ( \"__kmp_affinity_determine_capable: \"\n                  \"affinity supported (mask size %d)\\n\",\n                  (int)__kmp_affin_mask_size));\n                KMP_INTERNAL_FREE(buf);\n                return;\n            }\n        }\n    }\n\n    //\n    // Call the getaffinity system call repeatedly with increasing set sizes\n    // until we succeed, or reach an upper bound on the search.\n    //\n    KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n      \"searching for proper set size\\n\"));\n    int size;\n    for (size = 1; size <= KMP_CPU_SET_SIZE_LIMIT; size *= 2) {\n        gCode = syscall( __NR_sched_getaffinity, 0,  size, buf );\n        KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n          \"getaffinity for mask size %d returned %d errno = %d\\n\", size,\n            gCode, errno));\n\n        if (gCode < 0) {\n            if ( errno == ENOSYS )\n            {\n                //\n                // We shouldn't get here\n                //\n                KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n                  \"inconsistent OS call behavior: errno == ENOSYS for mask size %d\\n\",\n                   size));\n                if (__kmp_affinity_verbose || (__kmp_affinity_warnings\n                  && (__kmp_affinity_type != affinity_none)\n                  && (__kmp_affinity_type != affinity_default)\n                  && (__kmp_affinity_type != affinity_disabled))) {\n                    int error = errno;\n                    __kmp_msg(\n                        kmp_ms_warning,\n                        KMP_MSG( GetAffSysCallNotSupported, env_var ),\n                        KMP_ERR( error ),\n                        __kmp_msg_null\n                    );\n                }\n                KMP_AFFINITY_DISABLE();\n                KMP_INTERNAL_FREE(buf);\n                return;\n            }\n            continue;\n        }\n\n        sCode = syscall( __NR_sched_setaffinity, 0, gCode, NULL );\n        KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n           \"setaffinity for mask size %d returned %d errno = %d\\n\",\n           gCode, sCode, errno));\n        if (sCode < 0) {\n            if (errno == ENOSYS) { // Linux* OS only\n                //\n                // We shouldn't get here\n                //\n                KA_TRACE(30, ( \"__kmp_affinity_determine_capable: \"\n                  \"inconsistent OS call behavior: errno == ENOSYS for mask size %d\\n\",\n                   size));\n                if (__kmp_affinity_verbose || (__kmp_affinity_warnings\n                  && (__kmp_affinity_type != affinity_none)\n                  && (__kmp_affinity_type != affinity_default)\n                  && (__kmp_affinity_type != affinity_disabled))) {\n                    int error = errno;\n                    __kmp_msg(\n                        kmp_ms_warning,\n                        KMP_MSG( SetAffSysCallNotSupported, env_var ),\n                        KMP_ERR( error ),\n                        __kmp_msg_null\n                    );\n                }\n                KMP_AFFINITY_DISABLE();\n                KMP_INTERNAL_FREE(buf);\n                return;\n            }\n            if (errno == EFAULT) {\n                KMP_AFFINITY_ENABLE(gCode);\n                KA_TRACE(10, ( \"__kmp_affinity_determine_capable: \"\n                  \"affinity supported (mask size %d)\\n\",\n                   (int)__kmp_affin_mask_size));\n                KMP_INTERNAL_FREE(buf);\n                return;\n            }\n        }\n    }\n    //int error = errno;  // save uncaught error code\n    KMP_INTERNAL_FREE(buf);\n    // errno = error;  // restore uncaught error code, will be printed at the next KMP_WARNING below\n\n    //\n    // Affinity is not supported\n    //\n    KMP_AFFINITY_DISABLE();\n    KA_TRACE(10, ( \"__kmp_affinity_determine_capable: \"\n      \"cannot determine mask size - affinity not supported\\n\"));\n    if (__kmp_affinity_verbose || (__kmp_affinity_warnings\n      && (__kmp_affinity_type != affinity_none)\n      && (__kmp_affinity_type != affinity_default)\n      && (__kmp_affinity_type != affinity_disabled))) {\n        KMP_WARNING( AffCantGetMaskSize, env_var );\n    }\n}\n\n#endif // KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n#if KMP_OS_LINUX && (KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_ARCH_ARM || KMP_ARCH_AARCH64) && !KMP_OS_CNK\n\nint\n__kmp_futex_determine_capable()\n{\n    int loc = 0;\n    int rc = syscall( __NR_futex, &loc, FUTEX_WAKE, 1, NULL, NULL, 0 );\n    int retval = ( rc == 0 ) || ( errno != ENOSYS );\n\n    KA_TRACE(10, ( \"__kmp_futex_determine_capable: rc = %d errno = %d\\n\", rc,\n      errno ) );\n    KA_TRACE(10, ( \"__kmp_futex_determine_capable: futex syscall%s supported\\n\",\n        retval ? \"\" : \" not\" ) );\n\n    return retval;\n}\n\n#endif // KMP_OS_LINUX && (KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_ARCH_ARM) && !KMP_OS_CNK\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (! KMP_ASM_INTRINS)\n/*\n * Only 32-bit \"add-exchange\" instruction on IA-32 architecture causes us to\n * use compare_and_store for these routines\n */\n\nkmp_int8\n__kmp_test_then_or8( volatile kmp_int8 *p, kmp_int8 d )\n{\n    kmp_int8 old_value, new_value;\n\n    old_value = TCR_1( *p );\n    new_value = old_value | d;\n\n    while ( ! KMP_COMPARE_AND_STORE_REL8 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_1( *p );\n        new_value = old_value | d;\n    }\n    return old_value;\n}\n\nkmp_int8\n__kmp_test_then_and8( volatile kmp_int8 *p, kmp_int8 d )\n{\n    kmp_int8 old_value, new_value;\n\n    old_value = TCR_1( *p );\n    new_value = old_value & d;\n\n    while ( ! KMP_COMPARE_AND_STORE_REL8 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_1( *p );\n        new_value = old_value & d;\n    }\n    return old_value;\n}\n\nkmp_int32\n__kmp_test_then_or32( volatile kmp_int32 *p, kmp_int32 d )\n{\n    kmp_int32 old_value, new_value;\n\n    old_value = TCR_4( *p );\n    new_value = old_value | d;\n\n    while ( ! KMP_COMPARE_AND_STORE_REL32 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_4( *p );\n        new_value = old_value | d;\n    }\n    return old_value;\n}\n\nkmp_int32\n__kmp_test_then_and32( volatile kmp_int32 *p, kmp_int32 d )\n{\n    kmp_int32 old_value, new_value;\n\n    old_value = TCR_4( *p );\n    new_value = old_value & d;\n\n    while ( ! KMP_COMPARE_AND_STORE_REL32 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_4( *p );\n        new_value = old_value & d;\n    }\n    return old_value;\n}\n\n# if KMP_ARCH_X86 || KMP_ARCH_PPC64 || KMP_ARCH_AARCH64\nkmp_int8\n__kmp_test_then_add8( volatile kmp_int8 *p, kmp_int8 d )\n{\n    kmp_int8 old_value, new_value;\n\n    old_value = TCR_1( *p );\n    new_value = old_value + d;\n\n    while ( ! KMP_COMPARE_AND_STORE_REL8 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_1( *p );\n        new_value = old_value + d;\n    }\n    return old_value;\n}\n\nkmp_int64\n__kmp_test_then_add64( volatile kmp_int64 *p, kmp_int64 d )\n{\n    kmp_int64 old_value, new_value;\n\n    old_value = TCR_8( *p );\n    new_value = old_value + d;\n\n    while ( ! KMP_COMPARE_AND_STORE_REL64 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_8( *p );\n        new_value = old_value + d;\n    }\n    return old_value;\n}\n# endif /* KMP_ARCH_X86 */\n\nkmp_int64\n__kmp_test_then_or64( volatile kmp_int64 *p, kmp_int64 d )\n{\n    kmp_int64 old_value, new_value;\n\n    old_value = TCR_8( *p );\n    new_value = old_value | d;\n    while ( ! KMP_COMPARE_AND_STORE_REL64 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_8( *p );\n        new_value = old_value | d;\n    }\n    return old_value;\n}\n\nkmp_int64\n__kmp_test_then_and64( volatile kmp_int64 *p, kmp_int64 d )\n{\n    kmp_int64 old_value, new_value;\n\n    old_value = TCR_8( *p );\n    new_value = old_value & d;\n    while ( ! KMP_COMPARE_AND_STORE_REL64 ( p, old_value, new_value ) )\n    {\n        KMP_CPU_PAUSE();\n        old_value = TCR_8( *p );\n        new_value = old_value & d;\n    }\n    return old_value;\n}\n\n#endif /* (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (! KMP_ASM_INTRINS) */\n\nvoid\n__kmp_terminate_thread( int gtid )\n{\n    int status;\n    kmp_info_t  *th = __kmp_threads[ gtid ];\n\n    if ( !th ) return;\n\n    #ifdef KMP_CANCEL_THREADS\n        KA_TRACE( 10, (\"__kmp_terminate_thread: kill (%d)\\n\", gtid ) );\n        status = pthread_cancel( th->th.th_info.ds.ds_thread );\n        if ( status != 0 && status != ESRCH ) {\n            __kmp_msg(\n                kmp_ms_fatal,\n                KMP_MSG( CantTerminateWorkerThread ),\n                KMP_ERR( status ),\n                __kmp_msg_null\n            );\n        }; // if\n    #endif\n    __kmp_yield( TRUE );\n} //\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n/*\n * Set thread stack info according to values returned by\n * pthread_getattr_np().\n * If values are unreasonable, assume call failed and use\n * incremental stack refinement method instead.\n * Returns TRUE if the stack parameters could be determined exactly,\n * FALSE if incremental refinement is necessary.\n */\nstatic kmp_int32\n__kmp_set_stack_info( int gtid, kmp_info_t *th )\n{\n    int            stack_data;\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n    /* Linux* OS only -- no pthread_getattr_np support on OS X* */\n    pthread_attr_t attr;\n    int            status;\n    size_t         size = 0;\n    void *         addr = 0;\n\n    /* Always do incremental stack refinement for ubermaster threads since the initial\n       thread stack range can be reduced by sibling thread creation so pthread_attr_getstack\n       may cause thread gtid aliasing */\n    if ( ! KMP_UBER_GTID(gtid) ) {\n\n        /* Fetch the real thread attributes */\n        status = pthread_attr_init( &attr );\n        KMP_CHECK_SYSFAIL( \"pthread_attr_init\", status );\n#if KMP_OS_FREEBSD || KMP_OS_NETBSD\n        status = pthread_attr_get_np( pthread_self(), &attr );\n        KMP_CHECK_SYSFAIL( \"pthread_attr_get_np\", status );\n#else\n        status = pthread_getattr_np( pthread_self(), &attr );\n        KMP_CHECK_SYSFAIL( \"pthread_getattr_np\", status );\n#endif\n        status = pthread_attr_getstack( &attr, &addr, &size );\n        KMP_CHECK_SYSFAIL( \"pthread_attr_getstack\", status );\n        KA_TRACE( 60, ( \"__kmp_set_stack_info: T#%d pthread_attr_getstack returned size: %lu, \"\n                        \"low addr: %p\\n\",\n                        gtid, size, addr ));\n\n        status = pthread_attr_destroy( &attr );\n        KMP_CHECK_SYSFAIL( \"pthread_attr_destroy\", status );\n    }\n\n    if ( size != 0 && addr != 0 ) {     /* was stack parameter determination successful? */\n        /* Store the correct base and size */\n        TCW_PTR(th->th.th_info.ds.ds_stackbase, (((char *)addr) + size));\n        TCW_PTR(th->th.th_info.ds.ds_stacksize, size);\n        TCW_4(th->th.th_info.ds.ds_stackgrow, FALSE);\n        return TRUE;\n    }\n#endif /* KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD */\n    /* Use incremental refinement starting from initial conservative estimate */\n    TCW_PTR(th->th.th_info.ds.ds_stacksize, 0);\n    TCW_PTR(th -> th.th_info.ds.ds_stackbase, &stack_data);\n    TCW_4(th->th.th_info.ds.ds_stackgrow, TRUE);\n    return FALSE;\n}\n\nstatic void*\n__kmp_launch_worker( void *thr )\n{\n    int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n    sigset_t    new_set, old_set;\n#endif /* KMP_BLOCK_SIGNALS */\n    void *exit_val;\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n    void * volatile padding = 0;\n#endif\n    int gtid;\n\n    gtid = ((kmp_info_t*)thr) -> th.th_info.ds.ds_gtid;\n    __kmp_gtid_set_specific( gtid );\n#ifdef KMP_TDATA_GTID\n    __kmp_gtid = gtid;\n#endif\n#if KMP_STATS_ENABLED\n    // set __thread local index to point to thread-specific stats\n    __kmp_stats_thread_ptr = ((kmp_info_t*)thr)->th.th_stats;\n#endif\n\n#if USE_ITT_BUILD\n    __kmp_itt_thread_name( gtid );\n#endif /* USE_ITT_BUILD */\n\n#if KMP_AFFINITY_SUPPORTED\n    __kmp_affinity_set_init_mask( gtid, FALSE );\n#endif\n\n#ifdef KMP_CANCEL_THREADS\n    status = pthread_setcanceltype( PTHREAD_CANCEL_ASYNCHRONOUS, & old_type );\n    KMP_CHECK_SYSFAIL( \"pthread_setcanceltype\", status );\n    /* josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads? */\n    status = pthread_setcancelstate( PTHREAD_CANCEL_ENABLE, & old_state );\n    KMP_CHECK_SYSFAIL( \"pthread_setcancelstate\", status );\n#endif\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n    //\n    // Set the FP control regs to be a copy of\n    // the parallel initialization thread's.\n    //\n    __kmp_clear_x87_fpu_status_word();\n    __kmp_load_x87_fpu_control_word( &__kmp_init_x87_fpu_control_word );\n    __kmp_load_mxcsr( &__kmp_init_mxcsr );\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n#ifdef KMP_BLOCK_SIGNALS\n    status = sigfillset( & new_set );\n    KMP_CHECK_SYSFAIL_ERRNO( \"sigfillset\", status );\n    status = pthread_sigmask( SIG_BLOCK, & new_set, & old_set );\n    KMP_CHECK_SYSFAIL( \"pthread_sigmask\", status );\n#endif /* KMP_BLOCK_SIGNALS */\n\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n    if ( __kmp_stkoffset > 0 && gtid > 0 ) {\n        padding = KMP_ALLOCA( gtid * __kmp_stkoffset );\n    }\n#endif\n\n    KMP_MB();\n    __kmp_set_stack_info( gtid, (kmp_info_t*)thr );\n\n    __kmp_check_stack_overlap( (kmp_info_t*)thr );\n\n    exit_val = __kmp_launch_thread( (kmp_info_t *) thr );\n\n#ifdef KMP_BLOCK_SIGNALS\n    status = pthread_sigmask( SIG_SETMASK, & old_set, NULL );\n    KMP_CHECK_SYSFAIL( \"pthread_sigmask\", status );\n#endif /* KMP_BLOCK_SIGNALS */\n\n    return exit_val;\n}\n\n\n/* The monitor thread controls all of the threads in the complex */\n\nstatic void*\n__kmp_launch_monitor( void *thr )\n{\n    int         status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n    sigset_t    new_set;\n#endif /* KMP_BLOCK_SIGNALS */\n    struct timespec  interval;\n    int yield_count;\n    int yield_cycles = 0;\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n    KA_TRACE( 10, (\"__kmp_launch_monitor: #1 launched\\n\" ) );\n\n    /* register us as the monitor thread */\n    __kmp_gtid_set_specific( KMP_GTID_MONITOR );\n#ifdef KMP_TDATA_GTID\n    __kmp_gtid = KMP_GTID_MONITOR;\n#endif\n\n    KMP_MB();\n\n#if USE_ITT_BUILD\n    __kmp_itt_thread_ignore();    // Instruct Intel(R) Threading Tools to ignore monitor thread.\n#endif /* USE_ITT_BUILD */\n\n    __kmp_set_stack_info( ((kmp_info_t*)thr)->th.th_info.ds.ds_gtid, (kmp_info_t*)thr );\n\n    __kmp_check_stack_overlap( (kmp_info_t*)thr );\n\n#ifdef KMP_CANCEL_THREADS\n    status = pthread_setcanceltype( PTHREAD_CANCEL_ASYNCHRONOUS, & old_type );\n    KMP_CHECK_SYSFAIL( \"pthread_setcanceltype\", status );\n    /* josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads? */\n    status = pthread_setcancelstate( PTHREAD_CANCEL_ENABLE, & old_state );\n    KMP_CHECK_SYSFAIL( \"pthread_setcancelstate\", status );\n#endif\n\n    #if KMP_REAL_TIME_FIX\n    // This is a potential fix which allows application with real-time scheduling policy work.\n    // However, decision about the fix is not made yet, so it is disabled by default.\n    { // Are program started with real-time scheduling policy?\n        int sched = sched_getscheduler( 0 );\n        if ( sched == SCHED_FIFO || sched == SCHED_RR ) {\n            // Yes, we are a part of real-time application. Try to increase the priority of the\n            // monitor.\n            struct sched_param param;\n            int    max_priority = sched_get_priority_max( sched );\n            int    rc;\n            KMP_WARNING( RealTimeSchedNotSupported );\n            sched_getparam( 0, & param );\n            if ( param.sched_priority < max_priority ) {\n                param.sched_priority += 1;\n                rc = sched_setscheduler( 0, sched, & param );\n                if ( rc != 0 ) {\n                    int error = errno;\n                  __kmp_msg(\n                      kmp_ms_warning,\n                      KMP_MSG( CantChangeMonitorPriority ),\n                      KMP_ERR( error ),\n                      KMP_MSG( MonitorWillStarve ),\n                      __kmp_msg_null\n                  );\n                }; // if\n            } else {\n                // We cannot abort here, because number of CPUs may be enough for all the threads,\n                // including the monitor thread, so application could potentially work...\n                __kmp_msg(\n                    kmp_ms_warning,\n                    KMP_MSG( RunningAtMaxPriority ),\n                    KMP_MSG( MonitorWillStarve ),\n                    KMP_HNT( RunningAtMaxPriority ),\n                    __kmp_msg_null\n                );\n            }; // if\n        }; // if\n        TCW_4( __kmp_global.g.g_time.dt.t_value, 0 );  // AC: free thread that waits for monitor started\n    }\n    #endif // KMP_REAL_TIME_FIX\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n    if ( __kmp_monitor_wakeups == 1 ) {\n        interval.tv_sec  = 1;\n        interval.tv_nsec = 0;\n    } else {\n        interval.tv_sec  = 0;\n        interval.tv_nsec = (KMP_NSEC_PER_SEC / __kmp_monitor_wakeups);\n    }\n\n    KA_TRACE( 10, (\"__kmp_launch_monitor: #2 monitor\\n\" ) );\n\n    if (__kmp_yield_cycle) {\n        __kmp_yielding_on = 0;  /* Start out with yielding shut off */\n        yield_count = __kmp_yield_off_count;\n    } else {\n        __kmp_yielding_on = 1;  /* Yielding is on permanently */\n    }\n\n    while( ! TCR_4( __kmp_global.g.g_done ) ) {\n        struct timespec  now;\n        struct timeval   tval;\n\n        /*  This thread monitors the state of the system */\n\n        KA_TRACE( 15, ( \"__kmp_launch_monitor: update\\n\" ) );\n\n        status = gettimeofday( &tval, NULL );\n        KMP_CHECK_SYSFAIL_ERRNO( \"gettimeofday\", status );\n        TIMEVAL_TO_TIMESPEC( &tval, &now );\n\n        now.tv_sec  += interval.tv_sec;\n        now.tv_nsec += interval.tv_nsec;\n\n        if (now.tv_nsec >= KMP_NSEC_PER_SEC) {\n            now.tv_sec  += 1;\n            now.tv_nsec -= KMP_NSEC_PER_SEC;\n        }\n\n        status = pthread_mutex_lock( & __kmp_wait_mx.m_mutex );\n        KMP_CHECK_SYSFAIL( \"pthread_mutex_lock\", status );\n        // AC: the monitor should not fall asleep if g_done has been set\n        if ( !TCR_4(__kmp_global.g.g_done) ) {  // check once more under mutex\n            status = pthread_cond_timedwait( &__kmp_wait_cv.c_cond, &__kmp_wait_mx.m_mutex, &now );\n            if ( status != 0 ) {\n                if ( status != ETIMEDOUT && status != EINTR ) {\n                    KMP_SYSFAIL( \"pthread_cond_timedwait\", status );\n                };\n            };\n        };\n        status = pthread_mutex_unlock( & __kmp_wait_mx.m_mutex );\n        KMP_CHECK_SYSFAIL( \"pthread_mutex_unlock\", status );\n\n        if (__kmp_yield_cycle) {\n            yield_cycles++;\n            if ( (yield_cycles % yield_count) == 0 ) {\n                if (__kmp_yielding_on) {\n                    __kmp_yielding_on = 0;   /* Turn it off now */\n                    yield_count = __kmp_yield_off_count;\n                } else {\n                    __kmp_yielding_on = 1;   /* Turn it on now */\n                    yield_count = __kmp_yield_on_count;\n                }\n                yield_cycles = 0;\n            }\n        } else {\n            __kmp_yielding_on = 1;\n        }\n\n        TCW_4( __kmp_global.g.g_time.dt.t_value,\n          TCR_4( __kmp_global.g.g_time.dt.t_value ) + 1 );\n\n        KMP_MB();       /* Flush all pending memory write invalidates.  */\n    }\n\n    KA_TRACE( 10, (\"__kmp_launch_monitor: #3 cleanup\\n\" ) );\n\n#ifdef KMP_BLOCK_SIGNALS\n    status = sigfillset( & new_set );\n    KMP_CHECK_SYSFAIL_ERRNO( \"sigfillset\", status );\n    status = pthread_sigmask( SIG_UNBLOCK, & new_set, NULL );\n    KMP_CHECK_SYSFAIL( \"pthread_sigmask\", status );\n#endif /* KMP_BLOCK_SIGNALS */\n\n    KA_TRACE( 10, (\"__kmp_launch_monitor: #4 finished\\n\" ) );\n\n    if( __kmp_global.g.g_abort != 0 ) {\n        /* now we need to terminate the worker threads  */\n        /* the value of t_abort is the signal we caught */\n\n        int gtid;\n\n        KA_TRACE( 10, (\"__kmp_launch_monitor: #5 terminate sig=%d\\n\", __kmp_global.g.g_abort ) );\n\n        /* terminate the OpenMP worker threads */\n        /* TODO this is not valid for sibling threads!!\n         * the uber master might not be 0 anymore.. */\n        for (gtid = 1; gtid < __kmp_threads_capacity; ++gtid)\n            __kmp_terminate_thread( gtid );\n\n        __kmp_cleanup();\n\n        KA_TRACE( 10, (\"__kmp_launch_monitor: #6 raise sig=%d\\n\", __kmp_global.g.g_abort ) );\n\n        if (__kmp_global.g.g_abort > 0)\n            raise( __kmp_global.g.g_abort );\n\n    }\n\n    KA_TRACE( 10, (\"__kmp_launch_monitor: #7 exit\\n\" ) );\n\n    return thr;\n}\n\nvoid\n__kmp_create_worker( int gtid, kmp_info_t *th, size_t stack_size )\n{\n    pthread_t      handle;\n    pthread_attr_t thread_attr;\n    int            status;\n\n\n    th->th.th_info.ds.ds_gtid = gtid;\n\n#if KMP_STATS_ENABLED\n    // sets up worker thread stats\n    __kmp_acquire_tas_lock(&__kmp_stats_lock, gtid);\n\n    // th->th.th_stats is used to transfer thread specific stats-pointer to __kmp_launch_worker\n    // So when thread is created (goes into __kmp_launch_worker) it will\n    // set it's __thread local pointer to th->th.th_stats\n    th->th.th_stats = __kmp_stats_list.push_back(gtid);\n    if(KMP_UBER_GTID(gtid)) {\n        __kmp_stats_start_time = tsc_tick_count::now();\n        __kmp_stats_thread_ptr = th->th.th_stats;\n        __kmp_stats_init();\n        KMP_START_EXPLICIT_TIMER(OMP_serial);\n        KMP_START_EXPLICIT_TIMER(OMP_start_end);\n    }\n    __kmp_release_tas_lock(&__kmp_stats_lock, gtid);\n\n#endif // KMP_STATS_ENABLED\n\n    if ( KMP_UBER_GTID(gtid) ) {\n        KA_TRACE( 10, (\"__kmp_create_worker: uber thread (%d)\\n\", gtid ) );\n        th -> th.th_info.ds.ds_thread = pthread_self();\n        __kmp_set_stack_info( gtid, th );\n        __kmp_check_stack_overlap( th );\n        return;\n    }; // if\n\n    KA_TRACE( 10, (\"__kmp_create_worker: try to create thread (%d)\\n\", gtid ) );\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n#ifdef KMP_THREAD_ATTR\n        {\n            status = pthread_attr_init( &thread_attr );\n            if ( status != 0 ) {\n                __kmp_msg(\n                          kmp_ms_fatal,\n                          KMP_MSG( CantInitThreadAttrs ),\n                          KMP_ERR( status ),\n                          __kmp_msg_null\n                          );\n            }; // if\n            status = pthread_attr_setdetachstate( & thread_attr, PTHREAD_CREATE_JOINABLE );\n            if ( status != 0 ) {\n                __kmp_msg(\n                          kmp_ms_fatal,\n                          KMP_MSG( CantSetWorkerState ),\n                          KMP_ERR( status ),\n                          __kmp_msg_null\n                          );\n            }; // if\n\n            /* Set stack size for this thread now. \n             * The multiple of 2 is there because on some machines, requesting an unusual stacksize\n             * causes the thread to have an offset before the dummy alloca() takes place to create the\n             * offset.  Since we want the user to have a sufficient stacksize AND support a stack offset, we \n             * alloca() twice the offset so that the upcoming alloca() does not eliminate any premade\n             * offset, and also gives the user the stack space they requested for all threads */\n            stack_size += gtid * __kmp_stkoffset * 2;\n\n            KA_TRACE( 10, ( \"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                            \"__kmp_stksize = %lu bytes, final stacksize = %lu bytes\\n\",\n                            gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size ) );\n\n# ifdef _POSIX_THREAD_ATTR_STACKSIZE\n                status = pthread_attr_setstacksize( & thread_attr, stack_size );\n#  ifdef KMP_BACKUP_STKSIZE\n            if ( status != 0 ) {\n                if ( ! __kmp_env_stksize ) {\n                    stack_size = KMP_BACKUP_STKSIZE + gtid * __kmp_stkoffset;\n                    __kmp_stksize = KMP_BACKUP_STKSIZE;\n                    KA_TRACE( 10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                                   \"__kmp_stksize = %lu bytes, (backup) final stacksize = %lu \"\n                                   \"bytes\\n\",\n                                   gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size )\n                              );\n                    status = pthread_attr_setstacksize( &thread_attr, stack_size );\n                }; // if\n            }; // if\n#  endif /* KMP_BACKUP_STKSIZE */\n            if ( status != 0 ) {\n                __kmp_msg(\n                          kmp_ms_fatal,\n                          KMP_MSG( CantSetWorkerStackSize, stack_size ),\n                          KMP_ERR( status ),\n                          KMP_HNT( ChangeWorkerStackSize  ),\n                          __kmp_msg_null\n                          );\n            }; // if\n# endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n        }\n#endif /* KMP_THREAD_ATTR */\n\n        {\n            status = pthread_create( & handle, & thread_attr, __kmp_launch_worker, (void *) th );\n            if ( status != 0 || ! handle ) { // ??? Why do we check handle??\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n                if ( status == EINVAL ) {\n                    __kmp_msg(\n                              kmp_ms_fatal,\n                              KMP_MSG( CantSetWorkerStackSize, stack_size ),\n                              KMP_ERR( status ),\n                              KMP_HNT( IncreaseWorkerStackSize ),\n                              __kmp_msg_null\n                              );\n                };\n                if ( status == ENOMEM ) {\n                    __kmp_msg(\n                              kmp_ms_fatal,\n                              KMP_MSG( CantSetWorkerStackSize, stack_size ),\n                              KMP_ERR( status ),\n                              KMP_HNT( DecreaseWorkerStackSize ),\n                              __kmp_msg_null\n                              );\n                };\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n                if ( status == EAGAIN ) {\n                    __kmp_msg(\n                              kmp_ms_fatal,\n                              KMP_MSG( NoResourcesForWorkerThread ),\n                              KMP_ERR( status ),\n                              KMP_HNT( Decrease_NUM_THREADS ),\n                              __kmp_msg_null\n                              );\n                }; // if\n                KMP_SYSFAIL( \"pthread_create\", status );\n            }; // if\n\n            th->th.th_info.ds.ds_thread = handle;\n        }\n\n#ifdef KMP_THREAD_ATTR\n        {\n            status = pthread_attr_destroy( & thread_attr );\n            if ( status ) {\n                __kmp_msg(\n                          kmp_ms_warning,\n                          KMP_MSG( CantDestroyThreadAttrs ),\n                          KMP_ERR( status ),\n                          __kmp_msg_null\n                          );\n            }; // if\n        }\n#endif /* KMP_THREAD_ATTR */\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n    KA_TRACE( 10, (\"__kmp_create_worker: done creating thread (%d)\\n\", gtid ) );\n\n} // __kmp_create_worker\n\n\nvoid\n__kmp_create_monitor( kmp_info_t *th )\n{\n    pthread_t           handle;\n    pthread_attr_t      thread_attr;\n    size_t              size;\n    int                 status;\n    int                 auto_adj_size = FALSE;\n\n    KA_TRACE( 10, (\"__kmp_create_monitor: try to create monitor\\n\" ) );\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n    th->th.th_info.ds.ds_tid  = KMP_GTID_MONITOR;\n    th->th.th_info.ds.ds_gtid = KMP_GTID_MONITOR;\n    #if KMP_REAL_TIME_FIX\n        TCW_4( __kmp_global.g.g_time.dt.t_value, -1 ); // Will use it for synchronization a bit later.\n    #else\n        TCW_4( __kmp_global.g.g_time.dt.t_value, 0 );\n    #endif // KMP_REAL_TIME_FIX\n\n    #ifdef KMP_THREAD_ATTR\n        if ( __kmp_monitor_stksize == 0 ) {\n            __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n            auto_adj_size = TRUE;\n        }\n        status = pthread_attr_init( &thread_attr );\n        if ( status != 0 ) {\n            __kmp_msg(\n                kmp_ms_fatal,\n                KMP_MSG( CantInitThreadAttrs ),\n                KMP_ERR( status ),\n                __kmp_msg_null\n            );\n        }; // if\n        status = pthread_attr_setdetachstate( & thread_attr, PTHREAD_CREATE_JOINABLE );\n        if ( status != 0 ) {\n            __kmp_msg(\n                kmp_ms_fatal,\n                KMP_MSG( CantSetMonitorState ),\n                KMP_ERR( status ),\n                __kmp_msg_null\n            );\n        }; // if\n\n        #ifdef _POSIX_THREAD_ATTR_STACKSIZE\n            status = pthread_attr_getstacksize( & thread_attr, & size );\n            KMP_CHECK_SYSFAIL( \"pthread_attr_getstacksize\", status );\n        #else\n            size = __kmp_sys_min_stksize;\n        #endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    #endif /* KMP_THREAD_ATTR */\n\n    if ( __kmp_monitor_stksize == 0 ) {\n        __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n    }\n    if ( __kmp_monitor_stksize < __kmp_sys_min_stksize ) {\n        __kmp_monitor_stksize = __kmp_sys_min_stksize;\n    }\n\n    KA_TRACE( 10, ( \"__kmp_create_monitor: default stacksize = %lu bytes,\"\n                    \"requested stacksize = %lu bytes\\n\",\n                    size, __kmp_monitor_stksize ) );\n\n    retry:\n\n    /* Set stack size for this thread now. */\n\n    #ifdef _POSIX_THREAD_ATTR_STACKSIZE\n        KA_TRACE( 10, ( \"__kmp_create_monitor: setting stacksize = %lu bytes,\",\n                        __kmp_monitor_stksize ) );\n        status = pthread_attr_setstacksize( & thread_attr, __kmp_monitor_stksize );\n        if ( status != 0 ) {\n            if ( auto_adj_size ) {\n                __kmp_monitor_stksize *= 2;\n                goto retry;\n            }\n            __kmp_msg(\n                kmp_ms_warning,  // should this be fatal?  BB\n                KMP_MSG( CantSetMonitorStackSize, (long int) __kmp_monitor_stksize ),\n                KMP_ERR( status ),\n                KMP_HNT( ChangeMonitorStackSize ),\n                __kmp_msg_null\n            );\n        }; // if\n    #endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n    status = pthread_create( &handle, & thread_attr, __kmp_launch_monitor, (void *) th );\n\n    if ( status != 0 ) {\n        #ifdef _POSIX_THREAD_ATTR_STACKSIZE\n            if ( status == EINVAL ) {\n                if ( auto_adj_size  && ( __kmp_monitor_stksize < (size_t)0x40000000 ) ) {\n                    __kmp_monitor_stksize *= 2;\n                    goto retry;\n                }\n                __kmp_msg(\n                    kmp_ms_fatal,\n                    KMP_MSG( CantSetMonitorStackSize, __kmp_monitor_stksize ),\n                    KMP_ERR( status ),\n                    KMP_HNT( IncreaseMonitorStackSize ),\n                    __kmp_msg_null\n                );\n            }; // if\n            if ( status == ENOMEM ) {\n                __kmp_msg(\n                    kmp_ms_fatal,\n                    KMP_MSG( CantSetMonitorStackSize, __kmp_monitor_stksize ),\n                    KMP_ERR( status ),\n                    KMP_HNT( DecreaseMonitorStackSize ),\n                    __kmp_msg_null\n                );\n            }; // if\n        #endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n        if ( status == EAGAIN ) {\n            __kmp_msg(\n                kmp_ms_fatal,\n                KMP_MSG( NoResourcesForMonitorThread ),\n                KMP_ERR( status ),\n                KMP_HNT( DecreaseNumberOfThreadsInUse ),\n                __kmp_msg_null\n            );\n        }; // if\n        KMP_SYSFAIL( \"pthread_create\", status );\n    }; // if\n\n    th->th.th_info.ds.ds_thread = handle;\n\n    #if KMP_REAL_TIME_FIX\n        // Wait for the monitor thread is really started and set its *priority*.\n        KMP_DEBUG_ASSERT( sizeof( kmp_uint32 ) == sizeof( __kmp_global.g.g_time.dt.t_value ) );\n        __kmp_wait_yield_4(\n            (kmp_uint32 volatile *) & __kmp_global.g.g_time.dt.t_value, -1, & __kmp_neq_4, NULL\n        );\n    #endif // KMP_REAL_TIME_FIX\n\n    #ifdef KMP_THREAD_ATTR\n        status = pthread_attr_destroy( & thread_attr );\n        if ( status != 0 ) {\n            __kmp_msg(    //\n                kmp_ms_warning,\n                KMP_MSG( CantDestroyThreadAttrs ),\n                KMP_ERR( status ),\n                __kmp_msg_null\n            );\n        }; // if\n    #endif\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n    KA_TRACE( 10, ( \"__kmp_create_monitor: monitor created %#.8lx\\n\", th->th.th_info.ds.ds_thread ) );\n\n} // __kmp_create_monitor\n\nvoid\n__kmp_exit_thread(\n    int exit_status\n) {\n    pthread_exit( (void *)(intptr_t) exit_status );\n} // __kmp_exit_thread\n\nvoid __kmp_resume_monitor();\n\nvoid\n__kmp_reap_monitor( kmp_info_t *th )\n{\n    int          status;\n    void        *exit_val;\n\n    KA_TRACE( 10, (\"__kmp_reap_monitor: try to reap monitor thread with handle %#.8lx\\n\",\n                   th->th.th_info.ds.ds_thread ) );\n\n    // If monitor has been created, its tid and gtid should be KMP_GTID_MONITOR.\n    // If both tid and gtid are 0, it means the monitor did not ever start.\n    // If both tid and gtid are KMP_GTID_DNE, the monitor has been shut down.\n    KMP_DEBUG_ASSERT( th->th.th_info.ds.ds_tid == th->th.th_info.ds.ds_gtid );\n    if ( th->th.th_info.ds.ds_gtid != KMP_GTID_MONITOR ) {\n        return;\n    }; // if\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n\n    /* First, check to see whether the monitor thread exists.  This could prevent a hang,\n       but if the monitor dies after the pthread_kill call and before the pthread_join\n       call, it will still hang. */\n\n    status = pthread_kill( th->th.th_info.ds.ds_thread, 0 );\n    if (status == ESRCH) {\n\n        KA_TRACE( 10, (\"__kmp_reap_monitor: monitor does not exist, returning\\n\") );\n\n    } else\n    {\n        __kmp_resume_monitor();   // Wake up the monitor thread\n        status = pthread_join( th->th.th_info.ds.ds_thread, & exit_val);\n        if (exit_val != th) {\n            __kmp_msg(\n                kmp_ms_fatal,\n                KMP_MSG( ReapMonitorError ),\n                KMP_ERR( status ),\n                __kmp_msg_null\n            );\n        }\n    }\n\n    th->th.th_info.ds.ds_tid  = KMP_GTID_DNE;\n    th->th.th_info.ds.ds_gtid = KMP_GTID_DNE;\n\n    KA_TRACE( 10, (\"__kmp_reap_monitor: done reaping monitor thread with handle %#.8lx\\n\",\n                   th->th.th_info.ds.ds_thread ) );\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n}\n\nvoid\n__kmp_reap_worker( kmp_info_t *th )\n{\n    int          status;\n    void        *exit_val;\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n\n    KA_TRACE( 10, (\"__kmp_reap_worker: try to reap T#%d\\n\", th->th.th_info.ds.ds_gtid ) );\n\n    /* First, check to see whether the worker thread exists.  This could prevent a hang,\n       but if the worker dies after the pthread_kill call and before the pthread_join\n       call, it will still hang. */\n\n        {\n            status = pthread_kill( th->th.th_info.ds.ds_thread, 0 );\n            if (status == ESRCH) {\n                KA_TRACE( 10, (\"__kmp_reap_worker: worker T#%d does not exist, returning\\n\",\n                               th->th.th_info.ds.ds_gtid ) );\n            }\n            else {\n                KA_TRACE( 10, (\"__kmp_reap_worker: try to join with worker T#%d\\n\",\n                               th->th.th_info.ds.ds_gtid ) );\n\n                status = pthread_join( th->th.th_info.ds.ds_thread, & exit_val);\n#ifdef KMP_DEBUG\n                /* Don't expose these to the user until we understand when they trigger */\n                if ( status != 0 ) {\n                    __kmp_msg(\n                              kmp_ms_fatal,\n                              KMP_MSG( ReapWorkerError ),\n                              KMP_ERR( status ),\n                              __kmp_msg_null\n                              );\n                }\n                if ( exit_val != th ) {\n                    KA_TRACE( 10, ( \"__kmp_reap_worker: worker T#%d did not reap properly, \"\n                                    \"exit_val = %p\\n\",\n                                    th->th.th_info.ds.ds_gtid, exit_val ) );\n                }\n#endif /* KMP_DEBUG */\n            }\n        }\n\n    KA_TRACE( 10, (\"__kmp_reap_worker: done reaping T#%d\\n\", th->th.th_info.ds.ds_gtid ) );\n\n    KMP_MB();       /* Flush all pending memory write invalidates.  */\n}\n\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n#if KMP_HANDLE_SIGNALS\n\n\nstatic void\n__kmp_null_handler( int signo )\n{\n    //  Do nothing, for doing SIG_IGN-type actions.\n} // __kmp_null_handler\n\n\nstatic void\n__kmp_team_handler( int signo )\n{\n    if ( __kmp_global.g.g_abort == 0 ) {\n        /* Stage 1 signal handler, let's shut down all of the threads */\n        #ifdef KMP_DEBUG\n            __kmp_debug_printf( \"__kmp_team_handler: caught signal = %d\\n\", signo );\n        #endif\n        switch ( signo ) {\n            case SIGHUP  :\n            case SIGINT  :\n            case SIGQUIT :\n            case SIGILL  :\n            case SIGABRT :\n            case SIGFPE  :\n            case SIGBUS  :\n            case SIGSEGV :\n            #ifdef SIGSYS\n                case SIGSYS :\n            #endif\n            case SIGTERM :\n                if ( __kmp_debug_buf ) {\n                    __kmp_dump_debug_buffer( );\n                }; // if\n                KMP_MB();       // Flush all pending memory write invalidates.\n                TCW_4( __kmp_global.g.g_abort, signo );\n                KMP_MB();       // Flush all pending memory write invalidates.\n                TCW_4( __kmp_global.g.g_done, TRUE );\n                KMP_MB();       // Flush all pending memory write invalidates.\n                break;\n            default:\n                #ifdef KMP_DEBUG\n                    __kmp_debug_printf( \"__kmp_team_handler: unknown signal type\" );\n                #endif\n                break;\n        }; // switch\n    }; // if\n} // __kmp_team_handler\n\n\nstatic\nvoid __kmp_sigaction( int signum, const struct sigaction * act, struct sigaction * oldact ) {\n    int rc = sigaction( signum, act, oldact );\n    KMP_CHECK_SYSFAIL_ERRNO( \"sigaction\", rc );\n}\n\n\nstatic void\n__kmp_install_one_handler( int sig, sig_func_t handler_func, int parallel_init )\n{\n    KMP_MB();       // Flush all pending memory write invalidates.\n    KB_TRACE( 60, ( \"__kmp_install_one_handler( %d, ..., %d )\\n\", sig, parallel_init ) );\n    if ( parallel_init ) {\n        struct sigaction new_action;\n        struct sigaction old_action;\n        new_action.sa_handler = handler_func;\n        new_action.sa_flags   = 0;\n        sigfillset( & new_action.sa_mask );\n        __kmp_sigaction( sig, & new_action, & old_action );\n        if ( old_action.sa_handler == __kmp_sighldrs[ sig ].sa_handler ) {\n            sigaddset( & __kmp_sigset, sig );\n        } else {\n            // Restore/keep user's handler if one previously installed.\n            __kmp_sigaction( sig, & old_action, NULL );\n        }; // if\n    } else {\n        // Save initial/system signal handlers to see if user handlers installed.\n        __kmp_sigaction( sig, NULL, & __kmp_sighldrs[ sig ] );\n    }; // if\n    KMP_MB();       // Flush all pending memory write invalidates.\n} // __kmp_install_one_handler\n\n\nstatic void\n__kmp_remove_one_handler( int sig )\n{\n    KB_TRACE( 60, ( \"__kmp_remove_one_handler( %d )\\n\", sig ) );\n    if ( sigismember( & __kmp_sigset, sig ) ) {\n        struct sigaction old;\n        KMP_MB();       // Flush all pending memory write invalidates.\n        __kmp_sigaction( sig, & __kmp_sighldrs[ sig ], & old );\n        if ( ( old.sa_handler != __kmp_team_handler ) && ( old.sa_handler != __kmp_null_handler ) ) {\n            // Restore the users signal handler.\n            KB_TRACE( 10, ( \"__kmp_remove_one_handler: oops, not our handler, restoring: sig=%d\\n\", sig ) );\n            __kmp_sigaction( sig, & old, NULL );\n        }; // if\n        sigdelset( & __kmp_sigset, sig );\n        KMP_MB();       // Flush all pending memory write invalidates.\n    }; // if\n} // __kmp_remove_one_handler\n\n\nvoid\n__kmp_install_signals( int parallel_init )\n{\n    KB_TRACE( 10, ( \"__kmp_install_signals( %d )\\n\", parallel_init ) );\n    if ( __kmp_handle_signals || ! parallel_init ) {\n        // If ! parallel_init, we do not install handlers, just save original handlers.\n        // Let us do it even __handle_signals is 0.\n        sigemptyset( & __kmp_sigset );\n        __kmp_install_one_handler( SIGHUP,  __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGINT,  __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGQUIT, __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGILL,  __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGABRT, __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGFPE,  __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGBUS,  __kmp_team_handler, parallel_init );\n        __kmp_install_one_handler( SIGSEGV, __kmp_team_handler, parallel_init );\n        #ifdef SIGSYS\n            __kmp_install_one_handler( SIGSYS,  __kmp_team_handler, parallel_init );\n        #endif // SIGSYS\n        __kmp_install_one_handler( SIGTERM, __kmp_team_handler, parallel_init );\n        #ifdef SIGPIPE\n            __kmp_install_one_handler( SIGPIPE, __kmp_team_handler, parallel_init );\n        #endif // SIGPIPE\n    }; // if\n} // __kmp_install_signals\n\n\nvoid\n__kmp_remove_signals( void )\n{\n    int    sig;\n    KB_TRACE( 10, ( \"__kmp_remove_signals()\\n\" ) );\n    for ( sig = 1; sig < NSIG; ++ sig ) {\n        __kmp_remove_one_handler( sig );\n    }; // for sig\n} // __kmp_remove_signals\n\n\n#endif // KMP_HANDLE_SIGNALS\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\nvoid\n__kmp_enable( int new_state )\n{\n    #ifdef KMP_CANCEL_THREADS\n        int status, old_state;\n        status = pthread_setcancelstate( new_state, & old_state );\n        KMP_CHECK_SYSFAIL( \"pthread_setcancelstate\", status );\n        KMP_DEBUG_ASSERT( old_state == PTHREAD_CANCEL_DISABLE );\n    #endif\n}\n\nvoid\n__kmp_disable( int * old_state )\n{\n    #ifdef KMP_CANCEL_THREADS\n        int status;\n        status = pthread_setcancelstate( PTHREAD_CANCEL_DISABLE, old_state );\n        KMP_CHECK_SYSFAIL( \"pthread_setcancelstate\", status );\n    #endif\n}\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\nstatic void\n__kmp_atfork_prepare (void)\n{\n    /*  nothing to do  */\n}\n\nstatic void\n__kmp_atfork_parent (void)\n{\n    /*  nothing to do  */\n}\n\n/*\n    Reset the library so execution in the child starts \"all over again\" with\n    clean data structures in initial states.  Don't worry about freeing memory\n    allocated by parent, just abandon it to be safe.\n*/\nstatic void\n__kmp_atfork_child (void)\n{\n    /* TODO make sure this is done right for nested/sibling */\n    // ATT:  Memory leaks are here? TODO: Check it and fix.\n    /* KMP_ASSERT( 0 ); */\n\n    ++__kmp_fork_count;\n\n    __kmp_init_runtime = FALSE;\n    __kmp_init_monitor = 0;\n    __kmp_init_parallel = FALSE;\n    __kmp_init_middle = FALSE;\n    __kmp_init_serial = FALSE;\n    TCW_4(__kmp_init_gtid, FALSE);\n    __kmp_init_common = FALSE;\n\n    TCW_4(__kmp_init_user_locks, FALSE);\n#if ! KMP_USE_DYNAMIC_LOCK\n    __kmp_user_lock_table.used = 1;\n    __kmp_user_lock_table.allocated = 0;\n    __kmp_user_lock_table.table = NULL;\n    __kmp_lock_blocks = NULL;\n#endif\n\n    __kmp_all_nth = 0;\n    TCW_4(__kmp_nth, 0);\n\n    /* Must actually zero all the *cache arguments passed to __kmpc_threadprivate here\n       so threadprivate doesn't use stale data */\n    KA_TRACE( 10, ( \"__kmp_atfork_child: checking cache address list %p\\n\",\n                 __kmp_threadpriv_cache_list ) );\n\n    while ( __kmp_threadpriv_cache_list != NULL ) {\n\n        if ( *__kmp_threadpriv_cache_list -> addr != NULL ) {\n            KC_TRACE( 50, ( \"__kmp_atfork_child: zeroing cache at address %p\\n\",\n                        &(*__kmp_threadpriv_cache_list -> addr) ) );\n\n            *__kmp_threadpriv_cache_list -> addr = NULL;\n        }\n        __kmp_threadpriv_cache_list = __kmp_threadpriv_cache_list -> next;\n    }\n\n    __kmp_init_runtime = FALSE;\n\n    /* reset statically initialized locks */\n    __kmp_init_bootstrap_lock( &__kmp_initz_lock );\n    __kmp_init_bootstrap_lock( &__kmp_stdio_lock );\n    __kmp_init_bootstrap_lock( &__kmp_console_lock );\n\n    /* This is necessary to make sure no stale data is left around */\n    /* AC: customers complain that we use unsafe routines in the atfork\n       handler. Mathworks: dlsym() is unsafe. We call dlsym and dlopen\n       in dynamic_link when check the presence of shared tbbmalloc library.\n       Suggestion is to make the library initialization lazier, similar\n       to what done for __kmpc_begin(). */\n    // TODO: synchronize all static initializations with regular library\n    //       startup; look at kmp_global.c and etc.\n    //__kmp_internal_begin ();\n\n}\n\nvoid\n__kmp_register_atfork(void) {\n    if ( __kmp_need_register_atfork ) {\n        int status = pthread_atfork( __kmp_atfork_prepare, __kmp_atfork_parent, __kmp_atfork_child );\n        KMP_CHECK_SYSFAIL( \"pthread_atfork\", status );\n        __kmp_need_register_atfork = FALSE;\n    }\n}\n\nvoid\n__kmp_suspend_initialize( void )\n{\n    int status;\n    status = pthread_mutexattr_init( &__kmp_suspend_mutex_attr );\n    KMP_CHECK_SYSFAIL( \"pthread_mutexattr_init\", status );\n    status = pthread_condattr_init( &__kmp_suspend_cond_attr );\n    KMP_CHECK_SYSFAIL( \"pthread_condattr_init\", status );\n}\n\nstatic void\n__kmp_suspend_initialize_thread( kmp_info_t *th )\n{\n    if ( th->th.th_suspend_init_count <= __kmp_fork_count ) {\n        /* this means we haven't initialized the suspension pthread objects for this thread\n           in this instance of the process */\n        int     status;\n        status = pthread_cond_init( &th->th.th_suspend_cv.c_cond, &__kmp_suspend_cond_attr );\n        KMP_CHECK_SYSFAIL( \"pthread_cond_init\", status );\n        status = pthread_mutex_init( &th->th.th_suspend_mx.m_mutex, & __kmp_suspend_mutex_attr );\n        KMP_CHECK_SYSFAIL( \"pthread_mutex_init\", status );\n        *(volatile int*)&th->th.th_suspend_init_count = __kmp_fork_count + 1;\n    };\n}\n\nvoid\n__kmp_suspend_uninitialize_thread( kmp_info_t *th )\n{\n    if(th->th.th_suspend_init_count > __kmp_fork_count) {\n        /* this means we have initialize the suspension pthread objects for this thread\n           in this instance of the process */\n        int status;\n\n        status = pthread_cond_destroy( &th->th.th_suspend_cv.c_cond );\n        if ( status != 0 && status != EBUSY ) {\n            KMP_SYSFAIL( \"pthread_cond_destroy\", status );\n        };\n        status = pthread_mutex_destroy( &th->th.th_suspend_mx.m_mutex );\n        if ( status != 0 && status != EBUSY ) {\n            KMP_SYSFAIL( \"pthread_mutex_destroy\", status );\n        };\n        --th->th.th_suspend_init_count;\n        KMP_DEBUG_ASSERT(th->th.th_suspend_init_count == __kmp_fork_count);\n    }\n}\n\n/* This routine puts the calling thread to sleep after setting the\n * sleep bit for the indicated flag variable to true.\n */\ntemplate <class C>\nstatic inline void __kmp_suspend_template( int th_gtid, C *flag )\n{\n    KMP_TIME_DEVELOPER_BLOCK(USER_suspend);\n    kmp_info_t *th = __kmp_threads[th_gtid];\n    int status;\n    typename C::flag_t old_spin;\n\n    KF_TRACE( 30, (\"__kmp_suspend_template: T#%d enter for flag = %p\\n\", th_gtid, flag->get() ) );\n\n    __kmp_suspend_initialize_thread( th );\n\n    status = pthread_mutex_lock( &th->th.th_suspend_mx.m_mutex );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_lock\", status );\n\n    KF_TRACE( 10, ( \"__kmp_suspend_template: T#%d setting sleep bit for spin(%p)\\n\",\n                    th_gtid, flag->get() ) );\n\n    /* TODO: shouldn't this use release semantics to ensure that __kmp_suspend_initialize_thread\n       gets called first?\n    */\n    old_spin = flag->set_sleeping();\n\n    KF_TRACE( 5, ( \"__kmp_suspend_template: T#%d set sleep bit for spin(%p)==%x, was %x\\n\",\n                   th_gtid, flag->get(), *(flag->get()), old_spin ) );\n\n    if ( flag->done_check_val(old_spin) ) {\n        old_spin = flag->unset_sleeping();\n        KF_TRACE( 5, ( \"__kmp_suspend_template: T#%d false alarm, reset sleep bit for spin(%p)\\n\",\n                       th_gtid, flag->get()) );\n    } else {\n        /* Encapsulate in a loop as the documentation states that this may\n         * \"with low probability\" return when the condition variable has\n         * not been signaled or broadcast\n         */\n        int deactivated = FALSE;\n        TCW_PTR(th->th.th_sleep_loc, (void *)flag);\n        while ( flag->is_sleeping() ) {\n#ifdef DEBUG_SUSPEND\n            char buffer[128];\n            __kmp_suspend_count++;\n            __kmp_print_cond( buffer, &th->th.th_suspend_cv );\n            __kmp_printf( \"__kmp_suspend_template: suspending T#%d: %s\\n\", th_gtid, buffer );\n#endif\n            // Mark the thread as no longer active (only in the first iteration of the loop).\n            if ( ! deactivated ) {\n                th->th.th_active = FALSE;\n                if ( th->th.th_active_in_pool ) {\n                    th->th.th_active_in_pool = FALSE;\n                    KMP_TEST_THEN_DEC32(\n                      (kmp_int32 *) &__kmp_thread_pool_active_nth );\n                    KMP_DEBUG_ASSERT( TCR_4(__kmp_thread_pool_active_nth) >= 0 );\n                }\n                deactivated = TRUE;\n\n\n            }\n\n#if USE_SUSPEND_TIMEOUT\n            struct timespec  now;\n            struct timeval   tval;\n            int msecs;\n\n            status = gettimeofday( &tval, NULL );\n            KMP_CHECK_SYSFAIL_ERRNO( \"gettimeofday\", status );\n            TIMEVAL_TO_TIMESPEC( &tval, &now );\n\n            msecs = (4*__kmp_dflt_blocktime) + 200;\n            now.tv_sec  += msecs / 1000;\n            now.tv_nsec += (msecs % 1000)*1000;\n\n            KF_TRACE( 15, ( \"__kmp_suspend_template: T#%d about to perform pthread_cond_timedwait\\n\",\n                            th_gtid ) );\n            status = pthread_cond_timedwait( &th->th.th_suspend_cv.c_cond, &th->th.th_suspend_mx.m_mutex, & now );\n#else\n            KF_TRACE( 15, ( \"__kmp_suspend_template: T#%d about to perform pthread_cond_wait\\n\",\n                            th_gtid ) );\n            status = pthread_cond_wait( &th->th.th_suspend_cv.c_cond, &th->th.th_suspend_mx.m_mutex );\n#endif\n\n            if ( (status != 0) && (status != EINTR) && (status != ETIMEDOUT) ) {\n                KMP_SYSFAIL( \"pthread_cond_wait\", status );\n            }\n#ifdef KMP_DEBUG\n            if (status == ETIMEDOUT) {\n                if ( flag->is_sleeping() ) {\n                    KF_TRACE( 100, ( \"__kmp_suspend_template: T#%d timeout wakeup\\n\", th_gtid ) );\n                } else {\n                    KF_TRACE( 2, ( \"__kmp_suspend_template: T#%d timeout wakeup, sleep bit not set!\\n\",\n                                   th_gtid ) );\n                }\n            } else if ( flag->is_sleeping() ) {\n                KF_TRACE( 100, ( \"__kmp_suspend_template: T#%d spurious wakeup\\n\", th_gtid ) );\n            }\n#endif\n        } // while\n\n        // Mark the thread as active again (if it was previous marked as inactive)\n        if ( deactivated ) {\n            th->th.th_active = TRUE;\n            if ( TCR_4(th->th.th_in_pool) ) {\n                KMP_TEST_THEN_INC32( (kmp_int32 *) &__kmp_thread_pool_active_nth );\n                th->th.th_active_in_pool = TRUE;\n            }\n        }\n    }\n\n#ifdef DEBUG_SUSPEND\n    {\n        char buffer[128];\n        __kmp_print_cond( buffer, &th->th.th_suspend_cv);\n        __kmp_printf( \"__kmp_suspend_template: T#%d has awakened: %s\\n\", th_gtid, buffer );\n    }\n#endif\n\n\n    status = pthread_mutex_unlock( &th->th.th_suspend_mx.m_mutex );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_unlock\", status );\n\n    KF_TRACE( 30, (\"__kmp_suspend_template: T#%d exit\\n\", th_gtid ) );\n}\n\nvoid __kmp_suspend_32(int th_gtid, kmp_flag_32 *flag) {\n    __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_64(int th_gtid, kmp_flag_64 *flag) {\n    __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_oncore(int th_gtid, kmp_flag_oncore *flag) {\n    __kmp_suspend_template(th_gtid, flag);\n}\n\n\n/* This routine signals the thread specified by target_gtid to wake up\n * after setting the sleep bit indicated by the flag argument to FALSE.\n * The target thread must already have called __kmp_suspend_template()\n */\ntemplate <class C>\nstatic inline void __kmp_resume_template( int target_gtid, C *flag )\n{\n    KMP_TIME_DEVELOPER_BLOCK(USER_resume);\n    kmp_info_t *th = __kmp_threads[target_gtid];\n    int status;\n\n#ifdef KMP_DEBUG\n    int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n#endif\n\n    KF_TRACE( 30, ( \"__kmp_resume_template: T#%d wants to wakeup T#%d enter\\n\", gtid, target_gtid ) );\n    KMP_DEBUG_ASSERT( gtid != target_gtid );\n\n    __kmp_suspend_initialize_thread( th );\n\n    status = pthread_mutex_lock( &th->th.th_suspend_mx.m_mutex );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_lock\", status );\n\n    if (!flag) {\n        flag = (C *)th->th.th_sleep_loc;\n    }\n\n    if (!flag) {\n        KF_TRACE( 5, ( \"__kmp_resume_template: T#%d exiting, thread T#%d already awake: flag(%p)\\n\",\n                       gtid, target_gtid, NULL ) );\n        status = pthread_mutex_unlock( &th->th.th_suspend_mx.m_mutex );\n        KMP_CHECK_SYSFAIL( \"pthread_mutex_unlock\", status );\n        return;\n    }\n    else { // if multiple threads are sleeping, flag should be internally referring to a specific thread here\n        typename C::flag_t old_spin = flag->unset_sleeping();\n        if ( ! flag->is_sleeping_val(old_spin) ) {\n            KF_TRACE( 5, ( \"__kmp_resume_template: T#%d exiting, thread T#%d already awake: flag(%p): \"\n                           \"%u => %u\\n\",\n                           gtid, target_gtid, flag->get(), old_spin, *flag->get() ) );\n\n            status = pthread_mutex_unlock( &th->th.th_suspend_mx.m_mutex );\n            KMP_CHECK_SYSFAIL( \"pthread_mutex_unlock\", status );\n            return;\n        }\n        KF_TRACE( 5, ( \"__kmp_resume_template: T#%d about to wakeup T#%d, reset sleep bit for flag's loc(%p): \"\n                       \"%u => %u\\n\",\n                       gtid, target_gtid, flag->get(), old_spin, *flag->get() ) );\n    }\n    TCW_PTR(th->th.th_sleep_loc, NULL);\n\n\n#ifdef DEBUG_SUSPEND\n    {\n        char buffer[128];\n        __kmp_print_cond( buffer, &th->th.th_suspend_cv );\n        __kmp_printf( \"__kmp_resume_template: T#%d resuming T#%d: %s\\n\", gtid, target_gtid, buffer );\n    }\n#endif\n\n\n    status = pthread_cond_signal( &th->th.th_suspend_cv.c_cond );\n    KMP_CHECK_SYSFAIL( \"pthread_cond_signal\", status );\n    status = pthread_mutex_unlock( &th->th.th_suspend_mx.m_mutex );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_unlock\", status );\n    KF_TRACE( 30, ( \"__kmp_resume_template: T#%d exiting after signaling wake up for T#%d\\n\",\n                    gtid, target_gtid ) );\n}\n\nvoid __kmp_resume_32(int target_gtid, kmp_flag_32 *flag) {\n    __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_64(int target_gtid, kmp_flag_64 *flag) {\n    __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_oncore(int target_gtid, kmp_flag_oncore *flag) {\n    __kmp_resume_template(target_gtid, flag);\n}\n\nvoid\n__kmp_resume_monitor()\n{\n    int status;\n#ifdef KMP_DEBUG\n    int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n    KF_TRACE( 30, ( \"__kmp_resume_monitor: T#%d wants to wakeup T#%d enter\\n\",\n                    gtid, KMP_GTID_MONITOR ) );\n    KMP_DEBUG_ASSERT( gtid != KMP_GTID_MONITOR );\n#endif\n    status = pthread_mutex_lock( &__kmp_wait_mx.m_mutex );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_lock\", status );\n#ifdef DEBUG_SUSPEND\n    {\n        char buffer[128];\n        __kmp_print_cond( buffer, &__kmp_wait_cv.c_cond );\n        __kmp_printf( \"__kmp_resume_monitor: T#%d resuming T#%d: %s\\n\", gtid, KMP_GTID_MONITOR, buffer );\n    }\n#endif\n    status = pthread_cond_signal( &__kmp_wait_cv.c_cond );\n    KMP_CHECK_SYSFAIL( \"pthread_cond_signal\", status );\n    status = pthread_mutex_unlock( &__kmp_wait_mx.m_mutex );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_unlock\", status );\n    KF_TRACE( 30, ( \"__kmp_resume_monitor: T#%d exiting after signaling wake up for T#%d\\n\",\n                    gtid, KMP_GTID_MONITOR ) );\n}\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\nvoid\n__kmp_yield( int cond )\n{\n    if (cond && __kmp_yielding_on) {\n        sched_yield();\n    }\n}\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\nvoid\n__kmp_gtid_set_specific( int gtid )\n{\n    int status;\n    KMP_ASSERT( __kmp_init_runtime );\n    status = pthread_setspecific( __kmp_gtid_threadprivate_key, (void*)(intptr_t)(gtid+1) );\n    KMP_CHECK_SYSFAIL( \"pthread_setspecific\", status );\n}\n\nint\n__kmp_gtid_get_specific()\n{\n    int gtid;\n    if ( !__kmp_init_runtime ) {\n        KA_TRACE( 50, (\"__kmp_get_specific: runtime shutdown, returning KMP_GTID_SHUTDOWN\\n\" ) );\n        return KMP_GTID_SHUTDOWN;\n    }\n    gtid = (int)(size_t)pthread_getspecific( __kmp_gtid_threadprivate_key );\n    if ( gtid == 0 ) {\n        gtid = KMP_GTID_DNE;\n    }\n    else {\n        gtid--;\n    }\n    KA_TRACE( 50, (\"__kmp_gtid_get_specific: key:%d gtid:%d\\n\",\n               __kmp_gtid_threadprivate_key, gtid ));\n    return gtid;\n}\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\ndouble\n__kmp_read_cpu_time( void )\n{\n    /*clock_t   t;*/\n    struct tms  buffer;\n\n    /*t =*/  times( & buffer );\n\n    return (buffer.tms_utime + buffer.tms_cutime) / (double) CLOCKS_PER_SEC;\n}\n\nint\n__kmp_read_system_info( struct kmp_sys_info *info )\n{\n    int status;\n    struct rusage r_usage;\n\n    memset( info, 0, sizeof( *info ) );\n\n    status = getrusage( RUSAGE_SELF, &r_usage);\n    KMP_CHECK_SYSFAIL_ERRNO( \"getrusage\", status );\n\n    info->maxrss  = r_usage.ru_maxrss;  /* the maximum resident set size utilized (in kilobytes)     */\n    info->minflt  = r_usage.ru_minflt;  /* the number of page faults serviced without any I/O        */\n    info->majflt  = r_usage.ru_majflt;  /* the number of page faults serviced that required I/O      */\n    info->nswap   = r_usage.ru_nswap;   /* the number of times a process was \"swapped\" out of memory */\n    info->inblock = r_usage.ru_inblock; /* the number of times the file system had to perform input  */\n    info->oublock = r_usage.ru_oublock; /* the number of times the file system had to perform output */\n    info->nvcsw   = r_usage.ru_nvcsw;   /* the number of times a context switch was voluntarily      */\n    info->nivcsw  = r_usage.ru_nivcsw;  /* the number of times a context switch was forced           */\n\n    return (status != 0);\n}\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n\nvoid\n__kmp_read_system_time( double *delta )\n{\n    double              t_ns;\n    struct timeval      tval;\n    struct timespec     stop;\n    int status;\n\n    status = gettimeofday( &tval, NULL );\n    KMP_CHECK_SYSFAIL_ERRNO( \"gettimeofday\", status );\n    TIMEVAL_TO_TIMESPEC( &tval, &stop );\n    t_ns = TS2NS(stop) - TS2NS(__kmp_sys_timer_data.start);\n    *delta = (t_ns * 1e-9);\n}\n\nvoid\n__kmp_clear_system_time( void )\n{\n    struct timeval tval;\n    int status;\n    status = gettimeofday( &tval, NULL );\n    KMP_CHECK_SYSFAIL_ERRNO( \"gettimeofday\", status );\n    TIMEVAL_TO_TIMESPEC( &tval, &__kmp_sys_timer_data.start );\n}\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\n#ifdef BUILD_TV\n\nvoid\n__kmp_tv_threadprivate_store( kmp_info_t *th, void *global_addr, void *thread_addr )\n{\n    struct tv_data *p;\n\n    p = (struct tv_data *) __kmp_allocate( sizeof( *p ) );\n\n    p->u.tp.global_addr = global_addr;\n    p->u.tp.thread_addr = thread_addr;\n\n    p->type = (void *) 1;\n\n    p->next =  th->th.th_local.tv_data;\n    th->th.th_local.tv_data = p;\n\n    if ( p->next == 0 ) {\n        int rc = pthread_setspecific( __kmp_tv_key, p );\n        KMP_CHECK_SYSFAIL( \"pthread_setspecific\", rc );\n    }\n}\n\n#endif /* BUILD_TV */\n\n/* ------------------------------------------------------------------------ */\n/* ------------------------------------------------------------------------ */\n\nstatic int\n__kmp_get_xproc( void ) {\n\n    int r = 0;\n\n    #if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n\n        r = sysconf( _SC_NPROCESSORS_ONLN );\n\n    #elif KMP_OS_DARWIN\n\n        // Bug C77011 High \"OpenMP Threads and number of active cores\".\n\n        // Find the number of available CPUs.\n        kern_return_t          rc;\n        host_basic_info_data_t info;\n        mach_msg_type_number_t num = HOST_BASIC_INFO_COUNT;\n        rc = host_info( mach_host_self(), HOST_BASIC_INFO, (host_info_t) & info, & num );\n        if ( rc == 0 && num == HOST_BASIC_INFO_COUNT ) {\n            // Cannot use KA_TRACE() here because this code works before trace support is\n            // initialized.\n            r = info.avail_cpus;\n        } else {\n            KMP_WARNING( CantGetNumAvailCPU );\n            KMP_INFORM( AssumedNumCPU );\n        }; // if\n\n    #else\n\n        #error \"Unknown or unsupported OS.\"\n\n    #endif\n\n    return r > 0 ? r : 2; /* guess value of 2 if OS told us 0 */\n\n} // __kmp_get_xproc\n\nint\n__kmp_read_from_file( char const *path, char const *format, ... )\n{\n    int result;\n    va_list args;\n\n    va_start(args, format);\n    FILE *f = fopen(path, \"rb\");\n    if ( f == NULL )\n        return 0;\n    result = vfscanf(f, format, args);\n    fclose(f);\n\n    return result;\n}\n\nvoid\n__kmp_runtime_initialize( void )\n{\n    int status;\n    pthread_mutexattr_t mutex_attr;\n    pthread_condattr_t  cond_attr;\n\n    if ( __kmp_init_runtime ) {\n        return;\n    }; // if\n\n    #if ( KMP_ARCH_X86 || KMP_ARCH_X86_64 )\n        if ( ! __kmp_cpuinfo.initialized ) {\n            __kmp_query_cpuid( &__kmp_cpuinfo );\n        }; // if\n    #endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n    __kmp_xproc = __kmp_get_xproc();\n\n    if ( sysconf( _SC_THREADS ) ) {\n\n        /* Query the maximum number of threads */\n        __kmp_sys_max_nth = sysconf( _SC_THREAD_THREADS_MAX );\n        if ( __kmp_sys_max_nth == -1 ) {\n            /* Unlimited threads for NPTL */\n            __kmp_sys_max_nth = INT_MAX;\n        }\n        else if ( __kmp_sys_max_nth <= 1 ) {\n            /* Can't tell, just use PTHREAD_THREADS_MAX */\n            __kmp_sys_max_nth = KMP_MAX_NTH;\n        }\n\n        /* Query the minimum stack size */\n        __kmp_sys_min_stksize = sysconf( _SC_THREAD_STACK_MIN );\n        if ( __kmp_sys_min_stksize <= 1 ) {\n            __kmp_sys_min_stksize = KMP_MIN_STKSIZE;\n        }\n    }\n\n    /* Set up minimum number of threads to switch to TLS gtid */\n    __kmp_tls_gtid_min = KMP_TLS_GTID_MIN;\n\n\n    #ifdef BUILD_TV\n        {\n            int rc = pthread_key_create( & __kmp_tv_key, 0 );\n            KMP_CHECK_SYSFAIL( \"pthread_key_create\", rc );\n        }\n    #endif\n\n    status = pthread_key_create( &__kmp_gtid_threadprivate_key, __kmp_internal_end_dest );\n    KMP_CHECK_SYSFAIL( \"pthread_key_create\", status );\n    status = pthread_mutexattr_init( & mutex_attr );\n    KMP_CHECK_SYSFAIL( \"pthread_mutexattr_init\", status );\n    status = pthread_mutex_init( & __kmp_wait_mx.m_mutex, & mutex_attr );\n    KMP_CHECK_SYSFAIL( \"pthread_mutex_init\", status );\n    status = pthread_condattr_init( & cond_attr );\n    KMP_CHECK_SYSFAIL( \"pthread_condattr_init\", status );\n    status = pthread_cond_init( & __kmp_wait_cv.c_cond, & cond_attr );\n    KMP_CHECK_SYSFAIL( \"pthread_cond_init\", status );\n#if USE_ITT_BUILD\n    __kmp_itt_initialize();\n#endif /* USE_ITT_BUILD */\n\n    __kmp_init_runtime = TRUE;\n}\n\nvoid\n__kmp_runtime_destroy( void )\n{\n    int status;\n\n    if ( ! __kmp_init_runtime ) {\n        return; // Nothing to do.\n    };\n\n#if USE_ITT_BUILD\n    __kmp_itt_destroy();\n#endif /* USE_ITT_BUILD */\n\n    status = pthread_key_delete( __kmp_gtid_threadprivate_key );\n    KMP_CHECK_SYSFAIL( \"pthread_key_delete\", status );\n    #ifdef BUILD_TV\n        status = pthread_key_delete( __kmp_tv_key );\n        KMP_CHECK_SYSFAIL( \"pthread_key_delete\", status );\n    #endif\n\n    status = pthread_mutex_destroy( & __kmp_wait_mx.m_mutex );\n    if ( status != 0 && status != EBUSY ) {\n        KMP_SYSFAIL( \"pthread_mutex_destroy\", status );\n    }\n    status = pthread_cond_destroy( & __kmp_wait_cv.c_cond );\n    if ( status != 0 && status != EBUSY ) {\n        KMP_SYSFAIL( \"pthread_cond_destroy\", status );\n    }\n    #if KMP_AFFINITY_SUPPORTED\n        __kmp_affinity_uninitialize();\n    #endif\n\n    __kmp_init_runtime = FALSE;\n}\n\n\n/* Put the thread to sleep for a time period */\n/* NOTE: not currently used anywhere */\nvoid\n__kmp_thread_sleep( int millis )\n{\n    sleep(  ( millis + 500 ) / 1000 );\n}\n\n/* Calculate the elapsed wall clock time for the user */\nvoid\n__kmp_elapsed( double *t )\n{\n    int status;\n# ifdef FIX_SGI_CLOCK\n    struct timespec ts;\n\n    status = clock_gettime( CLOCK_PROCESS_CPUTIME_ID, &ts );\n    KMP_CHECK_SYSFAIL_ERRNO( \"clock_gettime\", status );\n    *t = (double) ts.tv_nsec * (1.0 / (double) KMP_NSEC_PER_SEC) +\n        (double) ts.tv_sec;\n# else\n    struct timeval tv;\n\n    status = gettimeofday( & tv, NULL );\n    KMP_CHECK_SYSFAIL_ERRNO( \"gettimeofday\", status );\n    *t = (double) tv.tv_usec * (1.0 / (double) KMP_USEC_PER_SEC) +\n        (double) tv.tv_sec;\n# endif\n}\n\n/* Calculate the elapsed wall clock tick for the user */\nvoid\n__kmp_elapsed_tick( double *t )\n{\n    *t = 1 / (double) CLOCKS_PER_SEC;\n}\n\n/*\n    Determine whether the given address is mapped into the current address space.\n*/\n\nint\n__kmp_is_address_mapped( void * addr ) {\n\n    int found = 0;\n    int rc;\n\n    #if KMP_OS_LINUX || KMP_OS_FREEBSD\n\n        /*\n            On Linux* OS, read the /proc/<pid>/maps pseudo-file to get all the address ranges mapped\n            into the address space.\n        */\n\n        char * name = __kmp_str_format( \"/proc/%d/maps\", getpid() );\n        FILE * file  = NULL;\n\n        file = fopen( name, \"r\" );\n        KMP_ASSERT( file != NULL );\n\n        for ( ; ; ) {\n\n            void * beginning = NULL;\n            void * ending    = NULL;\n            char   perms[ 5 ];\n\n            rc = fscanf( file, \"%p-%p %4s %*[^\\n]\\n\", & beginning, & ending, perms );\n            if ( rc == EOF ) {\n                break;\n            }; // if\n            KMP_ASSERT( rc == 3 && KMP_STRLEN( perms ) == 4 ); // Make sure all fields are read.\n\n            // Ending address is not included in the region, but beginning is.\n            if ( ( addr >= beginning ) && ( addr < ending ) ) {\n                perms[ 2 ] = 0;    // 3th and 4th character does not matter.\n                if ( strcmp( perms, \"rw\" ) == 0 ) {\n                    // Memory we are looking for should be readable and writable.\n                    found = 1;\n                }; // if\n                break;\n            }; // if\n\n        }; // forever\n\n        // Free resources.\n        fclose( file );\n        KMP_INTERNAL_FREE( name );\n\n    #elif KMP_OS_DARWIN\n\n        /*\n            On OS X*, /proc pseudo filesystem is not available. Try to read memory using vm\n            interface.\n        */\n\n        int       buffer;\n        vm_size_t count;\n        rc =\n            vm_read_overwrite(\n                mach_task_self(),           // Task to read memory of.\n                (vm_address_t)( addr ),     // Address to read from.\n                1,                          // Number of bytes to be read.\n                (vm_address_t)( & buffer ), // Address of buffer to save read bytes in.\n                & count                     // Address of var to save number of read bytes in.\n            );\n        if ( rc == 0 ) {\n            // Memory successfully read.\n            found = 1;\n        }; // if\n\n    #elif KMP_OS_FREEBSD || KMP_OS_NETBSD\n\n        // FIXME(FreeBSD, NetBSD): Implement this\n        found = 1;\n\n    #else\n\n        #error \"Unknown or unsupported OS\"\n\n    #endif\n\n    return found;\n\n} // __kmp_is_address_mapped\n\n#ifdef USE_LOAD_BALANCE\n\n\n# if KMP_OS_DARWIN\n\n// The function returns the rounded value of the system load average\n// during given time interval which depends on the value of\n// __kmp_load_balance_interval variable (default is 60 sec, other values\n// may be 300 sec or 900 sec).\n// It returns -1 in case of error.\nint\n__kmp_get_load_balance( int max )\n{\n    double averages[3];\n    int ret_avg = 0;\n\n    int res = getloadavg( averages, 3 );\n\n    //Check __kmp_load_balance_interval to determine which of averages to use.\n    // getloadavg() may return the number of samples less than requested that is\n    // less than 3.\n    if ( __kmp_load_balance_interval < 180 && ( res >= 1 ) ) {\n        ret_avg = averages[0];// 1 min\n    } else if ( ( __kmp_load_balance_interval >= 180\n                  && __kmp_load_balance_interval < 600 ) && ( res >= 2 ) ) {\n        ret_avg = averages[1];// 5 min\n    } else if ( ( __kmp_load_balance_interval >= 600 ) && ( res == 3 ) ) {\n        ret_avg = averages[2];// 15 min\n    } else {// Error occurred\n        return -1;\n    }\n\n    return ret_avg;\n}\n\n# else // Linux* OS\n\n// The fuction returns number of running (not sleeping) threads, or -1 in case of error.\n// Error could be reported if Linux* OS kernel too old (without \"/proc\" support).\n// Counting running threads stops if max running threads encountered.\nint\n__kmp_get_load_balance( int max )\n{\n    static int permanent_error = 0;\n\n    static int     glb_running_threads          = 0;  /* Saved count of the running threads for the thread balance algortihm */\n    static double  glb_call_time = 0;  /* Thread balance algorithm call time */\n\n    int running_threads = 0;              // Number of running threads in the system.\n\n    DIR  *          proc_dir   = NULL;    // Handle of \"/proc/\" directory.\n    struct dirent * proc_entry = NULL;\n\n    kmp_str_buf_t   task_path;            // \"/proc/<pid>/task/<tid>/\" path.\n    DIR  *          task_dir   = NULL;    // Handle of \"/proc/<pid>/task/<tid>/\" directory.\n    struct dirent * task_entry = NULL;\n    int             task_path_fixed_len;\n\n    kmp_str_buf_t   stat_path;            // \"/proc/<pid>/task/<tid>/stat\" path.\n    int             stat_file = -1;\n    int             stat_path_fixed_len;\n\n    int total_processes = 0;              // Total number of processes in system.\n    int total_threads   = 0;              // Total number of threads in system.\n\n    double call_time = 0.0;\n\n    __kmp_str_buf_init( & task_path );\n    __kmp_str_buf_init( & stat_path );\n\n     __kmp_elapsed( & call_time );\n\n    if ( glb_call_time &&\n            ( call_time - glb_call_time < __kmp_load_balance_interval ) ) {\n        running_threads = glb_running_threads;\n        goto finish;\n    }\n\n    glb_call_time = call_time;\n\n    // Do not spend time on scanning \"/proc/\" if we have a permanent error.\n    if ( permanent_error ) {\n        running_threads = -1;\n        goto finish;\n    }; // if\n\n    if ( max <= 0 ) {\n        max = INT_MAX;\n    }; // if\n\n    // Open \"/proc/\" directory.\n    proc_dir = opendir( \"/proc\" );\n    if ( proc_dir == NULL ) {\n        // Cannot open \"/prroc/\". Probably the kernel does not support it. Return an error now and\n        // in subsequent calls.\n        running_threads = -1;\n        permanent_error = 1;\n        goto finish;\n    }; // if\n\n    // Initialize fixed part of task_path. This part will not change.\n    __kmp_str_buf_cat( & task_path, \"/proc/\", 6 );\n    task_path_fixed_len = task_path.used;    // Remember number of used characters.\n\n    proc_entry = readdir( proc_dir );\n    while ( proc_entry != NULL ) {\n        // Proc entry is a directory and name starts with a digit. Assume it is a process'\n        // directory.\n        if ( proc_entry->d_type == DT_DIR && isdigit( proc_entry->d_name[ 0 ] ) ) {\n\n            ++ total_processes;\n            // Make sure init process is the very first in \"/proc\", so we can replace\n            // strcmp( proc_entry->d_name, \"1\" ) == 0 with simpler total_processes == 1.\n            // We are going to check that total_processes == 1 => d_name == \"1\" is true (where\n            // \"=>\" is implication). Since C++ does not have => operator, let us replace it with its\n            // equivalent: a => b == ! a || b.\n            KMP_DEBUG_ASSERT( total_processes != 1 || strcmp( proc_entry->d_name, \"1\" ) == 0 );\n\n            // Construct task_path.\n            task_path.used = task_path_fixed_len;    // Reset task_path to \"/proc/\".\n            __kmp_str_buf_cat( & task_path, proc_entry->d_name, KMP_STRLEN( proc_entry->d_name ) );\n            __kmp_str_buf_cat( & task_path, \"/task\", 5 );\n\n            task_dir = opendir( task_path.str );\n            if ( task_dir == NULL ) {\n                // Process can finish between reading \"/proc/\" directory entry and opening process'\n                // \"task/\" directory. So, in general case we should not complain, but have to skip\n                // this process and read the next one.\n                // But on systems with no \"task/\" support we will spend lot of time to scan \"/proc/\"\n                // tree again and again without any benefit. \"init\" process (its pid is 1) should\n                // exist always, so, if we cannot open \"/proc/1/task/\" directory, it means \"task/\"\n                // is not supported by kernel. Report an error now and in the future.\n                if ( strcmp( proc_entry->d_name, \"1\" ) == 0 ) {\n                    running_threads = -1;\n                    permanent_error = 1;\n                    goto finish;\n                }; // if\n            } else {\n                 // Construct fixed part of stat file path.\n                __kmp_str_buf_clear( & stat_path );\n                __kmp_str_buf_cat( & stat_path, task_path.str, task_path.used );\n                __kmp_str_buf_cat( & stat_path, \"/\", 1 );\n                stat_path_fixed_len = stat_path.used;\n\n                task_entry = readdir( task_dir );\n                while ( task_entry != NULL ) {\n                    // It is a directory and name starts with a digit.\n                    if ( proc_entry->d_type == DT_DIR && isdigit( task_entry->d_name[ 0 ] ) ) {\n\n                        ++ total_threads;\n\n                        // Consruct complete stat file path. Easiest way would be:\n                        //  __kmp_str_buf_print( & stat_path, \"%s/%s/stat\", task_path.str, task_entry->d_name );\n                        // but seriae of __kmp_str_buf_cat works a bit faster.\n                        stat_path.used = stat_path_fixed_len;    // Reset stat path to its fixed part.\n                        __kmp_str_buf_cat( & stat_path, task_entry->d_name, KMP_STRLEN( task_entry->d_name ) );\n                        __kmp_str_buf_cat( & stat_path, \"/stat\", 5 );\n\n                        // Note: Low-level API (open/read/close) is used. High-level API\n                        // (fopen/fclose)  works ~ 30 % slower.\n                        stat_file = open( stat_path.str, O_RDONLY );\n                        if ( stat_file == -1 ) {\n                            // We cannot report an error because task (thread) can terminate just\n                            // before reading this file.\n                        } else {\n                            /*\n                                Content of \"stat\" file looks like:\n\n                                    24285 (program) S ...\n\n                                It is a single line (if program name does not include fanny\n                                symbols). First number is a thread id, then name of executable file\n                                name in paretheses, then state of the thread. We need just thread\n                                state.\n\n                                Good news: Length of program name is 15 characters max. Longer\n                                names are truncated.\n\n                                Thus, we need rather short buffer: 15 chars for program name +\n                                2 parenthesis, + 3 spaces + ~7 digits of pid = 37.\n\n                                Bad news: Program name may contain special symbols like space,\n                                closing parenthesis, or even new line. This makes parsing \"stat\"\n                                file not 100 % reliable. In case of fanny program names parsing\n                                may fail (report incorrect thread state).\n\n                                Parsing \"status\" file looks more promissing (due to different\n                                file structure and escaping special symbols) but reading and\n                                parsing of \"status\" file works slower.\n\n                                -- ln\n                            */\n                            char buffer[ 65 ];\n                            int len;\n                            len = read( stat_file, buffer, sizeof( buffer ) - 1 );\n                            if ( len >= 0 ) {\n                                buffer[ len ] = 0;\n                                // Using scanf:\n                                //     sscanf( buffer, \"%*d (%*s) %c \", & state );\n                                // looks very nice, but searching for a closing parenthesis works a\n                                // bit faster.\n                                char * close_parent = strstr( buffer, \") \" );\n                                if ( close_parent != NULL ) {\n                                    char state = * ( close_parent + 2 );\n                                    if ( state == 'R' ) {\n                                        ++ running_threads;\n                                        if ( running_threads >= max ) {\n                                            goto finish;\n                                        }; // if\n                                    }; // if\n                                }; // if\n                            }; // if\n                            close( stat_file );\n                            stat_file = -1;\n                        }; // if\n                    }; // if\n                    task_entry = readdir( task_dir );\n                }; // while\n                closedir( task_dir );\n                task_dir = NULL;\n            }; // if\n        }; // if\n        proc_entry = readdir( proc_dir );\n    }; // while\n\n    //\n    // There _might_ be a timing hole where the thread executing this\n    // code get skipped in the load balance, and running_threads is 0.\n    // Assert in the debug builds only!!!\n    //\n    KMP_DEBUG_ASSERT( running_threads > 0 );\n    if ( running_threads <= 0 ) {\n        running_threads = 1;\n    }\n\n    finish: // Clean up and exit.\n        if ( proc_dir != NULL ) {\n            closedir( proc_dir );\n        }; // if\n        __kmp_str_buf_free( & task_path );\n        if ( task_dir != NULL ) {\n            closedir( task_dir );\n        }; // if\n        __kmp_str_buf_free( & stat_path );\n        if ( stat_file != -1 ) {\n            close( stat_file );\n        }; // if\n\n    glb_running_threads = running_threads;\n\n    return running_threads;\n\n} // __kmp_get_load_balance\n\n# endif // KMP_OS_DARWIN\n\n#endif // USE_LOAD_BALANCE\n\n#if !(KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_MIC)\n\n// we really only need the case with 1 argument, because CLANG always build\n// a struct of pointers to shared variables referenced in the outlined function\nint\n__kmp_invoke_microtask( microtask_t pkfn,\n                        int gtid, int tid,\n                        int argc, void *p_argv[] \n#if OMPT_SUPPORT\n                        , void **exit_frame_ptr\n#endif\n) \n{\n#if OMPT_SUPPORT\n  *exit_frame_ptr = __builtin_frame_address(0);\n#endif\n\n  switch (argc) {\n  default:\n    fprintf(stderr, \"Too many args to microtask: %d!\\n\", argc);\n    fflush(stderr);\n    exit(-1);\n  case 0:\n    (*pkfn)(&gtid, &tid);\n    break;\n  case 1:\n    (*pkfn)(&gtid, &tid, p_argv[0]);\n    break;\n  case 2:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1]);\n    break;\n  case 3:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2]);\n    break;\n  case 4:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3]);\n    break;\n  case 5:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4]);\n    break;\n  case 6:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5]);\n    break;\n  case 7:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6]);\n    break;\n  case 8:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7]);\n    break;\n  case 9:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8]);\n    break;\n  case 10:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9]);\n    break;\n  case 11:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10]);\n    break;\n  case 12:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11]);\n    break;\n  case 13:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12]);\n    break;\n  case 14:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13]);\n    break;\n  case 15:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14]);\n    break;\n  }\n\n#if OMPT_SUPPORT\n  *exit_frame_ptr = 0;\n#endif\n\n  return 1;\n}\n\n#endif\n\n// end of file //\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/runtime/src/thirdparty/ittnotify/ittnotify_config.h": "\n//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _ITTNOTIFY_CONFIG_H_\n#define _ITTNOTIFY_CONFIG_H_\n\n/** @cond exclude_from_documentation */\n#ifndef ITT_OS_WIN\n#  define ITT_OS_WIN   1\n#endif /* ITT_OS_WIN */\n\n#ifndef ITT_OS_LINUX\n#  define ITT_OS_LINUX 2\n#endif /* ITT_OS_LINUX */\n\n#ifndef ITT_OS_MAC\n#  define ITT_OS_MAC   3\n#endif /* ITT_OS_MAC */\n\n#ifndef ITT_OS\n#  if defined WIN32 || defined _WIN32\n#    define ITT_OS ITT_OS_WIN\n#  elif defined( __APPLE__ ) && defined( __MACH__ )\n#    define ITT_OS ITT_OS_MAC\n#  else\n#    define ITT_OS ITT_OS_LINUX\n#  endif\n#endif /* ITT_OS */\n\n#ifndef ITT_PLATFORM_WIN\n#  define ITT_PLATFORM_WIN 1\n#endif /* ITT_PLATFORM_WIN */\n\n#ifndef ITT_PLATFORM_POSIX\n#  define ITT_PLATFORM_POSIX 2\n#endif /* ITT_PLATFORM_POSIX */\n\n#ifndef ITT_PLATFORM_MAC\n#  define ITT_PLATFORM_MAC 3\n#endif /* ITT_PLATFORM_MAC */\n\n#ifndef ITT_PLATFORM\n#  if ITT_OS==ITT_OS_WIN\n#    define ITT_PLATFORM ITT_PLATFORM_WIN\n#  elif ITT_OS==ITT_OS_MAC\n#    define ITT_PLATFORM ITT_PLATFORM_MAC\n#  else\n#    define ITT_PLATFORM ITT_PLATFORM_POSIX\n#  endif\n#endif /* ITT_PLATFORM */\n\n#if defined(_UNICODE) && !defined(UNICODE)\n#define UNICODE\n#endif\n\n#include <stddef.h>\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <tchar.h>\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <stdint.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE || _UNICODE */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#ifndef CDECL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define CDECL __cdecl\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__ \n#      define CDECL __attribute__ ((cdecl))\n#    else  /* _M_IX86 || __i386__ */\n#      define CDECL /* actual only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* CDECL */\n\n#ifndef STDCALL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define STDCALL __stdcall\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define STDCALL __attribute__ ((stdcall)) \n#    else  /* _M_IX86 || __i386__ */\n#      define STDCALL /* supported only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* STDCALL */\n\n#define ITTAPI    CDECL\n#define LIBITTAPI CDECL\n\n/* TODO: Temporary for compatibility! */\n#define ITTAPI_CALL    CDECL\n#define LIBITTAPI_CALL CDECL\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n/* use __forceinline (VC++ specific) */\n#define ITT_INLINE           __forceinline\n#define ITT_INLINE_ATTRIBUTE /* nothing */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/*\n * Generally, functions are not inlined unless optimization is specified.\n * For functions declared inline, this attribute inlines the function even\n * if no optimization level was specified.\n */\n#ifdef __STRICT_ANSI__\n#define ITT_INLINE           static\n#define ITT_INLINE_ATTRIBUTE __attribute__((unused))\n#else  /* __STRICT_ANSI__ */\n#define ITT_INLINE           static inline\n#define ITT_INLINE_ATTRIBUTE __attribute__((always_inline, unused))\n#endif /* __STRICT_ANSI__ */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/** @endcond */\n\n#ifndef ITT_ARCH_IA32\n#  define ITT_ARCH_IA32  1\n#endif /* ITT_ARCH_IA32 */\n\n#ifndef ITT_ARCH_IA32E\n#  define ITT_ARCH_IA32E 2\n#endif /* ITT_ARCH_IA32E */\n\n/* Was there a magical reason we didn't have 3 here before? */\n#ifndef ITT_ARCH_AARCH64\n#  define ITT_ARCH_AARCH64  3\n#endif /* ITT_ARCH_AARCH64 */\n\n#ifndef ITT_ARCH_ARM\n#  define ITT_ARCH_ARM  4\n#endif /* ITT_ARCH_ARM */\n\n#ifndef ITT_ARCH_PPC64\n#  define ITT_ARCH_PPC64  5\n#endif /* ITT_ARCH_PPC64 */\n\n\n#ifndef ITT_ARCH\n#  if defined _M_IX86 || defined __i386__\n#    define ITT_ARCH ITT_ARCH_IA32\n#  elif defined _M_X64 || defined _M_AMD64 || defined __x86_64__\n#    define ITT_ARCH ITT_ARCH_IA32E\n#  elif defined _M_IA64 || defined __ia64__\n#    define ITT_ARCH ITT_ARCH_IA64\n#  elif defined _M_ARM || __arm__\n#    define ITT_ARCH ITT_ARCH_ARM\n#  elif defined __powerpc64__\n#    define ITT_ARCH ITT_ARCH_PPC64\n#  elif defined __aarch64__\n#    define ITT_ARCH ITT_ARCH_AARCH64\n#  endif\n#endif\n\n#ifdef __cplusplus\n#  define ITT_EXTERN_C extern \"C\"\n#  define ITT_EXTERN_C_BEGIN extern \"C\" {\n#  define ITT_EXTERN_C_END }\n#else\n#  define ITT_EXTERN_C /* nothing */\n#  define ITT_EXTERN_C_BEGIN /* nothing */\n#  define ITT_EXTERN_C_END /* nothing */\n#endif /* __cplusplus */\n\n#define ITT_TO_STR_AUX(x) #x\n#define ITT_TO_STR(x)     ITT_TO_STR_AUX(x)\n\n#define __ITT_BUILD_ASSERT(expr, suffix) do { \\\n    static char __itt_build_check_##suffix[(expr) ? 1 : -1]; \\\n    __itt_build_check_##suffix[0] = 0; \\\n} while(0)\n#define _ITT_BUILD_ASSERT(expr, suffix)  __ITT_BUILD_ASSERT((expr), suffix)\n#define ITT_BUILD_ASSERT(expr)           _ITT_BUILD_ASSERT((expr), __LINE__)\n\n#define ITT_MAGIC { 0xED, 0xAB, 0xAB, 0xEC, 0x0D, 0xEE, 0xDA, 0x30 }\n\n/* Replace with snapshot date YYYYMMDD for promotion build. */\n#define API_VERSION_BUILD    20111111\n\n#ifndef API_VERSION_NUM\n#define API_VERSION_NUM 0.0.0\n#endif /* API_VERSION_NUM */\n\n#define API_VERSION \"ITT-API-Version \" ITT_TO_STR(API_VERSION_NUM) \\\n                                \" (\" ITT_TO_STR(API_VERSION_BUILD) \")\"\n\n/* OS communication functions */\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <windows.h>\ntypedef HMODULE           lib_t;\ntypedef DWORD             TIDT;\ntypedef CRITICAL_SECTION  mutex_t;\n#define MUTEX_INITIALIZER { 0 }\n#define strong_alias(name, aliasname) /* empty for Windows */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <dlfcn.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE */\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1 /* need for PTHREAD_MUTEX_RECURSIVE */\n#endif /* _GNU_SOURCE */\n#ifndef __USE_UNIX98\n#define __USE_UNIX98 1 /* need for PTHREAD_MUTEX_RECURSIVE, on SLES11.1 with gcc 4.3.4 wherein pthread.h missing dependency on __USE_XOPEN2K8 */\n#endif /*__USE_UNIX98*/\n#include <pthread.h>\ntypedef void*             lib_t;\ntypedef pthread_t         TIDT;\ntypedef pthread_mutex_t   mutex_t;\n#define MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n#define _strong_alias(name, aliasname) \\\n            extern __typeof (name) aliasname __attribute__ ((alias (#name)));\n#define strong_alias(name, aliasname) _strong_alias(name, aliasname)\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#define __itt_get_proc(lib, name) GetProcAddress(lib, name)\n#define __itt_mutex_init(mutex)   InitializeCriticalSection(mutex)\n#define __itt_mutex_lock(mutex)   EnterCriticalSection(mutex)\n#define __itt_mutex_unlock(mutex) LeaveCriticalSection(mutex)\n#define __itt_load_lib(name)      LoadLibraryA(name)\n#define __itt_unload_lib(handle)  FreeLibrary(handle)\n#define __itt_system_error()      (int)GetLastError()\n#define __itt_fstrcmp(s1, s2)     lstrcmpA(s1, s2)\n#define __itt_fstrlen(s)          lstrlenA(s)\n#define __itt_fstrcpyn(s1, s2, l) lstrcpynA(s1, s2, l)\n#define __itt_fstrdup(s)          _strdup(s)\n#define __itt_thread_id()         GetCurrentThreadId()\n#define __itt_thread_yield()      SwitchToThread()\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return InterlockedIncrement(ptr);\n}\n#endif /* ITT_SIMPLE_INIT */\n#else /* ITT_PLATFORM!=ITT_PLATFORM_WIN */\n#define __itt_get_proc(lib, name) dlsym(lib, name)\n#define __itt_mutex_init(mutex)   {\\\n    pthread_mutexattr_t mutex_attr;                                         \\\n    int error_code = pthread_mutexattr_init(&mutex_attr);                   \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_init\",    \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_settype(&mutex_attr,                     \\\n                                           PTHREAD_MUTEX_RECURSIVE);        \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_settype\", \\\n                           error_code);                                     \\\n    error_code = pthread_mutex_init(mutex, &mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutex_init\",        \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_destroy(&mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_destroy\", \\\n                           error_code);                                     \\\n}\n#define __itt_mutex_lock(mutex)   pthread_mutex_lock(mutex)\n#define __itt_mutex_unlock(mutex) pthread_mutex_unlock(mutex)\n#define __itt_load_lib(name)      dlopen(name, RTLD_LAZY)\n#define __itt_unload_lib(handle)  dlclose(handle)\n#define __itt_system_error()      errno\n#define __itt_fstrcmp(s1, s2)     strcmp(s1, s2)\n#define __itt_fstrlen(s)          strlen(s)\n#define __itt_fstrcpyn(s1, s2, l) strncpy(s1, s2, l)\n#define __itt_fstrdup(s)          strdup(s)\n#define __itt_thread_id()         pthread_self()\n#define __itt_thread_yield()      sched_yield()\n#if ITT_ARCH==ITT_ARCH_IA64\n#ifdef __INTEL_COMPILER\n#define __TBB_machine_fetchadd4(addr, val) __fetchadd4_acq((void *)addr, val)\n#else  /* __INTEL_COMPILER */\n/* TODO: Add Support for not Intel compilers for IA-64 architecture */\n#endif /* __INTEL_COMPILER */\n#elif ITT_ARCH==ITT_ARCH_IA32 || ITT_ARCH==ITT_ARCH_IA32E /* ITT_ARCH!=ITT_ARCH_IA64 */\nITT_INLINE long\n__TBB_machine_fetchadd4(volatile void* ptr, long addend) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __TBB_machine_fetchadd4(volatile void* ptr, long addend)\n{\n    long result;\n    __asm__ __volatile__(\"lock\\nxadd %0,%1\"\n                          : \"=r\"(result),\"=m\"(*(int*)ptr)\n                          : \"0\"(addend), \"m\"(*(int*)ptr)\n                          : \"memory\");\n    return result;\n}\n#elif ITT_ARCH==ITT_ARCH_ARM || ITT_ARCH==ITT_ARCH_PPC64 || ITT_ARCH==ITT_ARCH_AARCH64\n#define __TBB_machine_fetchadd4(addr, val) __sync_fetch_and_add(addr, val)\n#endif /* ITT_ARCH==ITT_ARCH_IA64 */\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return __TBB_machine_fetchadd4(ptr, 1) + 1L;\n}\n#endif /* ITT_SIMPLE_INIT */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\ntypedef enum {\n    __itt_collection_normal = 0,\n    __itt_collection_paused = 1\n} __itt_collection_state;\n\ntypedef enum {\n    __itt_thread_normal  = 0,\n    __itt_thread_ignored = 1\n} __itt_thread_state;\n\n#pragma pack(push, 8)\n\ntypedef struct ___itt_thread_info\n{\n    const char* nameA; /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    TIDT               tid;\n    __itt_thread_state state;   /*!< Thread state (paused or normal) */\n    int                extra1;  /*!< Reserved to the runtime */\n    void*              extra2;  /*!< Reserved to the runtime */\n    struct ___itt_thread_info* next;\n} __itt_thread_info;\n\n#include \"ittnotify_types.h\" /* For __itt_group_id definition */\n\ntypedef struct ___itt_api_info_20101001\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    __itt_group_id group;\n}  __itt_api_info_20101001;\n\ntypedef struct ___itt_api_info\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    void*          null_func;\n    __itt_group_id group;\n}  __itt_api_info;\n\nstruct ___itt_domain;\nstruct ___itt_string_handle;\n\ntypedef struct ___itt_global\n{\n    unsigned char          magic[8];\n    unsigned long          version_major;\n    unsigned long          version_minor;\n    unsigned long          version_build;\n    volatile long          api_initialized;\n    volatile long          mutex_initialized;\n    volatile long          atomic_counter;\n    mutex_t                mutex;\n    lib_t                  lib;\n    void*                  error_handler;\n    const char**           dll_path_ptr;\n    __itt_api_info*        api_list_ptr;\n    struct ___itt_global*  next;\n    /* Joinable structures below */\n    __itt_thread_info*     thread_list;\n    struct ___itt_domain*  domain_list;\n    struct ___itt_string_handle* string_list;\n    __itt_collection_state state;\n} __itt_global;\n\n#pragma pack(pop)\n\n#define NEW_THREAD_INFO_W(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = NULL; \\\n        h->nameW  = n ? _wcsdup(n) : NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_THREAD_INFO_A(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = n ? __itt_fstrdup(n) : NULL; \\\n        h->nameW  = NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_W(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 0;    /* domain is disabled by default */ \\\n        h->nameA  = NULL; \\\n        h->nameW  = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_A(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 0;    /* domain is disabled by default */ \\\n        h->nameA  = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW  = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_W(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = NULL; \\\n        h->strW   = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_A(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->strW   = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#endif /* _ITTNOTIFY_CONFIG_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/offload/src/offload_util.cpp": "//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n\n\n#include \"offload_util.h\"\n#include <errno.h>\n#include \"liboffload_error_codes.h\"\n\n#ifdef TARGET_WINNT\nvoid *thread_getspecific(pthread_key_t key)\n{\n    if (key == 0) {\n        return NULL;\n    }\n    else {\n        return TlsGetValue(key);\n    }\n}\n\nint thread_setspecific(pthread_key_t key, const void *value)\n{\n    return (TlsSetValue(key, (LPVOID)value)) ? 0 : GetLastError();\n}\n#endif // TARGET_WINNT\n\nbool __offload_parse_size_string(const char *str, uint64_t &new_size)\n{\n    uint64_t val;\n    char *suffix;\n\n    errno = 0;\n#ifdef TARGET_WINNT\n    val = strtoul(str, &suffix, 10);\n#else // TARGET_WINNT\n    val = strtoull(str, &suffix, 10);\n#endif // TARGET_WINNT\n    if (errno != 0 || suffix == str) {\n        return false;\n    }\n\n    if (suffix[0] == '\\0') {\n        // default is Kilobytes\n        new_size = val * 1024;\n        return true;\n    }\n    else if (suffix[1] == '\\0') {\n        // Optional suffixes: B (bytes), K (Kilobytes), M (Megabytes),\n        // G (Gigabytes), or T (Terabytes) specify the units.\n        switch (suffix[0]) {\n            case 'b':\n            case 'B':\n                new_size = val;\n                break;\n\n            case 'k':\n            case 'K':\n                new_size = val * 1024;\n                break;\n\n            case 'm':\n            case 'M':\n                new_size = val * 1024 * 1024;\n                break;\n\n            case 'g':\n            case 'G':\n                new_size = val * 1024 * 1024 * 1024;\n                break;\n\n            case 't':\n            case 'T':\n                new_size = val * 1024 * 1024 * 1024 * 1024;\n                break;\n\n            default:\n                return false;\n        }\n        return true;\n    }\n\n    return false;\n}\n\nbool __offload_parse_int_string(const char *str, int64_t &value)\n{\n    int64_t val;\n    char *suffix;\n\n    errno = 0;\n#ifdef TARGET_WINNT\n    val = strtol(str, &suffix, 0);\n#else\n    val = strtoll(str, &suffix, 0);\n#endif\n    if (errno == 0 && suffix != str && *suffix == '\\0') {\n        value = val;\n        return true;\n    }\n    return false;\n}\n\n#ifdef TARGET_WINNT\nextern void* DL_open(const char *path)\n{\n    void *handle;\n    int error_mode;\n\n    /*\n     * do not display message box with error if it the call below fails to\n     * load dynamic library.\n     */\n    error_mode = SetErrorMode(SEM_FAILCRITICALERRORS | SEM_NOOPENFILEERRORBOX);\n\n    /* load dynamic library */\n    handle = (void*) LoadLibrary(path);\n\n    /* restore error mode */\n    SetErrorMode(error_mode);\n\n    return handle;\n}\n\nextern int DL_addr(const void *addr, Dl_info *dl_info)\n{\n    MEMORY_BASIC_INFORMATION mem_info;\n    char mod_name[MAX_PATH];\n    HMODULE mod_handle;\n\n    /* Fill MEMORY_BASIC_INFORMATION struct */\n    if (!VirtualQuery(addr, &mem_info, sizeof(mem_info))) {\n        return 0;\n    }\n    mod_handle = (HMODULE)mem_info.AllocationBase;\n\n    /* ANSI file name for module */\n    if (!GetModuleFileNameA(mod_handle, (char*) mod_name, sizeof(mod_name))) {\n        return 0;\n    }\n    strcpy(dl_info->dli_fname, mod_name);\n    dl_info->dli_fbase = mem_info.BaseAddress;\n    dl_info->dli_saddr = addr;\n    strcpy(dl_info->dli_sname, mod_name);\n    return 1;\n}\n\n// Run once\nstatic BOOL CALLBACK __offload_run_once_wrapper(\n    PINIT_ONCE initOnce,\n    PVOID parameter,\n    PVOID *context\n)\n{\n    void (*init_routine)(void) = (void(*)(void)) parameter;\n    init_routine();\n    return true;\n}\n\nvoid __offload_run_once(OffloadOnceControl *ctrl, void (*func)(void))\n{\n    InitOnceExecuteOnce(ctrl, __offload_run_once_wrapper, (void*) func, 0);\n}\n#endif // TARGET_WINNT\n\n/* ARGSUSED */ // version is not used on windows\nvoid* DL_sym(void *handle, const char *name, const char *version)\n{\n#ifdef TARGET_WINNT\n    return GetProcAddress((HMODULE) handle, name);\n#else // TARGET_WINNT\n    if (version == 0) {\n        return dlsym(handle, name);\n    }\n    else {\n        return dlvsym(handle, name, version);\n    }\n#endif // TARGET_WINNT\n}\n\nint64_t get_el_value(\n                     char *base,\n                     int64_t offset,\n                     int64_t size)\n{\n    int64_t val = 0;\n    switch (size) {\n        case 1:\n            val = static_cast<int64_t>(*((char *)(base + offset)));\n            break;\n        case 2:\n            val = static_cast<int64_t>(*((short *)(base + offset)));\n            break;\n        case 4:\n            val = static_cast<int64_t>(*((int *)(base + offset)));\n            break;\n        default:\n            val = *((int64_t *)(base + offset));\n            break;\n    }\n    return val;\n}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/runtime/doc/Reference.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/www/Reference.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/offload/src/liboffload_msg.h",
        "/tmp/vanessa/spack-stage/spack-stage-ompt-openmp-0.1-gjnlb6lewv4mjd2cjtb53q3neads6lyy/spack-src/offload/doc/Reference.pdf"
    ],
    "total_files": 416
}