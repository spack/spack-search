{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.4.2-1-dyxfkphyrzsextqutdm6sstd4s2f3xsj/spack-src/src/init.cu": "/*************************************************************************\n * Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nccl.h\"\n#include \"core.h\"\n#include \"channel.h\"\n#include \"param.h\"\n#include \"nvmlwrap.h\"\n#include \"rings.h\"\n#include \"trees.h\"\n#include \"bootstrap.h\"\n#include \"transport.h\"\n#include \"group.h\"\n#include \"utils.h\"\n#include \"net.h\"\n#include \"checks.h\"\n#include \"enqueue.h\"\n#include \"topo.h\"\n#include \"nvlink.h\"\n#include \"cpuset.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/mman.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sched.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <cuda_runtime.h>\n#include <string.h>\n#include <errno.h>\n#include <assert.h>\n#include <dlfcn.h>\n\n#define STR2(v) #v\n#define STR(v) STR2(v)\n\nint ncclDebugLevel;\nuint64_t ncclDebugMask = NCCL_INIT; // Default debug sub-system mask is INIT\npthread_mutex_t ncclDebugOutputLock;\nFILE *ncclDebugFile = stdout;\n\n#ifdef ENABLE_TRACE\nstd::chrono::high_resolution_clock::time_point ncclEpoch;\n#endif\n\n#if CUDART_VERSION >= 9200\n#define NCCL_GROUP_CUDA_STREAM 0 // CGMD: CUDA 9.2,10.X Don't need to use an internal CUDA stream\n#else\n#define NCCL_GROUP_CUDA_STREAM 1 // CGMD: CUDA 9.0,9.1 Need to use an internal CUDA stream\n#endif\n\nNCCL_PARAM(GroupCudaStream, \"GROUP_CUDA_STREAM\", NCCL_GROUP_CUDA_STREAM);\n\nNCCL_PARAM(CheckPointers, \"CHECK_POINTERS\", 0);\n\nncclNet_t* ncclNet = NULL;\n\n// We define this as weak to let tests redefine their own\n#pragma weak ncclNvlinkGpu\nncclResult_t ncclNvlinkGpu(int* nvlink) {\n  int cudaDev;\n  CUDACHECK(cudaGetDevice(&cudaDev));\n  char busId[NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE];\n  CUDACHECK(cudaDeviceGetPCIBusId(busId, NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE, cudaDev));\n  *nvlink = getNvlinkGpu(busId, NULL);\n  return ncclSuccess;\n}\n// We define this as weak to let tests redefine their own\n#pragma weak ncclCudaCompCap\nint ncclCudaCompCap() {\n  int cudaDev;\n  if (cudaGetDevice(&cudaDev) != cudaSuccess) return 0;\n  int ccMajor;\n  if (cudaDeviceGetAttribute(&ccMajor, cudaDevAttrComputeCapabilityMajor, cudaDev) != cudaSuccess) return 0;\n  return ccMajor;\n}\nint ncclCudaFullCompCap() {\n  int cudaDev;\n  if (cudaGetDevice(&cudaDev) != cudaSuccess) return 0;\n  int ccMajor, ccMinor;\n  if (cudaDeviceGetAttribute(&ccMajor, cudaDevAttrComputeCapabilityMajor, cudaDev) != cudaSuccess) return 0;\n  if (cudaDeviceGetAttribute(&ccMinor, cudaDevAttrComputeCapabilityMinor, cudaDev) != cudaSuccess) return 0;\n  return ccMajor*10+ccMinor;\n}\n\n// Returns ncclInternalError if anything fails, causing that network to be ignored.\nncclResult_t initNet(ncclNet_t* net) {\n  int ndev;\n  if (net->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (net->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initNetPlugin(ncclNet_t** net) {\n  void* netPluginLib = dlopen(\"libnccl-net.so\", RTLD_NOW | RTLD_LOCAL);\n  if (netPluginLib == NULL) {\n    // dlopen does not guarantee to set errno, but dlerror only gives us a\n    // string, so checking errno doesn't hurt to try to provide a better\n    // error message\n    if (errno == ENOENT) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : No plugin found (libnccl-net.so).\");\n    } else {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : Plugin load returned %d : %s.\", errno, dlerror());\n    }\n    return ncclSuccess;\n  }\n  ncclNet_t* extNet = (ncclNet_t*) dlsym(netPluginLib, STR(NCCL_PLUGIN_SYMBOL));\n  if (extNet == NULL) {\n    INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_PLUGIN_SYMBOL) \" symbol.\");\n    goto cleanup;\n  }\n  if (initNet(extNet) == ncclSuccess) {\n    *net = extNet;\n    return ncclSuccess;\n  }\ncleanup:\n  if (netPluginLib != NULL) dlclose(netPluginLib);\n  return ncclSuccess;\n}\n\nncclResult_t initNet() {\n  // Always initialize sockets as we use it for bootstrap\n  NCCLCHECK(initNet(&ncclNetSocket));\n\n  NCCLCHECK(initNetPlugin(&ncclNet));\n  if (ncclNet != NULL) return ncclSuccess;\n  if (initNet(&ncclNetIb) == ncclSuccess) {\n    ncclNet = &ncclNetIb;\n  } else {\n    ncclNet = &ncclNetSocket;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(LlThreshold, \"LL_THRESHOLD\", -2);\nNCCL_PARAM(ThreadThreshold, \"THREAD_THRESHOLD\", -2);\nNCCL_PARAM(TreeThreshold, \"TREE_THRESHOLD\", -2);\n\nint ncclThreadThreshold(int minCompCap, int multiNode) {\n  int threshold = ncclParamThreadThreshold();\n  if (threshold == -2) { // user has not set this env variable\n    threshold = (minCompCap <= 6) ? NCCL_THREAD_THRESHOLD_PREVOLTA : NCCL_THREAD_THRESHOLD;\n    // multiply by 2 if running on multiple nodes\n    if (multiNode) {\n      threshold *= 2;\n    }\n  }\n  return threshold;\n}\n\npthread_mutex_t initLock = PTHREAD_MUTEX_INITIALIZER;\nstatic bool initialized = false;\nstatic ncclResult_t ncclInit() {\n  if (initialized) return ncclSuccess;\n  pthread_mutex_lock(&initLock);\n  if (!initialized) {\n    initEnv();\n    initDebug();\n    initNet();\n    initialized = true;\n  }\n  pthread_mutex_unlock(&initLock);\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetVersion, int* version);\nncclResult_t ncclGetVersion(int* version) {\n  if (version == NULL) return ncclInvalidArgument;\n  *version = NCCL_VERSION_CODE;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetUniqueId, ncclUniqueId* out);\nncclResult_t ncclGetUniqueId(ncclUniqueId* out) {\n  NCCLCHECK(ncclInit());\n  NCCLCHECK(PtrCheck(out, \"GetUniqueId\", \"out\"));\n  return bootstrapGetUniqueId(out);\n}\n\nstatic ncclResult_t commFree(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  free(comm->peerInfo);\n\n  if (comm->bootstrap)\n    NCCLCHECK(bootstrapClose(comm->bootstrap));\n\n  CUDACHECK(cudaFree(comm->devComm));\n\n  for (int channel=0; channel<comm->nChannels; channel++)\n    NCCLCHECK(freeChannel(comm->channels+channel, comm->nRanks));\n\n  if (comm->doneEvent != NULL)\n    CUDACHECK(cudaEventDestroy(comm->doneEvent));\n\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamDestroy(comm->groupStream));\n  }\n\n  // Last rank frees shared resources between threads\n  int isLast;\n  NCCLCHECK(ncclCpuBarrierIn(comm, &isLast));\n  if (isLast) {\n    free(comm->intraBarrier);\n    free(comm->intraParams);\n    free(comm->intraCudaDevs);\n    free(comm->intraCGMode);\n    free(comm->intraCC);\n  }\n  CUDACHECK(cudaFreeHost((void *)comm->abortFlag));\n  CUDACHECK(cudaFreeHost((void *)comm->fatalDevError));\n\n  free(comm);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commAlloc(ncclComm_t* comret, int ndev, int rank) {\n  if (ndev < 1) {\n    WARN(\"invalid device count (%d) requested\", ndev);\n    return ncclInvalidArgument;\n  }\n  if (rank >= ndev || rank < 0) {\n    WARN(\"rank %d exceeds ndev=%d\", rank, ndev);\n    return ncclInvalidArgument;\n  }\n\n  // Try to create a CUDA object right away. If there is something wrong with\n  // the device we're on (failure cause #1) , better know it early.\n  cudaEvent_t doneEvent;\n  CUDACHECK(cudaEventCreateWithFlags(&doneEvent, cudaEventDisableTiming));\n\n  struct ncclComm* comm;\n  NCCLCHECK(ncclCalloc(&comm, 1));\n\n  comm->rank = rank;\n  comm->nRanks = ndev;\n  cudaGetDevice(&comm->cudaDev);\n  getNvmlDevice(comm->cudaDev, &comm->nvmlDev);\n  INFO(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d nvmlDev %d\", comm, rank, ndev, comm->cudaDev, comm->nvmlDev);\n\n  comm->doneEvent = doneEvent;\n  comm->llThreshold = ncclParamLlThreshold();\n  comm->treeThreshold = ncclParamTreeThreshold();\n  comm->checkPointers = ncclParamCheckPointers() == 1 ? true : false;\n#if CUDART_VERSION >= 9200\n  comm->groupCudaStream = ncclParamGroupCudaStream();\n#else\n  // Don't allow the user to overload the default setting in older CUDA builds\n  comm->groupCudaStream = NCCL_GROUP_CUDA_STREAM;\n#endif\n  comm->fatalError = ncclSuccess;\n\n  CUDACHECK(cudaHostAlloc((void**) &comm->fatalDevError, sizeof(ncclDevError_t), cudaHostAllocMapped));\n  *comm->fatalDevError = ncclDevSuccess;\n\n  CUDACHECK(cudaHostAlloc((void**) &comm->abortFlag, sizeof(uint32_t), cudaHostAllocMapped));\n  *comm->abortFlag = 0;\n\n  comm->argsptr = &comm->args;\n\n  *comret = comm;\n  return ncclSuccess;\n}\n\nstatic ncclResult_t devCommSetup(ncclComm_t comm) {\n  // Fully duplicate the comm on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->devComm, 1));\n  // Copy the comm on the device\n  NCCLCHECK(ncclCudaMemcpy(comm->devComm, comm, 1));\n  // Copy userRanks\n  for (int r=0; r<comm->nChannels; r++) {\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].ring.devUserRanks, comm->channels[r].ring.userRanks, comm->nRanks));\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].devPeers, comm->channels[r].peers, comm->nRanks));\n  }\n  // Copy the device-accessible pointer to comm->abortFlag\n  void *devAbortFlag;\n  CUDACHECK(cudaHostGetDevicePointer(&devAbortFlag, (uint32_t *)comm->abortFlag, 0));\n  CUDACHECK(cudaMemcpy(&comm->devComm->abortFlag, &devAbortFlag, sizeof(int *), cudaMemcpyHostToDevice));\n  // Copy the device-accessible pointer to comm->fatalDevError\n  void *devFatalError;\n  CUDACHECK(cudaHostGetDevicePointer(&devFatalError, (ncclDevError_t *)comm->fatalDevError, 0));\n  CUDACHECK(cudaMemcpy(&comm->devComm->fatalDevError, &devFatalError, sizeof(ncclDevError_t *), cudaMemcpyHostToDevice));\n  return ncclSuccess;\n}\n\n// Pre-process the string so that running \"strings\" on the lib can quickly reveal the version.\n#define VERSION_STRING \"NCCL version \" STR(NCCL_MAJOR) \".\" STR(NCCL_MINOR) \".\" STR(NCCL_PATCH) NCCL_SUFFIX \"+cuda\" STR(CUDA_MAJOR) \".\" STR(CUDA_MINOR)\nstatic void showVersion() {\n  static int shown = 0;\n  if (shown == 0 && ncclDebugLevel >= NCCL_LOG_VERSION) {\n    printf(\"%s\\n\", VERSION_STRING);\n    fflush(stdout);\n    if (ncclDebugFile != stdout)\n      INFO(NCCL_ALL,\"%s\", VERSION_STRING); // Also log NCCL version in one of the files\n    shown = 1;\n  }\n}\n\nstatic ncclResult_t fillInfo(struct ncclPeerInfo* info, int rank) {\n  info->rank = rank;\n  CUDACHECK(cudaGetDevice(&info->cudaDev));\n  NCCLCHECK(getNvmlDevice(info->cudaDev, &info->nvmlDev))\n  info->hostHash=getHostHash();\n  info->pidHash=getPidHash();\n\n  // Get PCI Bus Id. We need to get the bus ID through CUDA first, since the\n  // cudaDev is a CUDA runtime dev number which could be different from the\n  // NVML device number. Then we get the busID from NVML to be sure it is\n  // consistent with NVML remote PCI bus Ids.\n  CUDACHECK(cudaDeviceGetPCIBusId(info->busId, NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE, info->cudaDev));\n  nvmlDevice_t nvmlDevice;\n  NCCLCHECK(wrapNvmlDeviceGetHandleByPciBusId(info->busId, &nvmlDevice));\n  nvmlPciInfo_t pciInfo;\n  NCCLCHECK(wrapNvmlDeviceGetPciInfo(nvmlDevice, &pciInfo));\n  strncpy(info->busId, pciInfo.busId, NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE);\n  return ncclSuccess;\n}\n\ntemplate <int type>\nstatic ncclResult_t selectTransport(struct ncclPeerInfo* myInfo, struct ncclPeerInfo* peerInfo, struct ncclConnect* connect, struct ncclConnector* connector, int buffSize, int channelId) {\n  for (int t=0; t<NTRANSPORTS; t++) {\n    struct ncclTransport *transport = ncclTransports+t;\n    struct ncclTransportComm* transportComm = type == 1 ? &transport->send : &transport->recv;\n    ncclTvalue_t ret = 0;\n    NCCLCHECK(transport->canConnect(&ret, myInfo, peerInfo));\n    if (ret > 0) {\n      connector->transportComm = transportComm;\n      NCCLCHECK(transportComm->setup(myInfo, peerInfo, connect, connector, buffSize, channelId));\n      return ncclSuccess;\n    }\n  }\n  WARN(\"No transport found !\");\n  return ncclInternalError;\n}\n\nstatic int log2(int n) {\n int l = 0;\n while (n>>=1) l++;\n return l;\n}\n\nstatic ncclResult_t ncclTreeThreshold(int nnodes, int nranks, int nChannels, ssize_t *treeThreshold) {\n  int nvlink;\n  NCCLCHECK(ncclNvlinkGpu(&nvlink));\n  float ringbw = nvlink ? 5000*nChannels : 5000; // approx, in MB/s or B/us\n  float ringlatinter = 6;\n  float treelatintra = 4;\n  float treelatinter = 15;\n  float treebw;\n  if (!nvlink) {\n    treebw = ringbw * 2 / 3;\n  } else {\n    treebw = ringbw * 3 / 4;\n    if (nnodes == 2) treebw *= 2;\n  }\n  float ringlat = ringlatinter*(nranks-1);\n  float treelat = treelatinter*log2(nnodes)+treelatintra*(nranks/nnodes-1);\n  if (nnodes < 2 || ringlat <= treelat)\n    *treeThreshold = 0;\n  else if (treebw > ringbw)\n    *treeThreshold = 0x7fffffffffffffff;\n  else\n    *treeThreshold = (ssize_t)(((ringbw*treebw/(ringbw-treebw)))*(ringlat-treelat));\n  return ncclSuccess;\n}\n\nstatic ncclResult_t setupChannel(struct ncclComm* comm, int channelId, int rank, int nranks, int* ringRanks, int* treeMasters) {\n  TRACE(NCCL_INIT, \"rank %d nranks %d\", rank, nranks);\n  NCCLCHECK(initChannel(comm, channelId));\n\n  struct ncclChannel* channel = comm->channels+channelId;\n  struct ncclRing* ring = &channel->ring;\n\n  // Reorganize ranks to start with rank.\n  int shift;\n  for (shift = 0; shift<nranks; shift++) {\n    if (ringRanks[shift] == rank) {\n      break;\n    }\n  }\n  for (int i=0; i<nranks; i++) {\n    ring->userRanks[i] = ringRanks[(i+shift)%nranks];\n  }\n  int prev = ring->prev = ring->userRanks[nranks-1];\n  int next = ring->next = ring->userRanks[1];\n\n  struct ncclTree* tree = &channel->tree;\n  tree->up = -1;\n  tree->down[0] = tree->down[1] = tree->down[2] = -1;\n\n  //\n  // Find per-node masters and connect them via a binary tree\n  //\n\n  int nMasters = 0;\n  for (int r=0; r<nranks; r++) nMasters += treeMasters[r];\n  if (nMasters == 0) {\n    nMasters = 1;\n    treeMasters[0] = 1;\n  }\n\n  if (comm->treeThreshold == -2)\n    NCCLCHECK(ncclTreeThreshold(nMasters, comm->nRanks, comm->nChannels, &comm->treeThreshold));\n\n  if (comm->treeThreshold > 0) {\n    // Compute tree depth. Not an exact value but a good approximation in most\n    // cases and consistent across nodes\n    tree->depth = nranks/nMasters + log2(nMasters);\n\n    // Find my master : go backwards in the ring to find my root\n    int master = 0;\n    for (int i = 0; i<nranks; i++) {\n      int r = ring->userRanks[(nranks-i)%nranks];\n      if (treeMasters[r]) {\n        master = r;\n        break;\n      }\n    }\n\n    int ranks[nMasters];\n    int i = 0, masterIndex = -1;\n    // Build binary tree\n    for (int r=0; r<nranks; r++) {\n      // Create index table\n      if (r == master) masterIndex = i;\n      if (treeMasters[r]) ranks[i++] = r;\n    }\n    int btreeUp, btreeDown0, btreeDown1;\n    int u0, d0_0, d0_1, u1, d1_0, d1_1;\n    NCCLCHECK(ncclGetDtree(nMasters, masterIndex, &u0, &d0_0, &d0_1, &u1, &d1_0, &d1_1));\n    if (channelId < DIVUP(comm->nChannels, 2)) {\n      btreeUp = u0; btreeDown0 = d0_0; btreeDown1 = d0_1;\n    } else {\n      btreeUp = u1; btreeDown0 = d1_0; btreeDown1 = d1_1;\n    }\n\n    //\n    // Now build the full tree, combining the intra-node ring and the\n    // inter-node binary tree.\n    //\n\n    if (rank == master) {\n      int nDown = 0;\n      if (btreeUp != -1) tree->up = ranks[btreeUp];\n      if (treeMasters[next] == 0) tree->down[nDown++] = next;\n      if (btreeDown0 != -1) tree->down[nDown++] = ranks[btreeDown0];\n      if (btreeDown1 != -1) tree->down[nDown++] = ranks[btreeDown1];\n    } else {\n      tree->up = prev;\n      if (treeMasters[next] == 0) tree->down[0] = next;\n    }\n  }\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - DONE\", rank, nranks);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t fillConnect(struct ncclPeerInfo* peerInfo, int nranks, int rank, int* connectTransport, ncclTvalue_t* connectValue) {\n  for (int r=0; r<nranks; r++) {\n    connectTransport[r] = -1;\n    for (int t=0; t<NTRANSPORTS; t++) {\n      NCCLCHECK(ncclTransports[t].canConnect(connectValue+r, peerInfo+rank, peerInfo+r));\n      if (connectValue[r] > 0) {\n        connectTransport[r] = t;\n        break;\n      }\n    }\n  }\n  return ncclSuccess;\n}\n\n#define MAXWIDTH 20\n#define PREFIXLEN 15\n#define STRLENGTH (PREFIXLEN+5*MAXWIDTH)\nvoid dumpMatrix(int* connectMatrix, int nranks) {\n  char line[STRLENGTH+1];\n  line[STRLENGTH] = '\\0';\n  memset(line, ' ', STRLENGTH);\n  for (int j=0; j<nranks && j<MAXWIDTH; j++) sprintf(4+line+4*j, \" %3d\", j);\n  INFO(NCCL_INIT,\"%s\", line);\n  for (int i=0; i<nranks; i++) {\n    memset(line, ' ', STRLENGTH);\n    sprintf(line, \"%3d \", i);\n    for (int j=0; j<nranks && j<MAXWIDTH; j++) sprintf(4+line+4*j, \" %3d\", connectMatrix[i*nranks+j]);\n    INFO(NCCL_INIT,\"%s\", line);\n  }\n}\n\nvoid dumpMatrixTvalue(ncclTvalue_t* connectMatrix, int nranks) {\n  char line[STRLENGTH+1];\n  line[STRLENGTH] = '\\0';\n  memset(line, ' ', STRLENGTH);\n  for (int j=0; j<nranks && j<MAXWIDTH; j++) sprintf(4+line+5*j, \" %4d\", j);\n  INFO(NCCL_INIT,\"%s\", line);\n  for (int i=0; i<nranks; i++) {\n    memset(line, ' ', STRLENGTH);\n    sprintf(line, \"%3d \", i);\n    for (int j=0; j<nranks && j<MAXWIDTH; j++) sprintf(4+line+5*j, \" %4o\", (int)connectMatrix[i*nranks+j]);\n    INFO(NCCL_INIT,\"%s\", line);\n  }\n}\n\n\nvoid dumpLine(int* values, int nranks, const char* prefix) {\n  int prefixlen = strlen(prefix);\n  char line[STRLENGTH+1];\n  line[STRLENGTH] = '\\0';\n  memset(line, ' ', STRLENGTH);\n  strncpy(line, prefix, PREFIXLEN);\n  for (int i=0; i<nranks && i<MAXWIDTH; i++) sprintf(line+prefixlen+4*i, \" %3d\", values[i]);\n  INFO(NCCL_INIT,\"%s\", line);\n}\n\nstatic ncclResult_t buildRings(int nrings, int* rings, int rank, int nranks, int* prev, int* next) {\n  for (int r=0; r<nrings; r++) {\n    char prefix[30];\n    /*sprintf(prefix, \"[%d] Channel %d Prev : \", rank, r);\n    dumpLine(prev+r*nranks, nranks, prefix);\n    sprintf(prefix, \"[%d] Channel %d Next : \", rank, r);\n    dumpLine(next+r*nranks, nranks, prefix);*/\n\n    int current = rank;\n    for (int i=0; i<nranks; i++) {\n      rings[r*nranks+i] = current;\n      current = next[r*nranks+current];\n    }\n    sprintf(prefix, \"Channel %02d : \", r);\n    if (rank == 0) dumpLine(rings+r*nranks, nranks, prefix);\n    if (current != rank) {\n      WARN(\"Error : ring %d does not loop back to start (%d != %d)\", r, current, rank);\n      return ncclInternalError;\n    }\n    // Check that all ranks are there\n    for (int i=0; i<nranks; i++) {\n      int found = 0;\n      for (int j=0; j<nranks; j++) {\n        if (rings[r*nranks+j] == i) {\n          found = 1;\n          break;\n        }\n      }\n      if (found == 0) {\n        WARN(\"Error : ring %d does not contain rank %d\", r, i);\n        return ncclInternalError;\n      }\n    }\n  }\n  return ncclSuccess;\n}\n\nvoid* waitForNonNullPtr(void* p) {\n  volatile void** ptr = (volatile void**) p;\n  while (*ptr == NULL) sched_yield();\n  return (void*)*ptr;\n}\n\nncclResult_t initParams(struct ncclComm* comm) {\n  struct cudaLaunchParams* params = comm->myParams = comm->intraParams+comm->intraRank;\n  params->args = &comm->argsptr;\n  params->stream = NULL;\n  params->sharedMem = 0;\n  params->blockDim.x = 0; params->blockDim.y = params->blockDim.z = 1;\n  params->gridDim.x = 0; params->gridDim.y = params->gridDim.z = 1;\n  return ncclSuccess;\n}\n\n// Allocate/Set Intra Process Structures and set CG options\nncclResult_t ncclCommSetIntra(struct ncclComm* comm, int rank, int ranks, struct ncclComm* comm0) {\n  comm->intraRank = rank;\n  comm->intraRanks = ranks;\n  comm->intraPhase = 0;\n\n  // Alloc shared structures\n  if (rank == 0) {\n    assert(comm == comm0);\n    int* bar;\n    NCCLCHECK(ncclCalloc(&bar, 2));\n    bar[0] = bar[1] = 0;\n    comm->intraBarrier = bar;\n    NCCLCHECK(ncclCalloc(&comm->intraParams, comm->intraRanks));\n    NCCLCHECK(ncclCalloc(&comm->intraCudaDevs, comm->intraRanks));\n    int* CGMode;\n    NCCLCHECK(ncclCalloc(&CGMode, 1));\n    *CGMode = 0x11;\n    comm->intraCGMode = CGMode;\n    int* CC;\n    NCCLCHECK(ncclCalloc(&CC, 1));\n    *CC = ncclCudaFullCompCap();\n    comm->intraCC = CC;\n  } else {\n    comm->intraBarrier = (int*)waitForNonNullPtr(&comm0->intraBarrier);\n    comm->intraParams = (struct cudaLaunchParams*)waitForNonNullPtr(&comm0->intraParams);\n    comm->intraCudaDevs = (int*)waitForNonNullPtr(&comm0->intraCudaDevs);\n    comm->intraCGMode = (int*)waitForNonNullPtr(&comm0->intraCGMode);\n    comm->intraCC = (int*)waitForNonNullPtr(&comm0->intraCC);\n  }\n  comm->intraCudaDevs[comm->intraRank] = comm->cudaDev;\n  NCCLCHECK(initParams(comm));\n\n  int cgMdLaunch = 0;\n\n  // Set CG Mode\n  comm->launchMode = ncclComm::GROUP;\n  char* str = getenv(\"NCCL_LAUNCH_MODE\");\n  if (comm->intraRanks == 1 || (str && strcmp(str, \"PARALLEL\") == 0)) {\n    comm->launchMode = ncclComm::PARALLEL;\n  }\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamCreateWithFlags(&comm->groupStream, cudaStreamNonBlocking));\n#if CUDART_VERSION >= 9000\n    if (*comm->intraCC && (ncclCudaFullCompCap() == *comm->intraCC)) {\n      // Check whether the GPU supports Cooperative Group Multi Device Launch\n      (void) cudaDeviceGetAttribute(&cgMdLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, comm->cudaDev);\n    }\n#endif\n  }\n\n  // Disable cgMdLaunch if any rank does not support it\n  if (cgMdLaunch == 0) {\n    *comm->intraCGMode = 0x10;\n  }\n  return ncclSuccess;\n}\n\nstatic ncclResult_t p2pSetup(struct ncclComm* comm, struct ncclChannel* channel, int nrecv, int* peerRecv, int nsend, int* peerSend) {\n  TRACE(NCCL_INIT, \"nsend %d nrecv %d\", nsend, nrecv);\n  uint32_t nSkippedSend = 0, nSkippedRecv = 0; /* for tracing */\n  struct ncclConnect connect;\n  struct ncclConnector* conn;\n  for (int i=0; i<nrecv; i++) {\n    int peer = peerRecv[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].recv;\n    if (conn->connected) { ++nSkippedRecv; continue; }\n    NCCLCHECK(selectTransport<0>(comm->peerInfo+comm->rank, comm->peerInfo+peer, &connect, conn, channel->buffSize, channel->id));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n  }\n  for (int i=0; i<nsend; i++) {\n    int peer = peerSend[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].send;\n    if (conn->connected) { ++nSkippedSend; continue; }\n    NCCLCHECK(selectTransport<1>(comm->peerInfo+comm->rank, comm->peerInfo+peer, &connect, conn, channel->buffSize, channel->id));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n  }\n  for (int i=0; i<nsend; i++) {\n    int peer = peerSend[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].send;\n    if (conn->connected) {++nSkippedSend; continue; }\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n    NCCLCHECK(conn->transportComm->connect(&connect, conn));\n    conn->connected = 1;\n  }\n  for (int i=0; i<nrecv; i++) {\n    int peer = peerRecv[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].recv;\n    if (conn->connected) {++nSkippedRecv; continue; }\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n    NCCLCHECK(conn->transportComm->connect(&connect, conn));\n    conn->connected = 1;\n  }\n  TRACE(NCCL_INIT, \"nsend %d nrecv %d nSkippedSend %u nSkippedRecv %u - DONE\", nsend, nrecv, nSkippedSend, nSkippedRecv);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t initTransportsRank(struct ncclComm* comm, ncclUniqueId* commId) {\n  // We use 3 AllGathers\n  // 1. { peerInfo, comm }\n  // 2. ConnectTransport[nranks], ConnectValue[nranks]\n  // 3. { nThreads, nrings, compCap, prev[MAXCHANNELS], next[MAXCHANNELS] }\n\n  int rank = comm->rank;\n  int nranks = comm->nRanks;\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BEGIN\", rank, nranks);\n  NCCLCHECK(bootstrapInit(commId, rank, nranks, &comm->bootstrap));\n\n  // AllGather1 - begin\n  struct {\n    struct ncclPeerInfo peerInfo;\n    struct ncclComm* comm;\n  } *allGather1Data;\n\n  NCCLCHECK(ncclCalloc(&allGather1Data, nranks));\n  allGather1Data[rank].comm = comm;\n  NCCLCHECK(fillInfo(&allGather1Data[rank].peerInfo, rank));\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather1Data, sizeof(*allGather1Data)));\n\n  NCCLCHECK(ncclCalloc(&comm->peerInfo, nranks));\n  for (int i = 0; i < nranks; i++) {\n    memcpy(comm->peerInfo+i, &allGather1Data[i].peerInfo, sizeof(struct ncclPeerInfo));\n  }\n  // AllGather1 data is used again below\n  // AllGather1 - end\n\n  // AllGather2 - begin\n  size_t allGather2DataRowSize = sizeof(int)*nranks + sizeof(ncclTvalue_t)*nranks;\n  void *allGather2Data;\n  NCCLCHECK(ncclCalloc((char **)&allGather2Data, allGather2DataRowSize*nranks));\n  int *myTransportRow = (int *)((char *)allGather2Data + allGather2DataRowSize*rank);\n  ncclTvalue_t *myValueRow = (ncclTvalue_t *)(myTransportRow + nranks);\n\n  NCCLCHECK(fillConnect(comm->peerInfo, nranks, rank, myTransportRow, myValueRow));\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather2Data, allGather2DataRowSize));\n\n  int* connectTransport;\n  ncclTvalue_t* connectValue;\n  NCCLCHECK(ncclCalloc(&connectTransport, nranks*nranks));\n  NCCLCHECK(ncclCalloc(&connectValue, nranks*nranks));\n  for (int i = 0; i < nranks; i++) {\n    memcpy(connectTransport + i*nranks, (char *)allGather2Data + i*allGather2DataRowSize, sizeof(int)*nranks);\n    memcpy(connectValue + i*nranks, (char *)allGather2Data + i*allGather2DataRowSize + nranks*sizeof(int), sizeof(ncclTvalue_t)*nranks);\n  }\n  free(allGather2Data);\n  // AllGather2 - end\n\n  //if (rank == 0) dumpMatrix(connectTransport, nranks);\n  //if (rank == 0) dumpMatrixTvalue(connectValue, nranks);\n\n  // Get my rings\n  int nrings;\n  int* prev, *next, *treeIn, *treeOut;\n  NCCLCHECK(ncclCalloc(&prev, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&next, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&treeIn, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&treeOut, nranks*MAXCHANNELS));\n  comm->nThreads = getDefaultThreads();\n  NCCLCHECK(ncclGetRings(&nrings, &comm->nThreads, rank, nranks, connectTransport, connectValue, prev, next, treeIn, treeOut));\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BUILD %d RINGS\", rank, nranks, nrings);\n  assert(nrings <= MAXCHANNELS);\n  free(connectTransport);\n  free(connectValue);\n\n  // AllGather3 - begin\n  struct {\n    int nThreads;\n    int nrings;\n    int cudaCompCap;\n    int prev[MAXCHANNELS];\n    int next[MAXCHANNELS];\n  } *allGather3Data;\n\n  NCCLCHECK(ncclCalloc(&allGather3Data, nranks));\n  allGather3Data[rank].nThreads = comm->nThreads;\n  allGather3Data[rank].nrings = nrings;\n  allGather3Data[rank].cudaCompCap = ncclCudaCompCap();\n  for (int r=0; r<nrings; r++) {\n    allGather3Data[rank].prev[r] = *(prev+r*nranks+rank);\n    allGather3Data[rank].next[r] = *(next+r*nranks+rank);\n  }\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather3Data, sizeof(*allGather3Data)));\n\n  // Find max nThreads\n  for (int i=0; i<nranks; i++)\n    comm->nThreads = std::max(allGather3Data[i].nThreads, comm->nThreads);\n\n  // Determine the minimum CUDA Compute capability of all GPUs\n  int myCompCap = allGather3Data[rank].cudaCompCap;\n  int minCompCap = myCompCap;\n  for (int i = 0; i < nranks; i++)\n    minCompCap = std::min(allGather3Data[i].cudaCompCap, minCompCap);\n\n  // Determine thread threshold across all GPUs\n  int nnodes = 0;\n  for (int r=0; r<nranks; r++) nnodes += treeIn[r];\n  comm->threadThreshold = ncclThreadThreshold(minCompCap, nnodes);\n\n  // Find min nrings across ranks\n  for (int i=0; i<nranks; i++)\n    nrings = std::min(allGather3Data[i].nrings, nrings);\n  comm->nChannels = nrings;\n\n  // Unpack the per ring prev/next arrays\n  for (int i = 0; i < nranks; i++) {\n    for (int r = 0; r < nrings; r++) {\n      prev[r*nranks+i] = allGather3Data[i].prev[r];\n      next[r*nranks+i] = allGather3Data[i].next[r];\n    }\n  }\n  free(allGather3Data);\n  // AllGather3 - end\n\n  int *rings;\n  NCCLCHECK(ncclCalloc(&rings, nranks*MAXCHANNELS));\n  NCCLCHECK(buildRings(nrings, rings, rank, nranks, prev, next));\n  free(prev);\n  free(next);\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BUILT %d RINGS\", rank, nranks, nrings);\n\n  // Connect with prev/next for each ring\n  struct ncclConnect *connect;\n  NCCLCHECK(ncclCalloc(&connect, 2));\n  for (int r=0; r<nrings; r++) {\n    struct ncclChannel* channel = comm->channels+r;\n    NCCLCHECK(setupChannel(comm, r, rank, nranks, rings+r*nranks, treeIn+r*nranks));\n    NCCLCHECK(p2pSetup(comm, channel, 1, &channel->ring.prev, 1, &channel->ring.next));\n    NCCLCHECK(p2pSetup(comm, channel, NCCL_MAX_TREE_ARITY, channel->tree.down, 1, &channel->tree.up));\n    NCCLCHECK(p2pSetup(comm, channel, 1, &channel->tree.up, NCCL_MAX_TREE_ARITY, channel->tree.down));\n  }\n  if (comm->treeThreshold > 0) {\n    char line[1024];\n    line[0]='\\0';\n    for (int c=0; c<nrings; c++) {\n      struct ncclTree* tree = &comm->channels[c].tree;\n      snprintf(line+strlen(line), 1023-strlen(line), \" [%d] %d->%d->%d/%d/%d\",\n          c, tree->up, rank, tree->down[0], tree->down[1], tree->down[2]);\n    }\n    line[1023] = '\\0';\n    INFO(NCCL_INIT, \"Trees%s\", line);\n  }\n  if (rank == 0) {\n    char treeline[64];\n    snprintf(treeline, 64, \"enabled up to size %ld\", comm->treeThreshold);\n    INFO(NCCL_INIT,\"Using %d threads, Min Comp Cap %d, Trees %s\", comm->nThreads, minCompCap,\n       comm->treeThreshold == 0 ? \"disabled\" :\n       comm->treeThreshold == 0x7fffffffffffffff ? \"enabled for all sizes\" :\n       treeline);\n  }\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - CONNECTED %d RINGS AND TREES\", rank, nranks, nrings);\n  free(connect);\n  free(rings);\n  free(treeIn);\n  free(treeOut);\n\n  // Compute intra ranks (using AllGather1 data)\n  int intraRank0 = -1, intraRank = -1, intraRanks = 0;\n  for (int i = 0; i < nranks; i++) {\n    if ((allGather1Data[i].peerInfo.hostHash == allGather1Data[rank].peerInfo.hostHash) &&\n        (allGather1Data[i].peerInfo.pidHash == allGather1Data[rank].peerInfo.pidHash)) {\n      if (intraRanks == 0) intraRank0 = i;\n      if (i == rank) intraRank = intraRanks;\n      intraRanks++;\n    }\n  }\n  TRACE(NCCL_INIT,\"hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n        rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n  if (intraRank == -1 || intraRank0 == -1 || allGather1Data[intraRank0].comm == NULL) {\n    WARN(\"Failed to determine intra ranks hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n         rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n    return ncclInternalError;\n  }\n  NCCLCHECK(ncclCommSetIntra(comm, intraRank, intraRanks, allGather1Data[intraRank0].comm));\n\n  // Done with AllGather1 data\n  free(allGather1Data);\n\n  if (nnodes) NCCLCHECK(transportCreateProxy(comm));\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - DONE\", rank, nranks);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t getCpuGpuAffinity(int cudaDev, cpu_set_t* mask) {\n  CPU_ZERO_S(sizeof(cpu_set_t), mask);\n  char* cudaPath;\n  NCCLCHECK(getCudaPath(cudaDev, &cudaPath));\n  char path[PATH_MAX];\n  strncpy(path, cudaPath, PATH_MAX-1);\n  snprintf(path+strlen(path), PATH_MAX-1-strlen(path), \"/local_cpus\");\n  path[PATH_MAX-1] = '\\0';\n  int fd;\n  SYSCHECKVAL(open(path, O_RDONLY), \"open\", fd);\n  char affinityStr[sizeof(cpu_set_t)*2];\n  int r = read(fd, affinityStr, sizeof(cpu_set_t)*2);\n  if (r > 0)\n    NCCLCHECK(ncclStrToCpuset(affinityStr, mask));\n  close(fd);\n  free(cudaPath);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t setCpuAffinity(int cudaDev) {\n  // Work within the enveloppe we were provided\n  cpu_set_t mask;\n  SYSCHECK(sched_getaffinity(0, sizeof(cpu_set_t), &mask), \"sched_getaffinity\");\n\n  // Find the subpart that is local to our GPU\n  cpu_set_t gpuMask;\n  NCCLCHECK(getCpuGpuAffinity(cudaDev, &gpuMask));\n  cpu_set_t finalMask;\n  CPU_AND(&finalMask, &mask, &gpuMask);\n\n  // If those are not disjoint, try to stay local\n  if (CPU_COUNT(&finalMask)) {\n    char affinityStr[sizeof(cpu_set_t)*2];\n    NCCLCHECK(ncclCpusetToStr(&finalMask, affinityStr));\n    INFO(NCCL_INIT, \"Setting affinity for GPU %d to %s\", cudaDev, affinityStr);\n    SYSCHECK(sched_setaffinity(0, sizeof(cpu_set_t), &finalMask), \"sched_setaffinity\");\n  }\n  return ncclSuccess;\n}\n\nncclResult_t ncclCommInitRankSync(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank) {\n  cpu_set_t affinitySave;\n  sched_getaffinity(0, sizeof(cpu_set_t), &affinitySave);\n\n  NCCLCHECK(wrapNvmlSymbols());\n  NCCLCHECK(wrapNvmlInit());\n\n  // Make sure all host memory allocation are close to the GPU\n  int cudaDev;\n  CUDACHECK(cudaGetDevice(&cudaDev));\n  NCCLCHECK(setCpuAffinity(cudaDev));\n  ncclResult_t res;\n\n  NCCLCHECKGOTO(commAlloc(newcomm, nranks, myrank), res, cleanup);\n  NCCLCHECKGOTO(initTransportsRank(*newcomm, &commId), res, cleanup);\n  NCCLCHECKGOTO(devCommSetup(*newcomm), res, cleanup);\n\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  NCCLCHECKGOTO(wrapNvmlShutdown(), res, cleanup);\n\n  INFO(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d nvmlDev %d - Init COMPLETE\", *newcomm, myrank, nranks, (*newcomm)->cudaDev, (*newcomm)->nvmlDev);\n\n  return ncclSuccess;\ncleanup:\n  *newcomm = NULL;\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  return res;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitRank, ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank);\nncclResult_t ncclCommInitRank(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank) {\n  char* env = getenv(\"NCCL_COMM_ID\");\n  if (env && myrank == 0) {\n    NCCLCHECK(bootstrapCreateRoot(&commId, true));\n  }\n\n  NCCLCHECK(ncclInit());\n  if (myrank == 0) showVersion();\n\n  // Make sure the CUDA runtime is initialized.\n  CUDACHECK(cudaFree(NULL));\n\n  NCCLCHECK(PtrCheck(newcomm, \"CommInitRank\", \"newcomm\"));\n  if (nranks < 1 || myrank < 0 || myrank >= nranks) {\n    WARN(\"Invalid rank requested : %d/%d\", myrank, nranks);\n    return ncclInvalidArgument;\n  }\n\n  if (ncclAsyncMode()) {\n    int cudaDev;\n    CUDACHECK(cudaGetDevice(&cudaDev));\n    return ncclAsyncInit(ncclCommInitRankSync, cudaDev, newcomm, nranks, commId, myrank);\n  } else {\n    return ncclCommInitRankSync(newcomm, nranks, commId, myrank);\n  }\n}\n\nstatic ncclResult_t initTransportsAll(struct ncclComm** comms, const int* devs, int nranks) {\n  struct ncclPeerInfo* allInfo;\n  NCCLCHECK(ncclCalloc(&allInfo, nranks));\n  for (int rank=0; rank<nranks; rank++) {\n    CUDACHECK(cudaSetDevice(devs[rank]));\n    NCCLCHECK(fillInfo(allInfo+rank, rank));\n  }\n\n  int* connectTransport;\n  ncclTvalue_t* connectValue;\n  NCCLCHECK(ncclCalloc(&connectTransport, nranks*nranks));\n  NCCLCHECK(ncclCalloc(&connectValue, nranks*nranks));\n  for (int rank=0; rank<nranks; rank++)\n    NCCLCHECK(fillConnect(allInfo, nranks, rank, connectTransport+nranks*rank, connectValue+nranks*rank));\n\n  int* prev, *prevFinal, *next, *nextFinal, *treeIn, *treeOut;\n  NCCLCHECK(ncclCalloc(&prev, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&prevFinal, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&next, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&nextFinal, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&treeIn, nranks*MAXCHANNELS));\n  NCCLCHECK(ncclCalloc(&treeOut, nranks*MAXCHANNELS));\n  int nrings = MAXCHANNELS;\n  int nthreads=0;\n  int myCompCap = ncclCudaCompCap();\n  int minCompCap = myCompCap;\n  for (int rank=0; rank<nranks; rank++) {\n    CUDACHECK(cudaSetDevice(devs[rank]));\n    int nringsRank;\n    int nthreadsRank = getDefaultThreads();\n    myCompCap = ncclCudaCompCap();\n    NCCLCHECK(ncclGetRings(&nringsRank, &nthreadsRank, rank, nranks, connectTransport, connectValue, prev, next, treeIn, treeOut));\n    nrings = std::min(nrings, nringsRank);\n    nthreads = std::max(nthreads, nthreadsRank);\n    minCompCap = std::min(minCompCap, myCompCap);\n    for (int ring=0; ring<nrings; ring++) {\n      int index = ring*nranks+rank;\n      prevFinal[index] = prev[index];\n      nextFinal[index] = next[index];\n    }\n  }\n  free(connectTransport);\n  free(connectValue);\n  free(prev);\n  free(next);\n\n  INFO(NCCL_INIT,\"Using %d threads, Min Comp Cap %d, Trees disabled\", nthreads, minCompCap);\n\n  int* rings;\n  NCCLCHECK(ncclCalloc(&rings, nranks*MAXCHANNELS));\n  NCCLCHECK(buildRings(nrings, rings, 0, nranks, prevFinal, nextFinal));\n  free(prevFinal);\n  free(nextFinal);\n\n  // Determine thread threshold across all GPUs\n  int threadThreshold = ncclThreadThreshold(minCompCap, 0);\n\n  for (int rank=0; rank<nranks; rank++) {\n    comms[rank]->nChannels = nrings;\n    comms[rank]->nThreads = nthreads;\n    comms[rank]->threadThreshold = threadThreshold;\n  }\n\n  for (int r=0; r<nrings; r++) {\n    struct ncclConnect connect[2*nranks];\n    int* ringRanks = rings+r*nranks;\n    for (int rank=0; rank<nranks; rank++) {\n      CUDACHECK(cudaSetDevice(devs[rank]));\n      struct ncclChannel* channel = comms[rank]->channels+r;\n      struct ncclRing *ring = &channel->ring;\n      NCCLCHECK(setupChannel(comms[rank], r, rank, nranks, ringRanks, treeIn));\n      // Make sure we don't use trees, we cannot use them with initAll\n      comms[rank]->treeThreshold = 0;\n      int prev = channel->ring.prev = ring->userRanks[nranks-1];\n      int next = channel->ring.next = ring->userRanks[1];\n      struct ncclConnector* recv = &channel->peers[prev].recv;\n      struct ncclConnector* send = &channel->peers[next].send;\n      NCCLCHECK(selectTransport<0>(allInfo+rank, allInfo+prev, connect+rank*2+0, recv, channel->buffSize, channel->id));\n      NCCLCHECK(selectTransport<1>(allInfo+rank, allInfo+next, connect+rank*2+1, send, channel->buffSize, channel->id));\n    }\n    for (int rank=0; rank<nranks; rank++) {\n      CUDACHECK(cudaSetDevice(devs[rank]));\n      struct ncclChannel* channel = comms[rank]->channels+r;\n      struct ncclRing *ring = &channel->ring;\n      struct ncclConnector* recv = &channel->peers[ring->prev].recv;\n      struct ncclConnector* send = &channel->peers[ring->next].send;\n      NCCLCHECK(recv->transportComm->connect(connect+ring->prev*2+1, recv));\n      NCCLCHECK(send->transportComm->connect(connect+ring->next*2+0, send));\n    }\n  }\n  free(allInfo);\n  free(rings);\n  free(treeIn);\n  free(treeOut);\n  return ncclSuccess;\n}\n\n\nNCCL_API(ncclResult_t, ncclCommInitAll, ncclComm_t* comms, int ndev, const int* devlist);\nncclResult_t ncclCommInitAll(ncclComm_t* comms, int ndev, const int* devlist) {\n  NCCLCHECK(ncclInit());\n  NCCLCHECK(wrapNvmlSymbols());\n  NCCLCHECK(wrapNvmlInit());\n  showVersion();\n\n  INFO(NCCL_INIT,\"nranks %d\", ndev);\n\n  NCCLCHECK(PtrCheck(comms, \"CommInitAll\", \"comms\"));\n  if (ndev < 1) {\n    WARN(\"Invalid device count requested : %d\", ndev);\n    return ncclInvalidArgument;\n  }\n\n  ncclResult_t res;\n  int savedDevice;\n  int rank, cudaDev;\n  ncclComm_t comm = NULL;\n  int ncclDevList[ndev];\n  for (int i=0; i<ndev; i++) {\n    ncclDevList[i] = devlist ? devlist[i] : i;\n  }\n\n  cudaGetDevice(&savedDevice);\n\n  for(rank=0; rank<ndev; ++rank)\n    comms[rank] = NULL;\n\n  cpu_set_t affinitySave;\n  sched_getaffinity(0, sizeof(cpu_set_t), &affinitySave);\n\n  for (rank=0; rank<ndev; ++rank) {\n    cudaDev = ncclDevList[rank];\n    CUDACHECKGOTO(cudaSetDevice(cudaDev), res, cleanup);\n\n    NCCLCHECK(setCpuAffinity(cudaDev));\n\n    NCCLCHECKGOTO(commAlloc(&comm, ndev, rank), res, cleanup);\n    comms[rank] = comm;\n\n    NCCLCHECKGOTO(ncclCommSetIntra(comm, rank, ndev, comms[0]), res, cleanup);\n  }\n\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n\n  NCCLCHECKGOTO(initTransportsAll(comms, ncclDevList, ndev), res, cleanup);\n\n  for(rank=0; rank<ndev; ++rank) {\n    cudaDev = ncclDevList[rank];\n    CUDACHECKGOTO(cudaSetDevice(cudaDev), res, cleanup);\n    NCCLCHECKGOTO(devCommSetup(comms[rank]), res, cleanup);\n  }\n\n  res = ncclSuccess;\n  goto final;\n\ncleanup:\n  for(rank=0; rank<ndev; ++rank) {\n    if(comms[rank] != NULL) {\n      commFree(comms[rank]);\n    }\n  }\n\nfinal:\n  if(wrapNvmlShutdown() != ncclSuccess)\n    INFO(NCCL_INIT,\"NCCL did not shutdown nvml properly\");\n  cudaSetDevice(savedDevice);\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  return res;\n}\n\n\nstatic ncclResult_t commDestroy(ncclComm_t comm) {\n  int savedDevice;\n  CUDACHECK(cudaGetDevice(&savedDevice));\n  int commDevice = comm->cudaDev;\n  int rank = comm->rank;\n\n  if (savedDevice != commDevice) {\n    CUDACHECK(cudaSetDevice(commDevice));\n  }\n\n  TRACE(NCCL_INIT, \"Destroying comm %p rank %d abortFlag %d fatalError %d\", comm, rank, *comm->abortFlag, comm->fatalError);\n\n  CUDACHECK(cudaStreamSynchronize(comm->groupStream));\n  NCCLCHECK(transportDestroyProxy(comm));\n  NCCLCHECK(commFree(comm));\n\n  if (savedDevice != commDevice)\n    CUDACHECK(cudaSetDevice(savedDevice));\n\n  INFO(NCCL_INIT, \"Destroyed comm %p rank %d\", comm, rank);\n\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommDestroy, ncclComm_t comm);\nncclResult_t ncclCommDestroy(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  return commDestroy(comm);\n}\n\nNCCL_API(ncclResult_t, ncclCommAbort, ncclComm_t comm);\nncclResult_t ncclCommAbort(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  // Ask anything that might still be running on the device to quit\n  *comm->abortFlag = 1;\n\n  return commDestroy(comm);\n}\n\nNCCL_API(const char*, ncclGetErrorString, ncclResult_t code);\nconst char* ncclGetErrorString(ncclResult_t code) {\n  switch (code) {\n    case ncclSuccess                : return \"no error\";\n    case ncclUnhandledCudaError     : return \"unhandled cuda error\";\n    case ncclSystemError            : return \"unhandled system error\";\n    case ncclInternalError          : return \"internal error\";\n    case ncclInvalidArgument        : return \"invalid argument\";\n    case ncclInvalidUsage           : return \"invalid usage\";\n    default                         : return \"unknown result code\";\n  }\n}\n\nNCCL_API(ncclResult_t, ncclCommGetAsyncError, ncclComm_t comm, ncclResult_t *asyncError);\nncclResult_t ncclCommGetAsyncError(ncclComm_t comm, ncclResult_t *asyncError) {\n  NCCLCHECK(PtrCheck(comm, \"ncclGetAsyncError\", \"comm\"));\n  NCCLCHECK(PtrCheck(asyncError, \"ncclGetAsyncError\", \"asyncError\"));\n\n  // Check device reported error\n  static ncclDevError_t printedDevErr = ncclDevSuccess;\n  switch(*comm->fatalDevError) {\n    case ncclDevSuccess :\n      break;\n    case ncclDevAssertedMismatch :\n      if (printedDevErr != ncclDevAssertedMismatch) {\n        WARN(\"Mismatched collective detected, please check your collective calls at and around rank %d. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\", comm->rank);\n        printedDevErr = ncclDevAssertedMismatch;\n      }\n      if (comm->fatalError == ncclSuccess) {\n        comm->fatalError = ncclInvalidUsage;\n      }\n      break;\n    case ncclDevSuspectedMismatch :\n      if (printedDevErr != ncclDevSuspectedMismatch) {\n        WARN(\"Your program may be hanging, this may be caused by a collective mismatch around rank %d. Please check your collective calls at and around this rank. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\", comm->rank);\n        printedDevErr = ncclDevSuspectedMismatch;\n      }\n      break;\n    default:\n      WARN(\"Unknown device error %d\", *comm->fatalDevError);\n      return ncclInternalError;\n  }\n  *asyncError = comm->fatalError;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCount, const ncclComm_t comm, int* count);\nncclResult_t ncclCommCount(const ncclComm_t comm, int* count) {\n  NCCLCHECK(PtrCheck(comm, \"CommCount\", \"comm\"));\n  NCCLCHECK(PtrCheck(count, \"CommCount\", \"count\"));\n  *count = comm->nRanks;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCuDevice, const ncclComm_t comm, int* devid);\nncclResult_t ncclCommCuDevice(const ncclComm_t comm, int* devid) {\n  NCCLCHECK(PtrCheck(comm, \"CommCuDevice\", \"comm\"));\n  NCCLCHECK(PtrCheck(devid, \"CommCuDevice\", \"devid\"));\n  *devid = comm->cudaDev;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommUserRank, const ncclComm_t comm, int* rank);\nncclResult_t ncclCommUserRank(const ncclComm_t comm, int* rank) {\n  NCCLCHECK(PtrCheck(comm, \"CommUserRank\", \"comm\"));\n  NCCLCHECK(PtrCheck(rank, \"CommUserRank\", \"rank\"));\n  *rank = comm->rank;\n  return ncclSuccess;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.4.2-1-dyxfkphyrzsextqutdm6sstd4s2f3xsj/spack-src/src/misc/nvmlwrap.cu": "/*************************************************************************\n * Copyright (c) 2015-2018, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nvmlwrap.h\"\n\n#ifndef NVML_DIRECT\n#include <dlfcn.h>\n#include \"core.h\"\n\nstatic enum { nvmlUninitialized, nvmlInitializing, nvmlInitialized, nvmlError } nvmlState = nvmlUninitialized;\n\nstatic nvmlReturn_t (*nvmlInternalInit)(void);\nstatic nvmlReturn_t (*nvmlInternalShutdown)(void);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByPciBusId)(const char* pciBusId, nvmlDevice_t* device);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetIndex)(nvmlDevice_t device, unsigned* index);\nstatic const char* (*nvmlInternalErrorString)(nvmlReturn_t r);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkState)(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetPciInfo)(nvmlDevice_t device, nvmlPciInfo_t* pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkRemotePciInfo)(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkCapability)(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetMinorNumber)(nvmlDevice_t device, unsigned int* minorNumber);\n\n\nncclResult_t wrapNvmlSymbols(void) {\n  if (nvmlState == nvmlInitialized)\n    return ncclSuccess;\n  if (nvmlState == nvmlError)\n    return ncclSystemError;\n\n  if (__sync_bool_compare_and_swap(&nvmlState, nvmlUninitialized, nvmlInitializing) == false) {\n    // Another thread raced in front of us. Wait for it to be done.\n    while (nvmlState == nvmlInitializing) pthread_yield();\n    return (nvmlState == nvmlInitialized) ? ncclSuccess : ncclSystemError;\n  }\n\n  static void* nvmlhandle = NULL;\n  void* tmp;\n  void** cast;\n\n  nvmlhandle=dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n  if (!nvmlhandle) {\n    WARN(\"Failed to open libnvidia-ml.so.1\");\n    goto teardown;\n  }\n\n#define LOAD_SYM(handle, symbol, funcptr) do {         \\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      WARN(\"dlsym failed on %s - %s\", symbol, dlerror());\\\n      goto teardown;                                     \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n#define LOAD_SYM_OPTIONAL(handle, symbol, funcptr) do {\\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      INFO(NCCL_INIT,\"dlsym failed on %s, ignoring\", symbol); \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n  LOAD_SYM(nvmlhandle, \"nvmlInit\", nvmlInternalInit);\n  LOAD_SYM(nvmlhandle, \"nvmlShutdown\", nvmlInternalShutdown);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByPciBusId\", nvmlInternalDeviceGetHandleByPciBusId);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetIndex\", nvmlInternalDeviceGetIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlErrorString\", nvmlInternalErrorString);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetPciInfo\", nvmlInternalDeviceGetPciInfo);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetMinorNumber\", nvmlInternalDeviceGetMinorNumber);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkState\", nvmlInternalDeviceGetNvLinkState);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkRemotePciInfo\", nvmlInternalDeviceGetNvLinkRemotePciInfo);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkCapability\", nvmlInternalDeviceGetNvLinkCapability);\n\n  nvmlState = nvmlInitialized;\n  return ncclSuccess;\n\nteardown:\n  nvmlInternalInit = NULL;\n  nvmlInternalShutdown = NULL;\n  nvmlInternalDeviceGetHandleByPciBusId = NULL;\n  nvmlInternalDeviceGetIndex = NULL;\n  nvmlInternalDeviceGetPciInfo = NULL;\n  nvmlInternalDeviceGetMinorNumber = NULL;\n  nvmlInternalDeviceGetNvLinkState = NULL;\n  nvmlInternalDeviceGetNvLinkRemotePciInfo = NULL;\n  nvmlInternalDeviceGetNvLinkCapability = NULL;\n\n  if (nvmlhandle != NULL) dlclose(nvmlhandle);\n  nvmlState = nvmlError;\n  return ncclSystemError;\n}\n\n\nncclResult_t wrapNvmlInit(void) {\n  if (nvmlInternalInit == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalInit();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlInit() failed: %s\",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlShutdown(void) {\n  if (nvmlInternalShutdown == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalShutdown();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlShutdown() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByPciBusId(const char* pciBusId, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByPciBusId == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetHandleByPciBusId(pciBusId, device);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByPciBusId() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetIndex(nvmlDevice_t device, unsigned* index) {\n  if (nvmlInternalDeviceGetIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetIndex(device, index);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetPciInfo(nvmlDevice_t device, nvmlPciInfo_t* pci) {\n  if (nvmlInternalDeviceGetPciInfo == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetPciInfo(device, pci);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetPciInfo() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetMinorNumber(nvmlDevice_t device, unsigned int* minorNumber) {\n  if (nvmlInternalDeviceGetMinorNumber == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetMinorNumber(device, minorNumber);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetMinorNumber() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkState(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive) {\n  if (nvmlInternalDeviceGetNvLinkState == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetNvLinkState(device, link, isActive);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkState() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkRemotePciInfo(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci) {\n  if (nvmlInternalDeviceGetNvLinkRemotePciInfo == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetNvLinkRemotePciInfo(device, link, pci);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkRemotePciInfo() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkCapability(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalDeviceGetNvLinkCapability(device, link, capability, capResult);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkCapability() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n#endif\n"
    },
    "skipped": [],
    "total_files": 77
}