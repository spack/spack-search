{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.7.8-1-2fie5mmnboduvr5o52gbeaawkfbp3dko/spack-src/src/init.cc": "/*************************************************************************\n * Copyright (c) 2015-2020, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nccl.h\"\n#include \"channel.h\"\n#include \"nvmlwrap.h\"\n#include \"bootstrap.h\"\n#include \"transport.h\"\n#include \"group.h\"\n#include \"net.h\"\n#include \"coll_net.h\"\n#include \"enqueue.h\"\n#include \"graph.h\"\n#include \"argcheck.h\"\n#include <fcntl.h>\n#include <string.h>\n#include <errno.h>\n#include <assert.h>\n#include <dlfcn.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n\n#define STR2(v) #v\n#define STR(v) STR2(v)\n\n#ifdef ENABLE_TRACE\nstd::chrono::high_resolution_clock::time_point ncclEpoch;\n#endif\n\n#if CUDART_VERSION >= 9020\n#define NCCL_GROUP_CUDA_STREAM 0 // CGMD: CUDA 9.2,10.X Don't need to use an internal CUDA stream\n#else\n#define NCCL_GROUP_CUDA_STREAM 1 // CGMD: CUDA 9.0,9.1 Need to use an internal CUDA stream\n#endif\n\nconst char* ncclFuncStr[NCCL_NUM_FUNCTIONS] = { \"Broadcast\", \"Reduce\", \"AllGather\", \"ReduceScatter\", \"AllReduce\" };\nconst char* ncclAlgoStr[NCCL_NUM_ALGORITHMS] = { \"Tree\", \"Ring\", \"CollNet\" };\nconst char* ncclProtoStr[NCCL_NUM_PROTOCOLS] = { \"LL\", \"LL128\", \"Simple\" };\n\nNCCL_PARAM(GroupCudaStream, \"GROUP_CUDA_STREAM\", NCCL_GROUP_CUDA_STREAM);\n\nNCCL_PARAM(CheckPointers, \"CHECK_POINTERS\", 0);\n\nncclNet_t* ncclNet = NULL;\nncclCollNet_t* ncclCollNet = NULL;\n\n// Returns ncclInternalError if anything fails, causing that network to be ignored.\nncclResult_t initNet(ncclNet_t* net) {\n  int ndev;\n  if (net->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (net->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initCollNet(ncclCollNet_t* collnet) {\n  int ndev;\n  if (collnet->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (collnet->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initNetPlugin(ncclNet_t** net, ncclCollNet_t** collnet) {\n  void* netPluginLib = dlopen(\"libnccl-net.so\", RTLD_NOW | RTLD_LOCAL);\n  if (netPluginLib == NULL) {\n    // dlopen does not guarantee to set errno, but dlerror only gives us a\n    // string, so checking errno doesn't hurt to try to provide a better\n    // error message\n    if (errno == ENOENT) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\");\n    } else {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : Plugin load returned %d : %s.\", errno, dlerror());\n    }\n    return ncclSuccess;\n  }\n  ncclNet_t* extNet = (ncclNet_t*) dlsym(netPluginLib, STR(NCCL_PLUGIN_SYMBOL));\n  if (extNet == NULL) {\n    INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_PLUGIN_SYMBOL) \" symbol.\");\n  } else if (initNet(extNet) == ncclSuccess) {\n    *net = extNet;\n    // Check for CollNet\n    ncclCollNet_t* extCollNet = (ncclCollNet_t*) dlsym(netPluginLib, STR(NCCL_COLLNET_PLUGIN_SYMBOL));\n    if (extCollNet == NULL) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_COLLNET_PLUGIN_SYMBOL) \" symbol.\");\n    } else if (initCollNet(extCollNet) == ncclSuccess) {\n      *collnet = extCollNet;\n    }\n    return ncclSuccess;\n  }\n  if (netPluginLib != NULL) dlclose(netPluginLib);\n  return ncclSuccess;\n}\n\nncclResult_t initNet() {\n  // Always initialize bootstrap network\n  NCCLCHECK(bootstrapNetInit());\n\n  NCCLCHECK(initNetPlugin(&ncclNet, &ncclCollNet));\n  if (ncclNet != NULL) return ncclSuccess;\n  if (initNet(&ncclNetIb) == ncclSuccess) {\n    ncclNet = &ncclNetIb;\n  } else {\n    NCCLCHECK(initNet(&ncclNetSocket));\n    ncclNet = &ncclNetSocket;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CollNetEnable, \"COLLNET_ENABLE\", 0);\n\npthread_mutex_t initLock = PTHREAD_MUTEX_INITIALIZER;\nstatic bool initialized = false;\nstatic ncclResult_t ncclInit() {\n  if (initialized) return ncclSuccess;\n  pthread_mutex_lock(&initLock);\n  if (!initialized) {\n    initEnv();\n    NCCLCHECK(initNet());\n    INFO(NCCL_INIT, \"Using network %s\", ncclNetName());\n    initialized = true;\n  }\n  pthread_mutex_unlock(&initLock);\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetVersion, int* version);\nncclResult_t ncclGetVersion(int* version) {\n  if (version == NULL) return ncclInvalidArgument;\n  *version = NCCL_VERSION_CODE;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetUniqueId, ncclUniqueId* out);\nncclResult_t ncclGetUniqueId(ncclUniqueId* out) {\n  NCCLCHECK(ncclInit());\n  NCCLCHECK(PtrCheck(out, \"GetUniqueId\", \"out\"));\n  return bootstrapGetUniqueId(out);\n}\n\n// Prevent compiler from optimizing out these operations\n#ifdef __clang__\n#define NCCL_NO_OPTIMIZE __attribute__((optnone))\n#else\n#define NCCL_NO_OPTIMIZE __attribute__((optimize(\"O0\")))\n#endif\n\nvoid NCCL_NO_OPTIMIZE commPoison(ncclComm_t comm) {\n  comm->rank = comm->cudaDev = comm->busId = comm->nRanks = -1;\n}\n\n#undef NCCL_NO_OPTIMIZE\n\nstatic ncclResult_t commFree(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n  free(comm->p2plist.peerlist);\n  free(comm->p2plist.connect.recv);\n  free(comm->p2plist.connect.send);\n\n  free(comm->peerInfo);\n  ncclTopoFree(comm->topo);\n\n  if (comm->bootstrap)\n    NCCLCHECK(bootstrapClose(comm->bootstrap));\n\n  CUDACHECK(cudaFree(comm->hostDevComm.channels));\n  CUDACHECK(cudaFree(comm->devComm));\n\n  for (int channel=0; channel<MAXCHANNELS; channel++)\n    NCCLCHECK(freeChannel(comm->channels+channel, comm->nRanks));\n\n  if (comm->doneEvent != NULL)\n    CUDACHECK(cudaEventDestroy(comm->doneEvent));\n\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamDestroy(comm->groupStream));\n  }\n\n  // Last rank frees shared resources between threads\n  int isLast;\n  NCCLCHECK(ncclCpuBarrierIn(comm, &isLast));\n  if (isLast) {\n    free(comm->intraBarrier);\n    free(comm->intraParams);\n    free(comm->intraCudaDevs);\n    free(comm->intraCGMode);\n    free(comm->intraCC);\n  }\n  CUDACHECK(cudaFreeHost((void *)comm->abortFlag));\n\n  // Poison comm to try and catch a double free\n  commPoison(comm);\n\n  free(comm);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commAlloc(ncclComm_t* comret, int ndev, int rank) {\n  if (ndev < 1) {\n    WARN(\"invalid device count (%d) requested\", ndev);\n    return ncclInvalidArgument;\n  }\n  if (rank >= ndev || rank < 0) {\n    WARN(\"rank %d exceeds ndev=%d\", rank, ndev);\n    return ncclInvalidArgument;\n  }\n\n  // Try to create a CUDA object right away. If there is something wrong with\n  // the device we're on (failure cause #1) , better know it early.\n  cudaEvent_t doneEvent;\n  CUDACHECK(cudaEventCreateWithFlags(&doneEvent, cudaEventDisableTiming));\n\n  struct ncclComm* comm;\n  NCCLCHECK(ncclCalloc(&comm, 1));\n\n  comm->rank = comm->hostDevComm.rank =rank;\n  comm->nRanks = comm->hostDevComm.nRanks = ndev;\n  cudaGetDevice(&comm->cudaDev);\n  NCCLCHECK(getBusId(comm->cudaDev, &comm->busId));\n  TRACE(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x\", comm, rank, ndev, comm->cudaDev, comm->busId);\n\n  comm->doneEvent = doneEvent;\n  comm->checkPointers = ncclParamCheckPointers() == 1 ? true : false;\n#if CUDART_VERSION >= 9020\n  comm->groupCudaStream = ncclParamGroupCudaStream();\n#else\n  // Don't allow the user to overload the default setting in older CUDA builds\n  comm->groupCudaStream = NCCL_GROUP_CUDA_STREAM;\n#endif\n  comm->fatalError = ncclSuccess;\n\n  NCCLCHECK(ncclCudaHostCalloc((uint32_t**)&comm->abortFlag, 1));\n  comm->hostDevComm.abortFlag = comm->abortFlag;\n  *comm->abortFlag = 0;\n\n  comm->argsptr = &comm->args;\n  comm->collNetSupport = 0;\n  comm->p2plist.count=0;\n  NCCLCHECK(ncclCalloc(&comm->p2plist.peerlist, comm->nRanks));\n  for (int r=0; r<comm->nRanks; r++) comm->p2plist.peerlist[r].sendbytes = comm->p2plist.peerlist[r].recvbytes = -1;\n  NCCLCHECK(ncclCalloc(&comm->p2plist.connect.recv, MAXCHANNELS*comm->nRanks));\n  NCCLCHECK(ncclCalloc(&comm->p2plist.connect.send, MAXCHANNELS*comm->nRanks));\n\n  // Mark channels as non initialized.\n  for (int c=0; c<MAXCHANNELS; c++) comm->channels[c].id = -1;\n\n  *comret = comm;\n  return ncclSuccess;\n}\n\nstatic ncclResult_t devCommSetup(ncclComm_t comm) {\n  // Duplicate the channels on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->hostDevComm.channels, comm->p2pnChannels));\n  NCCLCHECK(ncclCudaMemcpy(comm->hostDevComm.channels, comm->channels, comm->p2pnChannels));\n\n  // Copy userRanks and peers\n  for (int r=0; r<comm->p2pnChannels; r++) {\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].ring.devUserRanks, comm->channels[r].ring.userRanks, comm->nRanks));\n  }\n\n  // Duplicate the dev comm on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->devComm, 1));\n  NCCLCHECK(ncclCudaMemcpy(comm->devComm, &comm->hostDevComm, 1));\n  return ncclSuccess;\n}\n\n// Pre-process the string so that running \"strings\" on the lib can quickly reveal the version.\n#define VERSION_STRING \"NCCL version \" STR(NCCL_MAJOR) \".\" STR(NCCL_MINOR) \".\" STR(NCCL_PATCH) NCCL_SUFFIX \"+cuda\" STR(CUDA_MAJOR) \".\" STR(CUDA_MINOR)\nstatic void showVersion() {\n  static int shown = 0;\n  if (shown == 0 && ncclDebugLevel >= NCCL_LOG_VERSION) {\n    printf(\"%s\\n\", VERSION_STRING);\n    fflush(stdout);\n    if (ncclDebugFile != stdout)\n      INFO(NCCL_ALL,\"%s\", VERSION_STRING); // Also log NCCL version in one of the files\n    shown = 1;\n  }\n}\n\nstatic ncclResult_t fillInfo(struct ncclComm* comm, struct ncclPeerInfo* info, uint64_t commHash) {\n  info->rank = comm->rank;\n  CUDACHECK(cudaGetDevice(&info->cudaDev));\n  info->hostHash=getHostHash()+commHash;\n  info->pidHash=getPidHash()+commHash;\n\n  // Get the device MAJOR:MINOR of /dev/shm so we can use that\n  // information to decide whether we can use SHM for inter-process\n  // communication in a container environment\n  struct stat statbuf;\n  SYSCHECK(stat(\"/dev/shm\", &statbuf), \"stat\");\n  info->shmDev = statbuf.st_dev;\n\n  info->busId = comm->busId;\n\n  NCCLCHECK(ncclGpuGdrSupport(&info->gdrSupport));\n  return ncclSuccess;\n}\n\nstatic ncclResult_t setupChannel(struct ncclComm* comm, int channelId, int rank, int nranks, int* ringRanks) {\n  TRACE(NCCL_INIT, \"rank %d nranks %d\", rank, nranks);\n  NCCLCHECK(initChannel(comm, channelId));\n\n  struct ncclRing* ring = &comm->channels[channelId].ring;\n  // Reorganize ranks to start with rank.\n  int shift;\n  for (shift = 0; shift<nranks; shift++) {\n    if (ringRanks[shift] == rank) {\n      break;\n    }\n  }\n  for (int i=0; i<nranks; i++) {\n    ring->userRanks[i] = ringRanks[(i+shift)%nranks];\n  }\n  return ncclSuccess;\n}\n\nvoid* waitForNonNullPtr(void* p) {\n  volatile void** ptr = (volatile void**) p;\n  while (*ptr == NULL) sched_yield();\n  return (void*)*ptr;\n}\n\nncclResult_t initParams(struct ncclComm* comm) {\n  struct cudaLaunchParams* params = comm->myParams = comm->intraParams+comm->intraRank;\n  params->args = &comm->argsptr;\n  params->stream = NULL;\n  params->sharedMem = 0;\n  params->blockDim.x = 0; params->blockDim.y = params->blockDim.z = 1;\n  params->gridDim.x = 0; params->gridDim.y = params->gridDim.z = 1;\n  return ncclSuccess;\n}\n\n// Allocate/Set Intra Process Structures and set CG options\nncclResult_t ncclCommSetIntra(struct ncclComm* comm, int rank, int ranks, struct ncclComm* comm0) {\n  comm->intraRank = rank;\n  comm->intraRanks = ranks;\n  comm->intraPhase = 0;\n\n  // Alloc shared structures\n  if (rank == 0) {\n    assert(comm == comm0);\n    int* bar;\n    NCCLCHECK(ncclCalloc(&bar, 2));\n    bar[0] = bar[1] = 0;\n    comm->intraBarrier = bar;\n    NCCLCHECK(ncclCalloc(&comm->intraParams, comm->intraRanks));\n    NCCLCHECK(ncclCalloc(&comm->intraCudaDevs, comm->intraRanks));\n    int* CGMode;\n    NCCLCHECK(ncclCalloc(&CGMode, 1));\n    *CGMode = 0x11;\n    comm->intraCGMode = CGMode;\n    int* CC;\n    NCCLCHECK(ncclCalloc(&CC, 1));\n    *CC = ncclCudaCompCap();\n    comm->intraCC = CC;\n  } else {\n    comm->intraBarrier = (int*)waitForNonNullPtr(&comm0->intraBarrier);\n    comm->intraParams = (struct cudaLaunchParams*)waitForNonNullPtr(&comm0->intraParams);\n    comm->intraCudaDevs = (int*)waitForNonNullPtr(&comm0->intraCudaDevs);\n    comm->intraCGMode = (int*)waitForNonNullPtr(&comm0->intraCGMode);\n    comm->intraCC = (int*)waitForNonNullPtr(&comm0->intraCC);\n  }\n  comm->intraCudaDevs[comm->intraRank] = comm->cudaDev;\n  NCCLCHECK(initParams(comm));\n\n  int cgMdLaunch = 0;\n\n  // Set CG Mode\n  comm->launchMode = ncclComm::GROUP;\n  char* str = getenv(\"NCCL_LAUNCH_MODE\");\n  if (str) INFO(NCCL_ENV, \"NCCL_LAUNCH_MODE set by environment to %s\", str);\n  if (comm->intraRanks == 1 || (str && strcmp(str, \"PARALLEL\") == 0)) {\n    comm->launchMode = ncclComm::PARALLEL;\n  }\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamCreateWithFlags(&comm->groupStream, cudaStreamNonBlocking));\n#if CUDART_VERSION >= 9000\n    if (*comm->intraCC && (ncclCudaCompCap() == *comm->intraCC)) {\n      // Check whether the GPU supports Cooperative Group Multi Device Launch\n      (void) cudaDeviceGetAttribute(&cgMdLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, comm->cudaDev);\n    }\n#endif\n  }\n\n  // Disable cgMdLaunch if any rank does not support it\n  if (cgMdLaunch == 0) {\n    *comm->intraCGMode = 0x10;\n  }\n  return ncclSuccess;\n}\n\n#define DEFAULT_LL_BUFFSIZE (NCCL_LL_LINES_PER_THREAD*NCCL_LL_MAX_NTHREADS*NCCL_STEPS*sizeof(union ncclLLFifoLine))\n#define DEFAULT_LL128_BUFFSIZE (NCCL_LL128_ELEMS_PER_THREAD*NCCL_LL128_MAX_NTHREADS*NCCL_STEPS*sizeof(uint64_t))\n#define DEFAULT_BUFFSIZE (1LL << 22) /* 4MiB */\n#define DEFAULT_BUFFSIZE_ARM (1LL << 20) /* 1MiB */\nNCCL_PARAM(BuffSize, \"BUFFSIZE\", -2);\nNCCL_PARAM(LlBuffSize, \"LL_BUFFSIZE\", -2);\nNCCL_PARAM(Ll128BuffSize, \"LL128_BUFFSIZE\", -2);\n\nstatic ncclResult_t computeBuffSizes(struct ncclComm* comm) {\n  int cpuArch, cpuVendor, cpuModel;\n  NCCLCHECK(ncclTopoCpuType(comm->topo, &cpuArch, &cpuVendor, &cpuModel));\n\n  int64_t envs[NCCL_NUM_PROTOCOLS] = { ncclParamLlBuffSize(), ncclParamLl128BuffSize(), ncclParamBuffSize() };\n  int defaults[NCCL_NUM_PROTOCOLS] = { DEFAULT_LL_BUFFSIZE, DEFAULT_LL128_BUFFSIZE, DEFAULT_BUFFSIZE };\n\n  if (cpuArch == NCCL_TOPO_CPU_ARCH_ARM) defaults[NCCL_PROTO_SIMPLE] = DEFAULT_BUFFSIZE_ARM;\n\n  for (int p=0; p<NCCL_NUM_PROTOCOLS; p++) {\n    comm->buffSizes[p] = comm->hostDevComm.buffSizes[p] = envs[p] != -2 ? envs[p] : defaults[p];\n  }\n  return ncclSuccess;\n}\n\nextern struct ncclTransport collNetTransport;\n\n// All ranks must participate in collNetSetup call\n// type: 0 for send, 1 for recv\n// return: 0 - unsupported, 1 - supported\n// We do not NCCLCHECK this call because we would fall back to P2P network in case CollNet setup fails\nstatic int collNetSetup(struct ncclComm* comm, struct ncclTopoGraph* collNetGraph, struct ncclChannel* channel, int rank, int nranks,  int masterRank, int masterPeer, int nMasters, int type) {\n  int rankInCollNet = -1;\n  int supported = 0;\n  int isMaster = (rank == masterRank) ? 1 : 0;\n  struct {\n    int collNetRank;\n    ncclConnect connect;\n  } sendrecvExchange;\n\n  // check if we can connect to collnet, whose root is the nranks-th rank\n  struct ncclPeerInfo *myInfo = comm->peerInfo+rank, *peerInfo = comm->peerInfo+nranks;\n  peerInfo->rank = nranks;\n  int ret = 1;\n  if (isMaster) {\n    NCCLCHECK(collNetTransport.canConnect(&ret, comm->topo, collNetGraph, myInfo, peerInfo));\n  }\n\n  // send master receives connect info from peer recv master\n  if (isMaster && type == 0) {\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, masterPeer, &sendrecvExchange, sizeof(sendrecvExchange)));\n    rankInCollNet = sendrecvExchange.collNetRank;\n    INFO(NCCL_INIT, \"CollNet [send] : rank %d collNetRank %d collNetNranks %d received connect from rank %d\", rank, rankInCollNet, nMasters, masterPeer);\n  }\n\n  // select\n  struct ncclPeer* root = channel->peers+nranks;\n  struct ncclConnector* conn = (type == 1) ? &root->recv : &root->send;\n  struct ncclTransportComm* transportComm = (type == 1) ? &(collNetTransport.recv) : &(collNetTransport.send);\n  conn->transportComm = transportComm;\n  // setup\n  struct ncclConnect myConnect;\n  if (isMaster && ret > 0) {\n    NCCLCHECK(transportComm->setup(comm->topo, collNetGraph, myInfo, peerInfo, &myConnect, conn, channel->id));\n  }\n  // prepare connect handles\n  ncclResult_t res;\n  struct {\n    int isMaster;\n    ncclConnect connect;\n  } *allConnects = NULL;\n  ncclConnect *masterConnects = NULL;\n  NCCLCHECK(ncclCalloc(&masterConnects, nMasters));\n  if (type == 1) {  // recv side: AllGather\n    // all ranks must participate\n    NCCLCHECK(ncclCalloc(&allConnects, nranks));\n    allConnects[rank].isMaster = isMaster;\n    memcpy(&(allConnects[rank].connect), &myConnect, sizeof(struct ncclConnect));\n    NCCLCHECKGOTO(bootstrapAllGather(comm->bootstrap, allConnects, sizeof(*allConnects)), res, cleanup);\n    // consolidate\n    int c = 0;\n    for (int r = 0; r < nranks; r++) {\n      if (allConnects[r].isMaster) {\n        memcpy(masterConnects+c, &(allConnects[r].connect), sizeof(struct ncclConnect));\n        if (r == rank) rankInCollNet = c;\n        c++;\n      }\n    }\n  } else { // send side : copy in connect info received from peer recv master\n    if (isMaster) memcpy(masterConnects+rankInCollNet, &(sendrecvExchange.connect), sizeof(struct ncclConnect));\n  }\n  // connect\n  if (isMaster && ret > 0) {\n    NCCLCHECKGOTO(transportComm->connect(masterConnects, nMasters, rankInCollNet, conn), res, cleanup);\n    struct ncclPeer* devRoot = channel->devPeers+nranks;\n    struct ncclConnector* devConn = (type == 1) ? &devRoot->recv : &devRoot->send;\n    CUDACHECKGOTO(cudaMemcpy(devConn, conn, sizeof(struct ncclConnector), cudaMemcpyHostToDevice), res, cleanup);\n  }\n  // recv side sends connect info to send side\n  if (isMaster && type == 1) {\n    sendrecvExchange.collNetRank = rankInCollNet;\n    memcpy(&sendrecvExchange.connect, masterConnects+rankInCollNet, sizeof(struct ncclConnect));\n    NCCLCHECKGOTO(bootstrapSend(comm->bootstrap, masterPeer, &sendrecvExchange, sizeof(sendrecvExchange)), res, cleanup);\n    INFO(NCCL_INIT, \"CollNet [recv] : rank %d collNetRank %d collNetNranks %d sent connect to rank %d\", rank, rankInCollNet, nMasters, masterPeer);\n  }\n  if (ret > 0) {\n    supported = 1;\n  }\ncleanup:\n  if (allConnects != NULL) free(allConnects);\n  if (masterConnects != NULL) free(masterConnects);\n  return supported;\n}\n\nstatic ncclResult_t checkCollNetSetup(struct ncclComm* comm, int rank, int collNetSetupFail) {\n  int nranks = comm->nRanks;\n  // AllGather collNet setup results\n  int* allGatherFailures;\n  NCCLCHECK(ncclCalloc(&allGatherFailures, nranks));\n  allGatherFailures[rank] = collNetSetupFail;\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGatherFailures, sizeof(int)));\n  for (int i=0; i<nranks; i++) {\n    if (allGatherFailures[i] != 0) {\n      collNetSetupFail = 1;\n      break;\n    }\n  }\n  free(allGatherFailures);\n  if (collNetSetupFail) {\n    if (rank == 0) WARN(\"Cannot initialize CollNet, using %s instead\", ncclNetName());\n    // Free collNet resources\n    for (int r=0; r<comm->nChannels; r++) {\n      struct ncclChannel* channel = comm->channels+r;\n      struct ncclPeer* peer = channel->peers+nranks;\n      if (peer->send.transportResources && peer->send.transportComm) NCCLCHECK(peer->send.transportComm->free(peer->send.transportResources));\n      if (peer->recv.transportResources && peer->recv.transportComm) NCCLCHECK(peer->recv.transportComm->free(peer->recv.transportResources));\n      peer->send.transportResources = NULL; // avoid double free\n      peer->recv.transportResources = NULL; // avoid double free\n    }\n    // Set support to 0\n    comm->collNetSupport = 0;\n  } else {\n    comm->collNetSupport = 1;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CrossNic, \"CROSS_NIC\", 2);\nNCCL_PARAM(GraphDumpFileRank, \"GRAPH_DUMP_FILE_RANK\", 0);\n\nstatic ncclResult_t initTransportsRank(struct ncclComm* comm, ncclUniqueId* commId) {\n  // We use 3 AllGathers\n  // 1. { peerInfo, comm }\n  // 2. ConnectTransport[nranks], ConnectValue[nranks]\n  // 3. { nThreads, nrings, compCap, prev[MAXCHANNELS], next[MAXCHANNELS] }\n\n  int rank = comm->rank;\n  int nranks = comm->nRanks;\n  uint64_t commHash = getHash(commId->internal, NCCL_UNIQUE_ID_BYTES);\n  TRACE(NCCL_INIT, \"comm %p, commHash %lx, rank %d nranks %d - BEGIN\", comm, commHash, rank, nranks);\n  NCCLCHECK(bootstrapInit(commId, rank, nranks, &comm->bootstrap));\n\n  // AllGather1 - begin\n  struct {\n    struct ncclPeerInfo peerInfo;\n    struct ncclComm* comm;\n  } *allGather1Data;\n\n  NCCLCHECK(ncclCalloc(&allGather1Data, nranks));\n  allGather1Data[rank].comm = comm;\n  struct ncclPeerInfo* myInfo = &allGather1Data[rank].peerInfo;\n  NCCLCHECK(fillInfo(comm, myInfo, commHash));\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather1Data, sizeof(*allGather1Data)));\n\n  NCCLCHECK(ncclCalloc(&comm->peerInfo, nranks+1)); // Extra rank to represent CollNet root\n  for (int i = 0; i < nranks; i++) {\n    memcpy(comm->peerInfo+i, &allGather1Data[i].peerInfo, sizeof(struct ncclPeerInfo));\n    if ((i != rank) && (comm->peerInfo[i].hostHash == myInfo->hostHash) && (comm->peerInfo[i].busId == myInfo->busId)) {\n      WARN(\"Duplicate GPU detected : rank %d and rank %d both on CUDA device %x\", rank, i, myInfo->busId);\n      return ncclInvalidUsage;\n    }\n  }\n  // AllGather1 data is used again below\n  // AllGather1 - end\n\n  // Topo detection / System graph creation\n  NCCLCHECK(ncclTopoGetSystem(comm, &comm->topo));\n  // Compute paths between GPUs and NICs\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Remove inaccessible GPUs and unused NICs\n  NCCLCHECK(ncclTopoTrimSystem(comm->topo, comm));\n  // Recompute paths after trimming\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Init search\n  NCCLCHECK(ncclTopoSearchInit(comm->topo));\n  // Print final topology\n  NCCLCHECK(ncclTopoPrint(comm->topo));\n\n  // Get rings and trees\n  struct ncclTopoGraph ringGraph;\n  ringGraph.id = 0;\n  ringGraph.pattern = NCCL_TOPO_PATTERN_RING;\n  ringGraph.crossNic = ncclParamCrossNic();\n  ringGraph.collNet = 0;\n  ringGraph.minChannels = 1;\n  ringGraph.maxChannels = MAXCHANNELS/2;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &ringGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &ringGraph));\n\n  struct ncclTopoGraph treeGraph;\n  treeGraph.id = 1;\n  treeGraph.pattern = NCCL_TOPO_PATTERN_SPLIT_TREE;\n  treeGraph.crossNic = ncclParamCrossNic();\n  treeGraph.collNet = 0;\n  treeGraph.minChannels = 1;\n  treeGraph.maxChannels = ringGraph.nChannels;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &treeGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &treeGraph));\n\n  struct ncclTopoGraph collNetGraph;\n  collNetGraph.id = 2;\n  collNetGraph.pattern = NCCL_TOPO_PATTERN_TREE;\n  collNetGraph.collNet = 1;\n  collNetGraph.crossNic = ncclParamCrossNic();\n  collNetGraph.minChannels = collNetGraph.maxChannels = ringGraph.nChannels;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &collNetGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &collNetGraph));\n\n  if (comm->rank == ncclParamGraphDumpFileRank()) {\n    struct ncclTopoGraph* graphs[3] = { &ringGraph, &treeGraph, &collNetGraph };\n    NCCLCHECK(ncclTopoDumpGraphs(comm->topo, 3, graphs));\n  }\n\n  // AllGather3 - begin\n  struct ncclGraphInfo {\n    int sameChannels;\n    float speedIntra;\n    float speedInter;\n    int typeIntra;\n  };\n\n  struct {\n    int cudaCompCap;\n    int fullCudaCompCap;\n    int nChannels;\n    struct ncclGraphInfo tree;\n    struct ncclGraphInfo ring;\n    struct ncclGraphInfo collNet;\n    struct ncclTopoRanks topoRanks;\n  } *allGather3Data;\n\n  NCCLCHECK(ncclCalloc(&allGather3Data, nranks));\n  allGather3Data[rank].cudaCompCap = ncclCudaCompCap();\n  allGather3Data[rank].nChannels = comm->nChannels = treeGraph.nChannels = ringGraph.nChannels =\n    std::min(treeGraph.nChannels, ringGraph.nChannels);\n  allGather3Data[rank].tree.sameChannels = treeGraph.sameChannels;\n  allGather3Data[rank].tree.speedIntra = treeGraph.speedIntra;\n  allGather3Data[rank].tree.speedInter = treeGraph.speedInter;\n  allGather3Data[rank].tree.typeIntra = treeGraph.typeIntra;\n  allGather3Data[rank].ring.sameChannels = ringGraph.sameChannels;\n  allGather3Data[rank].ring.speedIntra = ringGraph.speedIntra;\n  allGather3Data[rank].ring.speedInter = ringGraph.speedInter;\n  allGather3Data[rank].ring.typeIntra = ringGraph.typeIntra;\n  allGather3Data[rank].collNet.sameChannels = collNetGraph.sameChannels;\n  allGather3Data[rank].collNet.speedIntra = collNetGraph.speedIntra;\n  allGather3Data[rank].collNet.speedInter = collNetGraph.speedInter;\n  allGather3Data[rank].collNet.typeIntra = collNetGraph.typeIntra;\n\n  NCCLCHECK(ncclTopoPreset(comm, &treeGraph, &ringGraph, &collNetGraph, &allGather3Data[rank].topoRanks));\n\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather3Data, sizeof(*allGather3Data)));\n\n  // Determine nNodes, firstRanks, ...\n  int* nodesFirstRank;\n  NCCLCHECK(ncclCalloc(&nodesFirstRank, nranks));\n  for (int i=0; i<nranks; i++) {\n    int node = -1;\n    int firstRank = allGather3Data[i].topoRanks.ringRecv[0];\n    for (int n=0; n<comm->nNodes; n++) {\n      if (nodesFirstRank[n] == firstRank) node = n;\n    }\n    if (node == -1) {\n      node = comm->nNodes++;\n      nodesFirstRank[node] = firstRank;\n    }\n    if (i == comm->rank) comm->node = node;\n  }\n\n  // Determine the minimum CUDA Compute capability of all GPUs\n  int myCompCap = allGather3Data[rank].cudaCompCap;\n  int minCompCap = myCompCap, maxCompCap = myCompCap;\n  for (int i = 0; i < nranks; i++) {\n    minCompCap = std::min(allGather3Data[i].cudaCompCap, minCompCap);\n    maxCompCap = std::max(allGather3Data[i].cudaCompCap, maxCompCap);\n  }\n\n  int nChannelsOrig = comm->nChannels;\n  struct ncclTopoRanks** allTopoRanks;\n  NCCLCHECK(ncclCalloc(&allTopoRanks, comm->nRanks));\n  for (int i=0; i<nranks; i++) {\n    allTopoRanks[i] = &allGather3Data[i].topoRanks;\n    // Make sure we align all ranks so that the tuning is consistent across ranks\n    treeGraph.nChannels = ringGraph.nChannels = comm->nChannels = std::min(allGather3Data[i].nChannels, comm->nChannels);\n    treeGraph.sameChannels = std::min(allGather3Data[i].tree.sameChannels, treeGraph.sameChannels);\n    treeGraph.speedIntra = std::min(allGather3Data[i].tree.speedIntra, treeGraph.speedIntra);\n    treeGraph.speedInter = std::min(allGather3Data[i].tree.speedInter, treeGraph.speedInter);\n    treeGraph.typeIntra = std::min(allGather3Data[i].tree.typeIntra, treeGraph.typeIntra);\n    ringGraph.sameChannels = std::min(allGather3Data[i].ring.sameChannels, ringGraph.sameChannels);\n    ringGraph.speedIntra = std::min(allGather3Data[i].ring.speedIntra, ringGraph.speedIntra);\n    ringGraph.speedInter = std::min(allGather3Data[i].ring.speedInter, ringGraph.speedInter);\n    ringGraph.typeIntra = std::min(allGather3Data[i].ring.typeIntra, ringGraph.typeIntra);\n    collNetGraph.sameChannels = std::min(allGather3Data[i].collNet.sameChannels, collNetGraph.sameChannels);\n    collNetGraph.speedIntra = std::min(allGather3Data[i].collNet.speedIntra, collNetGraph.speedIntra);\n    collNetGraph.speedInter = std::min(allGather3Data[i].collNet.speedInter, collNetGraph.speedInter);\n    collNetGraph.typeIntra = std::min(allGather3Data[i].collNet.typeIntra, collNetGraph.typeIntra);\n  }\n\n  if (comm->nChannels < nChannelsOrig) {\n    // We started duplicating channels during Preset(), so we need to move the\n    // duplicated channels since we have removed some.\n    for (int i=0; i<comm->nChannels; i++) memcpy(comm->channels+comm->nChannels+i, comm->channels+nChannelsOrig+i, sizeof(struct ncclChannel));\n  }\n\n  int *rings;\n  NCCLCHECK(ncclCalloc(&rings, nranks*MAXCHANNELS));\n\n  NCCLCHECK(ncclTopoPostset(comm, nodesFirstRank, allTopoRanks, rings));\n  if (comm->nNodes > 1 &&\n      ncclParamCollNetEnable() == 1 &&\n      collNetSupport() && collNetGraph.nChannels) {\n    NCCLCHECK(ncclTopoConnectCollNet(comm, &collNetGraph, rank));\n  }\n\n  free(allTopoRanks);\n  free(nodesFirstRank);\n  free(allGather3Data);\n\n  // AllGather3 - end\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BUILT %d TREES/RINGS\", rank, nranks, comm->nChannels);\n\n  NCCLCHECK(ncclTopoTuneModel(comm, minCompCap, maxCompCap, &treeGraph, &ringGraph, &collNetGraph));\n\n  char line[1024];\n  line[0]='\\0';\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclTree* treeUp = &comm->channels[c].treeUp;\n    struct ncclTree* treeDn = &comm->channels[c].treeDn;\n    snprintf(line+strlen(line), 1023-strlen(line), \" [%d] %d/%d/%d->%d->%d|%d->%d->%d/%d/%d\",\n        c, treeUp->down[0], treeUp->down[1], treeUp->down[2], rank, treeUp->up,\n        treeDn->up, rank, treeDn->down[0], treeDn->down[1], treeDn->down[2]);\n  }\n  line[1023] = '\\0';\n  INFO(NCCL_INIT, \"Trees%s\", line);\n\n  // Set Affinity to a CPU local the our GPU, so that all memory we allocate\n  // on the host is local.\n  cpu_set_t affinitySave;\n  sched_getaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  NCCLCHECK(ncclTopoSetAffinity(comm->topo, comm->rank));\n  ncclResult_t ret;\n\n  NCCLCHECK(computeBuffSizes(comm));\n\n  // Connect with prev/next for each ring\n  struct ncclConnect *connect;\n  NCCLCHECKGOTO(ncclCalloc(&connect, 2), ret, affinity_restore);\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclChannel* channel = comm->channels+c;\n    NCCLCHECKGOTO(setupChannel(comm, c, rank, nranks, rings+c*nranks), ret, affinity_restore);\n    if (comm->nRanks == 1) continue;\n    NCCLCHECKGOTO(ncclTransportP2pSetup(comm, &ringGraph, channel, 1, &channel->ring.prev, 1, &channel->ring.next), ret, affinity_restore);\n    NCCLCHECKGOTO(ncclTransportP2pSetup(comm, &treeGraph, channel, NCCL_MAX_TREE_ARITY, channel->treeUp.down, 1, &channel->treeUp.up), ret, affinity_restore);\n    NCCLCHECKGOTO(ncclTransportP2pSetup(comm, &treeGraph, channel, 1, &channel->treeDn.up, NCCL_MAX_TREE_ARITY, channel->treeDn.down), ret, affinity_restore);\n  }\n\n  // Check if we can setup CollNet\n  if (comm->nNodes > 1 &&\n      ncclParamCollNetEnable() == 1 &&\n      collNetSupport() && collNetGraph.nChannels) {\n    int logicChannels = comm->nChannels/2;\n    int collNetSetupFail = 0;\n    const int recvIndex = 0;  // recv GPU index is always 0\n    const int sendIndex = collNetGraph.pattern == NCCL_TOPO_PATTERN_TREE ? 0 : 1;  // send GPU index depends on topo pattern\n    for (int c=0; c<logicChannels; c++) {\n      struct ncclChannel* channelRecv = comm->channels+logicChannels+c;\n      struct ncclChannel* channelSend = comm->channels+c;\n      NCCLCHECK(ncclTransportP2pSetup(comm, &collNetGraph, channelRecv, 1, &channelRecv->collTreeDn.up, 1, channelRecv->collTreeDn.down));\n      NCCLCHECK(ncclTransportP2pSetup(comm, &collNetGraph, channelSend, 1, channelSend->collTreeUp.down, 1, &channelSend->collTreeUp.up));\n      const int recvMaster = collNetGraph.intra[c*comm->localRanks+recvIndex];\n      const int sendMaster = collNetGraph.intra[c*comm->localRanks+sendIndex];\n      if (collNetSetup(comm, &collNetGraph, channelRecv, rank, nranks, recvMaster, sendMaster, comm->nNodes, 1) != 1)\n        collNetSetupFail = 1;\n      else if (collNetSetup(comm, &collNetGraph, channelSend, rank, nranks, sendMaster, recvMaster, comm->nNodes, 0) != 1)\n        collNetSetupFail = 1;\n    }\n    // Verify CollNet setup across ranks\n    NCCLCHECK(checkCollNetSetup(comm, rank, collNetSetupFail));\n  }\n  TRACE(NCCL_INIT, \"rank %d nranks %d - CONNECTED %d RINGS AND TREES\", rank, nranks, comm->nChannels);\n  free(connect);\n  free(rings);\n\n  // Compute nChannels per peer for p2p\n  NCCLCHECK(ncclTopoComputeP2pChannels(comm));\n\n  // We should have allocated all buffers, collective fifos, ... we can\n  // restore the affinity.\naffinity_restore:\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  if (ret != ncclSuccess) return ret;\n\n  // Compute intra ranks (using AllGather1 data)\n  int intraRank0 = -1, intraRank = -1, intraRanks = 0;\n  for (int i = 0; i < nranks; i++) {\n    if ((allGather1Data[i].peerInfo.hostHash == allGather1Data[rank].peerInfo.hostHash) &&\n        (allGather1Data[i].peerInfo.pidHash == allGather1Data[rank].peerInfo.pidHash)) {\n      if (intraRanks == 0) intraRank0 = i;\n      if (i == rank) intraRank = intraRanks;\n      intraRanks++;\n    }\n  }\n  TRACE(NCCL_INIT,\"hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n        rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n  if (intraRank == -1 || intraRank0 == -1 || allGather1Data[intraRank0].comm == NULL) {\n    WARN(\"Failed to determine intra ranks hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n         rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n    return ncclInternalError;\n  }\n  NCCLCHECK(ncclCommSetIntra(comm, intraRank, intraRanks, allGather1Data[intraRank0].comm));\n\n  // Done with AllGather1 data\n  free(allGather1Data);\n\n  if (comm->nNodes) NCCLCHECK(ncclProxyCreate(comm));\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - DONE\", rank, nranks);\n  return ncclSuccess;\n}\n\nncclResult_t ncclCommInitRankSync(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n\n  CUDACHECK(cudaSetDevice(cudaDev));\n  NCCLCHECKGOTO(commAlloc(newcomm, nranks, myrank), res, cleanup);\n  NCCLCHECKGOTO(initTransportsRank(*newcomm, &commId), res, cleanup);\n  NCCLCHECKGOTO(devCommSetup(*newcomm), res, cleanup);\n\n  INFO(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x - Init COMPLETE\", *newcomm, myrank, nranks, (*newcomm)->cudaDev, (*newcomm)->busId);\n\n  return ncclSuccess;\ncleanup:\n  if ((*newcomm) && (*newcomm)->bootstrap) bootstrapAbort((*newcomm)->bootstrap);\n  *newcomm = NULL;\n  return res;\n}\n\nstatic ncclResult_t ncclCommInitRankDev(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n  char* env = getenv(\"NCCL_COMM_ID\");\n  if (env && myrank == 0) {\n    INFO(NCCL_ENV, \"NCCL_COMM_ID set by environment to %s\", env);\n    NCCLCHECKGOTO(bootstrapCreateRoot(&commId, true), res, end);\n  }\n\n  NCCLCHECKGOTO(ncclInit(), res, end);\n  if (myrank == 0) showVersion();\n\n  // Make sure the CUDA runtime is initialized.\n  CUDACHECKGOTO(cudaFree(NULL), res, end);\n\n  NCCLCHECKGOTO(PtrCheck(newcomm, \"CommInitRank\", \"newcomm\"), res, end);\n  if (nranks < 1 || myrank < 0 || myrank >= nranks) {\n    WARN(\"Invalid rank requested : %d/%d\", myrank, nranks);\n    res = ncclInvalidArgument;\n    goto end;\n  }\n\n  if (ncclAsyncMode()) {\n    NCCLCHECKGOTO(ncclAsyncInit(ncclCommInitRankSync, newcomm, nranks, commId, myrank, cudaDev), res, end);\n  } else {\n    NCCLCHECKGOTO(ncclCommInitRankSync(newcomm, nranks, commId, myrank, cudaDev), res, end);\n  }\nend:\n  if (ncclAsyncMode()) return ncclAsyncErrCheck(res);\n  else return res;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitRank, ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank);\nncclResult_t ncclCommInitRank(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank) {\n  int cudaDev;\n  CUDACHECK(cudaGetDevice(&cudaDev));\n  NCCLCHECK(ncclCommInitRankDev(newcomm, nranks, commId, myrank, cudaDev));\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitAll, ncclComm_t* comms, int ndev, const int* devlist);\nncclResult_t ncclCommInitAll(ncclComm_t* comms, int ndev, const int* devlist) {\n  NCCLCHECK(PtrCheck(comms, \"CommInitAll\", \"comms\"));\n  if (ndev < 0) {\n    WARN(\"Invalid device count requested : %d\", ndev);\n    return ncclInvalidArgument;\n  }\n\n  ncclUniqueId uniqueId;\n  NCCLCHECK(ncclGetUniqueId(&uniqueId));\n  NCCLCHECK(ncclGroupStart());\n  for (int i=0; i<ndev; i++) {\n    // Ignore return codes .. we need to call ncclGroupEnd to clean up anyway\n    ncclCommInitRankDev(comms+i, ndev, uniqueId, i, devlist ? devlist[i] : i);\n  }\n  NCCLCHECK(ncclGroupEnd());\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commDestroy(ncclComm_t comm) {\n  int savedDevice;\n#ifdef ENABLE_TRACE\n  int rank = comm->rank;\n#endif\n  CUDACHECK(cudaGetDevice(&savedDevice));\n  int commDevice = comm->cudaDev;\n\n  if (savedDevice != commDevice) {\n    CUDACHECK(cudaSetDevice(commDevice));\n  }\n\n  TRACE(NCCL_INIT, \"Destroying comm %p rank %d abortFlag %d fatalError %d\", comm, rank, *comm->abortFlag, comm->fatalError);\n\n  CUDACHECK(cudaStreamSynchronize(comm->groupStream));\n  NCCLCHECK(ncclProxyDestroy(comm));\n  NCCLCHECK(commFree(comm));\n\n  if (savedDevice != commDevice)\n    CUDACHECK(cudaSetDevice(savedDevice));\n\n  TRACE(NCCL_INIT, \"Destroyed comm %p rank %d\", comm, rank);\n\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommDestroy, ncclComm_t comm);\nncclResult_t ncclCommDestroy(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  TRACE(NCCL_INIT, \"comm %p rank %d nRanks %d cudaDev %d busId %x\", comm, comm->rank, comm->nRanks, comm->cudaDev, comm->busId);\n\n  // Try and prevent a double free of the comm struct (user error)\n  if (comm->rank == -1 || comm->nRanks <= 0 || comm->cudaDev == -1 || comm->busId == -1) {\n    WARN(\"comm %p has already been destroyed\", comm);\n    return ncclInvalidArgument;\n  }\n\n  return commDestroy(comm);\n}\n\nNCCL_API(ncclResult_t, ncclCommAbort, ncclComm_t comm);\nncclResult_t ncclCommAbort(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  // Ask anything that might still be running on the device to quit\n  *comm->abortFlag = 1;\n\n  return commDestroy(comm);\n}\n\nNCCL_API(const char*, ncclGetErrorString, ncclResult_t code);\nconst char* ncclGetErrorString(ncclResult_t code) {\n  switch (code) {\n    case ncclSuccess                : return \"no error\";\n    case ncclUnhandledCudaError     : return \"unhandled cuda error\";\n    case ncclSystemError            : return \"unhandled system error\";\n    case ncclInternalError          : return \"internal error\";\n    case ncclInvalidArgument        : return \"invalid argument\";\n    case ncclInvalidUsage           : return \"invalid usage\";\n    default                         : return \"unknown result code\";\n  }\n}\n\nNCCL_API(ncclResult_t, ncclCommGetAsyncError, ncclComm_t comm, ncclResult_t *asyncError);\nncclResult_t ncclCommGetAsyncError(ncclComm_t comm, ncclResult_t *asyncError) {\n  NCCLCHECK(PtrCheck(comm, \"ncclGetAsyncError\", \"comm\"));\n  NCCLCHECK(PtrCheck(asyncError, \"ncclGetAsyncError\", \"asyncError\"));\n  *asyncError = comm->fatalError;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCount, const ncclComm_t comm, int* count);\nncclResult_t ncclCommCount(const ncclComm_t comm, int* count) {\n  NCCLCHECK(PtrCheck(comm, \"CommCount\", \"comm\"));\n  NCCLCHECK(PtrCheck(count, \"CommCount\", \"count\"));\n  *count = comm->nRanks;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCuDevice, const ncclComm_t comm, int* devid);\nncclResult_t ncclCommCuDevice(const ncclComm_t comm, int* devid) {\n  NCCLCHECK(PtrCheck(comm, \"CommCuDevice\", \"comm\"));\n  NCCLCHECK(PtrCheck(devid, \"CommCuDevice\", \"devid\"));\n  *devid = comm->cudaDev;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommUserRank, const ncclComm_t comm, int* rank);\nncclResult_t ncclCommUserRank(const ncclComm_t comm, int* rank) {\n  NCCLCHECK(PtrCheck(comm, \"CommUserRank\", \"comm\"));\n  NCCLCHECK(PtrCheck(rank, \"CommUserRank\", \"rank\"));\n  *rank = comm->rank;\n  return ncclSuccess;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.7.8-1-2fie5mmnboduvr5o52gbeaawkfbp3dko/spack-src/src/misc/nvmlwrap.cc": "/*************************************************************************\n * Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nvmlwrap.h\"\n\n#ifndef NVML_DIRECT\n#include <dlfcn.h>\n#include \"core.h\"\n\nstatic enum { nvmlUninitialized, nvmlInitializing, nvmlInitialized, nvmlError } nvmlState = nvmlUninitialized;\n\nstatic nvmlReturn_t (*nvmlInternalInit)(void);\nstatic nvmlReturn_t (*nvmlInternalShutdown)(void);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByPciBusId)(const char* pciBusId, nvmlDevice_t* device);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetIndex)(nvmlDevice_t device, unsigned* index);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByIndex)(unsigned int index, nvmlDevice_t* device);\nstatic const char* (*nvmlInternalErrorString)(nvmlReturn_t r);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkState)(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetPciInfo)(nvmlDevice_t device, nvmlPciInfo_t* pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkRemotePciInfo)(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkCapability)(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetMinorNumber)(nvmlDevice_t device, unsigned int* minorNumber);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetCudaComputeCapability)(nvmlDevice_t device, int* major, int* minor);\n\n// Used to make the NVML library calls thread safe\npthread_mutex_t nvmlLock = PTHREAD_MUTEX_INITIALIZER;\n\nncclResult_t wrapNvmlSymbols(void) {\n  if (nvmlState == nvmlInitialized)\n    return ncclSuccess;\n  if (nvmlState == nvmlError)\n    return ncclSystemError;\n\n  if (__sync_bool_compare_and_swap(&nvmlState, nvmlUninitialized, nvmlInitializing) == false) {\n    // Another thread raced in front of us. Wait for it to be done.\n    while (nvmlState == nvmlInitializing) pthread_yield();\n    return (nvmlState == nvmlInitialized) ? ncclSuccess : ncclSystemError;\n  }\n\n  static void* nvmlhandle = NULL;\n  void* tmp;\n  void** cast;\n\n  nvmlhandle=dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n  if (!nvmlhandle) {\n    WARN(\"Failed to open libnvidia-ml.so.1\");\n    goto teardown;\n  }\n\n#define LOAD_SYM(handle, symbol, funcptr) do {         \\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      WARN(\"dlsym failed on %s - %s\", symbol, dlerror());\\\n      goto teardown;                                     \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n#define LOAD_SYM_OPTIONAL(handle, symbol, funcptr) do {\\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      INFO(NCCL_INIT,\"dlsym failed on %s, ignoring\", symbol); \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n  LOAD_SYM(nvmlhandle, \"nvmlInit\", nvmlInternalInit);\n  LOAD_SYM(nvmlhandle, \"nvmlShutdown\", nvmlInternalShutdown);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByPciBusId\", nvmlInternalDeviceGetHandleByPciBusId);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetIndex\", nvmlInternalDeviceGetIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByIndex\", nvmlInternalDeviceGetHandleByIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlErrorString\", nvmlInternalErrorString);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetPciInfo\", nvmlInternalDeviceGetPciInfo);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetMinorNumber\", nvmlInternalDeviceGetMinorNumber);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkState\", nvmlInternalDeviceGetNvLinkState);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkRemotePciInfo\", nvmlInternalDeviceGetNvLinkRemotePciInfo);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkCapability\", nvmlInternalDeviceGetNvLinkCapability);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetCudaComputeCapability\", nvmlInternalDeviceGetCudaComputeCapability);\n\n  nvmlState = nvmlInitialized;\n  return ncclSuccess;\n\nteardown:\n  nvmlInternalInit = NULL;\n  nvmlInternalShutdown = NULL;\n  nvmlInternalDeviceGetHandleByPciBusId = NULL;\n  nvmlInternalDeviceGetIndex = NULL;\n  nvmlInternalDeviceGetHandleByIndex = NULL;\n  nvmlInternalDeviceGetPciInfo = NULL;\n  nvmlInternalDeviceGetMinorNumber = NULL;\n  nvmlInternalDeviceGetNvLinkState = NULL;\n  nvmlInternalDeviceGetNvLinkRemotePciInfo = NULL;\n  nvmlInternalDeviceGetNvLinkCapability = NULL;\n\n  if (nvmlhandle != NULL) dlclose(nvmlhandle);\n  nvmlState = nvmlError;\n  return ncclSystemError;\n}\n\n\nncclResult_t wrapNvmlInit(void) {\n  if (nvmlInternalInit == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalInit();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlInit() failed: %s\",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlShutdown(void) {\n  if (nvmlInternalShutdown == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalShutdown();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlShutdown() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByPciBusId(const char* pciBusId, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByPciBusId == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByPciBusId(pciBusId, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByPciBusId() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetIndex(nvmlDevice_t device, unsigned* index) {\n  if (nvmlInternalDeviceGetIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetIndex(device, index), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByIndex(unsigned int index, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByIndex(index, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetPciInfo(nvmlDevice_t device, nvmlPciInfo_t* pci) {\n  if (nvmlInternalDeviceGetPciInfo == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetPciInfo(device, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetPciInfo() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetMinorNumber(nvmlDevice_t device, unsigned int* minorNumber) {\n  if (nvmlInternalDeviceGetMinorNumber == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetMinorNumber(device, minorNumber), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetMinorNumber() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkState(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive) {\n  if (nvmlInternalDeviceGetNvLinkState == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkState(device, link, isActive), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkState() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkRemotePciInfo(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci) {\n  if (nvmlInternalDeviceGetNvLinkRemotePciInfo == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkRemotePciInfo(device, link, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkRemotePciInfo() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkCapability(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkCapability(device, link, capability, capResult), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkCapability() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetCudaComputeCapability(nvmlDevice_t device, int* major, int* minor) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetCudaComputeCapability(device, major, minor), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetCudaComputeCapability() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n#endif\n"
    },
    "skipped": [],
    "total_files": 103
}