{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.5.6-2-rc24r6pmtkh4yen5el4y34cu3arhdmf5/spack-src/src/init.cc": "/*************************************************************************\n * Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nccl.h\"\n#include \"channel.h\"\n#include \"nvmlwrap.h\"\n#include \"bootstrap.h\"\n#include \"transport.h\"\n#include \"group.h\"\n#include \"net.h\"\n#include \"enqueue.h\"\n#include \"graph.h\"\n#include \"argcheck.h\"\n#include \"cpuset.h\"\n#include <sched.h>\n#include <fcntl.h>\n#include <string.h>\n#include <errno.h>\n#include <assert.h>\n#include <dlfcn.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n\n#define STR2(v) #v\n#define STR(v) STR2(v)\n\n#ifdef ENABLE_TRACE\nstd::chrono::high_resolution_clock::time_point ncclEpoch;\n#endif\n\n#if CUDART_VERSION >= 9020\n#define NCCL_GROUP_CUDA_STREAM 0 // CGMD: CUDA 9.2,10.X Don't need to use an internal CUDA stream\n#else\n#define NCCL_GROUP_CUDA_STREAM 1 // CGMD: CUDA 9.0,9.1 Need to use an internal CUDA stream\n#endif\n\nNCCL_PARAM(GroupCudaStream, \"GROUP_CUDA_STREAM\", NCCL_GROUP_CUDA_STREAM);\n\nNCCL_PARAM(CheckPointers, \"CHECK_POINTERS\", 0);\n\nncclNet_t* ncclNet = NULL;\n\n// Returns ncclInternalError if anything fails, causing that network to be ignored.\nncclResult_t initNet(ncclNet_t* net) {\n  int ndev;\n  if (net->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (net->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initNetPlugin(ncclNet_t** net) {\n  void* netPluginLib = dlopen(\"libnccl-net.so\", RTLD_NOW | RTLD_LOCAL);\n  if (netPluginLib == NULL) {\n    // dlopen does not guarantee to set errno, but dlerror only gives us a\n    // string, so checking errno doesn't hurt to try to provide a better\n    // error message\n    if (errno == ENOENT) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\");\n    } else {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : Plugin load returned %d : %s.\", errno, dlerror());\n    }\n    return ncclSuccess;\n  }\n  ncclNet_t* extNet = (ncclNet_t*) dlsym(netPluginLib, STR(NCCL_PLUGIN_SYMBOL));\n  if (extNet == NULL) {\n    INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_PLUGIN_SYMBOL) \" symbol.\");\n    goto cleanup;\n  }\n  if (initNet(extNet) == ncclSuccess) {\n    *net = extNet;\n    return ncclSuccess;\n  }\ncleanup:\n  if (netPluginLib != NULL) dlclose(netPluginLib);\n  return ncclSuccess;\n}\n\nncclResult_t initNet() {\n  // Always initialize bootstrap network\n  NCCLCHECK(bootstrapNetInit());\n\n  NCCLCHECK(initNetPlugin(&ncclNet));\n  if (ncclNet != NULL) return ncclSuccess;\n  if (initNet(&ncclNetIb) == ncclSuccess) {\n    ncclNet = &ncclNetIb;\n  } else {\n    NCCLCHECK(initNet(&ncclNetSocket));\n    ncclNet = &ncclNetSocket;\n  }\n  return ncclSuccess;\n}\n\npthread_mutex_t initLock = PTHREAD_MUTEX_INITIALIZER;\nstatic bool initialized = false;\nstatic ncclResult_t ncclInit() {\n  if (initialized) return ncclSuccess;\n  pthread_mutex_lock(&initLock);\n  if (!initialized) {\n    initEnv();\n    initNet();\n    initialized = true;\n  }\n  pthread_mutex_unlock(&initLock);\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetVersion, int* version);\nncclResult_t ncclGetVersion(int* version) {\n  if (version == NULL) return ncclInvalidArgument;\n  *version = NCCL_VERSION_CODE;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetUniqueId, ncclUniqueId* out);\nncclResult_t ncclGetUniqueId(ncclUniqueId* out) {\n  NCCLCHECK(ncclInit());\n  NCCLCHECK(PtrCheck(out, \"GetUniqueId\", \"out\"));\n  return bootstrapGetUniqueId(out);\n}\n\n// Prevent compiler from optimizing out these operations\nvoid __attribute__((optimize(\"O0\"))) commPoison(ncclComm_t comm) {\n  comm->rank = comm->cudaDev = comm->busId = comm->nRanks = -1;\n}\n\nstatic ncclResult_t commFree(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  free(comm->peerInfo);\n  ncclTopoFree(comm->topo);\n\n  if (comm->bootstrap)\n    NCCLCHECK(bootstrapClose(comm->bootstrap));\n\n  CUDACHECK(cudaFree(comm->hostDevComm.channels));\n  CUDACHECK(cudaFree(comm->devComm));\n\n  for (int channel=0; channel<comm->nChannels; channel++)\n    NCCLCHECK(freeChannel(comm->channels+channel, comm->nRanks));\n\n  if (comm->doneEvent != NULL)\n    CUDACHECK(cudaEventDestroy(comm->doneEvent));\n\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamDestroy(comm->groupStream));\n  }\n\n  // Last rank frees shared resources between threads\n  int isLast;\n  NCCLCHECK(ncclCpuBarrierIn(comm, &isLast));\n  if (isLast) {\n    free(comm->intraBarrier);\n    free(comm->intraParams);\n    free(comm->intraCudaDevs);\n    free(comm->intraCGMode);\n    free(comm->intraCC);\n  }\n  CUDACHECK(cudaFreeHost((void *)comm->abortFlag));\n  CUDACHECK(cudaFreeHost((void *)comm->fatalDevError));\n\n  // Poison comm to try and catch a double free\n  commPoison(comm);\n\n  free(comm);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commAlloc(ncclComm_t* comret, int ndev, int rank) {\n  if (ndev < 1) {\n    WARN(\"invalid device count (%d) requested\", ndev);\n    return ncclInvalidArgument;\n  }\n  if (rank >= ndev || rank < 0) {\n    WARN(\"rank %d exceeds ndev=%d\", rank, ndev);\n    return ncclInvalidArgument;\n  }\n\n  // Try to create a CUDA object right away. If there is something wrong with\n  // the device we're on (failure cause #1) , better know it early.\n  cudaEvent_t doneEvent;\n  CUDACHECK(cudaEventCreateWithFlags(&doneEvent, cudaEventDisableTiming));\n\n  struct ncclComm* comm;\n  NCCLCHECK(ncclCalloc(&comm, 1));\n\n  comm->rank = comm->hostDevComm.rank =rank;\n  comm->nRanks = comm->hostDevComm.nRanks = ndev;\n  cudaGetDevice(&comm->cudaDev);\n  NCCLCHECK(getBusId(comm->cudaDev, &comm->busId));\n  TRACE(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x\", comm, rank, ndev, comm->cudaDev, comm->busId);\n\n  comm->doneEvent = doneEvent;\n  comm->checkPointers = ncclParamCheckPointers() == 1 ? true : false;\n#if CUDART_VERSION >= 9020\n  comm->groupCudaStream = ncclParamGroupCudaStream();\n#else\n  // Don't allow the user to overload the default setting in older CUDA builds\n  comm->groupCudaStream = NCCL_GROUP_CUDA_STREAM;\n#endif\n  comm->fatalError = ncclSuccess;\n\n  NCCLCHECK(ncclCudaHostAlloc((void**) &comm->fatalDevError, (void**) &comm->hostDevComm.fatalDevError, sizeof(ncclDevError_t)));\n  *comm->fatalDevError = ncclDevSuccess;\n\n  NCCLCHECK(ncclCudaHostAlloc((void**) &comm->abortFlag, (void**) &comm->hostDevComm.abortFlag, sizeof(uint32_t)));\n  *comm->abortFlag = 0;\n\n  comm->argsptr = &comm->args;\n\n  *comret = comm;\n  return ncclSuccess;\n}\n\nstatic ncclResult_t devCommSetup(ncclComm_t comm) {\n  // Duplicate the channels on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->hostDevComm.channels, comm->nChannels));\n  NCCLCHECK(ncclCudaMemcpy(comm->hostDevComm.channels, comm->channels, comm->nChannels));\n\n  // Copy userRanks and peers\n  for (int r=0; r<comm->nChannels; r++) {\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].ring.devUserRanks, comm->channels[r].ring.userRanks, comm->nRanks));\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].devPeers, comm->channels[r].peers, comm->nRanks));\n  }\n\n  // Duplicate the dev comm on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->devComm, 1));\n  NCCLCHECK(ncclCudaMemcpy(comm->devComm, &comm->hostDevComm, 1));\n  return ncclSuccess;\n}\n\n// Pre-process the string so that running \"strings\" on the lib can quickly reveal the version.\n#define VERSION_STRING \"NCCL version \" STR(NCCL_MAJOR) \".\" STR(NCCL_MINOR) \".\" STR(NCCL_PATCH) NCCL_SUFFIX \"+cuda\" STR(CUDA_MAJOR) \".\" STR(CUDA_MINOR)\nstatic void showVersion() {\n  static int shown = 0;\n  if (shown == 0 && ncclDebugLevel >= NCCL_LOG_VERSION) {\n    printf(\"%s\\n\", VERSION_STRING);\n    fflush(stdout);\n    if (ncclDebugFile != stdout)\n      INFO(NCCL_ALL,\"%s\", VERSION_STRING); // Also log NCCL version in one of the files\n    shown = 1;\n  }\n}\n\nstatic ncclResult_t fillInfo(struct ncclComm* comm, struct ncclPeerInfo* info, uint64_t commHash) {\n  info->rank = comm->rank;\n  CUDACHECK(cudaGetDevice(&info->cudaDev));\n  info->hostHash=getHostHash()+commHash;\n  info->pidHash=getPidHash()+commHash;\n\n  // Get the device MAJOR:MINOR of /dev/shm so we can use that\n  // information to decide whether we can use SHM for inter-process\n  // communication in a container environment\n  struct stat statbuf;\n  SYSCHECK(stat(\"/dev/shm\", &statbuf), \"stat\");\n  info->shmDev = statbuf.st_dev;\n\n  info->busId = comm->busId;\n  int netDevs;\n\n  NCCLCHECK(ncclNetDevices(&netDevs));\n  for (int n=0; n<netDevs; n++) {\n    int ptrSupport;\n    NCCLCHECK(ncclNetPtrSupport(n, &ptrSupport));\n    if (ptrSupport & NCCL_PTR_CUDA) info->gdrSupport |= (1 << n);\n  }\n  return ncclSuccess;\n}\n\ntemplate <int type>\nstatic ncclResult_t selectTransport(struct ncclTopoSystem* topo, struct ncclTopoGraph* graph, struct ncclPeerInfo* myInfo, struct ncclPeerInfo* peerInfo, struct ncclConnect* connect, struct ncclConnector* connector, int buffSize, int channelId) {\n  for (int t=0; t<NTRANSPORTS; t++) {\n    struct ncclTransport *transport = ncclTransports+t;\n    struct ncclTransportComm* transportComm = type == 1 ? &transport->send : &transport->recv;\n    int ret = 0;\n    NCCLCHECK(transport->canConnect(&ret, topo, graph, myInfo, peerInfo));\n    if (ret) {\n      connector->transportComm = transportComm;\n      NCCLCHECK(transportComm->setup(topo, graph, myInfo, peerInfo, connect, connector, buffSize, channelId));\n      return ncclSuccess;\n    }\n  }\n  WARN(\"No transport found !\");\n  return ncclInternalError;\n}\n\nstatic ncclResult_t setupChannel(struct ncclComm* comm, int channelId, int rank, int nranks, int* ringRanks) {\n  TRACE(NCCL_INIT, \"rank %d nranks %d\", rank, nranks);\n  NCCLCHECK(initChannel(comm, channelId));\n\n  struct ncclRing* ring = &comm->channels[channelId].ring;\n  // Reorganize ranks to start with rank.\n  int shift;\n  for (shift = 0; shift<nranks; shift++) {\n    if (ringRanks[shift] == rank) {\n      break;\n    }\n  }\n  for (int i=0; i<nranks; i++) {\n    ring->userRanks[i] = ringRanks[(i+shift)%nranks];\n  }\n  return ncclSuccess;\n}\n\nvoid* waitForNonNullPtr(void* p) {\n  volatile void** ptr = (volatile void**) p;\n  while (*ptr == NULL) sched_yield();\n  return (void*)*ptr;\n}\n\nncclResult_t initParams(struct ncclComm* comm) {\n  struct cudaLaunchParams* params = comm->myParams = comm->intraParams+comm->intraRank;\n  params->args = &comm->argsptr;\n  params->stream = NULL;\n  params->sharedMem = 0;\n  params->blockDim.x = 0; params->blockDim.y = params->blockDim.z = 1;\n  params->gridDim.x = 0; params->gridDim.y = params->gridDim.z = 1;\n  return ncclSuccess;\n}\n\n// Allocate/Set Intra Process Structures and set CG options\nncclResult_t ncclCommSetIntra(struct ncclComm* comm, int rank, int ranks, struct ncclComm* comm0) {\n  comm->intraRank = rank;\n  comm->intraRanks = ranks;\n  comm->intraPhase = 0;\n\n  // Alloc shared structures\n  if (rank == 0) {\n    assert(comm == comm0);\n    int* bar;\n    NCCLCHECK(ncclCalloc(&bar, 2));\n    bar[0] = bar[1] = 0;\n    comm->intraBarrier = bar;\n    NCCLCHECK(ncclCalloc(&comm->intraParams, comm->intraRanks));\n    NCCLCHECK(ncclCalloc(&comm->intraCudaDevs, comm->intraRanks));\n    int* CGMode;\n    NCCLCHECK(ncclCalloc(&CGMode, 1));\n    *CGMode = 0x11;\n    comm->intraCGMode = CGMode;\n    int* CC;\n    NCCLCHECK(ncclCalloc(&CC, 1));\n    *CC = ncclCudaCompCap();\n    comm->intraCC = CC;\n  } else {\n    comm->intraBarrier = (int*)waitForNonNullPtr(&comm0->intraBarrier);\n    comm->intraParams = (struct cudaLaunchParams*)waitForNonNullPtr(&comm0->intraParams);\n    comm->intraCudaDevs = (int*)waitForNonNullPtr(&comm0->intraCudaDevs);\n    comm->intraCGMode = (int*)waitForNonNullPtr(&comm0->intraCGMode);\n    comm->intraCC = (int*)waitForNonNullPtr(&comm0->intraCC);\n  }\n  comm->intraCudaDevs[comm->intraRank] = comm->cudaDev;\n  NCCLCHECK(initParams(comm));\n\n  int cgMdLaunch = 0;\n\n  // Set CG Mode\n  comm->launchMode = ncclComm::GROUP;\n  char* str = getenv(\"NCCL_LAUNCH_MODE\");\n  if (comm->intraRanks == 1 || (str && strcmp(str, \"PARALLEL\") == 0)) {\n    comm->launchMode = ncclComm::PARALLEL;\n  }\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamCreateWithFlags(&comm->groupStream, cudaStreamNonBlocking));\n#if CUDART_VERSION >= 9000\n    if (*comm->intraCC && (ncclCudaCompCap() == *comm->intraCC)) {\n      // Check whether the GPU supports Cooperative Group Multi Device Launch\n      (void) cudaDeviceGetAttribute(&cgMdLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, comm->cudaDev);\n    }\n#endif\n  }\n\n  // Disable cgMdLaunch if any rank does not support it\n  if (cgMdLaunch == 0) {\n    *comm->intraCGMode = 0x10;\n  }\n  return ncclSuccess;\n}\n\nstatic ncclResult_t p2pSetup(struct ncclComm* comm, struct ncclTopoGraph* graph, struct ncclChannel* channel, int nrecv, int* peerRecv, int nsend, int* peerSend) {\n  TRACE(NCCL_INIT, \"nsend %d nrecv %d\", nsend, nrecv);\n  uint32_t nSkippedSend = 0, nSkippedRecv = 0; /* for tracing */\n  struct ncclConnect connect;\n  struct ncclConnector* conn;\n  for (int i=0; i<nrecv; i++) {\n    int peer = peerRecv[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].recv;\n    if (conn->connected) { ++nSkippedRecv; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(selectTransport<0>(comm->topo, graph, comm->peerInfo+comm->rank, comm->peerInfo+peer, &connect, conn, channel->buffSize, channel->id));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n  }\n  for (int i=0; i<nsend; i++) {\n    int peer = peerSend[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].send;\n    if (conn->connected) { ++nSkippedSend; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(selectTransport<1>(comm->topo, graph, comm->peerInfo+comm->rank, comm->peerInfo+peer, &connect, conn, channel->buffSize, channel->id));\n    NCCLCHECK(bootstrapSend(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n  }\n  for (int i=0; i<nsend; i++) {\n    int peer = peerSend[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].send;\n    if (conn->connected) {++nSkippedSend; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n    NCCLCHECK(conn->transportComm->connect(&connect, conn));\n    conn->connected = 1;\n  }\n  for (int i=0; i<nrecv; i++) {\n    int peer = peerRecv[i];\n    if (peer == -1) continue;\n    conn = &channel->peers[peer].recv;\n    if (conn->connected) {++nSkippedRecv; continue; }\n    memset(&connect, 0, sizeof(connect));\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, peer, &connect, sizeof(struct ncclConnect)));\n    NCCLCHECK(conn->transportComm->connect(&connect, conn));\n    conn->connected = 1;\n  }\n  TRACE(NCCL_INIT, \"nsend %d nrecv %d nSkippedSend %u nSkippedRecv %u - DONE\", nsend, nrecv, nSkippedSend, nSkippedRecv);\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CrossNic, \"CROSS_NIC\", 2);\n\nstatic ncclResult_t initTransportsRank(struct ncclComm* comm, ncclUniqueId* commId) {\n  // We use 3 AllGathers\n  // 1. { peerInfo, comm }\n  // 2. ConnectTransport[nranks], ConnectValue[nranks]\n  // 3. { nThreads, nrings, compCap, prev[MAXCHANNELS], next[MAXCHANNELS] }\n\n  int rank = comm->rank;\n  int nranks = comm->nRanks;\n  uint64_t commHash = getHash(commId->internal, NCCL_UNIQUE_ID_BYTES);\n  TRACE(NCCL_INIT, \"comm %p, commHash %lx, rank %d nranks %d - BEGIN\", comm, commHash, rank, nranks);\n  NCCLCHECK(bootstrapInit(commId, rank, nranks, &comm->bootstrap));\n\n  // AllGather1 - begin\n  struct {\n    struct ncclPeerInfo peerInfo;\n    struct ncclComm* comm;\n  } *allGather1Data;\n\n  NCCLCHECK(ncclCalloc(&allGather1Data, nranks));\n  allGather1Data[rank].comm = comm;\n  struct ncclPeerInfo* myInfo = &allGather1Data[rank].peerInfo;\n  NCCLCHECK(fillInfo(comm, myInfo, commHash));\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather1Data, sizeof(*allGather1Data)));\n\n  NCCLCHECK(ncclCalloc(&comm->peerInfo, nranks));\n  for (int i = 0; i < nranks; i++) {\n    memcpy(comm->peerInfo+i, &allGather1Data[i].peerInfo, sizeof(struct ncclPeerInfo));\n    if ((i != rank) && (comm->peerInfo[i].hostHash == myInfo->hostHash) && (comm->peerInfo[i].busId == myInfo->busId)) {\n      WARN(\"Duplicate GPU detected : rank %d and rank %d both on CUDA device %x\", rank, i, myInfo->busId);\n      return ncclInvalidUsage;\n    }\n  }\n  // AllGather1 data is used again below\n  // AllGather1 - end\n\n  // Topo detection / System graph creation\n  NCCLCHECK(ncclTopoGetSystem(comm, &comm->topo));\n  // Compute paths between GPUs and NICs\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Remove inaccessible GPUs and unused NICs\n  NCCLCHECK(ncclTopoTrimSystem(comm->topo, comm));\n  // Recompute paths after trimming\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Compute max speed to accelerate search\n  NCCLCHECK(ncclTopoGetMaxSpeed(comm->topo));\n  // Print final topology\n  NCCLCHECK(ncclTopoPrint(comm->topo));\n\n  // Get rings and trees\n  struct ncclTopoGraph treeGraph;\n  treeGraph.pattern = NCCL_TOPO_PATTERN_SPLIT_TREE;\n  treeGraph.crossNic = ncclParamCrossNic();\n  // We communicate only half the data between node with trees on 2 nodes.\n  NCCLCHECK(ncclTopoCompute(comm->topo, &treeGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &treeGraph));\n  struct ncclTopoGraph ringGraph;\n  ringGraph.pattern = NCCL_TOPO_PATTERN_RING;\n  ringGraph.crossNic = ncclParamCrossNic();\n  NCCLCHECK(ncclTopoCompute(comm->topo, &ringGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &ringGraph));\n\n  // AllGather3 - begin\n\n  struct {\n    int cudaCompCap;\n    int fullCudaCompCap;\n    int nvlink;\n    int nChannels;\n    struct {\n      int sameChannels;\n      int speedIntra;\n      int speedInter;\n      int nvlink;\n    } tree;\n    struct {\n      int sameChannels;\n      int speedIntra;\n      int speedInter;\n      int nvlink;\n    } ring;\n    struct ncclTopoRanks topoRanks;\n  } *allGather3Data;\n\n  NCCLCHECK(ncclCalloc(&allGather3Data, nranks));\n  allGather3Data[rank].cudaCompCap = ncclCudaCompCap();\n  allGather3Data[rank].nvlink = treeGraph.nvlink;\n  allGather3Data[rank].nChannels = comm->nChannels = std::min(treeGraph.nChannels, ringGraph.nChannels);\n  allGather3Data[rank].tree.sameChannels = treeGraph.sameChannels;\n  allGather3Data[rank].tree.speedIntra = treeGraph.speedIntra;\n  allGather3Data[rank].tree.speedInter = treeGraph.speedInter;\n  allGather3Data[rank].tree.nvlink = treeGraph.nvlink;\n  allGather3Data[rank].ring.sameChannels = ringGraph.sameChannels;\n  allGather3Data[rank].ring.speedIntra = ringGraph.speedIntra;\n  allGather3Data[rank].ring.speedInter = ringGraph.speedInter;\n  allGather3Data[rank].ring.nvlink = ringGraph.nvlink;\n\n  NCCLCHECK(ncclTopoPreset(comm, &treeGraph, &ringGraph, &allGather3Data[rank].topoRanks));\n\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather3Data, sizeof(*allGather3Data)));\n\n  // Determine nNodes, firstRanks, ...\n  int* nodesFirstRank;\n  NCCLCHECK(ncclCalloc(&nodesFirstRank, nranks));\n  for (int i=0; i<nranks; i++) {\n    int node = -1;\n    int firstRank = allGather3Data[i].topoRanks.ringRecv[0];\n    for (int n=0; n<comm->nNodes; n++) {\n      if (nodesFirstRank[n] == firstRank) node = n;\n    }\n    if (node == -1) {\n      node = comm->nNodes++;\n      nodesFirstRank[node] = firstRank;\n    }\n    if (i == comm->rank) comm->node = node;\n  }\n\n  // Determine the minimum CUDA Compute capability of all GPUs\n  int myCompCap = allGather3Data[rank].cudaCompCap;\n  int minCompCap = myCompCap, maxCompCap = myCompCap;\n  for (int i = 0; i < nranks; i++) {\n    minCompCap = std::min(allGather3Data[i].cudaCompCap, minCompCap);\n    maxCompCap = std::max(allGather3Data[i].cudaCompCap, maxCompCap);\n  }\n\n  comm->nvlink = 1;\n  for (int i = 0; i < nranks; i++) comm->nvlink &= allGather3Data[i].nvlink;\n\n  int nChannelsOrig = comm->nChannels;\n  struct ncclTopoRanks** allTopoRanks;\n  NCCLCHECK(ncclCalloc(&allTopoRanks, comm->nRanks));\n  for (int i=0; i<nranks; i++) {\n    allTopoRanks[i] = &allGather3Data[i].topoRanks;\n    // Make sure we align all ranks so that the tuning is consistent across ranks\n    treeGraph.nChannels = ringGraph.nChannels = comm->nChannels = std::min(allGather3Data[i].nChannels, comm->nChannels);\n    treeGraph.sameChannels = std::min(allGather3Data[i].tree.sameChannels, treeGraph.sameChannels);\n    treeGraph.speedIntra = std::min(allGather3Data[i].tree.speedIntra, treeGraph.speedIntra);\n    treeGraph.speedInter = std::min(allGather3Data[i].tree.speedInter, treeGraph.speedInter);\n    treeGraph.nvlink = std::min(allGather3Data[i].tree.nvlink, treeGraph.nvlink);\n    ringGraph.sameChannels = std::min(allGather3Data[i].ring.sameChannels, ringGraph.sameChannels);\n    ringGraph.speedIntra = std::min(allGather3Data[i].ring.speedIntra, ringGraph.speedIntra);\n    ringGraph.speedInter = std::min(allGather3Data[i].ring.speedInter, ringGraph.speedInter);\n    ringGraph.nvlink = std::min(allGather3Data[i].ring.nvlink, ringGraph.nvlink);\n  }\n\n  if (comm->nChannels < nChannelsOrig) {\n    // We started duplicating channels during Preset(), so we need to move the\n    // duplicated channels since we have removed some.\n    for (int i=0; i<comm->nChannels; i++) memcpy(comm->channels+comm->nChannels+i, comm->channels+nChannelsOrig+i, sizeof(struct ncclChannel));\n  }\n\n  int *rings;\n  NCCLCHECK(ncclCalloc(&rings, nranks*MAXCHANNELS));\n\n  NCCLCHECK(ncclTopoPostset(comm, nodesFirstRank, allTopoRanks, rings));\n\n  free(allTopoRanks);\n  free(nodesFirstRank);\n  free(allGather3Data);\n\n  // AllGather3 - end\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BUILT %d TREES/RINGS\", rank, nranks, comm->nChannels);\n\n  NCCLCHECK(ncclSetThresholds(comm, minCompCap, maxCompCap, &treeGraph, &ringGraph));\n\n  char line[1024];\n  line[0]='\\0';\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclTree* treeUp = &comm->channels[c].treeUp;\n    struct ncclTree* treeDn = &comm->channels[c].treeDn;\n    snprintf(line+strlen(line), 1023-strlen(line), \" [%d] %d/%d/%d->%d->%d|%d->%d->%d/%d/%d\",\n        c, treeUp->down[0], treeUp->down[1], treeUp->down[2], rank, treeUp->up,\n        treeDn->up, rank, treeDn->down[0], treeDn->down[1], treeDn->down[2]);\n  }\n  line[1023] = '\\0';\n  INFO(NCCL_INIT, \"Trees%s\", line);\n\n  // Connect with prev/next for each ring\n  struct ncclConnect *connect;\n  NCCLCHECK(ncclCalloc(&connect, 2));\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclChannel* channel = comm->channels+c;\n    NCCLCHECK(setupChannel(comm, c, rank, nranks, rings+c*nranks));\n    if (comm->nRanks == 1) continue;\n    NCCLCHECK(p2pSetup(comm, &ringGraph, channel, 1, &channel->ring.prev, 1, &channel->ring.next));\n    NCCLCHECK(p2pSetup(comm, &treeGraph, channel, NCCL_MAX_TREE_ARITY, channel->treeUp.down, 1, &channel->treeUp.up));\n    NCCLCHECK(p2pSetup(comm, &treeGraph, channel, 1, &channel->treeDn.up, NCCL_MAX_TREE_ARITY, channel->treeDn.down));\n  }\n  TRACE(NCCL_INIT, \"rank %d nranks %d - CONNECTED %d RINGS AND TREES\", rank, nranks, comm->nChannels);\n  free(connect);\n  free(rings);\n\n  // Compute intra ranks (using AllGather1 data)\n  int intraRank0 = -1, intraRank = -1, intraRanks = 0;\n  for (int i = 0; i < nranks; i++) {\n    if ((allGather1Data[i].peerInfo.hostHash == allGather1Data[rank].peerInfo.hostHash) &&\n        (allGather1Data[i].peerInfo.pidHash == allGather1Data[rank].peerInfo.pidHash)) {\n      if (intraRanks == 0) intraRank0 = i;\n      if (i == rank) intraRank = intraRanks;\n      intraRanks++;\n    }\n  }\n  TRACE(NCCL_INIT,\"hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n        rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n  if (intraRank == -1 || intraRank0 == -1 || allGather1Data[intraRank0].comm == NULL) {\n    WARN(\"Failed to determine intra ranks hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n         rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n    return ncclInternalError;\n  }\n  NCCLCHECK(ncclCommSetIntra(comm, intraRank, intraRanks, allGather1Data[intraRank0].comm));\n\n  // Done with AllGather1 data\n  free(allGather1Data);\n\n  if (comm->nNodes) NCCLCHECK(transportCreateProxy(comm));\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - DONE\", rank, nranks);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t getCpuGpuAffinity(int cudaDev, cpu_set_t* mask) {\n  CPU_ZERO_S(sizeof(cpu_set_t), mask);\n  char* cudaPath;\n  NCCLCHECK(ncclTopoCudaPath(cudaDev, &cudaPath));\n  char path[PATH_MAX];\n  strncpy(path, cudaPath, PATH_MAX-1);\n  snprintf(path+strlen(path), PATH_MAX-1-strlen(path), \"/local_cpus\");\n  path[PATH_MAX-1] = '\\0';\n  int fd;\n  SYSCHECKVAL(open(path, O_RDONLY), \"open\", fd);\n  char affinityStr[sizeof(cpu_set_t)*2 + 1];\n  int r = read(fd, affinityStr, sizeof(cpu_set_t)*2);\n  if (r > 0) {\n    affinityStr[r] = '\\0';\n    NCCLCHECK(ncclStrToCpuset(affinityStr, mask));\n  }\n  close(fd);\n  free(cudaPath);\n  return ncclSuccess;\n}\n\nNCCL_PARAM(IgnoreCpuAffinity, \"IGNORE_CPU_AFFINITY\", 0);\n\nstatic ncclResult_t setCpuAffinity(int cudaDev) {\n  // Query the CPU affinity set we were provided\n  cpu_set_t mask;\n  SYSCHECK(sched_getaffinity(0, sizeof(cpu_set_t), &mask), \"sched_getaffinity\");\n\n#ifdef ENABLE_TRACE\n  {\n    char affinityStr[sizeof(cpu_set_t)*2];\n    NCCLCHECK(ncclCpusetToStr(&mask, affinityStr));\n    TRACE(NCCL_INIT, \"Current affinity for GPU %d is %s\", cudaDev, affinityStr);\n  }\n#endif\n\n  // Find the CPUs that are local to the supplied GPU\n  cpu_set_t gpuMask;\n  NCCLCHECK(getCpuGpuAffinity(cudaDev, &gpuMask));\n\n#ifdef ENABLE_TRACE\n  {\n    char affinityStr[sizeof(cpu_set_t)*2];\n    NCCLCHECK(ncclCpusetToStr(&gpuMask, affinityStr));\n    TRACE(NCCL_INIT, \"CPU GPU affinity for GPU %d is %s\", cudaDev, affinityStr);\n  }\n#endif\n\n  cpu_set_t finalMask;\n  if (ncclParamIgnoreCpuAffinity())\n    // Ignore the CPU affinity set and use the GPU one instead\n    finalMask = gpuMask;\n  else\n    // Use a subset of the GPU affinity set\n    CPU_AND(&finalMask, &mask, &gpuMask);\n\n  // If there is a non empty set, use it to set affinity\n  if (CPU_COUNT(&finalMask)) {\n    char affinityStr[sizeof(cpu_set_t)*2];\n    NCCLCHECK(ncclCpusetToStr(&finalMask, affinityStr));\n    INFO(NCCL_INIT, \"Setting affinity for GPU %d to %s\", cudaDev, affinityStr);\n    SYSCHECK(sched_setaffinity(0, sizeof(cpu_set_t), &finalMask), \"sched_setaffinity\");\n  }\n  return ncclSuccess;\n}\n\nncclResult_t ncclCommInitRankSync(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  cpu_set_t affinitySave;\n  sched_getaffinity(0, sizeof(cpu_set_t), &affinitySave);\n\n  NCCLCHECK(wrapNvmlSymbols());\n  NCCLCHECK(wrapNvmlInit());\n\n  // Make sure all host memory allocation are close to the GPU\n  CUDACHECK(cudaSetDevice(cudaDev));\n  NCCLCHECK(setCpuAffinity(cudaDev));\n  ncclResult_t res;\n\n  NCCLCHECKGOTO(commAlloc(newcomm, nranks, myrank), res, cleanup);\n  NCCLCHECKGOTO(initTransportsRank(*newcomm, &commId), res, cleanup);\n  NCCLCHECKGOTO(devCommSetup(*newcomm), res, cleanup);\n\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  NCCLCHECKGOTO(wrapNvmlShutdown(), res, cleanup);\n\n  INFO(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x - Init COMPLETE\", *newcomm, myrank, nranks, (*newcomm)->cudaDev, (*newcomm)->busId);\n\n  return ncclSuccess;\ncleanup:\n  if ((*newcomm) && (*newcomm)->bootstrap) bootstrapAbort((*newcomm)->bootstrap);\n  *newcomm = NULL;\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  return res;\n}\n\nstatic ncclResult_t ncclCommInitRankDev(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n  char* env = getenv(\"NCCL_COMM_ID\");\n  if (env && myrank == 0) {\n    NCCLCHECKGOTO(bootstrapCreateRoot(&commId, true), res, end);\n  }\n\n  NCCLCHECKGOTO(ncclInit(), res, end);\n  if (myrank == 0) showVersion();\n\n  // Make sure the CUDA runtime is initialized.\n  CUDACHECKGOTO(cudaFree(NULL), res, end);\n\n  NCCLCHECKGOTO(PtrCheck(newcomm, \"CommInitRank\", \"newcomm\"), res, end);\n  if (nranks < 1 || myrank < 0 || myrank >= nranks) {\n    WARN(\"Invalid rank requested : %d/%d\", myrank, nranks);\n    res = ncclInvalidArgument;\n    goto end;\n  }\n\n  if (ncclAsyncMode()) {\n    NCCLCHECKGOTO(ncclAsyncInit(ncclCommInitRankSync, newcomm, nranks, commId, myrank, cudaDev), res, end);\n  } else {\n    NCCLCHECKGOTO(ncclCommInitRankSync(newcomm, nranks, commId, myrank, cudaDev), res, end);\n  }\nend:\n  if (ncclAsyncMode()) return ncclAsyncErrCheck(res);\n  else return res;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitRank, ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank);\nncclResult_t ncclCommInitRank(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank) {\n  int cudaDev;\n  CUDACHECK(cudaGetDevice(&cudaDev));\n  NCCLCHECK(ncclCommInitRankDev(newcomm, nranks, commId, myrank, cudaDev));\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitAll, ncclComm_t* comms, int ndev, const int* devlist);\nncclResult_t ncclCommInitAll(ncclComm_t* comms, int ndev, const int* devlist) {\n  NCCLCHECK(PtrCheck(comms, \"CommInitAll\", \"comms\"));\n  if (ndev < 0) {\n    WARN(\"Invalid device count requested : %d\", ndev);\n    return ncclInvalidArgument;\n  }\n\n  ncclUniqueId uniqueId;\n  NCCLCHECK(ncclGetUniqueId(&uniqueId));\n  NCCLCHECK(ncclGroupStart());\n  for (int i=0; i<ndev; i++) {\n    // Ignore return codes .. we need to call ncclGroupEnd to clean up anyway\n    ncclCommInitRankDev(comms+i, ndev, uniqueId, i, devlist ? devlist[i] : i);\n  }\n  NCCLCHECK(ncclGroupEnd());\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commDestroy(ncclComm_t comm) {\n  int savedDevice;\n#ifdef ENABLE_TRACE\n  int rank = comm->rank;\n#endif\n  CUDACHECK(cudaGetDevice(&savedDevice));\n  int commDevice = comm->cudaDev;\n\n  if (savedDevice != commDevice) {\n    CUDACHECK(cudaSetDevice(commDevice));\n  }\n\n  TRACE(NCCL_INIT, \"Destroying comm %p rank %d abortFlag %d fatalError %d\", comm, rank, *comm->abortFlag, comm->fatalError);\n\n  CUDACHECK(cudaStreamSynchronize(comm->groupStream));\n  NCCLCHECK(transportDestroyProxy(comm));\n  NCCLCHECK(commFree(comm));\n\n  if (savedDevice != commDevice)\n    CUDACHECK(cudaSetDevice(savedDevice));\n\n  TRACE(NCCL_INIT, \"Destroyed comm %p rank %d\", comm, rank);\n\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommDestroy, ncclComm_t comm);\nncclResult_t ncclCommDestroy(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  TRACE(NCCL_INIT, \"comm %p rank %d nRanks %d cudaDev %d busId %x\", comm, comm->rank, comm->nRanks, comm->cudaDev, comm->busId);\n\n  // Try and prevent a double free of the comm struct (user error)\n  if (comm->rank == -1 || comm->nRanks <= 0 || comm->cudaDev == -1 || comm->busId == -1) {\n    WARN(\"comm %p has already been destroyed\", comm);\n    return ncclInvalidArgument;\n  }\n\n  return commDestroy(comm);\n}\n\nNCCL_API(ncclResult_t, ncclCommAbort, ncclComm_t comm);\nncclResult_t ncclCommAbort(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n\n  // Ask anything that might still be running on the device to quit\n  *comm->abortFlag = 1;\n\n  return commDestroy(comm);\n}\n\nNCCL_API(const char*, ncclGetErrorString, ncclResult_t code);\nconst char* ncclGetErrorString(ncclResult_t code) {\n  switch (code) {\n    case ncclSuccess                : return \"no error\";\n    case ncclUnhandledCudaError     : return \"unhandled cuda error\";\n    case ncclSystemError            : return \"unhandled system error\";\n    case ncclInternalError          : return \"internal error\";\n    case ncclInvalidArgument        : return \"invalid argument\";\n    case ncclInvalidUsage           : return \"invalid usage\";\n    default                         : return \"unknown result code\";\n  }\n}\n\nNCCL_API(ncclResult_t, ncclCommGetAsyncError, ncclComm_t comm, ncclResult_t *asyncError);\nncclResult_t ncclCommGetAsyncError(ncclComm_t comm, ncclResult_t *asyncError) {\n  NCCLCHECK(PtrCheck(comm, \"ncclGetAsyncError\", \"comm\"));\n  NCCLCHECK(PtrCheck(asyncError, \"ncclGetAsyncError\", \"asyncError\"));\n\n  // Check device reported error\n  static ncclDevError_t printedDevErr = ncclDevSuccess;\n  switch(*comm->fatalDevError) {\n    case ncclDevSuccess :\n      break;\n    case ncclDevAssertedMismatch :\n      if (printedDevErr != ncclDevAssertedMismatch) {\n        WARN(\"Mismatched collective detected, please check your collective calls at and around rank %d. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\", comm->rank);\n        printedDevErr = ncclDevAssertedMismatch;\n      }\n      if (comm->fatalError == ncclSuccess) {\n        comm->fatalError = ncclInvalidUsage;\n      }\n      break;\n    case ncclDevSuspectedMismatch :\n      if (printedDevErr != ncclDevSuspectedMismatch) {\n        WARN(\"Your program may be hanging, this may be caused by a collective mismatch around rank %d. Please check your collective calls at and around this rank. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\", comm->rank);\n        printedDevErr = ncclDevSuspectedMismatch;\n      }\n      break;\n    default:\n      WARN(\"Unknown device error %d\", *comm->fatalDevError);\n      return ncclInternalError;\n  }\n  *asyncError = comm->fatalError;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCount, const ncclComm_t comm, int* count);\nncclResult_t ncclCommCount(const ncclComm_t comm, int* count) {\n  NCCLCHECK(PtrCheck(comm, \"CommCount\", \"comm\"));\n  NCCLCHECK(PtrCheck(count, \"CommCount\", \"count\"));\n  *count = comm->nRanks;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCuDevice, const ncclComm_t comm, int* devid);\nncclResult_t ncclCommCuDevice(const ncclComm_t comm, int* devid) {\n  NCCLCHECK(PtrCheck(comm, \"CommCuDevice\", \"comm\"));\n  NCCLCHECK(PtrCheck(devid, \"CommCuDevice\", \"devid\"));\n  *devid = comm->cudaDev;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommUserRank, const ncclComm_t comm, int* rank);\nncclResult_t ncclCommUserRank(const ncclComm_t comm, int* rank) {\n  NCCLCHECK(PtrCheck(comm, \"CommUserRank\", \"comm\"));\n  NCCLCHECK(PtrCheck(rank, \"CommUserRank\", \"rank\"));\n  *rank = comm->rank;\n  return ncclSuccess;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.5.6-2-rc24r6pmtkh4yen5el4y34cu3arhdmf5/spack-src/src/misc/nvmlwrap.cc": "/*************************************************************************\n * Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nvmlwrap.h\"\n\n#ifndef NVML_DIRECT\n#include <dlfcn.h>\n#include \"core.h\"\n\nstatic enum { nvmlUninitialized, nvmlInitializing, nvmlInitialized, nvmlError } nvmlState = nvmlUninitialized;\n\nstatic nvmlReturn_t (*nvmlInternalInit)(void);\nstatic nvmlReturn_t (*nvmlInternalShutdown)(void);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByPciBusId)(const char* pciBusId, nvmlDevice_t* device);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetIndex)(nvmlDevice_t device, unsigned* index);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByIndex)(unsigned int index, nvmlDevice_t* device);\nstatic const char* (*nvmlInternalErrorString)(nvmlReturn_t r);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkState)(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetPciInfo)(nvmlDevice_t device, nvmlPciInfo_t* pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkRemotePciInfo)(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkCapability)(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetMinorNumber)(nvmlDevice_t device, unsigned int* minorNumber);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetCudaComputeCapability)(nvmlDevice_t device, int* major, int* minor);\n\n// Used to make the NVML library calls thread safe\npthread_mutex_t nvmlLock = PTHREAD_MUTEX_INITIALIZER;\n\nncclResult_t wrapNvmlSymbols(void) {\n  if (nvmlState == nvmlInitialized)\n    return ncclSuccess;\n  if (nvmlState == nvmlError)\n    return ncclSystemError;\n\n  if (__sync_bool_compare_and_swap(&nvmlState, nvmlUninitialized, nvmlInitializing) == false) {\n    // Another thread raced in front of us. Wait for it to be done.\n    while (nvmlState == nvmlInitializing) pthread_yield();\n    return (nvmlState == nvmlInitialized) ? ncclSuccess : ncclSystemError;\n  }\n\n  static void* nvmlhandle = NULL;\n  void* tmp;\n  void** cast;\n\n  nvmlhandle=dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n  if (!nvmlhandle) {\n    WARN(\"Failed to open libnvidia-ml.so.1\");\n    goto teardown;\n  }\n\n#define LOAD_SYM(handle, symbol, funcptr) do {         \\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      WARN(\"dlsym failed on %s - %s\", symbol, dlerror());\\\n      goto teardown;                                     \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n#define LOAD_SYM_OPTIONAL(handle, symbol, funcptr) do {\\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      INFO(NCCL_INIT,\"dlsym failed on %s, ignoring\", symbol); \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n  LOAD_SYM(nvmlhandle, \"nvmlInit\", nvmlInternalInit);\n  LOAD_SYM(nvmlhandle, \"nvmlShutdown\", nvmlInternalShutdown);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByPciBusId\", nvmlInternalDeviceGetHandleByPciBusId);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetIndex\", nvmlInternalDeviceGetIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByIndex\", nvmlInternalDeviceGetHandleByIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlErrorString\", nvmlInternalErrorString);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetPciInfo\", nvmlInternalDeviceGetPciInfo);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetMinorNumber\", nvmlInternalDeviceGetMinorNumber);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkState\", nvmlInternalDeviceGetNvLinkState);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkRemotePciInfo\", nvmlInternalDeviceGetNvLinkRemotePciInfo);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkCapability\", nvmlInternalDeviceGetNvLinkCapability);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetCudaComputeCapability\", nvmlInternalDeviceGetCudaComputeCapability);\n\n  nvmlState = nvmlInitialized;\n  return ncclSuccess;\n\nteardown:\n  nvmlInternalInit = NULL;\n  nvmlInternalShutdown = NULL;\n  nvmlInternalDeviceGetHandleByPciBusId = NULL;\n  nvmlInternalDeviceGetIndex = NULL;\n  nvmlInternalDeviceGetHandleByIndex = NULL;\n  nvmlInternalDeviceGetPciInfo = NULL;\n  nvmlInternalDeviceGetMinorNumber = NULL;\n  nvmlInternalDeviceGetNvLinkState = NULL;\n  nvmlInternalDeviceGetNvLinkRemotePciInfo = NULL;\n  nvmlInternalDeviceGetNvLinkCapability = NULL;\n\n  if (nvmlhandle != NULL) dlclose(nvmlhandle);\n  nvmlState = nvmlError;\n  return ncclSystemError;\n}\n\n\nncclResult_t wrapNvmlInit(void) {\n  if (nvmlInternalInit == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalInit();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlInit() failed: %s\",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlShutdown(void) {\n  if (nvmlInternalShutdown == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalShutdown();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlShutdown() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByPciBusId(const char* pciBusId, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByPciBusId == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByPciBusId(pciBusId, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByPciBusId() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetIndex(nvmlDevice_t device, unsigned* index) {\n  if (nvmlInternalDeviceGetIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetIndex(device, index), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByIndex(unsigned int index, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByIndex(index, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetPciInfo(nvmlDevice_t device, nvmlPciInfo_t* pci) {\n  if (nvmlInternalDeviceGetPciInfo == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetPciInfo(device, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetPciInfo() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetMinorNumber(nvmlDevice_t device, unsigned int* minorNumber) {\n  if (nvmlInternalDeviceGetMinorNumber == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetMinorNumber(device, minorNumber), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetMinorNumber() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkState(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive) {\n  if (nvmlInternalDeviceGetNvLinkState == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkState(device, link, isActive), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkState() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkRemotePciInfo(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci) {\n  if (nvmlInternalDeviceGetNvLinkRemotePciInfo == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkRemotePciInfo(device, link, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkRemotePciInfo() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkCapability(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkCapability(device, link, capability, capResult), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkCapability() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetCudaComputeCapability(nvmlDevice_t device, int* major, int* minor) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetCudaComputeCapability(device, major, minor), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetCudaComputeCapability() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n#endif\n"
    },
    "skipped": [],
    "total_files": 92
}