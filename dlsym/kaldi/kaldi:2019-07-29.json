{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/cudamatrix/cu-allocator.cc": "// cudamatrix/cu-allocator.cc\n\n// Copyright      2015-2018  Johns Hopkins University (author: Daniel Povey)\n\n// See ../../COPYING for clarification regarding multiple authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//  http://www.apache.org/licenses/LICENSE-2.0\n//\n// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n// MERCHANTABLITY OR NON-INFRINGEMENT.\n// See the Apache 2 License for the specific language governing permissions and\n// limitations under the License.\n\n\n\n#include \"cudamatrix/cu-allocator.h\"\n\n#if HAVE_CUDA == 1\n\n#include <cublas_v2.h>\n#include <cuda.h>\n#include <cuda_runtime_api.h>\n\n#include <string>\n#include <vector>\n#include <algorithm>\n#ifndef _MSC_VER\n#include <dlfcn.h>\n#endif\n\n#include \"cudamatrix/cu-common.h\"\n#include \"cudamatrix/cu-device.h\"\n#include \"cudamatrix/cu-matrix.h\"\n#include \"base/kaldi-error.h\"\n#include \"base/kaldi-utils.h\"\n#include \"util/common-utils.h\"\n\nnamespace kaldi {\n\n\nvoid* CuMemoryAllocator::Malloc(size_t size) {\n  Timer tim;\n  if (!opts_.cache_memory) {\n    void *ans;\n    CU_SAFE_CALL(cudaMalloc(&ans, size));\n    double elapsed = tim.Elapsed();\n    tot_time_taken_ += elapsed;\n    malloc_time_taken_ += elapsed;\n    t_++;\n    return ans;\n  }\n\n  // We could perhaps change this to KALDI_PARANOID_ASSERT to save time.\n  KALDI_ASSERT(size != 0);\n\n  // Round up 'size' to a multiple of 256; this ensures the right kind of\n  // memory alignment.\n  size = (size + 255) & ~((size_t)255);\n  void *ans = MallocInternal(size);\n  tot_time_taken_ += tim.Elapsed();\n  return ans;\n}\n\n\nCuMemoryAllocator::MemoryBlock *CuMemoryAllocator::SplitBlock(\n    MemoryBlock *block, size_t size) {\n  SubRegion *subregion = block->subregion;\n  // new_block will become the right-most part of 'block', and 'block' will\n  // be the left-most part.\n  MemoryBlock *new_block = new MemoryBlock;\n  bool return_new_block;\n  char *new_begin;\n\n  // We now decide whether to make the left part of 'block' be of size ('size')\n  // and return it (the 'if' branch of the if-else block below), or the right\n  // part (the 'else' branch).  We decide this based on heuristics.  Basically,\n  // we want to allocate the sub-block that's either next to the edge of the\n  // MemoryRegion, or next to something that was allocated long ago (and which,\n  // we assume won't be deallocated for a relatively long time).  That is: we\n  // want to leave the un-allocated memory next to a memory block that was\n  // recently allocated (and thus is likely to be freed sooner), so that when\n  // that block is freed we can merge it with the still-unallocated piece into a\n  // larger block; this will reduce fragmentation.  But if this block spans\n  // multiple sub-regions we don't want to do that, as that would be against our\n  // heuristic of, where possible, allocating memory from lower-numbered\n  // sub-regions.\n  //\n  // Bear in mind that we can assume block->next and block->prev, if they are\n  // non-NULL, are both currently allocated, since 'block' is un-allocated and\n  // we would have merged any adjacent un-allocated sub-regions.\n  if (block->next != NULL && block->prev != NULL &&\n      block->prev->t < block->next->t &&\n      block->next->subregion == subregion) {\n    // We'll allocate the right part of the block, since the left side is next\n    // to a relatively recently-allocated block.\n    return_new_block = true;\n    new_begin = block->end - size;\n  } else {\n    // We'll allocate the left part of the block.\n    return_new_block = false;\n    new_begin = block->begin + size;\n  }\n\n  // The following code makes sure the SubRegion for 'new_block' is correct,\n  // i.e. its 'begin' is >= the 'begin' of the subregion and < the 'end' of the\n  // subregion.  If the following loop segfaults, it indicates a bug somewhere\n  // else.\n  while (new_begin >= subregion->end)\n    subregion = subregion->next;\n  MemoryBlock *next_block = block->next;\n  new_block->begin = new_begin;\n  new_block->end = block->end;\n  new_block->subregion = subregion;\n  new_block->allocated = false;\n  new_block->thread_id = block->thread_id;\n  new_block->t = block->t;\n  new_block->next = next_block;\n  new_block->prev = block;\n  if (next_block)\n    next_block->prev = new_block;\n  block->next = new_block;\n  block->end = new_begin;\n\n  // Add the split-up piece that we won't be allocating, to the\n  // 'free_blocks' member of its subregion.\n  if (return_new_block) {\n    AddToFreeBlocks(block);\n    return new_block;\n  } else {\n    AddToFreeBlocks(new_block);\n    return block;\n  }\n}\n\n\nvoid CuMemoryAllocator::RemoveFromFreeBlocks(MemoryBlock *block) {\n  SubRegion *subregion = block->subregion;\n  size_t block_size = block->end - block->begin;\n  std::pair<size_t, MemoryBlock*> p(block_size, block);\n  size_t num_removed = subregion->free_blocks.erase(p);\n  KALDI_ASSERT(num_removed != 0);\n  // Update largest_free_block_, if needed.\n  size_t subregion_index = subregion->subregion_index;\n  if (block_size == largest_free_block_[subregion_index]) {\n    if (subregion->free_blocks.empty())\n      largest_free_block_[subregion_index] = 0;\n    else\n      largest_free_block_[subregion_index] =\n          subregion->free_blocks.begin()->first;\n  }\n}\n\nvoid CuMemoryAllocator::AddToFreeBlocks(MemoryBlock *block) {\n  SubRegion *subregion = block->subregion;\n  KALDI_PARANOID_ASSERT(block->begin >= subregion->begin &&\n                        block->begin < subregion->end);\n  size_t block_size = block->end - block->begin,\n       subregion_index = subregion->subregion_index;\n  // Update largest_free_block_, if needed.\n  if (block_size > largest_free_block_[subregion_index]) {\n    largest_free_block_[subregion_index] = block_size;\n  }\n  subregion->free_blocks.insert(std::pair<size_t, MemoryBlock*>(block_size, block));\n}\n\n\nvoid* CuMemoryAllocator::MallocFromSubregion(SubRegion *subregion,\n                                             size_t size) {\n  // NULL is implementation defined and doesn't have to be zero so we can't\n  // guarantee that NULL will be <= a valid pointer-- so we cast to a pointer\n  // from zero instead of using NULL.\n  std::pair<size_t, MemoryBlock*> p(size, (MemoryBlock*)0);\n\n  std::set<std::pair<size_t, MemoryBlock*> >::iterator iter =\n      subregion->free_blocks.lower_bound(p);\n  // so now 'iter' is the first member of free_blocks whose size_t value is >=\n  // size.  If 'iter' was equal to the end() of that multi_map, it would be a\n  // bug because the calling code checked that the largest free block in this\n  // region was sufficiently large.  We don't check this; if it segfaults, we'll\n  // debug.\n\n  // search for a block that we don't have to synchronize on\n  int max_iters = 20;\n  auto search_iter = iter;\n  for (int32 i = 0;\n       search_iter != subregion->free_blocks.end() && i < max_iters;\n       ++i, ++search_iter) {\n    if (search_iter->second->thread_id == std::this_thread::get_id() ||\n        search_iter->second->t <= synchronize_gpu_t_) {\n      iter = search_iter;\n      break;\n    }\n  }\n\n  MemoryBlock *block = iter->second;\n  // Erase 'block' from its subregion's free blocks list... the next lines are\n  // similar to RemoveFromFreeBlocks(), but we code it directly as we have the\n  // iterator here, and it would be wasteful to do another lookup.\n  subregion->free_blocks.erase(iter);\n  // Update largest_free_block_, if needed.  The following few lines of code also appear\n  // in RemoveFromFreeBlocks().\n  size_t block_size = block->end - block->begin,\n      subregion_index = subregion->subregion_index;\n  if (block_size == largest_free_block_[subregion_index]) {\n    if (subregion->free_blocks.empty())\n      largest_free_block_[subregion_index] = 0;\n    else\n      largest_free_block_[subregion_index] =\n          subregion->free_blocks.begin()->first;\n  }\n\n  KALDI_PARANOID_ASSERT(block_size >= size && block->allocated == false);\n\n  // the most memory we allow to be 'wasted' by failing to split a block, is the\n  // smaller of: 1/16 of the size we're allocating, or half a megabyte.\n  size_t allowed_extra_size = std::min<size_t>(size >> 4, 524288);\n  if (block_size > size + allowed_extra_size) {\n    // If the requested block is substantially larger than what was requested,\n    // split it so we don't waste memory.\n    block = SplitBlock(block, size);\n  }\n\n  if (std::this_thread::get_id() != block->thread_id &&\n      block->t > synchronize_gpu_t_) {\n    // see NOTE ON SYNCHRONIZATION in the header.\n    SynchronizeGpu();\n    synchronize_gpu_t_ = t_;\n    num_synchronizations_++;\n  }\n  block->allocated = true;\n  block->t = t_;\n  allocated_block_map_[block->begin] = block;\n  allocated_memory_ += (block->end - block->begin);\n  if (allocated_memory_ > max_allocated_memory_) \n    max_allocated_memory_ = allocated_memory_;\n  return block->begin;\n}\n\n// By the time MallocInternal is called, we will have ensured that 'size' is\n// a nonzero multiple of 256 (for memory aligment reasons).\n// inline\nvoid* CuMemoryAllocator::MallocInternal(size_t size) {\nstart:\n  std::vector<size_t>::const_iterator iter = largest_free_block_.begin(),\n      end = largest_free_block_.end();\n  size_t subregion_index = 0;\n  for (; iter != end; ++iter, ++subregion_index) {\n    if (*iter > size) {\n      return MallocFromSubregion(subregions_[subregion_index], size);\n    }\n  }\n  // We dropped off the loop without finding a subregion with enough memory\n  // to satisfy the request -> allocate a new region.\n  AllocateNewRegion(size);\n  // An infinite loop shouldn't be possible because after calling\n  // AllocateNewRegion(size), there should always be a SubRegion\n  // with that size available.\n  goto start;\n}\n\n// Returns max(0, floor(log_2(i))).   Not tested independently.\nstatic inline size_t IntegerLog2(size_t i) {\n  size_t ans = 0;\n  while (i > 256) {\n    i >>= 8;\n    ans += 8;\n  }\n  while (i > 16) {\n    i >>= 4;\n    ans += 4;\n  }\n  while (i > 1) {\n    i >>= 1;\n    ans++;\n  }\n  return ans;\n}\n\nstd::string GetFreeGpuMemory(int64* free, int64* total) {\n#ifdef _MSC_VER\n  size_t mem_free, mem_total;\n  cuMemGetInfo_v2(&mem_free, &mem_total);\n#else\n  // define the function signature type\n  size_t mem_free, mem_total;\n  {\n    // we will load cuMemGetInfo_v2 dynamically from libcuda.so\n    // pre-fill ``safe'' values that will not cause problems\n    mem_free = 1; mem_total = 1;\n    // open libcuda.so\n    void* libcuda = dlopen(\"libcuda.so\", RTLD_LAZY);\n    if (NULL == libcuda) {\n      KALDI_WARN << \"cannot open libcuda.so\";\n    } else {\n      // define the function signature type\n      // and get the symbol\n      typedef CUresult (*cu_fun_ptr)(size_t*, size_t*);\n      cu_fun_ptr dl_cuMemGetInfo = (cu_fun_ptr)dlsym(libcuda,\"cuMemGetInfo_v2\");\n      if (NULL == dl_cuMemGetInfo) {\n        KALDI_WARN << \"cannot load cuMemGetInfo from libcuda.so\";\n      } else {\n        // call the function\n        dl_cuMemGetInfo(&mem_free, &mem_total);\n      }\n      // close the library\n      dlclose(libcuda);\n    }\n  }\n#endif\n  // copy the output values outside\n  if (NULL != free) *free = mem_free;\n  if (NULL != total) *total = mem_total;\n  // prepare the text output\n  std::ostringstream os;\n  os << \"free:\" << mem_free/(1024*1024) << \"M, \"\n     << \"used:\" << (mem_total-mem_free)/(1024*1024) << \"M, \"\n     << \"total:\" << mem_total/(1024*1024) << \"M, \"\n     << \"free/total:\" << mem_free/(float)mem_total;\n  return os.str();\n}\n\nvoid CuMemoryAllocator::PrintMemoryUsage() const {\n  if (!opts_.cache_memory) {\n    KALDI_LOG << \"Not caching allocations; time taken in \"\n              << \"malloc/free is \" << malloc_time_taken_\n              << \"/\" << (tot_time_taken_ - malloc_time_taken_)\n              << \", num operations is \" << t_\n              << \"; device memory info: \"\n              << GetFreeGpuMemory(NULL, NULL);\n    return;\n  }\n\n  size_t num_blocks_allocated = 0, num_blocks_free = 0,\n      memory_allocated = 0, memory_held = 0,\n      largest_free_block = 0, largest_allocated_block = 0;\n\n  for (size_t i = 0; i < memory_regions_.size(); i++) {\n    MemoryBlock *m = memory_regions_[i].block_begin;\n    KALDI_ASSERT(m->begin == memory_regions_[i].begin);\n    for (; m != NULL; m = m->next) {\n      size_t size = m->end - m->begin;\n      if (m->allocated) {\n        num_blocks_allocated++;\n        memory_allocated += size;\n        if (size > largest_allocated_block)\n          largest_allocated_block = size;\n      } else {\n        num_blocks_free++;\n        if (size > largest_free_block)\n          largest_free_block = size;\n      }\n      memory_held += size;\n      // The following is just some sanity checks; this code is rarely called so\n      // it's a reasonable place to put them.\n      if (m->next) {\n        KALDI_ASSERT(m->next->prev == m && m->end == m->next->begin);\n      } else {\n        KALDI_ASSERT(m->end == memory_regions_[m->subregion->memory_region].end);\n      }\n    }\n  }\n  KALDI_LOG << \"Memory usage: \" << memory_allocated << \"/\"\n            << memory_held << \" bytes currently allocated/total-held; \"\n            << num_blocks_allocated << \"/\" << num_blocks_free\n            << \" blocks currently allocated/free; largest \"\n            << \"free/allocated block sizes are \"\n            << largest_allocated_block << \"/\" << largest_free_block\n            << \"; time taken total/cudaMalloc is \"\n            << tot_time_taken_ << \"/\" << malloc_time_taken_\n            << \", synchronized the GPU \" << num_synchronizations_\n            << \" times out of \" << (t_/2) << \" frees; \"\n            << \"device memory info: \" << GetFreeGpuMemory(NULL, NULL)\n            << \"maximum allocated: \" << max_allocated_memory_  \n            << \"current allocated: \" << allocated_memory_; \n}\n\n// Note: we just initialize with the default options, but we can change it later\n// (as long as it's before we first use the class) by calling SetOptions().\nCuMemoryAllocator::CuMemoryAllocator():\n    opts_(CuAllocatorOptions()),\n    t_(0),\n    synchronize_gpu_t_(0),\n    num_synchronizations_(0),\n    tot_time_taken_(0.0),\n    malloc_time_taken_(0.0),\n    max_allocated_memory_(0),\n    allocated_memory_(0) {\n  // Note: we don't allocate any memory regions at the start; we wait for the user\n  // to call Malloc() or MallocPitch(), and then allocate one when needed.\n}\n\n\nvoid* CuMemoryAllocator::MallocPitch(size_t row_bytes,\n                                     size_t num_rows,\n                                     size_t *pitch) {\n  Timer tim;\n  if (!opts_.cache_memory) {\n    void *ans;\n    CU_SAFE_CALL(cudaMallocPitch(&ans, pitch, row_bytes, num_rows));\n    double elapsed = tim.Elapsed();\n    tot_time_taken_ += elapsed;\n    malloc_time_taken_ += elapsed;\n    return ans;\n  }\n\n  // Round up row_bytes to a multiple of 256.\n  row_bytes = (row_bytes + 255) & ~((size_t)255);\n  *pitch = row_bytes;\n  void *ans = MallocInternal(row_bytes * num_rows);\n  tot_time_taken_ += tim.Elapsed();\n  return ans;\n}\n\nvoid CuMemoryAllocator::Free(void *ptr) {\n  Timer tim;\n  if (!opts_.cache_memory) {\n    CU_SAFE_CALL(cudaFree(ptr));\n    tot_time_taken_ += tim.Elapsed();\n    t_++;\n    return;\n  }\n  t_++;\n  unordered_map<void*, MemoryBlock*>::iterator iter =\n      allocated_block_map_.find(ptr);\n  if (iter == allocated_block_map_.end()) {\n    KALDI_ERR << \"Attempt to free CUDA memory pointer that was not allocated: \"\n              << ptr;\n  }\n  MemoryBlock *block = iter->second;\n  allocated_memory_ -= (block->end - block->begin);\n  allocated_block_map_.erase(iter);\n  block->t = t_;\n  block->thread_id = std::this_thread::get_id();\n  block->allocated = false;\n\n  // If this is not the first block of the memory region and the previous block\n  // is not allocated, merge this block into the previous block.\n  MemoryBlock *prev_block = block->prev;\n  if (prev_block != NULL && !prev_block->allocated) {\n    RemoveFromFreeBlocks(prev_block);\n    prev_block->end = block->end;\n    if (prev_block->thread_id != block->thread_id) {\n      // the two blocks we're merging were freed by different threads, so we\n      // give the 'nonexistent thread' as their thread, which means that\n      // whichever thread requests that block, we force synchronization.  We can\n      // assume that prev_block was previously allocated (prev_block->t > 0)\n      // because we always start from the left when allocating blocks, and we\n      // know that this block was previously allocated.\n      prev_block->thread_id = std::thread::id();\n    }\n    prev_block->t = t_;\n    prev_block->next = block->next;\n    if (block->next)\n      block->next->prev = prev_block;\n    delete block;\n    block = prev_block;\n  }\n\n  // If this is not the last block of the memory region and the next block is\n  // not allocated, merge the next block into this block.\n  MemoryBlock *next_block = block->next;\n  if (next_block != NULL && !next_block->allocated) {\n    // merge next_block into 'block', deleting 'next_block'.  Note: at this\n    // point, if we merged with the previous block, the variable 'block' may now\n    // be pointing to that previous block, so it would be a 3-way merge.\n    RemoveFromFreeBlocks(next_block);\n    block->end = next_block->end;\n    if (next_block->thread_id != block->thread_id && next_block->t > 0) {\n      // the two blocks we're merging were freed by different threads, so we\n      // give the 'nonexistent thread' as their thread, which means that\n      // whichever thread requests that block, we force synchronization.  there\n      // is no need to do this if next_block->t == 0, which would mean it had\n      // never been allocated.\n      block->thread_id = std::thread::id();\n    }\n    // We don't need to inspect the 't' value of next_block; it can't be\n    // larger than t_ because t_ is now.\n    block->next = next_block->next;\n    if (block->next)\n      block->next->prev = block;\n    delete next_block;\n  }\n  AddToFreeBlocks(block);\n  tot_time_taken_ += tim.Elapsed();\n}\n\nvoid CuMemoryAllocator::AllocateNewRegion(size_t size) {\n  int64 free_memory, total_memory;\n  std::string mem_info = GetFreeGpuMemory(&free_memory, &total_memory);\n  opts_.Check();\n  size_t region_size = static_cast<size_t>(free_memory * opts_.memory_proportion);\n  if (region_size < size)\n    region_size = size;\n  // Round up region_size to an exact multiple of 1M (note: we expect it will\n  // be much larger than that).  1048575 is 2^20 - 1.\n  region_size = (region_size + 1048575) & ~((size_t)1048575);\n\n  if (!memory_regions_.empty()) {\n    // If this is not the first region allocated, print some information.\n    KALDI_LOG << \"About to allocate new memory region of \" << region_size\n              << \" bytes; current memory info is: \" << mem_info;\n  }\n  void *memory_region;\n  cudaError_t e;\n  {\n    Timer tim;\n    e = cudaMalloc(&memory_region, region_size);\n    malloc_time_taken_ += tim.Elapsed();\n  }\n  if (e != cudaSuccess) {\n    PrintMemoryUsage();\n    if (!CuDevice::Instantiate().IsComputeExclusive()) {\n      KALDI_ERR << \"Failed to allocate a memory region of \" << region_size\n                << \" bytes.  Possibly this is due to sharing the GPU.  Try \"\n                << \"switching the GPUs to exclusive mode (nvidia-smi -c 3) and using \"\n                << \"the option --use-gpu=wait to scripts like \"\n                << \"steps/nnet3/chain/train.py.  Memory info: \"\n                << mem_info\n                << \" CUDA error: '\" << cudaGetErrorString(e) << \"'\";\n    } else {\n      KALDI_ERR << \"Failed to allocate a memory region of \" << region_size\n                << \" bytes.  Possibly smaller minibatch size would help.  \"\n                << \"Memory info: \" << mem_info\n                << \" CUDA error: '\" << cudaGetErrorString(e) << \"'\";\n    }\n  }\n  // this_num_subregions would be approximately 'opts_.num_subregions' if\n  // 'region_size' was all the device's memory.  (We add one to round up).\n  // We're aiming to get a number of sub-regions approximately equal to\n  // opts_.num_subregions by the time we allocate all the device's memory.\n  size_t this_num_subregions = 1 +\n      (region_size * opts_.num_subregions) / total_memory;\n\n  size_t memory_region_index = memory_regions_.size();\n  memory_regions_.resize(memory_region_index + 1);\n  MemoryRegion &this_region = memory_regions_.back();\n\n  this_region.begin = static_cast<char*>(memory_region);\n  this_region.end = this_region.begin + region_size;\n  // subregion_size will be hundreds of megabytes.\n  size_t subregion_size = region_size / this_num_subregions;\n\n  std::vector<SubRegion*> new_subregions;\n  char* subregion_begin = static_cast<char*>(memory_region);\n  for (size_t i = 0; i < this_num_subregions; i++) {\n    SubRegion *subregion = new SubRegion();\n    subregion->memory_region = memory_region_index;\n    subregion->begin = subregion_begin;\n    if (i + 1 == this_num_subregions) {\n      subregion->end = this_region.end;\n      KALDI_ASSERT(subregion->end > subregion->begin);\n    } else {\n      subregion->end = subregion_begin + subregion_size;\n      subregion_begin = subregion->end;\n    }\n    subregion->next = NULL;\n    if (i > 0) {\n      new_subregions.back()->next = subregion;\n    }\n    new_subregions.push_back(subregion);\n  }\n  // Initially the memory is in a single block, owned by\n  // the first subregion.  It will be split up gradually.\n  MemoryBlock *block = new MemoryBlock();\n  block->begin = this_region.begin;\n  block->end = this_region.end;\n  block->subregion = new_subregions.front();\n  block->allocated = false;\n  block->t = 0; // was never allocated.\n  block->next = NULL;\n  block->prev = NULL;\n  for (size_t i = 0; i < this_num_subregions; i++)\n    subregions_.push_back(new_subregions[i]);\n  SortSubregions();\n  this_region.block_begin = block;\n\n  AddToFreeBlocks(block);\n}\n\n// We sort the sub-regions according to the distance between the start of the\n// MemoryRegion of which they are a part, and the start of the SubRegion.  This\n// will generally mean that the highest-numbered SubRegion-- the one we keep\n// free at all costs-- will be the end of the first block which we allocated\n// (which under most situations will be the largest block).\nvoid CuMemoryAllocator::SortSubregions() {\n  largest_free_block_.resize(subregions_.size());\n\n  std::vector<std::pair<size_t, SubRegion*> > pairs;\n  for (size_t i = 0; i < subregions_.size(); i++) {\n    SubRegion *subregion = subregions_[i];\n    MemoryRegion &memory_region = memory_regions_[subregion->memory_region];\n    size_t distance = subregion->begin - memory_region.begin;\n    pairs.push_back(std::pair<size_t, SubRegion*>(distance, subregion));\n  }\n  std::sort(pairs.begin(), pairs.end());\n  for (size_t i = 0; i < subregions_.size(); i++) {\n    subregions_[i] = pairs[i].second;\n    subregions_[i]->subregion_index = i;\n    if (subregions_[i]->free_blocks.empty())\n      largest_free_block_[i] = 0;\n    else\n      largest_free_block_[i] = subregions_[i]->free_blocks.begin()->first;\n  }\n}\n\nCuMemoryAllocator::~CuMemoryAllocator() {\n  // We mainly free these blocks of memory so that cuda-memcheck doesn't report\n  // spurious errors.\n  for (size_t i = 0; i < memory_regions_.size(); i++) {\n    // No need to check the return status here-- the program is exiting anyway.\n    cudaFree(memory_regions_[i].begin);\n  }\n  for (size_t i = 0; i < subregions_.size(); i++) {\n    SubRegion *subregion = subregions_[i];\n    for (auto iter = subregion->free_blocks.begin();\n         iter != subregion->free_blocks.end(); ++iter)\n      delete iter->second;\n    delete subregion;\n  }\n}\n\n\nCuMemoryAllocator g_cuda_allocator;\n\n\n}  // namespace kaldi\n\n\n#endif // HAVE_CUDA\n\n\nnamespace kaldi {\n\n// Define/initialize this global variable.  It was declared in cu-allocator.h.\n// This has to be done outside of the ifdef, because we register the options\n// whether or not CUDA is compiled in (so that the binaries accept the same\n// options).\nCuAllocatorOptions g_allocator_options;\n\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/cudamatrix/cu-device.cc": "// cudamatrix/cu-device.cc\n\n// Copyright 2009-2012  Karel Vesely\n//                2013  Lucas Ondel\n//           2013-2015  Johns Hopkins University (author: Daniel Povey)\n//                2015  Guoguo Chen\n\n// See ../../COPYING for clarification regarding multiple authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//  http://www.apache.org/licenses/LICENSE-2.0\n//\n// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n// MERCHANTABLITY OR NON-INFRINGEMENT.\n// See the Apache 2 License for the specific language governing permissions and\n// limitations under the License.\n\n\n\n#if HAVE_CUDA == 1\n#include <cublas_v2.h>\n#include <cuda.h>\n#include <cuda_runtime_api.h>\n\n#include <string>\n#include <vector>\n#include <algorithm>\n#ifndef _MSC_VER\n#include <dlfcn.h>\n#endif\n\n#include \"cudamatrix/cu-common.h\"\n#include \"cudamatrix/cu-device.h\"\n#include \"cudamatrix/cu-matrix.h\"\n#include \"base/kaldi-error.h\"\n#include \"base/kaldi-utils.h\"\n#include \"util/common-utils.h\"\n#include \"util/kaldi-io.h\"\n// the following is for cuda_legacy_noop().\n#include \"cudamatrix/cu-kernels-ansi.h\"\n\nnamespace kaldi {\n\n/// This function attempts to get a CUDA device context on some available device\n/// by doing 'cudaFree(0)'.  If it succeeds it returns true; if it fails, it\n/// outputs some debugging information into 'debug_str' and returns false.\nstatic bool GetCudaContext(int32 num_gpus, std::string *debug_str) {\n  // Our first attempt to get a device context is: we do cudaFree(0) and see if\n  // that returns no error code.  If it succeeds then we have a device\n  // context.  Apparently this is the canonical way to get a context.\n  if (cudaFree(0) == 0) {\n    cudaGetLastError();  // Clear any error status.\n    return true;\n  }\n\n  // The rest of this code represents how we used to get a device context, but\n  // now its purpose is mainly a debugging one.\n  std::ostringstream debug_stream;\n  debug_stream << \"num-gpus=\" << num_gpus << \". \";\n  for (int32 device = 0; device < num_gpus; device++) {\n    cudaSetDevice(device);\n    cudaError_t e = cudaFree(0);  // CUDA context gets created here.\n    if (e == cudaSuccess) {\n      if (debug_str)\n        *debug_str = debug_stream.str();\n      cudaGetLastError();  // Make sure the error state doesn't get returned in\n                           // the next cudaGetLastError().\n      return true;\n    }\n    debug_stream << \"Device \" << device << \": \" << cudaGetErrorString(e) << \".  \";\n  }\n  if (debug_str)\n    *debug_str = debug_stream.str();\n  return false;\n}\n\n\nvoid CuDevice::Initialize() {\n  // This function may be called in the following two situations:\n  //\n  // (1) in the main thread, only when a GPU is not currently being used, either\n  // within a call like CuDevice()::Instantiate().SelectGpuId(..)\n  // (where the Instantiate() call will call Initialize() before SelectGpuId()\n  // is called, just because of how Instantiate() works), or in a call\n  // to 'CuDevice::Instantiate().Enabled()'.  In this case it will just\n  // set initialized_ to true and notice that device_id_ == 1, and do nothing.\n  //\n  // (2) in threads created by the user, as soon as someone calls something that\n  //   might potentially use the GPU, via CuDevice()::Instantiate().\n  //   If device_id_ is >= 0, this will create the cuBLAS and cuSparse handles.\n  KALDI_ASSERT(!initialized_);\n  initialized_ = true;\n  if (device_id_ == -1) {\n    // There is nothing to do; we are not using a GPU.\n    return;\n  } else {\n    if (!multi_threaded_) {\n      multi_threaded_ = true;\n      KALDI_WARN << \"For multi-threaded code that might use GPU, you should call \"\n          \"CuDevice::Instantiate().AllowMultithreading() at the start of \"\n          \"the program.\";\n    }\n    device_id_copy_ = device_id_;\n    cudaSetDevice(device_id_);\n    // Initialize CUBLAS.\n    CUBLAS_SAFE_CALL(cublasCreate(&cublas_handle_));\n    CUBLAS_SAFE_CALL(cublasSetStream(cublas_handle_, cudaStreamPerThread));\n\n#if CUDA_VERSION >= 9010\n    CUSOLVER_SAFE_CALL(cusolverDnCreate(&cusolverdn_handle_));\n    CUSOLVER_SAFE_CALL(cusolverDnSetStream(cusolverdn_handle_, \n            cudaStreamPerThread));\n#endif\n    \n#if CUDA_VERSION >= 9000 \n    if (device_options_.use_tensor_cores) {\n      // Enable tensor cores in CUBLAS\n      // Note if the device does not support tensor cores this will fall back to normal math mode\n      CUBLAS_SAFE_CALL(cublasSetMathMode(cublas_handle_, \n            CUBLAS_TENSOR_OP_MATH));\n    }\n#endif\n\n    // Initialize the cuSPARSE library\n    CUSPARSE_SAFE_CALL(cusparseCreate(&cusparse_handle_));\n    CUSPARSE_SAFE_CALL(cusparseSetStream(cusparse_handle_, cudaStreamPerThread));\n\n    // Initialize the generator,\n    CURAND_SAFE_CALL(curandCreateGenerator(\n          &curand_handle_, CURAND_RNG_PSEUDO_DEFAULT));\n    // To get same random sequence, call srand() before the constructor is invoked,\n    CURAND_SAFE_CALL(curandSetGeneratorOrdering(\n          curand_handle_, CURAND_ORDERING_PSEUDO_DEFAULT));\n    CURAND_SAFE_CALL(curandSetStream(curand_handle_, cudaStreamPerThread));\n    SeedGpu();\n  }\n}\n\nvoid CuDevice::SelectGpuId(std::string use_gpu) {\n  if (device_id_ != -1) {\n    KALDI_ERR << \"You cannot call SelectGpuId twice if, on the first time, \"\n        \"you requested a GPU.\";\n  }\n  if (use_gpu != \"yes\" && use_gpu != \"no\" && use_gpu != \"optional\" && use_gpu != \"wait\") {\n    KALDI_ERR << \"Please choose : --use-gpu=yes|no|optional|wait, passed '\" << use_gpu << \"'\";\n  }\n  if (use_gpu == \"no\") {\n    KALDI_LOG << \"Manually selected to compute on CPU.\";\n    return;\n  }\n  // Check that we have a gpu available\n  int32 num_gpus = 0;\n\n  cudaError_t e = cudaGetDeviceCount(&num_gpus);\n\n  // Make sure the global allocator object has the up-to-date options.\n  g_cuda_allocator.SetOptions(g_allocator_options);\n\n  if (num_gpus == 0) {\n    if (use_gpu == \"yes\" || use_gpu == \"wait\") {\n      KALDI_CUDA_ERR(e, \"No CUDA GPU detected!\");\n    }\n    if (use_gpu == \"optional\") {\n      KALDI_WARN << \"No CUDA GPU detected; running on CPU since --use-gpu=optional specified.\";\n      return;\n    }\n  }\n\n  // Create a CUDA context.\n  std::string debug_str;\n  bool got_context = GetCudaContext(num_gpus, &debug_str);\n\n  if (use_gpu != \"wait\") {\n    if (!got_context) {\n      // So far no we don't have context, sleep a bit and retry.\n      int32 sec_sleep = (use_gpu == \"yes\" ? 20 : 2);\n      KALDI_WARN << \"Will try again to get a GPU after \" << sec_sleep\n                 << \" seconds.\";\n      Sleep(sec_sleep);\n      if (!GetCudaContext(num_gpus, &debug_str)) {\n        if (use_gpu == \"yes\") {\n          {\n            Input input;\n            input.Open(\"nvidia-smi 1>&2 |\");\n          }\n          KALDI_LOG << debug_str;\n          KALDI_ERR << \"Failed to create CUDA context, no more unused GPUs? \";\n        }\n        if (use_gpu == \"optional\") {\n          KALDI_WARN << \"Running on CPU!!! No more unused CUDA GPUs?\";\n          return;\n        }\n      }\n    }\n  } else {\n    int32 num_times = 0;\n    BaseFloat wait_time = 0.0;\n    while (!got_context) {\n      int32 sec_sleep = 5;\n      if (num_times == 0)\n        KALDI_WARN << \"Will try again indefinitely every \" << sec_sleep\n                   << \" seconds to get a GPU.\";\n      num_times++;\n      wait_time += sec_sleep;\n      Sleep(sec_sleep);\n      got_context = GetCudaContext(num_gpus, NULL);\n    }\n\n    KALDI_WARN << \"Waited \" << wait_time\n               << \" seconds before creating CUDA context\";\n  }\n\n  // Double check that we have the context\n  KALDI_ASSERT(cudaSuccess == cudaDeviceSynchronize());\n\n  // Check if the machine use compute exclusive mode\n  if (IsComputeExclusive()) {\n    KALDI_LOG << \"CUDA setup operating under Compute Exclusive Mode.\";\n    FinalizeActiveGpu();\n    return;\n  } else {\n    // Suggest to use compute exclusive mode\n    KALDI_WARN << \"Not in compute-exclusive mode.  Suggestion: use \"\n        \"'nvidia-smi -c 3' to set compute exclusive mode\";\n    // We want to choose the device more carefully, so release the CUDA context.\n    e = cudaDeviceReset();\n    if (e != cudaSuccess) {\n      KALDI_CUDA_ERR(e, \"Failed to release CUDA context on a GPU\");\n    }\n\n    // And select the GPU according to proportion of free memory\n    if (SelectGpuIdAuto()) {\n      FinalizeActiveGpu();\n      return;\n    } else {\n      // We could not get a GPU the second time, after prevously having the CUDA\n      // context.  Strange but not impossible.\n      if (use_gpu == \"yes\") {\n        KALDI_ERR << \"Error acquiring GPU.\";\n      }\n      if (use_gpu == \"optional\") {\n        KALDI_WARN << \"Running on CPU!!! Error acquiring GPU.\";\n        return;\n      }\n    }\n  }\n}\n\n\nvoid CuDevice::FinalizeActiveGpu() {\n  // The device at this point should have an active GPU, so we can query its\n  // name and memory stats and notify user which GPU is being used.\n\n  // Get the device-id of the active device.\n  {\n    int device_id;\n    cudaError_t e = cudaGetDevice(&device_id);\n    if (e != cudaSuccess) {\n      KALDI_CUDA_ERR(e, \"Failed to get device-id of active device.\");\n    }\n    device_id_ = device_id;\n    device_id_copy_ = device_id;\n    initialized_ = true;  // Prevent Initialize() from being called on this,\n                          // the main thread.\n    // Initialize CUBLAS.\n    CUBLAS_SAFE_CALL(cublasCreate(&cublas_handle_));\n    CUBLAS_SAFE_CALL(cublasSetStream(cublas_handle_, cudaStreamPerThread));\n    \n#if CUDA_VERSION >= 9010 \n    CUSOLVER_SAFE_CALL(cusolverDnCreate(&cusolverdn_handle_));\n    CUSOLVER_SAFE_CALL(cusolverDnSetStream(cusolverdn_handle_,\n            cudaStreamPerThread));\n#endif\n\n#if CUDA_VERSION >= 9000 \n    if (device_options_.use_tensor_cores) {\n      // Enable tensor cores in CUBLAS\n      // Note if the device does not support tensor cores this will fall back to normal math mode\n      CUBLAS_SAFE_CALL(cublasSetMathMode(cublas_handle_, \n            CUBLAS_TENSOR_OP_MATH));\n    }\n#endif\n\n    \n    // Initialize the cuSPARSE library\n    CUSPARSE_SAFE_CALL(cusparseCreate(&cusparse_handle_));\n    CUSPARSE_SAFE_CALL(cusparseSetStream(cusparse_handle_, cudaStreamPerThread));\n    \n    // Initialize the generator,\n    CURAND_SAFE_CALL(curandCreateGenerator(\n          &curand_handle_, CURAND_RNG_PSEUDO_DEFAULT));\n    // To get same random sequence, call srand() before the constructor is invoked,\n    CURAND_SAFE_CALL(curandSetGeneratorOrdering(\n          curand_handle_, CURAND_ORDERING_PSEUDO_DEFAULT));\n    SeedGpu();\n\n    // Notify the user which GPU is being userd.\n    char name[128];\n    DeviceGetName(name,128, device_id);\n\n    CU_SAFE_CALL(cudaGetDeviceProperties(&properties_, device_id));\n\n    KALDI_LOG << \"The active GPU is [\" << device_id << \"]: \" << name << \"\\t\"\n              << GetFreeGpuMemory(&free_memory_at_startup_, NULL) << \" version \"\n              << properties_.major << \".\" << properties_.minor;\n  }\n  return;\n}\n\nbool CuDevice::DoublePrecisionSupported() {\n  if (!Enabled()) return true;\n  return properties_.major > 1 || (properties_.major == 1 && properties_.minor >= 3);\n  // Double precision is supported from version 1.3\n}\n\n\nbool CuDevice::IsComputeExclusive() {\n  // assume we already have an CUDA context created\n  KALDI_ASSERT(cudaSuccess == cudaDeviceSynchronize());\n\n  // get the device-id and its device-properties\n  int gpu_id = -1;\n  cudaError_t e = cudaGetDevice(&gpu_id);\n  if (e != cudaSuccess) {\n    KALDI_CUDA_ERR(e, \"Failed to get current device\");\n  }\n  struct cudaDeviceProp gpu_prop;\n  e = cudaGetDeviceProperties(&gpu_prop, gpu_id);\n  if (e != cudaSuccess) {\n    KALDI_CUDA_ERR(e,  \"Failed to get device properties\");\n  }\n  // find out whether compute exclusive mode is used\n  switch (gpu_prop.computeMode) {\n    case cudaComputeModeExclusive :\n      return true;\n      break;\n    case cudaComputeModeExclusiveProcess :\n      return true;\n      break;\n    default :\n      // in this case we release the GPU context...\n      return false;\n  }\n}\n\ntemplate<typename TA, typename TB>\nbool greater_pair(const std::pair<TA, TB> &left, const std::pair<TA, TB>& right) {\n  return left.second > right.second;\n}\n\nbool CuDevice::SelectGpuIdAuto() {\n  // Check that we have at least one gpu\n  int32 num_gpus = 0;\n  cudaError_t e = cudaGetDeviceCount(&num_gpus);\n  if (num_gpus == 0) {\n    KALDI_WARN << \"No CUDA devices found\";\n    if (e != cudaSuccess) {\n      KALDI_WARN << \"cudaGetDeviceCount() returned \" << e\n                 <<\", meaning: \\\"\" << cudaGetErrorString(e)  << \"\\\"\";\n    }\n    return false;\n  }\n\n  // The GPU is selected according to maximal free memory ratio\n  std::vector< std::pair<int, float> > free_mem_ratio(num_gpus);\n\n  // Get ratios of memory use, if possible\n  KALDI_LOG << \"Selecting from \" << num_gpus << \" GPUs\";\n  for(int32 n = 0; n < num_gpus; n++) {\n    int32 ret = cudaSetDevice(n);\n    switch(ret) {\n      case cudaSuccess : {\n        // create the CUDA context for the thread\n        cudaDeviceSynchronize();\n        // get GPU name\n        char name[128];\n        DeviceGetName(name,128,n);\n        // get GPU memory stats\n        int64 free, total;\n        std::string mem_stats;\n        mem_stats = GetFreeGpuMemory(&free, &total);\n        // log\n        KALDI_LOG << \"cudaSetDevice(\" << n << \"): \"\n                  << name << \"\\t\" << mem_stats;\n\n        // We have seen that in some cases GetFreeGpuMemory returns zero\n        // That will produce nan after division, which might confuse\n        // the sorting routine. Or maybe not, but let's keep it clean\n        if (total <= 0) {\n          KALDI_LOG << \"Total memory reported for device \" << n\n                    << \" is zero (or less).\";\n        }\n        float mem_ratio = total > 0 ? free/(float)total : 0;\n        free_mem_ratio[n] = std::make_pair(n, mem_ratio);\n\n        // destroy the CUDA context for the thread\n        cudaDeviceReset();\n      } break;\n      case cudaErrorDeviceAlreadyInUse :\n        KALDI_LOG << \"cudaSetDevice(\" << n << \"): \"\n                  << \"Device cannot be accessed, used EXCLUSIVE-THREAD mode...\";\n        break;\n      case cudaErrorInvalidDevice :\n        KALDI_LOG << \"cudaSetDevice(\" << n << \"): \"\n                  << \"Device cannot be accessed, not a VALID CUDA device!\";\n        break;\n      default :\n        KALDI_LOG << \"cudaSetDevice(\" << n << \"): \"\n                  << \"returned \" << ret << \", \"\n                  << cudaGetErrorString((cudaError_t)ret);\n    }\n  }\n  // find GPU with max free memory\n  int32 max_id=0;\n  std::sort(free_mem_ratio.begin(), free_mem_ratio.end(),\n            greater_pair<int, float>);\n  // the free_mem_ratio should be bigger than zero\n  KALDI_ASSERT(free_mem_ratio[max_id].second > 0.0);\n\n  int dev_id;\n  float mem_ratio;\n  do {\n    // try to select the GPU in the best to worst order\n    // Note we have to check the return codes manually, as the CU_SAFE_CALL\n    // contains call to KALDI_ERR (which will cause the program to abort)\n\n    dev_id = free_mem_ratio[max_id].first;\n    mem_ratio = free_mem_ratio[max_id].second;\n\n    KALDI_LOG << \"Trying to select device: \" << dev_id << \" (automatically), mem_ratio: \" << mem_ratio;\n    e = cudaSetDevice(dev_id);\n    if (e != cudaSuccess) {\n      KALDI_WARN << \"Cannot select this device: return code \" << e\n                 << \", Error message: \\\"\" << cudaGetErrorString(e) << \"\\\"\";\n    } else {\n      e = cudaDeviceSynchronize();\n      if (e != cudaSuccess) {\n        KALDI_WARN << \"Cannot select this device: return code \" << e\n                   << \", Error message: \\\"\" << cudaGetErrorString(e) << \"\\\"\";\n      }\n    }\n    max_id++;\n  } while ((e != cudaSuccess) && (max_id < free_mem_ratio.size()));\n\n  if (e != cudaSuccess) {\n    KALDI_WARN << \"Failed to (automatically) select any device\";\n    return false;\n  }\n  KALDI_LOG << \"Success selecting device \" << dev_id << \" free mem ratio: \" << mem_ratio;\n  return true;\n}\n\n\nvoid CuDevice::AccuProfile(const char *function_name,\n                           const CuTimer &timer) {\n  if (GetVerboseLevel() >= 1) {\n    std::unique_lock<std::mutex> lock(profile_mutex_, std::defer_lock_t());\n    if (multi_threaded_)\n      lock.lock();\n    std::string key(function_name);\n    // by passing 0 as the stream to cudaStreamSynchronize, we are using the\n    // per-thread default stream.  Since we compile with\n    // -DCUDA_API_PER_THREAD_DEFAULT_STREAM, this equates to a per-thread\n    // stream.\n    CU_SAFE_CALL(cudaStreamSynchronize(0));\n    double elapsed = timer.Elapsed();\n    if (profile_map_.find(key) == profile_map_.end())\n      profile_map_[key] = elapsed;\n    else\n      profile_map_[key] += elapsed;\n  }\n}\n\nvoid CuDevice::PrintMemoryUsage() const {\n  if (Enabled())\n    g_cuda_allocator.PrintMemoryUsage();\n}\n\nvoid CuDevice::PrintProfile() {\n  if (GetVerboseLevel() >= 1) {\n    std::ostringstream os;\n    os << \"-----\\n[cudevice profile]\\n\";\n    unordered_map<std::string, double, StringHasher>::iterator it;\n    std::vector<std::pair<double, std::string> > pairs;\n    double total_time = 0.0;\n    for(it = profile_map_.begin(); it != profile_map_.end(); ++it) {\n      std::string function_name = it->first;\n      double elapsed_time = it->second;\n      total_time += elapsed_time;\n      pairs.push_back(std::make_pair(elapsed_time, function_name));\n    }\n    // display from shortest to longest time, so tail will show the longest\n    // times at the end.\n    std::sort(pairs.begin(), pairs.end());\n    size_t max_print = 15, start_pos = (pairs.size() <= max_print ?\n                                        0 : pairs.size() - max_print);\n    for (size_t i = start_pos; i < pairs.size(); i++)\n      os << pairs[i].second << \"\\t\" << pairs[i].first << \"s\\n\";\n    os << \"Total GPU time:\\t\" << total_time << \"s (may involve some double-counting)\\n\";\n    os << \"-----\";\n    KALDI_LOG << os.str();\n    PrintMemoryUsage();\n  }\n}\n\n\nvoid CuDevice::DeviceGetName(char* name, int32 len, int32 dev) {\n  // prefill with something reasonable\n  strncpy(name,\"Unknown GPU\",len);\n#ifdef _MSC_VER\n  cuDeviceGetName(name, len, dev);\n#else\n  // open libcuda.so\n  void* libcuda = dlopen(\"libcuda.so\",RTLD_LAZY);\n  if (NULL == libcuda) {\n    KALDI_WARN << \"cannot open libcuda.so\";\n  } else {\n    // define the function signature type\n    typedef CUresult (*cu_fun_ptr)(char*,int,CUdevice);\n    // get the symbol\n    cu_fun_ptr cuDeviceGetName_ptr = (cu_fun_ptr)dlsym(libcuda,\"cuDeviceGetName\");\n    if (NULL == cuDeviceGetName_ptr) {\n      KALDI_WARN << \"cannot load cuDeviceGetName from libcuda.so\";\n    } else {\n      // call the function\n      cuDeviceGetName_ptr(name, len, dev);\n    }\n    // close the library\n    dlclose(libcuda);\n  }\n#endif\n}\n\n\nvoid CuDevice::CheckGpuHealth() {\n  if (!Enabled()) return;\n  CuTimer t;\n  // prepare small matrices for a quick test\n  Matrix<BaseFloat> a(50, 100);\n  Matrix<BaseFloat> b(100 ,50);\n  a.SetRandn();\n  b.SetRandUniform();\n  // multiply 2 small matrices in CPU:\n  Matrix<BaseFloat> c(50, 50);\n  c.AddMatMat(1.0, a, kNoTrans, b, kNoTrans, 0.0);\n  // multiply same matrices in GPU:\n  CuMatrix<BaseFloat> c1(50, 50);\n  c1.AddMatMat(1.0, CuMatrix<BaseFloat>(a), kNoTrans, CuMatrix<BaseFloat>(b), kNoTrans, 0.0);\n  // check that relative differnence is <1%\n  AssertEqual(c, Matrix<BaseFloat>(c1), 0.01);\n  // measure time spent in this check\n  AccuProfile(__func__, t);\n}\n\nCuDevice::CuDevice():\n    initialized_(false),\n    device_id_copy_(-1),\n    cublas_handle_(NULL),\n    cusparse_handle_(NULL),\n    cusolverdn_handle_(NULL) {\n}\n\nCuDevice::~CuDevice() {\n  if (cublas_handle_)\n    CUBLAS_SAFE_CALL(cublasDestroy(cublas_handle_));\n  if (cusparse_handle_)\n    CUSPARSE_SAFE_CALL(cusparseDestroy(cusparse_handle_));\n  if (curand_handle_) {\n    CURAND_SAFE_CALL(curandDestroyGenerator(curand_handle_));\n  }\n#if CUDA_VERSION >= 9010\n  if (cusolverdn_handle_) {\n    CUSOLVER_SAFE_CALL(cusolverDnDestroy(cusolverdn_handle_));\n  }\n#endif\n}\n\n\n// Each thread has its own copy of the CuDevice object.\n// Note: this was declared \"static\".\nthread_local CuDevice CuDevice::this_thread_device_;\n  \nCuDevice::CuDeviceOptions CuDevice::device_options_;\n\n// define and initialize the static members of the CuDevice object.\nint32 CuDevice::device_id_ = -1;\nbool CuDevice::multi_threaded_ = false;\nunordered_map<std::string, double, StringHasher> CuDevice::profile_map_;\nstd::mutex CuDevice::profile_mutex_;\nint64 CuDevice::free_memory_at_startup_;\ncudaDeviceProp CuDevice::properties_;\nbool CuDevice::debug_stride_mode_ = false;\n\n\nvoid SynchronizeGpu() {\n  cuda_legacy_noop();\n  CU_SAFE_CALL(cudaGetLastError());\n}\n\n}  // namespace kaldi\n\n#else  // #if HAVE_CUDA == 1\n\nnamespace kaldi {\n// SynchronizeGpu() does nothing if we didn't compile for GPU.\nvoid SynchronizeGpu() { }\n}\n\n#endif  // #if HAVE_CUDA == 1\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/doc/KaldiModels.pptx",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/doc/KaldiScripts.pptx",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/doc/KaldiMatrix.pptx",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/doc/Kaldi.pptx",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fbank_htk.2",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fbank_htk.4",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fbank_htk.1",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fbank_htk.3",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fea_htk.1",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fea_htk.2",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fea_htk.6",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fea_htk.3",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fea_htk.4",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.plp_htk.1",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/src/feat/test_data/test.wav.fea_htk.5",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/.git/objects/pack/pack-2a5c55b15a5a31de6b3710ba9272b03fbdce7536.pack",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/.git/objects/pack/pack-2a5c55b15a5a31de6b3710ba9272b03fbdce7536.idx",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/egs/ami/s5/local/english.glm",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/egs/ami/s5b/local/english.glm",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/egs/gp/s5/local/gp_norm_trans_CZ.pl",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/windows/NewGuidCmd.exe",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/papers/asru11_toolkit_poster/kaldi-poster.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/papers/asru11_toolkit_poster/figures/KaldiTextAndLogo.png",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/papers/asru11_toolkit_poster/figures/kaldi-lib.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/papers/asru11_toolkit/kaldi_asru.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/papers/asru11_toolkit/figs/kaldi-lib.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/logo/KaldiLogo.docx",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/logo/KaldiTextAndLogoSmall.png",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/logo/KaldiTextAndLogo.png",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/logo/KaldiIco.png",
        "/tmp/vanessa/spack-stage/spack-stage-kaldi-2019-07-29-z7mwwajongxijkxtseblwx27byfwdcxa/spack-src/misc/logo/KaldiLogo.png"
    ],
    "total_files": 6340
}