{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/src/tbb_wrapper.c": "/*\n * Copyright (C) 2017 Intel Corporation.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * 1. Redistributions of source code must retain the above copyright notice(s),\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice(s),\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n * EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <memkind/internal/memkind_default.h>\n#include <memkind/internal/memkind_log.h>\n#include <memkind/internal/memkind_private.h>\n#include <memkind/internal/tbb_wrapper.h>\n#include <memkind/internal/tbb_mem_pool_policy.h>\n#include <limits.h>\n\n#include <stdint.h>\n#include <errno.h>\n#include <stdio.h>\n#include <sys/mman.h>\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <string.h>\n\nvoid *(*pool_malloc)(void*, size_t);\nvoid *(*pool_realloc)(void*, void *, size_t);\nvoid *(*pool_aligned_malloc)(void*, size_t, size_t);\nbool (*pool_free)(void*, void *);\nint (*pool_create_v1)(intptr_t, const struct MemPoolPolicy*, void**);\nbool (*pool_destroy)(void*);\nvoid* (*pool_identify)(void *object);\n\nstatic void* tbb_handle = NULL;\n\nstatic int load_tbb_symbols()\n{\n    const char so_name[]=\"libtbbmalloc.so.2\";\n    tbb_handle = dlopen(so_name, RTLD_LAZY);\n    if(!tbb_handle) {\n        log_err(\"%s not found.\", so_name);\n        return -1;\n    }\n\n    pool_malloc = dlsym(tbb_handle, \"_ZN3rml11pool_mallocEPNS_10MemoryPoolEm\");\n    pool_realloc = dlsym(tbb_handle, \"_ZN3rml12pool_reallocEPNS_10MemoryPoolEPvm\");\n    pool_aligned_malloc = dlsym(tbb_handle, \"_ZN3rml19pool_aligned_mallocEPNS_10MemoryPoolEmm\");\n    pool_free = dlsym(tbb_handle, \"_ZN3rml9pool_freeEPNS_10MemoryPoolEPv\");\n    pool_create_v1 = dlsym(tbb_handle, \"_ZN3rml14pool_create_v1ElPKNS_13MemPoolPolicyEPPNS_10MemoryPoolE\");\n    pool_destroy = dlsym(tbb_handle, \"_ZN3rml12pool_destroyEPNS_10MemoryPoolE\");\n    pool_identify = dlsym(tbb_handle, \"_ZN3rml13pool_identifyEPv\");\n\n    if(!pool_malloc ||\n        !pool_realloc ||\n        !pool_aligned_malloc ||\n        !pool_free ||\n        !pool_create_v1 ||\n        !pool_destroy ||\n        !pool_identify)\n\n    {\n        log_err(\"Could not find symbols in %s.\", so_name);\n        dlclose(tbb_handle);\n        return -1;\n    }\n\n    return 0;\n}\n\n//Granularity of raw_alloc allocations\n#define GRANULARITY 2*1024*1024\nstatic void *raw_alloc(intptr_t pool_id, size_t* bytes/*=n*GRANULARITY*/)\n{\n   void* ptr = kind_mmap((struct memkind*)pool_id, NULL, *bytes);\n   return (ptr==MAP_FAILED) ? NULL : ptr;\n}\n\nstatic int raw_free(intptr_t pool_id, void* raw_ptr, size_t raw_bytes)\n{\n  return munmap(raw_ptr, raw_bytes);\n}\n\nstatic void *tbb_pool_malloc(struct memkind* kind, size_t size)\n{\n    if(size_out_of_bounds(size)) return NULL;\n    void* result = pool_malloc(kind->priv, size);\n    if (!result)\n        errno = ENOMEM;\n    return result;\n}\n\nstatic void *tbb_pool_calloc(struct memkind *kind, size_t num, size_t size)\n{\n    if (size_out_of_bounds(num) || size_out_of_bounds(size)) return NULL;\n\n    const size_t array_size = num*size;\n    if (array_size/num != size) {\n        errno = ENOMEM;\n        return NULL;\n    }\n    void *result = pool_malloc(kind->priv, array_size);\n    if (result) {\n        memset(result, 0, array_size);\n    }\n    else {\n        errno = ENOMEM;\n    }\n    return result;\n}\n\nstatic void *tbb_pool_realloc(struct memkind *kind, void *ptr, size_t size)\n{\n    if(size_out_of_bounds(size)) return NULL;\n    void *result = pool_realloc(kind->priv, ptr, size);\n    if (!result && size)\n        errno = ENOMEM;\n    return result;\n}\n\nstatic int tbb_pool_posix_memalign(struct memkind *kind, void **memptr, size_t alignment, size_t size)\n{\n    //Check if alignment is \"at least as large as sizeof(void *)\".\n    if(!alignment && (0 != (alignment & (alignment-sizeof(void*))))) return EINVAL;\n    //Check if alignment is \"a power of 2\".\n    if(alignment & (alignment-1)) return EINVAL;\n    if(size_out_of_bounds(size)) return ENOMEM;\n    void *result = pool_aligned_malloc(kind->priv, size, alignment);\n    if (!result) {\n        return ENOMEM;\n    }\n    *memptr = result;\n    return 0;\n}\n\nvoid tbb_pool_free(struct memkind *kind, void *ptr)\n{\n    if(kind) {\n        pool_free(kind->priv, ptr);\n    } else {\n        pool_free(pool_identify(ptr), ptr);\n    }\n}\n\nstatic int tbb_destroy(struct memkind* kind)\n{\n    bool pool_destroy_ret = pool_destroy(kind->priv);\n    dlclose(tbb_handle);\n\n    if(!pool_destroy_ret) {\n        log_err(\"TBB pool destroy failure.\");\n        return MEMKIND_ERROR_OPERATION_FAILED;\n    }\n    return MEMKIND_SUCCESS;\n}\n\nvoid tbb_initialize(struct memkind *kind)\n{\n    if(!kind || load_tbb_symbols()) {\n        log_fatal(\"Failed to initialize TBB.\");\n        abort();\n    }\n\n    struct MemPoolPolicy policy = {\n        .pAlloc = raw_alloc,\n        .pFree = raw_free,\n        .granularity = GRANULARITY,\n        .version = 1,\n        .fixedPool = false,\n        .keepAllMemory = false,\n        .reserved = 0\n    };\n\n    pool_create_v1((intptr_t)kind, &policy, &kind->priv);\n    if (!kind->priv) {\n        log_fatal(\"Unable to create TBB memory pool.\");\n        abort();\n    }\n\n    kind->ops->malloc = tbb_pool_malloc;\n    kind->ops->calloc = tbb_pool_calloc;\n    kind->ops->posix_memalign = tbb_pool_posix_memalign;\n    kind->ops->realloc = tbb_pool_realloc;\n    kind->ops->free = tbb_pool_free;\n    kind->ops->finalize = tbb_destroy;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/test/dlopen_test.cpp": "/*\n * Copyright (C) 2017 Intel Corporation.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * 1. Redistributions of source code must retain the above copyright notice(s),\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice(s),\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n * EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n#include \"allocator_perf_tool/HugePageOrganizer.hpp\"\n\n#include <dlfcn.h>\n\n#include \"common.h\"\n\nclass DlopenTest: public :: testing::Test\n{\nprotected:\n    DlopenTest()\n    {\n        const char *path = \"/usr/lib64/libmemkind.so\";\n        if (!pathExists(path))\n        {\n            path = \"/usr/lib/libmemkind.so\";\n        }\n        dlerror();\n        handle = dlopen(path, RTLD_LAZY);\n        assert((handle != NULL && dlerror() == NULL) && \"Couldn't open libmemkind.so\");\n        memkind_malloc = (memkind_malloc_t)dlsym(handle, \"memkind_malloc\");\n        assert(dlerror() == NULL && \"Couldn't get memkind_malloc from memkind library\");\n        memkind_free = (memkind_free_t)dlsym(handle, \"memkind_free\");\n        assert(dlerror() == NULL && \"Couldn't get memkind_free from memkind library\");\n    }\n\n    ~DlopenTest()\n    {\n        dlclose(handle);\n    }\n\n    void test(const char* kind_name, size_t alloc_size)\n    {\n        void** kind_ptr = (void**)dlsym(handle, kind_name);\n        EXPECT_TRUE(dlerror() == NULL) << \"Couldn't get kind from memkind library\";\n        EXPECT_TRUE(kind_ptr != NULL) << \"Kind ptr to memkind library is NULL\";\n\n        void* allocation_ptr = memkind_malloc((*kind_ptr), alloc_size);\n        EXPECT_TRUE(allocation_ptr != NULL) << \"Allocation with memkind_malloc failed\";\n\n        memset(allocation_ptr, 0, alloc_size);\n\n        memkind_free((*kind_ptr), allocation_ptr);\n    }\n\n    bool pathExists(const char *p)\n    {\n        struct stat info;\n        if (0 != stat(p, &info))\n        {\n            return false;\n        }\n        return true;\n    }\n\nprivate:\n    void* handle;\n    typedef void* (*memkind_malloc_t)(void*, size_t);\n    typedef void (*memkind_free_t)(void*, void*);\n    memkind_malloc_t memkind_malloc;\n    memkind_free_t memkind_free;\n};\n\nTEST_F(DlopenTest, test_TC_MEMKIND_DEFAULT_4194305_bytes)\n{\n    test(\"MEMKIND_DEFAULT\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_4194305_bytes)\n{\n    test(\"MEMKIND_HBW\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_HUGETLB_4194305_bytes)\n{\n    HugePageOrganizer huge_page_organizer(8);\n    test(\"MEMKIND_HBW_HUGETLB\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_PREFERRED_4194305_bytes)\n{\n    test(\"MEMKIND_HBW_PREFERRED\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_INTERLEAVE_4194305_bytes)\n{\n    test(\"MEMKIND_HBW_INTERLEAVE\", 4194305);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/test/load_tbbmalloc_symbols.c": "/*\n * Copyright (C) 2016 Intel Corporation.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * 1. Redistributions of source code must retain the above copyright notice(s),\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice(s),\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n * EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"tbbmalloc.h\"\n\nint load_tbbmalloc_symbols()\n{\n    const char so_name[]=\"libtbbmalloc.so.2\";\n    void* tbb_handle = dlopen(so_name, RTLD_LAZY);\n    if(!tbb_handle) {\n       printf(\"Cannot load %s\\n\", so_name);\n       return -1;\n    }\n\n    scalable_malloc = dlsym(tbb_handle, \"scalable_malloc\");\n    if(!scalable_malloc) {\n        printf(\"Cannot load scalable_malloc symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_realloc = dlsym(tbb_handle, \"scalable_realloc\");\n    if(!scalable_realloc) {\n        printf(\"Cannot load scalable_realloc symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_calloc = dlsym(tbb_handle, \"scalable_calloc\");\n    if(!scalable_calloc) {\n        printf(\"Cannot load scalable_calloc symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_free = dlsym(tbb_handle, \"scalable_free\");\n    if(!scalable_free) {\n        printf(\"Cannot load scalable_free symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    return 0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/jemalloc/configure.ac": "dnl Process this file with autoconf to produce a configure script.\nAC_INIT([Makefile.in])\n\nAC_CONFIG_AUX_DIR([build-aux])\n\ndnl ============================================================================\ndnl Custom macro definitions.\n\ndnl JE_CONCAT_VVV(r, a, b)\ndnl \ndnl Set $r to the concatenation of $a and $b, with a space separating them iff\ndnl both $a and $b are non-emty.\nAC_DEFUN([JE_CONCAT_VVV],\nif test \"x[$]{$2}\" = \"x\" -o \"x[$]{$3}\" = \"x\" ; then\n  $1=\"[$]{$2}[$]{$3}\"\nelse\n  $1=\"[$]{$2} [$]{$3}\"\nfi\n)\n\ndnl JE_APPEND_VS(a, b)\ndnl \ndnl Set $a to the concatenation of $a and b, with a space separating them iff\ndnl both $a and b are non-empty.\nAC_DEFUN([JE_APPEND_VS],\n  T_APPEND_V=$2\n  JE_CONCAT_VVV($1, $1, T_APPEND_V)\n)\n\nCONFIGURE_CFLAGS=\nSPECIFIED_CFLAGS=\"${CFLAGS}\"\ndnl JE_CFLAGS_ADD(cflag)\ndnl \ndnl CFLAGS is the concatenation of CONFIGURE_CFLAGS and SPECIFIED_CFLAGS\ndnl (ignoring EXTRA_CFLAGS, which does not impact configure tests.  This macro\ndnl appends to CONFIGURE_CFLAGS and regenerates CFLAGS.\nAC_DEFUN([JE_CFLAGS_ADD],\n[\nAC_MSG_CHECKING([whether compiler supports $1])\nT_CONFIGURE_CFLAGS=\"${CONFIGURE_CFLAGS}\"\nJE_APPEND_VS(CONFIGURE_CFLAGS, $1)\nJE_CONCAT_VVV(CFLAGS, CONFIGURE_CFLAGS, SPECIFIED_CFLAGS)\nAC_COMPILE_IFELSE([AC_LANG_PROGRAM(\n[[\n]], [[\n    return 0;\n]])],\n              [je_cv_cflags_added=$1]\n              AC_MSG_RESULT([yes]),\n              [je_cv_cflags_added=]\n              AC_MSG_RESULT([no])\n              [CONFIGURE_CFLAGS=\"${T_CONFIGURE_CFLAGS}\"]\n)\nJE_CONCAT_VVV(CFLAGS, CONFIGURE_CFLAGS, SPECIFIED_CFLAGS)\n])\n\ndnl JE_CFLAGS_SAVE()\ndnl JE_CFLAGS_RESTORE()\ndnl \ndnl Save/restore CFLAGS.  Nesting is not supported.\nAC_DEFUN([JE_CFLAGS_SAVE],\nSAVED_CONFIGURE_CFLAGS=\"${CONFIGURE_CFLAGS}\"\n)\nAC_DEFUN([JE_CFLAGS_RESTORE],\nCONFIGURE_CFLAGS=\"${SAVED_CONFIGURE_CFLAGS}\"\nJE_CONCAT_VVV(CFLAGS, CONFIGURE_CFLAGS, SPECIFIED_CFLAGS)\n)\n\nCONFIGURE_CXXFLAGS=\nSPECIFIED_CXXFLAGS=\"${CXXFLAGS}\"\ndnl JE_CXXFLAGS_ADD(cxxflag)\nAC_DEFUN([JE_CXXFLAGS_ADD],\n[\nAC_MSG_CHECKING([whether compiler supports $1])\nT_CONFIGURE_CXXFLAGS=\"${CONFIGURE_CXXFLAGS}\"\nJE_APPEND_VS(CONFIGURE_CXXFLAGS, $1)\nJE_CONCAT_VVV(CXXFLAGS, CONFIGURE_CXXFLAGS, SPECIFIED_CXXFLAGS)\nAC_COMPILE_IFELSE([AC_LANG_PROGRAM(\n[[\n]], [[\n    return 0;\n]])],\n              [je_cv_cxxflags_added=$1]\n              AC_MSG_RESULT([yes]),\n              [je_cv_cxxflags_added=]\n              AC_MSG_RESULT([no])\n              [CONFIGURE_CXXFLAGS=\"${T_CONFIGURE_CXXFLAGS}\"]\n)\nJE_CONCAT_VVV(CXXFLAGS, CONFIGURE_CXXFLAGS, SPECIFIED_CXXFLAGS)\n])\n\ndnl JE_COMPILABLE(label, hcode, mcode, rvar)\ndnl \ndnl Use AC_LINK_IFELSE() rather than AC_COMPILE_IFELSE() so that linker errors\ndnl cause failure.\nAC_DEFUN([JE_COMPILABLE],\n[\nAC_CACHE_CHECK([whether $1 is compilable],\n               [$4],\n               [AC_LINK_IFELSE([AC_LANG_PROGRAM([$2],\n                                                [$3])],\n                               [$4=yes],\n                               [$4=no])])\n])\n\ndnl ============================================================================\n\nCONFIG=`echo ${ac_configure_args} | sed -e 's#'\"'\"'\\([^ ]*\\)'\"'\"'#\\1#g'`\nAC_SUBST([CONFIG])\n\ndnl Library revision.\nrev=2\nAC_SUBST([rev])\n\nsrcroot=$srcdir\nif test \"x${srcroot}\" = \"x.\" ; then\n  srcroot=\"\"\nelse\n  srcroot=\"${srcroot}/\"\nfi\nAC_SUBST([srcroot])\nabs_srcroot=\"`cd \\\"${srcdir}\\\"; pwd`/\"\nAC_SUBST([abs_srcroot])\n\nobjroot=\"\"\nAC_SUBST([objroot])\nabs_objroot=\"`pwd`/\"\nAC_SUBST([abs_objroot])\n\ndnl Munge install path variables.\nif test \"x$prefix\" = \"xNONE\" ; then\n  prefix=\"/usr/local\"\nfi\nif test \"x$exec_prefix\" = \"xNONE\" ; then\n  exec_prefix=$prefix\nfi\nPREFIX=$prefix\nAC_SUBST([PREFIX])\nBINDIR=`eval echo $bindir`\nBINDIR=`eval echo $BINDIR`\nAC_SUBST([BINDIR])\nINCLUDEDIR=`eval echo $includedir`\nINCLUDEDIR=`eval echo $INCLUDEDIR`\nAC_SUBST([INCLUDEDIR])\nLIBDIR=`eval echo $libdir`\nLIBDIR=`eval echo $LIBDIR`\nAC_SUBST([LIBDIR])\nDATADIR=`eval echo $datadir`\nDATADIR=`eval echo $DATADIR`\nAC_SUBST([DATADIR])\nMANDIR=`eval echo $mandir`\nMANDIR=`eval echo $MANDIR`\nAC_SUBST([MANDIR])\n\ndnl Support for building documentation.\nAC_PATH_PROG([XSLTPROC], [xsltproc], [false], [$PATH])\nif test -d \"/usr/share/xml/docbook/stylesheet/docbook-xsl\" ; then\n  DEFAULT_XSLROOT=\"/usr/share/xml/docbook/stylesheet/docbook-xsl\"\nelif test -d \"/usr/share/sgml/docbook/xsl-stylesheets\" ; then\n  DEFAULT_XSLROOT=\"/usr/share/sgml/docbook/xsl-stylesheets\"\nelse\n  dnl Documentation building will fail if this default gets used.\n  DEFAULT_XSLROOT=\"\"\nfi\nAC_ARG_WITH([xslroot],\n  [AS_HELP_STRING([--with-xslroot=<path>], [XSL stylesheet root path])], [\nif test \"x$with_xslroot\" = \"xno\" ; then\n  XSLROOT=\"${DEFAULT_XSLROOT}\"\nelse\n  XSLROOT=\"${with_xslroot}\"\nfi\n],\n  XSLROOT=\"${DEFAULT_XSLROOT}\"\n)\nAC_SUBST([XSLROOT])\n\ndnl If CFLAGS isn't defined, set CFLAGS to something reasonable.  Otherwise,\ndnl just prevent autoconf from molesting CFLAGS.\nCFLAGS=$CFLAGS\nAC_PROG_CC\n\nif test \"x$GCC\" != \"xyes\" ; then\n  AC_CACHE_CHECK([whether compiler is MSVC],\n                 [je_cv_msvc],\n                 [AC_COMPILE_IFELSE([AC_LANG_PROGRAM([],\n                                                     [\n#ifndef _MSC_VER\n  int fail[-1];\n#endif\n])],\n                               [je_cv_msvc=yes],\n                               [je_cv_msvc=no])])\nfi\n\ndnl check if a cray prgenv wrapper compiler is being used\nje_cv_cray_prgenv_wrapper=\"\"\nif test \"x${PE_ENV}\" != \"x\" ; then\n  case \"${CC}\" in\n    CC|cc)\n\tje_cv_cray_prgenv_wrapper=\"yes\"\n\t;;\n    *)\n       ;;\n  esac\nfi\n\nAC_CACHE_CHECK([whether compiler is cray],\n              [je_cv_cray],\n              [AC_COMPILE_IFELSE([AC_LANG_PROGRAM([],\n                                                  [\n#ifndef _CRAYC\n  int fail[-1];\n#endif\n])],\n                            [je_cv_cray=yes],\n                            [je_cv_cray=no])])\n\nif test \"x${je_cv_cray}\" = \"xyes\" ; then\n  AC_CACHE_CHECK([whether cray compiler version is 8.4],\n                [je_cv_cray_84],\n                [AC_COMPILE_IFELSE([AC_LANG_PROGRAM([],\n                                                      [\n#if !(_RELEASE_MAJOR == 8 && _RELEASE_MINOR == 4)\n  int fail[-1];\n#endif\n])],\n                              [je_cv_cray_84=yes],\n                              [je_cv_cray_84=no])])\nfi\n\nif test \"x$GCC\" = \"xyes\" ; then\n  JE_CFLAGS_ADD([-std=gnu11])\n  if test \"x$je_cv_cflags_added\" = \"x-std=gnu11\" ; then\n    AC_DEFINE_UNQUOTED([JEMALLOC_HAS_RESTRICT])\n  else\n    JE_CFLAGS_ADD([-std=gnu99])\n    if test \"x$je_cv_cflags_added\" = \"x-std=gnu99\" ; then\n      AC_DEFINE_UNQUOTED([JEMALLOC_HAS_RESTRICT])\n    fi\n  fi\n  JE_CFLAGS_ADD([-Wall])\n  JE_CFLAGS_ADD([-Wshorten-64-to-32])\n  JE_CFLAGS_ADD([-Wsign-compare])\n  JE_CFLAGS_ADD([-Wundef])\n  JE_CFLAGS_ADD([-pipe])\n  JE_CFLAGS_ADD([-g3])\nelif test \"x$je_cv_msvc\" = \"xyes\" ; then\n  CC=\"$CC -nologo\"\n  JE_CFLAGS_ADD([-Zi])\n  JE_CFLAGS_ADD([-MT])\n  JE_CFLAGS_ADD([-W3])\n  JE_CFLAGS_ADD([-FS])\n  JE_APPEND_VS(CPPFLAGS, -I${srcdir}/include/msvc_compat)\nfi\nif test \"x$je_cv_cray\" = \"xyes\" ; then\n  dnl cray compiler 8.4 has an inlining bug\n  if test \"x$je_cv_cray_84\" = \"xyes\" ; then\n    JE_CFLAGS_ADD([-hipa2])\n    JE_CFLAGS_ADD([-hnognu])\n  fi\n  dnl ignore unreachable code warning\n  JE_CFLAGS_ADD([-hnomessage=128])\n  dnl ignore redefinition of \"malloc\", \"free\", etc warning\n  JE_CFLAGS_ADD([-hnomessage=1357])\nfi\nAC_SUBST([CONFIGURE_CFLAGS])\nAC_SUBST([SPECIFIED_CFLAGS])\nAC_SUBST([EXTRA_CFLAGS])\nAC_PROG_CPP\n\nAC_ARG_ENABLE([cxx],\n  [AS_HELP_STRING([--disable-cxx], [Disable C++ integration])],\nif test \"x$enable_cxx\" = \"xno\" ; then\n  enable_cxx=\"0\"\nelse\n  enable_cxx=\"1\"\nfi\n,\nenable_cxx=\"1\"\n)\nif test \"x$enable_cxx\" = \"x1\" ; then\n  dnl Require at least c++14, which is the first version to support sized\n  dnl deallocation.  C++ support is not compiled otherwise.\n  m4_include([m4/ax_cxx_compile_stdcxx.m4])\n  AX_CXX_COMPILE_STDCXX([14], [noext], [optional])\n  if test \"x${HAVE_CXX14}\" = \"x1\" ; then\n    JE_CXXFLAGS_ADD([-Wall])\n    JE_CXXFLAGS_ADD([-g3])\n\n    SAVED_LIBS=\"${LIBS}\"\n    JE_APPEND_VS(LIBS, -lstdc++)\n    JE_COMPILABLE([libstdc++ linkage], [\n#include <stdlib.h>\n], [[\n\tint *arr = (int *)malloc(sizeof(int) * 42);\n\tif (arr == NULL)\n\t\treturn 1;\n]], [je_cv_libstdcxx])\n    if test \"x${je_cv_libstdcxx}\" = \"xno\" ; then\n      LIBS=\"${SAVED_LIBS}\"\n    fi\n  else\n    enable_cxx=\"0\"\n  fi\nfi\nAC_SUBST([enable_cxx])\nAC_SUBST([CONFIGURE_CXXFLAGS])\nAC_SUBST([SPECIFIED_CXXFLAGS])\nAC_SUBST([EXTRA_CXXFLAGS])\n\nAC_C_BIGENDIAN([ac_cv_big_endian=1], [ac_cv_big_endian=0])\nif test \"x${ac_cv_big_endian}\" = \"x1\" ; then\n  AC_DEFINE_UNQUOTED([JEMALLOC_BIG_ENDIAN], [ ])\nfi\n\nif test \"x${je_cv_msvc}\" = \"xyes\" -a \"x${ac_cv_header_inttypes_h}\" = \"xno\"; then\n  JE_APPEND_VS(CPPFLAGS, -I${srcdir}/include/msvc_compat/C99)\nfi\n\nif test \"x${je_cv_msvc}\" = \"xyes\" ; then\n  LG_SIZEOF_PTR=LG_SIZEOF_PTR_WIN\n  AC_MSG_RESULT([Using a predefined value for sizeof(void *): 4 for 32-bit, 8 for 64-bit])\nelse\n  AC_CHECK_SIZEOF([void *])\n  if test \"x${ac_cv_sizeof_void_p}\" = \"x8\" ; then\n    LG_SIZEOF_PTR=3\n  elif test \"x${ac_cv_sizeof_void_p}\" = \"x4\" ; then\n    LG_SIZEOF_PTR=2\n  else\n    AC_MSG_ERROR([Unsupported pointer size: ${ac_cv_sizeof_void_p}])\n  fi\nfi\nAC_DEFINE_UNQUOTED([LG_SIZEOF_PTR], [$LG_SIZEOF_PTR])\n\nAC_CHECK_SIZEOF([int])\nif test \"x${ac_cv_sizeof_int}\" = \"x8\" ; then\n  LG_SIZEOF_INT=3\nelif test \"x${ac_cv_sizeof_int}\" = \"x4\" ; then\n  LG_SIZEOF_INT=2\nelse\n  AC_MSG_ERROR([Unsupported int size: ${ac_cv_sizeof_int}])\nfi\nAC_DEFINE_UNQUOTED([LG_SIZEOF_INT], [$LG_SIZEOF_INT])\n\nAC_CHECK_SIZEOF([long])\nif test \"x${ac_cv_sizeof_long}\" = \"x8\" ; then\n  LG_SIZEOF_LONG=3\nelif test \"x${ac_cv_sizeof_long}\" = \"x4\" ; then\n  LG_SIZEOF_LONG=2\nelse\n  AC_MSG_ERROR([Unsupported long size: ${ac_cv_sizeof_long}])\nfi\nAC_DEFINE_UNQUOTED([LG_SIZEOF_LONG], [$LG_SIZEOF_LONG])\n\nAC_CHECK_SIZEOF([long long])\nif test \"x${ac_cv_sizeof_long_long}\" = \"x8\" ; then\n  LG_SIZEOF_LONG_LONG=3\nelif test \"x${ac_cv_sizeof_long_long}\" = \"x4\" ; then\n  LG_SIZEOF_LONG_LONG=2\nelse\n  AC_MSG_ERROR([Unsupported long long size: ${ac_cv_sizeof_long_long}])\nfi\nAC_DEFINE_UNQUOTED([LG_SIZEOF_LONG_LONG], [$LG_SIZEOF_LONG_LONG])\n\nAC_CHECK_SIZEOF([intmax_t])\nif test \"x${ac_cv_sizeof_intmax_t}\" = \"x16\" ; then\n  LG_SIZEOF_INTMAX_T=4\nelif test \"x${ac_cv_sizeof_intmax_t}\" = \"x8\" ; then\n  LG_SIZEOF_INTMAX_T=3\nelif test \"x${ac_cv_sizeof_intmax_t}\" = \"x4\" ; then\n  LG_SIZEOF_INTMAX_T=2\nelse\n  AC_MSG_ERROR([Unsupported intmax_t size: ${ac_cv_sizeof_intmax_t}])\nfi\nAC_DEFINE_UNQUOTED([LG_SIZEOF_INTMAX_T], [$LG_SIZEOF_INTMAX_T])\n\nAC_CANONICAL_HOST\ndnl CPU-specific settings.\nCPU_SPINWAIT=\"\"\ncase \"${host_cpu}\" in\n  i686|x86_64)\n\tif test \"x${je_cv_msvc}\" = \"xyes\" ; then\n\t    AC_CACHE_VAL([je_cv_pause_msvc],\n\t      [JE_COMPILABLE([pause instruction MSVC], [],\n\t\t\t\t\t[[_mm_pause(); return 0;]],\n\t\t\t\t\t[je_cv_pause_msvc])])\n\t    if test \"x${je_cv_pause_msvc}\" = \"xyes\" ; then\n\t\tCPU_SPINWAIT='_mm_pause()'\n\t    fi\n\telse\n\t    AC_CACHE_VAL([je_cv_pause],\n\t      [JE_COMPILABLE([pause instruction], [],\n\t\t\t\t\t[[__asm__ volatile(\"pause\"); return 0;]],\n\t\t\t\t\t[je_cv_pause])])\n\t    if test \"x${je_cv_pause}\" = \"xyes\" ; then\n\t\tCPU_SPINWAIT='__asm__ volatile(\"pause\")'\n\t    fi\n\tfi\n\t;;\n  powerpc*)\n\tAC_DEFINE_UNQUOTED([HAVE_ALTIVEC], [ ])\n\tCPU_SPINWAIT='__asm__ volatile(\"or 31,31,31\")'\n\t;;\n  *)\n\t;;\nesac\nAC_DEFINE_UNQUOTED([CPU_SPINWAIT], [$CPU_SPINWAIT])\n\ncase \"${host_cpu}\" in\n  aarch64)\n    AC_MSG_CHECKING([number of significant virtual address bits])\n    LG_VADDR=48\n    AC_MSG_RESULT([$LG_VADDR])\n    ;;\n  x86_64)\n    AC_CACHE_CHECK([number of significant virtual address bits],\n                   [je_cv_lg_vaddr],\n                   AC_RUN_IFELSE([AC_LANG_PROGRAM(\n[[\n#include <stdio.h>\n#ifdef _WIN32\n#include <limits.h>\n#include <intrin.h>\ntypedef unsigned __int32 uint32_t;\n#else\n#include <stdint.h>\n#endif\n]], [[\n\tuint32_t r[[4]];\n\tuint32_t eax_in = 0x80000008U;\n#ifdef _WIN32\n\t__cpuid((int *)r, (int)eax_in);\n#else\n\tasm volatile (\"cpuid\"\n\t    : \"=a\" (r[[0]]), \"=b\" (r[[1]]), \"=c\" (r[[2]]), \"=d\" (r[[3]])\n\t    : \"a\" (eax_in), \"c\" (0)\n\t);\n#endif\n\tuint32_t eax_out = r[[0]];\n\tuint32_t vaddr = ((eax_out & 0x0000ff00U) >> 8);\n\tFILE *f = fopen(\"conftest.out\", \"w\");\n\tif (f == NULL) {\n\t\treturn 1;\n\t}\n\tif (vaddr > (sizeof(void *) << 3)) {\n\t\tvaddr = sizeof(void *) << 3;\n\t}\n\tfprintf(f, \"%u\", vaddr);\n\tfclose(f);\n\treturn 0;\n]])],\n                   [je_cv_lg_vaddr=`cat conftest.out`],\n                   [je_cv_lg_vaddr=error],\n                   [je_cv_lg_vaddr=57]))\n    if test \"x${je_cv_lg_vaddr}\" != \"x\" ; then\n      LG_VADDR=\"${je_cv_lg_vaddr}\"\n    fi\n    if test \"x${LG_VADDR}\" != \"xerror\" ; then\n      AC_DEFINE_UNQUOTED([LG_VADDR], [$LG_VADDR])\n    else\n      AC_MSG_ERROR([cannot determine number of significant virtual address bits])\n    fi\n    ;;\n  *)\n    AC_MSG_CHECKING([number of significant virtual address bits])\n    if test \"x${LG_SIZEOF_PTR}\" = \"x3\" ; then\n      LG_VADDR=64\n    elif test \"x${LG_SIZEOF_PTR}\" = \"x2\" ; then\n      LG_VADDR=32\n    elif test \"x${LG_SIZEOF_PTR}\" = \"xLG_SIZEOF_PTR_WIN\" ; then\n      LG_VADDR=\"(1U << (LG_SIZEOF_PTR_WIN+3))\"\n    else\n      AC_MSG_ERROR([Unsupported lg(pointer size): ${LG_SIZEOF_PTR}])\n    fi\n    AC_MSG_RESULT([$LG_VADDR])\n    ;;\nesac\nAC_DEFINE_UNQUOTED([LG_VADDR], [$LG_VADDR])\n\nLD_PRELOAD_VAR=\"LD_PRELOAD\"\nso=\"so\"\nimportlib=\"${so}\"\no=\"$ac_objext\"\na=\"a\"\nexe=\"$ac_exeext\"\nlibprefix=\"lib\"\nlink_whole_archive=\"0\"\nDSO_LDFLAGS='-shared -Wl,-soname,$(@F)'\nRPATH='-Wl,-rpath,$(1)'\nSOREV=\"${so}.${rev}\"\nPIC_CFLAGS='-fPIC -DPIC'\nCTARGET='-o $@'\nLDTARGET='-o $@'\nTEST_LD_MODE=\nEXTRA_LDFLAGS=\nARFLAGS='crus'\nAROUT=' $@'\nCC_MM=1\n\nif test \"x$je_cv_cray_prgenv_wrapper\" = \"xyes\" ; then\n  TEST_LD_MODE='-dynamic'\nfi\n\nif test \"x${je_cv_cray}\" = \"xyes\" ; then\n  CC_MM=\nfi\n\nAN_MAKEVAR([AR], [AC_PROG_AR])\nAN_PROGRAM([ar], [AC_PROG_AR])\nAC_DEFUN([AC_PROG_AR], [AC_CHECK_TOOL(AR, ar, :)])\nAC_PROG_AR\n\nAC_PROG_AWK\n\ndnl Platform-specific settings.  abi and RPATH can probably be determined\ndnl programmatically, but doing so is error-prone, which makes it generally\ndnl not worth the trouble.\ndnl \ndnl Define cpp macros in CPPFLAGS, rather than doing AC_DEFINE(macro), since the\ndnl definitions need to be seen before any headers are included, which is a pain\ndnl to make happen otherwise.\ndefault_retain=\"0\"\nmaps_coalesce=\"1\"\nDUMP_SYMS=\"nm -a\"\nSYM_PREFIX=\"\"\ncase \"${host}\" in\n  *-*-darwin* | *-*-ios*)\n\tabi=\"macho\"\n\tRPATH=\"\"\n\tLD_PRELOAD_VAR=\"DYLD_INSERT_LIBRARIES\"\n\tso=\"dylib\"\n\timportlib=\"${so}\"\n\tforce_tls=\"0\"\n\tDSO_LDFLAGS='-shared -Wl,-install_name,$(LIBDIR)/$(@F)'\n\tSOREV=\"${rev}.${so}\"\n\tsbrk_deprecated=\"1\"\n\tSYM_PREFIX=\"_\"\n\t;;\n  *-*-freebsd*)\n\tabi=\"elf\"\n\tAC_DEFINE([JEMALLOC_SYSCTL_VM_OVERCOMMIT], [ ])\n\tforce_lazy_lock=\"1\"\n\t;;\n  *-*-dragonfly*)\n\tabi=\"elf\"\n\t;;\n  *-*-openbsd*)\n\tabi=\"elf\"\n\tforce_tls=\"0\"\n\t;;\n  *-*-bitrig*)\n\tabi=\"elf\"\n\t;;\n  *-*-linux-android)\n\tdnl syscall(2) and secure_getenv(3) are exposed by _GNU_SOURCE.\n\tJE_APPEND_VS(CPPFLAGS, -D_GNU_SOURCE)\n\tabi=\"elf\"\n\tAC_DEFINE([JEMALLOC_PURGE_MADVISE_DONTNEED_ZEROS])\n\tAC_DEFINE([JEMALLOC_HAS_ALLOCA_H])\n\tAC_DEFINE([JEMALLOC_PROC_SYS_VM_OVERCOMMIT_MEMORY], [ ])\n\tAC_DEFINE([JEMALLOC_THREADED_INIT], [ ])\n\tAC_DEFINE([JEMALLOC_C11_ATOMICS])\n\tforce_tls=\"0\"\n\tif test \"${LG_SIZEOF_PTR}\" = \"3\"; then\n\t  default_retain=\"1\"\n\tfi\n\t;;\n  *-*-linux* | *-*-kfreebsd*)\n\tdnl syscall(2) and secure_getenv(3) are exposed by _GNU_SOURCE.\n\tJE_APPEND_VS(CPPFLAGS, -D_GNU_SOURCE)\n\tabi=\"elf\"\n\tAC_DEFINE([JEMALLOC_PURGE_MADVISE_DONTNEED_ZEROS])\n\tAC_DEFINE([JEMALLOC_HAS_ALLOCA_H])\n\tAC_DEFINE([JEMALLOC_PROC_SYS_VM_OVERCOMMIT_MEMORY], [ ])\n\tAC_DEFINE([JEMALLOC_THREADED_INIT], [ ])\n\tAC_DEFINE([JEMALLOC_USE_CXX_THROW], [ ])\n\tif test \"${LG_SIZEOF_PTR}\" = \"3\"; then\n\t  default_retain=\"1\"\n\tfi\n\t;;\n  *-*-netbsd*)\n\tAC_MSG_CHECKING([ABI])\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM(\n[[#ifdef __ELF__\n/* ELF */\n#else\n#error aout\n#endif\n]])],\n                          [abi=\"elf\"],\n                          [abi=\"aout\"])\n\tAC_MSG_RESULT([$abi])\n\t;;\n  *-*-solaris2*)\n\tabi=\"elf\"\n\tRPATH='-Wl,-R,$(1)'\n\tdnl Solaris needs this for sigwait().\n\tJE_APPEND_VS(CPPFLAGS, -D_POSIX_PTHREAD_SEMANTICS)\n\tJE_APPEND_VS(LIBS, -lposix4 -lsocket -lnsl)\n\t;;\n  *-ibm-aix*)\n\tif test \"${LG_SIZEOF_PTR}\" = \"3\"; then\n\t  dnl 64bit AIX\n\t  LD_PRELOAD_VAR=\"LDR_PRELOAD64\"\n\telse\n\t  dnl 32bit AIX\n\t  LD_PRELOAD_VAR=\"LDR_PRELOAD\"\n\tfi\n\tabi=\"xcoff\"\n\t;;\n  *-*-mingw* | *-*-cygwin*)\n\tabi=\"pecoff\"\n\tforce_tls=\"0\"\n\tmaps_coalesce=\"0\"\n\tRPATH=\"\"\n\tso=\"dll\"\n\tif test \"x$je_cv_msvc\" = \"xyes\" ; then\n\t  importlib=\"lib\"\n\t  DSO_LDFLAGS=\"-LD\"\n\t  EXTRA_LDFLAGS=\"-link -DEBUG\"\n\t  CTARGET='-Fo$@'\n\t  LDTARGET='-Fe$@'\n\t  AR='lib'\n\t  ARFLAGS='-nologo -out:'\n\t  AROUT='$@'\n\t  CC_MM=\n        else\n\t  importlib=\"${so}\"\n\t  DSO_LDFLAGS=\"-shared\"\n\t  link_whole_archive=\"1\"\n\tfi\n\tDUMP_SYMS=\"dumpbin /SYMBOLS\"\n\ta=\"lib\"\n\tlibprefix=\"\"\n\tSOREV=\"${so}\"\n\tPIC_CFLAGS=\"\"\n\t;;\n  *)\n\tAC_MSG_RESULT([Unsupported operating system: ${host}])\n\tabi=\"elf\"\n\t;;\nesac\n\nJEMALLOC_USABLE_SIZE_CONST=const\nAC_CHECK_HEADERS([malloc.h], [\n  AC_MSG_CHECKING([whether malloc_usable_size definition can use const argument])\n  AC_COMPILE_IFELSE([AC_LANG_PROGRAM(\n    [#include <malloc.h>\n     #include <stddef.h>\n    size_t malloc_usable_size(const void *ptr);\n    ],\n    [])],[\n                AC_MSG_RESULT([yes])\n         ],[\n                JEMALLOC_USABLE_SIZE_CONST=\n                AC_MSG_RESULT([no])\n         ])\n])\nAC_DEFINE_UNQUOTED([JEMALLOC_USABLE_SIZE_CONST], [$JEMALLOC_USABLE_SIZE_CONST])\nAC_SUBST([abi])\nAC_SUBST([RPATH])\nAC_SUBST([LD_PRELOAD_VAR])\nAC_SUBST([so])\nAC_SUBST([importlib])\nAC_SUBST([o])\nAC_SUBST([a])\nAC_SUBST([exe])\nAC_SUBST([libprefix])\nAC_SUBST([link_whole_archive])\nAC_SUBST([DSO_LDFLAGS])\nAC_SUBST([EXTRA_LDFLAGS])\nAC_SUBST([SOREV])\nAC_SUBST([PIC_CFLAGS])\nAC_SUBST([CTARGET])\nAC_SUBST([LDTARGET])\nAC_SUBST([TEST_LD_MODE])\nAC_SUBST([MKLIB])\nAC_SUBST([ARFLAGS])\nAC_SUBST([AROUT])\nAC_SUBST([DUMP_SYMS])\nAC_SUBST([CC_MM])\n\ndnl Determine whether libm must be linked to use e.g. log(3).\nAC_SEARCH_LIBS([log], [m], , [AC_MSG_ERROR([Missing math functions])])\nif test \"x$ac_cv_search_log\" != \"xnone required\" ; then\n  LM=\"$ac_cv_search_log\"\nelse\n  LM=\nfi\nAC_SUBST(LM)\n\nJE_COMPILABLE([__attribute__ syntax],\n              [static __attribute__((unused)) void foo(void){}],\n              [],\n              [je_cv_attribute])\nif test \"x${je_cv_attribute}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_ATTR], [ ])\n  if test \"x${GCC}\" = \"xyes\" -a \"x${abi}\" = \"xelf\"; then\n    JE_CFLAGS_ADD([-fvisibility=hidden])\n    JE_CXXFLAGS_ADD([-fvisibility=hidden])\n  fi\nfi\ndnl Check for tls-dialect support.\nJE_CFLAGS_ADD([-mtls-dialect=gnu2])\nif test \"x$je_cv_cflags_added\" = \"x-mtls-dialect=gnu2\" ; then\n  je_cv_tls_dialect_gnu2=yes\nelse\n  je_cv_tls_dialect_gnu2=no\nfi\nJE_CXXFLAGS_ADD([-mtls-dialect=gnu2])\ndnl Check for tls_model attribute support (clang 3.0 still lacks support).\nJE_CFLAGS_SAVE()\nJE_CFLAGS_ADD([-Werror])\nJE_CFLAGS_ADD([-herror_on_warning])\nJE_COMPILABLE([tls_model attribute], [],\n              [static __thread int\n               __attribute__((tls_model(\"initial-exec\"), unused)) foo;\n               foo = 0;],\n              [je_cv_tls_model])\nJE_CFLAGS_RESTORE()\nif test \"x${je_cv_tls_dialect_gnu2}\" = \"xno\" \\\n -a \"x${je_cv_tls_model}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_TLS_MODEL],\n            [__attribute__((tls_model(\"initial-exec\")))])\nelse\n  AC_DEFINE([JEMALLOC_TLS_MODEL], [ ])\nfi\ndnl Check for alloc_size attribute support.\nJE_CFLAGS_SAVE()\nJE_CFLAGS_ADD([-Werror])\nJE_CFLAGS_ADD([-herror_on_warning])\nJE_COMPILABLE([alloc_size attribute], [#include <stdlib.h>],\n              [void *foo(size_t size) __attribute__((alloc_size(1)));],\n              [je_cv_alloc_size])\nJE_CFLAGS_RESTORE()\nif test \"x${je_cv_alloc_size}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_ATTR_ALLOC_SIZE], [ ])\nfi\ndnl Check for format(gnu_printf, ...) attribute support.\nJE_CFLAGS_SAVE()\nJE_CFLAGS_ADD([-Werror])\nJE_CFLAGS_ADD([-herror_on_warning])\nJE_COMPILABLE([format(gnu_printf, ...) attribute], [#include <stdlib.h>],\n              [void *foo(const char *format, ...) __attribute__((format(gnu_printf, 1, 2)));],\n              [je_cv_format_gnu_printf])\nJE_CFLAGS_RESTORE()\nif test \"x${je_cv_format_gnu_printf}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_ATTR_FORMAT_GNU_PRINTF], [ ])\nfi\ndnl Check for format(printf, ...) attribute support.\nJE_CFLAGS_SAVE()\nJE_CFLAGS_ADD([-Werror])\nJE_CFLAGS_ADD([-herror_on_warning])\nJE_COMPILABLE([format(printf, ...) attribute], [#include <stdlib.h>],\n              [void *foo(const char *format, ...) __attribute__((format(printf, 1, 2)));],\n              [je_cv_format_printf])\nJE_CFLAGS_RESTORE()\nif test \"x${je_cv_format_printf}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_ATTR_FORMAT_PRINTF], [ ])\nfi\n\ndnl Support optional additions to rpath.\nAC_ARG_WITH([rpath],\n  [AS_HELP_STRING([--with-rpath=<rpath>], [Colon-separated rpath (ELF systems only)])],\nif test \"x$with_rpath\" = \"xno\" ; then\n  RPATH_EXTRA=\nelse\n  RPATH_EXTRA=\"`echo $with_rpath | tr \\\":\\\" \\\" \\\"`\"\nfi,\n  RPATH_EXTRA=\n)\nAC_SUBST([RPATH_EXTRA])\n\ndnl Disable rules that do automatic regeneration of configure output by default.\nAC_ARG_ENABLE([autogen],\n  [AS_HELP_STRING([--enable-autogen], [Automatically regenerate configure output])],\nif test \"x$enable_autogen\" = \"xno\" ; then\n  enable_autogen=\"0\"\nelse\n  enable_autogen=\"1\"\nfi\n,\nenable_autogen=\"0\"\n)\nAC_SUBST([enable_autogen])\n\nAC_PROG_INSTALL\nAC_PROG_RANLIB\nAC_PATH_PROG([LD], [ld], [false], [$PATH])\nAC_PATH_PROG([AUTOCONF], [autoconf], [false], [$PATH])\n\ndnl Perform no name mangling by default.\nAC_ARG_WITH([mangling],\n  [AS_HELP_STRING([--with-mangling=<map>], [Mangle symbols in <map>])],\n  [mangling_map=\"$with_mangling\"], [mangling_map=\"\"])\n\ndnl Do not prefix public APIs by default.\nAC_ARG_WITH([jemalloc_prefix],\n  [AS_HELP_STRING([--with-jemalloc-prefix=<prefix>], [Prefix to prepend to all public APIs])],\n  [JEMALLOC_PREFIX=\"$with_jemalloc_prefix\"],\n  [if test \"x$abi\" != \"xmacho\" -a \"x$abi\" != \"xpecoff\"; then\n  JEMALLOC_PREFIX=\"\"\nelse\n  JEMALLOC_PREFIX=\"je_\"\nfi]\n)\nif test \"x$JEMALLOC_PREFIX\" = \"x\" ; then\n  AC_DEFINE([JEMALLOC_IS_MALLOC])\nelse\n  JEMALLOC_CPREFIX=`echo ${JEMALLOC_PREFIX} | tr \"a-z\" \"A-Z\"`\n  AC_DEFINE_UNQUOTED([JEMALLOC_PREFIX], [\"$JEMALLOC_PREFIX\"])\n  AC_DEFINE_UNQUOTED([JEMALLOC_CPREFIX], [\"$JEMALLOC_CPREFIX\"])\nfi\nAC_SUBST([JEMALLOC_PREFIX])\nAC_SUBST([JEMALLOC_CPREFIX])\n\nAC_ARG_WITH([export],\n  [AS_HELP_STRING([--without-export], [disable exporting jemalloc public APIs])],\n  [if test \"x$with_export\" = \"xno\"; then\n  AC_DEFINE([JEMALLOC_EXPORT],[])\nfi]\n)\n\npublic_syms=\"aligned_alloc calloc dallocx free mallctl mallctlbymib mallctlnametomib malloc malloc_conf malloc_message malloc_stats_print malloc_usable_size mallocx nallocx posix_memalign rallocx realloc sallocx sdallocx xallocx\"\ndnl Check for additional platform-specific public API functions.\nAC_CHECK_FUNC([memalign],\n\t      [AC_DEFINE([JEMALLOC_OVERRIDE_MEMALIGN], [ ])\n\t       public_syms=\"${public_syms} memalign\"])\nAC_CHECK_FUNC([valloc],\n\t      [AC_DEFINE([JEMALLOC_OVERRIDE_VALLOC], [ ])\n\t       public_syms=\"${public_syms} valloc\"])\n\ndnl Check for allocator-related functions that should be wrapped.\nwrap_syms=\nif test \"x${JEMALLOC_PREFIX}\" = \"x\" ; then\n  AC_CHECK_FUNC([__libc_calloc],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___LIBC_CALLOC], [ ])\n\t\t wrap_syms=\"${wrap_syms} __libc_calloc\"])\n  AC_CHECK_FUNC([__libc_free],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___LIBC_FREE], [ ])\n\t\t wrap_syms=\"${wrap_syms} __libc_free\"])\n  AC_CHECK_FUNC([__libc_malloc],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___LIBC_MALLOC], [ ])\n\t\t wrap_syms=\"${wrap_syms} __libc_malloc\"])\n  AC_CHECK_FUNC([__libc_memalign],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___LIBC_MEMALIGN], [ ])\n\t\t wrap_syms=\"${wrap_syms} __libc_memalign\"])\n  AC_CHECK_FUNC([__libc_realloc],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___LIBC_REALLOC], [ ])\n\t\t wrap_syms=\"${wrap_syms} __libc_realloc\"])\n  AC_CHECK_FUNC([__libc_valloc],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___LIBC_VALLOC], [ ])\n\t\t wrap_syms=\"${wrap_syms} __libc_valloc\"])\n  AC_CHECK_FUNC([__posix_memalign],\n\t\t[AC_DEFINE([JEMALLOC_OVERRIDE___POSIX_MEMALIGN], [ ])\n\t\t wrap_syms=\"${wrap_syms} __posix_memalign\"])\nfi\n\ncase \"${host}\" in\n  *-*-mingw* | *-*-cygwin*)\n    wrap_syms=\"${wrap_syms} tls_callback\"\n    ;;\n  *)\n    ;;\nesac\n\ndnl Mangle library-private APIs.\nAC_ARG_WITH([private_namespace],\n  [AS_HELP_STRING([--with-private-namespace=<prefix>], [Prefix to prepend to all library-private APIs])],\n  [JEMALLOC_PRIVATE_NAMESPACE=\"${with_private_namespace}je_\"],\n  [JEMALLOC_PRIVATE_NAMESPACE=\"je_\"]\n)\nAC_DEFINE_UNQUOTED([JEMALLOC_PRIVATE_NAMESPACE], [$JEMALLOC_PRIVATE_NAMESPACE])\nprivate_namespace=\"$JEMALLOC_PRIVATE_NAMESPACE\"\nAC_SUBST([private_namespace])\n\ndnl Do not add suffix to installed files by default.\nAC_ARG_WITH([install_suffix],\n  [AS_HELP_STRING([--with-install-suffix=<suffix>], [Suffix to append to all installed files])],\n  [INSTALL_SUFFIX=\"$with_install_suffix\"],\n  [INSTALL_SUFFIX=]\n)\ninstall_suffix=\"$INSTALL_SUFFIX\"\nAC_SUBST([install_suffix])\n\ndnl Specify default malloc_conf.\nAC_ARG_WITH([malloc_conf],\n  [AS_HELP_STRING([--with-malloc-conf=<malloc_conf>], [config.malloc_conf options string])],\n  [JEMALLOC_CONFIG_MALLOC_CONF=\"$with_malloc_conf\"],\n  [JEMALLOC_CONFIG_MALLOC_CONF=\"\"]\n)\nconfig_malloc_conf=\"$JEMALLOC_CONFIG_MALLOC_CONF\"\nAC_DEFINE_UNQUOTED([JEMALLOC_CONFIG_MALLOC_CONF], [\"$config_malloc_conf\"])\n\ndnl Substitute @je_@ in jemalloc_protos.h.in, primarily to make generation of\ndnl jemalloc_protos_jet.h easy.\nje_=\"je_\"\nAC_SUBST([je_])\n\ncfgoutputs_in=\"Makefile.in\"\ncfgoutputs_in=\"${cfgoutputs_in} jemalloc.pc.in\"\ncfgoutputs_in=\"${cfgoutputs_in} doc/html.xsl.in\"\ncfgoutputs_in=\"${cfgoutputs_in} doc/manpages.xsl.in\"\ncfgoutputs_in=\"${cfgoutputs_in} doc/jemalloc.xml.in\"\ncfgoutputs_in=\"${cfgoutputs_in} include/jemalloc/jemalloc_macros.h.in\"\ncfgoutputs_in=\"${cfgoutputs_in} include/jemalloc/jemalloc_protos.h.in\"\ncfgoutputs_in=\"${cfgoutputs_in} include/jemalloc/jemalloc_typedefs.h.in\"\ncfgoutputs_in=\"${cfgoutputs_in} include/jemalloc/internal/jemalloc_preamble.h.in\"\ncfgoutputs_in=\"${cfgoutputs_in} test/test.sh.in\"\ncfgoutputs_in=\"${cfgoutputs_in} test/include/test/jemalloc_test.h.in\"\n\ncfgoutputs_out=\"Makefile\"\ncfgoutputs_out=\"${cfgoutputs_out} jemalloc.pc\"\ncfgoutputs_out=\"${cfgoutputs_out} doc/html.xsl\"\ncfgoutputs_out=\"${cfgoutputs_out} doc/manpages.xsl\"\ncfgoutputs_out=\"${cfgoutputs_out} doc/jemalloc.xml\"\ncfgoutputs_out=\"${cfgoutputs_out} include/jemalloc/jemalloc_macros.h\"\ncfgoutputs_out=\"${cfgoutputs_out} include/jemalloc/jemalloc_protos.h\"\ncfgoutputs_out=\"${cfgoutputs_out} include/jemalloc/jemalloc_typedefs.h\"\ncfgoutputs_out=\"${cfgoutputs_out} include/jemalloc/internal/jemalloc_preamble.h\"\ncfgoutputs_out=\"${cfgoutputs_out} test/test.sh\"\ncfgoutputs_out=\"${cfgoutputs_out} test/include/test/jemalloc_test.h\"\n\ncfgoutputs_tup=\"Makefile\"\ncfgoutputs_tup=\"${cfgoutputs_tup} jemalloc.pc:jemalloc.pc.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} doc/html.xsl:doc/html.xsl.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} doc/manpages.xsl:doc/manpages.xsl.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} doc/jemalloc.xml:doc/jemalloc.xml.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} include/jemalloc/jemalloc_macros.h:include/jemalloc/jemalloc_macros.h.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} include/jemalloc/jemalloc_protos.h:include/jemalloc/jemalloc_protos.h.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} include/jemalloc/jemalloc_typedefs.h:include/jemalloc/jemalloc_typedefs.h.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} include/jemalloc/internal/jemalloc_preamble.h\"\ncfgoutputs_tup=\"${cfgoutputs_tup} test/test.sh:test/test.sh.in\"\ncfgoutputs_tup=\"${cfgoutputs_tup} test/include/test/jemalloc_test.h:test/include/test/jemalloc_test.h.in\"\n\ncfghdrs_in=\"include/jemalloc/jemalloc_defs.h.in\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/internal/jemalloc_internal_defs.h.in\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/internal/private_symbols.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/internal/private_namespace.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/internal/public_namespace.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/internal/public_unnamespace.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/internal/size_classes.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/jemalloc_rename.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/jemalloc_mangle.sh\"\ncfghdrs_in=\"${cfghdrs_in} include/jemalloc/jemalloc.sh\"\ncfghdrs_in=\"${cfghdrs_in} test/include/test/jemalloc_test_defs.h.in\"\n\ncfghdrs_out=\"include/jemalloc/jemalloc_defs.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/jemalloc${install_suffix}.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/private_symbols.awk\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/private_symbols_jet.awk\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/public_symbols.txt\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/public_namespace.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/public_unnamespace.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/size_classes.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/jemalloc_protos_jet.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/jemalloc_rename.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/jemalloc_mangle.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/jemalloc_mangle_jet.h\"\ncfghdrs_out=\"${cfghdrs_out} include/jemalloc/internal/jemalloc_internal_defs.h\"\ncfghdrs_out=\"${cfghdrs_out} test/include/test/jemalloc_test_defs.h\"\n\ncfghdrs_tup=\"include/jemalloc/jemalloc_defs.h:include/jemalloc/jemalloc_defs.h.in\"\ncfghdrs_tup=\"${cfghdrs_tup} include/jemalloc/internal/jemalloc_internal_defs.h:include/jemalloc/internal/jemalloc_internal_defs.h.in\"\ncfghdrs_tup=\"${cfghdrs_tup} test/include/test/jemalloc_test_defs.h:test/include/test/jemalloc_test_defs.h.in\"\n\ndnl Do not compile with debugging by default.\nAC_ARG_ENABLE([debug],\n  [AS_HELP_STRING([--enable-debug],\n                  [Build debugging code])],\n[if test \"x$enable_debug\" = \"xno\" ; then\n  enable_debug=\"0\"\nelse\n  enable_debug=\"1\"\nfi\n],\n[enable_debug=\"0\"]\n)\nif test \"x$enable_debug\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_DEBUG], [ ])\nfi\nif test \"x$enable_debug\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_DEBUG], [ ])\nfi\nAC_SUBST([enable_debug])\n\ndnl Only optimize if not debugging.\nif test \"x$enable_debug\" = \"x0\" ; then\n  if test \"x$GCC\" = \"xyes\" ; then\n    JE_CFLAGS_ADD([-O3])\n    JE_CXXFLAGS_ADD([-O3])\n    JE_CFLAGS_ADD([-funroll-loops])\n  elif test \"x$je_cv_msvc\" = \"xyes\" ; then\n    JE_CFLAGS_ADD([-O2])\n    JE_CXXFLAGS_ADD([-O2])\n  else\n    JE_CFLAGS_ADD([-O])\n    JE_CXXFLAGS_ADD([-O])\n  fi\nfi\n\ndnl Enable statistics calculation by default.\nAC_ARG_ENABLE([stats],\n  [AS_HELP_STRING([--disable-stats],\n                  [Disable statistics calculation/reporting])],\n[if test \"x$enable_stats\" = \"xno\" ; then\n  enable_stats=\"0\"\nelse\n  enable_stats=\"1\"\nfi\n],\n[enable_stats=\"1\"]\n)\nif test \"x$enable_stats\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_STATS], [ ])\nfi\nAC_SUBST([enable_stats])\n\ndnl Do not enable profiling by default.\nAC_ARG_ENABLE([prof],\n  [AS_HELP_STRING([--enable-prof], [Enable allocation profiling])],\n[if test \"x$enable_prof\" = \"xno\" ; then\n  enable_prof=\"0\"\nelse\n  enable_prof=\"1\"\nfi\n],\n[enable_prof=\"0\"]\n)\nif test \"x$enable_prof\" = \"x1\" ; then\n  backtrace_method=\"\"\nelse\n  backtrace_method=\"N/A\"\nfi\n\nAC_ARG_ENABLE([prof-libunwind],\n  [AS_HELP_STRING([--enable-prof-libunwind], [Use libunwind for backtracing])],\n[if test \"x$enable_prof_libunwind\" = \"xno\" ; then\n  enable_prof_libunwind=\"0\"\nelse\n  enable_prof_libunwind=\"1\"\nfi\n],\n[enable_prof_libunwind=\"0\"]\n)\nAC_ARG_WITH([static_libunwind],\n  [AS_HELP_STRING([--with-static-libunwind=<libunwind.a>],\n  [Path to static libunwind library; use rather than dynamically linking])],\nif test \"x$with_static_libunwind\" = \"xno\" ; then\n  LUNWIND=\"-lunwind\"\nelse\n  if test ! -f \"$with_static_libunwind\" ; then\n    AC_MSG_ERROR([Static libunwind not found: $with_static_libunwind])\n  fi\n  LUNWIND=\"$with_static_libunwind\"\nfi,\n  LUNWIND=\"-lunwind\"\n)\nif test \"x$backtrace_method\" = \"x\" -a \"x$enable_prof_libunwind\" = \"x1\" ; then\n  AC_CHECK_HEADERS([libunwind.h], , [enable_prof_libunwind=\"0\"])\n  if test \"x$LUNWIND\" = \"x-lunwind\" ; then\n    AC_CHECK_LIB([unwind], [unw_backtrace], [JE_APPEND_VS(LIBS, $LUNWIND)],\n                 [enable_prof_libunwind=\"0\"])\n  else\n    JE_APPEND_VS(LIBS, $LUNWIND)\n  fi\n  if test \"x${enable_prof_libunwind}\" = \"x1\" ; then\n    backtrace_method=\"libunwind\"\n    AC_DEFINE([JEMALLOC_PROF_LIBUNWIND], [ ])\n  fi\nfi\n\nAC_ARG_ENABLE([prof-libgcc],\n  [AS_HELP_STRING([--disable-prof-libgcc],\n  [Do not use libgcc for backtracing])],\n[if test \"x$enable_prof_libgcc\" = \"xno\" ; then\n  enable_prof_libgcc=\"0\"\nelse\n  enable_prof_libgcc=\"1\"\nfi\n],\n[enable_prof_libgcc=\"1\"]\n)\nif test \"x$backtrace_method\" = \"x\" -a \"x$enable_prof_libgcc\" = \"x1\" \\\n     -a \"x$GCC\" = \"xyes\" ; then\n  AC_CHECK_HEADERS([unwind.h], , [enable_prof_libgcc=\"0\"])\n  if test \"x${enable_prof_libgcc}\" = \"x1\" ; then\n    AC_CHECK_LIB([gcc], [_Unwind_Backtrace], [JE_APPEND_VS(LIBS, -lgcc)], [enable_prof_libgcc=\"0\"])\n  fi\n  if test \"x${enable_prof_libgcc}\" = \"x1\" ; then\n    backtrace_method=\"libgcc\"\n    AC_DEFINE([JEMALLOC_PROF_LIBGCC], [ ])\n  fi\nelse\n  enable_prof_libgcc=\"0\"\nfi\n\nAC_ARG_ENABLE([prof-gcc],\n  [AS_HELP_STRING([--disable-prof-gcc],\n  [Do not use gcc intrinsics for backtracing])],\n[if test \"x$enable_prof_gcc\" = \"xno\" ; then\n  enable_prof_gcc=\"0\"\nelse\n  enable_prof_gcc=\"1\"\nfi\n],\n[enable_prof_gcc=\"1\"]\n)\nif test \"x$backtrace_method\" = \"x\" -a \"x$enable_prof_gcc\" = \"x1\" \\\n     -a \"x$GCC\" = \"xyes\" ; then\n  JE_CFLAGS_ADD([-fno-omit-frame-pointer])\n  backtrace_method=\"gcc intrinsics\"\n  AC_DEFINE([JEMALLOC_PROF_GCC], [ ])\nelse\n  enable_prof_gcc=\"0\"\nfi\n\nif test \"x$backtrace_method\" = \"x\" ; then\n  backtrace_method=\"none (disabling profiling)\"\n  enable_prof=\"0\"\nfi\nAC_MSG_CHECKING([configured backtracing method])\nAC_MSG_RESULT([$backtrace_method])\nif test \"x$enable_prof\" = \"x1\" ; then\n  dnl Heap profiling uses the log(3) function.\n  JE_APPEND_VS(LIBS, $LM)\n\n  AC_DEFINE([JEMALLOC_PROF], [ ])\nfi\nAC_SUBST([enable_prof])\n\ndnl Indicate whether adjacent virtual memory mappings automatically coalesce\ndnl (and fragment on demand).\nif test \"x${maps_coalesce}\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_MAPS_COALESCE], [ ])\nfi\n\ndnl Indicate whether to retain memory (rather than using munmap()) by default.\nif test \"x$default_retain\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_RETAIN], [ ])\nfi\n\ndnl Enable allocation from DSS if supported by the OS.\nhave_dss=\"1\"\ndnl Check whether the BSD/SUSv1 sbrk() exists.  If not, disable DSS support.\nAC_CHECK_FUNC([sbrk], [have_sbrk=\"1\"], [have_sbrk=\"0\"])\nif test \"x$have_sbrk\" = \"x1\" ; then\n  if test \"x$sbrk_deprecated\" = \"x1\" ; then\n    AC_MSG_RESULT([Disabling dss allocation because sbrk is deprecated])\n    have_dss=\"0\"\n  fi\nelse\n  have_dss=\"0\"\nfi\n\nif test \"x$have_dss\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_DSS], [ ])\nfi\n\ndnl Support the junk/zero filling option by default.\nAC_ARG_ENABLE([fill],\n  [AS_HELP_STRING([--disable-fill], [Disable support for junk/zero filling])],\n[if test \"x$enable_fill\" = \"xno\" ; then\n  enable_fill=\"0\"\nelse\n  enable_fill=\"1\"\nfi\n],\n[enable_fill=\"1\"]\n)\nif test \"x$enable_fill\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_FILL], [ ])\nfi\nAC_SUBST([enable_fill])\n\ndnl Disable utrace(2)-based tracing by default.\nAC_ARG_ENABLE([utrace],\n  [AS_HELP_STRING([--enable-utrace], [Enable utrace(2)-based tracing])],\n[if test \"x$enable_utrace\" = \"xno\" ; then\n  enable_utrace=\"0\"\nelse\n  enable_utrace=\"1\"\nfi\n],\n[enable_utrace=\"0\"]\n)\nJE_COMPILABLE([utrace(2)], [\n#include <sys/types.h>\n#include <sys/param.h>\n#include <sys/time.h>\n#include <sys/uio.h>\n#include <sys/ktrace.h>\n], [\n\tutrace((void *)0, 0);\n], [je_cv_utrace])\nif test \"x${je_cv_utrace}\" = \"xno\" ; then\n  enable_utrace=\"0\"\nfi\nif test \"x$enable_utrace\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_UTRACE], [ ])\nfi\nAC_SUBST([enable_utrace])\n\ndnl Do not support the xmalloc option by default.\nAC_ARG_ENABLE([xmalloc],\n  [AS_HELP_STRING([--enable-xmalloc], [Support xmalloc option])],\n[if test \"x$enable_xmalloc\" = \"xno\" ; then\n  enable_xmalloc=\"0\"\nelse\n  enable_xmalloc=\"1\"\nfi\n],\n[enable_xmalloc=\"0\"]\n)\nif test \"x$enable_xmalloc\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_XMALLOC], [ ])\nfi\nAC_SUBST([enable_xmalloc])\n\ndnl Support cache-oblivious allocation alignment by default.\nAC_ARG_ENABLE([cache-oblivious],\n  [AS_HELP_STRING([--disable-cache-oblivious],\n                  [Disable support for cache-oblivious allocation alignment])],\n[if test \"x$enable_cache_oblivious\" = \"xno\" ; then\n  enable_cache_oblivious=\"0\"\nelse\n  enable_cache_oblivious=\"1\"\nfi\n],\n[enable_cache_oblivious=\"1\"]\n)\nif test \"x$enable_cache_oblivious\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_CACHE_OBLIVIOUS], [ ])\nfi\nAC_SUBST([enable_cache_oblivious])\n\n\n\nJE_COMPILABLE([a program using __builtin_unreachable], [\nvoid foo (void) {\n  __builtin_unreachable();\n}\n], [\n\t{\n\t\tfoo();\n\t}\n], [je_cv_gcc_builtin_unreachable])\nif test \"x${je_cv_gcc_builtin_unreachable}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_INTERNAL_UNREACHABLE], [__builtin_unreachable])\nelse\n  AC_DEFINE([JEMALLOC_INTERNAL_UNREACHABLE], [abort])\nfi\n\ndnl ============================================================================\ndnl Check for  __builtin_ffsl(), then ffsl(3), and fail if neither are found.\ndnl One of those two functions should (theoretically) exist on all platforms\ndnl that jemalloc currently has a chance of functioning on without modification.\ndnl We additionally assume ffs[ll]() or __builtin_ffs[ll]() are defined if\ndnl ffsl() or __builtin_ffsl() are defined, respectively.\nJE_COMPILABLE([a program using __builtin_ffsl], [\n#include <stdio.h>\n#include <strings.h>\n#include <string.h>\n], [\n\t{\n\t\tint rv = __builtin_ffsl(0x08);\n\t\tprintf(\"%d\\n\", rv);\n\t}\n], [je_cv_gcc_builtin_ffsl])\nif test \"x${je_cv_gcc_builtin_ffsl}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_INTERNAL_FFSLL], [__builtin_ffsll])\n  AC_DEFINE([JEMALLOC_INTERNAL_FFSL], [__builtin_ffsl])\n  AC_DEFINE([JEMALLOC_INTERNAL_FFS], [__builtin_ffs])\nelse\n  JE_COMPILABLE([a program using ffsl], [\n  #include <stdio.h>\n  #include <strings.h>\n  #include <string.h>\n  ], [\n\t{\n\t\tint rv = ffsl(0x08);\n\t\tprintf(\"%d\\n\", rv);\n\t}\n  ], [je_cv_function_ffsl])\n  if test \"x${je_cv_function_ffsl}\" = \"xyes\" ; then\n    AC_DEFINE([JEMALLOC_INTERNAL_FFSLL], [ffsll])\n    AC_DEFINE([JEMALLOC_INTERNAL_FFSL], [ffsl])\n    AC_DEFINE([JEMALLOC_INTERNAL_FFS], [ffs])\n  else\n    AC_MSG_ERROR([Cannot build without ffsl(3) or __builtin_ffsl()])\n  fi\nfi\n\nAC_ARG_WITH([lg_quantum],\n  [AS_HELP_STRING([--with-lg-quantum=<lg-quantum>],\n   [Base 2 log of minimum allocation alignment])],\n  [LG_QUANTA=\"$with_lg_quantum\"],\n  [LG_QUANTA=\"3 4\"])\nif test \"x$with_lg_quantum\" != \"x\" ; then\n  AC_DEFINE_UNQUOTED([LG_QUANTUM], [$with_lg_quantum])\nfi\n\nAC_ARG_WITH([lg_page],\n  [AS_HELP_STRING([--with-lg-page=<lg-page>], [Base 2 log of system page size])],\n  [LG_PAGE=\"$with_lg_page\"], [LG_PAGE=\"detect\"])\nif test \"x$LG_PAGE\" = \"xdetect\"; then\n  AC_CACHE_CHECK([LG_PAGE],\n               [je_cv_lg_page],\n               AC_RUN_IFELSE([AC_LANG_PROGRAM(\n[[\n#include <strings.h>\n#ifdef _WIN32\n#include <windows.h>\n#else\n#include <unistd.h>\n#endif\n#include <stdio.h>\n]],\n[[\n    int result;\n    FILE *f;\n\n#ifdef _WIN32\n    SYSTEM_INFO si;\n    GetSystemInfo(&si);\n    result = si.dwPageSize;\n#else\n    result = sysconf(_SC_PAGESIZE);\n#endif\n    if (result == -1) {\n\treturn 1;\n    }\n    result = JEMALLOC_INTERNAL_FFSL(result) - 1;\n\n    f = fopen(\"conftest.out\", \"w\");\n    if (f == NULL) {\n\treturn 1;\n    }\n    fprintf(f, \"%d\", result);\n    fclose(f);\n\n    return 0;\n]])],\n                             [je_cv_lg_page=`cat conftest.out`],\n                             [je_cv_lg_page=undefined],\n                             [je_cv_lg_page=12]))\nfi\nif test \"x${je_cv_lg_page}\" != \"x\" ; then\n  LG_PAGE=\"${je_cv_lg_page}\"\nfi\nif test \"x${LG_PAGE}\" != \"xundefined\" ; then\n   AC_DEFINE_UNQUOTED([LG_PAGE], [$LG_PAGE])\nelse\n   AC_MSG_ERROR([cannot determine value for LG_PAGE])\nfi\n\nAC_ARG_WITH([lg_hugepage],\n  [AS_HELP_STRING([--with-lg-hugepage=<lg-hugepage>],\n   [Base 2 log of system huge page size])],\n  [je_cv_lg_hugepage=\"${with_lg_hugepage}\"],\n  [je_cv_lg_hugepage=\"\"])\nif test \"x${je_cv_lg_hugepage}\" = \"x\" ; then\n  dnl Look in /proc/meminfo (Linux-specific) for information on the default huge\n  dnl page size, if any.  The relevant line looks like:\n  dnl\n  dnl   Hugepagesize:       2048 kB\n  if test -e \"/proc/meminfo\" ; then\n    hpsk=[`cat /proc/meminfo 2>/dev/null | \\\n          grep -e '^Hugepagesize:[[:space:]]\\+[0-9]\\+[[:space:]]kB$' | \\\n          awk '{print $2}'`]\n    if test \"x${hpsk}\" != \"x\" ; then\n      je_cv_lg_hugepage=10\n      while test \"${hpsk}\" -gt 1 ; do\n        hpsk=\"$((hpsk / 2))\"\n        je_cv_lg_hugepage=\"$((je_cv_lg_hugepage + 1))\"\n      done\n    fi\n  fi\n\n  dnl Set default if unable to automatically configure.\n  if test \"x${je_cv_lg_hugepage}\" = \"x\" ; then\n    je_cv_lg_hugepage=21\n  fi\nfi\nAC_DEFINE_UNQUOTED([LG_HUGEPAGE], [${je_cv_lg_hugepage}])\n\nAC_ARG_WITH([lg_page_sizes],\n  [AS_HELP_STRING([--with-lg-page-sizes=<lg-page-sizes>],\n   [Base 2 logs of system page sizes to support])],\n  [LG_PAGE_SIZES=\"$with_lg_page_sizes\"], [LG_PAGE_SIZES=\"$LG_PAGE\"])\n\ndnl ============================================================================\ndnl jemalloc configuration.\ndnl \n\nAC_ARG_WITH([version],\n  [AS_HELP_STRING([--with-version=<major>.<minor>.<bugfix>-<nrev>-g<gid>],\n   [Version string])],\n  [\n    echo \"${with_version}\" | grep ['^[0-9]\\+\\.[0-9]\\+\\.[0-9]\\+-[0-9]\\+-g[0-9a-f]\\+$'] 2>&1 1>/dev/null\n    if test $? -eq 0 ; then\n      echo \"$with_version\" > \"${objroot}VERSION\"\n    else\n      echo \"${with_version}\" | grep ['^VERSION$'] 2>&1 1>/dev/null\n      if test $? -ne 0 ; then\n        AC_MSG_ERROR([${with_version} does not match <major>.<minor>.<bugfix>-<nrev>-g<gid> or VERSION])\n      fi\n    fi\n  ], [\n    dnl Set VERSION if source directory is inside a git repository.\n    if test \"x`test ! \\\"${srcroot}\\\" && cd \\\"${srcroot}\\\"; git rev-parse --is-inside-work-tree 2>/dev/null`\" = \"xtrue\" ; then\n      dnl Pattern globs aren't powerful enough to match both single- and\n      dnl double-digit version numbers, so iterate over patterns to support up\n      dnl to version 99.99.99 without any accidental matches.\n      for pattern in ['[0-9].[0-9].[0-9]' '[0-9].[0-9].[0-9][0-9]' \\\n                     '[0-9].[0-9][0-9].[0-9]' '[0-9].[0-9][0-9].[0-9][0-9]' \\\n                     '[0-9][0-9].[0-9].[0-9]' '[0-9][0-9].[0-9].[0-9][0-9]' \\\n                     '[0-9][0-9].[0-9][0-9].[0-9]' \\\n                     '[0-9][0-9].[0-9][0-9].[0-9][0-9]']; do\n        (test ! \"${srcroot}\" && cd \"${srcroot}\"; git describe --long --abbrev=40 --match=\"${pattern}\") > \"${objroot}VERSION.tmp\" 2>/dev/null\n        if test $? -eq 0 ; then\n          mv \"${objroot}VERSION.tmp\" \"${objroot}VERSION\"\n          break\n        fi\n      done\n    fi\n    rm -f \"${objroot}VERSION.tmp\"\n  ])\n\nif test ! -e \"${objroot}VERSION\" ; then\n  if test ! -e \"${srcroot}VERSION\" ; then\n    AC_MSG_RESULT(\n      [Missing VERSION file, and unable to generate it; creating bogus VERSION])\n    echo \"0.0.0-0-g0000000000000000000000000000000000000000\" > \"${objroot}VERSION\"\n  else\n    cp ${srcroot}VERSION ${objroot}VERSION\n  fi\nfi\njemalloc_version=`cat \"${objroot}VERSION\"`\njemalloc_version_major=`echo ${jemalloc_version} | tr \".g-\" \" \" | awk '{print [$]1}'`\njemalloc_version_minor=`echo ${jemalloc_version} | tr \".g-\" \" \" | awk '{print [$]2}'`\njemalloc_version_bugfix=`echo ${jemalloc_version} | tr \".g-\" \" \" | awk '{print [$]3}'`\njemalloc_version_nrev=`echo ${jemalloc_version} | tr \".g-\" \" \" | awk '{print [$]4}'`\njemalloc_version_gid=`echo ${jemalloc_version} | tr \".g-\" \" \" | awk '{print [$]5}'`\nAC_SUBST([jemalloc_version])\nAC_SUBST([jemalloc_version_major])\nAC_SUBST([jemalloc_version_minor])\nAC_SUBST([jemalloc_version_bugfix])\nAC_SUBST([jemalloc_version_nrev])\nAC_SUBST([jemalloc_version_gid])\n\ndnl ============================================================================\ndnl Configure pthreads.\n\nif test \"x$abi\" != \"xpecoff\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_PTHREAD], [ ])\n  AC_CHECK_HEADERS([pthread.h], , [AC_MSG_ERROR([pthread.h is missing])])\n  dnl Some systems may embed pthreads functionality in libc; check for libpthread\n  dnl first, but try libc too before failing.\n  AC_CHECK_LIB([pthread], [pthread_create], [JE_APPEND_VS(LIBS, -lpthread)],\n               [AC_SEARCH_LIBS([pthread_create], , ,\n                               AC_MSG_ERROR([libpthread is missing]))])\n  wrap_syms=\"${wrap_syms} pthread_create\"\n  have_pthread=\"1\"\n  dnl Check if we have dlsym support.\n  have_dlsym=\"1\"\n  AC_CHECK_HEADERS([dlfcn.h],\n    AC_CHECK_FUNC([dlsym], [],\n      [AC_CHECK_LIB([dl], [dlsym], [LIBS=\"$LIBS -ldl\"], [have_dlsym=\"0\"])]),\n    [have_dlsym=\"0\"])\n  if test \"x$have_dlsym\" = \"x1\" ; then\n    AC_DEFINE([JEMALLOC_HAVE_DLSYM], [ ])\n  fi\n  JE_COMPILABLE([pthread_atfork(3)], [\n#include <pthread.h>\n], [\n  pthread_atfork((void *)0, (void *)0, (void *)0);\n], [je_cv_pthread_atfork])\n  if test \"x${je_cv_pthread_atfork}\" = \"xyes\" ; then\n    AC_DEFINE([JEMALLOC_HAVE_PTHREAD_ATFORK], [ ])\n  fi\nfi\n\nJE_APPEND_VS(CPPFLAGS, -D_REENTRANT)\n\ndnl Check whether clock_gettime(2) is in libc or librt.\nAC_SEARCH_LIBS([clock_gettime], [rt])\n\ndnl Cray wrapper compiler often adds `-lrt` when using `-static`. Check with\ndnl `-dynamic` as well in case a user tries to dynamically link in jemalloc\nif test \"x$je_cv_cray_prgenv_wrapper\" = \"xyes\" ; then\n  if test \"$ac_cv_search_clock_gettime\" != \"-lrt\"; then\n    JE_CFLAGS_SAVE()\n\n    unset ac_cv_search_clock_gettime\n    JE_CFLAGS_ADD([-dynamic])\n    AC_SEARCH_LIBS([clock_gettime], [rt])\n\n    JE_CFLAGS_RESTORE()\n  fi\nfi\n\ndnl check for CLOCK_MONOTONIC_COARSE (Linux-specific).\nJE_COMPILABLE([clock_gettime(CLOCK_MONOTONIC_COARSE, ...)], [\n#include <time.h>\n], [\n\tstruct timespec ts;\n\n\tclock_gettime(CLOCK_MONOTONIC_COARSE, &ts);\n], [je_cv_clock_monotonic_coarse])\nif test \"x${je_cv_clock_monotonic_coarse}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_CLOCK_MONOTONIC_COARSE])\nfi\n\ndnl check for CLOCK_MONOTONIC.\nJE_COMPILABLE([clock_gettime(CLOCK_MONOTONIC, ...)], [\n#include <unistd.h>\n#include <time.h>\n], [\n\tstruct timespec ts;\n\n\tclock_gettime(CLOCK_MONOTONIC, &ts);\n#if !defined(_POSIX_MONOTONIC_CLOCK) || _POSIX_MONOTONIC_CLOCK < 0\n#  error _POSIX_MONOTONIC_CLOCK missing/invalid\n#endif\n], [je_cv_clock_monotonic])\nif test \"x${je_cv_clock_monotonic}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_CLOCK_MONOTONIC])\nfi\n\ndnl Check for mach_absolute_time().\nJE_COMPILABLE([mach_absolute_time()], [\n#include <mach/mach_time.h>\n], [\n\tmach_absolute_time();\n], [je_cv_mach_absolute_time])\nif test \"x${je_cv_mach_absolute_time}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_MACH_ABSOLUTE_TIME])\nfi\n\ndnl Use syscall(2) (if available) by default.\nAC_ARG_ENABLE([syscall],\n  [AS_HELP_STRING([--disable-syscall], [Disable use of syscall(2)])],\n[if test \"x$enable_syscall\" = \"xno\" ; then\n  enable_syscall=\"0\"\nelse\n  enable_syscall=\"1\"\nfi\n],\n[enable_syscall=\"1\"]\n)\nif test \"x$enable_syscall\" = \"x1\" ; then\n  dnl Check if syscall(2) is usable.  Treat warnings as errors, so that e.g. OS\n  dnl X 10.12's deprecation warning prevents use.\n  JE_CFLAGS_SAVE()\n  JE_CFLAGS_ADD([-Werror])\n  JE_COMPILABLE([syscall(2)], [\n#include <sys/syscall.h>\n#include <unistd.h>\n], [\n\tsyscall(SYS_write, 2, \"hello\", 5);\n],\n                [je_cv_syscall])\n  JE_CFLAGS_RESTORE()\n  if test \"x$je_cv_syscall\" = \"xyes\" ; then\n    AC_DEFINE([JEMALLOC_USE_SYSCALL], [ ])\n  fi\nfi\n\ndnl Check if the GNU-specific secure_getenv function exists.\nAC_CHECK_FUNC([secure_getenv],\n              [have_secure_getenv=\"1\"],\n              [have_secure_getenv=\"0\"]\n             )\nif test \"x$have_secure_getenv\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_SECURE_GETENV], [ ])\nfi\n\ndnl Check if the GNU-specific sched_getcpu function exists.\nAC_CHECK_FUNC([sched_getcpu],\n              [have_sched_getcpu=\"1\"],\n              [have_sched_getcpu=\"0\"]\n             )\nif test \"x$have_sched_getcpu\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_SCHED_GETCPU], [ ])\nfi\n\ndnl Check if the GNU-specific sched_setaffinity function exists.\nAC_CHECK_FUNC([sched_setaffinity],\n              [have_sched_setaffinity=\"1\"],\n              [have_sched_setaffinity=\"0\"]\n             )\nif test \"x$have_sched_setaffinity\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_SCHED_SETAFFINITY], [ ])\nfi\n\ndnl Check if the Solaris/BSD issetugid function exists.\nAC_CHECK_FUNC([issetugid],\n              [have_issetugid=\"1\"],\n              [have_issetugid=\"0\"]\n             )\nif test \"x$have_issetugid\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_ISSETUGID], [ ])\nfi\n\ndnl Check whether the BSD-specific _malloc_thread_cleanup() exists.  If so, use\ndnl it rather than pthreads TSD cleanup functions to support cleanup during\ndnl thread exit, in order to avoid pthreads library recursion during\ndnl bootstrapping.\nAC_CHECK_FUNC([_malloc_thread_cleanup],\n              [have__malloc_thread_cleanup=\"1\"],\n              [have__malloc_thread_cleanup=\"0\"]\n             )\nif test \"x$have__malloc_thread_cleanup\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_MALLOC_THREAD_CLEANUP], [ ])\n  wrap_syms=\"${wrap_syms} _malloc_thread_cleanup\"\n  force_tls=\"1\"\nfi\n\ndnl Check whether the BSD-specific _pthread_mutex_init_calloc_cb() exists.  If\ndnl so, mutex initialization causes allocation, and we need to implement this\ndnl callback function in order to prevent recursive allocation.\nAC_CHECK_FUNC([_pthread_mutex_init_calloc_cb],\n              [have__pthread_mutex_init_calloc_cb=\"1\"],\n              [have__pthread_mutex_init_calloc_cb=\"0\"]\n             )\nif test \"x$have__pthread_mutex_init_calloc_cb\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_MUTEX_INIT_CB])\n  wrap_syms=\"${wrap_syms} _malloc_prefork _malloc_postfork\"\nfi\n\ndnl Disable lazy locking by default.\nAC_ARG_ENABLE([lazy_lock],\n  [AS_HELP_STRING([--enable-lazy-lock],\n  [Enable lazy locking (only lock when multi-threaded)])],\n[if test \"x$enable_lazy_lock\" = \"xno\" ; then\n  enable_lazy_lock=\"0\"\nelse\n  enable_lazy_lock=\"1\"\nfi\n],\n[enable_lazy_lock=\"\"]\n)\nif test \"x${enable_lazy_lock}\" = \"x\" ; then\n  if test \"x${force_lazy_lock}\" = \"x1\" ; then\n    AC_MSG_RESULT([Forcing lazy-lock to avoid allocator/threading bootstrap issues])\n    enable_lazy_lock=\"1\"\n  else\n    enable_lazy_lock=\"0\"\n  fi\nfi\nif test \"x${enable_lazy_lock}\" = \"x1\" -a \"x${abi}\" = \"xpecoff\" ; then\n  AC_MSG_RESULT([Forcing no lazy-lock because thread creation monitoring is unimplemented])\n  enable_lazy_lock=\"0\"\nfi\nif test \"x$enable_lazy_lock\" = \"x1\" ; then\n  if test \"x$have_dlsym\" = \"x1\" ; then\n    AC_DEFINE([JEMALLOC_LAZY_LOCK], [ ])\n  else\n    AC_MSG_ERROR([Missing dlsym support: lazy-lock cannot be enabled.])\n  fi\nfi\nAC_SUBST([enable_lazy_lock])\n\ndnl Automatically configure TLS.\nif test \"x${force_tls}\" = \"x1\" ; then\n  enable_tls=\"1\"\nelif test \"x${force_tls}\" = \"x0\" ; then\n  enable_tls=\"0\"\nelse\n  enable_tls=\"1\"\nfi\nif test \"x${enable_tls}\" = \"x1\" ; then\nAC_MSG_CHECKING([for TLS])\nAC_COMPILE_IFELSE([AC_LANG_PROGRAM(\n[[\n    __thread int x;\n]], [[\n    x = 42;\n\n    return 0;\n]])],\n              AC_MSG_RESULT([yes]),\n              AC_MSG_RESULT([no])\n              enable_tls=\"0\")\nelse\n  enable_tls=\"0\"\nfi\nAC_SUBST([enable_tls])\nif test \"x${enable_tls}\" = \"x1\" ; then\n  AC_DEFINE_UNQUOTED([JEMALLOC_TLS], [ ])\nfi\n\ndnl ============================================================================\ndnl Check for C11 atomics.\n\nJE_COMPILABLE([C11 atomics], [\n#include <stdint.h>\n#if (__STDC_VERSION__ >= 201112L) && !defined(__STDC_NO_ATOMICS__)\n#include <stdatomic.h>\n#else\n#error Atomics not available\n#endif\n], [\n    uint64_t *p = (uint64_t *)0;\n    uint64_t x = 1;\n    volatile atomic_uint_least64_t *a = (volatile atomic_uint_least64_t *)p;\n    uint64_t r = atomic_fetch_add(a, x) + x;\n    return r == 0;\n], [je_cv_c11_atomics])\nif test \"x${je_cv_c11_atomics}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_C11_ATOMICS])\nfi\n\ndnl ============================================================================\ndnl Check for GCC-style __atomic atomics.\n\nJE_COMPILABLE([GCC __atomic atomics], [\n], [\n    int x = 0;\n    int val = 1;\n    int y = __atomic_fetch_add(&x, val, __ATOMIC_RELAXED);\n    int after_add = x;\n    return after_add == 1;\n], [je_cv_gcc_atomic_atomics])\nif test \"x${je_cv_gcc_atomic_atomics}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_GCC_ATOMIC_ATOMICS])\nfi\n\ndnl ============================================================================\ndnl Check for GCC-style __sync atomics.\n\nJE_COMPILABLE([GCC __sync atomics], [\n], [\n    int x = 0;\n    int before_add = __sync_fetch_and_add(&x, 1);\n    int after_add = x;\n    return (before_add == 0) && (after_add == 1);\n], [je_cv_gcc_sync_atomics])\nif test \"x${je_cv_gcc_sync_atomics}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_GCC_SYNC_ATOMICS])\nfi\n\ndnl ============================================================================\ndnl Check for atomic(3) operations as provided on Darwin.\ndnl We need this not for the atomic operations (which are provided above), but\ndnl rather for the OSSpinLock type it exposes.\n\nJE_COMPILABLE([Darwin OSAtomic*()], [\n#include <libkern/OSAtomic.h>\n#include <inttypes.h>\n], [\n\t{\n\t\tint32_t x32 = 0;\n\t\tvolatile int32_t *x32p = &x32;\n\t\tOSAtomicAdd32(1, x32p);\n\t}\n\t{\n\t\tint64_t x64 = 0;\n\t\tvolatile int64_t *x64p = &x64;\n\t\tOSAtomicAdd64(1, x64p);\n\t}\n], [je_cv_osatomic])\nif test \"x${je_cv_osatomic}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_OSATOMIC], [ ])\nfi\n\ndnl ============================================================================\ndnl Check for madvise(2).\n\nJE_COMPILABLE([madvise(2)], [\n#include <sys/mman.h>\n], [\n\tmadvise((void *)0, 0, 0);\n], [je_cv_madvise])\nif test \"x${je_cv_madvise}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_MADVISE], [ ])\n\n  dnl Check for madvise(..., MADV_FREE).\n  JE_COMPILABLE([madvise(..., MADV_FREE)], [\n#include <sys/mman.h>\n], [\n\tmadvise((void *)0, 0, MADV_FREE);\n], [je_cv_madv_free])\n  if test \"x${je_cv_madv_free}\" = \"xyes\" ; then\n    AC_DEFINE([JEMALLOC_PURGE_MADVISE_FREE], [ ])\n  fi\n\n  dnl Check for madvise(..., MADV_DONTNEED).\n  JE_COMPILABLE([madvise(..., MADV_DONTNEED)], [\n#include <sys/mman.h>\n], [\n\tmadvise((void *)0, 0, MADV_DONTNEED);\n], [je_cv_madv_dontneed])\n  if test \"x${je_cv_madv_dontneed}\" = \"xyes\" ; then\n    AC_DEFINE([JEMALLOC_PURGE_MADVISE_DONTNEED], [ ])\n  fi\n\n  dnl Check for madvise(..., MADV_[NO]HUGEPAGE).\n  JE_COMPILABLE([madvise(..., MADV_[[NO]]HUGEPAGE)], [\n#include <sys/mman.h>\n], [\n\tmadvise((void *)0, 0, MADV_HUGEPAGE);\n\tmadvise((void *)0, 0, MADV_NOHUGEPAGE);\n], [je_cv_thp])\nfi\n\ndnl Enable transparent huge page support by default.\nAC_ARG_ENABLE([thp],\n  [AS_HELP_STRING([--disable-thp],\n                  [Disable transparent huge page support])],\n[if test \"x$enable_thp\" = \"xno\" -o \"x${je_cv_thp}\" != \"xyes\" ; then\n  enable_thp=\"0\"\nelse\n  enable_thp=\"1\"\nfi\n],\n[if test \"x${je_cv_thp}\" = \"xyes\" ; then\n  enable_thp=\"1\"\nelse\n  enable_thp=\"0\"\nfi\n])\nif test \"x$enable_thp\" = \"x1\" ; then\n  AC_DEFINE([JEMALLOC_THP], [ ])\nfi\nAC_SUBST([enable_thp])\n\ndnl ============================================================================\ndnl Check whether __sync_{add,sub}_and_fetch() are available despite\ndnl __GCC_HAVE_SYNC_COMPARE_AND_SWAP_n macros being undefined.\n\nAC_DEFUN([JE_SYNC_COMPARE_AND_SWAP_CHECK],[\n  AC_CACHE_CHECK([whether to force $1-bit __sync_{add,sub}_and_fetch()],\n               [je_cv_sync_compare_and_swap_$2],\n               [AC_LINK_IFELSE([AC_LANG_PROGRAM([\n                                                 #include <stdint.h>\n                                                ],\n                                                [\n                                                 #ifndef __GCC_HAVE_SYNC_COMPARE_AND_SWAP_$2\n                                                 {\n                                                    uint$1_t x$1 = 0;\n                                                    __sync_add_and_fetch(&x$1, 42);\n                                                    __sync_sub_and_fetch(&x$1, 1);\n                                                 }\n                                                 #else\n                                                 #error __GCC_HAVE_SYNC_COMPARE_AND_SWAP_$2 is defined, no need to force\n                                                 #endif\n                                                ])],\n                               [je_cv_sync_compare_and_swap_$2=yes],\n                               [je_cv_sync_compare_and_swap_$2=no])])\n\n  if test \"x${je_cv_sync_compare_and_swap_$2}\" = \"xyes\" ; then\n    AC_DEFINE([JE_FORCE_SYNC_COMPARE_AND_SWAP_$2], [ ])\n  fi\n])\n\nif test \"x${je_cv_atomic9}\" != \"xyes\" -a \"x${je_cv_osatomic}\" != \"xyes\" ; then\n  JE_SYNC_COMPARE_AND_SWAP_CHECK(32, 4)\n  JE_SYNC_COMPARE_AND_SWAP_CHECK(64, 8)\nfi\n\ndnl ============================================================================\ndnl Check for __builtin_clz() and __builtin_clzl().\n\nAC_CACHE_CHECK([for __builtin_clz],\n               [je_cv_builtin_clz],\n               [AC_LINK_IFELSE([AC_LANG_PROGRAM([],\n                                                [\n                                                {\n                                                        unsigned x = 0;\n                                                        int y = __builtin_clz(x);\n                                                }\n                                                {\n                                                        unsigned long x = 0;\n                                                        int y = __builtin_clzl(x);\n                                                }\n                                                ])],\n                               [je_cv_builtin_clz=yes],\n                               [je_cv_builtin_clz=no])])\n\nif test \"x${je_cv_builtin_clz}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_BUILTIN_CLZ], [ ])\nfi\n\ndnl ============================================================================\ndnl Check for os_unfair_lock operations as provided on Darwin.\n\nJE_COMPILABLE([Darwin os_unfair_lock_*()], [\n#include <os/lock.h>\n#include <AvailabilityMacros.h>\n], [\n\t#if MAC_OS_X_VERSION_MIN_REQUIRED < 101200\n\t#error \"os_unfair_lock is not supported\"\n\t#else\n\tos_unfair_lock lock = OS_UNFAIR_LOCK_INIT;\n\tos_unfair_lock_lock(&lock);\n\tos_unfair_lock_unlock(&lock);\n\t#endif\n], [je_cv_os_unfair_lock])\nif test \"x${je_cv_os_unfair_lock}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_OS_UNFAIR_LOCK], [ ])\nfi\n\ndnl ============================================================================\ndnl Check for spinlock(3) operations as provided on Darwin.\n\nJE_COMPILABLE([Darwin OSSpin*()], [\n#include <libkern/OSAtomic.h>\n#include <inttypes.h>\n], [\n\tOSSpinLock lock = 0;\n\tOSSpinLockLock(&lock);\n\tOSSpinLockUnlock(&lock);\n], [je_cv_osspin])\nif test \"x${je_cv_osspin}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_OSSPIN], [ ])\nfi\n\ndnl ============================================================================\ndnl Darwin-related configuration.\n\nAC_ARG_ENABLE([zone-allocator],\n  [AS_HELP_STRING([--disable-zone-allocator],\n                  [Disable zone allocator for Darwin])],\n[if test \"x$enable_zone_allocator\" = \"xno\" ; then\n  enable_zone_allocator=\"0\"\nelse\n  enable_zone_allocator=\"1\"\nfi\n],\n[if test \"x${abi}\" = \"xmacho\"; then\n  enable_zone_allocator=\"1\"\nfi\n]\n)\nAC_SUBST([enable_zone_allocator])\n\nif test \"x${enable_zone_allocator}\" = \"x1\" ; then\n  if test \"x${abi}\" != \"xmacho\"; then\n    AC_MSG_ERROR([--enable-zone-allocator is only supported on Darwin])\n  fi\n  AC_DEFINE([JEMALLOC_ZONE], [ ])\nfi\n\ndnl ============================================================================\ndnl Enable background threads if possible.\n\nif test \"x${have_pthread}\" = \"x1\" -a \"x${have_dlsym}\" = \"x1\" \\\n    -a \"x${je_cv_os_unfair_lock}\" != \"xyes\" \\\n    -a \"x${je_cv_osspin}\" != \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_BACKGROUND_THREAD])\nfi\n\ndnl ============================================================================\ndnl Check for glibc malloc hooks\n\nJE_COMPILABLE([glibc malloc hook], [\n#include <stddef.h>\n\nextern void (* __free_hook)(void *ptr);\nextern void *(* __malloc_hook)(size_t size);\nextern void *(* __realloc_hook)(void *ptr, size_t size);\n], [\n  void *ptr = 0L;\n  if (__malloc_hook) ptr = __malloc_hook(1);\n  if (__realloc_hook) ptr = __realloc_hook(ptr, 2);\n  if (__free_hook && ptr) __free_hook(ptr);\n], [je_cv_glibc_malloc_hook])\nif test \"x${je_cv_glibc_malloc_hook}\" = \"xyes\" ; then\n  if test \"x${JEMALLOC_PREFIX}\" = \"x\" ; then\n    AC_DEFINE([JEMALLOC_GLIBC_MALLOC_HOOK], [ ])\n    wrap_syms=\"${wrap_syms} __free_hook __malloc_hook __realloc_hook\"\n  fi\nfi\n\nJE_COMPILABLE([glibc memalign hook], [\n#include <stddef.h>\n\nextern void *(* __memalign_hook)(size_t alignment, size_t size);\n], [\n  void *ptr = 0L;\n  if (__memalign_hook) ptr = __memalign_hook(16, 7);\n], [je_cv_glibc_memalign_hook])\nif test \"x${je_cv_glibc_memalign_hook}\" = \"xyes\" ; then\n  if test \"x${JEMALLOC_PREFIX}\" = \"x\" ; then\n    AC_DEFINE([JEMALLOC_GLIBC_MEMALIGN_HOOK], [ ])\n    wrap_syms=\"${wrap_syms} __memalign_hook\"\n  fi\nfi\n\nJE_COMPILABLE([pthreads adaptive mutexes], [\n#include <pthread.h>\n], [\n  pthread_mutexattr_t attr;\n  pthread_mutexattr_init(&attr);\n  pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_ADAPTIVE_NP);\n  pthread_mutexattr_destroy(&attr);\n], [je_cv_pthread_mutex_adaptive_np])\nif test \"x${je_cv_pthread_mutex_adaptive_np}\" = \"xyes\" ; then\n  AC_DEFINE([JEMALLOC_HAVE_PTHREAD_MUTEX_ADAPTIVE_NP], [ ])\nfi\n\ndnl ============================================================================\ndnl Check for typedefs, structures, and compiler characteristics.\nAC_HEADER_STDBOOL\n\ndnl ============================================================================\ndnl Define commands that generate output files.\n\nAC_CONFIG_COMMANDS([include/jemalloc/internal/public_symbols.txt], [\n  f=\"${objroot}include/jemalloc/internal/public_symbols.txt\"\n  mkdir -p \"${objroot}include/jemalloc/internal\"\n  cp /dev/null \"${f}\"\n  for nm in `echo ${mangling_map} |tr ',' ' '` ; do\n    n=`echo ${nm} |tr ':' ' ' |awk '{print $[]1}'`\n    m=`echo ${nm} |tr ':' ' ' |awk '{print $[]2}'`\n    echo \"${n}:${m}\" >> \"${f}\"\n    dnl Remove name from public_syms so that it isn't redefined later.\n    public_syms=`for sym in ${public_syms}; do echo \"${sym}\"; done |grep -v \"^${n}\\$\" |tr '\\n' ' '`\n  done\n  for sym in ${public_syms} ; do\n    n=\"${sym}\"\n    m=\"${JEMALLOC_PREFIX}${sym}\"\n    echo \"${n}:${m}\" >> \"${f}\"\n  done\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n  mangling_map=\"${mangling_map}\"\n  public_syms=\"${public_syms}\"\n  JEMALLOC_PREFIX=\"${JEMALLOC_PREFIX}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/internal/private_symbols.awk], [\n  f=\"${objroot}include/jemalloc/internal/private_symbols.awk\"\n  mkdir -p \"${objroot}include/jemalloc/internal\"\n  export_syms=`for sym in ${public_syms}; do echo \"${JEMALLOC_PREFIX}${sym}\"; done; for sym in ${wrap_syms}; do echo \"${sym}\"; done;`\n  \"${srcdir}/include/jemalloc/internal/private_symbols.sh\" \"${SYM_PREFIX}\" ${export_syms} > \"${objroot}include/jemalloc/internal/private_symbols.awk\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n  public_syms=\"${public_syms}\"\n  wrap_syms=\"${wrap_syms}\"\n  SYM_PREFIX=\"${SYM_PREFIX}\"\n  JEMALLOC_PREFIX=\"${JEMALLOC_PREFIX}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/internal/private_symbols_jet.awk], [\n  f=\"${objroot}include/jemalloc/internal/private_symbols_jet.awk\"\n  mkdir -p \"${objroot}include/jemalloc/internal\"\n  export_syms=`for sym in ${public_syms}; do echo \"jet_${sym}\"; done; for sym in ${wrap_syms}; do echo \"${sym}\"; done;`\n  \"${srcdir}/include/jemalloc/internal/private_symbols.sh\" \"${SYM_PREFIX}\" ${export_syms} > \"${objroot}include/jemalloc/internal/private_symbols_jet.awk\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n  public_syms=\"${public_syms}\"\n  wrap_syms=\"${wrap_syms}\"\n  SYM_PREFIX=\"${SYM_PREFIX}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/internal/public_namespace.h], [\n  mkdir -p \"${objroot}include/jemalloc/internal\"\n  \"${srcdir}/include/jemalloc/internal/public_namespace.sh\" \"${objroot}include/jemalloc/internal/public_symbols.txt\" > \"${objroot}include/jemalloc/internal/public_namespace.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/internal/public_unnamespace.h], [\n  mkdir -p \"${objroot}include/jemalloc/internal\"\n  \"${srcdir}/include/jemalloc/internal/public_unnamespace.sh\" \"${objroot}include/jemalloc/internal/public_symbols.txt\" > \"${objroot}include/jemalloc/internal/public_unnamespace.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/internal/size_classes.h], [\n  mkdir -p \"${objroot}include/jemalloc/internal\"\n  \"${SHELL}\" \"${srcdir}/include/jemalloc/internal/size_classes.sh\" \"${LG_QUANTA}\" 3 \"${LG_PAGE_SIZES}\" 2 > \"${objroot}include/jemalloc/internal/size_classes.h\"\n], [\n  SHELL=\"${SHELL}\"\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n  LG_QUANTA=\"${LG_QUANTA}\"\n  LG_PAGE_SIZES=\"${LG_PAGE_SIZES}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/jemalloc_protos_jet.h], [\n  mkdir -p \"${objroot}include/jemalloc\"\n  cat \"${srcdir}/include/jemalloc/jemalloc_protos.h.in\" | sed -e 's/@je_@/jet_/g' > \"${objroot}include/jemalloc/jemalloc_protos_jet.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/jemalloc_rename.h], [\n  mkdir -p \"${objroot}include/jemalloc\"\n  \"${srcdir}/include/jemalloc/jemalloc_rename.sh\" \"${objroot}include/jemalloc/internal/public_symbols.txt\" > \"${objroot}include/jemalloc/jemalloc_rename.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/jemalloc_mangle.h], [\n  mkdir -p \"${objroot}include/jemalloc\"\n  \"${srcdir}/include/jemalloc/jemalloc_mangle.sh\" \"${objroot}include/jemalloc/internal/public_symbols.txt\" je_ > \"${objroot}include/jemalloc/jemalloc_mangle.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/jemalloc_mangle_jet.h], [\n  mkdir -p \"${objroot}include/jemalloc\"\n  \"${srcdir}/include/jemalloc/jemalloc_mangle.sh\" \"${objroot}include/jemalloc/internal/public_symbols.txt\" jet_ > \"${objroot}include/jemalloc/jemalloc_mangle_jet.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n])\nAC_CONFIG_COMMANDS([include/jemalloc/jemalloc.h], [\n  mkdir -p \"${objroot}include/jemalloc\"\n  \"${srcdir}/include/jemalloc/jemalloc.sh\" \"${objroot}\" > \"${objroot}include/jemalloc/jemalloc${install_suffix}.h\"\n], [\n  srcdir=\"${srcdir}\"\n  objroot=\"${objroot}\"\n  install_suffix=\"${install_suffix}\"\n])\n\ndnl Process .in files.\nAC_SUBST([cfghdrs_in])\nAC_SUBST([cfghdrs_out])\nAC_CONFIG_HEADERS([$cfghdrs_tup])\n\ndnl ============================================================================\ndnl Generate outputs.\n\nAC_CONFIG_FILES([$cfgoutputs_tup config.stamp bin/jemalloc-config bin/jemalloc.sh bin/jeprof])\nAC_SUBST([cfgoutputs_in])\nAC_SUBST([cfgoutputs_out])\nAC_OUTPUT\n\ndnl ============================================================================\ndnl Print out the results of configuration.\nAC_MSG_RESULT([===============================================================================])\nAC_MSG_RESULT([jemalloc version   : ${jemalloc_version}])\nAC_MSG_RESULT([library revision   : ${rev}])\nAC_MSG_RESULT([])\nAC_MSG_RESULT([CONFIG             : ${CONFIG}])\nAC_MSG_RESULT([CC                 : ${CC}])\nAC_MSG_RESULT([CONFIGURE_CFLAGS   : ${CONFIGURE_CFLAGS}])\nAC_MSG_RESULT([SPECIFIED_CFLAGS   : ${SPECIFIED_CFLAGS}])\nAC_MSG_RESULT([EXTRA_CFLAGS       : ${EXTRA_CFLAGS}])\nAC_MSG_RESULT([CPPFLAGS           : ${CPPFLAGS}])\nAC_MSG_RESULT([CXX                : ${CXX}])\nAC_MSG_RESULT([CONFIGURE_CXXFLAGS : ${CONFIGURE_CXXFLAGS}])\nAC_MSG_RESULT([SPECIFIED_CXXFLAGS : ${SPECIFIED_CXXFLAGS}])\nAC_MSG_RESULT([EXTRA_CXXFLAGS     : ${EXTRA_CXXFLAGS}])\nAC_MSG_RESULT([LDFLAGS            : ${LDFLAGS}])\nAC_MSG_RESULT([EXTRA_LDFLAGS      : ${EXTRA_LDFLAGS}])\nAC_MSG_RESULT([DSO_LDFLAGS        : ${DSO_LDFLAGS}])\nAC_MSG_RESULT([LIBS               : ${LIBS}])\nAC_MSG_RESULT([RPATH_EXTRA        : ${RPATH_EXTRA}])\nAC_MSG_RESULT([])\nAC_MSG_RESULT([XSLTPROC           : ${XSLTPROC}])\nAC_MSG_RESULT([XSLROOT            : ${XSLROOT}])\nAC_MSG_RESULT([])\nAC_MSG_RESULT([PREFIX             : ${PREFIX}])\nAC_MSG_RESULT([BINDIR             : ${BINDIR}])\nAC_MSG_RESULT([DATADIR            : ${DATADIR}])\nAC_MSG_RESULT([INCLUDEDIR         : ${INCLUDEDIR}])\nAC_MSG_RESULT([LIBDIR             : ${LIBDIR}])\nAC_MSG_RESULT([MANDIR             : ${MANDIR}])\nAC_MSG_RESULT([])\nAC_MSG_RESULT([srcroot            : ${srcroot}])\nAC_MSG_RESULT([abs_srcroot        : ${abs_srcroot}])\nAC_MSG_RESULT([objroot            : ${objroot}])\nAC_MSG_RESULT([abs_objroot        : ${abs_objroot}])\nAC_MSG_RESULT([])\nAC_MSG_RESULT([JEMALLOC_PREFIX    : ${JEMALLOC_PREFIX}])\nAC_MSG_RESULT([JEMALLOC_PRIVATE_NAMESPACE])\nAC_MSG_RESULT([                   : ${JEMALLOC_PRIVATE_NAMESPACE}])\nAC_MSG_RESULT([install_suffix     : ${install_suffix}])\nAC_MSG_RESULT([malloc_conf        : ${config_malloc_conf}])\nAC_MSG_RESULT([autogen            : ${enable_autogen}])\nAC_MSG_RESULT([debug              : ${enable_debug}])\nAC_MSG_RESULT([stats              : ${enable_stats}])\nAC_MSG_RESULT([prof               : ${enable_prof}])\nAC_MSG_RESULT([prof-libunwind     : ${enable_prof_libunwind}])\nAC_MSG_RESULT([prof-libgcc        : ${enable_prof_libgcc}])\nAC_MSG_RESULT([prof-gcc           : ${enable_prof_gcc}])\nAC_MSG_RESULT([thp                : ${enable_thp}])\nAC_MSG_RESULT([fill               : ${enable_fill}])\nAC_MSG_RESULT([utrace             : ${enable_utrace}])\nAC_MSG_RESULT([xmalloc            : ${enable_xmalloc}])\nAC_MSG_RESULT([lazy_lock          : ${enable_lazy_lock}])\nAC_MSG_RESULT([cache-oblivious    : ${enable_cache_oblivious}])\nAC_MSG_RESULT([cxx                : ${enable_cxx}])\nAC_MSG_RESULT([===============================================================================])\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/jemalloc/src/background_thread.c": "#define JEMALLOC_BACKGROUND_THREAD_C_\n#include \"jemalloc/internal/jemalloc_preamble.h\"\n#include \"jemalloc/internal/jemalloc_internal_includes.h\"\n\n#include \"jemalloc/internal/assert.h\"\n\n/******************************************************************************/\n/* Data. */\n\n/* This option should be opt-in only. */\n#define BACKGROUND_THREAD_DEFAULT false\n/* Read-only after initialization. */\nbool opt_background_thread = BACKGROUND_THREAD_DEFAULT;\n\n/* Used for thread creation, termination and stats. */\nmalloc_mutex_t background_thread_lock;\n/* Indicates global state.  Atomic because decay reads this w/o locking. */\natomic_b_t background_thread_enabled_state;\nsize_t n_background_threads;\n/* Thread info per-index. */\nbackground_thread_info_t *background_thread_info;\n\n/* False if no necessary runtime support. */\nbool can_enable_background_thread;\n\n/******************************************************************************/\n\n#ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER\n#include <dlfcn.h>\n\nstatic int (*pthread_create_fptr)(pthread_t *__restrict, const pthread_attr_t *,\n    void *(*)(void *), void *__restrict);\nstatic pthread_once_t once_control = PTHREAD_ONCE_INIT;\n\nstatic void\npthread_create_wrapper_once(void) {\n#ifdef JEMALLOC_LAZY_LOCK\n\tisthreaded = true;\n#endif\n}\n\nint\npthread_create_wrapper(pthread_t *__restrict thread, const pthread_attr_t *attr,\n    void *(*start_routine)(void *), void *__restrict arg) {\n\tpthread_once(&once_control, pthread_create_wrapper_once);\n\n\treturn pthread_create_fptr(thread, attr, start_routine, arg);\n}\n#endif /* JEMALLOC_PTHREAD_CREATE_WRAPPER */\n\n#ifndef JEMALLOC_BACKGROUND_THREAD\n#define NOT_REACHED { not_reached(); }\nbool background_thread_create(tsd_t *tsd, unsigned arena_ind) NOT_REACHED\nbool background_threads_enable(tsd_t *tsd) NOT_REACHED\nbool background_threads_disable(tsd_t *tsd) NOT_REACHED\nvoid background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,\n    arena_decay_t *decay, size_t npages_new) NOT_REACHED\nvoid background_thread_prefork0(tsdn_t *tsdn) NOT_REACHED\nvoid background_thread_prefork1(tsdn_t *tsdn) NOT_REACHED\nvoid background_thread_postfork_parent(tsdn_t *tsdn) NOT_REACHED\nvoid background_thread_postfork_child(tsdn_t *tsdn) NOT_REACHED\nbool background_thread_stats_read(tsdn_t *tsdn,\n    background_thread_stats_t *stats) NOT_REACHED\nvoid background_thread_ctl_init(tsdn_t *tsdn) NOT_REACHED\n#undef NOT_REACHED\n#else\n\nstatic bool background_thread_enabled_at_fork;\n\nstatic void\nbackground_thread_info_init(tsdn_t *tsdn, background_thread_info_t *info) {\n\tbackground_thread_wakeup_time_set(tsdn, info, 0);\n\tinfo->npages_to_purge_new = 0;\n\tif (config_stats) {\n\t\tinfo->tot_n_runs = 0;\n\t\tnstime_init(&info->tot_sleep_time, 0);\n\t}\n}\n\nstatic inline bool\nset_current_thread_affinity(UNUSED int cpu) {\n#if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)\n\tcpu_set_t cpuset;\n\tCPU_ZERO(&cpuset);\n\tCPU_SET(cpu, &cpuset);\n\tint ret = sched_setaffinity(0, sizeof(cpu_set_t), &cpuset);\n\n\treturn (ret != 0);\n#else\n\treturn false;\n#endif\n}\n\n/* Threshold for determining when to wake up the background thread. */\n#define BACKGROUND_THREAD_NPAGES_THRESHOLD UINT64_C(1024)\n#define BILLION UINT64_C(1000000000)\n/* Minimal sleep interval 100 ms. */\n#define BACKGROUND_THREAD_MIN_INTERVAL_NS (BILLION / 10)\n\nstatic inline size_t\ndecay_npurge_after_interval(arena_decay_t *decay, size_t interval) {\n\tsize_t i;\n\tuint64_t sum = 0;\n\tfor (i = 0; i < interval; i++) {\n\t\tsum += decay->backlog[i] * h_steps[i];\n\t}\n\tfor (; i < SMOOTHSTEP_NSTEPS; i++) {\n\t\tsum += decay->backlog[i] * (h_steps[i] - h_steps[i - interval]);\n\t}\n\n\treturn (size_t)(sum >> SMOOTHSTEP_BFP);\n}\n\nstatic uint64_t\narena_decay_compute_purge_interval_impl(tsdn_t *tsdn, arena_decay_t *decay,\n    extents_t *extents) {\n\tif (malloc_mutex_trylock(tsdn, &decay->mtx)) {\n\t\t/* Use minimal interval if decay is contended. */\n\t\treturn BACKGROUND_THREAD_MIN_INTERVAL_NS;\n\t}\n\n\tuint64_t interval;\n\tssize_t decay_time = atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);\n\tif (decay_time <= 0) {\n\t\t/* Purging is eagerly done or disabled currently. */\n\t\tinterval = BACKGROUND_THREAD_INDEFINITE_SLEEP;\n\t\tgoto label_done;\n\t}\n\n\tuint64_t decay_interval_ns = nstime_ns(&decay->interval);\n\tassert(decay_interval_ns > 0);\n\tsize_t npages = extents_npages_get(extents);\n\tif (npages == 0) {\n\t\tunsigned i;\n\t\tfor (i = 0; i < SMOOTHSTEP_NSTEPS; i++) {\n\t\t\tif (decay->backlog[i] > 0) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == SMOOTHSTEP_NSTEPS) {\n\t\t\t/* No dirty pages recorded.  Sleep indefinitely. */\n\t\t\tinterval = BACKGROUND_THREAD_INDEFINITE_SLEEP;\n\t\t\tgoto label_done;\n\t\t}\n\t}\n\tif (npages <= BACKGROUND_THREAD_NPAGES_THRESHOLD) {\n\t\t/* Use max interval. */\n\t\tinterval = decay_interval_ns * SMOOTHSTEP_NSTEPS;\n\t\tgoto label_done;\n\t}\n\n\tsize_t lb = BACKGROUND_THREAD_MIN_INTERVAL_NS / decay_interval_ns;\n\tsize_t ub = SMOOTHSTEP_NSTEPS;\n\t/* Minimal 2 intervals to ensure reaching next epoch deadline. */\n\tlb = (lb < 2) ? 2 : lb;\n\tif ((decay_interval_ns * ub <= BACKGROUND_THREAD_MIN_INTERVAL_NS) ||\n\t    (lb + 2 > ub)) {\n\t\tinterval = BACKGROUND_THREAD_MIN_INTERVAL_NS;\n\t\tgoto label_done;\n\t}\n\n\tassert(lb + 2 <= ub);\n\tsize_t npurge_lb, npurge_ub;\n\tnpurge_lb = decay_npurge_after_interval(decay, lb);\n\tif (npurge_lb > BACKGROUND_THREAD_NPAGES_THRESHOLD) {\n\t\tinterval = decay_interval_ns * lb;\n\t\tgoto label_done;\n\t}\n\tnpurge_ub = decay_npurge_after_interval(decay, ub);\n\tif (npurge_ub < BACKGROUND_THREAD_NPAGES_THRESHOLD) {\n\t\tinterval = decay_interval_ns * ub;\n\t\tgoto label_done;\n\t}\n\n\tunsigned n_search = 0;\n\tsize_t target, npurge;\n\twhile ((npurge_lb + BACKGROUND_THREAD_NPAGES_THRESHOLD < npurge_ub)\n\t    && (lb + 2 < ub)) {\n\t\ttarget = (lb + ub) / 2;\n\t\tnpurge = decay_npurge_after_interval(decay, target);\n\t\tif (npurge > BACKGROUND_THREAD_NPAGES_THRESHOLD) {\n\t\t\tub = target;\n\t\t\tnpurge_ub = npurge;\n\t\t} else {\n\t\t\tlb = target;\n\t\t\tnpurge_lb = npurge;\n\t\t}\n\t\tassert(n_search++ < lg_floor(SMOOTHSTEP_NSTEPS) + 1);\n\t}\n\tinterval = decay_interval_ns * (ub + lb) / 2;\nlabel_done:\n\tinterval = (interval < BACKGROUND_THREAD_MIN_INTERVAL_NS) ?\n\t    BACKGROUND_THREAD_MIN_INTERVAL_NS : interval;\n\tmalloc_mutex_unlock(tsdn, &decay->mtx);\n\n\treturn interval;\n}\n\n/* Compute purge interval for background threads. */\nstatic uint64_t\narena_decay_compute_purge_interval(tsdn_t *tsdn, arena_t *arena) {\n\tuint64_t i1, i2;\n\ti1 = arena_decay_compute_purge_interval_impl(tsdn, &arena->decay_dirty,\n\t    &arena->extents_dirty);\n\tif (i1 == BACKGROUND_THREAD_MIN_INTERVAL_NS) {\n\t\treturn i1;\n\t}\n\ti2 = arena_decay_compute_purge_interval_impl(tsdn, &arena->decay_muzzy,\n\t    &arena->extents_muzzy);\n\n\treturn i1 < i2 ? i1 : i2;\n}\n\nstatic void\nbackground_thread_sleep(tsdn_t *tsdn, background_thread_info_t *info,\n    uint64_t interval) {\n\tif (config_stats) {\n\t\tinfo->tot_n_runs++;\n\t}\n\tinfo->npages_to_purge_new = 0;\n\n\tstruct timeval tv;\n\t/* Specific clock required by timedwait. */\n\tgettimeofday(&tv, NULL);\n\tnstime_t before_sleep;\n\tnstime_init2(&before_sleep, tv.tv_sec, tv.tv_usec * 1000);\n\n\tint ret;\n\tif (interval == BACKGROUND_THREAD_INDEFINITE_SLEEP) {\n\t\tassert(background_thread_indefinite_sleep(info));\n\t\tret = pthread_cond_wait(&info->cond, &info->mtx.lock);\n\t\tassert(ret == 0);\n\t} else {\n\t\tassert(interval >= BACKGROUND_THREAD_MIN_INTERVAL_NS &&\n\t\t    interval <= BACKGROUND_THREAD_INDEFINITE_SLEEP);\n\t\t/* We need malloc clock (can be different from tv). */\n\t\tnstime_t next_wakeup;\n\t\tnstime_init(&next_wakeup, 0);\n\t\tnstime_update(&next_wakeup);\n\t\tnstime_iadd(&next_wakeup, interval);\n\t\tassert(nstime_ns(&next_wakeup) <\n\t\t    BACKGROUND_THREAD_INDEFINITE_SLEEP);\n\t\tbackground_thread_wakeup_time_set(tsdn, info,\n\t\t    nstime_ns(&next_wakeup));\n\n\t\tnstime_t ts_wakeup;\n\t\tnstime_copy(&ts_wakeup, &before_sleep);\n\t\tnstime_iadd(&ts_wakeup, interval);\n\t\tstruct timespec ts;\n\t\tts.tv_sec = (size_t)nstime_sec(&ts_wakeup);\n\t\tts.tv_nsec = (size_t)nstime_nsec(&ts_wakeup);\n\n\t\tassert(!background_thread_indefinite_sleep(info));\n\t\tret = pthread_cond_timedwait(&info->cond, &info->mtx.lock, &ts);\n\t\tassert(ret == ETIMEDOUT || ret == 0);\n\t\tbackground_thread_wakeup_time_set(tsdn, info,\n\t\t    BACKGROUND_THREAD_INDEFINITE_SLEEP);\n\t}\n\tif (config_stats) {\n\t\tgettimeofday(&tv, NULL);\n\t\tnstime_t after_sleep;\n\t\tnstime_init2(&after_sleep, tv.tv_sec, tv.tv_usec * 1000);\n\t\tif (nstime_compare(&after_sleep, &before_sleep) > 0) {\n\t\t\tnstime_subtract(&after_sleep, &before_sleep);\n\t\t\tnstime_add(&info->tot_sleep_time, &after_sleep);\n\t\t}\n\t}\n}\n\nstatic bool\nbackground_thread_pause_check(tsdn_t *tsdn, background_thread_info_t *info) {\n\tif (unlikely(info->state == background_thread_paused)) {\n\t\tmalloc_mutex_unlock(tsdn, &info->mtx);\n\t\t/* Wait on global lock to update status. */\n\t\tmalloc_mutex_lock(tsdn, &background_thread_lock);\n\t\tmalloc_mutex_unlock(tsdn, &background_thread_lock);\n\t\tmalloc_mutex_lock(tsdn, &info->mtx);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic inline void\nbackground_work_sleep_once(tsdn_t *tsdn, background_thread_info_t *info, unsigned ind) {\n\tuint64_t min_interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;\n\tunsigned narenas = narenas_total_get();\n\n\tfor (unsigned i = ind; i < narenas; i += ncpus) {\n\t\tarena_t *arena = arena_get(tsdn, i, false);\n\t\tif (!arena) {\n\t\t\tcontinue;\n\t\t}\n\t\tarena_decay(tsdn, arena, true, false);\n\t\tif (min_interval == BACKGROUND_THREAD_MIN_INTERVAL_NS) {\n\t\t\t/* Min interval will be used. */\n\t\t\tcontinue;\n\t\t}\n\t\tuint64_t interval = arena_decay_compute_purge_interval(tsdn,\n\t\t    arena);\n\t\tassert(interval >= BACKGROUND_THREAD_MIN_INTERVAL_NS);\n\t\tif (min_interval > interval) {\n\t\t\tmin_interval = interval;\n\t\t}\n\t}\n\tbackground_thread_sleep(tsdn, info, min_interval);\n}\n\nstatic bool\nbackground_threads_disable_single(tsd_t *tsd, background_thread_info_t *info) {\n\tif (info == &background_thread_info[0]) {\n\t\tmalloc_mutex_assert_owner(tsd_tsdn(tsd),\n\t\t    &background_thread_lock);\n\t} else {\n\t\tmalloc_mutex_assert_not_owner(tsd_tsdn(tsd),\n\t\t    &background_thread_lock);\n\t}\n\n\tpre_reentrancy(tsd);\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\tbool has_thread;\n\tassert(info->state != background_thread_paused);\n\tif (info->state == background_thread_started) {\n\t\thas_thread = true;\n\t\tinfo->state = background_thread_stopped;\n\t\tpthread_cond_signal(&info->cond);\n\t} else {\n\t\thas_thread = false;\n\t}\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\n\tif (!has_thread) {\n\t\tpost_reentrancy(tsd);\n\t\treturn false;\n\t}\n\tvoid *ret;\n\tif (pthread_join(info->thread, &ret)) {\n\t\tpost_reentrancy(tsd);\n\t\treturn true;\n\t}\n\tassert(ret == NULL);\n\tn_background_threads--;\n\tpost_reentrancy(tsd);\n\n\treturn false;\n}\n\nstatic void *background_thread_entry(void *ind_arg);\n\nstatic int\nbackground_thread_create_signals_masked(pthread_t *thread,\n    const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg) {\n\t/*\n\t * Mask signals during thread creation so that the thread inherits\n\t * an empty signal set.\n\t */\n\tsigset_t set;\n\tsigemptyset(&set);\n\tsigset_t oldset;\n\tint mask_err = pthread_sigmask(SIG_SETMASK, &set, &oldset);\n\tif (mask_err != 0) {\n\t\treturn mask_err;\n\t}\n\tint create_err = pthread_create_wrapper(thread, attr, start_routine,\n\t    arg);\n\t/*\n\t * Restore the signal mask.  Failure to restore the signal mask here\n\t * changes program behavior.\n\t */\n\tint restore_err = pthread_sigmask(SIG_SETMASK, &oldset, NULL);\n\tif (restore_err != 0) {\n\t\tmalloc_printf(\"<jemalloc>: background thread creation \"\n\t\t    \"failed (%d), and signal mask restoration failed \"\n\t\t    \"(%d)\\n\", create_err, restore_err);\n\t\tif (opt_abort) {\n\t\t\tabort();\n\t\t}\n\t}\n\treturn create_err;\n}\n\nstatic void\ncheck_background_thread_creation(tsd_t *tsd, unsigned *n_created,\n    bool *created_threads) {\n\tif (likely(*n_created == n_background_threads)) {\n\t\treturn;\n\t}\n\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_info[0].mtx);\nlabel_restart:\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);\n\tfor (unsigned i = 1; i < ncpus; i++) {\n\t\tif (created_threads[i]) {\n\t\t\tcontinue;\n\t\t}\n\t\tbackground_thread_info_t *info = &background_thread_info[i];\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\t\tassert(info->state != background_thread_paused);\n\t\tbool create = (info->state == background_thread_started);\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\t\tif (!create) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * To avoid deadlock with prefork handlers (which waits for the\n\t\t * mutex held here), unlock before calling pthread_create().\n\t\t */\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);\n\n\t\tpre_reentrancy(tsd);\n\t\tint err = background_thread_create_signals_masked(&info->thread,\n\t\t    NULL, background_thread_entry, (void *)(uintptr_t)i);\n\t\tpost_reentrancy(tsd);\n\n\t\tif (err == 0) {\n\t\t\t(*n_created)++;\n\t\t\tcreated_threads[i] = true;\n\t\t} else {\n\t\t\tmalloc_printf(\"<jemalloc>: background thread \"\n\t\t\t    \"creation failed (%d)\\n\", err);\n\t\t\tif (opt_abort) {\n\t\t\t\tabort();\n\t\t\t}\n\t\t}\n\t\t/* Restart since we unlocked. */\n\t\tgoto label_restart;\n\t}\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &background_thread_info[0].mtx);\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);\n}\n\nstatic void\nbackground_thread0_work(tsd_t *tsd) {\n\t/* Thread0 is also responsible for launching / terminating threads. */\n\tVARIABLE_ARRAY(bool, created_threads, ncpus);\n\tunsigned i;\n\tfor (i = 1; i < ncpus; i++) {\n\t\tcreated_threads[i] = false;\n\t}\n\t/* Start working, and create more threads when asked. */\n\tunsigned n_created = 1;\n\twhile (background_thread_info[0].state != background_thread_stopped) {\n\t\tif (background_thread_pause_check(tsd_tsdn(tsd),\n\t\t    &background_thread_info[0])) {\n\t\t\tcontinue;\n\t\t}\n\t\tcheck_background_thread_creation(tsd, &n_created,\n\t\t    (bool *)&created_threads);\n\t\tbackground_work_sleep_once(tsd_tsdn(tsd),\n\t\t    &background_thread_info[0], 0);\n\t}\n\n\t/*\n\t * Shut down other threads at exit.  Note that the ctl thread is holding\n\t * the global background_thread mutex (and is waiting) for us.\n\t */\n\tassert(!background_thread_enabled());\n\tfor (i = 1; i < ncpus; i++) {\n\t\tbackground_thread_info_t *info = &background_thread_info[i];\n\t\tassert(info->state != background_thread_paused);\n\t\tif (created_threads[i]) {\n\t\t\tbackground_threads_disable_single(tsd, info);\n\t\t} else {\n\t\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\t\t\t/* Clear in case the thread wasn't created. */\n\t\t\tinfo->state = background_thread_stopped;\n\t\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\t\t}\n\t}\n\tbackground_thread_info[0].state = background_thread_stopped;\n\tassert(n_background_threads == 1);\n}\n\nstatic void\nbackground_work(tsd_t *tsd, unsigned ind) {\n\tbackground_thread_info_t *info = &background_thread_info[ind];\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\tbackground_thread_wakeup_time_set(tsd_tsdn(tsd), info,\n\t    BACKGROUND_THREAD_INDEFINITE_SLEEP);\n\tif (ind == 0) {\n\t\tbackground_thread0_work(tsd);\n\t} else {\n\t\twhile (info->state != background_thread_stopped) {\n\t\t\tif (background_thread_pause_check(tsd_tsdn(tsd),\n\t\t\t    info)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbackground_work_sleep_once(tsd_tsdn(tsd), info, ind);\n\t\t}\n\t}\n\tassert(info->state == background_thread_stopped);\n\tbackground_thread_wakeup_time_set(tsd_tsdn(tsd), info, 0);\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n}\n\nstatic void *\nbackground_thread_entry(void *ind_arg) {\n\tunsigned thread_ind = (unsigned)(uintptr_t)ind_arg;\n\tassert(thread_ind < ncpus);\n\n\tif (opt_percpu_arena != percpu_arena_disabled) {\n\t\tset_current_thread_affinity((int)thread_ind);\n\t}\n\t/*\n\t * Start periodic background work.  We use internal tsd which avoids\n\t * side effects, for example triggering new arena creation (which in\n\t * turn triggers another background thread creation).\n\t */\n\tbackground_work(tsd_internal_fetch(), thread_ind);\n\tassert(pthread_equal(pthread_self(),\n\t    background_thread_info[thread_ind].thread));\n\n\treturn NULL;\n}\n\nstatic void\nbackground_thread_init(tsd_t *tsd, background_thread_info_t *info) {\n\tmalloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);\n\tinfo->state = background_thread_started;\n\tbackground_thread_info_init(tsd_tsdn(tsd), info);\n\tn_background_threads++;\n}\n\n/* Create a new background thread if needed. */\nbool\nbackground_thread_create(tsd_t *tsd, unsigned arena_ind) {\n\tassert(have_background_thread);\n\tmalloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);\n\n\t/* We create at most NCPUs threads. */\n\tsize_t thread_ind = arena_ind % ncpus;\n\tbackground_thread_info_t *info = &background_thread_info[thread_ind];\n\n\tbool need_new_thread;\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\tneed_new_thread = background_thread_enabled() &&\n\t    (info->state == background_thread_stopped);\n\tif (need_new_thread) {\n\t\tbackground_thread_init(tsd, info);\n\t}\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\tif (!need_new_thread) {\n\t\treturn false;\n\t}\n\tif (arena_ind != 0) {\n\t\t/* Threads are created asynchronously by Thread 0. */\n\t\tbackground_thread_info_t *t0 = &background_thread_info[0];\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &t0->mtx);\n\t\tassert(t0->state == background_thread_started);\n\t\tpthread_cond_signal(&t0->cond);\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &t0->mtx);\n\n\t\treturn false;\n\t}\n\n\tpre_reentrancy(tsd);\n\t/*\n\t * To avoid complications (besides reentrancy), create internal\n\t * background threads with the underlying pthread_create.\n\t */\n\tint err = background_thread_create_signals_masked(&info->thread, NULL,\n\t    background_thread_entry, (void *)thread_ind);\n\tpost_reentrancy(tsd);\n\n\tif (err != 0) {\n\t\tmalloc_printf(\"<jemalloc>: arena 0 background thread creation \"\n\t\t    \"failed (%d)\\n\", err);\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\t\tinfo->state = background_thread_stopped;\n\t\tn_background_threads--;\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nbool\nbackground_threads_enable(tsd_t *tsd) {\n\tassert(n_background_threads == 0);\n\tassert(background_thread_enabled());\n\tmalloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);\n\n\tVARIABLE_ARRAY(bool, marked, ncpus);\n\tunsigned i, nmarked;\n\tfor (i = 0; i < ncpus; i++) {\n\t\tmarked[i] = false;\n\t}\n\tnmarked = 0;\n\t/* Mark the threads we need to create for thread 0. */\n\tunsigned n = narenas_total_get();\n\tfor (i = 1; i < n; i++) {\n\t\tif (marked[i % ncpus] ||\n\t\t    arena_get(tsd_tsdn(tsd), i, false) == NULL) {\n\t\t\tcontinue;\n\t\t}\n\t\tbackground_thread_info_t *info = &background_thread_info[i];\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\t\tassert(info->state == background_thread_stopped);\n\t\tbackground_thread_init(tsd, info);\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\t\tmarked[i % ncpus] = true;\n\t\tif (++nmarked == ncpus) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn background_thread_create(tsd, 0);\n}\n\nbool\nbackground_threads_disable(tsd_t *tsd) {\n\tassert(!background_thread_enabled());\n\tmalloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);\n\n\t/* Thread 0 will be responsible for terminating other threads. */\n\tif (background_threads_disable_single(tsd,\n\t    &background_thread_info[0])) {\n\t\treturn true;\n\t}\n\tassert(n_background_threads == 0);\n\n\treturn false;\n}\n\n/* Check if we need to signal the background thread early. */\nvoid\nbackground_thread_interval_check(tsdn_t *tsdn, arena_t *arena,\n    arena_decay_t *decay, size_t npages_new) {\n\tbackground_thread_info_t *info = arena_background_thread_info_get(\n\t    arena);\n\tif (malloc_mutex_trylock(tsdn, &info->mtx)) {\n\t\t/*\n\t\t * Background thread may hold the mutex for a long period of\n\t\t * time.  We'd like to avoid the variance on application\n\t\t * threads.  So keep this non-blocking, and leave the work to a\n\t\t * future epoch.\n\t\t */\n\t\treturn;\n\t}\n\n\tif (info->state != background_thread_started) {\n\t\tgoto label_done;\n\t}\n\tif (malloc_mutex_trylock(tsdn, &decay->mtx)) {\n\t\tgoto label_done;\n\t}\n\n\tssize_t decay_time = atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);\n\tif (decay_time <= 0) {\n\t\t/* Purging is eagerly done or disabled currently. */\n\t\tgoto label_done_unlock2;\n\t}\n\tuint64_t decay_interval_ns = nstime_ns(&decay->interval);\n\tassert(decay_interval_ns > 0);\n\n\tnstime_t diff;\n\tnstime_init(&diff, background_thread_wakeup_time_get(info));\n\tif (nstime_compare(&diff, &decay->epoch) <= 0) {\n\t\tgoto label_done_unlock2;\n\t}\n\tnstime_subtract(&diff, &decay->epoch);\n\tif (nstime_ns(&diff) < BACKGROUND_THREAD_MIN_INTERVAL_NS) {\n\t\tgoto label_done_unlock2;\n\t}\n\n\tif (npages_new > 0) {\n\t\tsize_t n_epoch = (size_t)(nstime_ns(&diff) / decay_interval_ns);\n\t\t/*\n\t\t * Compute how many new pages we would need to purge by the next\n\t\t * wakeup, which is used to determine if we should signal the\n\t\t * background thread.\n\t\t */\n\t\tuint64_t npurge_new;\n\t\tif (n_epoch >= SMOOTHSTEP_NSTEPS) {\n\t\t\tnpurge_new = npages_new;\n\t\t} else {\n\t\t\tuint64_t h_steps_max = h_steps[SMOOTHSTEP_NSTEPS - 1];\n\t\t\tassert(h_steps_max >=\n\t\t\t    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);\n\t\t\tnpurge_new = npages_new * (h_steps_max -\n\t\t\t    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);\n\t\t\tnpurge_new >>= SMOOTHSTEP_BFP;\n\t\t}\n\t\tinfo->npages_to_purge_new += npurge_new;\n\t}\n\n\tbool should_signal;\n\tif (info->npages_to_purge_new > BACKGROUND_THREAD_NPAGES_THRESHOLD) {\n\t\tshould_signal = true;\n\t} else if (unlikely(background_thread_indefinite_sleep(info)) &&\n\t    (extents_npages_get(&arena->extents_dirty) > 0 ||\n\t    extents_npages_get(&arena->extents_muzzy) > 0 ||\n\t    info->npages_to_purge_new > 0)) {\n\t\tshould_signal = true;\n\t} else {\n\t\tshould_signal = false;\n\t}\n\n\tif (should_signal) {\n\t\tinfo->npages_to_purge_new = 0;\n\t\tpthread_cond_signal(&info->cond);\n\t}\nlabel_done_unlock2:\n\tmalloc_mutex_unlock(tsdn, &decay->mtx);\nlabel_done:\n\tmalloc_mutex_unlock(tsdn, &info->mtx);\n}\n\nvoid\nbackground_thread_prefork0(tsdn_t *tsdn) {\n\tmalloc_mutex_prefork(tsdn, &background_thread_lock);\n\tbackground_thread_enabled_at_fork = background_thread_enabled();\n}\n\nvoid\nbackground_thread_prefork1(tsdn_t *tsdn) {\n\tfor (unsigned i = 0; i < ncpus; i++) {\n\t\tmalloc_mutex_prefork(tsdn, &background_thread_info[i].mtx);\n\t}\n}\n\nvoid\nbackground_thread_postfork_parent(tsdn_t *tsdn) {\n\tfor (unsigned i = 0; i < ncpus; i++) {\n\t\tmalloc_mutex_postfork_parent(tsdn,\n\t\t    &background_thread_info[i].mtx);\n\t}\n\tmalloc_mutex_postfork_parent(tsdn, &background_thread_lock);\n}\n\nvoid\nbackground_thread_postfork_child(tsdn_t *tsdn) {\n\tfor (unsigned i = 0; i < ncpus; i++) {\n\t\tmalloc_mutex_postfork_child(tsdn,\n\t\t    &background_thread_info[i].mtx);\n\t}\n\tmalloc_mutex_postfork_child(tsdn, &background_thread_lock);\n\tif (!background_thread_enabled_at_fork) {\n\t\treturn;\n\t}\n\n\t/* Clear background_thread state (reset to disabled for child). */\n\tmalloc_mutex_lock(tsdn, &background_thread_lock);\n\tn_background_threads = 0;\n\tbackground_thread_enabled_set(tsdn, false);\n\tfor (unsigned i = 0; i < ncpus; i++) {\n\t\tbackground_thread_info_t *info = &background_thread_info[i];\n\t\tmalloc_mutex_lock(tsdn, &info->mtx);\n\t\tinfo->state = background_thread_stopped;\n\t\tint ret = pthread_cond_init(&info->cond, NULL);\n\t\tassert(ret == 0);\n\t\tbackground_thread_info_init(tsdn, info);\n\t\tmalloc_mutex_unlock(tsdn, &info->mtx);\n\t}\n\tmalloc_mutex_unlock(tsdn, &background_thread_lock);\n}\n\nbool\nbackground_thread_stats_read(tsdn_t *tsdn, background_thread_stats_t *stats) {\n\tassert(config_stats);\n\tmalloc_mutex_lock(tsdn, &background_thread_lock);\n\tif (!background_thread_enabled()) {\n\t\tmalloc_mutex_unlock(tsdn, &background_thread_lock);\n\t\treturn true;\n\t}\n\n\tstats->num_threads = n_background_threads;\n\tuint64_t num_runs = 0;\n\tnstime_init(&stats->run_interval, 0);\n\tfor (unsigned i = 0; i < ncpus; i++) {\n\t\tbackground_thread_info_t *info = &background_thread_info[i];\n\t\tmalloc_mutex_lock(tsdn, &info->mtx);\n\t\tif (info->state != background_thread_stopped) {\n\t\t\tnum_runs += info->tot_n_runs;\n\t\t\tnstime_add(&stats->run_interval, &info->tot_sleep_time);\n\t\t}\n\t\tmalloc_mutex_unlock(tsdn, &info->mtx);\n\t}\n\tstats->num_runs = num_runs;\n\tif (num_runs > 0) {\n\t\tnstime_idivide(&stats->run_interval, num_runs);\n\t}\n\tmalloc_mutex_unlock(tsdn, &background_thread_lock);\n\n\treturn false;\n}\n\n#undef BACKGROUND_THREAD_NPAGES_THRESHOLD\n#undef BILLION\n#undef BACKGROUND_THREAD_MIN_INTERVAL_NS\n\n/*\n * When lazy lock is enabled, we need to make sure setting isthreaded before\n * taking any background_thread locks.  This is called early in ctl (instead of\n * wait for the pthread_create calls to trigger) because the mutex is required\n * before creating background threads.\n */\nvoid\nbackground_thread_ctl_init(tsdn_t *tsdn) {\n\tmalloc_mutex_assert_not_owner(tsdn, &background_thread_lock);\n#ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER\n\tpthread_once(&once_control, pthread_create_wrapper_once);\n#endif\n}\n\n#endif /* defined(JEMALLOC_BACKGROUND_THREAD) */\n\nbool\nbackground_thread_boot0(void) {\n\tif (!have_background_thread && opt_background_thread) {\n\t\tmalloc_printf(\"<jemalloc>: option background_thread currently \"\n\t\t    \"supports pthread only\\n\");\n\t\treturn true;\n\t}\n\n#ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER\n\tpthread_create_fptr = dlsym(RTLD_NEXT, \"pthread_create\");\n\tif (pthread_create_fptr == NULL) {\n\t\tcan_enable_background_thread = false;\n\t\tif (config_lazy_lock || opt_background_thread) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in dlsym(RTLD_NEXT, \"\n\t\t\t    \"\\\"pthread_create\\\")\\n\");\n\t\t\tabort();\n\t\t}\n\t} else {\n\t\tcan_enable_background_thread = true;\n\t}\n#endif\n\treturn false;\n}\n\nbool\nbackground_thread_boot1(tsdn_t *tsdn) {\n#ifdef JEMALLOC_BACKGROUND_THREAD\n\tassert(have_background_thread);\n\tassert(narenas_total_get() > 0);\n\n\tbackground_thread_enabled_set(tsdn, opt_background_thread);\n\tif (malloc_mutex_init(&background_thread_lock,\n\t    \"background_thread_global\",\n\t    WITNESS_RANK_BACKGROUND_THREAD_GLOBAL,\n\t    malloc_mutex_rank_exclusive)) {\n\t\treturn true;\n\t}\n\tif (opt_background_thread) {\n\t\tbackground_thread_ctl_init(tsdn);\n\t}\n\n\tbackground_thread_info = (background_thread_info_t *)base_alloc(tsdn,\n\t    b0get(), ncpus * sizeof(background_thread_info_t), CACHELINE);\n\tif (background_thread_info == NULL) {\n\t\treturn true;\n\t}\n\n\tfor (unsigned i = 0; i < ncpus; i++) {\n\t\tbackground_thread_info_t *info = &background_thread_info[i];\n\t\t/* Thread mutex is rank_inclusive because of thread0. */\n\t\tif (malloc_mutex_init(&info->mtx, \"background_thread\",\n\t\t    WITNESS_RANK_BACKGROUND_THREAD,\n\t\t    malloc_mutex_address_ordered)) {\n\t\t\treturn true;\n\t\t}\n\t\tif (pthread_cond_init(&info->cond, NULL)) {\n\t\t\treturn true;\n\t\t}\n\t\tmalloc_mutex_lock(tsdn, &info->mtx);\n\t\tinfo->state = background_thread_stopped;\n\t\tbackground_thread_info_init(tsdn, info);\n\t\tmalloc_mutex_unlock(tsdn, &info->mtx);\n\t}\n#endif\n\n\treturn false;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/jemalloc/src/ctl.c": "#define JEMALLOC_CTL_C_\n#include \"jemalloc/internal/jemalloc_preamble.h\"\n#include \"jemalloc/internal/jemalloc_internal_includes.h\"\n\n#include \"jemalloc/internal/assert.h\"\n#include \"jemalloc/internal/ctl.h\"\n#include \"jemalloc/internal/extent_dss.h\"\n#include \"jemalloc/internal/extent_mmap.h\"\n#include \"jemalloc/internal/mutex.h\"\n#include \"jemalloc/internal/nstime.h\"\n#include \"jemalloc/internal/size_classes.h\"\n#include \"jemalloc/internal/util.h\"\n\n/******************************************************************************/\n/* Data. */\n\n/*\n * ctl_mtx protects the following:\n * - ctl_stats->*\n */\nstatic malloc_mutex_t\tctl_mtx;\nstatic bool\t\tctl_initialized;\nstatic ctl_stats_t\t*ctl_stats;\nstatic ctl_arenas_t\t*ctl_arenas;\n\n/******************************************************************************/\n/* Helpers for named and indexed nodes. */\n\nstatic const ctl_named_node_t *\nctl_named_node(const ctl_node_t *node) {\n\treturn ((node->named) ? (const ctl_named_node_t *)node : NULL);\n}\n\nstatic const ctl_named_node_t *\nctl_named_children(const ctl_named_node_t *node, size_t index) {\n\tconst ctl_named_node_t *children = ctl_named_node(node->children);\n\n\treturn (children ? &children[index] : NULL);\n}\n\nstatic const ctl_indexed_node_t *\nctl_indexed_node(const ctl_node_t *node) {\n\treturn (!node->named ? (const ctl_indexed_node_t *)node : NULL);\n}\n\n/******************************************************************************/\n/* Function prototypes for non-inline static functions. */\n\n#define CTL_PROTO(n)\t\t\t\t\t\t\t\\\nstatic int\tn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\t\\\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen);\n\n#define INDEX_PROTO(n)\t\t\t\t\t\t\t\\\nstatic const ctl_named_node_t\t*n##_index(tsdn_t *tsdn,\t\t\\\n    const size_t *mib, size_t miblen, size_t i);\n\nCTL_PROTO(version)\nCTL_PROTO(epoch)\nCTL_PROTO(background_thread)\nCTL_PROTO(thread_tcache_enabled)\nCTL_PROTO(thread_tcache_flush)\nCTL_PROTO(thread_prof_name)\nCTL_PROTO(thread_prof_active)\nCTL_PROTO(thread_arena)\nCTL_PROTO(thread_allocated)\nCTL_PROTO(thread_allocatedp)\nCTL_PROTO(thread_deallocated)\nCTL_PROTO(thread_deallocatedp)\nCTL_PROTO(config_cache_oblivious)\nCTL_PROTO(config_debug)\nCTL_PROTO(config_fill)\nCTL_PROTO(config_lazy_lock)\nCTL_PROTO(config_malloc_conf)\nCTL_PROTO(config_prof)\nCTL_PROTO(config_prof_libgcc)\nCTL_PROTO(config_prof_libunwind)\nCTL_PROTO(config_stats)\nCTL_PROTO(config_thp)\nCTL_PROTO(config_utrace)\nCTL_PROTO(config_xmalloc)\nCTL_PROTO(opt_abort)\nCTL_PROTO(opt_abort_conf)\nCTL_PROTO(opt_retain)\nCTL_PROTO(opt_dss)\nCTL_PROTO(opt_narenas)\nCTL_PROTO(opt_percpu_arena)\nCTL_PROTO(opt_background_thread)\nCTL_PROTO(opt_dirty_decay_ms)\nCTL_PROTO(opt_muzzy_decay_ms)\nCTL_PROTO(opt_stats_print)\nCTL_PROTO(opt_stats_print_opts)\nCTL_PROTO(opt_junk)\nCTL_PROTO(opt_zero)\nCTL_PROTO(opt_utrace)\nCTL_PROTO(opt_xmalloc)\nCTL_PROTO(opt_tcache)\nCTL_PROTO(opt_lg_tcache_max)\nCTL_PROTO(opt_prof)\nCTL_PROTO(opt_prof_prefix)\nCTL_PROTO(opt_prof_active)\nCTL_PROTO(opt_prof_thread_active_init)\nCTL_PROTO(opt_lg_prof_sample)\nCTL_PROTO(opt_lg_prof_interval)\nCTL_PROTO(opt_prof_gdump)\nCTL_PROTO(opt_prof_final)\nCTL_PROTO(opt_prof_leak)\nCTL_PROTO(opt_prof_accum)\nCTL_PROTO(tcache_create)\nCTL_PROTO(tcache_flush)\nCTL_PROTO(tcache_destroy)\nCTL_PROTO(arena_i_initialized)\nCTL_PROTO(arena_i_decay)\nCTL_PROTO(arena_i_purge)\nCTL_PROTO(arena_i_reset)\nCTL_PROTO(arena_i_destroy)\nCTL_PROTO(arena_i_dss)\nCTL_PROTO(arena_i_dirty_decay_ms)\nCTL_PROTO(arena_i_muzzy_decay_ms)\nCTL_PROTO(arena_i_extent_hooks)\nINDEX_PROTO(arena_i)\nCTL_PROTO(arenas_bin_i_size)\nCTL_PROTO(arenas_bin_i_nregs)\nCTL_PROTO(arenas_bin_i_slab_size)\nINDEX_PROTO(arenas_bin_i)\nCTL_PROTO(arenas_lextent_i_size)\nINDEX_PROTO(arenas_lextent_i)\nCTL_PROTO(arenas_narenas)\nCTL_PROTO(arenas_dirty_decay_ms)\nCTL_PROTO(arenas_muzzy_decay_ms)\nCTL_PROTO(arenas_quantum)\nCTL_PROTO(arenas_page)\nCTL_PROTO(arenas_tcache_max)\nCTL_PROTO(arenas_nbins)\nCTL_PROTO(arenas_nhbins)\nCTL_PROTO(arenas_nlextents)\nCTL_PROTO(arenas_create)\nCTL_PROTO(prof_thread_active_init)\nCTL_PROTO(prof_active)\nCTL_PROTO(prof_dump)\nCTL_PROTO(prof_gdump)\nCTL_PROTO(prof_reset)\nCTL_PROTO(prof_interval)\nCTL_PROTO(lg_prof_sample)\nCTL_PROTO(stats_arenas_i_small_allocated)\nCTL_PROTO(stats_arenas_i_small_nmalloc)\nCTL_PROTO(stats_arenas_i_small_ndalloc)\nCTL_PROTO(stats_arenas_i_small_nrequests)\nCTL_PROTO(stats_arenas_i_large_allocated)\nCTL_PROTO(stats_arenas_i_large_nmalloc)\nCTL_PROTO(stats_arenas_i_large_ndalloc)\nCTL_PROTO(stats_arenas_i_large_nrequests)\nCTL_PROTO(stats_arenas_i_bins_j_nmalloc)\nCTL_PROTO(stats_arenas_i_bins_j_ndalloc)\nCTL_PROTO(stats_arenas_i_bins_j_nrequests)\nCTL_PROTO(stats_arenas_i_bins_j_curregs)\nCTL_PROTO(stats_arenas_i_bins_j_nfills)\nCTL_PROTO(stats_arenas_i_bins_j_nflushes)\nCTL_PROTO(stats_arenas_i_bins_j_nslabs)\nCTL_PROTO(stats_arenas_i_bins_j_nreslabs)\nCTL_PROTO(stats_arenas_i_bins_j_curslabs)\nINDEX_PROTO(stats_arenas_i_bins_j)\nCTL_PROTO(stats_arenas_i_lextents_j_nmalloc)\nCTL_PROTO(stats_arenas_i_lextents_j_ndalloc)\nCTL_PROTO(stats_arenas_i_lextents_j_nrequests)\nCTL_PROTO(stats_arenas_i_lextents_j_curlextents)\nINDEX_PROTO(stats_arenas_i_lextents_j)\nCTL_PROTO(stats_arenas_i_nthreads)\nCTL_PROTO(stats_arenas_i_uptime)\nCTL_PROTO(stats_arenas_i_dss)\nCTL_PROTO(stats_arenas_i_dirty_decay_ms)\nCTL_PROTO(stats_arenas_i_muzzy_decay_ms)\nCTL_PROTO(stats_arenas_i_pactive)\nCTL_PROTO(stats_arenas_i_pdirty)\nCTL_PROTO(stats_arenas_i_pmuzzy)\nCTL_PROTO(stats_arenas_i_mapped)\nCTL_PROTO(stats_arenas_i_retained)\nCTL_PROTO(stats_arenas_i_dirty_npurge)\nCTL_PROTO(stats_arenas_i_dirty_nmadvise)\nCTL_PROTO(stats_arenas_i_dirty_purged)\nCTL_PROTO(stats_arenas_i_muzzy_npurge)\nCTL_PROTO(stats_arenas_i_muzzy_nmadvise)\nCTL_PROTO(stats_arenas_i_muzzy_purged)\nCTL_PROTO(stats_arenas_i_base)\nCTL_PROTO(stats_arenas_i_internal)\nCTL_PROTO(stats_arenas_i_tcache_bytes)\nCTL_PROTO(stats_arenas_i_resident)\nINDEX_PROTO(stats_arenas_i)\nCTL_PROTO(stats_allocated)\nCTL_PROTO(stats_active)\nCTL_PROTO(stats_background_thread_num_threads)\nCTL_PROTO(stats_background_thread_num_runs)\nCTL_PROTO(stats_background_thread_run_interval)\nCTL_PROTO(stats_metadata)\nCTL_PROTO(stats_resident)\nCTL_PROTO(stats_mapped)\nCTL_PROTO(stats_retained)\n\n#define MUTEX_STATS_CTL_PROTO_GEN(n)\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_num_ops)\t\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_num_wait)\t\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_num_spin_acq)\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_num_owner_switch)\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_total_wait_time)\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_max_wait_time)\t\t\t\t\t\\\nCTL_PROTO(stats_##n##_max_num_thds)\n\n/* Global mutexes. */\n#define OP(mtx) MUTEX_STATS_CTL_PROTO_GEN(mutexes_##mtx)\nMUTEX_PROF_GLOBAL_MUTEXES\n#undef OP\n\n/* Per arena mutexes. */\n#define OP(mtx) MUTEX_STATS_CTL_PROTO_GEN(arenas_i_mutexes_##mtx)\nMUTEX_PROF_ARENA_MUTEXES\n#undef OP\n\n/* Arena bin mutexes. */\nMUTEX_STATS_CTL_PROTO_GEN(arenas_i_bins_j_mutex)\n#undef MUTEX_STATS_CTL_PROTO_GEN\n\nCTL_PROTO(stats_mutexes_reset)\n\n/******************************************************************************/\n/* mallctl tree. */\n\n#define NAME(n)\t{true},\tn\n#define CHILD(t, c)\t\t\t\t\t\t\t\\\n\tsizeof(c##_node) / sizeof(ctl_##t##_node_t),\t\t\t\\\n\t(ctl_node_t *)c##_node,\t\t\t\t\t\t\\\n\tNULL\n#define CTL(c)\t0, NULL, c##_ctl\n\n/*\n * Only handles internal indexed nodes, since there are currently no external\n * ones.\n */\n#define INDEX(i)\t{false},\ti##_index\n\nstatic const ctl_named_node_t\tthread_tcache_node[] = {\n\t{NAME(\"enabled\"),\tCTL(thread_tcache_enabled)},\n\t{NAME(\"flush\"),\t\tCTL(thread_tcache_flush)}\n};\n\nstatic const ctl_named_node_t\tthread_prof_node[] = {\n\t{NAME(\"name\"),\t\tCTL(thread_prof_name)},\n\t{NAME(\"active\"),\tCTL(thread_prof_active)}\n};\n\nstatic const ctl_named_node_t\tthread_node[] = {\n\t{NAME(\"arena\"),\t\tCTL(thread_arena)},\n\t{NAME(\"allocated\"),\tCTL(thread_allocated)},\n\t{NAME(\"allocatedp\"),\tCTL(thread_allocatedp)},\n\t{NAME(\"deallocated\"),\tCTL(thread_deallocated)},\n\t{NAME(\"deallocatedp\"),\tCTL(thread_deallocatedp)},\n\t{NAME(\"tcache\"),\tCHILD(named, thread_tcache)},\n\t{NAME(\"prof\"),\t\tCHILD(named, thread_prof)}\n};\n\nstatic const ctl_named_node_t\tconfig_node[] = {\n\t{NAME(\"cache_oblivious\"), CTL(config_cache_oblivious)},\n\t{NAME(\"debug\"),\t\tCTL(config_debug)},\n\t{NAME(\"fill\"),\t\tCTL(config_fill)},\n\t{NAME(\"lazy_lock\"),\tCTL(config_lazy_lock)},\n\t{NAME(\"malloc_conf\"),\tCTL(config_malloc_conf)},\n\t{NAME(\"prof\"),\t\tCTL(config_prof)},\n\t{NAME(\"prof_libgcc\"),\tCTL(config_prof_libgcc)},\n\t{NAME(\"prof_libunwind\"), CTL(config_prof_libunwind)},\n\t{NAME(\"stats\"),\t\tCTL(config_stats)},\n\t{NAME(\"thp\"),\t\tCTL(config_thp)},\n\t{NAME(\"utrace\"),\tCTL(config_utrace)},\n\t{NAME(\"xmalloc\"),\tCTL(config_xmalloc)}\n};\n\nstatic const ctl_named_node_t opt_node[] = {\n\t{NAME(\"abort\"),\t\tCTL(opt_abort)},\n\t{NAME(\"abort_conf\"),\tCTL(opt_abort_conf)},\n\t{NAME(\"retain\"),\tCTL(opt_retain)},\n\t{NAME(\"dss\"),\t\tCTL(opt_dss)},\n\t{NAME(\"narenas\"),\tCTL(opt_narenas)},\n\t{NAME(\"percpu_arena\"),\tCTL(opt_percpu_arena)},\n\t{NAME(\"background_thread\"),\tCTL(opt_background_thread)},\n\t{NAME(\"dirty_decay_ms\"), CTL(opt_dirty_decay_ms)},\n\t{NAME(\"muzzy_decay_ms\"), CTL(opt_muzzy_decay_ms)},\n\t{NAME(\"stats_print\"),\tCTL(opt_stats_print)},\n\t{NAME(\"stats_print_opts\"),\tCTL(opt_stats_print_opts)},\n\t{NAME(\"junk\"),\t\tCTL(opt_junk)},\n\t{NAME(\"zero\"),\t\tCTL(opt_zero)},\n\t{NAME(\"utrace\"),\tCTL(opt_utrace)},\n\t{NAME(\"xmalloc\"),\tCTL(opt_xmalloc)},\n\t{NAME(\"tcache\"),\tCTL(opt_tcache)},\n\t{NAME(\"lg_tcache_max\"),\tCTL(opt_lg_tcache_max)},\n\t{NAME(\"prof\"),\t\tCTL(opt_prof)},\n\t{NAME(\"prof_prefix\"),\tCTL(opt_prof_prefix)},\n\t{NAME(\"prof_active\"),\tCTL(opt_prof_active)},\n\t{NAME(\"prof_thread_active_init\"), CTL(opt_prof_thread_active_init)},\n\t{NAME(\"lg_prof_sample\"), CTL(opt_lg_prof_sample)},\n\t{NAME(\"lg_prof_interval\"), CTL(opt_lg_prof_interval)},\n\t{NAME(\"prof_gdump\"),\tCTL(opt_prof_gdump)},\n\t{NAME(\"prof_final\"),\tCTL(opt_prof_final)},\n\t{NAME(\"prof_leak\"),\tCTL(opt_prof_leak)},\n\t{NAME(\"prof_accum\"),\tCTL(opt_prof_accum)}\n};\n\nstatic const ctl_named_node_t\ttcache_node[] = {\n\t{NAME(\"create\"),\tCTL(tcache_create)},\n\t{NAME(\"flush\"),\t\tCTL(tcache_flush)},\n\t{NAME(\"destroy\"),\tCTL(tcache_destroy)}\n};\n\nstatic const ctl_named_node_t arena_i_node[] = {\n\t{NAME(\"initialized\"),\tCTL(arena_i_initialized)},\n\t{NAME(\"decay\"),\t\tCTL(arena_i_decay)},\n\t{NAME(\"purge\"),\t\tCTL(arena_i_purge)},\n\t{NAME(\"reset\"),\t\tCTL(arena_i_reset)},\n\t{NAME(\"destroy\"),\tCTL(arena_i_destroy)},\n\t{NAME(\"dss\"),\t\tCTL(arena_i_dss)},\n\t{NAME(\"dirty_decay_ms\"), CTL(arena_i_dirty_decay_ms)},\n\t{NAME(\"muzzy_decay_ms\"), CTL(arena_i_muzzy_decay_ms)},\n\t{NAME(\"extent_hooks\"),\tCTL(arena_i_extent_hooks)}\n};\nstatic const ctl_named_node_t super_arena_i_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, arena_i)}\n};\n\nstatic const ctl_indexed_node_t arena_node[] = {\n\t{INDEX(arena_i)}\n};\n\nstatic const ctl_named_node_t arenas_bin_i_node[] = {\n\t{NAME(\"size\"),\t\tCTL(arenas_bin_i_size)},\n\t{NAME(\"nregs\"),\t\tCTL(arenas_bin_i_nregs)},\n\t{NAME(\"slab_size\"),\tCTL(arenas_bin_i_slab_size)}\n};\nstatic const ctl_named_node_t super_arenas_bin_i_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, arenas_bin_i)}\n};\n\nstatic const ctl_indexed_node_t arenas_bin_node[] = {\n\t{INDEX(arenas_bin_i)}\n};\n\nstatic const ctl_named_node_t arenas_lextent_i_node[] = {\n\t{NAME(\"size\"),\t\tCTL(arenas_lextent_i_size)}\n};\nstatic const ctl_named_node_t super_arenas_lextent_i_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, arenas_lextent_i)}\n};\n\nstatic const ctl_indexed_node_t arenas_lextent_node[] = {\n\t{INDEX(arenas_lextent_i)}\n};\n\nstatic const ctl_named_node_t arenas_node[] = {\n\t{NAME(\"narenas\"),\tCTL(arenas_narenas)},\n\t{NAME(\"dirty_decay_ms\"), CTL(arenas_dirty_decay_ms)},\n\t{NAME(\"muzzy_decay_ms\"), CTL(arenas_muzzy_decay_ms)},\n\t{NAME(\"quantum\"),\tCTL(arenas_quantum)},\n\t{NAME(\"page\"),\t\tCTL(arenas_page)},\n\t{NAME(\"tcache_max\"),\tCTL(arenas_tcache_max)},\n\t{NAME(\"nbins\"),\t\tCTL(arenas_nbins)},\n\t{NAME(\"nhbins\"),\tCTL(arenas_nhbins)},\n\t{NAME(\"bin\"),\t\tCHILD(indexed, arenas_bin)},\n\t{NAME(\"nlextents\"),\tCTL(arenas_nlextents)},\n\t{NAME(\"lextent\"),\tCHILD(indexed, arenas_lextent)},\n\t{NAME(\"create\"),\tCTL(arenas_create)}\n};\n\nstatic const ctl_named_node_t\tprof_node[] = {\n\t{NAME(\"thread_active_init\"), CTL(prof_thread_active_init)},\n\t{NAME(\"active\"),\tCTL(prof_active)},\n\t{NAME(\"dump\"),\t\tCTL(prof_dump)},\n\t{NAME(\"gdump\"),\t\tCTL(prof_gdump)},\n\t{NAME(\"reset\"),\t\tCTL(prof_reset)},\n\t{NAME(\"interval\"),\tCTL(prof_interval)},\n\t{NAME(\"lg_sample\"),\tCTL(lg_prof_sample)}\n};\n\nstatic const ctl_named_node_t stats_arenas_i_small_node[] = {\n\t{NAME(\"allocated\"),\tCTL(stats_arenas_i_small_allocated)},\n\t{NAME(\"nmalloc\"),\tCTL(stats_arenas_i_small_nmalloc)},\n\t{NAME(\"ndalloc\"),\tCTL(stats_arenas_i_small_ndalloc)},\n\t{NAME(\"nrequests\"),\tCTL(stats_arenas_i_small_nrequests)}\n};\n\nstatic const ctl_named_node_t stats_arenas_i_large_node[] = {\n\t{NAME(\"allocated\"),\tCTL(stats_arenas_i_large_allocated)},\n\t{NAME(\"nmalloc\"),\tCTL(stats_arenas_i_large_nmalloc)},\n\t{NAME(\"ndalloc\"),\tCTL(stats_arenas_i_large_ndalloc)},\n\t{NAME(\"nrequests\"),\tCTL(stats_arenas_i_large_nrequests)}\n};\n\n#define MUTEX_PROF_DATA_NODE(prefix)\t\t\t\t\t\\\nstatic const ctl_named_node_t stats_##prefix##_node[] = {\t\t\\\n\t{NAME(\"num_ops\"),\t\t\t\t\t\t\\\n\t CTL(stats_##prefix##_num_ops)},\t\t\t\t\\\n\t{NAME(\"num_wait\"),\t\t\t\t\t\t\\\n\t CTL(stats_##prefix##_num_wait)},\t\t\t\t\\\n\t{NAME(\"num_spin_acq\"),\t\t\t\t\t\t\\\n\t CTL(stats_##prefix##_num_spin_acq)},\t\t\t\t\\\n\t{NAME(\"num_owner_switch\"),\t\t\t\t\t\\\n\t CTL(stats_##prefix##_num_owner_switch)},\t\t\t\\\n\t{NAME(\"total_wait_time\"),\t\t\t\t\t\\\n\t CTL(stats_##prefix##_total_wait_time)},\t\t\t\\\n\t{NAME(\"max_wait_time\"),\t\t\t\t\t\t\\\n\t CTL(stats_##prefix##_max_wait_time)},\t\t\t\t\\\n\t{NAME(\"max_num_thds\"),\t\t\t\t\t\t\\\n\t CTL(stats_##prefix##_max_num_thds)}\t\t\t\t\\\n\t/* Note that # of current waiting thread not provided. */\t\\\n};\n\nMUTEX_PROF_DATA_NODE(arenas_i_bins_j_mutex)\n\nstatic const ctl_named_node_t stats_arenas_i_bins_j_node[] = {\n\t{NAME(\"nmalloc\"),\tCTL(stats_arenas_i_bins_j_nmalloc)},\n\t{NAME(\"ndalloc\"),\tCTL(stats_arenas_i_bins_j_ndalloc)},\n\t{NAME(\"nrequests\"),\tCTL(stats_arenas_i_bins_j_nrequests)},\n\t{NAME(\"curregs\"),\tCTL(stats_arenas_i_bins_j_curregs)},\n\t{NAME(\"nfills\"),\tCTL(stats_arenas_i_bins_j_nfills)},\n\t{NAME(\"nflushes\"),\tCTL(stats_arenas_i_bins_j_nflushes)},\n\t{NAME(\"nslabs\"),\tCTL(stats_arenas_i_bins_j_nslabs)},\n\t{NAME(\"nreslabs\"),\tCTL(stats_arenas_i_bins_j_nreslabs)},\n\t{NAME(\"curslabs\"),\tCTL(stats_arenas_i_bins_j_curslabs)},\n\t{NAME(\"mutex\"),\t\tCHILD(named, stats_arenas_i_bins_j_mutex)}\n};\n\nstatic const ctl_named_node_t super_stats_arenas_i_bins_j_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, stats_arenas_i_bins_j)}\n};\n\nstatic const ctl_indexed_node_t stats_arenas_i_bins_node[] = {\n\t{INDEX(stats_arenas_i_bins_j)}\n};\n\nstatic const ctl_named_node_t stats_arenas_i_lextents_j_node[] = {\n\t{NAME(\"nmalloc\"),\tCTL(stats_arenas_i_lextents_j_nmalloc)},\n\t{NAME(\"ndalloc\"),\tCTL(stats_arenas_i_lextents_j_ndalloc)},\n\t{NAME(\"nrequests\"),\tCTL(stats_arenas_i_lextents_j_nrequests)},\n\t{NAME(\"curlextents\"),\tCTL(stats_arenas_i_lextents_j_curlextents)}\n};\nstatic const ctl_named_node_t super_stats_arenas_i_lextents_j_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, stats_arenas_i_lextents_j)}\n};\n\nstatic const ctl_indexed_node_t stats_arenas_i_lextents_node[] = {\n\t{INDEX(stats_arenas_i_lextents_j)}\n};\n\n#define OP(mtx)  MUTEX_PROF_DATA_NODE(arenas_i_mutexes_##mtx)\nMUTEX_PROF_ARENA_MUTEXES\n#undef OP\n\nstatic const ctl_named_node_t stats_arenas_i_mutexes_node[] = {\n#define OP(mtx) {NAME(#mtx), CHILD(named, stats_arenas_i_mutexes_##mtx)},\nMUTEX_PROF_ARENA_MUTEXES\n#undef OP\n};\n\nstatic const ctl_named_node_t stats_arenas_i_node[] = {\n\t{NAME(\"nthreads\"),\tCTL(stats_arenas_i_nthreads)},\n\t{NAME(\"uptime\"),\tCTL(stats_arenas_i_uptime)},\n\t{NAME(\"dss\"),\t\tCTL(stats_arenas_i_dss)},\n\t{NAME(\"dirty_decay_ms\"), CTL(stats_arenas_i_dirty_decay_ms)},\n\t{NAME(\"muzzy_decay_ms\"), CTL(stats_arenas_i_muzzy_decay_ms)},\n\t{NAME(\"pactive\"),\tCTL(stats_arenas_i_pactive)},\n\t{NAME(\"pdirty\"),\tCTL(stats_arenas_i_pdirty)},\n\t{NAME(\"pmuzzy\"),\tCTL(stats_arenas_i_pmuzzy)},\n\t{NAME(\"mapped\"),\tCTL(stats_arenas_i_mapped)},\n\t{NAME(\"retained\"),\tCTL(stats_arenas_i_retained)},\n\t{NAME(\"dirty_npurge\"),\tCTL(stats_arenas_i_dirty_npurge)},\n\t{NAME(\"dirty_nmadvise\"), CTL(stats_arenas_i_dirty_nmadvise)},\n\t{NAME(\"dirty_purged\"),\tCTL(stats_arenas_i_dirty_purged)},\n\t{NAME(\"muzzy_npurge\"),\tCTL(stats_arenas_i_muzzy_npurge)},\n\t{NAME(\"muzzy_nmadvise\"), CTL(stats_arenas_i_muzzy_nmadvise)},\n\t{NAME(\"muzzy_purged\"),\tCTL(stats_arenas_i_muzzy_purged)},\n\t{NAME(\"base\"),\t\tCTL(stats_arenas_i_base)},\n\t{NAME(\"internal\"),\tCTL(stats_arenas_i_internal)},\n\t{NAME(\"tcache_bytes\"),\tCTL(stats_arenas_i_tcache_bytes)},\n\t{NAME(\"resident\"),\tCTL(stats_arenas_i_resident)},\n\t{NAME(\"small\"),\t\tCHILD(named, stats_arenas_i_small)},\n\t{NAME(\"large\"),\t\tCHILD(named, stats_arenas_i_large)},\n\t{NAME(\"bins\"),\t\tCHILD(indexed, stats_arenas_i_bins)},\n\t{NAME(\"lextents\"),\tCHILD(indexed, stats_arenas_i_lextents)},\n\t{NAME(\"mutexes\"),\tCHILD(named, stats_arenas_i_mutexes)}\n};\nstatic const ctl_named_node_t super_stats_arenas_i_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, stats_arenas_i)}\n};\n\nstatic const ctl_indexed_node_t stats_arenas_node[] = {\n\t{INDEX(stats_arenas_i)}\n};\n\nstatic const ctl_named_node_t stats_background_thread_node[] = {\n\t{NAME(\"num_threads\"),\tCTL(stats_background_thread_num_threads)},\n\t{NAME(\"num_runs\"),\tCTL(stats_background_thread_num_runs)},\n\t{NAME(\"run_interval\"),\tCTL(stats_background_thread_run_interval)}\n};\n\n#define OP(mtx) MUTEX_PROF_DATA_NODE(mutexes_##mtx)\nMUTEX_PROF_GLOBAL_MUTEXES\n#undef OP\n\nstatic const ctl_named_node_t stats_mutexes_node[] = {\n#define OP(mtx) {NAME(#mtx), CHILD(named, stats_mutexes_##mtx)},\nMUTEX_PROF_GLOBAL_MUTEXES\n#undef OP\n\t{NAME(\"reset\"),\t\tCTL(stats_mutexes_reset)}\n};\n#undef MUTEX_PROF_DATA_NODE\n\nstatic const ctl_named_node_t stats_node[] = {\n\t{NAME(\"allocated\"),\tCTL(stats_allocated)},\n\t{NAME(\"active\"),\tCTL(stats_active)},\n\t{NAME(\"metadata\"),\tCTL(stats_metadata)},\n\t{NAME(\"resident\"),\tCTL(stats_resident)},\n\t{NAME(\"mapped\"),\tCTL(stats_mapped)},\n\t{NAME(\"retained\"),\tCTL(stats_retained)},\n\t{NAME(\"background_thread\"),\n\t CHILD(named, stats_background_thread)},\n\t{NAME(\"mutexes\"),\tCHILD(named, stats_mutexes)},\n\t{NAME(\"arenas\"),\tCHILD(indexed, stats_arenas)}\n};\n\nstatic const ctl_named_node_t\troot_node[] = {\n\t{NAME(\"version\"),\tCTL(version)},\n\t{NAME(\"epoch\"),\t\tCTL(epoch)},\n\t{NAME(\"background_thread\"),\tCTL(background_thread)},\n\t{NAME(\"thread\"),\tCHILD(named, thread)},\n\t{NAME(\"config\"),\tCHILD(named, config)},\n\t{NAME(\"opt\"),\t\tCHILD(named, opt)},\n\t{NAME(\"tcache\"),\tCHILD(named, tcache)},\n\t{NAME(\"arena\"),\t\tCHILD(indexed, arena)},\n\t{NAME(\"arenas\"),\tCHILD(named, arenas)},\n\t{NAME(\"prof\"),\t\tCHILD(named, prof)},\n\t{NAME(\"stats\"),\t\tCHILD(named, stats)}\n};\nstatic const ctl_named_node_t super_root_node[] = {\n\t{NAME(\"\"),\t\tCHILD(named, root)}\n};\n\n#undef NAME\n#undef CHILD\n#undef CTL\n#undef INDEX\n\n/******************************************************************************/\n\n/*\n * Sets *dst + *src non-atomically.  This is safe, since everything is\n * synchronized by the ctl mutex.\n */\nstatic void\naccum_arena_stats_u64(arena_stats_u64_t *dst, arena_stats_u64_t *src) {\n#ifdef JEMALLOC_ATOMIC_U64\n\tuint64_t cur_dst = atomic_load_u64(dst, ATOMIC_RELAXED);\n\tuint64_t cur_src = atomic_load_u64(src, ATOMIC_RELAXED);\n\tatomic_store_u64(dst, cur_dst + cur_src, ATOMIC_RELAXED);\n#else\n\t*dst += *src;\n#endif\n}\n\n/* Likewise: with ctl mutex synchronization, reading is simple. */\nstatic uint64_t\narena_stats_read_u64(arena_stats_u64_t *p) {\n#ifdef JEMALLOC_ATOMIC_U64\n\treturn atomic_load_u64(p, ATOMIC_RELAXED);\n#else\n\treturn *p;\n#endif\n}\n\nstatic void accum_atomic_zu(atomic_zu_t *dst, atomic_zu_t *src) {\n\tsize_t cur_dst = atomic_load_zu(dst, ATOMIC_RELAXED);\n\tsize_t cur_src = atomic_load_zu(src, ATOMIC_RELAXED);\n\tatomic_store_zu(dst, cur_dst + cur_src, ATOMIC_RELAXED);\n}\n\n/******************************************************************************/\n\nstatic unsigned\narenas_i2a_impl(size_t i, bool compat, bool validate) {\n\tunsigned a;\n\n\tswitch (i) {\n\tcase MALLCTL_ARENAS_ALL:\n\t\ta = 0;\n\t\tbreak;\n\tcase MALLCTL_ARENAS_DESTROYED:\n\t\ta = 1;\n\t\tbreak;\n\tdefault:\n\t\tif (compat && i == ctl_arenas->narenas) {\n\t\t\t/*\n\t\t\t * Provide deprecated backward compatibility for\n\t\t\t * accessing the merged stats at index narenas rather\n\t\t\t * than via MALLCTL_ARENAS_ALL.  This is scheduled for\n\t\t\t * removal in 6.0.0.\n\t\t\t */\n\t\t\ta = 0;\n\t\t} else if (validate && i >= ctl_arenas->narenas) {\n\t\t\ta = UINT_MAX;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This function should never be called for an index\n\t\t\t * more than one past the range of indices that have\n\t\t\t * initialized ctl data.\n\t\t\t */\n\t\t\tassert(i < ctl_arenas->narenas || (!validate && i ==\n\t\t\t    ctl_arenas->narenas));\n\t\t\ta = (unsigned)i + 2;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn a;\n}\n\nstatic unsigned\narenas_i2a(size_t i) {\n\treturn arenas_i2a_impl(i, true, false);\n}\n\nstatic ctl_arena_t *\narenas_i_impl(tsdn_t *tsdn, size_t i, bool compat, bool init) {\n\tctl_arena_t *ret;\n\n\tassert(!compat || !init);\n\n\tret = ctl_arenas->arenas[arenas_i2a_impl(i, compat, false)];\n\tif (init && ret == NULL) {\n\t\tif (config_stats) {\n\t\t\tstruct container_s {\n\t\t\t\tctl_arena_t\t\tctl_arena;\n\t\t\t\tctl_arena_stats_t\tastats;\n\t\t\t};\n\t\t\tstruct container_s *cont =\n\t\t\t    (struct container_s *)base_alloc(tsdn, b0get(),\n\t\t\t    sizeof(struct container_s), QUANTUM);\n\t\t\tif (cont == NULL) {\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tret = &cont->ctl_arena;\n\t\t\tret->astats = &cont->astats;\n\t\t} else {\n\t\t\tret = (ctl_arena_t *)base_alloc(tsdn, b0get(),\n\t\t\t    sizeof(ctl_arena_t), QUANTUM);\n\t\t\tif (ret == NULL) {\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n\t\tret->arena_ind = (unsigned)i;\n\t\tctl_arenas->arenas[arenas_i2a_impl(i, compat, false)] = ret;\n\t}\n\n\tassert(ret == NULL || arenas_i2a(ret->arena_ind) == arenas_i2a(i));\n\treturn ret;\n}\n\nstatic ctl_arena_t *\narenas_i(size_t i) {\n\tctl_arena_t *ret = arenas_i_impl(TSDN_NULL, i, true, false);\n\tassert(ret != NULL);\n\treturn ret;\n}\n\nstatic void\nctl_arena_clear(ctl_arena_t *ctl_arena) {\n\tctl_arena->nthreads = 0;\n\tctl_arena->dss = dss_prec_names[dss_prec_limit];\n\tctl_arena->dirty_decay_ms = -1;\n\tctl_arena->muzzy_decay_ms = -1;\n\tctl_arena->pactive = 0;\n\tctl_arena->pdirty = 0;\n\tctl_arena->pmuzzy = 0;\n\tif (config_stats) {\n\t\tmemset(&ctl_arena->astats->astats, 0, sizeof(arena_stats_t));\n\t\tctl_arena->astats->allocated_small = 0;\n\t\tctl_arena->astats->nmalloc_small = 0;\n\t\tctl_arena->astats->ndalloc_small = 0;\n\t\tctl_arena->astats->nrequests_small = 0;\n\t\tmemset(ctl_arena->astats->bstats, 0, NBINS *\n\t\t    sizeof(malloc_bin_stats_t));\n\t\tmemset(ctl_arena->astats->lstats, 0, (NSIZES - NBINS) *\n\t\t    sizeof(malloc_large_stats_t));\n\t}\n}\n\nstatic void\nctl_arena_stats_amerge(tsdn_t *tsdn, ctl_arena_t *ctl_arena, arena_t *arena) {\n\tunsigned i;\n\n\tif (config_stats) {\n\t\tarena_stats_merge(tsdn, arena, &ctl_arena->nthreads,\n\t\t    &ctl_arena->dss, &ctl_arena->dirty_decay_ms,\n\t\t    &ctl_arena->muzzy_decay_ms, &ctl_arena->pactive,\n\t\t    &ctl_arena->pdirty, &ctl_arena->pmuzzy,\n\t\t    &ctl_arena->astats->astats, ctl_arena->astats->bstats,\n\t\t    ctl_arena->astats->lstats);\n\n\t\tfor (i = 0; i < NBINS; i++) {\n\t\t\tctl_arena->astats->allocated_small +=\n\t\t\t    ctl_arena->astats->bstats[i].curregs *\n\t\t\t    sz_index2size(i);\n\t\t\tctl_arena->astats->nmalloc_small +=\n\t\t\t    ctl_arena->astats->bstats[i].nmalloc;\n\t\t\tctl_arena->astats->ndalloc_small +=\n\t\t\t    ctl_arena->astats->bstats[i].ndalloc;\n\t\t\tctl_arena->astats->nrequests_small +=\n\t\t\t    ctl_arena->astats->bstats[i].nrequests;\n\t\t}\n\t} else {\n\t\tarena_basic_stats_merge(tsdn, arena, &ctl_arena->nthreads,\n\t\t    &ctl_arena->dss, &ctl_arena->dirty_decay_ms,\n\t\t    &ctl_arena->muzzy_decay_ms, &ctl_arena->pactive,\n\t\t    &ctl_arena->pdirty, &ctl_arena->pmuzzy);\n\t}\n}\n\nstatic void\nctl_arena_stats_sdmerge(ctl_arena_t *ctl_sdarena, ctl_arena_t *ctl_arena,\n    bool destroyed) {\n\tunsigned i;\n\n\tif (!destroyed) {\n\t\tctl_sdarena->nthreads += ctl_arena->nthreads;\n\t\tctl_sdarena->pactive += ctl_arena->pactive;\n\t\tctl_sdarena->pdirty += ctl_arena->pdirty;\n\t\tctl_sdarena->pmuzzy += ctl_arena->pmuzzy;\n\t} else {\n\t\tassert(ctl_arena->nthreads == 0);\n\t\tassert(ctl_arena->pactive == 0);\n\t\tassert(ctl_arena->pdirty == 0);\n\t\tassert(ctl_arena->pmuzzy == 0);\n\t}\n\n\tif (config_stats) {\n\t\tctl_arena_stats_t *sdstats = ctl_sdarena->astats;\n\t\tctl_arena_stats_t *astats = ctl_arena->astats;\n\n\t\tif (!destroyed) {\n\t\t\taccum_atomic_zu(&sdstats->astats.mapped,\n\t\t\t    &astats->astats.mapped);\n\t\t\taccum_atomic_zu(&sdstats->astats.retained,\n\t\t\t    &astats->astats.retained);\n\t\t}\n\n\t\taccum_arena_stats_u64(&sdstats->astats.decay_dirty.npurge,\n\t\t    &astats->astats.decay_dirty.npurge);\n\t\taccum_arena_stats_u64(&sdstats->astats.decay_dirty.nmadvise,\n\t\t    &astats->astats.decay_dirty.nmadvise);\n\t\taccum_arena_stats_u64(&sdstats->astats.decay_dirty.purged,\n\t\t    &astats->astats.decay_dirty.purged);\n\n\t\taccum_arena_stats_u64(&sdstats->astats.decay_muzzy.npurge,\n\t\t    &astats->astats.decay_muzzy.npurge);\n\t\taccum_arena_stats_u64(&sdstats->astats.decay_muzzy.nmadvise,\n\t\t    &astats->astats.decay_muzzy.nmadvise);\n\t\taccum_arena_stats_u64(&sdstats->astats.decay_muzzy.purged,\n\t\t    &astats->astats.decay_muzzy.purged);\n\n#define OP(mtx) malloc_mutex_prof_merge(\t\t\t\t\\\n\t\t    &(sdstats->astats.mutex_prof_data[\t\t\t\\\n\t\t        arena_prof_mutex_##mtx]),\t\t\t\\\n\t\t    &(astats->astats.mutex_prof_data[\t\t\t\\\n\t\t        arena_prof_mutex_##mtx]));\nMUTEX_PROF_ARENA_MUTEXES\n#undef OP\n\t\tif (!destroyed) {\n\t\t\taccum_atomic_zu(&sdstats->astats.base,\n\t\t\t    &astats->astats.base);\n\t\t\taccum_atomic_zu(&sdstats->astats.internal,\n\t\t\t    &astats->astats.internal);\n\t\t\taccum_atomic_zu(&sdstats->astats.resident,\n\t\t\t    &astats->astats.resident);\n\t\t} else {\n\t\t\tassert(atomic_load_zu(\n\t\t\t    &astats->astats.internal, ATOMIC_RELAXED) == 0);\n\t\t}\n\n\t\tif (!destroyed) {\n\t\t\tsdstats->allocated_small += astats->allocated_small;\n\t\t} else {\n\t\t\tassert(astats->allocated_small == 0);\n\t\t}\n\t\tsdstats->nmalloc_small += astats->nmalloc_small;\n\t\tsdstats->ndalloc_small += astats->ndalloc_small;\n\t\tsdstats->nrequests_small += astats->nrequests_small;\n\n\t\tif (!destroyed) {\n\t\t\taccum_atomic_zu(&sdstats->astats.allocated_large,\n\t\t\t    &astats->astats.allocated_large);\n\t\t} else {\n\t\t\tassert(atomic_load_zu(&astats->astats.allocated_large,\n\t\t\t    ATOMIC_RELAXED) == 0);\n\t\t}\n\t\taccum_arena_stats_u64(&sdstats->astats.nmalloc_large,\n\t\t    &astats->astats.nmalloc_large);\n\t\taccum_arena_stats_u64(&sdstats->astats.ndalloc_large,\n\t\t    &astats->astats.ndalloc_large);\n\t\taccum_arena_stats_u64(&sdstats->astats.nrequests_large,\n\t\t    &astats->astats.nrequests_large);\n\n\t\taccum_atomic_zu(&sdstats->astats.tcache_bytes,\n\t\t    &astats->astats.tcache_bytes);\n\n\t\tif (ctl_arena->arena_ind == 0) {\n\t\t\tsdstats->astats.uptime = astats->astats.uptime;\n\t\t}\n\n\t\tfor (i = 0; i < NBINS; i++) {\n\t\t\tsdstats->bstats[i].nmalloc += astats->bstats[i].nmalloc;\n\t\t\tsdstats->bstats[i].ndalloc += astats->bstats[i].ndalloc;\n\t\t\tsdstats->bstats[i].nrequests +=\n\t\t\t    astats->bstats[i].nrequests;\n\t\t\tif (!destroyed) {\n\t\t\t\tsdstats->bstats[i].curregs +=\n\t\t\t\t    astats->bstats[i].curregs;\n\t\t\t} else {\n\t\t\t\tassert(astats->bstats[i].curregs == 0);\n\t\t\t}\n\t\t\tsdstats->bstats[i].nfills += astats->bstats[i].nfills;\n\t\t\tsdstats->bstats[i].nflushes +=\n\t\t\t    astats->bstats[i].nflushes;\n\t\t\tsdstats->bstats[i].nslabs += astats->bstats[i].nslabs;\n\t\t\tsdstats->bstats[i].reslabs += astats->bstats[i].reslabs;\n\t\t\tif (!destroyed) {\n\t\t\t\tsdstats->bstats[i].curslabs +=\n\t\t\t\t    astats->bstats[i].curslabs;\n\t\t\t} else {\n\t\t\t\tassert(astats->bstats[i].curslabs == 0);\n\t\t\t}\n\t\t\tmalloc_mutex_prof_merge(&sdstats->bstats[i].mutex_data,\n\t\t\t    &astats->bstats[i].mutex_data);\n\t\t}\n\n\t\tfor (i = 0; i < NSIZES - NBINS; i++) {\n\t\t\taccum_arena_stats_u64(&sdstats->lstats[i].nmalloc,\n\t\t\t    &astats->lstats[i].nmalloc);\n\t\t\taccum_arena_stats_u64(&sdstats->lstats[i].ndalloc,\n\t\t\t    &astats->lstats[i].ndalloc);\n\t\t\taccum_arena_stats_u64(&sdstats->lstats[i].nrequests,\n\t\t\t    &astats->lstats[i].nrequests);\n\t\t\tif (!destroyed) {\n\t\t\t\tsdstats->lstats[i].curlextents +=\n\t\t\t\t    astats->lstats[i].curlextents;\n\t\t\t} else {\n\t\t\t\tassert(astats->lstats[i].curlextents == 0);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void\nctl_arena_refresh(tsdn_t *tsdn, arena_t *arena, ctl_arena_t *ctl_sdarena,\n    unsigned i, bool destroyed) {\n\tctl_arena_t *ctl_arena = arenas_i(i);\n\n\tctl_arena_clear(ctl_arena);\n\tctl_arena_stats_amerge(tsdn, ctl_arena, arena);\n\t/* Merge into sum stats as well. */\n\tctl_arena_stats_sdmerge(ctl_sdarena, ctl_arena, destroyed);\n}\n\nstatic unsigned\nctl_arena_init(tsdn_t *tsdn, extent_hooks_t *extent_hooks) {\n\tunsigned arena_ind;\n\tctl_arena_t *ctl_arena;\n\n\tif ((ctl_arena = ql_last(&ctl_arenas->destroyed, destroyed_link)) !=\n\t    NULL) {\n\t\tql_remove(&ctl_arenas->destroyed, ctl_arena, destroyed_link);\n\t\tarena_ind = ctl_arena->arena_ind;\n\t} else {\n\t\tarena_ind = ctl_arenas->narenas;\n\t}\n\n\t/* Trigger stats allocation. */\n\tif (arenas_i_impl(tsdn, arena_ind, false, true) == NULL) {\n\t\treturn UINT_MAX;\n\t}\n\n\t/* Initialize new arena. */\n\tif (arena_init(tsdn, arena_ind, extent_hooks) == NULL) {\n\t\treturn UINT_MAX;\n\t}\n\n\tif (arena_ind == ctl_arenas->narenas) {\n\t\tctl_arenas->narenas++;\n\t}\n\n\treturn arena_ind;\n}\n\nstatic void\nctl_background_thread_stats_read(tsdn_t *tsdn) {\n\tbackground_thread_stats_t *stats = &ctl_stats->background_thread;\n\tif (!have_background_thread ||\n\t    background_thread_stats_read(tsdn, stats)) {\n\t\tmemset(stats, 0, sizeof(background_thread_stats_t));\n\t\tnstime_init(&stats->run_interval, 0);\n\t}\n}\n\nstatic void\nctl_refresh(tsdn_t *tsdn) {\n\tunsigned i;\n\tctl_arena_t *ctl_sarena = arenas_i(MALLCTL_ARENAS_ALL);\n\tVARIABLE_ARRAY(arena_t *, tarenas, ctl_arenas->narenas);\n\n\t/*\n\t * Clear sum stats, since they will be merged into by\n\t * ctl_arena_refresh().\n\t */\n\tctl_arena_clear(ctl_sarena);\n\n\tfor (i = 0; i < ctl_arenas->narenas; i++) {\n\t\ttarenas[i] = arena_get(tsdn, i, false);\n\t}\n\n\tfor (i = 0; i < ctl_arenas->narenas; i++) {\n\t\tctl_arena_t *ctl_arena = arenas_i(i);\n\t\tbool initialized = (tarenas[i] != NULL);\n\n\t\tctl_arena->initialized = initialized;\n\t\tif (initialized) {\n\t\t\tctl_arena_refresh(tsdn, tarenas[i], ctl_sarena, i,\n\t\t\t    false);\n\t\t}\n\t}\n\n\tif (config_stats) {\n\t\tctl_stats->allocated = ctl_sarena->astats->allocated_small +\n\t\t    atomic_load_zu(&ctl_sarena->astats->astats.allocated_large,\n\t\t\tATOMIC_RELAXED);\n\t\tctl_stats->active = (ctl_sarena->pactive << LG_PAGE);\n\t\tctl_stats->metadata = atomic_load_zu(\n\t\t    &ctl_sarena->astats->astats.base, ATOMIC_RELAXED) +\n\t\t    atomic_load_zu(&ctl_sarena->astats->astats.internal,\n\t\t\tATOMIC_RELAXED);\n\t\tctl_stats->resident = atomic_load_zu(\n\t\t    &ctl_sarena->astats->astats.resident, ATOMIC_RELAXED);\n\t\tctl_stats->mapped = atomic_load_zu(\n\t\t    &ctl_sarena->astats->astats.mapped, ATOMIC_RELAXED);\n\t\tctl_stats->retained = atomic_load_zu(\n\t\t    &ctl_sarena->astats->astats.retained, ATOMIC_RELAXED);\n\n\t\tctl_background_thread_stats_read(tsdn);\n\n#define READ_GLOBAL_MUTEX_PROF_DATA(i, mtx)\t\t\t\t\\\n    malloc_mutex_lock(tsdn, &mtx);\t\t\t\t\t\\\n    malloc_mutex_prof_read(tsdn, &ctl_stats->mutex_prof_data[i], &mtx);\t\\\n    malloc_mutex_unlock(tsdn, &mtx);\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tREAD_GLOBAL_MUTEX_PROF_DATA(global_prof_mutex_prof,\n\t\t\t    bt2gctx_mtx);\n\t\t}\n\t\tif (have_background_thread) {\n\t\t\tREAD_GLOBAL_MUTEX_PROF_DATA(\n\t\t\t    global_prof_mutex_background_thread,\n\t\t\t    background_thread_lock);\n\t\t} else {\n\t\t\tmemset(&ctl_stats->mutex_prof_data[\n\t\t\t    global_prof_mutex_background_thread], 0,\n\t\t\t    sizeof(mutex_prof_data_t));\n\t\t}\n\t\t/* We own ctl mutex already. */\n\t\tmalloc_mutex_prof_read(tsdn,\n\t\t    &ctl_stats->mutex_prof_data[global_prof_mutex_ctl],\n\t\t    &ctl_mtx);\n#undef READ_GLOBAL_MUTEX_PROF_DATA\n\t}\n\tctl_arenas->epoch++;\n}\n\nstatic bool\nctl_init(tsdn_t *tsdn) {\n\tbool ret;\n\n\tmalloc_mutex_lock(tsdn, &ctl_mtx);\n\tif (!ctl_initialized) {\n\t\tctl_arena_t *ctl_sarena, *ctl_darena;\n\t\tunsigned i;\n\n\t\t/*\n\t\t * Allocate demand-zeroed space for pointers to the full\n\t\t * range of supported arena indices.\n\t\t */\n\t\tif (ctl_arenas == NULL) {\n\t\t\tctl_arenas = (ctl_arenas_t *)base_alloc(tsdn,\n\t\t\t    b0get(), sizeof(ctl_arenas_t), QUANTUM);\n\t\t\tif (ctl_arenas == NULL) {\n\t\t\t\tret = true;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t}\n\n\t\tif (config_stats && ctl_stats == NULL) {\n\t\t\tctl_stats = (ctl_stats_t *)base_alloc(tsdn, b0get(),\n\t\t\t    sizeof(ctl_stats_t), QUANTUM);\n\t\t\tif (ctl_stats == NULL) {\n\t\t\t\tret = true;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Allocate space for the current full range of arenas\n\t\t * here rather than doing it lazily elsewhere, in order\n\t\t * to limit when OOM-caused errors can occur.\n\t\t */\n\t\tif ((ctl_sarena = arenas_i_impl(tsdn, MALLCTL_ARENAS_ALL, false,\n\t\t    true)) == NULL) {\n\t\t\tret = true;\n\t\t\tgoto label_return;\n\t\t}\n\t\tctl_sarena->initialized = true;\n\n\t\tif ((ctl_darena = arenas_i_impl(tsdn, MALLCTL_ARENAS_DESTROYED,\n\t\t    false, true)) == NULL) {\n\t\t\tret = true;\n\t\t\tgoto label_return;\n\t\t}\n\t\tctl_arena_clear(ctl_darena);\n\t\t/*\n\t\t * Don't toggle ctl_darena to initialized until an arena is\n\t\t * actually destroyed, so that arena.<i>.initialized can be used\n\t\t * to query whether the stats are relevant.\n\t\t */\n\n\t\tctl_arenas->narenas = narenas_total_get();\n\t\tfor (i = 0; i < ctl_arenas->narenas; i++) {\n\t\t\tif (arenas_i_impl(tsdn, i, false, true) == NULL) {\n\t\t\t\tret = true;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t}\n\n\t\tql_new(&ctl_arenas->destroyed);\n\t\tctl_refresh(tsdn);\n\n\t\tctl_initialized = true;\n\t}\n\n\tret = false;\nlabel_return:\n\tmalloc_mutex_unlock(tsdn, &ctl_mtx);\n\treturn ret;\n}\n\nstatic int\nctl_lookup(tsdn_t *tsdn, const char *name, ctl_node_t const **nodesp,\n    size_t *mibp, size_t *depthp) {\n\tint ret;\n\tconst char *elm, *tdot, *dot;\n\tsize_t elen, i, j;\n\tconst ctl_named_node_t *node;\n\n\telm = name;\n\t/* Equivalent to strchrnul(). */\n\tdot = ((tdot = strchr(elm, '.')) != NULL) ? tdot : strchr(elm, '\\0');\n\telen = (size_t)((uintptr_t)dot - (uintptr_t)elm);\n\tif (elen == 0) {\n\t\tret = ENOENT;\n\t\tgoto label_return;\n\t}\n\tnode = super_root_node;\n\tfor (i = 0; i < *depthp; i++) {\n\t\tassert(node);\n\t\tassert(node->nchildren > 0);\n\t\tif (ctl_named_node(node->children) != NULL) {\n\t\t\tconst ctl_named_node_t *pnode = node;\n\n\t\t\t/* Children are named. */\n\t\t\tfor (j = 0; j < node->nchildren; j++) {\n\t\t\t\tconst ctl_named_node_t *child =\n\t\t\t\t    ctl_named_children(node, j);\n\t\t\t\tif (strlen(child->name) == elen &&\n\t\t\t\t    strncmp(elm, child->name, elen) == 0) {\n\t\t\t\t\tnode = child;\n\t\t\t\t\tif (nodesp != NULL) {\n\t\t\t\t\t\tnodesp[i] =\n\t\t\t\t\t\t    (const ctl_node_t *)node;\n\t\t\t\t\t}\n\t\t\t\t\tmibp[i] = j;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (node == pnode) {\n\t\t\t\tret = ENOENT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t} else {\n\t\t\tuintmax_t index;\n\t\t\tconst ctl_indexed_node_t *inode;\n\n\t\t\t/* Children are indexed. */\n\t\t\tindex = malloc_strtoumax(elm, NULL, 10);\n\t\t\tif (index == UINTMAX_MAX || index > SIZE_T_MAX) {\n\t\t\t\tret = ENOENT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\n\t\t\tinode = ctl_indexed_node(node->children);\n\t\t\tnode = inode->index(tsdn, mibp, *depthp, (size_t)index);\n\t\t\tif (node == NULL) {\n\t\t\t\tret = ENOENT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\n\t\t\tif (nodesp != NULL) {\n\t\t\t\tnodesp[i] = (const ctl_node_t *)node;\n\t\t\t}\n\t\t\tmibp[i] = (size_t)index;\n\t\t}\n\n\t\tif (node->ctl != NULL) {\n\t\t\t/* Terminal node. */\n\t\t\tif (*dot != '\\0') {\n\t\t\t\t/*\n\t\t\t\t * The name contains more elements than are\n\t\t\t\t * in this path through the tree.\n\t\t\t\t */\n\t\t\t\tret = ENOENT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t\t/* Complete lookup successful. */\n\t\t\t*depthp = i + 1;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Update elm. */\n\t\tif (*dot == '\\0') {\n\t\t\t/* No more elements. */\n\t\t\tret = ENOENT;\n\t\t\tgoto label_return;\n\t\t}\n\t\telm = &dot[1];\n\t\tdot = ((tdot = strchr(elm, '.')) != NULL) ? tdot :\n\t\t    strchr(elm, '\\0');\n\t\telen = (size_t)((uintptr_t)dot - (uintptr_t)elm);\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nint\nctl_byname(tsd_t *tsd, const char *name, void *oldp, size_t *oldlenp,\n    void *newp, size_t newlen) {\n\tint ret;\n\tsize_t depth;\n\tctl_node_t const *nodes[CTL_MAX_DEPTH];\n\tsize_t mib[CTL_MAX_DEPTH];\n\tconst ctl_named_node_t *node;\n\n\tif (!ctl_initialized && ctl_init(tsd_tsdn(tsd))) {\n\t\tret = EAGAIN;\n\t\tgoto label_return;\n\t}\n\n\tdepth = CTL_MAX_DEPTH;\n\tret = ctl_lookup(tsd_tsdn(tsd), name, nodes, mib, &depth);\n\tif (ret != 0) {\n\t\tgoto label_return;\n\t}\n\n\tnode = ctl_named_node(nodes[depth-1]);\n\tif (node != NULL && node->ctl) {\n\t\tret = node->ctl(tsd, mib, depth, oldp, oldlenp, newp, newlen);\n\t} else {\n\t\t/* The name refers to a partial path through the ctl tree. */\n\t\tret = ENOENT;\n\t}\n\nlabel_return:\n\treturn(ret);\n}\n\nint\nctl_nametomib(tsdn_t *tsdn, const char *name, size_t *mibp, size_t *miblenp) {\n\tint ret;\n\n\tif (!ctl_initialized && ctl_init(tsdn)) {\n\t\tret = EAGAIN;\n\t\tgoto label_return;\n\t}\n\n\tret = ctl_lookup(tsdn, name, NULL, mibp, miblenp);\nlabel_return:\n\treturn(ret);\n}\n\nint\nctl_bymib(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tconst ctl_named_node_t *node;\n\tsize_t i;\n\n\tif (!ctl_initialized && ctl_init(tsd_tsdn(tsd))) {\n\t\tret = EAGAIN;\n\t\tgoto label_return;\n\t}\n\n\t/* Iterate down the tree. */\n\tnode = super_root_node;\n\tfor (i = 0; i < miblen; i++) {\n\t\tassert(node);\n\t\tassert(node->nchildren > 0);\n\t\tif (ctl_named_node(node->children) != NULL) {\n\t\t\t/* Children are named. */\n\t\t\tif (node->nchildren <= mib[i]) {\n\t\t\t\tret = ENOENT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t\tnode = ctl_named_children(node, mib[i]);\n\t\t} else {\n\t\t\tconst ctl_indexed_node_t *inode;\n\n\t\t\t/* Indexed element. */\n\t\t\tinode = ctl_indexed_node(node->children);\n\t\t\tnode = inode->index(tsd_tsdn(tsd), mib, miblen, mib[i]);\n\t\t\tif (node == NULL) {\n\t\t\t\tret = ENOENT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Call the ctl function. */\n\tif (node && node->ctl) {\n\t\tret = node->ctl(tsd, mib, miblen, oldp, oldlenp, newp, newlen);\n\t} else {\n\t\t/* Partial MIB. */\n\t\tret = ENOENT;\n\t}\n\nlabel_return:\n\treturn(ret);\n}\n\nbool\nctl_boot(void) {\n\tif (malloc_mutex_init(&ctl_mtx, \"ctl\", WITNESS_RANK_CTL,\n\t    malloc_mutex_rank_exclusive)) {\n\t\treturn true;\n\t}\n\n\tctl_initialized = false;\n\n\treturn false;\n}\n\nvoid\nctl_prefork(tsdn_t *tsdn) {\n\tmalloc_mutex_prefork(tsdn, &ctl_mtx);\n}\n\nvoid\nctl_postfork_parent(tsdn_t *tsdn) {\n\tmalloc_mutex_postfork_parent(tsdn, &ctl_mtx);\n}\n\nvoid\nctl_postfork_child(tsdn_t *tsdn) {\n\tmalloc_mutex_postfork_child(tsdn, &ctl_mtx);\n}\n\n/******************************************************************************/\n/* *_ctl() functions. */\n\n#define READONLY()\tdo {\t\t\t\t\t\t\\\n\tif (newp != NULL || newlen != 0) {\t\t\t\t\\\n\t\tret = EPERM;\t\t\t\t\t\t\\\n\t\tgoto label_return;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define WRITEONLY()\tdo {\t\t\t\t\t\t\\\n\tif (oldp != NULL || oldlenp != NULL) {\t\t\t\t\\\n\t\tret = EPERM;\t\t\t\t\t\t\\\n\t\tgoto label_return;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define READ_XOR_WRITE()\tdo {\t\t\t\t\t\\\n\tif ((oldp != NULL && oldlenp != NULL) && (newp != NULL ||\t\\\n\t    newlen != 0)) {\t\t\t\t\t\t\\\n\t\tret = EPERM;\t\t\t\t\t\t\\\n\t\tgoto label_return;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define READ(v, t)\tdo {\t\t\t\t\t\t\\\n\tif (oldp != NULL && oldlenp != NULL) {\t\t\t\t\\\n\t\tif (*oldlenp != sizeof(t)) {\t\t\t\t\\\n\t\t\tsize_t\tcopylen = (sizeof(t) <= *oldlenp)\t\\\n\t\t\t    ? sizeof(t) : *oldlenp;\t\t\t\\\n\t\t\tmemcpy(oldp, (void *)&(v), copylen);\t\t\\\n\t\t\tret = EINVAL;\t\t\t\t\t\\\n\t\t\tgoto label_return;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t*(t *)oldp = (v);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define WRITE(v, t)\tdo {\t\t\t\t\t\t\\\n\tif (newp != NULL) {\t\t\t\t\t\t\\\n\t\tif (newlen != sizeof(t)) {\t\t\t\t\\\n\t\t\tret = EINVAL;\t\t\t\t\t\\\n\t\t\tgoto label_return;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t(v) = *(t *)newp;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define MIB_UNSIGNED(v, i) do {\t\t\t\t\t\t\\\n\tif (mib[i] > UINT_MAX) {\t\t\t\t\t\\\n\t\tret = EFAULT;\t\t\t\t\t\t\\\n\t\tgoto label_return;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tv = (unsigned)mib[i];\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * There's a lot of code duplication in the following macros due to limitations\n * in how nested cpp macros are expanded.\n */\n#define CTL_RO_CLGEN(c, l, n, v, t)\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!(c)) {\t\t\t\t\t\t\t\\\n\t\treturn ENOENT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tif (l) {\t\t\t\t\t\t\t\\\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = (v);\t\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\tif (l) {\t\t\t\t\t\t\t\\\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n#define CTL_RO_CGEN(c, n, v, t)\t\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!(c)) {\t\t\t\t\t\t\t\\\n\t\treturn ENOENT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = (v);\t\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n#define CTL_RO_GEN(n, v, t)\t\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = (v);\t\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n/*\n * ctl_mtx is not acquired, under the assumption that no pertinent data will\n * mutate during the call.\n */\n#define CTL_RO_NL_CGEN(c, n, v, t)\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!(c)) {\t\t\t\t\t\t\t\\\n\t\treturn ENOENT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = (v);\t\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n#define CTL_RO_NL_GEN(n, v, t)\t\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = (v);\t\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n#define CTL_TSD_RO_NL_CGEN(c, n, m, t)\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!(c)) {\t\t\t\t\t\t\t\\\n\t\treturn ENOENT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = (m(tsd));\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n#define CTL_RO_CONFIG_GEN(n, t)\t\t\t\t\t\t\\\nstatic int\t\t\t\t\t\t\t\t\\\nn##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\t\\\n    size_t *oldlenp, void *newp, size_t newlen) {\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\tt oldval;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tREADONLY();\t\t\t\t\t\t\t\\\n\toldval = n;\t\t\t\t\t\t\t\\\n\tREAD(oldval, t);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\nlabel_return:\t\t\t\t\t\t\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\n/******************************************************************************/\n\nCTL_RO_NL_GEN(version, JEMALLOC_VERSION, const char *)\n\nstatic int\nepoch_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tUNUSED uint64_t newval;\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\n\tWRITE(newval, uint64_t);\n\tif (newp != NULL) {\n\t\tctl_refresh(tsd_tsdn(tsd));\n\t}\n\tREAD(ctl_arenas->epoch, uint64_t);\n\n\tret = 0;\nlabel_return:\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\n\treturn ret;\n}\n\nstatic int\nbackground_thread_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tbool oldval;\n\n\tif (!have_background_thread) {\n\t\treturn ENOENT;\n\t}\n\tbackground_thread_ctl_init(tsd_tsdn(tsd));\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);\n\tif (newp == NULL) {\n\t\toldval = background_thread_enabled();\n\t\tREAD(oldval, bool);\n\t} else {\n\t\tif (newlen != sizeof(bool)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\toldval = background_thread_enabled();\n\t\tREAD(oldval, bool);\n\n\t\tbool newval = *(bool *)newp;\n\t\tif (newval == oldval) {\n\t\t\tret = 0;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tbackground_thread_enabled_set(tsd_tsdn(tsd), newval);\n\t\tif (newval) {\n\t\t\tif (!can_enable_background_thread) {\n\t\t\t\tmalloc_printf(\"<jemalloc>: Error in dlsym(\"\n\t\t\t            \"RTLD_NEXT, \\\"pthread_create\\\"). Cannot \"\n\t\t\t\t    \"enable background_thread\\n\");\n\t\t\t\tret = EFAULT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t\tif (background_threads_enable(tsd)) {\n\t\t\t\tret = EFAULT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t} else {\n\t\t\tif (background_threads_disable(tsd)) {\n\t\t\t\tret = EFAULT;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t}\n\t}\n\tret = 0;\nlabel_return:\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\n\n\treturn ret;\n}\n\n/******************************************************************************/\n\nCTL_RO_CONFIG_GEN(config_cache_oblivious, bool)\nCTL_RO_CONFIG_GEN(config_debug, bool)\nCTL_RO_CONFIG_GEN(config_fill, bool)\nCTL_RO_CONFIG_GEN(config_lazy_lock, bool)\nCTL_RO_CONFIG_GEN(config_malloc_conf, const char *)\nCTL_RO_CONFIG_GEN(config_prof, bool)\nCTL_RO_CONFIG_GEN(config_prof_libgcc, bool)\nCTL_RO_CONFIG_GEN(config_prof_libunwind, bool)\nCTL_RO_CONFIG_GEN(config_stats, bool)\nCTL_RO_CONFIG_GEN(config_thp, bool)\nCTL_RO_CONFIG_GEN(config_utrace, bool)\nCTL_RO_CONFIG_GEN(config_xmalloc, bool)\n\n/******************************************************************************/\n\nCTL_RO_NL_GEN(opt_abort, opt_abort, bool)\nCTL_RO_NL_GEN(opt_abort_conf, opt_abort_conf, bool)\nCTL_RO_NL_GEN(opt_retain, opt_retain, bool)\nCTL_RO_NL_GEN(opt_dss, opt_dss, const char *)\nCTL_RO_NL_GEN(opt_narenas, opt_narenas, unsigned)\nCTL_RO_NL_GEN(opt_percpu_arena, percpu_arena_mode_names[opt_percpu_arena],\n    const char *)\nCTL_RO_NL_GEN(opt_background_thread, opt_background_thread, bool)\nCTL_RO_NL_GEN(opt_dirty_decay_ms, opt_dirty_decay_ms, ssize_t)\nCTL_RO_NL_GEN(opt_muzzy_decay_ms, opt_muzzy_decay_ms, ssize_t)\nCTL_RO_NL_GEN(opt_stats_print, opt_stats_print, bool)\nCTL_RO_NL_GEN(opt_stats_print_opts, opt_stats_print_opts, const char *)\nCTL_RO_NL_CGEN(config_fill, opt_junk, opt_junk, const char *)\nCTL_RO_NL_CGEN(config_fill, opt_zero, opt_zero, bool)\nCTL_RO_NL_CGEN(config_utrace, opt_utrace, opt_utrace, bool)\nCTL_RO_NL_CGEN(config_xmalloc, opt_xmalloc, opt_xmalloc, bool)\nCTL_RO_NL_GEN(opt_tcache, opt_tcache, bool)\nCTL_RO_NL_GEN(opt_lg_tcache_max, opt_lg_tcache_max, ssize_t)\nCTL_RO_NL_CGEN(config_prof, opt_prof, opt_prof, bool)\nCTL_RO_NL_CGEN(config_prof, opt_prof_prefix, opt_prof_prefix, const char *)\nCTL_RO_NL_CGEN(config_prof, opt_prof_active, opt_prof_active, bool)\nCTL_RO_NL_CGEN(config_prof, opt_prof_thread_active_init,\n    opt_prof_thread_active_init, bool)\nCTL_RO_NL_CGEN(config_prof, opt_lg_prof_sample, opt_lg_prof_sample, size_t)\nCTL_RO_NL_CGEN(config_prof, opt_prof_accum, opt_prof_accum, bool)\nCTL_RO_NL_CGEN(config_prof, opt_lg_prof_interval, opt_lg_prof_interval, ssize_t)\nCTL_RO_NL_CGEN(config_prof, opt_prof_gdump, opt_prof_gdump, bool)\nCTL_RO_NL_CGEN(config_prof, opt_prof_final, opt_prof_final, bool)\nCTL_RO_NL_CGEN(config_prof, opt_prof_leak, opt_prof_leak, bool)\n\n/******************************************************************************/\n\nstatic int\nthread_arena_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tarena_t *oldarena;\n\tunsigned newind, oldind;\n\n\toldarena = arena_choose(tsd, NULL);\n\tif (oldarena == NULL) {\n\t\treturn EAGAIN;\n\t}\n\tnewind = oldind = arena_ind_get(oldarena);\n\tWRITE(newind, unsigned);\n\tREAD(oldind, unsigned);\n\n\tif (newind != oldind) {\n\t\tarena_t *newarena;\n\n\t\tif (newind >= narenas_total_get()) {\n\t\t\t/* New arena index is out of range. */\n\t\t\tret = EFAULT;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tif (have_percpu_arena &&\n\t\t    PERCPU_ARENA_ENABLED(opt_percpu_arena)) {\n\t\t\tif (newind < percpu_arena_ind_limit(opt_percpu_arena)) {\n\t\t\t\t/*\n\t\t\t\t * If perCPU arena is enabled, thread_arena\n\t\t\t\t * control is not allowed for the auto arena\n\t\t\t\t * range.\n\t\t\t\t */\n\t\t\t\tret = EPERM;\n\t\t\t\tgoto label_return;\n\t\t\t}\n\t\t}\n\n\t\t/* Initialize arena if necessary. */\n\t\tnewarena = arena_get(tsd_tsdn(tsd), newind, true);\n\t\tif (newarena == NULL) {\n\t\t\tret = EAGAIN;\n\t\t\tgoto label_return;\n\t\t}\n\t\t/* Set new arena/tcache associations. */\n\t\tarena_migrate(tsd, oldind, newind);\n\t\tif (tcache_available(tsd)) {\n\t\t\ttcache_arena_reassociate(tsd_tsdn(tsd),\n\t\t\t    tsd_tcachep_get(tsd), newarena);\n\t\t}\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nCTL_TSD_RO_NL_CGEN(config_stats, thread_allocated, tsd_thread_allocated_get,\n    uint64_t)\nCTL_TSD_RO_NL_CGEN(config_stats, thread_allocatedp, tsd_thread_allocatedp_get,\n    uint64_t *)\nCTL_TSD_RO_NL_CGEN(config_stats, thread_deallocated, tsd_thread_deallocated_get,\n    uint64_t)\nCTL_TSD_RO_NL_CGEN(config_stats, thread_deallocatedp,\n    tsd_thread_deallocatedp_get, uint64_t *)\n\nstatic int\nthread_tcache_enabled_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tbool oldval;\n\n\toldval = tcache_enabled_get(tsd);\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(bool)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\ttcache_enabled_set(tsd, *(bool *)newp);\n\t}\n\tREAD(oldval, bool);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nthread_tcache_flush_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\n\tif (!tcache_available(tsd)) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\n\tREADONLY();\n\tWRITEONLY();\n\n\ttcache_flush(tsd);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nthread_prof_name_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\tREAD_XOR_WRITE();\n\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(const char *)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tif ((ret = prof_thread_name_set(tsd, *(const char **)newp)) !=\n\t\t    0) {\n\t\t\tgoto label_return;\n\t\t}\n\t} else {\n\t\tconst char *oldname = prof_thread_name_get(tsd);\n\t\tREAD(oldname, const char *);\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nthread_prof_active_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tbool oldval;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\toldval = prof_thread_active_get(tsd);\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(bool)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tif (prof_thread_active_set(tsd, *(bool *)newp)) {\n\t\t\tret = EAGAIN;\n\t\t\tgoto label_return;\n\t\t}\n\t}\n\tREAD(oldval, bool);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\n/******************************************************************************/\n\nstatic int\ntcache_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned tcache_ind;\n\n\tREADONLY();\n\tif (tcaches_create(tsd, &tcache_ind)) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\tREAD(tcache_ind, unsigned);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\ntcache_flush_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned tcache_ind;\n\n\tWRITEONLY();\n\ttcache_ind = UINT_MAX;\n\tWRITE(tcache_ind, unsigned);\n\tif (tcache_ind == UINT_MAX) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\ttcaches_flush(tsd, tcache_ind);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\ntcache_destroy_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned tcache_ind;\n\n\tWRITEONLY();\n\ttcache_ind = UINT_MAX;\n\tWRITE(tcache_ind, unsigned);\n\tif (tcache_ind == UINT_MAX) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\ttcaches_destroy(tsd, tcache_ind);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\n/******************************************************************************/\n\nstatic int\narena_i_initialized_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\ttsdn_t *tsdn = tsd_tsdn(tsd);\n\tunsigned arena_ind;\n\tbool initialized;\n\n\tREADONLY();\n\tMIB_UNSIGNED(arena_ind, 1);\n\n\tmalloc_mutex_lock(tsdn, &ctl_mtx);\n\tinitialized = arenas_i(arena_ind)->initialized;\n\tmalloc_mutex_unlock(tsdn, &ctl_mtx);\n\n\tREAD(initialized, bool);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic void\narena_i_decay(tsdn_t *tsdn, unsigned arena_ind, bool all) {\n\tmalloc_mutex_lock(tsdn, &ctl_mtx);\n\t{\n\t\tunsigned narenas = ctl_arenas->narenas;\n\n\t\t/*\n\t\t * Access via index narenas is deprecated, and scheduled for\n\t\t * removal in 6.0.0.\n\t\t */\n\t\tif (arena_ind == MALLCTL_ARENAS_ALL || arena_ind == narenas) {\n\t\t\tunsigned i;\n\t\t\tVARIABLE_ARRAY(arena_t *, tarenas, narenas);\n\n\t\t\tfor (i = 0; i < narenas; i++) {\n\t\t\t\ttarenas[i] = arena_get(tsdn, i, false);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * No further need to hold ctl_mtx, since narenas and\n\t\t\t * tarenas contain everything needed below.\n\t\t\t */\n\t\t\tmalloc_mutex_unlock(tsdn, &ctl_mtx);\n\n\t\t\tfor (i = 0; i < narenas; i++) {\n\t\t\t\tif (tarenas[i] != NULL) {\n\t\t\t\t\tarena_decay(tsdn, tarenas[i], false,\n\t\t\t\t\t    all);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tarena_t *tarena;\n\n\t\t\tassert(arena_ind < narenas);\n\n\t\t\ttarena = arena_get(tsdn, arena_ind, false);\n\n\t\t\t/* No further need to hold ctl_mtx. */\n\t\t\tmalloc_mutex_unlock(tsdn, &ctl_mtx);\n\n\t\t\tif (tarena != NULL) {\n\t\t\t\tarena_decay(tsdn, tarena, false, all);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int\narena_i_decay_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned arena_ind;\n\n\tREADONLY();\n\tWRITEONLY();\n\tMIB_UNSIGNED(arena_ind, 1);\n\tarena_i_decay(tsd_tsdn(tsd), arena_ind, false);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\narena_i_purge_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned arena_ind;\n\n\tREADONLY();\n\tWRITEONLY();\n\tMIB_UNSIGNED(arena_ind, 1);\n\tarena_i_decay(tsd_tsdn(tsd), arena_ind, true);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\narena_i_reset_destroy_helper(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen, unsigned *arena_ind,\n    arena_t **arena) {\n\tint ret;\n\n\tREADONLY();\n\tWRITEONLY();\n\tMIB_UNSIGNED(*arena_ind, 1);\n\n\t*arena = arena_get(tsd_tsdn(tsd), *arena_ind, false);\n\tif (*arena == NULL || arena_is_auto(*arena)) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic void\narena_reset_prepare_background_thread(tsd_t *tsd, unsigned arena_ind) {\n\t/* Temporarily disable the background thread during arena reset. */\n\tif (have_background_thread) {\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);\n\t\tif (background_thread_enabled()) {\n\t\t\tunsigned ind = arena_ind % ncpus;\n\t\t\tbackground_thread_info_t *info =\n\t\t\t    &background_thread_info[ind];\n\t\t\tassert(info->state == background_thread_started);\n\t\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\t\t\tinfo->state = background_thread_paused;\n\t\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\t\t}\n\t}\n}\n\nstatic void\narena_reset_finish_background_thread(tsd_t *tsd, unsigned arena_ind) {\n\tif (have_background_thread) {\n\t\tif (background_thread_enabled()) {\n\t\t\tunsigned ind = arena_ind % ncpus;\n\t\t\tbackground_thread_info_t *info =\n\t\t\t    &background_thread_info[ind];\n\t\t\tassert(info->state = background_thread_paused);\n\t\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);\n\t\t\tinfo->state = background_thread_started;\n\t\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);\n\t\t}\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);\n\t}\n}\n\nstatic int\narena_i_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned arena_ind;\n\tarena_t *arena;\n\n\tret = arena_i_reset_destroy_helper(tsd, mib, miblen, oldp, oldlenp,\n\t    newp, newlen, &arena_ind, &arena);\n\tif (ret != 0) {\n\t\treturn ret;\n\t}\n\n\tarena_reset_prepare_background_thread(tsd, arena_ind);\n\tarena_reset(tsd, arena);\n\tarena_reset_finish_background_thread(tsd, arena_ind);\n\n\treturn ret;\n}\n\nstatic int\narena_i_destroy_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned arena_ind;\n\tarena_t *arena;\n\tctl_arena_t *ctl_darena, *ctl_arena;\n\n\tret = arena_i_reset_destroy_helper(tsd, mib, miblen, oldp, oldlenp,\n\t    newp, newlen, &arena_ind, &arena);\n\tif (ret != 0) {\n\t\tgoto label_return;\n\t}\n\n\tif (arena_nthreads_get(arena, false) != 0 || arena_nthreads_get(arena,\n\t    true) != 0) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\n\tarena_reset_prepare_background_thread(tsd, arena_ind);\n\t/* Merge stats after resetting and purging arena. */\n\tarena_reset(tsd, arena);\n\tarena_decay(tsd_tsdn(tsd), arena, false, true);\n\tctl_darena = arenas_i(MALLCTL_ARENAS_DESTROYED);\n\tctl_darena->initialized = true;\n\tctl_arena_refresh(tsd_tsdn(tsd), arena, ctl_darena, arena_ind, true);\n\t/* Destroy arena. */\n\tarena_destroy(tsd, arena);\n\tctl_arena = arenas_i(arena_ind);\n\tctl_arena->initialized = false;\n\t/* Record arena index for later recycling via arenas.create. */\n\tql_elm_new(ctl_arena, destroyed_link);\n\tql_tail_insert(&ctl_arenas->destroyed, ctl_arena, destroyed_link);\n\tarena_reset_finish_background_thread(tsd, arena_ind);\n\n\tassert(ret == 0);\nlabel_return:\n\treturn ret;\n}\n\nstatic int\narena_i_dss_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tconst char *dss = NULL;\n\tunsigned arena_ind;\n\tdss_prec_t dss_prec_old = dss_prec_limit;\n\tdss_prec_t dss_prec = dss_prec_limit;\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\n\tWRITE(dss, const char *);\n\tMIB_UNSIGNED(arena_ind, 1);\n\tif (dss != NULL) {\n\t\tint i;\n\t\tbool match = false;\n\n\t\tfor (i = 0; i < dss_prec_limit; i++) {\n\t\t\tif (strcmp(dss_prec_names[i], dss) == 0) {\n\t\t\t\tdss_prec = i;\n\t\t\t\tmatch = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!match) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t}\n\n\t/*\n\t * Access via index narenas is deprecated, and scheduled for removal in\n\t * 6.0.0.\n\t */\n\tif (arena_ind == MALLCTL_ARENAS_ALL || arena_ind ==\n\t    ctl_arenas->narenas) {\n\t\tif (dss_prec != dss_prec_limit &&\n\t\t    extent_dss_prec_set(dss_prec)) {\n\t\t\tret = EFAULT;\n\t\t\tgoto label_return;\n\t\t}\n\t\tdss_prec_old = extent_dss_prec_get();\n\t} else {\n\t\tarena_t *arena = arena_get(tsd_tsdn(tsd), arena_ind, false);\n\t\tif (arena == NULL || (dss_prec != dss_prec_limit &&\n\t\t    arena_dss_prec_set(arena, dss_prec))) {\n\t\t\tret = EFAULT;\n\t\t\tgoto label_return;\n\t\t}\n\t\tdss_prec_old = arena_dss_prec_get(arena);\n\t}\n\n\tdss = dss_prec_names[dss_prec_old];\n\tREAD(dss, const char *);\n\n\tret = 0;\nlabel_return:\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\n\treturn ret;\n}\n\nstatic int\narena_i_decay_ms_ctl_impl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen, bool dirty) {\n\tint ret;\n\tunsigned arena_ind;\n\tarena_t *arena;\n\n\tMIB_UNSIGNED(arena_ind, 1);\n\tarena = arena_get(tsd_tsdn(tsd), arena_ind, false);\n\tif (arena == NULL) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\n\tif (oldp != NULL && oldlenp != NULL) {\n\t\tsize_t oldval = dirty ? arena_dirty_decay_ms_get(arena) :\n\t\t    arena_muzzy_decay_ms_get(arena);\n\t\tREAD(oldval, ssize_t);\n\t}\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(ssize_t)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tif (dirty ? arena_dirty_decay_ms_set(tsd_tsdn(tsd), arena,\n\t\t    *(ssize_t *)newp) : arena_muzzy_decay_ms_set(tsd_tsdn(tsd),\n\t\t    arena, *(ssize_t *)newp)) {\n\t\t\tret = EFAULT;\n\t\t\tgoto label_return;\n\t\t}\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\narena_i_dirty_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\treturn arena_i_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,\n\t    newlen, true);\n}\n\nstatic int\narena_i_muzzy_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\treturn arena_i_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,\n\t    newlen, false);\n}\n\nstatic int\narena_i_extent_hooks_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned arena_ind;\n\tarena_t *arena;\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\n\tMIB_UNSIGNED(arena_ind, 1);\n\tif (arena_ind < narenas_total_get() && (arena =\n\t    arena_get(tsd_tsdn(tsd), arena_ind, false)) != NULL) {\n\t\tif (newp != NULL) {\n\t\t\textent_hooks_t *old_extent_hooks;\n\t\t\textent_hooks_t *new_extent_hooks\n\t\t\t    JEMALLOC_CC_SILENCE_INIT(NULL);\n\t\t\tWRITE(new_extent_hooks, extent_hooks_t *);\n\t\t\told_extent_hooks = extent_hooks_set(tsd, arena,\n\t\t\t    new_extent_hooks);\n\t\t\tREAD(old_extent_hooks, extent_hooks_t *);\n\t\t} else {\n\t\t\textent_hooks_t *old_extent_hooks =\n\t\t\t    extent_hooks_get(arena);\n\t\t\tREAD(old_extent_hooks, extent_hooks_t *);\n\t\t}\n\t} else {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\tret = 0;\nlabel_return:\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\n\treturn ret;\n}\n\nstatic const ctl_named_node_t *\narena_i_index(tsdn_t *tsdn, const size_t *mib, size_t miblen, size_t i) {\n\tconst ctl_named_node_t *ret;\n\n\tmalloc_mutex_lock(tsdn, &ctl_mtx);\n\tswitch (i) {\n\tcase MALLCTL_ARENAS_ALL:\n\tcase MALLCTL_ARENAS_DESTROYED:\n\t\tbreak;\n\tdefault:\n\t\tif (i > ctl_arenas->narenas) {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tbreak;\n\t}\n\n\tret = super_arena_i_node;\nlabel_return:\n\tmalloc_mutex_unlock(tsdn, &ctl_mtx);\n\treturn ret;\n}\n\n/******************************************************************************/\n\nstatic int\narenas_narenas_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tunsigned narenas;\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\n\tREADONLY();\n\tif (*oldlenp != sizeof(unsigned)) {\n\t\tret = EINVAL;\n\t\tgoto label_return;\n\t}\n\tnarenas = ctl_arenas->narenas;\n\tREAD(narenas, unsigned);\n\n\tret = 0;\nlabel_return:\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\n\treturn ret;\n}\n\nstatic int\narenas_decay_ms_ctl_impl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen, bool dirty) {\n\tint ret;\n\n\tif (oldp != NULL && oldlenp != NULL) {\n\t\tsize_t oldval = (dirty ? arena_dirty_decay_ms_default_get() :\n\t\t    arena_muzzy_decay_ms_default_get());\n\t\tREAD(oldval, ssize_t);\n\t}\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(ssize_t)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tif (dirty ?  arena_dirty_decay_ms_default_set(*(ssize_t *)newp)\n\t\t    : arena_muzzy_decay_ms_default_set(*(ssize_t *)newp)) {\n\t\t\tret = EFAULT;\n\t\t\tgoto label_return;\n\t\t}\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\narenas_dirty_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\treturn arenas_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,\n\t    newlen, true);\n}\n\nstatic int\narenas_muzzy_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\treturn arenas_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,\n\t    newlen, false);\n}\n\nCTL_RO_NL_GEN(arenas_quantum, QUANTUM, size_t)\nCTL_RO_NL_GEN(arenas_page, PAGE, size_t)\nCTL_RO_NL_GEN(arenas_tcache_max, tcache_maxclass, size_t)\nCTL_RO_NL_GEN(arenas_nbins, NBINS, unsigned)\nCTL_RO_NL_GEN(arenas_nhbins, nhbins, unsigned)\nCTL_RO_NL_GEN(arenas_bin_i_size, arena_bin_info[mib[2]].reg_size, size_t)\nCTL_RO_NL_GEN(arenas_bin_i_nregs, arena_bin_info[mib[2]].nregs, uint32_t)\nCTL_RO_NL_GEN(arenas_bin_i_slab_size, arena_bin_info[mib[2]].slab_size, size_t)\nstatic const ctl_named_node_t *\narenas_bin_i_index(tsdn_t *tsdn, const size_t *mib, size_t miblen, size_t i) {\n\tif (i > NBINS) {\n\t\treturn NULL;\n\t}\n\treturn super_arenas_bin_i_node;\n}\n\nCTL_RO_NL_GEN(arenas_nlextents, NSIZES - NBINS, unsigned)\nCTL_RO_NL_GEN(arenas_lextent_i_size, sz_index2size(NBINS+(szind_t)mib[2]),\n    size_t)\nstatic const ctl_named_node_t *\narenas_lextent_i_index(tsdn_t *tsdn, const size_t *mib, size_t miblen,\n    size_t i) {\n\tif (i > NSIZES - NBINS) {\n\t\treturn NULL;\n\t}\n\treturn super_arenas_lextent_i_node;\n}\n\nstatic int\narenas_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\textent_hooks_t *extent_hooks;\n\tunsigned arena_ind;\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);\n\n\textent_hooks = (extent_hooks_t *)&extent_hooks_default;\n\tWRITE(extent_hooks, extent_hooks_t *);\n\tif ((arena_ind = ctl_arena_init(tsd_tsdn(tsd), extent_hooks)) ==\n\t    UINT_MAX) {\n\t\tret = EAGAIN;\n\t\tgoto label_return;\n\t}\n\tREAD(arena_ind, unsigned);\n\n\tret = 0;\nlabel_return:\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);\n\treturn ret;\n}\n\n/******************************************************************************/\n\nstatic int\nprof_thread_active_init_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tbool oldval;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(bool)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\toldval = prof_thread_active_init_set(tsd_tsdn(tsd),\n\t\t    *(bool *)newp);\n\t} else {\n\t\toldval = prof_thread_active_init_get(tsd_tsdn(tsd));\n\t}\n\tREAD(oldval, bool);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nprof_active_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tbool oldval;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(bool)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\toldval = prof_active_set(tsd_tsdn(tsd), *(bool *)newp);\n\t} else {\n\t\toldval = prof_active_get(tsd_tsdn(tsd));\n\t}\n\tREAD(oldval, bool);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nprof_dump_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tconst char *filename = NULL;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\tWRITEONLY();\n\tWRITE(filename, const char *);\n\n\tif (prof_mdump(tsd, filename)) {\n\t\tret = EFAULT;\n\t\tgoto label_return;\n\t}\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nprof_gdump_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tbool oldval;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\tif (newp != NULL) {\n\t\tif (newlen != sizeof(bool)) {\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\t\toldval = prof_gdump_set(tsd_tsdn(tsd), *(bool *)newp);\n\t} else {\n\t\toldval = prof_gdump_get(tsd_tsdn(tsd));\n\t}\n\tREAD(oldval, bool);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nstatic int\nprof_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,\n    size_t *oldlenp, void *newp, size_t newlen) {\n\tint ret;\n\tsize_t lg_sample = lg_prof_sample;\n\n\tif (!config_prof) {\n\t\treturn ENOENT;\n\t}\n\n\tWRITEONLY();\n\tWRITE(lg_sample, size_t);\n\tif (lg_sample >= (sizeof(uint64_t) << 3)) {\n\t\tlg_sample = (sizeof(uint64_t) << 3) - 1;\n\t}\n\n\tprof_reset(tsd, lg_sample);\n\n\tret = 0;\nlabel_return:\n\treturn ret;\n}\n\nCTL_RO_NL_CGEN(config_prof, prof_interval, prof_interval, uint64_t)\nCTL_RO_NL_CGEN(config_prof, lg_prof_sample, lg_prof_sample, size_t)\n\n/******************************************************************************/\n\nCTL_RO_CGEN(config_stats, stats_allocated, ctl_stats->allocated, size_t)\nCTL_RO_CGEN(config_stats, stats_active, ctl_stats->active, size_t)\nCTL_RO_CGEN(config_stats, stats_metadata, ctl_stats->metadata, size_t)\nCTL_RO_CGEN(config_stats, stats_resident, ctl_stats->resident, size_t)\nCTL_RO_CGEN(config_stats, stats_mapped, ctl_stats->mapped, size_t)\nCTL_RO_CGEN(config_stats, stats_retained, ctl_stats->retained, size_t)\n\nCTL_RO_CGEN(config_stats, stats_background_thread_num_threads,\n    ctl_stats->background_thread.num_threads, size_t)\nCTL_RO_CGEN(config_stats, stats_background_thread_num_runs,\n    ctl_stats->background_thread.num_runs, uint64_t)\nCTL_RO_CGEN(config_stats, stats_background_thread_run_interval,\n    nstime_ns(&ctl_stats->background_thread.run_interval), uint64_t)\n\nCTL_RO_GEN(stats_arenas_i_dss, arenas_i(mib[2])->dss, const char *)\nCTL_RO_GEN(stats_arenas_i_dirty_decay_ms, arenas_i(mib[2])->dirty_decay_ms,\n    ssize_t)\nCTL_RO_GEN(stats_arenas_i_muzzy_decay_ms, arenas_i(mib[2])->muzzy_decay_ms,\n    ssize_t)\nCTL_RO_GEN(stats_arenas_i_nthreads, arenas_i(mib[2])->nthreads, unsigned)\nCTL_RO_GEN(stats_arenas_i_uptime,\n    nstime_ns(&arenas_i(mib[2])->astats->astats.uptime), uint64_t)\nCTL_RO_GEN(stats_arenas_i_pactive, arenas_i(mib[2])->pactive, size_t)\nCTL_RO_GEN(stats_arenas_i_pdirty, arenas_i(mib[2])->pdirty, size_t)\nCTL_RO_GEN(stats_arenas_i_pmuzzy, arenas_i(mib[2])->pmuzzy, size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_mapped,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.mapped, ATOMIC_RELAXED),\n    size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_retained,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.retained, ATOMIC_RELAXED),\n    size_t)\n\nCTL_RO_CGEN(config_stats, stats_arenas_i_dirty_npurge,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.decay_dirty.npurge),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_dirty_nmadvise,\n    arena_stats_read_u64(\n    &arenas_i(mib[2])->astats->astats.decay_dirty.nmadvise), uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_dirty_purged,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.decay_dirty.purged),\n    uint64_t)\n\nCTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_npurge,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.decay_muzzy.npurge),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_nmadvise,\n    arena_stats_read_u64(\n    &arenas_i(mib[2])->astats->astats.decay_muzzy.nmadvise), uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_purged,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.decay_muzzy.purged),\n    uint64_t)\n\nCTL_RO_CGEN(config_stats, stats_arenas_i_base,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.base, ATOMIC_RELAXED),\n    size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_internal,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.internal, ATOMIC_RELAXED),\n    size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_tcache_bytes,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.tcache_bytes,\n    ATOMIC_RELAXED), size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_resident,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.resident, ATOMIC_RELAXED),\n    size_t)\n\nCTL_RO_CGEN(config_stats, stats_arenas_i_small_allocated,\n    arenas_i(mib[2])->astats->allocated_small, size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_small_nmalloc,\n    arenas_i(mib[2])->astats->nmalloc_small, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_small_ndalloc,\n    arenas_i(mib[2])->astats->ndalloc_small, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_small_nrequests,\n    arenas_i(mib[2])->astats->nrequests_small, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_large_allocated,\n    atomic_load_zu(&arenas_i(mib[2])->astats->astats.allocated_large,\n    ATOMIC_RELAXED), size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_large_nmalloc,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.nmalloc_large),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_large_ndalloc,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.ndalloc_large),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_large_nrequests,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->astats.nmalloc_large),\n    uint64_t) /* Intentional. */\n\n/* Lock profiling related APIs below. */\n#define RO_MUTEX_CTL_GEN(n, l)\t\t\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_num_ops,\t\t\t\t\\\n    l.n_lock_ops, uint64_t)\t\t\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_num_wait,\t\t\t\t\\\n    l.n_wait_times, uint64_t)\t\t\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_num_spin_acq,\t\t\t\\\n    l.n_spin_acquired, uint64_t)\t\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_num_owner_switch,\t\t\t\\\n    l.n_owner_switches, uint64_t) \t\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_total_wait_time,\t\t\t\\\n    nstime_ns(&l.tot_wait_time), uint64_t)\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_max_wait_time,\t\t\t\\\n    nstime_ns(&l.max_wait_time), uint64_t)\t\t\t\t\\\nCTL_RO_CGEN(config_stats, stats_##n##_max_num_thds,\t\t\t\\\n    l.max_n_thds, uint32_t)\n\n/* Global mutexes. */\n#define OP(mtx)\t\t\t\t\t\t\t\t\\\n    RO_MUTEX_CTL_GEN(mutexes_##mtx,\t\t\t\t\t\\\n        ctl_stats->mutex_prof_data[global_prof_mutex_##mtx])\nMUTEX_PROF_GLOBAL_MUTEXES\n#undef OP\n\n/* Per arena mutexes */\n#define OP(mtx) RO_MUTEX_CTL_GEN(arenas_i_mutexes_##mtx,\t\t\\\n    arenas_i(mib[2])->astats->astats.mutex_prof_data[arena_prof_mutex_##mtx])\nMUTEX_PROF_ARENA_MUTEXES\n#undef OP\n\n/* tcache bin mutex */\nRO_MUTEX_CTL_GEN(arenas_i_bins_j_mutex,\n    arenas_i(mib[2])->astats->bstats[mib[4]].mutex_data)\n#undef RO_MUTEX_CTL_GEN\n\n/* Resets all mutex stats, including global, arena and bin mutexes. */\nstatic int\nstats_mutexes_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,\n    void *oldp, size_t *oldlenp, void *newp, size_t newlen) {\n\tif (!config_stats) {\n\t\treturn ENOENT;\n\t}\n\n\ttsdn_t *tsdn = tsd_tsdn(tsd);\n\n#define MUTEX_PROF_RESET(mtx)\t\t\t\t\t\t\\\n    malloc_mutex_lock(tsdn, &mtx);\t\t\t\t\t\\\n    malloc_mutex_prof_data_reset(tsdn, &mtx);\t\t\t\t\\\n    malloc_mutex_unlock(tsdn, &mtx);\n\n\t/* Global mutexes: ctl and prof. */\n\tMUTEX_PROF_RESET(ctl_mtx);\n\tif (have_background_thread) {\n\t\tMUTEX_PROF_RESET(background_thread_lock);\n\t}\n\tif (config_prof && opt_prof) {\n\t\tMUTEX_PROF_RESET(bt2gctx_mtx);\n\t}\n\n\n\t/* Per arena mutexes. */\n\tunsigned n = narenas_total_get();\n\n\tfor (unsigned i = 0; i < n; i++) {\n\t\tarena_t *arena = arena_get(tsdn, i, false);\n\t\tif (!arena) {\n\t\t\tcontinue;\n\t\t}\n\t\tMUTEX_PROF_RESET(arena->large_mtx);\n\t\tMUTEX_PROF_RESET(arena->extent_avail_mtx);\n\t\tMUTEX_PROF_RESET(arena->extents_dirty.mtx);\n\t\tMUTEX_PROF_RESET(arena->extents_muzzy.mtx);\n\t\tMUTEX_PROF_RESET(arena->extents_retained.mtx);\n\t\tMUTEX_PROF_RESET(arena->decay_dirty.mtx);\n\t\tMUTEX_PROF_RESET(arena->decay_muzzy.mtx);\n\t\tMUTEX_PROF_RESET(arena->tcache_ql_mtx);\n\t\tMUTEX_PROF_RESET(arena->base->mtx);\n\n\t\tfor (szind_t i = 0; i < NBINS; i++) {\n\t\t\tarena_bin_t *bin = &arena->bins[i];\n\t\t\tMUTEX_PROF_RESET(bin->lock);\n\t\t}\n\t}\n#undef MUTEX_PROF_RESET\n\treturn 0;\n}\n\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nmalloc,\n    arenas_i(mib[2])->astats->bstats[mib[4]].nmalloc, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_ndalloc,\n    arenas_i(mib[2])->astats->bstats[mib[4]].ndalloc, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nrequests,\n    arenas_i(mib[2])->astats->bstats[mib[4]].nrequests, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_curregs,\n    arenas_i(mib[2])->astats->bstats[mib[4]].curregs, size_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nfills,\n    arenas_i(mib[2])->astats->bstats[mib[4]].nfills, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nflushes,\n    arenas_i(mib[2])->astats->bstats[mib[4]].nflushes, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nslabs,\n    arenas_i(mib[2])->astats->bstats[mib[4]].nslabs, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nreslabs,\n    arenas_i(mib[2])->astats->bstats[mib[4]].reslabs, uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_curslabs,\n    arenas_i(mib[2])->astats->bstats[mib[4]].curslabs, size_t)\n\nstatic const ctl_named_node_t *\nstats_arenas_i_bins_j_index(tsdn_t *tsdn, const size_t *mib, size_t miblen,\n    size_t j) {\n\tif (j > NBINS) {\n\t\treturn NULL;\n\t}\n\treturn super_stats_arenas_i_bins_j_node;\n}\n\nCTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_nmalloc,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->lstats[mib[4]].nmalloc),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_ndalloc,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->lstats[mib[4]].ndalloc),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_nrequests,\n    arena_stats_read_u64(&arenas_i(mib[2])->astats->lstats[mib[4]].nrequests),\n    uint64_t)\nCTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_curlextents,\n    arenas_i(mib[2])->astats->lstats[mib[4]].curlextents, size_t)\n\nstatic const ctl_named_node_t *\nstats_arenas_i_lextents_j_index(tsdn_t *tsdn, const size_t *mib, size_t miblen,\n    size_t j) {\n\tif (j > NSIZES - NBINS) {\n\t\treturn NULL;\n\t}\n\treturn super_stats_arenas_i_lextents_j_node;\n}\n\nstatic const ctl_named_node_t *\nstats_arenas_i_index(tsdn_t *tsdn, const size_t *mib, size_t miblen, size_t i) {\n\tconst ctl_named_node_t *ret;\n\tsize_t a;\n\n\tmalloc_mutex_lock(tsdn, &ctl_mtx);\n\ta = arenas_i2a_impl(i, true, true);\n\tif (a == UINT_MAX || !ctl_arenas->arenas[a]->initialized) {\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tret = super_stats_arenas_i_node;\nlabel_return:\n\tmalloc_mutex_unlock(tsdn, &ctl_mtx);\n\treturn ret;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.7.0-vflcwcmehmn6mxtprs2nfncaerzke2qq/spack-src/jemalloc/include/jemalloc/internal/jemalloc_internal_defs.h.in": "#ifndef JEMALLOC_INTERNAL_DEFS_H_\n#define JEMALLOC_INTERNAL_DEFS_H_\n/*\n * If JEMALLOC_PREFIX is defined via --with-jemalloc-prefix, it will cause all\n * public APIs to be prefixed.  This makes it possible, with some care, to use\n * multiple allocators simultaneously.\n */\n#undef JEMALLOC_PREFIX\n#undef JEMALLOC_CPREFIX\n\n/*\n * Define overrides for non-standard allocator-related functions if they are\n * present on the system.\n */\n#undef JEMALLOC_OVERRIDE___LIBC_CALLOC\n#undef JEMALLOC_OVERRIDE___LIBC_FREE\n#undef JEMALLOC_OVERRIDE___LIBC_MALLOC\n#undef JEMALLOC_OVERRIDE___LIBC_MEMALIGN\n#undef JEMALLOC_OVERRIDE___LIBC_REALLOC\n#undef JEMALLOC_OVERRIDE___LIBC_VALLOC\n#undef JEMALLOC_OVERRIDE___POSIX_MEMALIGN\n\n/*\n * JEMALLOC_PRIVATE_NAMESPACE is used as a prefix for all library-private APIs.\n * For shared libraries, symbol visibility mechanisms prevent these symbols\n * from being exported, but for static libraries, naming collisions are a real\n * possibility.\n */\n#undef JEMALLOC_PRIVATE_NAMESPACE\n\n/*\n * Hyper-threaded CPUs may need a special instruction inside spin loops in\n * order to yield to another virtual CPU.\n */\n#undef CPU_SPINWAIT\n\n/*\n * Number of significant bits in virtual addresses.  This may be less than the\n * total number of bits in a pointer, e.g. on x64, for which the uppermost 16\n * bits are the same as bit 47.\n */\n#undef LG_VADDR\n\n/* Defined if C11 atomics are available. */\n#undef JEMALLOC_C11_ATOMICS\n\n/* Defined if GCC __atomic atomics are available. */\n#undef JEMALLOC_GCC_ATOMIC_ATOMICS\n\n/* Defined if GCC __sync atomics are available. */\n#undef JEMALLOC_GCC_SYNC_ATOMICS\n\n/*\n * Defined if __sync_add_and_fetch(uint32_t *, uint32_t) and\n * __sync_sub_and_fetch(uint32_t *, uint32_t) are available, despite\n * __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 not being defined (which means the\n * functions are defined in libgcc instead of being inlines).\n */\n#undef JE_FORCE_SYNC_COMPARE_AND_SWAP_4\n\n/*\n * Defined if __sync_add_and_fetch(uint64_t *, uint64_t) and\n * __sync_sub_and_fetch(uint64_t *, uint64_t) are available, despite\n * __GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 not being defined (which means the\n * functions are defined in libgcc instead of being inlines).\n */\n#undef JE_FORCE_SYNC_COMPARE_AND_SWAP_8\n\n/*\n * Defined if __builtin_clz() and __builtin_clzl() are available.\n */\n#undef JEMALLOC_HAVE_BUILTIN_CLZ\n\n/*\n * Defined if os_unfair_lock_*() functions are available, as provided by Darwin.\n */\n#undef JEMALLOC_OS_UNFAIR_LOCK\n\n/*\n * Defined if OSSpin*() functions are available, as provided by Darwin, and\n * documented in the spinlock(3) manual page.\n */\n#undef JEMALLOC_OSSPIN\n\n/* Defined if syscall(2) is usable. */\n#undef JEMALLOC_USE_SYSCALL\n\n/*\n * Defined if secure_getenv(3) is available.\n */\n#undef JEMALLOC_HAVE_SECURE_GETENV\n\n/*\n * Defined if issetugid(2) is available.\n */\n#undef JEMALLOC_HAVE_ISSETUGID\n\n/* Defined if pthread_atfork(3) is available. */\n#undef JEMALLOC_HAVE_PTHREAD_ATFORK\n\n/*\n * Defined if clock_gettime(CLOCK_MONOTONIC_COARSE, ...) is available.\n */\n#undef JEMALLOC_HAVE_CLOCK_MONOTONIC_COARSE\n\n/*\n * Defined if clock_gettime(CLOCK_MONOTONIC, ...) is available.\n */\n#undef JEMALLOC_HAVE_CLOCK_MONOTONIC\n\n/*\n * Defined if mach_absolute_time() is available.\n */\n#undef JEMALLOC_HAVE_MACH_ABSOLUTE_TIME\n\n/*\n * Defined if _malloc_thread_cleanup() exists.  At least in the case of\n * FreeBSD, pthread_key_create() allocates, which if used during malloc\n * bootstrapping will cause recursion into the pthreads library.  Therefore, if\n * _malloc_thread_cleanup() exists, use it as the basis for thread cleanup in\n * malloc_tsd.\n */\n#undef JEMALLOC_MALLOC_THREAD_CLEANUP\n\n/*\n * Defined if threaded initialization is known to be safe on this platform.\n * Among other things, it must be possible to initialize a mutex without\n * triggering allocation in order for threaded allocation to be safe.\n */\n#undef JEMALLOC_THREADED_INIT\n\n/*\n * Defined if the pthreads implementation defines\n * _pthread_mutex_init_calloc_cb(), in which case the function is used in order\n * to avoid recursive allocation during mutex initialization.\n */\n#undef JEMALLOC_MUTEX_INIT_CB\n\n/* Non-empty if the tls_model attribute is supported. */\n#undef JEMALLOC_TLS_MODEL\n\n/*\n * JEMALLOC_DEBUG enables assertions and other sanity checks, and disables\n * inline functions.\n */\n#undef JEMALLOC_DEBUG\n\n/* JEMALLOC_STATS enables statistics calculation. */\n#undef JEMALLOC_STATS\n\n/* JEMALLOC_PROF enables allocation profiling. */\n#undef JEMALLOC_PROF\n\n/* Use libunwind for profile backtracing if defined. */\n#undef JEMALLOC_PROF_LIBUNWIND\n\n/* Use libgcc for profile backtracing if defined. */\n#undef JEMALLOC_PROF_LIBGCC\n\n/* Use gcc intrinsics for profile backtracing if defined. */\n#undef JEMALLOC_PROF_GCC\n\n/*\n * JEMALLOC_DSS enables use of sbrk(2) to allocate extents from the data storage\n * segment (DSS).\n */\n#undef JEMALLOC_DSS\n\n/* Support memory filling (junk/zero). */\n#undef JEMALLOC_FILL\n\n/* Support utrace(2)-based tracing. */\n#undef JEMALLOC_UTRACE\n\n/* Support optional abort() on OOM. */\n#undef JEMALLOC_XMALLOC\n\n/* Support lazy locking (avoid locking unless a second thread is launched). */\n#undef JEMALLOC_LAZY_LOCK\n\n/*\n * Minimum allocation alignment is 2^LG_QUANTUM bytes (ignoring tiny size\n * classes).\n */\n#undef LG_QUANTUM\n\n/* One page is 2^LG_PAGE bytes. */\n#undef LG_PAGE\n\n/*\n * One huge page is 2^LG_HUGEPAGE bytes.  Note that this is defined even if the\n * system does not explicitly support huge pages; system calls that require\n * explicit huge page support are separately configured.\n */\n#undef LG_HUGEPAGE\n\n/*\n * If defined, adjacent virtual memory mappings with identical attributes\n * automatically coalesce, and they fragment when changes are made to subranges.\n * This is the normal order of things for mmap()/munmap(), but on Windows\n * VirtualAlloc()/VirtualFree() operations must be precisely matched, i.e.\n * mappings do *not* coalesce/fragment.\n */\n#undef JEMALLOC_MAPS_COALESCE\n\n/*\n * If defined, retain memory for later reuse by default rather than using e.g.\n * munmap() to unmap freed extents.  This is enabled on 64-bit Linux because\n * common sequences of mmap()/munmap() calls will cause virtual memory map\n * holes.\n */\n#undef JEMALLOC_RETAIN\n\n/* TLS is used to map arenas and magazine caches to threads. */\n#undef JEMALLOC_TLS\n\n/*\n * Used to mark unreachable code to quiet \"end of non-void\" compiler warnings.\n * Don't use this directly; instead use unreachable() from util.h\n */\n#undef JEMALLOC_INTERNAL_UNREACHABLE\n\n/*\n * ffs*() functions to use for bitmapping.  Don't use these directly; instead,\n * use ffs_*() from util.h.\n */\n#undef JEMALLOC_INTERNAL_FFSLL\n#undef JEMALLOC_INTERNAL_FFSL\n#undef JEMALLOC_INTERNAL_FFS\n\n/*\n * If defined, explicitly attempt to more uniformly distribute large allocation\n * pointer alignments across all cache indices.\n */\n#undef JEMALLOC_CACHE_OBLIVIOUS\n\n/*\n * Darwin (OS X) uses zones to work around Mach-O symbol override shortcomings.\n */\n#undef JEMALLOC_ZONE\n\n/*\n * Methods for determining whether the OS overcommits.\n * JEMALLOC_PROC_SYS_VM_OVERCOMMIT_MEMORY: Linux's\n *                                         /proc/sys/vm.overcommit_memory file.\n * JEMALLOC_SYSCTL_VM_OVERCOMMIT: FreeBSD's vm.overcommit sysctl.\n */\n#undef JEMALLOC_SYSCTL_VM_OVERCOMMIT\n#undef JEMALLOC_PROC_SYS_VM_OVERCOMMIT_MEMORY\n\n/* Defined if madvise(2) is available. */\n#undef JEMALLOC_HAVE_MADVISE\n\n/*\n * Methods for purging unused pages differ between operating systems.\n *\n *   madvise(..., MADV_FREE) : This marks pages as being unused, such that they\n *                             will be discarded rather than swapped out.\n *   madvise(..., MADV_DONTNEED) : If JEMALLOC_PURGE_MADVISE_DONTNEED_ZEROS is\n *                                 defined, this immediately discards pages,\n *                                 such that new pages will be demand-zeroed if\n *                                 the address region is later touched;\n *                                 otherwise this behaves similarly to\n *                                 MADV_FREE, though typically with higher\n *                                 system overhead.\n */\n#undef JEMALLOC_PURGE_MADVISE_FREE\n#undef JEMALLOC_PURGE_MADVISE_DONTNEED\n#undef JEMALLOC_PURGE_MADVISE_DONTNEED_ZEROS\n\n/*\n * Defined if transparent huge pages (THPs) are supported via the\n * MADV_[NO]HUGEPAGE arguments to madvise(2), and THP support is enabled.\n */\n#undef JEMALLOC_THP\n\n/* Define if operating system has alloca.h header. */\n#undef JEMALLOC_HAS_ALLOCA_H\n\n/* C99 restrict keyword supported. */\n#undef JEMALLOC_HAS_RESTRICT\n\n/* For use by hash code. */\n#undef JEMALLOC_BIG_ENDIAN\n\n/* sizeof(int) == 2^LG_SIZEOF_INT. */\n#undef LG_SIZEOF_INT\n\n/* sizeof(long) == 2^LG_SIZEOF_LONG. */\n#undef LG_SIZEOF_LONG\n\n/* sizeof(long long) == 2^LG_SIZEOF_LONG_LONG. */\n#undef LG_SIZEOF_LONG_LONG\n\n/* sizeof(intmax_t) == 2^LG_SIZEOF_INTMAX_T. */\n#undef LG_SIZEOF_INTMAX_T\n\n/* glibc malloc hooks (__malloc_hook, __realloc_hook, __free_hook). */\n#undef JEMALLOC_GLIBC_MALLOC_HOOK\n\n/* glibc memalign hook. */\n#undef JEMALLOC_GLIBC_MEMALIGN_HOOK\n\n/* pthread support */\n#undef JEMALLOC_HAVE_PTHREAD\n\n/* dlsym() support */\n#undef JEMALLOC_HAVE_DLSYM\n\n/* Adaptive mutex support in pthreads. */\n#undef JEMALLOC_HAVE_PTHREAD_MUTEX_ADAPTIVE_NP\n\n/* GNU specific sched_getcpu support */\n#undef JEMALLOC_HAVE_SCHED_GETCPU\n\n/* GNU specific sched_setaffinity support */\n#undef JEMALLOC_HAVE_SCHED_SETAFFINITY\n\n/*\n * If defined, all the features necessary for background threads are present.\n */\n#undef JEMALLOC_BACKGROUND_THREAD\n\n/*\n * If defined, jemalloc symbols are not exported (doesn't work when\n * JEMALLOC_PREFIX is not defined).\n */\n#undef JEMALLOC_EXPORT\n\n/* config.malloc_conf options string. */\n#undef JEMALLOC_CONFIG_MALLOC_CONF\n\n/* If defined, jemalloc takes the malloc/free/etc. symbol names. */\n#undef JEMALLOC_IS_MALLOC\n\n#endif /* JEMALLOC_INTERNAL_DEFS_H_ */\n"
    },
    "skipped": [],
    "total_files": 445
}