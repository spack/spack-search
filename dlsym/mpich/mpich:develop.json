{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/mpl/src/gpu/mpl_gpu_cuda.c": "/*\n *  Copyright (C) by Argonne National Laboratory.\n *      See COPYRIGHT in top-level directory.\n */\n\n#include <dlfcn.h>\n#include \"mpl.h\"\n#include <assert.h>\n\n#define CUDA_ERR_CHECK(ret) if (unlikely((ret) != cudaSuccess)) goto fn_fail\n#define CU_ERR_CHECK(ret) if (unlikely((ret) != CUDA_SUCCESS)) goto fn_fail\n\ntypedef struct gpu_free_hook {\n    void (*free_hook) (void *dptr);\n    struct gpu_free_hook *next;\n} gpu_free_hook_s;\n\nstatic gpu_free_hook_s *free_hook_chain = NULL;\n\nstatic CUresult CUDAAPI(*sys_cuMemFree) (CUdeviceptr dptr);\nstatic cudaError_t CUDARTAPI(*sys_cudaFree) (void *dptr);\n\nstatic int gpu_mem_hook_init();\n\nint MPL_gpu_query_pointer_attr(const void *ptr, MPL_pointer_attr_t * attr)\n{\n    cudaError_t ret;\n    struct cudaPointerAttributes ptr_attr;\n    ret = cudaPointerGetAttributes(&ptr_attr, ptr);\n    if (ret == cudaSuccess) {\n        switch (ptr_attr.type) {\n            case cudaMemoryTypeUnregistered:\n                attr->type = MPL_GPU_POINTER_UNREGISTERED_HOST;\n                attr->device = ptr_attr.device;\n                break;\n            case cudaMemoryTypeHost:\n                attr->type = MPL_GPU_POINTER_REGISTERED_HOST;\n                attr->device = ptr_attr.device;\n                break;\n            case cudaMemoryTypeDevice:\n                attr->type = MPL_GPU_POINTER_DEV;\n                attr->device = ptr_attr.device;\n                break;\n            case cudaMemoryTypeManaged:\n                attr->type = MPL_GPU_POINTER_MANAGED;\n                attr->device = ptr_attr.device;\n                break;\n        }\n    } else if (ret == cudaErrorInvalidValue) {\n        attr->type = MPL_GPU_POINTER_UNREGISTERED_HOST;\n        attr->device = -1;\n    } else {\n        goto fn_fail;\n    }\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_ipc_handle_create(const void *ptr, MPL_gpu_ipc_mem_handle_t * ipc_handle)\n{\n    cudaError_t ret;\n\n    ret = cudaIpcGetMemHandle(ipc_handle, (void *) ptr);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_ipc_handle_map(MPL_gpu_ipc_mem_handle_t ipc_handle, MPL_gpu_device_handle_t dev_handle,\n                           void **ptr)\n{\n    cudaError_t ret;\n    int prev_devid;\n\n    cudaGetDevice(&prev_devid);\n    cudaSetDevice(dev_handle);\n    ret = cudaIpcOpenMemHandle(ptr, ipc_handle, cudaIpcMemLazyEnablePeerAccess);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    cudaSetDevice(prev_devid);\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_ipc_handle_unmap(void *ptr)\n{\n    cudaError_t ret;\n    ret = cudaIpcCloseMemHandle(ptr);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_malloc_host(void **ptr, size_t size)\n{\n    cudaError_t ret;\n    ret = cudaMallocHost(ptr, size);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_free_host(void *ptr)\n{\n    cudaError_t ret;\n    ret = cudaFreeHost(ptr);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_register_host(const void *ptr, size_t size)\n{\n    cudaError_t ret;\n    ret = cudaHostRegister((void *) ptr, size, cudaHostRegisterDefault);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_unregister_host(const void *ptr)\n{\n    cudaError_t ret;\n    ret = cudaHostUnregister((void *) ptr);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_malloc(void **ptr, size_t size, MPL_gpu_device_handle_t h_device)\n{\n    int mpl_errno = MPL_SUCCESS;\n    int prev_devid;\n    cudaError_t ret;\n    cudaGetDevice(&prev_devid);\n    cudaSetDevice(h_device);\n    ret = cudaMalloc(ptr, size);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    cudaSetDevice(prev_devid);\n    return mpl_errno;\n  fn_fail:\n    mpl_errno = MPL_ERR_GPU_INTERNAL;\n    goto fn_exit;\n}\n\nint MPL_gpu_free(void *ptr)\n{\n    cudaError_t ret;\n    ret = cudaFree(ptr);\n    CUDA_ERR_CHECK(ret);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_init(int *device_count, int *max_dev_id_ptr)\n{\n    int count, max_dev_id = -1;\n    cudaError_t ret = cudaGetDeviceCount(&count);\n    CUDA_ERR_CHECK(ret);\n\n    char *visible_devices = getenv(\"CUDA_VISIBLE_DEVICES\");\n    if (visible_devices) {\n        uintptr_t len = strlen(visible_devices);\n        char *devices = MPL_malloc(len + 1, MPL_MEM_OTHER);\n        char *free_ptr = devices;\n        memcpy(devices, visible_devices, len + 1);\n        for (int i = 0; i < count; i++) {\n            int global_dev_id;\n            char *tmp = strtok(devices, \",\");\n            assert(tmp);\n            global_dev_id = atoi(tmp);\n            if (global_dev_id > max_dev_id)\n                max_dev_id = global_dev_id;\n            devices = NULL;\n        }\n        MPL_free(free_ptr);\n    } else {\n        max_dev_id = count - 1;\n    }\n\n    *max_dev_id_ptr = max_dev_id;\n    *device_count = count;\n\n    gpu_mem_hook_init();\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_finalize()\n{\n    gpu_free_hook_s *prev;\n    while (free_hook_chain) {\n        prev = free_hook_chain;\n        free_hook_chain = free_hook_chain->next;\n        MPL_free(prev);\n    }\n    return MPL_SUCCESS;\n}\n\nint MPL_gpu_get_dev_id(MPL_gpu_device_handle_t dev_handle, int *dev_id)\n{\n    *dev_id = dev_handle;\n    return MPL_SUCCESS;\n}\n\nint MPL_gpu_get_dev_handle(int dev_id, MPL_gpu_device_handle_t * dev_handle)\n{\n    *dev_handle = dev_id;\n    return MPL_SUCCESS;\n}\n\nint MPL_gpu_get_global_dev_ids(int *global_ids, int count)\n{\n    char *visible_devices = getenv(\"CUDA_VISIBLE_DEVICES\");\n\n    if (visible_devices) {\n        uintptr_t len = strlen(visible_devices);\n        char *devices = MPL_malloc(len + 1, MPL_MEM_OTHER);\n        char *free_ptr = devices;\n        memcpy(devices, visible_devices, len + 1);\n        for (int i = 0; i < count; i++) {\n            char *tmp = strtok(devices, \",\");\n            assert(tmp);\n            global_ids[i] = atoi(tmp);\n            devices = NULL;\n        }\n        MPL_free(free_ptr);\n    } else {\n        for (int i = 0; i < count; i++) {\n            global_ids[i] = i;\n        }\n    }\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nint MPL_gpu_get_buffer_bounds(const void *ptr, void **pbase, uintptr_t * len)\n{\n    CUresult curet;\n\n    curet = cuMemGetAddressRange((CUdeviceptr *) pbase, (size_t *) len, (CUdeviceptr) ptr);\n    CU_ERR_CHECK(curet);\n\n  fn_exit:\n    return MPL_SUCCESS;\n  fn_fail:\n    return MPL_ERR_GPU_INTERNAL;\n}\n\nstatic void gpu_free_hooks_cb(void *dptr)\n{\n    gpu_free_hook_s *current = free_hook_chain;\n    if (dptr != NULL) {\n        /* we call gpu hook only when dptr != NULL */\n        while (current) {\n            current->free_hook(dptr);\n            current = current->next;\n        }\n    }\n    return;\n}\n\nstatic int gpu_mem_hook_init()\n{\n    void *libcuda_handle;\n    void *libcudart_handle;\n\n    libcuda_handle = dlopen(\"libcuda.so\", RTLD_LAZY | RTLD_GLOBAL);\n    assert(libcuda_handle);\n    libcudart_handle = dlopen(\"libcudart.so\", RTLD_LAZY | RTLD_GLOBAL);\n    assert(libcudart_handle);\n\n    sys_cuMemFree = (void *) dlsym(libcuda_handle, \"cuMemFree\");\n    assert(sys_cuMemFree);\n    sys_cudaFree = (void *) dlsym(libcudart_handle, \"cudaFree\");\n    assert(sys_cudaFree);\n    return MPL_SUCCESS;\n}\n\nint MPL_gpu_free_hook_register(void (*free_hook) (void *dptr))\n{\n    gpu_free_hook_s *hook_obj = MPL_malloc(sizeof(gpu_free_hook_s), MPL_MEM_OTHER);\n    assert(hook_obj);\n    hook_obj->free_hook = free_hook;\n    hook_obj->next = NULL;\n    if (!free_hook_chain)\n        free_hook_chain = hook_obj;\n    else {\n        hook_obj->next = free_hook_chain;\n        free_hook_chain = hook_obj;\n    }\n\n    return MPL_SUCCESS;\n}\n\nCUresult CUDAAPI cuMemFree(CUdeviceptr dptr)\n{\n    CUresult result;\n    gpu_free_hooks_cb((void *) dptr);\n    result = sys_cuMemFree(dptr);\n    return (result);\n}\n\ncudaError_t CUDARTAPI cudaFree(void *dptr)\n{\n    cudaError_t result;\n    gpu_free_hooks_cb(dptr);\n    result = sys_cudaFree(dptr);\n    return result;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/libfabric/src/fabric.c": "/*\n * Copyright (c) 2004, 2005 Topspin Communications.  All rights reserved.\n * Copyright (c) 2006-2016 Cisco Systems, Inc.  All rights reserved.\n * Copyright (c) 2013-2017 Intel Corp., Inc.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include \"config.h\"\n\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <dirent.h>\n\n#include <rdma/fi_errno.h>\n#include \"ofi_util.h\"\n#include \"ofi.h\"\n#include \"shared/ofi_str.h\"\n#include \"ofi_prov.h\"\n#include \"ofi_perf.h\"\n\n#ifdef HAVE_LIBDL\n#include <dlfcn.h>\n#endif\n\nstruct ofi_prov {\n\tstruct ofi_prov\t\t*next;\n\tchar\t\t\t*prov_name;\n\tstruct fi_provider\t*provider;\n\tvoid\t\t\t*dlhandle;\n\tbool\t\t\thidden;\n};\n\nstatic struct ofi_prov *prov_head, *prov_tail;\nint ofi_init = 0;\nextern struct ofi_common_locks common_locks;\n\nstatic struct fi_filter prov_filter;\n\nstatic int ofi_find_name(char **names, const char *name)\n{\n\tint i;\n\n\tfor (i = 0; names[i]; i++) {\n\t\tif (!strcasecmp(name, names[i]))\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\n\n/* matches if names[i] == \"xxx;yyy\" and name == \"xxx\" */\nstatic int ofi_find_layered_name(char **names, const char *name)\n{\n\tint i, len;\n\n\tlen = strlen(name);\n\tfor (i = 0; names[i]; i++) {\n\t\tif (!strncasecmp(name, names[i], len) && names[i][len] == ';' )\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\n\n/* matches if names[i] == \"xxx\" and name == \"xxx;yyy\" */\nstatic int ofi_find_core_name(char **names, const char *name)\n{\n\tint i, len;\n\n\tfor (i = 0; names[i]; i++) {\n\t\tlen = strlen(names[i]);\n\t\tif (!strncasecmp(name, names[i], len) && name[len] == ';' )\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\n\nstatic void ofi_closest_prov_names(char *prov_name, char* miss_prov_name, int n)\n{\n\tif (strncasecmp( prov_name, miss_prov_name, n ) == 0 ) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Instead misspelled provider: %s, you may want: %s?\\n\",\n\t\t\tmiss_prov_name, prov_name);\n\t}\n}\n\nstatic void ofi_suggest_prov_names(char *name_to_match)\n{\n\tstruct ofi_prov *prov;\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif (strlen(prov->prov_name) != strlen(name_to_match)\n\t\t    && !strncasecmp(prov->prov_name, name_to_match,\n\t\t\t\t    strlen(name_to_match))) {\n\t\t\tif (strlen(name_to_match) > 5)\n\t\t\t\tofi_closest_prov_names(prov->prov_name,\n\t\t\t\t\t\t       name_to_match, 5);\n\t\t\telse\n\t\t\t\tofi_closest_prov_names(prov->prov_name,\n\t\t\t\t\t\t       name_to_match, 2);\n\t\t}\n\t}\n}\n\nstatic enum ofi_prov_type ofi_prov_type(const struct fi_provider *provider)\n{\n\tconst struct fi_prov_context *ctx;\n\tctx = (const struct fi_prov_context *) &provider->context;\n\treturn ctx->type;\n}\n\nstatic int ofi_is_util_prov(const struct fi_provider *provider)\n{\n\treturn ofi_prov_type(provider) == OFI_PROV_UTIL;\n}\n\nstatic int ofi_is_core_prov(const struct fi_provider *provider)\n{\n\treturn ofi_prov_type(provider) == OFI_PROV_CORE;\n}\n\nstatic int ofi_is_hook_prov(const struct fi_provider *provider)\n{\n\treturn ofi_prov_type(provider) == OFI_PROV_HOOK;\n}\n\nint ofi_apply_filter(struct fi_filter *filter, const char *name)\n{\n\tif (!filter->names)\n\t\treturn 0;\n\n\tif (ofi_find_name(filter->names, name) >= 0)\n\t\treturn filter->negated ? 1 : 0;\n\n\treturn filter->negated ? 0 : 1;\n}\n\n/*\n * The provider init filter is used to filter out unnecessary core providers\n * at the initialization time. Utility providers are not concerned.\n *\n * Special handling is needed for layered provider names:\n *\n * If the filter is not negated, a name \"xxx;yyy\" in the filter should match\n * input \"xxx\" to ensure that the core provider \"xxx\" is included.\n *\n * If the filter is negated, a name \"xxx;yyy\" in the filter should not match\n * input \"xxx\" otherwise the core provider \"xxx\" may be incorrectly filtered\n * out.\n */\nint ofi_apply_prov_init_filter(struct fi_filter *filter, const char *name)\n{\n\tif (!filter->names)\n\t\treturn 0;\n\n\tif (ofi_find_name(filter->names, name) >= 0)\n\t\treturn filter->negated ? 1 : 0;\n\n\tif (filter->negated)\n\t\treturn 0;\n\n\tif (ofi_find_layered_name(filter->names, name) >= 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * The provider post filter is used to remove unwanted entries from the fi_info\n * list before returning from fi_getinfo().\n *\n * Layered provider names are handled in the same way as non-layered provider\n * names -- requiring full match.\n *\n * In addition, a name \"xxx\" in the filter should be able to match an input\n * \"xxx;yyy\" to allow extra layering on top of what is requested by the user.\n */\nint ofi_apply_prov_post_filter(struct fi_filter *filter, const char *name)\n{\n\tif (!filter->names)\n\t\treturn 0;\n\n\tif (ofi_find_name(filter->names, name) >= 0 ||\n\t    ofi_find_core_name(filter->names, name) >= 0)\n\t\treturn filter->negated ? 1 : 0;\n\n\treturn filter->negated ? 0 : 1;\n}\n\nstatic int ofi_getinfo_filter(const struct fi_provider *provider)\n{\n\t/* Positive filters only apply to core providers.  They must be\n\t * explicitly enabled by the filter.  Other providers (i.e. utility)\n\t * are automatically enabled in this case, so that they can work\n\t * over any enabled core filter.  Negative filters may be used\n\t * to disable any provider.\n\t */\n\tif (!prov_filter.negated && !ofi_is_core_prov(provider))\n\t\treturn 0;\n\n\treturn ofi_apply_prov_init_filter(&prov_filter, provider->name);\n}\n\nstatic void ofi_filter_info(struct fi_info **info)\n{\n\tstruct fi_info *cur, *prev, *tmp;\n\n\tif (!prov_filter.names)\n\t\treturn;\n\n\tprev = NULL;\n\tcur = *info;\n\twhile (cur) {\n\t\tassert(cur->fabric_attr && cur->fabric_attr->prov_name);\n\n\t\tif (ofi_apply_prov_post_filter(&prov_filter, cur->fabric_attr->prov_name)) {\n\t\t\ttmp = cur;\n\t\t\tcur = cur->next;\n\t\t\tif (prev)\n\t\t\t\tprev->next = cur;\n\t\t\telse\n\t\t\t\t*info = cur;\n\t\t\ttmp->next = NULL;\n\t\t\tfi_freeinfo(tmp);\n\t\t} else {\n\t\t\tprev = cur;\n\t\t\tcur = cur->next;\n\t\t}\n\t}\n}\n\nstatic struct ofi_prov *ofi_getprov(const char *prov_name, size_t len)\n{\n\tstruct ofi_prov *prov;\n\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif ((strlen(prov->prov_name) == len) &&\n\t\t    !strncmp(prov->prov_name, prov_name, len))\n\t\t\treturn prov;\n\t}\n\n\treturn NULL;\n}\n\nstruct fi_provider *ofi_get_hook(const char *name)\n{\n\tstruct ofi_prov *prov;\n\tstruct fi_provider *provider = NULL;\n\tchar *try_name = NULL;\n\tint ret;\n\n\tprov = ofi_getprov(name, strlen(name));\n\tif (!prov) {\n\t\tret = asprintf(&try_name, \"ofi_hook_%s\", name);\n\t\tif (ret > 0)\n\t\t\tprov = ofi_getprov(try_name, ret);\n\t\telse\n\t\t\ttry_name = NULL;\n\t}\n\n\tif (prov) {\n\t\tif (prov->provider && ofi_is_hook_prov(prov->provider)) {\n\t\t\tprovider = prov->provider;\n\t\t} else {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Specified provider is not a hook: %s\\n\", name);\n\t\t}\n\t} else {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"No hook found for: %s\\n\", name);\n\t}\n\n\tfree(try_name);\n\treturn provider;\n}\n\nstatic void cleanup_provider(struct fi_provider *provider, void *dlhandle)\n{\n\tOFI_UNUSED(dlhandle);\n\n\tif (provider) {\n\t\tfi_param_undefine(provider);\n\n\t\tif (provider->cleanup)\n\t\t\tprovider->cleanup();\n\t}\n\n#ifdef HAVE_LIBDL\n\tif (dlhandle)\n\t\tdlclose(dlhandle);\n#endif\n}\n\nstatic struct ofi_prov *ofi_create_prov_entry(const char *prov_name)\n{\n\tstruct ofi_prov *prov = NULL;\n\tprov = calloc(sizeof *prov, 1);\n\tif (!prov) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Not enough memory to allocate provider registry\\n\");\n\t\treturn NULL;\n\t}\n\n\tprov->prov_name = strdup(prov_name);\n\tif (!prov->prov_name) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Failed to init pre-registered provider name\\n\");\n\t\tfree(prov);\n\t\treturn NULL;\n\t}\n\tif (prov_tail)\n\t\tprov_tail->next = prov;\n\telse\n\t\tprov_head = prov;\n\tprov_tail = prov;\n\n\tprov->hidden = false;\n\n\treturn prov;\n}\n\n/* This is the default order that providers will be reported when a provider\n * is available.  Initialize the socket(s) provider last.  This will result in\n * it being the least preferred provider.\n */\nstatic void ofi_ordered_provs_init(void)\n{\n\tchar *ordered_prov_names[] = {\n\t\t\"psm2\", \"psm\", \"efa\", \"usnic\", \"gni\", \"bgq\", \"verbs\",\n\t\t\"netdir\", \"ofi_rxm\", \"ofi_rxd\", \"shm\",\n\t\t/* Initialize the socket based providers last of the\n\t\t * standard providers.  This will result in them being\n\t\t * the least preferred providers.\n\t\t */\n\n\t\t/* Before you add ANYTHING here, read the comment above!!! */\n\t\t\"UDP\", \"tcp\", \"sockets\", /* NOTHING GOES HERE! */\n\t\t/* Seriously, read it! */\n\n\t\t/* These are hooking providers only.  Their order\n\t\t * doesn't matter\n\t\t */\n\t\t\"ofi_hook_perf\", \"ofi_hook_debug\", \"ofi_hook_noop\",\n\t};\n\tint num_provs = sizeof(ordered_prov_names)/sizeof(ordered_prov_names[0]), i;\n\n\tfor (i = 0; i < num_provs; i++)\n\t\tofi_create_prov_entry(ordered_prov_names[i]);\n}\n\nstatic void ofi_set_prov_type(struct fi_prov_context *ctx,\n\t\t\t      struct fi_provider *provider)\n{\n\tif (!provider->getinfo)\n\t\tctx->type = OFI_PROV_HOOK;\n\telse if (ofi_has_util_prefix(provider->name))\n\t\tctx->type = OFI_PROV_UTIL;\n\telse\n\t\tctx->type = OFI_PROV_CORE;\n}\n\nstatic void ofi_register_provider(struct fi_provider *provider, void *dlhandle)\n{\n\tstruct fi_prov_context *ctx;\n\tstruct ofi_prov *prov = NULL;\n\tbool hidden = false;\n\n\tif (!provider || !provider->name) {\n\t\tFI_DBG(&core_prov, FI_LOG_CORE,\n\t\t       \"no provider structure or name\\n\");\n\t\tgoto cleanup;\n\t}\n\n\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t       \"registering provider: %s (%d.%d)\\n\", provider->name,\n\t       FI_MAJOR(provider->version), FI_MINOR(provider->version));\n\n\tif (!provider->fabric) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"provider missing mandatory entry points\\n\");\n\t\tgoto cleanup;\n\t}\n\n\t/* The current core implementation is not backward compatible\n\t * with providers that support a release earlier than v1.3.\n\t * See commit 0f4b6651.\n\t */\n\tif (provider->fi_version < FI_VERSION(1, 3)) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"provider has unsupported FI version \"\n\t\t\t\"(provider %d.%d != libfabric %d.%d); ignoring\\n\",\n\t\t\tFI_MAJOR(provider->fi_version),\n\t\t\tFI_MINOR(provider->fi_version), FI_MAJOR_VERSION,\n\t\t\tFI_MINOR_VERSION);\n\t\tgoto cleanup;\n\t}\n\n\tctx = (struct fi_prov_context *) &provider->context;\n\tofi_set_prov_type(ctx, provider);\n\n\tif (ofi_getinfo_filter(provider)) {\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"\\\"%s\\\" filtered by provider include/exclude \"\n\t\t\t\"list, skipping\\n\", provider->name);\n\t\thidden = true;\n\t}\n\n\tif (ofi_apply_filter(&prov_log_filter, provider->name))\n\t\tctx->disable_logging = 1;\n\n\tprov = ofi_getprov(provider->name, strlen(provider->name));\n\tif (prov) {\n\t\t/* If this provider has not been init yet, then we add the\n\t\t * provider and dlhandle to the struct and exit.\n\t\t */\n\t\tif (prov->provider == NULL)\n\t\t\tgoto update_prov_registry;\n\n\t\t/* If this provider is older than an already-loaded\n\t\t * provider of the same name, then discard this one.\n\t\t */\n\t\tif (FI_VERSION_GE(prov->provider->version, provider->version)) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"a newer %s provider was already loaded; \"\n\t\t\t\t\"ignoring this one\\n\", provider->name);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t/* This provider is newer than an already-loaded\n\t\t * provider of the same name, so discard the\n\t\t * already-loaded one.\n\t\t */\n\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\"an older %s provider was already loaded; \"\n\t\t\t\"keeping this one and ignoring the older one\\n\",\n\t\t\tprovider->name);\n\t\tcleanup_provider(prov->provider, prov->dlhandle);\n\t} else {\n\t\tprov = ofi_create_prov_entry(provider->name);\n\t\tif (!prov)\n\t\t\tgoto cleanup;\n\t}\n\n\tif (hidden)\n\t\tprov->hidden = true;\n\nupdate_prov_registry:\n\tprov->dlhandle = dlhandle;\n\tprov->provider = provider;\n\treturn;\n\ncleanup:\n\tcleanup_provider(provider, dlhandle);\n}\n\n#ifdef HAVE_LIBDL\nstatic int lib_filter(const struct dirent *entry)\n{\n\tsize_t l = strlen(entry->d_name);\n\tsize_t sfx = sizeof (FI_LIB_SUFFIX) - 1;\n\n\tif (l > sfx)\n\t\treturn !strcmp(&(entry->d_name[l-sfx]), FI_LIB_SUFFIX);\n\telse\n\t\treturn 0;\n}\n#endif\n\nstatic int verify_filter_names(char **names)\n{\n\tint i, j;\n\tchar** split_names;\n\tfor (i = 0; names[i]; i++) {\n\t\tsplit_names = ofi_split_and_alloc(names[i], \";\", NULL);\n\t\tif (!split_names) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"unable to parse given filter string\\n\");\n\t\t\treturn -FI_ENODATA;\n\t\t}\n\n\t\tfor(j = 0; split_names[j]; j++) {\n\t\t\tif(!ofi_getprov(split_names[j], strlen(split_names[j]))) {\n\t\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\t\"provider %s is unknown, misspelled\"\n\t\t\t\t\t\" or DL provider?\\n\", split_names[j]);\n\t\t\t\tofi_suggest_prov_names(split_names[j]);\n\t\t\t}\n\t\t}\n\t\tofi_free_string_array(split_names);\n\t}\n\n\treturn FI_SUCCESS;\n}\n\nvoid ofi_free_filter(struct fi_filter *filter)\n{\n\tofi_free_string_array(filter->names);\n}\n\nvoid ofi_create_filter(struct fi_filter *filter, const char *raw_filter)\n{\n\tmemset(filter, 0, sizeof *filter);\n\tif (raw_filter == NULL)\n\t\treturn;\n\n\tif (*raw_filter == '^') {\n\t\tfilter->negated = 1;\n\t\t++raw_filter;\n\t}\n\n\tfilter->names = ofi_split_and_alloc(raw_filter, \",\", NULL);\n\tif (!filter->names)\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"unable to parse filter from: %s\\n\", raw_filter);\n\n\tif(verify_filter_names(filter->names))\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t        \"unable to verify filter name\\n\");\n}\n\n#ifdef HAVE_LIBDL\nstatic void ofi_ini_dir(const char *dir)\n{\n\tint n = 0;\n\tchar *lib;\n\tvoid *dlhandle;\n\tstruct dirent **liblist = NULL;\n\tstruct fi_provider* (*inif)(void);\n\n\tn = scandir(dir, &liblist, lib_filter, NULL);\n\tif (n < 0)\n\t\tgoto libdl_done;\n\n\twhile (n--) {\n\t\tif (asprintf(&lib, \"%s/%s\", dir, liblist[n]->d_name) < 0) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t       \"asprintf failed to allocate memory\\n\");\n\t\t\tgoto libdl_done;\n\t\t}\n\t\tFI_DBG(&core_prov, FI_LOG_CORE, \"opening provider lib %s\\n\", lib);\n\n\t\tdlhandle = dlopen(lib, RTLD_NOW);\n\t\tfree(liblist[n]);\n\t\tif (dlhandle == NULL) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t       \"dlopen(%s): %s\\n\", lib, dlerror());\n\t\t\tfree(lib);\n\t\t\tcontinue;\n\t\t}\n\t\tfree(lib);\n\n\t\tinif = dlsym(dlhandle, \"fi_prov_ini\");\n\t\tif (inif == NULL) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE, \"dlsym: %s\\n\", dlerror());\n\t\t\tdlclose(dlhandle);\n\t\t} else {\n\t\t\tofi_register_provider((inif)(), dlhandle);\n\t\t}\n\t}\n\nlibdl_done:\n\twhile (n-- > 0)\n\t\tfree(liblist[n]);\n\tfree(liblist);\n}\n#endif\n\nvoid fi_ini(void)\n{\n\tchar *param_val = NULL;\n\n\tpthread_mutex_lock(&common_locks.ini_lock);\n\n\tif (ofi_init)\n\t\tgoto unlock;\n\n\tofi_ordered_provs_init();\n\tfi_param_init();\n\tfi_log_init();\n\tofi_osd_init();\n\tofi_mem_init();\n\tofi_pmem_init();\n\tofi_perf_init();\n\tofi_hook_init();\n\tofi_monitor_init();\n\n\tfi_param_define(NULL, \"provider\", FI_PARAM_STRING,\n\t\t\t\"Only use specified provider (default: all available)\");\n\tfi_param_define(NULL, \"fork_unsafe\", FI_PARAM_BOOL,\n\t\t\t\"Whether use of fork() may be unsafe for some providers\"\n\t\t\t\" (default: no). Setting this to yes could improve\"\n\t\t\t\" performance at the expense of making fork() potentially\"\n\t\t\t\" unsafe\");\n\tfi_param_define(NULL, \"universe_size\", FI_PARAM_SIZE_T,\n\t\t\t\"Defines the maximum number of processes that will be\"\n\t\t\t\" used by distribute OFI application. The provider uses\"\n\t\t\t\" this to optimize resource allocations\"\n\t\t\t\" (default: OFI service specific)\");\n\tfi_param_get_str(NULL, \"provider\", &param_val);\n\tofi_create_filter(&prov_filter, param_val);\n\n#ifdef HAVE_LIBDL\n\tint n = 0;\n\tchar **dirs;\n\tchar *provdir = NULL;\n\tvoid *dlhandle;\n\n\t/* If dlopen fails, assume static linking and just return\n\t   without error */\n\tdlhandle = dlopen(NULL, RTLD_NOW);\n\tif (dlhandle == NULL) {\n\t\tgoto libdl_done;\n\t}\n\tdlclose(dlhandle);\n\n\tfi_param_define(NULL, \"provider_path\", FI_PARAM_STRING,\n\t\t\t\"Search for providers in specific path (default: \"\n\t\t\tPROVDLDIR \")\");\n\tfi_param_get_str(NULL, \"provider_path\", &provdir);\n\tif (!provdir)\n\t\tprovdir = PROVDLDIR;\n\n\tdirs = ofi_split_and_alloc(provdir, \":\", NULL);\n\tif (dirs) {\n\t\tfor (n = 0; dirs[n]; ++n) {\n\t\t\tofi_ini_dir(dirs[n]);\n\t\t}\n\t\tofi_free_string_array(dirs);\n\t}\nlibdl_done:\n#endif\n\n\tofi_register_provider(PSM2_INIT, NULL);\n\tofi_register_provider(PSM_INIT, NULL);\n\tofi_register_provider(USNIC_INIT, NULL);\n\tofi_register_provider(GNI_INIT, NULL);\n\tofi_register_provider(BGQ_INIT, NULL);\n\tofi_register_provider(NETDIR_INIT, NULL);\n\tofi_register_provider(SHM_INIT, NULL);\n\tofi_register_provider(RXM_INIT, NULL);\n\tofi_register_provider(VERBS_INIT, NULL);\n\t/* ofi_register_provider(RSTREAM_INIT, NULL); - no support */\n\tofi_register_provider(MRAIL_INIT, NULL);\n\tofi_register_provider(RXD_INIT, NULL);\n\tofi_register_provider(EFA_INIT, NULL);\n\tofi_register_provider(UDP_INIT, NULL);\n\tofi_register_provider(SOCKETS_INIT, NULL);\n\tofi_register_provider(TCP_INIT, NULL);\n\n\tofi_register_provider(HOOK_PERF_INIT, NULL);\n\tofi_register_provider(HOOK_DEBUG_INIT, NULL);\n\tofi_register_provider(HOOK_NOOP_INIT, NULL);\n\n\tofi_init = 1;\n\nunlock:\n\tpthread_mutex_unlock(&common_locks.ini_lock);\n}\n\nFI_DESTRUCTOR(fi_fini(void))\n{\n\tstruct ofi_prov *prov;\n\n\tif (!ofi_init)\n\t\treturn;\n\n\twhile (prov_head) {\n\t\tprov = prov_head;\n\t\tprov_head = prov->next;\n\t\tcleanup_provider(prov->provider, prov->dlhandle);\n\t\tfree(prov->prov_name);\n\t\tfree(prov);\n\t}\n\n\tofi_free_filter(&prov_filter);\n\tofi_monitor_cleanup();\n\tofi_mem_fini();\n\tfi_log_fini();\n\tfi_param_fini();\n\tofi_osd_fini();\n}\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nvoid DEFAULT_SYMVER_PRE(fi_freeinfo)(struct fi_info *info)\n{\n\tstruct fi_info *next;\n\n\tfor (; info; info = next) {\n\t\tnext = info->next;\n\n\t\tfree(info->src_addr);\n\t\tfree(info->dest_addr);\n\t\tfree(info->tx_attr);\n\t\tfree(info->rx_attr);\n\t\tif (info->ep_attr) {\n\t\t\tfree(info->ep_attr->auth_key);\n\t\t\tfree(info->ep_attr);\n\t\t}\n\t\tif (info->domain_attr) {\n\t\t\tfree(info->domain_attr->auth_key);\n\t\t\tfree(info->domain_attr->name);\n\t\t\tfree(info->domain_attr);\n\t\t}\n\t\tif (info->fabric_attr) {\n\t\t\tfree(info->fabric_attr->name);\n\t\t\tfree(info->fabric_attr->prov_name);\n\t\t\tfree(info->fabric_attr);\n\t\t}\n\t\tif (info->nic &&\n\t\t    FI_CHECK_OP(info->nic->fid.ops, struct fi_ops, close)) {\n\t\t\tfi_close(&info->nic->fid);\n\t\t}\n\t\tfree(info);\n\t}\n}\nCURRENT_SYMVER(fi_freeinfo_, fi_freeinfo);\n\n/*\n * Make a dummy info object for each provider, and copy in the\n * provider name and version.  We report utility providers directly\n * to export their version.\n */\nstatic int ofi_getprovinfo(struct fi_info **info)\n{\n\tstruct ofi_prov *prov;\n\tstruct fi_info *tail, *cur;\n\tint ret = -FI_ENODATA;\n\n\t*info = tail = NULL;\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif (!prov->provider)\n\t\t\tcontinue;\n\n\t\tcur = fi_allocinfo();\n\t\tif (!cur) {\n\t\t\tret = -FI_ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tcur->fabric_attr->prov_name = strdup(prov->provider->name);\n\t\tcur->fabric_attr->prov_version = prov->provider->version;\n\n\t\tif (!*info) {\n\t\t\t*info = tail = cur;\n\t\t} else {\n\t\t\ttail->next = cur;\n\t\t}\n\t\ttail = cur;\n\n\t\tret = 0;\n\t}\n\n\treturn ret;\n\nerr:\n\twhile (tail) {\n\t\tcur = tail->next;\n\t\tfi_freeinfo(tail);\n\t\ttail = cur;\n\t}\n\treturn ret;\n}\n\nstatic void ofi_set_prov_attr(struct fi_fabric_attr *attr,\n\t\t\t      struct fi_provider *prov)\n{\n\tchar *core_name;\n\n\tcore_name = attr->prov_name;\n\tif (core_name) {\n\t\tassert(ofi_is_util_prov(prov));\n\t\tattr->prov_name = ofi_strdup_append(core_name, prov->name);\n\t\tfree(core_name);\n\t} else {\n\t\tassert(ofi_is_core_prov(prov));\n\t\tattr->prov_name = strdup(prov->name);\n\t}\n\tattr->prov_version = prov->version;\n}\n\n/*\n * The layering of utility providers over core providers follows these rules.\n * 0. Provider names are delimited by \";\"\n * 1. Rules when # of providers <= 2:\n *    1a. If both are specified, then only return that layering\n *    1b. If a utility provider is specified, return it over any* core provider.\n *    1c. If a core provider is specified, return any utility provider that can\n *        layer over it, plus the core provider itself, if possible.\n *    1d. A utility provider will not layer over the sockets provider unless the\n *        user explicitly requests that combination.\n *    1e. OFI_CORE_PROV_ONLY flag prevents utility providers layering over other\n *        utility providers.\n * 2. If both the providers are utility providers or if more than two providers\n *    are specified, the rightmost provider would be compared.\n * 3. If any provider has a caret symbol \"^\" is prefixed before any provider\n *    name it would be excluded (internal use only). These excluded providers\n *    should be listed only at the end.\n */\nstatic int ofi_layering_ok(const struct fi_provider *provider,\n\t\t\t   char **prov_vec, size_t count,\n\t\t\t   uint64_t flags)\n{\n\tchar *prov_name;\n\tint i;\n\n\t/* Excluded providers must be at the end */\n\tfor (i = count - 1; i >= 0; i--) {\n\t\tif (prov_vec[i][0] != '^')\n\t\t    break;\n\n\t\tif (!strcasecmp(&prov_vec[i][1], provider->name))\n\t\t\treturn 0;\n\t}\n\tcount = i + 1;\n\n\tif (flags & OFI_CORE_PROV_ONLY) {\n\t\tassert((count == 1) || (count == 0));\n\t\tif (!ofi_is_core_prov(provider)) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Need core provider, skipping %s\\n\",\n\t\t\t\tprovider->name);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif ((count == 0) && !strcasecmp(provider->name, \"sockets\")) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Skipping util;sockets layering\\n\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (!count)\n\t\treturn 1;\n\n\t/* To maintain backward compatibility with the previous behavior of\n\t * ofi_layering_ok we need to check if the # of providers is two or\n\t * fewer. In such a case, we have to be agnostic to the ordering of\n\t * core and utility providers */\n\n\tif ((count == 1) && ofi_is_util_prov(provider) &&\n\t    !ofi_has_util_prefix(prov_vec[0])) {\n\t\tif (!strcasecmp(prov_vec[0], \"sockets\")) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Sockets requested, skipping util layering\\n\");\n\t\t\treturn 0;\n\t\t}\n\t\tif (!strcasecmp(prov_vec[0], \"shm\")) {\n\t\t\tFI_INFO(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Shm requested, skipping util layering\\n\");\n\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tif ((count == 2) && ofi_has_util_prefix(prov_vec[0]) &&\n\t    !ofi_has_util_prefix(prov_vec[1]))\n\t\tprov_name = prov_vec[0];\n\telse\n\t\tprov_name = prov_vec[count - 1];\n\n\treturn !strcasecmp(provider->name, prov_name);\n}\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nint DEFAULT_SYMVER_PRE(fi_getinfo)(uint32_t version, const char *node,\n\t\tconst char *service, uint64_t flags,\n\t\tconst struct fi_info *hints, struct fi_info **info)\n{\n\tstruct ofi_prov *prov;\n\tstruct fi_info *tail, *cur;\n\tchar **prov_vec = NULL;\n\tsize_t count = 0;\n\tenum fi_log_level level;\n\tint ret;\n\n\tif (!ofi_init)\n\t\tfi_ini();\n\n\tif (FI_VERSION_LT(fi_version(), version)) {\n\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\"Requested version is newer than library\\n\");\n\t\treturn -FI_ENOSYS;\n\t}\n\n\tif (flags == FI_PROV_ATTR_ONLY) {\n\t\treturn ofi_getprovinfo(info);\n\t}\n\n\tif (hints && hints->fabric_attr && hints->fabric_attr->prov_name) {\n\t\tprov_vec = ofi_split_and_alloc(hints->fabric_attr->prov_name,\n\t\t\t\t\t       \";\", &count);\n\t\tif (!prov_vec)\n\t\t\treturn -FI_ENOMEM;\n\t\tFI_DBG(&core_prov, FI_LOG_CORE, \"hints prov_name: %s\\n\",\n\t\t       hints->fabric_attr->prov_name);\n\t}\n\n\t*info = tail = NULL;\n\tfor (prov = prov_head; prov; prov = prov->next) {\n\t\tif (!prov->provider || !prov->provider->getinfo)\n\t\t\tcontinue;\n\n\t\tif (prov->hidden && !(flags & OFI_GETINFO_HIDDEN))\n\t\t\tcontinue;\n\n\t\tif (!ofi_layering_ok(prov->provider, prov_vec, count, flags))\n\t\t\tcontinue;\n\n\t\tif (FI_VERSION_LT(prov->provider->fi_version, version)) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"Provider %s fi_version %d.%d < requested %d.%d\\n\",\n\t\t\t\tprov->provider->name,\n\t\t\t\tFI_MAJOR(prov->provider->fi_version),\n\t\t\t\tFI_MINOR(prov->provider->fi_version),\n\t\t\t\tFI_MAJOR(version), FI_MINOR(version));\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur = NULL;\n\t\tret = prov->provider->getinfo(version, node, service, flags,\n\t\t\t\t\t      hints, &cur);\n\t\tif (ret) {\n\t\t\tlevel = ((hints && hints->fabric_attr &&\n\t\t\t\t  hints->fabric_attr->prov_name) ?\n\t\t\t\t FI_LOG_WARN : FI_LOG_INFO);\n\n\t\t\tFI_LOG(&core_prov, level, FI_LOG_CORE,\n\t\t\t       \"fi_getinfo: provider %s returned -%d (%s)\\n\",\n\t\t\t       prov->provider->name, -ret, fi_strerror(-ret));\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!cur) {\n\t\t\tFI_WARN(&core_prov, FI_LOG_CORE,\n\t\t\t\t\"fi_getinfo: provider %s output empty list\\n\",\n\t\t\t\tprov->provider->name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tFI_DBG(&core_prov, FI_LOG_CORE, \"fi_getinfo: provider %s \"\n\t\t       \"returned success\\n\", prov->provider->name);\n\n\t\tif (!*info)\n\t\t\t*info = cur;\n\t\telse\n\t\t\ttail->next = cur;\n\n\t\tfor (tail = cur; tail->next; tail = tail->next) {\n\t\t\tofi_set_prov_attr(tail->fabric_attr, prov->provider);\n\t\t\ttail->fabric_attr->api_version = version;\n\t\t}\n\t\tofi_set_prov_attr(tail->fabric_attr, prov->provider);\n\t\ttail->fabric_attr->api_version = version;\n\t}\n\tofi_free_string_array(prov_vec);\n\n\tif (!(flags & (OFI_CORE_PROV_ONLY | OFI_GETINFO_INTERNAL |\n\t               OFI_GETINFO_HIDDEN)))\n\t\tofi_filter_info(info);\n\n\treturn *info ? 0 : -FI_ENODATA;\n}\nCURRENT_SYMVER(fi_getinfo_, fi_getinfo);\n\nstruct fi_info *ofi_allocinfo_internal(void)\n{\n\tstruct fi_info *info;\n\n\tinfo = calloc(1, sizeof(*info));\n\tif (!info)\n\t\treturn NULL;\n\n\tinfo->tx_attr = calloc(1, sizeof(*info->tx_attr));\n\tinfo->rx_attr = calloc(1, sizeof(*info->rx_attr));\n\tinfo->ep_attr = calloc(1, sizeof(*info->ep_attr));\n\tinfo->domain_attr = calloc(1, sizeof(*info->domain_attr));\n\tinfo->fabric_attr = calloc(1, sizeof(*info->fabric_attr));\n\tif (!info->tx_attr|| !info->rx_attr || !info->ep_attr ||\n\t    !info->domain_attr || !info->fabric_attr)\n\t\tgoto err;\n\n\treturn info;\nerr:\n\tfi_freeinfo(info);\n\treturn NULL;\n}\n\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nstruct fi_info *DEFAULT_SYMVER_PRE(fi_dupinfo)(const struct fi_info *info)\n{\n\tstruct fi_info *dup;\n\tint ret;\n\n\tif (!info)\n\t\treturn ofi_allocinfo_internal();\n\n\tdup = mem_dup(info, sizeof(*dup));\n\tif (dup == NULL) {\n\t\treturn NULL;\n\t}\n\tdup->src_addr = NULL;\n\tdup->dest_addr = NULL;\n\tdup->tx_attr = NULL;\n\tdup->rx_attr = NULL;\n\tdup->ep_attr = NULL;\n\tdup->domain_attr = NULL;\n\tdup->fabric_attr = NULL;\n\tdup->next = NULL;\n\n\tif (info->src_addr != NULL) {\n\t\tdup->src_addr = mem_dup(info->src_addr, info->src_addrlen);\n\t\tif (dup->src_addr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->dest_addr != NULL) {\n\t\tdup->dest_addr = mem_dup(info->dest_addr, info->dest_addrlen);\n\t\tif (dup->dest_addr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->tx_attr != NULL) {\n\t\tdup->tx_attr = mem_dup(info->tx_attr, sizeof(*info->tx_attr));\n\t\tif (dup->tx_attr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->rx_attr != NULL) {\n\t\tdup->rx_attr = mem_dup(info->rx_attr, sizeof(*info->rx_attr));\n\t\tif (dup->rx_attr == NULL)\n\t\t\tgoto fail;\n\t}\n\tif (info->ep_attr != NULL) {\n\t\tdup->ep_attr = mem_dup(info->ep_attr, sizeof(*info->ep_attr));\n\t\tif (dup->ep_attr == NULL)\n\t\t\tgoto fail;\n\t\tif (info->ep_attr->auth_key != NULL) {\n\t\t\tdup->ep_attr->auth_key =\n\t\t\t\tmem_dup(info->ep_attr->auth_key,\n\t\t\t\t\tinfo->ep_attr->auth_key_size);\n\t\t\tif (dup->ep_attr->auth_key == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\tif (info->domain_attr) {\n\t\tdup->domain_attr = mem_dup(info->domain_attr,\n\t\t\t\t\t   sizeof(*info->domain_attr));\n\t\tif (dup->domain_attr == NULL)\n\t\t\tgoto fail;\n\t\tdup->domain_attr->name = NULL;\n\t\tdup->domain_attr->auth_key = NULL;\n\t\tif (info->domain_attr->name != NULL) {\n\t\t\tdup->domain_attr->name = strdup(info->domain_attr->name);\n\t\t\tif (dup->domain_attr->name == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tif (info->domain_attr->auth_key != NULL) {\n\t\t\tdup->domain_attr->auth_key =\n\t\t\t\tmem_dup(info->domain_attr->auth_key,\n\t\t\t\t\tinfo->domain_attr->auth_key_size);\n\t\t\tif (dup->domain_attr->auth_key == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\tif (info->fabric_attr) {\n\t\tdup->fabric_attr = mem_dup(info->fabric_attr,\n\t\t\t\t\t   sizeof(*info->fabric_attr));\n\t\tif (dup->fabric_attr == NULL)\n\t\t\tgoto fail;\n\t\tdup->fabric_attr->name = NULL;\n\t\tdup->fabric_attr->prov_name = NULL;\n\t\tif (info->fabric_attr->name != NULL) {\n\t\t\tdup->fabric_attr->name = strdup(info->fabric_attr->name);\n\t\t\tif (dup->fabric_attr->name == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tif (info->fabric_attr->prov_name != NULL) {\n\t\t\tdup->fabric_attr->prov_name = strdup(info->fabric_attr->prov_name);\n\t\t\tif (dup->fabric_attr->prov_name == NULL)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (info->nic) {\n\t\tret = fi_control(&info->nic->fid, FI_DUP, &dup->nic);\n\t\tif (ret && ret != -FI_ENOSYS)\n\t\t\tgoto fail;\n\t}\n\n\treturn dup;\n\nfail:\n\tfi_freeinfo(dup);\n\treturn NULL;\n}\nCURRENT_SYMVER(fi_dupinfo_, fi_dupinfo);\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nint DEFAULT_SYMVER_PRE(fi_fabric)(struct fi_fabric_attr *attr,\n\t\tstruct fid_fabric **fabric, void *context)\n{\n\tstruct ofi_prov *prov;\n\tconst char *top_name;\n\tint ret;\n\n\tif (!attr || !attr->prov_name || !attr->name)\n\t\treturn -FI_EINVAL;\n\n\tif (!ofi_init)\n\t\tfi_ini();\n\n\ttop_name = strrchr(attr->prov_name, OFI_NAME_DELIM);\n\tif (top_name)\n\t\ttop_name++;\n\telse\n\t\ttop_name = attr->prov_name;\n\n\tif (!top_name)\n\t\treturn -FI_EINVAL;\n\n\tprov = ofi_getprov(top_name, strlen(top_name));\n\tif (!prov || !prov->provider || !prov->provider->fabric)\n\t\treturn -FI_ENODEV;\n\n\tret = prov->provider->fabric(attr, fabric, context);\n\tif (!ret) {\n\t\tif (FI_VERSION_GE(prov->provider->fi_version, FI_VERSION(1, 5)))\n\t\t\t(*fabric)->api_version = attr->api_version;\n\t\tFI_INFO(&core_prov, FI_LOG_CORE, \"Opened fabric: %s\\n\",\n\t\t\tattr->name);\n\n\t\tofi_hook_install(*fabric, fabric, prov->provider);\n\t}\n\n\treturn ret;\n}\nDEFAULT_SYMVER(fi_fabric_, fi_fabric, FABRIC_1.1);\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nuint32_t DEFAULT_SYMVER_PRE(fi_version)(void)\n{\n\treturn FI_VERSION(FI_MAJOR_VERSION, FI_MINOR_VERSION);\n}\nDEFAULT_SYMVER(fi_version_, fi_version, FABRIC_1.0);\n\nstatic const char *const errstr[] = {\n\t[FI_EOTHER - FI_ERRNO_OFFSET] = \"Unspecified error\",\n\t[FI_ETOOSMALL - FI_ERRNO_OFFSET] = \"Provided buffer is too small\",\n\t[FI_EOPBADSTATE - FI_ERRNO_OFFSET] = \"Operation not permitted in current state\",\n\t[FI_EAVAIL - FI_ERRNO_OFFSET]  = \"Error available\",\n\t[FI_EBADFLAGS - FI_ERRNO_OFFSET] = \"Flags not supported\",\n\t[FI_ENOEQ - FI_ERRNO_OFFSET] = \"Missing or unavailable event queue\",\n\t[FI_EDOMAIN - FI_ERRNO_OFFSET] = \"Invalid resource domain\",\n\t[FI_ENOCQ - FI_ERRNO_OFFSET] = \"Missing or unavailable completion queue\",\n\t[FI_ECRC - FI_ERRNO_OFFSET] = \"CRC error\",\n\t[FI_ETRUNC - FI_ERRNO_OFFSET] = \"Truncation error\",\n\t[FI_ENOKEY - FI_ERRNO_OFFSET] = \"Required key not available\",\n\t[FI_ENOAV - FI_ERRNO_OFFSET] = \"Missing or unavailable address vector\",\n\t[FI_EOVERRUN - FI_ERRNO_OFFSET] = \"Queue has been overrun\",\n};\n\n__attribute__((visibility (\"default\"),EXTERNALLY_VISIBLE))\nconst char *DEFAULT_SYMVER_PRE(fi_strerror)(int errnum)\n{\n\tif (errnum < FI_ERRNO_OFFSET)\n\t\treturn strerror(errnum);\n\telse if (errnum < FI_ERRNO_MAX)\n\t\treturn errstr[errnum - FI_ERRNO_OFFSET];\n\telse\n\t\treturn errstr[FI_EOTHER - FI_ERRNO_OFFSET];\n}\nDEFAULT_SYMVER(fi_strerror_, fi_strerror, FABRIC_1.0);\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/libfabric/prov/psm/src/psmx_am.c": "/*\n * Copyright (c) 2013-2017 Intel Corporation. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\n#include \"psmx.h\"\n\nstruct psm_am_parameters psmx_am_param;\n\nstatic psm_am_handler_fn_t psmx_am_handlers[3] = {\n\tpsmx_am_rma_handler,\n\tpsmx_am_msg_handler,\n\tpsmx_am_atomic_handler,\n};\n\nstatic int psmx_am_handlers_idx[3];\nstatic int psmx_am_handlers_initialized = 0;\n\n/* The AM handler signature is different between PSM1 and PSM2. The compat\n * handlers are used when compiled with PSM1 headers and run over the\n * psm2-compat library.\n */\n\nint psmx_am_compat_mode = 0;\nstatic int (*psmx_am_get_source)(psm_am_token_t token, psm_epaddr_t *epaddr);\n\nstatic int psmx_am_compat_rma_handler(psm_am_token_t token,\n\t\t\t\t      psm_amarg_t *args, int nargs,\n\t\t\t\t      void *src, uint32_t len)\n{\n\tpsm_epaddr_t epaddr;\n\t(*psmx_am_get_source)(token, &epaddr);\n\treturn psmx_am_rma_handler(token, epaddr, args, nargs, src, len);\n}\n\nstatic int psmx_am_compat_msg_handler(psm_am_token_t token,\n\t\t\t\t      psm_amarg_t *args, int nargs,\n\t\t\t\t      void *src, uint32_t len)\n{\n\tpsm_epaddr_t epaddr;\n\t(*psmx_am_get_source)(token, &epaddr);\n\treturn psmx_am_msg_handler(token, epaddr, args, nargs, src, len);\n}\n\nstatic int psmx_am_compat_atomic_handler(psm_am_token_t token,\n\t\t\t\t\t psm_amarg_t *args, int nargs,\n\t\t\t\t\t void *src, uint32_t len)\n{\n\tpsm_epaddr_t epaddr;\n\t(*psmx_am_get_source)(token, &epaddr);\n\treturn psmx_am_atomic_handler(token, epaddr, args, nargs, src, len);\n}\n\nint psmx_am_progress(struct psmx_fid_domain *domain)\n{\n\tstruct slist_entry *item;\n\tstruct psmx_am_request *req;\n\tstruct psmx_trigger *trigger;\n\n\tif (psmx_env.am_msg) {\n\t\tfastlock_acquire(&domain->send_queue.lock);\n\t\twhile (!slist_empty(&domain->send_queue.list)) {\n\t\t\titem = slist_remove_head(&domain->send_queue.list);\n\t\t\treq = container_of(item, struct psmx_am_request, list_entry);\n\t\t\tfastlock_release(&domain->send_queue.lock);\n\t\t\tpsmx_am_process_send(domain, req);\n\t\t\tfastlock_acquire(&domain->send_queue.lock);\n\t\t}\n\t\tfastlock_release(&domain->send_queue.lock);\n\t}\n\n\tif (psmx_env.tagged_rma) {\n\t\tfastlock_acquire(&domain->rma_queue.lock);\n\t\twhile (!slist_empty(&domain->rma_queue.list)) {\n\t\t\titem = slist_remove_head(&domain->rma_queue.list);\n\t\t\treq = container_of(item, struct psmx_am_request, list_entry);\n\t\t\tfastlock_release(&domain->rma_queue.lock);\n\t\t\tpsmx_am_process_rma(domain, req);\n\t\t\tfastlock_acquire(&domain->rma_queue.lock);\n\t\t}\n\t\tfastlock_release(&domain->rma_queue.lock);\n\t}\n\n\tfastlock_acquire(&domain->trigger_queue.lock);\n\twhile (!slist_empty(&domain->trigger_queue.list)) {\n\t\titem = slist_remove_head(&domain->trigger_queue.list);\n\t\ttrigger = container_of(item, struct psmx_trigger, list_entry);\n\t\tfastlock_release(&domain->trigger_queue.lock);\n\t\tpsmx_process_trigger(domain, trigger);\n\t\tfastlock_acquire(&domain->trigger_queue.lock);\n\t}\n\tfastlock_release(&domain->trigger_queue.lock);\n\n\treturn 0;\n}\n\nint psmx_am_init(struct psmx_fid_domain *domain)\n{\n\tpsm_ep_t psm_ep = domain->psm_ep;\n\tsize_t size;\n\tint err = 0;\n\n\tFI_INFO(&psmx_prov, FI_LOG_CORE, \"\\n\");\n\n\tpsmx_atomic_init();\n\n\tif (!psmx_am_handlers_initialized) {\n\t\terr = psm_am_get_parameters(psm_ep, &psmx_am_param,\n\t\t\t\t\t\tsizeof(psmx_am_param), &size);\n\t\tif (err)\n\t\t\treturn psmx_errno(err);\n\n\t\tif (psmx_am_compat_mode) {\n\t\t\tvoid *dlsym(void*, const char *);\n\t\t\tpsmx_am_get_source = dlsym(NULL, \"psm2_am_get_source\");\n\t\t\tif (!psmx_am_get_source) {\n\t\t\t\tFI_WARN(&psmx_prov, FI_LOG_CORE,\n\t\t\t\t\t\"failed to load function psm2_am_get_source\\n\");\n\t\t\t\treturn -FI_EOTHER;\n\t\t\t}\n\n\t\t\tpsmx_am_handlers[0] = (void *)psmx_am_compat_rma_handler;\n\t\t\tpsmx_am_handlers[1] = (void *)psmx_am_compat_msg_handler;\n\t\t\tpsmx_am_handlers[2] = (void *)psmx_am_compat_atomic_handler;\n\t\t}\n\n\t\terr = psm_am_register_handlers(psm_ep, psmx_am_handlers, 3,\n\t\t\t\t\t\tpsmx_am_handlers_idx);\n\t\tif (err)\n\t\t\treturn psmx_errno(err);\n\n\t\tif ((psmx_am_handlers_idx[0] != PSMX_AM_RMA_HANDLER) ||\n\t\t    (psmx_am_handlers_idx[1] != PSMX_AM_MSG_HANDLER) ||\n\t\t    (psmx_am_handlers_idx[2] != PSMX_AM_ATOMIC_HANDLER)) {\n\t\t\tFI_WARN(&psmx_prov, FI_LOG_CORE,\n\t\t\t\t\"failed to register one or more AM handlers \"\n\t\t\t\t\"at indecies %d, %d, %d\\n\", PSMX_AM_RMA_HANDLER,\n\t\t\t\tPSMX_AM_MSG_HANDLER, PSMX_AM_ATOMIC_HANDLER);\n\t\t\treturn -FI_EBUSY;\n\t\t}\n\n\t\tpsmx_am_handlers_initialized = 1;\n\t}\n\n\tslist_init(&domain->rma_queue.list);\n\tslist_init(&domain->recv_queue.list);\n\tslist_init(&domain->unexp_queue.list);\n\tslist_init(&domain->trigger_queue.list);\n\tslist_init(&domain->send_queue.list);\n\tfastlock_init(&domain->rma_queue.lock);\n\tfastlock_init(&domain->recv_queue.lock);\n\tfastlock_init(&domain->unexp_queue.lock);\n\tfastlock_init(&domain->trigger_queue.lock);\n\tfastlock_init(&domain->send_queue.lock);\n\n\treturn err;\n}\n\nint psmx_am_fini(struct psmx_fid_domain *domain)\n{\n\tfastlock_destroy(&domain->rma_queue.lock);\n\tfastlock_destroy(&domain->recv_queue.lock);\n\tfastlock_destroy(&domain->unexp_queue.lock);\n\tfastlock_destroy(&domain->trigger_queue.lock);\n\tfastlock_destroy(&domain->send_queue.lock);\n\n\tpsmx_atomic_fini();\n\n\treturn 0;\n}\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/libfabric/prov/util/src/util_mem_hooks.c": "/*\n * Copyright (c) 2016 Los Alamos National Security, LLC. All rights reserved.\n * Copyright (c) 2019 Intel Corporation, Inc.  All rights reserved.\n *\n * License text from Open-MPI (www.open-mpi.org/community/license.php)\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are\n * met:\n *\n * - Redistributions of source code must retain the above copyright\n * notice, this list of conditions and the following disclaimer.\n *\n * - Redistributions in binary form must reproduce the above copyright\n * notice, this list of conditions and the following disclaimer listed\n * in this license in the documentation and/or other materials\n * provided with the distribution.\n *\n * - Neither the name of the copyright holders nor the names of its\n * contributors may be used to endorse or promote products derived from\n * this software without specific prior written permission.\n *\n * The copyright holders provide no reassurances that the source code\n * provided does not infringe any patent, copyright, or any other\n * intellectual property rights of third parties.  The copyright holders\n * disclaim any liability to any recipient for claims brought against\n * recipient by any third party for infringement of that parties\n * intellectual property rights.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <ofi_mr.h>\n\nstruct ofi_memhooks memhooks;\nstruct ofi_mem_monitor *memhooks_monitor = &memhooks.monitor;\n\n\n#if defined(__linux__) && defined(HAVE_ELF_H) && defined(HAVE_SYS_AUXV_H)\n\n#include <elf.h>\n#include <sys/auxv.h>\n#include <sys/mman.h>\n#include <sys/syscall.h>\n#include <sys/types.h>\n#include <sys/shm.h>\n#include <unistd.h>\n#include <dlfcn.h>\n#include <fcntl.h>\n#include <link.h>\n\n\nstruct ofi_intercept {\n\tstruct dlist_entry \t\tentry;\n\tconst char\t\t\t*symbol;\n\tvoid\t\t\t\t*our_func;\n\tstruct dlist_entry\t\tdl_intercept_list;\n};\n\nstruct ofi_dl_intercept {\n\tstruct dlist_entry \t\tentry;\n\tvoid \t\t\t\t**dl_func_addr;\n\tvoid\t\t\t\t*dl_func;\n};\n\nenum {\n\tOFI_INTERCEPT_DLOPEN,\n\tOFI_INTERCEPT_MMAP,\n\tOFI_INTERCEPT_MUNMAP,\n\tOFI_INTERCEPT_MREMAP,\n\tOFI_INTERCEPT_MADVISE,\n\tOFI_INTERCEPT_SHMAT,\n\tOFI_INTERCEPT_SHMDT,\n\tOFI_INTERCEPT_BRK,\n\tOFI_INTERCEPT_MAX\n};\n\nstatic void *ofi_intercept_dlopen(const char *filename, int flag);\nstatic void *ofi_intercept_mmap(void *start, size_t length,\n\t\t\t\tint prot, int flags, int fd, off_t offset);\nstatic int ofi_intercept_munmap(void *start, size_t length);\nstatic void *ofi_intercept_mremap(void *old_address, size_t old_size,\n\t\tsize_t new_size, int flags, void *new_address);\nstatic int ofi_intercept_madvise(void *addr, size_t length, int advice);\nstatic void *ofi_intercept_shmat(int shmid, const void *shmaddr, int shmflg);\nstatic int ofi_intercept_shmdt(const void *shmaddr);\nstatic int ofi_intercept_brk(const void *brkaddr);\n\nstatic struct ofi_intercept intercepts[] = {\n\t[OFI_INTERCEPT_DLOPEN] = { .symbol = \"dlopen\",\n\t\t\t\t.our_func = ofi_intercept_dlopen},\n\t[OFI_INTERCEPT_MMAP] = { .symbol = \"mmap\",\n\t\t\t\t.our_func = ofi_intercept_mmap},\n\t[OFI_INTERCEPT_MUNMAP] = { .symbol = \"munmap\",\n\t\t\t\t.our_func = ofi_intercept_munmap},\n\t[OFI_INTERCEPT_MREMAP] = { .symbol = \"mremap\",\n\t\t\t\t.our_func = ofi_intercept_mremap},\n\t[OFI_INTERCEPT_MADVISE] = { .symbol = \"madvise\",\n\t\t\t\t.our_func = ofi_intercept_madvise},\n\t[OFI_INTERCEPT_SHMAT] = { .symbol = \"shmat\",\n\t\t\t\t.our_func = ofi_intercept_shmat},\n\t[OFI_INTERCEPT_SHMDT] = { .symbol = \"shmdt\",\n\t\t\t\t.our_func = ofi_intercept_shmdt},\n\t[OFI_INTERCEPT_BRK] = { .symbol = \"brk\",\n\t\t\t\t.our_func = ofi_intercept_brk},\n};\n\nstruct ofi_mem_calls {\n\tvoid *(*dlopen) (const char *, int);\n\tvoid *(*mmap)(void *, size_t, int, int, int, off_t);\n\tint (*munmap)(void *, size_t);\n\tvoid *(*mremap)(void *old_address, size_t old_size,\n\t\t\tsize_t new_size, int flags, ... /* void *new_address */ );\n\tint (*madvise)(void *addr, size_t length, int advice);\n\tvoid *(*shmat)(int shmid, const void *shmaddr, int shmflg);\n\tint (*shmdt)(const void *shmaddr);\n\tint (*brk)(const void *brkaddr);\n};\n\nstatic struct ofi_mem_calls real_calls;\n\n\nstatic const ElfW(Phdr) *\nofi_get_phdr_dynamic(const ElfW(Phdr) *phdr, uint16_t phnum, int phent)\n{\n\tuint16_t i;\n\n\tfor (i = 0 ; i < phnum; i++) {\n\t\tif (phdr->p_type == PT_DYNAMIC)\n\t\t\treturn phdr;\n\t\tphdr = (ElfW(Phdr)*) ((intptr_t) phdr + phent);\n\t}\n\n\treturn NULL;\n}\n\nstatic void *ofi_get_dynentry(ElfW(Addr) base, const ElfW(Phdr) *pdyn,\n\t\t\t      ElfW(Sxword) type)\n{\n\tElfW(Dyn) *dyn;\n\n\tfor (dyn = (ElfW(Dyn)*) (base + pdyn->p_vaddr); dyn->d_tag; ++dyn) {\n\t\tif (dyn->d_tag == type)\n\t\t\treturn (void *) (uintptr_t) dyn->d_un.d_val;\n\t}\n\n\treturn NULL;\n}\n\n#if SIZE_MAX > UINT_MAX\n#define OFI_ELF_R_SYM ELF64_R_SYM\n#else\n#define OFI_ELF_R_SYM ELF32_R_SYM\n#endif\n\nstatic void *ofi_dl_func_addr(ElfW(Addr) base, const ElfW(Phdr) *phdr,\n\t\t\t      int16_t phnum, int phent, const char *symbol)\n{\n\tconst ElfW(Phdr) *dphdr;\n\tElfW(Rela) *reloc;\n\tvoid *jmprel, *strtab;\n\tchar *elf_sym;\n\tuint32_t relsymidx;\n\tElfW(Sym) *symtab;\n\tsize_t pltrelsz;\n\n\tdphdr = ofi_get_phdr_dynamic(phdr, phnum, phent);\n\tjmprel = ofi_get_dynentry(base, dphdr, DT_JMPREL);\n\tsymtab = (ElfW(Sym) *) ofi_get_dynentry(base, dphdr, DT_SYMTAB);\n\tstrtab = ofi_get_dynentry (base, dphdr, DT_STRTAB);\n\tpltrelsz = (uintptr_t) ofi_get_dynentry(base, dphdr, DT_PLTRELSZ);\n\n\tfor (reloc = jmprel; (intptr_t) reloc < (intptr_t) jmprel + pltrelsz;\n\t     reloc++) {\n\t\trelsymidx = OFI_ELF_R_SYM(reloc->r_info);\n\t\telf_sym = (char *) strtab + symtab[relsymidx].st_name;\n\t\tif (!strcmp(symbol, elf_sym))\n\t\t\treturn (void *) (base + reloc->r_offset);\n        }\n\n        return NULL;\n}\n\nstatic int ofi_intercept_dl_calls(ElfW(Addr) base, const ElfW(Phdr) *phdr,\n\t\t\t\t  const char *phname, int16_t phnum, int phent,\n\t\t\t\t  struct ofi_intercept *intercept)\n{\n\tstruct ofi_dl_intercept *dl_entry;\n\tlong page_size = ofi_get_page_size();\n\tvoid **func_addr, *page;\n\tint ret;\n\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepting symbol %s from dl\\n\", intercept->symbol);\n\tfunc_addr = ofi_dl_func_addr(base, phdr, phnum, phent, intercept->symbol);\n\tif (!func_addr)\n\t\treturn FI_SUCCESS;\n\n\tpage = (void *) ((intptr_t) func_addr & ~(page_size - 1));\n\tret = mprotect(page, page_size, PROT_READ | PROT_WRITE);\n\tif (ret < 0)\n\t\treturn -FI_ENOSYS;\n\n\tif (*func_addr != intercept->our_func) {\n\t\tdl_entry = malloc(sizeof(*dl_entry));\n\t\tif (!dl_entry)\n\t\t\treturn -FI_ENOMEM;\n\n\t\tdl_entry->dl_func_addr = func_addr;\n\t\tdl_entry->dl_func = *func_addr;\n\t\t*func_addr = intercept->our_func;\n\t\tdlist_insert_tail(&dl_entry->entry, &intercept->dl_intercept_list);\n\t}\n\n\treturn FI_SUCCESS;\n}\n\nstatic int ofi_intercept_phdr_handler(struct dl_phdr_info *info,\n                                    size_t size, void *data)\n{\n\tstruct ofi_intercept *intercept = data;\n\tint phent, ret;\n\n\tphent = getauxval(AT_PHENT);\n\tif (phent <= 0) {\n\t\tFI_DBG(&core_prov, FI_LOG_MR, \"failed to read phent size\");\n\t\treturn -FI_EINVAL;\n\t}\n\n\tret = ofi_intercept_dl_calls(info->dlpi_addr, info->dlpi_phdr,\n\t\t\t\t     info->dlpi_name, info->dlpi_phnum,\n\t\t\t\t     phent, intercept);\n\treturn ret;\n}\n\nstatic void *ofi_intercept_dlopen(const char *filename, int flag)\n{\n\tstruct ofi_intercept  *intercept;\n\tvoid *handle;\n\n\thandle = real_calls.dlopen(filename, flag);\n\tif (!handle)\n\t\treturn NULL;\n\n\tpthread_mutex_lock(&memhooks_monitor->lock);\n\tdlist_foreach_container(&memhooks.intercept_list, struct ofi_intercept,\n\t\tintercept, entry) {\n\t\tdl_iterate_phdr(ofi_intercept_phdr_handler, intercept);\n\t}\n\tpthread_mutex_unlock(&memhooks_monitor->lock);\n\treturn handle;\n}\n\nstatic int ofi_restore_dl_calls(ElfW(Addr) base, const ElfW(Phdr) *phdr,\n\t\t\t\tconst char *phname, int16_t phnum, int phent,\n\t\t\t\tstruct ofi_intercept *intercept)\n{\n\tstruct ofi_dl_intercept *dl_entry;\n\tlong page_size = ofi_get_page_size();\n\tvoid **func_addr, *page;\n\tint ret;\n\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"releasing symbol %s from dl\\n\", intercept->symbol);\n\tfunc_addr = ofi_dl_func_addr(base, phdr, phnum, phent, intercept->symbol);\n\tif (!func_addr)\n\t\treturn FI_SUCCESS;\n\n\tpage = (void *) ((intptr_t) func_addr & ~(page_size - 1));\n\tret = mprotect(page, page_size, PROT_READ | PROT_WRITE);\n\tif (ret < 0)\n\t\treturn -FI_ENOSYS;\n\n\tdlist_foreach_container_reverse(&intercept->dl_intercept_list,\n\t\tstruct ofi_dl_intercept, dl_entry, entry) {\n\n\t\tif (dl_entry->dl_func_addr != func_addr)\n\t\t\tcontinue;\n\n\t\tassert(*func_addr == intercept->our_func);\n\t\t*func_addr = dl_entry->dl_func;\n\t\tdlist_remove(&dl_entry->entry);\n\t\tfree(dl_entry);\n\t\tFI_DBG(&core_prov, FI_LOG_MR,\n\t\t       \"dl symbol %s restored\\n\", intercept->symbol);\n\t\tbreak;\n\t}\n\n\treturn FI_SUCCESS;\n}\n\nstatic int ofi_restore_phdr_handler(struct dl_phdr_info *info,\n                                    size_t size, void *data)\n{\n\tstruct ofi_intercept *intercept = data;\n\tint phent, ret;\n\n\tphent = getauxval(AT_PHENT);\n\tif (phent <= 0) {\n\t\tFI_DBG(&core_prov, FI_LOG_MR, \"failed to read phent size\");\n\t\treturn -FI_EINVAL;\n\t}\n\n\tret = ofi_restore_dl_calls(info->dlpi_addr, info->dlpi_phdr,\n\t\t\t\t   info->dlpi_name, info->dlpi_phnum,\n\t\t\t\t   phent, intercept);\n\treturn ret;\n}\n\nstatic void ofi_restore_intercepts(void)\n{\n\tstruct ofi_intercept *intercept;\n\n\tdlist_foreach_container(&memhooks.intercept_list, struct ofi_intercept,\n\t\tintercept, entry) {\n\t\tdl_iterate_phdr(ofi_restore_phdr_handler, intercept);\n\t}\n}\n\nstatic int ofi_intercept_symbol(struct ofi_intercept *intercept, void **real_func)\n{\n\tint ret;\n\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepting symbol %s\\n\", intercept->symbol);\n\tret = dl_iterate_phdr(ofi_intercept_phdr_handler, intercept);\n\tif (ret)\n\t\treturn ret;\n\n\t*real_func = dlsym(RTLD_DEFAULT, intercept->symbol);\n\tif (*real_func == intercept->our_func) {\n\t\t(void) dlerror();\n\t\t*real_func = dlsym(RTLD_NEXT, intercept->symbol);\n\t}\n\n\tif (!*real_func) {\n\t\tFI_DBG(&core_prov, FI_LOG_MR,\n\t\t       \"could not find symbol %s\\n\", intercept->symbol);\n\t\tret = -FI_ENOMEM;\n\t\treturn ret;\n\t}\n\tdlist_insert_tail(&intercept->entry, &memhooks.intercept_list);\n\n\treturn ret;\n}\n\nvoid ofi_intercept_handler(const void *addr, size_t len)\n{\n\tpthread_mutex_lock(&memhooks_monitor->lock);\n\tofi_monitor_notify(memhooks_monitor, addr, len);\n\tpthread_mutex_unlock(&memhooks_monitor->lock);\n}\n\nstatic void *ofi_intercept_mmap(void *start, size_t length,\n                            int prot, int flags, int fd, off_t offset)\n{\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepted mmap start %p len %zu\\n\", start, length);\n\tofi_intercept_handler(start, length);\n\n\treturn real_calls.mmap(start, length, prot, flags, fd, offset);\n}\n\nstatic int ofi_intercept_munmap(void *start, size_t length)\n{\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepted munmap start %p len %zu\\n\", start, length);\n\tofi_intercept_handler(start, length);\n\n\treturn real_calls.munmap(start, length);\n}\n\nstatic void *ofi_intercept_mremap(void *old_address, size_t old_size,\n\t\tsize_t new_size, int flags, void *new_address)\n{\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepted mremap old_addr %p old_size %zu\\n\",\n\t       old_address, old_size);\n\tofi_intercept_handler(old_address, old_size);\n\n\treturn real_calls.mremap(old_address, old_size, new_size, flags,\n\t\t\t\t new_address);\n}\n\nstatic int ofi_intercept_madvise(void *addr, size_t length, int advice)\n{\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepted madvise addr %p len %zu\\n\", addr, length);\n\tofi_intercept_handler(addr, length);\n\n\treturn real_calls.madvise(addr, length, advice);\n}\n\nstatic void *ofi_intercept_shmat(int shmid, const void *shmaddr, int shmflg)\n{\n\tstruct shmid_ds ds;\n\tconst void *start;\n\tsize_t len;\n\tint ret;\n\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepted shmat addr %p\\n\", shmaddr);\n\n\tif (shmflg & SHM_REMAP) {\n\t\tret = shmctl(shmid, IPC_STAT, &ds);\n\t\tlen = (ret < 0) ? 0 : ds.shm_segsz;\n\n\t\tif (shmflg & SHM_RND) {\n\t\t\tstart = (char *) shmaddr + ((uintptr_t) shmaddr) % SHMLBA;\n\t\t\tlen += ((uintptr_t) shmaddr) % SHMLBA;\n\t\t} else {\n\t\t\tstart = shmaddr;\n\t\t}\n\n\t\tofi_intercept_handler(start, len);\n\t}\n\n\treturn real_calls.shmat(shmid, shmaddr, shmflg);\n}\n\nstatic int ofi_intercept_shmdt(const void *shmaddr)\n{\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t       \"intercepted shmdt addr %p\\n\", shmaddr);\n\t/* Overly aggressive, but simple.  Invalidate everything after shmaddr */\n\tofi_intercept_handler(shmaddr, SIZE_MAX - (uintptr_t) shmaddr);\n\n\treturn real_calls.shmdt(shmaddr);\n}\n\nstatic int ofi_intercept_brk(const void *brkaddr)\n{\n\tvoid *old_addr;\n\n\tFI_DBG(&core_prov, FI_LOG_MR,\n\t      \"intercepted brk addr %p\\n\", brkaddr);\n\n\told_addr = sbrk (0);\n\n\tif(brkaddr > old_addr) {\n\t\tofi_intercept_handler(brkaddr, (intptr_t) brkaddr -\n\t\t\t\t\t\t\t  (intptr_t) old_addr);\n\t}\n\n\treturn real_calls.brk(brkaddr);\n}\n\nstatic int ofi_memhooks_subscribe(struct ofi_mem_monitor *monitor,\n\t\t\t\t const void *addr, size_t len)\n{\n\t/* no-op */\n\treturn FI_SUCCESS;\n}\n\nstatic void ofi_memhooks_unsubscribe(struct ofi_mem_monitor *monitor,\n\t\t\t\t    const void *addr, size_t len)\n{\n\t/* no-op */\n}\n\nint ofi_memhooks_init(void)\n{\n\tint i, ret;\n\n\tif (memhooks_monitor->subscribe == ofi_memhooks_subscribe)\n\t\treturn 0;\n\n\tmemhooks_monitor->subscribe = ofi_memhooks_subscribe;\n\tmemhooks_monitor->unsubscribe = ofi_memhooks_unsubscribe;\n\tdlist_init(&memhooks.intercept_list);\n\n\tfor (i = 0; i < OFI_INTERCEPT_MAX; ++i)\n\t\tdlist_init(&intercepts[i].dl_intercept_list);\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_DLOPEN],\n\t\t\t\t   (void **) &real_calls.dlopen);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept dlopen failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MMAP],\n\t\t\t\t   (void **) &real_calls.mmap);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept mmap failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MUNMAP],\n\t\t\t\t   (void **) &real_calls.munmap);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept munmap failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MREMAP],\n\t\t\t\t   (void **) &real_calls.mremap);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept mremap failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_MADVISE],\n\t\t\t\t   (void **) &real_calls.madvise);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept madvise failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_SHMAT],\n\t\t\t\t   (void **) &real_calls.shmat);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept shmat failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_SHMDT],\n\t\t\t\t   (void **) &real_calls.shmdt);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept shmdt failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\tret = ofi_intercept_symbol(&intercepts[OFI_INTERCEPT_BRK],\n\t\t\t\t   (void **) &real_calls.brk);\n\tif (ret) {\n\t\tFI_WARN(&core_prov, FI_LOG_MR,\n\t\t       \"intercept brk failed %d %s\\n\", ret, fi_strerror(ret));\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nvoid ofi_memhooks_cleanup(void)\n{\n\tofi_restore_intercepts();\n\tmemhooks_monitor->subscribe = NULL;\n\tmemhooks_monitor->unsubscribe = NULL;\n}\n\n#else\n\nint ofi_memhooks_init(void)\n{\n\treturn -FI_ENOSYS;\n}\n\nvoid ofi_memhooks_cleanup(void)\n{\n}\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/include/hwloc/plugins.h": "/*\n * Copyright \u00a9 2013-2017 Inria.  All rights reserved.\n * Copyright \u00a9 2016 Cisco Systems, Inc.  All rights reserved.\n * See COPYING in top-level directory.\n */\n\n#ifndef HWLOC_PLUGINS_H\n#define HWLOC_PLUGINS_H\n\n/** \\file\n * \\brief Public interface for building hwloc plugins.\n */\n\nstruct hwloc_backend;\n\n#include <hwloc.h>\n#ifdef HWLOC_INSIDE_PLUGIN\n/* needed for hwloc_plugin_check_namespace() */\n#include <ltdl.h>\n#endif\n\n\n\n/** \\defgroup hwlocality_disc_components Components and Plugins: Discovery components\n * @{\n */\n\n/** \\brief Discovery component type */\ntypedef enum hwloc_disc_component_type_e {\n  /** \\brief CPU-only discovery through the OS, or generic no-OS support.\n   * \\hideinitializer */\n  HWLOC_DISC_COMPONENT_TYPE_CPU = (1<<0),\n\n  /** \\brief xml or synthetic,\n   * platform-specific components such as bgq.\n   * Anything the discovers CPU and everything else.\n   * No misc backend is expected to complement a global component.\n   * \\hideinitializer */\n  HWLOC_DISC_COMPONENT_TYPE_GLOBAL = (1<<1),\n\n  /** \\brief OpenCL, Cuda, etc.\n   * \\hideinitializer */\n  HWLOC_DISC_COMPONENT_TYPE_MISC = (1<<2)\n} hwloc_disc_component_type_t;\n\n/** \\brief Discovery component structure\n *\n * This is the major kind of components, taking care of the discovery.\n * They are registered by generic components, either statically-built or as plugins.\n */\nstruct hwloc_disc_component {\n  /** \\brief Discovery component type */\n  hwloc_disc_component_type_t type;\n\n  /** \\brief Name.\n   * If this component is built as a plugin, this name does not have to match the plugin filename.\n   */\n  const char *name;\n\n  /** \\brief Component types to exclude, as an OR'ed set of ::hwloc_disc_component_type_e.\n   *\n   * For a GLOBAL component, this usually includes all other types (~0).\n   *\n   * Other components only exclude types that may bring conflicting\n   * topology information. MISC components should likely not be excluded\n   * since they usually bring non-primary additional information.\n   */\n  unsigned excludes;\n\n  /** \\brief Instantiate callback to create a backend from the component.\n   * Parameters data1, data2, data3 are NULL except for components\n   * that have special enabling routines such as hwloc_topology_set_xml(). */\n  struct hwloc_backend * (*instantiate)(struct hwloc_disc_component *component, const void *data1, const void *data2, const void *data3);\n\n  /** \\brief Component priority.\n   * Used to sort topology->components, higher priority first.\n   * Also used to decide between two components with the same name.\n   *\n   * Usual values are\n   * 50 for native OS (or platform) components,\n   * 45 for x86,\n   * 40 for no-OS fallback,\n   * 30 for global components (xml, synthetic),\n   * 20 for pci,\n   * 10 for other misc components (opencl etc.).\n   */\n  unsigned priority;\n\n  /** \\brief Enabled by default.\n   * If unset, if will be disabled unless explicitly requested.\n   */\n  unsigned enabled_by_default;\n\n  /** \\private Used internally to list components by priority on topology->components\n   * (the component structure is usually read-only,\n   *  the core copies it before using this field for queueing)\n   */\n  struct hwloc_disc_component * next;\n};\n\n/** @} */\n\n\n\n\n/** \\defgroup hwlocality_disc_backends Components and Plugins: Discovery backends\n * @{\n */\n\n/** \\brief Discovery backend structure\n *\n * A backend is the instantiation of a discovery component.\n * When a component gets enabled for a topology,\n * its instantiate() callback creates a backend.\n *\n * hwloc_backend_alloc() initializes all fields to default values\n * that the component may change (except \"component\" and \"next\")\n * before enabling the backend with hwloc_backend_enable().\n */\nstruct hwloc_backend {\n  /** \\private Reserved for the core, set by hwloc_backend_alloc() */\n  struct hwloc_disc_component * component;\n  /** \\private Reserved for the core, set by hwloc_backend_enable() */\n  struct hwloc_topology * topology;\n  /** \\private Reserved for the core. Set to 1 if forced through envvar, 0 otherwise. */\n  int envvar_forced;\n  /** \\private Reserved for the core. Used internally to list backends topology->backends. */\n  struct hwloc_backend * next;\n\n  /** \\brief Backend flags, currently always 0. */\n  unsigned long flags;\n\n  /** \\brief Backend-specific 'is_thissystem' property.\n   * Set to 0 or 1 if the backend should enforce the thissystem flag when it gets enabled.\n   * Set to -1 if the backend doesn't care (default). */\n  int is_thissystem;\n\n  /** \\brief Backend private data, or NULL if none. */\n  void * private_data;\n  /** \\brief Callback for freeing the private_data.\n   * May be NULL.\n   */\n  void (*disable)(struct hwloc_backend *backend);\n\n  /** \\brief Main discovery callback.\n   * returns -1 on error, either because it couldn't add its objects ot the existing topology,\n   * or because of an actual discovery/gathering failure.\n   * May be NULL.\n   */\n  int (*discover)(struct hwloc_backend *backend);\n\n  /** \\brief Callback used by the PCI backend to retrieve the locality of a PCI object from the OS/cpu backend.\n   * May be NULL. */\n  int (*get_pci_busid_cpuset)(struct hwloc_backend *backend, struct hwloc_pcidev_attr_s *busid, hwloc_bitmap_t cpuset);\n};\n\n/** \\brief Allocate a backend structure, set good default values, initialize backend->component and topology, etc.\n * The caller will then modify whatever needed, and call hwloc_backend_enable().\n */\nHWLOC_DECLSPEC struct hwloc_backend * hwloc_backend_alloc(struct hwloc_disc_component *component);\n\n/** \\brief Enable a previously allocated and setup backend. */\nHWLOC_DECLSPEC int hwloc_backend_enable(struct hwloc_topology *topology, struct hwloc_backend *backend);\n\n/** @} */\n\n\n\n\n/** \\defgroup hwlocality_generic_components Components and Plugins: Generic components\n * @{\n */\n\n/** \\brief Generic component type */\ntypedef enum hwloc_component_type_e {\n  /** \\brief The data field must point to a struct hwloc_disc_component. */\n  HWLOC_COMPONENT_TYPE_DISC,\n\n  /** \\brief The data field must point to a struct hwloc_xml_component. */\n  HWLOC_COMPONENT_TYPE_XML\n} hwloc_component_type_t;\n\n/** \\brief Generic component structure\n *\n * Generic components structure, either statically listed by configure in static-components.h\n * or dynamically loaded as a plugin.\n */\nstruct hwloc_component {\n  /** \\brief Component ABI version, set to ::HWLOC_COMPONENT_ABI */\n  unsigned abi;\n\n  /** \\brief Process-wide component initialization callback.\n   *\n   * This optional callback is called when the component is registered\n   * to the hwloc core (after loading the plugin).\n   *\n   * When the component is built as a plugin, this callback\n   * should call hwloc_check_plugin_namespace()\n   * and return an negative error code on error.\n   *\n   * \\p flags is always 0 for now.\n   *\n   * \\return 0 on success, or a negative code on error.\n   *\n   * \\note If the component uses ltdl for loading its own plugins,\n   * it should load/unload them only in init() and finalize(),\n   * to avoid race conditions with hwloc's use of ltdl.\n   */\n  int (*init)(unsigned long flags);\n\n  /** \\brief Process-wide component termination callback.\n   *\n   * This optional callback is called after unregistering the component\n   * from the hwloc core (before unloading the plugin).\n   *\n   * \\p flags is always 0 for now.\n   *\n   * \\note If the component uses ltdl for loading its own plugins,\n   * it should load/unload them only in init() and finalize(),\n   * to avoid race conditions with hwloc's use of ltdl.\n   */\n  void (*finalize)(unsigned long flags);\n\n  /** \\brief Component type */\n  hwloc_component_type_t type;\n\n  /** \\brief Component flags, unused for now */\n  unsigned long flags;\n\n  /** \\brief Component data, pointing to a struct hwloc_disc_component or struct hwloc_xml_component. */\n  void * data;\n};\n\n/** @} */\n\n\n\n\n/** \\defgroup hwlocality_components_core_funcs Components and Plugins: Core functions to be used by components\n * @{\n */\n\n/** \\brief Add an object to the topology.\n *\n * It is sorted along the tree of other objects according to the inclusion of\n * cpusets, to eventually be added as a child of the smallest object including\n * this object.\n *\n * If the cpuset is empty, the type of the object (and maybe some attributes)\n * must be enough to find where to insert the object. This is especially true\n * for NUMA nodes with memory and no CPUs.\n *\n * The given object should not have children.\n *\n * This shall only be called before levels are built.\n *\n * In case of error, hwloc_report_os_error() is called.\n *\n * The caller should check whether the object type is filtered-out before calling this function.\n *\n * The topology cpuset/nodesets will be enlarged to include the object sets.\n *\n * Returns the object on success.\n * Returns NULL and frees obj on error.\n * Returns another object and frees obj if it was merged with an identical pre-existing object.\n */\nHWLOC_DECLSPEC struct hwloc_obj *hwloc_insert_object_by_cpuset(struct hwloc_topology *topology, hwloc_obj_t obj);\n\n/** \\brief Type of error callbacks during object insertion */\ntypedef void (*hwloc_report_error_t)(const char * msg, int line);\n/** \\brief Report an insertion error from a backend */\nHWLOC_DECLSPEC void hwloc_report_os_error(const char * msg, int line);\n/** \\brief Check whether insertion errors are hidden */\nHWLOC_DECLSPEC int hwloc_hide_errors(void);\n\n/** \\brief Add an object to the topology and specify which error callback to use.\n *\n * This function is similar to hwloc_insert_object_by_cpuset() but it allows specifying\n * where to start insertion from (if \\p root is NULL, the topology root object is used),\n * and specifying the error callback.\n */\nHWLOC_DECLSPEC struct hwloc_obj *hwloc__insert_object_by_cpuset(struct hwloc_topology *topology, hwloc_obj_t root, hwloc_obj_t obj, hwloc_report_error_t report_error);\n\n/** \\brief Insert an object somewhere in the topology.\n *\n * It is added as the last child of the given parent.\n * The cpuset is completely ignored, so strange objects such as I/O devices should\n * preferably be inserted with this.\n *\n * When used for \"normal\" children with cpusets (when importing from XML\n * when duplicating a topology), the caller should make sure that:\n * - children are inserted in order,\n * - children cpusets do not intersect.\n *\n * The given object may have normal, I/O or Misc children, as long as they are in order as well.\n * These children must have valid parent and next_sibling pointers.\n *\n * The caller should check whether the object type is filtered-out before calling this function.\n */\nHWLOC_DECLSPEC void hwloc_insert_object_by_parent(struct hwloc_topology *topology, hwloc_obj_t parent, hwloc_obj_t obj);\n\n/** \\brief Allocate and initialize an object of the given type and physical index.\n *\n * If \\p os_index is unknown or irrelevant, use \\c HWLOC_UNKNOWN_INDEX.\n */\nHWLOC_DECLSPEC hwloc_obj_t hwloc_alloc_setup_object(hwloc_topology_t topology, hwloc_obj_type_t type, unsigned os_index);\n\n/** \\brief Setup object cpusets/nodesets by OR'ing its children.\n *\n * Used when adding an object late in the topology.\n * Will update the new object by OR'ing all its new children sets.\n *\n * Used when PCI backend adds a hostbridge parent, when distances\n * add a new Group, etc.\n */\nHWLOC_DECLSPEC int hwloc_obj_add_children_sets(hwloc_obj_t obj);\n\n/** \\brief Request a reconnection of children and levels in the topology.\n *\n * May be used by backends during discovery if they need arrays or lists\n * of object within levels or children to be fully connected.\n *\n * \\p flags is currently unused, must 0.\n */\nHWLOC_DECLSPEC int hwloc_topology_reconnect(hwloc_topology_t topology, unsigned long flags __hwloc_attribute_unused);\n\n/** \\brief Make sure that plugins can lookup core symbols.\n *\n * This is a sanity check to avoid lazy-lookup failures when libhwloc\n * is loaded within a plugin, and later tries to load its own plugins.\n * This may fail (and abort the program) if libhwloc symbols are in a\n * private namespace.\n *\n * \\return 0 on success.\n * \\return -1 if the plugin cannot be successfully loaded. The caller\n * plugin init() callback should return a negative error code as well.\n *\n * Plugins should call this function in their init() callback to avoid\n * later crashes if lazy symbol resolution is used by the upper layer that\n * loaded hwloc (e.g. OpenCL implementations using dlopen with RTLD_LAZY).\n *\n * \\note The build system must define HWLOC_INSIDE_PLUGIN if and only if\n * building the caller as a plugin.\n *\n * \\note This function should remain inline so plugins can call it even\n * when they cannot find libhwloc symbols.\n */\nstatic __hwloc_inline int\nhwloc_plugin_check_namespace(const char *pluginname __hwloc_attribute_unused, const char *symbol __hwloc_attribute_unused)\n{\n#ifdef HWLOC_INSIDE_PLUGIN\n  lt_dlhandle handle;\n  void *sym;\n  handle = lt_dlopen(NULL);\n  if (!handle)\n    /* cannot check, assume things will work */\n    return 0;\n  sym = lt_dlsym(handle, symbol);\n  lt_dlclose(handle);\n  if (!sym) {\n    static int verboseenv_checked = 0;\n    static int verboseenv_value = 0;\n    if (!verboseenv_checked) {\n      const char *verboseenv = getenv(\"HWLOC_PLUGINS_VERBOSE\");\n      verboseenv_value = verboseenv ? atoi(verboseenv) : 0;\n      verboseenv_checked = 1;\n    }\n    if (verboseenv_value)\n      fprintf(stderr, \"Plugin `%s' disabling itself because it cannot find the `%s' core symbol.\\n\",\n\t      pluginname, symbol);\n    return -1;\n  }\n#endif /* HWLOC_INSIDE_PLUGIN */\n  return 0;\n}\n\n/** @} */\n\n\n\n\n/** \\defgroup hwlocality_components_filtering Components and Plugins: Filtering objects\n * @{\n */\n\n/** \\brief Check whether the given PCI device classid is important.\n *\n * \\return 1 if important, 0 otherwise.\n */\nstatic __hwloc_inline int\nhwloc_filter_check_pcidev_subtype_important(unsigned classid)\n{\n  unsigned baseclass = classid >> 8;\n  return (baseclass == 0x03 /* PCI_BASE_CLASS_DISPLAY */\n\t  || baseclass == 0x02 /* PCI_BASE_CLASS_NETWORK */\n\t  || baseclass == 0x01 /* PCI_BASE_CLASS_STORAGE */\n\t  || baseclass == 0x0b /* PCI_BASE_CLASS_PROCESSOR */\n\t  || classid == 0x0c04 /* PCI_CLASS_SERIAL_FIBER */\n\t  || classid == 0x0c06 /* PCI_CLASS_SERIAL_INFINIBAND */\n\t  || baseclass == 0x12 /* Processing Accelerators */);\n}\n\n/** \\brief Check whether the given OS device subtype is important.\n *\n * \\return 1 if important, 0 otherwise.\n */\nstatic __hwloc_inline int\nhwloc_filter_check_osdev_subtype_important(hwloc_obj_osdev_type_t subtype)\n{\n  return (subtype != HWLOC_OBJ_OSDEV_DMA);\n}\n\n/** \\brief Check whether a non-I/O object type should be filtered-out.\n *\n * Cannot be used for I/O objects.\n *\n * \\return 1 if the object type should be kept, 0 otherwise.\n */\nstatic __hwloc_inline int\nhwloc_filter_check_keep_object_type(hwloc_topology_t topology, hwloc_obj_type_t type)\n{\n  enum hwloc_type_filter_e filter = HWLOC_TYPE_FILTER_KEEP_NONE;\n  hwloc_topology_get_type_filter(topology, type, &filter);\n  assert(filter != HWLOC_TYPE_FILTER_KEEP_IMPORTANT); /* IMPORTANT only used for I/O */\n  return filter == HWLOC_TYPE_FILTER_KEEP_NONE ? 0 : 1;\n}\n\n/** \\brief Check whether the given object should be filtered-out.\n *\n * \\return 1 if the object type should be kept, 0 otherwise.\n */\nstatic __hwloc_inline int\nhwloc_filter_check_keep_object(hwloc_topology_t topology, hwloc_obj_t obj)\n{\n  hwloc_obj_type_t type = obj->type;\n  enum hwloc_type_filter_e filter = HWLOC_TYPE_FILTER_KEEP_NONE;\n  hwloc_topology_get_type_filter(topology, type, &filter);\n  if (filter == HWLOC_TYPE_FILTER_KEEP_NONE)\n    return 0;\n  if (filter == HWLOC_TYPE_FILTER_KEEP_IMPORTANT) {\n    if (type == HWLOC_OBJ_PCI_DEVICE)\n      return hwloc_filter_check_pcidev_subtype_important(obj->attr->pcidev.class_id);\n    if (type == HWLOC_OBJ_OS_DEVICE)\n      return hwloc_filter_check_osdev_subtype_important(obj->attr->osdev.type);\n  }\n  return 1;\n}\n\n/** @} */\n\n\n\n\n/** \\defgroup hwlocality_components_pcidisc Components and Plugins: helpers for PCI discovery\n * @{\n */\n\n/** \\brief Return the offset of the given capability in the PCI config space buffer\n *\n * This function requires a 256-bytes config space. Unknown/unavailable bytes should be set to 0xff.\n */\nHWLOC_DECLSPEC unsigned hwloc_pcidisc_find_cap(const unsigned char *config, unsigned cap);\n\n/** \\brief Fill linkspeed by reading the PCI config space where PCI_CAP_ID_EXP is at position offset.\n *\n * Needs 20 bytes of EXP capability block starting at offset in the config space\n * for registers up to link status.\n */\nHWLOC_DECLSPEC int hwloc_pcidisc_find_linkspeed(const unsigned char *config, unsigned offset, float *linkspeed);\n\n/** \\brief Return the hwloc object type (PCI device or Bridge) for the given class and configuration space.\n *\n * This function requires 16 bytes of common configuration header at the beginning of config.\n */\nHWLOC_DECLSPEC hwloc_obj_type_t hwloc_pcidisc_check_bridge_type(unsigned device_class, const unsigned char *config);\n\n/** \\brief Fills the attributes of the given PCI bridge using the given PCI config space.\n *\n * This function requires 32 bytes of common configuration header at the beginning of config.\n *\n * Returns -1 and destroys /p obj if bridge fields are invalid.\n */\nHWLOC_DECLSPEC int hwloc_pcidisc_setup_bridge_attr(hwloc_obj_t obj, const unsigned char *config);\n\n/** \\brief Insert a PCI object in the given PCI tree by looking at PCI bus IDs.\n *\n * If \\p treep points to \\c NULL, the new object is inserted there.\n */\nHWLOC_DECLSPEC void hwloc_pcidisc_tree_insert_by_busid(struct hwloc_obj **treep, struct hwloc_obj *obj);\n\n/** \\brief Add some hostbridges on top of the given tree of PCI objects and attach them to the topology.\n *\n * For now, they will be attached to the root object. The core will move them to their actual PCI\n * locality using hwloc_pci_belowroot_apply_locality() at the end of the discovery.\n *\n * In the meantime, other backends lookup PCI objects or localities (for instance to attach OS devices)\n * by using hwloc_pcidisc_find_by_busid() or hwloc_pcidisc_find_busid_parent().\n */\nHWLOC_DECLSPEC int hwloc_pcidisc_tree_attach(struct hwloc_topology *topology, struct hwloc_obj *tree);\n\n/** @} */\n\n\n\n\n/** \\defgroup hwlocality_components_pcifind Components and Plugins: finding PCI objects during other discoveries\n * @{\n */\n\n/** \\brief Find the PCI object that matches the bus ID.\n *\n * To be used after a PCI backend added PCI devices with hwloc_pcidisc_tree_attach()\n * and before the core moves them to their actual location with hwloc_pci_belowroot_apply_locality().\n *\n * If no exactly matching object is found, return the container bridge if any, or NULL.\n *\n * On failure, it may be possible to find the PCI locality (instead of the PCI device)\n * by calling hwloc_pcidisc_find_busid_parent().\n *\n * \\note This is semantically identical to hwloc_get_pcidev_by_busid() which only works\n * after the topology is fully loaded.\n */\nHWLOC_DECLSPEC struct hwloc_obj * hwloc_pcidisc_find_by_busid(struct hwloc_topology *topology, unsigned domain, unsigned bus, unsigned dev, unsigned func);\n\n/** \\brief Find the normal parent of a PCI bus ID.\n *\n * Look at PCI affinity to find out where the given PCI bus ID should be attached.\n *\n * This function should be used to attach an I/O device directly under a normal\n * (non-I/O) object, instead of below a PCI object.\n * It is usually used by backends when hwloc_pcidisc_find_by_busid() failed\n * to find the hwloc object corresponding to this bus ID, for instance because\n * PCI discovery is not supported on this platform.\n */\nHWLOC_DECLSPEC struct hwloc_obj * hwloc_pcidisc_find_busid_parent(struct hwloc_topology *topology, unsigned domain, unsigned bus, unsigned dev, unsigned func);\n\n/** @} */\n\n\n\n\n#endif /* HWLOC_PLUGINS_H */\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/hwloc/components.c": "/*\n * Copyright \u00a9 2009-2017 Inria.  All rights reserved.\n * Copyright \u00a9 2012 Universit\u00e9 Bordeaux\n * See COPYING in top-level directory.\n */\n\n#include <private/autogen/config.h>\n#include <hwloc.h>\n#include <private/private.h>\n#include <private/xml.h>\n#include <private/misc.h>\n\n#define HWLOC_COMPONENT_STOP_NAME \"stop\"\n#define HWLOC_COMPONENT_EXCLUDE_CHAR '-'\n#define HWLOC_COMPONENT_SEPS \",\"\n\n/* list of all registered discovery components, sorted by priority, higher priority first.\n * noos is last because its priority is 0.\n * others' priority is 10.\n */\nstatic struct hwloc_disc_component * hwloc_disc_components = NULL;\n\nstatic unsigned hwloc_components_users = 0; /* first one initializes, last ones destroys */\n\nstatic int hwloc_components_verbose = 0;\n#ifdef HWLOC_HAVE_PLUGINS\nstatic int hwloc_plugins_verbose = 0;\nstatic const char * hwloc_plugins_blacklist = NULL;\n#endif\n\n/* hwloc_components_mutex serializes:\n * - loading/unloading plugins, and modifications of the hwloc_plugins list\n * - calls to ltdl, including in hwloc_check_plugin_namespace()\n * - registration of components with hwloc_disc_component_register()\n *   and hwloc_xml_callbacks_register()\n */\n#ifdef HWLOC_WIN_SYS\n/* Basic mutex on top of InterlockedCompareExchange() on windows,\n * Far from perfect, but easy to maintain, and way enough given that this code will never be needed for real. */\n#include <windows.h>\nstatic LONG hwloc_components_mutex = 0;\n#define HWLOC_COMPONENTS_LOCK() do {\t\t\t\t\t\t\\\n  while (InterlockedCompareExchange(&hwloc_components_mutex, 1, 0) != 0)\t\\\n    SwitchToThread();\t\t\t\t\t\t\t\t\\\n} while (0)\n#define HWLOC_COMPONENTS_UNLOCK() do {\t\t\t\t\t\t\\\n  assert(hwloc_components_mutex == 1);\t\t\t\t\t\t\\\n  hwloc_components_mutex = 0;\t\t\t\t\t\t\t\\\n} while (0)\n\n#elif defined HWLOC_HAVE_PTHREAD_MUTEX\n/* pthread mutex if available (except on windows) */\n#include <pthread.h>\nstatic pthread_mutex_t hwloc_components_mutex = PTHREAD_MUTEX_INITIALIZER;\n#define HWLOC_COMPONENTS_LOCK() pthread_mutex_lock(&hwloc_components_mutex)\n#define HWLOC_COMPONENTS_UNLOCK() pthread_mutex_unlock(&hwloc_components_mutex)\n\n#else /* HWLOC_WIN_SYS || HWLOC_HAVE_PTHREAD_MUTEX */\n#error No mutex implementation available\n#endif\n\n\n#ifdef HWLOC_HAVE_PLUGINS\n\n#include <ltdl.h>\n\n/* array of pointers to dynamically loaded plugins */\nstatic struct hwloc__plugin_desc {\n  char *name;\n  struct hwloc_component *component;\n  char *filename;\n  lt_dlhandle handle;\n  struct hwloc__plugin_desc *next;\n} *hwloc_plugins = NULL;\n\nstatic int\nhwloc__dlforeach_cb(const char *filename, void *_data __hwloc_attribute_unused)\n{\n  const char *basename;\n  lt_dlhandle handle;\n  struct hwloc_component *component;\n  struct hwloc__plugin_desc *desc, **prevdesc;\n\n  if (hwloc_plugins_verbose)\n    fprintf(stderr, \"Plugin dlforeach found `%s'\\n\", filename);\n\n  basename = strrchr(filename, '/');\n  if (!basename)\n    basename = filename;\n  else\n    basename++;\n\n  if (hwloc_plugins_blacklist && strstr(hwloc_plugins_blacklist, basename)) {\n    if (hwloc_plugins_verbose)\n      fprintf(stderr, \"Plugin `%s' is blacklisted in the environment\\n\", basename);\n    goto out;\n  }\n\n  /* dlopen and get the component structure */\n  handle = lt_dlopenext(filename);\n  if (!handle) {\n    if (hwloc_plugins_verbose)\n      fprintf(stderr, \"Failed to load plugin: %s\\n\", lt_dlerror());\n    goto out;\n  }\n\n{\n  char componentsymbolname[strlen(basename)+10+1];\n  sprintf(componentsymbolname, \"%s_component\", basename);\n  component = lt_dlsym(handle, componentsymbolname);\n  if (!component) {\n    if (hwloc_plugins_verbose)\n      fprintf(stderr, \"Failed to find component symbol `%s'\\n\",\n\t      componentsymbolname);\n    goto out_with_handle;\n  }\n  if (component->abi != HWLOC_COMPONENT_ABI) {\n    if (hwloc_plugins_verbose)\n      fprintf(stderr, \"Plugin symbol ABI %u instead of %d\\n\",\n\t      component->abi, HWLOC_COMPONENT_ABI);\n    goto out_with_handle;\n  }\n  if (hwloc_plugins_verbose)\n    fprintf(stderr, \"Plugin contains expected symbol `%s'\\n\",\n\t    componentsymbolname);\n}\n\n  if (HWLOC_COMPONENT_TYPE_DISC == component->type) {\n    if (strncmp(basename, \"hwloc_\", 6)) {\n      if (hwloc_plugins_verbose)\n\tfprintf(stderr, \"Plugin name `%s' doesn't match its type DISCOVERY\\n\", basename);\n      goto out_with_handle;\n    }\n  } else if (HWLOC_COMPONENT_TYPE_XML == component->type) {\n    if (strncmp(basename, \"hwloc_xml_\", 10)) {\n      if (hwloc_plugins_verbose)\n\tfprintf(stderr, \"Plugin name `%s' doesn't match its type XML\\n\", basename);\n      goto out_with_handle;\n    }\n  } else {\n    if (hwloc_plugins_verbose)\n      fprintf(stderr, \"Plugin name `%s' has invalid type %u\\n\",\n\t      basename, (unsigned) component->type);\n    goto out_with_handle;\n  }\n\n  /* allocate a plugin_desc and queue it */\n  desc = malloc(sizeof(*desc));\n  if (!desc)\n    goto out_with_handle;\n  desc->name = strdup(basename);\n  desc->filename = strdup(filename);\n  desc->component = component;\n  desc->handle = handle;\n  desc->next = NULL;\n  if (hwloc_plugins_verbose)\n    fprintf(stderr, \"Plugin descriptor `%s' ready\\n\", basename);\n\n  /* append to the list */\n  prevdesc = &hwloc_plugins;\n  while (*prevdesc)\n    prevdesc = &((*prevdesc)->next);\n  *prevdesc = desc;\n  if (hwloc_plugins_verbose)\n    fprintf(stderr, \"Plugin descriptor `%s' queued\\n\", basename);\n  return 0;\n\n out_with_handle:\n  lt_dlclose(handle);\n out:\n  return 0;\n}\n\nstatic void\nhwloc_plugins_exit(void)\n{\n  struct hwloc__plugin_desc *desc, *next;\n\n  if (hwloc_plugins_verbose)\n    fprintf(stderr, \"Closing all plugins\\n\");\n\n  desc = hwloc_plugins;\n  while (desc) {\n    next = desc->next;\n    lt_dlclose(desc->handle);\n    free(desc->name);\n    free(desc->filename);\n    free(desc);\n    desc = next;\n  }\n  hwloc_plugins = NULL;\n\n  lt_dlexit();\n}\n\nstatic int\nhwloc_plugins_init(void)\n{\n  const char *verboseenv;\n  const char *path = HWLOC_PLUGINS_PATH;\n  const char *env;\n  int err;\n\n  verboseenv = getenv(\"HWLOC_PLUGINS_VERBOSE\");\n  hwloc_plugins_verbose = verboseenv ? atoi(verboseenv) : 0;\n\n  hwloc_plugins_blacklist = getenv(\"HWLOC_PLUGINS_BLACKLIST\");\n\n  err = lt_dlinit();\n  if (err)\n    goto out;\n\n  env = getenv(\"HWLOC_PLUGINS_PATH\");\n  if (env)\n    path = env;\n\n  hwloc_plugins = NULL;\n\n  if (hwloc_plugins_verbose)\n    fprintf(stderr, \"Starting plugin dlforeach in %s\\n\", path);\n  err = lt_dlforeachfile(path, hwloc__dlforeach_cb, NULL);\n  if (err)\n    goto out_with_init;\n\n  return 0;\n\n out_with_init:\n  hwloc_plugins_exit();\n out:\n  return -1;\n}\n\n#endif /* HWLOC_HAVE_PLUGINS */\n\nstatic const char *\nhwloc_disc_component_type_string(hwloc_disc_component_type_t type)\n{\n  switch (type) {\n  case HWLOC_DISC_COMPONENT_TYPE_CPU: return \"cpu\";\n  case HWLOC_DISC_COMPONENT_TYPE_GLOBAL: return \"global\";\n  case HWLOC_DISC_COMPONENT_TYPE_MISC: return \"misc\";\n  default: return \"**unknown**\";\n  }\n}\n\nstatic int\nhwloc_disc_component_register(struct hwloc_disc_component *component,\n\t\t\t      const char *filename)\n{\n  struct hwloc_disc_component **prev;\n\n  /* check that the component name is valid */\n  if (!strcmp(component->name, HWLOC_COMPONENT_STOP_NAME)) {\n    if (hwloc_components_verbose)\n      fprintf(stderr, \"Cannot register discovery component with reserved name `\" HWLOC_COMPONENT_STOP_NAME \"'\\n\");\n    return -1;\n  }\n  if (strchr(component->name, HWLOC_COMPONENT_EXCLUDE_CHAR)\n      || strcspn(component->name, HWLOC_COMPONENT_SEPS) != strlen(component->name)) {\n    if (hwloc_components_verbose)\n      fprintf(stderr, \"Cannot register discovery component with name `%s' containing reserved characters `%c\" HWLOC_COMPONENT_SEPS \"'\\n\",\n\t      component->name, HWLOC_COMPONENT_EXCLUDE_CHAR);\n    return -1;\n  }\n  /* check that the component type is valid */\n  switch ((unsigned) component->type) {\n  case HWLOC_DISC_COMPONENT_TYPE_CPU:\n  case HWLOC_DISC_COMPONENT_TYPE_GLOBAL:\n  case HWLOC_DISC_COMPONENT_TYPE_MISC:\n    break;\n  default:\n    fprintf(stderr, \"Cannot register discovery component `%s' with unknown type %u\\n\",\n\t    component->name, (unsigned) component->type);\n    return -1;\n  }\n\n  prev = &hwloc_disc_components;\n  while (NULL != *prev) {\n    if (!strcmp((*prev)->name, component->name)) {\n      /* if two components have the same name, only keep the highest priority one */\n      if ((*prev)->priority < component->priority) {\n\t/* drop the existing component */\n\tif (hwloc_components_verbose)\n\t  fprintf(stderr, \"Dropping previously registered discovery component `%s', priority %u lower than new one %u\\n\",\n\t\t  (*prev)->name, (*prev)->priority, component->priority);\n\t*prev = (*prev)->next;\n      } else {\n\t/* drop the new one */\n\tif (hwloc_components_verbose)\n\t  fprintf(stderr, \"Ignoring new discovery component `%s', priority %u lower than previously registered one %u\\n\",\n\t\t  component->name, component->priority, (*prev)->priority);\n\treturn -1;\n      }\n    }\n    prev = &((*prev)->next);\n  }\n  if (hwloc_components_verbose)\n    fprintf(stderr, \"Registered %s discovery component `%s' with priority %u (%s%s)\\n\",\n\t    hwloc_disc_component_type_string(component->type), component->name, component->priority,\n\t    filename ? \"from plugin \" : \"statically build\", filename ? filename : \"\");\n\n  prev = &hwloc_disc_components;\n  while (NULL != *prev) {\n    if ((*prev)->priority < component->priority)\n      break;\n    prev = &((*prev)->next);\n  }\n  component->next = *prev;\n  *prev = component;\n  return 0;\n}\n\n#include <static-components.h>\n\nstatic void (**hwloc_component_finalize_cbs)(unsigned long);\nstatic unsigned hwloc_component_finalize_cb_count;\n\nvoid\nhwloc_components_init(void)\n{\n#ifdef HWLOC_HAVE_PLUGINS\n  struct hwloc__plugin_desc *desc;\n#endif\n  const char *verboseenv;\n  unsigned i;\n\n  HWLOC_COMPONENTS_LOCK();\n  assert((unsigned) -1 != hwloc_components_users);\n  if (0 != hwloc_components_users++) {\n    HWLOC_COMPONENTS_UNLOCK();\n    return;\n  }\n\n  verboseenv = getenv(\"HWLOC_COMPONENTS_VERBOSE\");\n  hwloc_components_verbose = verboseenv ? atoi(verboseenv) : 0;\n\n#ifdef HWLOC_HAVE_PLUGINS\n  hwloc_plugins_init();\n#endif\n\n  hwloc_component_finalize_cbs = NULL;\n  hwloc_component_finalize_cb_count = 0;\n  /* count the max number of finalize callbacks */\n  for(i=0; NULL != hwloc_static_components[i]; i++)\n    hwloc_component_finalize_cb_count++;\n#ifdef HWLOC_HAVE_PLUGINS\n  for(desc = hwloc_plugins; NULL != desc; desc = desc->next)\n    hwloc_component_finalize_cb_count++;\n#endif\n  if (hwloc_component_finalize_cb_count) {\n    hwloc_component_finalize_cbs = calloc(hwloc_component_finalize_cb_count,\n\t\t\t\t\t  sizeof(*hwloc_component_finalize_cbs));\n    assert(hwloc_component_finalize_cbs);\n    /* forget that max number and recompute the real one below */\n    hwloc_component_finalize_cb_count = 0;\n  }\n\n  /* hwloc_static_components is created by configure in static-components.h */\n  for(i=0; NULL != hwloc_static_components[i]; i++) {\n    if (hwloc_static_components[i]->flags) {\n      fprintf(stderr, \"Ignoring static component with invalid flags %lx\\n\",\n\t      hwloc_static_components[i]->flags);\n      continue;\n    }\n\n    /* initialize the component */\n    if (hwloc_static_components[i]->init && hwloc_static_components[i]->init(0) < 0) {\n      if (hwloc_components_verbose)\n\tfprintf(stderr, \"Ignoring static component, failed to initialize\\n\");\n      continue;\n    }\n    /* queue ->finalize() callback if any */\n    if (hwloc_static_components[i]->finalize)\n      hwloc_component_finalize_cbs[hwloc_component_finalize_cb_count++] = hwloc_static_components[i]->finalize;\n\n    /* register for real now */\n    if (HWLOC_COMPONENT_TYPE_DISC == hwloc_static_components[i]->type)\n      hwloc_disc_component_register(hwloc_static_components[i]->data, NULL);\n    else if (HWLOC_COMPONENT_TYPE_XML == hwloc_static_components[i]->type)\n      hwloc_xml_callbacks_register(hwloc_static_components[i]->data);\n    else\n      assert(0);\n  }\n\n  /* dynamic plugins */\n#ifdef HWLOC_HAVE_PLUGINS\n  for(desc = hwloc_plugins; NULL != desc; desc = desc->next) {\n    if (desc->component->flags) {\n      fprintf(stderr, \"Ignoring plugin `%s' component with invalid flags %lx\\n\",\n\t      desc->name, desc->component->flags);\n      continue;\n    }\n\n    /* initialize the component */\n    if (desc->component->init && desc->component->init(0) < 0) {\n      if (hwloc_components_verbose)\n\tfprintf(stderr, \"Ignoring plugin `%s', failed to initialize\\n\", desc->name);\n      continue;\n    }\n    /* queue ->finalize() callback if any */\n    if (desc->component->finalize)\n      hwloc_component_finalize_cbs[hwloc_component_finalize_cb_count++] = desc->component->finalize;\n\n    /* register for real now */\n    if (HWLOC_COMPONENT_TYPE_DISC == desc->component->type)\n      hwloc_disc_component_register(desc->component->data, desc->filename);\n    else if (HWLOC_COMPONENT_TYPE_XML == desc->component->type)\n      hwloc_xml_callbacks_register(desc->component->data);\n    else\n      assert(0);\n  }\n#endif\n\n  HWLOC_COMPONENTS_UNLOCK();\n}\n\nvoid\nhwloc_backends_init(struct hwloc_topology *topology)\n{\n  topology->backends = NULL;\n  topology->backend_excludes = 0;\n}\n\nstatic struct hwloc_disc_component *\nhwloc_disc_component_find(int type /* hwloc_disc_component_type_t or -1 if any */,\n\t\t\t       const char *name /* name of NULL if any */)\n{\n  struct hwloc_disc_component *comp = hwloc_disc_components;\n  while (NULL != comp) {\n    if ((-1 == type || type == (int) comp->type)\n       && (NULL == name || !strcmp(name, comp->name)))\n      return comp;\n    comp = comp->next;\n  }\n  return NULL;\n}\n\n/* used by set_xml(), set_synthetic(), ... environment variables, ... to force the first backend */\nint\nhwloc_disc_component_force_enable(struct hwloc_topology *topology,\n\t\t\t\t  int envvar_forced,\n\t\t\t\t  int type, const char *name,\n\t\t\t\t  const void *data1, const void *data2, const void *data3)\n{\n  struct hwloc_disc_component *comp;\n  struct hwloc_backend *backend;\n\n  if (topology->is_loaded) {\n    errno = EBUSY;\n    return -1;\n  }\n\n  comp = hwloc_disc_component_find(type, name);\n  if (!comp) {\n    errno = ENOSYS;\n    return -1;\n  }\n\n  backend = comp->instantiate(comp, data1, data2, data3);\n  if (backend) {\n    backend->envvar_forced = envvar_forced;\n    if (topology->backends)\n      hwloc_backends_disable_all(topology);\n    return hwloc_backend_enable(topology, backend);\n  } else\n    return -1;\n}\n\nstatic int\nhwloc_disc_component_try_enable(struct hwloc_topology *topology,\n\t\t\t\tstruct hwloc_disc_component *comp,\n\t\t\t\tconst char *comparg,\n\t\t\t\tint envvar_forced)\n{\n  struct hwloc_backend *backend;\n\n  if (topology->backend_excludes & comp->type) {\n    if (hwloc_components_verbose)\n      /* do not warn if envvar_forced since system-wide HWLOC_COMPONENTS must be silently ignored after set_xml() etc.\n       */\n      fprintf(stderr, \"Excluding %s discovery component `%s', conflicts with excludes 0x%x\\n\",\n\t      hwloc_disc_component_type_string(comp->type), comp->name, topology->backend_excludes);\n    return -1;\n  }\n\n  backend = comp->instantiate(comp, comparg, NULL, NULL);\n  if (!backend) {\n    if (hwloc_components_verbose || envvar_forced)\n      fprintf(stderr, \"Failed to instantiate discovery component `%s'\\n\", comp->name);\n    return -1;\n  }\n\n  backend->envvar_forced = envvar_forced;\n  return hwloc_backend_enable(topology, backend);\n}\n\nvoid\nhwloc_disc_components_enable_others(struct hwloc_topology *topology)\n{\n  struct hwloc_disc_component *comp;\n  struct hwloc_backend *backend;\n  int tryall = 1;\n  const char *_env;\n  char *env; /* we'll to modify the env value, so duplicate it */\n\n  _env = getenv(\"HWLOC_COMPONENTS\");\n  env = _env ? strdup(_env) : NULL;\n\n  /* enable explicitly listed components */\n  if (env) {\n    char *curenv = env;\n    size_t s;\n\n    while (*curenv) {\n      s = strcspn(curenv, HWLOC_COMPONENT_SEPS);\n      if (s) {\n\tchar c;\n\n\t/* replace linuxpci with linuxio for backward compatibility with pre-v2.0 */\n\tif (!strncmp(curenv, \"linuxpci\", 8) && s == 8) {\n\t  curenv[5] = 'i';\n\t  curenv[6] = 'o';\n\t  curenv[7] = *HWLOC_COMPONENT_SEPS;\n\t} else if (curenv[0] == HWLOC_COMPONENT_EXCLUDE_CHAR && !strncmp(curenv+1, \"linuxpci\", 8) && s == 9) {\n\t  curenv[6] = 'i';\n\t  curenv[7] = 'o';\n\t  curenv[8] = *HWLOC_COMPONENT_SEPS;\n\t  /* skip this name, it's a negated one */\n\t  goto nextname;\n\t}\n\n\tif (curenv[0] == HWLOC_COMPONENT_EXCLUDE_CHAR)\n\t  goto nextname;\n\n\tif (!strncmp(curenv, HWLOC_COMPONENT_STOP_NAME, s)) {\n\t  tryall = 0;\n\t  break;\n\t}\n\n\t/* save the last char and replace with \\0 */\n\tc = curenv[s];\n\tcurenv[s] = '\\0';\n\n\tcomp = hwloc_disc_component_find(-1, curenv);\n\tif (comp) {\n\t  hwloc_disc_component_try_enable(topology, comp, NULL, 1 /* envvar forced */);\n\t} else {\n\t  fprintf(stderr, \"Cannot find discovery component `%s'\\n\", curenv);\n\t}\n\n\t/* restore chars (the second loop below needs env to be unmodified) */\n\tcurenv[s] = c;\n      }\n\nnextname:\n      curenv += s;\n      if (*curenv)\n\t/* Skip comma */\n\tcurenv++;\n    }\n  }\n\n  /* env is still the same, the above loop didn't modify it */\n\n  /* now enable remaining components (except the explicitly '-'-listed ones) */\n  if (tryall) {\n    comp = hwloc_disc_components;\n    while (NULL != comp) {\n      if (!comp->enabled_by_default)\n\tgoto nextcomp;\n      /* check if this component was explicitly excluded in env */\n      if (env) {\n\tchar *curenv = env;\n\twhile (*curenv) {\n\t  size_t s = strcspn(curenv, HWLOC_COMPONENT_SEPS);\n\t  if (curenv[0] == HWLOC_COMPONENT_EXCLUDE_CHAR && !strncmp(curenv+1, comp->name, s-1) && strlen(comp->name) == s-1) {\n\t    if (hwloc_components_verbose)\n\t      fprintf(stderr, \"Excluding %s discovery component `%s' because of HWLOC_COMPONENTS environment variable\\n\",\n\t    hwloc_disc_component_type_string(comp->type), comp->name);\n\t    goto nextcomp;\n\t  }\n\t  curenv += s;\n\t  if (*curenv)\n\t    /* Skip comma */\n\t    curenv++;\n\t}\n      }\n      hwloc_disc_component_try_enable(topology, comp, NULL, 0 /* defaults, not envvar forced */);\nnextcomp:\n      comp = comp->next;\n    }\n  }\n\n  if (hwloc_components_verbose) {\n    /* print a summary */\n    int first = 1;\n    backend = topology->backends;\n    fprintf(stderr, \"Final list of enabled discovery components: \");\n    while (backend != NULL) {\n      fprintf(stderr, \"%s%s\", first ? \"\" : \",\", backend->component->name);\n      backend = backend->next;\n      first = 0;\n    }\n    fprintf(stderr, \"\\n\");\n  }\n\n  free(env);\n}\n\nvoid\nhwloc_components_fini(void)\n{\n  unsigned i;\n\n  HWLOC_COMPONENTS_LOCK();\n  assert(0 != hwloc_components_users);\n  if (0 != --hwloc_components_users) {\n    HWLOC_COMPONENTS_UNLOCK();\n    return;\n  }\n\n  for(i=0; i<hwloc_component_finalize_cb_count; i++)\n    hwloc_component_finalize_cbs[hwloc_component_finalize_cb_count-i-1](0);\n  free(hwloc_component_finalize_cbs);\n  hwloc_component_finalize_cbs = NULL;\n  hwloc_component_finalize_cb_count = 0;\n\n  /* no need to unlink/free the list of components, they'll be unloaded below */\n\n  hwloc_disc_components = NULL;\n  hwloc_xml_callbacks_reset();\n\n#ifdef HWLOC_HAVE_PLUGINS\n  hwloc_plugins_exit();\n#endif\n\n  HWLOC_COMPONENTS_UNLOCK();\n}\n\nstruct hwloc_backend *\nhwloc_backend_alloc(struct hwloc_disc_component *component)\n{\n  struct hwloc_backend * backend = malloc(sizeof(*backend));\n  if (!backend) {\n    errno = ENOMEM;\n    return NULL;\n  }\n  backend->component = component;\n  backend->flags = 0;\n  backend->discover = NULL;\n  backend->get_pci_busid_cpuset = NULL;\n  backend->disable = NULL;\n  backend->is_thissystem = -1;\n  backend->next = NULL;\n  backend->envvar_forced = 0;\n  return backend;\n}\n\nstatic void\nhwloc_backend_disable(struct hwloc_backend *backend)\n{\n  if (backend->disable)\n    backend->disable(backend);\n  free(backend);\n}\n\nint\nhwloc_backend_enable(struct hwloc_topology *topology, struct hwloc_backend *backend)\n{\n  struct hwloc_backend **pprev;\n\n  /* check backend flags */\n  if (backend->flags) {\n    fprintf(stderr, \"Cannot enable %s discovery component `%s' with unknown flags %lx\\n\",\n\t    hwloc_disc_component_type_string(backend->component->type), backend->component->name, backend->flags);\n    return -1;\n  }\n\n  /* make sure we didn't already enable this backend, we don't want duplicates */\n  pprev = &topology->backends;\n  while (NULL != *pprev) {\n    if ((*pprev)->component == backend->component) {\n      if (hwloc_components_verbose)\n\tfprintf(stderr, \"Cannot enable %s discovery component `%s' twice\\n\",\n\t\thwloc_disc_component_type_string(backend->component->type), backend->component->name);\n      hwloc_backend_disable(backend);\n      errno = EBUSY;\n      return -1;\n    }\n    pprev = &((*pprev)->next);\n  }\n\n  if (hwloc_components_verbose)\n    fprintf(stderr, \"Enabling %s discovery component `%s'\\n\",\n\t    hwloc_disc_component_type_string(backend->component->type), backend->component->name);\n\n  /* enqueue at the end */\n  pprev = &topology->backends;\n  while (NULL != *pprev)\n    pprev = &((*pprev)->next);\n  backend->next = *pprev;\n  *pprev = backend;\n\n  backend->topology = topology;\n  topology->backend_excludes |= backend->component->excludes;\n  return 0;\n}\n\nvoid\nhwloc_backends_is_thissystem(struct hwloc_topology *topology)\n{\n  struct hwloc_backend *backend;\n  const char *local_env;\n\n  /* Apply is_thissystem topology flag before we enforce envvar backends.\n   * If the application changed the backend with set_foo(),\n   * it may use set_flags() update the is_thissystem flag here.\n   * If it changes the backend with environment variables below,\n   * it may use HWLOC_THISSYSTEM envvar below as well.\n   */\n\n  topology->is_thissystem = 1;\n\n  /* apply thissystem from normally-given backends (envvar_forced=0, either set_foo() or defaults) */\n  backend = topology->backends;\n  while (backend != NULL) {\n    if (backend->envvar_forced == 0 && backend->is_thissystem != -1) {\n      assert(backend->is_thissystem == 0);\n      topology->is_thissystem = 0;\n    }\n    backend = backend->next;\n  }\n\n  /* override set_foo() with flags */\n  if (topology->flags & HWLOC_TOPOLOGY_FLAG_IS_THISSYSTEM)\n    topology->is_thissystem = 1;\n\n  /* now apply envvar-forced backend (envvar_forced=1) */\n  backend = topology->backends;\n  while (backend != NULL) {\n    if (backend->envvar_forced == 1 && backend->is_thissystem != -1) {\n      assert(backend->is_thissystem == 0);\n      topology->is_thissystem = 0;\n    }\n    backend = backend->next;\n  }\n\n  /* override with envvar-given flag */\n  local_env = getenv(\"HWLOC_THISSYSTEM\");\n  if (local_env)\n    topology->is_thissystem = atoi(local_env);\n}\n\nvoid\nhwloc_backends_find_callbacks(struct hwloc_topology *topology)\n{\n  struct hwloc_backend *backend = topology->backends;\n  /* use the first backend's get_pci_busid_cpuset callback */\n  topology->get_pci_busid_cpuset_backend = NULL;\n  while (backend != NULL) {\n    if (backend->get_pci_busid_cpuset) {\n      topology->get_pci_busid_cpuset_backend = backend;\n      return;\n    }\n    backend = backend->next;\n  }\n  return;\n}\n\nvoid\nhwloc_backends_disable_all(struct hwloc_topology *topology)\n{\n  struct hwloc_backend *backend;\n\n  while (NULL != (backend = topology->backends)) {\n    struct hwloc_backend *next = backend->next;\n    if (hwloc_components_verbose)\n      fprintf(stderr, \"Disabling %s discovery component `%s'\\n\",\n\t      hwloc_disc_component_type_string(backend->component->type), backend->component->name);\n    hwloc_backend_disable(backend);\n    topology->backends = next;\n  }\n  topology->backends = NULL;\n  topology->backend_excludes = 0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/ucm/util/replace.h": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n\n#ifndef UCM_UTIL_REPLACE_H_\n#define UCM_UTIL_REPLACE_H_\n\n#include <ucm/bistro/bistro.h>\n#include <ucs/datastruct/list.h>\n#include <ucs/type/status.h>\n#include <pthread.h>\n\nextern pthread_mutex_t ucm_reloc_get_orig_lock;\nextern pthread_t volatile ucm_reloc_get_orig_thread;\n\n/**\n * Define a replacement function to a memory-mapping function call, which calls\n * the event handler, and if event handler returns error code - calls the original\n * function.\n */\n\n/* Due to CUDA API redifinition we have to create proxy macro to eliminate\n * redifinition of internal finction names */\n#define UCM_DEFINE_REPLACE_FUNC(_name, _rettype, _fail_val, ...) \\\n    _UCM_DEFINE_REPLACE_FUNC(ucm_override_##_name, ucm_##_name, _rettype, _fail_val, __VA_ARGS__)\n\n#define _UCM_DEFINE_REPLACE_FUNC(_over_name, _ucm_name, _rettype, _fail_val, ...) \\\n    \\\n    /* Define a symbol which goes to the replacement - in case we are loaded first */ \\\n    _rettype _over_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        _rettype res; \\\n        UCM_BISTRO_PROLOGUE; \\\n        ucm_trace(\"%s()\", __FUNCTION__); \\\n        \\\n        if (ucs_unlikely(ucm_reloc_get_orig_thread == pthread_self())) { \\\n            return (_rettype)_fail_val; \\\n        } \\\n        res = _ucm_name(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n        UCM_BISTRO_EPILOGUE; \\\n        return res; \\\n    }\n\n#define UCM_OVERRIDE_FUNC(_name, _rettype) \\\n    _rettype _name() __attribute__ ((alias (UCS_PP_QUOTE(ucm_override_##_name)))); \\\n\n#define UCM_DEFINE_DLSYM_FUNC(_name, _rettype, _fail_val, ...) \\\n    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name, ucm_override_##_name, \\\n                          _rettype, _fail_val, __VA_ARGS__)\n\n#define _UCM_DEFINE_DLSYM_FUNC(_name, _orig_name, _over_name, _rettype, _fail_val, ...) \\\n    _rettype _over_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)); \\\n    \\\n    /* Call the original function using dlsym(RTLD_NEXT) */ \\\n    _rettype _orig_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        typedef _rettype (*func_ptr_t) (__VA_ARGS__); \\\n        static func_ptr_t orig_func_ptr = NULL; \\\n        \\\n        ucm_trace(\"%s()\", __FUNCTION__); \\\n        \\\n        if (ucs_unlikely(orig_func_ptr == NULL)) { \\\n            pthread_mutex_lock(&ucm_reloc_get_orig_lock); \\\n            ucm_reloc_get_orig_thread = pthread_self(); \\\n            orig_func_ptr = (func_ptr_t)ucm_reloc_get_orig(UCS_PP_QUOTE(_name), \\\n                                                           _over_name); \\\n            ucm_reloc_get_orig_thread = (pthread_t)-1; \\\n            pthread_mutex_unlock(&ucm_reloc_get_orig_lock); \\\n        } \\\n        return orig_func_ptr(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n    }\n\n#define UCM_DEFINE_REPLACE_DLSYM_FUNC(_name, _rettype, _fail_val, ...) \\\n    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name, ucm_override_##_name, \\\n                          _rettype, _fail_val, __VA_ARGS__) \\\n    _UCM_DEFINE_REPLACE_FUNC(ucm_override_##_name, ucm_##_name, \\\n                             _rettype, _fail_val, __VA_ARGS__)\n\n#define UCM_DEFINE_SYSCALL_FUNC(_name, _rettype, _syscall_id, ...) \\\n    /* Call syscall */ \\\n    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        return (_rettype)syscall(_syscall_id, UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n    }\n\n#if UCM_BISTRO_HOOKS\n#  define UCM_DEFINE_SELECT_FUNC(_name, _rettype, _fail_val, _syscall_id, ...) \\\n    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name##_dlsym, ucm_override_##_name, \\\n                          _rettype, _fail_val, __VA_ARGS__) \\\n    UCM_DEFINE_SYSCALL_FUNC(_name##_syscall, _rettype, _syscall_id, __VA_ARGS__) \\\n    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \\\n    { \\\n        return (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_BISTRO) ? \\\n               ucm_orig_##_name##_syscall(UCM_FUNC_PASS_ARGS(__VA_ARGS__)) : \\\n               ucm_orig_##_name##_dlsym(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \\\n    }\n#else\n#  define UCM_DEFINE_SELECT_FUNC(_name, _rettype, _fail_val, _syscall_id, ...) \\\n    UCM_DEFINE_DLSYM_FUNC(_name, _rettype, _fail_val, __VA_ARGS__)\n#endif\n\n/*\n * Define argument list with given types.\n */\n#define UCM_FUNC_DEFINE_ARGS(...) \\\n    UCS_PP_FOREACH_SEP(_UCM_FUNC_ARG_DEFINE, _, \\\n                       UCS_PP_ZIP((UCS_PP_SEQ(UCS_PP_NUM_ARGS(__VA_ARGS__))), \\\n                                  (__VA_ARGS__)))\n\n/*\n * Pass auto-generated arguments to a function call.\n */\n#define UCM_FUNC_PASS_ARGS(...) \\\n    UCS_PP_FOREACH_SEP(_UCM_FUNC_ARG_PASS, _, UCS_PP_SEQ(UCS_PP_NUM_ARGS(__VA_ARGS__)))\n\n\n/*\n * Helpers\n */\n#define _UCM_FUNC_ARG_DEFINE(_, _bundle) \\\n    __UCM_FUNC_ARG_DEFINE(_, UCS_PP_TUPLE_0 _bundle, UCS_PP_TUPLE_1 _bundle)\n#define __UCM_FUNC_ARG_DEFINE(_, _index, _type) \\\n    _type UCS_PP_TOKENPASTE(arg, _index)\n#define _UCM_FUNC_ARG_PASS(_, _index) \\\n    UCS_PP_TOKENPASTE(arg, _index)\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/ucm/util/reloc.h": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n\n#ifndef UCM_UTIL_RELOC_H_\n#define UCM_UTIL_RELOC_H_\n\n#include <ucs/datastruct/list.h>\n#include <ucs/type/status.h>\n#include <ucm/util/log.h>\n#include <dlfcn.h>\n\n\n/**\n * Tracks which symbols need to be patched for currently loaded libraries and\n * for libraries to be loaded in the future. We have the 'list' field so the\n * library could put those on a list without extra memory allocations.\n */\ntypedef struct ucm_reloc_patch {\n    const char       *symbol;\n    void             *value;\n    void             *prev_value;\n    ucs_list_link_t  list;\n    char             **blacklist;\n} ucm_reloc_patch_t;\n\n\n/**\n * Modify process' relocation table.\n *\n * @param [in]  patch     What and how to modify. After this call, the structure\n *                         will be owned by the library and the same patching will\n *                         happen in all objects loaded subsequently.\n */\nucs_status_t ucm_reloc_modify(ucm_reloc_patch_t* patch);\n\n\n/**\n * Get the original implementation of 'symbol', which is not equal to 'replacement'.\n *\n * This function is static to make sure the symbol search is done from the context\n * of the shared object which defines the replacement function.\n * If the replacement function is defined in a loadbale module, the symbols it\n * imports from other libraries may not be available in global scope.\n *\n * @param [in]  symbol       Symbol name.\n * @param [in]  replacement  Symbol replacement, which should be ignored.\n *\n * @return Original function pointer for 'symbol'.\n */\nstatic void* UCS_F_MAYBE_UNUSED\nucm_reloc_get_orig(const char *symbol, void *replacement)\n{\n    const char *error;\n    void *func_ptr;\n\n    func_ptr = dlsym(RTLD_NEXT, symbol);\n    if (func_ptr == NULL) {\n        (void)dlerror();\n        func_ptr = dlsym(RTLD_DEFAULT, symbol);\n        if (func_ptr == replacement) {\n            error = dlerror();\n            ucm_fatal(\"could not find address of original %s(): %s\", symbol,\n                      error ? error : \"Unknown error\");\n        }\n    }\n\n    ucm_debug(\"original %s() is at %p\", symbol, func_ptr);\n    return func_ptr;\n}\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/ucm/util/replace.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <errno.h>\n#include <unistd.h>\n#include <sys/mman.h>\n#include <sys/syscall.h>\n\n#include <ucm/event/event.h>\n#include <ucm/util/log.h>\n#include <ucm/util/reloc.h>\n#include <ucm/util/replace.h>\n#include <ucm/mmap/mmap.h>\n#include <ucs/sys/compiler.h>\n#include <ucs/sys/preprocessor.h>\n\n#ifndef MAP_FAILED\n#define MAP_FAILED ((void*)-1)\n#endif\n\n#ifdef PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP\npthread_mutex_t ucm_reloc_get_orig_lock = PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP;\n#else\npthread_mutex_t ucm_reloc_get_orig_lock;\nstatic void ucm_reloc_get_orig_lock_init(void) __attribute__((constructor(101)));\nstatic void ucm_reloc_get_orig_lock_init(void)\n{\n\tpthread_mutexattr_t attr;\n\n\tpthread_mutexattr_init(&attr);\n\tpthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);\n\tpthread_mutex_init(&ucm_reloc_get_orig_lock, &attr);\n}\n#endif\npthread_t volatile ucm_reloc_get_orig_thread = (pthread_t)-1;\n\nUCM_DEFINE_REPLACE_FUNC(mmap,    void*, MAP_FAILED, void*, size_t, int, int, int, off_t)\nUCM_DEFINE_REPLACE_FUNC(munmap,  int,   -1,         void*, size_t)\n#if HAVE_MREMAP\nUCM_DEFINE_REPLACE_FUNC(mremap,  void*, MAP_FAILED, void*, size_t, size_t, int)\n#endif\nUCM_DEFINE_REPLACE_FUNC(shmat,   void*, MAP_FAILED, int, const void*, int)\nUCM_DEFINE_REPLACE_FUNC(shmdt,   int,   -1,         const void*)\nUCM_DEFINE_REPLACE_FUNC(sbrk,    void*, MAP_FAILED, intptr_t)\nUCM_DEFINE_REPLACE_FUNC(brk,     int,   -1,         void*)\nUCM_DEFINE_REPLACE_FUNC(madvise, int,   -1,         void*, size_t, int)\n\nUCM_DEFINE_SELECT_FUNC(mmap, void*, MAP_FAILED, SYS_mmap, void*, size_t, int, int, int, off_t)\nUCM_DEFINE_SELECT_FUNC(munmap, int, -1, SYS_munmap, void*, size_t)\n#if HAVE_MREMAP\nUCM_DEFINE_SELECT_FUNC(mremap, void*, MAP_FAILED, SYS_mremap, void*, size_t, size_t, int)\n#endif\nUCM_DEFINE_SELECT_FUNC(madvise, int, -1, SYS_madvise, void*, size_t, int)\n\n#if UCM_BISTRO_HOOKS\n#if HAVE_DECL_SYS_SHMAT\n\nUCM_DEFINE_SELECT_FUNC(shmat, void*, MAP_FAILED, SYS_shmat, int, const void*, int)\n\n#elif HAVE_DECL_SYS_IPC\n#  ifndef IPCOP_shmat\n#    define IPCOP_shmat 21\n#  endif\n\n_UCM_DEFINE_DLSYM_FUNC(shmat, ucm_orig_dlsym_shmat, ucm_override_shmat,\n                       void*, MAP_FAILED, int, const void*, int)\n\nvoid *ucm_orig_shmat(int shmid, const void *shmaddr, int shmflg)\n{\n    unsigned long res;\n    void *addr;\n\n    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {\n        return ucm_orig_dlsym_shmat(shmid, shmaddr, shmflg);\n    } else {\n        /* Using IPC syscall of shmat implementation */\n        res = syscall(SYS_ipc, IPCOP_shmat, shmid, shmflg, &addr, shmaddr);\n\n        return res ? MAP_FAILED : addr;\n    }\n}\n\n#endif\n\n#if HAVE_DECL_SYS_SHMDT\n\nUCM_DEFINE_SELECT_FUNC(shmdt, int, -1, SYS_shmdt, const void*)\n\n#elif HAVE_DECL_SYS_IPC\n#  ifndef IPCOP_shmdt\n#    define IPCOP_shmdt 22\n#  endif\n\n_UCM_DEFINE_DLSYM_FUNC(shmdt, ucm_orig_dlsym_shmdt, ucm_override_shmdt,\n                       int, -1, const void*)\n\nint ucm_orig_shmdt(const void *shmaddr)\n{\n    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {\n        return ucm_orig_dlsym_shmdt(shmaddr);\n    } else {\n        /* Using IPC syscall of shmdt implementation */\n        return syscall(SYS_ipc, IPCOP_shmdt, 0, 0, 0, shmaddr);\n    }\n}\n\n#endif\n\n#if HAVE___CURBRK\nextern void *__curbrk;\n#endif\n\n_UCM_DEFINE_DLSYM_FUNC(brk, ucm_orig_dlsym_brk, ucm_override_brk, int, -1, void*)\n\nvoid *ucm_brk_syscall(void *addr)\n{\n    return (void*)syscall(SYS_brk, addr);\n}\n\nint ucm_orig_brk(void *addr)\n{\n    void *new_addr;\n\n#if HAVE___CURBRK\n    __curbrk =\n#endif\n    new_addr = ucm_brk_syscall(addr);\n\n    if (new_addr < addr) {\n        errno = ENOMEM;\n        return -1;\n    } else {\n        return 0;\n    }\n}\n\n_UCM_DEFINE_DLSYM_FUNC(sbrk, ucm_orig_dlsym_sbrk, ucm_override_sbrk,\n                       void*, MAP_FAILED, intptr_t)\n\nvoid *ucm_orig_sbrk(intptr_t increment)\n{\n    void *prev;\n\n    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {\n        return ucm_orig_dlsym_sbrk(increment);\n    } else {\n        prev = ucm_brk_syscall(0);\n        return ucm_orig_brk(UCS_PTR_BYTE_OFFSET(prev, increment)) ? (void*)-1 : prev;\n    }\n}\n\n#else /* UCM_BISTRO_HOOKS */\n\nUCM_DEFINE_DLSYM_FUNC(sbrk, void*, MAP_FAILED, intptr_t)\nUCM_DEFINE_DLSYM_FUNC(shmat, void*, MAP_FAILED, int, const void*, int)\nUCM_DEFINE_DLSYM_FUNC(shmdt, int, -1, const void*)\n\n#endif /* UCM_BISTRO_HOOKS */\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/ucm/bistro/bistro_int.h": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifndef UCM_BISTRO_BISTRO_INT_H_\n#define UCM_BISTRO_BISTRO_INT_H_\n\n#include <sys/mman.h>\n#include <dlfcn.h>\n#include <string.h>\n#include <stdlib.h>\n\n#include <ucm/bistro/bistro.h>\n#include <ucm/util/sys.h>\n#include <ucm/util/log.h>\n#include <ucs/sys/math.h>\n#include <ucs/arch/cpu.h>\n#include <ucs/debug/assert.h>\n\n#define UCM_PROT_READ_WRITE_EXEC (PROT_READ | PROT_WRITE | PROT_EXEC)\n#define UCM_PROT_READ_EXEC       (PROT_READ | PROT_EXEC)\n\n#define UCM_LOOKUP_SYMBOL(_func, _symbol) \\\n    _func = ucm_bistro_lookup(_symbol);   \\\n    if (!_func) {                         \\\n        return UCS_ERR_NO_ELEM;           \\\n    }\n\nucs_status_t ucm_bistro_apply_patch(void *dst, void *patch, size_t len);\n\nucs_status_t ucm_bistro_create_restore_point(void *addr, ucm_bistro_restore_point_t **rp);\n\nstatic inline void *ucm_bistro_lookup(const char *symbol)\n{\n    void *addr;\n\n    ucs_assert(symbol != NULL);\n\n    addr = dlsym(RTLD_NEXT, symbol);\n    if (!addr) {\n        addr = dlsym(RTLD_DEFAULT, symbol);\n    }\n    return addr;\n}\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/ucs/sys/module.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#ifndef _GNU_SOURCE\n#  define _GNU_SOURCE /* for dladdr(3) */\n#endif\n\n#include \"module.h\"\n\n#include <ucs/sys/preprocessor.h>\n#include <ucs/debug/memtrack.h>\n#include <ucs/debug/assert.h>\n#include <ucs/debug/log.h>\n#include <ucs/sys/string.h>\n#include <ucs/sys/math.h>\n#include <string.h>\n#include <limits.h>\n#include <dlfcn.h>\n#include <link.h>\n#include <libgen.h>\n\n\n#define UCS_MODULE_PATH_MEMTRACK_NAME   \"module_path\"\n#define UCS_MODULE_SRCH_PATH_MAX        2\n\n#define ucs_module_debug(_fmt, ...) \\\n    ucs_log(ucs_min(UCS_LOG_LEVEL_DEBUG, ucs_global_opts.module_log_level), \\\n            _fmt, ##  __VA_ARGS__)\n#define ucs_module_trace(_fmt, ...) \\\n    ucs_log(ucs_min(UCS_LOG_LEVEL_TRACE, ucs_global_opts.module_log_level), \\\n            _fmt, ##  __VA_ARGS__)\n\nstatic struct {\n    ucs_init_once_t  init;\n    char             module_ext[NAME_MAX];\n    unsigned         srchpath_cnt;\n    char             *srch_path[UCS_MODULE_SRCH_PATH_MAX];\n} ucs_module_loader_state = {\n    .init         = UCS_INIT_ONCE_INITIALIZER,\n    .module_ext   = \".so\", /* default extension */\n    .srchpath_cnt = 0,\n    .srch_path    = { NULL, NULL}\n};\n\n/* Should be called with lock held */\nstatic void ucs_module_loader_add_dl_dir()\n{\n    char *dlpath_dup = NULL;\n    size_t max_length;\n    Dl_info dl_info;\n    char *p, *path;\n    int ret;\n\n    (void)dlerror();\n    ret = dladdr((void*)&ucs_module_loader_state, &dl_info);\n    if (ret == 0) {\n        ucs_error(\"dladdr failed: %s\", dlerror());\n        return;\n    }\n\n    ucs_module_debug(\"ucs library path: %s\", dl_info.dli_fname);\n\n    /* copy extension */\n    dlpath_dup = ucs_strdup(dl_info.dli_fname,\n                            UCS_MODULE_PATH_MEMTRACK_NAME);\n    if (dlpath_dup == NULL) {\n        return;\n    }\n\n    p = basename(dlpath_dup);\n    p = strchr(p, '.');\n    if (p != NULL) {\n        strncpy(ucs_module_loader_state.module_ext, p,\n                sizeof(ucs_module_loader_state.module_ext) - 1);\n    }\n    ucs_free(dlpath_dup);\n\n    /* copy directory component */\n    dlpath_dup = ucs_strdup(dl_info.dli_fname,\n                            UCS_MODULE_PATH_MEMTRACK_NAME);\n    if (dlpath_dup == NULL) {\n        return;\n    }\n\n    /* construct module directory path */\n    max_length = strlen(dlpath_dup) +         /* directory */\n                 1 +                          /* '/' */\n                 strlen(UCX_MODULE_SUBDIR) +  /* sub-directory */\n                 1;                           /* '\\0' */\n    path = ucs_malloc(max_length, UCS_MODULE_PATH_MEMTRACK_NAME);\n    if (path == NULL) {\n        goto out;\n    }\n\n    snprintf(path, max_length, \"%s/%s\", dirname(dlpath_dup), UCX_MODULE_SUBDIR);\n    ucs_module_loader_state.srch_path[ucs_module_loader_state.srchpath_cnt++] = path;\n\nout:\n    ucs_free(dlpath_dup);\n}\n\n/* Should be called with lock held */\nstatic void ucs_module_loader_add_install_dir()\n{\n    ucs_module_loader_state.srch_path[ucs_module_loader_state.srchpath_cnt++] =\n                    ucs_global_opts.module_dir;\n}\n\nstatic void ucs_module_loader_init_paths()\n{\n    UCS_INIT_ONCE(&ucs_module_loader_state.init) {\n        ucs_assert(ucs_module_loader_state.srchpath_cnt == 0);\n        ucs_module_loader_add_dl_dir();\n        ucs_module_loader_add_install_dir();\n        ucs_assert(ucs_module_loader_state.srchpath_cnt <= UCS_MODULE_SRCH_PATH_MAX);\n    }\n}\n\n/* Perform shallow search for a symbol */\nstatic void *ucs_module_dlsym_shallow(const char *module_path, void *dl,\n                                      const char *symbol)\n{\n    struct link_map *lm_entry;\n    Dl_info dl_info;\n    void *addr;\n    int ret;\n\n    addr = dlsym(dl, symbol);\n    if (addr == NULL) {\n        return NULL;\n    }\n\n    (void)dlerror();\n    ret = dladdr(addr, &dl_info);\n    if (ret == 0) {\n        ucs_module_debug(\"dladdr(%p) [%s] failed: %s\", addr, symbol, dlerror());\n        return NULL;\n    }\n\n    (void)dlerror();\n    ret = dlinfo(dl, RTLD_DI_LINKMAP, &lm_entry);\n    if (ret) {\n        ucs_module_debug(\"dlinfo(%p) [%s] failed: %s\", dl, module_path, dlerror());\n        return NULL;\n    }\n\n    /* return the symbol only if it was found in the requested library, and not,\n     * for example, in one of its dependencies.\n     */\n    if (lm_entry->l_addr != (uintptr_t)dl_info.dli_fbase) {\n        ucs_module_debug(\"ignoring '%s' (%p) from %s (%p), expected in %s (%lx)\",\n                         symbol, addr, ucs_basename(dl_info.dli_fname),\n                         dl_info.dli_fbase, ucs_basename(module_path),\n                         lm_entry->l_addr);\n        return NULL;\n    }\n\n    return addr;\n}\n\nstatic void ucs_module_init(const char *module_path, void *dl)\n{\n    typedef ucs_status_t (*init_func_t)();\n\n    const char *module_init_name =\n                    UCS_PP_MAKE_STRING(UCS_MODULE_CONSTRUCTOR_NAME);\n    char *fullpath, buffer[PATH_MAX];\n    init_func_t init_func;\n    ucs_status_t status;\n\n    fullpath = realpath(module_path, buffer);\n    ucs_module_trace(\"loaded %s [%p]\", fullpath, dl);\n\n    init_func = (init_func_t)ucs_module_dlsym_shallow(module_path, dl,\n                                                      module_init_name);\n    if (init_func == NULL) {\n        ucs_module_trace(\"not calling constructor '%s' in %s\", module_init_name,\n                         module_path);\n        return;\n    }\n\n    ucs_module_trace(\"calling '%s' in '%s': [%p]\", module_init_name, fullpath,\n                     init_func);\n    status = init_func();\n    if (status != UCS_OK) {\n        ucs_module_debug(\"initializing '%s' failed: %s, unloading\", fullpath,\n                         ucs_status_string(status));\n        dlclose(dl);\n    }\n}\n\nstatic void ucs_module_load_one(const char *framework, const char *module_name,\n                                unsigned flags)\n{\n    char module_path[PATH_MAX] = {0};\n    const char *error;\n    unsigned i;\n    void *dl;\n    int mode;\n\n    mode = RTLD_LAZY;\n    if (flags & UCS_MODULE_LOAD_FLAG_NODELETE) {\n        mode |= RTLD_NODELETE;\n    }\n    if (flags & UCS_MODULE_LOAD_FLAG_GLOBAL) {\n        mode |= RTLD_GLOBAL;\n    } else {\n        mode |= RTLD_LOCAL;\n    }\n\n    for (i = 0; i < ucs_module_loader_state.srchpath_cnt; ++i) {\n        snprintf(module_path, sizeof(module_path) - 1, \"%s/lib%s_%s%s\",\n                 ucs_module_loader_state.srch_path[i], framework, module_name,\n                 ucs_module_loader_state.module_ext);\n\n        /* Clear error state */\n        (void)dlerror();\n        dl = dlopen(module_path, mode);\n        if (dl != NULL) {\n            ucs_module_init(module_path, dl);\n            break;\n        } else {\n            /* If a module fails to load, silently give up */\n            error = dlerror();\n            ucs_module_debug(\"dlopen('%s', mode=0x%x) failed: %s\", module_path,\n                             mode, error ? error : \"Unknown error\");\n        }\n    }\n\n    /* coverity[leaked_storage] : a loaded module is never unloaded */\n}\n\nvoid ucs_load_modules(const char *framework, const char *modules,\n                      ucs_init_once_t *init_once, unsigned flags)\n{\n    char *modules_str;\n    char *saveptr;\n    char *module_name;\n\n    ucs_module_loader_init_paths();\n\n    UCS_INIT_ONCE(init_once) {\n        ucs_module_debug(\"loading modules for %s\", framework);\n        modules_str = ucs_strdup(modules, \"modules_list\");\n        if (modules_str != NULL) {\n            saveptr     = NULL;\n            module_name = strtok_r(modules_str, \":\", &saveptr);\n            while (module_name != NULL) {\n                ucs_module_load_one(framework, module_name, flags);\n                module_name = strtok_r(NULL, \":\", &saveptr);\n            }\n            ucs_free(modules_str);\n        } else {\n            ucs_error(\"failed to allocate module names list\");\n        }\n    }\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/ucs/debug/debug.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2014.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include \"debug.h\"\n#include \"log.h\"\n\n#include <ucs/datastruct/khash.h>\n#include <ucs/profile/profile.h>\n#include <ucs/sys/checker.h>\n#include <ucs/sys/string.h>\n#include <ucs/sys/math.h>\n#include <ucs/sys/sys.h>\n#include <ucs/type/spinlock.h>\n#include <sys/wait.h>\n#include <execinfo.h>\n#include <dlfcn.h>\n#include <link.h>\n#include <dirent.h>\n#ifdef HAVE_DETAILED_BACKTRACE\n#  include <bfd.h>\n#endif /* HAVE_DETAILED_BACKTRACE */\n\n\nKHASH_MAP_INIT_INT64(ucs_debug_symbol, char*);\nKHASH_MAP_INIT_INT(ucs_signal_orig_action, struct sigaction*);\n\n#define UCS_GDB_MAX_ARGS         32\n#define BACKTRACE_MAX            64\n#define UCS_DEBUG_UNKNOWN_SYM    \"???\"\n\n#ifdef HAVE_DETAILED_BACKTRACE\n#    define UCS_DEBUG_BACKTRACE_LINE_FMT \"%2d 0x%016lx %s()  %s:%u\\n\"\n#    define UCS_DEBUG_BACKTRACE_LINE_ARG(_n, _line) \\\n         _n, (_line)->address, \\\n         (_line)->function ? (_line)->function : \"??\", \\\n         (_line)->file ? (_line)->file : \"??\", \\\n         (_line)->lineno\n#else\n#    define UCS_DEBUG_BACKTRACE_LINE_FMT \"%2d  %s\\n\"\n#    define UCS_DEBUG_BACKTRACE_LINE_ARG(_n, _line) _n, (_line)->symbol\n#endif\n\nstruct dl_address_search {\n    unsigned long            address;\n    const char               *filename;\n    unsigned long            base;\n};\n\n#ifdef HAVE_DETAILED_BACKTRACE\n\n#if HAVE_DECL_BFD_GET_SECTION_FLAGS\n#  define ucs_debug_bfd_section_flags(_abfd, _section) \\\n    bfd_get_section_flags(_abfd, _section)\n#elif HAVE_DECL_BFD_SECTION_FLAGS\n#  define ucs_debug_bfd_section_flags(_abfd, _section) \\\n    bfd_section_flags(_section)\n#else\n#  error \"Unsupported BFD API\"\n#endif\n\n#if HAVE_DECL_BFD_GET_SECTION_VMA\n#  define ucs_debug_bfd_section_vma(_abfd, _section) \\\n    bfd_get_section_vma(_abfd, _section)\n#elif HAVE_DECL_BFD_SECTION_VMA\n#  define ucs_debug_bfd_section_vma(_abfd, _section) \\\n    bfd_section_vma(_section)\n#else\n#  error \"Unsupported BFD API\"\n#endif\n\n#if HAVE_1_ARG_BFD_SECTION_SIZE\n#  define ucs_debug_bfd_section_size(_abfd, _section) \\\n    bfd_section_size(_section)\n#else\n#  define ucs_debug_bfd_section_size(_abfd, _section) \\\n    bfd_section_size(_abfd, _section);\n#endif\n\nstruct backtrace_line {\n    unsigned long            address;\n    char                     *file;\n    char                     *function;\n    unsigned                 lineno;\n};\n\nstruct backtrace_file {\n    struct dl_address_search dl;\n    bfd                      *abfd;\n    asymbol                  **syms;\n};\n\nstruct backtrace {\n    struct backtrace_line    lines[BACKTRACE_MAX];\n    int                      size;\n    int                      position;\n};\n\nstruct backtrace_search {\n    int                      count;\n    struct backtrace_file    *file;\n    int                      backoff; /* search the line where the function call\n                                         took place, instead of return address */\n    struct backtrace_line    *lines;\n    int                      max_lines;\n};\n\n#else /* HAVE_DETAILED_BACKTRACE */\n\nstruct backtrace_line {\n    void                     *address;\n    char                     *symbol;\n};\n\nstruct backtrace {\n    char                     **symbols;\n    void                     *addresses[BACKTRACE_MAX];\n    int                      size;\n    int                      position;\n    struct backtrace_line    line;\n};\n\n#endif /* HAVE_DETAILED_BACKTRACE */\n\n#define UCS_SYS_SIGNAME(signame) [SIG ## signame] = #signame\nconst char *ucs_signal_names[] = {\n    [0] = \"SIGNAL0\",\n    UCS_SYS_SIGNAME(HUP),\n    UCS_SYS_SIGNAME(INT),\n    UCS_SYS_SIGNAME(QUIT),\n    UCS_SYS_SIGNAME(ILL),\n    UCS_SYS_SIGNAME(TRAP),\n    UCS_SYS_SIGNAME(ABRT),\n    UCS_SYS_SIGNAME(BUS),\n    UCS_SYS_SIGNAME(FPE),\n    UCS_SYS_SIGNAME(KILL),\n    UCS_SYS_SIGNAME(USR1),\n    UCS_SYS_SIGNAME(SEGV),\n    UCS_SYS_SIGNAME(USR2),\n    UCS_SYS_SIGNAME(PIPE),\n    UCS_SYS_SIGNAME(ALRM),\n    UCS_SYS_SIGNAME(TERM),\n#ifdef SIGSTKFLT\n    UCS_SYS_SIGNAME(STKFLT),\n#endif\n    UCS_SYS_SIGNAME(CHLD),\n    UCS_SYS_SIGNAME(CONT),\n    UCS_SYS_SIGNAME(STOP),\n    UCS_SYS_SIGNAME(TSTP),\n    UCS_SYS_SIGNAME(TTIN),\n    UCS_SYS_SIGNAME(TTOU),\n    UCS_SYS_SIGNAME(URG),\n    UCS_SYS_SIGNAME(XCPU),\n    UCS_SYS_SIGNAME(XFSZ),\n    UCS_SYS_SIGNAME(VTALRM),\n    UCS_SYS_SIGNAME(PROF),\n    UCS_SYS_SIGNAME(WINCH),\n    UCS_SYS_SIGNAME(IO),\n#ifdef SIGPWR\n    UCS_SYS_SIGNAME(PWR),\n#endif\n    UCS_SYS_SIGNAME(SYS),\n#if defined __linux__\n    [SIGSYS + 1] = NULL\n#elif defined __FreeBSD__\n    [SIGRTMIN] = NULL\n#else\n#error \"Port me\"\n#endif\n};\n\n#if HAVE_SIGACTION_SA_RESTORER\nstatic void    *ucs_debug_signal_restorer = &ucs_debug_signal_restorer;\n#endif\nstatic stack_t  ucs_debug_signal_stack    = {NULL, 0, 0};\n\nstatic khash_t(ucs_debug_symbol) ucs_debug_symbols_cache;\nstatic khash_t(ucs_signal_orig_action) ucs_signal_orig_action_map;\n\nstatic ucs_recursive_spinlock_t ucs_kh_lock;\n\nstatic int ucs_debug_initialized = 0;\n\n#ifdef HAVE_CPLUS_DEMANGLE\nextern char *cplus_demangle(const char *, int);\n#endif\n\nstatic int ucs_debug_backtrace_is_excluded(void *address, const char *symbol);\n\n\nstatic char *ucs_debug_strdup(const char *str)\n{\n    size_t length;\n    char *newstr;\n\n    length = strlen(str) + 1;\n    newstr = ucs_sys_realloc(NULL, 0, length);\n    if (newstr != NULL) {\n        strncpy(newstr, str, length);\n    }\n    return newstr;\n}\n\n#ifdef HAVE_DETAILED_BACKTRACE\n\nstatic int dl_match_address(struct dl_phdr_info *info, size_t size, void *data)\n{\n    struct dl_address_search *dl = data;\n    const ElfW(Phdr) *phdr;\n    ElfW(Addr) load_base = info->dlpi_addr;\n    long n;\n\n    phdr = info->dlpi_phdr;\n    for (n = info->dlpi_phnum; --n >= 0; phdr++) {\n        if (phdr->p_type == PT_LOAD) {\n            ElfW(Addr) vbaseaddr = phdr->p_vaddr + load_base;\n            if (dl->address >= vbaseaddr && dl->address < vbaseaddr + phdr->p_memsz) {\n                dl->filename = info->dlpi_name;\n                dl->base     = info->dlpi_addr;\n            }\n        }\n    }\n    return 0;\n}\n\nstatic int dl_lookup_address(struct dl_address_search *dl)\n{\n    dl->filename = NULL;\n    dl->base     = 0;\n\n    dl_iterate_phdr(dl_match_address, dl);\n    if (dl->filename == NULL) {\n        return 0;\n    }\n\n    if (strlen(dl->filename) == 0) {\n        dl->filename = ucs_get_exe();\n    }\n    return 1;\n}\n\n/*\n * The dl member in file should be initialized\n */\nstatic int load_file(struct backtrace_file *file)\n{\n    long symcount;\n    unsigned int size;\n    char **matching;\n\n    file->syms = NULL;\n    file->abfd = bfd_openr(file->dl.filename, NULL);\n    if (!file->abfd) {\n        goto err;\n    }\n\n    if (bfd_check_format(file->abfd, bfd_archive)) {\n        goto err_close;\n    }\n\n    if (!bfd_check_format_matches(file->abfd, bfd_object, &matching)) {\n        goto err_close;\n    }\n\n    if ((bfd_get_file_flags(file->abfd) & HAS_SYMS) == 0) {\n        goto err_close;\n    }\n\n    symcount = bfd_read_minisymbols(file->abfd, 0, (PTR)&file->syms, &size);\n    if (symcount == 0) {\n        free(file->syms);\n        symcount = bfd_read_minisymbols(file->abfd, 1, (PTR)&file->syms, &size);\n    }\n    if (symcount < 0) {\n        goto err_close;\n    }\n\n    return 1;\n\nerr_close:\n    bfd_close(file->abfd);\nerr:\n    return 0;\n}\n\nstatic void unload_file(struct backtrace_file *file)\n{\n    free(file->syms);\n    bfd_close(file->abfd);\n}\n\nstatic char *ucs_debug_demangle(const char *name)\n{\n    char *demangled = NULL;\n#ifdef HAVE_CPLUS_DEMANGLE\n    demangled = cplus_demangle(name, 0);\n#endif\n    return demangled ? demangled : strdup(name);\n}\n\nstatic void find_address_in_section(bfd *abfd, asection *section, void *data)\n{\n    struct backtrace_search *search = data;\n    bfd_size_type size;\n    bfd_vma vma;\n    unsigned long address;\n    const char *filename, *function;\n    unsigned lineno;\n    int found;\n\n    if ((search->count > 0) || (search->max_lines == 0) ||\n        ((ucs_debug_bfd_section_flags(abfd, section) & SEC_ALLOC) == 0)) {\n        return;\n    }\n\n    address = search->file->dl.address - search->file->dl.base;\n    vma = ucs_debug_bfd_section_vma(abfd, section);\n    if (address < vma) {\n        return;\n    }\n\n    size = ucs_debug_bfd_section_size(abfd, section);\n    if (address >= vma + size) {\n        return;\n    }\n\n    /* Search in address-1 to get the calling line instead of return address */\n    found = bfd_find_nearest_line(abfd, section, search->file->syms,\n                                  address - vma - search->backoff,\n                                  &filename, &function, &lineno);\n    do {\n        search->lines[search->count].address  = address;\n        search->lines[search->count].file     = strdup(filename ? filename :\n                                                       UCS_DEBUG_UNKNOWN_SYM);\n        search->lines[search->count].function = function ?\n                                                ucs_debug_demangle(function) :\n                                                strdup(UCS_DEBUG_UNKNOWN_SYM);\n        search->lines[search->count].lineno   = lineno;\n        if (search->count == 0) {\n            /* To get the inliner info, search at the original address */\n            bfd_find_nearest_line(abfd, section, search->file->syms, address - vma,\n                                  &filename, &function, &lineno);\n        }\n\n        ++search->count;\n        found = bfd_find_inliner_info(abfd, &filename, &function, &lineno);\n    } while (found && (search->count < search->max_lines));\n}\n\nstatic int get_line_info(struct backtrace_file *file, int backoff,\n                         struct backtrace_line *lines, int max)\n{\n    struct backtrace_search search;\n\n    search.file      = file;\n    search.backoff   = backoff;\n    search.count     = 0;\n    search.lines     = lines;\n    search.max_lines = max;\n    bfd_map_over_sections(file->abfd, find_address_in_section, &search);\n    return search.count;\n}\n\n/**\n * Create a backtrace from the calling location.\n *\n * @param bckt          Backtrace object.\n * @param strip         How many frames to strip.\n*/\nucs_status_t ucs_debug_backtrace_create(backtrace_h *bckt, int strip)\n{\n    size_t size = sizeof(**bckt);\n    struct backtrace_file file;\n    void *addresses[BACKTRACE_MAX];\n    int i, num_addresses;\n    ucs_status_t status;\n\n    *bckt  = NULL;\n    status = ucs_mmap_alloc(&size, (void**)bckt, 0\n                            UCS_MEMTRACK_NAME(\"debug backtrace object\"));\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    num_addresses = backtrace(addresses, BACKTRACE_MAX);\n\n    (*bckt)->size     = 0;\n    (*bckt)->position = strip;\n    for (i = 0; i < num_addresses; ++i) {\n        file.dl.address = (unsigned long)addresses[i];\n        if (dl_lookup_address(&file.dl) && load_file(&file)) {\n            (*bckt)->size += get_line_info(&file, 1,\n                                           (*bckt)->lines + (*bckt)->size,\n                                           BACKTRACE_MAX - (*bckt)->size);\n            unload_file(&file);\n        }\n    }\n\n    return UCS_OK;\n}\n\n/**\n * Destroy a backtrace and free all memory.\n *\n * @param bckt          Backtrace object.\n */\nvoid ucs_debug_backtrace_destroy(backtrace_h bckt)\n{\n    int i;\n\n    for (i = 0; i < bckt->size; ++i) {\n        free(bckt->lines[i].function);\n        free(bckt->lines[i].file);\n    }\n    bckt->size = 0;\n    ucs_mmap_free(bckt, sizeof(*bckt));\n}\n\nstatic ucs_status_t\nucs_debug_get_line_info(const char *filename, unsigned long base,\n                        unsigned long address, ucs_debug_address_info_t *info)\n{\n    struct backtrace_file file;\n    struct backtrace_line line;\n    int count;\n\n    file.dl.filename = filename;\n    file.dl.base     = base;\n    file.dl.address  = address;\n\n    if (!load_file(&file)) {\n        goto err;\n    }\n\n    count = get_line_info(&file, 0, &line, 1);\n    if (count == 0) {\n        goto err_unload;\n    }\n\n    if (line.function) {\n        ucs_strncpy_zero(info->function, line.function, sizeof(info->function));\n    } else {\n        strcpy(info->function, UCS_DEBUG_UNKNOWN_SYM);\n    }\n    if (line.file) {\n        ucs_strncpy_zero(info->source_file, line.file, sizeof(info->source_file));\n    } else {\n        strcpy(info->function, UCS_DEBUG_UNKNOWN_SYM);\n    }\n    info->line_number = line.lineno;\n\n    free(line.function);\n    free(line.file);\n    unload_file(&file);\n    return UCS_OK;\n\nerr_unload:\n    unload_file(&file);\nerr:\n    strcpy(info->function,    UCS_DEBUG_UNKNOWN_SYM);\n    strcpy(info->source_file, UCS_DEBUG_UNKNOWN_SYM);\n    info->line_number = 0;\n    return UCS_ERR_NO_ELEM;\n}\n\nucs_status_t ucs_debug_lookup_address(void *address, ucs_debug_address_info_t *info)\n{\n    struct dl_address_search dl;\n\n    dl.address = (unsigned long)address;\n    if (!dl_lookup_address(&dl)) {\n        return UCS_ERR_NO_ELEM;\n    }\n\n    memset(info, 0, sizeof(*info));\n    info->file.base = dl.base;\n    ucs_expand_path(dl.filename, info->file.path, sizeof(info->file.path));\n    return ucs_debug_get_line_info(dl.filename, dl.base, dl.address, info);\n}\n\n/**\n * Walk to the next backtrace line information.\n *\n * @param bckt          Backtrace object.\n * @param line          Filled with backtrace frame info.\n *\n * NOTE: the line remains valid as long as the backtrace object is not destroyed.\n */\nint ucs_debug_backtrace_next(backtrace_h bckt, backtrace_line_h *line)\n{\n    backtrace_line_h ln;\n\n    do {\n        if (bckt->position >= bckt->size) {\n            return 0;\n        }\n\n        ln = &bckt->lines[bckt->position++];\n    } while (ucs_debug_backtrace_is_excluded((void*)ln->address, ln->function));\n\n    *line = ln;\n    return 1;\n}\n\nstatic void ucs_debug_print_source_file(const char *file, unsigned line,\n                                        const char *function, FILE *stream)\n{\n    static const int context = 3;\n    char srcline[256];\n    unsigned n;\n    FILE *f;\n\n    f = fopen(file, \"r\");\n    if (f == NULL) {\n        return;\n    }\n\n    n = 0;\n    fprintf(stream, \"\\n\");\n    fprintf(stream, \"%s: [ %s() ]\\n\", file, function);\n    if (line > context) {\n        fprintf(stream, \"      ...\\n\");\n    }\n    while (fgets(srcline, sizeof(srcline), f) != NULL) {\n        if (abs((int)line - (int)n) <= context) {\n            fprintf(stream, \"%s %5u %s\",\n                    (n == line) ? \"==>\" : \"   \", n, srcline);\n        }\n        ++n;\n    }\n    fprintf(stream, \"\\n\");\n\n    fclose(f);\n}\n\nstatic void ucs_debug_show_innermost_source_file(FILE *stream)\n{\n    backtrace_h bckt;\n    backtrace_line_h bckt_line;\n    ucs_status_t status;\n\n    status = ucs_debug_backtrace_create(&bckt, 0);\n    if (status != UCS_OK) {\n        return;\n    }\n\n    if (ucs_debug_backtrace_next(bckt, &bckt_line)) {\n        ucs_debug_print_source_file(bckt_line->file, bckt_line->lineno,\n                                    bckt_line->function, stream);\n    }\n    ucs_debug_backtrace_destroy(bckt);\n}\n\n#else /* HAVE_DETAILED_BACKTRACE */\n\nucs_status_t ucs_debug_lookup_address(void *address, ucs_debug_address_info_t *info)\n{\n    Dl_info dl_info;\n    int ret;\n\n    ret = dladdr(address, &dl_info);\n    if (!ret) {\n        return UCS_ERR_NO_ELEM;\n    }\n\n    ucs_strncpy_safe(info->file.path, dl_info.dli_fname, sizeof(info->file.path));\n    info->file.base = (uintptr_t)dl_info.dli_fbase;\n    ucs_strncpy_safe(info->function,\n                     (dl_info.dli_sname != NULL) ? dl_info.dli_sname : UCS_DEBUG_UNKNOWN_SYM,\n                     sizeof(info->function));\n    ucs_strncpy_safe(info->source_file, UCS_DEBUG_UNKNOWN_SYM, sizeof(info->source_file));\n    info->line_number = 0;\n\n    return UCS_OK;\n}\n\n/**\n * Create a backtrace from the calling location.\n */\nucs_status_t ucs_debug_backtrace_create(backtrace_h *bckt, int strip)\n{\n    size_t size = sizeof(**bckt);\n    ucs_status_t status;\n\n    *bckt  = NULL;\n    status = ucs_mmap_alloc(&size, (void**)bckt, 0\n                            UCS_MEMTRACK_NAME(\"debug backtrace object\"));\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    (*bckt)->size     = backtrace((*bckt)->addresses, BACKTRACE_MAX);\n    (*bckt)->symbols  = backtrace_symbols((*bckt)->addresses, (*bckt)->size);\n    (*bckt)->position = strip;\n\n    return UCS_OK;\n}\n\n/**\n * Destroy a backtrace and free all memory.\n *\n * @param bckt          Backtrace object.\n */\nvoid ucs_debug_backtrace_destroy(backtrace_h bckt)\n{\n    free(bckt->symbols);\n    ucs_mmap_free(bckt, sizeof(*bckt));\n}\n\n/**\n * Walk to the next backtrace line information.\n *\n * @param bckt          Backtrace object.\n * @param line          Filled with backtrace frame info.\n *\n * NOTE: the line remains valid as long as the backtrace object is not destroyed.\n */\nint ucs_debug_backtrace_next(backtrace_h bckt, backtrace_line_h *line)\n{\n    while (bckt->position < bckt->size) {\n        bckt->line.address = bckt->addresses[bckt->position];\n        bckt->line.symbol  = bckt->symbols[bckt->position];\n        bckt->position++;\n\n        if (!ucs_debug_backtrace_is_excluded(bckt->line.address,\n                                             bckt->line.symbol)) {\n            *line = &bckt->line;\n            return 1;\n        }\n    }\n\n    return 0;\n}\n\nstatic void ucs_debug_show_innermost_source_file(FILE *stream)\n{\n}\n\n#endif /* HAVE_DETAILED_BACKTRACE */\n\n/*\n * Filter specific functions from the head of the backtrace.\n */\nvoid ucs_debug_print_backtrace(FILE *stream, int strip)\n{\n    backtrace_h bckt;\n    backtrace_line_h bckt_line;\n    int i;\n\n    ucs_debug_backtrace_create(&bckt, strip);\n    fprintf(stream, \"==== backtrace (tid:%7d) ====\\n\", ucs_get_tid());\n    for (i = 0; ucs_debug_backtrace_next(bckt, &bckt_line); ++i) {\n         fprintf(stream, UCS_DEBUG_BACKTRACE_LINE_FMT,\n                 UCS_DEBUG_BACKTRACE_LINE_ARG(i, bckt_line));\n    }\n    fprintf(stream, \"=================================\\n\");\n\n    ucs_debug_backtrace_destroy(bckt);\n}\n\n/*\n * Filter specific functions from the head of the backtrace.\n */\nvoid ucs_debug_print_backtrace_line(char *buffer, size_t maxlen,\n                                    int frame_num,\n                                    backtrace_line_h line)\n{\n    snprintf(buffer, maxlen, UCS_DEBUG_BACKTRACE_LINE_FMT,\n             UCS_DEBUG_BACKTRACE_LINE_ARG(frame_num, line));\n}\n\nconst char *ucs_debug_get_symbol_name(void *address)\n{\n    static pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n    static ucs_debug_address_info_t info;\n    int hash_extra_status;\n    ucs_status_t status;\n    khiter_t hash_it;\n    size_t length;\n    char *sym;\n\n    pthread_mutex_lock(&lock);\n    hash_it = kh_put(ucs_debug_symbol, &ucs_debug_symbols_cache,\n                     (uintptr_t)address, &hash_extra_status);\n    if (hash_extra_status == 0) {\n         sym = kh_value(&ucs_debug_symbols_cache, hash_it);\n    } else {\n        status = ucs_debug_lookup_address(address, &info);\n        if (status == UCS_OK) {\n            if (hash_extra_status == -1) {\n                /* could not add to hash, return pointer to the static buffer */\n                sym = info.function;\n                goto out;\n            }\n\n            /* add new symbol to hash */\n            ucs_assert_always(hash_it != kh_end(&ucs_debug_symbols_cache));\n            length = strlen(info.function);\n            sym = ucs_malloc(length + 1, \"debug_symbol\");\n            if (sym != NULL) {\n                ucs_strncpy_safe(sym, info.function, length + 1);\n            }\n        } else {\n            /* could not resolve symbol */\n            sym = NULL;\n        }\n        kh_value(&ucs_debug_symbols_cache, hash_it) = sym;\n    }\n\nout:\n    pthread_mutex_unlock(&lock);\n    return sym ? sym : UCS_DEBUG_UNKNOWN_SYM;\n}\n\nstatic void ucs_debugger_attach()\n{\n    static const char *vg_cmds_fmt = \"file %s\\n\"\n                                     \"target remote | vgdb\\n\";\n    static const char *bt_cmds     = \"bt\\n\"\n                                     \"list\\n\";\n    static char pid_str[16];\n    char *vg_cmds;\n    char *gdb_cmdline;\n    char gdb_commands_file[256];\n    char* argv[6 + UCS_GDB_MAX_ARGS];\n    pid_t pid, debug_pid;\n    int fd, ret, narg;\n    char UCS_V_UNUSED *self_exe;\n\n    /* Fork a process which will execute gdb and attach to the current process.\n     * We must avoid trigerring calls to malloc/free, since the heap may be corrupted.\n     * Therefore all allocations are done with mmap() or use static arrays.\n     */\n\n    debug_pid = getpid();\n\n    pid = fork();\n    if (pid < 0) {\n        ucs_log_fatal_error(\"fork returned %d: %m\", pid);\n        return;\n    }\n\n    /* retrieve values from original process, before forking */\n    self_exe = ucs_debug_strdup(ucs_get_exe());\n\n    if (pid == 0) {\n        gdb_cmdline = ucs_debug_strdup(ucs_global_opts.gdb_command);\n        narg = 0;\n        argv[narg] = strtok(gdb_cmdline, \" \\t\");\n        while (argv[narg] != NULL) {\n            ++narg;\n            argv[narg] = strtok(NULL, \" \\t\");\n        }\n\n        /* Make coverity know that argv[0] will not be affected by TMPDIR */\n        if (narg == 0) {\n            return;\n        }\n\n        if (!RUNNING_ON_VALGRIND) {\n            snprintf(pid_str, sizeof(pid_str), \"%d\", debug_pid);\n            argv[narg++] = \"-p\";\n            argv[narg++] = pid_str;\n        }\n\n        /* Generate a file name for gdb commands */\n        memset(gdb_commands_file, 0, sizeof(gdb_commands_file));\n        snprintf(gdb_commands_file, sizeof(gdb_commands_file) - 1,\n                 \"%s/.gdbcommands.uid-%d\", ucs_get_tmpdir(), geteuid());\n\n        /* Write gdb commands and add the file to argv is successful */\n        fd = open(gdb_commands_file, O_WRONLY|O_TRUNC|O_CREAT, 0600);\n        if (fd >= 0) {\n            if (RUNNING_ON_VALGRIND) {\n                vg_cmds = ucs_sys_realloc(NULL, 0, strlen(vg_cmds_fmt) + strlen(self_exe));\n                sprintf(vg_cmds, vg_cmds_fmt, self_exe);\n                if (write(fd, vg_cmds, strlen(vg_cmds)) != strlen(vg_cmds)) {\n                    ucs_log_fatal_error(\"Unable to write to command file: %m\");\n                }\n            }\n\n            if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_BACKTRACE)) {\n                if (write(fd, bt_cmds, strlen(bt_cmds)) != strlen(bt_cmds)) {\n                    ucs_log_fatal_error(\"Unable to write to command file: %m\");\n                }\n            }\n            close(fd);\n\n            argv[narg++] = \"-x\";\n            argv[narg++] = gdb_commands_file;\n        } else {\n            ucs_log_fatal_error(\"Unable to open '%s' for writing: %m\",\n                                gdb_commands_file);\n        }\n\n        argv[narg++] = NULL;\n\n        /* Execute GDB */\n        /* coverity[tainted_string] */\n        ret = execvp(argv[0], argv);\n        if (ret < 0) {\n            ucs_log_fatal_error(\"Failed to execute %s: %m\", argv[0]);\n            exit(-1);\n        }\n    }\n\n    waitpid(pid, &ret, 0);\n}\n\nstatic void UCS_F_NOINLINE ucs_debug_freeze()\n{\n    static volatile int freeze = 1;\n    while (freeze) {\n        pause();\n    }\n}\n\nstatic void ucs_debug_stop_handler(int signo)\n{\n    ucs_debug_freeze();\n}\n\nstatic ucs_status_t ucs_debug_enum_threads_cb(pid_t tid, void *ctx)\n{\n    int ret;\n\n    if ((tid != 0) && (tid != ucs_get_tid())) {\n        ret = ucs_tgkill(getpid(), tid, SIGUSR1);\n        if (ret < 0) {\n            return UCS_ERR_NO_MESSAGE;\n        }\n    }\n\n    return UCS_OK;\n}\n\nstatic void ucs_debug_stop_other_threads()\n{\n    signal(SIGUSR1, ucs_debug_stop_handler);\n    ucs_sys_enum_threads(ucs_debug_enum_threads_cb, NULL);\n}\n\nstatic void ucs_debug_send_mail(const char *message)\n{\n    FILE *stream;\n\n    if (!strlen(ucs_global_opts.error_mail_to)) {\n        return;\n    }\n\n    stream = popen(\"/usr/lib/sendmail -t\", \"w\");\n    if (stream == NULL) {\n        return;\n    }\n\n    fprintf(stdout, \"Sending notification to %s\\n\", ucs_global_opts.error_mail_to);\n    fflush(stdout);\n\n    fprintf(stream, \"To:           %s\\n\", ucs_global_opts.error_mail_to);\n    fprintf(stream, \"From:         %s\\n\", \"ucx@openucx.org\");\n    fprintf(stream, \"Subject:      ucx error report on %s\\n\",\n            ucs_get_host_name());\n    fprintf(stream, \"Content-Type: text/plain\\n\");\n    fprintf(stream, \"\\n\");\n\n    fprintf(stream, \"program: %s\\n\", ucs_get_exe());\n    fprintf(stream, \"hostname: %s\\n\", ucs_get_host_name());\n    fprintf(stream, \"process id: %d\\n\", getpid());\n    fprintf(stream, \"\\n\");\n\n    fprintf(stream, \"\\n\");\n    fprintf(stream, \"%s\\n\", message);\n    fprintf(stream, \"\\n\");\n\n    ucs_debug_show_innermost_source_file(stream);\n    ucs_debug_print_backtrace(stream, 2);\n\n    if (strlen(ucs_global_opts.error_mail_footer)) {\n        fprintf(stream, \"\\n\");\n        fprintf(stream, \"%s\\n\", ucs_global_opts.error_mail_footer);\n    }\n    fprintf(stream, \"\\n\");\n\n    pclose(stream);\n}\n\nstatic void ucs_error_freeze(const char *message)\n{\n    static pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n    char response;\n    int ret;\n\n    ucs_debug_stop_other_threads();\n\n    if (pthread_mutex_trylock(&lock) == 0) {\n        if (strlen(ucs_global_opts.gdb_command) && isatty(fileno(stdout)) &&\n            isatty(fileno(stdin)))\n        {\n            ucs_log_fatal_error(\"Process frozen, press Enter to attach a debugger...\");\n            ret = read(fileno(stdin), &response, 1); /* Use low-level input to avoid deadlock */\n            if ((ret == 1) && (response == '\\n')) {\n                ucs_debugger_attach();\n            } else {\n                ucs_debug_freeze();\n            }\n        } else {\n            ucs_debug_send_mail(message);\n            ucs_log_fatal_error(\"Process frozen...\");\n            ucs_debug_freeze();\n        }\n\n        pthread_mutex_unlock(&lock);\n    } else {\n        ucs_debug_freeze();\n    }\n}\n\nstatic const char *ucs_signal_cause_common(int si_code)\n{\n    switch (si_code) {\n    case SI_USER      : return \"kill(2) or raise(3)\";\n    case SI_KERNEL    : return \"Sent by the kernel\";\n    case SI_QUEUE     : return \"sigqueue(2)\";\n    case SI_TIMER     : return \"POSIX timer expired\";\n    case SI_MESGQ     : return \"POSIX message queue state changed\";\n    case SI_ASYNCIO   : return \"AIO completed\";\n#ifdef SI_SIGIO\n    case SI_SIGIO     : return \"queued SIGIO\";\n#endif\n#ifdef SI_TKILL\n    case SI_TKILL     : return \"tkill(2) or tgkill(2)\";\n#endif\n    default           : return \"<unknown si_code>\";\n    }\n}\n\nstatic const char *ucs_signal_cause_ill(int si_code)\n{\n    switch (si_code) {\n    case ILL_ILLOPC   : return \"illegal opcode\";\n    case ILL_ILLOPN   : return \"illegal operand\";\n    case ILL_ILLADR   : return \"illegal addressing mode\";\n    case ILL_ILLTRP   : return \"illegal trap\";\n    case ILL_PRVOPC   : return \"privileged opcode\";\n    case ILL_PRVREG   : return \"privileged register\";\n    case ILL_COPROC   : return \"coprocessor error\";\n    case ILL_BADSTK   : return \"internal stack error\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_fpe(int si_code)\n{\n    switch (si_code) {\n    case FPE_INTDIV   : return \"integer divide by zero\";\n    case FPE_INTOVF   : return \"integer overflow\";\n    case FPE_FLTDIV   : return \"floating-point divide by zero\";\n    case FPE_FLTOVF   : return \"floating-point overflow\";\n    case FPE_FLTUND   : return \"floating-point underflow\";\n    case FPE_FLTRES   : return \"floating-point inexact result\";\n    case FPE_FLTINV   : return \"floating-point invalid operation\";\n    case FPE_FLTSUB   : return \"subscript out of range\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_segv(int si_code)\n{\n    switch (si_code) {\n    case SEGV_MAPERR  : return \"address not mapped to object\";\n    case SEGV_ACCERR  : return \"invalid permissions for mapped object\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_bus(int si_code)\n{\n    switch (si_code) {\n    case BUS_ADRALN   : return \"invalid address alignment\";\n    case BUS_ADRERR   : return \"nonexistent physical address\";\n    case BUS_OBJERR   : return \"object-specific hardware error\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_trap(int si_code)\n{\n    switch (si_code) {\n    case TRAP_BRKPT   : return \"process breakpoint\";\n    case TRAP_TRACE   : return \"process trace trap\";\n    default           : return ucs_signal_cause_common(si_code);\n    }\n}\n\nstatic const char *ucs_signal_cause_cld(int si_code)\n{\n    switch (si_code) {\n    case CLD_EXITED   : return \"child has exited\";\n    case CLD_KILLED   : return \"child was killed\";\n    case CLD_DUMPED   : return \"child terminated abnormally\";\n    case CLD_TRAPPED  : return \"traced child has trapped\";\n    case CLD_STOPPED  : return \"child has stopped\";\n    case CLD_CONTINUED: return \"stopped child has continued\";\n    default           : return NULL;\n    }\n}\n\nstatic void ucs_debug_handle_error_signal(int signo, const char *cause,\n                                          const char *fmt, ...)\n{\n    char buf[256];\n    va_list ap;\n\n    va_start(ap, fmt);\n    vsnprintf(buf, sizeof(buf), fmt, ap);\n    va_end(ap);\n\n    ucs_log_flush();\n    ucs_log_fatal_error(\"Caught signal %d (%s: %s%s)\", signo,\n                        strsignal(signo), cause, buf);\n    ucs_handle_error(cause);\n}\n\nstatic void ucs_error_signal_handler(int signo, siginfo_t *info, void *context)\n{\n    ucs_debug_cleanup(1);\n    ucs_log_flush();\n\n    switch (signo) {\n    case SIGILL:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_ill(info->si_code), \"\");\n        break;\n    case SIGTRAP:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_trap(info->si_code), \"\");\n        break;\n    case SIGBUS:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_bus(info->si_code), \"\");\n        break;\n    case SIGFPE:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_fpe(info->si_code), \"\");\n        break;\n    case SIGSEGV:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_segv(info->si_code),\n                                      \" at address %p\", info->si_addr);\n        break;\n    case SIGCHLD:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_cld(info->si_code), \"\");\n        break;\n    case SIGINT:\n    case SIGTERM:\n        break;\n    default:\n        ucs_debug_handle_error_signal(signo, ucs_signal_cause_common(info->si_code), \"\");\n        break;\n    }\n\n    raise(signo);\n}\n\nvoid ucs_handle_error(const char *message)\n{\n    ucs_debug_cleanup(1);\n\n    if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_DEBUG)) {\n        ucs_debugger_attach();\n    } else {\n        if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_BACKTRACE)) {\n            ucs_debug_show_innermost_source_file(stderr);\n            ucs_debug_print_backtrace(stderr, 2);\n        }\n        if (ucs_global_opts.handle_errors & UCS_BIT(UCS_HANDLE_ERROR_FREEZE)) {\n            ucs_error_freeze(message);\n        }\n    }\n}\n\nstatic int ucs_debug_is_error_signal(int signum)\n{\n    khiter_t hash_it;\n    int result;\n\n    if (!ucs_global_opts.handle_errors) {\n        return 0;\n    }\n\n    /* If this signal is error, but was disabled. */\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    hash_it = kh_get(ucs_signal_orig_action, &ucs_signal_orig_action_map, signum);\n    result = (hash_it != kh_end(&ucs_signal_orig_action_map));\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n    return result;\n}\n\nstatic void* ucs_debug_get_orig_func(const char *symbol, void *replacement)\n{\n    void *func_ptr;\n\n    func_ptr = dlsym(RTLD_NEXT, symbol);\n    if (func_ptr == NULL) {\n        func_ptr = dlsym(RTLD_DEFAULT, symbol);\n    }\n    return func_ptr;\n}\n\n#if !HAVE_SIGHANDLER_T\n#if HAVE___SIGHANDLER_T\ntypedef __sighandler_t *sighandler_t;\n#else\n#error \"Port me\"\n#endif\n#endif\nsighandler_t signal(int signum, sighandler_t handler)\n{\n    typedef sighandler_t (*sighandler_func_t)(int, sighandler_t);\n\n    static sighandler_func_t orig = NULL;\n\n    if (ucs_debug_initialized && ucs_debug_is_error_signal(signum)) {\n        return SIG_DFL;\n    }\n\n    if (orig == NULL) {\n        orig = (sighandler_func_t)ucs_debug_get_orig_func(\"signal\", signal);\n    }\n\n    return orig(signum, handler);\n}\n\nstatic int orig_sigaction(int signum, const struct sigaction *act,\n                          struct sigaction *oact)\n{\n    typedef int (*sigaction_func_t)(int, const struct sigaction*, struct sigaction*);\n\n    static sigaction_func_t orig = NULL;\n\n    if (orig == NULL) {\n        orig = (sigaction_func_t)ucs_debug_get_orig_func(\"sigaction\", sigaction);\n    }\n\n    return orig(signum, act, oact);\n}\n\nint sigaction(int signum, const struct sigaction *act, struct sigaction *oact)\n{\n    if (ucs_debug_initialized && ucs_debug_is_error_signal(signum)) {\n        return orig_sigaction(signum, NULL, oact); /* Return old, do not set new */\n    }\n\n    return orig_sigaction(signum, act, oact);\n}\n\nstatic void ucs_debug_signal_handler(int signo)\n{\n    ucs_log_flush();\n    ucs_global_opts.log_component.log_level = UCS_LOG_LEVEL_TRACE_DATA;\n    ucs_profile_dump();\n}\n\nstatic void ucs_debug_set_signal_alt_stack()\n{\n    int ret;\n\n    ucs_debug_signal_stack.ss_size = SIGSTKSZ +\n                                     (2 * ucs_log_get_buffer_size()) +\n                                     (sizeof(void*) * BACKTRACE_MAX) +\n                                     (128 * UCS_KBYTE);\n    ucs_debug_signal_stack.ss_sp =\n                    ucs_sys_realloc(NULL, 0, ucs_debug_signal_stack.ss_size);\n    if (ucs_debug_signal_stack.ss_sp == NULL) {\n        return;\n    }\n\n    ucs_debug_signal_stack.ss_flags = 0;\n    ret = sigaltstack(&ucs_debug_signal_stack, NULL);\n    if (ret) {\n        ucs_warn(\"sigaltstack(ss_sp=%p, ss_size=%zu) failed: %m\",\n                 ucs_debug_signal_stack.ss_sp, ucs_debug_signal_stack.ss_size);\n        ucs_sys_free(ucs_debug_signal_stack.ss_sp,\n                     ucs_debug_signal_stack.ss_size);\n        ucs_debug_signal_stack.ss_sp = NULL;\n        return;\n    }\n\n    ucs_debug(\"using signal stack %p size %zu\", ucs_debug_signal_stack.ss_sp,\n              ucs_debug_signal_stack.ss_size);\n}\n\nstatic inline void ucs_debug_save_original_sighandler(int signum,\n                                                      const struct sigaction* orig_handler)\n{\n    struct sigaction *oact_copy;\n    khiter_t hash_it;\n    int hash_extra_status;\n\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    hash_it = kh_get(ucs_signal_orig_action, &ucs_signal_orig_action_map, signum);\n    if (hash_it != kh_end(&ucs_signal_orig_action_map)) {\n        goto out;\n    }\n\n    oact_copy = ucs_malloc(sizeof(*orig_handler), \"orig_sighandler\");\n    if (oact_copy == NULL) {\n        goto out;\n    }\n\n    *oact_copy = *orig_handler;\n    hash_it = kh_put(ucs_signal_orig_action,\n                     &ucs_signal_orig_action_map,\n                     signum, &hash_extra_status);\n    kh_value(&ucs_signal_orig_action_map, hash_it) = oact_copy;\n\nout:\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n}\n\nstatic void ucs_set_signal_handler(void (*handler)(int, siginfo_t*, void *))\n{\n    struct sigaction sigact, old_action;\n    int i;\n    int ret;\n\n    sigact.sa_sigaction = handler;\n    sigact.sa_flags     = SA_SIGINFO;\n    if (ucs_debug_signal_stack.ss_sp != NULL) {\n        sigact.sa_flags |= SA_ONSTACK;\n    }\n    sigemptyset(&sigact.sa_mask);\n\n    for (i = 0; i < ucs_global_opts.error_signals.count; ++i) {\n        ret = orig_sigaction(ucs_global_opts.error_signals.signals[i], &sigact,\n                             &old_action);\n        if (ret < 0) {\n            ucs_warn(\"failed to set signal handler for sig %d : %m\",\n                     ucs_global_opts.error_signals.signals[i]);\n        }\n#if HAVE_SIGACTION_SA_RESTORER\n        ucs_debug_signal_restorer = old_action.sa_restorer;\n#endif\n        ucs_debug_save_original_sighandler(ucs_global_opts.error_signals.signals[i], &old_action);\n    }\n}\n\nstatic int ucs_debug_backtrace_is_excluded(void *address, const char *symbol)\n{\n    return\n#if HAVE_SIGACTION_SA_RESTORER\n           address == ucs_debug_signal_restorer ||\n#endif\n           !strcmp(symbol, \"ucs_handle_error\") ||\n           !strcmp(symbol, \"ucs_fatal_error_format\") ||\n           !strcmp(symbol, \"ucs_fatal_error_message\") ||\n           !strcmp(symbol, \"ucs_error_freeze\") ||\n           !strcmp(symbol, \"ucs_error_signal_handler\") ||\n           !strcmp(symbol, \"ucs_debug_handle_error_signal\") ||\n           !strcmp(symbol, \"ucs_debug_backtrace_create\") ||\n           !strcmp(symbol, \"ucs_debug_show_innermost_source_file\") ||\n           !strcmp(symbol, \"ucs_log_default_handler\") ||\n           !strcmp(symbol, \"__ucs_abort\") ||\n           !strcmp(symbol, \"ucs_log_dispatch\") ||\n           !strcmp(symbol, \"__ucs_log\") ||\n           !strcmp(symbol, \"ucs_debug_send_mail\") ||\n           (strstr(symbol, \"_L_unlock_\") == symbol);\n}\n\nstatic ucs_status_t ucs_debug_get_lib_info(Dl_info *dl_info)\n{\n    int ret;\n\n    (void)dlerror();\n    ret = dladdr(ucs_debug_get_lib_info, dl_info);\n    if (ret == 0) {\n        return UCS_ERR_NO_MEMORY;\n    }\n\n    return UCS_OK;\n}\n\nconst char *ucs_debug_get_lib_path()\n{\n    ucs_status_t status;\n    Dl_info dl_info;\n\n    status = ucs_debug_get_lib_info(&dl_info);\n    if (status != UCS_OK) {\n        return \"<failed to resolve libucs path>\";\n    }\n\n    return dl_info.dli_fname;\n}\n\nunsigned long ucs_debug_get_lib_base_addr()\n{\n    ucs_status_t status;\n    Dl_info dl_info;\n\n    status = ucs_debug_get_lib_info(&dl_info);\n    if (status != UCS_OK) {\n        return 0;\n    }\n\n    return (uintptr_t)dl_info.dli_fbase;\n}\n\nvoid ucs_debug_init()\n{\n    ucs_recursive_spinlock_init(&ucs_kh_lock, 0);\n\n    kh_init_inplace(ucs_signal_orig_action, &ucs_signal_orig_action_map);\n    kh_init_inplace(ucs_debug_symbol, &ucs_debug_symbols_cache);\n\n    if (ucs_global_opts.handle_errors) {\n        ucs_debug_set_signal_alt_stack();\n        ucs_set_signal_handler(ucs_error_signal_handler);\n    }\n    if (ucs_global_opts.debug_signo > 0) {\n        struct sigaction sigact, old_action;\n        memset(&sigact, 0, sizeof(sigact));\n        memset(&old_action, 0, sizeof(old_action));\n        sigact.sa_handler = ucs_debug_signal_handler;\n        orig_sigaction(ucs_global_opts.debug_signo, &sigact, &old_action);\n        ucs_debug_save_original_sighandler(ucs_global_opts.debug_signo, &old_action);\n    }\n\n#ifdef HAVE_DETAILED_BACKTRACE\n    bfd_init();\n#endif\n\n    ucs_debug_initialized = 1;\n}\n\nvoid ucs_debug_cleanup(int on_error)\n{\n    char *sym;\n    int signum;\n    struct sigaction *hndl;\n    ucs_status_t status;\n\n    ucs_debug_initialized = 0;\n\n    kh_foreach_key(&ucs_signal_orig_action_map, signum,\n                   ucs_debug_disable_signal(signum));\n\n    if (!on_error) {\n        kh_foreach_value(&ucs_debug_symbols_cache, sym, ucs_free(sym));\n        kh_foreach_value(&ucs_signal_orig_action_map, hndl, ucs_free(hndl));\n        kh_destroy_inplace(ucs_debug_symbol, &ucs_debug_symbols_cache);\n        kh_destroy_inplace(ucs_signal_orig_action, &ucs_signal_orig_action_map);\n    }\n\n    status = ucs_recursive_spinlock_destroy(&ucs_kh_lock);\n    if (status != UCS_OK) {\n        ucs_warn(\"ucs_recursive_spinlock_destroy() failed (%d)\", status);\n    }\n}\n\nstatic inline void ucs_debug_disable_signal_nolock(int signum)\n{\n    khiter_t hash_it;\n    struct sigaction *original_action, ucs_action;\n    int ret;\n\n    hash_it = kh_get(ucs_signal_orig_action, &ucs_signal_orig_action_map,\n                     signum);\n    if (hash_it == kh_end(&ucs_signal_orig_action_map)) {\n        ucs_warn(\"ucs_debug_disable_signal: signal %d was not set in ucs\",\n                 signum);\n        return;\n    }\n\n    original_action = kh_val(&ucs_signal_orig_action_map, hash_it);\n    ret = orig_sigaction(signum, original_action, &ucs_action);\n    if (ret < 0) {\n        ucs_warn(\"failed to set signal handler for sig %d : %m\", signum);\n    }\n\n    kh_del(ucs_signal_orig_action, &ucs_signal_orig_action_map, hash_it);\n    ucs_free(original_action);\n}\n\nvoid ucs_debug_disable_signal(int signum)\n{\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    ucs_debug_disable_signal_nolock(signum);\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n}\n\nvoid ucs_debug_disable_signals()\n{\n    int signum;\n\n    ucs_recursive_spin_lock(&ucs_kh_lock);\n    kh_foreach_key(&ucs_signal_orig_action_map, signum,\n                   ucs_debug_disable_signal_nolock(signum));\n    ucs_recursive_spin_unlock(&ucs_kh_lock);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/src/tools/perf/lib/libperf.c": "/**\n* Copyright (C) Mellanox Technologies Ltd. 2001-2019.  ALL RIGHTS RESERVED.\n* Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.\n* Copyright (C) The University of Tennessee and The University\n*               of Tennessee Research Foundation. 2015-2016. ALL RIGHTS RESERVED.\n* Copyright (C) ARM Ltd. 2017.  ALL RIGHTS RESERVED.\n* See file LICENSE for terms.\n*/\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <ucs/debug/log.h>\n#include <ucs/arch/bitops.h>\n#include <ucs/sys/module.h>\n#include <ucs/sys/string.h>\n#include <string.h>\n#include <tools/perf/lib/libperf_int.h>\n#include <unistd.h>\n\n#if _OPENMP\n#include <omp.h>\n#endif /* _OPENMP */\n\n#define ATOMIC_OP_CONFIG(_size, _op32, _op64, _op, _msg, _params, _status) \\\n    _status = __get_atomic_flag((_size), (_op32), (_op64), (_op)); \\\n    if (_status != UCS_OK) { \\\n        ucs_error(UCT_PERF_TEST_PARAMS_FMT\" does not support atomic %s for \" \\\n                  \"message size %zu bytes\", UCT_PERF_TEST_PARAMS_ARG(_params), \\\n                  (_msg)[_op], (_size)); \\\n        return _status; \\\n    }\n\n#define ATOMIC_OP_CHECK(_size, _attr, _required, _params, _msg) \\\n    if (!ucs_test_all_flags(_attr, _required)) { \\\n        if ((_params)->flags & UCX_PERF_TEST_FLAG_VERBOSE) { \\\n            ucs_error(UCT_PERF_TEST_PARAMS_FMT\" does not support required \" \\\n                      #_size\"-bit atomic: %s\", UCT_PERF_TEST_PARAMS_ARG(_params), \\\n                      (_msg)[ucs_ffs64(~(_attr) & (_required))]); \\\n        } \\\n        return UCS_ERR_UNSUPPORTED; \\\n    }\n\ntypedef struct {\n    union {\n        struct {\n            size_t     dev_addr_len;\n            size_t     iface_addr_len;\n            size_t     ep_addr_len;\n        } uct;\n        struct {\n            size_t     worker_addr_len;\n            size_t     total_wireup_len;\n        } ucp;\n    };\n    size_t             rkey_size;\n    unsigned long      recv_buffer;\n} ucx_perf_ep_info_t;\n\n\nconst ucx_perf_allocator_t* ucx_perf_mem_type_allocators[UCS_MEMORY_TYPE_LAST];\n\nstatic const char *perf_iface_ops[] = {\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_SHORT)]         = \"am short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_BCOPY)]         = \"am bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_ZCOPY)]         = \"am zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PUT_SHORT)]        = \"put short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PUT_BCOPY)]        = \"put bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PUT_ZCOPY)]        = \"put zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_GET_SHORT)]        = \"get short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_GET_BCOPY)]        = \"get bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_GET_ZCOPY)]        = \"get zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE)] = \"peer failure handler\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_IFACE)] = \"connect to iface\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_EP)]    = \"connect to ep\",\n    [ucs_ilog2(UCT_IFACE_FLAG_AM_DUP)]           = \"full reliability\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CB_SYNC)]          = \"sync callback\",\n    [ucs_ilog2(UCT_IFACE_FLAG_CB_ASYNC)]         = \"async callback\",\n    [ucs_ilog2(UCT_IFACE_FLAG_PENDING)]          = \"pending\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_SHORT)]  = \"tag eager short\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_BCOPY)]  = \"tag eager bcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_ZCOPY)]  = \"tag eager zcopy\",\n    [ucs_ilog2(UCT_IFACE_FLAG_TAG_RNDV_ZCOPY)]   = \"tag rndv zcopy\"\n};\n\nstatic const char *perf_atomic_op[] = {\n     [UCT_ATOMIC_OP_ADD]   = \"add\",\n     [UCT_ATOMIC_OP_AND]   = \"and\",\n     [UCT_ATOMIC_OP_OR]    = \"or\" ,\n     [UCT_ATOMIC_OP_XOR]   = \"xor\"\n};\n\nstatic const char *perf_atomic_fop[] = {\n     [UCT_ATOMIC_OP_ADD]   = \"fetch-add\",\n     [UCT_ATOMIC_OP_AND]   = \"fetch-and\",\n     [UCT_ATOMIC_OP_OR]    = \"fetch-or\",\n     [UCT_ATOMIC_OP_XOR]   = \"fetch-xor\",\n     [UCT_ATOMIC_OP_SWAP]  = \"swap\",\n     [UCT_ATOMIC_OP_CSWAP] = \"cswap\"\n};\n\n/*\n *  This Quickselect routine is based on the algorithm described in\n *  \"Numerical recipes in C\", Second Edition,\n *  Cambridge University Press, 1992, Section 8.5, ISBN 0-521-43108-5\n *  This code by Nicolas Devillard - 1998. Public domain.\n */\nstatic ucs_time_t __find_median_quick_select(ucs_time_t arr[], int n)\n{\n    int low, high ;\n    int median;\n    int middle, ll, hh;\n\n#define ELEM_SWAP(a,b) { register ucs_time_t t=(a);(a)=(b);(b)=t; }\n\n    low = 0 ; high = n-1 ; median = (low + high) / 2;\n    for (;;) {\n        if (high <= low) /* One element only */\n            return arr[median] ;\n\n        if (high == low + 1) {  /* Two elements only */\n            if (arr[low] > arr[high])\n                ELEM_SWAP(arr[low], arr[high]) ;\n            return arr[median] ;\n        }\n\n        /* Find median of low, middle and high items; swap into position low */\n        middle = (low + high) / 2;\n        if (arr[middle] > arr[high])    ELEM_SWAP(arr[middle], arr[high]) ;\n        if (arr[low] > arr[high])       ELEM_SWAP(arr[low], arr[high]) ;\n        if (arr[middle] > arr[low])     ELEM_SWAP(arr[middle], arr[low]) ;\n\n        /* Swap low item (now in position middle) into position (low+1) */\n        ELEM_SWAP(arr[middle], arr[low+1]) ;\n\n        /* Nibble from each end towards middle, swapping items when stuck */\n        ll = low + 1;\n        hh = high;\n        for (;;) {\n            do ll++; while (arr[low] > arr[ll]) ;\n            do hh--; while (arr[hh]  > arr[low]) ;\n\n            if (hh < ll)\n                break;\n\n            ELEM_SWAP(arr[ll], arr[hh]) ;\n        }\n\n        /* Swap middle item (in position low) back into correct position */\n        ELEM_SWAP(arr[low], arr[hh]) ;\n\n        /* Re-set active partition */\n        if (hh <= median)\n            low = ll;\n        if (hh >= median)\n            high = hh - 1;\n    }\n}\n\nstatic ucs_status_t\nuct_perf_test_alloc_host(const ucx_perf_context_t *perf, size_t length,\n                         unsigned flags, uct_allocated_memory_t *alloc_mem)\n{\n    ucs_status_t status;\n\n    status = uct_iface_mem_alloc(perf->uct.iface, length,\n                                 flags, \"perftest\", alloc_mem);\n    if (status != UCS_OK) {\n        ucs_free(alloc_mem);\n        ucs_error(\"failed to allocate memory: %s\", ucs_status_string(status));\n        return status;\n    }\n\n    ucs_assert(alloc_mem->md == perf->uct.md);\n\n    return UCS_OK;\n}\n\nstatic void uct_perf_test_free_host(const ucx_perf_context_t *perf,\n                                    uct_allocated_memory_t *alloc_mem)\n{\n    uct_iface_mem_free(alloc_mem);\n}\n\nstatic void ucx_perf_test_memcpy_host(void *dst, ucs_memory_type_t dst_mem_type,\n                                      const void *src, ucs_memory_type_t src_mem_type,\n                                      size_t count)\n{\n    if ((dst_mem_type != UCS_MEMORY_TYPE_HOST) ||\n        (src_mem_type != UCS_MEMORY_TYPE_HOST)) {\n        ucs_error(\"wrong memory type passed src - %d, dst - %d\",\n                  src_mem_type, dst_mem_type);\n    } else {\n        memcpy(dst, src, count);\n    }\n}\n\nstatic ucs_status_t uct_perf_test_alloc_mem(ucx_perf_context_t *perf)\n{\n    ucx_perf_params_t *params = &perf->params;\n    ucs_status_t status;\n    unsigned flags;\n    size_t buffer_size;\n\n    if ((UCT_PERF_DATA_LAYOUT_ZCOPY == params->uct.data_layout) && params->iov_stride) {\n        buffer_size = params->msg_size_cnt * params->iov_stride;\n    } else {\n        buffer_size = ucx_perf_get_message_size(params);\n    }\n\n    /* TODO use params->alignment  */\n\n    flags = (params->flags & UCX_PERF_TEST_FLAG_MAP_NONBLOCK) ?\n             UCT_MD_MEM_FLAG_NONBLOCK : 0;\n    flags |= UCT_MD_MEM_ACCESS_ALL;\n\n    /* Allocate send buffer memory */\n    status = perf->allocator->uct_alloc(perf, buffer_size * params->thread_count,\n                                        flags, &perf->uct.send_mem);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    perf->send_buffer = perf->uct.send_mem.address;\n\n    /* Allocate receive buffer memory */\n    status = perf->allocator->uct_alloc(perf, buffer_size * params->thread_count,\n                                        flags, &perf->uct.recv_mem);\n    if (status != UCS_OK) {\n        goto err_free_send;\n    }\n\n    perf->recv_buffer = perf->uct.recv_mem.address;\n\n    /* Allocate IOV datatype memory */\n    perf->params.msg_size_cnt = params->msg_size_cnt;\n    perf->uct.iov             = malloc(sizeof(*perf->uct.iov) *\n                                       perf->params.msg_size_cnt *\n                                       params->thread_count);\n    if (NULL == perf->uct.iov) {\n        status = UCS_ERR_NO_MEMORY;\n        ucs_error(\"Failed allocate send IOV(%lu) buffer: %s\",\n                  perf->params.msg_size_cnt, ucs_status_string(status));\n        goto err_free_recv;\n    }\n\n    ucs_debug(\"allocated memory. Send buffer %p, Recv buffer %p\",\n              perf->send_buffer, perf->recv_buffer);\n    return UCS_OK;\n\nerr_free_recv:\n    perf->allocator->uct_free(perf, &perf->uct.recv_mem);\nerr_free_send:\n    perf->allocator->uct_free(perf, &perf->uct.send_mem);\nerr:\n    return status;\n}\n\nstatic void uct_perf_test_free_mem(ucx_perf_context_t *perf)\n{\n    perf->allocator->uct_free(perf, &perf->uct.send_mem);\n    perf->allocator->uct_free(perf, &perf->uct.recv_mem);\n    free(perf->uct.iov);\n}\n\nvoid ucx_perf_test_start_clock(ucx_perf_context_t *perf)\n{\n    ucs_time_t start_time = ucs_get_time();\n\n    perf->start_time_acc   = ucs_get_accurate_time();\n    perf->end_time         = (perf->params.max_time == 0.0) ? UINT64_MAX :\n                              ucs_time_from_sec(perf->params.max_time) + start_time;\n    perf->prev_time        = start_time;\n    perf->prev.time        = start_time;\n    perf->prev.time_acc    = perf->start_time_acc;\n    perf->current.time_acc = perf->start_time_acc;\n}\n\n/* Initialize/reset all parameters that could be modified by the warm-up run */\nstatic void ucx_perf_test_prepare_new_run(ucx_perf_context_t *perf,\n                                          const ucx_perf_params_t *params)\n{\n    unsigned i;\n\n    perf->max_iter          = (perf->params.max_iter == 0) ? UINT64_MAX :\n                               perf->params.max_iter;\n    perf->report_interval   = ucs_time_from_sec(perf->params.report_interval);\n    perf->current.time      = 0;\n    perf->current.msgs      = 0;\n    perf->current.bytes     = 0;\n    perf->current.iters     = 0;\n    perf->prev.msgs         = 0;\n    perf->prev.bytes        = 0;\n    perf->prev.iters        = 0;\n    perf->timing_queue_head = 0;\n\n    for (i = 0; i < TIMING_QUEUE_SIZE; ++i) {\n        perf->timing_queue[i] = 0;\n    }\n    ucx_perf_test_start_clock(perf);\n}\n\nstatic void ucx_perf_test_init(ucx_perf_context_t *perf,\n                               const ucx_perf_params_t *params)\n{\n    unsigned group_index;\n\n    perf->params = *params;\n    group_index  = rte_call(perf, group_index);\n\n    if (0 == group_index) {\n        perf->allocator = ucx_perf_mem_type_allocators[params->send_mem_type];\n    } else {\n        perf->allocator = ucx_perf_mem_type_allocators[params->recv_mem_type];\n    }\n\n    ucx_perf_test_prepare_new_run(perf, params);\n}\n\nvoid ucx_perf_calc_result(ucx_perf_context_t *perf, ucx_perf_result_t *result)\n{\n    ucs_time_t median;\n    double factor;\n\n    if (perf->params.test_type == UCX_PERF_TEST_TYPE_PINGPONG) {\n        factor = 2.0;\n    } else {\n        factor = 1.0;\n    }\n\n    result->iters = perf->current.iters;\n    result->bytes = perf->current.bytes;\n    result->elapsed_time = perf->current.time_acc - perf->start_time_acc;\n\n    /* Latency */\n    median = __find_median_quick_select(perf->timing_queue, TIMING_QUEUE_SIZE);\n    result->latency.typical = ucs_time_to_sec(median) / factor;\n\n    result->latency.moment_average =\n        (perf->current.time_acc - perf->prev.time_acc)\n        / (perf->current.iters - perf->prev.iters)\n        / factor;\n\n    result->latency.total_average =\n        (perf->current.time_acc - perf->start_time_acc)\n        / perf->current.iters\n        / factor;\n\n\n    /* Bandwidth */\n\n    result->bandwidth.typical = 0.0; // Undefined\n\n    result->bandwidth.moment_average =\n        (perf->current.bytes - perf->prev.bytes) /\n        (perf->current.time_acc - perf->prev.time_acc) * factor;\n\n    result->bandwidth.total_average =\n        perf->current.bytes /\n        (perf->current.time_acc - perf->start_time_acc) * factor;\n\n\n    /* Packet rate */\n\n    result->msgrate.typical = 0.0; // Undefined\n\n    result->msgrate.moment_average =\n        (perf->current.msgs - perf->prev.msgs) /\n        (perf->current.time_acc - perf->prev.time_acc) * factor;\n\n    result->msgrate.total_average =\n        perf->current.msgs /\n        (perf->current.time_acc - perf->start_time_acc) * factor;\n\n}\n\nstatic ucs_status_t ucx_perf_test_check_params(ucx_perf_params_t *params)\n{\n    size_t it;\n\n    /* check if zero-size messages are requested and supported */\n    if ((/* they are not supported by: */\n         /* - UCT tests, except UCT AM Short/Bcopy */\n         (params->api == UCX_PERF_API_UCT) ||\n         (/* - UCP RMA and AMO tests */\n          (params->api == UCX_PERF_API_UCP) &&\n          (params->command != UCX_PERF_CMD_AM) &&\n          (params->command != UCX_PERF_CMD_TAG) &&\n          (params->command != UCX_PERF_CMD_TAG_SYNC) &&\n          (params->command != UCX_PERF_CMD_STREAM))) &&\n        ucx_perf_get_message_size(params) < 1) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Message size too small, need to be at least 1\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    if ((params->api == UCX_PERF_API_UCP) &&\n        ((params->send_mem_type != UCS_MEMORY_TYPE_HOST) ||\n         (params->recv_mem_type != UCS_MEMORY_TYPE_HOST)) &&\n        ((params->command == UCX_PERF_CMD_PUT) ||\n         (params->command == UCX_PERF_CMD_GET) ||\n         (params->command == UCX_PERF_CMD_ADD) ||\n         (params->command == UCX_PERF_CMD_FADD) ||\n         (params->command == UCX_PERF_CMD_SWAP) ||\n         (params->command == UCX_PERF_CMD_CSWAP))) {\n        /* TODO: remove when support for non-HOST memory types will be added */\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"UCP doesn't support RMA/AMO for \\\"%s\\\"<->\\\"%s\\\" memory types\",\n                      ucs_memory_type_names[params->send_mem_type],\n                      ucs_memory_type_names[params->recv_mem_type]);\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    if (params->max_outstanding < 1) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"max_outstanding, need to be at least 1\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    /* check if particular message size fit into stride size */\n    if (params->iov_stride) {\n        for (it = 0; it < params->msg_size_cnt; ++it) {\n            if (params->msg_size_list[it] > params->iov_stride) {\n                if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                    ucs_error(\"Buffer size %lu bigger than stride %lu\",\n                              params->msg_size_list[it], params->iov_stride);\n                }\n                return UCS_ERR_INVALID_PARAM;\n            }\n        }\n    }\n\n    return UCS_OK;\n}\n\nvoid uct_perf_ep_flush_b(ucx_perf_context_t *perf, int peer_index)\n{\n    uct_ep_h ep = perf->uct.peers[peer_index].ep;\n    uct_completion_t comp;\n    ucs_status_t status;\n    int started;\n\n    started    = 0;\n    comp.func  = NULL;\n    comp.count = 2;\n    do {\n        if (!started) {\n            status = uct_ep_flush(ep, 0, &comp);\n            if (status == UCS_OK) {\n                --comp.count;\n            } else if (status == UCS_INPROGRESS) {\n                started = 1;\n            } else if (status != UCS_ERR_NO_RESOURCE) {\n                ucs_error(\"uct_ep_flush() failed: %s\", ucs_status_string(status));\n                return;\n            }\n        }\n        uct_worker_progress(perf->uct.worker);\n    } while (comp.count > 1);\n}\n\nvoid uct_perf_iface_flush_b(ucx_perf_context_t *perf)\n{\n    ucs_status_t status;\n\n    do {\n        status = uct_iface_flush(perf->uct.iface, 0, NULL);\n        uct_worker_progress(perf->uct.worker);\n    } while (status == UCS_INPROGRESS);\n    if (status != UCS_OK) {\n        ucs_error(\"uct_iface_flush() failed: %s\", ucs_status_string(status));\n    }\n}\n\nstatic inline uint64_t __get_flag(uct_perf_data_layout_t layout, uint64_t short_f,\n                                  uint64_t bcopy_f, uint64_t zcopy_f)\n{\n    return (layout == UCT_PERF_DATA_LAYOUT_SHORT) ? short_f :\n           (layout == UCT_PERF_DATA_LAYOUT_BCOPY) ? bcopy_f :\n           (layout == UCT_PERF_DATA_LAYOUT_ZCOPY) ? zcopy_f :\n           0;\n}\n\nstatic inline ucs_status_t __get_atomic_flag(size_t size, uint64_t *op32,\n                                             uint64_t *op64, uint64_t op)\n{\n    if (size == sizeof(uint32_t)) {\n        *op32 = UCS_BIT(op);\n        return UCS_OK;\n    } else if (size == sizeof(uint64_t)) {\n        *op64 = UCS_BIT(op);\n        return UCS_OK;\n    }\n    return UCS_ERR_UNSUPPORTED;\n}\n\nstatic inline size_t __get_max_size(uct_perf_data_layout_t layout, size_t short_m,\n                                    size_t bcopy_m, uint64_t zcopy_m)\n{\n    return (layout == UCT_PERF_DATA_LAYOUT_SHORT) ? short_m :\n           (layout == UCT_PERF_DATA_LAYOUT_BCOPY) ? bcopy_m :\n           (layout == UCT_PERF_DATA_LAYOUT_ZCOPY) ? zcopy_m :\n           0;\n}\n\nstatic ucs_status_t uct_perf_test_check_md_support(ucx_perf_params_t *params,\n                                                   ucs_memory_type_t mem_type,\n                                                   uct_md_attr_t *md_attr)\n{\n    if (!(md_attr->cap.access_mem_type == mem_type) &&\n        !(md_attr->cap.reg_mem_types & UCS_BIT(mem_type))) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Unsupported memory type %s by \"UCT_PERF_TEST_PARAMS_FMT,\n                      ucs_memory_type_names[mem_type],\n                      UCT_PERF_TEST_PARAMS_ARG(params));\n            return UCS_ERR_INVALID_PARAM;\n        }\n    }\n    return UCS_OK;\n}\n\nstatic ucs_status_t uct_perf_test_check_capabilities(ucx_perf_params_t *params,\n                                                     uct_iface_h iface, uct_md_h md)\n{\n    uint64_t required_flags = 0;\n    uint64_t atomic_op32    = 0;\n    uint64_t atomic_op64    = 0;\n    uint64_t atomic_fop32   = 0;\n    uint64_t atomic_fop64   = 0;\n    uct_md_attr_t md_attr;\n    uct_iface_attr_t attr;\n    ucs_status_t status;\n    size_t min_size, max_size, max_iov, message_size;\n\n    status = uct_md_query(md, &md_attr);\n    if (status != UCS_OK) {\n        ucs_error(\"uct_md_query(%s) failed: %s\",\n                  params->uct.md_name, ucs_status_string(status));\n        return status;\n    }\n\n    status = uct_iface_query(iface, &attr);\n    if (status != UCS_OK) {\n        ucs_error(\"uct_iface_query(\"UCT_PERF_TEST_PARAMS_FMT\") failed: %s\",\n                  UCT_PERF_TEST_PARAMS_ARG(params),\n                  ucs_status_string(status));\n        return status;\n    }\n\n    min_size = 0;\n    max_iov  = 1;\n    message_size = ucx_perf_get_message_size(params);\n    switch (params->command) {\n    case UCX_PERF_CMD_AM:\n        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_AM_SHORT,\n                                    UCT_IFACE_FLAG_AM_BCOPY, UCT_IFACE_FLAG_AM_ZCOPY);\n        required_flags |= UCT_IFACE_FLAG_CB_SYNC;\n        min_size = __get_max_size(params->uct.data_layout, 0, 0,\n                                  attr.cap.am.min_zcopy);\n        max_size = __get_max_size(params->uct.data_layout, attr.cap.am.max_short,\n                                  attr.cap.am.max_bcopy, attr.cap.am.max_zcopy);\n        max_iov  = attr.cap.am.max_iov;\n        break;\n    case UCX_PERF_CMD_PUT:\n        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_PUT_SHORT,\n                                    UCT_IFACE_FLAG_PUT_BCOPY, UCT_IFACE_FLAG_PUT_ZCOPY);\n        min_size = __get_max_size(params->uct.data_layout, 0, 0,\n                                  attr.cap.put.min_zcopy);\n        max_size = __get_max_size(params->uct.data_layout, attr.cap.put.max_short,\n                                  attr.cap.put.max_bcopy, attr.cap.put.max_zcopy);\n        max_iov  = attr.cap.put.max_iov;\n        break;\n    case UCX_PERF_CMD_GET:\n        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_GET_SHORT,\n                                    UCT_IFACE_FLAG_GET_BCOPY, UCT_IFACE_FLAG_GET_ZCOPY);\n        min_size = __get_max_size(params->uct.data_layout, 0, 0,\n                                  attr.cap.get.min_zcopy);\n        max_size = __get_max_size(params->uct.data_layout, attr.cap.get.max_short,\n                                  attr.cap.get.max_bcopy, attr.cap.get.max_zcopy);\n        max_iov  = attr.cap.get.max_iov;\n        break;\n    case UCX_PERF_CMD_ADD:\n        ATOMIC_OP_CONFIG(message_size, &atomic_op32, &atomic_op64, UCT_ATOMIC_OP_ADD,\n                         perf_atomic_op, params, status);\n        max_size = 8;\n        break;\n    case UCX_PERF_CMD_FADD:\n        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_ADD,\n                         perf_atomic_fop, params, status);\n        max_size = 8;\n        break;\n    case UCX_PERF_CMD_SWAP:\n        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_SWAP,\n                         perf_atomic_fop, params, status);\n        max_size = 8;\n        break;\n    case UCX_PERF_CMD_CSWAP:\n        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_CSWAP,\n                         perf_atomic_fop, params, status);\n        max_size = 8;\n        break;\n    default:\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Invalid test command\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    status = ucx_perf_test_check_params(params);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    /* check atomics first */\n    ATOMIC_OP_CHECK(32, attr.cap.atomic32.op_flags, atomic_op32, params, perf_atomic_op);\n    ATOMIC_OP_CHECK(64, attr.cap.atomic64.op_flags, atomic_op64, params, perf_atomic_op);\n    ATOMIC_OP_CHECK(32, attr.cap.atomic32.fop_flags, atomic_fop32, params, perf_atomic_fop);\n    ATOMIC_OP_CHECK(64, attr.cap.atomic64.fop_flags, atomic_fop64, params, perf_atomic_fop);\n\n    /* check iface flags */\n    if (!(atomic_op32 | atomic_op64 | atomic_fop32 | atomic_fop64) &&\n        (!ucs_test_all_flags(attr.cap.flags, required_flags) || !required_flags)) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(UCT_PERF_TEST_PARAMS_FMT\" does not support operation %s\",\n                      UCT_PERF_TEST_PARAMS_ARG(params),\n                      perf_iface_ops[ucs_ffs64(~attr.cap.flags & required_flags)]);\n        }\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    if (message_size < min_size) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Message size (%zu) is smaller than min supported (%zu)\",\n                      message_size, min_size);\n        }\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    if (message_size > max_size) {\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Message size (%zu) is larger than max supported (%zu)\",\n                      message_size, max_size);\n        }\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    if (params->command == UCX_PERF_CMD_AM) {\n        if ((params->uct.data_layout == UCT_PERF_DATA_LAYOUT_SHORT) &&\n            (params->am_hdr_size != sizeof(uint64_t)))\n        {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"Short AM header size must be 8 bytes\");\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        if ((params->uct.data_layout == UCT_PERF_DATA_LAYOUT_ZCOPY) &&\n            (params->am_hdr_size > attr.cap.am.max_hdr))\n        {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"AM header size (%zu) is larger than max supported (%zu)\",\n                          params->am_hdr_size, attr.cap.am.max_hdr);\n            }\n            return UCS_ERR_UNSUPPORTED;\n        }\n\n        if (params->am_hdr_size > message_size) {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"AM header size (%zu) is larger than message size (%zu)\",\n                          params->am_hdr_size, message_size);\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        if (params->uct.fc_window > UCT_PERF_TEST_MAX_FC_WINDOW) {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"AM flow-control window (%d) too large (should be <= %d)\",\n                          params->uct.fc_window, UCT_PERF_TEST_MAX_FC_WINDOW);\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        if ((params->flags & UCX_PERF_TEST_FLAG_ONE_SIDED) &&\n            (params->flags & UCX_PERF_TEST_FLAG_VERBOSE))\n        {\n            ucs_warn(\"Running active-message test with on-sided progress\");\n        }\n    }\n\n    if (UCT_PERF_DATA_LAYOUT_ZCOPY == params->uct.data_layout) {\n        if (params->msg_size_cnt > max_iov) {\n            if ((params->flags & UCX_PERF_TEST_FLAG_VERBOSE) ||\n                !params->msg_size_cnt) {\n                ucs_error(\"Wrong number of IOV entries. Requested is %lu, \"\n                          \"should be in the range 1...%lu\", params->msg_size_cnt,\n                          max_iov);\n            }\n            return UCS_ERR_UNSUPPORTED;\n        }\n        /* if msg_size_cnt == 1 the message size checked above */\n        if ((UCX_PERF_CMD_AM == params->command) && (params->msg_size_cnt > 1)) {\n            if (params->am_hdr_size > params->msg_size_list[0]) {\n                if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                    ucs_error(\"AM header size (%lu) larger than the first IOV \"\n                              \"message size (%lu)\", params->am_hdr_size,\n                              params->msg_size_list[0]);\n                }\n                return UCS_ERR_INVALID_PARAM;\n            }\n        }\n    }\n\n    status = uct_perf_test_check_md_support(params, params->send_mem_type, &md_attr);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    status = uct_perf_test_check_md_support(params, params->recv_mem_type, &md_attr);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    return UCS_OK;\n}\n\nstatic ucs_status_t uct_perf_test_setup_endpoints(ucx_perf_context_t *perf)\n{\n    const size_t buffer_size = ADDR_BUF_SIZE;\n    ucx_perf_ep_info_t info, *remote_info;\n    unsigned group_size, i, group_index;\n    uct_device_addr_t *dev_addr;\n    uct_iface_addr_t *iface_addr;\n    uct_ep_addr_t *ep_addr;\n    uct_iface_attr_t iface_attr;\n    uct_md_attr_t md_attr;\n    uct_ep_params_t ep_params;\n    void *rkey_buffer;\n    ucs_status_t status;\n    struct iovec vec[5];\n    void *buffer;\n    void *req;\n\n    buffer = malloc(buffer_size);\n    if (buffer == NULL) {\n        ucs_error(\"Failed to allocate RTE buffer\");\n        status = UCS_ERR_NO_MEMORY;\n        goto err;\n    }\n\n    status = uct_iface_query(perf->uct.iface, &iface_attr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_iface_query: %s\", ucs_status_string(status));\n        goto err_free;\n    }\n\n    status = uct_md_query(perf->uct.md, &md_attr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_md_query: %s\", ucs_status_string(status));\n        goto err_free;\n    }\n\n    if (md_attr.cap.flags & (UCT_MD_FLAG_ALLOC|UCT_MD_FLAG_REG)) {\n        info.rkey_size      = md_attr.rkey_packed_size;\n    } else {\n        info.rkey_size      = 0;\n    }\n    info.uct.dev_addr_len   = iface_attr.device_addr_len;\n    info.uct.iface_addr_len = iface_attr.iface_addr_len;\n    info.uct.ep_addr_len    = iface_attr.ep_addr_len;\n    info.recv_buffer        = (uintptr_t)perf->recv_buffer;\n\n    rkey_buffer             = buffer;\n    dev_addr                = UCS_PTR_BYTE_OFFSET(rkey_buffer, info.rkey_size);\n    iface_addr              = UCS_PTR_BYTE_OFFSET(dev_addr, info.uct.dev_addr_len);\n    ep_addr                 = UCS_PTR_BYTE_OFFSET(iface_addr, info.uct.iface_addr_len);\n    ucs_assert_always(UCS_PTR_BYTE_OFFSET(ep_addr, info.uct.ep_addr_len) <=\n                      UCS_PTR_BYTE_OFFSET(buffer, buffer_size));\n\n    status = uct_iface_get_device_address(perf->uct.iface, dev_addr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_iface_get_device_address: %s\",\n                  ucs_status_string(status));\n        goto err_free;\n    }\n\n    status = uct_iface_get_address(perf->uct.iface, iface_addr);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to uct_iface_get_address: %s\", ucs_status_string(status));\n        goto err_free;\n    }\n\n    if (info.rkey_size > 0) {\n        memset(rkey_buffer, 0, info.rkey_size);\n        status = uct_md_mkey_pack(perf->uct.md, perf->uct.recv_mem.memh, rkey_buffer);\n        if (status != UCS_OK) {\n            ucs_error(\"Failed to uct_rkey_pack: %s\", ucs_status_string(status));\n            goto err_free;\n        }\n    }\n\n    group_size  = rte_call(perf, group_size);\n    group_index = rte_call(perf, group_index);\n\n    perf->uct.peers = calloc(group_size, sizeof(*perf->uct.peers));\n    if (perf->uct.peers == NULL) {\n        goto err_free;\n    }\n\n    ep_params.field_mask = UCT_EP_PARAM_FIELD_IFACE;\n    ep_params.iface      = perf->uct.iface;\n    if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_EP) {\n        for (i = 0; i < group_size; ++i) {\n            if (i == group_index) {\n                continue;\n            }\n\n            status = uct_ep_create(&ep_params, &perf->uct.peers[i].ep);\n            if (status != UCS_OK) {\n                ucs_error(\"Failed to uct_ep_create: %s\", ucs_status_string(status));\n                goto err_destroy_eps;\n            }\n            status = uct_ep_get_address(perf->uct.peers[i].ep, ep_addr);\n            if (status != UCS_OK) {\n                ucs_error(\"Failed to uct_ep_get_address: %s\", ucs_status_string(status));\n                goto err_destroy_eps;\n            }\n        }\n    } else if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE) {\n        ep_params.field_mask |= UCT_EP_PARAM_FIELD_DEV_ADDR |\n                                UCT_EP_PARAM_FIELD_IFACE_ADDR;\n    }\n\n    vec[0].iov_base         = &info;\n    vec[0].iov_len          = sizeof(info);\n    vec[1].iov_base         = buffer;\n    vec[1].iov_len          = info.rkey_size + info.uct.dev_addr_len +\n                              info.uct.iface_addr_len + info.uct.ep_addr_len;\n\n    rte_call(perf, post_vec, vec, 2, &req);\n    rte_call(perf, exchange_vec, req);\n\n    for (i = 0; i < group_size; ++i) {\n        if (i == group_index) {\n            continue;\n        }\n\n        rte_call(perf, recv, i, buffer, buffer_size, req);\n\n        remote_info = buffer;\n        rkey_buffer = remote_info + 1;\n        dev_addr    = UCS_PTR_BYTE_OFFSET(rkey_buffer, remote_info->rkey_size);\n        iface_addr  = UCS_PTR_BYTE_OFFSET(dev_addr, remote_info->uct.dev_addr_len);\n        ep_addr     = UCS_PTR_BYTE_OFFSET(iface_addr, remote_info->uct.iface_addr_len);\n        perf->uct.peers[i].remote_addr = remote_info->recv_buffer;\n\n        if (!uct_iface_is_reachable(perf->uct.iface, dev_addr,\n                                    remote_info->uct.iface_addr_len ?\n                                    iface_addr : NULL)) {\n            ucs_error(\"Destination is unreachable\");\n            status = UCS_ERR_UNREACHABLE;\n            goto err_destroy_eps;\n        }\n\n        if (remote_info->rkey_size > 0) {\n            status = uct_rkey_unpack(perf->uct.cmpt, rkey_buffer,\n                                     &perf->uct.peers[i].rkey);\n            if (status != UCS_OK) {\n                ucs_error(\"Failed to uct_rkey_unpack: %s\", ucs_status_string(status));\n                goto err_destroy_eps;\n            }\n        } else {\n            perf->uct.peers[i].rkey.handle = NULL;\n            perf->uct.peers[i].rkey.rkey   = UCT_INVALID_RKEY;\n        }\n\n        if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_EP) {\n            status = uct_ep_connect_to_ep(perf->uct.peers[i].ep, dev_addr, ep_addr);\n        } else if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE) {\n            ep_params.dev_addr   = dev_addr;\n            ep_params.iface_addr = iface_addr;\n            status = uct_ep_create(&ep_params, &perf->uct.peers[i].ep);\n        } else {\n            status = UCS_ERR_UNSUPPORTED;\n        }\n        if (status != UCS_OK) {\n            ucs_error(\"Failed to connect endpoint: %s\", ucs_status_string(status));\n            goto err_destroy_eps;\n        }\n    }\n    uct_perf_iface_flush_b(perf);\n\n    free(buffer);\n    uct_perf_barrier(perf);\n    return UCS_OK;\n\nerr_destroy_eps:\n    for (i = 0; i < group_size; ++i) {\n        if (perf->uct.peers[i].rkey.rkey != UCT_INVALID_RKEY) {\n            uct_rkey_release(perf->uct.cmpt, &perf->uct.peers[i].rkey);\n        }\n        if (perf->uct.peers[i].ep != NULL) {\n            uct_ep_destroy(perf->uct.peers[i].ep);\n        }\n    }\n    free(perf->uct.peers);\nerr_free:\n    free(buffer);\nerr:\n    return status;\n}\n\nstatic void uct_perf_test_cleanup_endpoints(ucx_perf_context_t *perf)\n{\n    unsigned group_size, group_index, i;\n\n    uct_perf_barrier(perf);\n\n    uct_iface_set_am_handler(perf->uct.iface, UCT_PERF_TEST_AM_ID, NULL, NULL, 0);\n\n    group_size  = rte_call(perf, group_size);\n    group_index = rte_call(perf, group_index);\n\n    for (i = 0; i < group_size; ++i) {\n        if (i != group_index) {\n            if (perf->uct.peers[i].rkey.rkey != UCT_INVALID_RKEY) {\n                uct_rkey_release(perf->uct.cmpt, &perf->uct.peers[i].rkey);\n            }\n            if (perf->uct.peers[i].ep) {\n                uct_ep_destroy(perf->uct.peers[i].ep);\n            }\n        }\n    }\n    free(perf->uct.peers);\n}\n\nstatic ucs_status_t ucp_perf_test_fill_params(ucx_perf_params_t *params,\n                                               ucp_params_t *ucp_params)\n{\n    ucs_status_t status;\n    size_t message_size;\n\n    message_size = ucx_perf_get_message_size(params);\n    switch (params->command) {\n    case UCX_PERF_CMD_PUT:\n    case UCX_PERF_CMD_GET:\n        ucp_params->features |= UCP_FEATURE_RMA;\n        break;\n    case UCX_PERF_CMD_ADD:\n    case UCX_PERF_CMD_FADD:\n    case UCX_PERF_CMD_SWAP:\n    case UCX_PERF_CMD_CSWAP:\n        if (message_size == sizeof(uint32_t)) {\n            ucp_params->features |= UCP_FEATURE_AMO32;\n        } else if (message_size == sizeof(uint64_t)) {\n            ucp_params->features |= UCP_FEATURE_AMO64;\n        } else {\n            if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"Atomic size should be either 32 or 64 bit\");\n            }\n            return UCS_ERR_INVALID_PARAM;\n        }\n\n        break;\n    case UCX_PERF_CMD_TAG:\n    case UCX_PERF_CMD_TAG_SYNC:\n        ucp_params->features |= UCP_FEATURE_TAG;\n        break;\n    case UCX_PERF_CMD_STREAM:\n        ucp_params->features |= UCP_FEATURE_STREAM;\n        break;\n    default:\n        if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Invalid test command\");\n        }\n        return UCS_ERR_INVALID_PARAM;\n    }\n\n    status = ucx_perf_test_check_params(params);\n    if (status != UCS_OK) {\n        return status;\n    }\n\n    return UCS_OK;\n}\n\nstatic ucs_status_t ucp_perf_test_alloc_iov_mem(ucp_perf_datatype_t datatype,\n                                                size_t iovcnt, unsigned thread_count,\n                                                ucp_dt_iov_t **iov_p)\n{\n    ucp_dt_iov_t *iov;\n\n    if (UCP_PERF_DATATYPE_IOV == datatype) {\n        iov = malloc(sizeof(*iov) * iovcnt * thread_count);\n        if (NULL == iov) {\n            ucs_error(\"Failed allocate IOV buffer with iovcnt=%lu\", iovcnt);\n            return UCS_ERR_NO_MEMORY;\n        }\n        *iov_p = iov;\n    }\n\n    return UCS_OK;\n}\n\nstatic ucs_status_t\nucp_perf_test_alloc_host(const ucx_perf_context_t *perf, size_t length,\n                         void **address_p, ucp_mem_h *memh, int non_blk_flag)\n{\n    ucp_mem_map_params_t mem_map_params;\n    ucp_mem_attr_t mem_attr;\n    ucs_status_t status;\n\n    mem_map_params.field_mask = UCP_MEM_MAP_PARAM_FIELD_ADDRESS |\n                                UCP_MEM_MAP_PARAM_FIELD_LENGTH |\n                                UCP_MEM_MAP_PARAM_FIELD_FLAGS;\n    mem_map_params.address    = *address_p;\n    mem_map_params.length     = length;\n    mem_map_params.flags      = UCP_MEM_MAP_ALLOCATE;\n    if (perf->params.flags & UCX_PERF_TEST_FLAG_MAP_NONBLOCK) {\n        mem_map_params.flags |= non_blk_flag;\n    }\n\n    status = ucp_mem_map(perf->ucp.context, &mem_map_params, memh);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    mem_attr.field_mask = UCP_MEM_ATTR_FIELD_ADDRESS;\n    status = ucp_mem_query(*memh, &mem_attr);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    *address_p = mem_attr.address;\n    return UCS_OK;\n\nerr:\n    return status;\n}\n\nstatic void ucp_perf_test_free_host(const ucx_perf_context_t *perf,\n                                    void *address, ucp_mem_h memh)\n{\n    ucs_status_t status;\n\n    status = ucp_mem_unmap(perf->ucp.context, memh);\n    if (status != UCS_OK) {\n        ucs_warn(\"ucp_mem_unmap() failed: %s\", ucs_status_string(status));\n    }\n}\n\nstatic ucs_status_t ucp_perf_test_alloc_mem(ucx_perf_context_t *perf)\n{\n    ucx_perf_params_t *params = &perf->params;\n    ucs_status_t status;\n    size_t buffer_size;\n\n    if (params->iov_stride) {\n        buffer_size = params->msg_size_cnt * params->iov_stride;\n    } else {\n        buffer_size = ucx_perf_get_message_size(params);\n    }\n\n    /* Allocate send buffer memory */\n    perf->send_buffer = NULL;\n    status = perf->allocator->ucp_alloc(perf, buffer_size * params->thread_count,\n                                        &perf->send_buffer, &perf->ucp.send_memh,\n                                        UCP_MEM_MAP_NONBLOCK);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    /* Allocate receive buffer memory */\n    perf->recv_buffer = NULL;\n    status = perf->allocator->ucp_alloc(perf, buffer_size * params->thread_count,\n                                        &perf->recv_buffer, &perf->ucp.recv_memh,\n                                        0);\n    if (status != UCS_OK) {\n        goto err_free_send_buffer;\n    }\n\n    /* Allocate IOV datatype memory */\n    perf->ucp.send_iov = NULL;\n    status = ucp_perf_test_alloc_iov_mem(params->ucp.send_datatype,\n                                         perf->params.msg_size_cnt,\n                                         params->thread_count,\n                                         &perf->ucp.send_iov);\n    if (UCS_OK != status) {\n        goto err_free_buffers;\n    }\n\n    perf->ucp.recv_iov = NULL;\n    status = ucp_perf_test_alloc_iov_mem(params->ucp.recv_datatype,\n                                         perf->params.msg_size_cnt,\n                                         params->thread_count,\n                                         &perf->ucp.recv_iov);\n    if (UCS_OK != status) {\n        goto err_free_send_iov_buffers;\n    }\n\n    return UCS_OK;\n\nerr_free_send_iov_buffers:\n    free(perf->ucp.send_iov);\nerr_free_buffers:\n    perf->allocator->ucp_free(perf, perf->recv_buffer, perf->ucp.recv_memh);\nerr_free_send_buffer:\n    perf->allocator->ucp_free(perf, perf->send_buffer, perf->ucp.send_memh);\nerr:\n    return UCS_ERR_NO_MEMORY;\n}\n\nstatic void ucp_perf_test_free_mem(ucx_perf_context_t *perf)\n{\n    free(perf->ucp.recv_iov);\n    free(perf->ucp.send_iov);\n    perf->allocator->ucp_free(perf, perf->recv_buffer, perf->ucp.recv_memh);\n    perf->allocator->ucp_free(perf, perf->send_buffer, perf->ucp.send_memh);\n}\n\nstatic void ucp_perf_test_destroy_eps(ucx_perf_context_t* perf)\n{\n    unsigned i, thread_count = perf->params.thread_count;\n    ucs_status_ptr_t    *req;\n    ucs_status_t        status;\n\n    for (i = 0; i < thread_count; ++i) {\n        if (perf->ucp.tctx[i].perf.ucp.rkey != NULL) {\n            ucp_rkey_destroy(perf->ucp.tctx[i].perf.ucp.rkey);\n        }\n\n        if (perf->ucp.tctx[i].perf.ucp.ep != NULL) {\n            req = ucp_ep_close_nb(perf->ucp.tctx[i].perf.ucp.ep,\n                                  UCP_EP_CLOSE_MODE_FLUSH);\n\n            if (UCS_PTR_IS_PTR(req)) {\n                do {\n                    ucp_worker_progress(perf->ucp.tctx[i].perf.ucp.worker);\n                    status = ucp_request_check_status(req);\n                } while (status == UCS_INPROGRESS);\n\n                ucp_request_release(req);\n            } else if (UCS_PTR_STATUS(req) != UCS_OK) {\n                ucs_warn(\"failed to close ep %p on thread %d: %s\\n\",\n                         perf->ucp.tctx[i].perf.ucp.ep, i,\n                         ucs_status_string(UCS_PTR_STATUS(req)));\n            }\n        }\n    }\n}\n\nstatic ucs_status_t ucp_perf_test_exchange_status(ucx_perf_context_t *perf,\n                                                  ucs_status_t status)\n{\n    unsigned group_size  = rte_call(perf, group_size);\n    ucs_status_t collective_status = status;\n    struct iovec vec;\n    void *req = NULL;\n    unsigned i;\n\n    vec.iov_base = &status;\n    vec.iov_len  = sizeof(status);\n\n    rte_call(perf, post_vec, &vec, 1, &req);\n    rte_call(perf, exchange_vec, req);\n    for (i = 0; i < group_size; ++i) {\n        rte_call(perf, recv, i, &status, sizeof(status), req);\n        if (status != UCS_OK) {\n            collective_status = status;\n        }\n    }\n    return collective_status;\n}\n\nstatic ucs_status_t ucp_perf_test_receive_remote_data(ucx_perf_context_t *perf)\n{\n    unsigned thread_count = perf->params.thread_count;\n    void *rkey_buffer     = NULL;\n    void *req             = NULL;\n    unsigned group_size, group_index, i;\n    ucx_perf_ep_info_t *remote_info;\n    ucp_ep_params_t ep_params;\n    ucp_address_t *address;\n    ucs_status_t status;\n    size_t buffer_size;\n    void *buffer;\n\n    group_size  = rte_call(perf, group_size);\n    group_index = rte_call(perf, group_index);\n\n    if (group_size != 2) {\n        ucs_error(\"perftest requires group size to be exactly 2 \"\n                  \"(actual group size: %u)\", group_size);\n        return UCS_ERR_UNSUPPORTED;\n    }\n\n    buffer_size = ADDR_BUF_SIZE * thread_count;\n\n    buffer = malloc(buffer_size);\n    if (buffer == NULL) {\n        ucs_error(\"failed to allocate RTE receive buffer\");\n        status = UCS_ERR_NO_MEMORY;\n        goto err;\n    }\n\n    /* Initialize all endpoints and rkeys to NULL to handle error flow */\n    for (i = 0; i < thread_count; i++) {\n        perf->ucp.tctx[i].perf.ucp.ep   = NULL;\n        perf->ucp.tctx[i].perf.ucp.rkey = NULL;\n    }\n\n    /* receive the data from the remote peer, extract the address from it\n     * (along with additional wireup info) and create an endpoint to the peer */\n    rte_call(perf, recv, 1 - group_index, buffer, buffer_size, req);\n\n    remote_info = buffer;\n    for (i = 0; i < thread_count; i++) {\n        address                                = (ucp_address_t*)(remote_info + 1);\n        rkey_buffer                            = UCS_PTR_BYTE_OFFSET(address,\n                                                                     remote_info->ucp.worker_addr_len);\n        perf->ucp.tctx[i].perf.ucp.remote_addr = remote_info->recv_buffer;\n\n        ep_params.field_mask = UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;\n        ep_params.address    = address;\n\n        status = ucp_ep_create(perf->ucp.tctx[i].perf.ucp.worker, &ep_params,\n                               &perf->ucp.tctx[i].perf.ucp.ep);\n        if (status != UCS_OK) {\n            if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"ucp_ep_create() failed: %s\", ucs_status_string(status));\n            }\n            goto err_free_eps_buffer;\n        }\n\n        if (remote_info->rkey_size > 0) {\n            status = ucp_ep_rkey_unpack(perf->ucp.tctx[i].perf.ucp.ep, rkey_buffer,\n                                        &perf->ucp.tctx[i].perf.ucp.rkey);\n            if (status != UCS_OK) {\n                if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                    ucs_fatal(\"ucp_rkey_unpack() failed: %s\", ucs_status_string(status));\n                }\n                goto err_free_eps_buffer;\n            }\n        } else {\n            perf->ucp.tctx[i].perf.ucp.rkey = NULL;\n        }\n\n        remote_info = UCS_PTR_BYTE_OFFSET(remote_info,\n                                          remote_info->ucp.total_wireup_len);\n    }\n\n    free(buffer);\n    return UCS_OK;\n\nerr_free_eps_buffer:\n    ucp_perf_test_destroy_eps(perf);\n    free(buffer);\nerr:\n    return status;\n}\n\nstatic ucs_status_t ucp_perf_test_send_local_data(ucx_perf_context_t *perf,\n                                                  uint64_t features)\n{\n    unsigned i, j, thread_count = perf->params.thread_count;\n    size_t address_length       = 0;\n    void *rkey_buffer           = NULL;\n    void *req                   = NULL;\n    ucx_perf_ep_info_t *info;\n    ucp_address_t *address;\n    ucs_status_t status;\n    struct iovec *vec;\n    size_t rkey_size;\n\n    if (features & (UCP_FEATURE_RMA|UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {\n        status = ucp_rkey_pack(perf->ucp.context, perf->ucp.recv_memh,\n                               &rkey_buffer, &rkey_size);\n        if (status != UCS_OK) {\n            if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"ucp_rkey_pack() failed: %s\", ucs_status_string(status));\n            }\n            goto err;\n        }\n    } else {\n        rkey_size = 0;\n    }\n\n    /* each thread has an iovec with 3 entries to send to the remote peer:\n     * ep_info, worker_address and rkey buffer */\n    vec = calloc(3 * thread_count, sizeof(struct iovec));\n    if (vec == NULL) {\n        ucs_error(\"failed to allocate iovec\");\n        status = UCS_ERR_NO_MEMORY;\n        goto err_rkey_release;\n    }\n\n    /* get the worker address created for every thread and send it to the remote\n     * peer */\n    for (i = 0; i < thread_count; i++) {\n        status = ucp_worker_get_address(perf->ucp.tctx[i].perf.ucp.worker,\n                                        &address, &address_length);\n        if (status != UCS_OK) {\n            if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n                ucs_error(\"ucp_worker_get_address() failed: %s\",\n                          ucs_status_string(status));\n            }\n            goto err_free_workers_vec;\n        }\n\n        vec[i * 3].iov_base = malloc(sizeof(*info));\n        if (vec[i * 3].iov_base == NULL) {\n            ucs_error(\"failed to allocate vec entry for info\");\n            status = UCS_ERR_NO_MEMORY;\n            ucp_worker_destroy(perf->ucp.tctx[i].perf.ucp.worker);\n            goto err_free_workers_vec;\n        }\n\n        info                       = vec[i * 3].iov_base;\n        info->ucp.worker_addr_len  = address_length;\n        info->ucp.total_wireup_len = sizeof(*info) + address_length + rkey_size;\n        info->rkey_size            = rkey_size;\n        info->recv_buffer          = (uintptr_t)perf->ucp.tctx[i].perf.recv_buffer;\n\n        vec[(i * 3) + 0].iov_len  = sizeof(*info);\n        vec[(i * 3) + 1].iov_base = address;\n        vec[(i * 3) + 1].iov_len  = address_length;\n        vec[(i * 3) + 2].iov_base = rkey_buffer;\n        vec[(i * 3) + 2].iov_len  = info->rkey_size;\n\n        address_length = 0;\n    }\n\n    /* send to the remote peer */\n    rte_call(perf, post_vec, vec, 3 * thread_count, &req);\n    rte_call(perf, exchange_vec, req);\n\n    if (features & (UCP_FEATURE_RMA|UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {\n        ucp_rkey_buffer_release(rkey_buffer);\n    }\n\n    for (i = 0; i < thread_count; i++) {\n        free(vec[i * 3].iov_base);\n        ucp_worker_release_address(perf->ucp.tctx[i].perf.ucp.worker,\n                                   vec[(i * 3) + 1].iov_base);\n    }\n\n    free(vec);\n\n    return UCS_OK;\n\nerr_free_workers_vec:\n    for (j = 0; j < i; j++) {\n        ucp_worker_destroy(perf->ucp.tctx[i].perf.ucp.worker);\n    }\n    free(vec);\nerr_rkey_release:\n    if (features & (UCP_FEATURE_RMA|UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {\n        ucp_rkey_buffer_release(rkey_buffer);\n    }\nerr:\n    return status;\n}\n\nstatic ucs_status_t ucp_perf_test_setup_endpoints(ucx_perf_context_t *perf,\n                                                  uint64_t features)\n{\n    ucs_status_t status;\n    unsigned i;\n\n    /* pack the local endpoints data and send to the remote peer */\n    status = ucp_perf_test_send_local_data(perf, features);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    /* receive remote peer's endpoints' data and connect to them */\n    status = ucp_perf_test_receive_remote_data(perf);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    /* sync status across all processes */\n    status = ucp_perf_test_exchange_status(perf, UCS_OK);\n    if (status != UCS_OK) {\n        goto err_destroy_eps;\n    }\n\n    /* force wireup completion */\n    for (i = 0; i < perf->params.thread_count; i++) {\n        status = ucp_worker_flush(perf->ucp.tctx[i].perf.ucp.worker);\n        if (status != UCS_OK) {\n            ucs_warn(\"ucp_worker_flush() failed on theread %d: %s\",\n                     i, ucs_status_string(status));\n        }\n    }\n\n    return status;\n\nerr_destroy_eps:\n    ucp_perf_test_destroy_eps(perf);\nerr:\n    (void)ucp_perf_test_exchange_status(perf, status);\n    return status;\n}\n\nstatic void ucp_perf_test_cleanup_endpoints(ucx_perf_context_t *perf)\n{\n    ucp_perf_barrier(perf);\n    ucp_perf_test_destroy_eps(perf);\n}\n\nstatic void ucp_perf_test_destroy_workers(ucx_perf_context_t *perf)\n{\n    unsigned i;\n\n    for (i = 0; i < perf->params.thread_count; i++) {\n        if (perf->ucp.tctx[i].perf.ucp.worker != NULL) {\n            ucp_worker_destroy(perf->ucp.tctx[i].perf.ucp.worker);\n        }\n    }\n}\n\nstatic void ucx_perf_set_warmup(ucx_perf_context_t* perf,\n                                const ucx_perf_params_t* params)\n{\n    perf->max_iter = ucs_min(params->warmup_iter,\n                             ucs_div_round_up(params->max_iter, 10));\n    perf->report_interval = ULONG_MAX;\n}\n\nstatic ucs_status_t uct_perf_create_md(ucx_perf_context_t *perf)\n{\n    uct_component_h *uct_components;\n    uct_component_attr_t component_attr;\n    uct_tl_resource_desc_t *tl_resources;\n    unsigned md_index, num_components;\n    unsigned tl_index, num_tl_resources;\n    unsigned cmpt_index;\n    ucs_status_t status;\n    uct_md_h md;\n    uct_md_config_t *md_config;\n\n\n    status = uct_query_components(&uct_components, &num_components);\n    if (status != UCS_OK) {\n        goto out;\n    }\n\n    for (cmpt_index = 0; cmpt_index < num_components; ++cmpt_index) {\n\n        component_attr.field_mask = UCT_COMPONENT_ATTR_FIELD_MD_RESOURCE_COUNT;\n        status = uct_component_query(uct_components[cmpt_index], &component_attr);\n        if (status != UCS_OK) {\n            goto out_release_components_list;\n        }\n\n        component_attr.field_mask   = UCT_COMPONENT_ATTR_FIELD_MD_RESOURCES;\n        component_attr.md_resources = alloca(sizeof(*component_attr.md_resources) *\n                                             component_attr.md_resource_count);\n        status = uct_component_query(uct_components[cmpt_index], &component_attr);\n        if (status != UCS_OK) {\n            goto out_release_components_list;\n        }\n\n        for (md_index = 0; md_index < component_attr.md_resource_count; ++md_index) {\n            status = uct_md_config_read(uct_components[cmpt_index], NULL, NULL,\n                                        &md_config);\n            if (status != UCS_OK) {\n                goto out_release_components_list;\n            }\n\n            ucs_strncpy_zero(perf->params.uct.md_name,\n                             component_attr.md_resources[md_index].md_name,\n                             UCT_MD_NAME_MAX);\n\n            status = uct_md_open(uct_components[cmpt_index],\n                                 component_attr.md_resources[md_index].md_name,\n                                 md_config, &md);\n            uct_config_release(md_config);\n            if (status != UCS_OK) {\n                goto out_release_components_list;\n            }\n\n            status = uct_md_query_tl_resources(md, &tl_resources, &num_tl_resources);\n            if (status != UCS_OK) {\n                uct_md_close(md);\n                goto out_release_components_list;\n            }\n\n            for (tl_index = 0; tl_index < num_tl_resources; ++tl_index) {\n                if (!strcmp(perf->params.uct.tl_name,  tl_resources[tl_index].tl_name) &&\n                    !strcmp(perf->params.uct.dev_name, tl_resources[tl_index].dev_name))\n                {\n                    uct_release_tl_resource_list(tl_resources);\n                    perf->uct.cmpt = uct_components[cmpt_index];\n                    perf->uct.md   = md;\n                    status         = UCS_OK;\n                    goto out_release_components_list;\n                }\n            }\n\n            uct_md_close(md);\n            uct_release_tl_resource_list(tl_resources);\n        }\n    }\n\n    ucs_error(\"Cannot use \"UCT_PERF_TEST_PARAMS_FMT,\n              UCT_PERF_TEST_PARAMS_ARG(&perf->params));\n    status = UCS_ERR_NO_DEVICE;\n\nout_release_components_list:\n    uct_release_component_list(uct_components);\nout:\n    return status;\n}\n\nvoid uct_perf_barrier(ucx_perf_context_t *perf)\n{\n    rte_call(perf, barrier, (void(*)(void*))uct_worker_progress,\n             (void*)perf->uct.worker);\n}\n\nvoid ucp_perf_barrier(ucx_perf_context_t *perf)\n{\n    rte_call(perf, barrier, (void(*)(void*))ucp_worker_progress,\n#if _OPENMP\n             (void*)perf->ucp.tctx[omp_get_thread_num()].perf.ucp.worker);\n#else\n             (void*)perf->ucp.tctx[0].perf.ucp.worker);\n#endif\n}\n\nstatic ucs_status_t uct_perf_setup(ucx_perf_context_t *perf)\n{\n    ucx_perf_params_t *params = &perf->params;\n    uct_iface_config_t *iface_config;\n    ucs_status_t status;\n    uct_iface_params_t iface_params = {\n        .field_mask           = UCT_IFACE_PARAM_FIELD_OPEN_MODE   |\n                                UCT_IFACE_PARAM_FIELD_STATS_ROOT  |\n                                UCT_IFACE_PARAM_FIELD_RX_HEADROOM |\n                                UCT_IFACE_PARAM_FIELD_CPU_MASK,\n        .open_mode            = UCT_IFACE_OPEN_MODE_DEVICE,\n        .mode.device.tl_name  = params->uct.tl_name,\n        .mode.device.dev_name = params->uct.dev_name,\n        .stats_root           = ucs_stats_get_root(),\n        .rx_headroom          = 0\n    };\n    UCS_CPU_ZERO(&iface_params.cpu_mask);\n\n    status = ucs_async_context_init(&perf->uct.async, params->async_mode);\n    if (status != UCS_OK) {\n        goto out;\n    }\n\n    status = uct_worker_create(&perf->uct.async, params->thread_mode,\n                               &perf->uct.worker);\n    if (status != UCS_OK) {\n        goto out_cleanup_async;\n    }\n\n    status = uct_perf_create_md(perf);\n    if (status != UCS_OK) {\n        goto out_destroy_worker;\n    }\n\n    status = uct_md_iface_config_read(perf->uct.md, params->uct.tl_name, NULL,\n                                      NULL, &iface_config);\n    if (status != UCS_OK) {\n        goto out_destroy_md;\n    }\n\n    status = uct_iface_open(perf->uct.md, perf->uct.worker, &iface_params,\n                            iface_config, &perf->uct.iface);\n    uct_config_release(iface_config);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to open iface: %s\", ucs_status_string(status));\n        goto out_destroy_md;\n    }\n\n    status = uct_perf_test_check_capabilities(params, perf->uct.iface,\n                                              perf->uct.md);\n    /* sync status across all processes */\n    status = ucp_perf_test_exchange_status(perf, status);\n    if (status != UCS_OK) {\n        goto out_iface_close;\n    }\n\n    status = uct_perf_test_alloc_mem(perf);\n    if (status != UCS_OK) {\n        goto out_iface_close;\n    }\n\n    /* Enable progress before `uct_iface_flush` and `uct_worker_progress` called\n     * to give a chance to finish connection for some tranports (ib/ud, tcp).\n     * They may return UCS_INPROGRESS from `uct_iface_flush` when connections are\n     * in progress */\n    uct_iface_progress_enable(perf->uct.iface,\n                              UCT_PROGRESS_SEND | UCT_PROGRESS_RECV);\n\n    status = uct_perf_test_setup_endpoints(perf);\n    if (status != UCS_OK) {\n        ucs_error(\"Failed to setup endpoints: %s\", ucs_status_string(status));\n        goto out_free_mem;\n    }\n\n    return UCS_OK;\n\nout_free_mem:\n    uct_perf_test_free_mem(perf);\nout_iface_close:\n    uct_iface_close(perf->uct.iface);\nout_destroy_md:\n    uct_md_close(perf->uct.md);\nout_destroy_worker:\n    uct_worker_destroy(perf->uct.worker);\nout_cleanup_async:\n    ucs_async_context_cleanup(&perf->uct.async);\nout:\n    return status;\n}\n\nstatic void uct_perf_cleanup(ucx_perf_context_t *perf)\n{\n    uct_perf_test_cleanup_endpoints(perf);\n    uct_perf_test_free_mem(perf);\n    uct_iface_close(perf->uct.iface);\n    uct_md_close(perf->uct.md);\n    uct_worker_destroy(perf->uct.worker);\n    ucs_async_context_cleanup(&perf->uct.async);\n}\n\nstatic void ucp_perf_request_init(void *req)\n{\n    ucp_perf_request_t *request = req;\n\n    request->context = NULL;\n}\n\nstatic ucs_status_t ucp_perf_setup(ucx_perf_context_t *perf)\n{\n    ucp_params_t ucp_params;\n    ucp_worker_params_t worker_params;\n    ucp_config_t *config;\n    ucs_status_t status;\n    unsigned i, thread_count;\n    size_t message_size;\n\n    ucp_params.field_mask   = UCP_PARAM_FIELD_FEATURES |\n                              UCP_PARAM_FIELD_REQUEST_SIZE |\n                              UCP_PARAM_FIELD_REQUEST_INIT;\n    ucp_params.features     = 0;\n    ucp_params.request_size = sizeof(ucp_perf_request_t);\n    ucp_params.request_init = ucp_perf_request_init;\n\n    if (perf->params.thread_count > 1) {\n        /* when there is more than one thread, a ucp_worker would be created for\n         * each. all of them will share the same ucp_context */\n        ucp_params.features          |= UCP_PARAM_FIELD_MT_WORKERS_SHARED;\n        ucp_params.mt_workers_shared  = 1;\n    }\n\n    status = ucp_perf_test_fill_params(&perf->params, &ucp_params);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    status = ucp_config_read(NULL, NULL, &config);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    status = ucp_init(&ucp_params, config, &perf->ucp.context);\n    ucp_config_release(config);\n    if (status != UCS_OK) {\n        goto err;\n    }\n\n    thread_count = perf->params.thread_count;\n    message_size = ucx_perf_get_message_size(&perf->params);\n\n    status = ucp_perf_test_alloc_mem(perf);\n    if (status != UCS_OK) {\n        ucs_warn(\"ucp test failed to allocate memory\");\n        goto err_cleanup;\n    }\n\n    perf->ucp.tctx = calloc(thread_count, sizeof(ucx_perf_thread_context_t));\n    if (perf->ucp.tctx == NULL) {\n        ucs_warn(\"ucp test failed to allocate memory for thread contexts\");\n        goto err_free_mem;\n    }\n\n    worker_params.field_mask  = UCP_WORKER_PARAM_FIELD_THREAD_MODE;\n    worker_params.thread_mode = perf->params.thread_mode;\n\n    for (i = 0; i < thread_count; i++) {\n        perf->ucp.tctx[i].tid              = i;\n        perf->ucp.tctx[i].perf             = *perf;\n        /* Doctor the src and dst buffers to make them thread specific */\n        perf->ucp.tctx[i].perf.send_buffer =\n                        UCS_PTR_BYTE_OFFSET(perf->send_buffer, i * message_size);\n        perf->ucp.tctx[i].perf.recv_buffer =\n                        UCS_PTR_BYTE_OFFSET(perf->recv_buffer, i * message_size);\n\n        status = ucp_worker_create(perf->ucp.context, &worker_params,\n                                   &perf->ucp.tctx[i].perf.ucp.worker);\n        if (status != UCS_OK) {\n            goto err_free_tctx_destroy_workers;\n        }\n    }\n\n    status = ucp_perf_test_setup_endpoints(perf, ucp_params.features);\n    if (status != UCS_OK) {\n        if (perf->params.flags & UCX_PERF_TEST_FLAG_VERBOSE) {\n            ucs_error(\"Failed to setup endpoints: %s\", ucs_status_string(status));\n        }\n        goto err_free_tctx_destroy_workers;\n    }\n\n    return UCS_OK;\n\nerr_free_tctx_destroy_workers:\n    ucp_perf_test_destroy_workers(perf);\n    free(perf->ucp.tctx);\nerr_free_mem:\n    ucp_perf_test_free_mem(perf);\nerr_cleanup:\n    ucp_cleanup(perf->ucp.context);\nerr:\n    return status;\n}\n\nstatic void ucp_perf_cleanup(ucx_perf_context_t *perf)\n{\n    ucp_perf_test_cleanup_endpoints(perf);\n    ucp_perf_barrier(perf);\n    ucp_perf_test_free_mem(perf);\n    ucp_perf_test_destroy_workers(perf);\n    free(perf->ucp.tctx);\n    ucp_cleanup(perf->ucp.context);\n}\n\nstatic struct {\n    ucs_status_t (*setup)(ucx_perf_context_t *perf);\n    void         (*cleanup)(ucx_perf_context_t *perf);\n    ucs_status_t (*run)(ucx_perf_context_t *perf);\n    void         (*barrier)(ucx_perf_context_t *perf);\n} ucx_perf_funcs[] = {\n    [UCX_PERF_API_UCT] = {uct_perf_setup, uct_perf_cleanup,\n                          uct_perf_test_dispatch, uct_perf_barrier},\n    [UCX_PERF_API_UCP] = {ucp_perf_setup, ucp_perf_cleanup,\n                          ucp_perf_test_dispatch, ucp_perf_barrier}\n};\n\nstatic ucs_status_t ucx_perf_thread_spawn(ucx_perf_context_t *perf,\n                                          ucx_perf_result_t* result);\n\nucs_status_t ucx_perf_run(const ucx_perf_params_t *params,\n                          ucx_perf_result_t *result)\n{\n    ucx_perf_context_t *perf;\n    ucs_status_t status;\n\n    ucx_perf_global_init();\n\n    if (params->command == UCX_PERF_CMD_LAST) {\n        ucs_error(\"Test is not selected\");\n        status = UCS_ERR_INVALID_PARAM;\n        goto out;\n    }\n\n    if ((params->api != UCX_PERF_API_UCT) && (params->api != UCX_PERF_API_UCP)) {\n        ucs_error(\"Invalid test API parameter (should be UCT or UCP)\");\n        status = UCS_ERR_INVALID_PARAM;\n        goto out;\n    }\n\n    perf = malloc(sizeof(*perf));\n    if (perf == NULL) {\n        status = UCS_ERR_NO_MEMORY;\n        goto out;\n    }\n\n    ucx_perf_test_init(perf, params);\n\n    if (perf->allocator == NULL) {\n        ucs_error(\"Unsupported memory types %s<->%s\",\n                  ucs_memory_type_names[params->send_mem_type],\n                  ucs_memory_type_names[params->recv_mem_type]);\n        status = UCS_ERR_UNSUPPORTED;\n        goto out_free;\n    }\n\n    if ((params->api == UCX_PERF_API_UCT) &&\n        (perf->allocator->mem_type != UCS_MEMORY_TYPE_HOST)) {\n        ucs_warn(\"UCT tests also copy 2-byte values from %s memory to \"\n                 \"%s memory, which may impact performance results\",\n                 ucs_memory_type_names[perf->allocator->mem_type],\n                 ucs_memory_type_names[UCS_MEMORY_TYPE_HOST]);\n    }\n\n    status = perf->allocator->init(perf);\n    if (status != UCS_OK) {\n        goto out_free;\n    }\n\n    status = ucx_perf_funcs[params->api].setup(perf);\n    if (status != UCS_OK) {\n        goto out_free;\n    }\n\n    if (params->thread_count == 1) {\n        if (params->api == UCX_PERF_API_UCP) {\n            perf->ucp.worker      = perf->ucp.tctx[0].perf.ucp.worker;\n            perf->ucp.ep          = perf->ucp.tctx[0].perf.ucp.ep;\n            perf->ucp.remote_addr = perf->ucp.tctx[0].perf.ucp.remote_addr;\n            perf->ucp.rkey        = perf->ucp.tctx[0].perf.ucp.rkey;\n        }\n\n        if (params->warmup_iter > 0) {\n            ucx_perf_set_warmup(perf, params);\n            status = ucx_perf_funcs[params->api].run(perf);\n            if (status != UCS_OK) {\n                goto out_cleanup;\n            }\n\n            ucx_perf_funcs[params->api].barrier(perf);\n            ucx_perf_test_prepare_new_run(perf, params);\n        }\n\n        /* Run test */\n        status = ucx_perf_funcs[params->api].run(perf);\n        ucx_perf_funcs[params->api].barrier(perf);\n        if (status == UCS_OK) {\n            ucx_perf_calc_result(perf, result);\n            rte_call(perf, report, result, perf->params.report_arg, 1, 0);\n        }\n    } else {\n        status = ucx_perf_thread_spawn(perf, result);\n    }\n\nout_cleanup:\n    ucx_perf_funcs[params->api].cleanup(perf);\nout_free:\n    free(perf);\nout:\n    return status;\n}\n\n#if _OPENMP\n\nstatic ucs_status_t ucx_perf_thread_run_test(void* arg)\n{\n    ucx_perf_thread_context_t* tctx = (ucx_perf_thread_context_t*) arg; /* a single thread context */\n    ucx_perf_result_t* result       = &tctx->result;\n    ucx_perf_context_t* perf        = &tctx->perf;\n    ucx_perf_params_t* params       = &perf->params;\n    ucs_status_t status;\n\n    if (params->warmup_iter > 0) {\n        ucx_perf_set_warmup(perf, params);\n        status = ucx_perf_funcs[params->api].run(perf);\n        ucx_perf_funcs[params->api].barrier(perf);\n        if (UCS_OK != status) {\n            goto out;\n        }\n        ucx_perf_test_prepare_new_run(perf, params);\n    }\n\n    /* Run test */\n#pragma omp barrier\n    status = ucx_perf_funcs[params->api].run(perf);\n    ucx_perf_funcs[params->api].barrier(perf);\n    if (UCS_OK != status) {\n        goto out;\n    }\n\n    ucx_perf_calc_result(perf, result);\n\nout:\n    return status;\n}\n\nstatic void ucx_perf_thread_report_aggregated_results(ucx_perf_context_t *perf)\n{\n    ucx_perf_thread_context_t* tctx = perf->ucp.tctx;  /* all the thread contexts on perf */\n    unsigned i, thread_count        = perf->params.thread_count;\n    double lat_sum_total_avegare    = 0.0;\n    ucx_perf_result_t agg_result;\n\n    agg_result.iters        = tctx[0].result.iters;\n    agg_result.bytes        = tctx[0].result.bytes;\n    agg_result.elapsed_time = tctx[0].result.elapsed_time;\n\n    agg_result.bandwidth.total_average  = 0.0;\n    agg_result.bandwidth.typical        = 0.0; /* Undefined since used only for latency calculations */\n    agg_result.latency.total_average    = 0.0;\n    agg_result.msgrate.total_average    = 0.0;\n    agg_result.msgrate.typical          = 0.0; /* Undefined since used only for latency calculations */\n\n    /* when running with multiple threads, the moment average value is\n     * undefined since we don't capture the values of the last iteration */\n    agg_result.msgrate.moment_average   = 0.0;\n    agg_result.bandwidth.moment_average = 0.0;\n    agg_result.latency.moment_average   = 0.0;\n    agg_result.latency.typical          = 0.0;\n\n    /* in case of multiple threads, we have to aggregate the results so that the\n     * final output of the result would show the performance numbers that were\n     * collected from all the threads.\n     * BW and message rate values will be the sum of their values from all\n     * the threads, while the latency value is the average latency from the\n     * threads. */\n\n    for (i = 0; i < thread_count; i++) {\n        agg_result.bandwidth.total_average  += tctx[i].result.bandwidth.total_average;\n        agg_result.msgrate.total_average    += tctx[i].result.msgrate.total_average;\n        lat_sum_total_avegare               += tctx[i].result.latency.total_average;\n    }\n\n    agg_result.latency.total_average = lat_sum_total_avegare / thread_count;\n\n    rte_call(perf, report, &agg_result, perf->params.report_arg, 1, 1);\n}\n\nstatic ucs_status_t ucx_perf_thread_spawn(ucx_perf_context_t *perf,\n                                          ucx_perf_result_t* result)\n{\n    ucx_perf_thread_context_t* tctx = perf->ucp.tctx;   /* all the thread contexts on perf */\n    int ti, thread_count            = perf->params.thread_count;\n    ucs_status_t* statuses;\n    ucs_status_t status;\n\n    omp_set_num_threads(thread_count);\n\n    statuses = calloc(thread_count, sizeof(ucs_status_t));\n    if (statuses == NULL) {\n        status = UCS_ERR_NO_MEMORY;\n        goto out;\n    }\n\n#pragma omp parallel private(ti)\n{\n    ti              = omp_get_thread_num();\n    tctx[ti].status = ucx_perf_thread_run_test((void*)&tctx[ti]);\n}\n\n    status = UCS_OK;\n    for (ti = 0; ti < thread_count; ti++) {\n        if (UCS_OK != tctx[ti].status) {\n            ucs_error(\"Thread %d failed to run test: %s\", tctx[ti].tid,\n                      ucs_status_string(tctx[ti].status));\n            status = tctx[ti].status;\n        }\n    }\n\n    ucx_perf_thread_report_aggregated_results(perf);\n\n    free(statuses);\nout:\n    return status;\n}\n#else\nstatic ucs_status_t ucx_perf_thread_spawn(ucx_perf_context_t *perf,\n                                          ucx_perf_result_t* result) {\n    ucs_error(\"Invalid test parameter (thread mode requested without OpenMP capabilities)\");\n    return UCS_ERR_INVALID_PARAM;\n}\n#endif /* _OPENMP */\n\nvoid ucx_perf_global_init()\n{\n    static ucx_perf_allocator_t host_allocator = {\n        .mem_type  = UCS_MEMORY_TYPE_HOST,\n        .init      = ucs_empty_function_return_success,\n        .ucp_alloc = ucp_perf_test_alloc_host,\n        .ucp_free  = ucp_perf_test_free_host,\n        .uct_alloc = uct_perf_test_alloc_host,\n        .uct_free  = uct_perf_test_free_host,\n        .memcpy    = ucx_perf_test_memcpy_host,\n        .memset    = memset\n    };\n    UCS_MODULE_FRAMEWORK_DECLARE(ucx_perftest);\n\n    ucx_perf_mem_type_allocators[UCS_MEMORY_TYPE_HOST] = &host_allocator;\n\n    /* FIXME Memtype allocator modules must be loaded to global scope, otherwise\n     * alloc hooks, which are using dlsym() to get pointer to original function,\n     * do not work. Need to use bistro for memtype hooks to fix it.\n     */\n    UCS_MODULE_FRAMEWORK_LOAD(ucx_perftest, UCS_MODULE_LOAD_FLAG_GLOBAL);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/test/mpi/test_memhooks.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n#define _GNU_SOURCE /* For basename */\n#include <mpi.h>\n\n#include <ucs/sys/preprocessor.h>\n#include <ucm/api/ucm.h>\n#include <sys/mman.h>\n#include <sys/shm.h>\n#include <malloc.h>\n#include <dlfcn.h>\n#include <unistd.h>\n#include <string.h>\n#include <stdlib.h>\n\n#define CHKERR_JUMP(cond, msg, label) \\\n    do { \\\n        if (cond) { \\\n            printf(\"%s:%d: %s\\n\", basename(__FILE__), __LINE__, msg); \\\n            goto label; \\\n        } \\\n    } while (0)\n\n#define DL_FIND_FUNC(dl, func_name, func, err_action) \\\n    do { \\\n        char *error; \\\n        dlerror(); /* clear existing errors */ \\\n        func = dlsym(dl, func_name); \\\n        if (((error = dlerror()) != NULL) || (func == NULL)) { \\\n            error = error ? error : \"not found\"; \\\n            fprintf(stderr, \"Failed to resolve symbol '%s': %s\\n\", \\\n                    func_name, error); \\\n            err_action; \\\n        } \\\n    } while (0);\n\n#define SHMAT_FAILED ((void*)-1)\n\nvoid* open_dyn_lib(const char *lib_path);\nvoid* flag_no_install_init(const char *path);\nint malloc_hooks_run_all(void *dl);\nint malloc_hooks_run_unmapped(void *dl);\nint ext_event_run(void *dl);\nvoid *ext_event_init(const char *path);\n\ntypedef struct memtest_type {\n    const char *name;\n    void*      (*init)(const char *path);\n    int        (*run) (void *arg);\n} memtest_type_t;\n\nmemtest_type_t tests[] = {\n    {\"malloc_hooks\",          open_dyn_lib,         malloc_hooks_run_all},\n    {\"malloc_hooks_unmapped\", open_dyn_lib,         malloc_hooks_run_unmapped},\n    {\"external_events\",       ext_event_init,       ext_event_run},\n    {\"flag_no_install\",       flag_no_install_init, ext_event_run},\n    {NULL}\n};\n\nstatic volatile size_t total_mapped = 0;\nstatic volatile size_t total_unmapped = 0;\n\nstatic void usage() {\n    printf(\"Usage: test_memhooks [options]\\n\");\n    printf(\"Options are:\\n\");\n    printf(\"  -h         Print this info.\\n\");\n    printf(\"  -t <name>  Test name to execute (malloc_hooks)\\n\");\n    printf(\"                 malloc_hooks          : General UCM test for VM_MAPPED and VM_UNMAPPED\\n\");\n    printf(\"                 malloc_hooks_unmapped : Test VM_UNMAPPED event only\\n\");\n    printf(\"                 external_events       : Test of ucm_set_external_event() API\\n\");\n    printf(\"                 flag_no_install       : Test of UCM_EVENT_FLAG_NO_INSTALL flag\\n\");\n    printf(\"\\n\");\n}\n\nstatic void event_callback(ucm_event_type_t event_type, ucm_event_t *event,\n                           void *arg)\n{\n    if (event_type == UCM_EVENT_VM_MAPPED) {\n        total_mapped += event->vm_mapped.size;\n    } else if (event_type == UCM_EVENT_VM_UNMAPPED) {\n        total_unmapped += event->vm_unmapped.size;\n    }\n}\n\nstatic ucs_status_t set_event_handler(void *dl, int events)\n{\n    ucs_status_t (*set_handler)(int events, int priority,\n                                ucm_event_callback_t cb, void *arg);\n\n    DL_FIND_FUNC(dl, \"ucm_set_event_handler\", set_handler,\n                 return UCS_ERR_UNSUPPORTED);\n\n    return set_handler(events, 0, event_callback, NULL);\n}\n\nstatic ucs_status_t disable_memory_hooks(void *dl)\n{\n    setenv(\"UCX_MEM_MALLOC_HOOKS\", \"n\", 1);\n    setenv(\"UCX_MEM_MMAP_RELOC\",   \"n\", 1);\n    return UCS_OK;\n}\n\nvoid* open_dyn_lib(const char *lib_path)\n{\n    void *dl = dlopen(lib_path, RTLD_LAZY);\n    char *error;\n\n    if (dl == NULL) {\n        error = dlerror();\n        error = error ? error : \"unknown error\";\n        fprintf(stderr, \"Failed to load '%s': %s\\n\", lib_path, error);\n    }\n    return dl;\n}\n\n\nvoid *ext_event_init(const char *path)\n{\n    void (*set_ext_event)(int events);\n    ucs_status_t status;\n    void *dl_ucm;\n\n    dl_ucm = open_dyn_lib(path);\n    if (dl_ucm == NULL) {\n        return NULL;\n    }\n\n    status = disable_memory_hooks(dl_ucm);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to disable memory hooks\", fail);\n\n    DL_FIND_FUNC(dl_ucm, \"ucm_set_external_event\", set_ext_event, goto fail);\n    set_ext_event(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n\n    status = set_event_handler(dl_ucm, UCM_EVENT_VM_MAPPED |\n                                       UCM_EVENT_VM_UNMAPPED);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to set event handler\", fail);\n\n    return dl_ucm;\n\nfail:\n    dlclose(dl_ucm);\n    return NULL;\n}\n\nvoid* flag_no_install_init(const char *path)\n{\n    void *dl_ucm;\n    ucs_status_t status;\n\n    dl_ucm = open_dyn_lib(path);\n    if (dl_ucm == NULL) {\n        return NULL;\n    }\n\n    status = disable_memory_hooks(dl_ucm);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to disable memory hooks\", fail);\n\n    status = set_event_handler(dl_ucm, UCM_EVENT_VM_MAPPED   |\n                                       UCM_EVENT_VM_UNMAPPED |\n                                       UCM_EVENT_FLAG_NO_INSTALL);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to set event handler\", fail);\n    return dl_ucm;\n\nfail:\n    dlclose(dl_ucm);\n    return NULL;\n}\n\nint malloc_hooks_run_flags(void *dl, ucm_event_type_t events)\n{\n    ucs_status_t status;\n    void *ptr_malloc_core = NULL;\n    void *ptr_malloc_mmap = NULL;\n    void *ptr_direct_mmap = MAP_FAILED;\n    int  shmid            = -1;\n    void *ptr_shmat       = SHMAT_FAILED;\n    void *dl_test;\n    const size_t size = 1024 * 1024;\n    const char *lib_path = UCS_PP_MAKE_STRING(TEST_LIB_DIR) \"/\" \"libtest_memhooks.so\";\n    const char *cust_mmap_name  = \"memhook_test_lib_call_mmap\";\n    void * (*cust_mmap)(size_t size);\n\n    status = set_event_handler(dl, events);\n    CHKERR_JUMP(status != UCS_OK, \"Failed to set event handler\", fail_close_ucm);\n\n    printf(\"Allocating memory\\n\");\n\n    /* Create SysV segment */\n    shmid = shmget(IPC_PRIVATE, size, IPC_CREAT|SHM_R|SHM_W);\n    CHKERR_JUMP(shmid == -1, \"Failed to create shared memory segment: %m\",\n                fail_close_ucm);\n\n    /*\n     * Test shmat/shmdt before malloc() because shmat() add entires to an internal\n     * hash of pointers->size, which makes previous pointers un-releasable\n     */\n\n    /* Attach SysV segment */\n    total_mapped = 0;\n    ptr_shmat = shmat(shmid, NULL, 0);\n    CHKERR_JUMP(ptr_shmat == SHMAT_FAILED, \"Failed to attach shared memory segment\",\n                fail_close_ucm);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped < size, \"No callback for shmat\", fail_close_ucm);\n    }\n    printf(\"After shmat: reported mapped=%zu\\n\", total_mapped);\n\n    /* Detach SysV segment */\n    total_unmapped = 0;\n    shmdt(ptr_shmat);\n    ptr_shmat = SHMAT_FAILED;\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No callback for shmdt\", fail_close_ucm);\n    }\n    printf(\"After shmdt: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Attach SysV segment at fixed address */\n    total_mapped = 0;\n    total_unmapped = 0;\n    ptr_shmat = shmat(shmid, (void*)0xff000000, SHM_REMAP);\n    CHKERR_JUMP(ptr_shmat == SHMAT_FAILED, \"Failed to attach shared memory segment\",\n                fail_close_ucm);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped < size, \"No map callback for shmat(REMAP)\", fail_close_ucm);\n    }\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No unmap callback for shmat(REMAP)\",\n                    fail_close_ucm);\n    }\n    printf(\"After shmat(REMAP): reported mapped=%zu unmapped=%zu\\n\", total_mapped,\n           total_unmapped);\n\n    /* Detach SysV segment */\n    total_unmapped = 0;\n    shmdt(ptr_shmat);\n    ptr_shmat = SHMAT_FAILED;\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No callback for shmdt\", fail_close_ucm);\n    }\n    printf(\"After shmdt: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Destroy SysV segment */\n    shmctl(shmid, IPC_RMID, NULL);\n    shmid = -1;\n\n    /* Allocate using morecore */\n    mallopt(M_MMAP_THRESHOLD, size * 2);\n    mallopt(M_TRIM_THRESHOLD, size / 2);\n    total_mapped = 0;\n    ptr_malloc_core = malloc(1024 * 1024);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped == 0, \"No callback for core malloc\",\n                    fail_close_ucm);\n    }\n    printf(\"After core malloc: reported mapped=%zu\\n\", total_mapped);\n\n    /* Allocate using mmap */\n    mallopt(M_MMAP_THRESHOLD, size / 2);\n    total_mapped = 0;\n    ptr_malloc_mmap = malloc(2 * 1024 * 1024);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped == 0, \"No callback for mmap malloc\",\n                    fail_close_ucm);\n    }\n    printf(\"After mmap malloc: reported mapped=%zu\\n\", total_mapped);\n\n    /* Allocate directly with mmap */\n    total_mapped = 0;\n    ptr_direct_mmap = mmap(NULL, size, PROT_READ|PROT_WRITE,\n                           MAP_PRIVATE|MAP_ANON, -1, 0);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped < size, \"No callback for mmap\", fail_close_ucm);\n    }\n    printf(\"After mmap: reported mapped=%zu\\n\", total_mapped);\n\n    /* Remap */\n    total_unmapped = 0;\n    ptr_direct_mmap = mmap(ptr_direct_mmap, size, PROT_READ|PROT_WRITE,\n                           MAP_PRIVATE|MAP_ANON|MAP_FIXED, -1, 0);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped < size, \"No unmap callback for mmap(FIXED)\",\n                    fail_close_ucm);\n    }\n    printf(\"After mmap(FIXED): reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Call munmap directly */\n    total_unmapped = 0;\n    munmap(ptr_direct_mmap, size);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped == 0, \"No callback for munmap\", fail_close_ucm);\n    }\n    printf(\"After munmap: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Release indirectly */\n    total_unmapped = 0;\n    free(ptr_malloc_mmap);\n    ptr_malloc_mmap = NULL;\n    malloc_trim(0);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped == 0, \"No callback for munmap from free\",\n                    fail_close_ucm);\n    }\n    printf(\"After mmap free + trim: reported unmapped=%zu\\n\", total_unmapped);\n\n    /* Call mmap from a library we load after hooks are installed */\n    dl_test = open_dyn_lib(lib_path);\n    CHKERR_JUMP(dl_test == NULL, \"Failed to load test lib\", fail_close_ucm);\n\n    DL_FIND_FUNC(dl_test, cust_mmap_name, cust_mmap, goto fail_close_all);\n    total_mapped = 0;\n    ptr_direct_mmap = cust_mmap(size);\n    CHKERR_JUMP(ptr_direct_mmap == MAP_FAILED, \"Failed to mmap from dynamic lib\",\n                fail_close_all);\n    if (events & UCM_EVENT_VM_MAPPED) {\n        CHKERR_JUMP(total_mapped == 0,\"No callback for mmap from dynamic lib\",\n                    fail_close_all);\n    }\n    printf(\"After another mmap from dynamic lib: reported mapped=%zu\\n\", total_mapped);\n    munmap(ptr_direct_mmap, size);\n    ptr_direct_mmap = MAP_FAILED;\n\n    /*\n     * Test closing UCM.\n     * The library should not really be unloaded, because the memory hooks still\n     * point to functions inside it.\n     */\n    total_unmapped = 0;\n    dlclose(dl);\n    dlclose(dl_test);\n    free(ptr_malloc_core); /* This should still work */\n    ptr_malloc_core = NULL;\n    malloc_trim(0);\n    if (events & UCM_EVENT_VM_UNMAPPED) {\n        CHKERR_JUMP(total_unmapped == 0, \"No callback for munmap from malloc\", fail);\n    }\n    printf(\"After core malloc free: reported unmapped=%zu\\n\", total_unmapped);\n\n    return 0;\n\nfail_close_all:\n    dlclose(dl_test);\nfail_close_ucm:\n    dlclose(dl);\nfail:\n    if (ptr_shmat != SHMAT_FAILED) {\n        shmdt(ptr_shmat);\n    }\n    if (shmid != -1) {\n        shmctl(shmid, IPC_RMID, NULL);\n    }\n    free(ptr_malloc_mmap);\n    free(ptr_malloc_core);\n    if (ptr_direct_mmap != MAP_FAILED) {\n        munmap(ptr_direct_mmap, size);\n    }\n\n    return  -1;\n}\n\nint malloc_hooks_run_all(void *dl)\n{\n    return malloc_hooks_run_flags(dl, UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n}\n\nint malloc_hooks_run_unmapped(void *dl)\n{\n    return malloc_hooks_run_flags(dl, UCM_EVENT_VM_UNMAPPED);\n}\n\nint ext_event_run(void *dl)\n{\n    void *ptr_direct_mmap;\n    void (*ucm_event)(void *addr, size_t length);\n    const size_t size = 1024 * 1024;\n    int ret = -1;\n\n    /* Allocate directly with mmap */\n    total_mapped = 0;\n    ptr_direct_mmap = mmap(NULL, size, PROT_READ|PROT_WRITE,\n                           MAP_PRIVATE|MAP_ANON, -1, 0);\n    printf(\"totmapped %lu\\n\", total_mapped);\n    /* No callback should be called as we registered events to be external */\n    CHKERR_JUMP(total_mapped != 0,\n                \"Callback for mmap invoked, while hooks were not set\", fail);\n    DL_FIND_FUNC(dl, \"ucm_vm_mmap\", ucm_event, goto fail);\n    ucm_event(ptr_direct_mmap, size);\n    CHKERR_JUMP(total_mapped == 0, \"Callback for mmap is not called\", fail);\n    printf(\"After ucm_vm_mmap called: mapped=%zu\\n\", total_mapped);\n\n    /* Call munmap directly */\n    total_unmapped = 0;\n    munmap(ptr_direct_mmap, size);\n    CHKERR_JUMP(total_unmapped != 0,\n                \"Callback for munmap invoked, while hooks were not set\\n\", fail);\n\n    DL_FIND_FUNC(dl, \"ucm_vm_munmap\", ucm_event, goto fail);\n    ucm_event(ptr_direct_mmap, size);\n    CHKERR_JUMP(total_unmapped == 0, \"Callback for mmap is not called\", fail);\n    printf(\"After ucm_vm_munmap: unmapped=%zu\\n\", total_unmapped);\n\n    ret = 0;\n\nfail:\n    dlclose(dl);\n    return ret;\n}\n\nint main(int argc, char **argv)\n{\n    const char *ucm_path = UCS_PP_MAKE_STRING(UCM_LIB_DIR) \"/\" \"libucm.so\";\n    memtest_type_t *test = tests;\n    void *dl;\n    int ret;\n    int c;\n\n    while ((c = getopt(argc, argv, \"t:h\")) != -1) {\n        switch (c) {\n        case 't':\n            for (test = tests; test->name != NULL; ++test) {\n                if (!strcmp(test->name, optarg)){\n                    break;\n                }\n            }\n            if (test->name == NULL) {\n                fprintf(stderr, \"Wrong test name %s\\n\", optarg);\n                return -1;\n            }\n            break;\n        case 'h':\n        default:\n            usage();\n            return -1;\n        }\n    }\n\n    /* Some tests need to modify UCM config before to call ucp_init,\n     * which may be called by MPI_Init */\n    dl = test->init(ucm_path);\n    if (dl == NULL) {\n        return -1;\n    }\n\n    printf(\"%s: initialized\\n\", test->name);\n\n    MPI_Init(&argc, &argv);\n\n    ret = test->run(dl);\n\n    printf(\"%s: %s\\n\", test->name, ret == 0 ? \"PASS\" : \"FAIL\");\n\n    MPI_Finalize();\n    return ret;\n}\n\n\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/test/gtest/ucm/malloc_hook.cc": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#define __STDC_LIMIT_MACROS\n\n#include <ucm/api/ucm.h>\n\n#include <ucs/arch/atomic.h>\n#include <ucs/type/status.h>\n#include <common/test.h>\n#include <common/test_helpers.h>\n#include <pthread.h>\n#include <sstream>\n#include <stdint.h>\n#include <dlfcn.h>\n#include <libgen.h>\n\nextern \"C\" {\n#include <ucs/time/time.h>\n#include <ucm/malloc/malloc_hook.h>\n#include <ucm/bistro/bistro.h>\n#include <ucs/sys/sys.h>\n#include <malloc.h>\n}\n\n#if HAVE_MALLOC_SET_STATE && HAVE_MALLOC_GET_STATE\n#  define HAVE_MALLOC_STATES 1\n#endif /* HAVE_MALLOC_SET_STATE && HAVE_MALLOC_GET_STATE */\n\n#define EXPECT_INCREASED(_value, _prev, _size, _name)  \\\n    {                                                  \\\n        EXPECT_GE(_value, (_prev) + (_size)) << _name; \\\n        _prev = _value;                                \\\n    }\n\n\nclass malloc_hook_test_no_events : public ucs::test {\nprotected:\n    virtual ~malloc_hook_test_no_events()\n    {\n    }\n\n    static void empty_event_callback(ucm_event_type_t event_type,\n                                     ucm_event_t *event, void *arg)\n    {\n    }\n\n    virtual void init()\n    {\n        ucs::test::init();\n        m_enable_events = ucm_global_opts.enable_events;\n        ucm_global_opts.enable_events = 0;\n    }\n\n    virtual void cleanup()\n    {\n        ucm_global_opts.enable_events = m_enable_events;\n        ucs::test::cleanup();\n    }\n\n    int m_enable_events;\n};\n\nUCS_TEST_F(malloc_hook_test_no_events, empty_event) {\n    ucs_status_t status;\n    status = ucm_set_event_handler(0, 0, empty_event_callback, NULL);\n    ASSERT_UCS_OK(status);\n    ucm_unset_event_handler(0, empty_event_callback, NULL);\n}\n\n\ntemplate <class T>\nclass mhook_thread {\npublic:\n    mhook_thread(T *test): m_test(test)\n    {\n        pthread_create(&m_thread, NULL, thread_func, reinterpret_cast<void*>(m_test));\n    }\n\n    ~mhook_thread() {\n        join();\n        delete m_test;\n    }\n\n    void join() {\n        void *retval;\n        pthread_join(m_thread, &retval);\n    }\n\nprotected:\n    T         *m_test;\n    pthread_t m_thread;\n\n    static void *thread_func(void *arg) {\n        T *test = reinterpret_cast<T*>(arg);\n        test->test();\n        return NULL;\n    }\n};\n\ntemplate <class T>\nclass mmap_event {\npublic:\n    mmap_event(T *test): m_test(test), m_events(0), m_external_events(0)\n    {\n    }\n\n    ~mmap_event()\n    {\n        unset();\n        unset_external();\n    }\n\n    ucs_status_t set(int events)\n    {\n        ucs_status_t status;\n\n        status = ucm_set_event_handler(events, 0, mem_event_callback,\n                                       reinterpret_cast<void*>(m_test));\n        ASSERT_UCS_OK(status);\n        m_events |= events;\n        return status;\n    }\n\n    ucs_status_t set_external(int events)\n    {\n        ucm_set_external_event(events);\n        m_external_events |= events;\n        return UCS_OK;\n    }\n\n    void unset()\n    {\n        if (m_events) {\n            ucm_unset_event_handler(m_events, mem_event_callback,\n                                    reinterpret_cast<void*>(m_test));\n            m_events = 0;\n        }\n    }\n\n    void unset_external()\n    {\n        if (m_external_events) {\n            ucm_unset_external_event(m_external_events);\n            m_external_events = 0;\n        }\n    }\n\nprotected:\n    T   *m_test;\n    int m_events;\n    int m_external_events;\n\n    static void mem_event_callback(ucm_event_type_t event_type,\n                                   ucm_event_t *event,\n                                   void *arg)\n    {\n        T *test = reinterpret_cast<T*>(arg);\n        test->mem_event(event_type, event);\n    }\n};\n\n\nclass malloc_hook : public ucs::test {\n    friend class mmap_event<malloc_hook>;\nprotected:\n    /* use template argument to call/not call vm_unmap handler */\n    /* GCC 4.4.7 doesn't allow to define template static function\n     * with integer template argument. using template inner class\n     * to define static function */\n    template <int C> class bistro_hook {\n    public:\n        static int munmap(void *addr, size_t length)\n        {\n            UCM_BISTRO_PROLOGUE;\n            malloc_hook::bistro_call_counter++;\n            if (C) {\n                /* notify aggregate vm_munmap event only */\n                ucm_vm_munmap(addr, length);\n            }\n            int res = (intptr_t)syscall(SYS_munmap, addr, length);\n            UCM_BISTRO_EPILOGUE;\n            return res;\n        }\n\n        static int madvise(void *addr, size_t length, int advise)\n        {\n            UCM_BISTRO_PROLOGUE;\n            malloc_hook::bistro_call_counter++;\n            if (C) {\n                /* notify aggregate vm_munmap event only */\n                ucm_vm_munmap(addr, length);\n            }\n            int res = (intptr_t)syscall(SYS_madvise, addr, length, advise);\n            UCM_BISTRO_EPILOGUE;\n            return res;\n        }\n    };\n\n    class bistro_patch {\n    public:\n        bistro_patch(const char* symbol, void *hook)\n        {\n            ucs_status_t status;\n\n            status = ucm_bistro_patch(symbol, hook, &m_rp);\n            ASSERT_UCS_OK(status);\n            EXPECT_NE((intptr_t)m_rp, 0);\n        }\n\n        ~bistro_patch()\n        {\n            ucm_bistro_restore(m_rp);\n        }\n\n    protected:\n        ucm_bistro_restore_point_t *m_rp;\n    };\n\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n    {\n        m_got_event = 1;\n    }\n\n    virtual void init() {\n        ucs_status_t status;\n        mmap_event<malloc_hook> event(this);\n\n        ucs::skip_on_address_sanitizer();\n\n        m_got_event = 0;\n        ucm_malloc_state_reset(128 * 1024, 128 * 1024);\n        malloc_trim(0);\n        status = event.set(UCM_EVENT_VM_MAPPED);\n        ASSERT_UCS_OK(status);\n\n        for (;;) {\n            void *ptr = malloc(small_alloc_size);\n            if (m_got_event) {\n                /* If the heap grew, the minimal size is the previous one */\n                free(ptr);\n                break;\n            } else {\n                m_pts.push_back(ptr);\n            }\n        }\n        event.unset();\n    }\n\npublic:\n    static int            small_alloc_count;\n    static const size_t   small_alloc_size = 10000;\n    ucs::ptr_vector<void> m_pts;\n    int                   m_got_event;\n    static volatile int   bistro_call_counter;\n};\n\nstatic bool skip_on_bistro() {\n    return (ucm_global_opts.mmap_hook_mode == UCM_MMAP_HOOK_BISTRO);\n}\n\nstatic bool skip_on_bistro_without_valgrind() {\n    /* BISTRO is disabled under valgrind, we may run tests */\n    return (skip_on_bistro() && !RUNNING_ON_VALGRIND);\n}\n\nint malloc_hook::small_alloc_count            = 1000 / ucs::test_time_multiplier();\nvolatile int malloc_hook::bistro_call_counter = 0;\n\nclass test_thread {\npublic:\n    test_thread(const std::string& name, int num_threads, pthread_barrier_t *barrier,\n                malloc_hook *test, void (test_thread::*test_func)() = &test_thread::test) :\n        m_name(name), m_num_threads(num_threads), m_barrier(barrier),\n        m_map_size(0), m_unmap_size(0), m_test(test), m_event(this)\n    {\n        pthread_mutex_init(&m_stats_lock, NULL);\n    }\n\n    ~test_thread() {\n        pthread_mutex_destroy(&m_stats_lock);\n    }\n\n    void test();\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event);\n\nprivate:\n    typedef std::pair<void*, void*> range;\n\n    bool is_ptr_in_range(void *ptr, size_t size, const std::vector<range> &ranges) {\n        for (std::vector<range>::const_iterator iter = ranges.begin(); iter != ranges.end(); ++iter) {\n            if ((ptr >= iter->first) && ((char*)ptr < iter->second)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    static pthread_mutex_t   lock;\n    static pthread_barrier_t barrier;\n\n    std::string        m_name;\n    int                m_num_threads;\n    pthread_barrier_t  *m_barrier;\n\n    pthread_mutex_t    m_stats_lock;\n    size_t             m_map_size;\n    size_t             m_unmap_size;\n    std::vector<range> m_map_ranges;\n    std::vector<range> m_unmap_ranges;\n\n    malloc_hook        *m_test;\n    mmap_event<test_thread> m_event;\n};\n\npthread_mutex_t test_thread::lock = PTHREAD_MUTEX_INITIALIZER;\n\nvoid test_thread::mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n{\n    pthread_mutex_lock(&m_stats_lock);\n    switch (event_type) {\n    case UCM_EVENT_VM_MAPPED:\n        m_map_ranges.push_back(range(event->vm_mapped.address,\n                                     (char*)event->vm_mapped.address + event->vm_mapped.size));\n        m_map_size += event->vm_mapped.size;\n        break;\n    case UCM_EVENT_VM_UNMAPPED:\n        m_unmap_ranges.push_back(range(event->vm_unmapped.address,\n                                       (char*)event->vm_unmapped.address + event->vm_unmapped.size));\n        m_unmap_size += event->vm_unmapped.size;\n        break;\n    default:\n        break;\n    }\n    pthread_mutex_unlock(&m_stats_lock);\n}\n\nvoid test_thread::test() {\n    static const size_t large_alloc_size = 40 * 1024 * 1024;\n    ucs_status_t result;\n    ucs::ptr_vector<void> old_ptrs;\n    ucs::ptr_vector<void> new_ptrs;\n    void *ptr_r;\n    size_t small_map_size;\n    const size_t small_alloc_size = malloc_hook::small_alloc_size;\n    int num_ptrs_in_range;\n    static volatile uint32_t total_ptrs_in_range = 0;\n    char *test_str;\n\n    /* Allocate some pointers with old heap manager */\n    for (unsigned i = 0; i < 10; ++i) {\n        old_ptrs.push_back(malloc(small_alloc_size));\n    }\n\n    ptr_r = malloc(small_alloc_size);\n\n    m_map_ranges.reserve  ((m_test->small_alloc_count * 8 + 10) * m_num_threads);\n    m_unmap_ranges.reserve((m_test->small_alloc_count * 8 + 10) * m_num_threads);\n\n    total_ptrs_in_range = 0;\n\n    pthread_barrier_wait(m_barrier);\n\n    /* Install memory hooks */\n    result = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(result);\n\n    /* Allocate small pointers with new heap manager */\n    for (int i = 0; i < m_test->small_alloc_count; ++i) {\n        new_ptrs.push_back(malloc(small_alloc_size));\n    }\n    small_map_size = m_map_size;\n\n    /* If this test runs more than once, then sbrk may not really allocate new\n     * memory\n     */\n    EXPECT_GT(m_map_size, 0lu) << m_name;\n\n    num_ptrs_in_range = 0;\n    for (ucs::ptr_vector<void>::const_iterator iter = new_ptrs.begin();\n                    iter != new_ptrs.end(); ++iter)\n    {\n        if (is_ptr_in_range(*iter, small_alloc_size, m_map_ranges)) {\n            ++num_ptrs_in_range;\n        }\n    }\n\n    /* Need at least one ptr in the mapped ranges in one the threads */\n    ucs_atomic_add32(&total_ptrs_in_range, num_ptrs_in_range);\n    pthread_barrier_wait(m_barrier);\n\n    EXPECT_GT(total_ptrs_in_range, 0u);\n\n    /* Allocate large chunk */\n    void *ptr = malloc(large_alloc_size);\n    EXPECT_GE(m_map_size, large_alloc_size + small_map_size) << m_name;\n    EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_map_ranges)) << m_name;\n    EXPECT_GE(malloc_usable_size(ptr), large_alloc_size);\n\n    free(ptr);\n    EXPECT_GE(m_unmap_size, large_alloc_size) << m_name;\n    /* coverity[pass_freed_arg] */\n    EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_unmap_ranges)) << m_name;\n\n    /* Test strdup */\n    void *s = strdup(\"test\");\n    free(s);\n\n    /* Test setenv */\n    pthread_mutex_lock(&lock);\n    setenv(\"TEST\", \"VALUE\", 1);\n    test_str = getenv(\"TEST\");\n    if (test_str != NULL) {\n        EXPECT_EQ(std::string(\"VALUE\"), test_str);\n    } else {\n        UCS_TEST_ABORT(\"getenv(\\\"TEST\\\") returned NULL\");\n    }\n    pthread_mutex_unlock(&lock);\n\n    /* Test username */\n    ucs_get_user_name();\n\n    /* Test usable size */\n    EXPECT_GE(malloc_usable_size(ptr_r), small_alloc_size);\n\n    /* Test realloc */\n    ptr_r = realloc(ptr_r, small_alloc_size / 2);\n    free(ptr_r);\n\n    /* Test C++ allocations */\n    {\n        std::vector<char> vec(large_alloc_size, 0);\n        ptr = &vec[0];\n        EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_map_ranges)) << m_name;\n    }\n\n    /* coverity[use_after_free] - we don't dereference ptr, just search it*/\n    EXPECT_TRUE(is_ptr_in_range(ptr, large_alloc_size, m_unmap_ranges)) << m_name;\n\n    /* Release old pointers (should not crash) */\n    old_ptrs.clear();\n\n    m_map_ranges.clear();\n    m_unmap_ranges.clear();\n\n    /* Don't release pointers before other threads exit, so they will map new memory\n     * and not reuse memory from other threads.\n     */\n    pthread_barrier_wait(m_barrier);\n\n    /* This must be done when all other threads are inactive, otherwise we may leak */\n#if HAVE_MALLOC_STATES\n    if (!RUNNING_ON_VALGRIND) {\n        pthread_mutex_lock(&lock);\n        void *state = malloc_get_state();\n        malloc_set_state(state);\n        free(state);\n        pthread_mutex_unlock(&lock);\n    }\n#endif /* HAVE_MALLOC_STATES */\n\n    pthread_barrier_wait(m_barrier);\n\n    /* Release new pointers  */\n    new_ptrs.clear();\n\n    /* Call several malloc routines */\n    malloc_trim(0);\n\n    ptr = malloc(large_alloc_size);\n\n    free(ptr);\n\n    /* shmat/shmdt */\n    size_t shm_seg_size = ucs_get_page_size() * 2;\n    int shmid = shmget(IPC_PRIVATE, shm_seg_size, IPC_CREAT | SHM_R | SHM_W);\n    EXPECT_NE(-1, shmid) << strerror(errno);\n\n    ptr = shmat(shmid, NULL, 0);\n    EXPECT_NE(MAP_FAILED, ptr) << strerror(errno);\n\n    shmdt(ptr);\n    shmctl(shmid, IPC_RMID, NULL);\n\n    EXPECT_TRUE(is_ptr_in_range(ptr, shm_seg_size, m_unmap_ranges));\n\n    ptr = mmap(NULL, shm_seg_size, PROT_READ|PROT_WRITE,\n               MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    ASSERT_NE(MAP_FAILED, ptr) << strerror(errno);\n    madvise(ptr, shm_seg_size, MADV_DONTNEED);\n\n    EXPECT_TRUE(is_ptr_in_range(ptr, shm_seg_size, m_unmap_ranges));\n    munmap(ptr, shm_seg_size);\n\n    /* Print results */\n    pthread_mutex_lock(&lock);\n    UCS_TEST_MESSAGE << m_name\n                     << \": small mapped: \" << small_map_size\n                     <<  \", total mapped: \" << m_map_size\n                     <<  \", total unmapped: \" << m_unmap_size;\n    std::cout.flush();\n    pthread_mutex_unlock(&lock);\n\n    m_event.unset();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, single_thread,\n                     skip_on_bistro_without_valgrind()) {\n    pthread_barrier_t barrier;\n    pthread_barrier_init(&barrier, NULL, 1);\n    {\n        mhook_thread<test_thread>(new test_thread(\"single-thread\", 1, &barrier, this));\n    }\n    pthread_barrier_destroy(&barrier);\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, multi_threads,\n                     skip_on_bistro_without_valgrind()) {\n    typedef mhook_thread<test_thread> thread_t;\n\n    static const int num_threads = 8;\n    ucs::ptr_vector<thread_t> threads;\n    pthread_barrier_t barrier;\n\n    malloc_trim(0);\n\n    pthread_barrier_init(&barrier, NULL, num_threads);\n    for (int i = 0; i < num_threads; ++i) {\n        std::stringstream ss;\n        ss << \"thread \" << i << \"/\" << num_threads;\n        threads.push_back(new thread_t(new test_thread(ss.str(), num_threads, &barrier, this)));\n    }\n\n    threads.clear();\n    pthread_barrier_destroy(&barrier);\n}\n\nUCS_TEST_F(malloc_hook, asprintf) {\n    /* Install memory hooks */\n    (void)dlerror();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, fork, \"broken\") {\n    static const int num_processes = 4;\n    pthread_barrier_t barrier;\n    std::vector<pid_t> pids;\n    pid_t pid;\n\n    for (int i = 0; i < num_processes; ++i) {\n        pid = fork();\n        if (pid == 0) {\n            pthread_barrier_init(&barrier, NULL, 1);\n            {\n                std::stringstream ss;\n                ss << \"process \" << i << \"/\" << num_processes;\n                test_thread thread(ss.str(), 1, &barrier, this);\n            }\n            pthread_barrier_destroy(&barrier);\n            throw ucs::exit_exception(HasFailure());\n        }\n        pids.push_back(pid);\n    }\n\n    for (int i = 0; i < num_processes; ++i) {\n        int status;\n        waitpid(pids[i], &status, 0);\n        EXPECT_EQ(0, WEXITSTATUS(status)) << \"Process \" << i << \" failed\";\n    }\n}\n\nclass malloc_hook_cplusplus : public malloc_hook {\npublic:\n\n    malloc_hook_cplusplus() :\n        m_mapped_size(0), m_unmapped_size(0),\n        m_dynamic_mmap_config(ucm_global_opts.enable_dynamic_mmap_thresh),\n        m_event(this) {\n    }\n\n    ~malloc_hook_cplusplus() {\n        ucm_global_opts.enable_dynamic_mmap_thresh = m_dynamic_mmap_config;\n    }\n\n    void set() {\n        ucs_status_t status;\n        status = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);\n        ASSERT_UCS_OK(status);\n    }\n\n    void unset() {\n        m_event.unset();\n    }\n\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n    {\n        switch (event_type) {\n        case UCM_EVENT_VM_MAPPED:\n            m_mapped_size   += event->vm_mapped.size;\n            break;\n        case UCM_EVENT_VM_UNMAPPED:\n            m_unmapped_size += event->vm_unmapped.size;\n            break;\n        default:\n            break;\n        }\n    }\n\nprotected:\n    double measure_alloc_time(size_t size, unsigned iters)\n    {\n        ucs_time_t start_time = ucs_get_time();\n        for (unsigned i = 0; i < iters; ++i) {\n            void *ptr = malloc(size);\n            /* prevent the compiler from optimizing-out the memory allocation */\n            *(volatile char*)ptr = '5';\n            free(ptr);\n        }\n        return ucs_time_to_sec(ucs_get_time() - start_time);\n    }\n\n    void test_dynamic_mmap_thresh()\n    {\n        const size_t size = 8 * UCS_MBYTE;\n\n        set();\n\n        std::vector<std::string> strs;\n\n        m_mapped_size = 0;\n        while (m_mapped_size < size) {\n            strs.push_back(std::string(size, 't'));\n        }\n\n        m_unmapped_size = 0;\n        strs.clear();\n        EXPECT_GE(m_unmapped_size, size);\n\n        m_mapped_size = 0;\n        while (m_mapped_size < size) {\n            strs.push_back(std::string());\n            strs.back().resize(size);\n        }\n\n        m_unmapped_size = 0;\n        strs.clear();\n        if (ucm_global_opts.enable_dynamic_mmap_thresh) {\n            EXPECT_EQ(0ul, m_unmapped_size);\n        } else {\n            EXPECT_GE(m_unmapped_size, size);\n        }\n\n        unset();\n    }\n\n    size_t m_mapped_size;\n    size_t m_unmapped_size;\n    int    m_dynamic_mmap_config;\n    mmap_event<malloc_hook_cplusplus> m_event;\n};\n\n\nclass mmap_hooks {\npublic:\n    mmap_hooks(const std::string& name, int num_threads, pthread_barrier_t *barrier):\n        m_num_threads(num_threads), m_mapped_size(0), m_unmapped_size(0),\n        m_search_ptr(NULL), m_is_ptr_found(false), m_name(name),\n        m_barrier(barrier), m_event(this)\n    {\n        pthread_spin_init(&m_lock,0);\n    }\n\n    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)\n    {\n        pthread_spin_lock(&m_lock);\n        switch (event_type) {\n        case UCM_EVENT_VM_MAPPED:\n            m_mapped_size += event->vm_mapped.size;\n            break;\n        case UCM_EVENT_VM_UNMAPPED:\n            m_unmapped_size += event->vm_unmapped.size;\n            if (m_search_ptr == event->vm_unmapped.address) {\n                m_is_ptr_found = true;\n            }\n            break;\n        default:\n            break;\n        }\n        pthread_spin_unlock(&m_lock);\n    }\n\n    void test()\n    {\n        /*\n         * Test memory mapping functions which override an existing mapping\n         */\n        size_t size          = ucs_get_page_size() * 800;\n        size_t mapped_size   = 0;\n        size_t unmapped_size = 0;\n        void *buffer;\n        int shmid;\n        ucs_status_t status;\n        int num_threads;\n\n        EXPECT_EQ(0u, m_mapped_size) << m_name;\n        EXPECT_EQ(0u, m_unmapped_size) << m_name;\n\n        status = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);\n        ASSERT_UCS_OK(status);\n\n        pthread_barrier_wait(m_barrier);\n\n        /* 1. Map a large buffer */\n        {\n            buffer = mmap(NULL, size, PROT_READ|PROT_WRITE,\n                                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n            ASSERT_NE(MAP_FAILED, buffer) << strerror(errno);\n\n            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, 0, m_name);\n        }\n\n        /*\n         * 2. Map another buffer in the same place.\n         *    Expected behavior: unmap event on the old buffer\n         */\n        {\n            void *remap = mmap(buffer, size, PROT_READ|PROT_WRITE,\n                               MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED, -1, 0);\n            ASSERT_EQ(buffer, remap);\n\n            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 3. Create a shared memory segment */\n        {\n            shmid = shmget(IPC_PRIVATE, size, IPC_CREAT | SHM_R | SHM_W);\n            ASSERT_NE(-1, shmid) << strerror(errno) << m_name;\n        }\n\n        /*\n         * 4. Attach the segment at the same buffer address.\n         *    Expected behavior: unmap event on the old buffer\n         */\n        {\n            m_is_ptr_found = false;\n            m_search_ptr   = buffer;\n\n            /* Make sure every thread will have a unique value of 'buffer' - no\n             * thread will release its buffer before all others already\n             * allocated theirs */\n            pthread_barrier_wait(m_barrier);\n\n            /* adding 0x1 to 'buffer' with SHM_RND flag should still send event\n             * for 'buffer', because it aligns to SHMLBA\n             */\n            void *shmaddr = shmat(shmid, (char*)buffer + 0x1, SHM_REMAP | SHM_RND);\n            ASSERT_EQ(buffer, shmaddr) << m_name;\n\n            if (SHMLBA > 0x1) {\n                EXPECT_TRUE(m_is_ptr_found);\n            }\n            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 5. Detach the sysv segment */\n        {\n            shmdt(buffer);\n\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 6. Remove the shared memory segment */\n        {\n            int ret = shmctl(shmid, IPC_RMID, NULL);\n            ASSERT_NE(-1, ret) << strerror(errno);\n        }\n\n        /* 7. Unmap the buffer */\n        {\n            munmap(buffer, size);\n\n            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n        }\n\n        /* 8. sbrk call - single thread only */\n        if (!RUNNING_ON_VALGRIND && (m_num_threads < 2)) {\n            num_threads = 0;\n            ucs_sys_enum_threads(enum_threads_cb, &num_threads);\n            // use sbrk() only if there are 3 threads in the system:\n            //   1. main thread\n            //   2. watchdog thread\n            //   3. test thread\n            // otherwise, another thread can call use malloc/free in same time,\n            // leading to heap corruption\n\n            if (num_threads <= 3) {\n                /* valgrind failed when sbrk is called directly,\n                 * also sbrk is not thread safe */\n\n                /* sbrk call is used to extend/cut memory heap,\n                 * don't add any evaluations between calls sbrk+/sbrk- - it\n                 * may break heap */\n                sbrk(size);\n                sbrk(-size);\n\n                EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);\n                EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);\n            }\n        }\n        pthread_barrier_wait(m_barrier);\n    }\n\nprotected:\n    int                     m_num_threads;\n    size_t                  m_mapped_size;\n    size_t                  m_unmapped_size;\n    void                    *m_search_ptr;\n    bool                    m_is_ptr_found;\n    pthread_spinlock_t      m_lock;\n    std::string             m_name;\n    pthread_barrier_t       *m_barrier;\n    mmap_event<mmap_hooks>  m_event;\n\n    static ucs_status_t enum_threads_cb(pid_t tid, void *ctx)\n    {\n        (*(int*)ctx)++;\n        return UCS_OK;\n    }\n};\n\n\nUCS_TEST_F(malloc_hook_cplusplus, new_delete) {\n    const size_t size = 8 * 1000 * 1000;\n\n    set();\n\n    {\n        std::vector<char> vec1(size, 0);\n        std::vector<char> vec2(size, 0);\n        std::vector<char> vec3(size, 0);\n    }\n\n    {\n        std::vector<char> vec1(size, 0);\n        std::vector<char> vec2(size, 0);\n        std::vector<char> vec3(size, 0);\n    }\n\n    malloc_trim(0);\n\n    EXPECT_GE(m_unmapped_size, size);\n\n    unset();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, dynamic_mmap_enable,\n                     RUNNING_ON_VALGRIND || skip_on_bistro()) {\n    EXPECT_TRUE(ucm_global_opts.enable_dynamic_mmap_thresh);\n    test_dynamic_mmap_thresh();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, dynamic_mmap_disable,\n                     skip_on_bistro_without_valgrind()) {\n    ucm_global_opts.enable_dynamic_mmap_thresh = 0;\n\n    test_dynamic_mmap_thresh();\n}\n\nextern \"C\" {\n    int ucm_dlmallopt_get(int);\n};\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, mallopt,\n                     skip_on_bistro_without_valgrind()) {\n\n    int v;\n    int trim_thresh, mmap_thresh;\n    char *p;\n    size_t size;\n\n    /* This test can not be run with the other\n     * tests because it assumes that malloc hooks\n     * are not initialized\n     */\n    p = getenv(\"MALLOC_TRIM_THRESHOLD_\");\n    if (p == NULL) {\n        UCS_TEST_SKIP_R(\"MALLOC_TRIM_THRESHOLD_ is not set\");\n    }\n    trim_thresh = atoi(p);\n\n    p = getenv(\"MALLOC_MMAP_THRESHOLD_\");\n    if (p == NULL) {\n        UCS_TEST_SKIP_R(\"MALLOC_MMAP_THRESHOLD_ is not set\");\n    }\n    mmap_thresh = atoi(p);\n\n    /* make sure that rcache is explicitly disabled so\n     * that the malloc hooks are installed after the setenv()\n     */\n    p = getenv(\"UCX_IB_RCACHE\");\n    if ((p == NULL) || (p[0] != 'n')) {\n        UCS_TEST_SKIP_R(\"rcache must be disabled\");\n    }\n\n    set();\n\n    v = ucm_dlmallopt_get(M_TRIM_THRESHOLD);\n    EXPECT_EQ(trim_thresh, v);\n\n    v = ucm_dlmallopt_get(M_MMAP_THRESHOLD);\n    EXPECT_EQ(mmap_thresh, v);\n\n    /* give a lot of extra space since the same block\n     * can be also used by other allocations\n     */\n    if (trim_thresh > 0) {\n        size = trim_thresh/2;\n    } else if (mmap_thresh > 0) {\n        size = mmap_thresh/2;\n    } else {\n        size = 10 * 1024 * 1024;\n    }\n\n    UCS_TEST_MESSAGE << \"trim_thresh=\" << trim_thresh << \" mmap_thresh=\" << mmap_thresh <<\n                        \" allocating=\" << size;\n    p = new char [size];\n    ASSERT_TRUE(p != NULL);\n    delete [] p;\n\n    EXPECT_EQ(m_unmapped_size, size_t(0));\n\n    unset();\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook_cplusplus, mmap_ptrs, RUNNING_ON_VALGRIND) {\n    ucm_global_opts.enable_dynamic_mmap_thresh = 0;\n    set();\n\n    const size_t   size    = ucm_dlmallopt_get(M_MMAP_THRESHOLD) * 2;\n    const size_t   max_mem = ucs_min(ucs_get_phys_mem_size() / 4, 4 * UCS_GBYTE);\n    const unsigned count   = ucs_min(400000ul, max_mem / size);\n    const unsigned iters   = 100000;\n\n    std::vector< std::vector<char> > ptrs;\n\n    size_t large_blocks = 0;\n\n    /* Allocate until we get MMAP event\n     * Lock memory to avoid going to swap and ensure consistet test results.\n     */\n    while (m_mapped_size == 0) {\n        std::vector<char> str(size, 'r');\n        ptrs.push_back(str);\n        ++large_blocks;\n    }\n\n    /* Remove memory off the heap top, to ensure the following large allocations\n     * will use mmap()\n     */\n    malloc_trim(0);\n\n    /* Measure allocation time with \"clear\" heap state */\n    double alloc_time = measure_alloc_time(size, iters);\n    UCS_TEST_MESSAGE << \"With \" << large_blocks << \" large blocks:\"\n                     << \" allocated \" << iters << \" buffers of \" << size\n                     << \" bytes in \" << alloc_time << \" sec\";\n\n    /* Allocate many large strings to trigger mmap() based allocation. */\n    ptrs.resize(count);\n    for (unsigned i = 0; i < count; ++i) {\n        ptrs[i].resize(size, 't');\n        ++large_blocks;\n    }\n\n    /* Measure allocation time with many large blocks on the heap */\n    bool success = false;\n    unsigned attempt = 0;\n    while (!success && (attempt < 5)) {\n        double alloc_time_with_ptrs = measure_alloc_time(size, iters);\n        UCS_TEST_MESSAGE << \"With \" << large_blocks << \" large blocks:\"\n                         << \" allocated \" << iters << \" buffers of \" << size\n                         << \" bytes in \" << alloc_time_with_ptrs << \" sec\";\n\n        /* Allow up to 75% difference */\n        success = (alloc_time < 0.25) || (alloc_time_with_ptrs < (1.75 * alloc_time));\n        ++attempt;\n    }\n\n    if (!success) {\n        ADD_FAILURE() << \"Failed after \" << attempt << \" attempts\";\n    }\n\n    ptrs.clear();\n\n    unset();\n\n}\n\nUCS_TEST_F(malloc_hook_cplusplus, remap_override_single_thread) {\n    pthread_barrier_t barrier;\n    pthread_barrier_init(&barrier, NULL, 1);\n    {\n        mhook_thread<mmap_hooks>(new mmap_hooks(\"single-thread\", 1, &barrier));\n    }\n    pthread_barrier_destroy(&barrier);\n}\n\nUCS_TEST_F(malloc_hook_cplusplus, remap_override_multi_threads) {\n    typedef mhook_thread<mmap_hooks> thread_t;\n\n    static const int num_threads = 8;\n    ucs::ptr_vector<thread_t> threads;\n    pthread_barrier_t barrier;\n\n    pthread_barrier_init(&barrier, NULL, num_threads);\n    for (int i = 0; i < num_threads; ++i) {\n        std::stringstream ss;\n        ss << \"thread \" << i << \"/\" << num_threads;\n        threads.push_back(new thread_t(new mmap_hooks(ss.str(), num_threads, &barrier)));\n    }\n\n    threads.clear();\n    pthread_barrier_destroy(&barrier);\n}\n\ntypedef int (munmap_f_t)(void *addr, size_t len);\n\nUCS_TEST_SKIP_COND_F(malloc_hook, bistro_patch, RUNNING_ON_VALGRIND) {\n    const char *symbol = \"munmap\";\n    ucm_bistro_restore_point_t *rp = NULL;\n    ucs_status_t status;\n    munmap_f_t *munmap_f;\n    void *ptr;\n    int res;\n    uint64_t UCS_V_UNUSED patched;\n    uint64_t UCS_V_UNUSED origin;\n\n    /* set hook to mmap call */\n    status = ucm_bistro_patch(symbol, (void*)bistro_hook<0>::munmap, &rp);\n    ASSERT_UCS_OK(status);\n    EXPECT_NE((intptr_t)rp, 0);\n\n    munmap_f = (munmap_f_t*)ucm_bistro_restore_addr(rp);\n    EXPECT_NE((intptr_t)munmap_f, 0);\n\n    /* save partial body of patched function */\n    patched = *(uint64_t*)munmap_f;\n\n    bistro_call_counter = 0;\n    ptr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);\n    EXPECT_NE(ptr, MAP_FAILED);\n\n    /* try to call munmap, we should jump into munmap_hook instead */\n    res = munmap_f(ptr, 4096);\n    EXPECT_EQ(res, 0);\n    /* due to cache coherency issues on ARM systems could be executed\n     * original function body, so, skip counter evaluation */\n    EXPECT_GT(bistro_call_counter, 0);\n\n    /* restore original mmap body */\n    status = ucm_bistro_restore(rp);\n    ASSERT_UCS_OK(status);\n\n    bistro_call_counter = 0;\n    /* now try to call mmap, we should NOT jump into mmap_hook */\n    ptr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);\n    EXPECT_NE(ptr, MAP_FAILED);\n    res = munmap_f(ptr, 4096);\n    EXPECT_EQ(res, 0);\n    EXPECT_EQ(bistro_call_counter, 0);  /* hook is not called */\n    /* save partial body of restored function */\n    origin = *(uint64_t*)munmap_f;\n\n#if !defined (__powerpc64__)\n    EXPECT_NE(patched, origin);\n#endif\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_event, RUNNING_ON_VALGRIND) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n\n    status = event.set(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    status = ucm_test_events(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_event_failed,\n                     RUNNING_ON_VALGRIND || !skip_on_bistro()) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n    const char *symbol_munmap  = \"munmap\";\n    const char *symbol_madvise = \"madvise\";\n\n    status = event.set(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    /* set hook to munmap call */\n    {\n        bistro_patch patch(symbol_munmap, (void*)bistro_hook<0>::munmap);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_MUNMAP)      == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_ERR_UNSUPPORTED);\n    }\n    /* set hook to madvise call */\n    {\n        bistro_patch patch(symbol_madvise, (void*)bistro_hook<0>::madvise);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_MADVISE)     == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n    }\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_external_event,\n                     RUNNING_ON_VALGRIND || !skip_on_bistro()) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n    const char *symbol_munmap  = \"munmap\";\n    const char *symbol_madvise = \"madvise\";\n\n    status = event.set_external(UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    /* set hook to munmap call */\n    {\n        bistro_patch patch(symbol_munmap, (void*)bistro_hook<0>::munmap);\n        /* OK due to UCM_EVENT_MUNMAP is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP)      == UCS_OK);\n        /* should fail */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n        /* should fail */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_ERR_UNSUPPORTED);\n    }\n    /* set hook to madvise call */\n    {\n        bistro_patch patch(symbol_madvise, (void*)bistro_hook<0>::madvise);\n        /* OK due to UCM_EVENT_MADVISE is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MADVISE)     == UCS_OK);\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n    }\n    /* set hook to munmap/madvise call which notify vm_unmap */\n    {\n        bistro_patch patch_unmap(symbol_munmap, (void*)bistro_hook<1>::munmap);\n        bistro_patch patch_advise(symbol_madvise, (void*)bistro_hook<1>::madvise);\n        /* OK due to UCM_EVENT_MUNMAP is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP)      == UCS_OK);\n        /* OK due to UCM_EVENT_MADVISE is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MADVISE)     == UCS_OK);\n        /* should be OK */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_OK);\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_OK);\n    }\n    /* set hook to munmap & madvise call, but madvise does NOT notify vm_unmap */\n    {\n        bistro_patch patch_unmap(symbol_munmap, (void*)bistro_hook<1>::munmap);\n        bistro_patch patch_advise(symbol_madvise, (void*)bistro_hook<0>::madvise);\n        /* OK due to UCM_EVENT_MUNMAP is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP)      == UCS_OK);\n        /* OK due to UCM_EVENT_MADVISE is not external */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MADVISE)     == UCS_OK);\n        /* should fail */\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_VM_UNMAPPED) == UCS_ERR_UNSUPPORTED);\n        EXPECT_TRUE(ucm_test_external_events(UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED) ==\n                    UCS_ERR_UNSUPPORTED);\n    }\n}\n\nUCS_TEST_SKIP_COND_F(malloc_hook, test_event_unmap,\n                     RUNNING_ON_VALGRIND || !skip_on_bistro()) {\n    mmap_event<malloc_hook> event(this);\n    ucs_status_t status;\n    const char *symbol = \"munmap\";\n\n    status = event.set(UCM_EVENT_MMAP | UCM_EVENT_MUNMAP | UCM_EVENT_VM_UNMAPPED);\n    ASSERT_UCS_OK(status);\n\n    /* set hook to mmap call */\n    bistro_patch patch(symbol, (void*)bistro_hook<1>::munmap);\n\n    /* munmap should be broken */\n    status = ucm_test_events(UCM_EVENT_MUNMAP);\n    EXPECT_TRUE(status == UCS_ERR_UNSUPPORTED);\n\n    /* vm_unmap should be broken as well, because munmap is broken */\n    status = ucm_test_events(UCM_EVENT_MUNMAP);\n    EXPECT_TRUE(status == UCS_ERR_UNSUPPORTED);\n\n    /* mmap should still work */\n    status = ucm_test_events(UCM_EVENT_MMAP);\n    EXPECT_TRUE(status == UCS_OK);\n}\n\nclass malloc_hook_dlopen : public malloc_hook {\nprotected:\n    class library {\n    public:\n        typedef void* (*loader_t)(const char*, int);\n\n        library(loader_t loader, const std::string &name = \"\"):\n            m_loader(loader), m_name(name), m_lib(NULL)\n        {\n        }\n\n        ~library()\n        {\n            close();\n        }\n\n        void *open(const std::string name = \"\")\n        {\n            if (!name.empty()) {\n                m_name = name;\n            }\n\n            close();\n\n            return (m_lib = m_loader(m_name.empty() ? NULL : m_name.c_str(), RTLD_NOW));\n        }\n\n        void attach(void *lib)\n        {\n            close();\n            m_lib = lib;\n        }\n\n        void close()\n        {\n            if (m_lib != NULL) {\n                dlclose(m_lib);\n                m_lib = NULL;\n            }\n        }\n\n        operator bool()\n        {\n            return m_lib != NULL;\n        }\n\n        void* sym(const char *name)\n        {\n            return dlsym(m_lib, name);\n        }\n\n    protected:\n        loader_t    m_loader;\n        std::string m_name;\n        void       *m_lib;\n    };\n\npublic:\n    typedef library::loader_t loader_t;\n\n    static std::string get_lib_dir() {\n#ifndef GTEST_UCM_HOOK_LIB_DIR\n#  error \"Missing build configuration\"\n#else\n        return GTEST_UCM_HOOK_LIB_DIR;\n#endif\n    }\n\n    static std::string get_lib_path_do_load() {\n        return get_lib_dir() + \"/libdlopen_test_do_load.so\";\n    }\n\n    static std::string get_lib_path_do_mmap() {\n        return get_lib_dir() + \"/libdlopen_test_do_mmap.so\";\n    }\n\n    static std::string get_lib_path_do_load_rpath() {\n        return get_lib_dir() + \"/libdlopen_test_do_load_rpath.so\";\n    }\n\n    static std::string get_lib_path_do_load_sub_rpath() {\n        return \"libdlopen_test_rpath.so\"; // library should be located using rpath\n    }\n\n    /* test for mmap events are fired from non-direct load modules\n     * we are trying to load lib1, from lib1 load lib2, and\n     * fire mmap event from lib2 */\n    void test_indirect_dlopen(loader_t loader)\n    {\n        typedef void (*fire_mmap_f)(void);\n        typedef void* (*load_lib_f)(const char *path, void* (*func)(const char*, int));\n\n        const char *load_lib  = \"load_lib\";\n        const char *fire_mmap = \"fire_mmap\";\n\n        library lib(loader, get_lib_path_do_load());\n        library lib2(NULL); // lib2 is used for attach only\n        load_lib_f load;\n        fire_mmap_f fire;\n        ucs_status_t status;\n        mmap_event<malloc_hook> event(this);\n\n        status = event.set(UCM_EVENT_VM_MAPPED);\n        ASSERT_UCS_OK(status);\n\n        lib.open();\n        ASSERT_TRUE(lib);\n\n        load = (load_lib_f)lib.sym(load_lib);\n        ASSERT_TRUE(load != NULL);\n\n        lib2.attach(load(get_lib_path_do_mmap().c_str(), loader));\n        ASSERT_TRUE(lib2);\n\n        fire = (fire_mmap_f)lib2.sym(fire_mmap);\n        ASSERT_TRUE(fire != NULL);\n\n        m_got_event = 0;\n        fire();\n        EXPECT_GT(m_got_event, 0);\n    }\n\n    /* Test for rpath section of caller module is processes */\n    void test_rpath_dlopen(loader_t loader)\n    {\n        typedef void* (*load_lib_f)(const char *path, void* (*func)(const char*, int));\n\n        const char *load_lib = \"load_lib\";\n\n        library lib(loader);\n        library lib2(NULL); // lib2 is used for attach only\n        load_lib_f load;\n        ucs_status_t status;\n        mmap_event<malloc_hook> event(this);\n\n        /* in case if reloc mode is used - it force hook dlopen */\n        status = event.set(UCM_EVENT_VM_MAPPED);\n        ASSERT_UCS_OK(status);\n\n        /* first check that without rpath library located in subdirectory could not be loaded */\n        lib.open(get_lib_path_do_load());\n        ASSERT_TRUE(lib);\n        if (!lib) {\n            return;\n        }\n\n        load = (load_lib_f)lib.sym(load_lib);\n        ASSERT_TRUE(load != NULL);\n\n        lib2.attach(load(get_lib_path_do_load_sub_rpath().c_str(), loader));\n        ASSERT_FALSE(lib2);\n\n        /* next check that rpath helps to load library located in subdirectory */\n        /* don't care about opened libs - it will be closed automatically */\n        lib.open(get_lib_path_do_load_rpath());\n        ASSERT_TRUE(lib);\n        if (!lib) {\n            return;\n        }\n\n        load = (load_lib_f)lib.sym(load_lib);\n        ASSERT_TRUE(load != NULL);\n\n        lib2.attach(load(get_lib_path_do_load_sub_rpath().c_str(), loader));\n        ASSERT_TRUE(lib2);\n    }\n\n    void test_dlopen_null(loader_t loader)\n    {\n        library lib(loader);\n\n        lib.open();\n        ASSERT_TRUE(lib);\n    }\n};\n\nUCS_TEST_F(malloc_hook_dlopen, indirect_dlopen) {\n    test_indirect_dlopen(dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, indirect_ucm_dlopen) {\n    test_indirect_dlopen(ucm_dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, rpath_dlopen) {\n    test_rpath_dlopen(dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, rpath_ucm_dlopen) {\n    test_rpath_dlopen(ucm_dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, ucm_dlopen_null_dlopen) {\n    test_dlopen_null(dlopen);\n}\n\nUCS_TEST_F(malloc_hook_dlopen, ucm_dlopen_null_ucm_dlopen) {\n    test_dlopen_null(ucm_dlopen);\n}\n\nUCS_MT_TEST_F(malloc_hook_dlopen, dlopen_mt_with_memtype, 2) {\n#ifndef GTEST_UCM_HOOK_LIB_DIR\n#  error \"Missing build configuration\"\n#endif\n    mmap_event<malloc_hook> event(this);\n\n    ucs_status_t status = event.set(UCM_EVENT_VM_MAPPED |\n                                    UCM_EVENT_MEM_TYPE_ALLOC |\n                                    UCM_EVENT_MEM_TYPE_FREE);\n    ASSERT_UCS_OK(status);\n\n    const std::string path = get_lib_path_do_mmap();\n    static uint32_t count = 0;\n\n    for (int i = 0; i < 100 / ucs::test_time_multiplier(); ++i) {\n        /* Tests that calling dlopen() from 2 threads does not deadlock, if for\n         * example we install memtype relocation patches and call dladdr() while\n         * iterating over loaded libraries.\n         */\n        if (ucs_atomic_fadd32(&count, 1) % 2) {\n            void *lib1 = dlopen(get_lib_path_do_mmap().c_str(), RTLD_LAZY);\n            ASSERT_TRUE(lib1 != NULL);\n            dlclose(lib1);\n        } else {\n            void *lib2 = dlopen(get_lib_path_do_load().c_str(), RTLD_LAZY);\n            ASSERT_TRUE(lib2 != NULL);\n            dlclose(lib2);\n        }\n\n        barrier();\n    }\n\n    event.unset();\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/test/gtest/ucs/test_debug.cc": "/**\n* Copyright (C) Mellanox Technologies Ltd. 2001-2012.  ALL RIGHTS RESERVED.\n*\n* See file LICENSE for terms.\n*/\n\n#include <common/test.h>\nextern \"C\" {\n#include <ucs/debug/debug.h>\n#include <ucs/sys/compiler.h>\n#include <ucs/sys/sys.h>\n}\n\n#include <dlfcn.h>\n\nextern \"C\" {\n\nvoid UCS_F_NOINLINE my_cool_function(unsigned *lineno) { *lineno = __LINE__; };\n\n}\n\nclass test_debug : public ucs::test {\n};\n\nstd::string __basename(const std::string& path) {\n    char *p = strdup(path.c_str());\n    std::string bn(::basename(p));\n    free(p);\n    return bn;\n}\n\nUCS_TEST_F(test_debug, lookup_ucs_func) {\n    const char sym[] = \"ucs_log_flush\";\n\n    ucs_debug_address_info info;\n    ucs_status_t status = ucs_debug_lookup_address(dlsym(RTLD_DEFAULT, sym), &info);\n    ASSERT_UCS_OK(status);\n\n    EXPECT_NE(std::string::npos, std::string(info.file.path).find(\"libucs.so\"));\n#ifdef HAVE_DETAILED_BACKTRACE\n    EXPECT_EQ(sym, std::string(info.function));\n#endif\n}\n\nUCS_TEST_F(test_debug, lookup_invalid) {\n    ucs_debug_address_info info;\n    ucs_status_t status = ucs_debug_lookup_address((void*)0xffffffffffff, &info);\n    EXPECT_EQ(UCS_ERR_NO_ELEM, status);\n}\n\nUCS_TEST_SKIP_COND_F(test_debug, lookup_address, BULLSEYE_ON) {\n    unsigned lineno;\n\n    my_cool_function(&lineno);\n\n    ucs_debug_address_info info;\n    ucs_status_t status = ucs_debug_lookup_address((void*)&my_cool_function,\n                                                   &info);\n    ASSERT_UCS_OK(status);\n\n    UCS_TEST_MESSAGE << info.source_file << \":\" << info.line_number <<\n                        \" \" << info.function << \"()\";\n\n    EXPECT_NE(std::string::npos, std::string(info.file.path).find(\"gtest\"));\n\n#ifdef HAVE_DETAILED_BACKTRACE\n    EXPECT_EQ(\"my_cool_function\", std::string(info.function));\n    EXPECT_EQ(lineno, info.line_number);\n    EXPECT_EQ(__basename(__FILE__), __basename(info.source_file));\n#else\n    EXPECT_EQ(0u, info.line_number);\n    EXPECT_EQ(\"???\", std::string(info.source_file));\n#endif\n}\n\nUCS_TEST_F(test_debug, print_backtrace) {\n    char *data;\n    size_t size;\n\n    FILE *f = open_memstream(&data, &size);\n    ucs_debug_print_backtrace(f, 0);\n    fclose(f);\n\n    /* Some functions that should appear */\n    EXPECT_TRUE(strstr(data, \"print_backtrace\") != NULL);\n#ifdef HAVE_DETAILED_BACKTRACE\n    EXPECT_TRUE(strstr(data, \"main\") != NULL);\n#endif\n\n    free(data);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/test/apps/test_dlopen_cfg_print.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <stdio.h>\n\n#define _QUOTE(x) #x\n#define QUOTE(x) _QUOTE(x)\n\n\nstatic void* do_dlopen_or_exit(const char *filename)\n{\n    void *handle;\n\n    (void)dlerror();\n    printf(\"opening '%s'\\n\", filename);\n    handle = dlopen(filename, RTLD_LAZY);\n    if (handle == NULL) {\n        fprintf(stderr, \"failed to open %s: %s\\n\", filename,\n                dlerror());\n        exit(1);\n    }\n\n    return handle;\n}\n\nint main(int argc, char **argv)\n{\n    typedef void (*print_all_opts_func_t)(FILE*, const char *, int);\n\n    const char *ucs_filename = QUOTE(UCS_LIB_PATH);\n    const char *uct_filename = QUOTE(UCT_LIB_PATH);\n    void *ucs_handle, *uct_handle;\n    int i;\n\n    /* unload and reload uct while ucs is loaded\n     * would fail if uct global vars are kept on global lists in ucs */\n    ucs_handle = do_dlopen_or_exit(ucs_filename);\n    for (i = 0; i < 2; ++i) {\n        uct_handle = do_dlopen_or_exit(uct_filename);\n        dlclose(uct_handle);\n    }\n\n    /* print all config table, to force going over the global list in ucs */\n    print_all_opts_func_t print_all_opts =\n        (print_all_opts_func_t)dlsym(ucs_handle, \"ucs_config_parser_print_all_opts\");\n    print_all_opts(stdout, \"TEST_\", 0);\n    dlclose(ucs_handle);\n\n    printf(\"done\\n\");\n    return 0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/test/apps/test_ucs_dlopen.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <sys/mman.h>\n#include <dlfcn.h>\n#include <stdio.h>\n#include <unistd.h>\n#include <stdlib.h>\n\n#include <ucm/api/ucm.h>\n\n#define _QUOTE(x) #x\n#define QUOTE(x) _QUOTE(x)\n\n\nstatic void vm_unmap_cb(ucm_event_type_t event_type, ucm_event_t *event,\n                        void *arg)\n{\n}\n\nint test_ucm_set_event_handler(void *handle)\n{\n    typedef ucs_status_t (*ucm_set_event_handler_func_t)(int events,\n                                                         int priority,\n                                                         ucm_event_callback_t cb,\n                                                         void *arg);\n\n    ucm_set_event_handler_func_t ucm_set_event_handler_f;\n    ucs_status_t status;\n\n    dlerror();\n    ucm_set_event_handler_f = (ucm_set_event_handler_func_t)dlsym(handle,\n                                                                  \"ucm_set_event_handler\");\n    if (ucm_set_event_handler_f == NULL) {\n        fprintf(stderr, \"failed to resolve ucm_set_event_handler(): %s\\n\",\n                dlerror());\n        return -1;\n    }\n\n    status = ucm_set_event_handler_f(UCM_EVENT_VM_UNMAPPED, 0, vm_unmap_cb,\n                                     NULL);\n    if (status != UCS_OK) {\n        fprintf(stderr, \"ucm_set_event_handler() failed\\n\");\n        return -1;\n    }\n\n    return 0;\n}\n\nint main(int argc, char **argv)\n{\n    const char *filename = QUOTE(LIB_PATH);\n    void *handle;\n    void *ptr1, *ptr2;\n    size_t alloc_size;\n    long ret;\n\n    /* get page size */\n    ret = sysconf(_SC_PAGESIZE);\n    if (ret < 0) {\n        fprintf(stderr, \"sysconf(_SC_PAGESIZE) failed: %m\\n\");\n        return -1;\n    }\n    alloc_size = ret;\n\n    /* allocate some memory */\n    ptr1 = malloc(alloc_size);\n    if (!ptr1) {\n        fprintf(stderr, \"malloc() failed\\n\");\n        return -1;\n    }\n\n    ptr2 = mmap(NULL, alloc_size, PROT_READ|PROT_WRITE,\n                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    if (ptr2 == MAP_FAILED) {\n        fprintf(stderr, \"mmmap() failed: %m\\n\");\n        ret = -1;\n        goto failed_mmap;\n    }\n\n    /* load ucm */\n    printf(\"opening '%s'\\n\", filename);\n    dlerror();\n    handle = dlopen(filename, RTLD_NOW);\n    if (handle == NULL) {\n        fprintf(stderr, \"failed to open %s: %s\\n\", filename, dlerror());\n        ret = -1;\n        goto failed_dlopen;\n    }\n\n    /* init ucm */\n    ret = test_ucm_set_event_handler(handle);\n\n    /* unload ucp */\n    dlclose(handle);\n\nfailed_dlopen:\n    /* release the memory - could break if UCM is unloaded */\n    munmap(ptr2, alloc_size);\nfailed_mmap:\n    free(ptr1);\n\n    printf(\"done\\n\");\n    return ret;\n}\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/test/apps/test_ucp_dlopen.c": "/**\n * Copyright (C) Mellanox Technologies Ltd. 2019.  ALL RIGHTS RESERVED.\n *\n * See file LICENSE for terms.\n */\n\n#ifdef HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <sys/mman.h>\n#include <dlfcn.h>\n#include <stdio.h>\n#include <unistd.h>\n#include <stdlib.h>\n\n#include <ucp/api/ucp.h>\n\n#define _QUOTE(x) #x\n#define QUOTE(x) _QUOTE(x)\n\n\nint test_ucp_init(void *handle)\n{\n    typedef ucs_status_t (*ucp_init_version_func_t)(unsigned, unsigned,\n                                                    const ucp_params_t *,\n                                                    const ucp_config_t *,\n                                                    ucp_context_h *);\n    typedef void (*ucp_context_print_info_func_t)(const ucp_context_h, FILE*);\n    typedef void (*ucp_cleanup_func_t)(ucp_context_h);\n\n    ucp_init_version_func_t ucp_init_version_f;\n    ucp_context_print_info_func_t ucp_context_print_info_f;\n    ucp_cleanup_func_t ucp_cleanup_f;\n    ucp_params_t ucp_params;\n    ucs_status_t status;\n    ucp_context_h ucph;\n\n    ucp_init_version_f       = (ucp_init_version_func_t)dlsym(handle,\n                                                              \"ucp_init_version\");\n    ucp_cleanup_f            = (ucp_cleanup_func_t)dlsym(handle, \"ucp_cleanup\");\n    ucp_context_print_info_f = (ucp_context_print_info_func_t)dlsym(handle,\n                                                                    \"ucp_context_print_info\");\n\n    if (!ucp_init_version_f || !ucp_cleanup_f || !ucp_context_print_info_f) {\n        fprintf(stderr, \"failed to get UCP function pointers\\n\");\n        return -1;\n    }\n\n    ucp_params.field_mask = UCP_PARAM_FIELD_FEATURES;\n    ucp_params.features   = UCP_FEATURE_RMA;\n    status = ucp_init_version_f(UCP_API_MAJOR, UCP_API_MINOR, &ucp_params,\n                                NULL, &ucph);\n    if (status != UCS_OK) {\n        fprintf(stderr, \"ucp_init_version() failed\\n\");\n        return -1;\n    }\n\n    ucp_context_print_info_f(ucph, stdout);\n    ucp_cleanup_f(ucph);\n\n    return 0;\n}\n\nint main(int argc, char **argv)\n{\n    const char *filename = QUOTE(LIB_PATH);\n    void *handle;\n    void *ptr1, *ptr2;\n    size_t alloc_size;\n    long ret;\n\n    /* get page size */\n    ret = sysconf(_SC_PAGESIZE);\n    if (ret < 0) {\n        fprintf(stderr, \"sysconf(_SC_PAGESIZE) failed: %m\\n\");\n        return -1;\n    }\n    alloc_size = ret;\n\n    /* allocate some memory */\n    ptr1 = malloc(alloc_size);\n    if (!ptr1) {\n        fprintf(stderr, \"malloc() failed\\n\");\n        return -1;\n    }\n\n    ptr2 = mmap(NULL, alloc_size, PROT_READ|PROT_WRITE,\n                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    if (ptr2 == MAP_FAILED) {\n        fprintf(stderr, \"mmmap() failed: %m\\n\");\n        ret = -1;\n        goto failed_mmap;\n    }\n\n    /* load ucp */\n    printf(\"opening '%s'\\n\", filename);\n    handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n    if (handle == NULL) {\n        fprintf(stderr, \"failed to open %s: %m\\n\", filename);\n        ret = -1;\n        goto failed_dlopen;\n    }\n\n    /* init ucp */\n    ret = test_ucp_init(handle);\n\n    /* unload ucp */\n    dlclose(handle);\n\nfailed_dlopen:\n    /* relase the memory - could break if UCM is unloaded */\n    munmap(ptr2, alloc_size);\nfailed_mmap:\n    free(ptr1);\n\n    printf(\"done\\n\");\n    return ret;\n}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/mpi/romio/adio/common/ad_iwrite_coll.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/mpi/romio/adio/common/ad_iread_coll.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/mpi/romio/doc/source-guide.tex",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/util/logging/rlog/rimshot/res/Toolbar.bmp",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/util/logging/rlog/rimshot/res/rimshotDoc.ico",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/src/util/logging/rlog/rimshot/res/rimshot.ico",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/maint/setup.jpg",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/maint/local_perl/YAML-Tiny-1.41.tar.gz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/objects/pack/pack-1a37501defbe09656fc2a3331317f3840dcf696c.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/objects/pack/pack-1a37501defbe09656fc2a3331317f3840dcf696c.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/libfabric/objects/pack/pack-140218e5ec23a242eda0fcd7a82232b12843281c.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/libfabric/objects/pack/pack-140218e5ec23a242eda0fcd7a82232b12843281c.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/izem/objects/pack/pack-18b8833261a4900246da8fca2ae12c88ba783b41.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/izem/objects/pack/pack-18b8833261a4900246da8fca2ae12c88ba783b41.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/json-c/objects/pack/pack-2acf87c8bf8b98ca89091b37e34db3b02b5cb88e.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/json-c/objects/pack/pack-2acf87c8bf8b98ca89091b37e34db3b02b5cb88e.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/hwloc/objects/pack/pack-b977ea6f3fa340c3c31d0ace7bd1f2afa1721eeb.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/hwloc/objects/pack/pack-b977ea6f3fa340c3c31d0ace7bd1f2afa1721eeb.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/ucx/objects/pack/pack-fa77d8d2000e7bf216aab2a4157f75d227069012.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/ucx/objects/pack/pack-fa77d8d2000e7bf216aab2a4157f75d227069012.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/ucx/modules/ucg/objects/pack/pack-254adb453a099a5780a9796a7a59287c2a69ef78.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/ucx/modules/ucg/objects/pack/pack-254adb453a099a5780a9796a7a59287c2a69ef78.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/yaksa/objects/pack/pack-c6e3c6e97aad225e68efa51a30e8cab88e631dae.pack",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/.git/modules/modules/yaksa/objects/pack/pack-c6e3c6e97aad225e68efa51a30e8cab88e631dae.idx",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/json-c/tests/test_parse.expected",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/utils/hwloc/test-hwloc-compress-dir.input.tar.gz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/utils/hwloc/test-hwloc-compress-dir.output.tar.gz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/utils/hwloc/test-hwloc-dump-hwdata/knl-snc4h50.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/netloc/data/plafrim2.txz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/netloc/data/avakas.txz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/netloc/data/plafrim.txz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/netloc/data/scotch.txz",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/1alpha.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8ia64-2n2s2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/4ia64-4s.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16em64t-4s2c2t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8amd64-4n2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/256ppc-8n8s4t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8em64t-2s2ca2c-buggynuma.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/96em64t-4n4d3ca2co.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/28em64t-2s2n7c-buggycoresiblings.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/2arm-2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/32em64t-2n8c+1mic.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16ia64-8n2s.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/20s390-2g6s4c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/48amd64-4d2n6c-sparse.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8em64t-2s2ca2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/2i386-2c-nohugepage.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/4em64t-2c2t-ignore-reorder.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16em64t-4s2c2t-offlines.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/2ps3-2t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/40intel64-2g2n4c+pci.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/64intel64-fakeKNL-A2A-cache.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8ia64-2s2c2t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8em64t-4c2t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/40intel64-4n10c+pci-conflicts.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16amd64-8n2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/256ia64-64n2s2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/64fake-4n2s2ca2c2t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/32ppc-4n4c2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/32amd64-4s2n4c-cgroup.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/64intel64-fakeKNL-SNC4-hybrid.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8ia64-4s2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/4fake-4gr1nu1pu.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/4ppc-4c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/4qs22-2s2t.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/2amd64-2n.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16amd64-8n2c-cpusets.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/2i386-2t-hugepagesizecount.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16amd64-4n4c-cgroup-distance-merge.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/8em64t-2s4c-heterogeneous.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/2s390-2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/64amd64-4s2n4ca2co.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/128ia64-17n4s2c.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/16em64t-4s2ca2c-cpusetreorder.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/linux/allowed/test1.fsroot.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-SandyBridge-2xXeon-E5-2650.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-17h-Zen-2xEpyc-7451.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-15h-Bulldozer-4xOpteron-6272.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-15h-Piledriver-4xOpteron-6348.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-K8-SantaRosa-2xOpteron-2218.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-KnightsLanding-XeonPhi-7210.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Westmere-2xXeon-X5650.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Skylake-2xXeon6140.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-K10-MagnyCours-2xOpteron-6164HE.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Haswell-2xXeon-E5-2680v3.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Nehalem-2xXeon-X5550.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-K10-Istanbul-8xOpteron-8439SE.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/AMD-K8-SledgeHammer-2xOpteron-250.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-KnightsCorner-XeonPhi-SE10P.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Zhaoxin-Shanghai-KaiSheng-ZXC+-FC1081.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Penryn-4xXeon-X7460.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Zhaoxin-CentaurHauls-ZXD-4600.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-IvyBridge-12xXeon-E5-4620v2.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Core-2xXeon-E5345.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/tests/hwloc/x86/Intel-Broadwell-2xXeon-E5-2650Lv4.tar.bz2",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/ppc64-with-smt.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/emmett.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/hagrid.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/ppc64-full-with-smt.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/dudley.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/devel09-pci.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/netloc_design.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/netloc_draw.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/hwloc/doc/images/ppc64-without-smt.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/docs/source/_static/ucxlogo.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/docs/source/_static/UCX_Layers.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/docs/doxygen/Architecture.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/docs/doxygen/Architecture.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/docs/doxygen/UCX_Logo_930x933.png",
        "/tmp/vanessa/spack-stage/spack-stage-mpich-develop-723musosxdgndkfjflu46x76hhv5ladb/spack-src/modules/ucx/docs/doxygen/UCX_Logo_80x80.png"
    ],
    "total_files": 7758
}