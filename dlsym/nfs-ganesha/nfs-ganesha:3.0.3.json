{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-nfs-ganesha-3.0.3-nsvhncc7miczmpey7jvxkhq3mjyiz5yf/spack-src/src/SAL/nfs4_recovery.c": "/*\n * vim:noexpandtab:shiftwidth=8:tabstop=8:\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 3 of the License, or (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with this library; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n * 02110-1301 USA\n *\n * ---------------------------------------\n */\n\n/**\n * @defgroup SAL State abstraction layer\n * @{\n */\n\n/**\n * @file nfs4_recovery.c\n * @brief NFSv4 recovery\n */\n\n#include \"config.h\"\n#include \"log.h\"\n#include \"nfs_core.h\"\n#include \"nfs4.h\"\n#include \"sal_functions.h\"\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <fcntl.h>\n#include <ctype.h>\n#include <dlfcn.h>\n#include \"bsd-base64.h\"\n#include \"client_mgr.h\"\n#include \"fsal.h\"\n\n/* The grace_mutex protects current_grace, clid_list, and clid_count */\nstatic pthread_mutex_t grace_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic struct timespec current_grace; /* current grace period timeout */\nstatic int clid_count; /* number of active clients */\nstatic struct glist_head clid_list = GLIST_HEAD_INIT(clid_list);  /* clients */\n\n/*\n * Low two bits of grace_status word are flags. One for whether we're currently\n * in a grace period and one if a change was requested.\n */\n#define GRACE_STATUS_ACTIVE_SHIFT\t0\n#define GRACE_STATUS_CHANGE_REQ_SHIFT\t1\n\n/* The remaining bits are for the refcount */\n#define GRACE_STATUS_COUNTER_SHIFT\t2\n\n#define GRACE_STATUS_ACTIVE\t\t(1U << GRACE_STATUS_ACTIVE_SHIFT)\n#define GRACE_STATUS_CHANGE_REQ\t\t(1U << GRACE_STATUS_CHANGE_REQ_SHIFT)\n#define GRACE_STATUS_REF_INCREMENT\t(1U << GRACE_STATUS_COUNTER_SHIFT)\n#define GRACE_STATUS_COUNT_MASK\t\t((~0U) << GRACE_STATUS_COUNTER_SHIFT)\n\nstatic uint32_t\tgrace_status;\n\nstatic int default_recovery_init(void)\n{\n\treturn 0;\n}\n\nstatic void default_end_grace(void)\n{\n}\n\nstatic void default_recovery_read_clids(nfs_grace_start_t *gsp,\n\t\t\t\t\tadd_clid_entry_hook add_clid_entry,\n\t\t\t\t\tadd_rfh_entry_hook add_rfs_entry)\n{\n}\n\nstatic void default_add_clid(nfs_client_id_t *clientid)\n{\n}\n\nstatic void default_rm_clid(nfs_client_id_t *clientid)\n{\n}\n\nstatic void default_add_revoke_fh(nfs_client_id_t *dlr_clid,\n\t\t\t\t  nfs_fh4 *dlr_handle)\n{\n}\n\nstatic struct nfs4_recovery_backend default_recovery_backend = {\n\t.recovery_init = default_recovery_init,\n\t.end_grace = default_end_grace,\n\t.recovery_read_clids = default_recovery_read_clids,\n\t.add_clid = default_add_clid,\n\t.rm_clid = default_rm_clid,\n\t.add_revoke_fh = default_add_revoke_fh,\n};\n\n\nstatic struct nfs4_recovery_backend *recovery_backend =\n\t\t\t\t\t&default_recovery_backend;\nint32_t reclaim_completes; /* atomic */\n\nstatic void nfs4_recovery_load_clids(nfs_grace_start_t *gsp);\nstatic void nfs_release_nlm_state(char *release_ip);\nstatic void nfs_release_v4_clients(char *ip);\n\n\n\n\nclid_entry_t *nfs4_add_clid_entry(char *cl_name)\n{\n\tclid_entry_t *new_ent = gsh_malloc(sizeof(clid_entry_t));\n\n\tglist_init(&new_ent->cl_rfh_list);\n\t(void) strlcpy(new_ent->cl_name, cl_name, sizeof(new_ent->cl_name));\n\tglist_add(&clid_list, &new_ent->cl_list);\n\t++clid_count;\n\treturn new_ent;\n}\n\nrdel_fh_t *nfs4_add_rfh_entry(clid_entry_t *clid_ent, char *rfh_name)\n{\n\trdel_fh_t *new_ent = gsh_malloc(sizeof(rdel_fh_t));\n\n\tnew_ent->rdfh_handle_str = gsh_strdup(rfh_name);\n\tglist_add(&clid_ent->cl_rfh_list, &new_ent->rdfh_list);\n\treturn new_ent;\n}\n\nvoid nfs4_cleanup_clid_entries(void)\n{\n\tstruct clid_entry *clid_entry;\n\t/* when not doing a takeover, start with an empty list */\n\twhile ((clid_entry = glist_first_entry(&clid_list,\n\t\t\t\t\t       struct clid_entry,\n\t\t\t\t\t       cl_list)) != NULL) {\n\t\tglist_del(&clid_entry->cl_list);\n\t\tgsh_free(clid_entry);\n\t\t--clid_count;\n\t}\n\tassert(clid_count == 0);\n\tatomic_store_int32_t(&reclaim_completes, 0);\n}\n\n/*\n * Check the current status of the grace period against what the caller needs.\n * If it's different then return false without taking a reference. If a change\n * has been requested, then we also don't want to give out a reference.\n */\nbool nfs_get_grace_status(bool want_grace)\n{\n\tuint32_t cur, pro, old;\n\n\told = atomic_fetch_uint32_t(&grace_status);\n\tdo {\n\t\tcur = old;\n\n\t\t/* If it's not the state we want, then no reference */\n\t\tif (want_grace != (bool)(cur & GRACE_STATUS_ACTIVE))\n\t\t\treturn false;\n\n\t\t/* If a change was requested, no reference */\n\t\tif (cur & GRACE_STATUS_CHANGE_REQ)\n\t\t\treturn false;\n\n\t\t/* Bump the counter */\n\t\tpro = cur + GRACE_STATUS_REF_INCREMENT;\n\t\told = __sync_val_compare_and_swap(&grace_status, cur, pro);\n\t} while (old != cur);\n\treturn true;\n}\n\n/*\n * Put grace status. If the refcount goes to zero, and a change was requested,\n * then wake the reaper thread to do its thing.\n */\nvoid nfs_put_grace_status(void)\n{\n\tuint32_t cur;\n\n\tcur = __sync_fetch_and_sub(&grace_status, GRACE_STATUS_REF_INCREMENT);\n\tif (cur & GRACE_STATUS_CHANGE_REQ &&\n\t    !(cur >> GRACE_STATUS_COUNTER_SHIFT))\n\t\treaper_wake();\n}\n\n/**\n * Lift the grace period if it's still active.\n */\nstatic void\nnfs_lift_grace_locked(void)\n{\n\tuint32_t cur;\n\n\t/*\n\t * Caller must hold grace_mutex. Only the thread that actually sets\n\t * the value to 0 gets to clean up the recovery db.\n\t */\n\tif (nfs_in_grace()) {\n\t\tnfs_end_grace();\n\t\t__sync_synchronize();\n\t\t/* Now change the actual status */\n\t\tcur = __sync_and_and_fetch(&grace_status,\n\t\t\t~(GRACE_STATUS_ACTIVE|GRACE_STATUS_CHANGE_REQ));\n\t\tassert(!(cur & GRACE_STATUS_COUNT_MASK));\n\t\tLogEvent(COMPONENT_STATE, \"NFS Server Now NOT IN GRACE\");\n\t}\n}\n\n/*\n * Report our new state to the cluster\n */\nstatic void nfs4_set_enforcing(void)\n{\n\tif (recovery_backend->set_enforcing)\n\t\trecovery_backend->set_enforcing();\n}\n\n/**\n * @brief Start grace period\n *\n * This routine can be called due to server start/restart or from\n * failover code.  If this node is taking over for a node, that nodeid\n * will be passed to this routine inside of the grace start structure.\n *\n * @param[in] gsp Grace period start information\n */\nvoid nfs_start_grace(nfs_grace_start_t *gsp)\n{\n\tint ret;\n\tbool was_grace;\n\tuint32_t cur, old, pro;\n\n\tPTHREAD_MUTEX_lock(&grace_mutex);\n\n\tif (nfs_param.nfsv4_param.graceless) {\n\t\tnfs_lift_grace_locked();\n\t\tLogEvent(COMPONENT_STATE,\n\t\t\t \"NFS Server skipping GRACE (Graceless is true)\");\n\t\tgoto out;\n\t}\n\n\t/* grace should always be greater than or equal to lease time,\n\t * some clients are known to have problems with grace greater than 60\n\t * seconds Lease_Lifetime should be set to a smaller value for those\n\t * setups.\n\t *\n\t * Checks against the grace period are lockless, so we want to ensure\n\t * that the callers see the\n\t * Full barrier to ensure enforcement begins ASAP.\n\t */\n\n\t/*\n\t * Ensure there are no outstanding references to the current state of\n\t * grace. If there are, set flag indicating that a change has been\n\t * requested and that no more references will be handed out until it\n\t * takes effect.\n\t */\n\tret = clock_gettime(CLOCK_MONOTONIC, &current_grace);\n\tif (ret != 0) {\n\t\tLogCrit(COMPONENT_MAIN, \"Failed to get timestamp\");\n\t\tassert(0);\t/* if this is broken, we are toast so die */\n\t}\n\n\tcur = atomic_fetch_uint32_t(&grace_status);\n\tdo {\n\t\told = cur;\n\t\twas_grace = cur & GRACE_STATUS_ACTIVE;\n\n\t\t/* If we're already in a grace period then we're done */\n\t\tif (was_grace)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Are there outstanding refs? If so, then set the change req\n\t\t * flag and nothing else. If not, then clear the change req\n\t\t * flag and flip the active bit.\n\t\t */\n\t\tif (old & GRACE_STATUS_COUNT_MASK) {\n\t\t\tpro = old | GRACE_STATUS_CHANGE_REQ;\n\t\t} else {\n\t\t\tpro = old | GRACE_STATUS_ACTIVE;\n\t\t\tpro &= ~GRACE_STATUS_CHANGE_REQ;\n\t\t}\n\n\t\t/* If there are no changes, then we don't need to update */\n\t\tif (pro == old)\n\t\t\tbreak;\n\t\tcur = __sync_val_compare_and_swap(&grace_status, old, pro);\n\t} while (cur != old);\n\n\t/*\n\t * If we were not in a grace period before and there were still\n\t * references outstanding, then we can't do anything else.\n\t */\n\tif (!was_grace && (old & GRACE_STATUS_COUNT_MASK))\n\t\tgoto out;\n\n\t__sync_synchronize();\n\n\tif ((int)nfs_param.nfsv4_param.grace_period <\n\t\t(int)nfs_param.nfsv4_param.lease_lifetime) {\n\t\tLogWarn(COMPONENT_STATE,\n\t\t \"NFS Server GRACE duration should at least match LEASE period. Current configured values are GRACE(%d), LEASE(%d)\",\n\t\t (int)nfs_param.nfsv4_param.grace_period,\n\t\t (int)nfs_param.nfsv4_param.lease_lifetime);\n\t}\n\n\tLogEvent(COMPONENT_STATE, \"NFS Server Now IN GRACE, duration %d\",\n\t\t (int)nfs_param.nfsv4_param.grace_period);\n\n\t/* Set enforcing flag here */\n\tif (!was_grace)\n\t\tnfs4_set_enforcing();\n\n\t/*\n\t * If we're just starting the grace period, then load the\n\t * clid database. Don't load it however if we're extending the\n\t * existing grace period.\n\t */\n\tif (!gsp && !was_grace) {\n\t\tnfs4_recovery_load_clids(NULL);\n\t} else if (gsp && gsp->event != EVENT_JUST_GRACE) {\n\t\t/*\n\t\t * if called from failover code and given a nodeid, then this\n\t\t * node is doing a take over.  read in the client ids from the\n\t\t * failing node.\n\t\t */\n\t\tLogEvent(COMPONENT_STATE,\n\t\t\t \"NFS Server recovery event %d nodeid %d ip %s\",\n\t\t\t gsp->event, gsp->nodeid, gsp->ipaddr);\n\n\t\tif (gsp->event == EVENT_CLEAR_BLOCKED)\n\t\t\tcancel_all_nlm_blocked();\n\t\telse {\n\t\t\tnfs_release_nlm_state(gsp->ipaddr);\n\t\t\tif (gsp->event == EVENT_RELEASE_IP) {\n\t\t\t\tPTHREAD_MUTEX_unlock(&grace_mutex);\n\t\t\t\tnfs_release_v4_clients(gsp->ipaddr);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tnfs4_recovery_load_clids(gsp);\n\t\t\t}\n\t\t}\n\t}\nout:\n\tPTHREAD_MUTEX_unlock(&grace_mutex);\n}\n\n/**\n * @brief Check if we are in the grace period\n *\n * @retval true if so.\n * @retval false if not.\n */\nbool nfs_in_grace(void)\n{\n\treturn atomic_fetch_uint32_t(&grace_status) & GRACE_STATUS_ACTIVE;\n}\n\n/**\n * @brief Enter the grace period if another node in the cluster needs it\n *\n * Singleton servers generally won't use this operation. Clustered servers\n * call this function to check whether another node might need a grace period.\n */\nvoid nfs_maybe_start_grace(void)\n{\n\tif (!nfs_in_grace() && recovery_backend->maybe_start_grace)\n\t\trecovery_backend->maybe_start_grace();\n}\n\n/**\n * @brief Are all hosts in cluster enforcing the grace period?\n *\n * Singleton servers always return true here since the only grace period that\n * matters is the local one. Clustered backends should check to make sure that\n * the whole cluster is in grace.\n */\nbool nfs_grace_enforcing(void)\n{\n\tif (recovery_backend->grace_enforcing)\n\t\treturn recovery_backend->grace_enforcing();\n\treturn true;\n}\n\n/**\n * @brief Is this host still a member of the cluster?\n *\n * Singleton servers are always considered to be cluster members. This call\n * is mainly for clustered servers, which may need to handle things differently\n * on a clean shutdown depending on whether they are still a member of the\n * cluster.\n */\nbool nfs_grace_is_member(void)\n{\n\tif (recovery_backend->is_member)\n\t\treturn recovery_backend->is_member();\n\treturn true;\n}\n\n/**\n * @brief Return nodeid for the server\n *\n * If the recovery backend specifies a nodeid, return it. If it does not\n * specify one, default to using the server's hostname.\n *\n * Returns 0 on success and fills out pnodeid. Caller must free the returned\n * value with gsh_free. Returns negative POSIX error code on error.\n */\nint nfs_recovery_get_nodeid(char **pnodeid)\n{\n\tint rc;\n\tlong maxlen;\n\tchar *nodeid = NULL;\n\n\tif (recovery_backend->get_nodeid) {\n\t\trc = recovery_backend->get_nodeid(&nodeid);\n\n\t\t/* Return error if we got one */\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t/* If we got a nodeid, then we're done */\n\t\tif (nodeid) {\n\t\t\t*pnodeid = nodeid;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Either the backend doesn't support get_nodeid or it handed back a\n\t * NULL pointer. Just use hostname.\n\t */\n\tmaxlen = sysconf(_SC_HOST_NAME_MAX);\n\tnodeid = gsh_malloc(maxlen);\n\trc = gethostname(nodeid, maxlen);\n\tif (rc != 0) {\n\t\tLogEvent(COMPONENT_CLIENTID, \"gethostname failed: %d\", errno);\n\t\trc = -errno;\n\t\tgsh_free(nodeid);\n\t} else {\n\t\t*pnodeid = nodeid;\n\t}\n\treturn rc;\n}\n\nvoid nfs_try_lift_grace(void)\n{\n\tbool in_grace = true;\n\tint32_t rc_count = 0;\n\tuint32_t cur, old, pro;\n\n\t/* Already lifted? Just return */\n\tif (!(atomic_fetch_uint32_t(&grace_status) & GRACE_STATUS_ACTIVE))\n\t\treturn;\n\n\t/*\n\t * If we know there are no NLM clients, then we can consider the grace\n\t * period done when all previous clients have sent a RECLAIM_COMPLETE.\n\t */\n\tPTHREAD_MUTEX_lock(&grace_mutex);\n\trc_count = atomic_fetch_int32_t(&reclaim_completes);\n\tif (!nfs_param.core_param.enable_NLM)\n\t\tin_grace = (rc_count != clid_count);\n\n\t/* Otherwise, wait for the timeout */\n\tif (in_grace) {\n\t\tstruct timespec timeout, now;\n\t\tint ret = clock_gettime(CLOCK_MONOTONIC, &now);\n\n\t\tif (ret != 0) {\n\t\t\tLogCrit(COMPONENT_MAIN, \"Failed to get timestamp\");\n\t\t\tassert(0);\n\t\t}\n\n\t\ttimeout = current_grace;\n\t\ttimeout.tv_sec += nfs_param.nfsv4_param.grace_period;\n\t\tin_grace = gsh_time_cmp(&timeout, &now) > 0;\n\t}\n\n\t/*\n\t * Ok, we're basically ready to lift. Ensure there are no outstanding\n\t * references to the current status of the grace period. If there are,\n\t * then set the flag saying that there is an upcoming change.\n\t */\n\n\t/*\n\t * Can we lift the grace period now? If there are any outstanding refs,\n\t * then just set the grace_change_req flag to indicate that we don't\n\t * want to hand any more refs out. Otherwise, we try to lift.\n\t *\n\t * Clustered backends may need extra checks before they can do so. If\n\t * the backend does not implement a try_lift_grace operation, then we\n\t * assume there are no external conditions and that it's always ok.\n\t */\n\tif (!in_grace) {\n\t\tcur = atomic_fetch_uint32_t(&grace_status);\n\t\tdo {\n\t\t\told = cur;\n\n\t\t\t/* Are we already done? Exit if so */\n\t\t\tif (!(cur & GRACE_STATUS_ACTIVE)) {\n\t\t\t\tPTHREAD_MUTEX_unlock(&grace_mutex);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t/* Record that a change has now been requested */\n\t\t\tpro = old | GRACE_STATUS_CHANGE_REQ;\n\t\t\tif (pro == old)\n\t\t\t\tbreak;\n\t\t\tcur = __sync_val_compare_and_swap(&grace_status,\n\t\t\t\t\t\t\t  old, pro);\n\t\t} while (cur != old);\n\n\t\t/* Otherwise, go ahead and lift if we can */\n\t\tif (!(old & GRACE_STATUS_COUNT_MASK) &&\n\t\t    (!recovery_backend->try_lift_grace ||\n\t\t     recovery_backend->try_lift_grace()))\n\t\t\tnfs_lift_grace_locked();\n\t}\n\tPTHREAD_MUTEX_unlock(&grace_mutex);\n}\n\nstatic pthread_cond_t enforcing_cond = PTHREAD_COND_INITIALIZER;\nstatic pthread_mutex_t enforcing_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n/* Poll every 5s, just in case we miss the wakeup for some reason */\nvoid nfs_wait_for_grace_enforcement(void)\n{\n\tnfs_grace_start_t gsp = { .event = EVENT_JUST_GRACE };\n\n\tpthread_mutex_lock(&enforcing_mutex);\n\tnfs_try_lift_grace();\n\twhile (nfs_in_grace() && !nfs_grace_enforcing()) {\n\t\tstruct timespec\ttimeo = { .tv_sec = time(NULL) + 5,\n\t\t\t\t\t  .tv_nsec = 0 };\n\n\t\tpthread_cond_timedwait(&enforcing_cond, &enforcing_mutex,\n\t\t\t\t\t\t&timeo);\n\n\t\tpthread_mutex_unlock(&enforcing_mutex);\n\t\tnfs_start_grace(&gsp);\n\t\tnfs_try_lift_grace();\n\t\tpthread_mutex_lock(&enforcing_mutex);\n\t}\n\tpthread_mutex_unlock(&enforcing_mutex);\n}\n\nvoid nfs_notify_grace_waiters(void)\n{\n\tpthread_mutex_lock(&enforcing_mutex);\n\tpthread_cond_broadcast(&enforcing_cond);\n\tpthread_mutex_unlock(&enforcing_mutex);\n}\n\n/**\n * @brief Create an entry in the recovery directory\n *\n * This entry allows the client to reclaim state after a server\n * reboot/restart.\n *\n * @param[in] clientid Client record\n */\nvoid nfs4_add_clid(nfs_client_id_t *clientid)\n{\n\tPTHREAD_MUTEX_lock(&clientid->cid_mutex);\n\trecovery_backend->add_clid(clientid);\n\tPTHREAD_MUTEX_unlock(&clientid->cid_mutex);\n}\n\n/**\n * @brief Remove a client entry from the recovery directory\n *\n * This function would be called when a client expires.\n *\n */\nvoid nfs4_rm_clid(nfs_client_id_t *clientid)\n{\n\tPTHREAD_MUTEX_lock(&clientid->cid_mutex);\n\trecovery_backend->rm_clid(clientid);\n\tPTHREAD_MUTEX_unlock(&clientid->cid_mutex);\n}\n\nstatic bool check_clid(nfs_client_id_t *clientid, clid_entry_t *clid_ent)\n{\n\tbool ret = false;\n\n\n\tLogDebug(COMPONENT_CLIENTID, \"compare %s to %s\",\n\t\t clientid->cid_recov_tag, clid_ent->cl_name);\n\n\tif (clientid->cid_recov_tag &&\n\t    !strncmp(clientid->cid_recov_tag,\n\t\t     clid_ent->cl_name, PATH_MAX))\n\t\tret = true;\n\n\treturn ret;\n}\n\n/**\n * @brief Determine whether or not this client may reclaim state\n *\n * If the server is not in grace period, then no reclaim can happen.\n *\n * @param[in] clientid Client record\n */\nvoid  nfs4_chk_clid_impl(nfs_client_id_t *clientid, clid_entry_t **clid_ent_arg)\n{\n\tstruct glist_head *node;\n\tclid_entry_t *clid_ent;\n\t*clid_ent_arg = NULL;\n\n\tLogDebug(COMPONENT_CLIENTID, \"chk for %lu\",\n\t\t clientid->cid_clientid);\n\n\t/* If there were no clients at time of restart, we're done */\n\tif (clid_count == 0)\n\t\treturn;\n\n\t/*\n\t * loop through the list and try to find this client. If we\n\t * find it, mark it to allow reclaims.\n\t */\n\tPTHREAD_MUTEX_lock(&clientid->cid_mutex);\n\tglist_for_each(node, &clid_list) {\n\t\tclid_ent = glist_entry(node, clid_entry_t, cl_list);\n\t\tif (check_clid(clientid, clid_ent)) {\n\t\t\tif (isDebug(COMPONENT_CLIENTID)) {\n\t\t\t\tchar str[LOG_BUFF_LEN] = \"\\0\";\n\t\t\t\tstruct display_buffer dspbuf = {\n\t\t\t\t\tsizeof(str), str, str};\n\n\t\t\t\tdisplay_client_id_rec(&dspbuf, clientid);\n\n\t\t\t\tLogFullDebug(COMPONENT_CLIENTID,\n\t\t\t\t\t     \"Allowed to reclaim ClientId %s\",\n\t\t\t\t\t     str);\n\t\t\t}\n\t\t\tclientid->cid_allow_reclaim = true;\n\t\t\t*clid_ent_arg = clid_ent;\n\t\t\tbreak;\n\t\t}\n\t}\n\tPTHREAD_MUTEX_unlock(&clientid->cid_mutex);\n}\n\nvoid  nfs4_chk_clid(nfs_client_id_t *clientid)\n{\n\tclid_entry_t *dummy_clid_ent;\n\n\tPTHREAD_MUTEX_lock(&grace_mutex);\n\tnfs4_chk_clid_impl(clientid, &dummy_clid_ent);\n\tPTHREAD_MUTEX_unlock(&grace_mutex);\n}\n\n/**\n * @brief Load clients for recovery\n *\n * @param[in] nodeid Node, on takeover\n *\n * Caller must hold grace_mutex.\n */\nstatic void nfs4_recovery_load_clids(nfs_grace_start_t *gsp)\n{\n\tLogDebug(COMPONENT_STATE, \"Load recovery cli %p\", gsp);\n\n\t/* A NULL gsp pointer indicates an initial startup grace period */\n\tif (gsp == NULL)\n\t\tnfs4_cleanup_clid_entries();\n\trecovery_backend->recovery_read_clids(gsp, nfs4_add_clid_entry,\n\t\t\t\t\t\tnfs4_add_rfh_entry);\n}\n\n#ifdef USE_RADOS_RECOV\nstatic struct {\n\tvoid *dl;\n\tvoid (*kv_init)(struct nfs4_recovery_backend **);\n\tvoid (*ng_init)(struct nfs4_recovery_backend **);\n\tvoid (*cluster_init)(struct nfs4_recovery_backend **);\n\tint (*load_config_from_parse)(config_file_t,\n\t\t\t\t      struct config_error_type *);\n} rados = { NULL,};\n\nstatic int load_rados_recov(void)\n{\n\trados.dl = dlopen(\"libganesha_rados_recov.so\",\n#if defined(LINUX) && !defined(SANITIZE_ADDRESS)\n\t\t\t  RTLD_NOW | RTLD_LOCAL | RTLD_DEEPBIND);\n#elif defined(FREEBSD) || defined(SANITIZE_ADDRESS)\n\t\t\t  RTLD_NOW | RTLD_LOCAL);\n#endif\n\n\tif (rados.dl) {\n\t\trados.kv_init = dlsym(rados.dl, \"rados_kv_backend_init\");\n\t\trados.ng_init = dlsym(rados.dl, \"rados_ng_backend_init\");\n\t\trados.cluster_init = dlsym(rados.dl,\n\t\t\t\t\t   \"rados_cluster_backend_init\");\n\t\trados.load_config_from_parse = dlsym(rados.dl,\n\t\t\t\t\t   \"rados_load_config_from_parse\");\n\n\t\tif (!rados.kv_init || !rados.ng_init || !rados.cluster_init ||\n\t\t    !rados.load_config_from_parse) {\n\t\t\tdlclose(rados.dl);\n\t\t\trados.dl = NULL;\n\t\t\treturn -1;\n\t\t}\n\t} else {\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n#endif\n\nconst char *recovery_backend_str(enum recovery_backend recovery_backend)\n{\n\tswitch (recovery_backend) {\n\tcase RECOVERY_BACKEND_FS:\n\t\treturn \"fs\";\n\tcase RECOVERY_BACKEND_FS_NG:\n\t\treturn \"fs_ng\";\n\tcase RECOVERY_BACKEND_RADOS_KV:\n\t\treturn \"rados_kv\";\n\tcase RECOVERY_BACKEND_RADOS_NG:\n\t\treturn \"rados_ng\";\n\tcase RECOVERY_BACKEND_RADOS_CLUSTER:\n\t\treturn \"rados_cluster\";\n\t}\n\n\treturn \"Unknown recovery backend\";\n}\n\n/**\n * @brief Create the recovery directory\n *\n * The recovery directory may not exist yet, so create it.  This\n * should only need to be done once (if at all).  Also, the location\n * of the directory could be configurable.\n */\nint nfs4_recovery_init(void)\n{\n\tLogInfo(COMPONENT_CLIENTID, \"Recovery Backend Init for %s\",\n\t\trecovery_backend_str(nfs_param.nfsv4_param.recovery_backend));\n\n\tswitch (nfs_param.nfsv4_param.recovery_backend) {\n\tcase RECOVERY_BACKEND_FS:\n\t\tfs_backend_init(&recovery_backend);\n\t\tbreak;\n\tcase RECOVERY_BACKEND_FS_NG:\n\t\tfs_ng_backend_init(&recovery_backend);\n\t\tbreak;\n#ifdef USE_RADOS_RECOV\n\tcase RECOVERY_BACKEND_RADOS_KV:\n\t\trados.kv_init(&recovery_backend);\n\t\tbreak;\n\tcase RECOVERY_BACKEND_RADOS_NG:\n\t\trados.ng_init(&recovery_backend);\n\t\tbreak;\n\tcase RECOVERY_BACKEND_RADOS_CLUSTER:\n\t\trados.cluster_init(&recovery_backend);\n\t\tbreak;\n#else\n\tcase RECOVERY_BACKEND_RADOS_KV:\n\tcase RECOVERY_BACKEND_RADOS_NG:\n\tcase RECOVERY_BACKEND_RADOS_CLUSTER:\n#endif\n\tdefault:\n\t\tLogCrit(COMPONENT_CLIENTID, \"Unsupported Backend %s\",\n\t\t\trecovery_backend_str(\n\t\t\t\tnfs_param.nfsv4_param.recovery_backend));\n\t\treturn -ENOENT;\n\t}\n\n\treturn recovery_backend->recovery_init();\n}\n\n/**\n * @brief Shut down the recovery backend\n *\n * Shut down the recovery backend, cleaning up any clients or tracking\n * structures in preparation for server shutdown.\n */\nvoid nfs4_recovery_shutdown(void)\n{\n\tif (recovery_backend->recovery_shutdown)\n\t\trecovery_backend->recovery_shutdown();\n#ifdef USE_RADOS_RECOV\n\tif (rados.dl)\n\t\t(void) dlclose(rados.dl);\n\trados.dl = NULL;\n#endif\n}\n\n/**\n * @brief Clean up recovery directory\n */\nvoid nfs_end_grace(void)\n{\n\trecovery_backend->end_grace();\n}\n\n/**\n * @brief Record revoked filehandle under the client.\n *\n * @param[in] clientid Client record\n * @param[in] filehandle of the revoked file.\n */\nvoid nfs4_record_revoke(nfs_client_id_t *delr_clid, nfs_fh4 *delr_handle)\n{\n\t/* A client's lease is reserved while recalling or revoking a\n\t * delegation which means the client will not expire until we\n\t * complete this revoke operation. The only exception is when\n\t * the reaper thread revokes delegations of an already expired\n\t * client!\n\t */\n\tPTHREAD_MUTEX_lock(&delr_clid->cid_mutex);\n\tif (delr_clid->cid_confirmed == EXPIRED_CLIENT_ID) {\n\t\t/* Called from reaper thread, no need to record\n\t\t * revoked file handles for an expired client.\n\t\t */\n\t\tPTHREAD_MUTEX_unlock(&delr_clid->cid_mutex);\n\t\treturn;\n\t}\n\trecovery_backend->add_revoke_fh(delr_clid, delr_handle);\n\tPTHREAD_MUTEX_unlock(&delr_clid->cid_mutex);\n}\n\n/**\n * @brief Decides if it is allowed to reclaim a given delegation\n *\n * @param[in] clientid Client record\n * @param[in] filehandle of the revoked file.\n * @retval true if allowed and false if not.\n *\n */\nbool nfs4_check_deleg_reclaim(nfs_client_id_t *clid, nfs_fh4 *fhandle)\n{\n\tchar rhdlstr[NAME_MAX];\n\tstruct glist_head *node;\n\trdel_fh_t *rfh_entry;\n\tclid_entry_t *clid_ent;\n\tint b64ret;\n\tbool retval = true;\n\n\t/* Convert nfs_fh4_val into base64 encoded string */\n\tb64ret = base64url_encode(fhandle->nfs_fh4_val, fhandle->nfs_fh4_len,\n\t\t\t\t  rhdlstr, sizeof(rhdlstr));\n\tassert(b64ret != -1);\n\n\tPTHREAD_MUTEX_lock(&grace_mutex);\n\tnfs4_chk_clid_impl(clid, &clid_ent);\n\tif (clid_ent) {\n\t\tglist_for_each(node, &clid_ent->cl_rfh_list) {\n\t\t\trfh_entry = glist_entry(node, rdel_fh_t, rdfh_list);\n\t\t\tassert(rfh_entry != NULL);\n\t\t\tif (!strcmp(rhdlstr, rfh_entry->rdfh_handle_str)) {\n\t\t\t\tLogFullDebug(COMPONENT_CLIENTID,\n\t\t\t\t\t\"Can't reclaim revoked fh:%s\",\n\t\t\t\t\trfh_entry->rdfh_handle_str);\n\t\t\t\tretval = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tPTHREAD_MUTEX_unlock(&grace_mutex);\n\tLogFullDebug(COMPONENT_CLIENTID, \"Returning %s\",\n\t\t     retval ? \"TRUE\" : \"FALSE\");\n\treturn retval;\n}\n\n#ifdef _USE_NLM\n/**\n * @brief Release NLM state\n */\nstatic void nlm_releasecall(struct fridgethr_context *ctx)\n{\n\tstate_nsm_client_t *nsm_cp;\n\tstate_status_t err;\n\n\tnsm_cp = ctx->arg;\n\terr = state_nlm_notify(nsm_cp, false, 0);\n\tif (err != STATE_SUCCESS)\n\t\tLogDebug(COMPONENT_STATE,\n\t\t\t\"state_nlm_notify failed with %d\",\n\t\t\terr);\n\tdec_nsm_client_ref(nsm_cp);\n}\n#endif /* _USE_NLM */\n\nvoid extractv4(char *ipv6, char *ipv4, size_t size)\n{\n\tchar *token, *saveptr;\n\tchar *delim = \":\";\n\n\ttoken = strtok_r(ipv6, delim, &saveptr);\n\twhile (token != NULL) {\n\t\t/* IPv4 delimiter is '.' */\n\t\tif (strchr(token, '.') != NULL) {\n\t\t\t(void) strlcpy(ipv4, token, size);\n\t\t\treturn;\n\t\t}\n\t\ttoken = strtok_r(NULL, delim, &saveptr);\n\t}\n\t/* failed, copy a null string */\n\tipv4[0] = '\\0';\n}\n\nbool ip_str_match(char *release_ip, char *server_ip)\n{\n\tbool ripv6, sipv6;\n\tchar ipv4[SOCK_NAME_MAX];\n\n\t/* IPv6 delimiter is ':' */\n\tripv6 = (strchr(release_ip, ':') != NULL);\n\tsipv6 = (strchr(server_ip, ':') != NULL);\n\n\tif (ripv6) {\n\t\tif (sipv6)\n\t\t\treturn !strcmp(release_ip, server_ip);\n\t\telse {\n\t\t\t/* extract v4 addr from release_ip*/\n\t\t\textractv4(release_ip, ipv4, sizeof(ipv4));\n\t\t\treturn !strcmp(ipv4, server_ip);\n\t\t}\n\t} else {\n\t\tif (sipv6) {\n\t\t\t/* extract v4 addr from server_ip*/\n\t\t\textractv4(server_ip, ipv4, sizeof(ipv4));\n\t\t\treturn !strcmp(ipv4, release_ip);\n\t\t}\n\t}\n\t/* Both are ipv4 addresses */\n\treturn !strcmp(release_ip, server_ip);\n}\n\n/**\n * @brief Release all NLM state\n */\nstatic void nfs_release_nlm_state(char *release_ip)\n{\n#ifdef _USE_NLM\n\thash_table_t *ht = ht_nlm_client;\n\tstate_nlm_client_t *nlm_cp;\n\tstate_nsm_client_t *nsm_cp;\n\tstruct rbt_head *head_rbt;\n\tstruct rbt_node *pn;\n\tstruct hash_data *pdata;\n\tstate_status_t state_status;\n\tchar serverip[SOCK_NAME_MAX];\n\tint i;\n\n\tLogDebug(COMPONENT_STATE, \"Release all NLM locks\");\n\n\tcancel_all_nlm_blocked();\n\n\t/* walk the client list and call state_nlm_notify */\n\tfor (i = 0; i < ht->parameter.index_size; i++) {\n\t\tPTHREAD_RWLOCK_wrlock(&ht->partitions[i].lock);\n\t\thead_rbt = &ht->partitions[i].rbt;\n\t\t/* go through all entries in the red-black-tree */\n\t\tRBT_LOOP(head_rbt, pn) {\n\t\t\tpdata = RBT_OPAQ(pn);\n\t\t\tnlm_cp = (state_nlm_client_t *) pdata->val.addr;\n\n\t\t\tif (sprint_sockip(&nlm_cp->slc_server_addr,\n\t\t\t\t\t  serverip, sizeof(serverip)) &&\n\t\t\t    ip_str_match(release_ip, serverip)) {\n\t\t\t\tnsm_cp = nlm_cp->slc_nsm_client;\n\t\t\t\tinc_nsm_client_ref(nsm_cp);\n\t\t\t\tstate_status = fridgethr_submit(\n\t\t\t\t\t\tstate_async_fridge,\n\t\t\t\t\t\tnlm_releasecall,\n\t\t\t\t\t\tnsm_cp);\n\t\t\t\tif (state_status != STATE_SUCCESS) {\n\t\t\t\t\tdec_nsm_client_ref(nsm_cp);\n\t\t\t\t\tLogCrit(COMPONENT_STATE,\n\t\t\t\t\t\t\"failed to submit nlm release thread \");\n\t\t\t\t}\n\t\t\t}\n\t\t\tRBT_INCREMENT(pn);\n\t\t}\n\t\tPTHREAD_RWLOCK_unlock(&ht->partitions[i].lock);\n\t}\n#endif /* _USE_NLM */\n}\n\nstatic int ip_match(char *ip, nfs_client_id_t *cid)\n{\n\tchar *haystack;\n\tchar *value = cid->cid_client_record->cr_client_val;\n\tint len = cid->cid_client_record->cr_client_val_len;\n\n\tLogDebug(COMPONENT_STATE, \"NFS Server V4 match ip %s with (%.*s)\",\n\t\t ip, len, value);\n\n\tif (strlen(ip) == 0)\t/* No IP all are matching */\n\t\treturn 1;\n\n\thaystack = alloca(len + 1);\n\tmemcpy(haystack, value, len);\n\thaystack[len] = '\\0';\n\tif (strstr(haystack, ip) != NULL)\n\t\treturn 1;\n\n\treturn 0;\t\t/* no match */\n}\n\n/*\n * try to find a V4 clients which match the IP we are releasing.\n * only search the confirmed clients, unconfirmed clients won't\n * have any state to release.\n */\nstatic void nfs_release_v4_clients(char *ip)\n{\n\thash_table_t *ht = ht_confirmed_client_id;\n\tstruct rbt_head *head_rbt;\n\tstruct rbt_node *pn;\n\tstruct hash_data *pdata;\n\tnfs_client_id_t *cp;\n\tnfs_client_record_t *recp;\n\tint i;\n\n\tLogEvent(COMPONENT_STATE, \"NFS Server V4 recovery release ip %s\", ip);\n\n\t/* go through the confirmed clients looking for a match */\n\tfor (i = 0; i < ht->parameter.index_size; i++) {\n\t\thead_rbt = &ht->partitions[i].rbt;\n\nrestart:\n\t\tPTHREAD_RWLOCK_wrlock(&ht->partitions[i].lock);\n\n\t\t/* go through all entries in the red-black-tree */\n\t\tRBT_LOOP(head_rbt, pn) {\n\t\t\tpdata = RBT_OPAQ(pn);\n\n\t\t\tcp = (nfs_client_id_t *) pdata->val.addr;\n\t\t\tPTHREAD_MUTEX_lock(&cp->cid_mutex);\n\t\t\tif ((cp->cid_confirmed == CONFIRMED_CLIENT_ID)\n\t\t\t     && ip_match(ip, cp)) {\n\t\t\t\tinc_client_id_ref(cp);\n\n\t\t\t\t/* Take a reference to the client record\n\t\t\t\t * before we drop cid_mutex. client record\n\t\t\t\t * may be decoupled, so check if it is still\n\t\t\t\t * coupled!\n\t\t\t\t */\n\t\t\t\trecp = cp->cid_client_record;\n\t\t\t\tif (recp)\n\t\t\t\t\tinc_client_record_ref(recp);\n\n\t\t\t\tPTHREAD_MUTEX_unlock(&cp->cid_mutex);\n\n\t\t\t\tPTHREAD_RWLOCK_unlock(&ht->partitions[i].lock);\n\n\t\t\t\t/* nfs_client_id_expire requires cr_mutex\n\t\t\t\t * if not decoupled alread\n\t\t\t\t */\n\t\t\t\tif (recp)\n\t\t\t\t\tPTHREAD_MUTEX_lock(&recp->cr_mutex);\n\n\t\t\t\tnfs_client_id_expire(cp, true);\n\n\t\t\t\tif (recp) {\n\t\t\t\t\tPTHREAD_MUTEX_unlock(&recp->cr_mutex);\n\t\t\t\t\tdec_client_record_ref(recp);\n\t\t\t\t}\n\n\t\t\t\tdec_client_id_ref(cp);\n\t\t\t\tgoto restart;\n\n\t\t\t} else {\n\t\t\t\tPTHREAD_MUTEX_unlock(&cp->cid_mutex);\n\t\t\t}\n\t\t\tRBT_INCREMENT(pn);\n\t\t}\n\t\tPTHREAD_RWLOCK_unlock(&ht->partitions[i].lock);\n\t}\n}\n\nint load_recovery_param_from_conf(config_file_t parse_tree,\n\t\t\t\t  struct config_error_type *err_type)\n{\n\tswitch (nfs_param.nfsv4_param.recovery_backend) {\n\tcase RECOVERY_BACKEND_FS:\n\tcase RECOVERY_BACKEND_FS_NG:\n\t\treturn 0;\n\n\tcase RECOVERY_BACKEND_RADOS_KV:\n\tcase RECOVERY_BACKEND_RADOS_NG:\n\tcase RECOVERY_BACKEND_RADOS_CLUSTER:\n#ifdef USE_RADOS_RECOV\n\t\t/*\n\t\t * see if we actually need the rados_recovery shlib loaded\n\t\t *\n\t\t * we are here because the config (explicitly) calls\n\t\t * for this recovery class. If we can't do it because\n\t\t * the (package with the) libganesha_rados_recovery\n\t\t * library wasn't installed, then we should return\n\t\t * an error and eventually die.\n\t\t */\n\t\tif (!rados.dl && load_rados_recov() < 0) {\n\t\t\tLogCrit(COMPONENT_CLIENTID,\n\t\t\t\t\"Failed to load Backend %s. Please install the appropriate package\",\n\t\t\t\trecovery_backend_str(\n\t\t\t\t       nfs_param.nfsv4_param.recovery_backend));\n\t\t\treturn -1;\n\t\t}\n\n\t\treturn rados.load_config_from_parse(parse_tree, err_type);\n#endif\n\tdefault:\n\t\tLogCrit(COMPONENT_CLIENTID, \"Unsupported Backend %s\",\n\t\t\trecovery_backend_str(\n\t\t\t\tnfs_param.nfsv4_param.recovery_backend));\n\t}\n\n\treturn -1;\n}\n\n/** @} */\n",
        "/tmp/vanessa/spack-stage/spack-stage-nfs-ganesha-3.0.3-nsvhncc7miczmpey7jvxkhq3mjyiz5yf/spack-src/src/config_parsing/conf_url.c": "/* ----------------------------------------------------------------------------\n * Copyright (C) 2017, Red Hat, Inc.\n * contributeur : Matt Benjamin  mbenjamin@redhat.com\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 3 of the License, or (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with this library; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n * 02110-1301 USA\n * ---------------------------------------\n */\n\n#include \"config.h\"\n#include <regex.h>\n#include <dlfcn.h>\n#include \"log.h\"\n#include \"sal_functions.h\"\n\n#include \"conf_url.h\"\n\nstatic pthread_rwlock_t url_rwlock = PTHREAD_RWLOCK_INITIALIZER;\nstatic struct glist_head url_providers;\nstatic regex_t url_regex;\n\n/** @brief register handler for new url type\n */\nint register_url_provider(struct gsh_url_provider *nurl_p)\n{\n\tstruct gsh_url_provider *url_p;\n\tstruct glist_head *gl;\n\tint code = 0;\n\n\tPTHREAD_RWLOCK_wrlock(&url_rwlock);\n\tglist_for_each(gl, &url_providers) {\n\t\turl_p = glist_entry(gl, struct gsh_url_provider, link);\n\t\tif (!strcasecmp(url_p->name, nurl_p->name)) {\n\t\t\tcode = EEXIST;\n\t\t\tbreak;\n\t\t}\n\t}\n\tnurl_p->url_init();\n\tglist_add_tail(&url_providers, &nurl_p->link);\n\n\tPTHREAD_RWLOCK_unlock(&url_rwlock);\n\treturn code;\n}\n\n/* simplistic URL syntax */\n#define CONFIG_URL_REGEX \\\n\t\"^\\\"?(rados)://([^\\\"]+)\\\"?\"\n\n/** @brief url regex initializer\n */\nstatic void init_url_regex(void)\n{\n\tint r;\n\n\tr = regcomp(&url_regex, CONFIG_URL_REGEX, REG_EXTENDED);\n\tif (!!r) {\n\t\tLogFatal(COMPONENT_INIT,\n\t\t\t\"Error initializing config url regex\");\n\t}\n}\n\n#ifdef RADOS_URLS\nstatic struct {\n\tvoid *dl;\n\tvoid (*pkginit)(void);\n\tint (*setup_watch)(void);\n\tvoid (*shutdown_watch)(void);\n} rados_urls = { NULL,};\n\nstatic void load_rados_config(void)\n{\n\trados_urls.dl = dlopen(\"libganesha_rados_urls.so\",\n#if defined(LINUX) && !defined(SANITIZE_ADDRESS)\n\t\t\t      RTLD_NOW | RTLD_LOCAL | RTLD_DEEPBIND);\n#elif defined(FREEBSD) || defined(SANITIZE_ADDRESS)\n\t\t\t      RTLD_NOW | RTLD_LOCAL);\n#endif\n\n\tif (rados_urls.dl) {\n\t\trados_urls.pkginit = dlsym(rados_urls.dl,\n\t\t\t\t\t   \"conf_url_rados_pkginit\");\n\t\trados_urls.setup_watch = dlsym(rados_urls.dl,\n\t\t\t\t\t       \"rados_url_setup_watch\");\n\t\trados_urls.shutdown_watch = dlsym(rados_urls.dl,\n\t\t\t\t\t\t  \"rados_url_shutdown_watch\");\n\n\t\tif (!rados_urls.pkginit || !rados_urls.setup_watch ||\n\t\t    !rados_urls.shutdown_watch) {\n\t\t\tdlclose(rados_urls.dl);\n\t\t\trados_urls.dl = NULL;\n\t\t\tLogCrit(COMPONENT_CONFIG, \"Unknown urls backend\");\n\t\t}\n\t} else {\n\t\tLogCrit(COMPONENT_CONFIG, \"Unknown urls backend\");\n\t}\n}\n\n#endif\n\n/** @brief package initializer\n */\nvoid config_url_init(void)\n{\n\tglist_init(&url_providers);\n\n/* init well-known URL providers */\n#ifdef RADOS_URLS\n\tif (!rados_urls.dl)\n\t\tload_rados_config();\n\n\tif (rados_urls.pkginit)\n\t\trados_urls.pkginit();\n#endif\n\tinit_url_regex();\n}\n\n/** @brief package shutdown\n */\nvoid config_url_shutdown(void)\n{\n\tstruct gsh_url_provider *url_p;\n\n\tPTHREAD_RWLOCK_wrlock(&url_rwlock);\n\twhile ((url_p = glist_first_entry(\n\t\t\t      &url_providers, struct gsh_url_provider, link))) {\n\t\tglist_del(&url_p->link);\n\t\turl_p->url_shutdown();\n\t}\n\tPTHREAD_RWLOCK_unlock(&url_rwlock);\n\n\tregfree(&url_regex);\n\n#ifdef RADOS_URLS\n\tif (rados_urls.dl)\n\t\tdlclose(rados_urls.dl);\n\trados_urls.dl = NULL;\n#endif\n}\n\nint gsh_rados_url_setup_watch(void)\n{\n#ifdef RADOS_URLS\n\treturn rados_urls.setup_watch ? rados_urls.setup_watch() : -1;\n#else\n\treturn -1;\n#endif\n}\n\nvoid gsh_rados_url_shutdown_watch(void)\n{\n#ifdef RADOS_URLS\n\tif (rados_urls.shutdown_watch)\n\t\trados_urls.shutdown_watch();\n#else\n\treturn; /* non-empty fn to avoid compile warning/error */\n#endif\n}\n\nstatic inline char *match_dup(regmatch_t *m, char *in)\n{\n\tchar *s = NULL;\n\n\tif (m->rm_so >= 0) {\n\t\tint size;\n\n\t\tsize = m->rm_eo - m->rm_so + 1;\n\t\ts = (char *)gsh_malloc(size);\n\t\t(void) snprintf(s, size, \"%s\", in + m->rm_so);\n\t}\n\treturn s;\n}\n\n/** @brief generic url dispatch\n */\nint config_url_fetch(const char *url, FILE **f, char **fbuf)\n{\n\tstruct gsh_url_provider *url_p;\n\tstruct glist_head *gl;\n\tregmatch_t match[3];\n\tchar *url_type = NULL, *m_url = NULL;\n\tint code = EINVAL;\n\n\tcode = regexec(&url_regex, url, 3, match, 0);\n\tif (likely(!code)) {\n\t\t/* matched */\n\t\tregmatch_t *m;\n\n\t\tm = &(match[1]);\n\t\turl_type = match_dup(m, (char *)url);\n\t\tm = &(match[2]);\n\t\tm_url = match_dup(m, (char *)url);\n\t\tif (!(url_type && m_url)) {\n\t\t\tLogWarn(COMPONENT_CONFIG,\n\t\t\t\t\"%s: Failed to match %s as a config URL\",\n\t\t\t\t__func__, url);\n\t\t\tgoto out;\n\t\t}\n\t} else if (code == REG_NOMATCH) {\n\t\tLogWarn(COMPONENT_CONFIG,\n\t\t\t\"%s: Failed to match %s as a config URL\",\n\t\t\t__func__, url);\n\t\tgoto out;\n\t} else {\n\t\tchar ebuf[100];\n\n\t\tregerror(code, &url_regex, ebuf, sizeof(ebuf));\n\t\tLogWarn(COMPONENT_CONFIG,\n\t\t\t\"%s: Error in regexec: %s\",\n\t\t\t__func__, ebuf);\n\t\tgoto out;\n\t}\n\n\tPTHREAD_RWLOCK_rdlock(&url_rwlock);\n\tglist_for_each(gl, &url_providers) {\n\t\turl_p = glist_entry(gl, struct gsh_url_provider, link);\n\t\tif (!strcasecmp(url_type, url_p->name)) {\n\t\t\tcode = url_p->url_fetch(m_url, f, fbuf);\n\t\t\tbreak;\n\t\t}\n\t}\n\tPTHREAD_RWLOCK_unlock(&url_rwlock);\nout:\n\tgsh_free(url_type);\n\tgsh_free(m_url);\n\n\treturn code;\n}\n\n/** @brief return resources allocated by url_fetch\n */\nvoid config_url_release(FILE *f, char *fbuf)\n{\n\tfclose(f);\n\tfree(fbuf);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nfs-ganesha-3.0.3-nsvhncc7miczmpey7jvxkhq3mjyiz5yf/spack-src/src/FSAL/fsal_manager.c": "/*\n * vim:noexpandtab:shiftwidth=8:tabstop=8:\n *\n * Copyright (C) Panasas Inc., 2011\n * Author: Jim Lieb jlieb@panasas.com\n *\n * contributeur : Philippe DENIEL   philippe.deniel@cea.fr\n *                Thomas LEIBOVICI  thomas.leibovici@cea.fr\n *\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 3 of the License, or (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with this library; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n * 02110-1301 USA\n *\n * -------------\n */\n\n/**\n * @addtogroup FSAL\n * @{\n */\n\n/**\n * @file fsal_manager.c\n * @author Jim Lieb <jlieb@panasas.com>\n * @brief FSAL module manager\n */\n\n#include \"config.h\"\n\n#include <stdint.h>\n#include <stddef.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <string.h>\n#include <ctype.h>\n#include <pthread.h>\n#include <dlfcn.h>\n#include \"log.h\"\n#include \"fsal.h\"\n#include \"nfs_core.h\"\n#include \"config_parsing.h\"\n#include \"pnfs_utils.h\"\n#include \"fsal_private.h\"\n\n/**\n * @brief List of loaded fsal modules\n *\n * Static to be private to the functions in this module\n * fsal_lock is taken whenever the list is walked.\n */\n\npthread_mutex_t fsal_lock = PTHREAD_MUTEX_INITIALIZER;\nGLIST_HEAD(fsal_list);\n\n/**\n * @{\n *\n * Variables for passing status/errors between shared object\n * and this module. They must be accessed under lock.\n */\n\nstatic char *dl_error;\nstatic int so_error;\nstatic struct fsal_module *new_fsal;\n\n/**\n * @}\n */\n\n/**\n * @brief FSAL load state\n */\n\nstatic enum load_state {\n\tinit,\t\t/*< In server start state. .init sections can run */\n\tidle,\t\t/*< Switch from init->idle early in main() */\n\tloading,\t/*< In dlopen(). set by load_fsal() just prior */\n\tregistered,\t/*< signal by registration that all is well */\n\terror\t\t/*< signal by registration that all is not well */\n} load_state = init;\n\n\n/**\n * @brief Start a static FSAL\n *\n * Start a FSAL that's statically linked in.\n *\n * @param[in] name\tFSAL name\n * @param[in] init\tInitialization function for FSAL\n */\n\nstatic void load_fsal_static(const char *name, void (*init)(void))\n{\n\tchar *dl_path = gsh_concat(\"Builtin-\", name);\n\tstruct fsal_module *fsal;\n\n\tPTHREAD_MUTEX_lock(&fsal_lock);\n\n\tif (load_state != idle)\n\t\tLogFatal(COMPONENT_INIT, \"Couldn't Register FSAL_%s\", name);\n\n\tif (dl_error) {\n\t\tgsh_free(dl_error);\n\t\tdl_error = NULL;\n\t}\n\n\tload_state = loading;\n\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\n\t/* now it is the module's turn to register itself */\n\tinit();\n\n\tPTHREAD_MUTEX_lock(&fsal_lock);\n\n\tif (load_state != registered)\n\t\tLogFatal(COMPONENT_INIT, \"Couldn't Register FSAL_%s\", name);\n\n\t/* we now finish things up, doing things the module can't see */\n\n\tfsal = new_fsal;   /* recover handle from .ctor and poison again */\n\tnew_fsal = NULL;\n\tfsal->path = dl_path;\n\tfsal->dl_handle = NULL;\n\tso_error = 0;\n\tload_state = idle;\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n}\n\n/**\n * @brief Start_fsals\n *\n * Called early server initialization.  Set load_state to idle\n * at this point as a check on dynamic loading not starting too early.\n */\n\nvoid start_fsals(void)\n{\n\n\t/* .init was a long time ago... */\n\tload_state = idle;\n\n\t/* Load FSAL_MDCACHE */\n\tload_fsal_static(\"MDCACHE\", mdcache_fsal_init);\n\n\t/* Load FSAL_PSEUDO */\n\tload_fsal_static(\"PSEUDO\", pseudo_fsal_init);\n}\n\n/**\n * Enforced filename for FSAL library objects.\n */\n\nstatic const char *pathfmt = \"%s/libfsal%s.so\";\n\n/**\n * @brief Load the fsal's shared object.\n *\n * The dlopen() will trigger a .init constructor which will do the\n * actual registration.  after a successful load, the returned handle\n * needs to be \"put\" back after any other initialization is done.\n *\n * @param[in]  name       Name of the FSAL to load\n * @param[out] fsal_hdl_p Newly allocated FSAL handle\n *\n * @retval 0 Success, when finished, put_fsal_handle() to free\n * @retval EBUSY the loader is busy (should not happen)\n * @retval EEXIST the module is already loaded\n * @retval ENOLCK register_fsal without load_fsal holding the lock.\n * @retval EINVAL wrong loading state for registration\n * @retval ENOMEM out of memory\n * @retval ENOENT could not find \"module_init\" function\n * @retval EFAULT module_init has a bad address\n * @retval other general dlopen errors are possible, all of them bad\n */\n\nint load_fsal(const char *name,\n\t      struct fsal_module **fsal_hdl_p)\n{\n\tvoid *dl = NULL;\n\tint retval = EBUSY;\t/* already loaded */\n\tchar *dl_path;\n\tstruct fsal_module *fsal;\n\tchar *bp;\n\tsize_t size = strlen(nfs_param.core_param.ganesha_modules_loc)\n\t\t      + strlen(name)\n\t\t      + strlen(pathfmt) + 1;\n\tchar *path = alloca(size);\n\n\t(void) snprintf(path, size, pathfmt,\n\t\t\tnfs_param.core_param.ganesha_modules_loc, name);\n\tbp = rindex(path, '/');\n\tbp++; /* now it is the basename, lcase it */\n\twhile (*bp != '\\0') {\n\t\tif (isupper(*bp))\n\t\t\t*bp = tolower(*bp);\n\t\tbp++;\n\t}\n\tdl_path = gsh_strdup(path);\n\n\tPTHREAD_MUTEX_lock(&fsal_lock);\n\tif (load_state != idle)\n\t\tgoto errout;\n\tif (dl_error) {\n\t\tgsh_free(dl_error);\n\t\tdl_error = NULL;\n\t}\n\n\tload_state = loading;\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\n\tLogDebug(COMPONENT_INIT, \"Loading FSAL %s with %s\", name, path);\n#if defined(LINUX) && !defined(SANITIZE_ADDRESS)\n\tdl = dlopen(path, RTLD_NOW | RTLD_LOCAL | RTLD_DEEPBIND);\n#elif defined(FREEBSD) || defined(SANITIZE_ADDRESS)\n\tdl = dlopen(path, RTLD_NOW | RTLD_LOCAL);\n#endif\n\n\tPTHREAD_MUTEX_lock(&fsal_lock);\n\tif (dl == NULL) {\n\t\tdl_error = dlerror();\n\t\tLogFatal(COMPONENT_INIT,\n\t\t\t \"Could not dlopen module: %s Error: %s. You might want to install the nfs-ganesha-%s package\",\n\t\t\t path, dl_error, name);\n\t}\n\tdlerror();\t/* clear it */\n\n/* now it is the module's turn to register itself */\n\n\n\tif (load_state == loading) {\n\t\t/* constructor didn't fire */\n\t\tvoid (*module_init)(void);\n\t\tchar *sym_error;\n\n\t\tmodule_init = dlsym(dl, \"fsal_init\");\n\t\tsym_error = (char *)dlerror();\n\t\tif (sym_error != NULL) {\n\t\t\tdl_error = gsh_strdup(sym_error);\n\t\t\tso_error = ENOENT;\n\t\t\tLogCrit(COMPONENT_INIT,\n\t\t\t\t\"Could not execute symbol fsal_init from module:%s Error:%s\",\n\t\t\t\tpath, dl_error);\n\t\t\tgoto dlerr;\n\t\t}\n\t\tif ((void *)module_init == NULL) {\n\t\t\tso_error = EFAULT;\n\t\t\tLogCrit(COMPONENT_INIT,\n\t\t\t\t\"Could not execute symbol fsal_init from module:%s Error:%s\",\n\t\t\t\tpath, dl_error);\n\t\t\tgoto dlerr;\n\t\t}\n\t\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\n\t\t(*module_init) ();\t/* try registering by hand this time */\n\n\t\tPTHREAD_MUTEX_lock(&fsal_lock);\n\t}\n\tif (load_state == error) {\t/* we are in registration hell */\n\t\tretval = so_error;\t/* this is the registration error */\n\t\tLogCrit(COMPONENT_INIT,\n\t\t\t\"Could not execute symbol fsal_init from module:%s Error:%s\",\n\t\t\tpath, dl_error);\n\t\tgoto dlerr;\n\t}\n\tif (load_state != registered) {\n\t\tretval = EPERM;\n\t\tLogCrit(COMPONENT_INIT,\n\t\t\t\"Could not execute symbol fsal_init from module:%s Error:%s\",\n\t\t\tpath, dl_error);\n\t\tgoto dlerr;\n\t}\n\n/* we now finish things up, doing things the module can't see */\n\n\tfsal = new_fsal;   /* recover handle from .ctor and poison again */\n\tnew_fsal = NULL;\n\n\t/* take initial ref so we can pass it back... */\n\tfsal_get(fsal);\n\n\tLogFullDebug(COMPONENT_FSAL,\n\t\t     \"FSAL %s refcount %\"PRIu32,\n\t\t     name, atomic_fetch_int32_t(&fsal->refcount));\n\n\tfsal->path = dl_path;\n\tfsal->dl_handle = dl;\n\tso_error = 0;\n\t*fsal_hdl_p = fsal;\n\tload_state = idle;\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\treturn 0;\n\ndlerr:\n\tdlclose(dl);\nerrout:\n\tload_state = idle;\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\tLogMajor(COMPONENT_INIT, \"Failed to load module (%s) because: %s\",\n\t\t path,\n\t\t strerror(retval));\n\tgsh_free(dl_path);\n\treturn retval;\n}\n\n/**\n * @brief Look up an FSAL\n *\n * Acquire a handle to the named FSAL and take a reference to it. This\n * must be done before using any methods.  Once done, release it with\n * @c put_fsal.\n *\n * @param[in] name Name to look up\n *\n * @return Module pointer or NULL if not found.\n */\n\nstruct fsal_module *lookup_fsal(const char *name)\n{\n\tstruct fsal_module *fsal;\n\tstruct glist_head *entry;\n\n\tPTHREAD_MUTEX_lock(&fsal_lock);\n\tglist_for_each(entry, &fsal_list) {\n\t\tfsal = glist_entry(entry, struct fsal_module, fsals);\n\t\tif (strcasecmp(name, fsal->name) == 0) {\n\t\t\tfsal_get(fsal);\n\t\t\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\t\t\top_ctx->fsal_module = fsal;\n\t\t\tLogFullDebug(COMPONENT_FSAL,\n\t\t\t\t     \"FSAL %s refcount %\"PRIu32,\n\t\t\t\t     name,\n\t\t\t\t     atomic_fetch_int32_t(&fsal->refcount));\n\t\t\treturn fsal;\n\t\t}\n\t}\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\treturn NULL;\n}\n\n/* functions only called by modules at ctor/dtor time\n */\n\n/**\n * @brief Register the fsal in the system\n *\n * This can be called from three places:\n *\n *  + the server program's .init section if the fsal was statically linked\n *  + the shared object's .init section when load_fsal() dynamically loads it.\n *  + from the shared object's 'fsal_init' function if dlopen does not support\n *    .init/.fini sections.\n *\n * Any other case is an error.\n * Change load_state only for dynamically loaded modules.\n *\n * @param[in] fsal_hdl      FSAL module handle\n * @param[in] name          FSAL name\n * @param[in] major_version Major version\n * @param[in] minor_version Minor version\n *\n * @return 0 on success, otherwise POSIX errors.\n */\n\n/** @todo implement api versioning and pass the major,minor here\n */\n\nint register_fsal(struct fsal_module *fsal_hdl, const char *name,\n\t\t  uint32_t major_version, uint32_t minor_version,\n\t\t  uint8_t fsal_id)\n{\n\tpthread_rwlockattr_t attrs;\n\n\tPTHREAD_MUTEX_lock(&fsal_lock);\n\tif ((major_version != FSAL_MAJOR_VERSION)\n\t    || (minor_version > FSAL_MINOR_VERSION)) {\n\t\tso_error = EINVAL;\n\t\tLogCrit(COMPONENT_INIT,\n\t\t\t\"FSAL \\\"%s\\\" failed to register because of version mismatch core = %d.%d, fsal = %d.%d\",\n\t\t\tname,\n\t\t\tFSAL_MAJOR_VERSION, FSAL_MINOR_VERSION, major_version,\n\t\t\tminor_version);\n\t\tload_state = error;\n\t\tgoto errout;\n\t}\n\tso_error = 0;\n\tif (!(load_state == loading || load_state == init)) {\n\t\tso_error = EACCES;\n\t\tgoto errout;\n\t}\n\tnew_fsal = fsal_hdl;\n\tif (name != NULL)\n\t\tnew_fsal->name = gsh_strdup(name);\n\n\t/* init ops vector to system wide defaults\n\t * from FSAL/default_methods.c\n\t */\n\tmemcpy(&fsal_hdl->m_ops, &def_fsal_ops, sizeof(struct fsal_ops));\n\n\tpthread_rwlockattr_init(&attrs);\n#ifdef GLIBC\n\tpthread_rwlockattr_setkind_np(\n\t\t&attrs,\n\t\tPTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP);\n#endif\n\tPTHREAD_RWLOCK_init(&fsal_hdl->lock, &attrs);\n\tpthread_rwlockattr_destroy(&attrs);\n\tglist_init(&fsal_hdl->servers);\n\tglist_init(&fsal_hdl->handles);\n\tglist_init(&fsal_hdl->exports);\n\tglist_add_tail(&fsal_list, &fsal_hdl->fsals);\n\tif (load_state == loading)\n\t\tload_state = registered;\n\tif (fsal_id != FSAL_ID_NO_PNFS && fsal_id < FSAL_ID_COUNT)\n\t\tpnfs_fsal[fsal_id] = fsal_hdl;\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\treturn 0;\n\n errout:\n\n\tgsh_free(fsal_hdl->path);\n\tgsh_free(fsal_hdl->name);\n\tload_state = error;\n\tPTHREAD_MUTEX_unlock(&fsal_lock);\n\tLogCrit(COMPONENT_INIT, \"FSAL \\\"%s\\\" failed to register because: %s\",\n\t\tname, strerror(so_error));\n\treturn so_error;\n}\n\n/**\n * @brief Unregisterx an FSAL\n *\n * Verify that the fsal is not busy and release all its resources\n * owned at this level.  RW Lock is already freed.  Called from the\n * module's MODULE_FINI\n *\n * @param[in] fsal_hdl FSAL handle\n *\n * @retval 0 on success.\n * @retval EBUSY if FSAL is in use.\n */\n\nint unregister_fsal(struct fsal_module *fsal_hdl)\n{\n\tint32_t refcount = atomic_fetch_int32_t(&fsal_hdl->refcount);\n\n\tif (refcount != 0) {\n\t\t/* this would be very bad */\n\t\tLogCrit(COMPONENT_FSAL,\n\t\t\t\"Unregister FSAL %s with non-zero refcount=%\"PRIi32,\n\t\t\tfsal_hdl->name, refcount);\n\t\treturn EBUSY;\n\t}\n\tgsh_free(fsal_hdl->path);\n\tgsh_free(fsal_hdl->name);\n\treturn 0;\n}\n\n/**\n * @brief Init and commit for FSAL sub-block\n */\n\n/**\n * @brief Initialize space for an FSAL sub-block.\n *\n * We allocate space to hold the name parameter so that\n * is available in the commit phase.\n */\n\nvoid *fsal_init(void *link_mem, void *self_struct)\n{\n\tstruct fsal_args *fp;\n\n\tassert(link_mem != NULL || self_struct != NULL);\n\n\tif (link_mem == NULL) {\n\t\treturn self_struct; /* NOP */\n\t} else if (self_struct == NULL) {\n\t\tvoid *args = gsh_calloc(1, sizeof(struct fsal_args));\n\n\t\tLogFullDebug(COMPONENT_CONFIG,\n\t\t\t     \"Allocating args %p/%p\",\n\t\t\t     link_mem, args);\n\t\treturn args;\n\t} else {\n\t\tfp = self_struct;\n\t\tgsh_free(fp->name);\n\t\tgsh_free(fp);\n\t\treturn NULL;\n\t}\n}\n\n/**\n * @brief Load and initialize FSAL module\n *\n * Use the name parameter to lookup the fsal. If the fsal is not\n * loaded (yet), load it and call its init. This will trigger the\n * processing of a top level block of the same name as the fsal, i.e.\n * the VFS fsal will look for a VFS block and process it (if found).\n *\n * @param[in]  node       parse node of FSAL block\n * @param[in]  name       name of the FSAL to load and initialize (if\n *                        not already loaded)\n * @param[out] fsal_hdl   Pointer to FSAL module or NULL if not found\n * @param[out] err_type   pointer to error type\n *\n * @retval 0 on success, error count on errors\n */\n\nint fsal_load_init(void *node, const char *name, struct fsal_module **fsal_hdl,\n\t\t   struct config_error_type *err_type)\n{\n\tfsal_status_t status;\n\n\tif (name == NULL || strlen(name) == 0) {\n\t\tconfig_proc_error(node, err_type,\n\t\t\t\t  \"Name of FSAL is missing\");\n\t\terr_type->missing = true;\n\t\treturn 1;\n\t}\n\n\t*fsal_hdl = lookup_fsal(name);\n\tif (*fsal_hdl == NULL) {\n\t\tint retval;\n\t\tconfig_file_t myconfig;\n\n\t\tretval = load_fsal(name, fsal_hdl);\n\t\tif (retval != 0) {\n\t\t\tconfig_proc_error(node, err_type,\n\t\t\t\t\t  \"Failed to load FSAL (%s) because: %s\",\n\t\t\t\t\t  name,\tstrerror(retval));\n\t\t\terr_type->fsal = true;\n\t\t\treturn 1;\n\t\t}\n\t\top_ctx->fsal_module = *fsal_hdl;\n\t\tmyconfig = get_parse_root(node);\n\t\tstatus = (*fsal_hdl)->m_ops.init_config(*fsal_hdl,\n\t\t\t\t\t\t\tmyconfig, err_type);\n\t\tif (FSAL_IS_ERROR(status)) {\n\t\t\tconfig_proc_error(node, err_type,\n\t\t\t\t\t  \"Failed to initialize FSAL (%s)\",\n\t\t\t\t\t  name);\n\t\t\tfsal_put(*fsal_hdl);\n\t\t\terr_type->fsal = true;\n\t\t\tLogFullDebug(COMPONENT_FSAL,\n\t\t\t\t     \"FSAL %s refcount %\"PRIu32,\n\t\t\t\t     name,\n\t\t\t\t     atomic_fetch_int32_t(\n\t\t\t\t\t\t&(*fsal_hdl)->refcount));\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * @brief Load and initialize sub-FSAL module\n *\n * @retval 0 on success, error count on errors\n */\n\nint subfsal_commit(void *node, void *link_mem, void *self_struct,\n\t\t   struct config_error_type *err_type)\n{\n\tstruct fsal_module *fsal_next;\n\tstruct subfsal_args *subfsal = (struct subfsal_args *)self_struct;\n\tint errcnt = fsal_load_init(node, subfsal->name, &fsal_next, err_type);\n\n\tif (errcnt == 0)\n\t\tsubfsal->fsal_node = node;\n\n\treturn errcnt;\n}\n\n/** @} */\n"
    },
    "skipped": [],
    "total_files": 834
}