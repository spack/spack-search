{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/compat/w32dlfcn.h": "/*\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#ifndef COMPAT_W32DLFCN_H\n#define COMPAT_W32DLFCN_H\n\n#ifdef _WIN32\n#include <windows.h>\n#include \"config.h\"\n#if (_WIN32_WINNT < 0x0602) || HAVE_WINRT\n#include \"libavutil/wchar_filename.h\"\n#endif\n/**\n * Safe function used to open dynamic libs. This attempts to improve program security\n * by removing the current directory from the dll search path. Only dll's found in the\n * executable or system directory are allowed to be loaded.\n * @param name  The dynamic lib name.\n * @return A handle to the opened lib.\n */\nstatic inline HMODULE win32_dlopen(const char *name)\n{\n#if _WIN32_WINNT < 0x0602\n    // Need to check if KB2533623 is available\n    if (!GetProcAddress(GetModuleHandleW(L\"kernel32.dll\"), \"SetDefaultDllDirectories\")) {\n        HMODULE module = NULL;\n        wchar_t *path = NULL, *name_w = NULL;\n        DWORD pathlen;\n        if (utf8towchar(name, &name_w))\n            goto exit;\n        path = (wchar_t *)av_mallocz_array(MAX_PATH, sizeof(wchar_t));\n        // Try local directory first\n        pathlen = GetModuleFileNameW(NULL, path, MAX_PATH);\n        pathlen = wcsrchr(path, '\\\\') - path;\n        if (pathlen == 0 || pathlen + wcslen(name_w) + 2 > MAX_PATH)\n            goto exit;\n        path[pathlen] = '\\\\';\n        wcscpy(path + pathlen + 1, name_w);\n        module = LoadLibraryExW(path, NULL, LOAD_WITH_ALTERED_SEARCH_PATH);\n        if (module == NULL) {\n            // Next try System32 directory\n            pathlen = GetSystemDirectoryW(path, MAX_PATH);\n            if (pathlen == 0 || pathlen + wcslen(name_w) + 2 > MAX_PATH)\n                goto exit;\n            path[pathlen] = '\\\\';\n            wcscpy(path + pathlen + 1, name_w);\n            module = LoadLibraryExW(path, NULL, LOAD_WITH_ALTERED_SEARCH_PATH);\n        }\nexit:\n        av_free(path);\n        av_free(name_w);\n        return module;\n    }\n#endif\n#ifndef LOAD_LIBRARY_SEARCH_APPLICATION_DIR\n#   define LOAD_LIBRARY_SEARCH_APPLICATION_DIR 0x00000200\n#endif\n#ifndef LOAD_LIBRARY_SEARCH_SYSTEM32\n#   define LOAD_LIBRARY_SEARCH_SYSTEM32        0x00000800\n#endif\n#if HAVE_WINRT\n    wchar_t *name_w = NULL;\n    int ret;\n    if (utf8towchar(name, &name_w))\n        return NULL;\n    ret = LoadPackagedLibrary(name_w, 0);\n    av_free(name_w);\n    return ret;\n#else\n    return LoadLibraryExA(name, NULL, LOAD_LIBRARY_SEARCH_APPLICATION_DIR | LOAD_LIBRARY_SEARCH_SYSTEM32);\n#endif\n}\n#define dlopen(name, flags) win32_dlopen(name)\n#define dlclose FreeLibrary\n#define dlsym GetProcAddress\n#else\n#include <dlfcn.h>\n#endif\n\n#endif /* COMPAT_W32DLFCN_H */\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/compat/cuda/dynlink_loader.h": "/*\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#ifndef AV_COMPAT_CUDA_DYNLINK_LOADER_H\n#define AV_COMPAT_CUDA_DYNLINK_LOADER_H\n\n#include \"libavutil/log.h\"\n#include \"compat/w32dlfcn.h\"\n\n#define FFNV_LOAD_FUNC(path) dlopen((path), RTLD_LAZY)\n#define FFNV_SYM_FUNC(lib, sym) dlsym((lib), (sym))\n#define FFNV_FREE_FUNC(lib) dlclose(lib)\n#define FFNV_LOG_FUNC(logctx, msg, ...) av_log(logctx, AV_LOG_ERROR, msg,  __VA_ARGS__)\n#define FFNV_DEBUG_LOG_FUNC(logctx, msg, ...) av_log(logctx, AV_LOG_DEBUG, msg,  __VA_ARGS__)\n\n#include <ffnvcodec/dynlink_loader.h>\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavformat/avisynth.c": "/*\n * AviSynth/AvxSynth support\n * Copyright (c) 2012 AvxSynth Team\n *\n * This file is part of FFmpeg\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#include \"libavutil/attributes.h\"\n#include \"libavutil/internal.h\"\n\n#include \"libavcodec/internal.h\"\n\n#include \"avformat.h\"\n#include \"internal.h\"\n#include \"config.h\"\n\n/* Enable function pointer definitions for runtime loading. */\n#define AVSC_NO_DECLSPEC\n\n/* Platform-specific directives for AviSynth vs AvxSynth. */\n#ifdef _WIN32\n  #include \"compat/w32dlfcn.h\"\n  #undef EXTERN_C\n  #include \"compat/avisynth/avisynth_c.h\"\n  #define AVISYNTH_LIB \"avisynth\"\n  #define USING_AVISYNTH\n#else\n  #include <dlfcn.h>\n  #include \"compat/avisynth/avxsynth_c.h\"\n  #define AVISYNTH_NAME \"libavxsynth\"\n  #define AVISYNTH_LIB AVISYNTH_NAME SLIBSUF\n#endif\n\ntypedef struct AviSynthLibrary {\n    void *library;\n#define AVSC_DECLARE_FUNC(name) name ## _func name\n    AVSC_DECLARE_FUNC(avs_bit_blt);\n    AVSC_DECLARE_FUNC(avs_clip_get_error);\n    AVSC_DECLARE_FUNC(avs_create_script_environment);\n    AVSC_DECLARE_FUNC(avs_delete_script_environment);\n    AVSC_DECLARE_FUNC(avs_get_audio);\n    AVSC_DECLARE_FUNC(avs_get_error);\n    AVSC_DECLARE_FUNC(avs_get_frame);\n    AVSC_DECLARE_FUNC(avs_get_version);\n    AVSC_DECLARE_FUNC(avs_get_video_info);\n    AVSC_DECLARE_FUNC(avs_invoke);\n    AVSC_DECLARE_FUNC(avs_release_clip);\n    AVSC_DECLARE_FUNC(avs_release_value);\n    AVSC_DECLARE_FUNC(avs_release_video_frame);\n    AVSC_DECLARE_FUNC(avs_take_clip);\n#ifdef USING_AVISYNTH\n    AVSC_DECLARE_FUNC(avs_bits_per_pixel);\n    AVSC_DECLARE_FUNC(avs_get_height_p);\n    AVSC_DECLARE_FUNC(avs_get_pitch_p);\n    AVSC_DECLARE_FUNC(avs_get_read_ptr_p);\n    AVSC_DECLARE_FUNC(avs_get_row_size_p);\n    AVSC_DECLARE_FUNC(avs_is_planar_rgb);\n    AVSC_DECLARE_FUNC(avs_is_planar_rgba);\n#endif\n#undef AVSC_DECLARE_FUNC\n} AviSynthLibrary;\n\ntypedef struct AviSynthContext {\n    AVS_ScriptEnvironment *env;\n    AVS_Clip *clip;\n    const AVS_VideoInfo *vi;\n\n    /* avisynth_read_packet_video() iterates over this. */\n    int n_planes;\n    const int *planes;\n\n    int curr_stream;\n    int curr_frame;\n    int64_t curr_sample;\n\n    int error;\n\n    /* Linked list pointers. */\n    struct AviSynthContext *next;\n} AviSynthContext;\n\nstatic const int avs_planes_packed[1] = { 0 };\nstatic const int avs_planes_grey[1]   = { AVS_PLANAR_Y };\nstatic const int avs_planes_yuv[3]    = { AVS_PLANAR_Y, AVS_PLANAR_U,\n                                          AVS_PLANAR_V };\n#ifdef USING_AVISYNTH\nstatic const int avs_planes_rgb[3]    = { AVS_PLANAR_G, AVS_PLANAR_B,\n                                          AVS_PLANAR_R };\nstatic const int avs_planes_yuva[4]   = { AVS_PLANAR_Y, AVS_PLANAR_U,\n                                          AVS_PLANAR_V, AVS_PLANAR_A };\nstatic const int avs_planes_rgba[4]   = { AVS_PLANAR_G, AVS_PLANAR_B,\n                                          AVS_PLANAR_R, AVS_PLANAR_A };\n#endif\n\n/* A conflict between C++ global objects, atexit, and dynamic loading requires\n * us to register our own atexit handler to prevent double freeing. */\nstatic AviSynthLibrary avs_library;\nstatic int avs_atexit_called        = 0;\n\n/* Linked list of AviSynthContexts. An atexit handler destroys this list. */\nstatic AviSynthContext *avs_ctx_list = NULL;\n\nstatic av_cold void avisynth_atexit_handler(void);\n\nstatic av_cold int avisynth_load_library(void)\n{\n    avs_library.library = dlopen(AVISYNTH_LIB, RTLD_NOW | RTLD_LOCAL);\n    if (!avs_library.library)\n        return AVERROR_UNKNOWN;\n\n#define LOAD_AVS_FUNC(name, continue_on_fail)                          \\\n        avs_library.name = dlsym(avs_library.library, #name);          \\\n        if (!continue_on_fail && !avs_library.name)                    \\\n            goto fail;\n\n    LOAD_AVS_FUNC(avs_bit_blt, 0);\n    LOAD_AVS_FUNC(avs_clip_get_error, 0);\n    LOAD_AVS_FUNC(avs_create_script_environment, 0);\n    LOAD_AVS_FUNC(avs_delete_script_environment, 0);\n    LOAD_AVS_FUNC(avs_get_audio, 0);\n    LOAD_AVS_FUNC(avs_get_error, 1); // New to AviSynth 2.6\n    LOAD_AVS_FUNC(avs_get_frame, 0);\n    LOAD_AVS_FUNC(avs_get_version, 0);\n    LOAD_AVS_FUNC(avs_get_video_info, 0);\n    LOAD_AVS_FUNC(avs_invoke, 0);\n    LOAD_AVS_FUNC(avs_release_clip, 0);\n    LOAD_AVS_FUNC(avs_release_value, 0);\n    LOAD_AVS_FUNC(avs_release_video_frame, 0);\n    LOAD_AVS_FUNC(avs_take_clip, 0);\n#ifdef USING_AVISYNTH\n    LOAD_AVS_FUNC(avs_bits_per_pixel, 1);\n    LOAD_AVS_FUNC(avs_get_height_p, 1);\n    LOAD_AVS_FUNC(avs_get_pitch_p, 1);\n    LOAD_AVS_FUNC(avs_get_read_ptr_p, 1);\n    LOAD_AVS_FUNC(avs_get_row_size_p, 1);\n    LOAD_AVS_FUNC(avs_is_planar_rgb, 1);\n    LOAD_AVS_FUNC(avs_is_planar_rgba, 1);\n#endif\n#undef LOAD_AVS_FUNC\n\n    atexit(avisynth_atexit_handler);\n    return 0;\n\nfail:\n    dlclose(avs_library.library);\n    return AVERROR_UNKNOWN;\n}\n\n/* Note that avisynth_context_create and avisynth_context_destroy\n * do not allocate or free the actual context! That is taken care of\n * by libavformat. */\nstatic av_cold int avisynth_context_create(AVFormatContext *s)\n{\n    AviSynthContext *avs = s->priv_data;\n    int ret;\n\n    if (!avs_library.library)\n        if (ret = avisynth_load_library())\n            return ret;\n\n    avs->env = avs_library.avs_create_script_environment(3);\n    if (avs_library.avs_get_error) {\n        const char *error = avs_library.avs_get_error(avs->env);\n        if (error) {\n            av_log(s, AV_LOG_ERROR, \"%s\\n\", error);\n            return AVERROR_UNKNOWN;\n        }\n    }\n\n    if (!avs_ctx_list) {\n        avs_ctx_list = avs;\n    } else {\n        avs->next    = avs_ctx_list;\n        avs_ctx_list = avs;\n    }\n\n    return 0;\n}\n\nstatic av_cold void avisynth_context_destroy(AviSynthContext *avs)\n{\n    if (avs_atexit_called)\n        return;\n\n    if (avs == avs_ctx_list) {\n        avs_ctx_list = avs->next;\n    } else {\n        AviSynthContext *prev = avs_ctx_list;\n        while (prev->next != avs)\n            prev = prev->next;\n        prev->next = avs->next;\n    }\n\n    if (avs->clip) {\n        avs_library.avs_release_clip(avs->clip);\n        avs->clip = NULL;\n    }\n    if (avs->env) {\n        avs_library.avs_delete_script_environment(avs->env);\n        avs->env = NULL;\n    }\n}\n\nstatic av_cold void avisynth_atexit_handler(void)\n{\n    AviSynthContext *avs = avs_ctx_list;\n\n    while (avs) {\n        AviSynthContext *next = avs->next;\n        avisynth_context_destroy(avs);\n        avs = next;\n    }\n    dlclose(avs_library.library);\n\n    avs_atexit_called = 1;\n}\n\n/* Create AVStream from audio and video data. */\nstatic int avisynth_create_stream_video(AVFormatContext *s, AVStream *st)\n{\n    AviSynthContext *avs = s->priv_data;\n    int planar = 0; // 0: packed, 1: YUV, 2: Y8, 3: Planar RGB, 4: YUVA, 5: Planar RGBA\n\n    st->codecpar->codec_type = AVMEDIA_TYPE_VIDEO;\n    st->codecpar->codec_id   = AV_CODEC_ID_RAWVIDEO;\n    st->codecpar->width      = avs->vi->width;\n    st->codecpar->height     = avs->vi->height;\n\n    st->avg_frame_rate    = (AVRational) { avs->vi->fps_numerator,\n                                           avs->vi->fps_denominator };\n    st->start_time        = 0;\n    st->duration          = avs->vi->num_frames;\n    st->nb_frames         = avs->vi->num_frames;\n    avpriv_set_pts_info(st, 32, avs->vi->fps_denominator, avs->vi->fps_numerator);\n\n    switch (avs->vi->pixel_type) {\n#ifdef USING_AVISYNTH\n    /* 10~16-bit YUV pix_fmts (AviSynth+) */\n    case AVS_CS_YUV444P10:\n        st->codecpar->format = AV_PIX_FMT_YUV444P10;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV422P10:\n        st->codecpar->format = AV_PIX_FMT_YUV422P10;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV420P10:\n        st->codecpar->format = AV_PIX_FMT_YUV420P10;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV444P12:\n        st->codecpar->format = AV_PIX_FMT_YUV444P12;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV422P12:\n        st->codecpar->format = AV_PIX_FMT_YUV422P12;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV420P12:\n        st->codecpar->format = AV_PIX_FMT_YUV420P12;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV444P14:\n        st->codecpar->format = AV_PIX_FMT_YUV444P14;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV422P14:\n        st->codecpar->format = AV_PIX_FMT_YUV422P14;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV420P14:\n        st->codecpar->format = AV_PIX_FMT_YUV420P14;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV444P16:\n        st->codecpar->format = AV_PIX_FMT_YUV444P16;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV422P16:\n        st->codecpar->format = AV_PIX_FMT_YUV422P16;\n        planar               = 1;\n        break;\n    case AVS_CS_YUV420P16:\n        st->codecpar->format = AV_PIX_FMT_YUV420P16;\n        planar               = 1;\n        break;\n    /* 8~16-bit YUV pix_fmts with Alpha (AviSynth+) */\n    case AVS_CS_YUVA444:\n        st->codecpar->format = AV_PIX_FMT_YUVA444P;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA422:\n        st->codecpar->format = AV_PIX_FMT_YUVA422P;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA420:\n        st->codecpar->format = AV_PIX_FMT_YUVA420P;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA444P10:\n        st->codecpar->format = AV_PIX_FMT_YUVA444P10;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA422P10:\n        st->codecpar->format = AV_PIX_FMT_YUVA422P10;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA420P10:\n        st->codecpar->format = AV_PIX_FMT_YUVA420P10;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA444P16:\n        st->codecpar->format = AV_PIX_FMT_YUVA444P16;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA422P16:\n        st->codecpar->format = AV_PIX_FMT_YUVA422P16;\n        planar               = 4;\n        break;\n    case AVS_CS_YUVA420P16:\n        st->codecpar->format = AV_PIX_FMT_YUVA420P16;\n        planar               = 4;\n        break;\n    /* Planar RGB pix_fmts (AviSynth+) */\n    case AVS_CS_RGBP:\n        st->codecpar->format = AV_PIX_FMT_GBRP;\n        planar               = 3;\n        break;\n    case AVS_CS_RGBP10:\n        st->codecpar->format = AV_PIX_FMT_GBRP10;\n        planar               = 3;\n        break;\n    case AVS_CS_RGBP12:\n        st->codecpar->format = AV_PIX_FMT_GBRP12;\n        planar               = 3;\n        break;\n    case AVS_CS_RGBP14:\n        st->codecpar->format = AV_PIX_FMT_GBRP14;\n        planar               = 3;\n        break;\n    case AVS_CS_RGBP16:\n        st->codecpar->format = AV_PIX_FMT_GBRP16;\n        planar               = 3;\n        break;\n    /* Planar RGB pix_fmts with Alpha (AviSynth+) */\n    case AVS_CS_RGBAP:\n        st->codecpar->format = AV_PIX_FMT_GBRAP;\n        planar               = 5;\n        break;\n    case AVS_CS_RGBAP10:\n        st->codecpar->format = AV_PIX_FMT_GBRAP10;\n        planar               = 5;\n        break;\n    case AVS_CS_RGBAP12:\n        st->codecpar->format = AV_PIX_FMT_GBRAP12;\n        planar               = 5;\n        break;\n    case AVS_CS_RGBAP16:\n        st->codecpar->format = AV_PIX_FMT_GBRAP16;\n        planar               = 5;\n        break;\n    /* GRAY16 (AviSynth+) */\n    case AVS_CS_Y16:\n        st->codecpar->format = AV_PIX_FMT_GRAY16;\n        planar               = 2;\n        break;\n    /* pix_fmts added in AviSynth 2.6 */\n    case AVS_CS_YV24:\n        st->codecpar->format = AV_PIX_FMT_YUV444P;\n        planar               = 1;\n        break;\n    case AVS_CS_YV16:\n        st->codecpar->format = AV_PIX_FMT_YUV422P;\n        planar               = 1;\n        break;\n    case AVS_CS_YV411:\n        st->codecpar->format = AV_PIX_FMT_YUV411P;\n        planar               = 1;\n        break;\n    case AVS_CS_Y8:\n        st->codecpar->format = AV_PIX_FMT_GRAY8;\n        planar               = 2;\n        break;\n    /* 16-bit packed RGB pix_fmts (AviSynth+) */\n    case AVS_CS_BGR48:\n        st->codecpar->format = AV_PIX_FMT_BGR48;\n        break;\n    case AVS_CS_BGR64:\n        st->codecpar->format = AV_PIX_FMT_BGRA64;\n        break;\n#endif\n    /* AviSynth 2.5 and AvxSynth pix_fmts */\n    case AVS_CS_BGR24:\n        st->codecpar->format = AV_PIX_FMT_BGR24;\n        break;\n    case AVS_CS_BGR32:\n        st->codecpar->format = AV_PIX_FMT_RGB32;\n        break;\n    case AVS_CS_YUY2:\n        st->codecpar->format = AV_PIX_FMT_YUYV422;\n        break;\n    case AVS_CS_YV12:\n        st->codecpar->format = AV_PIX_FMT_YUV420P;\n        planar               = 1;\n        break;\n    case AVS_CS_I420: // Is this even used anywhere?\n        st->codecpar->format = AV_PIX_FMT_YUV420P;\n        planar               = 1;\n        break;\n    default:\n        av_log(s, AV_LOG_ERROR,\n               \"unknown AviSynth colorspace %d\\n\", avs->vi->pixel_type);\n        avs->error = 1;\n        return AVERROR_UNKNOWN;\n    }\n\n    switch (planar) {\n#ifdef USING_AVISYNTH\n    case 5: // Planar RGB + Alpha\n        avs->n_planes = 4;\n        avs->planes   = avs_planes_rgba;\n        break;\n    case 4: // YUV + Alpha\n        avs->n_planes = 4;\n        avs->planes   = avs_planes_yuva;\n        break;\n    case 3: // Planar RGB\n        avs->n_planes = 3;\n        avs->planes   = avs_planes_rgb;\n        break;\n#endif\n    case 2: // Y8\n        avs->n_planes = 1;\n        avs->planes   = avs_planes_grey;\n        break;\n    case 1: // YUV\n        avs->n_planes = 3;\n        avs->planes   = avs_planes_yuv;\n        break;\n    default:\n        avs->n_planes = 1;\n        avs->planes   = avs_planes_packed;\n    }\n    return 0;\n}\n\nstatic int avisynth_create_stream_audio(AVFormatContext *s, AVStream *st)\n{\n    AviSynthContext *avs = s->priv_data;\n\n    st->codecpar->codec_type  = AVMEDIA_TYPE_AUDIO;\n    st->codecpar->sample_rate = avs->vi->audio_samples_per_second;\n    st->codecpar->channels    = avs->vi->nchannels;\n    st->duration              = avs->vi->num_audio_samples;\n    avpriv_set_pts_info(st, 64, 1, avs->vi->audio_samples_per_second);\n\n    switch (avs->vi->sample_type) {\n    case AVS_SAMPLE_INT8:\n        st->codecpar->codec_id = AV_CODEC_ID_PCM_U8;\n        break;\n    case AVS_SAMPLE_INT16:\n        st->codecpar->codec_id = AV_CODEC_ID_PCM_S16LE;\n        break;\n    case AVS_SAMPLE_INT24:\n        st->codecpar->codec_id = AV_CODEC_ID_PCM_S24LE;\n        break;\n    case AVS_SAMPLE_INT32:\n        st->codecpar->codec_id = AV_CODEC_ID_PCM_S32LE;\n        break;\n    case AVS_SAMPLE_FLOAT:\n        st->codecpar->codec_id = AV_CODEC_ID_PCM_F32LE;\n        break;\n    default:\n        av_log(s, AV_LOG_ERROR,\n               \"unknown AviSynth sample type %d\\n\", avs->vi->sample_type);\n        avs->error = 1;\n        return AVERROR_UNKNOWN;\n    }\n    return 0;\n}\n\nstatic int avisynth_create_stream(AVFormatContext *s)\n{\n    AviSynthContext *avs = s->priv_data;\n    AVStream *st;\n    int ret;\n    int id = 0;\n\n    if (avs_has_video(avs->vi)) {\n        st = avformat_new_stream(s, NULL);\n        if (!st)\n            return AVERROR_UNKNOWN;\n        st->id = id++;\n        if (ret = avisynth_create_stream_video(s, st))\n            return ret;\n    }\n    if (avs_has_audio(avs->vi)) {\n        st = avformat_new_stream(s, NULL);\n        if (!st)\n            return AVERROR_UNKNOWN;\n        st->id = id++;\n        if (ret = avisynth_create_stream_audio(s, st))\n            return ret;\n    }\n    return 0;\n}\n\nstatic int avisynth_open_file(AVFormatContext *s)\n{\n    AviSynthContext *avs = s->priv_data;\n    AVS_Value arg, val;\n    int ret;\n#ifdef USING_AVISYNTH\n    char filename_ansi[MAX_PATH * 4];\n    wchar_t filename_wc[MAX_PATH * 4];\n#endif\n\n    if (ret = avisynth_context_create(s))\n        return ret;\n\n#ifdef USING_AVISYNTH\n    /* Convert UTF-8 to ANSI code page */\n    MultiByteToWideChar(CP_UTF8, 0, s->filename, -1, filename_wc, MAX_PATH * 4);\n    WideCharToMultiByte(CP_THREAD_ACP, 0, filename_wc, -1, filename_ansi,\n                        MAX_PATH * 4, NULL, NULL);\n    arg = avs_new_value_string(filename_ansi);\n#else\n    arg = avs_new_value_string(s->filename);\n#endif\n    val = avs_library.avs_invoke(avs->env, \"Import\", arg, 0);\n    if (avs_is_error(val)) {\n        av_log(s, AV_LOG_ERROR, \"%s\\n\", avs_as_error(val));\n        ret = AVERROR_UNKNOWN;\n        goto fail;\n    }\n    if (!avs_is_clip(val)) {\n        av_log(s, AV_LOG_ERROR, \"AviSynth script did not return a clip\\n\");\n        ret = AVERROR_UNKNOWN;\n        goto fail;\n    }\n\n    avs->clip = avs_library.avs_take_clip(val, avs->env);\n    avs->vi   = avs_library.avs_get_video_info(avs->clip);\n\n#ifdef USING_AVISYNTH\n    /* On Windows, FFmpeg supports AviSynth interface version 6 or higher.\n     * This includes AviSynth 2.6 RC1 or higher, and AviSynth+ r1718 or higher,\n     * and excludes 2.5 and the 2.6 alphas. Since AvxSynth identifies itself\n     * as interface version 3 like 2.5.8, this needs to be special-cased. */\n\n    if (avs_library.avs_get_version(avs->clip) < 6) {\n        av_log(s, AV_LOG_ERROR,\n               \"AviSynth version is too old. Please upgrade to either AviSynth 2.6 >= RC1 or AviSynth+ >= r1718.\\n\");\n        ret = AVERROR_UNKNOWN;\n        goto fail;\n    }\n#endif\n\n    /* Release the AVS_Value as it will go out of scope. */\n    avs_library.avs_release_value(val);\n\n    if (ret = avisynth_create_stream(s))\n        goto fail;\n\n    return 0;\n\nfail:\n    avisynth_context_destroy(avs);\n    return ret;\n}\n\nstatic void avisynth_next_stream(AVFormatContext *s, AVStream **st,\n                                 AVPacket *pkt, int *discard)\n{\n    AviSynthContext *avs = s->priv_data;\n\n    avs->curr_stream++;\n    avs->curr_stream %= s->nb_streams;\n\n    *st = s->streams[avs->curr_stream];\n    if ((*st)->discard == AVDISCARD_ALL)\n        *discard = 1;\n    else\n        *discard = 0;\n\n    return;\n}\n\n/* Copy AviSynth clip data into an AVPacket. */\nstatic int avisynth_read_packet_video(AVFormatContext *s, AVPacket *pkt,\n                                      int discard)\n{\n    AviSynthContext *avs = s->priv_data;\n    AVS_VideoFrame *frame;\n    unsigned char *dst_p;\n    const unsigned char *src_p;\n    int n, i, plane, rowsize, planeheight, pitch, bits;\n    const char *error;\n    int avsplus av_unused;\n\n    if (avs->curr_frame >= avs->vi->num_frames)\n        return AVERROR_EOF;\n\n    /* This must happen even if the stream is discarded to prevent desync. */\n    n = avs->curr_frame++;\n    if (discard)\n        return 0;\n\n#ifdef USING_AVISYNTH\n    /* Detect whether we're using AviSynth 2.6 or AviSynth+ by\n     * looking for whether avs_is_planar_rgb exists. */\n    if (GetProcAddress(avs_library.library, \"avs_is_planar_rgb\") == NULL)\n        avsplus = 0;\n    else\n        avsplus = 1;\n\n    /* avs_bits_per_pixel changed to AVSC_API with AviSynth 2.6, which\n     * requires going through avs_library, while AvxSynth has it under\n     * the older AVSC_INLINE type, so special-case this. */\n\n    bits = avs_library.avs_bits_per_pixel(avs->vi);\n#else\n    bits = avs_bits_per_pixel(avs->vi);\n#endif\n\n    /* Without the cast to int64_t, calculation overflows at about 9k x 9k\n     * resolution. */\n    pkt->size = (((int64_t)avs->vi->width *\n                  (int64_t)avs->vi->height) * bits) / 8;\n    if (!pkt->size)\n        return AVERROR_UNKNOWN;\n\n    if (av_new_packet(pkt, pkt->size) < 0)\n        return AVERROR(ENOMEM);\n\n    pkt->pts      = n;\n    pkt->dts      = n;\n    pkt->duration = 1;\n    pkt->stream_index = avs->curr_stream;\n\n    frame = avs_library.avs_get_frame(avs->clip, n);\n    error = avs_library.avs_clip_get_error(avs->clip);\n    if (error) {\n        av_log(s, AV_LOG_ERROR, \"%s\\n\", error);\n        avs->error = 1;\n        av_packet_unref(pkt);\n        return AVERROR_UNKNOWN;\n    }\n\n    dst_p = pkt->data;\n    for (i = 0; i < avs->n_planes; i++) {\n        plane = avs->planes[i];\n#ifdef USING_AVISYNTH\n        src_p = avs_library.avs_get_read_ptr_p(frame, plane);\n        pitch = avs_library.avs_get_pitch_p(frame, plane);\n\n        rowsize     = avs_library.avs_get_row_size_p(frame, plane);\n        planeheight = avs_library.avs_get_height_p(frame, plane);\n#else\n        src_p = avs_get_read_ptr_p(frame, plane);\n        pitch = avs_get_pitch_p(frame, plane);\n\n        rowsize     = avs_get_row_size_p(frame, plane);\n        planeheight = avs_get_height_p(frame, plane);\n#endif\n\n        /* Flip RGB video. */\n        if (avs_is_rgb24(avs->vi) || avs_is_rgb(avs->vi)) {\n            src_p = src_p + (planeheight - 1) * pitch;\n            pitch = -pitch;\n        }\n\n#ifdef USING_AVISYNTH\n        /* Flip Planar RGB video */\n        if (avsplus && (avs_library.avs_is_planar_rgb(avs->vi) ||\n                        avs_library.avs_is_planar_rgba(avs->vi))) {\n            src_p = src_p + (planeheight - 1) * pitch;\n            pitch = -pitch;\n        }\n#endif\n\n        avs_library.avs_bit_blt(avs->env, dst_p, rowsize, src_p, pitch,\n                                 rowsize, planeheight);\n        dst_p += rowsize * planeheight;\n    }\n\n    avs_library.avs_release_video_frame(frame);\n    return 0;\n}\n\nstatic int avisynth_read_packet_audio(AVFormatContext *s, AVPacket *pkt,\n                                      int discard)\n{\n    AviSynthContext *avs = s->priv_data;\n    AVRational fps, samplerate;\n    int samples;\n    int64_t n;\n    const char *error;\n\n    if (avs->curr_sample >= avs->vi->num_audio_samples)\n        return AVERROR_EOF;\n\n    fps.num        = avs->vi->fps_numerator;\n    fps.den        = avs->vi->fps_denominator;\n    samplerate.num = avs->vi->audio_samples_per_second;\n    samplerate.den = 1;\n\n    if (avs_has_video(avs->vi)) {\n        if (avs->curr_frame < avs->vi->num_frames)\n            samples = av_rescale_q(avs->curr_frame, samplerate, fps) -\n                      avs->curr_sample;\n        else\n            samples = av_rescale_q(1, samplerate, fps);\n    } else {\n        samples = 1000;\n    }\n\n    /* After seeking, audio may catch up with video. */\n    if (samples <= 0) {\n        pkt->size = 0;\n        pkt->data = NULL;\n        return 0;\n    }\n\n    if (avs->curr_sample + samples > avs->vi->num_audio_samples)\n        samples = avs->vi->num_audio_samples - avs->curr_sample;\n\n    /* This must happen even if the stream is discarded to prevent desync. */\n    n                 = avs->curr_sample;\n    avs->curr_sample += samples;\n    if (discard)\n        return 0;\n\n    pkt->size = avs_bytes_per_channel_sample(avs->vi) *\n                samples * avs->vi->nchannels;\n    if (!pkt->size)\n        return AVERROR_UNKNOWN;\n\n    if (av_new_packet(pkt, pkt->size) < 0)\n        return AVERROR(ENOMEM);\n\n    pkt->pts      = n;\n    pkt->dts      = n;\n    pkt->duration = samples;\n    pkt->stream_index = avs->curr_stream;\n\n    avs_library.avs_get_audio(avs->clip, pkt->data, n, samples);\n    error = avs_library.avs_clip_get_error(avs->clip);\n    if (error) {\n        av_log(s, AV_LOG_ERROR, \"%s\\n\", error);\n        avs->error = 1;\n        av_packet_unref(pkt);\n        return AVERROR_UNKNOWN;\n    }\n    return 0;\n}\n\nstatic av_cold int avisynth_read_header(AVFormatContext *s)\n{\n    int ret;\n\n    // Calling library must implement a lock for thread-safe opens.\n    if (ret = ff_lock_avformat())\n        return ret;\n\n    if (ret = avisynth_open_file(s)) {\n        ff_unlock_avformat();\n        return ret;\n    }\n\n    ff_unlock_avformat();\n    return 0;\n}\n\nstatic int avisynth_read_packet(AVFormatContext *s, AVPacket *pkt)\n{\n    AviSynthContext *avs = s->priv_data;\n    AVStream *st;\n    int discard = 0;\n    int ret;\n\n    if (avs->error)\n        return AVERROR_UNKNOWN;\n\n    /* If either stream reaches EOF, try to read the other one before\n     * giving up. */\n    avisynth_next_stream(s, &st, pkt, &discard);\n    if (st->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {\n        ret = avisynth_read_packet_video(s, pkt, discard);\n        if (ret == AVERROR_EOF && avs_has_audio(avs->vi)) {\n            avisynth_next_stream(s, &st, pkt, &discard);\n            return avisynth_read_packet_audio(s, pkt, discard);\n        }\n    } else {\n        ret = avisynth_read_packet_audio(s, pkt, discard);\n        if (ret == AVERROR_EOF && avs_has_video(avs->vi)) {\n            avisynth_next_stream(s, &st, pkt, &discard);\n            return avisynth_read_packet_video(s, pkt, discard);\n        }\n    }\n\n    return ret;\n}\n\nstatic av_cold int avisynth_read_close(AVFormatContext *s)\n{\n    if (ff_lock_avformat())\n        return AVERROR_UNKNOWN;\n\n    avisynth_context_destroy(s->priv_data);\n    ff_unlock_avformat();\n    return 0;\n}\n\nstatic int avisynth_read_seek(AVFormatContext *s, int stream_index,\n                              int64_t timestamp, int flags)\n{\n    AviSynthContext *avs = s->priv_data;\n    AVStream *st;\n    AVRational fps, samplerate;\n\n    if (avs->error)\n        return AVERROR_UNKNOWN;\n\n    fps        = (AVRational) { avs->vi->fps_numerator,\n                                avs->vi->fps_denominator };\n    samplerate = (AVRational) { avs->vi->audio_samples_per_second, 1 };\n\n    st = s->streams[stream_index];\n    if (st->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {\n        /* AviSynth frame counts are signed int. */\n        if ((timestamp >= avs->vi->num_frames) ||\n            (timestamp > INT_MAX)              ||\n            (timestamp < 0))\n            return AVERROR_EOF;\n        avs->curr_frame = timestamp;\n        if (avs_has_audio(avs->vi))\n            avs->curr_sample = av_rescale_q(timestamp, samplerate, fps);\n    } else {\n        if ((timestamp >= avs->vi->num_audio_samples) || (timestamp < 0))\n            return AVERROR_EOF;\n        /* Force frame granularity for seeking. */\n        if (avs_has_video(avs->vi)) {\n            avs->curr_frame  = av_rescale_q(timestamp, fps, samplerate);\n            avs->curr_sample = av_rescale_q(avs->curr_frame, samplerate, fps);\n        } else {\n            avs->curr_sample = timestamp;\n        }\n    }\n\n    return 0;\n}\n\nAVInputFormat ff_avisynth_demuxer = {\n    .name           = \"avisynth\",\n    .long_name      = NULL_IF_CONFIG_SMALL(\"AviSynth script\"),\n    .priv_data_size = sizeof(AviSynthContext),\n    .read_header    = avisynth_read_header,\n    .read_packet    = avisynth_read_packet,\n    .read_close     = avisynth_read_close,\n    .read_seek      = avisynth_read_seek,\n    .extensions     = \"avs\",\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavfilter/vf_frei0r.c": "/*\n * Copyright (c) 2010 Stefano Sabatini\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n/**\n * @file\n * frei0r wrapper\n */\n\n#include <dlfcn.h>\n#include <frei0r.h>\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include \"config.h\"\n#include \"libavutil/avstring.h\"\n#include \"libavutil/common.h\"\n#include \"libavutil/eval.h\"\n#include \"libavutil/imgutils.h\"\n#include \"libavutil/internal.h\"\n#include \"libavutil/mathematics.h\"\n#include \"libavutil/mem.h\"\n#include \"libavutil/opt.h\"\n#include \"libavutil/parseutils.h\"\n#include \"avfilter.h\"\n#include \"formats.h\"\n#include \"internal.h\"\n#include \"video.h\"\n\ntypedef f0r_instance_t (*f0r_construct_f)(unsigned int width, unsigned int height);\ntypedef void (*f0r_destruct_f)(f0r_instance_t instance);\ntypedef void (*f0r_deinit_f)(void);\ntypedef int (*f0r_init_f)(void);\ntypedef void (*f0r_get_plugin_info_f)(f0r_plugin_info_t *info);\ntypedef void (*f0r_get_param_info_f)(f0r_param_info_t *info, int param_index);\ntypedef void (*f0r_update_f)(f0r_instance_t instance, double time, const uint32_t *inframe, uint32_t *outframe);\ntypedef void (*f0r_update2_f)(f0r_instance_t instance, double time, const uint32_t *inframe1, const uint32_t *inframe2, const uint32_t *inframe3, uint32_t *outframe);\ntypedef void (*f0r_set_param_value_f)(f0r_instance_t instance, f0r_param_t param, int param_index);\ntypedef void (*f0r_get_param_value_f)(f0r_instance_t instance, f0r_param_t param, int param_index);\n\ntypedef struct Frei0rContext {\n    const AVClass *class;\n    f0r_update_f update;\n    void *dl_handle;            /* dynamic library handle   */\n    f0r_instance_t instance;\n    f0r_plugin_info_t plugin_info;\n\n    f0r_get_param_info_f  get_param_info;\n    f0r_get_param_value_f get_param_value;\n    f0r_set_param_value_f set_param_value;\n    f0r_construct_f       construct;\n    f0r_destruct_f        destruct;\n    f0r_deinit_f          deinit;\n\n    char *dl_name;\n    char *params;\n    AVRational framerate;\n\n    /* only used by the source */\n    int w, h;\n    AVRational time_base;\n    uint64_t pts;\n} Frei0rContext;\n\nstatic void *load_sym(AVFilterContext *ctx, const char *sym_name)\n{\n    Frei0rContext *s = ctx->priv;\n    void *sym = dlsym(s->dl_handle, sym_name);\n    if (!sym)\n        av_log(ctx, AV_LOG_ERROR, \"Could not find symbol '%s' in loaded module.\\n\", sym_name);\n    return sym;\n}\n\nstatic int set_param(AVFilterContext *ctx, f0r_param_info_t info, int index, char *param)\n{\n    Frei0rContext *s = ctx->priv;\n    union {\n        double d;\n        f0r_param_color_t col;\n        f0r_param_position_t pos;\n        f0r_param_string *str;\n    } val;\n    char *tail;\n    uint8_t rgba[4];\n\n    switch (info.type) {\n    case F0R_PARAM_BOOL:\n        if      (!strcmp(param, \"y\")) val.d = 1.0;\n        else if (!strcmp(param, \"n\")) val.d = 0.0;\n        else goto fail;\n        break;\n\n    case F0R_PARAM_DOUBLE:\n        val.d = av_strtod(param, &tail);\n        if (*tail || val.d == HUGE_VAL)\n            goto fail;\n        break;\n\n    case F0R_PARAM_COLOR:\n        if (sscanf(param, \"%f/%f/%f\", &val.col.r, &val.col.g, &val.col.b) != 3) {\n            if (av_parse_color(rgba, param, -1, ctx) < 0)\n                goto fail;\n            val.col.r = rgba[0] / 255.0;\n            val.col.g = rgba[1] / 255.0;\n            val.col.b = rgba[2] / 255.0;\n        }\n        break;\n\n    case F0R_PARAM_POSITION:\n        if (sscanf(param, \"%lf/%lf\", &val.pos.x, &val.pos.y) != 2)\n            goto fail;\n        break;\n\n    case F0R_PARAM_STRING:\n        val.str = param;\n        break;\n    }\n\n    s->set_param_value(s->instance, &val, index);\n    return 0;\n\nfail:\n    av_log(ctx, AV_LOG_ERROR, \"Invalid value '%s' for parameter '%s'.\\n\",\n           param, info.name);\n    return AVERROR(EINVAL);\n}\n\nstatic int set_params(AVFilterContext *ctx, const char *params)\n{\n    Frei0rContext *s = ctx->priv;\n    int i;\n\n    if (!params)\n        return 0;\n\n    for (i = 0; i < s->plugin_info.num_params; i++) {\n        f0r_param_info_t info;\n        char *param;\n        int ret;\n\n        s->get_param_info(&info, i);\n\n        if (*params) {\n            if (!(param = av_get_token(&params, \"|\")))\n                return AVERROR(ENOMEM);\n            if (*params)\n                params++;               /* skip ':' */\n            ret = set_param(ctx, info, i, param);\n            av_free(param);\n            if (ret < 0)\n                return ret;\n        }\n    }\n\n    return 0;\n}\n\nstatic int load_path(AVFilterContext *ctx, void **handle_ptr, const char *prefix, const char *name)\n{\n    char *path = av_asprintf(\"%s%s%s\", prefix, name, SLIBSUF);\n    if (!path)\n        return AVERROR(ENOMEM);\n    av_log(ctx, AV_LOG_DEBUG, \"Looking for frei0r effect in '%s'.\\n\", path);\n    *handle_ptr = dlopen(path, RTLD_NOW|RTLD_LOCAL);\n    av_free(path);\n    return 0;\n}\n\nstatic av_cold int frei0r_init(AVFilterContext *ctx,\n                               const char *dl_name, int type)\n{\n    Frei0rContext *s = ctx->priv;\n    f0r_init_f            f0r_init;\n    f0r_get_plugin_info_f f0r_get_plugin_info;\n    f0r_plugin_info_t *pi;\n    char *path;\n    int ret = 0;\n    int i;\n    static const char* const frei0r_pathlist[] = {\n        \"/usr/local/lib/frei0r-1/\",\n        \"/usr/lib/frei0r-1/\",\n        \"/usr/local/lib64/frei0r-1/\",\n        \"/usr/lib64/frei0r-1/\"\n    };\n\n    if (!dl_name) {\n        av_log(ctx, AV_LOG_ERROR, \"No filter name provided.\\n\");\n        return AVERROR(EINVAL);\n    }\n\n    /* see: http://frei0r.dyne.org/codedoc/html/group__pluglocations.html */\n    if ((path = av_strdup(getenv(\"FREI0R_PATH\")))) {\n#ifdef _WIN32\n        const char *separator = \";\";\n#else\n        const char *separator = \":\";\n#endif\n        char *p, *ptr = NULL;\n        for (p = path; p = av_strtok(p, separator, &ptr); p = NULL) {\n            /* add additional trailing slash in case it is missing */\n            char *p1 = av_asprintf(\"%s/\", p);\n            if (!p1) {\n                ret = AVERROR(ENOMEM);\n                goto check_path_end;\n            }\n            ret = load_path(ctx, &s->dl_handle, p1, dl_name);\n            av_free(p1);\n            if (ret < 0)\n                goto check_path_end;\n            if (s->dl_handle)\n                break;\n        }\n\n    check_path_end:\n        av_free(path);\n        if (ret < 0)\n            return ret;\n    }\n    if (!s->dl_handle && (path = getenv(\"HOME\"))) {\n        char *prefix = av_asprintf(\"%s/.frei0r-1/lib/\", path);\n        if (!prefix)\n            return AVERROR(ENOMEM);\n        ret = load_path(ctx, &s->dl_handle, prefix, dl_name);\n        av_free(prefix);\n        if (ret < 0)\n            return ret;\n    }\n    for (i = 0; !s->dl_handle && i < FF_ARRAY_ELEMS(frei0r_pathlist); i++) {\n        ret = load_path(ctx, &s->dl_handle, frei0r_pathlist[i], dl_name);\n        if (ret < 0)\n            return ret;\n    }\n    if (!s->dl_handle) {\n        av_log(ctx, AV_LOG_ERROR, \"Could not find module '%s'.\\n\", dl_name);\n        return AVERROR(EINVAL);\n    }\n\n    if (!(f0r_init                = load_sym(ctx, \"f0r_init\"           )) ||\n        !(f0r_get_plugin_info     = load_sym(ctx, \"f0r_get_plugin_info\")) ||\n        !(s->get_param_info  = load_sym(ctx, \"f0r_get_param_info\" )) ||\n        !(s->get_param_value = load_sym(ctx, \"f0r_get_param_value\")) ||\n        !(s->set_param_value = load_sym(ctx, \"f0r_set_param_value\")) ||\n        !(s->update          = load_sym(ctx, \"f0r_update\"         )) ||\n        !(s->construct       = load_sym(ctx, \"f0r_construct\"      )) ||\n        !(s->destruct        = load_sym(ctx, \"f0r_destruct\"       )) ||\n        !(s->deinit          = load_sym(ctx, \"f0r_deinit\"         )))\n        return AVERROR(EINVAL);\n\n    if (f0r_init() < 0) {\n        av_log(ctx, AV_LOG_ERROR, \"Could not init the frei0r module.\\n\");\n        return AVERROR(EINVAL);\n    }\n\n    f0r_get_plugin_info(&s->plugin_info);\n    pi = &s->plugin_info;\n    if (pi->plugin_type != type) {\n        av_log(ctx, AV_LOG_ERROR,\n               \"Invalid type '%s' for this plugin\\n\",\n               pi->plugin_type == F0R_PLUGIN_TYPE_FILTER ? \"filter\" :\n               pi->plugin_type == F0R_PLUGIN_TYPE_SOURCE ? \"source\" :\n               pi->plugin_type == F0R_PLUGIN_TYPE_MIXER2 ? \"mixer2\" :\n               pi->plugin_type == F0R_PLUGIN_TYPE_MIXER3 ? \"mixer3\" : \"unknown\");\n        return AVERROR(EINVAL);\n    }\n\n    av_log(ctx, AV_LOG_VERBOSE,\n           \"name:%s author:'%s' explanation:'%s' color_model:%s \"\n           \"frei0r_version:%d version:%d.%d num_params:%d\\n\",\n           pi->name, pi->author, pi->explanation,\n           pi->color_model == F0R_COLOR_MODEL_BGRA8888 ? \"bgra8888\" :\n           pi->color_model == F0R_COLOR_MODEL_RGBA8888 ? \"rgba8888\" :\n           pi->color_model == F0R_COLOR_MODEL_PACKED32 ? \"packed32\" : \"unknown\",\n           pi->frei0r_version, pi->major_version, pi->minor_version, pi->num_params);\n\n    return 0;\n}\n\nstatic av_cold int filter_init(AVFilterContext *ctx)\n{\n    Frei0rContext *s = ctx->priv;\n\n    return frei0r_init(ctx, s->dl_name, F0R_PLUGIN_TYPE_FILTER);\n}\n\nstatic av_cold void uninit(AVFilterContext *ctx)\n{\n    Frei0rContext *s = ctx->priv;\n\n    if (s->destruct && s->instance)\n        s->destruct(s->instance);\n    if (s->deinit)\n        s->deinit();\n    if (s->dl_handle)\n        dlclose(s->dl_handle);\n}\n\nstatic int config_input_props(AVFilterLink *inlink)\n{\n    AVFilterContext *ctx = inlink->dst;\n    Frei0rContext *s = ctx->priv;\n\n    if (s->destruct && s->instance)\n        s->destruct(s->instance);\n    if (!(s->instance = s->construct(inlink->w, inlink->h))) {\n        av_log(ctx, AV_LOG_ERROR, \"Impossible to load frei0r instance.\\n\");\n        return AVERROR(EINVAL);\n    }\n\n    return set_params(ctx, s->params);\n}\n\nstatic int query_formats(AVFilterContext *ctx)\n{\n    Frei0rContext *s = ctx->priv;\n    AVFilterFormats *formats = NULL;\n    int ret;\n\n    if        (s->plugin_info.color_model == F0R_COLOR_MODEL_BGRA8888) {\n        if ((ret = ff_add_format(&formats, AV_PIX_FMT_BGRA)) < 0)\n            return ret;\n    } else if (s->plugin_info.color_model == F0R_COLOR_MODEL_RGBA8888) {\n        if ((ret = ff_add_format(&formats, AV_PIX_FMT_RGBA)) < 0)\n            return ret;\n    } else {                                   /* F0R_COLOR_MODEL_PACKED32 */\n        static const enum AVPixelFormat pix_fmts[] = {\n            AV_PIX_FMT_BGRA, AV_PIX_FMT_ARGB, AV_PIX_FMT_ABGR, AV_PIX_FMT_ARGB, AV_PIX_FMT_NONE\n        };\n        formats = ff_make_format_list(pix_fmts);\n    }\n\n    if (!formats)\n        return AVERROR(ENOMEM);\n\n    return ff_set_common_formats(ctx, formats);\n}\n\nstatic int filter_frame(AVFilterLink *inlink, AVFrame *in)\n{\n    Frei0rContext *s = inlink->dst->priv;\n    AVFilterLink *outlink = inlink->dst->outputs[0];\n    AVFrame *out;\n\n    out = ff_get_video_buffer(outlink, outlink->w, outlink->h);\n    if (!out) {\n        av_frame_free(&in);\n        return AVERROR(ENOMEM);\n    }\n    av_frame_copy_props(out, in);\n\n    s->update(s->instance, in->pts * av_q2d(inlink->time_base) * 1000,\n                   (const uint32_t *)in->data[0],\n                   (uint32_t *)out->data[0]);\n\n    av_frame_free(&in);\n\n    return ff_filter_frame(outlink, out);\n}\n\n#define OFFSET(x) offsetof(Frei0rContext, x)\n#define FLAGS AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM\nstatic const AVOption frei0r_options[] = {\n    { \"filter_name\",   NULL, OFFSET(dl_name), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"filter_params\", NULL, OFFSET(params),  AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { NULL }\n};\n\nAVFILTER_DEFINE_CLASS(frei0r);\n\nstatic const AVFilterPad avfilter_vf_frei0r_inputs[] = {\n    {\n        .name         = \"default\",\n        .type         = AVMEDIA_TYPE_VIDEO,\n        .config_props = config_input_props,\n        .filter_frame = filter_frame,\n    },\n    { NULL }\n};\n\nstatic const AVFilterPad avfilter_vf_frei0r_outputs[] = {\n    {\n        .name = \"default\",\n        .type = AVMEDIA_TYPE_VIDEO,\n    },\n    { NULL }\n};\n\nAVFilter ff_vf_frei0r = {\n    .name          = \"frei0r\",\n    .description   = NULL_IF_CONFIG_SMALL(\"Apply a frei0r effect.\"),\n    .query_formats = query_formats,\n    .init          = filter_init,\n    .uninit        = uninit,\n    .priv_size     = sizeof(Frei0rContext),\n    .priv_class    = &frei0r_class,\n    .inputs        = avfilter_vf_frei0r_inputs,\n    .outputs       = avfilter_vf_frei0r_outputs,\n};\n\nstatic av_cold int source_init(AVFilterContext *ctx)\n{\n    Frei0rContext *s = ctx->priv;\n\n    s->time_base.num = s->framerate.den;\n    s->time_base.den = s->framerate.num;\n\n    return frei0r_init(ctx, s->dl_name, F0R_PLUGIN_TYPE_SOURCE);\n}\n\nstatic int source_config_props(AVFilterLink *outlink)\n{\n    AVFilterContext *ctx = outlink->src;\n    Frei0rContext *s = ctx->priv;\n\n    if (av_image_check_size(s->w, s->h, 0, ctx) < 0)\n        return AVERROR(EINVAL);\n    outlink->w = s->w;\n    outlink->h = s->h;\n    outlink->time_base = s->time_base;\n    outlink->frame_rate = av_inv_q(s->time_base);\n    outlink->sample_aspect_ratio = (AVRational){1,1};\n\n    if (s->destruct && s->instance)\n        s->destruct(s->instance);\n    if (!(s->instance = s->construct(outlink->w, outlink->h))) {\n        av_log(ctx, AV_LOG_ERROR, \"Impossible to load frei0r instance.\\n\");\n        return AVERROR(EINVAL);\n    }\n    if (!s->params) {\n        av_log(ctx, AV_LOG_ERROR, \"frei0r filter parameters not set.\\n\");\n        return AVERROR(EINVAL);\n    }\n\n    return set_params(ctx, s->params);\n}\n\nstatic int source_request_frame(AVFilterLink *outlink)\n{\n    Frei0rContext *s = outlink->src->priv;\n    AVFrame *frame = ff_get_video_buffer(outlink, outlink->w, outlink->h);\n\n    if (!frame)\n        return AVERROR(ENOMEM);\n\n    frame->sample_aspect_ratio = (AVRational) {1, 1};\n    frame->pts = s->pts++;\n\n    s->update(s->instance, av_rescale_q(frame->pts, s->time_base, (AVRational){1,1000}),\n                   NULL, (uint32_t *)frame->data[0]);\n\n    return ff_filter_frame(outlink, frame);\n}\n\nstatic const AVOption frei0r_src_options[] = {\n    { \"size\",          \"Dimensions of the generated video.\", OFFSET(w),         AV_OPT_TYPE_IMAGE_SIZE, { .str = \"320x240\" }, .flags = FLAGS },\n    { \"framerate\",     NULL,                                 OFFSET(framerate), AV_OPT_TYPE_VIDEO_RATE, { .str = \"25\" }, 0, INT_MAX, .flags = FLAGS },\n    { \"filter_name\",   NULL,                                 OFFSET(dl_name),   AV_OPT_TYPE_STRING,                  .flags = FLAGS },\n    { \"filter_params\", NULL,                                 OFFSET(params),    AV_OPT_TYPE_STRING,                  .flags = FLAGS },\n    { NULL },\n};\n\nAVFILTER_DEFINE_CLASS(frei0r_src);\n\nstatic const AVFilterPad avfilter_vsrc_frei0r_src_outputs[] = {\n    {\n        .name          = \"default\",\n        .type          = AVMEDIA_TYPE_VIDEO,\n        .request_frame = source_request_frame,\n        .config_props  = source_config_props\n    },\n    { NULL }\n};\n\nAVFilter ff_vsrc_frei0r_src = {\n    .name          = \"frei0r_src\",\n    .description   = NULL_IF_CONFIG_SMALL(\"Generate a frei0r source.\"),\n    .priv_size     = sizeof(Frei0rContext),\n    .priv_class    = &frei0r_src_class,\n    .init          = source_init,\n    .uninit        = uninit,\n    .query_formats = query_formats,\n    .inputs        = NULL,\n    .outputs       = avfilter_vsrc_frei0r_src_outputs,\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavfilter/af_ladspa.c": "/*\n * Copyright (c) 2013 Paul B Mahol\n * Copyright (c) 2011 Mina Nagy Zaki\n *\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n/**\n * @file\n * LADSPA wrapper\n */\n\n#include <dlfcn.h>\n#include <ladspa.h>\n#include \"libavutil/avassert.h\"\n#include \"libavutil/avstring.h\"\n#include \"libavutil/channel_layout.h\"\n#include \"libavutil/opt.h\"\n#include \"audio.h\"\n#include \"avfilter.h\"\n#include \"internal.h\"\n\ntypedef struct LADSPAContext {\n    const AVClass *class;\n    char *dl_name;\n    char *plugin;\n    char *options;\n    void *dl_handle;\n\n    unsigned long nb_inputs;\n    unsigned long *ipmap;      /* map input number to port number */\n\n    unsigned long nb_inputcontrols;\n    unsigned long *icmap;      /* map input control number to port number */\n    LADSPA_Data *ictlv;        /* input controls values */\n\n    unsigned long nb_outputs;\n    unsigned long *opmap;      /* map output number to port number */\n\n    unsigned long nb_outputcontrols;\n    unsigned long *ocmap;      /* map output control number to port number */\n    LADSPA_Data *octlv;        /* output controls values */\n\n    const LADSPA_Descriptor *desc;\n    int *ctl_needs_value;\n    int nb_handles;\n    LADSPA_Handle *handles;\n\n    int sample_rate;\n    int nb_samples;\n    int64_t pts;\n    int64_t duration;\n} LADSPAContext;\n\n#define OFFSET(x) offsetof(LADSPAContext, x)\n#define FLAGS AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_FILTERING_PARAM\nstatic const AVOption ladspa_options[] = {\n    { \"file\", \"set library name or full path\", OFFSET(dl_name), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"f\",    \"set library name or full path\", OFFSET(dl_name), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"plugin\", \"set plugin name\", OFFSET(plugin), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"p\",      \"set plugin name\", OFFSET(plugin), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"controls\", \"set plugin options\", OFFSET(options), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"c\",        \"set plugin options\", OFFSET(options), AV_OPT_TYPE_STRING, .flags = FLAGS },\n    { \"sample_rate\", \"set sample rate\", OFFSET(sample_rate), AV_OPT_TYPE_INT, {.i64=44100}, 1, INT32_MAX, FLAGS },\n    { \"s\",           \"set sample rate\", OFFSET(sample_rate), AV_OPT_TYPE_INT, {.i64=44100}, 1, INT32_MAX, FLAGS },\n    { \"nb_samples\", \"set the number of samples per requested frame\", OFFSET(nb_samples), AV_OPT_TYPE_INT, {.i64=1024}, 1, INT_MAX, FLAGS },\n    { \"n\",          \"set the number of samples per requested frame\", OFFSET(nb_samples), AV_OPT_TYPE_INT, {.i64=1024}, 1, INT_MAX, FLAGS },\n    { \"duration\", \"set audio duration\", OFFSET(duration), AV_OPT_TYPE_DURATION, {.i64=-1}, -1, INT64_MAX, FLAGS },\n    { \"d\",        \"set audio duration\", OFFSET(duration), AV_OPT_TYPE_DURATION, {.i64=-1}, -1, INT64_MAX, FLAGS },\n    { NULL }\n};\n\nAVFILTER_DEFINE_CLASS(ladspa);\n\nstatic void print_ctl_info(AVFilterContext *ctx, int level,\n                           LADSPAContext *s, int ctl, unsigned long *map,\n                           LADSPA_Data *values, int print)\n{\n    const LADSPA_PortRangeHint *h = s->desc->PortRangeHints + map[ctl];\n\n    av_log(ctx, level, \"c%i: %s [\", ctl, s->desc->PortNames[map[ctl]]);\n\n    if (LADSPA_IS_HINT_TOGGLED(h->HintDescriptor)) {\n        av_log(ctx, level, \"toggled (1 or 0)\");\n\n        if (LADSPA_IS_HINT_HAS_DEFAULT(h->HintDescriptor))\n            av_log(ctx, level, \" (default %i)\", (int)values[ctl]);\n    } else {\n        if (LADSPA_IS_HINT_INTEGER(h->HintDescriptor)) {\n            av_log(ctx, level, \"<int>\");\n\n            if (LADSPA_IS_HINT_BOUNDED_BELOW(h->HintDescriptor))\n                av_log(ctx, level, \", min: %i\", (int)h->LowerBound);\n\n            if (LADSPA_IS_HINT_BOUNDED_ABOVE(h->HintDescriptor))\n                av_log(ctx, level, \", max: %i\", (int)h->UpperBound);\n\n            if (print)\n                av_log(ctx, level, \" (value %d)\", (int)values[ctl]);\n            else if (LADSPA_IS_HINT_HAS_DEFAULT(h->HintDescriptor))\n                av_log(ctx, level, \" (default %d)\", (int)values[ctl]);\n        } else {\n            av_log(ctx, level, \"<float>\");\n\n            if (LADSPA_IS_HINT_BOUNDED_BELOW(h->HintDescriptor))\n                av_log(ctx, level, \", min: %f\", h->LowerBound);\n\n            if (LADSPA_IS_HINT_BOUNDED_ABOVE(h->HintDescriptor))\n                av_log(ctx, level, \", max: %f\", h->UpperBound);\n\n            if (print)\n                av_log(ctx, level, \" (value %f)\", values[ctl]);\n            else if (LADSPA_IS_HINT_HAS_DEFAULT(h->HintDescriptor))\n                av_log(ctx, level, \" (default %f)\", values[ctl]);\n        }\n\n        if (LADSPA_IS_HINT_SAMPLE_RATE(h->HintDescriptor))\n            av_log(ctx, level, \", multiple of sample rate\");\n\n        if (LADSPA_IS_HINT_LOGARITHMIC(h->HintDescriptor))\n            av_log(ctx, level, \", logarithmic scale\");\n    }\n\n    av_log(ctx, level, \"]\\n\");\n}\n\nstatic int filter_frame(AVFilterLink *inlink, AVFrame *in)\n{\n    AVFilterContext *ctx = inlink->dst;\n    LADSPAContext *s = ctx->priv;\n    AVFrame *out;\n    int i, h, p;\n\n    av_assert0(in->channels == (s->nb_inputs * s->nb_handles));\n\n    if (!s->nb_outputs ||\n        (av_frame_is_writable(in) && s->nb_inputs == s->nb_outputs &&\n        !(s->desc->Properties & LADSPA_PROPERTY_INPLACE_BROKEN))) {\n        out = in;\n    } else {\n        out = ff_get_audio_buffer(ctx->outputs[0], in->nb_samples);\n        if (!out) {\n            av_frame_free(&in);\n            return AVERROR(ENOMEM);\n        }\n        av_frame_copy_props(out, in);\n    }\n\n    av_assert0(!s->nb_outputs || out->channels == (s->nb_outputs * s->nb_handles));\n\n    for (h = 0; h < s->nb_handles; h++) {\n        for (i = 0; i < s->nb_inputs; i++) {\n            p = s->nb_handles > 1 ? h : i;\n            s->desc->connect_port(s->handles[h], s->ipmap[i],\n                                  (LADSPA_Data*)in->extended_data[p]);\n        }\n\n        for (i = 0; i < s->nb_outputs; i++) {\n            p = s->nb_handles > 1 ? h : i;\n            s->desc->connect_port(s->handles[h], s->opmap[i],\n                                  (LADSPA_Data*)out->extended_data[p]);\n        }\n\n        s->desc->run(s->handles[h], in->nb_samples);\n    }\n\n    for (i = 0; i < s->nb_outputcontrols; i++)\n        print_ctl_info(ctx, AV_LOG_VERBOSE, s, i, s->ocmap, s->octlv, 1);\n\n    if (out != in)\n        av_frame_free(&in);\n\n    return ff_filter_frame(ctx->outputs[0], out);\n}\n\nstatic int request_frame(AVFilterLink *outlink)\n{\n    AVFilterContext *ctx = outlink->src;\n    LADSPAContext *s = ctx->priv;\n    AVFrame *out;\n    int64_t t;\n    int i;\n\n    if (ctx->nb_inputs)\n        return ff_request_frame(ctx->inputs[0]);\n\n    t = av_rescale(s->pts, AV_TIME_BASE, s->sample_rate);\n    if (s->duration >= 0 && t >= s->duration)\n        return AVERROR_EOF;\n\n    out = ff_get_audio_buffer(outlink, s->nb_samples);\n    if (!out)\n        return AVERROR(ENOMEM);\n\n    for (i = 0; i < s->nb_outputs; i++)\n        s->desc->connect_port(s->handles[0], s->opmap[i],\n                (LADSPA_Data*)out->extended_data[i]);\n\n    s->desc->run(s->handles[0], s->nb_samples);\n\n    for (i = 0; i < s->nb_outputcontrols; i++)\n        print_ctl_info(ctx, AV_LOG_INFO, s, i, s->ocmap, s->octlv, 1);\n\n    out->sample_rate = s->sample_rate;\n    out->pts         = s->pts;\n    s->pts          += s->nb_samples;\n\n    return ff_filter_frame(outlink, out);\n}\n\nstatic void set_default_ctl_value(LADSPAContext *s, int ctl,\n                                  unsigned long *map, LADSPA_Data *values)\n{\n    const LADSPA_PortRangeHint *h = s->desc->PortRangeHints + map[ctl];\n    const LADSPA_Data lower = h->LowerBound;\n    const LADSPA_Data upper = h->UpperBound;\n\n    if (LADSPA_IS_HINT_DEFAULT_MINIMUM(h->HintDescriptor)) {\n        values[ctl] = lower;\n    } else if (LADSPA_IS_HINT_DEFAULT_MAXIMUM(h->HintDescriptor)) {\n        values[ctl] = upper;\n    } else if (LADSPA_IS_HINT_DEFAULT_0(h->HintDescriptor)) {\n        values[ctl] = 0.0;\n    } else if (LADSPA_IS_HINT_DEFAULT_1(h->HintDescriptor)) {\n        values[ctl] = 1.0;\n    } else if (LADSPA_IS_HINT_DEFAULT_100(h->HintDescriptor)) {\n        values[ctl] = 100.0;\n    } else if (LADSPA_IS_HINT_DEFAULT_440(h->HintDescriptor)) {\n        values[ctl] = 440.0;\n    } else if (LADSPA_IS_HINT_DEFAULT_LOW(h->HintDescriptor)) {\n        if (LADSPA_IS_HINT_LOGARITHMIC(h->HintDescriptor))\n            values[ctl] = exp(log(lower) * 0.75 + log(upper) * 0.25);\n        else\n            values[ctl] = lower * 0.75 + upper * 0.25;\n    } else if (LADSPA_IS_HINT_DEFAULT_MIDDLE(h->HintDescriptor)) {\n        if (LADSPA_IS_HINT_LOGARITHMIC(h->HintDescriptor))\n            values[ctl] = exp(log(lower) * 0.5 + log(upper) * 0.5);\n        else\n            values[ctl] = lower * 0.5 + upper * 0.5;\n    } else if (LADSPA_IS_HINT_DEFAULT_HIGH(h->HintDescriptor)) {\n        if (LADSPA_IS_HINT_LOGARITHMIC(h->HintDescriptor))\n            values[ctl] = exp(log(lower) * 0.25 + log(upper) * 0.75);\n        else\n            values[ctl] = lower * 0.25 + upper * 0.75;\n    }\n}\n\nstatic int connect_ports(AVFilterContext *ctx, AVFilterLink *link)\n{\n    LADSPAContext *s = ctx->priv;\n    int i, j;\n\n    s->nb_handles = s->nb_inputs == 1 && s->nb_outputs == 1 ? link->channels : 1;\n    s->handles    = av_calloc(s->nb_handles, sizeof(*s->handles));\n    if (!s->handles)\n        return AVERROR(ENOMEM);\n\n    for (i = 0; i < s->nb_handles; i++) {\n        s->handles[i] = s->desc->instantiate(s->desc, link->sample_rate);\n        if (!s->handles[i]) {\n            av_log(ctx, AV_LOG_ERROR, \"Could not instantiate plugin.\\n\");\n            return AVERROR_EXTERNAL;\n        }\n\n        // Connect the input control ports\n        for (j = 0; j < s->nb_inputcontrols; j++)\n            s->desc->connect_port(s->handles[i], s->icmap[j], s->ictlv + j);\n\n        // Connect the output control ports\n        for (j = 0; j < s->nb_outputcontrols; j++)\n            s->desc->connect_port(s->handles[i], s->ocmap[j], &s->octlv[j]);\n\n        if (s->desc->activate)\n            s->desc->activate(s->handles[i]);\n    }\n\n    av_log(ctx, AV_LOG_DEBUG, \"handles: %d\\n\", s->nb_handles);\n\n    return 0;\n}\n\nstatic int config_input(AVFilterLink *inlink)\n{\n    AVFilterContext *ctx = inlink->dst;\n\n    return connect_ports(ctx, inlink);\n}\n\nstatic int config_output(AVFilterLink *outlink)\n{\n    AVFilterContext *ctx = outlink->src;\n    LADSPAContext *s = ctx->priv;\n    int ret;\n\n    if (ctx->nb_inputs) {\n        AVFilterLink *inlink = ctx->inputs[0];\n\n        outlink->format      = inlink->format;\n        outlink->sample_rate = inlink->sample_rate;\n        if (s->nb_inputs == s->nb_outputs) {\n            outlink->channel_layout = inlink->channel_layout;\n            outlink->channels = inlink->channels;\n        }\n\n        ret = 0;\n    } else {\n        outlink->sample_rate = s->sample_rate;\n        outlink->time_base   = (AVRational){1, s->sample_rate};\n\n        ret = connect_ports(ctx, outlink);\n    }\n\n    return ret;\n}\n\nstatic void count_ports(const LADSPA_Descriptor *desc,\n                        unsigned long *nb_inputs, unsigned long *nb_outputs)\n{\n    LADSPA_PortDescriptor pd;\n    int i;\n\n    for (i = 0; i < desc->PortCount; i++) {\n        pd = desc->PortDescriptors[i];\n\n        if (LADSPA_IS_PORT_AUDIO(pd)) {\n            if (LADSPA_IS_PORT_INPUT(pd)) {\n                (*nb_inputs)++;\n            } else if (LADSPA_IS_PORT_OUTPUT(pd)) {\n                (*nb_outputs)++;\n            }\n        }\n    }\n}\n\nstatic void *try_load(const char *dir, const char *soname)\n{\n    char *path = av_asprintf(\"%s/%s.so\", dir, soname);\n    void *ret = NULL;\n\n    if (path) {\n        ret = dlopen(path, RTLD_LOCAL|RTLD_NOW);\n        av_free(path);\n    }\n\n    return ret;\n}\n\nstatic int set_control(AVFilterContext *ctx, unsigned long port, LADSPA_Data value)\n{\n    LADSPAContext *s = ctx->priv;\n    const char *label = s->desc->Label;\n    LADSPA_PortRangeHint *h = (LADSPA_PortRangeHint *)s->desc->PortRangeHints +\n                              s->icmap[port];\n\n    if (port >= s->nb_inputcontrols) {\n        av_log(ctx, AV_LOG_ERROR, \"Control c%ld is out of range [0 - %lu].\\n\",\n               port, s->nb_inputcontrols);\n        return AVERROR(EINVAL);\n    }\n\n    if (LADSPA_IS_HINT_BOUNDED_BELOW(h->HintDescriptor) &&\n            value < h->LowerBound) {\n        av_log(ctx, AV_LOG_ERROR,\n                \"%s: input control c%ld is below lower boundary of %0.4f.\\n\",\n                label, port, h->LowerBound);\n        return AVERROR(EINVAL);\n    }\n\n    if (LADSPA_IS_HINT_BOUNDED_ABOVE(h->HintDescriptor) &&\n            value > h->UpperBound) {\n        av_log(ctx, AV_LOG_ERROR,\n                \"%s: input control c%ld is above upper boundary of %0.4f.\\n\",\n                label, port, h->UpperBound);\n        return AVERROR(EINVAL);\n    }\n\n    s->ictlv[port] = value;\n\n    return 0;\n}\n\nstatic av_cold int init(AVFilterContext *ctx)\n{\n    LADSPAContext *s = ctx->priv;\n    LADSPA_Descriptor_Function descriptor_fn;\n    const LADSPA_Descriptor *desc;\n    LADSPA_PortDescriptor pd;\n    AVFilterPad pad = { NULL };\n    char *p, *arg, *saveptr = NULL;\n    unsigned long nb_ports;\n    int i, j = 0;\n\n    if (!s->dl_name) {\n        av_log(ctx, AV_LOG_ERROR, \"No plugin name provided\\n\");\n        return AVERROR(EINVAL);\n    }\n\n    if (s->dl_name[0] == '/' || s->dl_name[0] == '.') {\n        // argument is a path\n        s->dl_handle = dlopen(s->dl_name, RTLD_LOCAL|RTLD_NOW);\n    } else {\n        // argument is a shared object name\n        char *paths = av_strdup(getenv(\"LADSPA_PATH\"));\n        const char *separator = \":\";\n\n        if (paths) {\n            p = paths;\n            while ((arg = av_strtok(p, separator, &saveptr)) && !s->dl_handle) {\n                s->dl_handle = try_load(arg, s->dl_name);\n                p = NULL;\n            }\n        }\n\n        av_free(paths);\n        if (!s->dl_handle && (paths = av_asprintf(\"%s/.ladspa/lib\", getenv(\"HOME\")))) {\n            s->dl_handle = try_load(paths, s->dl_name);\n            av_free(paths);\n        }\n\n        if (!s->dl_handle)\n            s->dl_handle = try_load(\"/usr/local/lib/ladspa\", s->dl_name);\n\n        if (!s->dl_handle)\n            s->dl_handle = try_load(\"/usr/lib/ladspa\", s->dl_name);\n    }\n    if (!s->dl_handle) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to load '%s'\\n\", s->dl_name);\n        return AVERROR(EINVAL);\n    }\n\n    descriptor_fn = dlsym(s->dl_handle, \"ladspa_descriptor\");\n    if (!descriptor_fn) {\n        av_log(ctx, AV_LOG_ERROR, \"Could not find ladspa_descriptor: %s\\n\", dlerror());\n        return AVERROR(EINVAL);\n    }\n\n    // Find the requested plugin, or list plugins\n    if (!s->plugin) {\n        av_log(ctx, AV_LOG_INFO, \"The '%s' library contains the following plugins:\\n\", s->dl_name);\n        av_log(ctx, AV_LOG_INFO, \"I = Input Channels\\n\");\n        av_log(ctx, AV_LOG_INFO, \"O = Output Channels\\n\");\n        av_log(ctx, AV_LOG_INFO, \"I:O %-25s %s\\n\", \"Plugin\", \"Description\");\n        av_log(ctx, AV_LOG_INFO, \"\\n\");\n        for (i = 0; desc = descriptor_fn(i); i++) {\n            unsigned long inputs = 0, outputs = 0;\n\n            count_ports(desc, &inputs, &outputs);\n            av_log(ctx, AV_LOG_INFO, \"%lu:%lu %-25s %s\\n\", inputs, outputs, desc->Label,\n                   (char *)av_x_if_null(desc->Name, \"?\"));\n            av_log(ctx, AV_LOG_VERBOSE, \"Maker: %s\\n\",\n                   (char *)av_x_if_null(desc->Maker, \"?\"));\n            av_log(ctx, AV_LOG_VERBOSE, \"Copyright: %s\\n\",\n                   (char *)av_x_if_null(desc->Copyright, \"?\"));\n        }\n        return AVERROR_EXIT;\n    } else {\n        for (i = 0;; i++) {\n            desc = descriptor_fn(i);\n            if (!desc) {\n                av_log(ctx, AV_LOG_ERROR, \"Could not find plugin: %s\\n\", s->plugin);\n                return AVERROR(EINVAL);\n            }\n\n            if (desc->Label && !strcmp(desc->Label, s->plugin))\n                break;\n        }\n    }\n\n    s->desc  = desc;\n    nb_ports = desc->PortCount;\n\n    s->ipmap = av_calloc(nb_ports, sizeof(*s->ipmap));\n    s->opmap = av_calloc(nb_ports, sizeof(*s->opmap));\n    s->icmap = av_calloc(nb_ports, sizeof(*s->icmap));\n    s->ocmap = av_calloc(nb_ports, sizeof(*s->ocmap));\n    s->ictlv = av_calloc(nb_ports, sizeof(*s->ictlv));\n    s->octlv = av_calloc(nb_ports, sizeof(*s->octlv));\n    s->ctl_needs_value = av_calloc(nb_ports, sizeof(*s->ctl_needs_value));\n    if (!s->ipmap || !s->opmap || !s->icmap ||\n        !s->ocmap || !s->ictlv || !s->octlv || !s->ctl_needs_value)\n        return AVERROR(ENOMEM);\n\n    for (i = 0; i < nb_ports; i++) {\n        pd = desc->PortDescriptors[i];\n\n        if (LADSPA_IS_PORT_AUDIO(pd)) {\n            if (LADSPA_IS_PORT_INPUT(pd)) {\n                s->ipmap[s->nb_inputs] = i;\n                s->nb_inputs++;\n            } else if (LADSPA_IS_PORT_OUTPUT(pd)) {\n                s->opmap[s->nb_outputs] = i;\n                s->nb_outputs++;\n            }\n        } else if (LADSPA_IS_PORT_CONTROL(pd)) {\n            if (LADSPA_IS_PORT_INPUT(pd)) {\n                s->icmap[s->nb_inputcontrols] = i;\n\n                if (LADSPA_IS_HINT_HAS_DEFAULT(desc->PortRangeHints[i].HintDescriptor))\n                    set_default_ctl_value(s, s->nb_inputcontrols, s->icmap, s->ictlv);\n                else\n                    s->ctl_needs_value[s->nb_inputcontrols] = 1;\n\n                s->nb_inputcontrols++;\n            } else if (LADSPA_IS_PORT_OUTPUT(pd)) {\n                s->ocmap[s->nb_outputcontrols] = i;\n                s->nb_outputcontrols++;\n            }\n        }\n    }\n\n    // List Control Ports if \"help\" is specified\n    if (s->options && !strcmp(s->options, \"help\")) {\n        if (!s->nb_inputcontrols) {\n            av_log(ctx, AV_LOG_INFO,\n                   \"The '%s' plugin does not have any input controls.\\n\",\n                   desc->Label);\n        } else {\n            av_log(ctx, AV_LOG_INFO,\n                   \"The '%s' plugin has the following input controls:\\n\",\n                   desc->Label);\n            for (i = 0; i < s->nb_inputcontrols; i++)\n                print_ctl_info(ctx, AV_LOG_INFO, s, i, s->icmap, s->ictlv, 0);\n        }\n        return AVERROR_EXIT;\n    }\n\n    // Parse control parameters\n    p = s->options;\n    while (s->options) {\n        LADSPA_Data val;\n        int ret;\n\n        if (!(arg = av_strtok(p, \" |\", &saveptr)))\n            break;\n        p = NULL;\n\n        if (sscanf(arg, \"c%d=%f\", &i, &val) != 2) {\n            if (sscanf(arg, \"%f\", &val) != 1) {\n                av_log(ctx, AV_LOG_ERROR, \"Invalid syntax.\\n\");\n                return AVERROR(EINVAL);\n            }\n            i = j++;\n        }\n\n        if ((ret = set_control(ctx, i, val)) < 0)\n            return ret;\n        s->ctl_needs_value[i] = 0;\n    }\n\n    // Check if any controls are not set\n    for (i = 0; i < s->nb_inputcontrols; i++) {\n        if (s->ctl_needs_value[i]) {\n            av_log(ctx, AV_LOG_ERROR, \"Control c%d must be set.\\n\", i);\n            print_ctl_info(ctx, AV_LOG_ERROR, s, i, s->icmap, s->ictlv, 0);\n            return AVERROR(EINVAL);\n        }\n    }\n\n    pad.type = AVMEDIA_TYPE_AUDIO;\n\n    if (s->nb_inputs) {\n        pad.name = av_asprintf(\"in0:%s%lu\", desc->Label, s->nb_inputs);\n        if (!pad.name)\n            return AVERROR(ENOMEM);\n\n        pad.filter_frame = filter_frame;\n        pad.config_props = config_input;\n        if (ff_insert_inpad(ctx, ctx->nb_inputs, &pad) < 0) {\n            av_freep(&pad.name);\n            return AVERROR(ENOMEM);\n        }\n    }\n\n    av_log(ctx, AV_LOG_DEBUG, \"ports: %lu\\n\", nb_ports);\n    av_log(ctx, AV_LOG_DEBUG, \"inputs: %lu outputs: %lu\\n\",\n                              s->nb_inputs, s->nb_outputs);\n    av_log(ctx, AV_LOG_DEBUG, \"input controls: %lu output controls: %lu\\n\",\n                              s->nb_inputcontrols, s->nb_outputcontrols);\n\n    return 0;\n}\n\nstatic int query_formats(AVFilterContext *ctx)\n{\n    LADSPAContext *s = ctx->priv;\n    AVFilterFormats *formats;\n    AVFilterChannelLayouts *layouts;\n    static const enum AVSampleFormat sample_fmts[] = {\n        AV_SAMPLE_FMT_FLTP, AV_SAMPLE_FMT_NONE };\n    int ret;\n\n    formats = ff_make_format_list(sample_fmts);\n    if (!formats)\n        return AVERROR(ENOMEM);\n    ret = ff_set_common_formats(ctx, formats);\n    if (ret < 0)\n        return ret;\n\n    if (s->nb_inputs) {\n        formats = ff_all_samplerates();\n        if (!formats)\n            return AVERROR(ENOMEM);\n\n        ret = ff_set_common_samplerates(ctx, formats);\n        if (ret < 0)\n            return ret;\n    } else {\n        int sample_rates[] = { s->sample_rate, -1 };\n\n        ret = ff_set_common_samplerates(ctx, ff_make_format_list(sample_rates));\n        if (ret < 0)\n            return ret;\n    }\n\n    if (s->nb_inputs == 1 && s->nb_outputs == 1) {\n        // We will instantiate multiple LADSPA_Handle, one over each channel\n        layouts = ff_all_channel_counts();\n        if (!layouts)\n            return AVERROR(ENOMEM);\n\n        ret = ff_set_common_channel_layouts(ctx, layouts);\n        if (ret < 0)\n            return ret;\n    } else if (s->nb_inputs == 2 && s->nb_outputs == 2) {\n        layouts = NULL;\n        ret = ff_add_channel_layout(&layouts, AV_CH_LAYOUT_STEREO);\n        if (ret < 0)\n            return ret;\n        ret = ff_set_common_channel_layouts(ctx, layouts);\n        if (ret < 0)\n            return ret;\n    } else {\n        AVFilterLink *outlink = ctx->outputs[0];\n\n        if (s->nb_inputs >= 1) {\n            AVFilterLink *inlink = ctx->inputs[0];\n            uint64_t inlayout = FF_COUNT2LAYOUT(s->nb_inputs);\n\n            layouts = NULL;\n            ret = ff_add_channel_layout(&layouts, inlayout);\n            if (ret < 0)\n                return ret;\n            ret = ff_channel_layouts_ref(layouts, &inlink->out_channel_layouts);\n            if (ret < 0)\n                return ret;\n\n            if (!s->nb_outputs) {\n                ret = ff_channel_layouts_ref(layouts, &outlink->in_channel_layouts);\n                if (ret < 0)\n                    return ret;\n            }\n        }\n\n        if (s->nb_outputs >= 1) {\n            uint64_t outlayout = FF_COUNT2LAYOUT(s->nb_outputs);\n\n            layouts = NULL;\n            ret = ff_add_channel_layout(&layouts, outlayout);\n            if (ret < 0)\n                return ret;\n            ret = ff_channel_layouts_ref(layouts, &outlink->in_channel_layouts);\n            if (ret < 0)\n                return ret;\n        }\n    }\n\n    return 0;\n}\n\nstatic av_cold void uninit(AVFilterContext *ctx)\n{\n    LADSPAContext *s = ctx->priv;\n    int i;\n\n    for (i = 0; i < s->nb_handles; i++) {\n        if (s->desc->deactivate)\n            s->desc->deactivate(s->handles[i]);\n        if (s->desc->cleanup)\n            s->desc->cleanup(s->handles[i]);\n    }\n\n    if (s->dl_handle)\n        dlclose(s->dl_handle);\n\n    av_freep(&s->ipmap);\n    av_freep(&s->opmap);\n    av_freep(&s->icmap);\n    av_freep(&s->ocmap);\n    av_freep(&s->ictlv);\n    av_freep(&s->octlv);\n    av_freep(&s->handles);\n    av_freep(&s->ctl_needs_value);\n\n    if (ctx->nb_inputs)\n        av_freep(&ctx->input_pads[0].name);\n}\n\nstatic int process_command(AVFilterContext *ctx, const char *cmd, const char *args,\n                           char *res, int res_len, int flags)\n{\n    LADSPA_Data value;\n    unsigned long port;\n\n    if (sscanf(cmd, \"c%ld\", &port) + sscanf(args, \"%f\", &value) != 2)\n        return AVERROR(EINVAL);\n\n    return set_control(ctx, port, value);\n}\n\nstatic const AVFilterPad ladspa_outputs[] = {\n    {\n        .name          = \"default\",\n        .type          = AVMEDIA_TYPE_AUDIO,\n        .config_props  = config_output,\n        .request_frame = request_frame,\n    },\n    { NULL }\n};\n\nAVFilter ff_af_ladspa = {\n    .name          = \"ladspa\",\n    .description   = NULL_IF_CONFIG_SMALL(\"Apply LADSPA effect.\"),\n    .priv_size     = sizeof(LADSPAContext),\n    .priv_class    = &ladspa_class,\n    .init          = init,\n    .uninit        = uninit,\n    .query_formats = query_formats,\n    .process_command = process_command,\n    .inputs        = 0,\n    .outputs       = ladspa_outputs,\n    .flags         = AVFILTER_FLAG_DYNAMIC_INPUTS,\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavcodec/omx.c": "/*\n * OMX Video encoder\n * Copyright (C) 2011 Martin Storsjo\n *\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#include \"config.h\"\n\n#if CONFIG_OMX_RPI\n#define OMX_SKIP64BIT\n#endif\n\n#include <dlfcn.h>\n#include <OMX_Core.h>\n#include <OMX_Component.h>\n#include <pthread.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/time.h>\n\n#include \"libavutil/avstring.h\"\n#include \"libavutil/avutil.h\"\n#include \"libavutil/common.h\"\n#include \"libavutil/imgutils.h\"\n#include \"libavutil/log.h\"\n#include \"libavutil/opt.h\"\n\n#include \"avcodec.h\"\n#include \"h264.h\"\n#include \"internal.h\"\n\n#ifdef OMX_SKIP64BIT\nstatic OMX_TICKS to_omx_ticks(int64_t value)\n{\n    OMX_TICKS s;\n    s.nLowPart  = value & 0xffffffff;\n    s.nHighPart = value >> 32;\n    return s;\n}\nstatic int64_t from_omx_ticks(OMX_TICKS value)\n{\n    return (((int64_t)value.nHighPart) << 32) | value.nLowPart;\n}\n#else\n#define to_omx_ticks(x) (x)\n#define from_omx_ticks(x) (x)\n#endif\n\n#define INIT_STRUCT(x) do {                                               \\\n        x.nSize = sizeof(x);                                              \\\n        x.nVersion = s->version;                                          \\\n    } while (0)\n#define CHECK(x) do {                                                     \\\n        if (x != OMX_ErrorNone) {                                         \\\n            av_log(avctx, AV_LOG_ERROR,                                   \\\n                   \"err %x (%d) on line %d\\n\", x, x, __LINE__);           \\\n            return AVERROR_UNKNOWN;                                       \\\n        }                                                                 \\\n    } while (0)\n\ntypedef struct OMXContext {\n    void *lib;\n    void *lib2;\n    OMX_ERRORTYPE (*ptr_Init)(void);\n    OMX_ERRORTYPE (*ptr_Deinit)(void);\n    OMX_ERRORTYPE (*ptr_ComponentNameEnum)(OMX_STRING, OMX_U32, OMX_U32);\n    OMX_ERRORTYPE (*ptr_GetHandle)(OMX_HANDLETYPE*, OMX_STRING, OMX_PTR, OMX_CALLBACKTYPE*);\n    OMX_ERRORTYPE (*ptr_FreeHandle)(OMX_HANDLETYPE);\n    OMX_ERRORTYPE (*ptr_GetComponentsOfRole)(OMX_STRING, OMX_U32*, OMX_U8**);\n    OMX_ERRORTYPE (*ptr_GetRolesOfComponent)(OMX_STRING, OMX_U32*, OMX_U8**);\n    void (*host_init)(void);\n} OMXContext;\n\nstatic av_cold void *dlsym_prefixed(void *handle, const char *symbol, const char *prefix)\n{\n    char buf[50];\n    snprintf(buf, sizeof(buf), \"%s%s\", prefix ? prefix : \"\", symbol);\n    return dlsym(handle, buf);\n}\n\nstatic av_cold int omx_try_load(OMXContext *s, void *logctx,\n                                const char *libname, const char *prefix,\n                                const char *libname2)\n{\n    if (libname2) {\n        s->lib2 = dlopen(libname2, RTLD_NOW | RTLD_GLOBAL);\n        if (!s->lib2) {\n            av_log(logctx, AV_LOG_WARNING, \"%s not found\\n\", libname);\n            return AVERROR_ENCODER_NOT_FOUND;\n        }\n        s->host_init = dlsym(s->lib2, \"bcm_host_init\");\n        if (!s->host_init) {\n            av_log(logctx, AV_LOG_WARNING, \"bcm_host_init not found\\n\");\n            dlclose(s->lib2);\n            s->lib2 = NULL;\n            return AVERROR_ENCODER_NOT_FOUND;\n        }\n    }\n    s->lib = dlopen(libname, RTLD_NOW | RTLD_GLOBAL);\n    if (!s->lib) {\n        av_log(logctx, AV_LOG_WARNING, \"%s not found\\n\", libname);\n        return AVERROR_ENCODER_NOT_FOUND;\n    }\n    s->ptr_Init                = dlsym_prefixed(s->lib, \"OMX_Init\", prefix);\n    s->ptr_Deinit              = dlsym_prefixed(s->lib, \"OMX_Deinit\", prefix);\n    s->ptr_ComponentNameEnum   = dlsym_prefixed(s->lib, \"OMX_ComponentNameEnum\", prefix);\n    s->ptr_GetHandle           = dlsym_prefixed(s->lib, \"OMX_GetHandle\", prefix);\n    s->ptr_FreeHandle          = dlsym_prefixed(s->lib, \"OMX_FreeHandle\", prefix);\n    s->ptr_GetComponentsOfRole = dlsym_prefixed(s->lib, \"OMX_GetComponentsOfRole\", prefix);\n    s->ptr_GetRolesOfComponent = dlsym_prefixed(s->lib, \"OMX_GetRolesOfComponent\", prefix);\n    if (!s->ptr_Init || !s->ptr_Deinit || !s->ptr_ComponentNameEnum ||\n        !s->ptr_GetHandle || !s->ptr_FreeHandle ||\n        !s->ptr_GetComponentsOfRole || !s->ptr_GetRolesOfComponent) {\n        av_log(logctx, AV_LOG_WARNING, \"Not all functions found in %s\\n\", libname);\n        dlclose(s->lib);\n        s->lib = NULL;\n        if (s->lib2)\n            dlclose(s->lib2);\n        s->lib2 = NULL;\n        return AVERROR_ENCODER_NOT_FOUND;\n    }\n    return 0;\n}\n\nstatic av_cold OMXContext *omx_init(void *logctx, const char *libname, const char *prefix)\n{\n    static const char * const libnames[] = {\n#if CONFIG_OMX_RPI\n        \"/opt/vc/lib/libopenmaxil.so\", \"/opt/vc/lib/libbcm_host.so\",\n#else\n        \"libOMX_Core.so\", NULL,\n        \"libOmxCore.so\", NULL,\n#endif\n        NULL\n    };\n    const char* const* nameptr;\n    int ret = AVERROR_ENCODER_NOT_FOUND;\n    OMXContext *omx_context;\n\n    omx_context = av_mallocz(sizeof(*omx_context));\n    if (!omx_context)\n        return NULL;\n    if (libname) {\n        ret = omx_try_load(omx_context, logctx, libname, prefix, NULL);\n        if (ret < 0) {\n            av_free(omx_context);\n            return NULL;\n        }\n    } else {\n        for (nameptr = libnames; *nameptr; nameptr += 2)\n            if (!(ret = omx_try_load(omx_context, logctx, nameptr[0], prefix, nameptr[1])))\n                break;\n        if (!*nameptr) {\n            av_free(omx_context);\n            return NULL;\n        }\n    }\n\n    if (omx_context->host_init)\n        omx_context->host_init();\n    omx_context->ptr_Init();\n    return omx_context;\n}\n\nstatic av_cold void omx_deinit(OMXContext *omx_context)\n{\n    if (!omx_context)\n        return;\n    omx_context->ptr_Deinit();\n    dlclose(omx_context->lib);\n    av_free(omx_context);\n}\n\ntypedef struct OMXCodecContext {\n    const AVClass *class;\n    char *libname;\n    char *libprefix;\n    OMXContext *omx_context;\n\n    AVCodecContext *avctx;\n\n    char component_name[OMX_MAX_STRINGNAME_SIZE];\n    OMX_VERSIONTYPE version;\n    OMX_HANDLETYPE handle;\n    int in_port, out_port;\n    OMX_COLOR_FORMATTYPE color_format;\n    int stride, plane_size;\n\n    int num_in_buffers, num_out_buffers;\n    OMX_BUFFERHEADERTYPE **in_buffer_headers;\n    OMX_BUFFERHEADERTYPE **out_buffer_headers;\n    int num_free_in_buffers;\n    OMX_BUFFERHEADERTYPE **free_in_buffers;\n    int num_done_out_buffers;\n    OMX_BUFFERHEADERTYPE **done_out_buffers;\n    pthread_mutex_t input_mutex;\n    pthread_cond_t input_cond;\n    pthread_mutex_t output_mutex;\n    pthread_cond_t output_cond;\n\n    pthread_mutex_t state_mutex;\n    pthread_cond_t state_cond;\n    OMX_STATETYPE state;\n    OMX_ERRORTYPE error;\n\n    int mutex_cond_inited;\n\n    int eos_sent, got_eos;\n\n    uint8_t *output_buf;\n    int output_buf_size;\n\n    int input_zerocopy;\n    int profile;\n} OMXCodecContext;\n\nstatic void append_buffer(pthread_mutex_t *mutex, pthread_cond_t *cond,\n                          int* array_size, OMX_BUFFERHEADERTYPE **array,\n                          OMX_BUFFERHEADERTYPE *buffer)\n{\n    pthread_mutex_lock(mutex);\n    array[(*array_size)++] = buffer;\n    pthread_cond_broadcast(cond);\n    pthread_mutex_unlock(mutex);\n}\n\nstatic OMX_BUFFERHEADERTYPE *get_buffer(pthread_mutex_t *mutex, pthread_cond_t *cond,\n                                        int* array_size, OMX_BUFFERHEADERTYPE **array,\n                                        int wait)\n{\n    OMX_BUFFERHEADERTYPE *buffer;\n    pthread_mutex_lock(mutex);\n    if (wait) {\n        while (!*array_size)\n           pthread_cond_wait(cond, mutex);\n    }\n    if (*array_size > 0) {\n        buffer = array[0];\n        (*array_size)--;\n        memmove(&array[0], &array[1], (*array_size) * sizeof(OMX_BUFFERHEADERTYPE*));\n    } else {\n        buffer = NULL;\n    }\n    pthread_mutex_unlock(mutex);\n    return buffer;\n}\n\nstatic OMX_ERRORTYPE event_handler(OMX_HANDLETYPE component, OMX_PTR app_data, OMX_EVENTTYPE event,\n                                   OMX_U32 data1, OMX_U32 data2, OMX_PTR event_data)\n{\n    OMXCodecContext *s = app_data;\n    // This uses casts in the printfs, since OMX_U32 actually is a typedef for\n    // unsigned long in official header versions (but there are also modified\n    // versions where it is something else).\n    switch (event) {\n    case OMX_EventError:\n        pthread_mutex_lock(&s->state_mutex);\n        av_log(s->avctx, AV_LOG_ERROR, \"OMX error %\"PRIx32\"\\n\", (uint32_t) data1);\n        s->error = data1;\n        pthread_cond_broadcast(&s->state_cond);\n        pthread_mutex_unlock(&s->state_mutex);\n        break;\n    case OMX_EventCmdComplete:\n        if (data1 == OMX_CommandStateSet) {\n            pthread_mutex_lock(&s->state_mutex);\n            s->state = data2;\n            av_log(s->avctx, AV_LOG_VERBOSE, \"OMX state changed to %\"PRIu32\"\\n\", (uint32_t) data2);\n            pthread_cond_broadcast(&s->state_cond);\n            pthread_mutex_unlock(&s->state_mutex);\n        } else if (data1 == OMX_CommandPortDisable) {\n            av_log(s->avctx, AV_LOG_VERBOSE, \"OMX port %\"PRIu32\" disabled\\n\", (uint32_t) data2);\n        } else if (data1 == OMX_CommandPortEnable) {\n            av_log(s->avctx, AV_LOG_VERBOSE, \"OMX port %\"PRIu32\" enabled\\n\", (uint32_t) data2);\n        } else {\n            av_log(s->avctx, AV_LOG_VERBOSE, \"OMX command complete, command %\"PRIu32\", value %\"PRIu32\"\\n\",\n                                             (uint32_t) data1, (uint32_t) data2);\n        }\n        break;\n    case OMX_EventPortSettingsChanged:\n        av_log(s->avctx, AV_LOG_VERBOSE, \"OMX port %\"PRIu32\" settings changed\\n\", (uint32_t) data1);\n        break;\n    default:\n        av_log(s->avctx, AV_LOG_VERBOSE, \"OMX event %d %\"PRIx32\" %\"PRIx32\"\\n\",\n                                         event, (uint32_t) data1, (uint32_t) data2);\n        break;\n    }\n    return OMX_ErrorNone;\n}\n\nstatic OMX_ERRORTYPE empty_buffer_done(OMX_HANDLETYPE component, OMX_PTR app_data,\n                                       OMX_BUFFERHEADERTYPE *buffer)\n{\n    OMXCodecContext *s = app_data;\n    if (s->input_zerocopy) {\n        if (buffer->pAppPrivate) {\n            if (buffer->pOutputPortPrivate)\n                av_free(buffer->pAppPrivate);\n            else\n                av_frame_free((AVFrame**)&buffer->pAppPrivate);\n            buffer->pAppPrivate = NULL;\n        }\n    }\n    append_buffer(&s->input_mutex, &s->input_cond,\n                  &s->num_free_in_buffers, s->free_in_buffers, buffer);\n    return OMX_ErrorNone;\n}\n\nstatic OMX_ERRORTYPE fill_buffer_done(OMX_HANDLETYPE component, OMX_PTR app_data,\n                                      OMX_BUFFERHEADERTYPE *buffer)\n{\n    OMXCodecContext *s = app_data;\n    append_buffer(&s->output_mutex, &s->output_cond,\n                  &s->num_done_out_buffers, s->done_out_buffers, buffer);\n    return OMX_ErrorNone;\n}\n\nstatic const OMX_CALLBACKTYPE callbacks = {\n    event_handler,\n    empty_buffer_done,\n    fill_buffer_done\n};\n\nstatic av_cold int find_component(OMXContext *omx_context, void *logctx,\n                                  const char *role, char *str, int str_size)\n{\n    OMX_U32 i, num = 0;\n    char **components;\n    int ret = 0;\n\n#if CONFIG_OMX_RPI\n    if (av_strstart(role, \"video_encoder.\", NULL)) {\n        av_strlcpy(str, \"OMX.broadcom.video_encode\", str_size);\n        return 0;\n    }\n#endif\n    omx_context->ptr_GetComponentsOfRole((OMX_STRING) role, &num, NULL);\n    if (!num) {\n        av_log(logctx, AV_LOG_WARNING, \"No component for role %s found\\n\", role);\n        return AVERROR_ENCODER_NOT_FOUND;\n    }\n    components = av_mallocz_array(num, sizeof(*components));\n    if (!components)\n        return AVERROR(ENOMEM);\n    for (i = 0; i < num; i++) {\n        components[i] = av_mallocz(OMX_MAX_STRINGNAME_SIZE);\n        if (!components[i]) {\n            ret = AVERROR(ENOMEM);\n            goto end;\n        }\n    }\n    omx_context->ptr_GetComponentsOfRole((OMX_STRING) role, &num, (OMX_U8**) components);\n    av_strlcpy(str, components[0], str_size);\nend:\n    for (i = 0; i < num; i++)\n        av_free(components[i]);\n    av_free(components);\n    return ret;\n}\n\nstatic av_cold int wait_for_state(OMXCodecContext *s, OMX_STATETYPE state)\n{\n    int ret = 0;\n    pthread_mutex_lock(&s->state_mutex);\n    while (s->state != state && s->error == OMX_ErrorNone)\n        pthread_cond_wait(&s->state_cond, &s->state_mutex);\n    if (s->error != OMX_ErrorNone)\n        ret = AVERROR_ENCODER_NOT_FOUND;\n    pthread_mutex_unlock(&s->state_mutex);\n    return ret;\n}\n\nstatic av_cold int omx_component_init(AVCodecContext *avctx, const char *role)\n{\n    OMXCodecContext *s = avctx->priv_data;\n    OMX_PARAM_COMPONENTROLETYPE role_params = { 0 };\n    OMX_PORT_PARAM_TYPE video_port_params = { 0 };\n    OMX_PARAM_PORTDEFINITIONTYPE in_port_params = { 0 }, out_port_params = { 0 };\n    OMX_VIDEO_PARAM_PORTFORMATTYPE video_port_format = { 0 };\n    OMX_VIDEO_PARAM_BITRATETYPE vid_param_bitrate = { 0 };\n    OMX_ERRORTYPE err;\n    int i;\n\n    s->version.s.nVersionMajor = 1;\n    s->version.s.nVersionMinor = 1;\n    s->version.s.nRevision     = 2;\n\n    err = s->omx_context->ptr_GetHandle(&s->handle, s->component_name, s, (OMX_CALLBACKTYPE*) &callbacks);\n    if (err != OMX_ErrorNone) {\n        av_log(avctx, AV_LOG_ERROR, \"OMX_GetHandle(%s) failed: %x\\n\", s->component_name, err);\n        return AVERROR_UNKNOWN;\n    }\n\n    // This one crashes the mediaserver on qcom, if used over IOMX\n    INIT_STRUCT(role_params);\n    av_strlcpy(role_params.cRole, role, sizeof(role_params.cRole));\n    // Intentionally ignore errors on this one\n    OMX_SetParameter(s->handle, OMX_IndexParamStandardComponentRole, &role_params);\n\n    INIT_STRUCT(video_port_params);\n    err = OMX_GetParameter(s->handle, OMX_IndexParamVideoInit, &video_port_params);\n    CHECK(err);\n\n    s->in_port = s->out_port = -1;\n    for (i = 0; i < video_port_params.nPorts; i++) {\n        int port = video_port_params.nStartPortNumber + i;\n        OMX_PARAM_PORTDEFINITIONTYPE port_params = { 0 };\n        INIT_STRUCT(port_params);\n        port_params.nPortIndex = port;\n        err = OMX_GetParameter(s->handle, OMX_IndexParamPortDefinition, &port_params);\n        if (err != OMX_ErrorNone) {\n            av_log(avctx, AV_LOG_WARNING, \"port %d error %x\\n\", port, err);\n            break;\n        }\n        if (port_params.eDir == OMX_DirInput && s->in_port < 0) {\n            in_port_params = port_params;\n            s->in_port = port;\n        } else if (port_params.eDir == OMX_DirOutput && s->out_port < 0) {\n            out_port_params = port_params;\n            s->out_port = port;\n        }\n    }\n    if (s->in_port < 0 || s->out_port < 0) {\n        av_log(avctx, AV_LOG_ERROR, \"No in or out port found (in %d out %d)\\n\", s->in_port, s->out_port);\n        return AVERROR_UNKNOWN;\n    }\n\n    s->color_format = 0;\n    for (i = 0; ; i++) {\n        INIT_STRUCT(video_port_format);\n        video_port_format.nIndex = i;\n        video_port_format.nPortIndex = s->in_port;\n        if (OMX_GetParameter(s->handle, OMX_IndexParamVideoPortFormat, &video_port_format) != OMX_ErrorNone)\n            break;\n        if (video_port_format.eColorFormat == OMX_COLOR_FormatYUV420Planar ||\n            video_port_format.eColorFormat == OMX_COLOR_FormatYUV420PackedPlanar) {\n            s->color_format = video_port_format.eColorFormat;\n            break;\n        }\n    }\n    if (s->color_format == 0) {\n        av_log(avctx, AV_LOG_ERROR, \"No supported pixel formats (%d formats available)\\n\", i);\n        return AVERROR_UNKNOWN;\n    }\n\n    in_port_params.bEnabled   = OMX_TRUE;\n    in_port_params.bPopulated = OMX_FALSE;\n    in_port_params.eDomain    = OMX_PortDomainVideo;\n\n    in_port_params.format.video.pNativeRender         = NULL;\n    in_port_params.format.video.bFlagErrorConcealment = OMX_FALSE;\n    in_port_params.format.video.eColorFormat          = s->color_format;\n    s->stride     = avctx->width;\n    s->plane_size = avctx->height;\n    // If specific codecs need to manually override the stride/plane_size,\n    // that can be done here.\n    in_port_params.format.video.nStride      = s->stride;\n    in_port_params.format.video.nSliceHeight = s->plane_size;\n    in_port_params.format.video.nFrameWidth  = avctx->width;\n    in_port_params.format.video.nFrameHeight = avctx->height;\n    if (avctx->framerate.den > 0 && avctx->framerate.num > 0)\n        in_port_params.format.video.xFramerate = (1 << 16) * avctx->framerate.num / avctx->framerate.den;\n    else\n        in_port_params.format.video.xFramerate = (1 << 16) * avctx->time_base.den / avctx->time_base.num;\n\n    err = OMX_SetParameter(s->handle, OMX_IndexParamPortDefinition, &in_port_params);\n    CHECK(err);\n    err = OMX_GetParameter(s->handle, OMX_IndexParamPortDefinition, &in_port_params);\n    CHECK(err);\n    s->stride         = in_port_params.format.video.nStride;\n    s->plane_size     = in_port_params.format.video.nSliceHeight;\n    s->num_in_buffers = in_port_params.nBufferCountActual;\n\n    err = OMX_GetParameter(s->handle, OMX_IndexParamPortDefinition, &out_port_params);\n    out_port_params.bEnabled   = OMX_TRUE;\n    out_port_params.bPopulated = OMX_FALSE;\n    out_port_params.eDomain    = OMX_PortDomainVideo;\n    out_port_params.format.video.pNativeRender = NULL;\n    out_port_params.format.video.nFrameWidth   = avctx->width;\n    out_port_params.format.video.nFrameHeight  = avctx->height;\n    out_port_params.format.video.nStride       = 0;\n    out_port_params.format.video.nSliceHeight  = 0;\n    out_port_params.format.video.nBitrate      = avctx->bit_rate;\n    out_port_params.format.video.xFramerate    = in_port_params.format.video.xFramerate;\n    out_port_params.format.video.bFlagErrorConcealment  = OMX_FALSE;\n    if (avctx->codec->id == AV_CODEC_ID_MPEG4)\n        out_port_params.format.video.eCompressionFormat = OMX_VIDEO_CodingMPEG4;\n    else if (avctx->codec->id == AV_CODEC_ID_H264)\n        out_port_params.format.video.eCompressionFormat = OMX_VIDEO_CodingAVC;\n\n    err = OMX_SetParameter(s->handle, OMX_IndexParamPortDefinition, &out_port_params);\n    CHECK(err);\n    err = OMX_GetParameter(s->handle, OMX_IndexParamPortDefinition, &out_port_params);\n    CHECK(err);\n    s->num_out_buffers = out_port_params.nBufferCountActual;\n\n    INIT_STRUCT(vid_param_bitrate);\n    vid_param_bitrate.nPortIndex     = s->out_port;\n    vid_param_bitrate.eControlRate   = OMX_Video_ControlRateVariable;\n    vid_param_bitrate.nTargetBitrate = avctx->bit_rate;\n    err = OMX_SetParameter(s->handle, OMX_IndexParamVideoBitrate, &vid_param_bitrate);\n    if (err != OMX_ErrorNone)\n        av_log(avctx, AV_LOG_WARNING, \"Unable to set video bitrate parameter\\n\");\n\n    if (avctx->codec->id == AV_CODEC_ID_H264) {\n        OMX_VIDEO_PARAM_AVCTYPE avc = { 0 };\n        INIT_STRUCT(avc);\n        avc.nPortIndex = s->out_port;\n        err = OMX_GetParameter(s->handle, OMX_IndexParamVideoAvc, &avc);\n        CHECK(err);\n        avc.nBFrames = 0;\n        avc.nPFrames = avctx->gop_size - 1;\n        switch (s->profile == FF_PROFILE_UNKNOWN ? avctx->profile : s->profile) {\n        case FF_PROFILE_H264_BASELINE:\n            avc.eProfile = OMX_VIDEO_AVCProfileBaseline;\n            break;\n        case FF_PROFILE_H264_MAIN:\n            avc.eProfile = OMX_VIDEO_AVCProfileMain;\n            break;\n        case FF_PROFILE_H264_HIGH:\n            avc.eProfile = OMX_VIDEO_AVCProfileHigh;\n            break;\n        default:\n            break;\n        }\n        err = OMX_SetParameter(s->handle, OMX_IndexParamVideoAvc, &avc);\n        CHECK(err);\n    }\n\n    err = OMX_SendCommand(s->handle, OMX_CommandStateSet, OMX_StateIdle, NULL);\n    CHECK(err);\n\n    s->in_buffer_headers  = av_mallocz(sizeof(OMX_BUFFERHEADERTYPE*) * s->num_in_buffers);\n    s->free_in_buffers    = av_mallocz(sizeof(OMX_BUFFERHEADERTYPE*) * s->num_in_buffers);\n    s->out_buffer_headers = av_mallocz(sizeof(OMX_BUFFERHEADERTYPE*) * s->num_out_buffers);\n    s->done_out_buffers   = av_mallocz(sizeof(OMX_BUFFERHEADERTYPE*) * s->num_out_buffers);\n    if (!s->in_buffer_headers || !s->free_in_buffers || !s->out_buffer_headers || !s->done_out_buffers)\n        return AVERROR(ENOMEM);\n    for (i = 0; i < s->num_in_buffers && err == OMX_ErrorNone; i++) {\n        if (s->input_zerocopy)\n            err = OMX_UseBuffer(s->handle, &s->in_buffer_headers[i], s->in_port, s, in_port_params.nBufferSize, NULL);\n        else\n            err = OMX_AllocateBuffer(s->handle, &s->in_buffer_headers[i],  s->in_port,  s, in_port_params.nBufferSize);\n        if (err == OMX_ErrorNone)\n            s->in_buffer_headers[i]->pAppPrivate = s->in_buffer_headers[i]->pOutputPortPrivate = NULL;\n    }\n    CHECK(err);\n    s->num_in_buffers = i;\n    for (i = 0; i < s->num_out_buffers && err == OMX_ErrorNone; i++)\n        err = OMX_AllocateBuffer(s->handle, &s->out_buffer_headers[i], s->out_port, s, out_port_params.nBufferSize);\n    CHECK(err);\n    s->num_out_buffers = i;\n\n    if (wait_for_state(s, OMX_StateIdle) < 0) {\n        av_log(avctx, AV_LOG_ERROR, \"Didn't get OMX_StateIdle\\n\");\n        return AVERROR_UNKNOWN;\n    }\n    err = OMX_SendCommand(s->handle, OMX_CommandStateSet, OMX_StateExecuting, NULL);\n    CHECK(err);\n    if (wait_for_state(s, OMX_StateExecuting) < 0) {\n        av_log(avctx, AV_LOG_ERROR, \"Didn't get OMX_StateExecuting\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    for (i = 0; i < s->num_out_buffers && err == OMX_ErrorNone; i++)\n        err = OMX_FillThisBuffer(s->handle, s->out_buffer_headers[i]);\n    if (err != OMX_ErrorNone) {\n        for (; i < s->num_out_buffers; i++)\n            s->done_out_buffers[s->num_done_out_buffers++] = s->out_buffer_headers[i];\n    }\n    for (i = 0; i < s->num_in_buffers; i++)\n        s->free_in_buffers[s->num_free_in_buffers++] = s->in_buffer_headers[i];\n    return err != OMX_ErrorNone ? AVERROR_UNKNOWN : 0;\n}\n\nstatic av_cold void cleanup(OMXCodecContext *s)\n{\n    int i, executing;\n\n    pthread_mutex_lock(&s->state_mutex);\n    executing = s->state == OMX_StateExecuting;\n    pthread_mutex_unlock(&s->state_mutex);\n\n    if (executing) {\n        OMX_SendCommand(s->handle, OMX_CommandStateSet, OMX_StateIdle, NULL);\n        wait_for_state(s, OMX_StateIdle);\n        OMX_SendCommand(s->handle, OMX_CommandStateSet, OMX_StateLoaded, NULL);\n        for (i = 0; i < s->num_in_buffers; i++) {\n            OMX_BUFFERHEADERTYPE *buffer = get_buffer(&s->input_mutex, &s->input_cond,\n                                                      &s->num_free_in_buffers, s->free_in_buffers, 1);\n            if (s->input_zerocopy)\n                buffer->pBuffer = NULL;\n            OMX_FreeBuffer(s->handle, s->in_port, buffer);\n        }\n        for (i = 0; i < s->num_out_buffers; i++) {\n            OMX_BUFFERHEADERTYPE *buffer = get_buffer(&s->output_mutex, &s->output_cond,\n                                                      &s->num_done_out_buffers, s->done_out_buffers, 1);\n            OMX_FreeBuffer(s->handle, s->out_port, buffer);\n        }\n        wait_for_state(s, OMX_StateLoaded);\n    }\n    if (s->handle) {\n        s->omx_context->ptr_FreeHandle(s->handle);\n        s->handle = NULL;\n    }\n\n    omx_deinit(s->omx_context);\n    s->omx_context = NULL;\n    if (s->mutex_cond_inited) {\n        pthread_cond_destroy(&s->state_cond);\n        pthread_mutex_destroy(&s->state_mutex);\n        pthread_cond_destroy(&s->input_cond);\n        pthread_mutex_destroy(&s->input_mutex);\n        pthread_cond_destroy(&s->output_cond);\n        pthread_mutex_destroy(&s->output_mutex);\n        s->mutex_cond_inited = 0;\n    }\n    av_freep(&s->in_buffer_headers);\n    av_freep(&s->out_buffer_headers);\n    av_freep(&s->free_in_buffers);\n    av_freep(&s->done_out_buffers);\n    av_freep(&s->output_buf);\n}\n\nstatic av_cold int omx_encode_init(AVCodecContext *avctx)\n{\n    OMXCodecContext *s = avctx->priv_data;\n    int ret = AVERROR_ENCODER_NOT_FOUND;\n    const char *role;\n    OMX_BUFFERHEADERTYPE *buffer;\n    OMX_ERRORTYPE err;\n\n#if CONFIG_OMX_RPI\n    s->input_zerocopy = 1;\n#endif\n\n    s->omx_context = omx_init(avctx, s->libname, s->libprefix);\n    if (!s->omx_context)\n        return AVERROR_ENCODER_NOT_FOUND;\n\n    pthread_mutex_init(&s->state_mutex, NULL);\n    pthread_cond_init(&s->state_cond, NULL);\n    pthread_mutex_init(&s->input_mutex, NULL);\n    pthread_cond_init(&s->input_cond, NULL);\n    pthread_mutex_init(&s->output_mutex, NULL);\n    pthread_cond_init(&s->output_cond, NULL);\n    s->mutex_cond_inited = 1;\n    s->avctx = avctx;\n    s->state = OMX_StateLoaded;\n    s->error = OMX_ErrorNone;\n\n    switch (avctx->codec->id) {\n    case AV_CODEC_ID_MPEG4:\n        role = \"video_encoder.mpeg4\";\n        break;\n    case AV_CODEC_ID_H264:\n        role = \"video_encoder.avc\";\n        break;\n    default:\n        return AVERROR(ENOSYS);\n    }\n\n    if ((ret = find_component(s->omx_context, avctx, role, s->component_name, sizeof(s->component_name))) < 0)\n        goto fail;\n\n    av_log(avctx, AV_LOG_INFO, \"Using %s\\n\", s->component_name);\n\n    if ((ret = omx_component_init(avctx, role)) < 0)\n        goto fail;\n\n    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {\n        while (1) {\n            buffer = get_buffer(&s->output_mutex, &s->output_cond,\n                                &s->num_done_out_buffers, s->done_out_buffers, 1);\n            if (buffer->nFlags & OMX_BUFFERFLAG_CODECCONFIG) {\n                if ((ret = av_reallocp(&avctx->extradata, avctx->extradata_size + buffer->nFilledLen + AV_INPUT_BUFFER_PADDING_SIZE)) < 0) {\n                    avctx->extradata_size = 0;\n                    goto fail;\n                }\n                memcpy(avctx->extradata + avctx->extradata_size, buffer->pBuffer + buffer->nOffset, buffer->nFilledLen);\n                avctx->extradata_size += buffer->nFilledLen;\n                memset(avctx->extradata + avctx->extradata_size, 0, AV_INPUT_BUFFER_PADDING_SIZE);\n            }\n            err = OMX_FillThisBuffer(s->handle, buffer);\n            if (err != OMX_ErrorNone) {\n                append_buffer(&s->output_mutex, &s->output_cond,\n                              &s->num_done_out_buffers, s->done_out_buffers, buffer);\n                av_log(avctx, AV_LOG_ERROR, \"OMX_FillThisBuffer failed: %x\\n\", err);\n                ret = AVERROR_UNKNOWN;\n                goto fail;\n            }\n            if (avctx->codec->id == AV_CODEC_ID_H264) {\n                // For H.264, the extradata can be returned in two separate buffers\n                // (the videocore encoder on raspberry pi does this);\n                // therefore check that we have got both SPS and PPS before continuing.\n                int nals[32] = { 0 };\n                int i;\n                for (i = 0; i + 4 < avctx->extradata_size; i++) {\n                     if (!avctx->extradata[i + 0] &&\n                         !avctx->extradata[i + 1] &&\n                         !avctx->extradata[i + 2] &&\n                         avctx->extradata[i + 3] == 1) {\n                         nals[avctx->extradata[i + 4] & 0x1f]++;\n                     }\n                }\n                if (nals[H264_NAL_SPS] && nals[H264_NAL_PPS])\n                    break;\n            } else {\n                if (avctx->extradata_size > 0)\n                    break;\n            }\n        }\n    }\n\n    return 0;\nfail:\n    return ret;\n}\n\n\nstatic int omx_encode_frame(AVCodecContext *avctx, AVPacket *pkt,\n                            const AVFrame *frame, int *got_packet)\n{\n    OMXCodecContext *s = avctx->priv_data;\n    int ret = 0;\n    OMX_BUFFERHEADERTYPE* buffer;\n    OMX_ERRORTYPE err;\n\n    if (frame) {\n        uint8_t *dst[4];\n        int linesize[4];\n        int need_copy;\n        buffer = get_buffer(&s->input_mutex, &s->input_cond,\n                            &s->num_free_in_buffers, s->free_in_buffers, 1);\n\n        buffer->nFilledLen = av_image_fill_arrays(dst, linesize, buffer->pBuffer, avctx->pix_fmt, s->stride, s->plane_size, 1);\n\n        if (s->input_zerocopy) {\n            uint8_t *src[4] = { NULL };\n            int src_linesize[4];\n            av_image_fill_arrays(src, src_linesize, frame->data[0], avctx->pix_fmt, s->stride, s->plane_size, 1);\n            if (frame->linesize[0] == src_linesize[0] &&\n                frame->linesize[1] == src_linesize[1] &&\n                frame->linesize[2] == src_linesize[2] &&\n                frame->data[1] == src[1] &&\n                frame->data[2] == src[2]) {\n                // If the input frame happens to have all planes stored contiguously,\n                // with the right strides, just clone the frame and set the OMX\n                // buffer header to point to it\n                AVFrame *local = av_frame_clone(frame);\n                if (!local) {\n                    // Return the buffer to the queue so it's not lost\n                    append_buffer(&s->input_mutex, &s->input_cond, &s->num_free_in_buffers, s->free_in_buffers, buffer);\n                    return AVERROR(ENOMEM);\n                } else {\n                    buffer->pAppPrivate = local;\n                    buffer->pOutputPortPrivate = NULL;\n                    buffer->pBuffer = local->data[0];\n                    need_copy = 0;\n                }\n            } else {\n                // If not, we need to allocate a new buffer with the right\n                // size and copy the input frame into it.\n                uint8_t *buf = NULL;\n                int image_buffer_size = av_image_get_buffer_size(avctx->pix_fmt, s->stride, s->plane_size, 1);\n                if (image_buffer_size >= 0)\n                    buf = av_malloc(image_buffer_size);\n                if (!buf) {\n                    // Return the buffer to the queue so it's not lost\n                    append_buffer(&s->input_mutex, &s->input_cond, &s->num_free_in_buffers, s->free_in_buffers, buffer);\n                    return AVERROR(ENOMEM);\n                } else {\n                    buffer->pAppPrivate = buf;\n                    // Mark that pAppPrivate is an av_malloc'ed buffer, not an AVFrame\n                    buffer->pOutputPortPrivate = (void*) 1;\n                    buffer->pBuffer = buf;\n                    need_copy = 1;\n                    buffer->nFilledLen = av_image_fill_arrays(dst, linesize, buffer->pBuffer, avctx->pix_fmt, s->stride, s->plane_size, 1);\n                }\n            }\n        } else {\n            need_copy = 1;\n        }\n        if (need_copy)\n            av_image_copy(dst, linesize, (const uint8_t**) frame->data, frame->linesize, avctx->pix_fmt, avctx->width, avctx->height);\n        buffer->nFlags = OMX_BUFFERFLAG_ENDOFFRAME;\n        buffer->nOffset = 0;\n        // Convert the timestamps to microseconds; some encoders can ignore\n        // the framerate and do VFR bit allocation based on timestamps.\n        buffer->nTimeStamp = to_omx_ticks(av_rescale_q(frame->pts, avctx->time_base, AV_TIME_BASE_Q));\n        err = OMX_EmptyThisBuffer(s->handle, buffer);\n        if (err != OMX_ErrorNone) {\n            append_buffer(&s->input_mutex, &s->input_cond, &s->num_free_in_buffers, s->free_in_buffers, buffer);\n            av_log(avctx, AV_LOG_ERROR, \"OMX_EmptyThisBuffer failed: %x\\n\", err);\n            return AVERROR_UNKNOWN;\n        }\n    } else if (!s->eos_sent) {\n        buffer = get_buffer(&s->input_mutex, &s->input_cond,\n                            &s->num_free_in_buffers, s->free_in_buffers, 1);\n\n        buffer->nFilledLen = 0;\n        buffer->nFlags = OMX_BUFFERFLAG_EOS;\n        buffer->pAppPrivate = buffer->pOutputPortPrivate = NULL;\n        err = OMX_EmptyThisBuffer(s->handle, buffer);\n        if (err != OMX_ErrorNone) {\n            append_buffer(&s->input_mutex, &s->input_cond, &s->num_free_in_buffers, s->free_in_buffers, buffer);\n            av_log(avctx, AV_LOG_ERROR, \"OMX_EmptyThisBuffer failed: %x\\n\", err);\n            return AVERROR_UNKNOWN;\n        }\n        s->eos_sent = 1;\n    }\n\n    while (!*got_packet && ret == 0 && !s->got_eos) {\n        // If not flushing, just poll the queue if there's finished packets.\n        // If flushing, do a blocking wait until we either get a completed\n        // packet, or get EOS.\n        buffer = get_buffer(&s->output_mutex, &s->output_cond,\n                            &s->num_done_out_buffers, s->done_out_buffers,\n                            !frame);\n        if (!buffer)\n            break;\n\n        if (buffer->nFlags & OMX_BUFFERFLAG_EOS)\n            s->got_eos = 1;\n\n        if (buffer->nFlags & OMX_BUFFERFLAG_CODECCONFIG && avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {\n            if ((ret = av_reallocp(&avctx->extradata, avctx->extradata_size + buffer->nFilledLen + AV_INPUT_BUFFER_PADDING_SIZE)) < 0) {\n                avctx->extradata_size = 0;\n                goto end;\n            }\n            memcpy(avctx->extradata + avctx->extradata_size, buffer->pBuffer + buffer->nOffset, buffer->nFilledLen);\n            avctx->extradata_size += buffer->nFilledLen;\n            memset(avctx->extradata + avctx->extradata_size, 0, AV_INPUT_BUFFER_PADDING_SIZE);\n        } else {\n            if (!(buffer->nFlags & OMX_BUFFERFLAG_ENDOFFRAME) || !pkt->data) {\n                // If the output packet isn't preallocated, just concatenate everything in our\n                // own buffer\n                int newsize = s->output_buf_size + buffer->nFilledLen + AV_INPUT_BUFFER_PADDING_SIZE;\n                if ((ret = av_reallocp(&s->output_buf, newsize)) < 0) {\n                    s->output_buf_size = 0;\n                    goto end;\n                }\n                memcpy(s->output_buf + s->output_buf_size, buffer->pBuffer + buffer->nOffset, buffer->nFilledLen);\n                s->output_buf_size += buffer->nFilledLen;\n                if (buffer->nFlags & OMX_BUFFERFLAG_ENDOFFRAME) {\n                    if ((ret = av_packet_from_data(pkt, s->output_buf, s->output_buf_size)) < 0) {\n                        av_freep(&s->output_buf);\n                        s->output_buf_size = 0;\n                        goto end;\n                    }\n                    s->output_buf = NULL;\n                    s->output_buf_size = 0;\n                }\n            } else {\n                // End of frame, and the caller provided a preallocated frame\n                if ((ret = ff_alloc_packet2(avctx, pkt, s->output_buf_size + buffer->nFilledLen, 0)) < 0) {\n                    av_log(avctx, AV_LOG_ERROR, \"Error getting output packet of size %d.\\n\",\n                           (int)(s->output_buf_size + buffer->nFilledLen));\n                    goto end;\n                }\n                memcpy(pkt->data, s->output_buf, s->output_buf_size);\n                memcpy(pkt->data + s->output_buf_size, buffer->pBuffer + buffer->nOffset, buffer->nFilledLen);\n                av_freep(&s->output_buf);\n                s->output_buf_size = 0;\n            }\n            if (buffer->nFlags & OMX_BUFFERFLAG_ENDOFFRAME) {\n                pkt->pts = av_rescale_q(from_omx_ticks(buffer->nTimeStamp), AV_TIME_BASE_Q, avctx->time_base);\n                // We don't currently enable B-frames for the encoders, so set\n                // pkt->dts = pkt->pts. (The calling code behaves worse if the encoder\n                // doesn't set the dts).\n                pkt->dts = pkt->pts;\n                if (buffer->nFlags & OMX_BUFFERFLAG_SYNCFRAME)\n                    pkt->flags |= AV_PKT_FLAG_KEY;\n                *got_packet = 1;\n            }\n        }\nend:\n        err = OMX_FillThisBuffer(s->handle, buffer);\n        if (err != OMX_ErrorNone) {\n            append_buffer(&s->output_mutex, &s->output_cond, &s->num_done_out_buffers, s->done_out_buffers, buffer);\n            av_log(avctx, AV_LOG_ERROR, \"OMX_FillThisBuffer failed: %x\\n\", err);\n            ret = AVERROR_UNKNOWN;\n        }\n    }\n    return ret;\n}\n\nstatic av_cold int omx_encode_end(AVCodecContext *avctx)\n{\n    OMXCodecContext *s = avctx->priv_data;\n\n    cleanup(s);\n    return 0;\n}\n\n#define OFFSET(x) offsetof(OMXCodecContext, x)\n#define VDE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_DECODING_PARAM | AV_OPT_FLAG_ENCODING_PARAM\n#define VE  AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM\nstatic const AVOption options[] = {\n    { \"omx_libname\", \"OpenMAX library name\", OFFSET(libname), AV_OPT_TYPE_STRING, { 0 }, 0, 0, VDE },\n    { \"omx_libprefix\", \"OpenMAX library prefix\", OFFSET(libprefix), AV_OPT_TYPE_STRING, { 0 }, 0, 0, VDE },\n    { \"zerocopy\", \"Try to avoid copying input frames if possible\", OFFSET(input_zerocopy), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 1, VE },\n    { \"profile\",  \"Set the encoding profile\", OFFSET(profile), AV_OPT_TYPE_INT,   { .i64 = FF_PROFILE_UNKNOWN },       FF_PROFILE_UNKNOWN, FF_PROFILE_H264_HIGH, VE, \"profile\" },\n    { \"baseline\", \"\",                         0,               AV_OPT_TYPE_CONST, { .i64 = FF_PROFILE_H264_BASELINE }, 0, 0, VE, \"profile\" },\n    { \"main\",     \"\",                         0,               AV_OPT_TYPE_CONST, { .i64 = FF_PROFILE_H264_MAIN },     0, 0, VE, \"profile\" },\n    { \"high\",     \"\",                         0,               AV_OPT_TYPE_CONST, { .i64 = FF_PROFILE_H264_HIGH },     0, 0, VE, \"profile\" },\n    { NULL }\n};\n\nstatic const enum AVPixelFormat omx_encoder_pix_fmts[] = {\n    AV_PIX_FMT_YUV420P, AV_PIX_FMT_NONE\n};\n\nstatic const AVClass omx_mpeg4enc_class = {\n    .class_name = \"mpeg4_omx\",\n    .item_name  = av_default_item_name,\n    .option     = options,\n    .version    = LIBAVUTIL_VERSION_INT,\n};\nAVCodec ff_mpeg4_omx_encoder = {\n    .name             = \"mpeg4_omx\",\n    .long_name        = NULL_IF_CONFIG_SMALL(\"OpenMAX IL MPEG-4 video encoder\"),\n    .type             = AVMEDIA_TYPE_VIDEO,\n    .id               = AV_CODEC_ID_MPEG4,\n    .priv_data_size   = sizeof(OMXCodecContext),\n    .init             = omx_encode_init,\n    .encode2          = omx_encode_frame,\n    .close            = omx_encode_end,\n    .pix_fmts         = omx_encoder_pix_fmts,\n    .capabilities     = AV_CODEC_CAP_DELAY,\n    .caps_internal    = FF_CODEC_CAP_INIT_THREADSAFE | FF_CODEC_CAP_INIT_CLEANUP,\n    .priv_class       = &omx_mpeg4enc_class,\n};\n\nstatic const AVClass omx_h264enc_class = {\n    .class_name = \"h264_omx\",\n    .item_name  = av_default_item_name,\n    .option     = options,\n    .version    = LIBAVUTIL_VERSION_INT,\n};\nAVCodec ff_h264_omx_encoder = {\n    .name             = \"h264_omx\",\n    .long_name        = NULL_IF_CONFIG_SMALL(\"OpenMAX IL H.264 video encoder\"),\n    .type             = AVMEDIA_TYPE_VIDEO,\n    .id               = AV_CODEC_ID_H264,\n    .priv_data_size   = sizeof(OMXCodecContext),\n    .init             = omx_encode_init,\n    .encode2          = omx_encode_frame,\n    .close            = omx_encode_end,\n    .pix_fmts         = omx_encoder_pix_fmts,\n    .capabilities     = AV_CODEC_CAP_DELAY,\n    .caps_internal    = FF_CODEC_CAP_INIT_THREADSAFE | FF_CODEC_CAP_INIT_CLEANUP,\n    .priv_class       = &omx_h264enc_class,\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavcodec/videotoolboxenc.c": "/*\n * copyright (c) 2015 Rick Kern <kernrj@gmail.com>\n *\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#include <VideoToolbox/VideoToolbox.h>\n#include <CoreVideo/CoreVideo.h>\n#include <CoreMedia/CoreMedia.h>\n#include <TargetConditionals.h>\n#include <Availability.h>\n#include \"avcodec.h\"\n#include \"libavutil/opt.h\"\n#include \"libavutil/avassert.h\"\n#include \"libavutil/avstring.h\"\n#include \"libavcodec/avcodec.h\"\n#include \"libavutil/pixdesc.h\"\n#include \"internal.h\"\n#include <pthread.h>\n#include \"h264.h\"\n#include \"h264_sei.h\"\n#include <dlfcn.h>\n\n#if !HAVE_KCMVIDEOCODECTYPE_HEVC\nenum { kCMVideoCodecType_HEVC = 'hvc1' };\n#endif\n\ntypedef OSStatus (*getParameterSetAtIndex)(CMFormatDescriptionRef videoDesc,\n                                           size_t parameterSetIndex,\n                                           const uint8_t **parameterSetPointerOut,\n                                           size_t *parameterSetSizeOut,\n                                           size_t *parameterSetCountOut,\n                                           int *NALUnitHeaderLengthOut);\n\n//These symbols may not be present\nstatic struct{\n    CFStringRef kCVImageBufferColorPrimaries_ITU_R_2020;\n    CFStringRef kCVImageBufferTransferFunction_ITU_R_2020;\n    CFStringRef kCVImageBufferYCbCrMatrix_ITU_R_2020;\n\n    CFStringRef kVTCompressionPropertyKey_H264EntropyMode;\n    CFStringRef kVTH264EntropyMode_CAVLC;\n    CFStringRef kVTH264EntropyMode_CABAC;\n\n    CFStringRef kVTProfileLevel_H264_Baseline_4_0;\n    CFStringRef kVTProfileLevel_H264_Baseline_4_2;\n    CFStringRef kVTProfileLevel_H264_Baseline_5_0;\n    CFStringRef kVTProfileLevel_H264_Baseline_5_1;\n    CFStringRef kVTProfileLevel_H264_Baseline_5_2;\n    CFStringRef kVTProfileLevel_H264_Baseline_AutoLevel;\n    CFStringRef kVTProfileLevel_H264_Main_4_2;\n    CFStringRef kVTProfileLevel_H264_Main_5_1;\n    CFStringRef kVTProfileLevel_H264_Main_5_2;\n    CFStringRef kVTProfileLevel_H264_Main_AutoLevel;\n    CFStringRef kVTProfileLevel_H264_High_3_0;\n    CFStringRef kVTProfileLevel_H264_High_3_1;\n    CFStringRef kVTProfileLevel_H264_High_3_2;\n    CFStringRef kVTProfileLevel_H264_High_4_0;\n    CFStringRef kVTProfileLevel_H264_High_4_1;\n    CFStringRef kVTProfileLevel_H264_High_4_2;\n    CFStringRef kVTProfileLevel_H264_High_5_1;\n    CFStringRef kVTProfileLevel_H264_High_5_2;\n    CFStringRef kVTProfileLevel_H264_High_AutoLevel;\n\n    CFStringRef kVTProfileLevel_HEVC_Main_AutoLevel;\n    CFStringRef kVTProfileLevel_HEVC_Main10_AutoLevel;\n\n    CFStringRef kVTCompressionPropertyKey_RealTime;\n\n    CFStringRef kVTVideoEncoderSpecification_EnableHardwareAcceleratedVideoEncoder;\n    CFStringRef kVTVideoEncoderSpecification_RequireHardwareAcceleratedVideoEncoder;\n\n    getParameterSetAtIndex CMVideoFormatDescriptionGetHEVCParameterSetAtIndex;\n} compat_keys;\n\n#define GET_SYM(symbol, defaultVal)                                     \\\ndo{                                                                     \\\n    CFStringRef* handle = (CFStringRef*)dlsym(RTLD_DEFAULT, #symbol);   \\\n    if(!handle)                                                         \\\n        compat_keys.symbol = CFSTR(defaultVal);                         \\\n    else                                                                \\\n        compat_keys.symbol = *handle;                                   \\\n}while(0)\n\nstatic pthread_once_t once_ctrl = PTHREAD_ONCE_INIT;\n\nstatic void loadVTEncSymbols(){\n    compat_keys.CMVideoFormatDescriptionGetHEVCParameterSetAtIndex =\n        (getParameterSetAtIndex)dlsym(\n            RTLD_DEFAULT,\n            \"CMVideoFormatDescriptionGetHEVCParameterSetAtIndex\"\n        );\n\n    GET_SYM(kCVImageBufferColorPrimaries_ITU_R_2020,   \"ITU_R_2020\");\n    GET_SYM(kCVImageBufferTransferFunction_ITU_R_2020, \"ITU_R_2020\");\n    GET_SYM(kCVImageBufferYCbCrMatrix_ITU_R_2020,      \"ITU_R_2020\");\n\n    GET_SYM(kVTCompressionPropertyKey_H264EntropyMode, \"H264EntropyMode\");\n    GET_SYM(kVTH264EntropyMode_CAVLC, \"CAVLC\");\n    GET_SYM(kVTH264EntropyMode_CABAC, \"CABAC\");\n\n    GET_SYM(kVTProfileLevel_H264_Baseline_4_0,       \"H264_Baseline_4_0\");\n    GET_SYM(kVTProfileLevel_H264_Baseline_4_2,       \"H264_Baseline_4_2\");\n    GET_SYM(kVTProfileLevel_H264_Baseline_5_0,       \"H264_Baseline_5_0\");\n    GET_SYM(kVTProfileLevel_H264_Baseline_5_1,       \"H264_Baseline_5_1\");\n    GET_SYM(kVTProfileLevel_H264_Baseline_5_2,       \"H264_Baseline_5_2\");\n    GET_SYM(kVTProfileLevel_H264_Baseline_AutoLevel, \"H264_Baseline_AutoLevel\");\n    GET_SYM(kVTProfileLevel_H264_Main_4_2,           \"H264_Main_4_2\");\n    GET_SYM(kVTProfileLevel_H264_Main_5_1,           \"H264_Main_5_1\");\n    GET_SYM(kVTProfileLevel_H264_Main_5_2,           \"H264_Main_5_2\");\n    GET_SYM(kVTProfileLevel_H264_Main_AutoLevel,     \"H264_Main_AutoLevel\");\n    GET_SYM(kVTProfileLevel_H264_High_3_0,           \"H264_High_3_0\");\n    GET_SYM(kVTProfileLevel_H264_High_3_1,           \"H264_High_3_1\");\n    GET_SYM(kVTProfileLevel_H264_High_3_2,           \"H264_High_3_2\");\n    GET_SYM(kVTProfileLevel_H264_High_4_0,           \"H264_High_4_0\");\n    GET_SYM(kVTProfileLevel_H264_High_4_1,           \"H264_High_4_1\");\n    GET_SYM(kVTProfileLevel_H264_High_4_2,           \"H264_High_4_2\");\n    GET_SYM(kVTProfileLevel_H264_High_5_1,           \"H264_High_5_1\");\n    GET_SYM(kVTProfileLevel_H264_High_5_2,           \"H264_High_5_2\");\n    GET_SYM(kVTProfileLevel_H264_High_AutoLevel,     \"H264_High_AutoLevel\");\n\n    GET_SYM(kVTProfileLevel_HEVC_Main_AutoLevel,     \"HEVC_Main_AutoLevel\");\n    GET_SYM(kVTProfileLevel_HEVC_Main10_AutoLevel,   \"HEVC_Main10_AutoLevel\");\n\n    GET_SYM(kVTCompressionPropertyKey_RealTime, \"RealTime\");\n\n    GET_SYM(kVTVideoEncoderSpecification_EnableHardwareAcceleratedVideoEncoder,\n            \"EnableHardwareAcceleratedVideoEncoder\");\n    GET_SYM(kVTVideoEncoderSpecification_RequireHardwareAcceleratedVideoEncoder,\n            \"RequireHardwareAcceleratedVideoEncoder\");\n}\n\ntypedef enum VT_H264Profile {\n    H264_PROF_AUTO,\n    H264_PROF_BASELINE,\n    H264_PROF_MAIN,\n    H264_PROF_HIGH,\n    H264_PROF_COUNT\n} VT_H264Profile;\n\ntypedef enum VTH264Entropy{\n    VT_ENTROPY_NOT_SET,\n    VT_CAVLC,\n    VT_CABAC\n} VTH264Entropy;\n\ntypedef enum VT_HEVCProfile {\n    HEVC_PROF_AUTO,\n    HEVC_PROF_MAIN,\n    HEVC_PROF_MAIN10,\n    HEVC_PROF_COUNT\n} VT_HEVCProfile;\n\nstatic const uint8_t start_code[] = { 0, 0, 0, 1 };\n\ntypedef struct ExtraSEI {\n  void *data;\n  size_t size;\n} ExtraSEI;\n\ntypedef struct BufNode {\n    CMSampleBufferRef cm_buffer;\n    ExtraSEI *sei;\n    struct BufNode* next;\n    int error;\n} BufNode;\n\ntypedef struct VTEncContext {\n    AVClass *class;\n    enum AVCodecID codec_id;\n    VTCompressionSessionRef session;\n    CFStringRef ycbcr_matrix;\n    CFStringRef color_primaries;\n    CFStringRef transfer_function;\n    getParameterSetAtIndex get_param_set_func;\n\n    pthread_mutex_t lock;\n    pthread_cond_t  cv_sample_sent;\n\n    int async_error;\n\n    BufNode *q_head;\n    BufNode *q_tail;\n\n    int64_t frame_ct_out;\n    int64_t frame_ct_in;\n\n    int64_t first_pts;\n    int64_t dts_delta;\n\n    int64_t profile;\n    int64_t level;\n    int64_t entropy;\n    int64_t realtime;\n    int64_t frames_before;\n    int64_t frames_after;\n\n    int64_t allow_sw;\n\n    bool flushing;\n    bool has_b_frames;\n    bool warned_color_range;\n    bool a53_cc;\n} VTEncContext;\n\nstatic int vtenc_populate_extradata(AVCodecContext   *avctx,\n                                    CMVideoCodecType codec_type,\n                                    CFStringRef      profile_level,\n                                    CFNumberRef      gamma_level,\n                                    CFDictionaryRef  enc_info,\n                                    CFDictionaryRef  pixel_buffer_info);\n\n/**\n * NULL-safe release of *refPtr, and sets value to NULL.\n */\nstatic void vt_release_num(CFNumberRef* refPtr){\n    if (!*refPtr) {\n        return;\n    }\n\n    CFRelease(*refPtr);\n    *refPtr = NULL;\n}\n\nstatic void set_async_error(VTEncContext *vtctx, int err)\n{\n    BufNode *info;\n\n    pthread_mutex_lock(&vtctx->lock);\n\n    vtctx->async_error = err;\n\n    info = vtctx->q_head;\n    vtctx->q_head = vtctx->q_tail = NULL;\n\n    while (info) {\n        BufNode *next = info->next;\n        CFRelease(info->cm_buffer);\n        av_free(info);\n        info = next;\n    }\n\n    pthread_mutex_unlock(&vtctx->lock);\n}\n\nstatic void clear_frame_queue(VTEncContext *vtctx)\n{\n    set_async_error(vtctx, 0);\n}\n\nstatic int vtenc_q_pop(VTEncContext *vtctx, bool wait, CMSampleBufferRef *buf, ExtraSEI **sei)\n{\n    BufNode *info;\n\n    pthread_mutex_lock(&vtctx->lock);\n\n    if (vtctx->async_error) {\n        pthread_mutex_unlock(&vtctx->lock);\n        return vtctx->async_error;\n    }\n\n    if (vtctx->flushing && vtctx->frame_ct_in == vtctx->frame_ct_out) {\n        *buf = NULL;\n\n        pthread_mutex_unlock(&vtctx->lock);\n        return 0;\n    }\n\n    while (!vtctx->q_head && !vtctx->async_error && wait) {\n        pthread_cond_wait(&vtctx->cv_sample_sent, &vtctx->lock);\n    }\n\n    if (!vtctx->q_head) {\n        pthread_mutex_unlock(&vtctx->lock);\n        *buf = NULL;\n        return 0;\n    }\n\n    info = vtctx->q_head;\n    vtctx->q_head = vtctx->q_head->next;\n    if (!vtctx->q_head) {\n        vtctx->q_tail = NULL;\n    }\n\n    pthread_mutex_unlock(&vtctx->lock);\n\n    *buf = info->cm_buffer;\n    if (sei && *buf) {\n        *sei = info->sei;\n    } else if (info->sei) {\n        if (info->sei->data) av_free(info->sei->data);\n        av_free(info->sei);\n    }\n    av_free(info);\n\n    vtctx->frame_ct_out++;\n\n    return 0;\n}\n\nstatic void vtenc_q_push(VTEncContext *vtctx, CMSampleBufferRef buffer, ExtraSEI *sei)\n{\n    BufNode *info = av_malloc(sizeof(BufNode));\n    if (!info) {\n        set_async_error(vtctx, AVERROR(ENOMEM));\n        return;\n    }\n\n    CFRetain(buffer);\n    info->cm_buffer = buffer;\n    info->sei = sei;\n    info->next = NULL;\n\n    pthread_mutex_lock(&vtctx->lock);\n    pthread_cond_signal(&vtctx->cv_sample_sent);\n\n    if (!vtctx->q_head) {\n        vtctx->q_head = info;\n    } else {\n        vtctx->q_tail->next = info;\n    }\n\n    vtctx->q_tail = info;\n\n    pthread_mutex_unlock(&vtctx->lock);\n}\n\nstatic int count_nalus(size_t length_code_size,\n                       CMSampleBufferRef sample_buffer,\n                       int *count)\n{\n    size_t offset = 0;\n    int status;\n    int nalu_ct = 0;\n    uint8_t size_buf[4];\n    size_t src_size = CMSampleBufferGetTotalSampleSize(sample_buffer);\n    CMBlockBufferRef block = CMSampleBufferGetDataBuffer(sample_buffer);\n\n    if (length_code_size > 4)\n        return AVERROR_INVALIDDATA;\n\n    while (offset < src_size) {\n        size_t curr_src_len;\n        size_t box_len = 0;\n        size_t i;\n\n        status = CMBlockBufferCopyDataBytes(block,\n                                            offset,\n                                            length_code_size,\n                                            size_buf);\n\n        for (i = 0; i < length_code_size; i++) {\n            box_len <<= 8;\n            box_len |= size_buf[i];\n        }\n\n        curr_src_len = box_len + length_code_size;\n        offset += curr_src_len;\n\n        nalu_ct++;\n    }\n\n    *count = nalu_ct;\n    return 0;\n}\n\nstatic CMVideoCodecType get_cm_codec_type(enum AVCodecID id)\n{\n    switch (id) {\n    case AV_CODEC_ID_H264: return kCMVideoCodecType_H264;\n    case AV_CODEC_ID_HEVC: return kCMVideoCodecType_HEVC;\n    default:               return 0;\n    }\n}\n\n/**\n * Get the parameter sets from a CMSampleBufferRef.\n * @param dst If *dst isn't NULL, the parameters are copied into existing\n *            memory. *dst_size must be set accordingly when *dst != NULL.\n *            If *dst is NULL, it will be allocated.\n *            In all cases, *dst_size is set to the number of bytes used starting\n *            at *dst.\n */\nstatic int get_params_size(\n    AVCodecContext              *avctx,\n    CMVideoFormatDescriptionRef vid_fmt,\n    size_t                      *size)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    size_t total_size = 0;\n    size_t ps_count;\n    int is_count_bad = 0;\n    size_t i;\n    int status;\n    status = vtctx->get_param_set_func(vid_fmt,\n                                       0,\n                                       NULL,\n                                       NULL,\n                                       &ps_count,\n                                       NULL);\n    if (status) {\n        is_count_bad = 1;\n        ps_count     = 0;\n        status       = 0;\n    }\n\n    for (i = 0; i < ps_count || is_count_bad; i++) {\n        const uint8_t *ps;\n        size_t ps_size;\n        status = vtctx->get_param_set_func(vid_fmt,\n                                           i,\n                                           &ps,\n                                           &ps_size,\n                                           NULL,\n                                           NULL);\n        if (status) {\n            /*\n             * When ps_count is invalid, status != 0 ends the loop normally\n             * unless we didn't get any parameter sets.\n             */\n            if (i > 0 && is_count_bad) status = 0;\n\n            break;\n        }\n\n        total_size += ps_size + sizeof(start_code);\n    }\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error getting parameter set sizes: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    *size = total_size;\n    return 0;\n}\n\nstatic int copy_param_sets(\n    AVCodecContext              *avctx,\n    CMVideoFormatDescriptionRef vid_fmt,\n    uint8_t                     *dst,\n    size_t                      dst_size)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    size_t ps_count;\n    int is_count_bad = 0;\n    int status;\n    size_t offset = 0;\n    size_t i;\n\n    status = vtctx->get_param_set_func(vid_fmt,\n                                       0,\n                                       NULL,\n                                       NULL,\n                                       &ps_count,\n                                       NULL);\n    if (status) {\n        is_count_bad = 1;\n        ps_count     = 0;\n        status       = 0;\n    }\n\n\n    for (i = 0; i < ps_count || is_count_bad; i++) {\n        const uint8_t *ps;\n        size_t ps_size;\n        size_t next_offset;\n\n        status = vtctx->get_param_set_func(vid_fmt,\n                                           i,\n                                           &ps,\n                                           &ps_size,\n                                           NULL,\n                                           NULL);\n        if (status) {\n            if (i > 0 && is_count_bad) status = 0;\n\n            break;\n        }\n\n        next_offset = offset + sizeof(start_code) + ps_size;\n        if (dst_size < next_offset) {\n            av_log(avctx, AV_LOG_ERROR, \"Error: buffer too small for parameter sets.\\n\");\n            return AVERROR_BUFFER_TOO_SMALL;\n        }\n\n        memcpy(dst + offset, start_code, sizeof(start_code));\n        offset += sizeof(start_code);\n\n        memcpy(dst + offset, ps, ps_size);\n        offset = next_offset;\n    }\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error getting parameter set data: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    return 0;\n}\n\nstatic int set_extradata(AVCodecContext *avctx, CMSampleBufferRef sample_buffer)\n{\n    CMVideoFormatDescriptionRef vid_fmt;\n    size_t total_size;\n    int status;\n\n    vid_fmt = CMSampleBufferGetFormatDescription(sample_buffer);\n    if (!vid_fmt) {\n        av_log(avctx, AV_LOG_ERROR, \"No video format.\\n\");\n        return AVERROR_EXTERNAL;\n    }\n\n    status = get_params_size(avctx, vid_fmt, &total_size);\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Could not get parameter sets.\\n\");\n        return status;\n    }\n\n    avctx->extradata = av_mallocz(total_size + AV_INPUT_BUFFER_PADDING_SIZE);\n    if (!avctx->extradata) {\n        return AVERROR(ENOMEM);\n    }\n    avctx->extradata_size = total_size;\n\n    status = copy_param_sets(avctx, vid_fmt, avctx->extradata, total_size);\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Could not copy param sets.\\n\");\n        return status;\n    }\n\n    return 0;\n}\n\nstatic void vtenc_output_callback(\n    void *ctx,\n    void *sourceFrameCtx,\n    OSStatus status,\n    VTEncodeInfoFlags flags,\n    CMSampleBufferRef sample_buffer)\n{\n    AVCodecContext *avctx = ctx;\n    VTEncContext   *vtctx = avctx->priv_data;\n    ExtraSEI *sei = sourceFrameCtx;\n\n    if (vtctx->async_error) {\n        if(sample_buffer) CFRelease(sample_buffer);\n        return;\n    }\n\n    if (status || !sample_buffer) {\n        av_log(avctx, AV_LOG_ERROR, \"Error encoding frame: %d\\n\", (int)status);\n        set_async_error(vtctx, AVERROR_EXTERNAL);\n        return;\n    }\n\n    if (!avctx->extradata && (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER)) {\n        int set_status = set_extradata(avctx, sample_buffer);\n        if (set_status) {\n            set_async_error(vtctx, set_status);\n            return;\n        }\n    }\n\n    vtenc_q_push(vtctx, sample_buffer, sei);\n}\n\nstatic int get_length_code_size(\n    AVCodecContext    *avctx,\n    CMSampleBufferRef sample_buffer,\n    size_t            *size)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    CMVideoFormatDescriptionRef vid_fmt;\n    int isize;\n    int status;\n\n    vid_fmt = CMSampleBufferGetFormatDescription(sample_buffer);\n    if (!vid_fmt) {\n        av_log(avctx, AV_LOG_ERROR, \"Error getting buffer format description.\\n\");\n        return AVERROR_EXTERNAL;\n    }\n\n    status = vtctx->get_param_set_func(vid_fmt,\n                                       0,\n                                       NULL,\n                                       NULL,\n                                       NULL,\n                                       &isize);\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error getting length code size: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    *size = isize;\n    return 0;\n}\n\n/*\n * Returns true on success.\n *\n * If profile_level_val is NULL and this method returns true, don't specify the\n * profile/level to the encoder.\n */\nstatic bool get_vt_h264_profile_level(AVCodecContext *avctx,\n                                      CFStringRef    *profile_level_val)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    int64_t profile = vtctx->profile;\n\n    if (profile == H264_PROF_AUTO && vtctx->level) {\n        //Need to pick a profile if level is not auto-selected.\n        profile = vtctx->has_b_frames ? H264_PROF_MAIN : H264_PROF_BASELINE;\n    }\n\n    *profile_level_val = NULL;\n\n    switch (profile) {\n        case H264_PROF_AUTO:\n            return true;\n\n        case H264_PROF_BASELINE:\n            switch (vtctx->level) {\n                case  0: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Baseline_AutoLevel; break;\n                case 13: *profile_level_val = kVTProfileLevel_H264_Baseline_1_3;       break;\n                case 30: *profile_level_val = kVTProfileLevel_H264_Baseline_3_0;       break;\n                case 31: *profile_level_val = kVTProfileLevel_H264_Baseline_3_1;       break;\n                case 32: *profile_level_val = kVTProfileLevel_H264_Baseline_3_2;       break;\n                case 40: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Baseline_4_0;       break;\n                case 41: *profile_level_val = kVTProfileLevel_H264_Baseline_4_1;       break;\n                case 42: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Baseline_4_2;       break;\n                case 50: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Baseline_5_0;       break;\n                case 51: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Baseline_5_1;       break;\n                case 52: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Baseline_5_2;       break;\n            }\n            break;\n\n        case H264_PROF_MAIN:\n            switch (vtctx->level) {\n                case  0: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Main_AutoLevel; break;\n                case 30: *profile_level_val = kVTProfileLevel_H264_Main_3_0;       break;\n                case 31: *profile_level_val = kVTProfileLevel_H264_Main_3_1;       break;\n                case 32: *profile_level_val = kVTProfileLevel_H264_Main_3_2;       break;\n                case 40: *profile_level_val = kVTProfileLevel_H264_Main_4_0;       break;\n                case 41: *profile_level_val = kVTProfileLevel_H264_Main_4_1;       break;\n                case 42: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Main_4_2;       break;\n                case 50: *profile_level_val = kVTProfileLevel_H264_Main_5_0;       break;\n                case 51: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Main_5_1;       break;\n                case 52: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_Main_5_2;       break;\n            }\n            break;\n\n        case H264_PROF_HIGH:\n            switch (vtctx->level) {\n                case  0: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_AutoLevel; break;\n                case 30: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_3_0;       break;\n                case 31: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_3_1;       break;\n                case 32: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_3_2;       break;\n                case 40: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_4_0;       break;\n                case 41: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_4_1;       break;\n                case 42: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_4_2;       break;\n                case 50: *profile_level_val = kVTProfileLevel_H264_High_5_0;       break;\n                case 51: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_5_1;       break;\n                case 52: *profile_level_val =\n                                  compat_keys.kVTProfileLevel_H264_High_5_2;       break;\n            }\n            break;\n    }\n\n    if (!*profile_level_val) {\n        av_log(avctx, AV_LOG_ERROR, \"Invalid Profile/Level.\\n\");\n        return false;\n    }\n\n    return true;\n}\n\n/*\n * Returns true on success.\n *\n * If profile_level_val is NULL and this method returns true, don't specify the\n * profile/level to the encoder.\n */\nstatic bool get_vt_hevc_profile_level(AVCodecContext *avctx,\n                                      CFStringRef    *profile_level_val)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    int64_t profile = vtctx->profile;\n\n    *profile_level_val = NULL;\n\n    switch (profile) {\n        case HEVC_PROF_AUTO:\n            return true;\n        case HEVC_PROF_MAIN:\n            *profile_level_val =\n                compat_keys.kVTProfileLevel_HEVC_Main_AutoLevel;\n            break;\n        case HEVC_PROF_MAIN10:\n            *profile_level_val =\n                compat_keys.kVTProfileLevel_HEVC_Main10_AutoLevel;\n            break;\n    }\n\n    if (!*profile_level_val) {\n        av_log(avctx, AV_LOG_ERROR, \"Invalid Profile/Level.\\n\");\n        return false;\n    }\n\n    return true;\n}\n\nstatic int get_cv_pixel_format(AVCodecContext* avctx,\n                               enum AVPixelFormat fmt,\n                               enum AVColorRange range,\n                               int* av_pixel_format,\n                               int* range_guessed)\n{\n    if (range_guessed) *range_guessed = range != AVCOL_RANGE_MPEG &&\n                                        range != AVCOL_RANGE_JPEG;\n\n    //MPEG range is used when no range is set\n    if (fmt == AV_PIX_FMT_NV12) {\n        *av_pixel_format = range == AVCOL_RANGE_JPEG ?\n                                        kCVPixelFormatType_420YpCbCr8BiPlanarFullRange :\n                                        kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange;\n    } else if (fmt == AV_PIX_FMT_YUV420P) {\n        *av_pixel_format = range == AVCOL_RANGE_JPEG ?\n                                        kCVPixelFormatType_420YpCbCr8PlanarFullRange :\n                                        kCVPixelFormatType_420YpCbCr8Planar;\n    } else {\n        return AVERROR(EINVAL);\n    }\n\n    return 0;\n}\n\nstatic void add_color_attr(AVCodecContext *avctx, CFMutableDictionaryRef dict) {\n    VTEncContext *vtctx = avctx->priv_data;\n\n    if (vtctx->color_primaries) {\n        CFDictionarySetValue(dict,\n                             kCVImageBufferColorPrimariesKey,\n                             vtctx->color_primaries);\n    }\n\n    if (vtctx->transfer_function) {\n        CFDictionarySetValue(dict,\n                             kCVImageBufferTransferFunctionKey,\n                             vtctx->transfer_function);\n    }\n\n    if (vtctx->ycbcr_matrix) {\n        CFDictionarySetValue(dict,\n                             kCVImageBufferYCbCrMatrixKey,\n                             vtctx->ycbcr_matrix);\n    }\n}\n\nstatic int create_cv_pixel_buffer_info(AVCodecContext* avctx,\n                                       CFMutableDictionaryRef* dict)\n{\n    CFNumberRef cv_color_format_num = NULL;\n    CFNumberRef width_num = NULL;\n    CFNumberRef height_num = NULL;\n    CFMutableDictionaryRef pixel_buffer_info = NULL;\n    int cv_color_format;\n    int status = get_cv_pixel_format(avctx,\n                                     avctx->pix_fmt,\n                                     avctx->color_range,\n                                     &cv_color_format,\n                                     NULL);\n    if (status) return status;\n\n    pixel_buffer_info = CFDictionaryCreateMutable(\n                            kCFAllocatorDefault,\n                            20,\n                            &kCFCopyStringDictionaryKeyCallBacks,\n                            &kCFTypeDictionaryValueCallBacks);\n\n    if (!pixel_buffer_info) goto pbinfo_nomem;\n\n    cv_color_format_num = CFNumberCreate(kCFAllocatorDefault,\n                                         kCFNumberSInt32Type,\n                                         &cv_color_format);\n    if (!cv_color_format_num) goto pbinfo_nomem;\n\n    CFDictionarySetValue(pixel_buffer_info,\n                         kCVPixelBufferPixelFormatTypeKey,\n                         cv_color_format_num);\n    vt_release_num(&cv_color_format_num);\n\n    width_num = CFNumberCreate(kCFAllocatorDefault,\n                               kCFNumberSInt32Type,\n                               &avctx->width);\n    if (!width_num) return AVERROR(ENOMEM);\n\n    CFDictionarySetValue(pixel_buffer_info,\n                         kCVPixelBufferWidthKey,\n                         width_num);\n    vt_release_num(&width_num);\n\n    height_num = CFNumberCreate(kCFAllocatorDefault,\n                                kCFNumberSInt32Type,\n                                &avctx->height);\n    if (!height_num) goto pbinfo_nomem;\n\n    CFDictionarySetValue(pixel_buffer_info,\n                         kCVPixelBufferHeightKey,\n                         height_num);\n    vt_release_num(&height_num);\n\n    add_color_attr(avctx, pixel_buffer_info);\n\n    *dict = pixel_buffer_info;\n    return 0;\n\npbinfo_nomem:\n    vt_release_num(&cv_color_format_num);\n    vt_release_num(&width_num);\n    vt_release_num(&height_num);\n    if (pixel_buffer_info) CFRelease(pixel_buffer_info);\n\n    return AVERROR(ENOMEM);\n}\n\nstatic int get_cv_color_primaries(AVCodecContext *avctx,\n                                  CFStringRef *primaries)\n{\n    enum AVColorPrimaries pri = avctx->color_primaries;\n    switch (pri) {\n        case AVCOL_PRI_UNSPECIFIED:\n            *primaries = NULL;\n            break;\n\n        case AVCOL_PRI_BT709:\n            *primaries = kCVImageBufferColorPrimaries_ITU_R_709_2;\n            break;\n\n        case AVCOL_PRI_BT2020:\n            *primaries = compat_keys.kCVImageBufferColorPrimaries_ITU_R_2020;\n            break;\n\n        default:\n            av_log(avctx, AV_LOG_ERROR, \"Color primaries %s is not supported.\\n\", av_color_primaries_name(pri));\n            *primaries = NULL;\n            return -1;\n    }\n\n    return 0;\n}\n\nstatic int get_cv_transfer_function(AVCodecContext *avctx,\n                                    CFStringRef *transfer_fnc,\n                                    CFNumberRef *gamma_level)\n{\n    enum AVColorTransferCharacteristic trc = avctx->color_trc;\n    Float32 gamma;\n    *gamma_level = NULL;\n\n    switch (trc) {\n        case AVCOL_TRC_UNSPECIFIED:\n            *transfer_fnc = NULL;\n            break;\n\n        case AVCOL_TRC_BT709:\n            *transfer_fnc = kCVImageBufferTransferFunction_ITU_R_709_2;\n            break;\n\n        case AVCOL_TRC_SMPTE240M:\n            *transfer_fnc = kCVImageBufferTransferFunction_SMPTE_240M_1995;\n            break;\n\n        case AVCOL_TRC_GAMMA22:\n            gamma = 2.2;\n            *transfer_fnc = kCVImageBufferTransferFunction_UseGamma;\n            *gamma_level = CFNumberCreate(NULL, kCFNumberFloat32Type, &gamma);\n            break;\n\n        case AVCOL_TRC_GAMMA28:\n            gamma = 2.8;\n            *transfer_fnc = kCVImageBufferTransferFunction_UseGamma;\n            *gamma_level = CFNumberCreate(NULL, kCFNumberFloat32Type, &gamma);\n            break;\n\n        case AVCOL_TRC_BT2020_10:\n        case AVCOL_TRC_BT2020_12:\n            *transfer_fnc = compat_keys.kCVImageBufferTransferFunction_ITU_R_2020;\n            break;\n\n        default:\n            av_log(avctx, AV_LOG_ERROR, \"Transfer function %s is not supported.\\n\", av_color_transfer_name(trc));\n            return -1;\n    }\n\n    return 0;\n}\n\nstatic int get_cv_ycbcr_matrix(AVCodecContext *avctx, CFStringRef *matrix) {\n    switch(avctx->colorspace) {\n        case AVCOL_SPC_BT709:\n            *matrix = kCVImageBufferYCbCrMatrix_ITU_R_709_2;\n            break;\n\n        case AVCOL_SPC_UNSPECIFIED:\n            *matrix = NULL;\n            break;\n\n        case AVCOL_SPC_BT470BG:\n        case AVCOL_SPC_SMPTE170M:\n            *matrix = kCVImageBufferYCbCrMatrix_ITU_R_601_4;\n            break;\n\n        case AVCOL_SPC_SMPTE240M:\n            *matrix = kCVImageBufferYCbCrMatrix_SMPTE_240M_1995;\n            break;\n\n        case AVCOL_SPC_BT2020_NCL:\n            *matrix = compat_keys.kCVImageBufferYCbCrMatrix_ITU_R_2020;\n            break;\n\n        default:\n            av_log(avctx, AV_LOG_ERROR, \"Color space %s is not supported.\\n\", av_color_space_name(avctx->colorspace));\n            return -1;\n    }\n\n    return 0;\n}\n\nstatic int vtenc_create_encoder(AVCodecContext   *avctx,\n                                CMVideoCodecType codec_type,\n                                CFStringRef      profile_level,\n                                CFNumberRef      gamma_level,\n                                CFDictionaryRef  enc_info,\n                                CFDictionaryRef  pixel_buffer_info,\n                                VTCompressionSessionRef *session)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    SInt32       bit_rate = avctx->bit_rate;\n    SInt32       max_rate = avctx->rc_max_rate;\n    CFNumberRef  bit_rate_num;\n    CFNumberRef  bytes_per_second;\n    CFNumberRef  one_second;\n    CFArrayRef   data_rate_limits;\n    int64_t      bytes_per_second_value = 0;\n    int64_t      one_second_value = 0;\n    void         *nums[2];\n\n    int status = VTCompressionSessionCreate(kCFAllocatorDefault,\n                                            avctx->width,\n                                            avctx->height,\n                                            codec_type,\n                                            enc_info,\n                                            pixel_buffer_info,\n                                            kCFAllocatorDefault,\n                                            vtenc_output_callback,\n                                            avctx,\n                                            session);\n\n    if (status || !vtctx->session) {\n        av_log(avctx, AV_LOG_ERROR, \"Error: cannot create compression session: %d\\n\", status);\n\n#if !TARGET_OS_IPHONE\n        if (!vtctx->allow_sw) {\n            av_log(avctx, AV_LOG_ERROR, \"Try -allow_sw 1. The hardware encoder may be busy, or not supported.\\n\");\n        }\n#endif\n\n        return AVERROR_EXTERNAL;\n    }\n\n    bit_rate_num = CFNumberCreate(kCFAllocatorDefault,\n                                  kCFNumberSInt32Type,\n                                  &bit_rate);\n    if (!bit_rate_num) return AVERROR(ENOMEM);\n\n    status = VTSessionSetProperty(vtctx->session,\n                                  kVTCompressionPropertyKey_AverageBitRate,\n                                  bit_rate_num);\n    CFRelease(bit_rate_num);\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error setting bitrate property: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    if (vtctx->codec_id == AV_CODEC_ID_H264 && max_rate > 0) {\n        // kVTCompressionPropertyKey_DataRateLimits is not available for HEVC\n        bytes_per_second_value = max_rate >> 3;\n        bytes_per_second = CFNumberCreate(kCFAllocatorDefault,\n                                          kCFNumberSInt64Type,\n                                          &bytes_per_second_value);\n        if (!bytes_per_second) {\n            return AVERROR(ENOMEM);\n        }\n        one_second_value = 1;\n        one_second = CFNumberCreate(kCFAllocatorDefault,\n                                    kCFNumberSInt64Type,\n                                    &one_second_value);\n        if (!one_second) {\n            CFRelease(bytes_per_second);\n            return AVERROR(ENOMEM);\n        }\n        nums[0] = (void *)bytes_per_second;\n        nums[1] = (void *)one_second;\n        data_rate_limits = CFArrayCreate(kCFAllocatorDefault,\n                                         (const void **)nums,\n                                         2,\n                                         &kCFTypeArrayCallBacks);\n\n        if (!data_rate_limits) {\n            CFRelease(bytes_per_second);\n            CFRelease(one_second);\n            return AVERROR(ENOMEM);\n        }\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_DataRateLimits,\n                                      data_rate_limits);\n\n        CFRelease(bytes_per_second);\n        CFRelease(one_second);\n        CFRelease(data_rate_limits);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting max bitrate property: %d\\n\", status);\n            return AVERROR_EXTERNAL;\n        }\n    }\n\n    if (vtctx->codec_id == AV_CODEC_ID_H264) {\n        // kVTCompressionPropertyKey_ProfileLevel is not available for HEVC\n        if (profile_level) {\n            status = VTSessionSetProperty(vtctx->session,\n                                        kVTCompressionPropertyKey_ProfileLevel,\n                                        profile_level);\n            if (status) {\n                av_log(avctx, AV_LOG_ERROR, \"Error setting profile/level property: %d\\n\", status);\n            }\n        }\n    }\n\n    if (avctx->gop_size > 0) {\n        CFNumberRef interval = CFNumberCreate(kCFAllocatorDefault,\n                                              kCFNumberIntType,\n                                              &avctx->gop_size);\n        if (!interval) {\n            return AVERROR(ENOMEM);\n        }\n\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_MaxKeyFrameInterval,\n                                      interval);\n        CFRelease(interval);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting 'max key-frame interval' property: %d\\n\", status);\n            return AVERROR_EXTERNAL;\n        }\n    }\n\n    if (vtctx->frames_before) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_MoreFramesBeforeStart,\n                                      kCFBooleanTrue);\n\n        if (status == kVTPropertyNotSupportedErr) {\n            av_log(avctx, AV_LOG_WARNING, \"frames_before property is not supported on this device. Ignoring.\\n\");\n        } else if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting frames_before property: %d\\n\", status);\n        }\n    }\n\n    if (vtctx->frames_after) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_MoreFramesAfterEnd,\n                                      kCFBooleanTrue);\n\n        if (status == kVTPropertyNotSupportedErr) {\n            av_log(avctx, AV_LOG_WARNING, \"frames_after property is not supported on this device. Ignoring.\\n\");\n        } else if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting frames_after property: %d\\n\", status);\n        }\n    }\n\n    if (avctx->sample_aspect_ratio.num != 0) {\n        CFNumberRef num;\n        CFNumberRef den;\n        CFMutableDictionaryRef par;\n        AVRational *avpar = &avctx->sample_aspect_ratio;\n\n        av_reduce(&avpar->num, &avpar->den,\n                   avpar->num,  avpar->den,\n                  0xFFFFFFFF);\n\n        num = CFNumberCreate(kCFAllocatorDefault,\n                             kCFNumberIntType,\n                             &avpar->num);\n\n        den = CFNumberCreate(kCFAllocatorDefault,\n                             kCFNumberIntType,\n                             &avpar->den);\n\n\n\n        par = CFDictionaryCreateMutable(kCFAllocatorDefault,\n                                        2,\n                                        &kCFCopyStringDictionaryKeyCallBacks,\n                                        &kCFTypeDictionaryValueCallBacks);\n\n        if (!par || !num || !den) {\n            if (par) CFRelease(par);\n            if (num) CFRelease(num);\n            if (den) CFRelease(den);\n\n            return AVERROR(ENOMEM);\n        }\n\n        CFDictionarySetValue(\n            par,\n            kCMFormatDescriptionKey_PixelAspectRatioHorizontalSpacing,\n            num);\n\n        CFDictionarySetValue(\n            par,\n            kCMFormatDescriptionKey_PixelAspectRatioVerticalSpacing,\n            den);\n\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_PixelAspectRatio,\n                                      par);\n\n        CFRelease(par);\n        CFRelease(num);\n        CFRelease(den);\n\n        if (status) {\n            av_log(avctx,\n                   AV_LOG_ERROR,\n                   \"Error setting pixel aspect ratio to %d:%d: %d.\\n\",\n                   avctx->sample_aspect_ratio.num,\n                   avctx->sample_aspect_ratio.den,\n                   status);\n\n            return AVERROR_EXTERNAL;\n        }\n    }\n\n\n    if (vtctx->transfer_function) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_TransferFunction,\n                                      vtctx->transfer_function);\n\n        if (status) {\n            av_log(avctx, AV_LOG_WARNING, \"Could not set transfer function: %d\\n\", status);\n        }\n    }\n\n\n    if (vtctx->ycbcr_matrix) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_YCbCrMatrix,\n                                      vtctx->ycbcr_matrix);\n\n        if (status) {\n            av_log(avctx, AV_LOG_WARNING, \"Could not set ycbcr matrix: %d\\n\", status);\n        }\n    }\n\n\n    if (vtctx->color_primaries) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_ColorPrimaries,\n                                      vtctx->color_primaries);\n\n        if (status) {\n            av_log(avctx, AV_LOG_WARNING, \"Could not set color primaries: %d\\n\", status);\n        }\n    }\n\n    if (gamma_level) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kCVImageBufferGammaLevelKey,\n                                      gamma_level);\n\n        if (status) {\n            av_log(avctx, AV_LOG_WARNING, \"Could not set gamma level: %d\\n\", status);\n        }\n    }\n\n    if (!vtctx->has_b_frames) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      kVTCompressionPropertyKey_AllowFrameReordering,\n                                      kCFBooleanFalse);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting 'allow frame reordering' property: %d\\n\", status);\n            return AVERROR_EXTERNAL;\n        }\n    }\n\n    if (vtctx->entropy != VT_ENTROPY_NOT_SET) {\n        CFStringRef entropy = vtctx->entropy == VT_CABAC ?\n                                compat_keys.kVTH264EntropyMode_CABAC:\n                                compat_keys.kVTH264EntropyMode_CAVLC;\n\n        status = VTSessionSetProperty(vtctx->session,\n                                      compat_keys.kVTCompressionPropertyKey_H264EntropyMode,\n                                      entropy);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting entropy property: %d\\n\", status);\n        }\n    }\n\n    if (vtctx->realtime) {\n        status = VTSessionSetProperty(vtctx->session,\n                                      compat_keys.kVTCompressionPropertyKey_RealTime,\n                                      kCFBooleanTrue);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error setting realtime property: %d\\n\", status);\n        }\n    }\n\n    status = VTCompressionSessionPrepareToEncodeFrames(vtctx->session);\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error: cannot prepare encoder: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    return 0;\n}\n\nstatic int vtenc_configure_encoder(AVCodecContext *avctx)\n{\n    CFMutableDictionaryRef enc_info;\n    CFMutableDictionaryRef pixel_buffer_info;\n    CMVideoCodecType       codec_type;\n    VTEncContext           *vtctx = avctx->priv_data;\n    CFStringRef            profile_level;\n    CFNumberRef            gamma_level = NULL;\n    int                    status;\n\n    codec_type = get_cm_codec_type(avctx->codec_id);\n    if (!codec_type) {\n        av_log(avctx, AV_LOG_ERROR, \"Error: no mapping for AVCodecID %d\\n\", avctx->codec_id);\n        return AVERROR(EINVAL);\n    }\n\n    vtctx->codec_id = avctx->codec_id;\n\n    if (vtctx->codec_id == AV_CODEC_ID_H264) {\n        vtctx->get_param_set_func = CMVideoFormatDescriptionGetH264ParameterSetAtIndex;\n\n        vtctx->has_b_frames = avctx->max_b_frames > 0;\n        if(vtctx->has_b_frames && vtctx->profile == H264_PROF_BASELINE){\n            av_log(avctx, AV_LOG_WARNING, \"Cannot use B-frames with baseline profile. Output will not contain B-frames.\\n\");\n            vtctx->has_b_frames = false;\n        }\n\n        if (vtctx->entropy == VT_CABAC && vtctx->profile == H264_PROF_BASELINE) {\n            av_log(avctx, AV_LOG_WARNING, \"CABAC entropy requires 'main' or 'high' profile, but baseline was requested. Encode will not use CABAC entropy.\\n\");\n            vtctx->entropy = VT_ENTROPY_NOT_SET;\n        }\n\n        if (!get_vt_h264_profile_level(avctx, &profile_level)) return AVERROR(EINVAL);\n    } else {\n        vtctx->get_param_set_func = compat_keys.CMVideoFormatDescriptionGetHEVCParameterSetAtIndex;\n        if (!vtctx->get_param_set_func) return AVERROR(EINVAL);\n        if (!get_vt_hevc_profile_level(avctx, &profile_level)) return AVERROR(EINVAL);\n    }\n\n    enc_info = CFDictionaryCreateMutable(\n        kCFAllocatorDefault,\n        20,\n        &kCFCopyStringDictionaryKeyCallBacks,\n        &kCFTypeDictionaryValueCallBacks\n    );\n\n    if (!enc_info) return AVERROR(ENOMEM);\n\n#if !TARGET_OS_IPHONE\n    if (!vtctx->allow_sw) {\n        CFDictionarySetValue(enc_info,\n                             compat_keys.kVTVideoEncoderSpecification_RequireHardwareAcceleratedVideoEncoder,\n                             kCFBooleanTrue);\n    } else {\n        CFDictionarySetValue(enc_info,\n                             compat_keys.kVTVideoEncoderSpecification_EnableHardwareAcceleratedVideoEncoder,\n                             kCFBooleanTrue);\n    }\n#endif\n\n    if (avctx->pix_fmt != AV_PIX_FMT_VIDEOTOOLBOX) {\n        status = create_cv_pixel_buffer_info(avctx, &pixel_buffer_info);\n        if (status)\n            goto init_cleanup;\n    } else {\n        pixel_buffer_info = NULL;\n    }\n\n    vtctx->dts_delta = vtctx->has_b_frames ? -1 : 0;\n\n    get_cv_transfer_function(avctx, &vtctx->transfer_function, &gamma_level);\n    get_cv_ycbcr_matrix(avctx, &vtctx->ycbcr_matrix);\n    get_cv_color_primaries(avctx, &vtctx->color_primaries);\n\n\n    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {\n        status = vtenc_populate_extradata(avctx,\n                                          codec_type,\n                                          profile_level,\n                                          gamma_level,\n                                          enc_info,\n                                          pixel_buffer_info);\n        if (status)\n            goto init_cleanup;\n    }\n\n    status = vtenc_create_encoder(avctx,\n                                  codec_type,\n                                  profile_level,\n                                  gamma_level,\n                                  enc_info,\n                                  pixel_buffer_info,\n                                  &vtctx->session);\n\ninit_cleanup:\n    if (gamma_level)\n        CFRelease(gamma_level);\n\n    if (pixel_buffer_info)\n        CFRelease(pixel_buffer_info);\n\n    CFRelease(enc_info);\n\n    return status;\n}\n\nstatic av_cold int vtenc_init(AVCodecContext *avctx)\n{\n    VTEncContext    *vtctx = avctx->priv_data;\n    CFBooleanRef    has_b_frames_cfbool;\n    int             status;\n\n    pthread_once(&once_ctrl, loadVTEncSymbols);\n\n    pthread_mutex_init(&vtctx->lock, NULL);\n    pthread_cond_init(&vtctx->cv_sample_sent, NULL);\n\n    vtctx->session = NULL;\n    status = vtenc_configure_encoder(avctx);\n    if (status) return status;\n\n    status = VTSessionCopyProperty(vtctx->session,\n                                   kVTCompressionPropertyKey_AllowFrameReordering,\n                                   kCFAllocatorDefault,\n                                   &has_b_frames_cfbool);\n\n    if (!status && has_b_frames_cfbool) {\n        //Some devices don't output B-frames for main profile, even if requested.\n        vtctx->has_b_frames = CFBooleanGetValue(has_b_frames_cfbool);\n        CFRelease(has_b_frames_cfbool);\n    }\n    avctx->has_b_frames = vtctx->has_b_frames;\n\n    return 0;\n}\n\nstatic void vtenc_get_frame_info(CMSampleBufferRef buffer, bool *is_key_frame)\n{\n    CFArrayRef      attachments;\n    CFDictionaryRef attachment;\n    CFBooleanRef    not_sync;\n    CFIndex         len;\n\n    attachments = CMSampleBufferGetSampleAttachmentsArray(buffer, false);\n    len = !attachments ? 0 : CFArrayGetCount(attachments);\n\n    if (!len) {\n        *is_key_frame = true;\n        return;\n    }\n\n    attachment = CFArrayGetValueAtIndex(attachments, 0);\n\n    if (CFDictionaryGetValueIfPresent(attachment,\n                                      kCMSampleAttachmentKey_NotSync,\n                                      (const void **)&not_sync))\n    {\n        *is_key_frame = !CFBooleanGetValue(not_sync);\n    } else {\n        *is_key_frame = true;\n    }\n}\n\nstatic int is_post_sei_nal_type(int nal_type){\n    return nal_type != H264_NAL_SEI &&\n           nal_type != H264_NAL_SPS &&\n           nal_type != H264_NAL_PPS &&\n           nal_type != H264_NAL_AUD;\n}\n\n/*\n * Finds the sei message start/size of type find_sei_type.\n * If more than one of that type exists, the last one is returned.\n */\nstatic int find_sei_end(AVCodecContext *avctx,\n                        uint8_t        *nal_data,\n                        size_t          nal_size,\n                        uint8_t       **sei_end)\n{\n    int nal_type;\n    size_t sei_payload_size = 0;\n    int sei_payload_type = 0;\n    *sei_end = NULL;\n    uint8_t *nal_start = nal_data;\n\n    if (!nal_size)\n        return 0;\n\n    nal_type = *nal_data & 0x1F;\n    if (nal_type != H264_NAL_SEI)\n        return 0;\n\n    nal_data++;\n    nal_size--;\n\n    if (nal_data[nal_size - 1] == 0x80)\n        nal_size--;\n\n    while (nal_size > 0 && *nal_data > 0) {\n        do{\n            sei_payload_type += *nal_data;\n            nal_data++;\n            nal_size--;\n        } while (nal_size > 0 && *nal_data == 0xFF);\n\n        if (!nal_size) {\n            av_log(avctx, AV_LOG_ERROR, \"Unexpected end of SEI NAL Unit parsing type.\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        do{\n            sei_payload_size += *nal_data;\n            nal_data++;\n            nal_size--;\n        } while (nal_size > 0 && *nal_data == 0xFF);\n\n        if (nal_size < sei_payload_size) {\n            av_log(avctx, AV_LOG_ERROR, \"Unexpected end of SEI NAL Unit parsing size.\\n\");\n            return AVERROR_INVALIDDATA;\n        }\n\n        nal_data += sei_payload_size;\n        nal_size -= sei_payload_size;\n    }\n\n    *sei_end = nal_data;\n\n    return nal_data - nal_start + 1;\n}\n\n/**\n * Copies the data inserting emulation prevention bytes as needed.\n * Existing data in the destination can be taken into account by providing\n * dst with a dst_offset > 0.\n *\n * @return The number of bytes copied on success. On failure, the negative of\n *         the number of bytes needed to copy src is returned.\n */\nstatic int copy_emulation_prev(const uint8_t *src,\n                               size_t         src_size,\n                               uint8_t       *dst,\n                               ssize_t        dst_offset,\n                               size_t         dst_size)\n{\n    int zeros = 0;\n    int wrote_bytes;\n    uint8_t* dst_start;\n    uint8_t* dst_end = dst + dst_size;\n    const uint8_t* src_end = src + src_size;\n    int start_at = dst_offset > 2 ? dst_offset - 2 : 0;\n    int i;\n    for (i = start_at; i < dst_offset && i < dst_size; i++) {\n        if (!dst[i])\n            zeros++;\n        else\n            zeros = 0;\n    }\n\n    dst += dst_offset;\n    dst_start = dst;\n    for (; src < src_end; src++, dst++) {\n        if (zeros == 2) {\n            int insert_ep3_byte = *src <= 3;\n            if (insert_ep3_byte) {\n                if (dst < dst_end)\n                    *dst = 3;\n                dst++;\n            }\n\n            zeros = 0;\n        }\n\n        if (dst < dst_end)\n            *dst = *src;\n\n        if (!*src)\n            zeros++;\n        else\n            zeros = 0;\n    }\n\n    wrote_bytes = dst - dst_start;\n\n    if (dst > dst_end)\n        return -wrote_bytes;\n\n    return wrote_bytes;\n}\n\nstatic int write_sei(const ExtraSEI *sei,\n                     int             sei_type,\n                     uint8_t        *dst,\n                     size_t          dst_size)\n{\n    uint8_t *sei_start = dst;\n    size_t remaining_sei_size = sei->size;\n    size_t remaining_dst_size = dst_size;\n    int header_bytes;\n    int bytes_written;\n    ssize_t offset;\n\n    if (!remaining_dst_size)\n        return AVERROR_BUFFER_TOO_SMALL;\n\n    while (sei_type && remaining_dst_size != 0) {\n        int sei_byte = sei_type > 255 ? 255 : sei_type;\n        *dst = sei_byte;\n\n        sei_type -= sei_byte;\n        dst++;\n        remaining_dst_size--;\n    }\n\n    if (!dst_size)\n        return AVERROR_BUFFER_TOO_SMALL;\n\n    while (remaining_sei_size && remaining_dst_size != 0) {\n        int size_byte = remaining_sei_size > 255 ? 255 : remaining_sei_size;\n        *dst = size_byte;\n\n        remaining_sei_size -= size_byte;\n        dst++;\n        remaining_dst_size--;\n    }\n\n    if (remaining_dst_size < sei->size)\n        return AVERROR_BUFFER_TOO_SMALL;\n\n    header_bytes = dst - sei_start;\n\n    offset = header_bytes;\n    bytes_written = copy_emulation_prev(sei->data,\n                                        sei->size,\n                                        sei_start,\n                                        offset,\n                                        dst_size);\n    if (bytes_written < 0)\n        return AVERROR_BUFFER_TOO_SMALL;\n\n    bytes_written += header_bytes;\n    return bytes_written;\n}\n\n/**\n * Copies NAL units and replaces length codes with\n * H.264 Annex B start codes. On failure, the contents of\n * dst_data may have been modified.\n *\n * @param length_code_size Byte length of each length code\n * @param sample_buffer NAL units prefixed with length codes.\n * @param sei Optional A53 closed captions SEI data.\n * @param dst_data Must be zeroed before calling this function.\n *                 Contains the copied NAL units prefixed with\n *                 start codes when the function returns\n *                 successfully.\n * @param dst_size Length of dst_data\n * @return 0 on success\n *         AVERROR_INVALIDDATA if length_code_size is invalid\n *         AVERROR_BUFFER_TOO_SMALL if dst_data is too small\n *         or if a length_code in src_data specifies data beyond\n *         the end of its buffer.\n */\nstatic int copy_replace_length_codes(\n    AVCodecContext *avctx,\n    size_t        length_code_size,\n    CMSampleBufferRef sample_buffer,\n    ExtraSEI      *sei,\n    uint8_t       *dst_data,\n    size_t        dst_size)\n{\n    size_t src_size = CMSampleBufferGetTotalSampleSize(sample_buffer);\n    size_t remaining_src_size = src_size;\n    size_t remaining_dst_size = dst_size;\n    size_t src_offset = 0;\n    int wrote_sei = 0;\n    int status;\n    uint8_t size_buf[4];\n    uint8_t nal_type;\n    CMBlockBufferRef block = CMSampleBufferGetDataBuffer(sample_buffer);\n\n    if (length_code_size > 4) {\n        return AVERROR_INVALIDDATA;\n    }\n\n    while (remaining_src_size > 0) {\n        size_t curr_src_len;\n        size_t curr_dst_len;\n        size_t box_len = 0;\n        size_t i;\n\n        uint8_t       *dst_box;\n\n        status = CMBlockBufferCopyDataBytes(block,\n                                            src_offset,\n                                            length_code_size,\n                                            size_buf);\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Cannot copy length: %d\\n\", status);\n            return AVERROR_EXTERNAL;\n        }\n\n        status = CMBlockBufferCopyDataBytes(block,\n                                            src_offset + length_code_size,\n                                            1,\n                                            &nal_type);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Cannot copy type: %d\\n\", status);\n            return AVERROR_EXTERNAL;\n        }\n\n        nal_type &= 0x1F;\n\n        for (i = 0; i < length_code_size; i++) {\n            box_len <<= 8;\n            box_len |= size_buf[i];\n        }\n\n        if (sei && !wrote_sei && is_post_sei_nal_type(nal_type)) {\n            //No SEI NAL unit - insert.\n            int wrote_bytes;\n\n            memcpy(dst_data, start_code, sizeof(start_code));\n            dst_data += sizeof(start_code);\n            remaining_dst_size -= sizeof(start_code);\n\n            *dst_data = H264_NAL_SEI;\n            dst_data++;\n            remaining_dst_size--;\n\n            wrote_bytes = write_sei(sei,\n                                    H264_SEI_TYPE_USER_DATA_REGISTERED,\n                                    dst_data,\n                                    remaining_dst_size);\n\n            if (wrote_bytes < 0)\n                return wrote_bytes;\n\n            remaining_dst_size -= wrote_bytes;\n            dst_data += wrote_bytes;\n\n            if (remaining_dst_size <= 0)\n                return AVERROR_BUFFER_TOO_SMALL;\n\n            *dst_data = 0x80;\n\n            dst_data++;\n            remaining_dst_size--;\n\n            wrote_sei = 1;\n        }\n\n        curr_src_len = box_len + length_code_size;\n        curr_dst_len = box_len + sizeof(start_code);\n\n        if (remaining_src_size < curr_src_len) {\n            return AVERROR_BUFFER_TOO_SMALL;\n        }\n\n        if (remaining_dst_size < curr_dst_len) {\n            return AVERROR_BUFFER_TOO_SMALL;\n        }\n\n        dst_box = dst_data + sizeof(start_code);\n\n        memcpy(dst_data, start_code, sizeof(start_code));\n        status = CMBlockBufferCopyDataBytes(block,\n                                            src_offset + length_code_size,\n                                            box_len,\n                                            dst_box);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Cannot copy data: %d\\n\", status);\n            return AVERROR_EXTERNAL;\n        }\n\n        if (sei && !wrote_sei && nal_type == H264_NAL_SEI) {\n            //Found SEI NAL unit - append.\n            int wrote_bytes;\n            int old_sei_length;\n            int extra_bytes;\n            uint8_t *new_sei;\n            old_sei_length = find_sei_end(avctx, dst_box, box_len, &new_sei);\n            if (old_sei_length < 0)\n                return status;\n\n            wrote_bytes = write_sei(sei,\n                                    H264_SEI_TYPE_USER_DATA_REGISTERED,\n                                    new_sei,\n                                    remaining_dst_size - old_sei_length);\n            if (wrote_bytes < 0)\n                return wrote_bytes;\n\n            if (new_sei + wrote_bytes >= dst_data + remaining_dst_size)\n                return AVERROR_BUFFER_TOO_SMALL;\n\n            new_sei[wrote_bytes++] = 0x80;\n            extra_bytes = wrote_bytes - (dst_box + box_len - new_sei);\n\n            dst_data += extra_bytes;\n            remaining_dst_size -= extra_bytes;\n\n            wrote_sei = 1;\n        }\n\n        src_offset += curr_src_len;\n        dst_data += curr_dst_len;\n\n        remaining_src_size -= curr_src_len;\n        remaining_dst_size -= curr_dst_len;\n    }\n\n    return 0;\n}\n\n/**\n * Returns a sufficient number of bytes to contain the sei data.\n * It may be greater than the minimum required.\n */\nstatic int get_sei_msg_bytes(const ExtraSEI* sei, int type){\n    int copied_size;\n    if (sei->size == 0)\n        return 0;\n\n    copied_size = -copy_emulation_prev(sei->data,\n                                       sei->size,\n                                       NULL,\n                                       0,\n                                       0);\n\n    if ((sei->size % 255) == 0) //may result in an extra byte\n        copied_size++;\n\n    return copied_size + sei->size / 255 + 1 + type / 255 + 1;\n}\n\nstatic int vtenc_cm_to_avpacket(\n    AVCodecContext    *avctx,\n    CMSampleBufferRef sample_buffer,\n    AVPacket          *pkt,\n    ExtraSEI          *sei)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n\n    int     status;\n    bool    is_key_frame;\n    bool    add_header;\n    size_t  length_code_size;\n    size_t  header_size = 0;\n    size_t  in_buf_size;\n    size_t  out_buf_size;\n    size_t  sei_nalu_size = 0;\n    int64_t dts_delta;\n    int64_t time_base_num;\n    int nalu_count;\n    CMTime  pts;\n    CMTime  dts;\n    CMVideoFormatDescriptionRef vid_fmt;\n\n\n    vtenc_get_frame_info(sample_buffer, &is_key_frame);\n    status = get_length_code_size(avctx, sample_buffer, &length_code_size);\n    if (status) return status;\n\n    add_header = is_key_frame && !(avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER);\n\n    if (add_header) {\n        vid_fmt = CMSampleBufferGetFormatDescription(sample_buffer);\n        if (!vid_fmt) {\n            av_log(avctx, AV_LOG_ERROR, \"Cannot get format description.\\n\");\n            return AVERROR_EXTERNAL;\n        }\n\n        int status = get_params_size(avctx, vid_fmt, &header_size);\n        if (status) return status;\n    }\n\n    status = count_nalus(length_code_size, sample_buffer, &nalu_count);\n    if(status)\n        return status;\n\n    if (sei) {\n        size_t msg_size = get_sei_msg_bytes(sei,\n                                            H264_SEI_TYPE_USER_DATA_REGISTERED);\n\n        sei_nalu_size = sizeof(start_code) + 1 + msg_size + 1;\n    }\n\n    in_buf_size = CMSampleBufferGetTotalSampleSize(sample_buffer);\n    out_buf_size = header_size +\n                   in_buf_size +\n                   sei_nalu_size +\n                   nalu_count * ((int)sizeof(start_code) - (int)length_code_size);\n\n    status = ff_alloc_packet2(avctx, pkt, out_buf_size, out_buf_size);\n    if (status < 0)\n        return status;\n\n    if (add_header) {\n        status = copy_param_sets(avctx, vid_fmt, pkt->data, out_buf_size);\n        if(status) return status;\n    }\n\n    status = copy_replace_length_codes(\n        avctx,\n        length_code_size,\n        sample_buffer,\n        sei,\n        pkt->data + header_size,\n        pkt->size - header_size\n    );\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error copying packet data: %d\\n\", status);\n        return status;\n    }\n\n    if (is_key_frame) {\n        pkt->flags |= AV_PKT_FLAG_KEY;\n    }\n\n    pts = CMSampleBufferGetPresentationTimeStamp(sample_buffer);\n    dts = CMSampleBufferGetDecodeTimeStamp      (sample_buffer);\n\n    if (CMTIME_IS_INVALID(dts)) {\n        if (!vtctx->has_b_frames) {\n            dts = pts;\n        } else {\n            av_log(avctx, AV_LOG_ERROR, \"DTS is invalid.\\n\");\n            return AVERROR_EXTERNAL;\n        }\n    }\n\n    dts_delta = vtctx->dts_delta >= 0 ? vtctx->dts_delta : 0;\n    time_base_num = avctx->time_base.num;\n    pkt->pts = pts.value / time_base_num;\n    pkt->dts = dts.value / time_base_num - dts_delta;\n    pkt->size = out_buf_size;\n\n    return 0;\n}\n\n/*\n * contiguous_buf_size is 0 if not contiguous, and the size of the buffer\n * containing all planes if so.\n */\nstatic int get_cv_pixel_info(\n    AVCodecContext *avctx,\n    const AVFrame  *frame,\n    int            *color,\n    int            *plane_count,\n    size_t         *widths,\n    size_t         *heights,\n    size_t         *strides,\n    size_t         *contiguous_buf_size)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    int av_format       = frame->format;\n    int av_color_range  = frame->color_range;\n    int i;\n    int range_guessed;\n    int status;\n\n    status = get_cv_pixel_format(avctx, av_format, av_color_range, color, &range_guessed);\n    if (status) {\n        av_log(avctx,\n            AV_LOG_ERROR,\n            \"Could not get pixel format for color format '%s' range '%s'.\\n\",\n            av_get_pix_fmt_name(av_format),\n            av_color_range > AVCOL_RANGE_UNSPECIFIED &&\n            av_color_range < AVCOL_RANGE_NB ?\n               av_color_range_name(av_color_range) :\n               \"Unknown\");\n\n        return AVERROR(EINVAL);\n    }\n\n    if (range_guessed) {\n        if (!vtctx->warned_color_range) {\n            vtctx->warned_color_range = true;\n            av_log(avctx,\n                   AV_LOG_WARNING,\n                   \"Color range not set for %s. Using MPEG range.\\n\",\n                   av_get_pix_fmt_name(av_format));\n        }\n    }\n\n    switch (av_format) {\n    case AV_PIX_FMT_NV12:\n        *plane_count = 2;\n\n        widths [0] = avctx->width;\n        heights[0] = avctx->height;\n        strides[0] = frame ? frame->linesize[0] : avctx->width;\n\n        widths [1] = (avctx->width  + 1) / 2;\n        heights[1] = (avctx->height + 1) / 2;\n        strides[1] = frame ? frame->linesize[1] : (avctx->width + 1) & -2;\n        break;\n\n    case AV_PIX_FMT_YUV420P:\n        *plane_count = 3;\n\n        widths [0] = avctx->width;\n        heights[0] = avctx->height;\n        strides[0] = frame ? frame->linesize[0] : avctx->width;\n\n        widths [1] = (avctx->width  + 1) / 2;\n        heights[1] = (avctx->height + 1) / 2;\n        strides[1] = frame ? frame->linesize[1] : (avctx->width + 1) / 2;\n\n        widths [2] = (avctx->width  + 1) / 2;\n        heights[2] = (avctx->height + 1) / 2;\n        strides[2] = frame ? frame->linesize[2] : (avctx->width + 1) / 2;\n        break;\n\n    default:\n        av_log(\n               avctx,\n               AV_LOG_ERROR,\n               \"Could not get frame format info for color %d range %d.\\n\",\n               av_format,\n               av_color_range);\n\n        return AVERROR(EINVAL);\n    }\n\n    *contiguous_buf_size = 0;\n    for (i = 0; i < *plane_count; i++) {\n        if (i < *plane_count - 1 &&\n            frame->data[i] + strides[i] * heights[i] != frame->data[i + 1]) {\n            *contiguous_buf_size = 0;\n            break;\n        }\n\n        *contiguous_buf_size += strides[i] * heights[i];\n    }\n\n    return 0;\n}\n\n#if !TARGET_OS_IPHONE\n//Not used on iOS - frame is always copied.\nstatic void free_avframe(\n    void       *release_ctx,\n    const void *data,\n    size_t      size,\n    size_t      plane_count,\n    const void *plane_addresses[])\n{\n    AVFrame *frame = release_ctx;\n    av_frame_free(&frame);\n}\n#else\n//Not used on OSX - frame is never copied.\nstatic int copy_avframe_to_pixel_buffer(AVCodecContext   *avctx,\n                                        const AVFrame    *frame,\n                                        CVPixelBufferRef cv_img,\n                                        const size_t     *plane_strides,\n                                        const size_t     *plane_rows)\n{\n    int i, j;\n    size_t plane_count;\n    int status;\n    int rows;\n    int src_stride;\n    int dst_stride;\n    uint8_t *src_addr;\n    uint8_t *dst_addr;\n    size_t copy_bytes;\n\n    status = CVPixelBufferLockBaseAddress(cv_img, 0);\n    if (status) {\n        av_log(\n            avctx,\n            AV_LOG_ERROR,\n            \"Error: Could not lock base address of CVPixelBuffer: %d.\\n\",\n            status\n        );\n    }\n\n    if (CVPixelBufferIsPlanar(cv_img)) {\n        plane_count = CVPixelBufferGetPlaneCount(cv_img);\n        for (i = 0; frame->data[i]; i++) {\n            if (i == plane_count) {\n                CVPixelBufferUnlockBaseAddress(cv_img, 0);\n                av_log(avctx,\n                    AV_LOG_ERROR,\n                    \"Error: different number of planes in AVFrame and CVPixelBuffer.\\n\"\n                );\n\n                return AVERROR_EXTERNAL;\n            }\n\n            dst_addr = (uint8_t*)CVPixelBufferGetBaseAddressOfPlane(cv_img, i);\n            src_addr = (uint8_t*)frame->data[i];\n            dst_stride = CVPixelBufferGetBytesPerRowOfPlane(cv_img, i);\n            src_stride = plane_strides[i];\n            rows = plane_rows[i];\n\n            if (dst_stride == src_stride) {\n                memcpy(dst_addr, src_addr, src_stride * rows);\n            } else {\n                copy_bytes = dst_stride < src_stride ? dst_stride : src_stride;\n\n                for (j = 0; j < rows; j++) {\n                    memcpy(dst_addr + j * dst_stride, src_addr + j * src_stride, copy_bytes);\n                }\n            }\n        }\n    } else {\n        if (frame->data[1]) {\n            CVPixelBufferUnlockBaseAddress(cv_img, 0);\n            av_log(avctx,\n                AV_LOG_ERROR,\n                \"Error: different number of planes in AVFrame and non-planar CVPixelBuffer.\\n\"\n            );\n\n            return AVERROR_EXTERNAL;\n        }\n\n        dst_addr = (uint8_t*)CVPixelBufferGetBaseAddress(cv_img);\n        src_addr = (uint8_t*)frame->data[0];\n        dst_stride = CVPixelBufferGetBytesPerRow(cv_img);\n        src_stride = plane_strides[0];\n        rows = plane_rows[0];\n\n        if (dst_stride == src_stride) {\n            memcpy(dst_addr, src_addr, src_stride * rows);\n        } else {\n            copy_bytes = dst_stride < src_stride ? dst_stride : src_stride;\n\n            for (j = 0; j < rows; j++) {\n                memcpy(dst_addr + j * dst_stride, src_addr + j * src_stride, copy_bytes);\n            }\n        }\n    }\n\n    status = CVPixelBufferUnlockBaseAddress(cv_img, 0);\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error: Could not unlock CVPixelBuffer base address: %d.\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    return 0;\n}\n#endif //!TARGET_OS_IPHONE\n\nstatic int create_cv_pixel_buffer(AVCodecContext   *avctx,\n                                  const AVFrame    *frame,\n                                  CVPixelBufferRef *cv_img)\n{\n    int plane_count;\n    int color;\n    size_t widths [AV_NUM_DATA_POINTERS];\n    size_t heights[AV_NUM_DATA_POINTERS];\n    size_t strides[AV_NUM_DATA_POINTERS];\n    int status;\n    size_t contiguous_buf_size;\n#if TARGET_OS_IPHONE\n    CVPixelBufferPoolRef pix_buf_pool;\n    VTEncContext* vtctx = avctx->priv_data;\n#else\n    CFMutableDictionaryRef pix_buf_attachments = CFDictionaryCreateMutable(\n                                                   kCFAllocatorDefault,\n                                                   10,\n                                                   &kCFCopyStringDictionaryKeyCallBacks,\n                                                   &kCFTypeDictionaryValueCallBacks);\n\n    if (!pix_buf_attachments) return AVERROR(ENOMEM);\n#endif\n\n    if (avctx->pix_fmt == AV_PIX_FMT_VIDEOTOOLBOX) {\n        av_assert0(frame->format == AV_PIX_FMT_VIDEOTOOLBOX);\n\n        *cv_img = (CVPixelBufferRef)frame->data[3];\n        av_assert0(*cv_img);\n\n        CFRetain(*cv_img);\n        return 0;\n    }\n\n    memset(widths,  0, sizeof(widths));\n    memset(heights, 0, sizeof(heights));\n    memset(strides, 0, sizeof(strides));\n\n    status = get_cv_pixel_info(\n        avctx,\n        frame,\n        &color,\n        &plane_count,\n        widths,\n        heights,\n        strides,\n        &contiguous_buf_size\n    );\n\n    if (status) {\n        av_log(\n            avctx,\n            AV_LOG_ERROR,\n            \"Error: Cannot convert format %d color_range %d: %d\\n\",\n            frame->format,\n            frame->color_range,\n            status\n        );\n\n        return AVERROR_EXTERNAL;\n    }\n\n#if TARGET_OS_IPHONE\n    pix_buf_pool = VTCompressionSessionGetPixelBufferPool(vtctx->session);\n    if (!pix_buf_pool) {\n        /* On iOS, the VT session is invalidated when the APP switches from\n         * foreground to background and vice versa. Fetch the actual error code\n         * of the VT session to detect that case and restart the VT session\n         * accordingly. */\n        OSStatus vtstatus;\n\n        vtstatus = VTCompressionSessionPrepareToEncodeFrames(vtctx->session);\n        if (vtstatus == kVTInvalidSessionErr) {\n            CFRelease(vtctx->session);\n            vtctx->session = NULL;\n            status = vtenc_configure_encoder(avctx);\n            if (status == 0)\n                pix_buf_pool = VTCompressionSessionGetPixelBufferPool(vtctx->session);\n        }\n        if (!pix_buf_pool) {\n            av_log(avctx, AV_LOG_ERROR, \"Could not get pixel buffer pool.\\n\");\n            return AVERROR_EXTERNAL;\n        }\n        else\n            av_log(avctx, AV_LOG_WARNING, \"VT session restarted because of a \"\n                   \"kVTInvalidSessionErr error.\\n\");\n    }\n\n    status = CVPixelBufferPoolCreatePixelBuffer(NULL,\n                                                pix_buf_pool,\n                                                cv_img);\n\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Could not create pixel buffer from pool: %d.\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    status = copy_avframe_to_pixel_buffer(avctx, frame, *cv_img, strides, heights);\n    if (status) {\n        CFRelease(*cv_img);\n        *cv_img = NULL;\n        return status;\n    }\n#else\n    AVFrame *enc_frame = av_frame_alloc();\n    if (!enc_frame) return AVERROR(ENOMEM);\n\n    status = av_frame_ref(enc_frame, frame);\n    if (status) {\n        av_frame_free(&enc_frame);\n        return status;\n    }\n\n    status = CVPixelBufferCreateWithPlanarBytes(\n        kCFAllocatorDefault,\n        enc_frame->width,\n        enc_frame->height,\n        color,\n        NULL,\n        contiguous_buf_size,\n        plane_count,\n        (void **)enc_frame->data,\n        widths,\n        heights,\n        strides,\n        free_avframe,\n        enc_frame,\n        NULL,\n        cv_img\n    );\n\n    add_color_attr(avctx, pix_buf_attachments);\n    CVBufferSetAttachments(*cv_img, pix_buf_attachments, kCVAttachmentMode_ShouldPropagate);\n    CFRelease(pix_buf_attachments);\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error: Could not create CVPixelBuffer: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n#endif\n\n    return 0;\n}\n\nstatic int create_encoder_dict_h264(const AVFrame *frame,\n                                    CFDictionaryRef* dict_out)\n{\n    CFDictionaryRef dict = NULL;\n    if (frame->pict_type == AV_PICTURE_TYPE_I) {\n        const void *keys[] = { kVTEncodeFrameOptionKey_ForceKeyFrame };\n        const void *vals[] = { kCFBooleanTrue };\n\n        dict = CFDictionaryCreate(NULL, keys, vals, 1, NULL, NULL);\n        if(!dict) return AVERROR(ENOMEM);\n    }\n\n    *dict_out = dict;\n    return 0;\n}\n\nstatic int vtenc_send_frame(AVCodecContext *avctx,\n                            VTEncContext   *vtctx,\n                            const AVFrame  *frame)\n{\n    CMTime time;\n    CFDictionaryRef frame_dict;\n    CVPixelBufferRef cv_img = NULL;\n    AVFrameSideData *side_data = NULL;\n    ExtraSEI *sei = NULL;\n    int status = create_cv_pixel_buffer(avctx, frame, &cv_img);\n\n    if (status) return status;\n\n    status = create_encoder_dict_h264(frame, &frame_dict);\n    if (status) {\n        CFRelease(cv_img);\n        return status;\n    }\n\n    side_data = av_frame_get_side_data(frame, AV_FRAME_DATA_A53_CC);\n    if (vtctx->a53_cc && side_data && side_data->size) {\n        sei = av_mallocz(sizeof(*sei));\n        if (!sei) {\n            av_log(avctx, AV_LOG_ERROR, \"Not enough memory for closed captions, skipping\\n\");\n        } else {\n            int ret = ff_alloc_a53_sei(frame, 0, &sei->data, &sei->size);\n            if (ret < 0) {\n                av_log(avctx, AV_LOG_ERROR, \"Not enough memory for closed captions, skipping\\n\");\n                av_free(sei);\n                sei = NULL;\n            }\n        }\n    }\n\n    time = CMTimeMake(frame->pts * avctx->time_base.num, avctx->time_base.den);\n    status = VTCompressionSessionEncodeFrame(\n        vtctx->session,\n        cv_img,\n        time,\n        kCMTimeInvalid,\n        frame_dict,\n        sei,\n        NULL\n    );\n\n    if (frame_dict) CFRelease(frame_dict);\n    CFRelease(cv_img);\n\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error: cannot encode frame: %d\\n\", status);\n        return AVERROR_EXTERNAL;\n    }\n\n    return 0;\n}\n\nstatic av_cold int vtenc_frame(\n    AVCodecContext *avctx,\n    AVPacket       *pkt,\n    const AVFrame  *frame,\n    int            *got_packet)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    bool get_frame;\n    int status;\n    CMSampleBufferRef buf = NULL;\n    ExtraSEI *sei = NULL;\n\n    if (frame) {\n        status = vtenc_send_frame(avctx, vtctx, frame);\n\n        if (status) {\n            status = AVERROR_EXTERNAL;\n            goto end_nopkt;\n        }\n\n        if (vtctx->frame_ct_in == 0) {\n            vtctx->first_pts = frame->pts;\n        } else if(vtctx->frame_ct_in == 1 && vtctx->has_b_frames) {\n            vtctx->dts_delta = frame->pts - vtctx->first_pts;\n        }\n\n        vtctx->frame_ct_in++;\n    } else if(!vtctx->flushing) {\n        vtctx->flushing = true;\n\n        status = VTCompressionSessionCompleteFrames(vtctx->session,\n                                                    kCMTimeIndefinite);\n\n        if (status) {\n            av_log(avctx, AV_LOG_ERROR, \"Error flushing frames: %d\\n\", status);\n            status = AVERROR_EXTERNAL;\n            goto end_nopkt;\n        }\n    }\n\n    *got_packet = 0;\n    get_frame = vtctx->dts_delta >= 0 || !frame;\n    if (!get_frame) {\n        status = 0;\n        goto end_nopkt;\n    }\n\n    status = vtenc_q_pop(vtctx, !frame, &buf, &sei);\n    if (status) goto end_nopkt;\n    if (!buf)   goto end_nopkt;\n\n    status = vtenc_cm_to_avpacket(avctx, buf, pkt, sei);\n    if (sei) {\n        if (sei->data) av_free(sei->data);\n        av_free(sei);\n    }\n    CFRelease(buf);\n    if (status) goto end_nopkt;\n\n    *got_packet = 1;\n    return 0;\n\nend_nopkt:\n    av_packet_unref(pkt);\n    return status;\n}\n\nstatic int vtenc_populate_extradata(AVCodecContext   *avctx,\n                                    CMVideoCodecType codec_type,\n                                    CFStringRef      profile_level,\n                                    CFNumberRef      gamma_level,\n                                    CFDictionaryRef  enc_info,\n                                    CFDictionaryRef  pixel_buffer_info)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n    AVFrame *frame = av_frame_alloc();\n    int y_size = avctx->width * avctx->height;\n    int chroma_size = (avctx->width / 2) * (avctx->height / 2);\n    CMSampleBufferRef buf = NULL;\n    int status;\n\n    if (!frame)\n        return AVERROR(ENOMEM);\n\n    frame->buf[0] = av_buffer_alloc(y_size + 2 * chroma_size);\n\n    if(!frame->buf[0]){\n        status = AVERROR(ENOMEM);\n        goto pe_cleanup;\n    }\n\n    status = vtenc_create_encoder(avctx,\n                                  codec_type,\n                                  profile_level,\n                                  gamma_level,\n                                  enc_info,\n                                  pixel_buffer_info,\n                                  &vtctx->session);\n    if (status)\n        goto pe_cleanup;\n\n    frame->data[0] = frame->buf[0]->data;\n    memset(frame->data[0],   0,      y_size);\n\n    frame->data[1] = frame->buf[0]->data + y_size;\n    memset(frame->data[1], 128, chroma_size);\n\n\n    if (avctx->pix_fmt == AV_PIX_FMT_YUV420P) {\n        frame->data[2] = frame->buf[0]->data + y_size + chroma_size;\n        memset(frame->data[2], 128, chroma_size);\n    }\n\n    frame->linesize[0] = avctx->width;\n\n    if (avctx->pix_fmt == AV_PIX_FMT_YUV420P) {\n        frame->linesize[1] =\n        frame->linesize[2] = (avctx->width + 1) / 2;\n    } else {\n        frame->linesize[1] = (avctx->width + 1) / 2;\n    }\n\n    frame->format          = avctx->pix_fmt;\n    frame->width           = avctx->width;\n    frame->height          = avctx->height;\n    frame->colorspace      = avctx->colorspace;\n    frame->color_range     = avctx->color_range;\n    frame->color_trc       = avctx->color_trc;\n    frame->color_primaries = avctx->color_primaries;\n\n    frame->pts = 0;\n    status = vtenc_send_frame(avctx, vtctx, frame);\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"Error sending frame: %d\\n\", status);\n        goto pe_cleanup;\n    }\n\n    //Populates extradata - output frames are flushed and param sets are available.\n    status = VTCompressionSessionCompleteFrames(vtctx->session,\n                                                kCMTimeIndefinite);\n\n    if (status)\n        goto pe_cleanup;\n\n    status = vtenc_q_pop(vtctx, 0, &buf, NULL);\n    if (status) {\n        av_log(avctx, AV_LOG_ERROR, \"popping: %d\\n\", status);\n        goto pe_cleanup;\n    }\n\n    CFRelease(buf);\n\n\n\npe_cleanup:\n    if(vtctx->session)\n        CFRelease(vtctx->session);\n\n    vtctx->session = NULL;\n    vtctx->frame_ct_out = 0;\n\n    av_frame_unref(frame);\n    av_frame_free(&frame);\n\n    av_assert0(status != 0 || (avctx->extradata && avctx->extradata_size > 0));\n\n    return status;\n}\n\nstatic av_cold int vtenc_close(AVCodecContext *avctx)\n{\n    VTEncContext *vtctx = avctx->priv_data;\n\n    pthread_cond_destroy(&vtctx->cv_sample_sent);\n    pthread_mutex_destroy(&vtctx->lock);\n\n    if(!vtctx->session) return 0;\n\n    VTCompressionSessionCompleteFrames(vtctx->session,\n                                       kCMTimeIndefinite);\n    clear_frame_queue(vtctx);\n    CFRelease(vtctx->session);\n    vtctx->session = NULL;\n\n    if (vtctx->color_primaries) {\n        CFRelease(vtctx->color_primaries);\n        vtctx->color_primaries = NULL;\n    }\n\n    if (vtctx->transfer_function) {\n        CFRelease(vtctx->transfer_function);\n        vtctx->transfer_function = NULL;\n    }\n\n    if (vtctx->ycbcr_matrix) {\n        CFRelease(vtctx->ycbcr_matrix);\n        vtctx->ycbcr_matrix = NULL;\n    }\n\n    return 0;\n}\n\nstatic const enum AVPixelFormat pix_fmts[] = {\n    AV_PIX_FMT_VIDEOTOOLBOX,\n    AV_PIX_FMT_NV12,\n    AV_PIX_FMT_YUV420P,\n    AV_PIX_FMT_NONE\n};\n\n#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM\n#define COMMON_OPTIONS \\\n    { \"allow_sw\", \"Allow software encoding\", OFFSET(allow_sw), AV_OPT_TYPE_BOOL, \\\n        { .i64 = 0 }, 0, 1, VE }, \\\n    { \"realtime\", \"Hint that encoding should happen in real-time if not faster (e.g. capturing from camera).\", \\\n        OFFSET(realtime), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VE }, \\\n    { \"frames_before\", \"Other frames will come before the frames in this session. This helps smooth concatenation issues.\", \\\n        OFFSET(frames_before), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VE }, \\\n    { \"frames_after\", \"Other frames will come after the frames in this session. This helps smooth concatenation issues.\", \\\n        OFFSET(frames_after), AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VE },\n\n#define OFFSET(x) offsetof(VTEncContext, x)\nstatic const AVOption h264_options[] = {\n    { \"profile\", \"Profile\", OFFSET(profile), AV_OPT_TYPE_INT, { .i64 = H264_PROF_AUTO }, H264_PROF_AUTO, H264_PROF_COUNT, VE, \"profile\" },\n    { \"baseline\", \"Baseline Profile\", 0, AV_OPT_TYPE_CONST, { .i64 = H264_PROF_BASELINE }, INT_MIN, INT_MAX, VE, \"profile\" },\n    { \"main\",     \"Main Profile\",     0, AV_OPT_TYPE_CONST, { .i64 = H264_PROF_MAIN     }, INT_MIN, INT_MAX, VE, \"profile\" },\n    { \"high\",     \"High Profile\",     0, AV_OPT_TYPE_CONST, { .i64 = H264_PROF_HIGH     }, INT_MIN, INT_MAX, VE, \"profile\" },\n\n    { \"level\", \"Level\", OFFSET(level), AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 52, VE, \"level\" },\n    { \"1.3\", \"Level 1.3, only available with Baseline Profile\", 0, AV_OPT_TYPE_CONST, { .i64 = 13 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"3.0\", \"Level 3.0\", 0, AV_OPT_TYPE_CONST, { .i64 = 30 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"3.1\", \"Level 3.1\", 0, AV_OPT_TYPE_CONST, { .i64 = 31 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"3.2\", \"Level 3.2\", 0, AV_OPT_TYPE_CONST, { .i64 = 32 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"4.0\", \"Level 4.0\", 0, AV_OPT_TYPE_CONST, { .i64 = 40 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"4.1\", \"Level 4.1\", 0, AV_OPT_TYPE_CONST, { .i64 = 41 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"4.2\", \"Level 4.2\", 0, AV_OPT_TYPE_CONST, { .i64 = 42 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"5.0\", \"Level 5.0\", 0, AV_OPT_TYPE_CONST, { .i64 = 50 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"5.1\", \"Level 5.1\", 0, AV_OPT_TYPE_CONST, { .i64 = 51 }, INT_MIN, INT_MAX, VE, \"level\" },\n    { \"5.2\", \"Level 5.2\", 0, AV_OPT_TYPE_CONST, { .i64 = 52 }, INT_MIN, INT_MAX, VE, \"level\" },\n\n    { \"coder\", \"Entropy coding\", OFFSET(entropy), AV_OPT_TYPE_INT, { .i64 = VT_ENTROPY_NOT_SET }, VT_ENTROPY_NOT_SET, VT_CABAC, VE, \"coder\" },\n    { \"cavlc\", \"CAVLC entropy coding\", 0, AV_OPT_TYPE_CONST, { .i64 = VT_CAVLC }, INT_MIN, INT_MAX, VE, \"coder\" },\n    { \"vlc\",   \"CAVLC entropy coding\", 0, AV_OPT_TYPE_CONST, { .i64 = VT_CAVLC }, INT_MIN, INT_MAX, VE, \"coder\" },\n    { \"cabac\", \"CABAC entropy coding\", 0, AV_OPT_TYPE_CONST, { .i64 = VT_CABAC }, INT_MIN, INT_MAX, VE, \"coder\" },\n    { \"ac\",    \"CABAC entropy coding\", 0, AV_OPT_TYPE_CONST, { .i64 = VT_CABAC }, INT_MIN, INT_MAX, VE, \"coder\" },\n\n    { \"a53cc\", \"Use A53 Closed Captions (if available)\", OFFSET(a53_cc), AV_OPT_TYPE_BOOL, {.i64 = 1}, 0, 1, VE },\n\n    COMMON_OPTIONS\n    { NULL },\n};\n\nstatic const AVClass h264_videotoolbox_class = {\n    .class_name = \"h264_videotoolbox\",\n    .item_name  = av_default_item_name,\n    .option     = h264_options,\n    .version    = LIBAVUTIL_VERSION_INT,\n};\n\nAVCodec ff_h264_videotoolbox_encoder = {\n    .name             = \"h264_videotoolbox\",\n    .long_name        = NULL_IF_CONFIG_SMALL(\"VideoToolbox H.264 Encoder\"),\n    .type             = AVMEDIA_TYPE_VIDEO,\n    .id               = AV_CODEC_ID_H264,\n    .priv_data_size   = sizeof(VTEncContext),\n    .pix_fmts         = pix_fmts,\n    .init             = vtenc_init,\n    .encode2          = vtenc_frame,\n    .close            = vtenc_close,\n    .capabilities     = AV_CODEC_CAP_DELAY,\n    .priv_class       = &h264_videotoolbox_class,\n    .caps_internal    = FF_CODEC_CAP_INIT_THREADSAFE |\n                        FF_CODEC_CAP_INIT_CLEANUP,\n};\n\nstatic const AVOption hevc_options[] = {\n    { \"profile\", \"Profile\", OFFSET(profile), AV_OPT_TYPE_INT, { .i64 = HEVC_PROF_AUTO }, HEVC_PROF_AUTO, HEVC_PROF_COUNT, VE, \"profile\" },\n    { \"main\",     \"Main Profile\",     0, AV_OPT_TYPE_CONST, { .i64 = HEVC_PROF_MAIN   }, INT_MIN, INT_MAX, VE, \"profile\" },\n    { \"main10\",   \"Main10 Profile\",   0, AV_OPT_TYPE_CONST, { .i64 = HEVC_PROF_MAIN10 }, INT_MIN, INT_MAX, VE, \"profile\" },\n\n    COMMON_OPTIONS\n    { NULL },\n};\n\nstatic const AVClass hevc_videotoolbox_class = {\n    .class_name = \"hevc_videotoolbox\",\n    .item_name  = av_default_item_name,\n    .option     = hevc_options,\n    .version    = LIBAVUTIL_VERSION_INT,\n};\n\nAVCodec ff_hevc_videotoolbox_encoder = {\n    .name             = \"hevc_videotoolbox\",\n    .long_name        = NULL_IF_CONFIG_SMALL(\"VideoToolbox H.265 Encoder\"),\n    .type             = AVMEDIA_TYPE_VIDEO,\n    .id               = AV_CODEC_ID_HEVC,\n    .priv_data_size   = sizeof(VTEncContext),\n    .pix_fmts         = pix_fmts,\n    .init             = vtenc_init,\n    .encode2          = vtenc_frame,\n    .close            = vtenc_close,\n    .capabilities     = AV_CODEC_CAP_DELAY | AV_CODEC_CAP_HARDWARE,\n    .priv_class       = &hevc_videotoolbox_class,\n    .caps_internal    = FF_CODEC_CAP_INIT_THREADSAFE |\n                        FF_CODEC_CAP_INIT_CLEANUP,\n    .wrapper_name     = \"videotoolbox\",\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavcodec/amfenc.c": "/*\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#include \"config.h\"\n\n#include \"libavutil/avassert.h\"\n#include \"libavutil/imgutils.h\"\n#include \"libavutil/hwcontext.h\"\n#if CONFIG_D3D11VA\n#include \"libavutil/hwcontext_d3d11va.h\"\n#endif\n#if CONFIG_DXVA2\n#define COBJMACROS\n#include \"libavutil/hwcontext_dxva2.h\"\n#endif\n#include \"libavutil/mem.h\"\n#include \"libavutil/pixdesc.h\"\n#include \"libavutil/time.h\"\n\n#include \"amfenc.h\"\n#include \"internal.h\"\n\n#if CONFIG_D3D11VA\n#include <d3d11.h>\n#endif\n\n#ifdef _WIN32\n#include \"compat/w32dlfcn.h\"\n#else\n#include <dlfcn.h>\n#endif\n\n#define FFMPEG_AMF_WRITER_ID L\"ffmpeg_amf\"\n\n#define PTS_PROP L\"PtsProp\"\n\nconst enum AVPixelFormat ff_amf_pix_fmts[] = {\n    AV_PIX_FMT_NV12,\n    AV_PIX_FMT_YUV420P,\n#if CONFIG_D3D11VA\n    AV_PIX_FMT_D3D11,\n#endif\n#if CONFIG_DXVA2\n    AV_PIX_FMT_DXVA2_VLD,\n#endif\n    AV_PIX_FMT_NONE\n};\n\ntypedef struct FormatMap {\n    enum AVPixelFormat       av_format;\n    enum AMF_SURFACE_FORMAT  amf_format;\n} FormatMap;\n\nstatic const FormatMap format_map[] =\n{\n    { AV_PIX_FMT_NONE,       AMF_SURFACE_UNKNOWN },\n    { AV_PIX_FMT_NV12,       AMF_SURFACE_NV12 },\n    { AV_PIX_FMT_BGR0,       AMF_SURFACE_BGRA },\n    { AV_PIX_FMT_RGB0,       AMF_SURFACE_RGBA },\n    { AV_PIX_FMT_GRAY8,      AMF_SURFACE_GRAY8 },\n    { AV_PIX_FMT_YUV420P,    AMF_SURFACE_YUV420P },\n    { AV_PIX_FMT_YUYV422,    AMF_SURFACE_YUY2 },\n};\n\nstatic enum AMF_SURFACE_FORMAT amf_av_to_amf_format(enum AVPixelFormat fmt)\n{\n    int i;\n    for (i = 0; i < amf_countof(format_map); i++) {\n        if (format_map[i].av_format == fmt) {\n            return format_map[i].amf_format;\n        }\n    }\n    return AMF_SURFACE_UNKNOWN;\n}\n\nstatic void AMF_CDECL_CALL AMFTraceWriter_Write(AMFTraceWriter *pThis,\n    const wchar_t *scope, const wchar_t *message)\n{\n    AmfTraceWriter *tracer = (AmfTraceWriter*)pThis;\n    av_log(tracer->avctx, AV_LOG_DEBUG, \"%ls: %ls\", scope, message); // \\n is provided from AMF\n}\n\nstatic void AMF_CDECL_CALL AMFTraceWriter_Flush(AMFTraceWriter *pThis)\n{\n}\n\nstatic AMFTraceWriterVtbl tracer_vtbl =\n{\n    .Write = AMFTraceWriter_Write,\n    .Flush = AMFTraceWriter_Flush,\n};\n\nstatic int amf_load_library(AVCodecContext *avctx)\n{\n    AmfContext        *ctx = avctx->priv_data;\n    AMFInit_Fn         init_fun;\n    AMFQueryVersion_Fn version_fun;\n    AMF_RESULT         res;\n\n    ctx->delayed_frame = av_frame_alloc();\n    if (!ctx->delayed_frame) {\n        return AVERROR(ENOMEM);\n    }\n    // hardcoded to current HW queue size - will realloc in timestamp_queue_enqueue() if too small\n    ctx->timestamp_list = av_fifo_alloc((avctx->max_b_frames + 16) * sizeof(int64_t));\n    if (!ctx->timestamp_list) {\n        return AVERROR(ENOMEM);\n    }\n    ctx->dts_delay = 0;\n\n\n    ctx->library = dlopen(AMF_DLL_NAMEA, RTLD_NOW | RTLD_LOCAL);\n    AMF_RETURN_IF_FALSE(ctx, ctx->library != NULL,\n        AVERROR_UNKNOWN, \"DLL %s failed to open\\n\", AMF_DLL_NAMEA);\n\n    init_fun = (AMFInit_Fn)dlsym(ctx->library, AMF_INIT_FUNCTION_NAME);\n    AMF_RETURN_IF_FALSE(ctx, init_fun != NULL, AVERROR_UNKNOWN, \"DLL %s failed to find function %s\\n\", AMF_DLL_NAMEA, AMF_INIT_FUNCTION_NAME);\n\n    version_fun = (AMFQueryVersion_Fn)dlsym(ctx->library, AMF_QUERY_VERSION_FUNCTION_NAME);\n    AMF_RETURN_IF_FALSE(ctx, version_fun != NULL, AVERROR_UNKNOWN, \"DLL %s failed to find function %s\\n\", AMF_DLL_NAMEA, AMF_QUERY_VERSION_FUNCTION_NAME);\n\n    res = version_fun(&ctx->version);\n    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"%s failed with error %d\\n\", AMF_QUERY_VERSION_FUNCTION_NAME, res);\n    res = init_fun(AMF_FULL_VERSION, &ctx->factory);\n    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"%s failed with error %d\\n\", AMF_INIT_FUNCTION_NAME, res);\n    res = ctx->factory->pVtbl->GetTrace(ctx->factory, &ctx->trace);\n    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"GetTrace() failed with error %d\\n\", res);\n    res = ctx->factory->pVtbl->GetDebug(ctx->factory, &ctx->debug);\n    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"GetDebug() failed with error %d\\n\", res);\n    return 0;\n}\n\n#if CONFIG_D3D11VA\nstatic int amf_init_from_d3d11_device(AVCodecContext *avctx, AVD3D11VADeviceContext *hwctx)\n{\n    AmfContext *ctx = avctx->priv_data;\n    AMF_RESULT res;\n\n    res = ctx->context->pVtbl->InitDX11(ctx->context, hwctx->device, AMF_DX11_1);\n    if (res != AMF_OK) {\n        if (res == AMF_NOT_SUPPORTED)\n            av_log(avctx, AV_LOG_ERROR, \"AMF via D3D11 is not supported on the given device.\\n\");\n        else\n            av_log(avctx, AV_LOG_ERROR, \"AMF failed to initialise on the given D3D11 device: %d.\\n\", res);\n        return AVERROR(ENODEV);\n    }\n\n    return 0;\n}\n#endif\n\n#if CONFIG_DXVA2\nstatic int amf_init_from_dxva2_device(AVCodecContext *avctx, AVDXVA2DeviceContext *hwctx)\n{\n    AmfContext *ctx = avctx->priv_data;\n    HANDLE device_handle;\n    IDirect3DDevice9 *device;\n    HRESULT hr;\n    AMF_RESULT res;\n    int ret;\n\n    hr = IDirect3DDeviceManager9_OpenDeviceHandle(hwctx->devmgr, &device_handle);\n    if (FAILED(hr)) {\n        av_log(avctx, AV_LOG_ERROR, \"Failed to open device handle for Direct3D9 device: %lx.\\n\", (unsigned long)hr);\n        return AVERROR_EXTERNAL;\n    }\n\n    hr = IDirect3DDeviceManager9_LockDevice(hwctx->devmgr, device_handle, &device, FALSE);\n    if (SUCCEEDED(hr)) {\n        IDirect3DDeviceManager9_UnlockDevice(hwctx->devmgr, device_handle, FALSE);\n        ret = 0;\n    } else {\n        av_log(avctx, AV_LOG_ERROR, \"Failed to lock device handle for Direct3D9 device: %lx.\\n\", (unsigned long)hr);\n        ret = AVERROR_EXTERNAL;\n    }\n\n    IDirect3DDeviceManager9_CloseDeviceHandle(hwctx->devmgr, device_handle);\n\n    if (ret < 0)\n        return ret;\n\n    res = ctx->context->pVtbl->InitDX9(ctx->context, device);\n\n    IDirect3DDevice9_Release(device);\n\n    if (res != AMF_OK) {\n        if (res == AMF_NOT_SUPPORTED)\n            av_log(avctx, AV_LOG_ERROR, \"AMF via D3D9 is not supported on the given device.\\n\");\n        else\n            av_log(avctx, AV_LOG_ERROR, \"AMF failed to initialise on given D3D9 device: %d.\\n\", res);\n        return AVERROR(ENODEV);\n    }\n\n    return 0;\n}\n#endif\n\nstatic int amf_init_context(AVCodecContext *avctx)\n{\n    AmfContext *ctx = avctx->priv_data;\n    AMF_RESULT  res;\n    av_unused int ret;\n\n    ctx->hwsurfaces_in_queue = 0;\n    ctx->hwsurfaces_in_queue_max = 16;\n\n    // configure AMF logger\n    // the return of these functions indicates old state and do not affect behaviour\n    ctx->trace->pVtbl->EnableWriter(ctx->trace, AMF_TRACE_WRITER_DEBUG_OUTPUT, ctx->log_to_dbg != 0 );\n    if (ctx->log_to_dbg)\n        ctx->trace->pVtbl->SetWriterLevel(ctx->trace, AMF_TRACE_WRITER_DEBUG_OUTPUT, AMF_TRACE_TRACE);\n    ctx->trace->pVtbl->EnableWriter(ctx->trace, AMF_TRACE_WRITER_CONSOLE, 0);\n    ctx->trace->pVtbl->SetGlobalLevel(ctx->trace, AMF_TRACE_TRACE);\n\n    // connect AMF logger to av_log\n    ctx->tracer.vtbl = &tracer_vtbl;\n    ctx->tracer.avctx = avctx;\n    ctx->trace->pVtbl->RegisterWriter(ctx->trace, FFMPEG_AMF_WRITER_ID,(AMFTraceWriter*)&ctx->tracer, 1);\n    ctx->trace->pVtbl->SetWriterLevel(ctx->trace, FFMPEG_AMF_WRITER_ID, AMF_TRACE_TRACE);\n\n    res = ctx->factory->pVtbl->CreateContext(ctx->factory, &ctx->context);\n    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"CreateContext() failed with error %d\\n\", res);\n\n    // If a device was passed to the encoder, try to initialise from that.\n    if (avctx->hw_frames_ctx) {\n        AVHWFramesContext *frames_ctx = (AVHWFramesContext*)avctx->hw_frames_ctx->data;\n\n        if (amf_av_to_amf_format(frames_ctx->sw_format) == AMF_SURFACE_UNKNOWN) {\n            av_log(avctx, AV_LOG_ERROR, \"Format of input frames context (%s) is not supported by AMF.\\n\",\n                   av_get_pix_fmt_name(frames_ctx->sw_format));\n            return AVERROR(EINVAL);\n        }\n\n        switch (frames_ctx->device_ctx->type) {\n#if CONFIG_D3D11VA\n        case AV_HWDEVICE_TYPE_D3D11VA:\n            ret = amf_init_from_d3d11_device(avctx, frames_ctx->device_ctx->hwctx);\n            if (ret < 0)\n                return ret;\n            break;\n#endif\n#if CONFIG_DXVA2\n        case AV_HWDEVICE_TYPE_DXVA2:\n            ret = amf_init_from_dxva2_device(avctx, frames_ctx->device_ctx->hwctx);\n            if (ret < 0)\n                return ret;\n            break;\n#endif\n        default:\n            av_log(avctx, AV_LOG_ERROR, \"AMF initialisation from a %s frames context is not supported.\\n\",\n                   av_hwdevice_get_type_name(frames_ctx->device_ctx->type));\n            return AVERROR(ENOSYS);\n        }\n\n        ctx->hw_frames_ctx = av_buffer_ref(avctx->hw_frames_ctx);\n        if (!ctx->hw_frames_ctx)\n            return AVERROR(ENOMEM);\n\n        if (frames_ctx->initial_pool_size > 0)\n            ctx->hwsurfaces_in_queue_max = frames_ctx->initial_pool_size - 1;\n\n    } else if (avctx->hw_device_ctx) {\n        AVHWDeviceContext *device_ctx = (AVHWDeviceContext*)avctx->hw_device_ctx->data;\n\n        switch (device_ctx->type) {\n#if CONFIG_D3D11VA\n        case AV_HWDEVICE_TYPE_D3D11VA:\n            ret = amf_init_from_d3d11_device(avctx, device_ctx->hwctx);\n            if (ret < 0)\n                return ret;\n            break;\n#endif\n#if CONFIG_DXVA2\n        case AV_HWDEVICE_TYPE_DXVA2:\n            ret = amf_init_from_dxva2_device(avctx, device_ctx->hwctx);\n            if (ret < 0)\n                return ret;\n            break;\n#endif\n        default:\n            av_log(avctx, AV_LOG_ERROR, \"AMF initialisation from a %s device is not supported.\\n\",\n                   av_hwdevice_get_type_name(device_ctx->type));\n            return AVERROR(ENOSYS);\n        }\n\n        ctx->hw_device_ctx = av_buffer_ref(avctx->hw_device_ctx);\n        if (!ctx->hw_device_ctx)\n            return AVERROR(ENOMEM);\n\n    } else {\n        res = ctx->context->pVtbl->InitDX11(ctx->context, NULL, AMF_DX11_1);\n        if (res == AMF_OK) {\n            av_log(avctx, AV_LOG_VERBOSE, \"AMF initialisation succeeded via D3D11.\\n\");\n        } else {\n            res = ctx->context->pVtbl->InitDX9(ctx->context, NULL);\n            if (res == AMF_OK) {\n                av_log(avctx, AV_LOG_VERBOSE, \"AMF initialisation succeeded via D3D9.\\n\");\n            } else {\n                av_log(avctx, AV_LOG_ERROR, \"AMF initialisation failed via D3D9: error %d.\\n\", res);\n                return AVERROR(ENOSYS);\n            }\n        }\n    }\n    return 0;\n}\n\nstatic int amf_init_encoder(AVCodecContext *avctx)\n{\n    AmfContext        *ctx = avctx->priv_data;\n    const wchar_t     *codec_id = NULL;\n    AMF_RESULT         res;\n    enum AVPixelFormat pix_fmt;\n\n    switch (avctx->codec->id) {\n        case AV_CODEC_ID_H264:\n            codec_id = AMFVideoEncoderVCE_AVC;\n            break;\n        case AV_CODEC_ID_HEVC:\n            codec_id = AMFVideoEncoder_HEVC;\n            break;\n        default:\n            break;\n    }\n    AMF_RETURN_IF_FALSE(ctx, codec_id != NULL, AVERROR(EINVAL), \"Codec %d is not supported\\n\", avctx->codec->id);\n\n    if (ctx->hw_frames_ctx)\n        pix_fmt = ((AVHWFramesContext*)ctx->hw_frames_ctx->data)->sw_format;\n    else\n        pix_fmt = avctx->pix_fmt;\n\n    ctx->format = amf_av_to_amf_format(pix_fmt);\n    AMF_RETURN_IF_FALSE(ctx, ctx->format != AMF_SURFACE_UNKNOWN, AVERROR(EINVAL),\n                        \"Format %s is not supported\\n\", av_get_pix_fmt_name(pix_fmt));\n\n    res = ctx->factory->pVtbl->CreateComponent(ctx->factory, ctx->context, codec_id, &ctx->encoder);\n    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_ENCODER_NOT_FOUND, \"CreateComponent(%ls) failed with error %d\\n\", codec_id, res);\n\n    return 0;\n}\n\nint av_cold ff_amf_encode_close(AVCodecContext *avctx)\n{\n    AmfContext *ctx = avctx->priv_data;\n\n    if (ctx->delayed_surface) {\n        ctx->delayed_surface->pVtbl->Release(ctx->delayed_surface);\n        ctx->delayed_surface = NULL;\n    }\n\n    if (ctx->encoder) {\n        ctx->encoder->pVtbl->Terminate(ctx->encoder);\n        ctx->encoder->pVtbl->Release(ctx->encoder);\n        ctx->encoder = NULL;\n    }\n\n    if (ctx->context) {\n        ctx->context->pVtbl->Terminate(ctx->context);\n        ctx->context->pVtbl->Release(ctx->context);\n        ctx->context = NULL;\n    }\n    av_buffer_unref(&ctx->hw_device_ctx);\n    av_buffer_unref(&ctx->hw_frames_ctx);\n\n    if (ctx->trace) {\n        ctx->trace->pVtbl->UnregisterWriter(ctx->trace, FFMPEG_AMF_WRITER_ID);\n    }\n    if (ctx->library) {\n        dlclose(ctx->library);\n        ctx->library = NULL;\n    }\n    ctx->trace = NULL;\n    ctx->debug = NULL;\n    ctx->factory = NULL;\n    ctx->version = 0;\n    ctx->delayed_drain = 0;\n    av_frame_free(&ctx->delayed_frame);\n    av_fifo_freep(&ctx->timestamp_list);\n\n    return 0;\n}\n\nstatic int amf_copy_surface(AVCodecContext *avctx, const AVFrame *frame,\n    AMFSurface* surface)\n{\n    AMFPlane *plane;\n    uint8_t  *dst_data[4];\n    int       dst_linesize[4];\n    int       planes;\n    int       i;\n\n    planes = surface->pVtbl->GetPlanesCount(surface);\n    av_assert0(planes < FF_ARRAY_ELEMS(dst_data));\n\n    for (i = 0; i < planes; i++) {\n        plane = surface->pVtbl->GetPlaneAt(surface, i);\n        dst_data[i] = plane->pVtbl->GetNative(plane);\n        dst_linesize[i] = plane->pVtbl->GetHPitch(plane);\n    }\n    av_image_copy(dst_data, dst_linesize,\n        (const uint8_t**)frame->data, frame->linesize, frame->format,\n        avctx->width, avctx->height);\n\n    return 0;\n}\n\nstatic inline int timestamp_queue_enqueue(AVCodecContext *avctx, int64_t timestamp)\n{\n    AmfContext         *ctx = avctx->priv_data;\n    if (av_fifo_space(ctx->timestamp_list) < sizeof(timestamp)) {\n        if (av_fifo_grow(ctx->timestamp_list, sizeof(timestamp)) < 0) {\n            return AVERROR(ENOMEM);\n        }\n    }\n    av_fifo_generic_write(ctx->timestamp_list, &timestamp, sizeof(timestamp), NULL);\n    return 0;\n}\n\nstatic int amf_copy_buffer(AVCodecContext *avctx, AVPacket *pkt, AMFBuffer *buffer)\n{\n    AmfContext      *ctx = avctx->priv_data;\n    int              ret;\n    AMFVariantStruct var = {0};\n    int64_t          timestamp = AV_NOPTS_VALUE;\n    int64_t          size = buffer->pVtbl->GetSize(buffer);\n\n    if ((ret = ff_alloc_packet2(avctx, pkt, size, 0)) < 0) {\n        return ret;\n    }\n    memcpy(pkt->data, buffer->pVtbl->GetNative(buffer), size);\n\n    switch (avctx->codec->id) {\n        case AV_CODEC_ID_H264:\n            buffer->pVtbl->GetProperty(buffer, AMF_VIDEO_ENCODER_OUTPUT_DATA_TYPE, &var);\n            if(var.int64Value == AMF_VIDEO_ENCODER_OUTPUT_DATA_TYPE_IDR) {\n                pkt->flags = AV_PKT_FLAG_KEY;\n            }\n            break;\n        case AV_CODEC_ID_HEVC:\n            buffer->pVtbl->GetProperty(buffer, AMF_VIDEO_ENCODER_HEVC_OUTPUT_DATA_TYPE, &var);\n            if (var.int64Value == AMF_VIDEO_ENCODER_HEVC_OUTPUT_DATA_TYPE_IDR) {\n                pkt->flags = AV_PKT_FLAG_KEY;\n            }\n            break;\n        default:\n            break;\n    }\n\n    buffer->pVtbl->GetProperty(buffer, PTS_PROP, &var);\n\n    pkt->pts = var.int64Value; // original pts\n\n\n    AMF_RETURN_IF_FALSE(ctx, av_fifo_size(ctx->timestamp_list) > 0, AVERROR_UNKNOWN, \"timestamp_list is empty\\n\");\n\n    av_fifo_generic_read(ctx->timestamp_list, &timestamp, sizeof(timestamp), NULL);\n\n    // calc dts shift if max_b_frames > 0\n    if (avctx->max_b_frames > 0 && ctx->dts_delay == 0) {\n        int64_t timestamp_last = AV_NOPTS_VALUE;\n        AMF_RETURN_IF_FALSE(ctx, av_fifo_size(ctx->timestamp_list) > 0, AVERROR_UNKNOWN,\n            \"timestamp_list is empty while max_b_frames = %d\\n\", avctx->max_b_frames);\n        av_fifo_generic_peek_at(\n            ctx->timestamp_list,\n            &timestamp_last,\n            (av_fifo_size(ctx->timestamp_list) / sizeof(timestamp) - 1) * sizeof(timestamp_last),\n            sizeof(timestamp_last),\n            NULL);\n        if (timestamp < 0 || timestamp_last < AV_NOPTS_VALUE) {\n            return AVERROR(ERANGE);\n        }\n        ctx->dts_delay = timestamp_last - timestamp;\n    }\n    pkt->dts = timestamp - ctx->dts_delay;\n    return 0;\n}\n\n// amfenc API implementation\nint ff_amf_encode_init(AVCodecContext *avctx)\n{\n    int ret;\n\n    if ((ret = amf_load_library(avctx)) == 0) {\n        if ((ret = amf_init_context(avctx)) == 0) {\n            if ((ret = amf_init_encoder(avctx)) == 0) {\n                return 0;\n            }\n        }\n    }\n    ff_amf_encode_close(avctx);\n    return ret;\n}\n\nstatic AMF_RESULT amf_set_property_buffer(AMFSurface *object, const wchar_t *name, AMFBuffer *val)\n{\n    AMF_RESULT res;\n    AMFVariantStruct var;\n    res = AMFVariantInit(&var);\n    if (res == AMF_OK) {\n        AMFGuid guid_AMFInterface = IID_AMFInterface();\n        AMFInterface *amf_interface;\n        res = val->pVtbl->QueryInterface(val, &guid_AMFInterface, (void**)&amf_interface);\n\n        if (res == AMF_OK) {\n            res = AMFVariantAssignInterface(&var, amf_interface);\n            amf_interface->pVtbl->Release(amf_interface);\n        }\n        if (res == AMF_OK) {\n            res = object->pVtbl->SetProperty(object, name, var);\n        }\n        AMFVariantClear(&var);\n    }\n    return res;\n}\n\nstatic AMF_RESULT amf_get_property_buffer(AMFData *object, const wchar_t *name, AMFBuffer **val)\n{\n    AMF_RESULT res;\n    AMFVariantStruct var;\n    res = AMFVariantInit(&var);\n    if (res == AMF_OK) {\n        res = object->pVtbl->GetProperty(object, name, &var);\n        if (res == AMF_OK) {\n            if (var.type == AMF_VARIANT_INTERFACE) {\n                AMFGuid guid_AMFBuffer = IID_AMFBuffer();\n                AMFInterface *amf_interface = AMFVariantInterface(&var);\n                res = amf_interface->pVtbl->QueryInterface(amf_interface, &guid_AMFBuffer, (void**)val);\n            } else {\n                res = AMF_INVALID_DATA_TYPE;\n            }\n        }\n        AMFVariantClear(&var);\n    }\n    return res;\n}\n\nstatic AMFBuffer *amf_create_buffer_with_frame_ref(const AVFrame *frame, AMFContext *context)\n{\n    AVFrame *frame_ref;\n    AMFBuffer *frame_ref_storage_buffer = NULL;\n    AMF_RESULT res;\n\n    res = context->pVtbl->AllocBuffer(context, AMF_MEMORY_HOST, sizeof(frame_ref), &frame_ref_storage_buffer);\n    if (res == AMF_OK) {\n        frame_ref = av_frame_clone(frame);\n        if (frame_ref) {\n            memcpy(frame_ref_storage_buffer->pVtbl->GetNative(frame_ref_storage_buffer), &frame_ref, sizeof(frame_ref));\n        } else {\n            frame_ref_storage_buffer->pVtbl->Release(frame_ref_storage_buffer);\n            frame_ref_storage_buffer = NULL;\n        }\n    }\n    return frame_ref_storage_buffer;\n}\n\nstatic void amf_release_buffer_with_frame_ref(AMFBuffer *frame_ref_storage_buffer)\n{\n    AVFrame *frame_ref;\n    memcpy(&frame_ref, frame_ref_storage_buffer->pVtbl->GetNative(frame_ref_storage_buffer), sizeof(frame_ref));\n    av_frame_free(&frame_ref);\n    frame_ref_storage_buffer->pVtbl->Release(frame_ref_storage_buffer);\n}\n\nint ff_amf_send_frame(AVCodecContext *avctx, const AVFrame *frame)\n{\n    AmfContext *ctx = avctx->priv_data;\n    AMFSurface *surface;\n    AMF_RESULT  res;\n    int         ret;\n\n    if (!ctx->encoder)\n        return AVERROR(EINVAL);\n\n    if (!frame) { // submit drain\n        if (!ctx->eof) { // submit drain one time only\n            if (ctx->delayed_surface != NULL) {\n                ctx->delayed_drain = 1; // input queue is full: resubmit Drain() in ff_amf_receive_packet\n            } else if(!ctx->delayed_drain) {\n                res = ctx->encoder->pVtbl->Drain(ctx->encoder);\n                if (res == AMF_INPUT_FULL) {\n                    ctx->delayed_drain = 1; // input queue is full: resubmit Drain() in ff_amf_receive_packet\n                } else {\n                    if (res == AMF_OK) {\n                        ctx->eof = 1; // drain started\n                    }\n                    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"Drain() failed with error %d\\n\", res);\n                }\n            }\n        } else{\n            return AVERROR_EOF;\n        }\n    } else { // submit frame\n        int hw_surface = 0;\n\n        if (ctx->delayed_surface != NULL) {\n            return AVERROR(EAGAIN); // should not happen when called from ffmpeg, other clients may resubmit\n        }\n        // prepare surface from frame\n        switch (frame->format) {\n#if CONFIG_D3D11VA\n        case AV_PIX_FMT_D3D11:\n            {\n                static const GUID AMFTextureArrayIndexGUID = { 0x28115527, 0xe7c3, 0x4b66, { 0x99, 0xd3, 0x4f, 0x2a, 0xe6, 0xb4, 0x7f, 0xaf } };\n                ID3D11Texture2D *texture = (ID3D11Texture2D*)frame->data[0]; // actual texture\n                int index = (intptr_t)frame->data[1]; // index is a slice in texture array is - set to tell AMF which slice to use\n\n                av_assert0(frame->hw_frames_ctx       && ctx->hw_frames_ctx &&\n                           frame->hw_frames_ctx->data == ctx->hw_frames_ctx->data);\n\n                texture->lpVtbl->SetPrivateData(texture, &AMFTextureArrayIndexGUID, sizeof(index), &index);\n\n                res = ctx->context->pVtbl->CreateSurfaceFromDX11Native(ctx->context, texture, &surface, NULL); // wrap to AMF surface\n                AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR(ENOMEM), \"CreateSurfaceFromDX11Native() failed  with error %d\\n\", res);\n\n                hw_surface = 1;\n            }\n            break;\n#endif\n#if CONFIG_DXVA2\n        case AV_PIX_FMT_DXVA2_VLD:\n            {\n                IDirect3DSurface9 *texture = (IDirect3DSurface9 *)frame->data[3]; // actual texture\n\n                res = ctx->context->pVtbl->CreateSurfaceFromDX9Native(ctx->context, texture, &surface, NULL); // wrap to AMF surface\n                AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR(ENOMEM), \"CreateSurfaceFromDX9Native() failed  with error %d\\n\", res);\n\n                hw_surface = 1;\n            }\n            break;\n#endif\n        default:\n            {\n                res = ctx->context->pVtbl->AllocSurface(ctx->context, AMF_MEMORY_HOST, ctx->format, avctx->width, avctx->height, &surface);\n                AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR(ENOMEM), \"AllocSurface() failed  with error %d\\n\", res);\n                amf_copy_surface(avctx, frame, surface);\n            }\n            break;\n        }\n\n        if (hw_surface) {\n            AMFBuffer *frame_ref_storage_buffer;\n\n            // input HW surfaces can be vertically aligned by 16; tell AMF the real size\n            surface->pVtbl->SetCrop(surface, 0, 0, frame->width, frame->height);\n\n            frame_ref_storage_buffer = amf_create_buffer_with_frame_ref(frame, ctx->context);\n            AMF_RETURN_IF_FALSE(ctx, frame_ref_storage_buffer != NULL, AVERROR(ENOMEM), \"create_buffer_with_frame_ref() returned NULL\\n\");\n\n            res = amf_set_property_buffer(surface, L\"av_frame_ref\", frame_ref_storage_buffer);\n            AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"SetProperty failed for \\\"av_frame_ref\\\" with error %d\\n\", res);\n            ctx->hwsurfaces_in_queue++;\n            frame_ref_storage_buffer->pVtbl->Release(frame_ref_storage_buffer);\n        }\n\n        surface->pVtbl->SetPts(surface, frame->pts);\n        AMF_ASSIGN_PROPERTY_INT64(res, surface, PTS_PROP, frame->pts);\n\n        switch (avctx->codec->id) {\n        case AV_CODEC_ID_H264:\n            AMF_ASSIGN_PROPERTY_INT64(res, surface, AMF_VIDEO_ENCODER_INSERT_AUD, !!ctx->aud);\n            break;\n        case AV_CODEC_ID_HEVC:\n            AMF_ASSIGN_PROPERTY_INT64(res, surface, AMF_VIDEO_ENCODER_HEVC_INSERT_AUD, !!ctx->aud);\n            break;\n        default:\n            break;\n        }\n\n\n        // submit surface\n        res = ctx->encoder->pVtbl->SubmitInput(ctx->encoder, (AMFData*)surface);\n        if (res == AMF_INPUT_FULL) { // handle full queue\n            //store surface for later submission\n            ctx->delayed_surface = surface;\n            if (surface->pVtbl->GetMemoryType(surface) == AMF_MEMORY_DX11) {\n                av_frame_ref(ctx->delayed_frame, frame);\n            }\n        } else {\n            surface->pVtbl->Release(surface);\n            AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"SubmitInput() failed with error %d\\n\", res);\n\n            if ((ret = timestamp_queue_enqueue(avctx, frame->pts)) < 0) {\n                return ret;\n            }\n\n        }\n    }\n    return 0;\n}\nint ff_amf_receive_packet(AVCodecContext *avctx, AVPacket *avpkt)\n{\n    int             ret;\n    AMF_RESULT      res;\n    AMF_RESULT      res_query;\n    AmfContext     *ctx = avctx->priv_data;\n    AMFData        *data = NULL;\n    int             block_and_wait;\n\n    if (!ctx->encoder)\n        return AVERROR(EINVAL);\n\n    do {\n        block_and_wait = 0;\n        // poll data\n        res_query = ctx->encoder->pVtbl->QueryOutput(ctx->encoder, &data);\n        if (data) {\n            // copy data to packet\n            AMFBuffer* buffer;\n            AMFGuid guid = IID_AMFBuffer();\n            data->pVtbl->QueryInterface(data, &guid, (void**)&buffer); // query for buffer interface\n            ret = amf_copy_buffer(avctx, avpkt, buffer);\n\n            buffer->pVtbl->Release(buffer);\n\n            if (data->pVtbl->HasProperty(data, L\"av_frame_ref\")) {\n                AMFBuffer *frame_ref_storage_buffer;\n                res = amf_get_property_buffer(data, L\"av_frame_ref\", &frame_ref_storage_buffer);\n                AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"GetProperty failed for \\\"av_frame_ref\\\" with error %d\\n\", res);\n                amf_release_buffer_with_frame_ref(frame_ref_storage_buffer);\n                ctx->hwsurfaces_in_queue--;\n            }\n\n            data->pVtbl->Release(data);\n\n            AMF_RETURN_IF_FALSE(ctx, ret >= 0, ret, \"amf_copy_buffer() failed with error %d\\n\", ret);\n\n            if (ctx->delayed_surface != NULL) { // try to resubmit frame\n                res = ctx->encoder->pVtbl->SubmitInput(ctx->encoder, (AMFData*)ctx->delayed_surface);\n                if (res != AMF_INPUT_FULL) {\n                    int64_t pts = ctx->delayed_surface->pVtbl->GetPts(ctx->delayed_surface);\n                    ctx->delayed_surface->pVtbl->Release(ctx->delayed_surface);\n                    ctx->delayed_surface = NULL;\n                    av_frame_unref(ctx->delayed_frame);\n                    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"Repeated SubmitInput() failed with error %d\\n\", res);\n\n                    if ((ret = timestamp_queue_enqueue(avctx, pts)) < 0) {\n                        return ret;\n                    }\n                } else {\n                    av_log(avctx, AV_LOG_WARNING, \"Data acquired but delayed frame submission got AMF_INPUT_FULL- should not happen\\n\");\n                }\n            } else if (ctx->delayed_drain) { // try to resubmit drain\n                res = ctx->encoder->pVtbl->Drain(ctx->encoder);\n                if (res != AMF_INPUT_FULL) {\n                    ctx->delayed_drain = 0;\n                    ctx->eof = 1; // drain started\n                    AMF_RETURN_IF_FALSE(ctx, res == AMF_OK, AVERROR_UNKNOWN, \"Repeated Drain() failed with error %d\\n\", res);\n                } else {\n                    av_log(avctx, AV_LOG_WARNING, \"Data acquired but delayed drain submission got AMF_INPUT_FULL- should not happen\\n\");\n                }\n            }\n        } else if (ctx->delayed_surface != NULL || ctx->delayed_drain || (ctx->eof && res_query != AMF_EOF) || (ctx->hwsurfaces_in_queue >= ctx->hwsurfaces_in_queue_max)) {\n            block_and_wait = 1;\n            av_usleep(1000); // wait and poll again\n        }\n    } while (block_and_wait);\n\n    if (res_query == AMF_EOF) {\n        ret = AVERROR_EOF;\n    } else if (data == NULL) {\n        ret = AVERROR(EAGAIN);\n    } else {\n        ret = 0;\n    }\n    return ret;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/libavutil/hwcontext_dxva2.c": "/*\n * This file is part of FFmpeg.\n *\n * FFmpeg is free software; you can redistribute it and/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * FFmpeg is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with FFmpeg; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n */\n\n#include <windows.h>\n\n#define DXVA2API_USE_BITFIELDS\n#define COBJMACROS\n\n#include <d3d9.h>\n#include <dxva2api.h>\n#include <initguid.h>\n\n#include \"avassert.h\"\n#include \"common.h\"\n#include \"hwcontext.h\"\n#include \"hwcontext_dxva2.h\"\n#include \"hwcontext_internal.h\"\n#include \"imgutils.h\"\n#include \"pixdesc.h\"\n#include \"pixfmt.h\"\n#include \"compat/w32dlfcn.h\"\n\ntypedef IDirect3D9* WINAPI pDirect3DCreate9(UINT);\ntypedef HRESULT WINAPI pDirect3DCreate9Ex(UINT, IDirect3D9Ex **);\ntypedef HRESULT WINAPI pCreateDeviceManager9(UINT *, IDirect3DDeviceManager9 **);\n\n#define FF_D3DCREATE_FLAGS (D3DCREATE_SOFTWARE_VERTEXPROCESSING | \\\n                            D3DCREATE_MULTITHREADED | \\\n                            D3DCREATE_FPU_PRESERVE)\n\nstatic const D3DPRESENT_PARAMETERS dxva2_present_params = {\n    .Windowed         = TRUE,\n    .BackBufferWidth  = 640,\n    .BackBufferHeight = 480,\n    .BackBufferCount  = 0,\n    .SwapEffect       = D3DSWAPEFFECT_DISCARD,\n    .Flags            = D3DPRESENTFLAG_VIDEO,\n};\n\ntypedef struct DXVA2Mapping {\n    uint32_t palette_dummy[256];\n} DXVA2Mapping;\n\ntypedef struct DXVA2FramesContext {\n    IDirect3DSurface9 **surfaces_internal;\n    int              nb_surfaces_used;\n\n    HANDLE  device_handle;\n    IDirectXVideoAccelerationService *service;\n\n    D3DFORMAT format;\n} DXVA2FramesContext;\n\ntypedef struct DXVA2DevicePriv {\n    HMODULE d3dlib;\n    HMODULE dxva2lib;\n\n    HANDLE device_handle;\n\n    IDirect3D9       *d3d9;\n    IDirect3DDevice9 *d3d9device;\n} DXVA2DevicePriv;\n\nstatic const struct {\n    D3DFORMAT d3d_format;\n    enum AVPixelFormat pix_fmt;\n} supported_formats[] = {\n    { MKTAG('N', 'V', '1', '2'), AV_PIX_FMT_NV12 },\n    { MKTAG('P', '0', '1', '0'), AV_PIX_FMT_P010 },\n    { D3DFMT_P8,                 AV_PIX_FMT_PAL8 },\n};\n\nDEFINE_GUID(video_decoder_service,   0xfc51a551, 0xd5e7, 0x11d9, 0xaf, 0x55, 0x00, 0x05, 0x4e, 0x43, 0xff, 0x02);\nDEFINE_GUID(video_processor_service, 0xfc51a552, 0xd5e7, 0x11d9, 0xaf, 0x55, 0x00, 0x05, 0x4e, 0x43, 0xff, 0x02);\n\nstatic void dxva2_frames_uninit(AVHWFramesContext *ctx)\n{\n    AVDXVA2DeviceContext *device_hwctx = ctx->device_ctx->hwctx;\n    AVDXVA2FramesContext *frames_hwctx = ctx->hwctx;\n    DXVA2FramesContext *s = ctx->internal->priv;\n    int i;\n\n    if (frames_hwctx->decoder_to_release)\n        IDirectXVideoDecoder_Release(frames_hwctx->decoder_to_release);\n\n    if (s->surfaces_internal) {\n        for (i = 0; i < frames_hwctx->nb_surfaces; i++) {\n            if (s->surfaces_internal[i])\n                IDirect3DSurface9_Release(s->surfaces_internal[i]);\n        }\n    }\n    av_freep(&s->surfaces_internal);\n\n    if (s->service) {\n        IDirectXVideoAccelerationService_Release(s->service);\n        s->service = NULL;\n    }\n\n    if (s->device_handle != INVALID_HANDLE_VALUE) {\n        IDirect3DDeviceManager9_CloseDeviceHandle(device_hwctx->devmgr, s->device_handle);\n        s->device_handle = INVALID_HANDLE_VALUE;\n    }\n}\n\nstatic void dxva2_pool_release_dummy(void *opaque, uint8_t *data)\n{\n    // important not to free anything here--data is a surface object\n    // associated with the call to CreateSurface(), and these surfaces are\n    // released in dxva2_frames_uninit()\n}\n\nstatic AVBufferRef *dxva2_pool_alloc(void *opaque, int size)\n{\n    AVHWFramesContext      *ctx = (AVHWFramesContext*)opaque;\n    DXVA2FramesContext       *s = ctx->internal->priv;\n    AVDXVA2FramesContext *hwctx = ctx->hwctx;\n\n    if (s->nb_surfaces_used < hwctx->nb_surfaces) {\n        s->nb_surfaces_used++;\n        return av_buffer_create((uint8_t*)s->surfaces_internal[s->nb_surfaces_used - 1],\n                                sizeof(*hwctx->surfaces), dxva2_pool_release_dummy, 0, 0);\n    }\n\n    return NULL;\n}\n\nstatic int dxva2_init_pool(AVHWFramesContext *ctx)\n{\n    AVDXVA2FramesContext *frames_hwctx = ctx->hwctx;\n    AVDXVA2DeviceContext *device_hwctx = ctx->device_ctx->hwctx;\n    DXVA2FramesContext              *s = ctx->internal->priv;\n    int decode = (frames_hwctx->surface_type == DXVA2_VideoDecoderRenderTarget);\n\n    int i;\n    HRESULT hr;\n\n    if (ctx->initial_pool_size <= 0)\n        return 0;\n\n    hr = IDirect3DDeviceManager9_OpenDeviceHandle(device_hwctx->devmgr, &s->device_handle);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to open device handle\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    hr = IDirect3DDeviceManager9_GetVideoService(device_hwctx->devmgr,\n                                                 s->device_handle,\n                                                 decode ? &video_decoder_service : &video_processor_service,\n                                                 (void **)&s->service);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to create the video service\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    for (i = 0; i < FF_ARRAY_ELEMS(supported_formats); i++) {\n        if (ctx->sw_format == supported_formats[i].pix_fmt) {\n            s->format = supported_formats[i].d3d_format;\n            break;\n        }\n    }\n    if (i == FF_ARRAY_ELEMS(supported_formats)) {\n        av_log(ctx, AV_LOG_ERROR, \"Unsupported pixel format: %s\\n\",\n               av_get_pix_fmt_name(ctx->sw_format));\n        return AVERROR(EINVAL);\n    }\n\n    s->surfaces_internal = av_mallocz_array(ctx->initial_pool_size,\n                                            sizeof(*s->surfaces_internal));\n    if (!s->surfaces_internal)\n        return AVERROR(ENOMEM);\n\n    hr = IDirectXVideoAccelerationService_CreateSurface(s->service,\n                                                        ctx->width, ctx->height,\n                                                        ctx->initial_pool_size - 1,\n                                                        s->format, D3DPOOL_DEFAULT, 0,\n                                                        frames_hwctx->surface_type,\n                                                        s->surfaces_internal, NULL);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Could not create the surfaces\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    ctx->internal->pool_internal = av_buffer_pool_init2(sizeof(*s->surfaces_internal),\n                                                        ctx, dxva2_pool_alloc, NULL);\n    if (!ctx->internal->pool_internal)\n        return AVERROR(ENOMEM);\n\n    frames_hwctx->surfaces    = s->surfaces_internal;\n    frames_hwctx->nb_surfaces = ctx->initial_pool_size;\n\n    return 0;\n}\n\nstatic int dxva2_frames_init(AVHWFramesContext *ctx)\n{\n    AVDXVA2FramesContext *hwctx = ctx->hwctx;\n    DXVA2FramesContext       *s = ctx->internal->priv;\n    int ret;\n\n    if (hwctx->surface_type != DXVA2_VideoDecoderRenderTarget &&\n        hwctx->surface_type != DXVA2_VideoProcessorRenderTarget) {\n        av_log(ctx, AV_LOG_ERROR, \"Unknown surface type: %lu\\n\",\n               hwctx->surface_type);\n        return AVERROR(EINVAL);\n    }\n\n    s->device_handle = INVALID_HANDLE_VALUE;\n\n    /* init the frame pool if the caller didn't provide one */\n    if (!ctx->pool) {\n        ret = dxva2_init_pool(ctx);\n        if (ret < 0) {\n            av_log(ctx, AV_LOG_ERROR, \"Error creating an internal frame pool\\n\");\n            return ret;\n        }\n    }\n\n    return 0;\n}\n\nstatic int dxva2_get_buffer(AVHWFramesContext *ctx, AVFrame *frame)\n{\n    frame->buf[0] = av_buffer_pool_get(ctx->pool);\n    if (!frame->buf[0])\n        return AVERROR(ENOMEM);\n\n    frame->data[3] = frame->buf[0]->data;\n    frame->format  = AV_PIX_FMT_DXVA2_VLD;\n    frame->width   = ctx->width;\n    frame->height  = ctx->height;\n\n    return 0;\n}\n\nstatic int dxva2_transfer_get_formats(AVHWFramesContext *ctx,\n                                      enum AVHWFrameTransferDirection dir,\n                                      enum AVPixelFormat **formats)\n{\n    enum AVPixelFormat *fmts;\n\n    fmts = av_malloc_array(2, sizeof(*fmts));\n    if (!fmts)\n        return AVERROR(ENOMEM);\n\n    fmts[0] = ctx->sw_format;\n    fmts[1] = AV_PIX_FMT_NONE;\n\n    *formats = fmts;\n\n    return 0;\n}\n\nstatic void dxva2_unmap_frame(AVHWFramesContext *ctx, HWMapDescriptor *hwmap)\n{\n    IDirect3DSurface9 *surface = (IDirect3DSurface9*)hwmap->source->data[3];\n    IDirect3DSurface9_UnlockRect(surface);\n    av_freep(&hwmap->priv);\n}\n\nstatic int dxva2_map_frame(AVHWFramesContext *ctx, AVFrame *dst, const AVFrame *src,\n                           int flags)\n{\n    IDirect3DSurface9 *surface = (IDirect3DSurface9*)src->data[3];\n    DXVA2Mapping      *map;\n    D3DSURFACE_DESC    surfaceDesc;\n    D3DLOCKED_RECT     LockedRect;\n    HRESULT            hr;\n    int i, err, nb_planes;\n    int lock_flags = 0;\n\n    nb_planes = av_pix_fmt_count_planes(dst->format);\n\n    hr = IDirect3DSurface9_GetDesc(surface, &surfaceDesc);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Error getting a surface description\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    if (!(flags & AV_HWFRAME_MAP_WRITE))\n        lock_flags |= D3DLOCK_READONLY;\n    if (flags & AV_HWFRAME_MAP_OVERWRITE)\n        lock_flags |= D3DLOCK_DISCARD;\n\n    hr = IDirect3DSurface9_LockRect(surface, &LockedRect, NULL, lock_flags);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Unable to lock DXVA2 surface\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    map = av_mallocz(sizeof(*map));\n    if (!map) {\n        err = AVERROR(ENOMEM);\n        goto fail;\n    }\n\n    err = ff_hwframe_map_create(src->hw_frames_ctx, dst, src,\n                                dxva2_unmap_frame, map);\n    if (err < 0) {\n        av_freep(&map);\n        goto fail;\n    }\n\n    for (i = 0; i < nb_planes; i++)\n        dst->linesize[i] = LockedRect.Pitch;\n\n    av_image_fill_pointers(dst->data, dst->format, surfaceDesc.Height,\n                           (uint8_t*)LockedRect.pBits, dst->linesize);\n\n    if (dst->format == AV_PIX_FMT_PAL8)\n        dst->data[1] = (uint8_t*)map->palette_dummy;\n\n    return 0;\nfail:\n    IDirect3DSurface9_UnlockRect(surface);\n    return err;\n}\n\nstatic int dxva2_transfer_data_to(AVHWFramesContext *ctx, AVFrame *dst,\n                                  const AVFrame *src)\n{\n    AVFrame *map;\n    int ret;\n\n    if (src->format != ctx->sw_format)\n        return AVERROR(ENOSYS);\n\n    map = av_frame_alloc();\n    if (!map)\n        return AVERROR(ENOMEM);\n    map->format = dst->format;\n\n    ret = dxva2_map_frame(ctx, map, dst, AV_HWFRAME_MAP_WRITE | AV_HWFRAME_MAP_OVERWRITE);\n    if (ret < 0)\n        goto fail;\n\n    av_image_copy(map->data, map->linesize, src->data, src->linesize,\n                  ctx->sw_format, src->width, src->height);\n\nfail:\n    av_frame_free(&map);\n    return ret;\n}\n\nstatic int dxva2_transfer_data_from(AVHWFramesContext *ctx, AVFrame *dst,\n                                    const AVFrame *src)\n{\n    AVFrame *map;\n    ptrdiff_t src_linesize[4], dst_linesize[4];\n    int ret, i;\n\n    if (dst->format != ctx->sw_format)\n        return AVERROR(ENOSYS);\n\n    map = av_frame_alloc();\n    if (!map)\n        return AVERROR(ENOMEM);\n    map->format = dst->format;\n\n    ret = dxva2_map_frame(ctx, map, src, AV_HWFRAME_MAP_READ);\n    if (ret < 0)\n        goto fail;\n\n    for (i = 0; i < 4; i++) {\n        dst_linesize[i] = dst->linesize[i];\n        src_linesize[i] = map->linesize[i];\n    }\n    av_image_copy_uc_from(dst->data, dst_linesize, map->data, src_linesize,\n                          ctx->sw_format, src->width, src->height);\nfail:\n    av_frame_free(&map);\n    return ret;\n}\n\nstatic int dxva2_map_from(AVHWFramesContext *ctx,\n                          AVFrame *dst, const AVFrame *src, int flags)\n{\n    int err;\n\n    if (dst->format != AV_PIX_FMT_NONE && dst->format != ctx->sw_format)\n        return AVERROR(ENOSYS);\n    dst->format = ctx->sw_format;\n\n    err = dxva2_map_frame(ctx, dst, src, flags);\n    if (err < 0)\n        return err;\n\n    err = av_frame_copy_props(dst, src);\n    if (err < 0)\n        return err;\n\n    return 0;\n}\n\nstatic void dxva2_device_free(AVHWDeviceContext *ctx)\n{\n    AVDXVA2DeviceContext *hwctx = ctx->hwctx;\n    DXVA2DevicePriv       *priv = ctx->user_opaque;\n\n    if (hwctx->devmgr && priv->device_handle != INVALID_HANDLE_VALUE)\n        IDirect3DDeviceManager9_CloseDeviceHandle(hwctx->devmgr, priv->device_handle);\n\n    if (hwctx->devmgr)\n        IDirect3DDeviceManager9_Release(hwctx->devmgr);\n\n    if (priv->d3d9device)\n        IDirect3DDevice9_Release(priv->d3d9device);\n\n    if (priv->d3d9)\n        IDirect3D9_Release(priv->d3d9);\n\n    if (priv->d3dlib)\n        dlclose(priv->d3dlib);\n\n    if (priv->dxva2lib)\n        dlclose(priv->dxva2lib);\n\n    av_freep(&ctx->user_opaque);\n}\n\nstatic int dxva2_device_create9(AVHWDeviceContext *ctx, UINT adapter)\n{\n    DXVA2DevicePriv *priv = ctx->user_opaque;\n    D3DPRESENT_PARAMETERS d3dpp = dxva2_present_params;\n    D3DDISPLAYMODE d3ddm;\n    HRESULT hr;\n    pDirect3DCreate9 *createD3D = (pDirect3DCreate9 *)dlsym(priv->d3dlib, \"Direct3DCreate9\");\n    if (!createD3D) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to locate Direct3DCreate9\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    priv->d3d9 = createD3D(D3D_SDK_VERSION);\n    if (!priv->d3d9) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to create IDirect3D object\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    IDirect3D9_GetAdapterDisplayMode(priv->d3d9, adapter, &d3ddm);\n\n    d3dpp.BackBufferFormat = d3ddm.Format;\n\n    hr = IDirect3D9_CreateDevice(priv->d3d9, adapter, D3DDEVTYPE_HAL, GetDesktopWindow(),\n                                 FF_D3DCREATE_FLAGS,\n                                 &d3dpp, &priv->d3d9device);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to create Direct3D device\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    return 0;\n}\n\nstatic int dxva2_device_create9ex(AVHWDeviceContext *ctx, UINT adapter)\n{\n    DXVA2DevicePriv *priv = ctx->user_opaque;\n    D3DPRESENT_PARAMETERS d3dpp = dxva2_present_params;\n    D3DDISPLAYMODEEX modeex = {0};\n    IDirect3D9Ex *d3d9ex = NULL;\n    IDirect3DDevice9Ex *exdev = NULL;\n    HRESULT hr;\n    pDirect3DCreate9Ex *createD3DEx = (pDirect3DCreate9Ex *)dlsym(priv->d3dlib, \"Direct3DCreate9Ex\");\n    if (!createD3DEx)\n        return AVERROR(ENOSYS);\n\n    hr = createD3DEx(D3D_SDK_VERSION, &d3d9ex);\n    if (FAILED(hr))\n        return AVERROR_UNKNOWN;\n\n    modeex.Size = sizeof(D3DDISPLAYMODEEX);\n    hr = IDirect3D9Ex_GetAdapterDisplayModeEx(d3d9ex, adapter, &modeex, NULL);\n    if (FAILED(hr)) {\n        IDirect3D9Ex_Release(d3d9ex);\n        return AVERROR_UNKNOWN;\n    }\n\n    d3dpp.BackBufferFormat = modeex.Format;\n\n    hr = IDirect3D9Ex_CreateDeviceEx(d3d9ex, adapter, D3DDEVTYPE_HAL, GetDesktopWindow(),\n                                     FF_D3DCREATE_FLAGS,\n                                     &d3dpp, NULL, &exdev);\n    if (FAILED(hr)) {\n        IDirect3D9Ex_Release(d3d9ex);\n        return AVERROR_UNKNOWN;\n    }\n\n    av_log(ctx, AV_LOG_VERBOSE, \"Using D3D9Ex device.\\n\");\n    priv->d3d9 = (IDirect3D9 *)d3d9ex;\n    priv->d3d9device = (IDirect3DDevice9 *)exdev;\n    return 0;\n}\n\nstatic int dxva2_device_create(AVHWDeviceContext *ctx, const char *device,\n                               AVDictionary *opts, int flags)\n{\n    AVDXVA2DeviceContext *hwctx = ctx->hwctx;\n    DXVA2DevicePriv *priv;\n    pCreateDeviceManager9 *createDeviceManager = NULL;\n    unsigned resetToken = 0;\n    UINT adapter = D3DADAPTER_DEFAULT;\n    HRESULT hr;\n    int err;\n\n    if (device)\n        adapter = atoi(device);\n\n    priv = av_mallocz(sizeof(*priv));\n    if (!priv)\n        return AVERROR(ENOMEM);\n\n    ctx->user_opaque = priv;\n    ctx->free        = dxva2_device_free;\n\n    priv->device_handle = INVALID_HANDLE_VALUE;\n\n    priv->d3dlib = dlopen(\"d3d9.dll\", 0);\n    if (!priv->d3dlib) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to load D3D9 library\\n\");\n        return AVERROR_UNKNOWN;\n    }\n    priv->dxva2lib = dlopen(\"dxva2.dll\", 0);\n    if (!priv->dxva2lib) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to load DXVA2 library\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    createDeviceManager = (pCreateDeviceManager9 *)dlsym(priv->dxva2lib,\n                                                         \"DXVA2CreateDirect3DDeviceManager9\");\n    if (!createDeviceManager) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to locate DXVA2CreateDirect3DDeviceManager9\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    if (dxva2_device_create9ex(ctx, adapter) < 0) {\n        // Retry with \"classic\" d3d9\n        err = dxva2_device_create9(ctx, adapter);\n        if (err < 0)\n            return err;\n    }\n\n    hr = createDeviceManager(&resetToken, &hwctx->devmgr);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to create Direct3D device manager\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    hr = IDirect3DDeviceManager9_ResetDevice(hwctx->devmgr, priv->d3d9device, resetToken);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to bind Direct3D device to device manager\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    hr = IDirect3DDeviceManager9_OpenDeviceHandle(hwctx->devmgr, &priv->device_handle);\n    if (FAILED(hr)) {\n        av_log(ctx, AV_LOG_ERROR, \"Failed to open device handle\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n    return 0;\n}\n\nconst HWContextType ff_hwcontext_type_dxva2 = {\n    .type                 = AV_HWDEVICE_TYPE_DXVA2,\n    .name                 = \"DXVA2\",\n\n    .device_hwctx_size    = sizeof(AVDXVA2DeviceContext),\n    .frames_hwctx_size    = sizeof(AVDXVA2FramesContext),\n    .frames_priv_size     = sizeof(DXVA2FramesContext),\n\n    .device_create        = dxva2_device_create,\n    .frames_init          = dxva2_frames_init,\n    .frames_uninit        = dxva2_frames_uninit,\n    .frames_get_buffer    = dxva2_get_buffer,\n    .transfer_get_formats = dxva2_transfer_get_formats,\n    .transfer_data_to     = dxva2_transfer_data_to,\n    .transfer_data_from   = dxva2_transfer_data_from,\n    .map_from             = dxva2_map_from,\n\n    .pix_fmts             = (const enum AVPixelFormat[]){ AV_PIX_FMT_DXVA2_VLD, AV_PIX_FMT_NONE },\n};\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-ffmpeg-4.1.1-ko53yolc66y2ojsbm6ukob5cmthydoye/spack-src/tests/reference.pnm"
    ],
    "total_files": 3987
}