{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-r-cubature-2.0.3-hbwemlwkkhssqjl2cw7x5e7orj42zfam/spack-src/src/Cuba-4.2/cuba.tex": "\\documentclass[12pt]{article}\n\\usepackage{a4wide,amsmath,amssymb,alltt}\n\\usepackage{colordvi}\n\n\\parskip=2pt\n\\raggedbottom\n\\sloppy\n\n\\newcommand\\cuba{\\textsc{Cuba}}\n\\newcommand\\ie{i.e.\\ }\n\\newcommand\\eg{e.g.\\ }\n\\newcommand\\rd{\\mathrm{d}}\n\\newcommand\\order{\\mathcal{O}}\n\\newcommand\\epsabs{\\varepsilon_{\\text{abs}}}\n\\newcommand\\epsrel{\\varepsilon_{\\text{rel}}}\n\\newcommand\\nnew{n_s^{\\text{new}}}\n\\newcommand\\nmin{n_s^{\\text{min}}}\n\\newcommand\\nmax{n_s^{\\text{max}}}\n\\newcommand\\nneed{n_{\\text{need}}}\n\\newcommand\\tot{_{\\text{tot}}}\n\\newcommand\\ctot{_{c,\\text{tot}}}\n\\newcommand\\cvec[1]{\\vec #1}\n\\newcommand\\dvec[1]{\\mathbf{#1}}\n\\newcommand\\norm[1]{\\|#1\\|_1}\n\\newcommand\\lbrac{\\symbol{123}}\n\\newcommand\\rbrac{\\symbol{125}}\n\\newcommand\\uscore{\\symbol{95}}\n\\newcommand\\accel{_{\\mathrm{accel}}}\n\\newcommand\\cores{_{\\mathrm{cores}}}\n\\newcommand\\Code[1]{\\ensuremath{\\texttt{#1}}}\n\\newcommand\\Var[1]{\\ensuremath{\\mathit{#1}}}\n\\newcommand\\VarIn[1]{\\item\\Code{#1} \\textit{$\\langle$in\\/$\\rangle$},}\n\\newcommand\\VarOut[1]{\\item\\Code{#1} \\textit{$\\langle$out\\,$\\rangle$},}\n\\newcommand\\Option[2]{\\item\\Code{#1 -> #2},}\n\\newcommand\\ul[1]{\\underline{\\vphantom{g}#1}}\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\n\\makeatletter\n\\def\\reportno#1{\\gdef\\@reportno{#1}}\n\\def\\@maketitle{%\n  \\hfill{\\small\\begin{tabular}[t]{r}%\n    \\@reportno\n  \\end{tabular}\\par}%\n  \\vskip 2em%\n  \\begin{center}%\n    \\let \\footnote \\thanks\n    {\\large \\@title \\par}%\n    \\vskip 1.5em%\n    {%\\large\n      \\lineskip .5em%\n      \\begin{tabular}[t]{c}%\n        \\@author  \n      \\end{tabular}\\par}%\n    \\vskip 1em%\n    {%\\large\n     \\@date}%\n  \\end{center}%\n  \\par\n  \\vskip 1.5em}\n\\makeatother\n\n\n\\begin{document}\n\n\\reportno{MPP--2004--40\\\\hep--ph/0404043}\n\n\\title{\\cuba\\ -- a library for multidimensional numerical integration}\n\n\\author{T. Hahn \\\\\nMax-Planck-Institut f\\\"ur Physik \\\\\nF\\\"ohringer Ring 6, D--80805 Munich, Germany}\n\n\\date{Mar 27, 2015}\n\n\\maketitle\n\n\\begin{abstract}\nThe \\cuba\\ library provides new implementations of four general-purpose\nmultidimensional integration algorithms: Vegas, Suave, Divonne, and\nCuhre.  Suave is a new algorithm, Divonne is a known algorithm to which\nimportant details have been added, and Vegas and Cuhre are new\nimplementations of existing algorithms with only few improvements over\nthe original versions.  All four algorithms can integrate vector\nintegrands and have very similar Fortran, C/C++, and Mathematica\ninterfaces.\n\\end{abstract}\n\n%========================================================================\n\n\\section{Introduction}\n\nMany problems in physics (and elsewhere) involve computing an integral,\nand often enough this has to be done numerically, as the analytical\nresult is known only in a limited number of cases.  In one dimension,\nthe situation is quite satisfactory: standard packages, such as\n\\textsc{Quadpack} \\cite{quadpack}, reliably integrate a broad class of\nfunctions in modest CPU time.  The same is unfortunately not true for\nmultidimensional integrals.\n\nThis paper presents the \\cuba\\ library with new implementations of four\nalgorithms for multidimensional numerical integration: Vegas, Suave,\nDivonne, and Cuhre.  They have a C/C++, Fortran, and Mathematica\ninterface each and are invoked in a very similar way, thus making them\neasily interchangeable, \\eg for comparison purposes.  All four can\nintegrate vector integrands.  Cuhre is a deterministic algorithm, the\nothers use Monte Carlo methods.\n\nVegas is the simplest of the four.  It uses importance sampling for\nvariance reduction, but is only in some cases competitive in terms of\nthe number of samples needed to reach a prescribed accuracy. \nNevertheless, it has a few improvements over the original algorithm\n\\cite{Vegas1,Vegas2} and comes in handy for cross-checking the results\nof other methods.\n\nSuave is a new algorithm which combines the advantages of two popular\nmethods: importance sampling as done by Vegas and subregion sampling in\na manner similar to Miser \\cite{Miser}.  By dividing into subregions,\nSuave manages to a certain extent to get around Vegas' difficulty to\nadapt its weight function to structures not aligned with the coordinate\naxes.\n\nDivonne is a further development of the CERNLIB routine D151\n\\cite{Divonne}.  Divonne works by stratified sampling, where the\npartitioning of the integration region is aided by methods from\nnumerical optimization.  A number of improvements have been added to\nthis algorithm, the most significant being the possibility to supply\nknowledge about the integrand.  Narrow peaks in particular are difficult\nto find without sampling very many points, especially in high\ndimensions.  Often the exact or approximate location of such peaks is\nknown from analytic considerations, however, and with such hints the\ndesired accuracy can be reached with far fewer points.\n\nCuhre\\footnote{%\n\tThe D from the original name was dropped since the \\cuba\\ \n\tlibrary uses double precision throughout.}\nemploys a cubature rule for subregion estimation in a globally adaptive\nsubdivision scheme \\cite{dcuhre}.  It is hence a deterministic, not a\nMonte Carlo method.  In each iteration, the subregion with the largest\nerror is halved along the axis where the integrand has the largest\nfourth difference.  Cuhre is quite powerful in moderate dimensions, and\nis usually the only viable method to obtain high precision, say relative\naccuracies much below $10^{-3}$.\n\nThe new algorithms were coded from scratch in C, which is a compromise\nof sorts between C++ and Fortran 77, combining ease of linking to\nFortran code with the availability of reasonable memory management.  The\ndeclarations have been chosen such that the routines can be called from\nFortran directly.  The Mathematica versions are based on the same C code\nand use the MathLink API to communicate with Mathematica.\n\n%========================================================================\n\n\\section{Vegas}\n\nVegas is a Monte Carlo algorithm that uses importance sampling as a\nvariance-reduction technique.  Vegas iteratively builds up a\npiecewise constant weight function, represented on a rectangular grid. \nEach iteration consists of a sampling step followed by a refinement of\nthe grid.  The exact details of the algorithm can be found in\n\\cite{Vegas1,Vegas2} and shall not be reproduced here.\n\nChanges with respect to the original version are:\n\\begin{itemize}\n\\item\nSobol quasi-random numbers \\cite{Sobol} rather than pseudo-random\nnumbers are used by default.  Empirically, this seems to accelerate\nconvergence quite a bit, most noticeably in the early stages of the\nintegration.\n\nFrom theoretical considerations it is of course known (see \\eg\n\\cite{Niederreiter}) that quasi-random sequences yield a convergence\nrate of $\\order(\\log^{n_d} n_s/n_s)$, where $n_d$ is the number of\ndimensions and $n_s$ the number of samples, which is much better than\nthe usual $\\order(1/\\sqrt{n_s})$ for ordinary Monte Carlo.  But these\nconvergence rates are meaningful only for large $n_s$ and so it came as\na pleasant surprise that the gains are considerable already at the\nbeginning of the sampling process.  It shows that quasi-Monte Carlo\nmethods blend well with variance-reduction techniques such as importance\nsampling.\n\nSimilarly, it was not clear from the outset whether the statistical\nstandard error would furnish a suitable error estimate since\nquasi-random numbers are decidedly non-random in a number of respects. \nYet also here empirical evidence suggests that the standard error works\njust as well as for pseudo-random numbers.\n\n\\item\nThe present implementation allows the number of samples to be increased\nin each iteration.  With this one can mimic the strategy of calling\nVegas with a small number of samples first to `get the grid right' and\nthen using an alternate entry point to perform the `full job' on the\nsame grid with a larger number of samples.\n\n\\item\nThe option to add simple stratified sampling on top of the importance\nsampling, as proposed in the appendix of \\cite{Vegas1}, has not been\nimplemented in the present version.  Tests with the Vegas version from\n\\cite{NumRecipes}, which contains this feature, showed that convergence\nwas accelerated only when the original pseudo-random numbers were used\nand that with quasi-random numbers convergence was in fact even slower \nin some cases.\n\\end{itemize}\nVegas' major weakness is that it uses a separable (product) weight\nfunction.  As a consequence, Vegas can offer significant improvements\nonly as far as the integrand's characteristic regions are aligned with\nthe coordinate axes.\n\n%========================================================================\n\n\\section{Suave}\n\nSuave (short for \\textsc{su}bregion-\\textsc{a}daptive \\textsc{ve}gas)\nuses Vegas-like importance sampling combined with a globally adaptive\nsubdivision strategy: Until the requested accuracy is reached, the\nregion with the largest error at the time is bisected in the dimension\nin which the fluctuations of the integrand are reduced most.  The number\nof new samples in each half is prorated for the fluctuation in that\nhalf.\n\nA similar method, known as recursive stratified sampling, is implemented\nin Miser \\cite{Miser}.  Miser always samples a fixed number of points,\nhowever, which is somewhat undesirable since it does not stop once the\nprescribed accuracy is reached.\n\nSuave first samples the integration region in a Vegas-like step, \\ie\nusing importance sampling with a separable weight function.  It then\nslices the integration region in two, as Miser would do.  Suave does not\nimmediately recurse on those subregions, however, but maintains a list\nof all subregions and selects the region with the largest absolute error\nfor the next cycle of sampling and subdivision.  That is, Suave uses\nglobal error estimation and terminates when the requested relative or\nabsolute accuracy is attained.\n\nThe information on the weight function collected in one Vegas step is\nnot lost.  Rather, the grid from which the weight function is computed\nis stretched and re-used on the subregions.  A region which is the\nresult of $m - 1$ subdivisions thus has had $m$ Vegas iterations\nperformed on it.\n\nThe improvements over Vegas and Miser come at a price, which is the\namount of memory required to hold all the samples.  Memory consumption\nis not really severe on modern hardware, however.  The component that\nscales worst is the one proportional to the number of samples, which is\n$$\n8 (n_d + n_c + 1) n_s\\text{ bytes}\\,,\n$$\nwhere $n_d$ is the number of dimensions of the integral, $n_c$ the\nnumber of components of the integrand, and $n_s$ the number of samples. \nFor a million samples on a scalar integrand of 10 variables, this works\nout to 96 megabytes -- not all that enormous these days. \n\n\n\\subsection{Description of the algorithm}\n\nAs Suave is a new algorithm, the following description will be fairly\ndetailed.  For greater notational clarity, $n_c$-dimensional vectors are\ndenoted with a vector arrow ($\\cvec f$\\,) and $n_d$-dimensional vectors\nwith boldface letters ($\\dvec x$) in the following, where $n_d$ is the\ndimension of the integral and $n_c$ the number of components of the\nintegrand.\n\nThe essential inputs are $\\epsrel$ and $\\epsabs$, the relative and\nabsolute accuracies, $\\nnew$, the number of samples added in each\niteration, $\\nmax$, the maximum number of samples allowed, and $p$, a\nflatness parameter described below.\n\nSuave has a main loop which calls a Vegas-like sampling step.  The main\nloop is responsible for subdividing the subregions and maintaining the\ntotals.  The sampling step does the actual sampling on the subregions\nand computes the region results.\n\n\n\\subsubsection{Main loop}\n\\label{sect:suavemain}\n\n\\begin{enumerate}\n\\item\nInitialize the random-number generator and allocate a data structure for\nthe entire integration region.  Initialize its Vegas grid with\nequidistant bins.\n\n\\item\nSample the entire integration region with $\\nnew$ points. This gives an\ninitial estimate of the integral $\\cvec I\\tot$, the variance \n$\\cvec\\sigma\\tot^{\\,2}$, and $\\cvec\\chi\\tot^{\\,2}$.\n\n\\item\nFind the component $c$ for which $r_c = \\sigma\\ctot/\n\\max(\\epsabs, \\epsrel |I\\ctot|)$ is maximal.\n\nIf none of the $r_c$'s exceeds unity, indicate success and return.\n\n\\item\nIf the number of samples spent so far equals or exceeds $\\nmax$,\nindicate failure and return.\n\n\\item\nFind the region $r$ with the largest $\\sigma_c^2$.\n\n\\item\nFind the dimension $d$ which minimizes $F_c(r_L^d) + F_c(r_R^d)$, where\n$r_{L,R}^d$ are the left and right halves of $r$ with respect to $d$. \n$F_c(r_{L,R}^d)$ is the fluctuation of the samples that fall into\n$r_{L,R}^d$ and is computed as\n\\begin{equation}\n\\label{eq:fluct}\nF_c(r_{L,R}^d)\n= \\biggl[\\left\\|\n    1 + \\tilde F_c(\\dvec x_i\\in r_{L,R}^d)\n  \\right\\|_p\\biggr]^{2/3}\n= \\biggl[\\sum\\left|\n    1 + \\tilde F_c(\\dvec x_i\\in r_{L,R}^d)\n  \\right|^p\\,\\biggr]^{2/(3p)},\n\\end{equation}\nwhere all samples $\\dvec x_i$ that fall into the respective half are \nused in the norm/sum and the single-sample fluctuation $\\tilde F_c$ is \ndefined as\n$$\n\\tilde F_c(\\dvec x) =\n  w(\\dvec x) \\, \\left|\\frac{f_c(\\dvec x) - I_c(r)}{I_c(r)}\\right|\n             \\, \\frac{|f_c(\\dvec x) - I_c(r)|}{\\sigma_c(r)}\\,.\n$$\nThis empirical recipe combines the relative deviation from the region\nmean, $(f - I)/I$, with the $\\chi$ value, $|f - I|/\\sigma$, weighted by \nthe Vegas weight $w$ corresponding to sample $\\dvec x$.  Note that the \n$I_c$ and $\\sigma_c$ values of the entire region $r$ are used.\n\nSamples strongly contribute to $F$ the more they lie away from the\npredicted mean \\emph{and} the more they lie out of the predicted error\nband.  Tests have shown that large values of $p$ are beneficial for\n`flat' integrands, whereas small values are preferred if the integrand\nis `volatile' and has high peaks.  $p$ has thus been dubbed a flatness\nparameter.  The effect comes from the fact that with increasing $p$, $F$\nbecomes more and more dominated by `outliers,' \\ie points with a large \n$\\tilde F$.\n\nThe power 2/3 in Eq.~(\\ref{eq:fluct}) is also used in Miser, where it is\nmotivated as the exponent that gives the best variance reduction\n(\\cite{NumRecipes}, p.~315).\n\n\\item\nRefine the grid associated with $r$, \\ie incorporate the information\ngathered on the integrand in the most recent sample over $r$ into the\nweight function.  This is done precisely as in Vegas (see\n\\cite{Vegas1}), with the extension that if the integrand has more than\none component, the marginal densities are computed not from $f^2$ but\nfrom the weighted sum\\footnote{%\n\tIt is fairly obvious that scale-invariant quantities must be \n\tused in the sum, otherwise the component with the largest \n\tabsolute scale would dominate.  It is less clear whether $\\eta_0 \n\t= (\\int f_c\\,\\rd\\dvec x)^2 = I\\ctot^2$, $\\eta_1 = (\\int \n\t|f_c|\\,\\rd\\dvec x)^2$, or $\\eta_2 = \\int f_c^2\\,\\rd\\dvec x$ (or \n\tany other) make the best weights.  Empirically, $\\eta_0$ turns \n\tout to be both slightly superior in convergence and easier to \n\tcompute than $\\eta_1$ and $\\eta_2$ and has thus been chosen in \n\tSuave.\n\n\tA possible explanation for this is that in cases where there \n\tare large compensations within the integral, \\ie when $\\int \n\tf_c\\,\\rd\\dvec x\\ll\\int |f_c|\\,\\rd\\dvec x$, it is particularly \n\tnecessary for the overall accuracy that component $c$ be sampled \n\taccurately, and thus be given more weight in $\\overline{f^2}$, \n\tand this is better accomplished by dividing $f_c^2$ by the \n\t``small'' number  $\\eta_0$ than by the ``large'' number $\\eta_1$ \n\tor $\\eta_2$.}\n$$\n\\overline{f^2} = \\sum_{c = 1}^{n_c} \\frac{f_c^2}{I\\ctot^2}\\,.\n$$\n\n\\item\nBisect $r$ in dimension $d$:\n\nAllocate a new region, $r_L$, and copy to $r_L$ those of $r$'s samples\nfalling into the left half.  Compute the Vegas grid for $r_L$ by\nappropriately ``stretching'' $r$'s grid, \\ie by interpolating all grid\npoints of $r$ with values less than 1/2.\n\nSet up $r_R$ for the right half analogously.\n\n\\item\nSample $r_L$ with $n_L = \\max\\Bigl(\\frac{F_c(r_L)}{F_c(r_L) + F_c(r_R)}\n\\nnew, \\nmin\\Bigr)$ and $r_R$ with $n_R = \\max(\\nnew - n_L, \\nmin)$ \npoints, where $\\nmin = 10$.\n\n\\item\nTo safeguard against underestimated errors, supplement the variances \nby the difference of the integral values in the following way:\n$$\n\\sigma_{c,\\text{new}}^2(r_{R,L}) = \\sigma_c^2(r_{R,L})\n  \\left(1 + \\frac{\\Delta_c}\n                 {\\sqrt{\\sigma_c^2(r_L) + \\sigma_c^2(r_R)}}\\right)^2 +   \n  \\Delta_c^2\n$$\nfor each component $c$, where $\\cvec\\Delta = \\frac 14 |\\cvec I(r_L) + \n\\cvec I(r_R) - \\cvec I(r)|$.\n\nThis acts as a penalty for regions whose integral value changes\nsignificantly by the subdivision and effectively moves them up in the\norder of regions to be subdivided next.\n\n\\item\nUpdate the totals: Subtract $r$'s integral, variance, and $\\chi^2$-value\nfrom the totals and add those of $r_L$ and $r_R$.\n\n\\item\nDiscard $r$, put $r_L$ and $r_R$ in the list of regions.\n\n\\item\nGo to Step 3.\n\\end{enumerate}\n\n\n\\subsubsection{Sampling step}\n\nThe function which does the actual sampling is a modified Vegas\niteration.  It is invoked with two arguments: $r$, the region to be\nsampled and $n_m$, the number of new samples.\n\n\\begin{enumerate}\n\\item\nSample a set of $n_m$ new points using the weight function given by the\ngrid associated with $r$.  For a region which is the result of $m - 1$\nsubdivisions, the list of samples now consists of $m$ sets of samples.\n\n\\item\nFor each set of samples, compute the mean $\\cvec I_i$ and variance\n$\\cvec\\sigma_i^{\\,2}$.\n\n\\item\nCompute the results for the region as\n$$\nI_c = \\frac{\\sum_{i = 1}^m w_{i,c} I_{i,c}}{\\sum_{i = 1}^m w_{i,c}},\n\\quad\n\\sigma_c^2 = \\frac 1{\\sum_{i = 1}^m w_{i,c}},\n\\quad\n\\chi_c^2 = \\frac 1{\\sigma_c^2}\\left[\n  \\frac{\\sum_{i = 1}^m w_{i,c} I_{i,c}^2}{\\sum_{i = 1}^m w_{i,c}} -\n  I_c^2\\right],\n$$\nwhere the inverse of the set variances are used as weights, $w_{i,c} =\n1/\\sigma_{i,c}^2$.  This is simply Gaussian error propagation.\n\nFor greater numerical stability, $\\chi_c^2$ is actually computed as\n$$\n\\chi_c^2\n= \\sum_{i = 1}^m w_{i,c} I_{i,c}^2 -\n  I_c \\sum_{i = 1}^m w_{i,c} I_{i,c}\n= \\sum_{i = 2}^m w_{i,c} I_{i,c} (I_{i,c} - I_{1,c}) -\n  I_c \\sum_{i = 2}^m w_{i,c} (I_{i,c} - I_{1,c})\\,.\n$$\n\\end{enumerate}\n\n%========================================================================\n\n\\section{Divonne}\n\nDivonne uses stratified sampling for variance reduction, that is, it \npartitions the integration region such that all subregions have an \napproximately equal value of a quantity called the spread $\\cvec s$,\ndefined as\n\\begin{equation}\n\\cvec s(r) = \\frac 12 V(r)\n  \\Bigl(\\max_{\\dvec x\\in r} \\cvec f(\\dvec x) - \n        \\min_{\\dvec x\\in r} \\cvec f(\\dvec x)\\Bigr)\\,,\n\\end{equation}\nwhere $V(r)$ is the volume of region $r$.  What sets Divonne apart from\nSuave is that the minimum and maximum of the integrand are sought using\nmethods from numerical optimization.  Particularly in high dimensions, \nthe chance that one of the previously sampled points lies in or even \nclose to the true extremum is fairly small.\n\nOn the other hand, the numerical minimization is beset with the usual\npitfalls, \\ie starting from the lowest of a (relatively small) number of\nsampled points, Divonne will move directly into the local minimum\nclosest to the starting point, which may or may not be close to the\nabsolute minimum.\n\nDivonne is a lot more complex than Suave and Vegas but also\nsignificantly faster for many integrands.  For details on the methods\nused in Divonne please consult the original references \\cite{Divonne}. \nNew features with respect to the CERNLIB version (Divonne 4) are:\n\\begin{itemize}\n\\item\nIntegration is possible in dimensions 2 through 33 (not 9 as before).  \nGoing to higher dimensions is a matter of extending internal tables \nonly.\n\n\\item\nThe possibility has been added to specify the location of possible\npeaks, if such are known from analytical considerations.  The idea here\nis to help the integrator find the extrema of the integrand, and narrow\npeaks in particular are a challenge for the algorithm.  Even if only the\napproximate location is known, this feature of hinting the integrator\ncan easily cut an order of magnitude out of the number of samples needed\nto reach the required accuracy for complicated integrands.  The points\ncan be specified either statically, by passing a list of points at the\ninvocation, or dynamically, through a subroutine called for each\nsubregion.\n\n\\item\nOften the integrand function cannot sample points lying on or very\nclose to the integration border.  This can be a problem with Divonne\nwhich actively searches for the extrema of the integrand and homes in on\npeaks regardless of whether they lie on the border.  The user may \nhowever specify a border region in which integrand values are not \nobtained directly, but extrapolated from two points inside the `safe' \ninterior.\n\n\\item\nThe present algorithm works in three phases, not two as before.  Phase 1\nperforms the partitioning as outlined above.  From the preliminary\nresults obtained in this phase, Divonne estimates the number of samples\nnecessary to reach the desired accuracy in phase 2, the final\nintegration phase.  Once the phase-2 sample for a particular subregion\nis in, a $\\chi^2$ test is used to assess whether the two sample averages\nare consistent with each other within their error bounds.  Subregions\nwhich fail this test move on to phase 3, the refinement phase, where\nthey can be subdivided again or sampled a third time with more points,\ndepending on the parameters set by the user.\n\n\\item\nFor all three phases the user has a selection of methods to obtain the\nintegral estimate: a Korobov \\cite{Korobov} or Sobol \\cite{Sobol}\nquasi-random sample of given size, a Mersenne Twister\n\\cite{MersenneTwister} or Ranlux \\cite{Ranlux} pseudo-random sample of\ngiven size, and the cubature rules of Genz and Malik \\cite{GenzMalik}\nof degree 7, 9, 11, and 13 that are also used in Cuhre.  The latter are\nembedded rules and hence provide an intrinsic error estimate (that is,\nan error estimate not based on the spread).  When this independent\nerror estimate is available, it supersedes the spread-based error when\ncomputing the total error.  Also, regions whose spread-based error\nexceeds the intrinsic error are selected for refinement, too.\n\nIn spite of these novel options, the cubature rules of the original \nDivonne algorithm were not implemented.\n\\end{itemize}\n\nDue to its complexity, the new Divonne implementation was painstakingly\ntested against the CERNLIB routine to make sure it produces the same\nresults before adding the new features.\n\n%========================================================================\n\n\\section{Cuhre}\n\nCuhre is a deterministic algorithm which uses one of several cubature\nrules of polynomial degree in a globally adaptive subdivision scheme.\nThe subdivision algorithm is similar to Suave's (see Sect.\\\n\\ref{sect:suavemain}) and works as follows:\n\nWhile the total estimated error exceeds the requested bounds:\n\n1) choose the region with the largest estimated error,\n\n2) bisect this region along the axis with the largest fourth \n   difference,\n\n3) apply the cubature rule to the two subregions,\n\n4) merge the subregions into the list of regions and update the \n   totals.\n\nDetails on the algorithm and on the cubature rules employed in Cuhre can\nbe found in the original references \\cite{dcuhre}.  The present\nimplementation offers only superficial improvements, such as an\ninterface consistent with the other \\cuba\\ routines and a slightly\nsimpler invocation, \\eg one does not have to allocate a workspace.\n\nIn moderate dimensions Cuhre is very competitive, particularly if the \nintegrand is well approximated by polynomials.  As the dimension \nincreases, the number of points sampled by the cubature rules rises \nconsiderably, however, and by the same token the usefulness declines.\nFor the lower dimensions, the actual number of points that are spent per \ninvocation of the basic integration rule are listed in the following \ntable.\n\\begin{center}\n\\begin{tabular}{l|ccccccccc}\nnumber of dimensions &\n\t4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\\\ \\hline\npoints in degree-7 rule &\n\t65 & 103 & 161 & 255 & 417 & 711 & 1265 & 2335 & 4433 \\\\\npoints in degree-9 rule &\n\t153 & 273 & 453 & 717 & 1105 & 1689 & 2605 & 4117 & 6745\n\\end{tabular}\n\\end{center}\n\n%========================================================================\n\n\\section{Download and Compilation}\n\nThe \\cuba\\ package can be downloaded from\n\\Code{http://feynarts.de/cuba}.  The gzipped tar file unpacks into\na directory \\Code{Cuba-$m$.$n$}.  Change into this directory and type\n\\begin{verbatim}\n   ./configure\n   make\n\\end{verbatim}\nThis should create\n\\begin{tabbing}\n\\verb=   =\\Code{libcuba.a} \\hspace{10em}\n\\= --- the \\cuba\\ library, \\\\\n\\verb=   =\\Code{Vegas}, \\Code{Suave}, \\Code{Divonne}, \\Code{Cuhre}\n\\> --- the MathLink executables, \\\\\n\\verb=   =\\Code{demo-c}, \\Code{demo-fortran}\n\\> --- the demonstration programs, \\\\\n\\verb=   =\\Code{partview}\n\\> --- the partition viewer.\n\\end{tabbing}\n\\cuba\\ can also be built in parts: ``make lib'' builds only the \\cuba\\\nlibrary, ``make math'' builds only the MathLink executables, ``make\ndemos'' builds only the demo programs, and ``make tools'' builds only \nthe partition viewer.\n\nThe MathLink executables require mcc, the MathLink compiler, and the\npartition viewer needs Qt.  Compilation of the corresponding parts will\nbe switched off by default if configure does not find these tools.\n\nBesides the usual autoconf options, configure understands\n\\begin{itemize}\n\\item\n\\Code{--with-maxdim=\\Var{n_d^{\\text{max}}}} sets an upper limit on the\nnumber of dimensions.\n\n\\item\n\\Code{--with-maxcomp=\\Var{n_c^{\\text{max}}}} sets an upper limit on the\nnumber of components.\n\n\\item\n\\Code{--with-real=10} uses \\Code{long double} instead of \\Code{double} \nfor all real variables.  On x86 hardware this typically corresponds to\n80-bit extended precision (\\Code{REAL*10}).  Resulting files are\nsuffixed with `$\\ell$' (\\Code{libcubal.a}, \\Code{cubal.h}, \n\\Code{Vegasl}, etc.).\n\n\\item\n\\Code{--with-real=16} uses \\Code{\\uscore\\uscore float128} (128-bit \nquadruple precision, \\Code{REAL*16}) for all real variables.  This \nrequires gcc version 4.6 or higher. No MathLink executables are built \nin this case as no MathLink functions are available for this type.  \nResulting files are suffixed with `q' (\\Code{libcubaq.a}, \n\\Code{cubaq.h}).\n\\end{itemize}\n\nLinking Fortran or C/C++ code that uses one of the algorithms is\nstraightforward, just add \\Code{-lcuba} (for the \\cuba\\ library) and\n\\Code{-lm} (for the math library) to the compiler command line, as in\n\\begin{verbatim}\n   f77 -o myexecutable mysource.f -lcuba -lm\n   cc -o myexecutable mysource.c -lcuba -lm\n\\end{verbatim}\nThe \\Code{demo} subdirectory contains the source for the demonstration\nprograms in Fortran 77, C, and Mathematica, as well as the test suite\nused in Sect.~\\ref{sect:tests}, which is also written in Mathematica.\n\n%========================================================================\n\n\\section{User Manual}\n\n\\subsection{Usage in Fortran}\n\nAlthough written in C, the declarations have been chosen such that the\nroutines are directly accessible from Fortran, \\ie no wrapper code is\nneeded.  In fact, Vegas, Suave, Divonne, and Cuhre can be called as if \nthey were Fortran subroutines respectively declared as\n\\begin{verbatim}\n        subroutine vegas(ndim, ncomp, integrand, userdata, nvec,\n     &    epsrel, epsabs, flags, seed, mineval, maxeval,\n     &    nstart, nincrease, nbatch, gridno, statefile, spin,\n     &    neval, fail, integral, error, prob)\n\\end{verbatim}\n\\begin{verbatim}\n        subroutine suave(ndim, ncomp, integrand, userdata, nvec,\n     &    epsrel, epsabs, flags, seed, mineval, maxeval,\n     &    nnew, nmin, flatness, statefile, spin,\n     &    nregions, neval, fail, integral, error, prob)\n\\end{verbatim}\n\\begin{verbatim}\n        subroutine divonne(ndim, ncomp, integrand, userdata, nvec,\n     &    epsrel, epsabs, flags, seed, mineval, maxeval,\n     &    key1, key2, key3, maxpass,\n     &    border, maxchisq, mindeviation,\n     &    ngiven, ldxgiven, xgiven, nextra, peakfinder,\n     &    statefile, spin,\n     &    nregions, neval, fail, integral, error, prob)\n\\end{verbatim}\n\\begin{verbatim}\n        subroutine cuhre(ndim, ncomp, integrand, userdata, nvec,\n     &    epsrel, epsabs, flags, mineval, maxeval,\n     &    key, statefile, spin,\n     &    nregions, neval, fail, integral, error, prob)\n\\end{verbatim}\n\n\n\\subsubsection{Common Arguments}\n\\label{sect:commonargs}\n\n\\begin{itemize}\n\\VarIn{integer ndim}\nthe number of dimensions of the integral.\n\n\\VarIn{integer ncomp}\nthe number of components of the integrand.\n\n\\VarIn{integer integrand}\nthe integrand.  The external function which computes the \nintegrand is expected to be declared as\n\\begin{verbatim}\n  integer function integrand(ndim, x, ncomp, f, userdata, nvec, core)\n  integer ndim, ncomp, nvec, core\n  double precision x(ndim,nvec), f(ncomp,nvec)\n\\end{verbatim}\nThe integrand receives \\Code{nvec} samples in \\Code{x} and is supposed \nto fill the array \\Code{f} with the corresponding integrand values.  \nNote that \\Code{nvec} indicates the actual number of points passed to \nthe integrand here and may be smaller than the \\Code{nvec} given to the \nintegrator.\n\nThe return value is irrelevant unless it is $-999$, in the case of which \nthe integration will be aborted immediately.  This might happen if a \nparameterized integrand turns out not to yield sensible values for a \nparticular parameter set (passed \\eg through \\Code{userdata}).\n\nThe worker process the integrand is running on is indicated in \n\\Code{core}, where $\\Code{core} < 0$ indicates an Accelerator, \n$\\Code{core}\\geqslant 0$ a regular (CPU) core, and \\Code{32768} the \nmaster itself (more details in Sect.~\\ref{sect:cores}).\n\nThe latter three arguments, \\Code{userdata}, \\Code{nvec}, and \n\\Code{core} are optional and may be omitted if unused, \\ie the integrand \nis minimally declared (for $\\Code{nvec} = 1$) as\n\\begin{verbatim}\n  integer function integrand(ndim, x, ncomp, f)\n  integer ndim, ncomp\n  double precision x(ndim), f(ncomp)\n\\end{verbatim}\n\n\\VarIn{(arbitrary type) userdata}\nuser data passed to the integrand.  Unlike its counterpart in the\nintegrand definition, this argument must be present though it may\ncontain a dummy value, \\eg \\Code{0}.\n\n\\VarIn{integer nvec}\nThe maximum number of points to be given to the integrand\nroutine in each invocation.  Usually this is 1 but if the integrand\ncan profit from \\eg SIMD vectorization, a larger value can be chosen.\n\n\\VarIn{double precision epsrel, epsabs}\nthe requested relative and absolute accuracies.\nThe integrator tries to find an estimate $\\hat I$ for the integral $I$\nwhich for every component $c$ fulfills $|\\hat I_c - I_c|\\leqslant\n\\max(\\epsabs, \\epsrel |I_c|)$.\n\n\\VarIn{integer flags}\nflags governing the integration:\n\\begin{itemize}\n\\item Bits 0 and 1 are taken as the verbosity level, \\ie 0 to 3, \nunless the \\Code{CUBAVERBOSE} environment variable contains an even\nhigher value (used for debugging).\n\nLevel 0 does not print any output, level 1 prints `reasonable'\ninformation on the progress of the integration, level 2 also echoes the\ninput parameters, and level 3 further prints the subregion results (if\napplicable).\n\n\\item Bit 2 = 0,\nall sets of samples collected on a subregion during the various \niterations or phases contribute to the final result.\n\nBit 2 = 1,\nonly the last (largest) set of samples is used in the final result.\n\n\\item (Vegas and Suave only)\n\nBit 3 = 0,\napply additional smoothing to the importance function, this moderately\nimproves convergence for many integrands,\n\nBit 3 = 1,\nuse the importance function without smoothing, this should be chosen if \nthe integrand has sharp edges.\n\n\\item Bit 4 = 0,\ndelete the state file (if one is chosen) when the integration terminates \nsuccessfully,\n\nBit 4 = 1,\nretain the state file.\n\n\\item \n(Vegas only)\n\nBit 5 = 0,\ntake the integrator's state from the state file, if one is present.\n\nBit 5 = 1,\nreset the integrator's state even if a state file is present, \\ie \nkeep only the grid.  Together with Bit 4 this allows a grid adapted by \none integration to be used for another integrand.\n\n\\item Bits 8--31 =: \\Code{level} determines the random-number generator\n(see below).\n\\end{itemize}\n\nTo select \\eg last samples only and verbosity level 2, pass 6 = 4 + 2\nfor the flags.\n\n\\VarIn{integer seed}\nthe seed for the pseudo-random-number generator.\n\nThe random-number generator is chosen as follows:\n\\begin{center}\n\\begin{tabular}{l|l|l}\n\\Code{seed} & \\Code{level} & Generator \\\\\n& {\\small (bits 8--31 of \\Code{flags})} & \\\\ \\hline\nzero & N/A & Sobol (quasi-random), \\\\\nnon-zero & zero & Mersenne Twister (pseudo-random), \\\\\nnon-zero & non-zero & Ranlux (pseudo-random).\n\\end{tabular}\n\\end{center}\n\nRanlux implements Marsaglia and Zaman's 24-bit RCARRY algorithm with\ngeneration period $p$, \\ie for every 24 generated numbers used, another\n$p - 24$ are skipped.  The luxury level is encoded in \\Code{level} as\nfollows:\n\\begin{itemize}\n\\item Level 1 ($p = 48$):\n\tvery long period, passes the gap test but fails spectral test.\n\\item Level 2 ($p = 97$):\n\tpasses all known tests, but theoretically still defective.\n\\item Level 3 ($p = 223$):\n\tany theoretically possible correlations have very small \n\tchance of being observed.\n\\item Level 4 ($p = 389$):\n\thighest possible luxury, all 24 bits chaotic.\n\\end{itemize}\nLevels 5--23 default to 3, values above 24 directly specify the period\n$p$.  Note that Ranlux's original level 0, (mis)used for selecting\nMersenne Twister in \\cuba, is equivalent to \\Code{level = 24}.\n\n\\VarIn{integer mineval}\nthe minimum number of integrand evaluations required.\n\n\\VarIn{integer maxeval}\nthe (approximate) maximum number of integrand evaluations allowed.\n\n\\VarIn{character*(*) statefile}\na filename for storing the internal state.  To not store the internal \nstate, put \\Code{\"\"} (empty string) or \\Code{\\%VAL(0)} (null pointer).\n\n\\cuba\\ can store its entire internal state (\\ie all the information to \nresume an interrupted integration) in an external file.  The state file \nis updated after every iteration.  If, on a subsequent invocation, a\n\\cuba\\ routine finds a file of the specified name, it loads the internal\nstate and continues from the point it left off.  Needless to say, using\nan existing state file with a different integrand generally leads to\nwrong results.\n\nThis feature is useful mainly to define `check-points' in long-running \nintegrations from which the calculation can be restarted.\n\nOnce the integration reaches the prescribed accuracy, the state file\nis removed, unless bit 4 of \\Code{flags} (see above) explicitly requests \nthat it be kept.\n\n\\VarIn{integer*8 spin}\nthe `spinning cores' pointer, which has three options:\n\\begin{itemize}\n\\item A value of \\Code{-1} or \\Code{\\%VAL(0)} (null pointer) means that \nthe integrator completely takes care of starting and terminating child \nprocesses for the integration (if available/enabled), \\ie after the \nintegrator returns there are no child processes running any longer.  \nNote that a `naive' \\Code{-1} (which is an \\Code{integer}, not an \n\\Code{integer*8}) is explicitly allowed.\n\n\\item A zero-initialized variable \\Code{spin} instructs the integrator \nto start child processes for the integration but keep them running and \nstore the `spinning cores' pointer in \\Code{spin} on exit.  Take care \nthat in this case you have to explicitly terminate the child processes \nusing \\Code{cubawait} later on (see Sect.~\\ref{sect:spinning}).\n\n\\item A non-zero variable \\Code{spin} means that the cores are already \nrunning as the result of some prior integration or an explicit \n\\Code{cubafork} call (see Sect.~\\ref{sect:spinning}).\n\\end{itemize}\nThe actual type of \\Code{spin} is irrelevant, the variable must merely\nbe wide enough to store a C \\Code{void *}.\n\n\\VarOut{integer nregions}\nthe actual number of subregions needed (not present in Vegas).\n\n\\VarOut{integer neval}\nthe actual number of integrand evaluations needed.\n\n\\VarOut{integer fail}\nan error flag:\n\\begin{itemize}\n\\item\n$\\Code{fail} = 0$, the desired accuracy was reached,\n\\item\n$\\Code{fail} = -1$, dimension out of range,\n\\item\n$\\Code{fail} > 0$, the accuracy goal was not met within the allowed\nmaximum number of integrand evaluations.  While Vegas, Suave, and Cuhre\nsimply return 1, Divonne can estimate the number of points by which\n\\Code{maxeval} needs to be increased to reach the desired accuracy and\nreturns this value.\n\\end{itemize}\n\n\\VarOut{double precision integral(ncomp)}\nthe integral of \\Code{integrand} over the unit hypercube.\n\n\\VarOut{double precision error(ncomp)}\nthe presumed absolute error of \\Code{integral}.\n\n\\VarOut{double precision prob(ncomp)}\nthe $\\chi^2$-probability (not the $\\chi^2$-value itself!) that\n\\Code{error} is not a reliable estimate of the true integration \nerror\\footnote{%\n\tTo judge the reliability of the result expressed through\n\t\\Code{prob}, remember that it is the null hypothesis that is \n\ttested by the $\\chi^2$ test, which is that \\Code{error} \n\t\\emph{is} a reliable estimate.  In statistics, the null \n\thypothesis may be rejected only if \\Code{prob} is fairly close \n\tto unity, say $\\Code{prob} > .95$.}.\n\\end{itemize}\n\n\n\\subsubsection{Vegas-specific Arguments}\n\\label{sect:vegasargs}\n\n\\begin{itemize}\n\\VarIn{integer nstart}\nthe number of integrand evaluations per iteration to start with.\n\n\\VarIn{integer nincrease}\nthe increase in the number of integrand evaluations per iteration.\n\n\\VarIn{integer nbatch}\nthe batch size for sampling.\n\nVegas samples points not all at once, but in batches of size\n\\Code{nbatch}, to avoid excessive memory consumption.  1000 is a\nreasonable value, though it should not affect performance too much.\n\n\\VarIn{integer gridno}\nthe slot in the internal grid table.\n\nIt may accelerate convergence to keep the grid accumulated during one\nintegration for the next one, if the integrands are reasonably similar \nto each other.  Vegas maintains an internal table with space for ten\ngrids for this purpose.  The slot in this grid is specified by\n\\Code{gridno}.\n\nIf a grid number between 1 and 10 is selected, the grid is not discarded\nat the end of the integration, but stored in the respective slot of the \ntable for a future invocation.  The grid is only re-used if the \ndimension of the subsequent integration is the same as the one it \noriginates from.\n\nIn repeated invocations it may become necessary to flush a slot in\nmemory, in which case the negative of the grid number should be set.\n\\end{itemize}\n\nVegas actually passes the integrand two more arguments, \\ie the \nintegrand function is really declared as\n\\begin{verbatim}\n  integer function integrand(ndim, x, ncomp, f, userdata, nvec, core,\n    weight, iter)\n  integer ndim, ncomp, nvec, core, iter\n  double precision x(ndim,nvec), f(ncomp,nvec), weight(nvec)\n\\end{verbatim}\nwhere \\Code{weight} contains the weight of the point being sampled and\n\\Code{iter} the current iteration number.  These extra arguments may\nsafely be ignored, however.\n\n\n\\subsubsection{Suave-specific Arguments}\n\n\\begin{itemize}\n\\VarIn{integer nnew}\nthe number of new integrand evaluations in each subdivision.\n\n\\VarIn{integer nmin}\nthe minimum number of samples a former pass must contribute to a \nsubregion to be considered in that region's compound integral value.\nIncreasing \\Code{nmin} may reduce jumps in the $\\chi^2$ value.\n\n\\VarIn{double precision flatness}\nthe parameter $p$ in Eq.~(\\ref{eq:fluct}), \\ie the type of norm used to\ncompute the fluctuation of a sample.  This determines how prominently\n`outliers,' \\ie individual samples with a large fluctuation, figure in\nthe total fluctuation, which in turn determines how a region is split\nup.  As suggested by its name, \\Code{flatness} should be chosen large\nfor `flat' integrands and small for `volatile' integrands with high\npeaks.  Note that since \\Code{flatness} appears in the exponent, one\nshould not use too large values (say, no more than a few hundred) lest\nterms be truncated internally to prevent overflow.\n\\end{itemize}\n\nLike Vegas, Suave also passes the two optional arguments \\Code{weight}\nand \\Code{iter} to the integrand (see Sect.~\\ref{sect:vegasargs}).\n\n\n\\subsubsection{Divonne-specific Arguments}\n\\label{sect:divonneargs}\n\n\\begin{itemize}\n\\VarIn{integer key1}\ndetermines sampling in the partitioning phase:\n\n$\\Code{key1} = 7, 9, 11, 13$ selects the cubature rule of degree \n\\Code{key1}.  Note that the degree-11 rule is available only in 3\ndimensions, the degree-13 rule only in 2 dimensions.\n\nFor other values of \\Code{key1}, a quasi-random sample of\n$n_1 = |\\Code{key1}|$ points is used, where the sign of \\Code{key1}\ndetermines the type of sample,\n\\begin{itemize}\n\\item\n$\\Code{key1} > 0$, use a Korobov quasi-random sample,\n\\item\n$\\Code{key1} < 0$, use a ``standard'' sample\n(a Sobol quasi-random sample if \\Code{seed} = 0, otherwise a\npseudo-random sample).\n\\end{itemize}\n\n\\VarIn{integer key2}\ndetermines sampling in the final integration phase:\n\n$\\Code{key2} = 7, 9, 11, 13$ selects the cubature rule of degree \n\\Code{key2}.  Note that the degree-11 rule is available only in 3\ndimensions, the degree-13 rule only in 2 dimensions.\n\nFor other values of \\Code{key2}, a quasi-random sample is used, where \nthe sign of \\Code{key2} determines the type of sample,\n\\begin{itemize}\n\\item\n$\\Code{key2} > 0$, use a Korobov quasi-random sample,\n\\item\n$\\Code{key2} < 0$, use a ``standard'' sample (see description of \n\\Code{key1} above),\n\\end{itemize}\nand $n_2 = |\\Code{key2}|$ determines the number of points,\n\\begin{itemize}\n\\item\n$n_2\\geqslant 40$, sample $n_2$ points,\n\\item\n$n_2 < 40$, sample $n_2\\,\\nneed$ points, where $\\nneed$ is the number of\npoints needed to reach the prescribed accuracy, as estimated by Divonne \nfrom the results of the partitioning phase.\n\\end{itemize}\n\n\\VarIn{integer key3}\nsets the strategy for the refinement phase:\n\n$\\Code{key3} = 0$, do not treat the subregion any further.\n\n$\\Code{key3} = 1$, split the subregion up once more.\n\nOtherwise, the subregion is sampled a third time with \\Code{key3}\nspecifying the sampling parameters exactly as \\Code{key2} above.\n\n\\VarIn{integer maxpass}\ncontrols the thoroughness of the partitioning phase:\nThe partitioning phase terminates when the estimated total number of \nintegrand evaluations (partitioning plus final integration) does not \ndecrease for \\Code{maxpass} successive iterations.\n\nA decrease in points generally indicates that Divonne discovered new\nstructures of the integrand and was able to find a more effective\npartitioning.  \\Code{maxpass} can be understood as the number of\n`safety' iterations that are performed before the partition is accepted\nas final and counting consequently restarts at zero whenever new\nstructures are found.\n\n\\VarIn{double precision border}\nthe width of the border of the integration region.  Points falling into\nthis border region will not be sampled directly, but will be\nextrapolated from two samples from the interior.  Use a non-zero \n\\Code{border} if the integrand function cannot produce values\ndirectly on the integration boundary.\n\n\\VarIn{double precision maxchisq}\nthe maximum $\\chi^2$ value a single subregion is allowed to have in the\nfinal integration phase.  Regions which fail this $\\chi^2$ test and\nwhose sample averages differ by more than \\Code{mindeviation} move on\nto the refinement phase.\n\n\\VarIn{double precision mindeviation}\na bound, given as the fraction of the requested error of the entire\nintegral, which determines whether it is worthwhile further examining a\nregion that failed the $\\chi^2$ test.  Only if the two sampling averages\nobtained for the region differ by more than this bound is the region\nfurther treated.\n\n\\VarIn{integer ngiven}\nthe number of points in the \\Code{xgiven} array.\n\n\\VarIn{integer ldxgiven}\nthe leading dimension of \\Code{xgiven}, \\ie the offset between one \npoint and the next in memory.\n\n\\VarIn{double precision xgiven(ldxgiven,ngiven)}\na list of points where the integrand might have peaks.  Divonne will\nconsider these points when partitioning the integration region.  The\nidea here is to help the integrator find the extrema of the integrand in\nthe presence of very narrow peaks.  Even if only the approximate\nlocation of such peaks is known, this can considerably speed up\nconvergence.\n\n\\VarIn{integer nextra}\nthe maximum number of extra points the peak-finder subroutine will\nreturn.  If \\Code{nextra} is zero, \\Code{peakfinder} is not called\nand an arbitrary object may be passed in its place, \\eg just 0.\n\n\\VarIn{external peakfinder}\nthe peak-finder subroutine.  This subroutine is called whenever a region \nis up for subdivision and is supposed to point out possible peaks lying \nin the region, thus acting as the dynamic counterpart of the static list \nof points supplied in \\Code{xgiven}.  It is expected to be declared as\n\\begin{verbatim}\n  subroutine peakfinder(ndim, b, n, x, userdata)\n  integer ndim, n\n  double precision b(2,ndim)\n  double precision x(ldxgiven,n)\n\\end{verbatim}\nThe bounds of the subregion are passed in the array \\Code{b}, where \n\\Code{b(1,\\Var{d})} is the lower and \\Code{b(2,\\Var{d})} the upper \nbound in dimension \\Var{d}.  On entry, \\Code{n} specifies the maximum \nnumber of points that may be written to \\Code{x}.  On exit, \\Code{n} \nmust contain the actual number of points in \\Code{x}.\n\\end{itemize}\nDivonne actually passes the integrand one more argument, \\ie the \nintegrand function is really declared as\n\\begin{verbatim}\n  integer function integrand(ndim, x, ncomp, f, userdata, nvec, core, phase)\n  integer ndim, ncomp, nvec, core, phase\n  double precision x(ndim,nvec), f(ncomp,nvec)\n\\end{verbatim}\nThe last argument, \\Code{phase}, indicates the integration phase:\n\\begin{itemize}\n\\item 0, sampling of the points in \\Code{xgiven},\n\\item 1, partitioning phase,\n\\item 2, final integration phase,\n\\item 3, refinement phase.\n\\end{itemize}\nThis information might be useful if the integrand takes long to compute\nand a sufficiently accurate approximation of the integrand is available. \nThe actual value of the integral is only of minor importance in the\npartitioning phase, which is instead much more dependent on the peak\nstructure of the integrand to find an appropriate tessellation.  An\napproximation which reproduces the peak structure while leaving out the\nfine details might hence be a perfectly viable and much faster\nsubstitute when $\\Code{phase} < 2$.\n\nIn all other instances, \\Code{phase} can be ignored and it is\nentirely admissible to define the integrand without it.\n\n\n\\subsubsection{Cuhre-specific Arguments}\n\n\\begin{itemize}\n\\VarIn{integer key}\nchooses the basic integration rule:\n\n$\\Code{key} = 7, 9, 11, 13$ selects the cubature rule of degree \n\\Code{key}.  Note that the degree-11 rule is available only in 3\ndimensions, the degree-13 rule only in 2 dimensions.\n\nFor other values, the default rule is taken, which is the degree-13 rule \nin 2 dimensions, the degree-11 rule in 3 dimensions, and the degree-9 \nrule otherwise.\n\\end{itemize}\n\n\n\\subsubsection{Visualizing the Tessellation}\n\nSuave, Divonne, and Cuhre work by dividing the integration region into \nsubregions for integration.  When verbosity level 3 is selected in the \nflags, the actual tessellation is written out on screen and can be \nvisualized with the partview tool.  To this end, the output of the \nprogram invoking \\cuba\\ is piped through partview, \\eg\n\\begin{verbatim}\n   mycubaprogram | partview 1 2 1 3\n\\end{verbatim}\nopens a window with two tabs showing the 1--2 and 1--3 plane of the \ntessellation.  The saturation of the colours corresponds to the area of\nthe region, \\ie smaller regions are displayed in a darker shade.\n\n\n\\subsection{Usage in C/C++}\n\nBeing written in C, the algorithms can of course be used in C/C++ \ndirectly.  The declarations are as follows:\n\\begin{verbatim}\ntypedef int (*integrand_t)(const int *ndim, const double x[],\n  const int *ncomp, double f[], void *userdata);\n\ntypedef void (*peakfinder_t)(const int *ndim, const double b[],\n  int *n, double x[], void *userdata);\n\\end{verbatim}\n\\begin{verbatim}\nvoid Vegas(const int ndim, const int ncomp,\n  integrand_t integrand, void *userdata, const int nvec,\n  const double epsrel, const double epsabs,\n  const int flags, const int seed,\n  const int mineval, const int maxeval,\n  const int nstart, const int nincrease, const int nbatch,\n  const int gridno, const char *statefile, void *spin,\n  int *neval, int *fail,\n  double integral[], double error[], double prob[])\n\\end{verbatim}\n\\begin{verbatim}\nvoid Suave(const int ndim, const int ncomp,\n  integrand_t integrand, void *userdata, const int nvec,\n  const double epsrel, const double epsabs,\n  const int flags, const int seed,\n  const int mineval, const int maxeval,\n  const int nnew, const int nmin,\n  const double flatness, const char *statefile, void *spin,\n  int *nregions, int *neval, int *fail,\n  double integral[], double error[], double prob[])\n\\end{verbatim}\n\\begin{verbatim}\nvoid Divonne(const int ndim, const int ncomp,\n  integrand_t integrand, void *userdata, const int nvec,\n  const double epsrel, const double epsabs,\n  const int flags, const int seed,\n  const int mineval, const int maxeval,\n  const int key1, const int key2, const int key3,\n  const int maxpass, const double border,\n  const double maxchisq, const double mindeviation,\n  const int ngiven, const int ldxgiven, double xgiven[],\n  const int nextra, peakfinder_t peakfinder,\n  const char *statefile, void *spin,\n  int *nregions, int *neval, int *fail,\n  double integral[], double error[], double prob[])\n\\end{verbatim}\n\\begin{verbatim}\nvoid Cuhre(const int ndim, const int ncomp,\n  integrand_t integrand, void *userdata, const int nvec,\n  const double epsrel, const double epsabs,\n  const int flags,\n  const int mineval, const int maxeval,\n  const int key, const char *statefile, void *spin,\n  int *nregions, int *neval, int *fail,\n  double integral[], double error[], double prob[])\n\\end{verbatim}\nThese prototypes are contained in \\Code{cuba.h} which should (in C) or\nmust (in C++) be included when using the \\cuba\\ routines.  The arguments\nare as in the Fortran case, with the obvious translations, \\eg\n\\Code{double precision} = \\Code{double}.  Note, however, the\ndeclarations of the integrand and peak-finder functions, which expect\npointers to integers rather than integers.  This is required for\ncompatibility with Fortran.\n\nThe \\verb=integrand_t= type glosses over the fact that the extra \n\\Code{nvec} argument is routinely passed to the integrand and neither \ndoes it mention the extra arguments passed by \\Code{Vegas}, \n\\Code{Suave}, and \\Code{Divonne} (see Sects.\\ \\ref{sect:vegasargs} and \n\\ref{sect:divonneargs}).  This is usually just what is needed for \n`simple' invocations, \\ie with the `correct' prototypes the compiler \nwould only generate unnecessary warnings (in C) or errors (in C++).  In \nthe rare cases where the integrand actually has more arguments, an \nexplicit typecast to \\verb=integrand_t= must be used in the invocation. \nIn the presence of an \\Code{nvec} argument, the \\Code{x} and \\Code{f} \narguments are actually two-dimensional arrays, \\Code{x[*nvec][*ndim]} \nand \\Code{f[*nvec][*ncomp]}.\n\n\n\\subsection{Usage in Mathematica}\n\nThe Mathematica versions are based on essentially the same C code and\ncommunicate with Mathematica via the MathLink API.  When building the\npackage, the executables \\Code{Vegas}, \\Code{Suave},\n\\Code{Divonne}, and \\Code{Cuhre} are compiled for use in\nMathematica.  In Mathematica one first needs to load them with the \n\\Code{Install} function, as in\n\\begin{verbatim}\n   Install[\"Divonne\"]\n\\end{verbatim}\nwhich makes a Mathematica function of the same name available.  These \nfunctions are used almost like \\Code{NIntegrate}, only some options \nare different.  For example,\n\\begin{verbatim}\n   Vegas[x^2/(Cos[x + y + 1] + 5), {x,0,5}, {y,0,5}]\n\\end{verbatim}\nintegrates a scalar function, or\n\\begin{verbatim}\n   Suave[{Sin[z] Exp[-x^2 - y^2],\n          Cos[z] Exp[-x^2 - y^2]}, {x,-1,1}, {y,-1,3}, {z,0,1}]\n\\end{verbatim}\nintegrates a vector.  As is evident, the integration region can be \nchosen different from the unit hypercube.  Innermore boundaries may \ndepend on outermore integration variables, \\eg\n\\verb=Cuhre[1, {x,0,1}, {y,0,x}]= gives the area of the unit triangle.\n\nThe sampling function uses \\Code{MapSample} to map the integrand over \nthe data points.  This is by default set to \\Code{Map}, but can be \nchanged (after \\Code{Install}) \\eg to \\Code{ParallelMap} to take \nadvantage of parallelization (see Sect.~\\ref{sect:parallel} for more \ndetails).\n\nThe functions return a list which contains the results for each\ncomponent of the integrand in a sublist \\{integral estimate, estimated \nabsolute error, $\\chi^2$ probability\\}.  For the Suave example above \nthis would be\n\\begin{verbatim}\n   {{1.1216, 0.000991577, 0.0000104605}, \n    {2.05246, 0.00146661, 0.00920716}}\n\\end{verbatim}\nThe other parameters are specified via the following options.  Default \nvalues are given on the right-hand sides of the rules.\n\n\n\\subsubsection{Common Options}\n\n\\begin{itemize}\n\\Option{PrecisionGoal}{3}\nthe number of digits of relative accuracy to seek, that is, $\\epsrel =\n10^{-\\Code{PrecisionGoal}}$.\n\n\\Option{AccuracyGoal}{12}\nthe number of digits of absolute accuracy to seek, that is, $\\epsabs =\n10^{-\\Code{AccuracyGoal}}$.  The integrator tries to find an estimate\n$\\hat I$ for the integral $I$ which for every component $c$ fulfills\n$|\\hat I_c - I_c|\\leqslant \\max(\\epsabs, \\epsrel I_c)$.\n\n\\Option{MinPoints}{0}\nthe minimum number of integrand evaluations required.\n\n\\Option{MaxPoints}{50000}\nthe (approximate) maximum number of integrand evaluations allowed.\n\n\\Option{Verbose}{1}\nhow much information to print on intermediate results, can take values\nfrom 0 to 3.\n\nLevel 0 does not print any output, level 1 prints `reasonable'\ninformation on the progress of the integration, level 2 also echoes the\ninput parameters, and level 3 further prints the subregion results (if\napplicable).  Note that the subregion boundaries in the level-3 printout\nrefer to the unit hypercube, \\ie are possibly scaled with respect to the\nintegration limits passed to Mathematica.  This is because the\nunderlying C code, which emits the output, is unaware of any scaling of\nthe integration region, which is done entirely in Mathematica.\n\n\\Option{Final}{All \\Var{or} Last}\nwhether only the last (largest) or all sets of samples collected on a\nsubregion during the various iterations or phases contribute to the\nfinal result.\n\n\\Option{PseudoRandom}{False}\nwhether pseudo-random numbers are used for sampling instead of Sobol\nquasi-random numbers.  Values \\Code{True} and \\Code{0} select the\nMersenne Twister algorithm, any other integer $n$ chooses Ranlux with\nluxury level $n$ (see Sect.~\\ref{sect:commonargs}).\n\n\\Option{PseudoRandomSeed}{Automatic}\nthe seed for the pseudo-random-number generator.\n\n\\Option{Regions}{False}\nwhether to return the tessellation of the integration region (thus not\npresent in Vegas, which does not partition the integration region).\n\nIf \\Code{Regions -> True} is chosen, a two-component list is returned,\nwhere the first element is the list of regions, and the second element\nis the integration result as described above.  Each region is specified\nin the form \\Code{Region[\\Var{x_{\\mathrm{ll}}},\\,\\Var{x_{\\mathrm{ur}}},%\n\\,\\Var{res},\\,\\Var{df}]}, where \\Var{x_{\\text{ll}}} and\n\\Var{x_{\\text{ur}}} are the multidimensional equivalents of the lower\nleft and upper right corner, \\Var{res} is the integration result for the\nsubregion, given in the same form as the total result but with the\n$\\chi^2$ value instead of the $\\chi^2$ probability, and \\Var{df} are the\ndegrees of freedom corresponding to the $\\chi^2$ values. \n\nCuhre cannot state a $\\chi^2$ value separately for each region, hence\nthe $\\chi^2$ values and degrees of freedom are omitted from the\n\\Code{Region} information.\n\n\\Option{StateFile}{\"\"}\nthe file name for storing the internal state.  If a non-empty string is\ngiven here, Vegas will store its entire internal state (\\ie all the\ninformation to resume an interrupted integration) in this file after\nevery iteration.  If, on a subsequent invocation, Vegas finds a file of\nthe specified name, it loads the internal state and continues from the\npoint it left off.  Needless to say, using an existing state file with a\ndifferent integrand generally leads to wrong results.\n\nThis feature is useful mainly to define `check-points' in long-running\nintegrations from which the calculation can be restarted.\n\n\\Option{RetainStateFile}{False}\nwhether the state file shall be kept even if the integration terminates \nnormally, \\ie reaches either the prescribed accuracy or the maximum \nnumber of points.\n\n\\Option{Compiled}{True}\nwhether to compile the integrand function before use.  Note two caveats:\n\\begin{itemize}\n\\item\nThe function values still have to pass through the MathLink interface,\nand in the course of this are truncated to machine precision.  Not\ncompiling the integrand will thus in general not deliver more accurate\nresults.\n\\item\nCompilation should be switched off if the compiled integrand shows\nunexpected behaviour.  As the Mathematica online help points out, ``the\nnumber of times and the order in which objects are evaluated by\n\\Code{Compile} may be different from ordinary Mathematica code.''\n\\end{itemize}\n\\end{itemize}\n\n\n\\subsubsection{Vegas-specific Options}\n\n\\begin{itemize}\n\\Option{NStart}{1000}\nthe number of integrand evaluations per iteration to start with.\n\n\\Option{NIncrease}{500}\nthe increase in the number of integrand evaluations per iteration.\n\n\\Option{NBatch}{1000}\nthe number of points sent in one MathLink packet to be sampled by\nMathematica.  This setting will at most affect performance and should \nnot normally need to be changed.\n\n\\Option{GridNo}{0}\nthe slot in the internal grid table.\n\nIt may accelerate convergence to keep the grid accumulated during one\nintegration for the next one, if the integrands are reasonably similar\nto each other.  Vegas maintains an internal table with space for ten\ngrids for this purpose.  If a \\Code{GridNo} between 1 and 10 is chosen,\nthe grid is not discarded at the end of the integration, but stored for\na future invocation.  The grid is only re-used if the dimension of the\nsubsequent integration is the same as the one it originates from.  \nA negative grid number initializes the indicated slot before the\nintegration (for details see Sect.~\\ref{sect:vegasargs}).\n\n\\Option{ResetState}{False}\nwhether the integrator's state should be reset even if a state file is\npresent, \\ie only the grid be kept.  Together with the\n\\Code{RetainStateFile} option this allows a grid adapted by one \nintegration to be used for another integrand.\n\n\\item\nDuring the evaluation of the integrand, the global variable\n\\Code{\\$Weight} is set to the weight of the point being sampled and\n\\Code{\\$Iteration} to the current iteration number.\n\\end{itemize}\n\n\n\\subsubsection{Suave-specific Options}\n\n\\begin{itemize}\n\\Option{NNew}{1000}\nthe number of new integrand evaluations in each subdivision.\n\n\\Option{NMin}{2}\nthe minimum number of samples a former pass must contribute to a \nsubregion to be considered in that region's compound integral value.\nIncreasing \\Code{NMin} may reduce jumps in the $\\chi^2$ value.\n\n\\Option{Flatness}{50}\nthe parameter $p$ in Eq.~(\\ref{eq:fluct}), \\ie the type of norm used to\ncompute the fluctuation of a sample.  This determines how prominently\n`outliers,' \\ie individual samples with a large fluctuation, figure in\nthe total fluctuation, which in turn determines how a region is split\nup.  As suggested by its name, \\Code{Flatness} should be chosen large\nfor `flat' integrands and small for `volatile' integrands with high\npeaks.  Note that since \\Code{Flatness} appears in the exponent, one\nshould not use too large values (say, no more than a few hundred) lest\nterms be truncated internally to prevent overflow.\n\n\\item\nDuring the evaluation of the integrand, the global variable\n\\Code{\\$Weight} is set to the weight of the point being sampled and\n\\Code{\\$Iteration} to the current iteration number.\n\\end{itemize}\n\n\n\\subsubsection{Divonne-specific Options}\n\n\\begin{itemize}\n\\Option{Key1}{47}\nan integer which governs sampling in the partitioning phase:\n\n$\\Code{Key1} = 7, 9, 11, 13$ selects the cubature rule of degree \n\\Code{Key1}.  Note that the degree-11 rule is available only in 3\ndimensions, the degree-13 rule only in 2 dimensions.\n\nFor other values of \\Code{Key1}, a quasi-random sample of\n$n_1 = |\\Code{Key1}|$ points is used, where the sign of \\Code{Key1}\ndetermines the type of sample,\n\\begin{itemize}\n\\item\n$\\Code{Key1} > 0$, use a Korobov quasi-random sample,\n\\item\n$\\Code{Key1} < 0$, use a ``standard'' sample\n(a Sobol quasi-random sample in the case \\Code{PseudoRandom -> False},\notherwise a pseudo-random sample).\n\\end{itemize}\n\n\\Option{Key2}{1}\nan integer which governs sampling in the final integration phase:\n\n$\\Code{Key2} = 7, 9, 11, 13$ selects the cubature rule of degree \n\\Code{Key2}.  Note that the degree-11 rule is available only in 3\ndimensions, the degree-13 rule only in 2 dimensions.\n\nFor other values of \\Code{Key2}, a quasi-random sample is used, where \nthe sign of \\Code{Key2} determines the type of sample,\n\\begin{itemize}\n\\item\n$\\Code{Key2} > 0$, use a Korobov quasi-random sample,\n\\item\n$\\Code{Key2} < 0$, use a ``standard'' sample\n(see description of \\Code{Key1} above),\n\\end{itemize}\nand $n_2 = |\\Code{Key2}|$ determines the number of points,\n\\begin{itemize}\n\\item\n$n_2\\geqslant 40$, sample $n_2$ points,\n\\item\n$n_2 < 40$, sample $n_2\\,\\nneed$ points, where $\\nneed$ is the number of\npoints needed to reach the prescribed accuracy, as estimated by Divonne \nfrom the results of the partitioning phase.\n\\end{itemize}\n\n\\Option{Key3}{1}\nan integer which sets the strategy for the refinement phase:\n\n$\\Code{Key3} = 0$, do not treat the subregion any further.\n\n$\\Code{Key3} = 1$, split the subregion up once more.\n\nOtherwise, the subregion is sampled a third time with \\Code{Key3}\nspecifying the sampling parameters exactly as \\Code{Key2} above.\n\n\\Option{MaxPass}{5}\nthe number of passes after which the partitioning phase terminates.\nThe partitioning phase terminates when the estimated total number of \nintegrand evaluations (partitioning plus final integration) does not \ndecrease for \\Code{MaxPass} successive iterations.\n\nA decrease in points generally indicates that Divonne discovered new\nstructures of the integrand and was able to find a more effective\npartitioning.  \\Code{MaxPass} can be understood as the number of\n`safety' iterations that are performed before the partition is accepted\nas final and counting consequently restarts at zero whenever new\nstructures are found.\n\n\\Option{Border}{0}\nthe width of the border of the integration region.  Points falling into\nthis border region are not sampled directly, but are extrapolated from\ntwo samples from the interior.  Use a non-zero \\Code{Border} if the\nintegrand function cannot produce values directly on the integration\nboundary.\n\nThe border width always refers to the unit hypercube, \\ie it is not\nrescaled if the integration region is not the unit hypercube.\n\n\\Option{MaxChisq}{10}\nthe maximum $\\chi^2$ value a single subregion is allowed to have in the\nfinal integration phase.  Regions which fail this $\\chi^2$ test and\nwhose sample averages differ by more than \\Code{MinDeviation} move on\nto the refinement phase.\n\n\\Option{MinDeviation}{.25}\na bound, given as the fraction of the requested error of the entire\nintegral, which determines whether it is worthwhile further examining a\nregion that failed the $\\chi^2$ test.  Only if the two sampling averages\nobtained for the region differ by more than this bound is the region\nfurther treated.\n\n\\Option{Given}{\\lbrac\\rbrac}\na list of points where the integrand might have peaks.  A point is a\nlist of $n_d$ real numbers, where $n_d$ is the dimension of the\nintegral.\n\nDivonne will consider these points when partitioning the integration\nregion.  The idea here is to help the integrator find the extrema of the\nintegrand in the presence of very narrow peaks.  Even if only the\napproximate location of such peaks is known, this can considerably speed\nup convergence.\n\n\\Option{NExtra}{0}\nthe maximum number of points that will be considered in the output of \nthe \\Code{PeakFinder} function.\n\n\\Option{PeakFinder}{(\\lbrac\\rbrac\\&)}\nthe peak-finder function.  This function is called whenever a region is\nup for subdivision and is supposed to point out possible peaks lying in\nthe region, thus acting as the dynamic counterpart of the static list of\npoints supplied with \\Code{Given}.  It is invoked with two arguments,\nthe multidimensional equivalents of the lower left and upper right\ncorners of the region being investigated, and must return a (possibly\nempty) list of points.  A point is a list of $n_d$ real numbers, where \n$n_d$ is the dimension of the integral.\n\\end{itemize}\n\n\n\\subsubsection{Cuhre-specific Options}\n\n\\begin{itemize}\n\\Option{Key}{0}\nchooses the basic integration rule:\n\n$\\Code{Key} = 7, 9, 11, 13$ selects the cubature rule of degree \n\\Code{Key}.  Note that the degree-11 rule is available only in 3\ndimensions, the degree-13 rule only in 2 dimensions.\n\nFor other values, the default rule is taken, which is the degree-13 rule \nin 2 dimensions, the degree-11 rule in 3 dimensions, and the degree-9 \nrule otherwise.\n\\end{itemize}\n\n\n\\subsubsection{Visualizing the Tessellation}\n\nSuave, Divonne, and Cuhre work by dividing the integration region into \nsubregions for integration.  The tessellation is returned together with \nthe integration results when the option \\Code{Regions -> True} is\nset.  Such output can be visualized using the Mathematica program \n\\Code{partview.m} that comes with \\cuba.  The invocation is \\eg\n\\begin{verbatim}\n  result = Divonne[..., Regions -> True]\n  << tools/partview.m\n  PartView[result, 1, 2]\n\\end{verbatim}\nwhich displays the 1--2 plane of the tessellation.  The saturation of \nthe colours corresponds to the area of the region, \\ie smaller regions\nare displayed in a darker shade.\n\n\n\\subsection{Long-integer Versions}\n\nFor both Fortran and C/C++ there exist versions of the integration \nroutines that take 64-bit integers for all number-of-points-type \nquantities.  These should be used in cases where convergence is not \nreached within the ordinary 32-bit integer range ($2^{31} - 1$).\n\nThe long-integer versions are distinguished by the ``\\Code{ll}''\nprefix.  Their specific invocations are, in Fortran,\n\\begin{alltt}\n        subroutine llvegas(ndim, ncomp, integrand, userdata, \\ul{nvec},\n     &    epsrel, epsabs, flags, seed, \\ul{mineval}, \\ul{maxeval},\n     &    \\ul{nstart}, \\ul{nincrease}, \\ul{nbatch}, gridno, statefile, spin,\n     &    \\ul{neval}, fail, integral, error, prob)\n\\end{alltt}\n\\begin{alltt}\n        subroutine llsuave(ndim, ncomp, integrand, userdata, \\ul{nvec},\n     &    epsrel, epsabs, flags, seed, \\ul{mineval}, \\ul{maxeval},\n     &    \\ul{nnew}, \\ul{nmin}, flatness, statefile, spin,\n     &    nregions, \\ul{neval}, fail, integral, error, prob)\n\\end{alltt}\n\\begin{alltt}\n        subroutine lldivonne(ndim, ncomp, integrand, userdata, \\ul{nvec},\n     &    epsrel, epsabs, flags, seed, \\ul{mineval}, \\ul{maxeval},\n     &    key1, key2, key3, maxpass,\n     &    border, maxchisq, mindeviation,\n     &    \\ul{ngiven}, ldxgiven, xgiven, \\ul{nextra}, peakfinder,\n     &    statefile, spin,\n     &    nregions, \\ul{neval}, fail, integral, error, prob)\n\\end{alltt}\n\\begin{alltt}\n        subroutine llcuhre(ndim, ncomp, integrand, userdata, \\ul{nvec},\n     &    epsrel, epsabs, flags, seed, \\ul{mineval}, \\ul{maxeval},\n     &    key, statefile, spin,\n     &    nregions, \\ul{neval}, fail, integral, error, prob)\n\\end{alltt}\nThe correspondences for C/C++ are obvious and are given explicitly in\nthe include file \\Code{cuba.h}.  The arguments are as for the normal\nversions except that all underlined variables are of type\n\\Code{integer*8} in Fortran and \\Code{long long int} in C/C++.\n\n%========================================================================\n\n\\section{Parallelization}\n\\label{sect:parallel}\n\nNumerical integration is perfectly suited for parallel execution, which \ncan significantly speed up the computation as it generally incurs only a \nvery small overhead.  The parallelization procedure is rather different \nin Fortran/C/C++ and in Mathematica.  We shall deal with the latter \nfirst because it needs only a short explanation.  The remainder of this \nchapter is then devoted to the Fortran/C/C++ case.\n\n%------------------------------------------------------------------------\n\n\\subsection{Parallelization in Mathematica}\n\nThe Mathematica version of \\cuba\\ performs its sampling through a \nfunction \\Code{MapSample}.  By default this is identical to \\Code{Map}, \n\\ie the serial version, so to parallelize one merely needs to redefine \n\\Code{MapSample = ParallelMap} (after loading \\cuba).\n\nIf the integrand depends on user-defined symbols or functions, their \ndefinitions must be distributed to the workers beforehand using \n\\Code{DistributeDefinitions} and likewise required packages must be \nloaded with \\Code{ParallelNeeds} instead of \\Code{Needs}; this is \nexplained in detail in the Mathematica manual.\n\n%------------------------------------------------------------------------\n\n\\subsection{Parallelization in Fortran and C/C++}\n\nIn Fortran and C/C++ the \\cuba\\ library can (and usually does) \nautomatically parallelize the sampling of the integrand.  It \nparallelizes through \\Code{fork} and \\Code{wait} which, though slightly \nless performant than pthreads, do not require reentrant code.  \n(Reentrancy may not even be under full control of the programmer, for \nexample Fortran's I/O is usually non-reentrant.)  Worker processes are \nstarted and shut down only as few times as possible, however, so the \nperformance penalty is really quite minor even for non-native fork \nimplementations such as Cygwin's.  Parallelization is not available on \nnative Windows for lack of the \\Code{fork} function.\n\nThe communication of samples to and from the workers happens through IPC \nshared memory (\\Code{shmget} and colleagues), or if that is not \navailable, through a \\Code{socketpair} (two-way pipe).  Remarkably, the \nformer's anticipated performance advantage turned out to be hardly \nperceptible.  Possibly there are cache-coherence issues introduced by \nseveral workers writing simultaneously to the same shared-memory area.\n\n%------------------------------------------------------------------------\n\n\\subsubsection{Starting and stopping the workers}\n\\label{sect:spinning}\n\nThe workers are usually started and stopped automatically by \\cuba's \nintegration routines, but the user may choose to start them manually or \nkeep them running after one integration and shut them down later, \\eg at \nthe end of the program, which can be slightly more efficient.  The \nlatter mode is referred to as `Spinning Cores' and must be employed with \ncertain care, for running workers will not `see' subsequent changes in \nthe main program's data (\\eg global variables, common blocks) or code \n(\\eg via \\Code{dlsym}) unless special arrangements are made (\\eg shared \nmemory).\n\nThe spinning cores are controlled through the `\\Code{spin}' argument of \nthe \\cuba\\ integration routines (Sect.~\\ref{sect:commonargs}):\n\\begin{itemize}\n\\item A value of \\Code{-1} or \\Code{\\%VAL(0)} (in Fortran) or \n\\Code{NULL} (in C/C++) tells the integrator to start and shut down the \nworkers autonomously.  This is the usual case.  No workers will still be \nrunning after the integrator returns.  No special precautions need to be \ntaken to communicate \\eg global data to the workers.  Note that it is \nexpressly allowed to pass a `naive' \\Code{-1} (which is an \n\\Code{integer}, not an \\Code{integer*8}) in Fortran.\n\n\\item Passing a zero-initialized variable for \\Code{spin} instructs the \nintegrator to start the workers but keep them running on return and \nstore the `spinning cores' pointer in \\Code{spin} for future use.  The \nspinning cores must later be terminated explicitly by \\Code{cubawait}, \nthus invocation would schematically look like this:\n\n\\hfill\\begin{minipage}{.4\\hsize}\n\\begin{verbatim}\ninteger*8 spin\nspin = 0\ncall vegas(..., spin, ...)\n...\ncall cubawait(spin)\n\\end{verbatim}\n\\end{minipage}\\hfill\\begin{minipage}{.4\\hsize}\n\\begin{verbatim}\nvoid *spin = NULL;\n\nVegas(..., &spin, ...);\n...\ncubawait(&spin);\n\\end{verbatim}\n\\end{minipage}\\hfill\n\n\\item A non-zero \\Code{spin} variable is assumed to contain a valid \n`spinning cores' pointer either from a former integration or an explicit \ninvocation of \\Code{cubafork}, as in:\n\n\\hfill\\begin{minipage}{.4\\hsize}\n\\begin{verbatim}\ninteger*8 spin\ncall cubafork(spin)\ncall vegas(..., spin, ...)\n...\ncall cubawait(spin)\n\\end{verbatim}\n\\end{minipage}\\hfill\\begin{minipage}{.4\\hsize}\n\\begin{verbatim}\nvoid *spin;\ncubafork(&spin);\nVegas(..., &spin, ...);\n...\ncubawait(&spin);\n\\end{verbatim}\n\\end{minipage}\\hfill\n\\end{itemize}\n\n%------------------------------------------------------------------------\n\n\\subsubsection{Accelerators and Cores}\n\\label{sect:cores}\n\nBased on the strategy used to distribute samples, \\cuba\\ distinguishes \ntwo kinds of workers.\n\nWorkers of the first kind are referred to as `Accelerators' even though \n\\cuba\\ does not actually send anything to a GPU or Accelerator in the \nsystem by itself -- this can only be done by the integrand routine.  The \nassumption behind this strategy is that the integrand evaluation is \nrunning on a device so highly parallel that the sampling time is more or \nless independent of the number of points, up to the number of threads \n$p\\accel$ available in hardware.  \\cuba\\ tries to send exactly $p\\accel$ \npoints to each core -- never more, less only for the last batch.  To \nsample \\eg 2400 points on three accelerators with $p\\accel = 1000$, \n\\cuba\\ sends batches of 1000/1000/400 and not, for example, 800/800/800 \nor 1200/1200.  The number of accelerators $n\\accel$ and their value of \n$p\\accel$ can be set through the environment variables\n\\begin{tabbing}\n\\verb|   CUBAACCEL=|$n\\accel$ \\hspace{10em}\\= (default: 0) \\\\\n\\verb|   CUBAACCELMAX=|$p\\accel$    \\> (default: 1000)\n\\end{tabbing}\nor, superseding the environment, an explicit\n\\begin{alltt}\n   call cubaaccel(\\(n\\accel\\), \\(p\\accel\\))\n\\end{alltt}\n\nCPU-bound workers are just called `Cores'.  Their distribution strategy \nis different in that all available cores are used and points are \ndistributed evenly.  In the example above, the batches would be \n800/800/800 thus.  Each core receives at least 10 points, or else fewer \ncores are used.  If no more than 10 points are requested in total, \n\\cuba\\ uses no workers at all but lets the master sample those few \npoints.  This happens during the partitioning phase of Divonne for \ninstance, where only single points are evaluated in the minimum/maximum \nsearch. Conversely, if the division of points by cores does not come out \neven, the remaining few points ($< n\\cores$) are simply added to the \nexisting batches, to avoid an extra batch because of rounding.  Sampling \n2001 points on two cores with $p\\cores = 1000$ will hence give two \nbatches 1001/1000 and not three batches 1000/1000/1.\n\nAlthough there is typically no hardware limit, a maximum number of \npoints per core, $p\\cores$, can be prescribed for Cores, too.  Unless \nthe integrand is known to evaluate equally fast at all points, a \nmoderate number for $p\\cores$ (10000, say) may actually increase \nperformance because it effectively load-levels the sampling.  For, a \nbatch always goes to the next free core so it doesn't matter much \nif one core is tied up with a batch that takes longer.\n\nThe number of cores $n\\cores$ and the value of $p\\cores$ can be set \nanalogously through the environment variables\n\\begin{tabbing}\n\\verb|   CUBACORES=|$n\\cores$ \\hspace{10em}\\= (default: no.\\ of idle cores) \\\\\n\\verb|   CUBACORESMAX=|$p\\cores$ \\> (default: 10000)\n\\end{tabbing}\nIf \\Code{CUBACORES} is unset, the idle cores on the present system are \ntaken (total cores minus load average), which means that a program \ncalling a \\cuba\\ routine will by default automatically parallelize on \nthe available cores.  Again, the environment can be overruled with an \nexplicit\n\\begin{alltt}\n   call cubacores(\\(n\\cores\\), \\(p\\cores\\))   \n\\end{alltt}\nUsing the environment has the advantage, though, that changing the \nnumber of cores to use does not require a re-compile, which is \nparticularly useful if one wants to run the program on several computers \n(with potentially different numbers of cores) simultaneously, say in a \nbatch queue.\n\nThe integrand function may use the `\\Code{core}' argument \n(Sect.~\\ref{sect:commonargs}) to distinguish Accelerators ($\\Code{core} \n< 0$) and Cores ($\\Code{core}\\geqslant 0$).  The special value \n$\\Code{core} = 32768$ ($2^{15}$) indicates that the master itself is \ndoing the sampling.\n\n%------------------------------------------------------------------------\n\n\\subsubsection{Worker initialization}\n\nUser subroutines for (de)initialization may be registered with\n\\begin{alltt}\n   call cubainit(initfun, initarg)           \\textrm{Fortran}\n   call cubaexit(exitfun, exitarg)\n\\end{alltt}\n\\begin{alltt}\n   cubainit(initfun, initarg);               \\textrm{C/C++}\n   cubaexit(exitfun, exitarg);\n\\end{alltt}\nand will be executed in every process before and after sampling.\nPassing a null pointer (\\Code{\\%VAL(0)} in Fortran, \\Code{NULL} in\nC/C++) as the first argument unregisters either subroutine.\n\nThe init/exit functions are actually called as\n\\begin{alltt}\n   call initfun(initarg, core)               \\textrm{Fortran}\n   call exitfun(exitarg, core)\n\\end{alltt}\n\\begin{alltt}\n   initfun(initarg, &core);                  \\textrm{C/C++}\n   exitfun(exitarg, &core);\n\\end{alltt}\nwhere \\Code{initarg} and \\Code{exitarg} are the user arguments given \nwith the registration (arbitrary in Fortran, \\Code{void *} in C/C++) \nand \\Code{core} indicates the core the function is being executed on, \nwith (as before) $\\Code{core} < 0$ for Accelerators, \n$\\Code{core}\\geqslant 0$ for Cores, and $\\Code{core} = 32768$ for the \nmaster.\n\nOn worker processes, the functions are respectively executed after \n\\Code{fork} and before \\Code{wait}, independently of whether the worker \nactually receives any samples.  The master executes them only when \nactual sampling is done.\nFor Accelerators, the init and exit functions are typically used to set \nup the device for the integrand evaluations, which for many devices must \nbe done per process, \\ie after the \\Code{fork}.\n\n%------------------------------------------------------------------------\n\n\\subsubsection{Concurrency issues}\n\nBy creating a new process image, \\Code{fork} circumvents all memory \nconcurrency, to wit: each worker modifies only its own copy of the \nparent's memory and never overwrites any other's data.  The programmer \nshould be aware of a few potential problems nevertheless:\n\\begin{itemize}\n\\item Communicating back results other than the intended output from the \nintegrand to the main program is not straightforward because, by the \nsame token, a worker cannot overwrite any common data of the master, it \nwill only modify its own copy.\n\nData exchange between workers is likewise not directly possible.  For \nexample, if one worker stores an intermediate result in a common block, \nthis will not be seen by the other workers.\n\nPossible solutions include using shared memory (\\Code{shmget} etc., see \nApp.~\\ref{app:shm}) and writing the output to file (but see next item \nbelow).\n\n\\item \\Code{fork} does not guard against competing use of other common \nresources.  For example, if the integrand function writes to a file \n(debug output, say), there is no telling in which order the lines will \nend up in the file, or even if they will end up as complete lines at \nall.  Buffered output should be avoided at the very least; better still, \nevery worker should write the output to its own file, \\eg with a \nfilename that includes the process id, as in:\n\\begin{verbatim}\n   character*32 filename\n   integer pid\n   data pid /0/\n   if( pid .eq. 0 ) then  \n     pid = getpid()\n     write(filename,'(\"output.\",I5.5)') pid\n     open(unit=4711, file=filename)\n   endif\n\\end{verbatim}\n\n\\item Fortran users are advised to flush (or close) any open files \nbefore calling \\cuba, \\ie \\Code{call flush(\\Var{unit})}.  The reason is \nthat the child processes inherit all file buffers, and \\emph{each} of \nthem will write out the buffer content at exit.  \\cuba\\ preemptively \nflushes the system buffers already (\\Code{fflush(NULL)}) but has no \ncontrol over Fortran's buffers.\n\\end{itemize}\nFor debugging, or if a malfunction due to concurrency issues is \nsuspected, a program should be tested in serial mode first, \\eg by \nsetting $\\Code{CUBACORES} = 0$ (Sect.~\\ref{sect:cores}).\n\n%------------------------------------------------------------------------\n\n\\subsubsection{Vectorization}\n\nVectorization means evaluating the integrand function for several points \nat once.  This is also known as Single Instruction Multiple Data (SIMD) \nparadigm and is different from ordinary parallelization where \nindependent threads are executed concurrently.  It is usually possible \nto employ vectorization on top of parallelization.\n\nVector instructions are commonly available in hardware, \\eg on x86 \nplatforms under acronyms such as SSE or AVX.  Language support varies: \nFortran 90's syntax naturally embeds vector operations.  Many C/C++ \ncompilers offer auto-vectorization options, some have extensions for \nvector data types (usually for a limited set of mathematical functions), \nand even hardware-specific access to the CPU's vector instructions.  And \nthen there are vectorized libraries of numerical functions available.\n\n\\cuba\\ cannot automatically vectorize the integrand function, of course, \nbut it does pass (up to) \\Code{nvec} points per integrand call \n(Sect.~\\ref{sect:commonargs}).  This value need not correspond to the \nhardware vector length -- computing several points in one call can also \nmake sense \\eg if the computations have significant intermediate results \nin common.  The actual number of points passed is indicated through the \ncorresponding \\Code{nvec} argument of the integrand.\n\n\\medskip\n\nA note for disambiguation: The \\Code{nbatch} argument of Vegas is \nrelated in purpose but not identical to \\Code{nvec}.  It internally \npartitions the sampling done by Vegas but has no bearing on the number \nof points given to the integrand.  On the other hand, it it pointless to \nchoose $\\Code{nvec} > \\Code{nbatch}$ for Vegas.\n\n%========================================================================\n\n\\section{Tests and Comparisons}\n\\label{sect:tests}\n\nFour integration routines may seem three too many, but as the following \ntests show, all have their strengths and weaknesses.  Fine-tuning the \nalgorithm parameters can also significantly affect performance.\n\nIn the following, the test suite of Genz \\cite{Genz} is used.  Rather\nthan testing individual integrands, Genz proposes the following six\nfamilies of integrands:\n\\begin{equation}\n\\label{eq:families}\n\\begin{array}{ll}\n\\text{1. Oscillatory:} &\n  f_1(\\dvec x) = \\cos(\\dvec c\\cdot\\dvec x + 2\\pi w_1)\\,, \\\\[2ex]\n\\text{2. Product peak:} &\n  f_2(\\dvec x) = \\prod\\limits_{i = 1}^{n_d}\n    \\dfrac 1{(x_i - w_i)^2 + c_i^{-2}}\\,, \\\\[3ex]\n\\text{3. Corner peak:} &\n  f_3(\\dvec x) = \\dfrac 1{(1 + \\dvec c\\cdot\\dvec x)^{n_d + 1}}\\,, \\\\[3ex]\n\\text{4. Gaussian:} &\n  f_4(\\dvec x) = \\exp(-\\dvec c^2 (\\dvec x - \\dvec w)^2)\\,, \\\\[2ex]\n\\text{5. $C^0$-continuous:}\\quad &\n  f_5(\\dvec x) = \\exp(-\\dvec c\\cdot |\\dvec x - \\dvec w|)\\,, \\\\[2ex]\n\\text{6. Discontinuous:} &\n  f_6(\\dvec x) = \\begin{cases}\n    0 & \\text{for }x_1 > w_1 \\vee x_2 > w_2\\,, \\\\\n    \\exp(\\dvec c\\cdot\\dvec x) & \\text{otherwise}.\n  \\end{cases}\n\\end{array}\n\\end{equation}\nParameters designated by $w$ are non-affective, they vary \\eg the \nlocation of peaks, but should in principle not affect the difficulty of \nthe integral.  Parameters designated by $c$ are affective and in a sense\n``define'' the difficulty of the integral, \\eg the width of peaks are of\nthis kind.  The $c_i$ are positive and the difficulty increases with\n$\\norm{\\dvec c} = \\sum_{i = 1}^{n_d} c_i$.\n\nThe testing procedure is thus: Choose uniform random numbers from\n$[0,1)$ for the $c_i$ and $w_i$.  Renormalize $\\dvec c$ for a given\ndifficulty.  Run the algorithms with the integrands thus determined. \nRepeat this procedure 20 times and take the average.\n\nFor comparison, Mathematica's \\Code{NIntegrate} function was included\nin the test.  Unfortunately, when a maximum number of samples is\nprescribed, \\Code{NIntegrate} invariably uses non-adaptive methods, by\ndefault the Halton--Hammersley--Wozniakowski quasi-Monte Carlo\nalgorithm.  The comparison may thus seem not quite balanced, but this is\nnot entirely true: Lacking an upper bound on the number of integrand\nevaluations, \\Code{NIntegrate}'s adaptive method in some cases `locks\nup' (spends an inordinate amount of time and samples) and the user can\nat most abort a running calculation, but not extract a preliminary\nresult.  The adaptive method could reasonably be used only for some of\nthe integrand families in the test, and it was felt that such a\nselection should not be done, as the comparisons should in the first\nplace give an idea about the \\emph{average} performance of the\nintegration methods, without any fine-tuning.\n\nTable \\ref{tab:comp} gives the results of the tests as described above. \nThis comparison chart should be interpreted with care, however, and\nserves only as a rough measure of the performance of the integration\nmethods.  Many integrands appearing in actual calculations bear few or\nno similarities with the integrand families tested here, and neither\nhave the integration parameters been tuned to `get the most' out of each\nmethod.\n\nThe Mathematica code of the test suite is included in the downloadable \n\\cuba\\ package.\n\n\\begin{table}\n$$\n\\begin{array}{|c|r@{\\,\\pm\\,}r|r@{\\,\\pm\\,}r|r@{\\,\\pm\\,}r\n                |r@{\\,\\pm\\,}r|r@{\\,\\pm\\,}r|}\n\\multicolumn{11}{c}{n_d = 5} \\\\ \\hline\nj &\n\\multicolumn{2}{|c|}{\\text{Vegas}} &\n\\multicolumn{2}{|c|}{\\text{Suave}} &\n\\multicolumn{2}{|c|}{\\text{Divonne}} &\n\\multicolumn{2}{|c|}{\\text{Cuhre}} &\n\\multicolumn{2}{|c|}{\\text{NIntegrate}} \\\\ \\hline\n1 & 162000 &     0 &\n    127300 & 32371 &\n     21313 & 11039 &\n       819 &     0 &\n    218281 &     0 \\\\\n2 &  11750 &  1795 &\n     13500 &  1539 &\n     17353 &  3743 &\n     56238 & 40917 &\n    218281 &     0 \\\\\n3 &  16125 &  2411 &\n     11500 &  1000 &\n     17208 &  2517 &\n      1174 &   444 &\n    218281 &     0 \\\\\n4 &  56975 & 11372 &\n     20100 &  4745 &\n     19636 &  6159 &\n     22577 & 31424 &\n    218281 &     0 \\\\\n5 &  14600 &  3085 &\n     15250 &  2337 &\n     21675 &  4697 &\n    150423 &     0 &\n    218281 &     0 \\\\\n6 &  19750 &  4999 &\n     23850 &  2700 &\n     39694 & 14001 &\n      1884 &   215 &\n    218281 &     0 \\\\ \\hline\n\\multicolumn{11}{c}{} \\\\\n\\multicolumn{11}{c}{n_d = 8} \\\\ \\hline\nj &\n\\multicolumn{2}{|c|}{\\text{Vegas}} &\n\\multicolumn{2}{|c|}{\\text{Suave}} &\n\\multicolumn{2}{|c|}{\\text{Divonne}} &\n\\multicolumn{2}{|c|}{\\text{Cuhre}} &\n\\multicolumn{2}{|c|}{\\text{NIntegrate}} \\\\ \\hline\n1 & 153325 & 20274 &\n    124350 & 35467 &\n     28463 & 31646 &\n      3315 &     0 &\n    212939 & 13557 \\\\\n2 &  12650 &  1987 &\n     21050 &  4594 &\n     22030 &  3041 &\n     91826 & 58513 &\n    218281 &     0 \\\\\n3 &  24325 &  3753 &\n     29350 &  3588 & \n     67104 & 16906 & \n     18785 & 22354 &\n    218281 &     0 \\\\\n4 &  38575 & 16169 & \n     29250 &  8873 &\n     24849 &  5015 &\n     62322 & 44328 &\n    218281 &     0 \\\\\n5 &  15150 &  2616 &\n     25500 &  6444 &\n     32885 &  5945 &\n    151385 &     0 &\n    218281 &     0 \\\\\n6 &  18875 &  2512 &\n     40900 &  7196 &\n    116744 & 32533 &    \n      9724 &  9151 &\n    218281 &     0 \\\\ \\hline\n\\multicolumn{11}{c}{} \\\\\n\\multicolumn{11}{c}{n_d = 10} \\\\ \\hline\nj &\n\\multicolumn{2}{|c|}{\\text{Vegas}} &\n\\multicolumn{2}{|c|}{\\text{Suave}} &\n\\multicolumn{2}{|c|}{\\text{Divonne}} &\n\\multicolumn{2}{|c|}{\\text{Cuhre}} &\n\\multicolumn{2}{|c|}{\\text{NIntegrate}} \\\\ \\hline\n1 & 156050 & 21549 & \n    129800 & 30595 & \n     32176 & 30424 & \n      7815 &     0 &\n    214596 & 16481 \\\\\n2 &  14175 &  2672 &\n     24800 &  5464 &\n     25684 &  7582 &\n    144056 & 25983 &\n    218281 &     0 \\\\\n3 &  30275 &  6296 &\n     51150 & 15608 & \n    139737 & 18505 & \n    109150 & 58224 &\n    218281 &     0 \\\\\n4 &  29475 & 10277 &\n     34050 & 10200 & \n     27385 &  8498 &\n    105763 & 49789 & \n    218281 &     0 \\\\\n5 &  16150 &  2791 &\n     31400 &  7715 &\n     44393 & 18654 & \n    153695 &     0 & \n    218281 &     0 \\\\\n6 &  22100 &  3085 &\n     74900 & 32203 &\n    136508 & 17067 & \n     73200 & 64621 &\n    218281 &     0 \\\\ \\hline\n\\end{array}\n$$\nTest parameters:\n\\begin{itemize}\n\\item number of dimensions: $n_d = 5, 8, 10$,\n\\item requested relative accuracy: $\\epsrel = 10^{-3}$,\n\\item maximum number of samples: $\\nmax = 150000$,\n\\item integrand difficulties:\n$\n\\begin{array}{r||c|c|c|c|c|c}\n\\text{Integrand family }j &  1  &  2   &  3  &  4   &  5   &  6  \\\\ \n\\hline\n\\norm{\\dvec c_j}          & 6.0 & 18.0 & 2.2 & 15.2 & 16.1 & 16.4\n\\end{array}\n$\n\\end{itemize}\n\n\\caption{\\label{tab:comp}The number of samples used, averaged from 20\nrandomly chosen integrands from each integrand family $j$ defined in\nEq.~(\\ref{eq:families}).  Values in the vicinity of $\\nmax$ generally\nindicate failure to converge. \\Code{NIntegrate} seems not to be able\nto stop at around the limit of \\Code{MaxPoints -> $\\nmax$}, but always\nsamples considerably more points.}\n\\end{table}\n\n%========================================================================\n\n\\section{Summary}\n\nThe \\cuba\\ library offers a choice of four independent routines for\nmultidimensional numerical integration: Vegas, Suave, Divonne, and\nCuhre.  They work by very different methods, summarized in the following\ntable:\n\\begin{center}\n\\begin{small}\n\\begin{tabular}{llll}\nRoutine  &\n\tBasic integration method &\n\tAlgorithm type &\n\tVariance reduction \\\\ \\hline \\\\[-1.5ex]\nVegas &\n\tSobol quasi-random sample &\n\tMonte Carlo &\n\timportance sampling \\\\\n&\n\t\\textit{or} pseudo-random sample &\n\tMonte Carlo \\\\[1.5ex]\nSuave &\n\tSobol quasi-random sample &\n\tMonte Carlo &\n\tglobally adaptive subdivision \\\\\n&\n\t\\textit{or} pseudo-random sample &\n\tMonte Carlo \\\\[1.5ex]\nDivonne &\n\tKorobov quasi-random sample &\n\tMonte Carlo &\n\tstratified sampling, \\\\\n&\n\t\\textit{or} Sobol quasi-random sample &\n\tMonte Carlo &\n\t\\quad aided by methods from \\\\\n&\n\t\\textit{or} pseudo-random sample &\n\tMonte Carlo &\n\t\\quad numerical optimization \\\\\n&\n\t\\textit{or} cubature rules &\n\tdeterministic \\\\[1.5ex]\nCuhre &\n\tcubature rules &\n\tdeterministic &\n\tglobally adaptive subdivision\n\\end{tabular}\n\\end{small}\n\\end{center}\n\nAll four have a C/C++, Fortran, and Mathematica interface and can\nintegrate vector integrands.  Their invocation is very similar, so it is\neasy to substitute one method by another for cross-checking.  For\nfurther safeguarding, the output is supplemented by a $\\chi^2$\nprobability which quantifies the reliability of the error estimate.\n\nThe source code is available from \\Code{http://feynarts.de/cuba}\nand compiles with gcc, the GNU C compiler.  The C functions can be\ncalled from Fortran directly, so there is no need for adapter code. \nSimilarly, linking Fortran code with the library is straightforward\nand requires no extra tools.\n\nThe routines in the \\cuba\\ library have all been carefully tested, but\nit would of course be folly to believe they are completely error-free. \nThe author welcomes any kind of feedback, in particular bug and \nperformance reports, at hahn@feynarts.de.\n\n%========================================================================\n\n\\section*{Acknowledgements}\n\nI thank A.~Hoang for involving me in a discussion out of which the \nconcept of the Mathematica interface was born and T.~Fritzsche, \nM.~Rauch, and A.M.~de~la~Ossa for testing.  B.~Chokoufe implemented\ncheck-pointing (state file) for Suave, Divonne, and Cuhre.\n\n%========================================================================\n\n\\begin{appendix}\n\n\\section{Shared Memory in Fortran}\n\\label{app:shm}\n\nIPC shared memory is not natively available in Fortran, but it is not \ndifficult to make it available using two small C functions \n\\Code{shmalloc} and \\Code{shmfree}:\n\\begin{verbatim}\n#include <sys/shm.h>\n#include <assert.h>\n\ntypedef long long int memindex;\ntypedef struct { void *addr; int id; } shminfo;\n\nvoid shmalloc_(shminfo *base, memindex *i, const int *n, const int *size) {\n  base->id = shmget(IPC_PRIVATE, *size*(*n + 1) - 1, IPC_CREAT | 0600);\n  assert(base->id != -1);\n  base->addr = shmat(base->id, NULL, 0);\n  assert(base->addr != (void *)-1);\n  *i = ((char *)(base->addr + *size - 1) - (char *)base)/(long)*size;\n}\n\nvoid shmfree_(shminfo *base) {\n  shmdt(base->addr);\n  shmctl(base->id, IPC_RMID, NULL);\n}\n\\end{verbatim}\nThe function \\Code{shmalloc} allocates (suitably aligned) \\Code{n} \nelements of size \\Code{size} and returns a mock index into \\Code{base}, \nthrough which the memory is addressed in Fortran.  The array \\Code{base} \nmust be of the desired type and large enough to store the struct \n\\Code{shminfo}, \\eg two doubles wide.  Be careful to invoke \n\\Code{shmfree} after use, for the memory will not automatically be freed \nupon exit but stay allocated until the next reboot (or explicit removal \nwith \\Code{ipcs}).\n\nThe following test program demonstrates how to use \\Code{shmalloc} and \n\\Code{shmfree}:\n\\begin{verbatim}\n     program test\n     implicit none\n     integer*8 i\n     double precision base(2)\n\n     call shmalloc(base, i, 100, 8)     ! allocate 100 doubles\n\n     base(i) = 1                        ! now use the memory\n     ...\n     base(i+99) = 100\n\n     call shmfree(base)                 ! don't forget to free it\n     end\n\\end{verbatim}\n\n\\end{appendix}\n\n%========================================================================\n\n\\begin{thebibliography}{99}\n\n\\newcommand{\\volyearpage}[3]{\\textbf{#1} (#2) #3}\n\\newcommand{\\cpc}{\\textsl{Comp.\\ Phys.\\ Commun.} \\volyearpage}\n\\newcommand{\\jpc}{\\textsl{J.\\ Comp.\\ Phys.} \\volyearpage}\n\\newcommand{\\cip}{\\textsl{Comp.\\ in Phys.} \\volyearpage}\n\\newcommand{\\toms}{\\textsl{ACM Trans.\\ Math.\\ Software} \\volyearpage}\n\\newcommand{\\tomacs}{\\textsl{ACM Trans.\\ Modeling Comp.\\ Simulation} \\volyearpage}\n\\newcommand{\\siam}{\\textsl{SIAM J.\\ Numer.\\ Anal.} \\volyearpage}\n\\newcommand{\\numa}{\\textsl{Numer.\\ Math.} \\volyearpage}\n\n\\bibitem{quadpack}\nR.~Piessens, E.~de~Doncker, C.~\\\"Uberhuber, D.~Kahaner,\n\\textsc{Quadpack} -- a subroutine package for automatic integration,\nSpringer-Verlag, 1983.\n\n\\bibitem{Vegas1}\nG.P.~Lepage, \\jpc{27}{1978}{192}.\n\n\\bibitem{Vegas2}\nG.P.~Lepage, Report CLNS-80/447, Cornell Univ., Ithaca, N.Y., 1980.\n\n\\bibitem{Miser}\nW.H.~Press, G.R.~Farrar, \\cip{4}{1990}{190}.\n\n\\bibitem{Divonne}\nJ.H.~Friedman, M.H.~Wright, \\toms{7}{1981}{76}; \\\\\nJ.H.~Friedman, M.H.~Wright, SLAC Report CGTM-193-REV, CGTM-193, 1981.\n\n\\bibitem{dcuhre}\nJ.~Berntsen, T.~Espelid, A.~Genz, \\toms{17}{1991}{437}; \\\\\nJ.~Berntsen, T.~Espelid, A.~Genz, \\toms{17}{1991}{452}; \\\\\nTOMS algorithm 698.\n\n\\bibitem{Sobol}\nP.~Bratley, B.L.~Fox, \\toms{14}{1988}{88}; \\\\\nTOMS algorithm 659.\n\n\\bibitem{Niederreiter}\nH.~Niederreiter, Random number generation and quasi-Monte Carlo methods,\nSIAM, 1992.\n\n\\bibitem{NumRecipes}\nW.H.~Press, S.A.~Teukolsky, W.T.~Vetterling, B.P.~Flannery, Numerical \nrecipes in Fortran, 2$^{\\text{nd}}$ edition, Cambridge University Press, \n1992.\n\n\\bibitem{Korobov}\nN.M.~Korobov, Number theoretic methods in approximate analysis\n(in Russian), Fizmatgiz, Moscow, 1963.\n\nA comprehensive English reference on the topic of good lattice points\n(of which the Korobov points are a special case) is H.L.~Keng, W.~Yuan,\nApplications of number theory to numerical analysis, Springer-Verlag,\n1981.\n\n\\bibitem{MersenneTwister}\nM.~Matsumoto, T.~Nishimura, \\tomacs{8}{1998}{3}. \\\\\nSee also \n\\Code{http://www.math.sci.hiroshima-u.ac.jp/$\\sim$m-mat/MT/emt.html}.\n\n\\bibitem{Ranlux}\nM.~L\\\"uscher, \\cpc{79}{1994}{100}; \\\\\nF.~James, \\cpc{79}{1994}{111}.\n\n\\bibitem{GenzMalik}\nA.~Genz, A.~Malik, \\siam{20}{1983}{580}.\n\n\\bibitem{Genz}\nA.~Genz, A package for testing multiple integration subroutines, in: \nP.~Keast, G.~Fairweather (eds.), Numerical Integration, Kluwer,\nDordrecht, 1986.\n\n\\end{thebibliography}\n\n\\end{document}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-r-cubature-2.0.3-hbwemlwkkhssqjl2cw7x5e7orj42zfam/spack-src/build/vignette.rds",
        "/tmp/vanessa/spack-stage/spack-stage-r-cubature-2.0.3-hbwemlwkkhssqjl2cw7x5e7orj42zfam/spack-src/src/cubature-1.0.3/doc/semi-infinite.png",
        "/tmp/vanessa/spack-stage/spack-stage-r-cubature-2.0.3-hbwemlwkkhssqjl2cw7x5e7orj42zfam/spack-src/src/cubature-1.0.3/doc/infinite.png",
        "/tmp/vanessa/spack-stage/spack-stage-r-cubature-2.0.3-hbwemlwkkhssqjl2cw7x5e7orj42zfam/spack-src/src/cubature-1.0.3/doc/integral.png",
        "/tmp/vanessa/spack-stage/spack-stage-r-cubature-2.0.3-hbwemlwkkhssqjl2cw7x5e7orj42zfam/spack-src/src/Cuba-4.2/cuba.pdf"
    ],
    "total_files": 140
}