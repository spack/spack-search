{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/runtime/realm/codedesc.cc": "/* Copyright 2018 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// constructs for describing code blobs to Realm\n\n#include \"realm/codedesc.h\"\n\n#include <dlfcn.h>\n\n#include \"realm/logging.h\"\n#include \"realm/utils.h\"\n\nnamespace Realm {\n\n  Logger log_codetrans(\"codetrans\");\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Type\n\n  std::ostream& operator<<(std::ostream& os, const Type& t)\n  {\n    switch(t.f_common.kind) {\n    case Type::InvalidKind: os << \"INVALIDTYPE\"; break;\n    case Type::OpaqueKind:\n      {\n\tif(t.size_bits() == 0)\n\t  os << \"void\";\n\telse\n\t  os << \"opaque(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::IntegerKind:\n      {\n\tos << (t.f_integer.is_signed ? 's' : 'u') << \"int(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::FloatingPointKind: os << \"float(\" << t.size_bits() << \")\"; break;\n    case Type::PointerKind:\n      {\n\tos << *t.f_pointer.base_type;\n\tif(t.f_pointer.is_const) os << \" const\";\n\tos << \" *\";\n\tbreak;\n      }\n    case Type::FunctionPointerKind:\n      {\n\tos << *t.f_funcptr.return_type << \"(*)(\";\n\tconst std::vector<Type>& p = *t.f_funcptr.param_types;\n\tif(p.size()) {\n\t  for(size_t i = 0; i < p.size(); i++) {\n\t    if(i) os << \", \";\n\t    os << p[i];\n\t  }\n\t} else\n\t  os << \"void\";\n\tos << \")\";\n\tbreak;\n      }\n    }\n    return os;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeDescriptor\n\n  CodeDescriptor::CodeDescriptor(void)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const Type& _t)\n    : m_type(_t)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const CodeDescriptor& rhs)\n  {\n    copy_from(rhs);\n  }\n\n  CodeDescriptor& CodeDescriptor::operator=(const CodeDescriptor& rhs)\n  {\n    if(this != &rhs) {\n      clear();\n      copy_from(rhs);\n    }\n    return *this;\n  }\n\n  CodeDescriptor::~CodeDescriptor(void)\n  {\n    clear();\n  }\n\n  void CodeDescriptor::clear(void)\n  {\n    m_type = Type();\n    delete_container_contents(m_impls);\n    delete_container_contents(m_props);\n  }\n\n  void CodeDescriptor::copy_from(const CodeDescriptor& rhs)\n  {\n    m_type = rhs.m_type;\n    {\n      size_t s = rhs.m_impls.size();\n      m_impls.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_impls[i] = rhs.m_impls[i]->clone();\n    }\n    {\n      size_t s = rhs.m_props.size();\n      m_props.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_props[i] = rhs.m_props[i]->clone();\n    }\n  }\n\n  // are any of the code implementations marked as \"portable\" (i.e.\n  //  usable in another process/address space)?\n  bool CodeDescriptor::has_portable_implementations(void) const\n  {\n    for(std::vector<CodeImplementation *>::const_iterator it = m_impls.begin();\n\tit != m_impls.end();\n\tit++)\n      if((*it)->is_portable())\n\treturn true;\n    return false;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class FunctionPointerImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      FunctionPointerImplementation> FunctionPointerImplementation::serdez_subclass;\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void)\n    : fnptr(0)\n  {}\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void (*_fnptr)())\n    : fnptr(_fnptr)\n  {}\n\n  FunctionPointerImplementation::~FunctionPointerImplementation(void)\n  {}\n\n  CodeImplementation *FunctionPointerImplementation::clone(void) const\n  {\n    return new FunctionPointerImplementation(fnptr);\n  }\n\n  bool FunctionPointerImplementation::is_portable(void) const\n  {\n    return false;\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOReferenceImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      DSOReferenceImplementation> DSOReferenceImplementation::serdez_subclass;\n\n  DSOReferenceImplementation::DSOReferenceImplementation(void)\n  {}\n\n  DSOReferenceImplementation::DSOReferenceImplementation(const std::string& _dso_name,\n\t\t\t\t\t\t\t const std::string& _symbol_name)\n    : dso_name(_dso_name), symbol_name(_symbol_name)\n  {}\n\n  DSOReferenceImplementation::~DSOReferenceImplementation(void)\n  {}\n\n  CodeImplementation *DSOReferenceImplementation::clone(void) const\n  {\n    return new DSOReferenceImplementation(dso_name, symbol_name);\n  }\n\n  bool DSOReferenceImplementation::is_portable(void) const\n  {\n    return true;\n  }\n\n#ifdef REALM_USE_DLADDR\n  namespace {\n    extern \"C\" { int main(int argc, const char *argv[]) __attribute__((weak)); };\n\n    DSOReferenceImplementation *dladdr_helper(void *ptr, bool quiet)\n    {\n      // if dladdr() gives us something with the same base pointer, assume that's portable\n      // note: return code is not-POSIX-y (i.e. 0 == failure)\n      Dl_info inf;\n      int ret = dladdr(ptr, &inf);\n      if(ret == 0) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"couldn't map fnptr \" << ptr << \" to a dynamic symbol\";\n\treturn 0;\n      }\n\n      if(inf.dli_saddr != ptr) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"pointer \" << ptr << \" in middle of symbol '\" << inf.dli_sname << \" (\" << inf.dli_saddr << \")?\";\n\treturn 0;\n      }\n\n      // try to detect symbols that are in the base executable and change the filename to \"\"\n      // only do this if the weak 'main' reference found an actual main\n      if(((void *)main) != 0) {\n\tconst char *fname = inf.dli_fname;\n\t{\n\t  static std::string local_fname;\n\t  if(local_fname.empty()) {\n\t    Dl_info inf2;\n\t    ret = dladdr((void *)main, &inf2);\n\t    assert(ret != 0);\n\t    local_fname = inf2.dli_fname;\n\t  }\n\t  if(local_fname.compare(fname) == 0)\n\t    fname = \"\";\n\t}\n\n\treturn new DSOReferenceImplementation(fname, inf.dli_sname);\n      }\n\n      return 0;\n    }\n  };\n#endif\n\n  /*static*/ DSOReferenceImplementation *DSOReferenceImplementation::cvt_fnptr_to_dsoref(const FunctionPointerImplementation *fpi,\n\t\t\t\t\t\t\t\t\t\t\t bool quiet /*= false*/)\n  {\n    return dladdr_helper((void *)(fpi->fnptr), quiet);\n  } \n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeTranslator\n\n  CodeTranslator::CodeTranslator(const std::string& _name)\n    : name(_name)\n  {}\n\n  CodeTranslator::~CodeTranslator(void)\n  {}\n\n  // default version just iterates over all the implementations in the source\n  bool CodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t     const std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn true;\n    }\n\n    return false;\n  }\n\n  // default version just iterates over all the implementations in the source\n  CodeImplementation *CodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn translate(*it, target_impl_type);\n    }\n\n    return 0;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOCodeTranslator\n\n#ifdef REALM_USE_DLFCN\n  DSOCodeTranslator::DSOCodeTranslator(void)\n    : CodeTranslator(\"dso\")\n  {}\n\n  DSOCodeTranslator::~DSOCodeTranslator(void)\n  {\n    // unload any modules we have loaded\n    for(std::map<std::string, void *>::iterator it = modules_loaded.begin();\n\tit != modules_loaded.end();\n\tit++) {\n      int ret = dlclose(it->second);\n      if(ret != 0)\n\tlog_codetrans.warning() << \"error on dlclose of '\" << it->first << \"': \" << dlerror();\n    }\n  }\n\n  bool DSOCodeTranslator::can_translate(const std::type_info& source_impl_type,\n\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    // DSO ref -> function pointer\n    if((source_impl_type == typeid(DSOReferenceImplementation)) &&\n       (target_impl_type == typeid(FunctionPointerImplementation)))\n      return true;\n\n#ifdef REALM_USE_DLADDR\n    if((source_impl_type == typeid(FunctionPointerImplementation)) &&\n       (target_impl_type == typeid(DSOReferenceImplementation)))\n      return true;\n#endif\n\n      return false;\n    }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeImplementation *source,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    if(target_impl_type == typeid(FunctionPointerImplementation)) {\n      const DSOReferenceImplementation *dsoref = dynamic_cast<const DSOReferenceImplementation *>(source);\n      assert(dsoref != 0);\n\n      void *handle = 0;\n      // check to see if we've already loaded the module?\n      std::map<std::string, void *>::iterator it = modules_loaded.find(dsoref->dso_name);\n      if(it != modules_loaded.end()) {\n\thandle = it->second;\n      } else {\n\t// try to load it - empty string for dso_name means the main executable\n\tconst char *dso_name = dsoref->dso_name.c_str();\n\thandle = dlopen(*dso_name ? dso_name : 0, RTLD_NOW | RTLD_LOCAL);\n\tif(!handle) {\n\t  log_codetrans.warning() << \"could not open DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\t  return 0;\n\t}\n\tmodules_loaded[dsoref->dso_name] = handle;\n      }\n\n      void *ptr = dlsym(handle, dsoref->symbol_name.c_str());\n      if(!ptr) {\n\tlog_codetrans.warning() << \"could not find symbol '\" << dsoref->symbol_name << \"' in  DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\treturn 0;\n      }\n\n      return new FunctionPointerImplementation((void(*)())ptr);\n    }\n\n#ifdef REALM_USE_DLADDR\n    if(target_impl_type == typeid(DSOReferenceImplementation)) {\n      const FunctionPointerImplementation *fpi = dynamic_cast<const FunctionPointerImplementation *>(source);\n      assert(fpi != 0);\n\n      return dladdr_helper((void *)(fpi->fnptr), false /*!quiet*/);\n    }\n#endif\n\n    return 0;\n  }\n\n  // these pass through to CodeTranslator's definitions\n  bool DSOCodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    return CodeTranslator::can_translate(source_codedesc, target_impl_type);\n  }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    return CodeTranslator::translate(source_codedesc, target_impl_type);\n  }\n#endif\n\n\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/runtime/realm/module.cc": "/* Copyright 2018 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Realm modules\n\n#include \"realm/realm_config.h\"\n\n#define REALM_MODULE_REGISTRATION_STATIC\n#include \"realm/module.h\"\n\n#include \"realm/logging.h\"\n\n#include <assert.h>\n#include <string.h>\n#include <stdlib.h>\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n// TODO: replace this with Makefile (or maybe cmake) magic that adapts automatically\n//  to the build-system-controlled list of statically-linked Realm modules\n#include \"realm/runtime_impl.h\"\n#include \"realm/numa/numa_module.h\"\n#ifdef REALM_USE_OPENMP\n#include \"realm/openmp/openmp_module.h\"\n#endif\n#include \"realm/procset/procset_module.h\"\n#ifdef REALM_USE_PYTHON\n#include \"realm/python/python_module.h\"\n#endif\n#ifdef USE_CUDA\n#include \"realm/cuda/cuda_module.h\"\n#endif\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit_module.h\"\n#endif\n#ifdef USE_HDF\n#include \"realm/hdf5/hdf5_module.h\"\n#endif\n\nnamespace Realm {\n\n  Logger log_module(\"module\");\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Module\n  //\n\n  Module::Module(const std::string& _name)\n    : name(_name)\n  {\n    log_module.debug() << \"module \" << name << \" created\";\n  }\n\n  Module::~Module(void)\n  {\n    log_module.debug() << \"module \" << name << \" destroyed\";\n  }\n\n  const std::string& Module::get_name(void) const\n  {\n    return name;\n  }\n\n  void Module::initialize(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" initialize\";\n  }\n\n  void Module::create_memories(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_memories\";\n  }\n\n  void Module::create_processors(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_processors\";\n  }\n  \n  void Module::create_dma_channels(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_dma_channels\";\n  }\n  \n  void Module::create_code_translators(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_code_translators\";\n  }\n\n  void Module::cleanup(void)\n  {\n    log_module.debug() << \"module \" << name << \" cleanup\";\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class ModuleRegistrar\n  //\n\n  std::vector<const ModuleRegistrar::StaticRegistrationBase *>& static_module_registrations(void)\n  {\n    static std::vector<const ModuleRegistrar::StaticRegistrationBase *> data;\n    return data;\n  }\n\n  ModuleRegistrar::ModuleRegistrar(RuntimeImpl *_runtime)\n    : runtime(_runtime)\n  {}\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_static_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t      std::vector<Module *>& modules)\n  {\n    // just iterate over the static module list, trying to create each module\n    for(std::vector<const StaticRegistrationBase *>::const_iterator it = static_module_registrations().begin();\n\tit != static_module_registrations().end();\n\tit++) {\n      Module *m = (*it)->create_module(runtime, cmdline);\n      if(m)\n\tmodules.push_back(m);\n    }\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  // accepts a colon-separated list of so files to try to load\n  static void load_module_list(const char *sonames,\n\t\t\t       RuntimeImpl *runtime,\n\t\t\t       std::vector<std::string>& cmdline,\n\t\t\t       std::vector<void *>& handles,\n\t\t\t       std::vector<Module *>& modules)\n  {\n    // null/empty strings are nops\n    if(!sonames || !*sonames) return;\n\n    const char *p1 = sonames;\n    while(true) {\n      // skip leading colons\n      while(*p1 == ':') p1++;\n      if(!*p1) break;\n\n      const char *p2 = p1 + 1;\n      while(*p2 && (*p2 != ':')) p2++;\n\n      char filename[1024];\n      strncpy(filename, p1, p2 - p1);\n\n      // no leftover errors from anybody else please...\n      assert(dlerror() == 0);\n\n      // open so file, resolving all symbols but not polluting global namespace\n      void *handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n\n      if(handle != 0) {\n\t// this file should have a \"create_realm_module\" symbol\n\tvoid *sym = dlsym(handle, \"create_realm_module\");\n\n\tif(sym != 0) {\n\t  // TODO: hold onto the handle even if it doesn't create a module?\n\t  handles.push_back(handle);\n\n\t  Module *m = ((Module *(*)(RuntimeImpl *, std::vector<std::string>&))dlsym)(runtime, cmdline);\n\t  if(m)\n\t    modules.push_back(m);\n\t} else {\n\t  log_module.error() << \"symbol 'create_realm_module' not found in \" << filename;\n#ifndef NDEBUG\n\t  int ret =\n#endif\n\t    dlclose(handle);\n\t  assert(ret == 0);\n\t}\n      } else {\n\tlog_module.error() << \"could not load \" << filename << \": \" << dlerror();\n      }\n\n      if(!*p2) break;\n      p1 = p2 + 1;\n    }\n  }\n#endif\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_dynamic_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t       std::vector<Module *>& modules)\n  {\n    // dynamic modules are requested in one of two ways:\n    // 1) REALM_DYNAMIC_MODULES=sonames environment variable\n    // 2) \"-ll:module sonames\" on command line\n    // in both cases, 'sonames' is a colon-separate listed of .so files that should be\n\n    // loading modules can also monkey with the cmdline, so do a pass first where we pull\n    //  out all the name we want to load\n    std::vector<std::string> sonames_list;\n\n    {\n      const char *e = getenv(\"REALM_DYNAMIC_MODULES\");\n      if(e)\n\tsonames_list.push_back(std::string(e));\n    }\n\n    {\n      std::vector<std::string>::iterator it = cmdline.begin();\n      while(it != cmdline.end()) {\n\tif(*it != \"-ll:module\") {\n\t  it++;\n\t  continue;\n\t}\n\n\t// eat this argument and move the next one to sonames_list\n\tit = cmdline.erase(it);\n\tassert(it != cmdline.end());\n\tsonames_list.push_back(*it);\n\tit = cmdline.erase(it);\n      }\n    }\n\n#ifdef REALM_USE_DLFCN\n    for(std::vector<std::string>::const_iterator it = sonames_list.begin();\n\tit != sonames_list.end();\n\tit++)\n      load_module_list(it->c_str(),\n\t\t       runtime, cmdline, sofile_handles, modules);\n#else\n    if(!sonames_list.empty()) {\n      log_module.error() << \"loading of dynamic Realm modules requested, but REALM_USE_DLFCN=0!\";\n      exit(1);\n    }\n#endif\n  }\n\n  // called by runtime after all modules have been cleaned up\n  void ModuleRegistrar::unload_module_sofiles(void)\n  {\n#ifdef REALM_USE_DLFCN\n    while(!sofile_handles.empty()) {\n      void *handle = sofile_handles.back();\n      sofile_handles.pop_back();\n\n#ifndef NDEBUG\n      int ret =\n#endif\n\tdlclose(handle);\n      assert(ret == 0);\n    }\n#endif\n  }\n\n  // called by the module registration helpers\n  /*static*/ void ModuleRegistrar::add_static_registration(const StaticRegistrationBase *reg)\n  {\n    // done during init, so single-threaded\n    static_module_registrations().push_back(reg);\n  }\n  \n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/runtime/realm/python/python_module.cc": "/* Copyright 2018 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"realm/python/python_module.h\"\n#include \"realm/python/python_internal.h\"\n\n#include \"realm/numa/numasysif.h\"\n#include \"realm/logging.h\"\n#include \"realm/cmdline.h\"\n#include \"realm/proc_impl.h\"\n#include \"realm/threads.h\"\n#include \"realm/runtime_impl.h\"\n#include \"realm/utils.h\"\n\n#include <dlfcn.h>\n#include <link.h>\n\n#include <list>\n\nnamespace Realm {\n\n  Logger log_py(\"python\");\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonAPI\n\n  PythonAPI::PythonAPI(void *_handle)\n    : handle(_handle)\n  {\n    get_symbol(this->Py_DecRef, \"Py_DecRef\");\n    get_symbol(this->Py_Finalize, \"Py_Finalize\");\n    get_symbol(this->Py_InitializeEx, \"Py_InitializeEx\");\n\n    get_symbol(this->PyByteArray_FromStringAndSize, \"PyByteArray_FromStringAndSize\");\n\n    get_symbol(this->PyEval_InitThreads, \"PyEval_InitThreads\");\n    get_symbol(this->PyEval_RestoreThread, \"PyEval_RestoreThread\");\n    get_symbol(this->PyEval_SaveThread, \"PyEval_SaveThread\");\n\n    get_symbol(this->PyThreadState_New, \"PyThreadState_New\");\n    get_symbol(this->PyThreadState_Clear, \"PyThreadState_Clear\");\n    get_symbol(this->PyThreadState_Delete, \"PyThreadState_Delete\");\n    get_symbol(this->PyThreadState_Get, \"PyThreadState_Get\");\n    get_symbol(this->PyThreadState_Swap, \"PyThreadState_Swap\");\n\n    get_symbol(this->PyErr_PrintEx, \"PyErr_PrintEx\");\n\n    get_symbol(this->PyImport_ImportModule, \"PyImport_ImportModule\");\n    get_symbol(this->PyModule_GetDict, \"PyModule_GetDict\");\n\n    get_symbol(this->PyLong_FromUnsignedLong, \"PyLong_FromUnsignedLong\");\n\n    get_symbol(this->PyObject_CallFunction, \"PyObject_CallFunction\");\n    get_symbol(this->PyObject_CallObject, \"PyObject_CallObject\");\n    get_symbol(this->PyObject_GetAttrString, \"PyObject_GetAttrString\");\n    get_symbol(this->PyObject_Print, \"PyObject_Print\");\n\n    get_symbol(this->PyRun_SimpleString, \"PyRun_SimpleString\");\n    get_symbol(this->PyRun_String, \"PyRun_String\");\n\n    get_symbol(this->PyTuple_New, \"PyTuple_New\");\n    get_symbol(this->PyTuple_SetItem, \"PyTuple_SetItem\");\n  }\n\n  template<typename T>\n  void PythonAPI::get_symbol(T &fn, const char *symbol,\n                             bool missing_ok /*= false*/)\n  {\n    fn = reinterpret_cast<T>(dlsym(handle, symbol));\n    if(!fn && !missing_ok) {\n      const char *error = dlerror();\n      log_py.fatal() << \"failed to find symbol '\" << symbol << \"': \" << error;\n      assert(false);\n    }\n  }\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonInterpreter\n\n#ifdef REALM_USE_DLMOPEN\n  // dlmproxy symbol lookups have to happen in a function we define so that\n  //  dl[v]sym searches in the right place\n  static void *dlmproxy_lookup(const char *symname, const char *symver)\n  {\n    \n    void *handle = 0;\n    void *sym = (symver ?\n\t\t   dlvsym(handle, symname, symver) :\n\t\t   dlsym(handle, symname));\n    if(sym)\n      log_py.debug() << \"found symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\") << \" ptr=\" << sym;\n    else\n      log_py.warning() << \"missing symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\");\n    return sym;\n  }\n#endif\n\n  PythonInterpreter::PythonInterpreter() \n  {\n#ifdef REALM_PYTHON_LIB\n    const char *python_lib = REALM_PYTHON_LIB;\n#else\n    const char *python_lib = \"libpython2.7.so\";\n#endif\n\n#ifdef REALM_USE_DLMOPEN\n    // loading libpython into its own namespace will cause it to try to bring\n    //   in a second copy of libpthread.so.0, which is fairly disastrous\n    // we deal with it by loading a \"dlmproxy\" of pthreads that tunnels all \n    //   pthreads calls back to the (only) version in the main executable\n    const char *dlmproxy_filename = getenv(\"DLMPROXY_LIBPTHREAD\");\n    if(!dlmproxy_filename)\n      dlmproxy_filename = \"dlmproxy_libpthread.so.0\";\n    dlmproxy_handle = dlmopen(LM_ID_NEWLM,\n\t\t\t      dlmproxy_filename,\n\t\t\t      RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_LAZY);\n    if(!dlmproxy_handle) {\n      const char *error = dlerror();\n      log_py.fatal() << \"HELP!  Use of dlmopen for python requires dlmproxy for pthreads!  Failed to\\n\"\n\t\t     << \"  load: \" << dlmproxy_filename << \"\\n\"\n\t\t     << \"  error: \" << error;\n      assert(false);\n    }\n\n    // now that the proxy is loaded, we need to tell it where the real\n    //  libpthreads functions are\n    {\n      void *sym = dlsym(dlmproxy_handle, \"dlmproxy_load_symbols\");\n      assert(sym != 0);\n      ((void (*)(void *(*)(const char *, const char *)))sym)(dlmproxy_lookup);\n    }\n\n    // now we can load libpython, but make sure we do it in the new namespace\n    Lmid_t lmid;\n    int ret = dlinfo(dlmproxy_handle, RTLD_DI_LMID, &lmid);\n    assert(ret == 0);\n\n    handle = dlmopen(lmid, python_lib, RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_NOW);\n#else\n    // life is so much easier if we use dlopen (but we only get one copy then)\n    handle = dlopen(python_lib, RTLD_GLOBAL | RTLD_LAZY);\n#endif\n    if (!handle) {\n      const char *error = dlerror();\n      log_py.fatal() << error;\n      assert(false);\n    }\n\n    api = new PythonAPI(handle);\n\n    (api->Py_InitializeEx)(0 /*!initsigs*/);\n    (api->PyEval_InitThreads)();\n    //(api->Py_Finalize)();\n\n    //PyThreadState *state;\n    //state = (api->PyEval_SaveThread)();\n    //(api->PyEval_RestoreThread)(state);\n\n    //(api->PyRun_SimpleString)(\"print 'hello Python world!'\");\n\n    //PythonSourceImplementation psi(\"taskreg_helper\", \"task1\");\n    //find_or_import_function(&psi);\n  }\n\n  PythonInterpreter::~PythonInterpreter()\n  {\n    (api->Py_Finalize)();\n\n    delete api;\n\n    if (dlclose(handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"libpython dlclose error: \" << error;\n      assert(false);\n    }\n\n#ifdef REALM_USE_DLMOPEN\n    if (dlclose(dlmproxy_handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"dlmproxy dlclose error: \" << error;\n      assert(false);\n    }\n#endif\n  }\n\n  PyObject *PythonInterpreter::find_or_import_function(const PythonSourceImplementation *psi)\n  {\n    //log_py.print() << \"attempting to acquire python lock\";\n    //(api->PyEval_AcquireLock)();\n    //log_py.print() << \"lock acquired\";\n\n    // not calling PythonInterpreter::import_module here because we want the\n    //  PyObject result\n    log_py.debug() << \"attempting to import module: \" << psi->module_name;\n    PyObject *module = (api->PyImport_ImportModule)(psi->module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    //(api->PyObject_Print)(module, stdout, 0); printf(\"\\n\");\n\n    log_py.debug() << \"finding attribute '\" << psi->function_name << \"' in module '\" << psi->module_name << \"'\";\n    PyObject *function = (api->PyObject_GetAttrString)(module, psi->function_name.c_str());\n    if (!function) {\n      log_py.fatal() << \"unable to import Python function \" << psi->function_name << \" from module\" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    //(api->PyObject_Print)(function, stdout, 0); printf(\"\\n\");\n\n    //(api->PyObject_CallFunction)(function, \"iii\", 1, 2, 3);\n\n    (api->Py_DecRef)(module);\n\n    return function;\n  }\n\n  void PythonInterpreter::import_module(const std::string& module_name)\n  {\n    log_py.debug() << \"attempting to import module: \" << module_name;\n    PyObject *module = (api->PyImport_ImportModule)(module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << module_name;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    (api->Py_DecRef)(module);\n  }\n\n  void PythonInterpreter::run_string(const std::string& script_text)\n  {\n    // from Python.h\n    const int Py_file_input = 257;\n\n    log_py.debug() << \"running python string: \" << script_text;\n    PyObject *mainmod = (api->PyImport_ImportModule)(\"__main__\");\n    assert(mainmod != 0);\n    PyObject *globals = (api->PyModule_GetDict)(mainmod);\n    assert(globals != 0);\n    PyObject *res = (api->PyRun_String)(script_text.c_str(),\n\t\t\t\t\tPy_file_input,\n\t\t\t\t\tglobals,\n\t\t\t\t\tglobals);\n    if(!res) {\n      log_py.fatal() << \"unable to run python string:\" << script_text;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    (api->Py_DecRef)(res);\n    (api->Py_DecRef)(globals);\n    (api->Py_DecRef)(mainmod);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonThreadTaskScheduler\n\n  PythonThreadTaskScheduler::PythonThreadTaskScheduler(LocalPythonProcessor *_pyproc,\n\t\t\t\t\t\t       CoreReservation& _core_rsrv)\n    : KernelThreadTaskScheduler(_pyproc->me, _core_rsrv)\n    , pyproc(_pyproc)\n    , interpreter_ready(false)\n  {}\n\n  void PythonThreadTaskScheduler::enqueue_taskreg(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    AutoHSLLock al(lock);\n    taskreg_queue.push_back(treg);\n    // we've added work to the system\n    work_counter.increment_counter();\n  }\n\n  void PythonThreadTaskScheduler::python_scheduler_loop(void)\n  {\n    // global startup of python interpreter if needed\n    if(!interpreter_ready) {\n      log_py.info() << \"creating interpreter\";\n      pyproc->create_interpreter();\n      interpreter_ready = true;\n    }\n\n    // always create and remember our own python thread - does NOT require GIL\n    PyThreadState *pythread = (pyproc->interpreter->api->PyThreadState_New)(pyproc->master_thread->interp);\n    log_py.debug() << \"created python thread: \" << pythread;\n    \n    assert(pythread != 0);\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = pythread;\n\n    // now go into main scheduler loop, holding scheduler lock for whole thing\n    AutoHSLLock al(lock);\n    while(true) {\n      // remember the work counter value before we start so that we don't iterate\n      //   unnecessarily\n      long long old_work_counter = work_counter.read_counter();\n\n      // first priority - task registration\n      while(!taskreg_queue.empty()) {\n\tLocalPythonProcessor::TaskRegistration *treg = taskreg_queue.front();\n\ttaskreg_queue.pop_front();\n\t\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\t\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = TaskQueue::PRI_POS_INF;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  pyproc->perform_task_registration(treg);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      }\n\n      // if we have both resumable and new ready tasks, we want the one that\n      //  is the highest priority, with ties going to resumable tasks - we\n      //  can do this cleanly by taking advantage of the fact that the\n      //  resumable_workers queue uses the scheduler lock, so can't change\n      //  during this call\n      // peek at the top thing (if any) in that queue, and then try to find\n      //  a ready task with higher priority\n      int resumable_priority = ResumableQueue::PRI_NEG_INF;\n      resumable_workers.peek(&resumable_priority);\n\n      // try to get a new task then\n      // remember where a task has come from in case we want to put it back\n      Task *task = 0;\n      TaskQueue *task_source = 0;\n      int task_priority = resumable_priority;\n      for(std::vector<TaskQueue *>::const_iterator it = task_queues.begin();\n\t  it != task_queues.end();\n\t  it++) {\n\tint new_priority;\n\tTask *new_task = (*it)->get(&new_priority, task_priority);\n\tif(new_task) {\n\t  // if we got something better, put back the old thing (if any)\n\t  if(task)\n\t    task_source->put(task, task_priority, false); // back on front of list\n\t  \n\t  task = new_task;\n\t  task_source = *it;\n\t  task_priority = new_priority;\n\t}\n      }\n\n      // did we find work to do?\n      if(task) {\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = task_priority;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n\t// make our python thread state active, acquiring the GIL\n\tassert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\tlog_py.debug() << \"RestoreThread <- \" << pythread;\n\t(pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  execute_task(task);\n\tassert(ok);  // no fault recovery yet\n\n\t// release the GIL\n\tPyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n\tlog_py.debug() << \"SaveThread -> \" << saved;\n\tassert(saved == pythread);\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n\tcontinue;\n      }\n\n      // having checked for higher-priority ready tasks, we can always\n      //  take the highest-priority resumable task, if any, and run it\n      if(!resumable_workers.empty()) {\n\tThread *yield_to = resumable_workers.get(0); // priority is irrelevant\n\tassert(yield_to != Thread::self());\n\n\t// this should only happen if we're at the max active worker count (otherwise\n\t//  somebody should have just woken this guy up earlier), and reduces the \n\t// unassigned worker count by one\n\tupdate_worker_count(0, -1);\n\n\tidle_workers.push_back(Thread::self());\n\tworker_sleep(yield_to);\n\n\t// loop around and check both queues again\n\tcontinue;\n      }\n\n      {\n\t// no ready or resumable tasks?  thumb twiddling time\n\n\t// are we shutting down?\n\tif(shutdown_flag) {\n\t  // yes, we can terminate - wake up an idler (if any) first though\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t  } else {\n\t    // nobody to wake, so -1 active/unassigned worker\n\t    update_worker_count(-1, -1, false); // ok to drop below mins\n\t    worker_terminate(0);\n\t  }\n\t  return;\n\t}\n\n\t// do we have more unassigned and idle tasks than we need?\n\tint total_idle_count = (unassigned_worker_count +\n\t\t\t\t(int)(idle_workers.size()));\n\tif(total_idle_count > cfg_max_idle_workers) {\n\t  // if there are sleeping idlers, terminate in favor of one of those - keeps\n\t  //  worker counts constant\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    assert(to_wake != Thread::self());\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t    return;\n\t  }\n\t}\n\n\t// no, stay awake but suspend until there's a chance that the next iteration\n\t//  of this loop would turn out different\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // should never get here\n    assert(0);\n  }\n\n  Thread *PythonThreadTaskScheduler::worker_create(bool make_active)\n  {\n    // lock is held by caller\n    ThreadLaunchParameters tlp;\n    Thread *t = Thread::create_kernel_thread<PythonThreadTaskScheduler,\n\t\t\t\t\t     &PythonThreadTaskScheduler::python_scheduler_loop>(this,\n\t\t\t\t\t\t\t\t\t\t\t\ttlp,\n\t\t\t\t\t\t\t\t\t\t\t\tcore_rsrv,\n\t\t\t\t\t\t\t\t\t\t\t\tthis);\n    all_workers.insert(t);\n    if(make_active)\n      active_workers.insert(t);\n    return t;\n  }\n \n  // called by a worker thread when it needs to wait for something (and we\n  //   should release the GIL)\n  void PythonThreadTaskScheduler::thread_blocking(Thread *thread)\n  {\n    // if this gets called before we're done initializing the interpreter,\n    //  we need a simple blocking wait\n    if(!interpreter_ready) {\n      AutoHSLLock al(lock);\n\n      log_py.debug() << \"waiting during initialization\";\n      bool really_blocked = try_update_thread_state(thread,\n\t\t\t\t\t\t    Thread::STATE_BLOCKING,\n\t\t\t\t\t\t    Thread::STATE_BLOCKED);\n      if(!really_blocked) return;\n\n      while(true) {\n\tlong long old_work_counter = work_counter.read_counter();\n\n\tif(!resumable_workers.empty()) {\n\t  Thread *t = resumable_workers.get(0);\n\t  assert(t == thread);\n\t  log_py.debug() << \"awake again\";\n\t  return;\n\t}\n\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // if we got here through a cffi call, the GIL has already been released,\n    //  so try to handle that case here - a call PyEval_SaveThread\n    //  if the GIL is not held will assert-fail, and while a call to\n    //  PyThreadState_Swap is technically illegal (and unsafe if python-created\n    //  threads exist), it does what we want for now\n    PyThreadState *saved = (pyproc->interpreter->api->PyThreadState_Swap)(0);\n    if(saved != 0) {\n      log_py.info() << \"python worker sleeping - releasing GIL\";\n      // put it back so we can save it properly\n      (pyproc->interpreter->api->PyThreadState_Swap)(saved);\n      // would like to sanity-check that this returns the expected thread state,\n      //  but that would require taking the PythonThreadTaskScheduler's lock\n      (pyproc->interpreter->api->PyEval_SaveThread)();\n      log_py.debug() << \"SaveThread -> \" << saved;\n    } else\n      log_py.info() << \"python worker sleeping - GIL already released\";\n    \n    KernelThreadTaskScheduler::thread_blocking(thread);\n\n    if(saved) {\n      log_py.info() << \"python worker awake - acquiring GIL\";\n      log_py.debug() << \"RestoreThread <- \" << saved;\n      (pyproc->interpreter->api->PyEval_RestoreThread)(saved);\n    } else\n      log_py.info() << \"python worker awake - not acquiring GIL\";\n  }\n\n  void PythonThreadTaskScheduler::thread_ready(Thread *thread)\n  {\n    // handle the wakening of the initialization thread specially\n    if(!interpreter_ready) {\n      AutoHSLLock al(lock);\n      resumable_workers.put(thread, 0);\n    } else {\n      KernelThreadTaskScheduler::thread_ready(thread);\n    }\n  }\n\n  void PythonThreadTaskScheduler::worker_terminate(Thread *switch_to)\n  {\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    PyThreadState *pythread = it->second;\n    pythreads.erase(it);\n\n    log_py.debug() << \"destroying python thread: \" << pythread;\n    \n    // our thread should not be active\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\n    // switch to the master thread, retaining the GIL\n    log_py.debug() << \"RestoreThread <- \" << pyproc->master_thread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pyproc->master_thread);\n\n    // clear and delete the worker thread\n    (pyproc->interpreter->api->PyThreadState_Clear)(pythread);\n    (pyproc->interpreter->api->PyThreadState_Delete)(pythread);\n\n    // release the GIL\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pyproc->master_thread);\n\n    // TODO: tear down interpreter if last thread\n    if(shutdown_flag && pythreads.empty())\n      pyproc->destroy_interpreter();\n\n    KernelThreadTaskScheduler::worker_terminate(switch_to);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class LocalPythonProcessor\n\n  LocalPythonProcessor::LocalPythonProcessor(Processor _me, int _numa_node,\n                                             CoreReservationSet& crs,\n                                             size_t _stack_size,\n\t\t\t\t\t     const std::vector<std::string>& _import_modules,\n\t\t\t\t\t     const std::vector<std::string>& _init_scripts)\n    : ProcessorImpl(_me, Processor::PY_PROC)\n    , numa_node(_numa_node)\n    , import_modules(_import_modules)\n    , init_scripts(_init_scripts)\n    , interpreter(0)\n    , ready_task_count(stringbuilder() << \"realm/proc \" << me << \"/ready tasks\")\n  {\n    task_queue.set_gauge(&ready_task_count);\n\n    CoreReservationParameters params;\n    params.set_num_cores(1);\n    params.set_numa_domain(numa_node);\n    params.set_alu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_fpu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_ldst_usage(params.CORE_USAGE_SHARED);\n    params.set_max_stack_size(_stack_size);\n\n    std::string name = stringbuilder() << \"Python\" << numa_node << \" proc \" << _me;\n\n    core_rsrv = new CoreReservation(name, crs, params);\n\n    sched = new PythonThreadTaskScheduler(this, *core_rsrv);\n    sched->add_task_queue(&task_queue);\n  }\n\n  LocalPythonProcessor::~LocalPythonProcessor(void)\n  {\n    delete core_rsrv;\n    delete sched;\n  }\n\n  // starts worker threads and performs any per-processor initialization\n  void LocalPythonProcessor::start_threads(void)\n  {\n    // finally, fire up the scheduler\n    sched->start();\n  }\n\n  void LocalPythonProcessor::shutdown(void)\n  {\n    log_py.info() << \"shutting down\";\n\n    sched->shutdown();\n  }\n\n  void LocalPythonProcessor::create_interpreter(void)\n  {\n    assert(interpreter == 0);\n  \n    // create a python interpreter that stays entirely within this thread\n    interpreter = new PythonInterpreter;\n    master_thread = (interpreter->api->PyThreadState_Get)();\n\n    // always need the python threading module\n    interpreter->import_module(\"threading\");\n    \n    // perform requested initialization\n    for(std::vector<std::string>::const_iterator it = import_modules.begin();\n\tit != import_modules.end();\n\t++it)\n      interpreter->import_module(*it);\n\n    for(std::vector<std::string>::const_iterator it = init_scripts.begin();\n\tit != init_scripts.end();\n\t++it)\n      interpreter->run_string(*it);\n\n    // default state is GIL _released_\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n  }\n\n  void LocalPythonProcessor::destroy_interpreter(void)\n  {\n    assert(interpreter != 0);\n\n    // take GIL with master thread\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n\n    // during shutdown, the threading module tries to remove the Thread object\n    //  associated with this kernel thread - if that doesn't exist (because we're\n    //  shutting down from a different thread that we initialized the interpreter\n    //  _and_ nobody called threading.current_thread() from this kernel thread),\n    //  we'll get a KeyError in threading.py\n    // resolve this by calling threading.current_thread() here, using __import__\n    //  to deal with the case where 'import threading' never got called\n    (interpreter->api->PyRun_SimpleString)(\"__import__('threading').current_thread()\");\n\n    delete interpreter;\n    interpreter = 0;\n    master_thread = 0;\n  }\n  \n  bool LocalPythonProcessor::perform_task_registration(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(treg->func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << treg->func_id;\n      assert(0);\n    }\n\n    // next, see if we have a Python function to register\n    const PythonSourceImplementation *psi = treg->codedesc->find_impl<PythonSourceImplementation>();\n    if(!psi) {\n      log_py.fatal() << \"invalid code descriptor for python proc: \" << *(treg->codedesc);\n      assert(0);\n    }\n\n    // perform import/compile on master thread\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n    \n    PyObject *fnptr = interpreter->find_or_import_function(psi);\n    assert(fnptr != 0);\n\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n\n    log_py.info() << \"task \" << treg->func_id << \" registered on \" << me << \": \" << *(treg->codedesc);\n\n    TaskTableEntry &tte = task_table[treg->func_id];\n    tte.fnptr = fnptr;\n    tte.user_data.swap(treg->user_data);\n\n    delete treg->codedesc;\n    delete treg;\n\n    return true;\n  }\n\n  void LocalPythonProcessor::enqueue_task(Task *task)\n  {\n    // just jam it into the task queue, scheduler will take care of the rest\n    if(task->mark_ready())\n      task_queue.put(task, task->priority);\n    else\n      task->mark_finished(false /*!successful*/);\n  }\n\n  void LocalPythonProcessor::spawn_task(Processor::TaskFuncID func_id,\n\t\t\t\t\tconst void *args, size_t arglen,\n\t\t\t\t\tconst ProfilingRequestSet &reqs,\n\t\t\t\t\tEvent start_event, Event finish_event,\n\t\t\t\t\tint priority)\n  {\n    // create a task object for this\n    Task *task = new Task(me, func_id, args, arglen, reqs,\n\t\t\t  start_event, finish_event, priority);\n    get_runtime()->optable.add_local_operation(finish_event, task);\n\n    // if the start event has already triggered, we can enqueue right away\n    bool poisoned = false;\n    if (start_event.has_triggered_faultaware(poisoned)) {\n      if(poisoned) {\n\tlog_poison.info() << \"cancelling poisoned task - task=\" << task << \" after=\" << task->get_finish_event();\n\ttask->handle_poisoned_precondition(start_event);\n      } else\n\tenqueue_task(task);\n    } else {\n      EventImpl::add_waiter(start_event, new DeferredTaskSpawn(this, task));\n    }\n  }\n\n  void LocalPythonProcessor::add_to_group(ProcessorGroup *group)\n  {\n    // add the group's task queue to our scheduler too\n    sched->add_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::register_task(Processor::TaskFuncID func_id,\n                                           CodeDescriptor& codedesc,\n                                           const ByteArrayRef& user_data)\n  {\n    TaskRegistration *treg = new TaskRegistration;\n    treg->func_id = func_id;\n    treg->codedesc = new CodeDescriptor(codedesc);\n    treg->user_data = user_data;\n    sched->enqueue_taskreg(treg);\n#if 0\n    {\n      AutoHSLLock al(mutex);\n      bool was_empty = taskreg_queue.empty() && task_queue.empty();\n      taskreg_queue.push_back(treg);\n      if(was_empty)\n\tcondvar.signal();\n    }\n#endif\n#if 0\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << func_id;\n      assert(0);\n    }\n\n    // next, get see if we have a Python function to register\n    const PythonSourceImplementation *psi = codedesc.find_impl<PythonSourceImplementation>();\n    assert(psi != 0);\n\n    PyObject *fnptr = interpreter->find_or_import_function(psi);\n\n    log_py.info() << \"task \" << func_id << \" registered on \" << me << \": \" << codedesc;\n\n    TaskTableEntry &tte = task_table[func_id];\n    tte.fnptr = fnptr;\n    tte.user_data = user_data;\n#endif\n  }\n\n  void LocalPythonProcessor::execute_task(Processor::TaskFuncID func_id,\n\t\t\t\t\t  const ByteArrayRef& task_args)\n  {\n    std::map<Processor::TaskFuncID, TaskTableEntry>::const_iterator it = task_table.find(func_id);\n    if(it == task_table.end()) {\n      // TODO: remove this hack once the tools are available to the HLR to call these directly\n      if(func_id < Processor::TASK_ID_FIRST_AVAILABLE) {\n\tlog_py.info() << \"task \" << func_id << \" not registered on \" << me << \": ignoring missing legacy setup/shutdown task\";\n\treturn;\n      }\n      log_py.fatal() << \"task \" << func_id << \" not registered on \" << me;\n      assert(0);\n    }\n\n    const TaskTableEntry& tte = it->second;\n\n    log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": \" << ((void *)(tte.fnptr));\n\n    PyObject *arg1 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)task_args.base(),\n\t\t\t\t\t\t   task_args.size());\n    assert(arg1 != 0);\n    PyObject *arg2 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)tte.user_data.base(),\n\t\t\t\t\t\t   tte.user_data.size());\n    assert(arg2 != 0);\n    // TODO: make into a Python realm.Processor object\n    PyObject *arg3 = (interpreter->api->PyLong_FromUnsignedLong)(me.id);\n    assert(arg3 != 0);\n\n    PyObject *args = (interpreter->api->PyTuple_New)(3);\n    assert(args != 0);\n    (interpreter->api->PyTuple_SetItem)(args, 0, arg1);\n    (interpreter->api->PyTuple_SetItem)(args, 1, arg2);\n    (interpreter->api->PyTuple_SetItem)(args, 2, arg3);\n\n    //printf(\"args = \"); (interpreter->api->PyObject_Print)(args, stdout, 0); printf(\"\\n\");\n\n    PyObject *res = (interpreter->api->PyObject_CallObject)(tte.fnptr, args);\n\n    (interpreter->api->Py_DecRef)(args);\n\n    //printf(\"res = \"); PyObject_Print(res, stdout, 0); printf(\"\\n\");\n    if(res != 0) {\n      (interpreter->api->Py_DecRef)(res);\n    } else {\n      log_py.fatal() << \"python exception occurred within task:\";\n      (interpreter->api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n  }\n\n  namespace Python {\n\n    ////////////////////////////////////////////////////////////////////////\n    //\n    // class PythonModule\n\n    /*static*/ std::vector<std::string> PythonModule::extra_import_modules;\n\n    PythonModule::PythonModule(void)\n      : Module(\"python\")\n      , cfg_num_python_cpus(0)\n      , cfg_use_numa(false)\n      , cfg_stack_size_in_mb(2)\n    {\n    }\n\n    PythonModule::~PythonModule(void)\n    {}\n\n    /*static*/ void PythonModule::import_python_module(const char *module_name)\n    {\n      extra_import_modules.push_back(module_name);\n    }\n\n    /*static*/ Module *PythonModule::create_module(RuntimeImpl *runtime,\n                                                 std::vector<std::string>& cmdline)\n    {\n      // create a module to fill in with stuff - we'll delete it if numa is\n      //  disabled\n      PythonModule *m = new PythonModule;\n\n      // first order of business - read command line parameters\n      {\n        CommandLineParser cp;\n\n        cp.add_option_int(\"-ll:py\", m->cfg_num_python_cpus)\n\t  .add_option_int(\"-ll:pynuma\", m->cfg_use_numa)\n\t  .add_option_int(\"-ll:pystack\", m->cfg_stack_size_in_mb)\n\t  .add_option_stringlist(\"-ll:pyimport\", m->cfg_import_modules)\n\t  .add_option_stringlist(\"-ll:pyinit\", m->cfg_init_scripts);\n\n        bool ok = cp.parse_command_line(cmdline);\n        if(!ok) {\n          log_py.fatal() << \"error reading Python command line parameters\";\n          assert(false);\n        }\n      }\n\n      // add extra module imports requested by the application\n      m->cfg_import_modules.insert(m->cfg_import_modules.end(),\n                                   extra_import_modules.begin(),\n                                   extra_import_modules.end());\n\n      // if no cpus were requested, there's no point\n      if(m->cfg_num_python_cpus == 0) {\n        log_py.debug() << \"no Python cpus requested\";\n        delete m;\n        return 0;\n      }\n\n#ifndef REALM_USE_DLMOPEN\n      // Multiple CPUs are only allowed if we're using dlmopen.\n      if(m->cfg_num_python_cpus > 1) {\n        log_py.fatal() << \"support for multiple Python CPUs is not available: recompile with USE_DLMOPEN\";\n        assert(false);\n      }\n#endif\n\n      // get number/sizes of NUMA nodes -\n      //   disable (with a warning) numa binding if support not found\n      if(m->cfg_use_numa) {\n        std::map<int, NumaNodeCpuInfo> cpuinfo;\n        if(numasysif_numa_available() &&\n           numasysif_get_cpu_info(cpuinfo) &&\n           !cpuinfo.empty()) {\n          // filter out any numa domains with insufficient core counts\n          int cores_needed = m->cfg_num_python_cpus;\n          for(std::map<int, NumaNodeCpuInfo>::const_iterator it = cpuinfo.begin();\n              it != cpuinfo.end();\n              ++it) {\n            const NumaNodeCpuInfo& ci = it->second;\n            if(ci.cores_available >= cores_needed) {\n              m->active_numa_domains.insert(ci.node_id);\n            } else {\n              log_py.warning() << \"not enough cores in NUMA domain \" << ci.node_id << \" (\" << ci.cores_available << \" < \" << cores_needed << \")\";\n            }\n          }\n        } else {\n          log_py.warning() << \"numa support not found (or not working)\";\n          m->cfg_use_numa = false;\n        }\n      }\n\n      // if we don't end up with any active numa domains,\n      //  use NUMA_DOMAIN_DONTCARE\n      // actually, use the value (-1) since it seems to cause link errors!?\n      if(m->active_numa_domains.empty())\n        m->active_numa_domains.insert(-1 /*CoreReservationParameters::NUMA_DOMAIN_DONTCARE*/);\n\n      return m;\n    }\n\n    // do any general initialization - this is called after all configuration is\n    //  complete\n    void PythonModule::initialize(RuntimeImpl *runtime)\n    {\n      Module::initialize(runtime);\n    }\n\n    // create any processors provided by the module (default == do nothing)\n    //  (each new ProcessorImpl should use a Processor from\n    //   RuntimeImpl::next_local_processor_id)\n    void PythonModule::create_processors(RuntimeImpl *runtime)\n    {\n      Module::create_processors(runtime);\n\n      for(std::set<int>::const_iterator it = active_numa_domains.begin();\n          it != active_numa_domains.end();\n          ++it) {\n        int cpu_node = *it;\n        for(int i = 0; i < cfg_num_python_cpus; i++) {\n          Processor p = runtime->next_local_processor_id();\n          ProcessorImpl *pi = new LocalPythonProcessor(p, cpu_node,\n                                                       runtime->core_reservation_set(),\n                                                       cfg_stack_size_in_mb << 20,\n\t\t\t\t\t\t       cfg_import_modules,\n\t\t\t\t\t\t       cfg_init_scripts);\n          runtime->add_processor(pi);\n\n          // create affinities between this processor and system/reg memories\n          // if the memory is one we created, use the kernel-reported distance\n          // to adjust the answer\n          std::vector<MemoryImpl *>& local_mems = runtime->nodes[my_node_id].memories;\n          for(std::vector<MemoryImpl *>::iterator it2 = local_mems.begin();\n              it2 != local_mems.end();\n              ++it2) {\n            Memory::Kind kind = (*it2)->get_kind();\n            if((kind != Memory::SYSTEM_MEM) && (kind != Memory::REGDMA_MEM))\n              continue;\n\n            Machine::ProcessorMemoryAffinity pma;\n            pma.p = p;\n            pma.m = (*it2)->me;\n\n            // use the same made-up numbers as in\n            //  runtime_impl.cc\n            if(kind == Memory::SYSTEM_MEM) {\n              pma.bandwidth = 100;  // \"large\"\n              pma.latency = 5;      // \"small\"\n            } else {\n              pma.bandwidth = 80;   // \"large\"\n              pma.latency = 10;     // \"small\"\n            }\n\n            runtime->add_proc_mem_affinity(pma);\n          }\n        }\n      }\n    }\n\n    // clean up any common resources created by the module - this will be called\n    //  after all memories/processors/etc. have been shut down and destroyed\n    void PythonModule::cleanup(void)\n    {\n      Module::cleanup();\n    }\n\n  }; // namespace Python\n\n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/examples/dynamic_registration/dynamic_registration.cc": "/* Copyright 2018 Stanford University\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n\n#include <cstdio>\n#include <cassert>\n#include <cstdlib>\n#define LEGION_ENABLE_C_BINDINGS\n#include \"legion.h\"\n#include \"mappers/default_mapper.h\"\n\nusing namespace Legion;\n\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit.h\"\n#endif\n\n/*\n * In this example we illustrate how the Legion\n * programming model supports multiple partitions\n * of the same logical region and the benefits it\n * provides by allowing multiple views onto the\n * same logical region.  We compute a simple 5-point\n * 1D stencil using the standard forumala:\n * f'(x) = (-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h))/12h\n * For simplicity we'll assume h=1.\n */\n\nenum TaskIDs {\n  TOP_LEVEL_TASK_ID,\n  INIT_FIELD_TASK_ID,\n  STENCIL_TASK_ID,\n  CHECK_TASK_ID,\n  WRAPPED_CPP_TASK_ID,\n  WRAPPED_C_TASK_ID,\n#ifdef REALM_USE_LLVM\n  WRAPPED_LLVM_TASK_ID,\n#endif\n};\n\nenum FieldIDs {\n  FID_VAL,\n  FID_DERIV,\n};\n\n// Forward declarations\n\nvoid init_field_task(const Task *task,\n                     const std::vector<PhysicalRegion> &regions,\n                     Context ctx, Runtime *runtime);\n\nvoid stencil_task(const Task *task,\n                  const std::vector<PhysicalRegion> &regions,\n                  Context ctx, Runtime *runtime);\n\nvoid check_task(const Task *task,\n                const std::vector<PhysicalRegion> &regions,\n                Context ctx, Runtime *runtime);\n\nvoid wrapped_cpp_task(const void *data, size_t datalen,\n\t\t      const void *userdata, size_t userlen, Processor p)\n{\n  const Task *task;\n  const std::vector<PhysicalRegion> *regions;\n  Context ctx;\n  Runtime *runtime;\n  LegionTaskWrapper::legion_task_preamble(data, datalen, p,\n\t\t\t\t\t  task,\n\t\t\t\t\t  regions,\n\t\t\t\t\t  ctx,\n\t\t\t\t\t  runtime);\n  printf(\"hello from wrapped_cpp_task (msg='%.*s')\\n\",\n\t (int)userlen, (const char *)userdata);\n  LegionTaskWrapper::legion_task_postamble(runtime, ctx);\n}\n\nvoid wrapped_c_task(const void *data, size_t datalen,\n\t\t    const void *userdata, size_t userlen, Processor p)\n{\n  legion_task_t task;\n  const legion_physical_region_t *regions;\n  unsigned num_regions;\n  legion_context_t ctx;\n  legion_runtime_t runtime;\n  legion_task_preamble(data, datalen, p.id,\n\t\t       &task,\n\t\t       &regions,\n\t\t       &num_regions,\n\t\t       &ctx,\n\t\t       &runtime);\n  printf(\"hello from wrapped_c_task (msg='%.*s')\\n\",\n\t (int)userlen, (const char *)userdata);\n  legion_task_postamble(runtime, ctx, 0, 0);\n}\n\n#ifdef REALM_USE_LLVM\nconst char llvm_ir[] = \n  \"%struct.legion_physical_region_t = type { i8* }\\n\"\n  \"%struct.legion_task_t = type { i8* }\\n\"\n  \"%struct.legion_context_t = type { i8* }\\n\"\n  \"%struct.legion_runtime_t = type { i8* }\\n\"\n  \"declare i32 @printf(i8*, ...)\\n\"\n  \"declare void @legion_task_preamble(i8*, i64, i64, %struct.legion_task_t*, %struct.legion_physical_region_t**, i32*, %struct.legion_context_t*, %struct.legion_runtime_t*)\\n\"\n  \"declare void @legion_task_postamble(%struct.legion_runtime_t, %struct.legion_context_t, i8*, i64)\\n\"\n  \"@.str = private unnamed_addr constant [31 x i8] c\\\"hello from llvm wrapped task!\\\\0A\\\\00\\\", align 1\\n\"\n  \"define void @body(%struct.legion_task_t %task, %struct.legion_physical_region_t* %regions, i32 %num_regions, %struct.legion_context_t %ctx, %struct.legion_runtime_t %runtime) {\\n\"\n  \"  %1 = bitcast [31 x i8]* @.str to i8*\\n\"\n#if REALM_LLVM_VERSION >= 37\n  \"  %2 = call i32 (i8*, ...) @printf(i8* %1)\\n\"\n#else\n  \"  %2 = call i32 (i8*, ...)* @printf(i8* %1)\\n\"\n#endif\n  \"  ret void\\n\"\n  \"}\\n\"\n  \"define void @llvm_wrapper(i8* %data, i64 %datalen, i8* %userdata, i64 %userlen, i64 %proc_id) {\\n\"\n  \"  %task_ptr = alloca %struct.legion_task_t, align 8\\n\"\n  \"  %regions_ptr = alloca %struct.legion_physical_region_t*, align 8\\n\"\n  \"  %num_regions_ptr = alloca i32, align 4\\n\"\n  \"  %ctx_ptr = alloca %struct.legion_context_t, align 8\\n\"\n  \"  %runtime_ptr = alloca %struct.legion_runtime_t, align 8\\n\"\n  \"  call void @legion_task_preamble(i8* %data, i64 %datalen, i64 %proc_id, %struct.legion_task_t* %task_ptr, %struct.legion_physical_region_t** %regions_ptr, i32* %num_regions_ptr, %struct.legion_context_t* %ctx_ptr, %struct.legion_runtime_t* %runtime_ptr)\\n\"\n#if REALM_LLVM_VERSION >= 37\n  \"  %task = load %struct.legion_task_t, %struct.legion_task_t* %task_ptr\\n\"\n  \"  %regions = load %struct.legion_physical_region_t*, %struct.legion_physical_region_t** %regions_ptr\\n\"\n  \"  %num_regions = load i32, i32* %num_regions_ptr\\n\"\n  \"  %ctx = load %struct.legion_context_t, %struct.legion_context_t* %ctx_ptr\\n\"\n  \"  %runtime = load %struct.legion_runtime_t, %struct.legion_runtime_t* %runtime_ptr\\n\"\n#else\n  \"  %task = load %struct.legion_task_t* %task_ptr\\n\"\n  \"  %regions = load %struct.legion_physical_region_t** %regions_ptr\\n\"\n  \"  %num_regions = load i32* %num_regions_ptr\\n\"\n  \"  %ctx = load %struct.legion_context_t* %ctx_ptr\\n\"\n  \"  %runtime = load %struct.legion_runtime_t* %runtime_ptr\\n\"\n#endif\n  \"  call void @body(%struct.legion_task_t %task, %struct.legion_physical_region_t* %regions, i32 %num_regions, %struct.legion_context_t %ctx, %struct.legion_runtime_t %runtime)\\n\"\n  \"  call void @legion_task_postamble(%struct.legion_runtime_t %runtime, %struct.legion_context_t %ctx, i8* null, i64 0)\\n\"\n  \"  ret void\\n\"\n  \"}\\n\"\n  ;\n#endif\n\nvoid top_level_task(const Task *task,\n                    const std::vector<PhysicalRegion> &regions,\n                    Context ctx, Runtime *runtime)\n{\n  FieldSpace fs = runtime->create_field_space(ctx);\n  {\n    FieldAllocator allocator = \n      runtime->create_field_allocator(ctx, fs);\n    allocator.allocate_field(sizeof(double),FID_VAL);\n    allocator.allocate_field(sizeof(double),FID_DERIV);\n  }\n  // Make an SOA constraint and use it as the layout constraint for\n  // all the different task variants that we are registering\n  LayoutConstraintRegistrar layout_registrar(fs, \"SOA layout\");\n  std::vector<DimensionKind> dim_order(2);\n  dim_order[0] = DIM_X;\n  dim_order[1] = DIM_F; // fields go last for SOA\n  layout_registrar.add_constraint(OrderingConstraint(dim_order, false/*contig*/));\n\n  LayoutConstraintID soa_layout_id = runtime->register_layout(layout_registrar);\n\n#ifdef USE_LIBDL\n  // rely on dladdr/dlsym to make function pointers portable for global\n  //  task registration\n  bool global_taskreg = true;\n#else\n  // function pointers will not be portable, so limit tasks to local node\n  const bool global_taskreg = false;\n#endif\n\n  // Dynamically register some more tasks\n  TaskVariantRegistrar init_registrar(INIT_FIELD_TASK_ID,\n                                      \"cpu_init_variant\",\n\t\t\t\t      global_taskreg);\n  // Add our constraints\n  init_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC))\n      .add_layout_constraint_set(0/*index*/, soa_layout_id);\n  runtime->register_task_variant<init_field_task>(init_registrar);\n\n  TaskVariantRegistrar stencil_registrar(STENCIL_TASK_ID,\n                                         \"cpu_stencil_variant\",\n\t\t\t\t\t global_taskreg);\n  stencil_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC))\n      .add_layout_constraint_set(0/*index*/, soa_layout_id)\n      .add_layout_constraint_set(1/*index*/, soa_layout_id);\n  runtime->register_task_variant<stencil_task>(stencil_registrar);\n\n  TaskVariantRegistrar check_registrar(CHECK_TASK_ID,\n                                       \"cpu_check_variant\",\n\t\t\t\t       global_taskreg);\n  check_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC))\n      .add_layout_constraint_set(0/*index*/, soa_layout_id)\n      .add_layout_constraint_set(1/*index*/, soa_layout_id);\n  runtime->register_task_variant<check_task>(check_registrar);\n\n  TaskVariantRegistrar wrapped_cpp_registrar(WRAPPED_CPP_TASK_ID,\n\t\t\t\t\t     \"wrapped_cpp_variant\",\n\t\t\t\t\t     global_taskreg);\n  wrapped_cpp_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  const char cpp_msg[] = \"user data for cpp task\";\n  runtime->register_task_variant(wrapped_cpp_registrar,\n\t\t\t\t CodeDescriptor(wrapped_cpp_task),\n\t\t\t\t cpp_msg, sizeof(cpp_msg));\n\n  TaskVariantRegistrar wrapped_c_registrar(WRAPPED_C_TASK_ID,\n\t\t\t\t\t   \"wrapped_c_variant\",\n\t\t\t\t\t   global_taskreg);\n  wrapped_c_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  const char c_msg[] = \"user data for c task\";\n  runtime->register_task_variant(wrapped_c_registrar,\n\t\t\t\t CodeDescriptor(wrapped_c_task),\n\t\t\t\t c_msg, sizeof(c_msg));\n\n#ifdef REALM_USE_LLVM\n  // LLVM IR is portable, so we can do global registration even without libdl\n  TaskVariantRegistrar wrapped_llvm_registrar(WRAPPED_LLVM_TASK_ID,\n\t\t\t\t\t      \"wrapped_llvm_variant\",\n\t\t\t\t\t      true /*global*/);\n  wrapped_llvm_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  const char llvm_msg[] = \"user data for llvm task\";\n  CodeDescriptor llvm_cd(Realm::Type::from_cpp_type<Processor::TaskFuncPtr>());\n  llvm_cd.add_implementation(new Realm::LLVMIRImplementation(llvm_ir, sizeof(llvm_ir),\n\t\t\t\t\t\t\t     \"llvm_wrapper\"));\n  runtime->register_task_variant(wrapped_llvm_registrar,\n\t\t\t\t llvm_cd,\n\t\t\t\t llvm_msg, sizeof(llvm_msg));\n#endif\n\n  // Attach semantic infos to the task names\n  runtime->attach_name(INIT_FIELD_TASK_ID, \"init task\");\n  runtime->attach_name(STENCIL_TASK_ID, \"stencil task\");\n  runtime->attach_name(CHECK_TASK_ID, \"check task\");\n  runtime->attach_name(WRAPPED_CPP_TASK_ID, \"wrapped cpp task\");\n  runtime->attach_name(WRAPPED_C_TASK_ID, \"wrapped c task\");\n#ifdef REALM_USE_LLVM\n  runtime->attach_name(WRAPPED_LLVM_TASK_ID, \"wrapped llvm task\");\n#endif\n\n  {\n    int val = 55;\n    TaskLauncher l(WRAPPED_CPP_TASK_ID, TaskArgument(&val, sizeof(val)));\n    if (!global_taskreg)\n      l.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n\n  {\n    int val = 66;\n    TaskLauncher l(WRAPPED_C_TASK_ID, TaskArgument(&val, sizeof(val)));\n    if (!global_taskreg)\n      l.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n\n#ifdef REALM_USE_LLVM\n  {\n    int val = 77;\n    TaskLauncher l(WRAPPED_LLVM_TASK_ID, TaskArgument(&val, sizeof(val)));\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n#endif\n\n  int num_elements = 1024;\n  int num_subregions = 4;\n  // Check for any command line arguments\n  {\n      const InputArgs &command_args = Runtime::get_input_args();\n    for (int i = 1; i < command_args.argc; i++)\n    {\n      if (!strcmp(command_args.argv[i],\"-n\"))\n        num_elements = atoi(command_args.argv[++i]);\n      if (!strcmp(command_args.argv[i],\"-b\"))\n        num_subregions = atoi(command_args.argv[++i]);\n    }\n  }\n  printf(\"Running stencil computation for %d elements...\\n\", num_elements);\n  printf(\"Partitioning data into %d sub-regions...\\n\", num_subregions);\n\n  Rect<1> elem_rect(0,num_elements-1);\n  IndexSpaceT<1> is = runtime->create_index_space(ctx, elem_rect);\n  LogicalRegion stencil_lr = runtime->create_logical_region(ctx, is, fs);\n  \n  Rect<1> color_bounds(0,num_subregions-1);\n  IndexSpaceT<1> color_is = runtime->create_index_space(ctx, color_bounds);\n\n  IndexPartition disjoint_ip = \n    runtime->create_equal_partition(ctx, is, color_is);\n  const int block_size = (num_elements + num_subregions - 1) / num_subregions;\n  Transform<1,1> transform;\n  transform[0][0] = block_size;\n  Rect<1> extent(-2, block_size + 1);\n  IndexPartition ghost_ip = \n    runtime->create_partition_by_restriction(ctx, is, color_is, transform, extent);\n\n  LogicalPartition disjoint_lp = \n    runtime->get_logical_partition(ctx, stencil_lr, disjoint_ip);\n  LogicalPartition ghost_lp = \n    runtime->get_logical_partition(ctx, stencil_lr, ghost_ip);\n\n  ArgumentMap arg_map;\n\n  IndexLauncher init_launcher(INIT_FIELD_TASK_ID, color_is,\n                              TaskArgument(NULL, 0), arg_map);\n  if (!global_taskreg)\n    init_launcher.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n  init_launcher.add_region_requirement(\n      RegionRequirement(disjoint_lp, 0/*projection ID*/,\n                        WRITE_DISCARD, EXCLUSIVE, stencil_lr));\n  init_launcher.add_field(0, FID_VAL);\n  runtime->execute_index_space(ctx, init_launcher);\n\n  IndexLauncher stencil_launcher(STENCIL_TASK_ID, color_is,\n       TaskArgument(&num_elements, sizeof(num_elements)), arg_map);\n  if (!global_taskreg)\n    stencil_launcher.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n  stencil_launcher.add_region_requirement(\n      RegionRequirement(ghost_lp, 0/*projection ID*/,\n                        READ_ONLY, EXCLUSIVE, stencil_lr));\n  stencil_launcher.add_field(0, FID_VAL);\n  stencil_launcher.add_region_requirement(\n      RegionRequirement(disjoint_lp, 0/*projection ID*/,\n                        READ_WRITE, EXCLUSIVE, stencil_lr));\n  stencil_launcher.add_field(1, FID_DERIV);\n  runtime->execute_index_space(ctx, stencil_launcher);\n\n  TaskLauncher check_launcher(CHECK_TASK_ID, \n      TaskArgument(&num_elements, sizeof(num_elements)));\n  if (!global_taskreg)\n    check_launcher.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n  check_launcher.add_region_requirement(\n      RegionRequirement(stencil_lr, READ_ONLY, EXCLUSIVE, stencil_lr));\n  check_launcher.add_field(0, FID_VAL);\n  check_launcher.add_region_requirement(\n      RegionRequirement(stencil_lr, READ_ONLY, EXCLUSIVE, stencil_lr));\n  check_launcher.add_field(1, FID_DERIV);\n  runtime->execute_task(ctx, check_launcher);\n\n  runtime->destroy_logical_region(ctx, stencil_lr);\n  runtime->destroy_field_space(ctx, fs);\n  runtime->destroy_index_space(ctx, is);\n}\n\n// The standard initialize field task from earlier examples\nvoid init_field_task(const Task *task,\n                     const std::vector<PhysicalRegion> &regions,\n                     Context ctx, Runtime *runtime)\n{\n  assert(regions.size() == 1); \n  assert(task->regions.size() == 1);\n  assert(task->regions[0].privilege_fields.size() == 1);\n\n  FieldID fid = *(task->regions[0].privilege_fields.begin());\n  const int point = task->index_point.point_data[0];\n  printf(\"Initializing field %d for block %d...\\n\", fid, point);\n\n  const FieldAccessor<WRITE_DISCARD,double,1> acc(regions[0], fid);\n\n  Rect<1> rect = runtime->get_index_space_domain(ctx,\n                  task->regions[0].region.get_index_space());\n  for (PointInRectIterator<1> pir(rect); pir(); pir++)\n    acc[*pir] = drand48();\n}\n\n// Our stencil tasks is interesting because it\n// has both slow and fast versions depending\n// on whether or not its bounds have been clamped.\nvoid stencil_task(const Task *task,\n                  const std::vector<PhysicalRegion> &regions,\n                  Context ctx, Runtime *runtime)\n{\n  assert(regions.size() == 2);\n  assert(task->regions.size() == 2);\n  assert(task->regions[0].privilege_fields.size() == 1);\n  assert(task->regions[1].privilege_fields.size() == 1);\n  assert(task->arglen == sizeof(int));\n  const int max_elements = *((const int*)task->args);\n  const int point = task->index_point.point_data[0];\n  \n  FieldID read_fid = *(task->regions[0].privilege_fields.begin());\n  FieldID write_fid = *(task->regions[1].privilege_fields.begin());\n\n  const FieldAccessor<READ_ONLY,double,1> read_acc(regions[0], read_fid);\n  const FieldAccessor<WRITE_DISCARD,double,1> write_acc(regions[1], write_fid);\n\n  Rect<1> rect = runtime->get_index_space_domain(ctx,\n                  task->regions[1].region.get_index_space());\n  // If we are on the edges of the entire space we are \n  // operating over, then we're going to do the slow\n  // path which checks for clamping when necessary.\n  // If not, then we can do the fast path without\n  // any checks.\n  if ((rect.lo[0] < 2) || (rect.hi[0] > (max_elements-3)))\n  {\n    printf(\"Running slow stencil path for point %d...\\n\", point);\n    // Note in the slow path that there are checks which\n    // perform clamps when necessary before reading values.\n    for (PointInRectIterator<1> pir(rect); pir(); pir++)\n    {\n      double l2, l1, r1, r2;\n      if (pir[0] < 2)\n        l2 = read_acc[0];\n      else\n        l2 = read_acc[*pir - 2];\n      if (pir[0] < 1)\n        l1 = read_acc[0];\n      else\n        l1 = read_acc[*pir - 1];\n      if (pir[0] > (max_elements-2))\n        r1 = read_acc[max_elements-1];\n      else\n        r1 = read_acc[*pir + 1];\n      if (pir[0] > (max_elements-3))\n        r2 = read_acc[max_elements-1];\n      else\n        r2 = read_acc[*pir + 2];\n      \n      double result = (-l2 + 8.0*l1 - 8.0*r1 + r2) / 12.0;\n      write_acc[*pir] = result;\n    }\n  }\n  else\n  {\n    printf(\"Running fast stencil path for point %d...\\n\", point);\n    // In the fast path, we don't need any checks\n    for (PointInRectIterator<1> pir(rect); pir(); pir++)\n    {\n      double l2 = read_acc[*pir - 2];\n      double l1 = read_acc[*pir - 1];\n      double r1 = read_acc[*pir + 1];\n      double r2 = read_acc[*pir + 2];\n\n      double result = (-l2 + 8.0*l1 - 8.0*r1 + r2) / 12.0;\n      write_acc[*pir] = result;\n    }\n  }\n}\n\nvoid check_task(const Task *task,\n                const std::vector<PhysicalRegion> &regions,\n                Context ctx, Runtime *runtime)\n{\n  assert(regions.size() == 2);\n  assert(task->regions.size() == 2);\n  assert(task->regions[0].privilege_fields.size() == 1);\n  assert(task->regions[1].privilege_fields.size() == 1);\n  assert(task->arglen == sizeof(int));\n  const int max_elements = *((const int*)task->args);\n\n  FieldID src_fid = *(task->regions[0].privilege_fields.begin());\n  FieldID dst_fid = *(task->regions[1].privilege_fields.begin());\n\n  const FieldAccessor<READ_ONLY,double,1> src_acc(regions[0], src_fid);\n  const FieldAccessor<READ_ONLY,double,1> dst_acc(regions[1], dst_fid);\n\n  Rect<1> rect = runtime->get_index_space_domain(ctx,\n                  task->regions[1].region.get_index_space());\n\n  // This is the checking task so we can just do the slow path\n  bool all_passed = true;\n  for (PointInRectIterator<1> pir(rect); pir(); pir++)\n  {\n    double l2, l1, r1, r2;\n    if (pir[0] < 2)\n      l2 = src_acc[0];\n    else\n      l2 = src_acc[*pir - 2];\n    if (pir[0] < 1)\n      l1 = src_acc[0];\n    else\n      l1 = src_acc[*pir - 1];\n    if (pir[0] > (max_elements-2))\n      r1 = src_acc[max_elements-1];\n    else\n      r1 = src_acc[*pir + 1];\n    if (pir[0] > (max_elements-3))\n      r2 = src_acc[max_elements-1];\n    else\n      r2 = src_acc[*pir + 2];\n    \n    double expected = (-l2 + 8.0*l1 - 8.0*r1 + r2) / 12.0;\n    double received = dst_acc[*pir];\n    // Probably shouldn't bitwise compare floating point\n    // numbers but the order of operations are the same so they\n    // should be bitwise equal.\n    if (expected != received)\n      all_passed = false;\n  }\n  if (all_passed)\n    printf(\"SUCCESS!\\n\");\n  else\n    printf(\"FAILURE!\\n\");\n}\n\nint main(int argc, char **argv)\n{\n  Runtime::set_top_level_task_id(TOP_LEVEL_TASK_ID);\n  // We'll only register our top-level task here\n  TaskVariantRegistrar registrar(TOP_LEVEL_TASK_ID,\n                                 \"top_level_variant\");\n  registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  Runtime::preregister_task_variant<top_level_task>(registrar,\"top_level_task\");\n\n  return Runtime::start(argc, argv);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/language/src/regent/cudahelper.t": "-- Copyright 2018 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal std_base = require(\"regent/std_base\")\nlocal config = require(\"regent/config\").args()\nlocal report = require(\"common/report\")\n\nlocal cudahelper = {}\ncudahelper.check_cuda_available = function() return false end\n\nif not config[\"cuda\"] or not terralib.cudacompile then\n  return cudahelper\nend\n\n-- copied and modified from cudalib.lua in Terra interpreter\n\nlocal ffi = require('ffi')\n\nlocal cudapaths = { OSX = \"/usr/local/cuda/lib/libcuda.dylib\";\n                    Linux =  \"libcuda.so\";\n                    Windows = \"nvcuda.dll\"; }\n\nlocal cudaruntimelinked = false\nfunction cudahelper.link_driver_library()\n    if cudaruntimelinked then return end\n    local path = assert(cudapaths[ffi.os],\"unknown OS?\")\n    terralib.linklibrary(path)\n    cudaruntimelinked = true\nend\n\n--\n\nlocal ef = terralib.externfunction\nlocal externcall_builtin = terralib.externfunction\n\nlocal RuntimeAPI = terralib.includec(\"cuda_runtime.h\")\nlocal HijackAPI = terralib.includec(\"legion_terra_cudart_hijack.h\")\n\nlocal C = terralib.includecstring [[\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n]]\n\nlocal struct CUctx_st\nlocal struct CUmod_st\nlocal struct CUlinkState_st\nlocal struct CUfunc_st\nlocal CUdevice = int32\nlocal CUjit_option = uint32\nlocal CU_JIT_ERROR_LOG_BUFFER = 5\nlocal CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES = 6\nlocal CU_JIT_INPUT_PTX = 1\nlocal CU_JIT_TARGET = 9\nlocal DriverAPI = {\n  cuInit = ef(\"cuInit\", {uint32} -> uint32);\n  cuCtxGetCurrent = ef(\"cuCtxGetCurrent\", {&&CUctx_st} -> uint32);\n  cuCtxGetDevice = ef(\"cuCtxGetDevice\",{&int32} -> uint32);\n  cuDeviceGet = ef(\"cuDeviceGet\",{&int32,int32} -> uint32);\n  cuCtxCreate_v2 = ef(\"cuCtxCreate_v2\",{&&CUctx_st,uint32,int32} -> uint32);\n  cuCtxDestroy = ef(\"cuCtxDestroy\",{&CUctx_st} -> uint32);\n  cuDeviceComputeCapability = ef(\"cuDeviceComputeCapability\",\n    {&int32,&int32,int32} -> uint32);\n}\n\nlocal dlfcn = terralib.includec(\"dlfcn.h\")\nlocal terra has_symbol(symbol : rawstring)\n  var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n  var has_symbol = dlfcn.dlsym(lib, symbol) ~= [&opaque](0)\n  dlfcn.dlclose(lib)\n  return has_symbol\nend\n\ndo\n  if not config[\"cuda-offline\"] then\n    if has_symbol(\"cuInit\") then\n      local r = DriverAPI.cuInit(0)\n      assert(r == 0)\n      terra cudahelper.check_cuda_available()\n        return [r] == 0;\n      end\n    else\n      terra cudahelper.check_cuda_available()\n        return false\n      end\n    end\n  else\n    terra cudahelper.check_cuda_available()\n      return true\n    end\n  end\nend\n\n-- copied and modified from cudalib.lua in Terra interpreter\n\nlocal c = terralib.includec(\"unistd.h\")\n\nlocal lua_assert = assert\nlocal terra assert(x : bool, message : rawstring)\n  if not x then\n    var stderr = C.fdopen(2, \"w\")\n    C.fprintf(stderr, \"assertion failed: %s\\n\", message)\n    -- Just because it's stderr doesn't mean it's unbuffered...\n    C.fflush(stderr)\n    C.abort()\n  end\nend\n\nlocal terra get_cuda_version() : uint64\n  var cx : &CUctx_st\n  var cx_created = false\n  var r = DriverAPI.cuCtxGetCurrent(&cx)\n  assert(r == 0, \"CUDA error in cuCtxGetCurrent\")\n  var device : int32\n  if cx ~= nil then\n    r = DriverAPI.cuCtxGetDevice(&device)\n    assert(r == 0, \"CUDA error in cuCtxGetDevice\")\n  else\n    r = DriverAPI.cuDeviceGet(&device, 0)\n    assert(r == 0, \"CUDA error in cuDeviceGet\")\n    r = DriverAPI.cuCtxCreate_v2(&cx, 0, device)\n    assert(r == 0, \"CUDA error in cuCtxCreate_v2\")\n    cx_created = true\n  end\n\n  var major : int, minor : int\n  r = DriverAPI.cuDeviceComputeCapability(&major, &minor, device)\n  assert(r == 0, \"CUDA error in cuDeviceComputeCapability\")\n  var version = [uint64](major * 10 + minor)\n  if cx_created then\n    DriverAPI.cuCtxDestroy(cx)\n  end\n  return version\nend\n\n--\n\nstruct fat_bin_t {\n  magic : int,\n  versions : int,\n  data : &opaque,\n  filename : &opaque,\n}\n\nlocal terra register_ptx(ptxc : rawstring, ptxSize : uint32, version : uint64) : &&opaque\n  var fat_bin : &fat_bin_t\n  -- TODO: this line is leaking memory\n  fat_bin = [&fat_bin_t](C.malloc(sizeof(fat_bin_t)))\n  fat_bin.magic = 1234\n  fat_bin.versions = 5678\n  fat_bin.data = C.malloc(ptxSize + 1)\n  fat_bin.data = ptxc\n  var handle = HijackAPI.hijackCudaRegisterFatBinary(fat_bin)\n  return handle\nend\n\nlocal terra register_function(handle : &&opaque, id : int, name : &int8)\n  HijackAPI.hijackCudaRegisterFunction(handle, [&int8](id), name)\nend\n\nlocal function find_device_library(target)\n  local device_lib_dir = terralib.cudahome .. \"/nvvm/libdevice/\"\n  local libdevice = nil\n  for f in io.popen(\"ls \" .. device_lib_dir):lines() do\n    local version = tonumber(string.match(string.match(f, \"[0-9][0-9][.]\"), \"[0-9][0-9]\"))\n    if version <= target then\n      libdevice = device_lib_dir .. f\n    end\n  end\n  assert(libdevice ~= nil, \"Failed to find a device library\")\n  return libdevice\nend\n\nlocal supported_archs = {\n  [\"fermi\"]   = 20,\n  [\"kepler\"]  = 30,\n  [\"k20\"]     = 35,\n  [\"maxwell\"] = 52,\n  [\"pascal\"]  = 60,\n  [\"volta\"]   = 70,\n}\n\nlocal function parse_cuda_arch(arch)\n  arch = string.lower(arch)\n  local sm = supported_archs[arch]\n  if sm == nil then\n    local archs\n    for k, v in pairs(supported_archs) do\n      archs = (not archs and k) or (archs and archs .. \", \" .. k)\n    end\n    print(\"Error: Unsupported GPU architecture \" .. arch ..\n          \". Supported architectures: \" .. archs)\n    os.exit(1)\n  end\n  return sm\nend\n\nfunction cudahelper.jit_compile_kernels_and_register(kernels)\n  local module = {}\n  for k, v in pairs(kernels) do\n    module[v.name] = v.kernel\n  end\n  local version\n  if not config[\"cuda-offline\"] then\n    version = get_cuda_version()\n  else\n    version = parse_cuda_arch(config[\"cuda-arch\"])\n  end\n  local libdevice = find_device_library(tonumber(version))\n  local llvmbc = terralib.linkllvm(libdevice)\n  externcall_builtin = function(name, ftype)\n    return llvmbc:extern(name, ftype)\n  end\n  local ptx = cudalib.toptx(module, nil, version)\n\n  local ptxc = terralib.constant(ptx)\n  local handle = terralib.newsymbol(&&opaque, \"handle\")\n  local register = quote\n    var [handle] = register_ptx(ptxc, [ptx:len() + 1], [version])\n  end\n\n  for k, v in pairs(kernels) do\n    register = quote\n      [register]\n      register_function([handle], [k], [v.name])\n    end\n  end\n\n  return register\nend\n\nlocal THREAD_BLOCK_SIZE = 128\nlocal MAX_NUM_BLOCK = 32768\nlocal GLOBAL_RED_BUFFER = 256\n\nlocal tid_x   = cudalib.nvvm_read_ptx_sreg_tid_x\nlocal n_tid_x = cudalib.nvvm_read_ptx_sreg_ntid_x\nlocal bid_x   = cudalib.nvvm_read_ptx_sreg_ctaid_x\nlocal n_bid_x = cudalib.nvvm_read_ptx_sreg_nctaid_x\n\nlocal tid_y   = cudalib.nvvm_read_ptx_sreg_tid_y\nlocal n_tid_y = cudalib.nvvm_read_ptx_sreg_ntid_y\nlocal bid_y   = cudalib.nvvm_read_ptx_sreg_ctaid_y\nlocal n_bid_y = cudalib.nvvm_read_ptx_sreg_nctaid_y\n\nlocal tid_z   = cudalib.nvvm_read_ptx_sreg_tid_z\nlocal n_tid_z = cudalib.nvvm_read_ptx_sreg_ntid_z\nlocal bid_z   = cudalib.nvvm_read_ptx_sreg_ctaid_z\nlocal n_bid_z = cudalib.nvvm_read_ptx_sreg_nctaid_z\n\nlocal barrier = cudalib.nvvm_barrier0\n\nlocal supported_scalar_red_ops = {\n  [\"+\"]   = true,\n  [\"*\"]   = true,\n  [\"max\"] = true,\n  [\"min\"] = true,\n}\n\nfunction cudahelper.global_thread_id()\n  --local bid = `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\n  --local num_threads = `(n_tid_x() * n_tid_y() * n_tid_z())\n  --return `([bid] * [num_threads] +\n  --         tid_x() +\n  --         n_tid_x() * tid_y() +\n  --         n_tid_x() * n_tid_y() * tid_z())\n  local bid = `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\n  local num_threads = `(n_tid_x())\n  return `([bid] * [num_threads] + tid_x())\nend\n\nfunction cudahelper.global_block_id()\n  return `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\nend\n\n-- Slow atomic operation implementations (copied and modified from Ebb)\nlocal terra cas_uint64(address : &uint64, compare : uint64, value : uint64)\n  return terralib.asm(terralib.types.uint64,\n                      \"atom.global.cas.b64 $0, [$1], $2, $3;\",\n                      \"=l,l,l,l\", true, address, compare, value)\nend\ncas_uint64:setinlined(true)\n\nlocal terra cas_uint32(address : &uint32, compare : uint32, value : uint32)\n  return terralib.asm(terralib.types.uint32,\n                      \"atom.global.cas.b32 $0, [$1], $2, $3;\",\n                      \"=r,l,r,r\", true, address, compare, value)\nend\ncas_uint32:setinlined(true)\n\nlocal function generate_atomic(op, typ)\n  if op == \"+\" and typ == float then\n    return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f32.p0f32\",\n                              {&float,float} -> {float})\n  end\n\n  local cas_type\n  local cas_func\n  if sizeof(typ) == 4 then\n    cas_type = uint32\n    cas_func = cas_uint32\n  else\n    lua_assert(sizeof(typ) == 8)\n    cas_type = uint64\n    cas_func = cas_uint64\n  end\n  local terra atomic_op(address : &typ, operand : typ)\n    var old : typ = @address\n    var assumed : typ\n    var new     : typ\n\n    var new_b     : &cas_type = [&cas_type](&new)\n    var assumed_b : &cas_type = [&cas_type](&assumed)\n    var res       :  cas_type\n\n    var mask = false\n    repeat\n      if not mask then\n        assumed = old\n        new     = [std_base.quote_binary_op(op, assumed, operand)]\n        res     = cas_func([&cas_type](address), @assumed_b, @new_b)\n        old     = @[&typ](&res)\n        mask    = assumed == old\n      end\n    until mask\n  end\n  atomic_op:setinlined(true)\n  return atomic_op\nend\n\nlocal vprintf = ef(\"cudart:vprintf\", {&int8,&int8} -> int)\n\nlocal function createbuffer(args)\n  local Buf = terralib.types.newstruct()\n  for i,e in ipairs(args) do\n    local typ = e:gettype()\n    local field = \"_\"..tonumber(i)\n    typ = typ == float and double or typ\n    table.insert(Buf.entries,{field,typ})\n  end\n  return quote\n    var buf : Buf\n    escape\n        for i,e in ipairs(args) do\n            emit quote\n               buf.[\"_\"..tonumber(i)] = e\n            end\n        end\n    end\n  in\n    [&int8](&buf)\n  end\nend\n\nlocal cuda_printf = macro(function(fmt,...)\n  local buf = createbuffer({...})\n  return `vprintf(fmt,buf)\nend)\n\nfunction cudahelper.compute_reduction_buffer_size(node, reductions)\n  local size = 0\n  for k, v in pairs(reductions) do\n    if size ~= 0 then\n      -- TODO: We assume there is only one scalar reduction for now\n      report.error(node,\n          \"Multiple scalar reductions in a CUDA task are not supported yet\")\n    elseif not supported_scalar_red_ops[v] then\n      report.error(node,\n          \"Scalar reduction with operator \" .. v .. \" is not supported yet\")\n    elseif not (sizeof(k.type) == 4 or sizeof(k.type) == 8) then\n      report.error(node,\n          \"Scalar reduction for type \" .. tostring(k.type) .. \" is not supported yet\")\n    end\n    size = size + THREAD_BLOCK_SIZE * sizeof(k.type)\n  end\n  return size\nend\n\nfunction cudahelper.generate_reduction_preamble(reductions)\n  local preamble = quote end\n  local device_ptrs = terralib.newlist()\n  local device_ptrs_map = {}\n\n  for red_var, red_op in pairs(reductions) do\n    local device_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local init = std_base.reduction_op_init[red_op][red_var.type]\n    preamble = quote\n      [preamble];\n      var [device_ptr] = [&red_var.type](nil)\n      do\n        var r = RuntimeAPI.cudaMalloc([&&opaque](&[device_ptr]),\n                                      [sizeof(red_var.type) * GLOBAL_RED_BUFFER])\n        assert([r] == 0 and [device_ptr] ~= [&red_var.type](nil), \"cudaMalloc failed\")\n        var v : (red_var.type)[GLOBAL_RED_BUFFER]\n        for i = 0, GLOBAL_RED_BUFFER do v[i] = [init] end\n        RuntimeAPI.cudaMemcpy([device_ptr], [&opaque]([&red_var.type](v)),\n                              [sizeof(red_var.type) * GLOBAL_RED_BUFFER],\n                              RuntimeAPI.cudaMemcpyHostToDevice)\n      end\n    end\n    device_ptrs:insert(device_ptr)\n    device_ptrs_map[device_ptr] = red_var\n  end\n\n  return device_ptrs, device_ptrs_map, preamble\nend\n\nfunction cudahelper.generate_reduction_kernel(reductions, device_ptrs_map)\n  local preamble = quote end\n  local postamble = quote end\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local shared_mem_ptr =\n      cudalib.sharedmemory(red_var.type, THREAD_BLOCK_SIZE)\n    local init = std_base.reduction_op_init[red_op][red_var.type]\n    preamble = quote\n      [preamble]\n      var [red_var] = [init]\n      [shared_mem_ptr][ tid_x() ] = [red_var]\n    end\n\n    local tid = terralib.newsymbol(c.size_t, \"tid\")\n    local reduction_tree = quote end\n    local step = THREAD_BLOCK_SIZE\n    while step > 64 do\n      step = step / 2\n      reduction_tree = quote\n        [reduction_tree]\n        if [tid] < step then\n          var v = [std_base.quote_binary_op(red_op,\n                                            `([shared_mem_ptr][ [tid] ]),\n                                            `([shared_mem_ptr][ [tid] + [step] ]))]\n\n          terralib.attrstore(&[shared_mem_ptr][ [tid] ], v, { isvolatile = true })\n        end\n        barrier()\n      end\n    end\n    local unrolled_reductions = terralib.newlist()\n    while step > 1 do\n      step = step / 2\n      unrolled_reductions:insert(quote\n        do\n          var v = [std_base.quote_binary_op(red_op,\n                                            `([shared_mem_ptr][ [tid] ]),\n                                            `([shared_mem_ptr][ [tid] + [step] ]))]\n          terralib.attrstore(&[shared_mem_ptr][ [tid] ], v, { isvolatile = true })\n        end\n        barrier()\n      end)\n    end\n    reduction_tree = quote\n      [reduction_tree]\n      if [tid] < 32 then\n        [unrolled_reductions]\n      end\n    end\n    postamble = quote\n      do\n        var [tid] = tid_x()\n        var bid = [cudahelper.global_block_id()]\n        [shared_mem_ptr][ [tid] ] = [red_var]\n        barrier()\n        [reduction_tree]\n        if [tid] == 0 then\n          [generate_atomic(red_op, red_var.type)](\n            &[device_ptr][bid % [GLOBAL_RED_BUFFER] ], [shared_mem_ptr][ [tid] ])\n        end\n      end\n    end\n  end\n  return preamble, postamble\nend\n\nfunction cudahelper.generate_reduction_postamble(reductions, device_ptrs_map)\n  local postamble = quote end\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local init = std_base.reduction_op_init[red_op][red_var.type]\n    postamble = quote\n      [postamble]\n      do\n        var v : (red_var.type)[GLOBAL_RED_BUFFER]\n        RuntimeAPI.cudaMemcpy([&opaque]([&red_var.type](v)), [device_ptr],\n                              [sizeof(red_var.type) * GLOBAL_RED_BUFFER],\n                              RuntimeAPI.cudaMemcpyDeviceToHost)\n        var tmp : red_var.type = [init]\n        for i = 0, GLOBAL_RED_BUFFER do\n          tmp = [std_base.quote_binary_op(red_op, tmp, `(v[i]))]\n        end\n        [red_var] = [std_base.quote_binary_op(red_op, red_var, tmp)]\n        RuntimeAPI.cudaFree([device_ptr])\n      end\n    end\n  end\n  return postamble\nend\n\nfunction cudahelper.codegen_kernel_call(kernel_id, count, args, shared_mem_size)\n  local setupArguments = terralib.newlist()\n\n  local offset = 0\n  for i = 1, #args do\n    local arg =  args[i]\n    local size = terralib.sizeof(arg.type)\n    setupArguments:insert(quote\n      RuntimeAPI.cudaSetupArgument(&[arg], size, offset)\n    end)\n    offset = offset + size\n  end\n\n  local grid = terralib.newsymbol(RuntimeAPI.dim3, \"grid\")\n  local block = terralib.newsymbol(RuntimeAPI.dim3, \"block\")\n\n  local function round_exp(v, n)\n    return `((v + (n - 1)) / n)\n  end\n\n  local launch_domain_init = quote\n    [block].x, [block].y, [block].z = THREAD_BLOCK_SIZE, 1, 1\n    var num_blocks = [round_exp(count, THREAD_BLOCK_SIZE)]\n    if num_blocks <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z = num_blocks, 1, 1\n    elseif [count] / MAX_NUM_BLOCK <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, [round_exp(num_blocks, MAX_NUM_BLOCK)], 1\n    else\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, MAX_NUM_BLOCK,\n        [round_exp(num_blocks, MAX_NUM_BLOCK, MAX_NUM_BLOCK)]\n    end\n  end\n\n  return quote\n    var [grid], [block]\n    [launch_domain_init]\n    RuntimeAPI.cudaConfigureCall([grid], [block], shared_mem_size, nil)\n    [setupArguments]\n    RuntimeAPI.cudaLaunch([&int8](kernel_id))\n  end\nend\n\nlocal builtin_gpu_fns = {\n  acos  = externcall_builtin(\"__nv_acos\"  , double -> double),\n  asin  = externcall_builtin(\"__nv_asin\"  , double -> double),\n  atan  = externcall_builtin(\"__nv_atan\"  , double -> double),\n  cbrt  = externcall_builtin(\"__nv_cbrt\"  , double -> double),\n  ceil  = externcall_builtin(\"__nv_ceil\"  , double -> double),\n  cos   = externcall_builtin(\"__nv_cos\"   , double -> double),\n  fabs  = externcall_builtin(\"__nv_fabs\"  , double -> double),\n  floor = externcall_builtin(\"__nv_floor\" , double -> double),\n  fmod  = externcall_builtin(\"__nv_fmod\"  , {double, double} -> double),\n  log   = externcall_builtin(\"__nv_log\"   , double -> double),\n  pow   = externcall_builtin(\"__nv_pow\"   , {double, double} -> double),\n  sin   = externcall_builtin(\"__nv_sin\"   , double -> double),\n  sqrt  = externcall_builtin(\"__nv_sqrt\"  , double -> double),\n  tan   = externcall_builtin(\"__nv_tan\"   , double -> double),\n}\n\nlocal cpu_fn_to_gpu_fn = {}\n\nfunction cudahelper.register_builtin(name, cpu_fn)\n  cpu_fn_to_gpu_fn[cpu_fn] = builtin_gpu_fns[name]\nend\n\nfunction cudahelper.replace_with_builtin(fn)\n  return cpu_fn_to_gpu_fn[fn] or fn\nend\n\nreturn cudahelper\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/language/src/regent/openmphelper.t": "-- Copyright 2018 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal std = require(\"regent/std\")\n\nlocal omp = {}\n\nlocal has_openmp = false\ndo\n  local dlfcn = terralib.includec(\"dlfcn.h\")\n  local terra find_openmp_symbols()\n    var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n    var has_openmp =\n      dlfcn.dlsym(lib, \"GOMP_parallel\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_num_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_max_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_thread_num\") ~= [&opaque](0)\n    dlfcn.dlclose(lib)\n    return has_openmp\n  end\n  has_openmp = find_openmp_symbols()\nend\n\nif not (std.config[\"openmp\"] and has_openmp) then\n  omp.check_openmp_available = function() return false end\n  terra omp.get_num_threads() return 1 end\n  terra omp.get_max_threads() return 1 end\n  terra omp.get_thread_num() return 0 end\n  local omp_worker_type =\n    terralib.types.functype(terralib.newlist({&opaque}), terralib.types.unit, false)\n  terra omp.launch(fnptr : &omp_worker_type, data : &opaque, nthreads : int32, flags : uint32)\n    fnptr(data)\n  end\nelse\n  omp.check_openmp_available = function() return true end\n  local omp_abi = terralib.includecstring [[\n    extern int omp_get_num_threads(void);\n    extern int omp_get_max_threads(void);\n    extern int omp_get_thread_num(void);\n    extern void GOMP_parallel(void (*fnptr)(void *data), void *data, int nthreads, unsigned flags);\n  ]]\n\n  omp.get_num_threads = omp_abi.omp_get_num_threads\n  omp.get_max_threads = omp_abi.omp_get_max_threads\n  omp.get_thread_num = omp_abi.omp_get_thread_num\n  omp.launch = omp_abi.GOMP_parallel\nend\n\n-- TODO: This might not be the right size in platforms other than x86\nomp.CACHE_LINE_SIZE = 64\n\nfunction omp.generate_preamble_structured(rect, idx, start_idx, end_idx)\n  return quote\n    var num_threads = [omp.get_num_threads]()\n    var thread_id = [omp.get_thread_num]()\n    var lo = [rect].lo.x[idx]\n    var hi = [rect].hi.x[idx] + 1\n    var chunk = (hi - lo + num_threads - 1) / num_threads\n    if chunk == 0 then chunk = 1 end\n    var [start_idx] = thread_id * chunk + lo\n    var [end_idx] = (thread_id + 1) * chunk + lo\n    if [end_idx] > hi then [end_idx] = hi end\n  end\nend\n\nfunction omp.generate_argument_type(symbols, reductions)\n  local arg_type = terralib.types.newstruct(\"omp_worker_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(symbols) do\n    local field_name\n    if reductions[symbol] == nil then\n      field_name = \"_arg\" .. tostring(i)\n      arg_type.entries:insert({ field_name, symbol.type })\n    else\n      field_name = \"_red\" .. tostring(i)\n      arg_type.entries:insert({ field_name, &symbol.type })\n    end\n    mapping[field_name] = symbol\n  end\n  return arg_type, mapping\nend\n\nfunction omp.generate_argument_init(arg, arg_type, mapping, reductions)\n  local worker_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      return quote var [symbol] = [init] end\n    else\n      return quote var [symbol] = [arg].[field_name] end\n    end\n  end)\n  local launch_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      assert(field_type:ispointer())\n      return quote\n        -- We don't like false sharing\n        [arg].[field_name] =\n          [field_type](std.c.malloc([omp.get_max_threads]()  * omp.CACHE_LINE_SIZE))\n      end\n    else\n      return quote [arg].[field_name] = [symbol] end\n    end\n  end)\n  return worker_init, launch_init\nend\n\nfunction omp.generate_worker_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      return quote\n        do\n          var idx = [omp.get_thread_num]() * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [arg].[field_name][idx] = [symbol]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nfunction omp.generate_launcher_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        for i = 0, [omp.get_max_threads]() do\n          var idx = i * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [symbol] = [std.quote_binary_op(op, symbol, `([arg].[field_name][idx]))]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nreturn omp\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/language/examples/manual_capi_tasklib.rg": "-- Copyright 2018 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\n-- This file is not meant to be run directly.\n\n-- runs-with:\n-- []\n\nlocal tasklib = {}\n\nterralib.linklibrary(\"libregent.so\")\n\nlocal c = terralib.includecstring([[\n#include \"legion.h\"\n#include \"legion_terra.h\"\n#include <stdio.h>\n#include <stdlib.h>\n]])\ntasklib.c = c\n\nlocal dlfcn = terralib.includec(\"dlfcn.h\")\nlocal terra legion_has_llvm_support() : bool\n  return (dlfcn.dlsym([&opaque](0), \"legion_runtime_register_task_variant_llvmir\") ~= [&opaque](0))\nend\nlocal use_llvm = legion_has_llvm_support()\n\nlocal function legion_task_wrapper(body)\n  -- look at the return type of the task we're wrapping to emit the right postamble code\n  local ft = body:gettype()\n  local rt = ft.returntype\n  local wrapper = nil\n  if terralib.sizeof(rt) > 0 then\n    wrapper = terra(data : &opaque, datalen : c.size_t, userdata : &opaque, userlen : c.size_t, proc_id : c.legion_proc_id_t)\n      var task : c.legion_task_t,\n          regions : &c.legion_physical_region_t,\n          num_regions : uint32,\n          ctx : c.legion_context_t,\n          runtime : c.legion_runtime_t\n      c.legion_task_preamble(data, datalen, proc_id, &task, &regions, &num_regions, &ctx, &runtime)\n      var rv : rt = body(task, regions, num_regions, ctx, runtime)\n      c.legion_task_postamble(runtime, ctx, [&opaque](&rv), terralib.sizeof(rt))\n    end\n  else\n    wrapper = terra(data : &opaque, datalen : c.size_t, userdata : &opaque, userlen : c.size_t, proc_id : c.legion_proc_id_t)\n      var task : c.legion_task_t,\n          regions : &c.legion_physical_region_t,\n          num_regions : uint32,\n          ctx : c.legion_context_t,\n          runtime : c.legion_runtime_t\n      c.legion_task_preamble(data, datalen, proc_id, &task, &regions, &num_regions, &ctx, &runtime)\n      body(task, regions, num_regions, ctx, runtime)\n      c.legion_task_postamble(runtime, ctx, [&opaque](0), 0)\n    end\n  end\n  return wrapper\nend\n\nfunction tasklib.preregister_task(terrafunc)\n  -- either way, we wrap the body with legion preamble and postamble first\n  local wrapped = legion_task_wrapper(terrafunc)\n  if use_llvm then\n    -- if we can register llvmir, ask Terra to generate that\n    local ir = terralib.saveobj(nil, \"llvmir\", { entry=wrapped } )\n    local rfunc = terra(id : c.legion_task_id_t,\n                        task_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_preregister_task_variant_llvmir(\n        id, task_name,\n        execution_constraints, layout_constraints, options,\n        ir, \"entry\", userdata, userlen)\n    end\n    return rfunc\n  else\n    -- use the terra function directly, which ffi will convert to a (non-portable) function pointer\n    local rfunc = terra(id : c.legion_task_id_t,\n                        task_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_preregister_task_variant_fnptr(\n        id, task_name,\n        execution_constraints, layout_constraints, options,\n        wrapped, userdata, userlen)\n    end\n    return rfunc\n  end\nend\n\nfunction tasklib.register_task(terrafunc)\n  -- either way, we wrap the body with legion preamble and postamble first\n  local wrapped = legion_task_wrapper(terrafunc)\n  if use_llvm then\n    -- if we can register llvmir, ask Terra to generate that\n    local ir = terralib.saveobj(nil, \"llvmir\", { entry=wrapped } )\n    local rfunc = terra(runtime : c.legion_runtime_t,\n                        id : c.legion_task_id_t,\n                        task_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_register_task_variant_llvmir(\n        runtime, id, task_name,\n        true, -- global registration possible with llvmir\n        execution_constraints, layout_constraints, options,\n        ir, \"entry\", userdata, userlen)\n    end\n    return rfunc\n  else\n    -- use the terra function directly, which ffi will convert to a (non-portable) function pointer\n    local rfunc = terra(runtime : c.legion_runtime_t,\n                        id : c.legion_task_id_t,\n                        task_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_register_task_variant_fnptr(\n        runtime, id, task_name,\n        false, -- global registration not possible with non-portable pointer\n        execution_constraints, layout_constraints, options,\n        wrapped, userdata, userlen)\n    end\n    return rfunc\n  end\nend\n\nif use_llvm then\n  print(\"LLVM support detected...  tasks will be registered as LLVM IR\")\nelse\n  print(\"LLVM support NOT detected...  tasks will be registered as function pointers\")\nend\n\nreturn tasklib\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/language/examples/mssp/small/edges.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/language/examples/mssp/small/result_3.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/doc/arch/persistent/hdf5/figs/high-level-design.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-18.05.0-2ixox4vvbgp6ojsffh2tq3w5zod3v42s/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout.pptx"
    ],
    "total_files": 1549
}