{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/runtime/realm/codedesc.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// constructs for describing code blobs to Realm\n\n#include \"realm/codedesc.h\"\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n#include \"realm/logging.h\"\n#include \"realm/utils.h\"\n\nnamespace Realm {\n\n  Logger log_codetrans(\"codetrans\");\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Type\n\n  std::ostream& operator<<(std::ostream& os, const Type& t)\n  {\n    switch(t.f_common.kind) {\n    case Type::InvalidKind: os << \"INVALIDTYPE\"; break;\n    case Type::OpaqueKind:\n      {\n\tif(t.size_bits() == 0)\n\t  os << \"void\";\n\telse\n\t  os << \"opaque(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::IntegerKind:\n      {\n\tos << (t.f_integer.is_signed ? 's' : 'u') << \"int(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::FloatingPointKind: os << \"float(\" << t.size_bits() << \")\"; break;\n    case Type::PointerKind:\n      {\n\tos << *t.f_pointer.base_type;\n\tif(t.f_pointer.is_const) os << \" const\";\n\tos << \" *\";\n\tbreak;\n      }\n    case Type::FunctionPointerKind:\n      {\n\tos << *t.f_funcptr.return_type << \"(*)(\";\n\tconst std::vector<Type>& p = *t.f_funcptr.param_types;\n\tif(p.size()) {\n\t  for(size_t i = 0; i < p.size(); i++) {\n\t    if(i) os << \", \";\n\t    os << p[i];\n\t  }\n\t} else\n\t  os << \"void\";\n\tos << \")\";\n\tbreak;\n      }\n    }\n    return os;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeDescriptor\n\n  CodeDescriptor::CodeDescriptor(void)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const Type& _t)\n    : m_type(_t)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const CodeDescriptor& rhs)\n  {\n    copy_from(rhs);\n  }\n\n  CodeDescriptor& CodeDescriptor::operator=(const CodeDescriptor& rhs)\n  {\n    if(this != &rhs) {\n      clear();\n      copy_from(rhs);\n    }\n    return *this;\n  }\n\n  CodeDescriptor::~CodeDescriptor(void)\n  {\n    clear();\n  }\n\n  void CodeDescriptor::clear(void)\n  {\n    m_type = Type();\n    delete_container_contents(m_impls);\n    delete_container_contents(m_props);\n  }\n\n  void CodeDescriptor::copy_from(const CodeDescriptor& rhs)\n  {\n    m_type = rhs.m_type;\n    {\n      size_t s = rhs.m_impls.size();\n      m_impls.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_impls[i] = rhs.m_impls[i]->clone();\n    }\n    {\n      size_t s = rhs.m_props.size();\n      m_props.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_props[i] = rhs.m_props[i]->clone();\n    }\n  }\n\n  // are any of the code implementations marked as \"portable\" (i.e.\n  //  usable in another process/address space)?\n  bool CodeDescriptor::has_portable_implementations(void) const\n  {\n    for(std::vector<CodeImplementation *>::const_iterator it = m_impls.begin();\n\tit != m_impls.end();\n\tit++)\n      if((*it)->is_portable())\n\treturn true;\n    return false;\n  }\n\n  // attempt to make a portable implementation from what we have\n  bool CodeDescriptor::create_portable_implementation(void)\n  {\n    // TODO: actually have translators registered where we can find them\n#if defined(REALM_USE_DLFCN) && defined(REALM_USE_DLADDR)\n    const FunctionPointerImplementation *fpi = find_impl<FunctionPointerImplementation>();\n    if(fpi) {\n      DSOReferenceImplementation *dsoref = DSOReferenceImplementation::cvt_fnptr_to_dsoref(fpi, true /*quiet*/);\n      if(dsoref) {\n\tm_impls.push_back(dsoref);\n\treturn true;\n      }\n    }\n#endif\n\n    return false;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class FunctionPointerImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      FunctionPointerImplementation> FunctionPointerImplementation::serdez_subclass;\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void)\n    : fnptr(0)\n  {}\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void (*_fnptr)())\n    : fnptr(_fnptr)\n  {}\n\n  FunctionPointerImplementation::~FunctionPointerImplementation(void)\n  {}\n\n  CodeImplementation *FunctionPointerImplementation::clone(void) const\n  {\n    return new FunctionPointerImplementation(fnptr);\n  }\n\n  bool FunctionPointerImplementation::is_portable(void) const\n  {\n    return false;\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOReferenceImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      DSOReferenceImplementation> DSOReferenceImplementation::serdez_subclass;\n\n  DSOReferenceImplementation::DSOReferenceImplementation(void)\n  {}\n\n  DSOReferenceImplementation::DSOReferenceImplementation(const std::string& _dso_name,\n\t\t\t\t\t\t\t const std::string& _symbol_name)\n    : dso_name(_dso_name), symbol_name(_symbol_name)\n  {}\n\n  DSOReferenceImplementation::~DSOReferenceImplementation(void)\n  {}\n\n  CodeImplementation *DSOReferenceImplementation::clone(void) const\n  {\n    return new DSOReferenceImplementation(dso_name, symbol_name);\n  }\n\n  bool DSOReferenceImplementation::is_portable(void) const\n  {\n    return true;\n  }\n\n#ifdef REALM_USE_DLADDR\n  namespace {\n    // pgcc doesn't let us declare a weak 'main'\n#ifndef __PGI\n    extern \"C\" { int main(int argc, const char *argv[]) __attribute__((weak)); };\n#endif\n\n    DSOReferenceImplementation *dladdr_helper(void *ptr, bool quiet)\n    {\n      // if dladdr() gives us something with the same base pointer, assume that's portable\n      // note: return code is not-POSIX-y (i.e. 0 == failure)\n      Dl_info inf;\n      int ret = dladdr(ptr, &inf);\n      if(ret == 0) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"couldn't map fnptr \" << ptr << \" to a dynamic symbol\";\n\treturn 0;\n      }\n\n      if(inf.dli_saddr != ptr) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"pointer \" << ptr << \" in middle of symbol '\" << inf.dli_sname << \" (\" << inf.dli_saddr << \")?\";\n\treturn 0;\n      }\n\n      // try to detect symbols that are in the base executable and change the filename to \"\"\n      // only do this if the weak 'main' reference found an actual main\n      const char *fname = inf.dli_fname;\n#ifndef __PGI\n      if(((void *)main) != 0) {\n\tstatic std::string local_fname;\n\tif(local_fname.empty()) {\n\t  Dl_info inf2;\n\t  ret = dladdr((void *)main, &inf2);\n\t  assert(ret != 0);\n\t  local_fname = inf2.dli_fname;\n\t}\n\tif(local_fname.compare(fname) == 0)\n\t  fname = \"\";\n      }\n#endif\n      return new DSOReferenceImplementation(fname, inf.dli_sname);\n    }\n  };\n\n  /*static*/ DSOReferenceImplementation *DSOReferenceImplementation::cvt_fnptr_to_dsoref(const FunctionPointerImplementation *fpi,\n\t\t\t\t\t\t\t\t\t\t\t bool quiet /*= false*/)\n  {\n    return dladdr_helper((void *)(fpi->fnptr), quiet);\n  } \n#endif\n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeTranslator\n\n  CodeTranslator::CodeTranslator(const std::string& _name)\n    : name(_name)\n  {}\n\n  CodeTranslator::~CodeTranslator(void)\n  {}\n\n  // default version just iterates over all the implementations in the source\n  bool CodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t     const std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn true;\n    }\n\n    return false;\n  }\n\n  // default version just iterates over all the implementations in the source\n  CodeImplementation *CodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn translate(*it, target_impl_type);\n    }\n\n    return 0;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOCodeTranslator\n\n#ifdef REALM_USE_DLFCN\n  DSOCodeTranslator::DSOCodeTranslator(void)\n    : CodeTranslator(\"dso\")\n  {}\n\n  DSOCodeTranslator::~DSOCodeTranslator(void)\n  {\n    // unload any modules we have loaded\n    for(std::map<std::string, void *>::iterator it = modules_loaded.begin();\n\tit != modules_loaded.end();\n\tit++) {\n      int ret = dlclose(it->second);\n      if(ret != 0)\n\tlog_codetrans.warning() << \"error on dlclose of '\" << it->first << \"': \" << dlerror();\n    }\n  }\n\n  bool DSOCodeTranslator::can_translate(const std::type_info& source_impl_type,\n\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    // DSO ref -> function pointer\n    if((source_impl_type == typeid(DSOReferenceImplementation)) &&\n       (target_impl_type == typeid(FunctionPointerImplementation)))\n      return true;\n\n#ifdef REALM_USE_DLADDR\n    if((source_impl_type == typeid(FunctionPointerImplementation)) &&\n       (target_impl_type == typeid(DSOReferenceImplementation)))\n      return true;\n#endif\n\n      return false;\n    }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeImplementation *source,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    if(target_impl_type == typeid(FunctionPointerImplementation)) {\n      const DSOReferenceImplementation *dsoref = dynamic_cast<const DSOReferenceImplementation *>(source);\n      assert(dsoref != 0);\n\n      void *handle = 0;\n      // check to see if we've already loaded the module?\n      std::map<std::string, void *>::iterator it = modules_loaded.find(dsoref->dso_name);\n      if(it != modules_loaded.end()) {\n\thandle = it->second;\n      } else {\n\t// try to load it - empty string for dso_name means the main executable\n\tconst char *dso_name = dsoref->dso_name.c_str();\n\thandle = dlopen(*dso_name ? dso_name : 0, RTLD_NOW | RTLD_LOCAL);\n\tif(!handle) {\n\t  log_codetrans.warning() << \"could not open DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\t  return 0;\n\t}\n\tmodules_loaded[dsoref->dso_name] = handle;\n      }\n\n      void *ptr = dlsym(handle, dsoref->symbol_name.c_str());\n      if(!ptr) {\n\tlog_codetrans.warning() << \"could not find symbol '\" << dsoref->symbol_name << \"' in  DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\treturn 0;\n      }\n\n      return new FunctionPointerImplementation((void(*)())ptr);\n    }\n\n#ifdef REALM_USE_DLADDR\n    if(target_impl_type == typeid(DSOReferenceImplementation)) {\n      const FunctionPointerImplementation *fpi = dynamic_cast<const FunctionPointerImplementation *>(source);\n      assert(fpi != 0);\n\n      return dladdr_helper((void *)(fpi->fnptr), false /*!quiet*/);\n    }\n#endif\n\n    return 0;\n  }\n\n  // these pass through to CodeTranslator's definitions\n  bool DSOCodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    return CodeTranslator::can_translate(source_codedesc, target_impl_type);\n  }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    return CodeTranslator::translate(source_codedesc, target_impl_type);\n  }\n#endif\n\n\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/runtime/realm/threads.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// generic Realm interface to threading libraries (e.g. pthreads)\n\n#include \"realm/threads.h\"\n\n#include \"realm/logging.h\"\n#include \"realm/faults.h\"\n#include \"realm/operation.h\"\n\n#ifdef DEBUG_USWITCH\n#include <stdio.h>\n#endif\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n#define REALM_USE_PTHREADS\n#define REALM_USE_ALTSTACK\n#include <pthread.h>\n#endif\n#ifdef REALM_ON_LINUX\n  #define HAVE_CPUSET\n#endif\n#ifdef REALM_ON_FREEBSD\n  #include <pthread_np.h>\n  typedef cpuset_t cpu_set_t;\n  #define HAVE_CPUSET\n#endif\n#include <errno.h>\n// for PTHREAD_STACK_MIN\n#include <limits.h>\n#ifdef REALM_ON_MACOS\n// for sched_yield\n#include <sched.h>\n#endif\n\n#ifdef REALM_ON_WINDOWS\n#include <windows.h>\n#include <processthreadsapi.h>\n#include <process.h>\n\n// Windows API uses DWORD_PTR for affinity masks\n#define HAVE_CPUSET\ntypedef DWORD_PTR cpu_set_t;\nstatic void CPU_ZERO(DWORD_PTR *set)\n{\n  *set = 0;\n}\nstatic void CPU_SET(int index, DWORD_PTR *set)\n{\n  *set |= DWORD_PTR(1) << index;\n}\n#endif\n\n#ifdef REALM_USE_USER_THREADS\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n#include <ucontext.h>\n\n#ifdef REALM_ON_MACOS\n// MacOS has (loudly) deprecated set/get/make/swapcontext,\n//  despite there being no POSIX replacement for them...\n// this check is on the use, not the declaration, so we wrap them here\n\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wdeprecated\"\ninline int getcontext_wrap(ucontext_t *u) { return getcontext(u); }\ninline int swapcontext_wrap(ucontext_t *u1, const ucontext_t *u2) { return swapcontext(u1, u2); }\ninline void makecontext_wrap(ucontext_t *u, void (*fn)(), int args, ...) { makecontext(u, fn, 0); }\n#pragma GCC diagnostic pop\n#define getcontext getcontext_wrap\n#define swapcontext swapcontext_wrap\n#define makecontext makecontext_wrap\n#endif\n#endif\n#endif\n\n#ifdef REALM_USE_HWLOC\n#include <hwloc.h>\n#endif\n\n#ifdef REALM_USE_LIBDL\n#include <dlfcn.h>\n#endif\n\n#include <string.h>\n#include <stdlib.h>\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n#include <unistd.h>\n#include <signal.h>\n#endif\n#include <string>\n#include <map>\n\n#ifdef REALM_ON_LINUX\n// needed for scanning Linux's /sys\n#include <dirent.h>\n#include <stdio.h>\n#ifdef REALM_USE_HWLOC\n#include <hwloc/linux.h>\n#endif\n#endif\n\n#ifndef CHECK_LIBC\n#define CHECK_LIBC(cmd) do { \\\n  errno = 0; \\\n  int ret = (cmd); \\\n  if(ret != 0) { \\\n    std::cerr << \"ERROR: \" __FILE__ \":\" << __LINE__ << \": \" #cmd \" = \" << ret << \" (\" << strerror(errno) << \")\" << std::endl;\t\\\n    assert(0); \\\n  } \\\n} while(0)\n#endif\n\n#ifndef CHECK_PTHREAD\n#define CHECK_PTHREAD(cmd) do { \\\n  int ret = (cmd); \\\n  if(ret != 0) { \\\n    std::cerr << \"PTHREAD: \" #cmd \" = \" << ret << \" (\" << strerror(ret) << \")\" << std::endl;\t\\\n    assert(0); \\\n  } \\\n} while(0)\n#endif\n\nnamespace Realm {\n\n  Logger log_thread(\"threads\");\n\n#ifdef REALM_USE_PAPI\n  Logger log_papi(\"papi\");\n  namespace PAPI {\n    bool papi_available = false;\n  };\n#endif\n\n  namespace ThreadLocal {\n    /*extern*/ REALM_THREAD_LOCAL Thread *current_thread = 0;\n  };\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CoreReservation\n\n  // we keep a global map of reservations to their allocations (this is inherently a global problem)\n  std::map<CoreReservation *, CoreReservation::Allocation *> allocations;\n\n  CoreReservation::CoreReservation(const std::string& _name, CoreReservationSet &crs,\n\t\t\t\t   const CoreReservationParameters& _params)\n    : name(_name), params(_params), allocation(0)\n  {\n    // reservations automatically add themselves to the set\n    crs.add_reservation(*this);\n\n    log_thread.info() << \"reservation created: \" << name;\n  }\n\n  // with pthreads on Linux, an allocation is the cpu_set used for affinity\n  struct CoreReservation::Allocation {\n    bool exclusive_ownership;\n    std::set<int> proc_ids;\n#ifdef HAVE_CPUSET\n    bool restrict_cpus;  // if true, thread is confined to set below\n    cpu_set_t allowed_cpus;\n#endif\n  };\n\n  void CoreReservation::add_listener(NotificationListener *listener)\n  {\n    if(allocation) {\n      // already have allocation - just call back immediately\n      listener->notify_allocation(*this);\n    } else {\n      listeners.push_back(listener);\n    }\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CoreReservationSet\n\n  CoreReservationSet::CoreReservationSet(bool hyperthread_sharing)\n    : owns_coremap(true), cm(0)\n  {\n    cm = CoreMap::discover_core_map(hyperthread_sharing);\n  }\n\n  CoreReservationSet::CoreReservationSet(const CoreMap *_cm)\n    : owns_coremap(false), cm(_cm)\n  {\n  }\n\n  CoreReservationSet::~CoreReservationSet(void)\n  {\n    if(owns_coremap)\n      delete const_cast<CoreMap *>(cm);\n    \n    // we don't own the CoreReservation *'s in the allocation map, but we do own the \n    //  allocations\n    for(std::map<CoreReservation *, CoreReservation::Allocation *>::iterator it = allocations.begin();\n\tit != allocations.end();\n\tit++)\n      delete it->second;\n    allocations.clear();\n  }\n\n  const CoreMap *CoreReservationSet::get_core_map(void) const\n  {\n    return cm;\n  }\n\n  void CoreReservationSet::add_reservation(CoreReservation& rsrv)\n  {\n    assert(allocations.count(&rsrv) == 0);\n    allocations[&rsrv] = 0;\n  }\n\n  static bool can_add_usage(CoreReservationParameters::CoreUsage current,\n\t\t\t    CoreReservationParameters::CoreUsage reqd)\n  {\n    switch(current) {\n    case CoreReservationParameters::CORE_USAGE_EXCLUSIVE:\n      {\n\t// exclusive cannot coexist with exclusive or shared\n\tif(reqd == CoreReservationParameters::CORE_USAGE_EXCLUSIVE) return false;\n\tif(reqd == CoreReservationParameters::CORE_USAGE_SHARED) return false;\n\treturn true;\n      }\n\n    case CoreReservationParameters::CORE_USAGE_SHARED:\n      {\n\t// shared cannot coexist with exclusive\n\tif(reqd == CoreReservationParameters::CORE_USAGE_EXCLUSIVE) return false;\n\treturn true;\n      }\n\n    default:\n      {\n\t// NONE and MINIMAL are fine\n\treturn true;\n      }\n    }\n  }\n\n  static void add_usage(CoreReservationParameters::CoreUsage& current,\n\t\t\tCoreReservationParameters::CoreUsage reqd)\n  {\n    // this ends up being a simple max\n    if(reqd > current)\n      current = reqd;\n  }\t\t\n\n  // versions of the above that understand shared cores\n  static bool can_add_usage(const std::map<const CoreMap::Proc *,\n\t\t\t                   CoreReservationParameters::CoreUsage>& current,\n\t\t\t    CoreReservationParameters::CoreUsage reqd,\n\t\t\t    const CoreMap::Proc *p,\n\t\t\t    const std::set<CoreMap::Proc *>& shared)\n  {\n    std::map<const CoreMap::Proc *, CoreReservationParameters::CoreUsage>::const_iterator it;\n    it = current.find(p);\n    if((it != current.end()) && !can_add_usage(it->second, reqd)) return false;\n\n    for(std::set<CoreMap::Proc *>::const_iterator it2 = shared.begin();\n\tit2 != shared.end();\n\tit2++) {\n      it = current.find(*it2);\n      if((it != current.end()) && !can_add_usage(it->second, reqd)) return false;\n    }\n\n    return true;\n  }\n\n  static void add_usage(std::map<const CoreMap::Proc *,\n\t\t\t         CoreReservationParameters::CoreUsage>& current,\n\t\t\tCoreReservationParameters::CoreUsage reqd,\n\t\t\tconst CoreMap::Proc *p,\n\t\t\tconst std::set<CoreMap::Proc *>& shared)\n  {\n    std::map<const CoreMap::Proc *, CoreReservationParameters::CoreUsage>::iterator it;\n    it = current.find(p);\n    if(it != current.end())\n      add_usage(it->second, reqd);\n    else\n      current.insert(std::make_pair(p, reqd));\n\n    for(std::set<CoreMap::Proc *>::const_iterator it2 = shared.begin();\n\tit2 != shared.end();\n\tit2++) {\n      it = current.find(*it2);\n      if(it != current.end())\n\tadd_usage(it->second, reqd);\n      else\n\tcurrent.insert(std::make_pair(*it2, reqd));\n    }\n  }\n\n  // attempts to find an allocation satisfying all the reservation requests in 'allocs' -\n  //  if any allocations are already present, those are preserved (possibly causing the\n  //  allocation attempt to fail)\n  static bool attempt_allocation(const CoreMap& cm,\n\t\t\t\t std::map<CoreReservation *, CoreReservation::Allocation *>& allocs)\n  {\n    // we'll need to keep track of the usage level of each core\n    std::map<const CoreMap::Proc *, CoreReservationParameters::CoreUsage> alu_usage, fpu_usage, ldst_usage;\n    std::map<const CoreMap::Proc *, int> user_count;\n\n    // iterate through the requests and sort them by whether or not they have any exclusivity\n    //  demands and whether they're limited to a particular numa domain\n    // also record pre-allocated reservations and their usage\n    std::map<std::pair<bool, int>, std::set<CoreReservation *> > to_satisfy;\n    for(std::map<CoreReservation *, CoreReservation::Allocation *>::iterator it = allocs.begin();\n\tit != allocs.end();\n\tit++) {\n      CoreReservation *rsrv = it->first;\n      CoreReservation::Allocation *alloc = it->second;\n      if(alloc) {\n\tfor(std::set<int>::const_iterator it = alloc->proc_ids.begin();\n\t    it != alloc->proc_ids.end();\n\t    it++) {\n\t  // get the corresponding CoreMap::Proc\n\t  CoreMap::ProcMap::const_iterator it2 = cm.all_procs.find(*it);\n\t  if(it2 == cm.all_procs.end()) {\n\t    log_thread.error() << \"existing allocation ('\" << rsrv->name << \"') has an unknown proc id (\" << *it << \")\";\n\t    return false; // no way to fix this\n\t  }\n\t  const CoreMap::Proc *p = it2->second;\n\n\t  // update/check user_count\n\t  if(alloc->exclusive_ownership && (user_count.count(p) > 0)) {\n\t    log_thread.error() << \"existing allocation ('\" << rsrv->name << \"') has unsatisfiable exclusivity on proc id (\" << p->id << \")\";\n\t    return false; // no way to fix this\n\t  }\n\t  user_count[p]++;\n\n\t  // update/check usage\n\t  if(!(can_add_usage(alu_usage, rsrv->params.alu_usage, p, p->shares_alu) &&\n\t       can_add_usage(fpu_usage, rsrv->params.fpu_usage, p, p->shares_fpu) &&\n\t       can_add_usage(ldst_usage, rsrv->params.ldst_usage, p, p->shares_ldst))) {\n\t    log_thread.error() << \"existing allocation ('\" << rsrv->name << \"') has unsatisfiable usage on proc id (\" << p->id << \")\";\n\t    return false; // no way to fix this\n\t  }\n\t  add_usage(alu_usage, rsrv->params.alu_usage, p, p->shares_alu);\n\t  add_usage(fpu_usage, rsrv->params.fpu_usage, p, p->shares_fpu);\n\t  add_usage(ldst_usage, rsrv->params.ldst_usage, p, p->shares_ldst);\n\t}\n      } else {\n\tstd::pair<bool, int> key = std::make_pair(((rsrv->params.alu_usage == CoreReservationParameters::CORE_USAGE_EXCLUSIVE) ||\n\t\t\t\t\t\t   (rsrv->params.fpu_usage == CoreReservationParameters::CORE_USAGE_EXCLUSIVE) ||\n\t\t\t\t\t\t   (rsrv->params.ldst_usage == CoreReservationParameters::CORE_USAGE_EXCLUSIVE)),\n\t\t\t\t\t\t  rsrv->params.numa_domain);\n\tto_satisfy[key].insert(rsrv);\n      }\n    }\n\n    // ok, now attempt to satisfy all the requests\n    // by _reverse_ iterating over to_satisfy, we consider exclusive requests before shared\n    //  and those that want a particular numa domain before those that don't care\n    std::map<CoreReservation *, std::set<const CoreMap::Proc *> > assigned_procs;\n    for(std::map<std::pair<bool, int>, std::set<CoreReservation *> >::reverse_iterator it = to_satisfy.rbegin();\n\tit != to_satisfy.rend();\n\tit++) {\n      bool has_exclusive = it->first.first;\n      int req_domain = it->first.second;\n\n      std::vector<const CoreMap::Proc *> pm;\n      if(req_domain >= 0) {\n\tCoreMap::DomainMap::const_iterator it2 = cm.by_domain.find(req_domain);\n\tif(it2 == cm.by_domain.end()) {\n\t  log_thread.error() << \"one or more reservations requiring unknown domain (\" << req_domain << \")\";\n\t  return false;\n\t}\n\tfor(CoreMap::ProcMap::const_iterator it3 = it2->second.begin(); it3 != it2->second.end(); it3++)\n\t  pm.push_back(it3->second);\n      } else {\n\t// shuffle the procs from the different domains to get a roughly-even distribution\n\tstd::list<std::pair<const CoreMap::ProcMap *, CoreMap::ProcMap::const_iterator> > rr;\n\tfor(CoreMap::DomainMap::const_iterator it2 = cm.by_domain.begin();\n\t    it2 != cm.by_domain.end();\n\t    it2++)\n\t  if(!(it2->second.empty()))\n\t    rr.push_back(std::make_pair(&(it2->second), it2->second.begin()));\n\twhile(!(rr.empty())) {\n\t  std::pair<const CoreMap::ProcMap *, CoreMap::ProcMap::const_iterator> x = rr.front();\n\t  rr.pop_front();\n\t  pm.push_back(x.second->second);\n\t  if(++x.second != x.first->end())\n\t    rr.push_back(x);\n\t}\n      }\n\n      for(std::set<CoreReservation *>::iterator it2 = it->second.begin();\n\t  it2 != it->second.end();\n\t  it2++) {\n\tCoreReservation *rsrv = *it2;\n\tstd::set<const CoreMap::Proc *>& procs = assigned_procs[rsrv];\n\n\t// iterate over all the possibly available processors and see if any fit\n\tfor(std::vector<const CoreMap::Proc *>::iterator it3 = pm.begin();\n\t    it3 != pm.end();\n\t    it3++)\n\t{\n\t  const CoreMap::Proc *p = *it3;\n\n\t  // is there already conflicting usage?\n\t  if(!(can_add_usage(alu_usage, rsrv->params.alu_usage, p, p->shares_alu) &&\n\t       can_add_usage(fpu_usage, rsrv->params.fpu_usage, p, p->shares_fpu) &&\n\t       can_add_usage(ldst_usage, rsrv->params.ldst_usage, p, p->shares_ldst)))\n\t    continue;\n\n\t  // yes, do so and add this to the assigned procs\n\t  add_usage(alu_usage, rsrv->params.alu_usage, p, p->shares_alu);\n\t  add_usage(fpu_usage, rsrv->params.fpu_usage, p, p->shares_fpu);\n\t  add_usage(ldst_usage, rsrv->params.ldst_usage, p, p->shares_ldst);\n\t  procs.insert(p);\n\n\t  // an exclusive reservation request stops as soon as we have enough, while\n\t  //  a shared reservation will use any/all compatible processors\n\t  if(has_exclusive && ((int)(procs.size()) >= rsrv->params.num_cores))\n\t    break;\n\t}\n\n\t// if we didn't get enough, we've failed this allocation\n\tif((int)(procs.size()) < rsrv->params.num_cores) {\n\t  log_thread.warning() << \"reservation ('\" << rsrv->name << \"') cannot be satisfied\";\n\t  return false;\n\t}\n      }\n    }\n\n    // if we got all the way through, we're successful and can now fill in the new allocations\n    for(std::map<CoreReservation *, std::set<const CoreMap::Proc *> >::iterator it = assigned_procs.begin();\n\tit != assigned_procs.end();\n\tit++) {\n      CoreReservation *rsrv = it->first;\n      CoreReservation::Allocation *alloc = new CoreReservation::Allocation;\n\n      alloc->exclusive_ownership = true;  // unless we set it false below\n#ifdef HAVE_CPUSET\n      alloc->restrict_cpus = false; // unless we set it to true below\n      CPU_ZERO(&alloc->allowed_cpus);\n#endif\n\n      for(std::set<const CoreMap::Proc *>::iterator it2 = it->second.begin();\n\t  it2 != it->second.end();\n\t  it2++) {\n\tconst CoreMap::Proc *p = *it2;\n\n\talloc->proc_ids.insert(p->id);\n\tif(user_count[p] > 1)\n\t  alloc->exclusive_ownership = false;\n#ifdef HAVE_CPUSET\n\tif(!(p->kernel_proc_ids.empty())) {\n\t  alloc->restrict_cpus = true;\n\t  for(std::set<int>::const_iterator it3 = p->kernel_proc_ids.begin();\n\t      it3 != p->kernel_proc_ids.end();\n\t      it3++)\n\t    CPU_SET(*it3, &alloc->allowed_cpus);\n\t}\n#endif\n      }\n\n      if(rsrv->allocation) {\n\tlog_thread.info() << \"replacing allocation for reservation '\" << rsrv->name << \"'\";\n\tCoreReservation::Allocation *old_alloc = rsrv->allocation;\n\trsrv->allocation = alloc;\n\tdelete old_alloc; // TODO: reference count once we allow updates\n      } else\n\trsrv->allocation = alloc;\n\n      allocs[rsrv] = alloc;\n    }\n\n    return true;\n  }\n\n  bool CoreReservationSet::satisfy_reservations(bool dummy_reservation_ok /*= false*/)\n  {\n    // remember who is missing an allocation - we'll need to notify them\n    std::set<CoreReservation *> missing;\n    for(std::map<CoreReservation *, CoreReservation::Allocation *>::iterator it = allocations.begin();\n\tit != allocations.end();\n\tit++)\n      if(!it->second)\n\tmissing.insert(it->first);\n\n    // one shot for now - eventually allow a reservation to say it's willing to be\n    //  adjusted if needed\n    bool ok = attempt_allocation(*cm,\n\t\t\t\t allocations);\n    if(!ok) {\n      if(!dummy_reservation_ok)\n\treturn false;\n\n      // dummy allocations for everyone!\n      for(std::set<CoreReservation *>::iterator it = missing.begin();\n\t  it != missing.end();\n\t  it++) {\n\tCoreReservation *rsrv = *it;\n\n\tCoreReservation::Allocation *alloc = new CoreReservation::Allocation;\n\n\talloc->exclusive_ownership = true;  // unless we set it false below\n#ifdef HAVE_CPUSET\n\talloc->restrict_cpus = false; // unless we set it to true below\n\tCPU_ZERO(&alloc->allowed_cpus);\n#endif\n\trsrv->allocation = alloc;\n\tallocations[rsrv] = alloc;\n      }\n    }      \n\n    // for all the reservations that were missing allocations, notify any registered listeners\n    for(std::set<CoreReservation *>::iterator it = missing.begin();\n\tit != missing.end();\n\tit++) {\n      CoreReservation *rsrv = *it;\n      for(std::list<CoreReservation::NotificationListener *>::iterator it = rsrv->listeners.begin();\n\t  it != rsrv->listeners.end();\n\t  it++)\n\t(*it)->notify_allocation(*rsrv);\n    }\n\n    return true;\n  }\n\n  template <typename T>\n  static std::ostream& operator<<(std::ostream &os, const std::set<T>& s)\n  {\n    os << '<';\n    if(!(s.empty())) {\n      typename std::set<T>::const_iterator it = s.begin();\n      while(true) {\n\tos << *it;\n\tif(++it == s.end()) break;\n\tos << ',';\n      }\n    }\n    os << '>';\n    return os;\n  }\n\n  void CoreReservationSet::report_reservations(std::ostream& os) const\n  {\n    // iterate over the allocation map and print stuff out\n    for(std::map<CoreReservation *, CoreReservation::Allocation *>::const_iterator it = allocations.begin();\n\tit != allocations.end();\n\tit++) {\n      const CoreReservation *rsrv = it->first;\n      const CoreReservation::Allocation *alloc = it->second;\n      os << rsrv->name << \": \";\n      if(alloc) {\n\tos << \"allocated \" << alloc->proc_ids;\n      } else {\n\tos << \"not allocated\";\n      }\n      os << std::endl;\n    }\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Thread\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n  static atomic<bool> handler_registered(false);\n  // Valgrind uses SIGUSR2 on Darwin\n  static int handler_signal = SIGUSR1;\n\n  static void signal_handler(int signal, siginfo_t *info, void *context)\n  {\n    if(signal == handler_signal) {\n      // somebody pinged us to look at our signals\n      Thread *t = ThreadLocal::current_thread;\n      assert(t);\n      t->process_signals();\n      return;\n    }\n    \n    Backtrace bt;\n    bt.capture_backtrace();\n    bt.lookup_symbols();\n    log_thread.error() << \"received unexpected signal \" << signal << \" backtrace=\" << bt;\n  }\n\n  static void register_handler(void)\n  {\n    bool expval = false;\n    if(!handler_registered.compare_exchange(expval, true))\n      return;\n\n    struct sigaction act;\n    bzero(&act, sizeof(act));\n    act.sa_sigaction = &signal_handler;\n    act.sa_flags = SA_SIGINFO;\n\n    CHECK_LIBC( sigaction(handler_signal, &act, 0) );\n  }\n#endif\n\n  void Thread::signal(Signal sig, bool asynchronous)\n  {\n    log_thread.info() << \"sending signal: target=\" << (void *)this << \" signal=\" << sig << \" async=\" << asynchronous;\n    {\n      AutoLock<> a(signal_mutex);\n      signal_queue.push_back(sig);\n    }\n    int prev = signal_count.fetch_add(1);\n    if((prev == 0) && asynchronous)\n      alert_thread();\n  }\n\n  Thread::Signal Thread::pop_signal(void)\n  {\n    if(signal_count.load() > 0) {\n      Signal sig;\n      AutoLock<> a(signal_mutex);\n      signal_count.fetch_sub(1);\n      sig = signal_queue.front();\n      signal_queue.pop_front();\n      return sig;\n    } else\n      return TSIG_NONE;\n  }\n\n  void Thread::process_signals(void)\n  {\n    // should only be called from the thread itself\n    assert(this == Thread::self());\n\n    while(signal_count.load() > 0) {\n      Signal sig;\n      {\n\tsignal_count.fetch_sub(1);\n\tAutoLock<> a(signal_mutex);\n\t// should never be empty, as there's no race conditions on emptying the queue\n\tassert(!signal_queue.empty());\n\tsig = signal_queue.front();\n\tsignal_queue.pop_front();\n      }\n\n      switch(sig) {\n      case Thread::TSIG_INTERRUPT: \n\t{\n\t  Operation *op = current_op;\n\t  if(op && op->cancellation_requested()) {\n#ifdef REALM_USE_EXCEPTIONS\n\t    if(exceptions_permitted()) {\n  \t      throw CancellationException();\n\t    } else\n#endif\n\t    {\n\t      log_thread.fatal() << \"no handler for TSIG_INTERRUPT: thread=\" << this << \" op=\" << op;\n\t      assert(0);\n\t    }\n\t  } else\n\t    log_thread.warning() << \"unwanted TSIG_INTERRUPT: thread=\" << this << \" op=\" << op;\n\t  break;\n\t}\n      default: \n\t{\n\t  assert(0);\n\t}\n      }\n    }\n  }\n\n  // changes the priority of the thread (and, by extension, the operation it\n  //   is working on)\n  void Thread::set_priority(int new_priority)\n  {\n    assert(scheduler != 0);\n    scheduler->set_thread_priority(this, new_priority);\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class KernelThread\n\n  class KernelThread : public Thread {\n  public:\n    KernelThread(void *_target, void (*_entry_wrapper)(void *),\n\t\t ThreadScheduler *_scheduler);\n\n    virtual ~KernelThread(void);\n\n    void start_thread(const ThreadLaunchParameters& params,\n\t\t      const CoreReservation& rsrv);\n\n    virtual void join(void);\n    virtual void detach(void);\n\n    static void detect_static_tls_size(void);\n\n  protected:\n#ifdef REALM_USE_PTHREADS\n    static void *pthread_entry(void *data);\n#endif\n#ifdef REALM_ON_WINDOWS\n    static DWORD WINAPI winthread_entry(LPVOID data);\n#endif\n\n    virtual void alert_thread(void);\n\n    void *target;\n    void (*entry_wrapper)(void *);\n#ifdef REALM_USE_PTHREADS\n    pthread_t thread;\n#endif\n#ifdef REALM_ON_WINDOWS\n    HANDLE thread;\n#endif\n    bool ok_to_delete;\n#ifdef REALM_USE_ALTSTACK\n    void *altstack_base;\n    size_t altstack_size;\n#endif\n    static size_t static_tls_size;\n  };\n\n  KernelThread::KernelThread(void *_target, void (*_entry_wrapper)(void *),\n\t\t\t     ThreadScheduler *_scheduler)\n    : Thread(_scheduler), target(_target), entry_wrapper(_entry_wrapper)\n    , ok_to_delete(false)\n  {\n  }\n\n  KernelThread::~KernelThread(void)\n  {\n    // insist that a thread be join()'d or detach()'d before deletion\n    assert(ok_to_delete);\n  }\n\n#ifdef REALM_USE_PTHREADS\n  /*static*/ void *KernelThread::pthread_entry(void *data)\n  {\n    KernelThread *thread = (KernelThread *)data;\n\n#ifdef REALM_USE_ALTSTACK\n    // install our alt stack (if it exists) for signal handling\n    if(thread->altstack_base != 0) {\n      stack_t altstack;\n      altstack.ss_sp = thread->altstack_base;\n      altstack.ss_flags = 0;\n      altstack.ss_size = thread->altstack_size;\n      int ret = sigaltstack(&altstack, 0);\n      assert(ret == 0);\n    }\n#endif\n\n    // set up TLS so people can find us\n    ThreadLocal::current_thread = thread;\n\n    log_thread.info() << \"thread \" << thread << \" started\";\n    thread->update_state(STATE_RUNNING);\n\n    if(thread->scheduler)\n      thread->scheduler->thread_starting(thread);\n    \n    // call the actual thread body\n    (*thread->entry_wrapper)(thread->target);\n\n    // on return, we update our status and terminate\n    log_thread.info() << \"thread \" << thread << \" finished\";\n    thread->update_state(STATE_FINISHED);\n\n#ifdef REALM_USE_ALTSTACK\n    // uninstall and free our alt stack (if it exists)\n    if(thread->altstack_base != 0) {\n      // so MacOS doesn't seem to want to let you disable a stack, returning\n      //  EINVAL even if the stack is not active - free the memory anyway\n#ifndef REALM_ON_MACOS\n      stack_t disabled;\n      disabled.ss_sp = 0;\n      disabled.ss_flags = SS_DISABLE;\n      disabled.ss_size = 0;\n      stack_t oldstack;\n      int ret = sigaltstack(&disabled, &oldstack);\n      assert(ret == 0);\n      // in a perfect world, we'd double-check that it's our stack we\n      //  unloaded, but some libraries (e.g. libpython 3.4) do not clean\n      //  up properly, so our stack may not have been active anyway\n      // either way though, it's not active after the call above, so it's\n      //  safe to free the memory now\n      //assert(oldstack.ss_sp == thread->altstack_base);\n#endif\n      free(thread->altstack_base);\n    }\n#endif\n\n    // this is last so that the scheduler can delete us if it wants to\n    if(thread->scheduler)\n      thread->scheduler->thread_terminating(thread);\n    \n    return 0;\n  }\n#endif\n\n#ifdef REALM_ON_WINDOWS\n  /*static*/ DWORD WINAPI KernelThread::winthread_entry(LPVOID data)\n  {\n    KernelThread *thread = (KernelThread *)data;\n\n    // set up TLS so people can find us\n    ThreadLocal::current_thread = thread;\n\n    log_thread.info() << \"thread \" << thread << \" started\";\n    thread->update_state(STATE_RUNNING);\n\n    if (thread->scheduler)\n      thread->scheduler->thread_starting(thread);\n\n    // call the actual thread body\n    (*thread->entry_wrapper)(thread->target);\n\n    // on return, we update our status and terminate\n    log_thread.info() << \"thread \" << thread << \" finished\";\n    thread->update_state(STATE_FINISHED);\n\n    // this is last so that the scheduler can delete us if it wants to\n    if (thread->scheduler)\n      thread->scheduler->thread_terminating(thread);\n\n    return 0;\n  }\n#endif\n\n  void KernelThread::start_thread(const ThreadLaunchParameters& params,\n\t\t\t\t  const CoreReservation& rsrv)\n  {\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    // before we create any threads, make sure we have our signal handler registered\n    register_handler();\n#endif\n\n#ifdef REALM_USE_PTHREADS\n    pthread_attr_t attr;\n\n    CHECK_PTHREAD( pthread_attr_init(&attr) );\n#endif\n\n    // allocation better exist...\n    assert(rsrv.allocation);\n\n#if defined(HAVE_CPUSET) && !defined(REALM_ON_WINDOWS)\n    if(rsrv.allocation->restrict_cpus)\n      CHECK_PTHREAD( pthread_attr_setaffinity_np(&attr, \n\t\t\t\t\t\t sizeof(rsrv.allocation->allowed_cpus),\n\t\t\t\t\t\t &(rsrv.allocation->allowed_cpus)) );\n#endif\n\n#ifdef REALM_USE_PTHREADS\n    // now that we try to detect the static TLS size, we can use the\n    //  advertised min stack size from the threading library as is\n    const ptrdiff_t MIN_STACK_SIZE = PTHREAD_STACK_MIN;\n\n    ptrdiff_t stack_size = 0;  // 0 == \"pthread default\"\n#endif\n#ifdef REALM_ON_WINDOWS\n    const ptrdiff_t MIN_STACK_SIZE = 0;\n    ptrdiff_t stack_size = 0;   // 0 == \"windows default\"\n#endif\n\n    if(params.stack_size != params.STACK_SIZE_DEFAULT) {\n      // make sure it's not too large\n      assert((rsrv.params.max_stack_size == rsrv.params.STACK_SIZE_DEFAULT) ||\n\t     (params.stack_size <= rsrv.params.max_stack_size));\n\n      stack_size = std::max<ptrdiff_t>(params.stack_size, MIN_STACK_SIZE);\n    } else {\n      // does the entire core reservation have a non-standard stack size?\n      if(rsrv.params.max_stack_size != rsrv.params.STACK_SIZE_DEFAULT) {\n\tstack_size = std::max<ptrdiff_t>(rsrv.params.max_stack_size,\n\t\t\t\t\t MIN_STACK_SIZE);\n      }\n    }\n#ifdef REALM_USE_PTHREADS\n    if(stack_size > 0) {\n      // add in our estimate of the static TLS size\n      CHECK_PTHREAD( pthread_attr_setstacksize(&attr,\n\t\t\t\t\t       (stack_size +\n\t\t\t\t\t\tKernelThread::static_tls_size)) );\n    }\n#endif\n\n    // TODO: actually use heap size\n\n#ifdef REALM_USE_ALTSTACK\n    // default altstack size is 64KB\n    altstack_size = 64 << 10;\n    if(params.alt_stack_size != params.ALTSTACK_SIZE_DEFAULT)\n      altstack_size = params.alt_stack_size;\n    else if(rsrv.params.alt_stack_size != rsrv.params.ALTSTACK_SIZE_DEFAULT)\n      altstack_size = rsrv.params.alt_stack_size;\n\n    if(altstack_size > 0) {\n      int ret = posix_memalign(&altstack_base,\n\t\t\t       sysconf(_SC_PAGESIZE),\n\t\t\t       altstack_size);\n      assert(ret == 0);\n    } else\n      altstack_base = 0;\n#endif\n\n    update_state(STATE_STARTUP);\n\n    // time to actually create the thread\n#ifdef REALM_USE_PTHREADS\n    CHECK_PTHREAD( pthread_create(&thread, &attr, pthread_entry, this) );\n\n    CHECK_PTHREAD( pthread_attr_destroy(&attr) );\n\n    log_thread.info() << \"thread created:\" << this << \" (\" << rsrv.name << \") - pthread \" << std::hex << thread << std::dec;\n#endif\n#ifdef REALM_ON_WINDOWS\n    // TODO: supposed to use _beginthreadex here?\n    thread = CreateThread(NULL,\n\t\t\t  (stack_size +\n\t\t\t   KernelThread::static_tls_size),\n\t\t\t  winthread_entry, this, 0, 0);\n#ifdef HAVE_CPUSET\n    if(rsrv.allocation->restrict_cpus)\n      if(SetThreadAffinityMask(thread, rsrv.allocation->allowed_cpus) == 0)\n        log_thread.warning() << \"failed to set affinity: thread=\" << thread\n                             << \" mask=\" << std::hex << rsrv.allocation->allowed_cpus << std::dec\n                             << \" error=\" << GetLastError();\n#endif\n\n    log_thread.info() << \"thread created:\" << this << \" (\" << rsrv.name << \") - handle \" << thread;\n#endif\n    log_thread.debug() << \"thread stack: \" << this << \" size=\" << stack_size;\n  }\n\n  void KernelThread::join(void)\n  {\n#ifdef REALM_USE_PTHREADS\n    CHECK_PTHREAD( pthread_join(thread, 0 /* ignore retval */) );\n#endif\n#ifdef REALM_ON_WINDOWS\n    WaitForSingleObject(thread, INFINITE);\n#endif\n    ok_to_delete = true;\n  }\n\n  void KernelThread::detach(void)\n  {\n#ifdef REALM_USE_PTHREADS\n    CHECK_PTHREAD( pthread_detach(thread) );\n#endif\n#ifdef REALM_ON_WINDOWS\n    CloseHandle(thread);\n#endif\n    ok_to_delete = true;\n  }\n\n  void KernelThread::alert_thread(void)\n  {\n    // are we alerting ourself?\n#ifdef REALM_USE_PTHREADS\n    if(this->thread == pthread_self()) {\n      // just process the signals right here and now\n      process_signals();\n    } else {\n      pthread_kill(this->thread, handler_signal);\n    }\n#endif\n#ifdef REALM_ON_WINDOWS\n    if(this->thread == GetCurrentThread()) {\n      // just process the signals right here and now\n      process_signals();\n    } else {\n      assert(0);\n    }\n#endif\n  }\n\n  /*static*/ size_t KernelThread::static_tls_size = 0;\n\n  // used in empirical testing of TLS size below\n  static void *empty_thread_body(void *data) { return data; }\n\n  /*static*/ void KernelThread::detect_static_tls_size(void)\n  {\n    // case 1: the environment variable REALM_STATIC_TLS_SIZE can be set to\n    //  skip all auto-detection attempts\n    do {\n      const char *s = getenv(\"REALM_STATIC_TLS_SIZE\");\n      if(!s) break;\n\n      const char *pos = 0;\n      size_t v = strtoull(s, const_cast<char **>(&pos), 10);\n      if((errno != 0) || (v == 0)) {\n\terrno = 0;\n\tbreak;\n      }\n\n      switch(*pos) {\n      case 'k': case 'K': { v <<= 10; break; }\n      case 'm': case 'M': { v <<= 20; break; }\n      default: break;\n      }\n      static_tls_size = v;\n      log_thread.debug() << \"static tls size = \" << static_tls_size << \" (from environment)\";\n      return;\n    } while(0);\n\n#if defined(REALM_ON_LINUX) && defined(REALM_USE_LIBDL)\n    // case 2: see if we can find glibc's __static_tls_size variable\n    //   (as of glibc 2.2.5, this is not exported, but if/when it is, it's\n    //   simpler than the __pthread_get_minstack version below)\n    do {\n      void *sym = dlsym(RTLD_DEFAULT, \"__static_tls_size\");\n      if(!sym) break;\n\n      static_tls_size = *reinterpret_cast<const size_t *>(sym);\n      log_thread.debug() << \"static tls size = \" << static_tls_size << \" (from glibc __static_tls_size)\";\n      return;\n    } while(0);\n\n    // case 3: try __pthread_get_minstack (subtracting out PTHREAD_STACK_MIN)\n    do {\n      void *sym = dlsym(RTLD_DEFAULT, \"__pthread_get_minstack\");\n      if(!sym) break;\n\n      pthread_attr_t attr;\n      CHECK_PTHREAD( pthread_attr_init(&attr) );\n      size_t minstack = (reinterpret_cast<size_t (*)(const pthread_attr_t *)>(sym))(&attr);\n      CHECK_PTHREAD( pthread_attr_destroy(&attr) );\n\n      // sanity-check the resulting value\n      if(minstack < PTHREAD_STACK_MIN) break;\n\n      static_tls_size = minstack - PTHREAD_STACK_MIN;\n      log_thread.debug() << \"static tls size = \" << static_tls_size << \" (from glibc __pthread_get_minstack)\";\n      return;\n    } while(0);\n#endif\n\n#ifdef REALM_USE_PTHREADS\n    // case 4: empirically determine it by trying to create threads with small\n    //  stacks (test up to 16MB)\n    {\n      pthread_attr_t attr;\n\n      CHECK_PTHREAD( pthread_attr_init(&attr) );\n\n      for(size_t v = 1024; v <= 16*1024*1024; v <<= 1) {\n\t// if pthreads doesn't like this stack size, skip to the next one\n\tif(pthread_attr_setstacksize(&attr, PTHREAD_STACK_MIN + v) != 0) {\n\t  // clear errno too\n\t  errno = 0;\n\t  continue;\n\t}\n\tpthread_t thread;\n\tint ret = pthread_create(&thread, &attr, empty_thread_body, 0);\n\tswitch(ret) {\n\tcase 0:\n\t  {\n\t    // success - this estimate of the TLS size is sufficient\n\t    void *result = 0;\n\t    CHECK_PTHREAD( pthread_join(thread, &result) );\n\t    CHECK_PTHREAD( pthread_attr_destroy(&attr) );\n\n\t    static_tls_size = v;\n\t    log_thread.debug() << \"static tls size = \" << static_tls_size << \" (from empirical testing)\";\n\t    return;\n\t  }\n\n\tcase EINVAL:\n\t  {\n\t    // invalid settings in attr (i.e. our stack size)\n\t    //  - clear errno and try again\n\t    errno = 0;\n\t    break;\n\t  }\n\n\tdefault:\n\t  {\n\t    // unexpected error\n\t    std::cerr << \"PTHREAD: pthread_create(...) = \" << ret << \" (\" << strerror(ret) << \")\" << std::endl;\n\t    ::abort();\n\t  }\n\t}\n\n      }\n\n      // none of the sizes we tried worked...\n      CHECK_PTHREAD( pthread_attr_destroy(&attr) );\n    }\n#endif\n\n    // if all else fails, guess it's about 32KB\n#ifdef REALM_ON_WINDOWS\n    static_tls_size = 0;  // not on stack in win32?\n#else\n    static_tls_size = 32768;\n#endif\n    log_thread.debug() << \"static tls size = \" << static_tls_size << \" (uneducated guess)\";\n  }\n\n  // used when we don't have an allocation yet\n  template <typename T>\n  class DeferredThreadStart : public CoreReservation::NotificationListener {\n  public:\n    DeferredThreadStart(T *_thread,\n                        const ThreadLaunchParameters& _params);\n\n    virtual ~DeferredThreadStart(void);\n\n    virtual void notify_allocation(const CoreReservation& rsrv);\n\n  protected:\n    T *thread;\n    ThreadLaunchParameters params;\n  };\n\n  template <typename T>\n  DeferredThreadStart<T>::DeferredThreadStart(T *_thread,\n                                              const ThreadLaunchParameters& _params)\n    : thread(_thread), params(_params)\n  {\n  }\n\n  template <typename T>\n  DeferredThreadStart<T>::~DeferredThreadStart(void)\n  {\n  }\n\n  template <typename T>\n  void DeferredThreadStart<T>::notify_allocation(const CoreReservation& rsrv)\n  {\n    // thread is allowed to start now\n    thread->start_thread(params, rsrv);\n    delete this;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class UserThread\n\n#ifdef REALM_USE_USER_THREADS\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n  namespace {\n    atomic<int> uswitch_test_check_flag(1);\n    ucontext_t uswitch_test_ctx1, uswitch_test_ctx2;\n\n    void uswitch_test_entry(int arg)\n    {\n      log_thread.debug() << \"uswitch test: adding: \" << uswitch_test_check_flag.load() << \" \" << arg;\n      uswitch_test_check_flag.fetch_add(arg);\n      errno = 0;\n      int ret = swapcontext(&uswitch_test_ctx2, &uswitch_test_ctx1);\n      if(ret != 0) {\n\tlog_thread.fatal() << \"uswitch test: swap out failed: \" << ret << \" \" << errno;\n\tassert(0);\n      }\n    }\n  }\n\n  // some systems do not appear to support user thread switching for\n  //  reasons unknown, so allow code to test to see if it's working first\n  /*static*/ bool Thread::test_user_switch_support(size_t stack_size /*= 1 << 20*/)\n  {\n    errno = 0;\n    int ret;\n    ret = getcontext(&uswitch_test_ctx2);\n    if(ret != 0) {\n      log_thread.info() << \"uswitch test: getcontext failed: \" << ret << \" \" << errno;\n      return false;\n    }\n    void *stack_base = malloc(stack_size);\n    if(!stack_base) {\n      log_thread.info() << \"uswitch test: stack malloc failed\";\n      return false;\n    }\n    uswitch_test_ctx2.uc_link = 0; // we don't expect it to ever fall through\n    uswitch_test_ctx2.uc_stack.ss_sp = stack_base;\n    uswitch_test_ctx2.uc_stack.ss_size = stack_size;\n    uswitch_test_ctx2.uc_stack.ss_flags = 0;\n    makecontext(&uswitch_test_ctx2,\n\t\treinterpret_cast<void(*)()>(uswitch_test_entry),\n\t\t1, 66);\n\n    // now try to swap and back\n    errno = 0;\n    ret = swapcontext(&uswitch_test_ctx1, &uswitch_test_ctx2);\n    if(ret != 0) {\n      log_thread.info() << \"uswitch test: swap in failed: \" << ret << \" \" << errno;\n      free(stack_base);\n      return false;\n    }\n\n    int val = uswitch_test_check_flag.load();\n    if(val != 67) {\n      log_thread.info() << \"uswitch test: val mismatch: \" << val << \" != 67\";\n      free(stack_base);\n      return false;\n    }\n\n    log_thread.debug() << \"uswitch test: check succeeded\";\n    free(stack_base);\n    return true;\n  }\n#endif\n#ifdef REALM_ON_WINDOWS\n  /*static*/ bool Thread::test_user_switch_support(size_t stack_size /*= 1 << 20*/)\n  {\n    return true;\n  }\n#endif\n\n  class UserThread : public Thread {\n  public:\n    UserThread(void *_target, void (*_entry_wrapper)(void *),\n\t       ThreadScheduler *_scheduler);\n\n    virtual ~UserThread(void);\n\n    void start_thread(const ThreadLaunchParameters& params,\n\t\t      const CoreReservation *rsrv);\n\n    virtual void join(void);\n    virtual void detach(void);\n\n    static void user_switch(UserThread *switch_to);\n\n  protected:\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    REALM_ATTR_NORETURN(static void uthread_entry(void));\n#endif\n#ifdef REALM_ON_WINDOWS\n    REALM_ATTR_NORETURN(static void uthread_entry(void *));\n#endif\n\n    virtual void alert_thread(void);\n\n    static const int MAGIC_VALUE = 0x11223344;\n\n    void *target;\n    void (*entry_wrapper)(void *);\n    int magic;\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    pthread_t host_pthread;\n    ucontext_t ctx;\n#ifdef REALM_ON_MACOS\n    // valgrind says Darwin's getcontext is writing past the end of ctx?\n    int padding[512];\n#endif\n    void *stack_base;\n#endif\n#ifdef REALM_ON_WINDOWS\n    LPVOID fiber;\n#endif\n    size_t stack_size;\n    bool ok_to_delete;\n    bool running;\n  };\n\n  UserThread::UserThread(void *_target, void (*_entry_wrapper)(void *),\n\t\t\t ThreadScheduler *_scheduler)\n    : Thread(_scheduler), target(_target), entry_wrapper(_entry_wrapper)\n    , magic(MAGIC_VALUE)\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    , stack_base(0)\n#endif\n    , stack_size(0), ok_to_delete(false)\n    , running(false)\n  {\n  }\n\n  UserThread::~UserThread(void)\n  {\n    // cannot delete an active thread...\n    assert(!running);\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    if(stack_base != 0)\n      free(stack_base);\n#endif\n#ifdef REALM_ON_WINDOWS\n    DeleteFiber(fiber);\n#endif\n  }\n\n  namespace ThreadLocal {\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    REALM_THREAD_LOCAL ucontext_t *host_context = 0;\n#endif\n#ifdef REALM_ON_WINDOWS\n    REALM_THREAD_LOCAL LPVOID host_context = 0;\n#endif\n    // current_user_thread is redundant with current_thread, but kept for debugging\n    //  purposes for now\n    REALM_THREAD_LOCAL UserThread *current_user_thread = 0;\n    REALM_THREAD_LOCAL Thread *current_host_thread = 0;\n  };\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n  /*static*/ void UserThread::uthread_entry(void)\n#endif\n#ifdef REALM_ON_WINDOWS\n  /*static*/ void UserThread::uthread_entry(void *)\n#endif\n  {\n    UserThread *thread = ThreadLocal::current_user_thread;\n    assert(thread != 0);\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    thread->host_pthread = pthread_self();\n#endif\n    thread->running = true;\n\n    log_thread.info() << \"thread \" << thread << \" started\";\n    thread->update_state(STATE_RUNNING);\n\n    if(thread->scheduler)\n      thread->scheduler->thread_starting(thread);\n    \n    // call the actual thread body\n    (*thread->entry_wrapper)(thread->target);\n\n    if(thread->scheduler)\n      thread->scheduler->thread_terminating(thread);\n    \n    // on return, we update our status and terminate\n    log_thread.info() << \"thread \" << thread << \" finished\";\n    thread->update_state(STATE_FINISHED);\n\n    // returning from this call is lethal, so hand control back to the host\n    //  thread and hope for the best\n    while(true) {\n      user_switch(0);\n      log_thread.warning() << \"HELP!  switched to a terminated thread \" << thread;\n    }\n  }\n\n  void UserThread::start_thread(const ThreadLaunchParameters& params,\n\t\t\t\tconst CoreReservation *rsrv)\n  {\n    // it turns out MacOS behaves REALLY strangely with a stack < 32KB, and there\n    //  make be some lower limit in Linux-land too, so clamp to 64KB to be safe\n    const ptrdiff_t MIN_STACK_SIZE = 64 << 10;\n\n    if(params.stack_size != params.STACK_SIZE_DEFAULT) {\n      // make sure it's not too large\n      if(rsrv)\n\tassert((rsrv->params.max_stack_size == rsrv->params.STACK_SIZE_DEFAULT) ||\n\t       (params.stack_size <= rsrv->params.max_stack_size));\n\n      stack_size = std::max<ptrdiff_t>(params.stack_size, MIN_STACK_SIZE);\n    } else {\n      // does the entire core reservation have a non-standard stack size?\n      if(rsrv &&\n\t (rsrv->params.max_stack_size != rsrv->params.STACK_SIZE_DEFAULT)) {\n\tstack_size = std::max<ptrdiff_t>(rsrv->params.max_stack_size,\n\t\t\t\t\t MIN_STACK_SIZE);\n      }\n    }\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    stack_base = malloc(stack_size);\n    assert(stack_base != 0);\n\n    CHECK_LIBC( getcontext(&ctx) );\n\n    ctx.uc_link = 0; // we don't expect it to ever fall through\n    ctx.uc_stack.ss_sp = stack_base;\n    ctx.uc_stack.ss_size = stack_size;\n    ctx.uc_stack.ss_flags = 0;\n\n    // grr...  entry point takes int's, which might not hold a void *\n    // we'll just fish our UserThread * out of TLS\n    makecontext(&ctx, uthread_entry, 0);\n#endif\n#ifdef REALM_ON_WINDOWS\n    fiber = CreateFiberEx(stack_size, stack_size,\n                          FIBER_FLAG_FLOAT_SWITCH, uthread_entry, 0);\n    if(fiber == 0) {\n      log_thread.fatal() << \"fiber creation failed: error=\" << GetLastError();\n      ::abort();\n    }\n#endif\n\n    update_state(STATE_STARTUP);    \n\n    log_thread.info() << \"thread created:\" << this << \" (\" << (rsrv ? rsrv->name : \"??\") << \") - user thread\";\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n    log_thread.debug() << \"thread stack: \" << this << \" size=\" << stack_size << \" base=\" << stack_base;\n#endif\n#ifdef REALM_ON_WINDOWS\n    log_thread.debug() << \"thread stack: \" << this << \" size=\" << stack_size;\n#endif\n  }\n\n  void UserThread::join(void)\n  {\n    assert(0); // not supported yet\n  }\n\n  void UserThread::detach(void)\n  {\n    assert(0); // not supported yet\n  }\n\n  /*static*/ void UserThread::user_switch(UserThread *switch_to)\n  {\n#ifdef DEBUG_USWITCH\n    printf(\"uswitch: %p: %p -> %p\\n\",\n\t   ThreadLocal::current_host_thread ? ThreadLocal::current_host_thread : ThreadLocal::current_thread,\n\t   ThreadLocal::current_user_thread,\n\t   switch_to);\n#endif\n\n    if(ThreadLocal::current_user_thread == 0) {\n      // called from a kernel thread, which will be used as the host\n\n      assert(switch_to != 0);\n      assert(switch_to->magic == MAGIC_VALUE);\n      assert(ThreadLocal::host_context == 0);\n\n      ThreadLocal::current_user_thread = switch_to;\n      ThreadLocal::current_host_thread = ThreadLocal::current_thread;\n      ThreadLocal::current_thread = switch_to;\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n      // this holds the host's state\n      ucontext_t host_ctx;\n\n      ThreadLocal::host_context = &host_ctx;\n\n      CHECK_LIBC( swapcontext(&host_ctx, &switch_to->ctx) );\n#endif\n#ifdef REALM_ON_WINDOWS\n      LPVOID host_ctx = ConvertThreadToFiberEx(0, 0);\n\n      ThreadLocal::host_context = host_ctx;\n\n      SwitchToFiber(switch_to->fiber);\n#endif\n\n      assert(ThreadLocal::current_user_thread == 0);\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n      assert(ThreadLocal::host_context == &host_ctx);\n#endif\n#ifdef REALM_ON_WINDOWS\n      assert(ThreadLocal::host_context == host_ctx);\n      BOOL ok = ConvertFiberToThread();\n      if(!ok) {\n        log_thread.fatal() << \"ConvertFiberToThread failed: error=\" << GetLastError();\n        ::abort();\n      }\n#endif\n      ThreadLocal::host_context = 0;\n    } else {\n      UserThread *switch_from = ThreadLocal::current_user_thread;\n      ThreadLocal::current_user_thread = switch_to;\n\n      assert(switch_from->running == true);\n      switch_from->running = false;\n\n      if(switch_to != 0) {\n\tassert(switch_to->magic == MAGIC_VALUE);\n\tassert(switch_to->running == false);\n\n\tThreadLocal::current_thread = switch_to;\n\n\t// a switch between two user contexts - nice and simple\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n\tCHECK_LIBC( swapcontext(&switch_from->ctx, &switch_to->ctx) );\n\tswitch_from->host_pthread = pthread_self();\n#endif\n#ifdef REALM_ON_WINDOWS\n  SwitchToFiber(switch_to->fiber);\n#endif\n\n\tassert(switch_from->running == false);\n\tswitch_from->running = true;\n      } else {\n\t// a return of control to the host thread\n\tassert(ThreadLocal::host_context != 0);\n\n\tThreadLocal::current_thread = ThreadLocal::current_host_thread;\n\tThreadLocal::current_host_thread = 0;\n\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n\tCHECK_LIBC( swapcontext(&switch_from->ctx, ThreadLocal::host_context) );\n\tswitch_from->host_pthread = pthread_self();\n#endif\n#ifdef REALM_ON_WINDOWS\n  SwitchToFiber(ThreadLocal::host_context);\n#endif\n\n\t// if we get control back\n\tassert(switch_from->running == false);\n\tswitch_from->running = true;\n      }\n    }\n  }\n\n  void UserThread::alert_thread(void)\n  {\n    if(ThreadLocal::current_thread == this) {\n      // just process the signals right here and now\n      process_signals();\n    } else {\n      // TODO: work out the race conditions inherent in this process\n      if(running) {\n#if defined(REALM_ON_LINUX) || defined(REALM_ON_MACOS) || defined(REALM_ON_FREEBSD)\n\tpthread_kill(host_pthread, handler_signal);\n#else\n        assert(0);\n#endif\n      } else {\n        assert(scheduler != 0);\n\tif(try_update_state(STATE_BLOCKED, STATE_ALERTED)) {\n          scheduler->thread_ready(this);\n        } else {\n\t  log_thread.fatal()  << \"HELP! couldn't alert: thread=\" << this << \" state=\" << get_state();\n\t  assert(0);\n\t}\n      }\n    }\n  }\n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Thread\n\n  /*static*/ Thread *Thread::create_kernel_thread_untyped(void *target, void (*entry_wrapper)(void *),\n\t\t\t\t\t\t\t  const ThreadLaunchParameters& params,\n\t\t\t\t\t\t\t  CoreReservation& rsrv,\n\t\t\t\t\t\t\t  ThreadScheduler *_scheduler)\n  {\n    KernelThread *t = new KernelThread(target, entry_wrapper, _scheduler);\n\n    // if we have an allocation, we can start the thread immediately\n    if(rsrv.allocation) {\n      t->start_thread(params, rsrv);\n    } else {\n      rsrv.add_listener(new DeferredThreadStart<KernelThread>(t, params));\n    }\n\n    return t;\n  }\n\n  /*static*/ void Thread::yield(void)\n  {\n#ifdef REALM_USE_PTHREADS\n#ifdef REALM_ON_MACOS\n    sched_yield();\n#else\n    pthread_yield();\n#endif\n#endif\n#ifdef REALM_ON_WINDOWS\n    SwitchToThread();\n#endif\n  }\n\n#ifdef REALM_USE_USER_THREADS\n  /*static*/ Thread *Thread::create_user_thread_untyped(void *target, void (*entry_wrapper)(void *),\n\t\t\t\t\t\t\tconst ThreadLaunchParameters& params,\n\t\t\t\t\t\t\tconst CoreReservation *rsrv,\n\t\t\t\t\t\t\tThreadScheduler *_scheduler)\n  {\n    UserThread *t = new UserThread(target, entry_wrapper, _scheduler);\n\n    // no need to wait on an allocation - the host thread will take care of that\n    t->start_thread(params, rsrv);\n\n    return t;\n  }\n\n  /*static*/ void Thread::user_switch(Thread *switch_to)\n  {\n    // just cast 'switch_to' to a UserThread - UserThread::user_switch will do a bit\n    //   of sanity-checking\n    UserThread::user_switch((UserThread *)switch_to);\n  }\n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CoreMap\n\n  CoreMap::CoreMap(void)\n  {\n  }\n\n  CoreMap::~CoreMap(void)\n  {\n    clear();\n  }\n\n  void CoreMap::clear(void)\n  {\n    // delete all the processor entries\n    for(ProcMap::iterator it = all_procs.begin();\n\tit != all_procs.end();\n\tit++)\n      delete it->second;\n\n    // now clear out the two maps\n    all_procs.clear();\n    by_domain.clear();\n  }\n\n  /*static*/ CoreMap *CoreMap::create_synthetic(int num_domains,\n\t\t\t\t\t\tint cores_per_domain,\n\t\t\t\t\t\tint hyperthreads /*= 1*/,\n\t\t\t\t\t\tint fp_cluster_size /*= 1*/)\n  {\n    CoreMap *cm = new CoreMap;\n\n    // processor ids will just be monotonically increasing\n    int next_id = 0;\n\n    for(int d = 0; d < num_domains; d++) {\n      for(int c = 0; c < cores_per_domain; c++) {\n\tstd::set<Proc *> fp_procs;\n\n\tfor(int f = 0; f < fp_cluster_size; f++) {\n\t  std::set<Proc *> ht_procs;\n\n\t  for(int h = 0; h < hyperthreads; h++) {\n\t    int id = next_id++;\n\n\t    Proc *p = new Proc;\n\n\t    p->id = id;\n\t    p->domain = d;\n\t    // kernel proc id list is empty - this is synthetic\n\n\t    cm->all_procs[id] = p;\n\t    cm->by_domain[d][id] = p;\n\n\t    ht_procs.insert(p);\n\t    fp_procs.insert(p);\n\t  }\n\n\t  // ALU and LD/ST shared with all other hyperthreads\n\t  for(std::set<Proc *>::iterator it1 = ht_procs.begin(); it1 != ht_procs.end(); it1++)\n\t    for(std::set<Proc *>::iterator it2 = ht_procs.begin(); it2 != ht_procs.end(); it2++)\n\t      if(it1 != it2) {\n\t\t(*it1)->shares_alu.insert(*it2);\n\t\t(*it1)->shares_ldst.insert(*it2);\n\t      }\n\t}\n\n\t// FPU shared with all other procs in cluster\n\tfor(std::set<Proc *>::iterator it1 = fp_procs.begin(); it1 != fp_procs.end(); it1++)\n\t  for(std::set<Proc *>::iterator it2 = fp_procs.begin(); it2 != fp_procs.end(); it2++)\n\t    if(it1 != it2) {\n\t      (*it1)->shares_fpu.insert(*it2);\n\t    }\n      }\n    }\n\n    return cm;\n  }\n\n  // this function is templated on the map key, since we don't really care \n  //  what was used to make the equivalence classes\n  template <typename K>\n  void update_core_sharing(const std::map<K, std::set<CoreMap::Proc *> >& core_sets,\n\t\t\t   bool share_alu, bool share_fpu, bool share_ldst)\n  {\n    for(typename std::map<K, std::set<CoreMap::Proc *> >::const_iterator it = core_sets.begin();\n        it != core_sets.end();\n        it++) {\n      const std::set<CoreMap::Proc *>& cset = it->second;\n      if(cset.size() == 1) continue;  // singleton set - no sharing\n\n      // all pairs dependencies\n      for(std::set<CoreMap::Proc *>::const_iterator it1 = cset.begin(); it1 != cset.end(); it1++) {\n        for(std::set<CoreMap::Proc *>::const_iterator it2 = cset.begin(); it2 != cset.end(); it2++) {\n          if(it1 != it2) {\n            CoreMap::Proc *p = *it1;\n            if(share_alu)\n\t      p->shares_alu.insert(*it2);\n\t    if(share_fpu)\n\t      p->shares_fpu.insert(*it2);\n\t    if(share_ldst)\n\t      p->shares_ldst.insert(*it2);\n          }\n        }\n      }\n    }\n  }\n\n#ifdef REALM_ON_LINUX\n  static CoreMap *extract_core_map_from_linux_sys(bool hyperthread_sharing)\n  {\n    cpu_set_t cset;\n    int ret = sched_getaffinity(0, sizeof(cset), &cset);\n    if(ret < 0) {\n      log_thread.warning() << \"failed to get affinity info\";\n      return 0;\n    }\n\n    DIR *nd = opendir(\"/sys/devices/system/node\");\n    if(!nd) {\n      log_thread.warning() << \"can't open /sys/devices/system/node\";\n      return 0;\n    }\n\n    CoreMap *cm = new CoreMap;\n    // hyperthreading sets are cores with the same node ID and physical core ID\n    //  they share ALU, FPU, and LDST units\n    std::map<std::pair<int, int>, std::set<CoreMap::Proc *> > ht_sets;\n\n    // \"thread_siblings\" can be used to detect Bulldozer's core pairs that \n    //  share the same FPU (this will also catch hyperthreads, but that's ok)\n    std::map<std::string, std::set<CoreMap::Proc *> > sibling_sets;\n\n    // look for entries named /sys/devices/system/node/node<N>\n    for(struct dirent *ne = readdir(nd); ne; ne = readdir(nd)) {\n      if(strncmp(ne->d_name, \"node\", 4)) continue;  // not a node directory\n      char *pos;\n      int node_id = strtol(ne->d_name + 4, &pos, 10);\n      if(pos && *pos) continue;  // doesn't match node[0-9]+\n\t  \n      char per_node_path[1024];\n      sprintf(per_node_path, \"/sys/devices/system/node/%s\", ne->d_name);\n      DIR *cd = opendir(per_node_path);\n      if(!cd) {\n\tlog_thread.warning() << \"can't open '\" << per_node_path << \"' - skipping\";\n\tcontinue;\n      }\n\n      // look for entries named /sys/devices/system/node/node<N>/cpu<N>\n      for(struct dirent *ce = readdir(cd); ce; ce = readdir(cd)) {\n\tif(strncmp(ce->d_name, \"cpu\", 3)) continue; // definitely not a cpu\n\tchar *pos;\n\tint cpu_id = strtol(ce->d_name + 3, &pos, 10);\n\tif(pos && *pos) continue;  // doesn't match cpu[0-9]+\n\t    \n\t// is this a cpu we're allowed to use?\n\tif(!CPU_ISSET(cpu_id, &cset)) {\n\t  log_thread.info() << \"cpu \" << cpu_id << \" not available - skipping\";\n\t  continue;\n\t}\n\n\t// figure out which physical core it is (i.e. detect hyperthreads)\n\tchar core_id_path[1024];\n\tsprintf(core_id_path, \"/sys/devices/system/node/%s/%s/topology/core_id\", ne->d_name, ce->d_name);\n\tFILE *f = fopen(core_id_path, \"r\");\n\tif(!f) {\n\t  log_thread.warning() << \"can't read '\" << core_id_path << \"' - skipping\";\n\t  continue;\n\t}\n\tint core_id;\n\tint count = fscanf(f, \"%d\", &core_id);\n\tfclose(f);\n\tif(count != 1) {\n\t  log_thread.warning() << \"can't find core id in '\" << core_id_path << \"' - skipping\";\n\t  continue;\n\t}\n\n\tCoreMap::Proc *p = new CoreMap::Proc;\n\n\tp->id = cpu_id;\n\tp->domain = node_id;\n\tp->kernel_proc_ids.insert(cpu_id);\n\n\tcm->all_procs[cpu_id] = p;\n\tcm->by_domain[node_id][cpu_id] = p;\n\n\t// add to HT sets to deal with in a bit\n\tht_sets[std::make_pair(node_id, core_id)].insert(p);\n\n\t// read the sibling set, if we can - no need to parse it because we\n\t//  expect symmetry across all cores in the same set\n\t{\n\t  char sibling_path[1024];\n\t  sprintf(sibling_path, \"/sys/devices/system/node/%s/%s/topology/thread_siblings_list\", ne->d_name, ce->d_name);\n\t  FILE *f = fopen(sibling_path, \"r\");\n\t  if(f) {\n\t    char line[256];\n\t    if(fgets(line, 255, f)) {\n\t      if(*line)\n\t\tsibling_sets[line].insert(p);\n\t    } else\n\t      log_thread.warning() << \"error reading '\" << sibling_path << \"' - no contents?\";\n\t    fclose(f);\n\t  } else\n\t    log_thread.warning() << \"can't read '\" << sibling_path << \"' - skipping\";\n\t}\n      }\n      closedir(cd);\n    }\n    closedir(nd);\n\n    if(hyperthread_sharing) {\n      update_core_sharing(ht_sets, true /*alu*/, true /*fpu*/, true /*ldst*/);\n      update_core_sharing(sibling_sets,\n\t\t\t  false /*!alu*/, true /*fpu*/, false /*!ldst*/);\n    }\n\n    // all done!\n    return cm;\n  }\n#endif\n\n#ifdef REALM_USE_HWLOC\n#ifdef REALM_ON_LINUX\n  // find bulldozer cpus that share fpu\n  static bool get_bd_sibling_id(int cpu_id, int core_id,\n\t\t\t\tstd::set<int>& sibling_ids) {\n    char str[1024];\n    sprintf(str, \"/sys/devices/system/cpu/cpu%d/topology/thread_siblings\", cpu_id);\n    FILE *f = fopen(str, \"r\");\n    if(!f) {\n      log_thread.warning() << \"can't read '\" << str << \"' - skipping\";\n      return false;\n    }\n    hwloc_bitmap_t set = hwloc_bitmap_alloc();\n    hwloc_linux_parse_cpumap_file(f, set);\n\n    fclose(f);\n\n    // loop over all siblings (except ourselves)\n    for(int siblingid = hwloc_bitmap_first(set);\n\tsiblingid != -1;\n\tsiblingid = hwloc_bitmap_next(set, siblingid)) {\n      if(siblingid == cpu_id) continue;\n\n      // don't filter siblings with the same core ID - this catches\n      //  hyperthreads too\n#if 0\n      sprintf(str, \"/sys/devices/system/cpu/cpu%d/topology/core_id\", siblingid);\n      f = fopen(str, \"r\");\n      if(!f) {\n\tlog_thread.warning() << \"can't read '\" << str << \"' - skipping\";\n\tcontinue;\n      }\n      int sib_core_id;\n      int count = fscanf(f, \"%d\", &sib_core_id);\n      fclose(f);\n      if(count != 1) {\n\tlog_thread.warning() << \"can't find core id in '\" << str << \"' - skipping\";\n\tcontinue;\n      }\n      if(sib_core_id == core_id) continue;\n#endif\n      sibling_ids.insert(siblingid);\n    }\n\n    hwloc_bitmap_free(set);\n\n    return true;\n  }\n#endif\n\n  static CoreMap *extract_core_map_from_hwloc(bool hyperthread_sharing)\n  {\n    CoreMap *cm = new CoreMap;\n\n    // hyperthreading sets are cores with the same node ID and physical core ID\n    std::map<std::pair<int, int>, std::set<CoreMap::Proc *> > ht_sets;\n    // bulldozer specific sets\n    std::map<int, std::set<CoreMap::Proc *> > bd_sets;\n\n    hwloc_topology_t topology;\n    hwloc_topology_init(&topology);\n    hwloc_topology_load(topology);\n\n    hwloc_obj_t obj = NULL;\n    int cpu_id, node_id, core_id;\n    while ((obj = hwloc_get_next_obj_by_type(topology, HWLOC_OBJ_CORE, obj)) != NULL) {\n      if(obj->online_cpuset && obj->allowed_cpuset) {\n        cpu_id = hwloc_bitmap_first(obj->cpuset);\n        while(cpu_id != -1) {\n\n          node_id = hwloc_bitmap_first(obj->nodeset);\n          core_id = obj->os_index;\n\n          CoreMap::Proc *p = new CoreMap::Proc;\n\n          p->id = cpu_id;\n          p->domain = node_id;\n          p->kernel_proc_ids.insert(cpu_id);\n\n          cm->all_procs[cpu_id] = p;\n          cm->by_domain[node_id][cpu_id] = p;\n\n          // add to HT sets to deal with in a bit\n          ht_sets[std::make_pair(node_id, core_id)].insert(p);\n\n#ifdef REALM_ON_LINUX\n          // add bulldozer sets\n\t  std::set<int> sibling_ids;\n\t  if(get_bd_sibling_id(cpu_id, core_id, sibling_ids) &&\n\t     !sibling_ids.empty()) {\n\t    bd_sets[cpu_id].insert(p);\n\t    for(std::set<int>::const_iterator it = sibling_ids.begin();\n\t\tit != sibling_ids.end();\n\t\t++it)\n\t      bd_sets[*it].insert(p);\n\t  }\n#endif\n\n          cpu_id = hwloc_bitmap_next(obj->cpuset, cpu_id);\n        }\n      }\n    }\n    hwloc_topology_destroy(topology);\n\n    if(hyperthread_sharing) {\n      update_core_sharing(ht_sets, true /*alu*/, true /*fpu*/, true /*ldst*/);\n      update_core_sharing(bd_sets,\n\t\t\t  false /*!alu*/, true /*fpu*/, false /*!ldst*/);\n    }\n\n    // all done!\n    return cm;\n  }\n#endif\n\n#ifdef REALM_ON_WINDOWS\n  static CoreMap *extract_core_map_from_windows_api(bool hyperthread_sharing)\n  {\n    DWORD_PTR process_mask, system_mask;\n    GetProcessAffinityMask(GetCurrentProcess(), &process_mask, &system_mask);\n    if(process_mask == 0) {\n      log_thread.warning() << \"process affinity mask is empty? (system = \" << system_mask << \")\";\n      return 0;\n    }\n    log_thread.debug() << \"affinity_mask = \" << process_mask << \" system=\" << system_mask;\n\n    PSYSTEM_LOGICAL_PROCESSOR_INFORMATION proc_info = NULL;\n    DWORD proc_info_size = 0;\n    DWORD rc;\n    rc = GetLogicalProcessorInformation(proc_info, &proc_info_size);\n    if((rc == TRUE) || (GetLastError() != ERROR_INSUFFICIENT_BUFFER) || (proc_info_size == 0)) {\n      log_thread.warning() << \"unable to query processor info size\";\n      return 0;\n    }\n    proc_info = (PSYSTEM_LOGICAL_PROCESSOR_INFORMATION)malloc(proc_info_size);\n    assert(proc_info != 0);\n    rc = GetLogicalProcessorInformation(proc_info, &proc_info_size);\n    assert(rc == TRUE);\n\n    // populate all_procs map\n    CoreMap *cm = new CoreMap;\n\n    for(int i = 0; (i < sizeof(DWORD_PTR)*8) && ((DWORD_PTR(1) << i) <= process_mask); i++)\n      if((process_mask & (DWORD_PTR(1) << i)) != 0) {\n        CoreMap::Proc *p = new CoreMap::Proc;\n        p->id = i;\n        p->domain = -1;  // fill in below\n        p->kernel_proc_ids.insert(i);\n        cm->all_procs[i] = p;\n      }\n\n    size_t num_infos = proc_info_size / sizeof(SYSTEM_LOGICAL_PROCESSOR_INFORMATION);\n    for(size_t i = 0; i < num_infos; i++) {\n      DWORD_PTR eff_mask = process_mask & proc_info[i].ProcessorMask;\n      if(eff_mask == 0) continue;\n\n      switch(proc_info[i].Relationship) {\n        case RelationNumaNode:\n        {\n          log_thread.debug() << \"info[\" << i << \"]: eff_mask=\" << proc_info[i].ProcessorMask << \" numa node=\" << proc_info[i].NumaNode.NodeNumber;\n          CoreMap::ProcMap& dm = cm->by_domain[proc_info[i].NumaNode.NodeNumber];\n          for(int i = 0; (i < sizeof(DWORD_PTR)*8) && ((DWORD_PTR(1) << i) <= eff_mask); i++)\n            if((eff_mask & (DWORD_PTR(1) << i)) != 0) {\n              CoreMap::Proc *p = cm->all_procs[i];\n              assert(p != 0);\n              p->domain = proc_info[i].NumaNode.NodeNumber;\n              dm[p->id] = p;\n            }\n          break;\n        }\n\n        case RelationProcessorCore:\n        {\n          log_thread.debug() << \"info[\" << i << \"]: eff_mask=\" << proc_info[i].ProcessorMask << \" hyperthreads\";\n\n          // these are hyperthreads - do we care?\n          if(hyperthread_sharing) {\n            for(int i = 0; (i < sizeof(DWORD_PTR)*8) && ((DWORD_PTR(1) << i) <= eff_mask); i++)\n              if((eff_mask & (DWORD_PTR(1) << i)) != 0) {\n                CoreMap::Proc *p1 = cm->all_procs[i];\n                for(int j = i + 1; (i < sizeof(DWORD_PTR)*8) && ((DWORD_PTR(1) << j) <= eff_mask); j++)\n                  if((eff_mask & (DWORD_PTR(1) << j)) != 0) {\n                    CoreMap::Proc *p2 = cm->all_procs[j];\n                    p1->shares_alu.insert(p2);\n                    p1->shares_fpu.insert(p2);\n                    p1->shares_ldst.insert(p2);\n\n                    p2->shares_alu.insert(p1);\n                    p2->shares_fpu.insert(p1);\n                    p2->shares_ldst.insert(p1);\n                  }\n              }\n          }\n          break;\n        }\n\n        default:\n        {\n          log_thread.debug() << \"info[\" << i << \"]: eff_mask=\" << proc_info[i].ProcessorMask << \" rel=\" << proc_info[i].Relationship;\n          break;\n        }\n      }\n    }\n\n    free(proc_info);\n\n    return cm;\n  }\n#endif\n\n  /*static*/ CoreMap *CoreMap::discover_core_map(bool hyperthread_sharing)\n  {\n    // we'll try a number of different strategies to discover the local cores:\n    // 1) a user-defined synthetic map, if REALM_SYNTHETIC_CORE_MAP is set\n    if(getenv(\"REALM_SYNTHETIC_CORE_MAP\")) {\n      const char *p = getenv(\"REALM_SYNTHETIC_CORE_MAP\");\n      int num_domains = 1;\n      int num_cores = 1;\n      int hyperthreads = 1;\n      int fp_cluster_size = 1;\n      while(true) {\n\tif(!(p[0] && (p[1] == '=') && isdigit(p[2]))) break;\n\n\tconst char *p2;\n\tint x = strtol(p+2, (char **)&p2, 10);\n\tif(x == 0) { p+=2; break; }  // zero of anything is bad\n\tif(p[0] == 'd') num_domains = x; else\n\tif(p[0] == 'c') num_cores = x; else\n\tif(p[0] == 'h') hyperthreads = x; else\n\tif(p[0] == 'f') fp_cluster_size = x; else\n\t  break;\n\tp = p2;\n\n\t// now we want a comma (to continue) or end-of-string\n\tif(*p != ',') break;\n\tp++;\n      }\n      // if parsing reached the end of string, we're good\n      if(*p == 0) {\n\treturn CoreMap::create_synthetic(num_domains, num_cores, hyperthreads, fp_cluster_size);\n      } else {\n\tconst char *orig = getenv(\"REALM_SYNTHETIC_CORE_MAP\");\n\tlog_thread.error(\"Error parsing REALM_SYNTHETIC_CORE_MAP: '%.*s(^)%s'\",\n\t\t\t (int)(p-orig), orig, p);\n      }\n    }\n\n    // 2) extracted from hwloc information\n#ifdef REALM_USE_HWLOC\n    {\n      CoreMap *cm = extract_core_map_from_hwloc(hyperthread_sharing);\n      if(cm) return cm;\n    }\n#endif\n\n    // 3) extracted from Linux's /sys\n#ifdef REALM_ON_LINUX\n    {\n      CoreMap *cm = extract_core_map_from_linux_sys(hyperthread_sharing);\n      if(cm) return cm;\n    }\n#endif\n\n    // 4) windows has an API for this\n#ifdef REALM_ON_WINDOWS\n    {\n      CoreMap *cm = extract_core_map_from_windows_api(hyperthread_sharing);\n      if(cm) return cm;\n    }\n#endif\n\n    // 5) as a final fallback a single-core synthetic map\n    {\n      CoreMap *cm = create_synthetic(1, 1);\n      return cm;\n    }\n  }\n  \n  static void show_share_set(std::ostream& os, const char *name,\n\t\t\t     const std::set<CoreMap::Proc *>& sset)\n  {\n    if(sset.empty()) return;\n\n    os << ' ' << name << \"=<\";\n    std::set<CoreMap::Proc *>::const_iterator it = sset.begin();\n    while(true) {\n      os << (*it)->id;\n      if(++it == sset.end()) break;\n      os << ',';\n    }\n    os << \">\";\n  }\n\n  /*friend*/ std::ostream& operator<<(std::ostream& os, const CoreMap& cm)\n  {\n    os << \"core map {\" << std::endl;\n    for(CoreMap::DomainMap::const_iterator it = cm.by_domain.begin();\n\tit != cm.by_domain.end();\n\tit++) {\n      os << \"  domain \" << it->first << \" {\" << std::endl;\n      for(CoreMap::ProcMap::const_iterator it2 = it->second.begin();\n\t  it2 != it->second.end();\n\t  it2++) {\n\tos << \"    core \" << it2->first << \" {\";\n\tconst CoreMap::Proc *p = it2->second;\n\tif(!p->kernel_proc_ids.empty()) {\n\t  os << \" ids=<\";\n\t  std::set<int>::const_iterator it3 = p->kernel_proc_ids.begin();\n\t  while(true) {\n\t    os << *it3;\n\t    if(++it3 == p->kernel_proc_ids.end()) break;\n\t    os << ',';\n\t  }\n\t  os << \">\";\n\t}\n\n\tshow_share_set(os, \"alu\", p->shares_alu);\n\tshow_share_set(os, \"fpu\", p->shares_fpu);\n\tshow_share_set(os, \"ldst\", p->shares_ldst);\n\n\tos << \" }\" << std::endl;\n      }\n      os << \"  }\" << std::endl;\n    }\n    os << \"}\";\n    return os;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PAPICounters\n\n\n#ifdef REALM_USE_PAPI\n  PAPICounters::PAPICounters(void)\n    : papi_event_set(PAPI_NULL)\n  {}\n\n  PAPICounters::~PAPICounters(void)\n  {\n    if(papi_event_set != PAPI_NULL) {\n      int ret = PAPI_cleanup_eventset(papi_event_set);\n      assert(ret == PAPI_OK);\n      int orig_event_set = papi_event_set;\n      ret = PAPI_destroy_eventset(&papi_event_set);\n      log_papi.debug() << \"destroy_eventset: \" << orig_event_set << \" (\" << ret << \")\";\n      assert(ret == PAPI_OK);\n      assert(papi_event_set == PAPI_NULL);\n    }\n  }\n\n  /*static*/ PAPICounters *PAPICounters::setup_counters(const ProfilingMeasurementCollection& pmc)\n  {\n    // if we didn't successfully initialize PAPI, don't try to use it...\n    if(!PAPI::papi_available)\n      return 0;\n\n    // first, check for all the things we know how to translate into PAPI events\n    std::vector<int> desired_events;\n\n    if(pmc.wants_measurement<ProfilingMeasurements::IPCPerfCounters>()) {\n      desired_events.push_back(PAPI_TOT_INS);\n      desired_events.push_back(PAPI_TOT_CYC);\n      desired_events.push_back(PAPI_FP_INS);\n      desired_events.push_back(PAPI_LD_INS);\n      desired_events.push_back(PAPI_SR_INS);\n      desired_events.push_back(PAPI_BR_INS);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L1ICachePerfCounters>()) {\n      desired_events.push_back(PAPI_L1_ICA);\n      desired_events.push_back(PAPI_L1_ICM);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L1DCachePerfCounters>()) {\n      desired_events.push_back(PAPI_L1_DCA);\n      desired_events.push_back(PAPI_L1_DCM);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L2CachePerfCounters>()) {\n      desired_events.push_back(PAPI_L2_TCA);\n      desired_events.push_back(PAPI_L2_TCM);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L3CachePerfCounters>()) {\n      desired_events.push_back(PAPI_L3_TCA);\n      desired_events.push_back(PAPI_L3_TCM);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::TLBPerfCounters>()) {\n      desired_events.push_back(PAPI_TLB_IM);\n      desired_events.push_back(PAPI_TLB_DM);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::BranchPredictionPerfCounters>()) {\n      desired_events.push_back(PAPI_BR_CN);\n      desired_events.push_back(PAPI_BR_TKN);\n      desired_events.push_back(PAPI_BR_MSP);\n    }\n\n    // exit early if none present\n    if(desired_events.empty()) return 0;\n\n    // otherwise create an event set and add as many of them as we can\n    PAPICounters *ctrs = new PAPICounters;\n\n    {\n      int ret = PAPI_create_eventset(&(ctrs->papi_event_set));\n      log_papi.debug() << \"create_eventset: \" << ctrs->papi_event_set << \" (\" << ret << \")\";\n      assert(ret == PAPI_OK);\n    }\n\n    size_t count = 0;\n    for(std::vector<int>::const_iterator it = desired_events.begin();\n\tit != desired_events.end();\n\t++it) {\n      // event might already have been added?\n      if(ctrs->event_codes.count(*it) > 0)\n\tcontinue;\n\n      int ret = PAPI_add_event(ctrs->papi_event_set, *it);\n      if(ret == PAPI_OK) {\n\tctrs->event_codes[*it] = count;\n\tctrs->event_counts.push_back(0);\n\tcount++;\n\tcontinue;\n      }\n      // two kinds of tolerable error\n      if(ret == PAPI_ENOEVNT) {\n\tlog_papi.debug() << \"event \" << *it << \" not available on hardware - skipping\";\n\tcontinue;\n      }\n      if(ret == PAPI_ECNFLCT) {\n\tlog_papi.debug() << \"event \" << *it << \" conflicts with previously added events - skipping\";\n\tcontinue;\n      }\n      // anything else is a real problem\n      log_papi.fatal() << \"add event: \" << PAPI_strerror(ret) << \" (\" << ret << \")\";\n      assert(false);\n    }\n\n    return ctrs;\n  }\n\n  void PAPICounters::cleanup(void)\n  {\n    // TODO: try to reuse these things?\n    delete this;\n  }\n\n  void PAPICounters::start(void)\n  {\n    int ret = PAPI_start(papi_event_set);\n    log_papi.debug() << \"start counters: \" << papi_event_set << \" (\" << ret << \")\";\n    assert(ret == PAPI_OK);\n  }\n\n  void PAPICounters::stop(void)\n  {\n    int ret = PAPI_accum(papi_event_set, &event_counts[0]);\n    assert(ret == PAPI_OK);\n    ret = PAPI_stop(papi_event_set, 0 /* don't read values again */);\n    log_papi.debug() << \"stop counters: \" << papi_event_set << \" (\" << ret << \")\";\n    assert(ret == PAPI_OK);\n  }\n\n  void PAPICounters::resume(void)\n  {\n    // same as start for now\n    start();\n  }\n\n  void PAPICounters::suspend(void)\n  {\n    // same as stop for now\n    stop();\n  }\n\n  // little helper to get a counter if present, or -1 if not\n  static inline long long get_counter_val(int code,\n\t\t\t\t\t  const std::map<int, size_t>& event_codes,\n\t\t\t\t\t  const std::vector<long long>& event_counts,\n\t\t\t\t\t  int& found_count)\n  {\n    std::map<int, size_t>::const_iterator it = event_codes.find(code);\n    if(it != event_codes.end()) {\n      found_count++;\n      return event_counts[it->second];\n    } else\n      return -1;\n  }\n\n  void PAPICounters::record(ProfilingMeasurementCollection& pmc)\n  {\n    if(pmc.wants_measurement<ProfilingMeasurements::IPCPerfCounters>()) {\n      ProfilingMeasurements::IPCPerfCounters ctrs;\n      int found_count = 0;\n      ctrs.total_insts  = get_counter_val(PAPI_TOT_INS, event_codes, event_counts, found_count);\n      ctrs.total_cycles = get_counter_val(PAPI_TOT_CYC, event_codes, event_counts, found_count);\n      ctrs.fp_insts     = get_counter_val(PAPI_FP_INS , event_codes, event_counts, found_count);\n      ctrs.ld_insts     = get_counter_val(PAPI_LD_INS , event_codes, event_counts, found_count);\n      ctrs.st_insts     = get_counter_val(PAPI_SR_INS , event_codes, event_counts, found_count);\n      ctrs.br_insts     = get_counter_val(PAPI_BR_INS , event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L1ICachePerfCounters>()) {\n      ProfilingMeasurements::L1ICachePerfCounters ctrs;\n      int found_count = 0;\n      ctrs.accesses = get_counter_val(PAPI_L1_ICA, event_codes, event_counts, found_count);\n      ctrs.misses   = get_counter_val(PAPI_L1_ICM, event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L1DCachePerfCounters>()) {\n      ProfilingMeasurements::L1DCachePerfCounters ctrs;\n      int found_count = 0;\n      ctrs.accesses = get_counter_val(PAPI_L1_DCA, event_codes, event_counts, found_count);\n      ctrs.misses   = get_counter_val(PAPI_L1_DCM, event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L2CachePerfCounters>()) {\n      ProfilingMeasurements::L2CachePerfCounters ctrs;\n      int found_count = 0;\n      ctrs.accesses = get_counter_val(PAPI_L2_TCA, event_codes, event_counts, found_count);\n      ctrs.misses   = get_counter_val(PAPI_L2_TCM, event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::L3CachePerfCounters>()) {\n      ProfilingMeasurements::L3CachePerfCounters ctrs;\n      int found_count = 0;\n      ctrs.accesses = get_counter_val(PAPI_L3_TCA, event_codes, event_counts, found_count);\n      ctrs.misses   = get_counter_val(PAPI_L3_TCM, event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::TLBPerfCounters>()) {\n      ProfilingMeasurements::TLBPerfCounters ctrs;\n      int found_count = 0;\n      ctrs.inst_misses = get_counter_val(PAPI_TLB_IM, event_codes, event_counts, found_count);\n      ctrs.data_misses = get_counter_val(PAPI_TLB_DM, event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n    if(pmc.wants_measurement<ProfilingMeasurements::BranchPredictionPerfCounters>()) {\n      ProfilingMeasurements::BranchPredictionPerfCounters ctrs;\n      int found_count = 0;\n      ctrs.total_branches = get_counter_val(PAPI_BR_CN , event_codes, event_counts, found_count);\n      ctrs.taken_branches = get_counter_val(PAPI_BR_TKN, event_codes, event_counts, found_count);\n      ctrs.mispredictions = get_counter_val(PAPI_BR_MSP, event_codes, event_counts, found_count);\n      if(found_count > 0)\n\tpmc.add_measurement(ctrs);\n    }\n\n#ifdef REALM_PAPI_DEBUG\n    for(std::map<int, size_t>::const_iterator it = event_codes.begin();\n\tit != event_codes.end();\n\t++it) {\n      log_papi.error() << \"counter[\" << (it->first & 0x7fffffff) << \"] = \" << event_counts[it->second];\n    }\n#endif\n  }\n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // initialize/cleanup\n\n  namespace Threading {\n\n    bool initialize(void)\n    {\n#ifdef REALM_USE_PAPI\n      {\n\tint ret = PAPI_library_init(PAPI_VER_CURRENT);\n\tif(ret == PAPI_VER_CURRENT) {\n\t  // initialized - now tell it we have threads\n\t  ret = PAPI_thread_init(pthread_self);\n\t  if(ret == PAPI_OK) {\n\t    PAPI::papi_available = true;\n\t    int numctrs = PAPI_get_opt(PAPI_MAX_HWCTRS, 0);\n\t    log_papi.debug() << \"initalized successfully - \" << numctrs << \" counters\";\n\t  } else {\n\t    log_papi.warning() << \"thread init error: \" << PAPI_strerror(ret) << \" (\" << ret << \")\";\n\t  }\n\t} else {\n\t  // failure could be due to a version mismatch or some other error\n\t  if(ret > 0) {\n\t    log_papi.warning() << \"version mismatch - wanted: \" << PAPI_VER_CURRENT << \", got: \" << ret;\n\t  } else {\n\t    log_papi.warning() << \"initialization error: \" << PAPI_strerror(ret) << \" (\" << ret << \")\";\n\t  }\n\t}\n      }\n#endif\n\n      KernelThread::detect_static_tls_size();\n\n      return true;\n    }\n\n    bool cleanup(void)\n    {\n#ifdef REALM_USE_PAPI\n      if(PAPI::papi_available) {\n\tPAPI_shutdown();\n      }\n#endif\n      return true;\n    }\n\n  };\n\n\n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/runtime/realm/module.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Realm modules\n\n#include \"realm/realm_config.h\"\n\n#define REALM_MODULE_REGISTRATION_STATIC\n#include \"realm/module.h\"\n\n#include \"realm/logging.h\"\n\n#include <assert.h>\n#include <string.h>\n#include <stdlib.h>\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n// TODO: replace this with Makefile (or maybe cmake) magic that adapts automatically\n//  to the build-system-controlled list of statically-linked Realm modules\n#include \"realm/runtime_impl.h\"\n#include \"realm/numa/numa_module.h\"\n#ifdef REALM_USE_OPENMP\n#include \"realm/openmp/openmp_module.h\"\n#endif\n#include \"realm/procset/procset_module.h\"\n#ifdef REALM_USE_PYTHON\n#include \"realm/python/python_module.h\"\n#endif\n#ifdef REALM_USE_CUDA\n#include \"realm/cuda/cuda_module.h\"\n#endif\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit_module.h\"\n#endif\n#ifdef REALM_USE_HDF5\n#include \"realm/hdf5/hdf5_module.h\"\n#endif\n#ifdef REALM_USE_GASNET1\n#include \"realm/gasnet1/gasnet1_module.h\"\n#endif\n#ifdef REALM_USE_GASNETEX\n#include \"realm/gasnetex/gasnetex_module.h\"\n#endif\n#if defined REALM_USE_MPI\n#include \"realm/mpi/mpi_module.h\"\n#endif\n\nnamespace Realm {\n\n  Logger log_module(\"module\");\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Module\n  //\n\n  Module::Module(const std::string& _name)\n    : name(_name)\n  {\n    log_module.debug() << \"module \" << name << \" created\";\n  }\n\n  Module::~Module(void)\n  {\n    log_module.debug() << \"module \" << name << \" destroyed\";\n  }\n\n  const std::string& Module::get_name(void) const\n  {\n    return name;\n  }\n\n  void Module::initialize(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" initialize\";\n  }\n\n  void Module::create_memories(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_memories\";\n  }\n\n  void Module::create_processors(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_processors\";\n  }\n  \n  void Module::create_dma_channels(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_dma_channels\";\n  }\n  \n  void Module::create_code_translators(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_code_translators\";\n  }\n\n  void Module::cleanup(void)\n  {\n    log_module.debug() << \"module \" << name << \" cleanup\";\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class ModuleRegistrar\n  //\n\n  namespace {\n    ModuleRegistrar::StaticRegistrationBase *static_modules_head = 0;\n    ModuleRegistrar::StaticRegistrationBase **static_modules_tail = &static_modules_head;\n  };\n\n  ModuleRegistrar::ModuleRegistrar(RuntimeImpl *_runtime)\n    : runtime(_runtime)\n  {}\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_static_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t      std::vector<Module *>& modules)\n  {\n    // just iterate over the static module list, trying to create each module\n    for(const StaticRegistrationBase *sreg = static_modules_head;\n\tsreg;\n\tsreg = sreg->next) {\n      Module *m = sreg->create_module(runtime, cmdline);\n      if(m)\n\tmodules.push_back(m);\n    }\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  // accepts a colon-separated list of so files to try to load\n  static void load_module_list(const char *sonames,\n\t\t\t       RuntimeImpl *runtime,\n\t\t\t       std::vector<std::string>& cmdline,\n\t\t\t       std::vector<void *>& handles,\n\t\t\t       std::vector<Module *>& modules)\n  {\n    // null/empty strings are nops\n    if(!sonames || !*sonames) return;\n\n    const char *p1 = sonames;\n    while(true) {\n      // skip leading colons\n      while(*p1 == ':') p1++;\n      if(!*p1) break;\n\n      const char *p2 = p1 + 1;\n      while(*p2 && (*p2 != ':')) p2++;\n\n      char filename[1024];\n      strncpy(filename, p1, p2 - p1);\n\n      // no leftover errors from anybody else please...\n      assert(dlerror() == 0);\n\n      // open so file, resolving all symbols but not polluting global namespace\n      void *handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n\n      if(handle != 0) {\n\t// this file should have a \"create_realm_module\" symbol\n\tvoid *sym = dlsym(handle, \"create_realm_module\");\n\n\tif(sym != 0) {\n\t  // TODO: hold onto the handle even if it doesn't create a module?\n\t  handles.push_back(handle);\n\n\t  Module *m = ((Module *(*)(RuntimeImpl *, std::vector<std::string>&))dlsym)(runtime, cmdline);\n\t  if(m)\n\t    modules.push_back(m);\n\t} else {\n\t  log_module.error() << \"symbol 'create_realm_module' not found in \" << filename;\n#ifndef NDEBUG\n\t  int ret =\n#endif\n\t    dlclose(handle);\n\t  assert(ret == 0);\n\t}\n      } else {\n\tlog_module.error() << \"could not load \" << filename << \": \" << dlerror();\n      }\n\n      if(!*p2) break;\n      p1 = p2 + 1;\n    }\n  }\n#endif\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_dynamic_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t       std::vector<Module *>& modules)\n  {\n    // dynamic modules are requested in one of two ways:\n    // 1) REALM_DYNAMIC_MODULES=sonames environment variable\n    // 2) \"-ll:module sonames\" on command line\n    // in both cases, 'sonames' is a colon-separate listed of .so files that should be\n\n    // loading modules can also monkey with the cmdline, so do a pass first where we pull\n    //  out all the name we want to load\n    std::vector<std::string> sonames_list;\n\n    {\n      const char *e = getenv(\"REALM_DYNAMIC_MODULES\");\n      if(e)\n\tsonames_list.push_back(std::string(e));\n    }\n\n    {\n      std::vector<std::string>::iterator it = cmdline.begin();\n      while(it != cmdline.end()) {\n\tif(*it != \"-ll:module\") {\n\t  it++;\n\t  continue;\n\t}\n\n\t// eat this argument and move the next one to sonames_list\n\tit = cmdline.erase(it);\n\tassert(it != cmdline.end());\n\tsonames_list.push_back(*it);\n\tit = cmdline.erase(it);\n      }\n    }\n\n#ifdef REALM_USE_DLFCN\n    for(std::vector<std::string>::const_iterator it = sonames_list.begin();\n\tit != sonames_list.end();\n\tit++)\n      load_module_list(it->c_str(),\n\t\t       runtime, cmdline, sofile_handles, modules);\n#else\n    if(!sonames_list.empty()) {\n      log_module.error() << \"loading of dynamic Realm modules requested, but REALM_USE_DLFCN=0!\";\n      exit(1);\n    }\n#endif\n  }\n\n  // called by runtime after all modules have been cleaned up\n  void ModuleRegistrar::unload_module_sofiles(void)\n  {\n#ifdef REALM_USE_DLFCN\n    while(!sofile_handles.empty()) {\n      void *handle = sofile_handles.back();\n      sofile_handles.pop_back();\n\n#ifndef NDEBUG\n      int ret =\n#endif\n\tdlclose(handle);\n      assert(ret == 0);\n    }\n#endif\n  }\n\n  // called by the module registration helpers\n  /*static*/ void ModuleRegistrar::add_static_registration(StaticRegistrationBase *reg)\n  {\n    // done during init, so single-threaded\n    *static_modules_tail = reg;\n    static_modules_tail = &(reg->next);\n  }\n  \n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/runtime/realm/python/python_module.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"realm/python/python_module.h\"\n#include \"realm/python/python_internal.h\"\n\n#include \"realm/numa/numasysif.h\"\n#include \"realm/logging.h\"\n#include \"realm/cmdline.h\"\n#include \"realm/proc_impl.h\"\n#include \"realm/mem_impl.h\"\n#include \"realm/threads.h\"\n#include \"realm/runtime_impl.h\"\n#include \"realm/utils.h\"\n\n#include <dlfcn.h>\n#ifdef REALM_USE_DLMOPEN\n#include <link.h>\n#endif // REALM_USE_DLMOPEN\n\n#include <list>\n\nnamespace Realm {\n\n  Logger log_py(\"python\");\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonAPI\n\n  PythonAPI::PythonAPI(void *_handle)\n    : handle(_handle)\n  {\n    get_symbol(this->Py_DecRef, \"Py_DecRef\");\n    get_symbol(this->Py_Finalize, \"Py_Finalize\");\n    get_symbol(this->Py_InitializeEx, \"Py_InitializeEx\");\n\n    get_symbol(this->PyByteArray_FromStringAndSize, \"PyByteArray_FromStringAndSize\");\n\n    get_symbol(this->PyEval_InitThreads, \"PyEval_InitThreads\");\n\n#ifdef USE_PYGILSTATE_CALLS\n    get_symbol(this->PyGILState_Ensure, \"PyGILState_Ensure\");\n    get_symbol(this->PyGILState_Release, \"PyGILState_Release\");\n#else\n    get_symbol(this->PyThreadState_New, \"PyThreadState_New\");\n    get_symbol(this->PyThreadState_Clear, \"PyThreadState_Clear\");\n    get_symbol(this->PyThreadState_Delete, \"PyThreadState_Delete\");\n#endif\n    get_symbol(this->PyEval_RestoreThread, \"PyEval_RestoreThread\");\n    get_symbol(this->PyEval_SaveThread, \"PyEval_SaveThread\");\n\n    get_symbol(this->PyThreadState_Swap, \"PyThreadState_Swap\");\n    get_symbol(this->PyThreadState_Get, \"PyThreadState_Get\");\n\n    get_symbol(this->PyErr_PrintEx, \"PyErr_PrintEx\");\n\n    get_symbol(this->PyImport_ImportModule, \"PyImport_ImportModule\");\n    get_symbol(this->PyModule_GetDict, \"PyModule_GetDict\");\n\n    get_symbol(this->PyLong_FromUnsignedLong, \"PyLong_FromUnsignedLong\");\n\n    get_symbol(this->PyObject_CallFunction, \"PyObject_CallFunction\");\n    get_symbol(this->PyObject_CallObject, \"PyObject_CallObject\");\n    get_symbol(this->PyObject_GetAttrString, \"PyObject_GetAttrString\");\n    get_symbol(this->PyObject_Print, \"PyObject_Print\");\n\n    get_symbol(this->PyRun_SimpleString, \"PyRun_SimpleString\");\n    get_symbol(this->PyRun_String, \"PyRun_String\");\n\n    get_symbol(this->PyTuple_New, \"PyTuple_New\");\n    get_symbol(this->PyTuple_SetItem, \"PyTuple_SetItem\");\n  }\n\n  template<typename T>\n  void PythonAPI::get_symbol(T &fn, const char *symbol,\n                             bool missing_ok /*= false*/)\n  {\n    fn = reinterpret_cast<T>(dlsym(handle, symbol));\n    if(!fn && !missing_ok) {\n      const char *error = dlerror();\n      log_py.fatal() << \"failed to find symbol '\" << symbol << \"': \" << error;\n      assert(false);\n    }\n  }\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonInterpreter\n\n#ifdef REALM_USE_DLMOPEN\n  // dlmproxy symbol lookups have to happen in a function we define so that\n  //  dl[v]sym searches in the right place\n  static void *dlmproxy_lookup(const char *symname, const char *symver)\n  {\n    \n    void *handle = 0;\n    void *sym = (symver ?\n\t\t   dlvsym(handle, symname, symver) :\n\t\t   dlsym(handle, symname));\n    if(sym)\n      log_py.debug() << \"found symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\") << \" ptr=\" << sym;\n    else\n      log_py.warning() << \"missing symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\");\n    return sym;\n  }\n#endif\n\n  PythonInterpreter::PythonInterpreter() \n  {\n#ifdef REALM_PYTHON_LIB\n    const char *python_lib = REALM_PYTHON_LIB;\n#else\n    const char *python_lib = \"libpython2.7.so\";\n#endif\n\n#ifdef REALM_USE_DLMOPEN\n    // loading libpython into its own namespace will cause it to try to bring\n    //   in a second copy of libpthread.so.0, which is fairly disastrous\n    // we deal with it by loading a \"dlmproxy\" of pthreads that tunnels all \n    //   pthreads calls back to the (only) version in the main executable\n    const char *dlmproxy_filename = getenv(\"DLMPROXY_LIBPTHREAD\");\n    if(!dlmproxy_filename)\n      dlmproxy_filename = \"dlmproxy_libpthread.so.0\";\n    dlmproxy_handle = dlmopen(LM_ID_NEWLM,\n\t\t\t      dlmproxy_filename,\n\t\t\t      RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_LAZY);\n    if(!dlmproxy_handle) {\n      const char *error = dlerror();\n      log_py.fatal() << \"HELP!  Use of dlmopen for python requires dlmproxy for pthreads!  Failed to\\n\"\n\t\t     << \"  load: \" << dlmproxy_filename << \"\\n\"\n\t\t     << \"  error: \" << error;\n      assert(false);\n    }\n\n    // now that the proxy is loaded, we need to tell it where the real\n    //  libpthreads functions are\n    {\n      void *sym = dlsym(dlmproxy_handle, \"dlmproxy_load_symbols\");\n      assert(sym != 0);\n      ((void (*)(void *(*)(const char *, const char *)))sym)(dlmproxy_lookup);\n    }\n\n    // now we can load libpython, but make sure we do it in the new namespace\n    Lmid_t lmid;\n    int ret = dlinfo(dlmproxy_handle, RTLD_DI_LMID, &lmid);\n    assert(ret == 0);\n\n    handle = dlmopen(lmid, python_lib, RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_NOW);\n#else\n    // life is so much easier if we use dlopen (but we only get one copy then)\n    handle = dlopen(python_lib, RTLD_GLOBAL | RTLD_LAZY);\n#endif\n    if (!handle) {\n      const char *error = dlerror();\n      log_py.fatal() << error;\n      assert(false);\n    }\n\n    api = new PythonAPI(handle);\n\n    (api->Py_InitializeEx)(0 /*!initsigs*/);\n    (api->PyEval_InitThreads)();\n    //(api->Py_Finalize)();\n\n    //PyThreadState *state;\n    //state = (api->PyEval_SaveThread)();\n    //(api->PyEval_RestoreThread)(state);\n\n    //(api->PyRun_SimpleString)(\"print 'hello Python world!'\");\n\n    //PythonSourceImplementation psi(\"taskreg_helper\", \"task1\");\n    //find_or_import_function(&psi);\n  }\n\n  PythonInterpreter::~PythonInterpreter()\n  {\n    (api->Py_Finalize)();\n\n    delete api;\n\n    if (dlclose(handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"libpython dlclose error: \" << error;\n      assert(false);\n    }\n\n#ifdef REALM_USE_DLMOPEN\n    if (dlclose(dlmproxy_handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"dlmproxy dlclose error: \" << error;\n      assert(false);\n    }\n#endif\n  }\n\n  PyObject *PythonInterpreter::find_or_import_function(const PythonSourceImplementation *psi)\n  {\n    //log_py.print() << \"attempting to acquire python lock\";\n    //(api->PyEval_AcquireLock)();\n    //log_py.print() << \"lock acquired\";\n\n    // not calling PythonInterpreter::import_module here because we want the\n    //  PyObject result\n    log_py.debug() << \"attempting to import module: \" << psi->module_name;\n    PyObject *module = (api->PyImport_ImportModule)(psi->module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(module, stdout, 0); printf(\"\\n\");\n\n    PyObject *function = module;\n    for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n           ie = psi->function_name.end(); function && it != ie; ++it) {\n      function = (api->PyObject_GetAttrString)(function, it->c_str());\n    }\n    if (!function) {\n      {\n        LoggerMessage m = log_py.fatal();\n        m << \"unable to import Python function \";\n        for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n               ie = psi->function_name.begin(); it != ie; ++it) {\n          m << *it;\n          if (it + 1 != ie) {\n            m << \".\";\n          }\n        }\n        m << \" from module \" << psi->module_name;\n      }\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(function, stdout, 0); printf(\"\\n\");\n\n    //(api->PyObject_CallFunction)(function, \"iii\", 1, 2, 3);\n\n    (api->Py_DecRef)(module);\n\n    return function;\n  }\n\n  void PythonInterpreter::import_module(const std::string& module_name)\n  {\n    log_py.debug() << \"attempting to import module: \" << module_name;\n    PyObject *module = (api->PyImport_ImportModule)(module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(module);\n  }\n\n  void PythonInterpreter::run_string(const std::string& script_text)\n  {\n    // from Python.h\n    const int Py_file_input = 257;\n\n    log_py.debug() << \"running python string: \" << script_text;\n    PyObject *mainmod = (api->PyImport_ImportModule)(\"__main__\");\n    assert(mainmod != 0);\n    PyObject *globals = (api->PyModule_GetDict)(mainmod);\n    assert(globals != 0);\n    PyObject *res = (api->PyRun_String)(script_text.c_str(),\n\t\t\t\t\tPy_file_input,\n\t\t\t\t\tglobals,\n\t\t\t\t\tglobals);\n    if(!res) {\n      log_py.fatal() << \"unable to run python string:\" << script_text;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(res);\n    (api->Py_DecRef)(globals);\n    (api->Py_DecRef)(mainmod);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonThreadTaskScheduler\n\n  PythonThreadTaskScheduler::PythonThreadTaskScheduler(LocalPythonProcessor *_pyproc,\n\t\t\t\t\t\t       CoreReservation& _core_rsrv)\n    : KernelThreadTaskScheduler(_pyproc->me, _core_rsrv)\n    , pyproc(_pyproc)\n    , interpreter_ready(false)\n  {}\n\n  // both real and internal tasks need to be wrapped with acquires of the GIL\n  bool PythonThreadTaskScheduler::execute_task(Task *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    bool ok = KernelThreadTaskScheduler::execute_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n\n    return ok;\n  }\n  \n  void PythonThreadTaskScheduler::execute_internal_task(InternalTask *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    KernelThreadTaskScheduler::execute_internal_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n  }\n    \n  void PythonThreadTaskScheduler::python_scheduler_loop(void)\n  {\n    // global startup of python interpreter if needed\n    if(!interpreter_ready) {\n      log_py.info() << \"creating interpreter\";\n      pyproc->create_interpreter();\n      interpreter_ready = true;\n    }\n\n#ifdef REALM_USE_OPENMP\n    // associate with an OpenMP thread pool if one is available\n    if(pyproc->omp_threadpool != 0)\n      pyproc->omp_threadpool->associate_as_master();\n#endif\n\n#ifdef USE_PYGILSTATE_CALLS\n    // our PyThreadState is implicit when using the PyGILState calls\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = 0;\n#else\n    // always create and remember our own python thread - does NOT require GIL\n    PyThreadState *pythread = (pyproc->interpreter->api->PyThreadState_New)(pyproc->master_thread->interp);\n    log_py.debug() << \"created python thread: \" << pythread;\n    \n    assert(pythread != 0);\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = pythread;\n#endif\n\n    // take lock and go into normal task scheduler loop\n    {\n      AutoLock<> al(lock);\n      KernelThreadTaskScheduler::scheduler_loop();\n    }\n#if 0\n    // now go into main scheduler loop, holding scheduler lock for whole thing\n    AutoLock<> al(lock);\n    while(true) {\n      // remember the work counter value before we start so that we don't iterate\n      //   unnecessarily\n      long long old_work_counter = work_counter.read_counter();\n\n      // first priority - task registration\n      while(!taskreg_queue.empty()) {\n\tLocalPythonProcessor::TaskRegistration *treg = taskreg_queue.front();\n\ttaskreg_queue.pop_front();\n\t\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\t\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = TaskQueue::PRI_POS_INF;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  pyproc->perform_task_registration(treg);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      }\n\n      // if we have both resumable and new ready tasks, we want the one that\n      //  is the highest priority, with ties going to resumable tasks - we\n      //  can do this cleanly by taking advantage of the fact that the\n      //  resumable_workers queue uses the scheduler lock, so can't change\n      //  during this call\n      // peek at the top thing (if any) in that queue, and then try to find\n      //  a ready task with higher priority\n      int resumable_priority = ResumableQueue::PRI_NEG_INF;\n      resumable_workers.peek(&resumable_priority);\n\n      // try to get a new task then\n      int task_priority = resumable_priority;\n      Task *task = TaskQueue::get_best_task(task_queues, task_priority);\n\n      // did we find work to do?\n      if(task) {\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = task_priority;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  execute_task(task);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n\tcontinue;\n      }\n\n      // having checked for higher-priority ready tasks, we can always\n      //  take the highest-priority resumable task, if any, and run it\n      if(!resumable_workers.empty()) {\n\tThread *yield_to = resumable_workers.get(0); // priority is irrelevant\n\tassert(yield_to != Thread::self());\n\n\t// this should only happen if we're at the max active worker count (otherwise\n\t//  somebody should have just woken this guy up earlier), and reduces the \n\t// unassigned worker count by one\n\tupdate_worker_count(0, -1);\n\n\tidle_workers.push_back(Thread::self());\n\tworker_sleep(yield_to);\n\n\t// loop around and check both queues again\n\tcontinue;\n      }\n\n      {\n\t// no ready or resumable tasks?  thumb twiddling time\n\n\t// are we shutting down?\n\tif(shutdown_flag.load()) {\n\t  // yes, we can terminate - wake up an idler (if any) first though\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t  } else {\n\t    // nobody to wake, so -1 active/unassigned worker\n\t    update_worker_count(-1, -1, false); // ok to drop below mins\n\t    worker_terminate(0);\n\t  }\n\t  return;\n\t}\n\n\t// do we have more unassigned and idle tasks than we need?\n\tint total_idle_count = (unassigned_worker_count +\n\t\t\t\t(int)(idle_workers.size()));\n\tif(total_idle_count > cfg_max_idle_workers) {\n\t  // if there are sleeping idlers, terminate in favor of one of those - keeps\n\t  //  worker counts constant\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    assert(to_wake != Thread::self());\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t    return;\n\t  }\n\t}\n\n\t// no, stay awake but suspend until there's a chance that the next iteration\n\t//  of this loop would turn out different\n\twait_for_work(old_work_counter);\n      }\n    }\n    // should never get here\n    assert(0);\n#endif\n  }\n\n  Thread *PythonThreadTaskScheduler::worker_create(bool make_active)\n  {\n    // lock is held by caller\n    ThreadLaunchParameters tlp;\n    Thread *t = Thread::create_kernel_thread<PythonThreadTaskScheduler,\n\t\t\t\t\t     &PythonThreadTaskScheduler::python_scheduler_loop>(this,\n\t\t\t\t\t\t\t\t\t\t\t\ttlp,\n\t\t\t\t\t\t\t\t\t\t\t\tcore_rsrv,\n\t\t\t\t\t\t\t\t\t\t\t\tthis);\n    all_workers.insert(t);\n    if(make_active)\n      active_workers.insert(t);\n    return t;\n  }\n \n  // called by a worker thread when it needs to wait for something (and we\n  //   should release the GIL)\n  void PythonThreadTaskScheduler::thread_blocking(Thread *thread)\n  {\n    // if this gets called before we're done initializing the interpreter,\n    //  we need a simple blocking wait\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n\n      log_py.debug() << \"waiting during initialization\";\n      bool really_blocked = try_update_thread_state(thread,\n\t\t\t\t\t\t    Thread::STATE_BLOCKING,\n\t\t\t\t\t\t    Thread::STATE_BLOCKED);\n      if(!really_blocked) return;\n\n      while(true) {\n\tlong long old_work_counter = work_counter.read_counter();\n\n\tif(!resumable_workers.empty()) {\n\t  Thread *t = resumable_workers.get(0);\n\t  assert(t == thread);\n\t  log_py.debug() << \"awake again\";\n\t  return;\n\t}\n\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // if we got here through a cffi call, the GIL has already been released,\n    //  so try to handle that case here - a call PyEval_SaveThread\n    //  if the GIL is not held will assert-fail, and while a call to\n    //  PyThreadState_Swap is technically illegal (and unsafe if python-created\n    //  threads exist), it does what we want for now\n    // NOTE: we use PyEval_{Save,Restore}Thread here even if USE_PYGILSTATE_CALLS\n    //  is defined, as a call to PyGILState_Release will destroy a thread\n    //  context - the Save/Restore take care of the actual lock, and since we\n    //  restore each python thread on the OS thread that owned it intially, the\n    //  PyGILState TLS stuff should remain consistent\n    PyThreadState *saved = (pyproc->interpreter->api->PyThreadState_Swap)(0);\n    if(saved != 0) {\n      log_py.info() << \"python worker sleeping - releasing GIL\";\n      // put it back so we can save it properly\n      (pyproc->interpreter->api->PyThreadState_Swap)(saved);\n      // would like to sanity-check that this returns the expected thread state,\n      //  but that would require taking the PythonThreadTaskScheduler's lock\n      (pyproc->interpreter->api->PyEval_SaveThread)();\n      log_py.debug() << \"SaveThread -> \" << saved;\n    } else\n      log_py.info() << \"python worker sleeping - GIL already released\";\n    \n    KernelThreadTaskScheduler::thread_blocking(thread);\n\n    if(saved) {\n      log_py.info() << \"python worker awake - acquiring GIL\";\n      log_py.debug() << \"RestoreThread <- \" << saved;\n      (pyproc->interpreter->api->PyEval_RestoreThread)(saved);\n    } else\n      log_py.info() << \"python worker awake - not acquiring GIL\";\n  }\n\n  void PythonThreadTaskScheduler::thread_ready(Thread *thread)\n  {\n    // handle the wakening of the initialization thread specially\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n      resumable_workers.put(thread, 0);\n    } else {\n      KernelThreadTaskScheduler::thread_ready(thread);\n    }\n  }\n\n  void PythonThreadTaskScheduler::worker_terminate(Thread *switch_to)\n  {\n#ifdef USE_PYGILSTATE_CALLS\n    // nothing to do?  pythreads entry was a placeholder\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    pythreads.erase(it);\n\n#else\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    PyThreadState *pythread = it->second;\n    pythreads.erase(it);\n\n    log_py.debug() << \"destroying python thread: \" << pythread;\n    \n    // our thread should not be active\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\n    // switch to the master thread, retaining the GIL\n    log_py.debug() << \"RestoreThread <- \" << pyproc->master_thread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pyproc->master_thread);\n\n    // clear and delete the worker thread\n    (pyproc->interpreter->api->PyThreadState_Clear)(pythread);\n    (pyproc->interpreter->api->PyThreadState_Delete)(pythread);\n\n    // release the GIL\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pyproc->master_thread);\n#endif\n\n    // TODO: tear down interpreter if last thread\n    if(shutdown_flag.load() && pythreads.empty())\n      pyproc->destroy_interpreter();\n\n    KernelThreadTaskScheduler::worker_terminate(switch_to);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class LocalPythonProcessor\n\n  LocalPythonProcessor::LocalPythonProcessor(Processor _me, int _numa_node,\n                                             CoreReservationSet& crs,\n                                             size_t _stack_size,\n#ifdef REALM_USE_OPENMP\n\t\t\t\t\t     int _omp_workers,\n#endif\n\t\t\t\t\t     const std::vector<std::string>& _import_modules,\n\t\t\t\t\t     const std::vector<std::string>& _init_scripts)\n    : ProcessorImpl(_me, Processor::PY_PROC)\n    , numa_node(_numa_node)\n    , import_modules(_import_modules)\n    , init_scripts(_init_scripts)\n    , interpreter(0)\n    , ready_task_count(stringbuilder() << \"realm/proc \" << me << \"/ready tasks\")\n  {\n    task_queue.set_gauge(&ready_task_count);\n    deferred_spawn_cache.clear();\n\n    CoreReservationParameters params;\n    params.set_num_cores(1);\n    params.set_numa_domain(numa_node);\n    params.set_alu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_fpu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_ldst_usage(params.CORE_USAGE_SHARED);\n    params.set_max_stack_size(_stack_size);\n\n    std::string name = stringbuilder() << \"Python\" << numa_node << \" proc \" << _me;\n\n    core_rsrv = new CoreReservation(name, crs, params);\n\n#ifdef REALM_USE_OPENMP\n    if(_omp_workers > 0) {\n      // create a pool (except for one thread, which is the main task thread)\n      omp_threadpool = new ThreadPool(_omp_workers - 1,\n\t\t\t\t      name, -1 /*numa_node*/, _stack_size, crs);\n    } else\n      omp_threadpool = 0;\n#endif\n\n    sched = new PythonThreadTaskScheduler(this, *core_rsrv);\n    sched->add_task_queue(&task_queue);\n  }\n\n  LocalPythonProcessor::~LocalPythonProcessor(void)\n  {\n    delete core_rsrv;\n    delete sched;\n#ifdef REALM_USE_OPENMP\n    if(omp_threadpool != 0)\n      delete omp_threadpool;\n#endif\n  }\n\n  // starts worker threads and performs any per-processor initialization\n  void LocalPythonProcessor::start_threads(void)\n  {\n    // finally, fire up the scheduler\n    sched->start();\n  }\n\n  void LocalPythonProcessor::shutdown(void)\n  {\n    log_py.info() << \"shutting down\";\n\n    sched->shutdown();\n#ifdef REALM_USE_OPENMP\n    if(omp_threadpool != 0)\n      omp_threadpool->stop_worker_threads();\n#endif\n    deferred_spawn_cache.flush();\n  }\n\n  void LocalPythonProcessor::create_interpreter(void)\n  {\n    assert(interpreter == 0);\n  \n    // create a python interpreter that stays entirely within this thread\n    interpreter = new PythonInterpreter;\n    // the call to PyEval_InitThreads in the PythonInterpreter constructor\n    //  acquired the GIL on our behalf already\n    master_thread = (interpreter->api->PyThreadState_Get)();\n\n    // always need the python threading module\n    interpreter->import_module(\"threading\");\n    \n    // perform requested initialization\n    for(std::vector<std::string>::const_iterator it = import_modules.begin();\n\tit != import_modules.end();\n\t++it)\n      interpreter->import_module(*it);\n\n    for(std::vector<std::string>::const_iterator it = init_scripts.begin();\n\tit != init_scripts.end();\n\t++it)\n      interpreter->run_string(*it);\n\n    // default state is GIL _released_ - even if using PyGILState_* calls,\n    //  use PyEval_SaveThread here to release the lock without decrementing\n    //  the use count of our master thread\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n  }\n\n  void LocalPythonProcessor::destroy_interpreter(void)\n  {\n    assert(interpreter != 0);\n\n    log_py.info() << \"destroying interpreter\";\n\n    // take GIL with master thread\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (interpreter->api->PyGILState_Ensure)();\n    assert(gilstate == PyGILState_UNLOCKED);\n#else\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n#endif\n\n    // during shutdown, the threading module tries to remove the Thread object\n    //  associated with this kernel thread - if that doesn't exist (because we're\n    //  shutting down from a different thread that we initialized the interpreter\n    //  _and_ nobody called threading.current_thread() from this kernel thread),\n    //  we'll get a KeyError in threading.py\n    // resolve this by calling threading.current_thread() here, using __import__\n    //  to deal with the case where 'import threading' never got called\n    (interpreter->api->PyRun_SimpleString)(\"__import__('threading').current_thread()\");\n\n    delete interpreter;\n    interpreter = 0;\n    master_thread = 0;\n  }\n  \n  bool LocalPythonProcessor::perform_task_registration(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(treg->func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << treg->func_id;\n      assert(0);\n    }\n\n    // this can run arbitrary python code, which might ask which processor it's\n    //  on\n    ThreadLocal::current_processor = me;\n\n    // we'll take either a python function or a cpp function\n    PyObject *python_fnptr = 0;\n    Processor::TaskFuncPtr cpp_fnptr = 0;\n\n    do {\n      // prefer a python function, if it's available\n      {\n\tconst PythonSourceImplementation *psi = treg->codedesc->find_impl<PythonSourceImplementation>();\n\tif(psi) {\n\t  python_fnptr = interpreter->find_or_import_function(psi);\n\t  assert(python_fnptr != 0);\n\t  break;\n\t}\n      }\n\n      // take a function pointer, if that's available\n      {\n\tconst FunctionPointerImplementation *fpi = treg->codedesc->find_impl<FunctionPointerImplementation>();\n\tif(fpi) {\n\t  cpp_fnptr = (Processor::TaskFuncPtr)(fpi->fnptr);\n\t  break;\n\t}\n      }\n\n      // last try: can we convert something to a function pointer?\n      {\n\tconst std::vector<CodeTranslator *>& translators = get_runtime()->get_code_translators();\n\tbool ok = false;\n\tfor(std::vector<CodeTranslator *>::const_iterator it = translators.begin();\n\t    it != translators.end();\n\t    it++)\n\t  if((*it)->can_translate<FunctionPointerImplementation>(*(treg->codedesc))) {\n\t    FunctionPointerImplementation *fpi = (*it)->translate<FunctionPointerImplementation>(*(treg->codedesc));\n\t    if(fpi) {\n\t      cpp_fnptr = (Processor::TaskFuncPtr)(fpi->fnptr);\n\t      ok = true;\n\t      break;\n\t    }\n\t  }\n\tif(ok) break;\n      }\n\n      log_py.fatal() << \"invalid code descriptor for python proc: \" << *(treg->codedesc);\n      assert(0);\n    } while(0);\n\n    log_py.info() << \"task \" << treg->func_id << \" registered on \" << me << \": \" << *(treg->codedesc);\n\n    TaskTableEntry &tte = task_table[treg->func_id];\n    tte.python_fnptr = python_fnptr;\n    tte.cpp_fnptr = cpp_fnptr;\n    tte.user_data.swap(treg->user_data);\n\n    delete treg->codedesc;\n    delete treg;\n\n    return true;\n  }\n\n  void LocalPythonProcessor::enqueue_task(Task *task)\n  {\n    task_queue.enqueue_task(task);\n  }\n\n  void LocalPythonProcessor::enqueue_tasks(Task::TaskList& tasks, size_t num_tasks)\n  {\n    task_queue.enqueue_tasks(tasks, num_tasks);\n  }\n\n  void LocalPythonProcessor::spawn_task(Processor::TaskFuncID func_id,\n\t\t\t\t\tconst void *args, size_t arglen,\n\t\t\t\t\tconst ProfilingRequestSet &reqs,\n\t\t\t\t\tEvent start_event,\n\t\t\t\t\tGenEventImpl *finish_event,\n\t\t\t\t\tEventImpl::gen_t finish_gen,\n\t\t\t\t\tint priority)\n  {\n    // create a task object for this\n    Task *task = new Task(me, func_id, args, arglen, reqs,\n\t\t\t  start_event, finish_event, finish_gen, priority);\n    get_runtime()->optable.add_local_operation(finish_event->make_event(finish_gen), task);\n\n    enqueue_or_defer_task(task, start_event, &deferred_spawn_cache);\n  }\n\n  void LocalPythonProcessor::add_to_group(ProcessorGroupImpl *group)\n  {\n    // add the group's task queue to our scheduler too\n    sched->add_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::remove_from_group(ProcessorGroupImpl *group)\n  {\n    // remove the group's task queue from our scheduler\n    sched->remove_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::register_task(Processor::TaskFuncID func_id,\n                                           CodeDescriptor& codedesc,\n                                           const ByteArrayRef& user_data)\n  {\n    TaskRegistration *treg = new TaskRegistration;\n    treg->proc = this;\n    treg->func_id = func_id;\n    treg->codedesc = new CodeDescriptor(codedesc);\n    treg->user_data = user_data;\n    sched->add_internal_task(treg);\n  }\n\n  void LocalPythonProcessor::execute_task(Processor::TaskFuncID func_id,\n\t\t\t\t\t  const ByteArrayRef& task_args)\n  {\n    std::map<Processor::TaskFuncID, TaskTableEntry>::const_iterator it = task_table.find(func_id);\n    if(it == task_table.end()) {\n      // TODO: remove this hack once the tools are available to the HLR to call these directly\n      if(func_id < Processor::TASK_ID_FIRST_AVAILABLE) {\n\tlog_py.info() << \"task \" << func_id << \" not registered on \" << me << \": ignoring missing legacy setup/shutdown task\";\n\treturn;\n      }\n      log_py.fatal() << \"task \" << func_id << \" not registered on \" << me;\n      assert(0);\n    }\n\n    const TaskTableEntry& tte = it->second;\n\n    if(tte.python_fnptr != 0) {\n      // task is a python function - wrap arguments in python objects and call\n      log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": python function \" << ((void *)(tte.python_fnptr));\n\n      PyObject *arg1 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)task_args.base(),\n\t\t\t\t\t\t   task_args.size());\n      assert(arg1 != 0);\n      PyObject *arg2 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)tte.user_data.base(),\n\t\t\t\t\t\t   tte.user_data.size());\n      assert(arg2 != 0);\n      // TODO: make into a Python realm.Processor object\n      PyObject *arg3 = (interpreter->api->PyLong_FromUnsignedLong)(me.id);\n      assert(arg3 != 0);\n\n      PyObject *args = (interpreter->api->PyTuple_New)(3);\n      assert(args != 0);\n      (interpreter->api->PyTuple_SetItem)(args, 0, arg1);\n      (interpreter->api->PyTuple_SetItem)(args, 1, arg2);\n      (interpreter->api->PyTuple_SetItem)(args, 2, arg3);\n\n      //printf(\"args = \"); (interpreter->api->PyObject_Print)(args, stdout, 0); printf(\"\\n\");\n\n      PyObject *res = (interpreter->api->PyObject_CallObject)(tte.python_fnptr, args);\n\n      (interpreter->api->Py_DecRef)(args);\n\n      //printf(\"res = \"); PyObject_Print(res, stdout, 0); printf(\"\\n\");\n      if(res != 0) {\n\t(interpreter->api->Py_DecRef)(res);\n      } else {\n\tlog_py.fatal() << \"python exception occurred within task:\";\n\t(interpreter->api->PyErr_PrintEx)(0);\n\t(interpreter->api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n\tassert(0);\n      }\n    } else {\n      // no python function - better have a cpp function\n      assert(tte.cpp_fnptr != 0);\n\n      log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": cpp function \" << ((void *)(tte.cpp_fnptr));\n\n      (tte.cpp_fnptr)(task_args.base(), task_args.size(),\n\t\t      tte.user_data.base(), tte.user_data.size(),\n\t\t      me);\n    }\n  }\n\n  namespace Python {\n\n    ////////////////////////////////////////////////////////////////////////\n    //\n    // class PythonModule\n\n    /*static*/ std::vector<std::string> PythonModule::extra_import_modules;\n\n    PythonModule::PythonModule(void)\n      : Module(\"python\")\n      , cfg_num_python_cpus(0)\n      , cfg_use_numa(false)\n      , cfg_stack_size(2 << 20)\n#ifdef REALM_USE_OPENMP\n      , cfg_pyomp_threads(0)\n#endif\n    {\n    }\n\n    PythonModule::~PythonModule(void)\n    {}\n\n    /*static*/ void PythonModule::import_python_module(const char *module_name)\n    {\n      extra_import_modules.push_back(module_name);\n    }\n\n    /*static*/ Module *PythonModule::create_module(RuntimeImpl *runtime,\n                                                 std::vector<std::string>& cmdline)\n    {\n      // create a module to fill in with stuff - we'll delete it if numa is\n      //  disabled\n      PythonModule *m = new PythonModule;\n\n      // first order of business - read command line parameters\n      {\n        CommandLineParser cp;\n\n        cp.add_option_int(\"-ll:py\", m->cfg_num_python_cpus)\n\t  .add_option_int(\"-ll:pynuma\", m->cfg_use_numa)\n\t  .add_option_int_units(\"-ll:pystack\", m->cfg_stack_size, 'm')\n\t  .add_option_stringlist(\"-ll:pyimport\", m->cfg_import_modules)\n\t  .add_option_stringlist(\"-ll:pyinit\", m->cfg_init_scripts);\n#ifdef REALM_USE_OPENMP\n\tcp.add_option_int(\"-ll:pyomp\", m->cfg_pyomp_threads);\n#endif\n\n        bool ok = cp.parse_command_line(cmdline);\n        if(!ok) {\n          log_py.fatal() << \"error reading Python command line parameters\";\n          assert(false);\n        }\n      }\n\n      // add extra module imports requested by the application\n      m->cfg_import_modules.insert(m->cfg_import_modules.end(),\n                                   extra_import_modules.begin(),\n                                   extra_import_modules.end());\n\n      // if no cpus were requested, there's no point\n      if(m->cfg_num_python_cpus == 0) {\n        log_py.debug() << \"no Python cpus requested\";\n        delete m;\n        return 0;\n      }\n\n#ifndef REALM_USE_DLMOPEN\n      // Multiple CPUs are only allowed if we're using dlmopen.\n      if(m->cfg_num_python_cpus > 1) {\n        log_py.fatal() << \"support for multiple Python CPUs is not available: recompile with USE_DLMOPEN\";\n        assert(false);\n      }\n#endif\n\n      // get number/sizes of NUMA nodes -\n      //   disable (with a warning) numa binding if support not found\n      if(m->cfg_use_numa) {\n        std::map<int, NumaNodeCpuInfo> cpuinfo;\n        if(numasysif_numa_available() &&\n           numasysif_get_cpu_info(cpuinfo) &&\n           !cpuinfo.empty()) {\n          // filter out any numa domains with insufficient core counts\n          int cores_needed = m->cfg_num_python_cpus;\n          for(std::map<int, NumaNodeCpuInfo>::const_iterator it = cpuinfo.begin();\n              it != cpuinfo.end();\n              ++it) {\n            const NumaNodeCpuInfo& ci = it->second;\n            if(ci.cores_available >= cores_needed) {\n              m->active_numa_domains.insert(ci.node_id);\n            } else {\n              log_py.warning() << \"not enough cores in NUMA domain \" << ci.node_id << \" (\" << ci.cores_available << \" < \" << cores_needed << \")\";\n            }\n          }\n        } else {\n          log_py.warning() << \"numa support not found (or not working)\";\n          m->cfg_use_numa = false;\n        }\n      }\n\n      // if we don't end up with any active numa domains,\n      //  use NUMA_DOMAIN_DONTCARE\n      // actually, use the value (-1) since it seems to cause link errors!?\n      if(m->active_numa_domains.empty())\n        m->active_numa_domains.insert(-1 /*CoreReservationParameters::NUMA_DOMAIN_DONTCARE*/);\n\n      return m;\n    }\n\n    // do any general initialization - this is called after all configuration is\n    //  complete\n    void PythonModule::initialize(RuntimeImpl *runtime)\n    {\n      Module::initialize(runtime);\n    }\n\n    // create any processors provided by the module (default == do nothing)\n    //  (each new ProcessorImpl should use a Processor from\n    //   RuntimeImpl::next_local_processor_id)\n    void PythonModule::create_processors(RuntimeImpl *runtime)\n    {\n      Module::create_processors(runtime);\n\n      for(std::set<int>::const_iterator it = active_numa_domains.begin();\n          it != active_numa_domains.end();\n          ++it) {\n        int cpu_node = *it;\n        for(int i = 0; i < cfg_num_python_cpus; i++) {\n          Processor p = runtime->next_local_processor_id();\n          ProcessorImpl *pi = new LocalPythonProcessor(p, cpu_node,\n                                                       runtime->core_reservation_set(),\n                                                       cfg_stack_size,\n#ifdef REALM_USE_OPENMP\n\t\t\t\t\t\t       cfg_pyomp_threads,\n#endif\n\t\t\t\t\t\t       cfg_import_modules,\n\t\t\t\t\t\t       cfg_init_scripts);\n          runtime->add_processor(pi);\n\n          // create affinities between this processor and system/reg memories\n          // if the memory is one we created, use the kernel-reported distance\n          // to adjust the answer\n          std::vector<MemoryImpl *>& local_mems = runtime->nodes[Network::my_node_id].memories;\n          for(std::vector<MemoryImpl *>::iterator it2 = local_mems.begin();\n              it2 != local_mems.end();\n              ++it2) {\n            Memory::Kind kind = (*it2)->get_kind();\n            if((kind != Memory::SYSTEM_MEM) && (kind != Memory::REGDMA_MEM))\n              continue;\n\n            Machine::ProcessorMemoryAffinity pma;\n            pma.p = p;\n            pma.m = (*it2)->me;\n\n            // use the same made-up numbers as in\n            //  runtime_impl.cc\n            if(kind == Memory::SYSTEM_MEM) {\n              pma.bandwidth = 100;  // \"large\"\n              pma.latency = 5;      // \"small\"\n            } else {\n              pma.bandwidth = 80;   // \"large\"\n              pma.latency = 10;     // \"small\"\n            }\n\n            runtime->add_proc_mem_affinity(pma);\n          }\n        }\n      }\n    }\n\n    // clean up any common resources created by the module - this will be called\n    //  after all memories/processors/etc. have been shut down and destroyed\n    void PythonModule::cleanup(void)\n    {\n      Module::cleanup();\n    }\n\n  }; // namespace Python\n\n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/test/realm/taskreg.cc": "#include \"realm.h\"\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit.h\"\n#endif\n#ifdef REALM_USE_PYTHON\n#include \"realm/python/python_source.h\"\n#endif\n\n#include <cstdio>\n#include <cstdlib>\n#include <cassert>\n#include <cstring>\n#include <cmath>\n#ifdef REALM_USE_LIBDL\n#include <dlfcn.h>\n#endif\n\n#include <time.h>\n\n#include \"osdep.h\"\n\nusing namespace Realm;\n\nLogger log_app(\"app\");\n\n// Task IDs, some IDs are reserved so start at first available number\nenum {\n  TOP_LEVEL_TASK = Processor::TASK_ID_FIRST_AVAILABLE+0,\n  CHILD_TASK_ID_START,\n#ifdef REALM_USE_LLVM\n  LLVM_TASK_ID = 100,\n#endif\n};\n\n#ifdef REALM_USE_LLVM\nconst char llvmir[] = \n\"@.str = private unnamed_addr constant [30 x i8] c\\\"hello from LLVM JIT! %d %lld\\\\0A\\\\00\\\", align 1\\n\"\n\"declare i32 @printf(i8*, ...)\\n\"\n\"define void @foo(i32* %a, i64 %b, i32* %c, i64 %d, i64 %e) {\\n\"\n#if REALM_LLVM_VERSION >= 37\n\"  %1 = load i32, i32* %a, align 4\\n\"\n\"  %2 = add i32 %1, 57\\n\"\n\"  %3 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([30 x i8], [30 x i8]* @.str, i32 0, i32 0), i32 %2, i64 %b)\\n\"\n#else\n\"  %1 = load i32* %a, align 4\\n\"\n\"  %2 = add i32 %1, 57\\n\"\n\"  %3 = call i32 (i8*, ...)* @printf(i8* getelementptr inbounds ([30 x i8]* @.str, i32 0, i32 0), i32 %2, i64 %b)\\n\"\n#endif\n\"  ret void\\n\"\n\"}\\n\";\n#endif\n\nvoid child_task(const void *args, size_t arglen, \n\t\tconst void *userdata, size_t userlen, Processor p)\n{\n  log_app.print() << \"child task on \" << p << \": arglen=\" << arglen << \", userlen=\" << userlen;\n}\n\n#ifdef REALM_USE_LIBDL\n// helper for looking up symbols\ntemplate <typename T>\nvoid lookup_symbol(const char *name, T& addr)\n{\n  void *res = dlsym(RTLD_DEFAULT, name);\n  if(!res) {\n    log_app.fatal() << \"symbol lookup error: cannot find '\" << name << \"'\";\n    abort();\n  }\n  addr = reinterpret_cast<T>(res);\n}\n#endif\n\n#ifdef REALM_USE_PYTHON\nvoid python_cpp_task(const void *args, size_t arglen,\n\t\t     const void *userdata, size_t userlen, Processor p)\n{\n  log_app.print() << \"python cpp task on \" << p << \": arglen=\" << arglen << \", userlen=\" << userlen;\n\n#ifdef REALM_USE_LIBDL\n  // try to talk to the python interpreter - use dlsym to find symbols so we\n  //  don't have a compile-time dependency on python\n  typedef void *PyGILState_STATE;\n  PyGILState_STATE (*PyGILState_Ensure)();\n  void (*PyGILState_Release)(PyGILState_STATE);\n  int (*PyRun_SimpleString)(const char *);\n\n  lookup_symbol(\"PyGILState_Ensure\", PyGILState_Ensure);\n  lookup_symbol(\"PyGILState_Release\", PyGILState_Release);\n  lookup_symbol(\"PyRun_SimpleString\", PyRun_SimpleString);\n\n  PyGILState_STATE state = PyGILState_Ensure();\n\n  int ret = PyRun_SimpleString(\"print(\\\"inside python interpreter from cpp task!\\\")\");\n  if(ret != 0) {\n    log_app.fatal() << \"PyRun_SimpleString returned \" << ret;\n    abort();\n  }\n\n  PyGILState_Release(state);\n#endif\n}\n#endif\n\nvoid top_level_task(const void *args, size_t arglen, \n\t\t    const void *userdata, size_t userlen, Processor p)\n{\n  log_app.print() << \"top task running on \" << p;\n\n  Machine machine = Machine::get_machine();\n  Processor::TaskFuncID func_id = CHILD_TASK_ID_START;\n \n  // first test - register a task individually on each processor and run it\n  {\n    std::set<Event> finish_events;\n\n    int count = 0;\n\n    std::set<Processor> all_processors;\n    machine.get_all_processors(all_processors);\n    for(std::set<Processor>::const_iterator it = all_processors.begin();\n\tit != all_processors.end();\n\tit++) {\n      Processor pp = (*it);\n\n      CodeDescriptor *task_desc = 0;\n      switch(pp.kind()) {\n      case Processor::LOC_PROC:\n      case Processor::UTIL_PROC:\n      case Processor::IO_PROC:\n\t{\n\t  task_desc = new CodeDescriptor(child_task);\n\t  break;\n\t}\n\n#ifdef REALM_USE_PYTHON\n      case Processor::PY_PROC:\n\t{\n\t  task_desc = new CodeDescriptor(Realm::Type::from_cpp_type<Processor::TaskFuncPtr>());\n\t  task_desc->add_implementation(new PythonSourceImplementation(\"taskreg_helper\",\n\t\t\t\t\t\t\t\t       \"task1\"));\n\t  break;\n\t}\n#endif\n\n      default:\n\t/* do nothing */\n\tbreak;\n      }\n      if(!task_desc) {\n\tlog_app.warning() << \"no task variant available for processor \" << p << \" (kind \" << pp.kind() << \")\";\n\tcontinue;\n      }\n\n      Event e = pp.register_task(func_id, *task_desc,\n\t\t\t\t ProfilingRequestSet(),\n\t\t\t\t &pp, sizeof(pp));\n\n      delete task_desc;\n\n      Event e2 = pp.spawn(func_id, &count, sizeof(count), e);\n\n      finish_events.insert(e2);\n\n      func_id++;\n    }\n\n    Event merged = Event::merge_events(finish_events);\n\n    merged.wait();\n  }\n\n  // second test - register a task on all LOC_PROCs\n  {\n    std::set<Event> finish_events;\n\n    CodeDescriptor child_task_desc(child_task);\n\n    Event e = Processor::register_task_by_kind(Processor::LOC_PROC,\n#ifdef _MSC_VER\n    // no portable task registration for windows yet\n                                               false /*!global*/,\n#else\n                                               true /*global*/,\n#endif\n\t\t\t\t\t       func_id,\n\t\t\t\t\t       child_task_desc,\n\t\t\t\t\t       ProfilingRequestSet());\n\n    int count = 0;\n\n    std::set<Processor> all_processors;\n    machine.get_all_processors(all_processors);\n    for(std::set<Processor>::const_iterator it = all_processors.begin();\n\tit != all_processors.end();\n\tit++) {\n      Processor pp = (*it);\n\n      // only LOC_PROCs\n      if(pp.kind() != Processor::LOC_PROC)\n\tcontinue;\n\n      Event e2 = pp.spawn(func_id, &count, sizeof(count), e);\n\n      finish_events.insert(e2);\n    }\n\n    func_id++;\n\n    Event merged = Event::merge_events(finish_events);\n\n    merged.wait();\n  }\n\n#ifdef REALM_USE_LLVM\n  // third test - LLVM (if available)\n  {\n    CodeDescriptor llvm_task_desc(TypeConv::from_cpp_type<Processor::TaskFuncPtr>());\n    llvm_task_desc.add_implementation(new LLVMIRImplementation(llvmir, sizeof(llvmir),\n\t\t\t\t\t\t\t       \"foo\"));\n\n    Event e = Processor::register_task_by_kind(Processor::LOC_PROC, true /*global*/,\n\t\t\t\t\t       LLVM_TASK_ID,\n\t\t\t\t\t       llvm_task_desc,\n\t\t\t\t\t       ProfilingRequestSet());\n\n    int count = 0;\n    std::set<Event> finish_events;\n    std::set<Processor> all_processors;\n    machine.get_all_processors(all_processors);\n    for(std::set<Processor>::const_iterator it = all_processors.begin();\n\tit != all_processors.end();\n\tit++) {\n      Processor pp = (*it);\n\n      // only LOC_PROCs\n      if(pp.kind() != Processor::LOC_PROC)\n\tcontinue;\n\n      Event e2 = pp.spawn(LLVM_TASK_ID, &count, sizeof(count), e);\n\n      count++;\n\n      finish_events.insert(e2);\n    }\n\n    Event merged = Event::merge_events(finish_events);\n\n    merged.wait();\n  }\n#endif\n\n#ifdef REALM_USE_PYTHON\n  // c++ task on python processor\n  {\n    Machine::ProcessorQuery pq(machine);\n    pq.only_kind(Processor::PY_PROC);\n    for(Machine::ProcessorQuery::iterator it = pq.begin(); it != pq.end(); ++it) {\n      Processor pp = *it;\n\n      Event e = pp.register_task(func_id,\n\t\t\t\t CodeDescriptor(python_cpp_task),\n\t\t\t\t ProfilingRequestSet(),\n\t\t\t\t &pp, sizeof(pp));\n\n      Event e2 = pp.spawn(func_id, 0, 0, e);\n\n      e2.wait();\n    }\n  }\n#endif\n\n  log_app.print() << \"all done!\";\n}\n\nint main(int argc, char **argv)\n{\n#ifdef REALM_USE_PYTHON\n  // do this before any threads are spawned\n  setenv(\"PYTHONPATH\", \".\", true /*overwrite*/);\n#endif\n\n  Runtime rt;\n\n  rt.init(&argc, &argv);\n\n  // select a processor to run the top level task on\n  Processor p = Machine::ProcessorQuery(Machine::get_machine())\n    .only_kind(Processor::LOC_PROC)\n    .first();\n  assert(p.exists());\n\n  Event e1 = Processor::register_task_by_kind(p.kind(),\n\t\t\t\t\t      false /*!global*/,\n\t\t\t\t\t      TOP_LEVEL_TASK,\n\t\t\t\t\t      CodeDescriptor(top_level_task),\n\t\t\t\t\t      ProfilingRequestSet());\n\n  // collective launch of a single task - everybody gets the same finish event\n  Event e2 = rt.collective_spawn(p, TOP_LEVEL_TASK, 0, 0, e1);\n\n  // request shutdown once that task is complete\n  rt.shutdown(e2);\n\n  // now sleep this thread until that shutdown actually happens\n  rt.wait_for_shutdown();\n  \n  return 0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/examples/dynamic_registration/dynamic_registration.cc": "/* Copyright 2021 Stanford University\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n\n#include <cstdio>\n#include <cassert>\n#include <cstdlib>\n#define LEGION_ENABLE_C_BINDINGS\n#include \"legion.h\"\n#include \"mappers/default_mapper.h\"\n\nusing namespace Legion;\n\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit.h\"\n#endif\n\n/*\n * In this example we illustrate how the Legion\n * programming model supports multiple partitions\n * of the same logical region and the benefits it\n * provides by allowing multiple views onto the\n * same logical region.  We compute a simple 5-point\n * 1D stencil using the standard forumala:\n * f'(x) = (-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h))/12h\n * For simplicity we'll assume h=1.\n */\n\nenum TaskIDs {\n  TOP_LEVEL_TASK_ID,\n  INIT_FIELD_TASK_ID,\n  STENCIL_TASK_ID,\n  CHECK_TASK_ID,\n  WRAPPED_CPP_TASK_ID,\n  WRAPPED_C_TASK_ID,\n  CLASS_METHOD_TASK_ID,\n#ifdef REALM_USE_LLVM\n  WRAPPED_LLVM_TASK_ID,\n#endif\n};\n\nenum FieldIDs {\n  FID_VAL,\n  FID_DERIV,\n};\n\n// Forward declarations\n\nvoid init_field_task(const Task *task,\n                     const std::vector<PhysicalRegion> &regions,\n                     Context ctx, Runtime *runtime);\n\nvoid stencil_task(const Task *task,\n                  const std::vector<PhysicalRegion> &regions,\n                  Context ctx, Runtime *runtime);\n\nvoid check_task(const Task *task,\n                const std::vector<PhysicalRegion> &regions,\n                Context ctx, Runtime *runtime);\n\nvoid wrapped_cpp_task(const void *data, size_t datalen,\n\t\t      const void *userdata, size_t userlen, Processor p)\n{\n  const Task *task;\n  const std::vector<PhysicalRegion> *regions;\n  Context ctx;\n  Runtime *runtime;\n  LegionTaskWrapper::legion_task_preamble(data, datalen, p,\n\t\t\t\t\t  task,\n\t\t\t\t\t  regions,\n\t\t\t\t\t  ctx,\n\t\t\t\t\t  runtime);\n  printf(\"hello from wrapped_cpp_task (msg='%.*s')\\n\",\n\t (int)userlen, (const char *)userdata);\n  LegionTaskWrapper::legion_task_postamble(runtime, ctx);\n}\n\nvoid wrapped_c_task(const void *data, size_t datalen,\n\t\t    const void *userdata, size_t userlen, Processor p)\n{\n  legion_task_t task;\n  const legion_physical_region_t *regions;\n  unsigned num_regions;\n  legion_context_t ctx;\n  legion_runtime_t runtime;\n  legion_task_preamble(data, datalen, p.id,\n\t\t       &task,\n\t\t       &regions,\n\t\t       &num_regions,\n\t\t       &ctx,\n\t\t       &runtime);\n  printf(\"hello from wrapped_c_task (msg='%.*s')\\n\",\n\t (int)userlen, (const char *)userdata);\n  legion_task_postamble(runtime, ctx, 0, 0);\n}\n\nclass ClassWithTaskMethods {\npublic:\n  ClassWithTaskMethods(int _x) : x(_x) {}\n\n  void method_task(const Task *task,\n\t\t   const std::vector<PhysicalRegion> &regions,\n\t\t   Context ctx, Runtime *runtime)\n  {\n    printf(\"hello from class method: this=%p x=%d\\n\", this, x);\n  }\n\n  static void static_entry_method(const Task *task,\n\t\t\t\t  const std::vector<PhysicalRegion> &regions,\n\t\t\t\t  Context ctx, Runtime *runtime,\n\t\t\t\t  ClassWithTaskMethods * const & _this)\n  {\n    // just call through to actual class method\n    _this->method_task(task, regions, ctx, runtime);\n  }\n\nprotected:\n  int x;\n};\n\n#ifdef REALM_USE_LLVM\nconst char llvm_ir[] = \n  \"%struct.legion_physical_region_t = type { i8* }\\n\"\n  \"%struct.legion_task_t = type { i8* }\\n\"\n  \"%struct.legion_context_t = type { i8* }\\n\"\n  \"%struct.legion_runtime_t = type { i8* }\\n\"\n  \"declare i32 @printf(i8*, ...)\\n\"\n  \"declare void @legion_task_preamble(i8*, i64, i64, %struct.legion_task_t*, %struct.legion_physical_region_t**, i32*, %struct.legion_context_t*, %struct.legion_runtime_t*)\\n\"\n  \"declare void @legion_task_postamble(%struct.legion_runtime_t, %struct.legion_context_t, i8*, i64)\\n\"\n  \"@.str = private unnamed_addr constant [31 x i8] c\\\"hello from llvm wrapped task!\\\\0A\\\\00\\\", align 1\\n\"\n  \"define void @body(%struct.legion_task_t %task, %struct.legion_physical_region_t* %regions, i32 %num_regions, %struct.legion_context_t %ctx, %struct.legion_runtime_t %runtime) {\\n\"\n  \"  %1 = bitcast [31 x i8]* @.str to i8*\\n\"\n#if REALM_LLVM_VERSION >= 37\n  \"  %2 = call i32 (i8*, ...) @printf(i8* %1)\\n\"\n#else\n  \"  %2 = call i32 (i8*, ...)* @printf(i8* %1)\\n\"\n#endif\n  \"  ret void\\n\"\n  \"}\\n\"\n  \"define void @llvm_wrapper(i8* %data, i64 %datalen, i8* %userdata, i64 %userlen, i64 %proc_id) {\\n\"\n  \"  %task_ptr = alloca %struct.legion_task_t, align 8\\n\"\n  \"  %regions_ptr = alloca %struct.legion_physical_region_t*, align 8\\n\"\n  \"  %num_regions_ptr = alloca i32, align 4\\n\"\n  \"  %ctx_ptr = alloca %struct.legion_context_t, align 8\\n\"\n  \"  %runtime_ptr = alloca %struct.legion_runtime_t, align 8\\n\"\n  \"  call void @legion_task_preamble(i8* %data, i64 %datalen, i64 %proc_id, %struct.legion_task_t* %task_ptr, %struct.legion_physical_region_t** %regions_ptr, i32* %num_regions_ptr, %struct.legion_context_t* %ctx_ptr, %struct.legion_runtime_t* %runtime_ptr)\\n\"\n#if REALM_LLVM_VERSION >= 37\n  \"  %task = load %struct.legion_task_t, %struct.legion_task_t* %task_ptr\\n\"\n  \"  %regions = load %struct.legion_physical_region_t*, %struct.legion_physical_region_t** %regions_ptr\\n\"\n  \"  %num_regions = load i32, i32* %num_regions_ptr\\n\"\n  \"  %ctx = load %struct.legion_context_t, %struct.legion_context_t* %ctx_ptr\\n\"\n  \"  %runtime = load %struct.legion_runtime_t, %struct.legion_runtime_t* %runtime_ptr\\n\"\n#else\n  \"  %task = load %struct.legion_task_t* %task_ptr\\n\"\n  \"  %regions = load %struct.legion_physical_region_t** %regions_ptr\\n\"\n  \"  %num_regions = load i32* %num_regions_ptr\\n\"\n  \"  %ctx = load %struct.legion_context_t* %ctx_ptr\\n\"\n  \"  %runtime = load %struct.legion_runtime_t* %runtime_ptr\\n\"\n#endif\n  \"  call void @body(%struct.legion_task_t %task, %struct.legion_physical_region_t* %regions, i32 %num_regions, %struct.legion_context_t %ctx, %struct.legion_runtime_t %runtime)\\n\"\n  \"  call void @legion_task_postamble(%struct.legion_runtime_t %runtime, %struct.legion_context_t %ctx, i8* null, i64 0)\\n\"\n  \"  ret void\\n\"\n  \"}\\n\"\n  ;\n#endif\n\nvoid top_level_task(const Task *task,\n                    const std::vector<PhysicalRegion> &regions,\n                    Context ctx, Runtime *runtime)\n{\n  FieldSpace fs = runtime->create_field_space(ctx);\n  {\n    FieldAllocator allocator = \n      runtime->create_field_allocator(ctx, fs);\n    allocator.allocate_field(sizeof(double),FID_VAL);\n    allocator.allocate_field(sizeof(double),FID_DERIV);\n  }\n  // Make an SOA constraint and use it as the layout constraint for\n  // all the different task variants that we are registering\n  LayoutConstraintRegistrar layout_registrar(fs, \"SOA layout\");\n  std::vector<DimensionKind> dim_order(2);\n  dim_order[0] = DIM_X;\n  dim_order[1] = DIM_F; // fields go last for SOA\n  layout_registrar.add_constraint(OrderingConstraint(dim_order, false/*contig*/));\n\n  LayoutConstraintID soa_layout_id = runtime->register_layout(layout_registrar);\n\n#ifdef REALM_USE_LIBDL\n  // rely on dladdr/dlsym to make function pointers portable for global\n  //  task registration\n  bool global_taskreg = true;\n#else\n  // function pointers will not be portable, so limit tasks to local node\n  const bool global_taskreg = false;\n#endif\n\n  // Dynamically register some more tasks\n  TaskVariantRegistrar init_registrar(INIT_FIELD_TASK_ID,\n                                      \"cpu_init_variant\",\n\t\t\t\t      global_taskreg);\n  // Add our constraints\n  init_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC))\n      .add_layout_constraint_set(0/*index*/, soa_layout_id);\n  runtime->register_task_variant<init_field_task>(init_registrar);\n\n  TaskVariantRegistrar stencil_registrar(STENCIL_TASK_ID,\n                                         \"cpu_stencil_variant\",\n\t\t\t\t\t global_taskreg);\n  stencil_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC))\n      .add_layout_constraint_set(0/*index*/, soa_layout_id)\n      .add_layout_constraint_set(1/*index*/, soa_layout_id);\n  runtime->register_task_variant<stencil_task>(stencil_registrar);\n\n  TaskVariantRegistrar check_registrar(CHECK_TASK_ID,\n                                       \"cpu_check_variant\",\n\t\t\t\t       global_taskreg);\n  check_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC))\n      .add_layout_constraint_set(0/*index*/, soa_layout_id)\n      .add_layout_constraint_set(1/*index*/, soa_layout_id);\n  runtime->register_task_variant<check_task>(check_registrar);\n\n  TaskVariantRegistrar wrapped_cpp_registrar(WRAPPED_CPP_TASK_ID,\n\t\t\t\t\t     \"wrapped_cpp_variant\",\n\t\t\t\t\t     global_taskreg);\n  wrapped_cpp_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  const char cpp_msg[] = \"user data for cpp task\";\n  runtime->register_task_variant(wrapped_cpp_registrar,\n\t\t\t\t CodeDescriptor(wrapped_cpp_task),\n\t\t\t\t cpp_msg, sizeof(cpp_msg));\n\n  TaskVariantRegistrar wrapped_c_registrar(WRAPPED_C_TASK_ID,\n\t\t\t\t\t   \"wrapped_c_variant\",\n\t\t\t\t\t   global_taskreg);\n  wrapped_c_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  const char c_msg[] = \"user data for c task\";\n  runtime->register_task_variant(wrapped_c_registrar,\n\t\t\t\t CodeDescriptor(wrapped_c_task),\n\t\t\t\t c_msg, sizeof(c_msg));\n\n  ClassWithTaskMethods object_with_task_methods(22);\n  TaskVariantRegistrar class_method_registrar(CLASS_METHOD_TASK_ID,\n\t\t\t\t\t      \"class_method_variant\",\n\t\t\t\t\t      false /*can't be global*/);\n  class_method_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  runtime->register_task_variant<ClassWithTaskMethods *,\n\t\t\t\t ClassWithTaskMethods::static_entry_method>\n    (class_method_registrar,\n     &object_with_task_methods /*pointer to object passed as 'user_data'*/);\n\n#ifdef REALM_USE_LLVM\n  // LLVM IR is portable, so we can do global registration even without libdl\n  TaskVariantRegistrar wrapped_llvm_registrar(WRAPPED_LLVM_TASK_ID,\n\t\t\t\t\t      \"wrapped_llvm_variant\",\n\t\t\t\t\t      true /*global*/);\n  wrapped_llvm_registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  const char llvm_msg[] = \"user data for llvm task\";\n  CodeDescriptor llvm_cd(Realm::Type::from_cpp_type<Processor::TaskFuncPtr>());\n  llvm_cd.add_implementation(new Realm::LLVMIRImplementation(llvm_ir, sizeof(llvm_ir),\n\t\t\t\t\t\t\t     \"llvm_wrapper\"));\n  runtime->register_task_variant(wrapped_llvm_registrar,\n\t\t\t\t llvm_cd,\n\t\t\t\t llvm_msg, sizeof(llvm_msg));\n#endif\n\n  // Attach semantic infos to the task names\n  runtime->attach_name(INIT_FIELD_TASK_ID, \"init task\");\n  runtime->attach_name(STENCIL_TASK_ID, \"stencil task\");\n  runtime->attach_name(CHECK_TASK_ID, \"check task\");\n  runtime->attach_name(WRAPPED_CPP_TASK_ID, \"wrapped cpp task\");\n  runtime->attach_name(WRAPPED_C_TASK_ID, \"wrapped c task\");\n#ifdef REALM_USE_LLVM\n  runtime->attach_name(WRAPPED_LLVM_TASK_ID, \"wrapped llvm task\");\n#endif\n\n  {\n    int val = 55;\n    TaskLauncher l(WRAPPED_CPP_TASK_ID, TaskArgument(&val, sizeof(val)));\n    if (!global_taskreg)\n      l.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n\n  {\n    int val = 66;\n    TaskLauncher l(WRAPPED_C_TASK_ID, TaskArgument(&val, sizeof(val)));\n    if (!global_taskreg)\n      l.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n\n#ifdef REALM_USE_LLVM\n  {\n    int val = 77;\n    TaskLauncher l(WRAPPED_LLVM_TASK_ID, TaskArgument(&val, sizeof(val)));\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n#endif\n\n  {\n    int val = 88;\n    TaskLauncher l(CLASS_METHOD_TASK_ID, TaskArgument(&val, sizeof(val)));\n    // task uses locally allocated object, so must stay local\n    l.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n    Future f = runtime->execute_task(ctx, l);\n    f.get_void_result();\n  }\n\n  int num_elements = 1024;\n  int num_subregions = 4;\n  // Check for any command line arguments\n  {\n      const InputArgs &command_args = Runtime::get_input_args();\n    for (int i = 1; i < command_args.argc; i++)\n    {\n      if (!strcmp(command_args.argv[i],\"-n\"))\n        num_elements = atoi(command_args.argv[++i]);\n      if (!strcmp(command_args.argv[i],\"-b\"))\n        num_subregions = atoi(command_args.argv[++i]);\n    }\n  }\n  printf(\"Running stencil computation for %d elements...\\n\", num_elements);\n  printf(\"Partitioning data into %d sub-regions...\\n\", num_subregions);\n\n  Rect<1> elem_rect(0,num_elements-1);\n  IndexSpaceT<1> is = runtime->create_index_space(ctx, elem_rect);\n  LogicalRegion stencil_lr = runtime->create_logical_region(ctx, is, fs);\n  \n  Rect<1> color_bounds(0,num_subregions-1);\n  IndexSpaceT<1> color_is = runtime->create_index_space(ctx, color_bounds);\n\n  IndexPartition disjoint_ip = \n    runtime->create_equal_partition(ctx, is, color_is);\n  const int block_size = (num_elements + num_subregions - 1) / num_subregions;\n  Transform<1,1> transform;\n  transform[0][0] = block_size;\n  Rect<1> extent(-2, block_size + 1);\n  IndexPartition ghost_ip = \n    runtime->create_partition_by_restriction(ctx, is, color_is, transform, extent);\n\n  LogicalPartition disjoint_lp = \n    runtime->get_logical_partition(ctx, stencil_lr, disjoint_ip);\n  LogicalPartition ghost_lp = \n    runtime->get_logical_partition(ctx, stencil_lr, ghost_ip);\n\n  ArgumentMap arg_map;\n\n  IndexLauncher init_launcher(INIT_FIELD_TASK_ID, color_is,\n                              TaskArgument(NULL, 0), arg_map);\n  if (!global_taskreg)\n    init_launcher.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n  init_launcher.add_region_requirement(\n      RegionRequirement(disjoint_lp, 0/*projection ID*/,\n                        WRITE_DISCARD, EXCLUSIVE, stencil_lr));\n  init_launcher.add_field(0, FID_VAL);\n  runtime->execute_index_space(ctx, init_launcher);\n\n  IndexLauncher stencil_launcher(STENCIL_TASK_ID, color_is,\n       TaskArgument(&num_elements, sizeof(num_elements)), arg_map);\n  if (!global_taskreg)\n    stencil_launcher.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n  stencil_launcher.add_region_requirement(\n      RegionRequirement(ghost_lp, 0/*projection ID*/,\n                        READ_ONLY, EXCLUSIVE, stencil_lr));\n  stencil_launcher.add_field(0, FID_VAL);\n  stencil_launcher.add_region_requirement(\n      RegionRequirement(disjoint_lp, 0/*projection ID*/,\n                        WRITE_DISCARD, EXCLUSIVE, stencil_lr));\n  stencil_launcher.add_field(1, FID_DERIV);\n  runtime->execute_index_space(ctx, stencil_launcher);\n\n  TaskLauncher check_launcher(CHECK_TASK_ID, \n      TaskArgument(&num_elements, sizeof(num_elements)));\n  if (!global_taskreg)\n    check_launcher.tag |= Legion::Mapping::DefaultMapper::SAME_ADDRESS_SPACE;\n  check_launcher.add_region_requirement(\n      RegionRequirement(stencil_lr, READ_ONLY, EXCLUSIVE, stencil_lr));\n  check_launcher.add_field(0, FID_VAL);\n  check_launcher.add_region_requirement(\n      RegionRequirement(stencil_lr, READ_ONLY, EXCLUSIVE, stencil_lr));\n  check_launcher.add_field(1, FID_DERIV);\n  runtime->execute_task(ctx, check_launcher);\n\n  runtime->destroy_logical_region(ctx, stencil_lr);\n  runtime->destroy_field_space(ctx, fs);\n  runtime->destroy_index_space(ctx, is);\n}\n\n// The standard initialize field task from earlier examples\nvoid init_field_task(const Task *task,\n                     const std::vector<PhysicalRegion> &regions,\n                     Context ctx, Runtime *runtime)\n{\n  assert(regions.size() == 1); \n  assert(task->regions.size() == 1);\n  assert(task->regions[0].privilege_fields.size() == 1);\n\n  FieldID fid = *(task->regions[0].privilege_fields.begin());\n  const int point = task->index_point.point_data[0];\n  printf(\"Initializing field %d for block %d...\\n\", fid, point);\n\n  const FieldAccessor<WRITE_DISCARD,double,1> acc(regions[0], fid);\n\n  Rect<1> rect = runtime->get_index_space_domain(ctx,\n                  task->regions[0].region.get_index_space());\n  for (PointInRectIterator<1> pir(rect); pir(); pir++)\n    acc[*pir] = drand48();\n}\n\n// Our stencil tasks is interesting because it\n// has both slow and fast versions depending\n// on whether or not its bounds have been clamped.\nvoid stencil_task(const Task *task,\n                  const std::vector<PhysicalRegion> &regions,\n                  Context ctx, Runtime *runtime)\n{\n  assert(regions.size() == 2);\n  assert(task->regions.size() == 2);\n  assert(task->regions[0].privilege_fields.size() == 1);\n  assert(task->regions[1].privilege_fields.size() == 1);\n  assert(task->arglen == sizeof(int));\n  const int max_elements = *((const int*)task->args);\n  const int point = task->index_point.point_data[0];\n  \n  FieldID read_fid = *(task->regions[0].privilege_fields.begin());\n  FieldID write_fid = *(task->regions[1].privilege_fields.begin());\n\n  const FieldAccessor<READ_ONLY,double,1> read_acc(regions[0], read_fid);\n  const FieldAccessor<WRITE_DISCARD,double,1> write_acc(regions[1], write_fid);\n\n  Rect<1> rect = runtime->get_index_space_domain(ctx,\n                  task->regions[1].region.get_index_space());\n  // If we are on the edges of the entire space we are \n  // operating over, then we're going to do the slow\n  // path which checks for clamping when necessary.\n  // If not, then we can do the fast path without\n  // any checks.\n  if ((rect.lo[0] < 2) || (rect.hi[0] > (max_elements-3)))\n  {\n    printf(\"Running slow stencil path for point %d...\\n\", point);\n    // Note in the slow path that there are checks which\n    // perform clamps when necessary before reading values.\n    for (PointInRectIterator<1> pir(rect); pir(); pir++)\n    {\n      double l2, l1, r1, r2;\n      if (pir[0] < 2)\n        l2 = read_acc[0];\n      else\n        l2 = read_acc[*pir - 2];\n      if (pir[0] < 1)\n        l1 = read_acc[0];\n      else\n        l1 = read_acc[*pir - 1];\n      if (pir[0] > (max_elements-2))\n        r1 = read_acc[max_elements-1];\n      else\n        r1 = read_acc[*pir + 1];\n      if (pir[0] > (max_elements-3))\n        r2 = read_acc[max_elements-1];\n      else\n        r2 = read_acc[*pir + 2];\n      \n      double result = (-l2 + 8.0*l1 - 8.0*r1 + r2) / 12.0;\n      write_acc[*pir] = result;\n    }\n  }\n  else\n  {\n    printf(\"Running fast stencil path for point %d...\\n\", point);\n    // In the fast path, we don't need any checks\n    for (PointInRectIterator<1> pir(rect); pir(); pir++)\n    {\n      double l2 = read_acc[*pir - 2];\n      double l1 = read_acc[*pir - 1];\n      double r1 = read_acc[*pir + 1];\n      double r2 = read_acc[*pir + 2];\n\n      double result = (-l2 + 8.0*l1 - 8.0*r1 + r2) / 12.0;\n      write_acc[*pir] = result;\n    }\n  }\n}\n\nvoid check_task(const Task *task,\n                const std::vector<PhysicalRegion> &regions,\n                Context ctx, Runtime *runtime)\n{\n  assert(regions.size() == 2);\n  assert(task->regions.size() == 2);\n  assert(task->regions[0].privilege_fields.size() == 1);\n  assert(task->regions[1].privilege_fields.size() == 1);\n  assert(task->arglen == sizeof(int));\n  const int max_elements = *((const int*)task->args);\n\n  FieldID src_fid = *(task->regions[0].privilege_fields.begin());\n  FieldID dst_fid = *(task->regions[1].privilege_fields.begin());\n\n  const FieldAccessor<READ_ONLY,double,1> src_acc(regions[0], src_fid);\n  const FieldAccessor<READ_ONLY,double,1> dst_acc(regions[1], dst_fid);\n\n  Rect<1> rect = runtime->get_index_space_domain(ctx,\n                  task->regions[1].region.get_index_space());\n\n  // This is the checking task so we can just do the slow path\n  bool all_passed = true;\n  for (PointInRectIterator<1> pir(rect); pir(); pir++)\n  {\n    double l2, l1, r1, r2;\n    if (pir[0] < 2)\n      l2 = src_acc[0];\n    else\n      l2 = src_acc[*pir - 2];\n    if (pir[0] < 1)\n      l1 = src_acc[0];\n    else\n      l1 = src_acc[*pir - 1];\n    if (pir[0] > (max_elements-2))\n      r1 = src_acc[max_elements-1];\n    else\n      r1 = src_acc[*pir + 1];\n    if (pir[0] > (max_elements-3))\n      r2 = src_acc[max_elements-1];\n    else\n      r2 = src_acc[*pir + 2];\n    \n    double expected = (-l2 + 8.0*l1 - 8.0*r1 + r2) / 12.0;\n    double received = dst_acc[*pir];\n    // Probably shouldn't bitwise compare floating point\n    // numbers but the order of operations are the same so they\n    // should be bitwise equal.\n    if (expected != received)\n      all_passed = false;\n  }\n  if (all_passed)\n    printf(\"SUCCESS!\\n\");\n  else\n    printf(\"FAILURE!\\n\");\n}\n\nint main(int argc, char **argv)\n{\n  Runtime::set_top_level_task_id(TOP_LEVEL_TASK_ID);\n  // We'll only register our top-level task here\n  TaskVariantRegistrar registrar(TOP_LEVEL_TASK_ID,\n                                 \"top_level_variant\");\n  registrar.add_constraint(ProcessorConstraint(Processor::LOC_PROC));\n  Runtime::preregister_task_variant<top_level_task>(registrar,\"top_level_task\");\n\n  return Runtime::start(argc, argv);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/language/src/regent/cudahelper.t": "-- Copyright 2021 Stanford University, Los Alamos National Laboratory\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal ast = require(\"regent/ast\")\nlocal base = require(\"regent/std_base\")\nlocal config = require(\"regent/config\").args()\nlocal data = require(\"common/data\")\nlocal report = require(\"common/report\")\n\nlocal cudahelper = {}\n\n-- Exit early if the user turned off CUDA code generation\n\nif config[\"cuda\"] == 0 then\n  function cudahelper.check_cuda_available()\n    return false\n  end\n  return cudahelper\nend\n\nlocal c = base.c\nlocal ef = terralib.externfunction\nlocal externcall_builtin = terralib.externfunction\nlocal cudapaths = { OSX = \"/usr/local/cuda/lib/libcuda.dylib\";\n                    Linux =  \"libcuda.so\";\n                    Windows = \"nvcuda.dll\"; }\n\n-- #####################################\n-- ## CUDA Hijack API\n-- #################\n\nlocal HijackAPI = terralib.includec(\"regent_cudart_hijack.h\")\n\nstruct fat_bin_t {\n  magic : int,\n  versions : int,\n  data : &opaque,\n  filename : &opaque,\n}\n\n-- #####################################\n-- ## CUDA Device API\n-- #################\n\nlocal struct CUctx_st\nlocal struct CUmod_st\nlocal struct CUlinkState_st\nlocal struct CUfunc_st\nlocal CUdevice = int32\nlocal CUjit_option = uint32\nlocal CU_JIT_ERROR_LOG_BUFFER = 5\nlocal CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES = 6\nlocal CU_JIT_INPUT_PTX = 1\nlocal CU_JIT_TARGET = 9\nlocal DriverAPI = {\n  cuInit = ef(\"cuInit\", {uint32} -> uint32);\n  cuCtxGetCurrent = ef(\"cuCtxGetCurrent\", {&&CUctx_st} -> uint32);\n  cuCtxGetDevice = ef(\"cuCtxGetDevice\",{&int32} -> uint32);\n  cuDeviceGet = ef(\"cuDeviceGet\",{&int32,int32} -> uint32);\n  cuCtxCreate_v2 = ef(\"cuCtxCreate_v2\",{&&CUctx_st,uint32,int32} -> uint32);\n  cuCtxDestroy = ef(\"cuCtxDestroy\",{&CUctx_st} -> uint32);\n  cuDeviceComputeCapability = ef(\"cuDeviceComputeCapability\",\n    {&int32,&int32,int32} -> uint32);\n}\n\nlocal RuntimeAPI = false\ndo\n  if not terralib.cudacompile then\n    function cudahelper.check_cuda_available()\n      return false, \"Terra is built without CUDA support\"\n    end\n  else\n    -- Try to load the CUDA runtime header\n    pcall(function() RuntimeAPI = terralib.includec(\"cuda_runtime.h\") end)\n\n    if RuntimeAPI == nil then\n      function cudahelper.check_cuda_available()\n        return false, \"cuda_runtime.h does not exist in INCLUDE_PATH\"\n      end\n    elseif config[\"offline\"] or config[\"cuda-offline\"] then\n      function cudahelper.check_cuda_available()\n        return true\n      end\n    else\n      local dlfcn = terralib.includec(\"dlfcn.h\")\n      local terra has_symbol(symbol : rawstring)\n        var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n        var has_symbol = dlfcn.dlsym(lib, symbol) ~= [&opaque](0)\n        dlfcn.dlclose(lib)\n        return has_symbol\n      end\n\n      if has_symbol(\"cuInit\") then\n        local r = DriverAPI.cuInit(0)\n        if r == 0 then\n          function cudahelper.check_cuda_available()\n            return true\n          end\n        else\n          function cudahelper.check_cuda_available()\n            return false, \"calling cuInit(0) failed for some reason (CUDA devices might not exist)\"\n          end\n        end\n      else\n        function cudahelper.check_cuda_available()\n          return false, \"the cuInit function is missing (Regent might have been installed without CUDA support)\"\n        end\n      end\n    end\n  end\nend\n\ndo\n  local available, error_message = cudahelper.check_cuda_available()\n  if not available then\n    if config[\"cuda\"] == 1 then\n      print(\"CUDA code generation failed since \" .. error_message)\n      os.exit(-1)\n    else\n      return cudahelper\n    end\n  end\nend\n\n-- Declare the API calls that are deprecated in CUDA SDK 10\n-- TODO: We must move on to the new execution control API as these old functions\n--       can be dropped in the future.\nlocal ExecutionAPI = {\n  cudaConfigureCall =\n    ef(\"cudaConfigureCall\", {RuntimeAPI.dim3, RuntimeAPI.dim3, uint64, RuntimeAPI.cudaStream_t} -> uint32);\n  cudaSetupArgument = ef(\"cudaSetupArgument\", {&opaque, uint64, uint64} -> uint32);\n  cudaLaunch = ef(\"cudaLaunch\", {&opaque} -> uint32);\n}\n\ndo\n  local ffi = require('ffi')\n  local cudaruntimelinked = false\n  function cudahelper.link_driver_library()\n    if cudaruntimelinked then return end\n    local path = assert(cudapaths[ffi.os],\"unknown OS?\")\n    base.linklibrary(path)\n    cudaruntimelinked = true\n  end\nend\n\n-- #####################################\n-- ## Printf for CUDA (not exposed to the user for the moment)\n-- #################\n\nlocal vprintf = ef(\"cudart:vprintf\", {&int8,&int8} -> int)\n\nlocal function createbuffer(args)\n  local Buf = terralib.types.newstruct()\n  for i,e in ipairs(args) do\n    local typ = e:gettype()\n    local field = \"_\"..tonumber(i)\n    typ = typ == float and double or typ\n    table.insert(Buf.entries,{field,typ})\n  end\n  return quote\n    var buf : Buf\n    escape\n        for i,e in ipairs(args) do\n            emit quote\n               buf.[\"_\"..tonumber(i)] = e\n            end\n        end\n    end\n  in\n    [&int8](&buf)\n  end\nend\n\nlocal cuda_printf = macro(function(fmt,...)\n  local buf = createbuffer({...})\n  return `vprintf(fmt,buf)\nend)\n\n-- #####################################\n-- ## Supported CUDA compute versions\n-- #################\n\nlocal supported_archs = {\n  [\"fermi\"]   = 20,\n  [\"kepler\"]  = 30,\n  [\"k20\"]     = 35,\n  [\"maxwell\"] = 52,\n  [\"pascal\"]  = 60,\n  [\"volta\"]   = 70,\n}\n\nlocal function parse_cuda_arch(arch)\n  arch = string.lower(arch)\n  local sm = supported_archs[arch]\n  if sm == nil then\n    local archs\n    for k, v in pairs(supported_archs) do\n      archs = (not archs and k) or (archs and archs .. \", \" .. k)\n    end\n    print(\"Error: Unsupported GPU architecture \" .. arch ..\n          \". Supported architectures: \" .. archs)\n    os.exit(1)\n  end\n  return sm\nend\n\n-- #####################################\n-- ## Registration functions\n-- #################\n\nlocal terra register_ptx(ptxc : rawstring, ptxSize : uint32, version : uint64) : &&opaque\n  var fat_bin : &fat_bin_t\n  var fat_size = sizeof(fat_bin_t)\n  -- TODO: this line is leaking memory\n  fat_bin = [&fat_bin_t](c.malloc(fat_size))\n  base.assert(fat_size == 0 or fat_bin ~= nil, \"malloc failed in register_ptx\")\n  fat_bin.magic = 1234\n  fat_bin.versions = 5678\n  var fat_data_size = ptxSize + 1\n  fat_bin.data = c.malloc(fat_data_size)\n  base.assert(fat_data_size == 0 or fat_bin.data ~= nil, \"malloc failed in register_ptx\")\n  fat_bin.data = ptxc\n  var handle = HijackAPI.hijackCudaRegisterFatBinary(fat_bin)\n  return handle\nend\n\nlocal terra register_function(handle : &&opaque, id : int, name : &int8)\n  HijackAPI.hijackCudaRegisterFunction(handle, [&int8](id), name)\nend\n\nlocal function find_device_library(target)\n  local device_lib_dir = terralib.cudahome .. \"/nvvm/libdevice/\"\n  local libdevice = nil\n  for f in io.popen(\"ls \" .. device_lib_dir):lines() do\n    local version = tonumber(string.match(string.match(f, \"[0-9][0-9][.]\"), \"[0-9][0-9]\"))\n    if version <= target then\n      libdevice = device_lib_dir .. f\n    end\n  end\n  assert(libdevice ~= nil, \"Failed to find a device library\")\n  return libdevice\nend\n\nlocal get_cuda_version\ndo\n  local cached_cuda_version = nil\n  local terra get_cuda_version_terra() : uint64\n    var cx : &CUctx_st\n    var cx_created = false\n    var r = DriverAPI.cuCtxGetCurrent(&cx)\n    base.assert(r == 0, \"CUDA error in cuCtxGetCurrent\")\n    var device : int32\n    if cx ~= nil then\n      r = DriverAPI.cuCtxGetDevice(&device)\n      base.assert(r == 0, \"CUDA error in cuCtxGetDevice\")\n    else\n      r = DriverAPI.cuDeviceGet(&device, 0)\n      base.assert(r == 0, \"CUDA error in cuDeviceGet\")\n      r = DriverAPI.cuCtxCreate_v2(&cx, 0, device)\n      base.assert(r == 0, \"CUDA error in cuCtxCreate_v2\")\n      cx_created = true\n    end\n\n    var major : int, minor : int\n    r = DriverAPI.cuDeviceComputeCapability(&major, &minor, device)\n    base.assert(r == 0, \"CUDA error in cuDeviceComputeCapability\")\n    var version = [uint64](major * 10 + minor)\n    if cx_created then\n      DriverAPI.cuCtxDestroy(cx)\n    end\n    return version\n  end\n\n  get_cuda_version = function()\n    if cached_cuda_version ~= nil then\n      return cached_cuda_version\n    end\n    if not (config[\"offline\"] or config[\"cuda-offline\"]) then\n      cached_cuda_version = get_cuda_version_terra()\n    else\n      cached_cuda_version = parse_cuda_arch(config[\"cuda-arch\"])\n    end\n    return cached_cuda_version\n  end\nend\n\nfunction cudahelper.jit_compile_kernels_and_register(kernels)\n  local module = {}\n  for k, v in pairs(kernels) do\n    module[v.name] = v.kernel\n  end\n  local version = get_cuda_version()\n  local libdevice = find_device_library(tonumber(version))\n  local llvmbc = terralib.linkllvm(libdevice)\n  externcall_builtin = function(name, ftype)\n    return llvmbc:extern(name, ftype)\n  end\n  local ptx = cudalib.toptx(module, nil, version)\n\n  local ptxc = terralib.constant(ptx)\n  local handle = terralib.newsymbol(&&opaque, \"handle\")\n  local register = quote\n    var [handle] = register_ptx(ptxc, [ptx:len() + 1], [version])\n  end\n\n  for k, v in pairs(kernels) do\n    register = quote\n      [register]\n      register_function([handle], [k], [v.name])\n    end\n  end\n\n  return register\nend\n\n-- #####################################\n-- ## Primitives\n-- #################\n\nlocal THREAD_BLOCK_SIZE = 128\nlocal NUM_THREAD_X = 16\nlocal NUM_THREAD_Y = THREAD_BLOCK_SIZE / NUM_THREAD_X\nlocal MAX_NUM_BLOCK = 32768\nlocal GLOBAL_RED_BUFFER = 256\nassert(GLOBAL_RED_BUFFER % THREAD_BLOCK_SIZE == 0)\n\nlocal tid_x   = cudalib.nvvm_read_ptx_sreg_tid_x\nlocal n_tid_x = cudalib.nvvm_read_ptx_sreg_ntid_x\nlocal bid_x   = cudalib.nvvm_read_ptx_sreg_ctaid_x\nlocal n_bid_x = cudalib.nvvm_read_ptx_sreg_nctaid_x\n\nlocal tid_y   = cudalib.nvvm_read_ptx_sreg_tid_y\nlocal n_tid_y = cudalib.nvvm_read_ptx_sreg_ntid_y\nlocal bid_y   = cudalib.nvvm_read_ptx_sreg_ctaid_y\nlocal n_bid_y = cudalib.nvvm_read_ptx_sreg_nctaid_y\n\nlocal tid_z   = cudalib.nvvm_read_ptx_sreg_tid_z\nlocal n_tid_z = cudalib.nvvm_read_ptx_sreg_ntid_z\nlocal bid_z   = cudalib.nvvm_read_ptx_sreg_ctaid_z\nlocal n_bid_z = cudalib.nvvm_read_ptx_sreg_nctaid_z\n\nlocal barrier = cudalib.nvvm_barrier0\n\nlocal supported_scalar_red_ops = {\n  [\"+\"]   = true,\n  [\"*\"]   = true,\n  [\"max\"] = true,\n  [\"min\"] = true,\n}\n\nfunction cudahelper.global_thread_id()\n  local bid = `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\n  local num_threads = `(n_tid_x())\n  return `([bid] * [num_threads] + tid_x())\nend\n\nfunction cudahelper.global_block_id()\n  return `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\nend\n\nfunction cudahelper.get_thread_block_size()\n  return THREAD_BLOCK_SIZE\nend\n\nfunction cudahelper.get_num_thread_x()\n  return NUM_THREAD_X\nend\n\nfunction cudahelper.get_num_thread_y()\n  return NUM_THREAD_Y\nend\n\n-- Slow atomic operation implementations (copied and modified from Ebb)\nlocal terra cas_uint64(address : &uint64, compare : uint64, value : uint64)\n  return terralib.asm(terralib.types.uint64,\n                      \"atom.global.cas.b64 $0, [$1], $2, $3;\",\n                      \"=l,l,l,l\", true, address, compare, value)\nend\ncas_uint64:setinlined(true)\n\nlocal terra cas_uint32(address : &uint32, compare : uint32, value : uint32)\n  return terralib.asm(terralib.types.uint32,\n                      \"atom.global.cas.b32 $0, [$1], $2, $3;\",\n                      \"=r,l,r,r\", true, address, compare, value)\nend\ncas_uint32:setinlined(true)\n\nfunction cudahelper.generate_atomic_update(op, typ)\n  if terralib.llvmversion <= 38 then\n    if op == \"+\" and typ == float then\n      return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f32.p0f32\",\n                                {&float,float} -> {float})\n    elseif op == \"+\" and typ == double and get_cuda_version() >= 60 then\n      return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f64.p0f64\",\n                                {&double,double} -> {double})\n    end\n  end\n\n  local cas_type\n  local cas_func\n  if sizeof(typ) == 4 then\n    cas_type = uint32\n    cas_func = cas_uint32\n  else\n    assert(sizeof(typ) == 8)\n    cas_type = uint64\n    cas_func = cas_uint64\n  end\n  local terra atomic_op(address : &typ, operand : typ)\n    var old : typ = @address\n    var assumed : typ\n    var new     : typ\n\n    var new_b     : &cas_type = [&cas_type](&new)\n    var assumed_b : &cas_type = [&cas_type](&assumed)\n    var res       :  cas_type\n\n    var mask = false\n    repeat\n      if not mask then\n        assumed = old\n        new     = [base.quote_binary_op(op, assumed, operand)]\n        res     = cas_func([&cas_type](address), @assumed_b, @new_b)\n        old     = @[&typ](&res)\n        mask    = @assumed_b == @[&cas_type](&old)\n      end\n    until mask\n  end\n  atomic_op:setinlined(true)\n  return atomic_op\nend\n\nlocal function generate_element_reduction(lhs, rhs, op, volatile)\n  if volatile then\n    return quote\n      do\n        var v = [base.quote_binary_op(op, lhs, rhs)]\n        terralib.attrstore(&[lhs], v, { isvolatile = true })\n      end\n    end\n  else\n    return quote\n      [lhs] = [base.quote_binary_op(op, lhs, rhs)]\n    end\n  end\nend\n\nlocal function generate_element_reductions(lhs, rhs, op, type, volatile)\n  local actions = terralib.newlist()\n  if type:isarray() then\n    for k = 1, type.N do -- inclusive!\n      local lhs = `([lhs][ [k - 1] ])\n      local rhs = `([rhs][ [k - 1] ])\n      actions:insert(generate_element_reduction(lhs, rhs, op, volatile))\n    end\n  else\n    assert(type:isprimitive())\n    actions:insert(generate_element_reduction(lhs, rhs, op, volatile))\n  end\n  return quote [actions] end\nend\n\n-- #####################################\n-- ## Code generation for scalar reduction\n-- #################\n\nfunction cudahelper.compute_reduction_buffer_size(cx, node, reductions)\n  local size = 0\n  for k, v in pairs(reductions) do\n    if not supported_scalar_red_ops[v] then\n      report.error(node,\n          \"Scalar reduction with operator \" .. v .. \" is not supported yet\")\n    elseif not (sizeof(k.type) == 4 or sizeof(k.type) == 8) then\n      report.error(node,\n          \"Scalar reduction for type \" .. tostring(k.type) .. \" is not supported yet\")\n    end\n    size = size + THREAD_BLOCK_SIZE * sizeof(k.type)\n  end\n  size = size + cx:compute_reduction_buffer_size()\n  return size\nend\n\nlocal internal_kernel_id = 2 ^ 30\nlocal internal_kernels = {}\nlocal INTERNAL_KERNEL_PREFIX = \"__internal\"\n\nfunction cudahelper.get_internal_kernels()\n  return internal_kernels\nend\n\ncudahelper.generate_buffer_init_kernel = terralib.memoize(function(type, op)\n  local value = base.reduction_op_init[op][type]\n  local op_name = base.reduction_ops[op].name\n  local kernel_id = internal_kernel_id\n  internal_kernel_id = internal_kernel_id - 1\n  local kernel_name =\n    INTERNAL_KERNEL_PREFIX .. \"__init__\" .. tostring(type) ..\n    \"__\" .. tostring(op_name) .. \"__\"\n  local terra init(buffer : &type)\n    var tid = tid_x() + bid_x() * n_tid_x()\n    buffer[tid] = [value]\n  end\n  init:setname(kernel_name)\n  internal_kernels[kernel_id] = {\n    name = kernel_name,\n    kernel = init,\n  }\n  return kernel_id\nend)\n\ncudahelper.generate_buffer_reduction_kernel = terralib.memoize(function(type, op)\n  local value = base.reduction_op_init[op][type]\n  local op_name = base.reduction_ops[op].name\n  local kernel_id = internal_kernel_id\n  internal_kernel_id = internal_kernel_id - 1\n  local kernel_name =\n    INTERNAL_KERNEL_PREFIX .. \"__red__\" .. tostring(type) ..\n    \"__\" .. tostring(op_name) .. \"__\"\n\n  local tid = terralib.newsymbol(c.size_t, \"tid\")\n  local input = terralib.newsymbol(&type, \"input\")\n  local result = terralib.newsymbol(&type, \"result\")\n  local shared_mem_ptr = cudalib.sharedmemory(type, THREAD_BLOCK_SIZE)\n\n  local shared_mem_init = `([input][ [tid] ])\n  for i = 1, (GLOBAL_RED_BUFFER / THREAD_BLOCK_SIZE) - 1 do\n    shared_mem_init =\n      base.quote_binary_op(op, shared_mem_init,\n                           `([input][ [tid] + [i * THREAD_BLOCK_SIZE] ]))\n  end\n  local terra red([input], [result])\n    var [tid] = tid_x()\n    [shared_mem_ptr][ [tid] ] = [shared_mem_init]\n    barrier()\n    [cudahelper.generate_reduction_tree(tid, shared_mem_ptr, THREAD_BLOCK_SIZE, op, type)]\n    barrier()\n    if [tid] == 0 then [result][0] = [shared_mem_ptr][ [tid] ] end\n  end\n\n  red:setname(kernel_name)\n  internal_kernels[kernel_id] = {\n    name = kernel_name,\n    kernel = red,\n  }\n  return kernel_id\nend)\n\nfunction cudahelper.generate_reduction_preamble(cx, reductions)\n  local preamble = terralib.newlist()\n  local device_ptrs = terralib.newlist()\n  local device_ptrs_map = {}\n  local host_ptrs_map = {}\n\n  for red_var, red_op in pairs(reductions) do\n    local device_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local host_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local init_kernel_id = cudahelper.generate_buffer_init_kernel(red_var.type, red_op)\n    local init_args = terralib.newlist({device_ptr})\n    preamble:insert(quote\n      var [device_ptr] = [&red_var.type](nil)\n      var [host_ptr] = [&red_var.type](nil)\n      do\n        var bounds : c.legion_rect_1d_t\n        bounds.lo.x[0] = 0\n        bounds.hi.x[0] = [sizeof(red_var.type) * GLOBAL_RED_BUFFER - 1]\n        var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.GPU_FB_MEM, [&int8](nil))\n        [device_ptr] =\n          [&red_var.type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n        [cudahelper.codegen_kernel_call(cx, init_kernel_id, GLOBAL_RED_BUFFER, init_args, 0, true)]\n      end\n      do\n        var bounds : c.legion_rect_1d_t\n        bounds.lo.x[0] = 0\n        bounds.hi.x[0] = [sizeof(red_var.type) - 1]\n        var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.Z_COPY_MEM, [&int8](nil))\n        [host_ptr] =\n          [&red_var.type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n      end\n    end)\n    device_ptrs:insert(device_ptr)\n    device_ptrs_map[device_ptr] = red_var\n    host_ptrs_map[device_ptr] = host_ptr\n  end\n\n  return device_ptrs, device_ptrs_map, host_ptrs_map, preamble\nend\n\nfunction cudahelper.generate_reduction_tree(tid, shared_mem_ptr, num_threads, red_op, type)\n  local outer_reductions = terralib.newlist()\n  local step = num_threads\n  while step > 64 do\n    step = step / 2\n    outer_reductions:insert(quote\n      if [tid] < step then\n        [generate_element_reductions(`([shared_mem_ptr][ [tid] ]),\n                                     `([shared_mem_ptr][ [tid] + [step] ]),\n                                     red_op, type, false)]\n      end\n      barrier()\n    end)\n  end\n  local unrolled_reductions = terralib.newlist()\n  while step > 1 do\n    step = step / 2\n    unrolled_reductions:insert(quote\n      [generate_element_reductions(`([shared_mem_ptr][ [tid] ]),\n                                   `([shared_mem_ptr][ [tid] + [step] ]),\n                                   red_op, type, false)]\n      barrier()\n    end)\n  end\n  if #outer_reductions > 0 then\n    return quote\n      [outer_reductions]\n      if [tid] < 32 then\n        [unrolled_reductions]\n      end\n    end\n  else\n    return quote\n      [unrolled_reductions]\n    end\n  end\nend\n\nfunction cudahelper.generate_reduction_kernel(cx, reductions, device_ptrs_map)\n  local preamble = terralib.newlist()\n  local postamble = terralib.newlist()\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local shared_mem_ptr =\n      cudalib.sharedmemory(red_var.type, THREAD_BLOCK_SIZE)\n    local init = base.reduction_op_init[red_op][red_var.type]\n    preamble:insert(quote\n      var [red_var] = [init]\n      [shared_mem_ptr][ tid_x() ] = [red_var]\n    end)\n\n    local tid = terralib.newsymbol(c.size_t, \"tid\")\n    local reduction_tree =\n      cudahelper.generate_reduction_tree(tid, shared_mem_ptr, THREAD_BLOCK_SIZE, red_op, red_var.type)\n    postamble:insert(quote\n      do\n        var [tid] = tid_x()\n        var bid = [cudahelper.global_block_id()]\n        [shared_mem_ptr][ [tid] ] = [red_var]\n        barrier()\n        [reduction_tree]\n        if [tid] == 0 then\n          [cudahelper.generate_atomic_update(red_op, red_var.type)](\n            &[device_ptr][bid % [GLOBAL_RED_BUFFER] ], [shared_mem_ptr][ [tid] ])\n        end\n      end\n    end)\n  end\n\n  preamble:insertall(cx:generate_preamble())\n  postamble:insertall(cx:generate_postamble())\n\n  return preamble, postamble\nend\n\nfunction cudahelper.generate_reduction_postamble(cx, reductions, device_ptrs_map, host_ptrs_map)\n  local postamble = quote end\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local red_kernel_id = cudahelper.generate_buffer_reduction_kernel(red_var.type, red_op)\n    local host_ptr = host_ptrs_map[device_ptr]\n    local red_args = terralib.newlist({device_ptr, host_ptr})\n    local shared_mem_size = terralib.sizeof(red_var.type) * THREAD_BLOCK_SIZE\n    postamble = quote\n      [postamble];\n      [cudahelper.codegen_kernel_call(cx, red_kernel_id, THREAD_BLOCK_SIZE, red_args, shared_mem_size, true)]\n    end\n  end\n\n  local needs_sync = true\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    if needs_sync then\n      postamble = quote\n        [postamble];\n        RuntimeAPI.cudaDeviceSynchronize()\n      end\n      needs_sync = false\n    end\n    local red_op = reductions[red_var]\n    local host_ptr = host_ptrs_map[device_ptr]\n    postamble = quote\n      [postamble];\n      [red_var] = [base.quote_binary_op(red_op, red_var, `([host_ptr][0]))]\n    end\n  end\n\n  return postamble\nend\n\n-- #####################################\n-- ## Code generation for parallel prefix operators\n-- #################\n\nlocal NUM_BANKS = 16\nlocal bank_offset = macro(function(e)\n  return `(e / [NUM_BANKS])\nend)\n\nlocal function generate_prefix_op_kernel(shmem, tid, num_leaves, op, init, left_to_right)\n  return quote\n    do\n      var oa = 2 * [tid] + 1\n      var ob = 2 * [tid] + 2 * [left_to_right]\n      var d : int = [num_leaves] >> 1\n      var offset = 1\n      while d > 0 do\n        barrier()\n        if [tid]  < d then\n          var ai : int = offset * oa - [left_to_right]\n          var bi : int = offset * ob - [left_to_right]\n          ai = ai + bank_offset(ai)\n          bi = bi + bank_offset(bi)\n          [shmem][bi] = [base.quote_binary_op(op, `([shmem][ai]), `([shmem][bi]))]\n        end\n        offset = offset << 1\n        d = d >> 1\n      end\n      if [tid] == 0 then\n        var idx = ([num_leaves] - [left_to_right]) % [num_leaves]\n        [shmem][idx + bank_offset(idx)] = [init]\n      end\n      d = 1\n      while d <= [num_leaves] do\n        offset = offset >> 1\n        barrier()\n        if [tid] < d and offset > 0 then\n          var ai = offset * oa - [left_to_right]\n          var bi = offset * ob - [left_to_right]\n          ai = ai + bank_offset(ai)\n          bi = bi + bank_offset(bi)\n          var x = [shmem][ai]\n          [shmem][ai] = [shmem][bi]\n          [shmem][bi] = [base.quote_binary_op(op, x, `([shmem][bi]))]\n        end\n        d = d << 1\n      end\n      barrier()\n    end\n  end\nend\n\nlocal function generate_prefix_op_prescan(shmem, lhs, rhs, lhs_ptr, rhs_ptr, res, idx, dir, op, init)\n  local prescan_full, prescan_arbitrary\n  local NUM_LEAVES = THREAD_BLOCK_SIZE * 2\n\n  local function advance_ptrs(lhs_ptr, rhs_ptr, bid)\n    if lhs_ptr == rhs_ptr then\n      return quote\n        [lhs_ptr] = &([lhs_ptr][ bid * [NUM_LEAVES] ])\n      end\n    else\n      return quote\n        [lhs_ptr] = &([lhs_ptr][ bid * [NUM_LEAVES] ])\n        [rhs_ptr] = &([rhs_ptr][ bid * [NUM_LEAVES] ])\n      end\n    end\n  end\n\n  terra prescan_full([lhs_ptr],\n                     [rhs_ptr],\n                     [dir])\n    var [idx]\n    var t = tid_x()\n    var bid = [cudahelper.global_block_id()]\n    [advance_ptrs(lhs_ptr, rhs_ptr, bid)]\n    var lr = [int]([dir] >= 0)\n\n    [idx].__ptr = t\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n    [idx].__ptr = [idx].__ptr + [THREAD_BLOCK_SIZE]\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n\n    [generate_prefix_op_kernel(shmem, t, NUM_LEAVES, op, init, lr)]\n\n    var [res]\n    [idx].__ptr = t\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n    [idx].__ptr = [idx].__ptr + [THREAD_BLOCK_SIZE]\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n  end\n\n  terra prescan_arbitrary([lhs_ptr],\n                          [rhs_ptr],\n                          num_elmts : c.size_t,\n                          num_leaves : c.size_t,\n                          [dir])\n    var [idx]\n    var t = tid_x()\n    var lr = [int]([dir] >= 0)\n\n    [idx].__ptr = t\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n    [idx].__ptr = [idx].__ptr + (num_leaves / 2)\n    if [idx].__ptr < num_elmts then\n      [rhs.actions]\n      [shmem][ [idx].__ptr + bank_offset([idx].__ptr) ] = [rhs.value]\n    else\n      [shmem][ [idx].__ptr + bank_offset([idx].__ptr) ] = [init]\n    end\n\n    [generate_prefix_op_kernel(shmem, t, num_leaves, op, init, lr)]\n\n    var [res]\n    [idx].__ptr = t\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op,\n        `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n    [idx].__ptr = [idx].__ptr + (num_leaves / 2)\n    if [idx].__ptr < num_elmts then\n      [rhs.actions]\n      [res] = [base.quote_binary_op(op,\n          `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n      [lhs.actions]\n    end\n  end\n\n  return prescan_full, prescan_arbitrary\nend\n\nlocal function generate_prefix_op_scan(shmem, lhs_wr, lhs_rd, lhs_ptr, res, idx, dir, op, init)\n  local scan_full, scan_arbitrary\n  local NUM_LEAVES = THREAD_BLOCK_SIZE * 2\n\n  terra scan_full([lhs_ptr],\n                  offset : uint64,\n                  [dir])\n    var [idx]\n    var t = tid_x()\n    var bid = [cudahelper.global_block_id()]\n    [lhs_ptr] = &([lhs_ptr][ bid * [offset] * [NUM_LEAVES] ])\n    var lr = [int]([dir] >= 0)\n\n    var tidx = t\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n    tidx = tidx + [THREAD_BLOCK_SIZE]\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n\n    [generate_prefix_op_kernel(shmem, t, NUM_LEAVES, op, init, lr)]\n\n    var [res]\n    tidx = t\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n    [lhs_wr.actions]\n    tidx = tidx + [THREAD_BLOCK_SIZE]\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n    [lhs_wr.actions]\n  end\n\n  terra scan_arbitrary([lhs_ptr],\n                       num_elmts : c.size_t,\n                       num_leaves : c.size_t,\n                       offset : c.size_t,\n                       [dir])\n    var [idx]\n    var t = tid_x()\n    var lr = [int]([dir] >= 0)\n\n    if lr == 1 then\n      var tidx = t\n      [idx].__ptr = (tidx + 1) * [offset] - 1\n      [lhs_rd.actions]\n      [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n\n      tidx = t + num_leaves / 2\n      if tidx < [num_elmts] then\n        [idx].__ptr = (tidx + 1) * [offset] - 1\n        [lhs_rd.actions]\n        [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      else\n        [shmem][tidx + bank_offset(tidx)] = [init]\n      end\n    else\n      var tidx = t\n      [idx].__ptr = tidx * [offset]\n      [lhs_rd.actions]\n      [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      tidx = t + num_leaves / 2\n      if tidx < [num_elmts] then\n        [idx].__ptr = tidx * [offset]\n        [lhs_rd.actions]\n        [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      else\n        [shmem][tidx + bank_offset(tidx)] = [init]\n      end\n    end\n\n    [generate_prefix_op_kernel(shmem, t, num_leaves, op, init, lr)]\n\n    var [res]\n    if lr == 1 then\n      var tidx = t\n      [idx].__ptr = (tidx + 1) * [offset] - 1\n      [lhs_rd.actions]\n      [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n      [lhs_wr.actions]\n      tidx = tidx + num_leaves / 2\n      if [tidx] < [num_elmts] then\n        [idx].__ptr = (tidx + 1) * [offset] - 1\n        [lhs_rd.actions]\n        [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n        [lhs_wr.actions]\n      end\n    else\n      var tidx = t\n      [idx].__ptr = tidx * [offset]\n      [lhs_rd.actions]\n      [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n      [lhs_wr.actions]\n      tidx = tidx + num_leaves / 2\n      if [tidx] < [num_elmts] then\n        [idx].__ptr = tidx * [offset]\n        [lhs_rd.actions]\n        [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n        [lhs_wr.actions]\n      end\n    end\n  end\n\n  return scan_full, scan_arbitrary\nend\n\n-- This function expects lhs and rhs to be the values from the following expressions.\n--\n--   * lhs: lhs[idx] = res\n--   * rhs: rhs[idx]\n--\n-- The code generator below captures 'idx' and 'res' to change the meaning of these values\nfunction cudahelper.generate_prefix_op_kernels(lhs_wr, lhs_rd, rhs, lhs_ptr, rhs_ptr,\n                                               res, idx, dir, op, elem_type)\n  local BLOCK_SIZE = THREAD_BLOCK_SIZE * 2\n  local shmem = cudalib.sharedmemory(elem_type, BLOCK_SIZE)\n  local init = base.reduction_op_init[op][elem_type]\n\n  local prescan_full, prescan_arbitrary =\n    generate_prefix_op_prescan(shmem, lhs_wr, rhs, lhs_ptr, rhs_ptr, res, idx, dir, op, init)\n\n  local scan_full, scan_arbitrary =\n    generate_prefix_op_scan(shmem, lhs_wr, lhs_rd, lhs_ptr, res, idx, dir, op, init)\n\n  local terra postscan_full([lhs_ptr],\n                            offset : uint64,\n                            num_elmts : uint64,\n                            [dir])\n    var t = [cudahelper.global_thread_id()]\n    if t >= num_elmts - [BLOCK_SIZE] or t % [BLOCK_SIZE] == [BLOCK_SIZE - 1] then return end\n\n    var sum_loc = t / [BLOCK_SIZE] * [BLOCK_SIZE] + [BLOCK_SIZE - 1]\n    var val_loc = t + [BLOCK_SIZE]\n    var [idx], [res]\n    if [dir] >= 0 then\n      [idx].__ptr = sum_loc * [offset] + ([offset] - 1)\n      [lhs_rd.actions]\n      var v1 = [lhs_rd.value]\n\n      [idx].__ptr = val_loc * [offset] + ([offset] - 1)\n      [lhs_rd.actions]\n      var v2 = [lhs_rd.value]\n\n      [res] = [base.quote_binary_op(op, v1, v2)]\n      [lhs_wr.actions]\n    else\n      var t = [cudahelper.global_thread_id()]\n      if t % [BLOCK_SIZE] == [BLOCK_SIZE - 1] then return end\n\n      [idx].__ptr = (num_elmts - 1 - sum_loc) * [offset]\n      [lhs_rd.actions]\n      var v1 = [lhs_rd.value]\n\n      [idx].__ptr = (num_elmts - 1 - val_loc) * [offset]\n      [lhs_rd.actions]\n      var v2 = [lhs_rd.value]\n\n      [res] = [base.quote_binary_op(op, v1, v2)]\n      [lhs_wr.actions]\n    end\n  end\n\n  return prescan_full, prescan_arbitrary, scan_full, scan_arbitrary, postscan_full\nend\n\nfunction cudahelper.generate_parallel_prefix_op(cx, variant, total, lhs_wr, lhs_rd, rhs, lhs_ptr,\n                                                rhs_ptr, res, idx, dir, op, elem_type)\n  local BLOCK_SIZE = THREAD_BLOCK_SIZE * 2\n  local SHMEM_SIZE = terralib.sizeof(elem_type) * THREAD_BLOCK_SIZE * 2\n\n  local pre_full, pre_arb, scan_full, scan_arb, post_full, post2, post3 =\n    cudahelper.generate_prefix_op_kernels(lhs_wr, lhs_rd, rhs, lhs_ptr, rhs_ptr,\n                                          res, idx, dir, op, elem_type)\n  local prescan_full_id = variant:add_cuda_kernel(pre_full)\n  local prescan_arb_id = variant:add_cuda_kernel(pre_arb)\n  local scan_full_id = variant:add_cuda_kernel(scan_full)\n  local scan_arb_id = variant:add_cuda_kernel(scan_arb)\n  local postscan_full_id = variant:add_cuda_kernel(post_full)\n\n  local num_leaves = terralib.newsymbol(c.size_t, \"num_leaves\")\n  local num_elmts = terralib.newsymbol(c.size_t, \"num_elmts\")\n  local num_threads = terralib.newsymbol(c.size_t, \"num_threads\")\n  local offset = terralib.newsymbol(uint64, \"offset\")\n  local lhs_ptr_arg = terralib.newsymbol(lhs_ptr.type, lhs_ptr.name)\n  local rhs_ptr_arg = terralib.newsymbol(rhs_ptr.type, rhs_ptr.name)\n\n  local prescan_full_args = terralib.newlist()\n  prescan_full_args:insertall({lhs_ptr_arg, rhs_ptr_arg, dir})\n  local call_prescan_full =\n    cudahelper.codegen_kernel_call(cx, prescan_full_id, num_threads, prescan_full_args, SHMEM_SIZE, true)\n\n  local prescan_arb_args = terralib.newlist()\n  prescan_arb_args:insertall({lhs_ptr_arg, rhs_ptr_arg, num_elmts, num_leaves, dir})\n  local call_prescan_arbitrary =\n    cudahelper.codegen_kernel_call(cx, prescan_arb_id, num_threads, prescan_arb_args, SHMEM_SIZE, true)\n\n  local scan_full_args = terralib.newlist()\n  scan_full_args:insertall({lhs_ptr_arg, offset, dir})\n  local call_scan_full =\n    cudahelper.codegen_kernel_call(cx, scan_full_id, num_threads, scan_full_args, SHMEM_SIZE, true)\n\n  local scan_arb_args = terralib.newlist()\n  scan_arb_args:insertall({lhs_ptr_arg, num_elmts, num_leaves, offset, dir})\n  local call_scan_arbitrary =\n    cudahelper.codegen_kernel_call(cx, scan_arb_id, num_threads, scan_arb_args, SHMEM_SIZE, true)\n\n  local postscan_full_args = terralib.newlist()\n  postscan_full_args:insertall({lhs_ptr, offset, num_elmts, dir})\n  local call_postscan_full =\n    cudahelper.codegen_kernel_call(cx, postscan_full_id, num_threads, postscan_full_args, 0, true)\n\n  local terra recursive_scan :: {uint64,uint64,uint64,lhs_ptr.type,dir.type} -> {}\n\n  terra recursive_scan(remaining : uint64,\n                       [offset],\n                       [total],\n                       [lhs_ptr],\n                       [dir])\n    if remaining <= 1 then return end\n\n    var num_blocks : uint64 = remaining / [BLOCK_SIZE]\n\n    if num_blocks > 0 then\n      var [num_threads] = num_blocks * [THREAD_BLOCK_SIZE]\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = [lhs_ptr]\n      else\n        [lhs_ptr_arg] = &[lhs_ptr][(remaining % [BLOCK_SIZE]) * [offset]]\n      end\n      [call_scan_full]\n    end\n    if remaining % [BLOCK_SIZE] > 0 then\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = &[lhs_ptr][ num_blocks * [BLOCK_SIZE] * [offset] ]\n      else\n        [lhs_ptr_arg] = [lhs_ptr]\n      end\n      var [num_elmts] = remaining % [BLOCK_SIZE]\n      var [num_leaves] = [BLOCK_SIZE]\n      while [num_leaves] / 2 > [num_elmts] do\n        [num_leaves] = [num_leaves] / 2\n      end\n      var [num_threads] = [num_leaves] / 2\n      [call_scan_arbitrary]\n    end\n\n    var [lhs_ptr_arg]\n    if [dir] >= 0 then\n      [lhs_ptr_arg] = [lhs_ptr]\n    else\n      [lhs_ptr_arg] = &[lhs_ptr][ (remaining % [BLOCK_SIZE]) * [offset] ]\n    end\n\n    recursive_scan(num_blocks,\n                   [offset] * [BLOCK_SIZE],\n                   [total],\n                   [lhs_ptr_arg],\n                   [dir])\n\n    if [remaining] > [BLOCK_SIZE] then\n      var [num_elmts] = remaining\n      var [num_threads] = remaining - [BLOCK_SIZE]\n      [call_postscan_full]\n    end\n  end\n\n\n  local launch = quote\n    do\n      var num_blocks : uint64 = total / [BLOCK_SIZE]\n      if num_blocks > 0 then\n        var [lhs_ptr_arg]\n        var [rhs_ptr_arg]\n        var [num_threads] = num_blocks * [THREAD_BLOCK_SIZE]\n        if [dir] >= 0 then\n          [lhs_ptr_arg] = [lhs_ptr]\n          [rhs_ptr_arg] = [rhs_ptr]\n        else\n          [lhs_ptr_arg] = &[lhs_ptr][ total % [BLOCK_SIZE] ]\n          [rhs_ptr_arg] = &[rhs_ptr][ total % [BLOCK_SIZE] ]\n        end\n        [call_prescan_full]\n      end\n      if total % [BLOCK_SIZE] > 0 then\n        var [lhs_ptr_arg]\n        var [rhs_ptr_arg]\n        if [dir] >= 0 then\n          [lhs_ptr_arg] = &[lhs_ptr][ num_blocks * [BLOCK_SIZE] ]\n          [rhs_ptr_arg] = &[rhs_ptr][ num_blocks * [BLOCK_SIZE] ]\n        else\n          [lhs_ptr_arg] = [lhs_ptr]\n          [rhs_ptr_arg] = [rhs_ptr]\n        end\n        var [num_elmts] = total % [BLOCK_SIZE]\n        var [num_leaves] = [BLOCK_SIZE]\n        while [num_leaves] / 2 > [num_elmts] do\n          [num_leaves] = [num_leaves] / 2\n        end\n        var [num_threads] = [num_leaves] / 2\n        [call_prescan_arbitrary]\n      end\n\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = [lhs_ptr]\n      else\n        [lhs_ptr_arg] = &[lhs_ptr][ total % [BLOCK_SIZE] ]\n      end\n\n      recursive_scan(total / [BLOCK_SIZE],\n                     [BLOCK_SIZE],\n                     [total],\n                     [lhs_ptr_arg],\n                     [dir])\n\n      if total > [BLOCK_SIZE] then\n        var [offset] = 1\n        var [num_elmts] = total\n        var [num_threads] = total - [BLOCK_SIZE]\n        [call_postscan_full]\n      end\n    end\n  end\n\n  return launch\nend\n\nfunction cudahelper.codegen_kernel_call(cx, kernel_id, count, args, shared_mem_size, tight)\n  local setupArguments = terralib.newlist()\n\n  local offset = 0\n  for i = 1, #args do\n    local arg =  args[i]\n    local size = terralib.sizeof(arg.type)\n    setupArguments:insert(quote\n      ExecutionAPI.cudaSetupArgument(&[arg], size, offset)\n    end)\n    offset = offset + size\n  end\n\n  local grid = terralib.newsymbol(RuntimeAPI.dim3, \"grid\")\n  local block = terralib.newsymbol(RuntimeAPI.dim3, \"block\")\n  local num_blocks = terralib.newsymbol(int64, \"num_blocks\")\n\n  local function round_exp(v, n)\n    return `((v + (n - 1)) / n)\n  end\n\n  local launch_domain_init = nil\n  if not cx.use_2d_launch then\n    launch_domain_init = quote\n      if [count] <= THREAD_BLOCK_SIZE and tight then\n        [block].x, [block].y, [block].z = [count], 1, 1\n      else\n        [block].x, [block].y, [block].z = THREAD_BLOCK_SIZE, 1, 1\n      end\n      var [num_blocks] = [round_exp(count, THREAD_BLOCK_SIZE)]\n    end\n  else\n    launch_domain_init = quote\n      if [count] <= NUM_THREAD_X and tight then\n        [block].x, [block].y, [block].z = [count], NUM_THREAD_Y, 1\n      else\n        [block].x, [block].y, [block].z = NUM_THREAD_X, NUM_THREAD_Y, 1\n      end\n      var [num_blocks] = [round_exp(count, NUM_THREAD_X)]\n    end\n  end\n\n  launch_domain_init = quote\n    [launch_domain_init]\n    if [num_blocks] <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z = [num_blocks], 1, 1\n    elseif [count] / MAX_NUM_BLOCK <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, [round_exp(num_blocks, MAX_NUM_BLOCK)], 1\n    else\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, MAX_NUM_BLOCK,\n        [round_exp(num_blocks, MAX_NUM_BLOCK, MAX_NUM_BLOCK)]\n    end\n  end\n\n  return quote\n    if [count] > 0 then\n      var [grid], [block]\n      [launch_domain_init]\n      ExecutionAPI.cudaConfigureCall([grid], [block], shared_mem_size, nil)\n      [setupArguments]\n      ExecutionAPI.cudaLaunch([&int8](kernel_id))\n    end\n  end\nend\n\nlocal function get_nv_fn_name(name, type)\n  assert(type:isfloat())\n  local nv_name = \"__nv_\" .. name\n\n  -- Okay. a little divergence from the C standard...\n  if name == \"isnan\" or name == \"isinf\" then\n    if type == double then\n      nv_name = nv_name .. \"d\"\n    else\n      nv_name = nv_name .. \"f\"\n    end\n  -- Seriously?\n  elseif name == \"finite\" then\n    if type == double then\n      nv_name = \"__nv_isfinited\"\n    else\n      nv_name = \"__nv_finitef\"\n    end\n  elseif type == float then\n    nv_name = nv_name .. \"f\"\n  end\n  return nv_name\nend\n\nlocal function get_cuda_definition(self)\n  if self:has_variant(\"cuda\") then\n    return self:get_variant(\"cuda\")\n  else\n    local fn_type = self.super:get_definition().type\n    local fn_name = get_nv_fn_name(self:get_name(), self:get_arg_type())\n    assert(fn_name ~= nil)\n    local fn = externcall_builtin(fn_name, fn_type)\n    self:set_variant(\"cuda\", fn)\n    return fn\n  end\nend\n\nfunction cudahelper.get_cuda_variant(math_fn)\n  return math_fn:override(get_cuda_definition)\nend\n\n-- #####################################\n-- ## CUDA Codegen Context\n-- #################\n\nlocal context = {}\n\nfunction context:__index(field)\n  local value = context[field]\n  if value ~= nil then\n    return value\n  end\n  error(\"context has no field '\" .. field .. \"' (in lookup)\", 2)\nend\n\nfunction context:__newindex(field, value)\n  error(\"context has no field '\" .. field .. \"' (in assignment)\", 2)\nend\n\nfunction context.new(use_2d_launch, offset_2d)\n  local offset_2d = offset_2d or false\n  return setmetatable({\n    use_2d_launch = use_2d_launch,\n    offset_2d = offset_2d,\n    buffered_reductions = data.newmap(),\n  }, context)\nend\n\nfunction context:reduction_buffer(ref_type, value_type, op, generator)\n  local tbl = self.buffered_reductions[ref_type]\n  if tbl == nil then\n    tbl = {\n      buffer = cudalib.sharedmemory(value_type, THREAD_BLOCK_SIZE),\n      type = value_type,\n      op = op,\n      generator = generator,\n    }\n    self.buffered_reductions[ref_type] = tbl\n  end\n  return tbl\nend\n\nfunction context:compute_reduction_buffer_size()\n  local size = 0\n  for k, tbl in self.buffered_reductions:items() do\n    size = size + sizeof(tbl.type) * THREAD_BLOCK_SIZE\n  end\n  return size\nend\n\nfunction context:generate_preamble()\n  local preamble = terralib.newlist()\n\n  if self.use_2d_launch then\n    preamble:insert(quote\n      var [self.offset_2d:getsymbol()] = tid_y()\n    end)\n  end\n\n  for k, tbl in self.buffered_reductions:items() do\n    if tbl.type:isarray() then\n      local init = base.reduction_op_init[tbl.op][tbl.type.type]\n      preamble:insert(quote\n        for k = 0, [tbl.type.N] do\n          [tbl.buffer][ tid_y() + tid_x() * [NUM_THREAD_Y] ][k] = [init]\n        end\n      end)\n    else\n      local init = base.reduction_op_init[tbl.op][tbl.type]\n      preamble:insert(quote\n        [tbl.buffer][ tid_y() + tid_x() * [NUM_THREAD_Y] ] = [init]\n      end)\n    end\n  end\n\n  return preamble\nend\n\nfunction context:generate_postamble()\n  local postamble = terralib.newlist()\n\n  for k, tbl in self.buffered_reductions:items() do\n    postamble:insert(quote\n      do\n        var tid = tid_y()\n        var buf = &[tbl.buffer][ tid_x() * [NUM_THREAD_Y] ]\n        barrier()\n        [cudahelper.generate_reduction_tree(tid, buf, NUM_THREAD_Y, tbl.op, tbl.type)]\n        if tid == 0 then [tbl.generator(`(@buf))] end\n      end\n    end)\n  end\n\n  return postamble\nend\n\nlocal function check_2d_launch_profitable(node)\n  if not base.config[\"cuda-2d-launch\"] or not node:is(ast.typed.stat.ForList) then\n    return false, false\n  end\n  -- TODO: This is a very simple heurstic that does not even extend to 3D case.\n  --       At least we need to check if the inner loop has any centered accesses with\n  --       respect to that loop. In the longer term, we need a better algorithm to detect\n  --       cases where multi-dimensional kernel launches are profitable.\n  if #node.block.stats == 1 and node.block.stats[1]:is(ast.typed.stat.ForNum) then\n    local inner_loop = node.block.stats[1]\n    if inner_loop.metadata and inner_loop.metadata.parallelizable then\n      assert(#inner_loop.values == 2)\n      return true, base.newsymbol(inner_loop.symbol:gettype(), \"offset\")\n    end\n  end\n  return false, false\nend\n\nfunction cudahelper.new_kernel_context(node)\n  local use_2d_launch, offset_2d = check_2d_launch_profitable(node)\n  return context.new(use_2d_launch, offset_2d)\nend\n\nfunction cudahelper.optimize_loop(cx, node, block)\n  if cx.use_2d_launch then\n    local inner_loop = block.stats[1]\n    local index_type = inner_loop.symbol:gettype()\n    -- If the inner loop is eligible to a 2D kernel launch, we change the stride of the inner\n    -- loop accordingly.\n    inner_loop = inner_loop {\n      values = terralib.newlist({\n        ast.typed.expr.Binary {\n          op = \"+\",\n          lhs = inner_loop.values[1],\n          rhs = ast.typed.expr.ID {\n            value = cx.offset_2d,\n            expr_type = index_type,\n            annotations = ast.default_annotations(),\n            span = inner_loop.span,\n          },\n          expr_type = inner_loop.values[1].expr_type,\n          annotations = ast.default_annotations(),\n          span = inner_loop.span,\n        },\n        inner_loop.values[2],\n        ast.typed.expr.Constant {\n          value = NUM_THREAD_Y,\n          expr_type = index_type,\n          annotations = ast.default_annotations(),\n          span = inner_loop.span,\n        }\n      })\n    }\n    block = block { stats = terralib.newlist({ inner_loop }) }\n  end\n  return block\nend\n\nfunction cudahelper.generate_region_reduction(cx, loop_symbol, node, rhs, lhs_type, value_type, gen)\n  if cx.use_2d_launch then\n    local needs_buffer = base.types.is_ref(lhs_type) and\n                         (value_type:isprimitive() or value_type:isarray()) and\n                         node.metadata and\n                         node.metadata.centers and\n                         node.metadata.centers:has(loop_symbol)\n    if needs_buffer then\n      local buffer = cx:reduction_buffer(lhs_type, value_type, node.op, gen).buffer\n      return quote\n        do\n          var idx = tid_y() + tid_x() * [NUM_THREAD_Y]\n          [generate_element_reductions(`([buffer][ [idx] ]), rhs, node.op, value_type, false)]\n        end\n      end\n    else\n      return gen(rhs)\n    end\n  else\n    return gen(rhs)\n  end\nend\n\n-- #####################################\n-- ## Code generation for kernel argument spill\n-- #################\n\nlocal MAX_SIZE_INLINE_KERNEL_PARAMS = 1024\n\nfunction cudahelper.check_arguments_need_spill(args)\n  local param_size = 0\n  args:map(function(arg) param_size = param_size + terralib.sizeof(arg.type) end)\n  return param_size > MAX_SIZE_INLINE_KERNEL_PARAMS\nend\n\nfunction cudahelper.generate_argument_spill(args)\n  local arg_type = terralib.types.newstruct(\"cuda_kernel_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(args) do\n    local field_name\n    field_name = \"_arg\" .. tostring(i)\n    arg_type.entries:insert({ field_name, symbol.type })\n    mapping[field_name] = symbol\n  end\n\n  local kernel_arg = terralib.newsymbol(&arg_type)\n  local buffer_size = sizeof(arg_type)\n  buffer_size = (buffer_size + 7) / 8 * 8\n\n  local param_pack = terralib.newlist()\n  local param_unpack = terralib.newlist()\n\n  param_pack:insert(quote\n    var [kernel_arg]\n    do\n      var bounds : c.legion_rect_1d_t\n      bounds.lo.x[0] = 0\n      bounds.hi.x[0] = [buffer_size - 1]\n      var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.Z_COPY_MEM, [&int8](nil))\n      [kernel_arg] =\n        [&arg_type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n    end\n  end)\n  arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local arg = mapping[field_name]\n    param_pack:insert(quote (@[kernel_arg]).[field_name] = [arg] end)\n    param_unpack:insert(quote var [arg] = (@[kernel_arg]).[field_name] end)\n  end)\n\n  return param_pack, param_unpack, kernel_arg\nend\n\nreturn cudahelper\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/language/src/regent/openmphelper.t": "-- Copyright 2021 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal base = require(\"regent/std_base\")\nlocal std = require(\"regent/std\")\n\nlocal omp = {}\n\n-- Exit early if the user turned off OpenMP code generation\n\nif std.config[\"openmp\"] == 0 then\n  function omp.check_openmp_available()\n    return false\n  end\n  return omp\nend\n\nlocal has_openmp = true\nif not (std.config[\"offline\"] or std.config[\"openmp-offline\"]) then\n  local dlfcn = terralib.includec(\"dlfcn.h\")\n  local terra find_openmp_symbols()\n    var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n    var has_openmp =\n      dlfcn.dlsym(lib, \"GOMP_parallel\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_num_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_max_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_thread_num\") ~= [&opaque](0)\n    dlfcn.dlclose(lib)\n    return has_openmp\n  end\n  has_openmp = find_openmp_symbols()\nend\n\nif not has_openmp then\n  function omp.check_openmp_available()\n    return false, \"Regent is installed without OpenMP support\"\n  end\n  if std.config[\"openmp\"] == 1 then\n    local available, message = omp.check_openmp_available()\n    print(\"OpenMP code generation failed since \" .. message)\n    os.exit(-1)\n  end\n\nelse\n  omp.check_openmp_available = function() return true end\n  local omp_abi = terralib.includecstring [[\n    extern int omp_get_num_threads(void);\n    extern int omp_get_max_threads(void);\n    extern int omp_get_thread_num(void);\n    extern void GOMP_parallel(void (*fnptr)(void *data), void *data, int nthreads, unsigned flags);\n  ]]\n\n  omp.get_num_threads = omp_abi.omp_get_num_threads\n  omp.get_max_threads = omp_abi.omp_get_max_threads\n  omp.get_thread_num = omp_abi.omp_get_thread_num\n  omp.launch = omp_abi.GOMP_parallel\nend\n\n-- TODO: This might not be the right size in platforms other than x86\nomp.CACHE_LINE_SIZE = 64\n\nlocal FAST_ATOMICS = {\n  [\"+\"] = \"add\",\n  [\"-\"] = \"sub\",\n}\n\nomp.generate_atomic_update = terralib.memoize(function(op, typ)\n  -- Build a C wrapper to use atomic intrinsics in LLVM\n  local atomic_update = nil\n  local op_name = base.reduction_ops[op].name\n  assert(op_name ~= nil)\n  -- Integer types\n  if typ:isintegral() then\n    local ctype = typ.cachedcstring or typ:cstring()\n    assert(ctype ~= nil)\n    -- If there is a native support for the operation, use it directly\n    if FAST_ATOMICS[op] ~= nil then\n      local fun_name = string.format(\"__atomic_update_%s_%s\", op_name, ctype)\n      local C = terralib.includecstring(string.format([[\n        #include <stdint.h>\n        inline void %s(%s *address, %s val) {\n          __sync_fetch_and_%s(address, val);\n        }\n      ]], fun_name, ctype, ctype, FAST_ATOMICS[op]))\n      terra atomic_update(address : &typ, val : typ)\n        [ C[fun_name] ](address, val)\n      end\n    else\n      local fun_name = string.format(\"__compare_and_swap_%s_%s\", op_name, ctype)\n      local C = terralib.includecstring(string.format([[\n        #include <stdint.h>\n        inline %s %s(%s *address, %s old, %s new) {\n          return __sync_val_compare_and_swap(address, old, new);\n        }\n      ]], ctype, fun_name, ctype, ctype, ctype))\n      terra atomic_update(address : &typ, val : typ)\n        var success = false\n        while not success do\n          var old = @address\n          var new = [std.quote_binary_op(op, old, val)]\n          var res = [ C[fun_name] ](address, old, new)\n          success = res == old\n        end\n      end\n    end\n  else\n    local size = terralib.sizeof(typ) * 8\n    local cas_type = _G[\"uint\" .. tostring(size)]\n    local ctype = typ.cachedcstring or typ:cstring()\n    local cas_ctype = cas_type.cachedcstring or cas_type:cstring()\n    local fun_name = string.format(\"__compare_and_swap_%s_%s\", op_name, ctype)\n    local C = terralib.includecstring(string.format([[\n      #include <stdint.h>\n      inline %s %s(%s *address, %s old, %s new) {\n        return __sync_val_compare_and_swap(address, old, new);\n      }\n    ]], cas_ctype, fun_name, cas_ctype, cas_ctype, cas_ctype))\n    terra atomic_update(address : &typ, val : typ)\n      var success = false\n      while not success do\n        var old = @address\n        var new = [std.quote_binary_op(op, old, val)]\n\n        var address_b : &cas_type = [&cas_type](address)\n        var old_b : &cas_type = [&cas_type](&old)\n        var new_b : &cas_type = [&cas_type](&new)\n        var res : cas_type = [ C[fun_name] ](address_b, @old_b, @new_b)\n        success = res == @old_b\n      end\n    end\n  end\n  assert(atomic_update ~= nil)\n  atomic_update:setinlined(true)\n  return atomic_update\nend)\n\nfunction omp.generate_preamble(rect, idx, start_idx, end_idx)\n  return quote\n    var num_threads = [omp.get_num_threads]()\n    var thread_id = [omp.get_thread_num]()\n    var lo = [rect].lo.x[idx]\n    var hi = [rect].hi.x[idx] + 1\n    var chunk = (hi - lo + num_threads - 1) / num_threads\n    if chunk == 0 then chunk = 1 end\n    var [start_idx] = thread_id * chunk + lo\n    var [end_idx] = (thread_id + 1) * chunk + lo\n    if [end_idx] > hi then [end_idx] = hi end\n  end\nend\n\nfunction omp.generate_argument_type(symbols, reductions)\n  local arg_type = terralib.types.newstruct(\"omp_worker_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(symbols) do\n    local field_name\n    if reductions[symbol] == nil then\n      field_name = \"_arg\" .. tostring(i)\n      arg_type.entries:insert({ field_name, symbol.type })\n    else\n      field_name = \"_red\" .. tostring(i)\n      arg_type.entries:insert({ field_name, &symbol.type })\n    end\n    mapping[field_name] = symbol\n  end\n  return arg_type, mapping\nend\n\nfunction omp.generate_argument_init(arg, arg_type, mapping, can_change, reductions)\n  local worker_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      return quote var [symbol] = [init] end\n    else\n      return quote var [symbol] = [arg].[field_name] end\n    end\n  end)\n\n  local launch_init = terralib.newlist()\n  launch_init:insert(quote\n    var arg_obj : arg_type\n    var [arg] = &arg_obj\n  end)\n  local launch_update = terralib.newlist()\n\n  arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      assert(field_type:ispointer())\n      launch_init:insert(quote\n        var num_threads = [omp.get_max_threads]()\n        -- We don't like false sharing\n        var size = num_threads  * omp.CACHE_LINE_SIZE\n        var data = std.c.malloc(size)\n        std.assert(size == 0 or data ~= nil, \"malloc failed in generate_argument_init\")\n        [arg].[field_name] = [field_type](data)\n        for i = 0, num_threads do\n          @[&symbol.type]([&int8](data) + i * omp.CACHE_LINE_SIZE) = [init]\n        end\n      end)\n    elseif not can_change[symbol] then\n      launch_init:insert(quote [arg].[field_name] = [symbol] end)\n    else\n      launch_update:insert(quote [arg].[field_name] = [symbol] end)\n    end\n  end)\n\n  return worker_init, launch_init, launch_update\nend\n\nfunction omp.generate_worker_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        do\n          var idx = [omp.get_thread_num]() * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [arg].[field_name][idx] = [std.quote_binary_op(op, symbol,\n            `([arg].[field_name][idx]))]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nfunction omp.generate_launcher_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        for i = 0, [omp.get_max_threads]() do\n          var idx = i * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [symbol] = [std.quote_binary_op(op, symbol, `([arg].[field_name][idx]))]\n        end\n        std.c.free([arg].[field_name])\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nreturn omp\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/language/examples/manual_capi_tasklib.rg": "-- Copyright 2021 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\n-- This file is not meant to be run directly.\n\n-- runs-with:\n-- []\n\nlocal tasklib = {}\n\nif os.execute(\"bash -c \\\"[ `uname` == 'Darwin' ]\\\"\") == 0 then\n  terralib.linklibrary(\"libregent.dylib\")\nelse\n  terralib.linklibrary(\"libregent.so\")\nend\n\nlocal c = terralib.includecstring([[\n#include \"legion.h\"\n#include \"regent.h\"\n#include <stdio.h>\n#include <stdlib.h>\n]])\ntasklib.c = c\n\nlocal dlfcn = terralib.includec(\"dlfcn.h\")\nlocal terra legion_has_llvm_support() : bool\n  return (dlfcn.dlsym([&opaque](0), \"legion_runtime_register_task_variant_llvmir\") ~= [&opaque](0))\nend\nlocal use_llvm = legion_has_llvm_support()\n\nlocal function legion_task_wrapper(body)\n  -- look at the return type of the task we're wrapping to emit the right postamble code\n  local ft = body:gettype()\n  local rt = ft.returntype\n  local wrapper = nil\n  if terralib.sizeof(rt) > 0 then\n    wrapper = terra(data : &opaque, datalen : c.size_t, userdata : &opaque, userlen : c.size_t, proc_id : c.legion_proc_id_t)\n      var task : c.legion_task_t,\n          regions : &c.legion_physical_region_t,\n          num_regions : uint32,\n          ctx : c.legion_context_t,\n          runtime : c.legion_runtime_t\n      c.legion_task_preamble(data, datalen, proc_id, &task, &regions, &num_regions, &ctx, &runtime)\n      var rv : rt = body(task, regions, num_regions, ctx, runtime)\n      c.legion_task_postamble(runtime, ctx, [&opaque](&rv), terralib.sizeof(rt))\n    end\n  else\n    wrapper = terra(data : &opaque, datalen : c.size_t, userdata : &opaque, userlen : c.size_t, proc_id : c.legion_proc_id_t)\n      var task : c.legion_task_t,\n          regions : &c.legion_physical_region_t,\n          num_regions : uint32,\n          ctx : c.legion_context_t,\n          runtime : c.legion_runtime_t\n      c.legion_task_preamble(data, datalen, proc_id, &task, &regions, &num_regions, &ctx, &runtime)\n      body(task, regions, num_regions, ctx, runtime)\n      c.legion_task_postamble(runtime, ctx, [&opaque](0), 0)\n    end\n  end\n  return wrapper\nend\n\nfunction tasklib.preregister_task(terrafunc)\n  -- either way, we wrap the body with legion preamble and postamble first\n  local wrapped = legion_task_wrapper(terrafunc)\n  if use_llvm then\n    -- if we can register llvmir, ask Terra to generate that\n    local ir = terralib.saveobj(nil, \"llvmir\", { entry=wrapped } )\n    local rfunc = terra(id : c.legion_task_id_t,\n                        variant_id : c.legion_variant_id_t,\n                        task_name : &int8,\n                        variant_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_preregister_task_variant_llvmir(\n        id, variant_id, task_name,\n        execution_constraints, layout_constraints, options,\n        ir, \"entry\", userdata, userlen)\n    end\n    return rfunc\n  else\n    -- use the terra function directly, which ffi will convert to a (non-portable) function pointer\n    local rfunc = terra(id : c.legion_task_id_t,\n                        variant_id : c.legion_variant_id_t,\n                        task_name : &int8,\n                        variant_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_preregister_task_variant_fnptr(\n        id, variant_id, task_name, variant_name,\n        execution_constraints, layout_constraints, options,\n        wrapped, userdata, userlen)\n    end\n    return rfunc\n  end\nend\n\nfunction tasklib.register_task(terrafunc)\n  -- either way, we wrap the body with legion preamble and postamble first\n  local wrapped = legion_task_wrapper(terrafunc)\n  if use_llvm then\n    -- if we can register llvmir, ask Terra to generate that\n    local ir = terralib.saveobj(nil, \"llvmir\", { entry=wrapped } )\n    local rfunc = terra(runtime : c.legion_runtime_t,\n                        id : c.legion_task_id_t,\n                        task_name : &int8,\n                        variant_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_register_task_variant_llvmir(\n        runtime, id, task_name,\n        true, -- global registration possible with llvmir\n        execution_constraints, layout_constraints, options,\n        ir, \"entry\", userdata, userlen)\n    end\n    return rfunc\n  else\n    -- use the terra function directly, which ffi will convert to a (non-portable) function pointer\n    local rfunc = terra(runtime : c.legion_runtime_t,\n                        id : c.legion_task_id_t,\n                        task_name : &int8,\n                        variant_name : &int8,\n                        execution_constraints : c.legion_execution_constraint_set_t,\n                        layout_constraints : c.legion_task_layout_constraint_set_t,\n                        options: c.legion_task_config_options_t,\n                        userdata : &opaque,\n                        userlen : c.size_t)\n      return c.legion_runtime_register_task_variant_fnptr(\n        runtime, id, task_name, variant_name,\n        false, -- global registration not possible with non-portable pointer\n        execution_constraints, layout_constraints, options,\n        wrapped, userdata, userlen)\n    end\n    return rfunc\n  end\nend\n\nif use_llvm then\n  print(\"LLVM support detected...  tasks will be registered as LLVM IR\")\nelse\n  print(\"LLVM support NOT detected...  tasks will be registered as function pointers\")\nend\n\nreturn tasklib\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/.git/objects/pack/pack-bfcbc15840a31f9b5bc95f3e0d1300dd84175c4c.idx",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/.git/objects/pack/pack-bfcbc15840a31f9b5bc95f3e0d1300dd84175c4c.pack",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/language/examples/pagerank/sample.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/language/examples/mssp/small/edges.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/language/examples/mssp/small/result_3.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/doc/arch/persistent/hdf5/figs/high-level-design.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-in7xzihjvftj5l3vqaqbvinznwxg7gin/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout.pptx"
    ],
    "total_files": 2133
}