{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/inst/doc/np_faq.Rnw": "%% $Id: np_faq.tex,v 1.90 2014/06/27 14:22:06 jracine Exp jracine $\n%\\VignetteIndexEntry{Frequently Asked Questions (np)}\n%\\VignetteDepends{np}\n%\\VignetteKeywords{nonparametric, kernel, categorical}\n%\\VignettePackage{np}\n\n\\documentclass[12pt]{amsart}\n\n\\tolerance=5000\n\n\\usepackage{setspace,hyperref}\n\n\\newcommand{\\field}[1]{\\mathbb{#1}} \\newcommand{\\R}{\\field{R}}\n\n%% Change the default page sizes.\n\n\\setlength{\\topmargin}{-0.25in} \\setlength{\\textheight}{8.5in}\n\\setlength{\\oddsidemargin}{.0in} \\setlength{\\evensidemargin}{.0in}\n\\setlength{\\textwidth}{6.5in} \\setlength{\\footskip}{.5in}\n\n\\title{Package \\texttt{np} FAQ} \\date{\\today} \\author{Jeffrey S.~Racine}\n\n\\begin{document}\n\n\\maketitle\n\n\\tableofcontents\n\n\\onehalfspacing\n\n\\section{Overview and Current Version}\n\nThis set of frequently asked questions is intended to help users who\nare encountering unexpected or undesired behavior when trying to use\nthe \\texttt{np} package.\n\nMany of the underlying C routines have been extensively tested over\nthe past two decades. However, the \\texttt{R} `hooks' needed to call\nthese routines along with processing of the data required for a\nseamless user experience may produce unexpected or undesired results\nin some settings.\n\nIf you encounter any issues with the \\texttt{np} package, kindly first\nensure that you have the most recent version of \\texttt{np},\n\\texttt{R}, and \\texttt{RStudio} (if appropriate) installed. Sometimes\nissues encountered using outdated versions of software have been\nresolved in the current versions, so this is the first thing one ought\nto investigate when the unexpected occurs.\n\nHaving ensured that the problem persists with the most recently\navailable versions of all software involved, kindly report any issues\nyou encounter to me, and \\textsl{please include} your code, data,\nversion of the \\texttt{np} package, version of \\texttt{R}, and\noperating system (with version number) used so that I can help track\ndown any such issues\n(\\href{mailto:racinej@mcmaster.ca}{racinej@mcmaster.ca}). And, of\ncourse, if you encounter an issue that you think might be of interest\nto others, kindly email me the relevant information and I will\nincorporate it into this FAQ.\n\nThis FAQ refers to the most recent version, which as of this writing\nis 0.60-2. Kindly update your version should you not be using the most\ncurrent (from within R, \\texttt{update.packages()} ought to do it,\nthough also see \\ref{update} below.). See the appendix in this file\nfor cumulative changes between this and previous versions of the\n\\texttt{np} package.\n\n\\section{Frequently Asked Questions}\n\n\\subsection{How do I cite the \\texttt{np} package?}\n\nOnce you have installed the \\texttt{np} package\n(\\texttt{install.packages(\"np\")}), if you load the \\texttt{np} package\n(\\texttt{library(\"np\")}) and type \\texttt{citation(\"np\")}\nyou will be presented with the following information.\n\n  \\begin{singlespacing}\n\\begin{verbatim}\n> citation(\"np\")\n\nTo cite np in publications use:\n\n  Tristen Hayfield and Jeffrey S. Racine (2008). Nonparametric\n  Econometrics: The np Package. Journal of Statistical Software\n  27(5). URL http://www.jstatsoft.org/v27/i05/.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Nonparametric Econometrics: The np Package},\n    author = {Tristen Hayfield and Jeffrey S. Racine},\n    journal = {Journal of Statistical Software},\n    year = {2008},\n    volume = {27},\n    number = {5},\n    url = {http://www.jstatsoft.org/v27/i05/},\n  }\n\\end{verbatim}\n  \\end{singlespacing}\n\n\\subsection{I have never used \\texttt{R} before. Can you direct me to some\n    introductory material that will guide me through the basics?}\n\n  There are many excellent introductions to the \\texttt{R} environment\n  with more on the way. First, I would recommend going directly to the\n  \\texttt{R} website (\\url{http://www.r-project.org}) and looking under\n  Documentation/Manuals (\\url{http://cran.r-project.org/manuals.html})\n  where you will discover a wealth of documentation for \\texttt{R} users\n  of all levels. See also the \\texttt{R} task views summary page\n  (\\url{http://cran.nedmirror.nl/web/views/index.html}) for\n  information grouped under field of interest. A few documents that I\n  mention to my students which are tailored to econometricians include\n  \\url{http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf},\n  Cribari-Neto \\& Zarkos (1999) \\cite{CRIBARI_NETO_ZARKOS:1999},\n  Racine \\& Hyndman (2002) \\cite{RACINE_HYNDMAN:2002} and Farnsworth\n  (2006) \\cite{FARNSWORTH:2006}, to name but a few.\n\n  Those looking for exemplar data sets outside of those contained in\n  the \\texttt{np} package are directed to the \\texttt{Ecdat} \\cite{Ecdat}\n  and \\texttt{AER} \\cite{AER} packages.\n\n  I maintain a `Gallery' to provide a forum for users to share code\n  and discover examples and illustrations which can be found at\n  \\url{http://socserv.mcmaster.ca/racinej/Gallery/Home.html}.\n\n  Often the best resource is right down the hall. Ask a colleague\n  whether they use or know anyone who uses R, then offer to buy that\n  person a coffee and along the way drop something like ``I keep\n  hearing about the \\texttt{R} project\\dots I feel like such a\n  Luddite\\dots''\n\n  \\subsection{\\label{update}How do I keep all \\texttt{R} packages on my\n    system current?}\n\n  Run the command \\texttt{update.packages(checkBuilt=TRUE,ask=FALSE)},\n  which will not only update all packages that are no longer current,\n  but will also update all packages built under outdated installed\n  versions of R, if appropriate.\n\n\\subsection{It seems that there are a lot of packages that must be\n    installed in order to conduct econometric analysis (tseries,\n    lmtest, np, etc.). Is there a way to avoid having to individually\n    install each package individually?}\n\n  Certainly. The Comprehensive R Archive Network (CRAN) is a network\n  of ftp and web servers around the world that store identical,\n  up-to-date, versions of code and documentation for \\texttt{R}.  The\n  CRAN `task view' for computational econometrics might be of\n  particular interest to econometricians.  The econometric task view\n  provides an excellent summary of both parametric and nonparametric\n  econometric packages that exist for the R environment and provides\n  one-stop installation for these packages.\n\nSee\n\\href{http://cran.r-project.org/web/views/Econometrics.html}{cran.r-project.org/web/views/Econometrics.html} for further information.\n\nTo automatically install a task view, the \\texttt{ctv} package first\nneeds to be installed and loaded, i.e.,\n\\begin{verbatim}\ninstall.packages(\"ctv\")\nlibrary(\"ctv\")\n\\end{verbatim}\n\nThe econometric task view can then be installed via\n\\texttt{install.views()} and updated via \\texttt{update.views()}\n(which first assesses which of the packages are already installed and\nup-to-date), i.e.,\n\\begin{verbatim}\ninstall.views(\"Econometrics\")\n\\end{verbatim}\nor\n\\begin{verbatim}\nupdate.views(\"Econometrics\")\n\\end{verbatim}\n\n\\subsection{Is there a `gentle guide' to the \\texttt{np} package that\n    contains some easy to follow examples?}\n\n  Perhaps the most gentle introduction is contained in the \\texttt{np}\n  package itself in the form of a `vignette'. To view the vignette run\n  R, install the \\texttt{np} package (\\texttt{install.packages(\"np\")}), then\n  type \\texttt{vignette(\"np\",package=\"np\")} to view or print the vignette\n  (this vignette is essentially the article that appeared in the\n  Journal of Statistical Software that describes the \\texttt{np} package\n  (Hayfield \\& Racine \\cite{HAYFIELD_RACINE:2008})).\n\n  See also \\texttt{vignette(\"entropy\\_np\",package=\"np\")} for a\n  vignette on the entropy-based functions and procedures introduced to\n  the \\texttt{np} package in versions 0.30-4 through 0.30-8.\n\n  In addition, you might be interested in the nonparametric\n  econometrics primer (Racine \\cite{RACINE:2008}) which is available\n  for download from my website as is the code that will generate the\n  examples contained therein.\n\n  For a listing of all routines in the \\texttt{np} package type:\n  \\texttt{library(help=\"np\")}.\n\n\\subsection{I noticed you have placed a new version of the \\texttt{np}\n    package on CRAN. How can I determine what has been changed,\n    modified, fixed etc?}\n\n  See the CHANGELOG on the CRAN site\n  (\\url{http://cran.r-project.org/web/packages/np/ChangeLog}), or go\n  to the end of this document where the CHANGELOG is provided for your\n  convenience.\n\n\\subsection{What is the difference between the \\texttt{np} package and the\n    previous stand-alone programs you wrote, namely, N \\copyright,\n    NPREG \\copyright, and NPDEN \\copyright?}\n\n  The \\texttt{np} package is built from the same C library that underlies\n  its predecessors N \\copyright, NPREG \\copyright, and NPDEN\n  \\copyright. In fact, \\texttt{R} calls the compiled C code that\n  underlies its predecessors (one of the beauties of \\texttt{R} is that\n  you can obtain the benefits of compiled code (i.e., speed) yet have\n  access to the rich superset of \\texttt{R} routines and \\texttt{R} packages\n  built by others). Therefore, there is no penalty in run-time when\n  using R versus the stand alone precompiled binary programs N\n  \\copyright, NPREG \\copyright, and NPDEN \\copyright\\, (unless of\n  course the compiler or compiler flags differ from those used to\n  build its predecessors).\n\n\\subsection{How can I read data stored in various formats such as\n    Stata, SAS, Minitab, SPSS etc.~into the R program?}\n\n  Install the foreign library via \\texttt{install.packages(\"foreign\")}\n  then do something like\n  \\begin{singlespacing}\n\\begin{verbatim}\nmydat <- read.dta(\"datafile.dta\"),\n\\end{verbatim}\n  \\end{singlespacing}\n  where \\texttt{datafile.dta} is the name of your Stata data\n  file. Note that, as of version 0.8-34, the foreign package function\n  \\texttt{read.dta} supports reading files directly over the Internet\n  making for more portable code. For instance, one could do something\n  like\n\\begin{verbatim}\nmydat <- read.dta(file=\"http://www.principlesofeconometrics.com/stata/mroz.dta\")\n\\end{verbatim}\nas one could always do with, say, \\texttt{read.table()}.\n\n\\subsection{I want to use so-and-so's semiparametric/nonparametric\n    method, however, the \\texttt{np} package does not include this\n    particular method\\dots}\n\n  This is why we have included the function \\texttt{npksum()}, which\n  exists so that you can create your own kernel objects and take\n  advantage of underlying kernel methods implemented in the \\texttt{np}\n  package without having to write, say, C or Fortran code.\n\n  With the options available, you could create new nonparametric tests\n  or even new kernel estimators. For instance, the convolution kernel\n  option would allow you to replicate, say, the least squares\n  cross-validation function for kernel density estimation found in\n  \\texttt{npudensbw()}. The function \\texttt{npksum()} uses highly-optimized\n  C code that strives to minimize its memory footprint, while there is\n  low overhead involved when using repeated calls to this function.\n\n  See, by way of illustration, the example in the \\texttt{npksum()}\n  help file that conducts leave-one-out cross-validation for a local\n  constant regression estimator via calls to the \\texttt{R} function\n  \\texttt{nlm()}, and compares this to the \\texttt{npregbw()}\n  function.\n\n  If you wish to have a method incorporated into a future version of\n  the \\texttt{np} package, the best way to achieve this is to\n  successfully code up the method using \\texttt{npksum()}, briefly\n  document it and write up an example, then send it to us. We will\n  then, at our discretion, do our best to adapt and incorporate this\n  into a future version and of course give credit where credit is due.\n\n\\subsection{Cross-validation takes forever, and I can't wait that\n    long\\dots}\n\n  This is the most common complaint from frustrated users coming to\n  terms with numerically demanding statistical methods.  I am fond of\n  saying `if you want the wrong answer, I can give it to you right\n  away', but this wears thin quickly.\n\n  \\begin{enumerate}\n\n  \\item Some background may be in order. Cross-validation methods have\n    run times that are proportional to the square of the number of\n    observations (of computational order $n^2$ hence a doubling of the\n    sample size will increase run time by a factor of four). The\n    solution I favor is to run the code in a parallel computing\n    environment. The underlying C code for \\texttt{np} is MPI-aware\n    (MPI denotes the `message passing interface', a popular parallel\n    programming library that is an international standard), and a\n    version of the \\texttt{np} package titled `\\texttt{npRmpi}' exists\n    for running jobs in a parallel environment that leverages the\n    \\texttt{Rmpi} package (\\cite{Rmpi}).\\footnote{We are extremely\n      grateful to Hao Yu for providing the MPI functionality contained\n      in the \\texttt{Rmpi} package.}\n\n    With respect to the \\texttt{npRmpi} package, kindly note that I\n    cannot assist with issues surrounding installation and setup due\n    to the vast heterogeneity of MPI implementations and methods for\n    executing such programs. You are instead strongly advised to seek\n    local advice from your sysadmin or others. The document\n    \\texttt{npRmpi.pdf} may contain some useful information for such\n    issues. You might also want to first get the \\texttt{Rmpi} package\n    installed and running properly as once you are at this stage it\n    ought to then be trivial to install the \\texttt{npRmpi}\n    package. Also note that running programs in a parallel environment\n    requires minor differences in how \\texttt{np} functions are\n    called, and you would be wise to examine the examples in the demo\n    directory (download and unpack the source code and look in this\n    directory) and the overview file \\texttt{npRmpi.pdf} in the\n    inst/doc/ directory of the \\texttt{npRmpi} package you downloaded\n    and unpacked.\n\n  \\item Alternatively, you can use the method outlined in Racine\n    (1993) \\cite{RACINE:1993}. The method is based on the fact that\n    the unknown constant $c_j$ (the `scale factor') in the formula\n    $c_j\\sigma_j n^{-1/(2p+r)}$ is independent of the sample size, so\n    one can conduct bandwidth selection on random subsets and do this\n    for a large number of subsets then take the mean/median over these\n    subsets and feed the scale factor into the final routine for the\n    entire sample. Below you will find simple \\texttt{R} code that\n    replicates the method using numerical search and resampling\n    without replacement rather than the grid method outlined in\n    \\cite{RACINE:1993} (both have equivalent properties but this is\n    perhaps simpler to implement using the \\texttt{np} package).\n\n    \\begin{singlespacing}\n\\begin{verbatim}\n## Regression example\n## Generate a moderately large data set\n\nset.seed(12345)\nn <- 100000\nx1 <- runif(n)\nx2 <- runif(n)\n\ny <- 1 + x1 + sin(pi*x2) + rnorm(n,sd=.1)\n\n## Set the number of resamples and the subsample size\n\nnum.res <- 50\nn.sub <- 250\n\n## Create a storage matrix\n\nbw.mat <- matrix(NA,nrow=num.res,ncol=2)\n\n## Get the scale factors for resamples from the full sample of size n.sub\n\noptions(np.messages=FALSE)\n\nfor(i in 1:num.res)  {\n\n  cat(paste(\" Replication\", i, \"of\", num.res, \"...\\r\"))\n\n  bw.mat[i,] <- npregbw(y~x1+x2,regtype=\"ll\",\n                        subset=sample(n,n.sub))$sfactor$x\n\n}\n\n## A function to compute the median of the columns of a matrix\n\ncolMedians <- function(data) {\n  colmed <- numeric(ncol(data))\n  for(i in 1:ncol(data)) {\n    colmed[i] <- median(data[,i])\n  }\n  return(colmed)\n}\n\n## Take the median scale factors\n\nbw <- colMedians(bw.mat)\n\n## The final model for the full dataset\n\nmodel.res <- npreg(y~x1+x2,bws=bw,regtype=\"ll\",bwscaling=TRUE)\n\\end{verbatim}\n    \\end{singlespacing}\n\n\\begin{singlespacing}\n\\begin{verbatim}\n## Hat tip to Yoshi Fujiwara <yoshi.fujiwara@gmail.com> for this \n## nice example\n\nn <- 100000\n\nlibrary(MASS)\nrho <- 0.25\nSigma <- matrix(c(1,rho,rho,1),2,2)\nmydat <- mvrnorm(n=n, rep(0, 2), Sigma)\nx <- mydat[,1]\ny <- mydat[,2]\nrm(mydat)\n\nnum.res <- 100\nn.sub <- 100\n\nbw.mat <- matrix(NA,nrow=num.res,ncol=2)\noptions(np.messages=FALSE)\nfor(i in 1:num.res) {\n  bw <- npcdensbw(y ~ x,subset=sample(n,n.sub))\n  bw.mat[i,] <- c(bw$sfactor$y,bw$sfactor$x)\n}\n\ncolMedians <- function(data) {\n colmed <- numeric(ncol(data))\n for(i in 1:ncol(data)) {\n   colmed[i] <- median(data[,i])\n }\n return(colmed)\n}\n\nbw <- colMedians(bw.mat)\nbw <- npcdensbw(y ~ x, bws=bw, bwscaling=TRUE, bandwidth.compute=FALSE)\nsummary(bw)\nplot(bw,xtrim=.01)\n\\end{verbatim}\n\\end{singlespacing}\n\n\\item Barring this, you can set the search tolerances to be a bit less\n  terse (at the expense of potential accuracy, i.e., becoming trapped\n  in local minima) by setting, say, \\texttt{tol=0.1} and\n  \\texttt{ftol=0.1} in the respective bandwidth routine (see the docs\n  for examples). Also, you can set \\texttt{nmulti=1} which overrides\n  the number of times the search procedure restarts from different\n  random starting values (the default is to restart $k$ times where\n  $k$ is the number of variables). Be warned, however, that this is\n  {\\em definitely not recommended} and should {\\em be avoided at all\n    costs} for all but the most casual examination of a\n  relationship. One ought to use multistarting for any final results\n  and never override default search tolerances {\\em unless increasing\n    multistarts beyond the default}. Results based upon exhaustive\n  search {\\em often differ dramatically} from that based on limited\n  search achieved by overriding default search tolerances.\n\n  \\item For those who like to tinker and who work on a *NIX system\n    with the gcc compiler suite, you can change the default compiler\n    switches used for building \\texttt{R} packages which may generate\n    some modest improvements in run time. The default compiler\n    switches are\n    \\begin{singlespacing}\n\\begin{verbatim}\n -g -O2\n\\end{verbatim}\n    \\end{singlespacing}\n    and are set in the R-*.*.*/etc/Makeconf file (where *.*.* refers\n    to your \\texttt{R} version number, e.g. R-2.10.1).  You can edit this\n    file and change these defaults to\n    \\begin{singlespacing}\n\\begin{verbatim}\n-O3 -ffast-math -fexpensive-optimizations -fomit-frame-pointer\n\\end{verbatim}\n    \\end{singlespacing}\n    then reinstall the \\texttt{np} package and you may experience some\n    improvements in run time. Note that the -g flag turns on\n    production of debugging information which can involve some\n    overhead, so we are disabling this feature. This is not a feature\n    used by the typical applied researcher but if you envision\n    requiring this it is clearly trivial to re-enable debugging. I\n    typically experience in the neighborhood of a 0-5\\% reduction in\n    run time for data-driven bandwidth selection on a variety of\n    systems depending on the method being used, though mileage will of\n    course vary.\n\n  \\item Finally, I have recently been working on extending regression\n    splines (as opposed to smoothing splines\\footnote{Unlike\n      regression splines, a smoothing spline places a knot at each\n      data point and penalizes the fit for lack of smoothness as\n      defined by the second derivative (typically cubic splines are\n      used). When the penalty is zero this produces a function that\n      interpolates the data points. When the penalty is infinite, this\n      delivers the linear OLS fit to the data.}) to admit categorical\n    predictors (see references for papers with Yang and Ma in the\n    \\texttt{crs} package). Because regression splines involve simple\n    least-squares fits, they scale much better with respect to the\n    sample size so for large sample sizes you may wish to instead\n    consider this approach (kindly see my webpage for further\n    details). You can run cross-validation with hundreds of thousands\n    of observations on a decent desktop in far less time than that\n    required for cross-validated kernel estimates.\n\n  \\end{enumerate}\n\n\\subsection{I estimated a parametric model using the\n  \\texttt{lm()} function. How can I compare the cross-validation score\n  from the \\texttt{npreg()} approach with that for the parametric\n  model?}\n\nThis can be readily achieved for the parametric model as follows:\n\\begin{verbatim}\ndata(wage1)\nmodel.lm <- lm(lwage ~ married + female + nonwhite + educ +\n                       exper + tenure, data = wage1)\ncv.lm <-mean(residuals(model.lm)^2/(1-hatvalues(model.lm))^2)                                      \ncv.lm\n\\end{verbatim}\nYou can then compare \\verb+cv.lm+ with the cross-validation score for\nthe \\verb+npreg+ object as follows: if your model is named \\verb+foo+\n(i.e.\\ you ran something like \\verb^foo <- npreg(y~x1+x2)^) then\n\\verb+summary(foo$bws)+ will give you a summary of the model's\nbandwidth object (the value of the objective function is the\ncross-validation score), while \\verb+foo$bws$fval+ will give you the\nfunction value, i.e.\\ the value of the cross-validation function, {\\em\n  provided} that the model was estimated via least-squares\ncross-validation (default, \\verb+bwmethod=\"cv.ls\"+). If the\ncross-validation score is lower for one model, that indicates that the\nmodel possessing the lowest score is to be preferred.\n\n\\subsection{Where can I get some examples of R code for the np\n  package in addition to the examples in the help files?}\n\nStart \\texttt{R} then type \\verb+demo(package=\"np\")+ and you will be\npresented with a list of demos for constrained estimation, inference,\nand so forth. To run one of these demos type, for example,\n\\verb+demo(npregiv)+ (note that you must first install the \\texttt{rgl} package to run this particular demo).\n\nTo find the location of a demo type\n\\verb+system.file(\"demo\",\"npregiv.R\",package=\"np\")+ for example,\nthen you can take the source code for this demo and modify it for your\nparticular application.\n\nYou can also find examples and illustrations at the `Gallery' located\nat \\url{http://socserv.mcmaster.ca/racinej/Gallery/Home.html}.\n\n\\subsection{Is\n    there a way to figure out roughly how long\n    cross-validation will take on a large sample?}\n\n  Certainly. You can run cross-validation on subsets of your data of\n  increasing size and time each run, then estimate a double-log model\n  of sample size on run time (run time can be approximated by a linear\n  log-log model) and then use this to predict approximate\n  run-time. The following example demonstrates this for a simple\n  model, but you can modify it trivially for your data. Note that the\n  larger is \\texttt{n.max} the more accurate it will likely be. Note that\n  we presume your data is in no particular order (if it is, you\n  perhaps ought to shuffle it first). We plot the log-log model fit\n  and prediction along with that expressed in hours.\n\n  \\begin{singlespacing}\n\\begin{verbatim}\n## Set the upper bound (n.max > 100) for the sub-samples on which you \n## will run cross-validation (perhaps n.max = 1000 (or 2000) ought to \n## suffice). For your application, n will be your sample size\n\nn <- 2000\nn.max <- 1000\n\nx <- runif(n)\ny <- 1 + x + rnorm(n)\n\nn.seq <- seq(100,n.max,by=100)\ntime.seq <- numeric(length(n.seq))\n\nfor(i in 1:length(n.seq)) {\n  time.seq[i] <- system.time(npregbw(y~x,subset=seq(1:n.seq[i])))[3]\n}\n\n## Now fit a double-log model and generate/plot actual values plus\n## prediction for n (i.e., approximate run time in hours)\n\nlog.time <- log(time.seq)\nlog.n <- log(n.seq)\n\nmodel <- lm(log.time~log.n)\n\nn.seq.aug <- c(n.seq,n)\ntime.fcst <- exp(predict(model,newdata=data.frame(log.n=log(n.seq.aug))))\n\npar(mfrow=c(2,1))\n\nplot(log(n.seq.aug),log(time.fcst),type=\"b\",\n     xlab=\"log(Sample Size)\",\n     ylab=\"log(Run Time)\",\n     main=\"Approximate Run Time (log seconds)\")\n\nplot(n.seq.aug,time.fcst/3600,type=\"b\",\n     xlab=\"Sample Size (n)\",\n     ylab=\"Hours\",\n     main=\"Approximate Run Time (hours)\",\n     sub=paste(\"Predicted run time for n =\", n, \"observations:\",\n     signif(time.fcst[length(time.fcst)]/3600, digits=2),\n     \"hours\"))\n\\end{verbatim}\n  \\end{singlespacing}\n\n\\subsection{I need to apply a boundary correction for a univariate\n  density estimate - how can I do this?}\n\nYou could do this using `data-reflection' (see the description in\n\\cite[Page 30]{SILVERMAN:1986}, as the following illustration\ndemonstrates.\n\n\\begin{verbatim}\n## Use trees to speed up estimation \noptions(np.tree=TRUE)\n\n## Generate data where there is an asymptote at the left boundary\n\nset.seed(42)\nn <- 10000\nx <- sort(rexp(n))\n\n## Compute the bandwidth for the original data\n\nbw <- npudensbw(~x,ckertype=\"epanechnikov\")\n\n## Construct the standard (i.e. no boundary correction) kernel estimate\n\nf.hat <- fitted(npudens(~x,bws=bw$bw,ckertype=\"epanechnikov\"))\n\n## Reflect the data, compute the reflected density then take the estimator which\n## is 2 times the reflected density estimate for x >= 0 (see Silverman (1986,\n## page 30))\n\nx.reflect <- c(x,-x)\nf.reflect <- npudens(~x.reflect,bws=bw$bw,ckertype=\"epanechnikov\")\nf.boundary <- 2*predict(f.reflect,newdata=data.frame(x.reflect=x))\n\n## Plot the nonsmooth histogram, the standard estimator, then the\n## boundary-corrected estimator\n\nhist(x,breaks=100,lty=3,freq=FALSE,ylim=c(0,1),main=\"\")\nrug(x)\nlines(x,f.hat,col=1,lwd=2,lty=1)\nlines(x,f.boundary,col=2,lwd=2,lty=2)\nlegend(\"topright\",\n       c(\"Standard Kernel Estimator (no boundary correction)\",\n         \"Data-reflected Kernel Estimator (boundary correction)\"),\n       lty=1:2,\n       col=1:2,\n       bty=\"n\")\n\\end{verbatim}\n\n\\subsection{I notice your density estimator supports manual bandwidths,\n    the normal-reference rule-of-thumb, likelihood cross-validation\n    and least squares cross-validation, but not plug-in rules. How can\n    I use various plug-in rules for estimating a univariate density?}\n\n  For the univariate case this is straightforward as the default R\n  installation supports a variety of univariate plug-in bandwidths\n  (see \\texttt{?bw.nrd} for details). For example, \\texttt{bs.SJ} computes\n  the bandwidths outlined in\n\n\\begin{verbatim}\n     Sheather, S. J. and Jones, M. C. (1991) A reliable data-based\n     bandwidth selection method for kernel density estimation.\n     _Journal of the Royal Statistical Society series B_, *53*,\n     683-690.\n\\end{verbatim}\n\n  Incorporating these univariate plug-in bandwidth selectors into the\n  univariate density estimation routines in \\texttt{np} is\n  straightforward as the following code snippet demonstrates. They\n  will certainly be faster than the likelihood and least-squares\n  cross-validation approaches. Also, you may not wish the density\n  estimate for all sample realizations, but rather for a shorter\n  grid. The following example demonstrates both and is closer in\n  spirit to the \\texttt{density()} function in base R.\n\n\\begin{verbatim}\nx <- rnorm(10000)\nxeval <- seq(min(x),max(x),length=100)\nf <- npudens(tdat=x,edat=xeval,bws=bw.SJ(x))\n\\end{verbatim}\n\n  You might even be tempted to use these in the multivariate case via\n  \\texttt{bws=c(bw.SJ(x1),bw.SJ(x2),...)} though these would be optimal\n  for univariate densities and most certainly not for a multivariate\n  density. However, for exploratory purposes these may be of interest\n  to some users.\n\n\\subsection{\\label{code_nowork}I wrote a program using \\texttt{np} and it\n    does not work as I expected\\dots}\n\n  There exist a rather extensive set of examples contained in the\n  docs. You can run these examples by typing\n  \\texttt{example(\"npfunctionname\")} where npfunctionname is, say,\n  \\texttt{w}, as in \\texttt{example(\"w\")}.\\footnote{For a listing of\n    all routines in the np package type: \\texttt{library(help=\"np\")}.}\n  These examples all pass quality control and produce the expected\n  results, so first see whether your problem fits into an existing\n  example, and if not, carefully follow the examples listed in a given\n  function for syntax issues etc.\n\n  If you are convinced that the problem lies with \\texttt{np} (there\n  certainly will be undiscovered `features', i.e., bugs), then kindly\n  send me your code and data so that I can replicate and help resolve\n  the issue.\n\n\\subsection{Under Mac OS X, when I run a command no progress is\n    displayed\\dots}\n\n  This should no longer occur for np versions 0.30-0 and up. For\n  previous versions, this reflected a peculiarity of console\n  input/output (I/O) under Mac OS X.  Note, however, that if you run {\n    \\tt R} in a terminal rather than \\texttt{Rgui} you will get the full\n  *NIX\\footnote{*NIX is often used to describe UNIX and other\n    UNIX-like platforms (i.e., UNIX, BSD, and GNU/Linux\n    distributions). I harbour strong preferences for *NIX computing\n    platforms.}  experience. \n\n  But also note that there is a platform-independent interface that\n  does not suffer from this limitation called `RStudio' that you might\n  prefer to existing interfaces (\\url{http://www.rstudio.org}).\n\n\\subsection{When some routines are running under MS Windows, \\texttt{R}\n    appears to be `not responding.' It appears that the program is not\n    `hung', rather is simply computing. The previous stand-alone\n    program (N \\copyright) always displayed useful information\\dots}\n\n  This should no longer occur for np versions 0.30-0 and up. For\n  previous versions, this reflected a peculiarity of the R Windows\n  GUI, and was not specific to the \\texttt{np} package.\n\n  From the \\texttt{R} Windows FAQ\\dots\n\n  ``When using Rgui the output to the console seems to be delayed.\n  This is deliberate: the console output is buffered and re-written in\n  chunks to be faster and less distracting. You can turn buffering off\n  or on from the `Misc' menu or the right-click menu: \\verb+<Ctrl-W>+\n  toggles the setting.''\n\n\\subsection{Some results take a while, and my MS Windows computer is\n    sluggish while \\texttt{R} is running\\dots}\n\n  You can easily change the priority of your \\texttt{R} job on the\n  fly, just as you might under *NIX.  Pull up the task manager\n  (\\verb+<Ctrl>-<Alt>-<Del>+), go to the process list, and find the\n  process \\texttt{Rgui.exe} (or \\texttt{R.exe} if you are running\n  \\texttt{Rterm}), select this process by left clicking on it, then\n  right clicking will bring up a menu, select \\texttt{Set priority},\n  then change priority to \\texttt{low} and hit $<$\\texttt{ok}$>$. For\n  lengthy jobs this will make your life much smoother, and you can,\n  say, run multiple jobs in low priority with no sluggishness\n  whatsoever for your other applications (useful for queuing a number\n  of long jobs). Alternatively, you could permanently change the\n  default priority of \\texttt{R} under MS Windows by modifying the\n  properties of your \\texttt{R} desktop icon.\n\n\\subsection{\\label{factor}A variable must be cast as, say, a factor in\n    order for \\texttt{np} to recognize this as an unordered factor. How\n    can I determine whether my data is already cast as a factor?}\n\n  Use the \\texttt{class()} function. For example, define \\texttt{x <-\n    factor(c(\"male\",\"female\"))}, then type \\texttt{class(x)}.\n\n\\subsection{When I use \\texttt{plot()} \\texttt{(npplot())} existing graphics\n    windows are overwritten. How can I display multiple graphics plots\n    in separate graphics windows so that I can see results from\n    previous runs and compare that to my new run?}\n\n  Use the \\texttt{dev.new()} command in between each call to \\texttt{plot()}. This will leave the existing graphics window open and\n  start a new one. The command \\texttt{dev.list()} will list all graphics\n  windows, and the command \\texttt{dev.set(integer.foo)} will allow you\n  to switch from one to another and overwrite existing graphics\n  windows should you so choose.\n\n  Alternately, you can use RStudio (\\url{http://www.rstudio.org})\n  where the plot window is such that previous plots can be recalled on\n  the fly, resized, saved in various formats etc.\n\n\\subsection{\\label{plot names}Sometimes \\texttt{plot()} fails to use my\n    variable names\\dots}\n\n  This should not occur unless you are using the data frame method and\n  not naming your variables (e.g., you are doing something like\n  data.frame(ordered(year))). To correct this, name your variables in\n  the respective data frame, as in\n\n  \\texttt{data <- data.frame(year=ordered(year),gdp)}\n\n  \\noindent so that the ordered factor appears as `year' and not\n  `ordered.year'\n\n\\subsection{Sometimes \\texttt{plot()} appends my variable names with\n    .ordered or .factor\\dots}\n\n  See also \\ref{plot names} above.\n\n\\subsection{I specify a variable as \\texttt{factor()} or \\texttt{ordered()}\n    in a data frame, then call this when I conduct bandwidth\n    selection. However, when I try to plot the resulting object, it\n    complains with the following message:}\n\n    \\texttt{Error in eval(expr, envir, enclos) : object \"variable\" not\n      found\\dots}\n\n  This arises because \\texttt{plot()} (\\texttt{npplot()}) tries to retrieve\n  the variable from the environment but you have changed the\n  definition when you called the bandwidth selection routine (e.g.,\n  \\texttt{npregbw(y\\textasciitilde x,data=dataframe)}).\n\n  To correct this, simply call \\texttt{plot()} with the argument\n  \\texttt{data=dataframe} where \\texttt{dataframe} is the name of your\n  data frame.\n\n\\subsection{My \\texttt{np} code produces errors when I attempt to run\n    it\\dots}\n\n  First, it is good practice to name all arguments (see the docs for\n  examples) as in \\texttt{npregbw(formula=y\\textasciitilde x)} (i.e.,\n  explicitly call formula for functions that use named formulas). This\n  will help the code return a potentially helpful error message.\n\n  Next, follow the examples listed at the end of each function help\n  page closely (i.e., \\texttt{?npreg} then scroll down to\n  \\texttt{Examples:}). See also \\ref{code_nowork} above.\n\n\\subsection{\\label{0/1}I have (numeric) 0/1 dummy variable regressors\n    in my parametric model. Can I just pass them to the \\texttt{np}\n    functions as I would in a parametric setting?}\n\n  In general, definitely not -- you need to correctly classify each\n  variable as type \\texttt{factor} and treat it as one variable only. By\n  way of example, suppose in your data you have created dummy\n  variables for year, for example, dummy06 which equals 1 for 2006, 0\n  otherwise, dummy07 which equals 1 for 2007, 0 otherwise etc. We\n  create these by habit for parametric models. But, the underlying\n  variable is simply year, which equals 2006, 2007, and so forth.\n\n  In \\texttt{np} (and \\texttt{R} in general), you get to economize by\n  just telling the function that the variable `year' is ordered, as in\n  ordered(year), where year is a vector containing elements 2006, 2007\n  etc. Of course, seasoned R users would appreciate that this is in\n  fact the simple way to do it with a parametric model as well.\n\n  You would {\\em never}, therefore, just pass dummy variables to an\n  \\texttt{np} function as you would for linear parametric models. The\n  {\\em only} exception is where you have only one 0/1 dummy for one\n  variable, say `\\texttt{sex}', and in this case you still would have\n  to enter this as \\texttt{factor(sex)} so that the \\texttt{np}\n  function recognizes this as a factor (otherwise it would treat it as\n  continuous and use a kernel function that is inappropriate for a\n  factor).\n\n\\subsection{\\label{terminates}I have a categorical variable, ignored\n    the advice in items \\ref{factor} and \\ref{0/1}, and R terminates\n    with the following error.}\n\n\\begin{verbatim}\n ** Fatal Error in routine kernel_bandwidth() ** variable 0 appears to be constant!\n ** Program terminated abnormally!\n\\end{verbatim}\n\n  Presuming your variable is not in fact a constant (i.e., is not in\n  fact a `variable'), this can only occur in the case where a variable\n  is `pathological' in that it has \\texttt{IQR=0} but \\texttt{std>0}\n  (see the section titled `Changes from Version 0.30-1 to 0.30-2\n  [19-Apr-2009]' for further details).  This should no longer occur\n  for np versions 0.30-2 and up.\n\n\\subsection{Rgui appears to crash. There must be a bug in the program.}\n\n  Try running your code using the terminal (i.e., \\texttt{Rterm} in\n  Windows or \\texttt{R} in a Mac OS X terminal) and see whether you get\n  the message in item \\ref{terminates}.\n\n\\subsection{Can I skip creating a bandwidth object and enter a\n    bandwidth directly?}\n\n  Certainly, though I would advise doing so for exploratory data\n  analysis only.  For example, attach a dataset via\n  \\begin{singlespacing}\n\\begin{verbatim}\ndata(cps71)\nattach(cps71)\n\\end{verbatim}\n  \\end{singlespacing}\n  then enter, say,\n  \\begin{singlespacing}\n\\begin{verbatim}\nplot(age,logwage,main=\"Manual Bandwidth Example\")\nlines(age,fitted(npreg(logwage~age,bws=1)),col=\"blue\",lty=1)\nlines(age,fitted(npreg(logwage~age,bws=2)),col=\"red\",lty=2)\nlines(age,fitted(npreg(logwage~age,bws=3)),col=\"green\",lty=3)\nlegend(20,15,\n       c(\"h=1\",\"h=2\",\"h=3\"),\n       col=c(\"blue\",\"red\",\"green\"),\n       lty=c(1,2,3))\n\\end{verbatim}\n  \\end{singlespacing}\n  to plot the local constant estimator with bandwidths of 1, 2, and 3\n  years. Note that the age variable is already sorted in this\n  dataset. If your data is not sorted you will need to do so prior to\n  plotting so that your \\texttt{lines} command works properly. Or see\n  \\ref{partial grad q} below for a multivariate example.\n\n\\subsection{\\label{partial grad q}When I estimate my gradients and\n    there are two or more covariates and then extract them with the\n    \\texttt{gradients()} function, they are not `smooth', though if I\n    plot a model with the \\texttt{gradients=TRUE} option, they are. The\n    \\texttt{gradients()} function must be broken\\dots}\n\n  The function \\texttt{plot()} (\\texttt{npplot()}) plots `partial'\n  means and gradients. In other words, it plots $x_1$ versus $\\hat\n  g(x_1,\\bar x_2)$ for the partial mean, where $\\bar x_2$ is, say, the\n  median/modal value of $x_2$. It also plots $x_1$ versus\n  $\\partial\\hat g(x_1,\\bar x_2)/\\partial x_1$ for the gradient. Note\n  that we are controlling for the values of the other\n  covariate(s). This is in effect what people expect when they play\n  with linear parametric models of the form $y=\\beta_0+\\beta_1\n  x_1+\\beta_2x_2+\\epsilon$ since, given the additive nature of the\n  model, $\\partial y/\\partial x_1=\\beta_1$ (i.e., does not vary with\n  $x_2$).\n\n  The example below shows how you could manually generate the partial\n  gradients (and means) for your data where the sample realizations\n  form the evaluation data for $x_1$ (unlike \\texttt{npplot()} which\n  uses an evenly spaced grid). Note we use the function\n  \\texttt{uocquantile()} to generate a vector that holds $x_2$\n  constant at is median/modal value (i.e., the 0.5 quantile) in the\n  evaluation data. The function \\texttt{uocquantile()} can compute\n  quantiles for ordered, unordered, and continuous data (see\n  \\texttt{?uocquantile} for details).\n\n\\begin{singlespacing}\n\\begin{verbatim}\nn <- 100\n\nx1 <- runif(n)\nx2 <- runif(n)\ny <- x1^2+x2^2 + rnorm(n,sd=.1)\n\ndata.train <- data.frame(x1,x2,y)\n\nbw <- npregbw(y~x1+x2,\n              data=data.train,\n              regtype=\"ll\",\n              bwmethod=\"cv.aic\")\n\ndata.eval <-  data.frame(x1 = sort(x1),\n                    x2 = rep(uocquantile(x2,.5),n))\n\nmodel <- npreg(bws=bw,\n               data=data.train, \n               newdata=data.eval,\n               gradients=TRUE)\n\nplot(data.eval[,1],model$grad[,1],xlab=\"X1\",ylab=\"Gradient\",type=\"l\")\n\\end{verbatim}\n\\end{singlespacing}\n\nNote that this uses the sorted sample realizations for \\texttt{x1}\nwhich is perfectly acceptable. If you want to mimic \\texttt{plot()}\nexactly you will see that by default plot uses a grid of equally\nspaced values of length \\texttt{neval=50} as per below. Both are\nperfectly acceptable and the point is they control for the level of\nthe non-axis variables.\n\\begin{singlespacing}\n\\begin{verbatim} \ndata.eval <-  data.frame(x1 = seq(min(x1),max(x1),length=50),\n                         x2 = rep(uocquantile(x2,.5),50))  \n\\end{verbatim}\n\\end{singlespacing}\n\n\\subsection{I use \\texttt{plot()} \\texttt{(npplot())} to plot, say, a density\n    and the resulting plot looks like an inverted density rather than\n    a density\\dots}\n\n  This can occur when the data-driven bandwidth is dramatically\n  undersmoothed. Data-driven (i.e., automatic) bandwidth selection\n  procedures are not guaranteed always to produce good results due to\n  perhaps the presence of outliers or the rounding/discretization of\n  continuous data, among others. By default, \\texttt{npplot()} takes\n  the two extremes of the data (minimum, maximum i.e., actual data\n  points) then creates an equally spaced grid of evaluation data\n  (i.e., not actual data points in general) and computes the density\n  for these points. Since the bandwidth is extremely small, the\n  density estimate at these evaluation points is correctly zero, while\n  those for the sample realizations (in this case only two, the min\n  and max) are non-zero, hence we get two peaks at the edges of the\n  plot and a flat bowl equal to zero everywhere else.\n\n  This can also happen when your data is heavily discretized and you\n  treat it as continuous. In such cases, treating the data as ordered\n  may result in more sensible estimates.\n\n\\subsection{Can \\texttt{npksum()} compute analytical derivatives with\n    respect to a continuous variable?}\n\n  As of version 0.20-0 and up, yes it can, using the \\texttt{operator\n    = \"derivative\"} argument, which is put to its paces in the\n  following code snippet (this supports multiple arguments including\n  \\texttt{\"integral\"} and \\texttt{\"convolution\"} in addition to\n  \\texttt{\"normal\"}, the default).\n\n  \\begin{singlespacing}\n\\begin{verbatim}\nZ <- seq(-2.23,2.23,length=100)\nZc <- seq(-4.47,4.47,length=100)\n\npar(mfrow=c(2,2))\n\nplot(Z,main=\"Kernel\",ylab=\"K()\",npksum(txdat=0,exdat=Z,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"normal\")$ksum,\n         col=\"blue\",type=\"l\")\n\nplot(Z,main=\"Kernel Derivative\",ylab=\"K()\",npksum(txdat=0,exdat=Z,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"derivative\")$ksum,\n         col=\"blue\",type=\"l\")\n\nplot(Z,main=\"Kernel Integral\",ylab=\"K()\",npksum(txdat=0,exdat=Z,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"integral\")$ksum,\n         col=\"blue\",type=\"l\")\n\nplot(Zc,main=\"Kernel Convolution\",ylab=\"K()\",npksum(txdat=0,exdat=Zc,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"convolution\")$ksum,\n         col=\"blue\",type=\"l\")\n\\end{verbatim}\n  \\end{singlespacing}\n\n  An alternative to computing analytical derivatives is to compute\n  them numerically using finite-differences. One simply computes the\n  kernel sum evaluating the sum with variable $j$ set at $x_j-h_j/2$\n  and calls this, say, $ksum_{j1}$, then again set at $x_j+h_j/2$ and\n  call this $ksum_{j}2$, then compute\n  $\\nabla=(ksum_{j2}-ksum_{j1})/h_j$. This method has been used for\n  both theoretical and applied work and produces consistent estimates\n  of the derivatives, as of course do the analytical derivatives,\n  providing that $h\\to0$ as $n\\to\\infty$ (which will {\\em not} be the\n  case in some settings, i.e., in the presence of irrelevant\n  covariates and so forth). The following example provides a simple\n  demonstration. See \\ref{partial grad q} above for multivariate\n  partial regression when using this method.\n\n  \\begin{singlespacing}\n    \\begin{singlespacing}\n\\begin{verbatim}\n## In this example we consider the local constant estimator computed\n## using npksum, and then use npksum to compute numerical derivatives\n## using finite-difference methods, then finally compare them with the\n## analytical ones.\n\ndata(cps71)\nattach(cps71)\n\n## Grab the cross-validated bandwidth\n\nbw <- npregbw(logwage~age)\nh <- bw$bw[1]\n\n## Evaluate the local constant regression at x-h/2, x+h/2...\n\nksum.1 <- npksum(txdat=age, exdat=age-h/2,tydat=logwage,bws=bw)$ksum/\n  npksum(txdat=age,exdat=age-h/2,bws=bw)$ksum\n\nksum.2 <- npksum(txdat=age, exdat=age+h/2,tydat=logwage,bws=bw)$ksum/\n  npksum(txdat=age,exdat=age+h/2,bws=bw)$ksum\n\n## Compute the numerical gradient...\n\ngrad.numerical <- (ksum.2-ksum.1)/h\n\n## Compare with the analytical gradient...\n\ngrad.analytical <- gradients(npreg(bws=bw,gradient=TRUE))\n\n## Plot the resulting estimates...\n\nplot(age,grad.numerical,type=\"l\",col=\"blue\",lty=1,ylab=\"gradient\")\nlines(age,grad.analytical,type=\"l\",col=\"red\",lty=2)\nlegend(20,-0.05,c(\"Numerical\",\"Analytic\"),col=c(\"blue\",\"red\"),lty=c(1,2))\n\\end{verbatim}\n    \\end{singlespacing}\n  \\end{singlespacing}\n\n\\subsection{Can I use the \\texttt{npcmstest()} function that implements\n    the consistent test for correct specification of parametric\n    regression models as described in Hsiao, Li, \\& Racine (2007)\n    \\cite{HSIAO_LI_RACINE:2007} to test for correct specification of\n    the semiparametric partially linear model?}\n\n  As Brennan Thompson points out, yes, you can.\n\n  To test a parametric linear specification against a semiparametric\n  partially linear alternative, i.e.,\n  \\begin{align*}\n    H_0: y &= X'\\beta + Z'\\gamma + u\\\\\n    H_1: y &= X'\\beta + g(Z) + u,\n  \\end{align*}\n  you could use \\texttt{npcmstest()} as follows:\n  \\begin{singlespacing}\n\\begin{verbatim}\n  lmodel <- lm(y~X+Z,y=TRUE,x=TRUE)\n  uhat <- resid(lmodel)\n  npcmstest(xdat=Z,ydat=uhat,model=lmodel)\n\\end{verbatim}\n  \\end{singlespacing}\n\n  A slightly better way (as discussed in Li \\& Wang (1998)\n  \\cite{LI_WANG:1998}) would be to use a `mixed' residual, i.e., $\\hat\n  u_i = y_i - X_i'\\tilde \\beta - Z_i'\\hat \\gamma$ in the test, where\n  $\\tilde\\beta$ is the semiparametric estimator of $\\beta$ (based on\n  the semiparametric partially linear model), and $\\hat \\gamma$ is the\n  OLS estimator of $\\gamma$ based on the linear model. This could lead\n  to potential power gains due to the improved efficiency of\n  $\\hat\\beta$ under the alternative.\n\n\\subsection{I am using \\texttt{npcmstest()} on a \\texttt{glm()} object (the\n    document says glm() objects are supported and I am estimating a\n    Logit model) but it returns an error saying}\n\\begin{verbatim}\n Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1.\n\\end{verbatim}\n\n  \\texttt{npcmstest()} supports conditional mean models with continuous\n  outcomes (\\texttt{glm()} objects are supported so that models that are\n  nonlinear in parameters can be estimated). The test is based on\n  residual bootstrapping to generate resamples for $Y$. In particular,\n  a resample for the residual vector ($\\hat\\epsilon^*$) is added to\n  the models' fit (i.e., $Y^*=\\hat Y + \\hat\\epsilon^*$) to generate a\n  resample under the null. This excludes binary outcome models and the\n  like because you would have generated a resample for $Y$ that no\n  longer contains zeros and ones, hence the error message.\n\n  Note that \\texttt{npcmstest} supports regression objects generated\n  by \\texttt{lm} and uses features specific to objects of type\n  \\texttt{lm} hence if you attempt to pass objects of a different type\n  the function cannot be expected to work.\n\n\\subsection{I want to plot the kernel function itself. How can I do\n    this?}\n\n  Use the \\texttt{npksum()} function and switch the evaluation and\n  training roles as in the following example that plots the 2nd, 4th,\n  6th and 8th order Epanechnikov kernels.\n  \\begin{singlespacing}\n\\begin{verbatim}\nZ <- seq(-sqrt(5),sqrt(5),length=100)\npar(mfrow=c(2,2))\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=2)$ksum,type=\"l\",main=\"Epanechnikov [order = 2]\")\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=4)$ksum,type=\"l\",main=\"Epanechnikov [order = 4]\")\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=6)$ksum,type=\"l\",main=\"Epanechnikov [order = 6]\")\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=8)$ksum,type=\"l\",main=\"Epanechnikov [order = 8]\")\n\\end{verbatim}\n  \\end{singlespacing}\n\n\\subsection{In version 0.20-0 and up I can `combine' steps such as\n    bandwidth selection and estimation. But when I do \\texttt{summary(model)} I don't get the same summary that I would\n    get from, say, \\texttt{summary(bw)} and then \\texttt{summary(model)}. How do I get bandwidth object summaries\n    when combining steps?}\n\n  Don't worry, the bandwidth object exists when you do the combined\n  steps and is easily accessed via \\texttt{summary(model\\$bws)} or\n  extracted via \\texttt{bw <- model\\$bws} where \\texttt{model} is\n  the name of your model.\n\n\\subsection{I estimated a semiparametric index model via \\texttt{model <-\n      npindex(y\\textasciitilde x1+x2) } but \\texttt{se(model)} returns NULL}.\n\n  Perhaps you want \\texttt{vcov(model)} instead (i.e., the asymptotic\n  variance-covariance matrix)? This is supported as of version 0.40-1\n  provided that you set \\texttt{gradients=TRUE} as the following snippet\n  demonstrates:\n\\begin{verbatim}\nset.seed(42)\nn <- 250\nx1 <- runif(n, min=-1, max=1)\nx2 <- runif(n, min=-1, max=1)\ny <- ifelse(x1 + x2 + rnorm(n) > 0, 1, 0)\n## Note that the first element of the vector beta is normalized to one\n## for identification purposes hence the first row and column of the\n## covariance matrix will contain zeros.\nmodel <- npindex(y~x1+x2, method=\"kleinspady\", gradients=TRUE)\nvcov(model)\nZ <- coef(model)[-1]/sqrt(diag(vcov(model)))[-1]\nZ\n\\end{verbatim}\n\n  Note that, alternatively, you can get robust bootstrapped standard\n  errors for the estimated model and gradients by adding the argument\n  \\texttt{errors=TRUE} to your call to npindex so that\n  \\texttt{se(model)} returns the vector of standard errors for the\n  estimated conditional mean where \\texttt{model} is the name of your\n  model.  Note that \\texttt{model\\$merr } will contain the standard\n  errors returned by \\texttt{se(model)} while \\texttt{model\\$gerr}\n  will return the matrix of standard errors for the gradients {\\em\n    provided} you have set \\texttt{gradients=TRUE} (furthermore,\n  \\texttt{model\\$mean.grad} and \\texttt{model\\$mean.gerr} will give\n  the average derivative and its bootstrapped standard errors). See\n  the documentation of npindex for further details. The following code\n  snippet demonstrates how one could do this for a simulated dataset.\n\\begin{verbatim}\n  n <- 100\n  x1 <- runif(n, min=-1, max=1)\n  x2 <- runif(n, min=-1, max=1)\n  y <- x1 - x2 + rnorm(n)\n\n  bw <- npindexbw(formula=y~x1+x2)\n  model <- npindex(bws=bw,errors=TRUE,gradients=TRUE)\n\n  se.mean <- model$merr\n  se.grad <- model$gerr\n\\end{verbatim}\n\n\\subsection{How do I interpret gradients from the conditional density\n    estimator?}\n\n  If you plot a conditional density $f(y|x)$ when $x$ is a scalar,\n  with gradients, by default you will get the following:\n\n  \\begin{enumerate}\n\n  \\item A plot of $\\partial f(y = \\text{median} | x)/\\partial x$\n    (admittedly not the most useful plot).  (If $y$ is discrete the\n    only difference is that you get a plot of $\\partial f(y =\n    \\text{(unconditional) mode} | x)/\\partial x$).\n\n  \\item A plot of $\\partial f(y | x = \\text{median})/\\partial x$.\n\n  \\end{enumerate}\n\n  If $x$ is multivariate (for example, 2D) you get:\n\n  \\begin{enumerate}\n\n  \\item A plot of $\\partial f(y = \\text{median} | x1, x2 =\n    \\text{median})/\\partial x_1$\n  \\item A plot of $\\partial f(y = \\text{median} | x1, x2 =\n    \\text{median})/\\partial x_2$\n\n  \\item A plot of $\\partial f(y = \\text{median} | x1 = \\text{median},\n    x2)/\\partial x_1$\n  \\item A plot of $\\partial f(y = \\text{median} | x1 = \\text{median},\n    x2)/\\partial x_2$\n\n  \\item A plot of $\\partial f(y | x1 = \\text{median}, x2 =\n    \\text{median})/\\partial x_1$\n  \\item A plot of $\\partial f(y | x1 = \\text{median}, x2 =\n    \\text{median})/\\partial x_2$\n\n  \\end{enumerate}\n\n\\subsection{When I run \\texttt{R} in batch mode via \\texttt{R CMD BATCH\n      filename.R} unwanted status messages (e.g., ```Multistart 1 of\n    10''') crowd out desired output. How can I turn off these unwanted\n    status messages?}\n\n  After loading the np library add the line\n  \\texttt{options(np.messages=FALSE)} and all such messages will be\n  disabled.\n\n\\subsection{I am getting an `unable to allocate...' message after\n    repeatedly interrupting large jobs.}\n\n  Repeated interruption of large jobs can reduce available memory\n  under R. This occurs because memory is allocated dynamically, and\n  memory that has been allocated is not freed when you interrupt the\n  job (the routine will clean up after itself only if it is allowed to\n  complete all computations - when you interrupt you never reach the\n  point in the code where the memory is freed). If this becomes an\n  issue simply restart R (i.e., exit then run a fresh R session).\n\n\\subsection{I have a large number of variables, and when using the\n    formula interface I get an error message stating `invoked with\n    improper formula'. I have double checked and everything looks\n    fine. Furthermore, it works fine with the data frame interface.}\n\n  The issue is that converting formulas into character strings in\n  \\texttt{R} appears to be limited to 500 characters. We are not aware\n  of a simple workaround so we simply advise that you use the data\n  frame interface when this occurs.\n\n\\subsection{How can I estimate additive semiparametric models?}\n\nGeneralized additive semiparametric models (see Hastie and Tibshirani\n(1990) \\cite{HASTIE_TIBSHIRANI:1990}) are implemented in the\n\\texttt{gam} package (though they do not support categorical\nvariables). The \\texttt{gam} package function \\texttt{gam()} (the\n\\texttt{mgcv} package also contains a similar function by the same\nname) uses iteratively reweighted least squares and either smoothing\nsplines (\\texttt{s($\\cdot$)}) or local polynomial regression fitting\n(\\texttt{`loess', lo($\\cdot$)}, with default manual `span' of 0.75,\nthe parameter which controls the degree of smoothing). The following\ncode snippet demonstrates the capabilities of the \\texttt{gam()}\nfunction via the \\verb+wage1+ dataset included in the \\texttt{np}\npackages using three numeric regressors.\n\n\\begin{verbatim}\nlibrary(gam)\ndata(wage1)\nattach(wage1)\nmodel.gam <- gam(lwage~s(educ)+s(exper)+s(tenure))\npar(mfrow=c(2,2))\nplot(model.gam,se=T)\ndetach(wage1)\n\\end{verbatim}\n\nThe \\texttt{mgcv} package also has an implementation of generalized\nadditive models via the identical function name, \\texttt{gam()}. The\nbiggest difference is that this uses generalized cross validation to\nselect the `span' (rather than manually) hence the degree of smoothing\nbecomes part of the method (as is the case for all functions in the\n\\texttt{np} package). The following code snippet demonstrates the\n\\texttt{mgcv} implementation of the \\texttt{gam()} function.\n\n\\begin{verbatim}\nlibrary(mgcv)\ndata(wage1)\nattach(wage1)\nmodel.gam <- gam(lwage~s(educ)+s(exper)+s(tenure))\npar(mfrow=c(2,2))\nplot(model.gam,se=T)\ndetach(wage1)\n\\end{verbatim}\n\n  \\subsection{I am using \\texttt{npRmpi} and am getting an error message\n  involving \\texttt{dlopen()}}\n\nOne of the most common problems experienced by users attempting to\ninstall and run MPI-aware programs is to first correctly identify the\nlocation of libraries and headers for the local MPI installation so\nthat installation can proceed.\n\nBy way of illustration, the following environment variables need to be\nset for the MacPorts version of OpenMPI (www.macports.org) running on\nMac OS X 10.8.3 (the environment commands listed below are for those\nusing the `bash' shell):\n\n\\begin{verbatim}\nexport LD_LIBRARY_PATH=/opt/local/lib\nexport RMPI_LIB_PATH=/opt/local/lib\nexport RMPI_TYPE=OPENMPI\nexport RMPI_INCLUDE=/opt/local/include/openmpi\n\nlaunchctl setenv LD_LIBRARY_PATH $LD_LIBRARY_PATH\nlaunchctl setenv RMPI_LIB_PATH $RMPI_LIB_PATH\nlaunchctl setenv RMPI_TYPE $RMPI_TYPE\nlaunchctl setenv RMPI_INCLUDE $RMPI_INCLUDE\n\\end{verbatim}\n\nOnce set, R (and optionally RStudio) ought to function as\nexpected. However, problems encountered during this phase are best\nresolved by someone with familiarity of the local installation.\n    \n  \\subsection{I am using the \\texttt{npRmpi} package but when I launch\n    one of the demo parallel jobs I get the error \\texttt{Error: could\n      not find function \"mpi.bcast.cmd\"}}\n\nWhen your demo program stops at the following point in your file\n\\begin{verbatim}\n> mpi.bcast.cmd(np.mpi.initialize(),\n+               caller.execute=TRUE)\nError: could not find function \"mpi.bcast.cmd\"\n\\end{verbatim}\nthis likely means that either you have failed to place the\n\\texttt{.Rprofile} file in the current or root directory as directed,\nor the \\texttt{.Rprofile} initialization code is not being loaded as\nexpected.\n\n\\begin{enumerate}\n\n\\item Make sure a copy of the initialization file \\texttt{Rprofile}\n  exists in your working or root directory and is named\n  \\texttt{.Rprofile}\n\n\\item Make sure you did not run \\texttt{R} with either the\n  \\texttt{--no-init-file} or \\texttt{--vanilla} option (this combines\n  a number of options including \\texttt{--no-init-file} which will\n  disable reading of \\texttt{.Rprofile})\n\n\\end{enumerate}\n\n\\subsection{I have estimated a partially linear model and want to\n  extract the gradient/fitted values of the nonparametric component}\n\nPeople use partially linear models because they focus on the\nparametric component and treat the nonparametric component as a\nnuisance. In fact, the partially linear model is estimated by\ncarefully getting rid of the nonparametric component $g(Z)$ prior to\nestimation, and then estimating a set of conditional moments\nnonparametrically.\n\nHowever, suppose after getting rid of this nonparametric nuisance\ncomponent we then wished to construct a consistent estimator of\n$g(Z)$, the nonparametric component for a partially linear model $y =\nX\\beta + g(Z) + u$. We might proceed as follows.\n\n\\begin{verbatim}\nlibrary(np)\n\nset.seed(42)\n\nn <- 250\nx1 <- rnorm(n)\nx2 <- rbinom(n, 1, .5)\n\nz1 <- rbinom(n, 1, .5)\nz2 <- rnorm(n)\n\ny <- 1 + x1 + x2 + z1 + sin(z2) + rnorm(n)\n\n## First, construct the partially linear model using local linear \n## regression.\n\nmodel <- npplreg(y~x1+factor(x2)|factor(z1)+z2,regtype=\"ll\")\n\n## Next, subtract the fitted parametric component from y so that we\n## have y-xbetahat. Since we can have factors we need to create the\n## `model matrix' but make sure we don't keep the intercept generated\n## by model.matrix hence the [,-1]. This gives us the numeric matrix X\n## which we multiply by the coefficient vector to obtain xbetahat which\n## we can subtract from y.\n\ny.sub <- y-model.matrix(~x1+factor(x2))[,-1]%*%coef(model)\n\n## Finally, regress this on the nonparametric components Z using npreg.\n\nmodel.sub <- npreg(y.sub~factor(z1)+z2,regtype=\"ll\",gradients=TRUE)\n\n## Now we can obtain derivatives etc. However, note that this is the\n## model containing the transformed y with respect to the nonparametric\n## components. We can use gradients(model.sub) etc. or plot them and\n## so forth.\n\nplot(model.sub,gradients=TRUE)\n\\end{verbatim}\n\n\\subsection{The R function `lag()' does not work as I expect it\n  to. How can I create the $l$th lag of a numeric variable in R to be\n  fed to functions in the \\texttt{np} package?}\n\nAs of version 0.60-2, time series objects are supported. However, if\nyou prefer you can use the function \\texttt{ts.intersect}, which can\nexploit R's lag function but return a suitable data frame, as per the\nfollowing illustration:\n\\begin{verbatim}\ndata(lynx)\nloglynx <- log(lynx)\nlynxdata <- ts.intersect(loglynx,\n                         loglynxlag1=lag(loglynx,-1),\n                         dframe=TRUE)\nmodel <- npregbw(loglynx~loglynxlag1,\n                 data=lynxdata)\nplot(model)\n\\end{verbatim}\nNote that in order for the above to work, the common argument fed to\n\\texttt{ts.intersect} must be a \\texttt{ts} object, so first cast it\nas such if it is not already (\\texttt{log(lynx)} is a \\texttt{ts}\nobject since \\texttt{lynx} was itself a \\texttt{ts} object).\n\nOr, you can use the \\texttt{embed} function to accomplish\nthis task. Here is a simple function that might work more along the\nlines that you expect. By default we `pad' the vector with NAs but you\ncan switch this to \\texttt{FALSE} if you prefer. The function will\nreturn a vector of the same length as the original vector with NA's\npadded for the missing values.\n\n\\begin{verbatim}\nlag.numeric <- function(x,l=1,pad.NA=TRUE) {\n  if(!is.numeric(x)) stop(\"x must be numeric\")\n  if(l < 1) stop(\"l (lag) must be a positive integer\")\n  if(pad.NA) x <- c(rep(NA,l),x)\n  return(embed(x,l+1)[,l+1])\n}\nx <- 1:10\nx.lag.1 <- lag.numeric(x,1)\n\\end{verbatim}\n\n\\subsection{Can I provide sample weights to be used by functions in\n  the \\texttt{np} package?}\n\nUnfortunately, at this stage the answer is `no', at least not directly\nwith many of the functions as they stand. However, the function\n\\texttt{npksum} underlies many of the functions in the \\texttt{np}\npackage and it supports passing of weights, so you may be able to use\nthis function and do a bit of coding to fulfill your needs. Kindly see\n\\texttt{?npksum} for illustrations.\n\nAlternatively, if reweighting of sample data is sufficient for your\nneeds then you can feed the weighted sample realizations directly to\nexisting functions.\n\n\\subsection{Quantile regression is slow, particularly for very\n  small/large values of tau. Can this be sped up?}\n\nYes, indeed this can be the case for extreme values of tau (and not so\nextreme values as well). The reason for this is because numerical\nmethods are used to invert the CDF. This must be done for each\npredictor observation requiring the solution to $n$ (or $neval$)\noptimization problems. An alternative is to compute the\n`pseudo-inverse' via a `lookup method'. In essence, one computes the\nconditional CDF for a range of $y$ values, and then computes the\npseudo-inverse which is defined as\n\\begin{align*}\n  q_{\\tau}(x) &= \\inf \\{ y: \\, F(y|x) \\geq \\tau \\}\\\\\n  & = \\sup \\{ y: \\, F(y|x) \\le \\tau \\}\n\\end{align*}\n\nThe following code demonstrates this approach for the example used in\nthe help file for \\texttt{npqreg} (see \\texttt{?npqreg}).\n\n\\begin{verbatim}\ndata(\"Italy\")\nattach(Italy)\n     \nbw <- npcdensbw(gdp~year)\n\n## Set a grid of values for which the conditional CDF will be computed\n## with a range that extends well beyond the range of the data\n\nn.eval <- 1000\ngdp.er <- extendrange(gdp,f=2)\ngdp.q <- quantile(gdp,seq(0,1,length=n.eval))\ngdp.eval <- sort(c(seq(gdp.er[1],gdp.er[2],length=n.eval),gdp.q))\nn.q <- length(gdp.eval)\n\n## We only need to compute the conditional quantiles for each unique\n## value of year\n\nyear.unique <- unique(year)\n\n## Consider a range of values for tau\n\ntau.seq <- c(0.01,0.05,0.25,0.5,0.75,0.95,0.99)\ngdp.tau <- matrix(NA,length(year.unique),length(tau.seq))\n\nfor(j in 1:length(tau.seq)) {\n  cat(\"\\r\",j,\"of\",length(tau.seq))\n  tau <- tau.seq[j]\n  for(i in 1:length(year.unique)) {\n    F <- fitted(npcdist(bws=c(bw$ybw,bw$xbw),\n                        txdat = year,\n                        tydat = gdp,\n                        exdat = rep(year.unique[i],n.q),\n                        eydat = gdp.eval))\n    ## For each realization of the predictor, compute the \n    ## the pseudo-inverse\n    gdp.tau[i,j] <- ifelse(tau>=0.5, max(gdp.eval[F<=tau]), \n                                     min(gdp.eval[F>=tau]))\n  }\n}\n\n## Plot the results\n\nplot(year,gdp,ylim=c(min(gdp.tau,gdp),max(gdp.tau,gdp)))     \nfor(j in 1:length(tau.seq)) {\n  lines(year.unique,gdp.tau[,j],col=j+1,lty=j,lwd=2)\n}\n\nlegend(min(year),max(gdp.tau,gdp),\n       paste(\"tau=\",tau.seq),\n       col=1:length(tau.seq)+1,\n       lty=1:length(tau.seq),\n       lwd=2)\n\\end{verbatim}\n\n\\subsection{How can I generate resamples from the unknown distribution\n  of a set of data based on my smooth kernel density estimate?}\n\nThis can be accomplished by picking a sample realization, uniformly at\nrandom, then drawing from the kernel distribution centered on that\ntraining point with scale equal to the bandwidth. Below is a\ndemonstration for the `Old Faithful' data where we draw a random\nsample of size $n=1,000$ where a Gaussian kernel was used for the\ndensity estimator.\n\n\\begin{verbatim}\ndata(faithful)\nn <- nrow(faithful)\n\nx1 <- faithful$eruptions\nx2 <- faithful$waiting\n\n## First compute the bandwidth vector\n\nbw <- npudensbw(~x1+x2,ckertype=\"gaussian\")\n\n## Next generate draws from the kernel density (Gaussian)\n\nn.boot <- 1000\n\ni.boot <- sample(1:n,n.boot,replace=TRUE)\nx1.boot <- rnorm(n.boot,x1[i.boot],bw$bw[1])\nx2.boot <- rnorm(n.boot,x2[i.boot],bw$bw[2])  \n\n## Plot the density for the bootstrap sample using the original \n## bandwidths\n  \nplot(npudens(~x1.boot+x2.boot,bws=bw$bw),view=\"fixed\",xtrim=-.2,neval=100)\n\\end{verbatim}\n\n\\subsection{Some of my variables are measured with an unusually large\n  number of digits\\dots should I rescale?}\n\nIn general there should be no need to rescale data for one's\nstatistical analysis. However, occasionally one can encounter issues\nwith numerical accuracy (e.g.\\ numerical `overflow') regardless of the\nmethod used. It is therefore prudent to be aware of this issue. For\ninstance, if your dependent variable represents housing prices in a\nparticular currency and entries are recorded as e.g.\\\n145,000,000,000,000,000,000 foounits, then it might be prudent to\ndeflate this variable by $10^{20}$ (i.e.\\ 1.45) so that e.g.\\ sums of\nsquares are numerically stable (say when using least squares\ncross-validation). Of course you can run your analysis with and\nwithout the adjustment and see whether it matters or not. But it is\nsometimes surprising that such things can in fact make a\ndifference. See \\texttt{?scale} for a function whose default method\ncenters and/or scales the columns of a numeric matrix.\n\n\\subsection{The local linear gradient estimates appear to be somewhat\n  `off' in that they do not correspond to the first derivative of the\n  estimated regression function.}\n\nIt is perhaps not a widely known fact that the local linear partial\nderivatives obtained from the coefficients of the Taylor\napproximation, $\\hat b(x)$, are not the analytical derivative of the\nestimated regression $\\hat g(x)$ with respect to $x$.  The local\nlinear estimator is obtained by minimizing the following weighted\nleast squares function (consider the one-predictor case to fix ideas)\n\\begin{equation*}\n\\sum_i(Y_i - a - b (x-X_i))^2 K\\left(\\frac{x-X_i}{h}\\right)\n\\end{equation*}\nand the estimated regression function $\\hat g(x)$ is given by\n\\begin{equation*}\n\\hat g(x) =\\hat a \n\\end{equation*}\nwhile the gradient is given by\n\\begin{equation*}\n\\hat \\beta(x)=\\hat b.\n\\end{equation*}\nBut the analytical gradient (i.e.\\ the partial derivative of $\\hat\ng(x)=\\hat a$ with respect to $x$) is \\textsl{not} $\\hat b$ unless the\nbandwidth is very large (i.e.\\ $h=\\infty$) in which case\n$K((x-X_i)/h)=K(0)$ and one gets the standard linear least squares\nestimates for $\\hat g(x)$ and $\\hat\\beta(x)$ (if you compute the\npartial algebraically you will see they are not the same).\n\nSo, the derivative estimates arising directly from the local linear\nestimator will differ from the analytical derivatives, even though\nthey are asymptotically equivalent under standard conditions required\nfor consistency. Thus, if economic constraints are imposed on the\ndirect derivatives, this may produce an estimated surface which is not\nconsistent with the constraints.  This can be avoided by imposing the\nconstraints on the analytical derivatives of the local polynomial\nestimator being used.\n\n\\subsection{How can I turn off all console I/O?}\n\nTo disable all console I/O, set \\texttt{options(np.messages=FALSE)}\nand wrap the function call in \\texttt{suppressWarnings()} to disable any\nwarnings printed to the console. For instance\n\\begin{verbatim}\nlibrary(np)\noptions(np.messages=FALSE)\nset.seed(42)\nn <- 100\nx <- sort(rnorm(n))\nz <- factor(rbinom(n,1,.5))\ny <- x^3 + rnorm(n)\nmodel.np <- suppressWarnings(npreg(y~x+z))\n\\end{verbatim}\nought to produce no console I/O whatsoever in the call to\n\\texttt{npreg}.\n\n\\bibliographystyle{plain} \n\n\\bibliography{np_faq}\n\n\\clearpage\n\n\\appendix\n\n\\section*{Changes from Version 0.60-1 to 0.60-2 [27-Jun-2014]}\n\n\\begin{itemize}\n\n\\item added timeseries support for all relevant objects (i.e.\\ for a\n  \\texttt{ts()} vector data object \\texttt{x},\n  \\verb^npreg(x~lag(x,-1)+lag(x,-2))^ is now supported)\n\n\\item added total time to summary for bandwidth objects\n\n\\item \\texttt{npqreg()} no longer accepts \\texttt{gradients=TRUE} when\n  gradients are in fact not supported\n\n\\item \\texttt{npqreg()} fails with an informative message when passed\n  a conditional density bandwidth object\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.60-0 to 0.60-1 [6-Jun-2014]}\n\n\\begin{itemize}\n\n\\item Fixed glitch in adaptive\\_nn/generalized\\_nn bandwidths that\n  affected all routines that rely on non-fixed bandwidths\n\n\\item Tweaks to search for adaptive\\_nn/generalized\\_nn initial search values\n\n\\item Fixed glitch in local linear estimation with adaptive\\_nn bandwidths\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.50-1 to 0.60-0 [1-Jun-2014]}\n\n\\begin{itemize}\n\n\\item Ordered kernel types now default to liracine/liracine\n  (normalized) for conditional/unconditional objects, respectively\n  (the previous default, i.e.\\ the Wang van Ryzin kernel, is poorly\n  behaved when smoothing out of ordered predictors is appropriate)\n\n\\item Added analytical ordered CDF kernels, resulting in significant\n  speedups for cross validation with ordered variables\n\n\\item Added analytical ordered convolution kernels, resulting in significant\n  speedups for least-squares cross validation with ordered variables\n\n\\item The entire C backend has been rewritten and improved in almost every\n  regard\n\n\\item The Rmpi backend has been updated to Rmpi version 0.6-5\n\n\\item Glitch in adaptive convolution kernels corrected\n\n\\item Added truncated gaussian kernel (see \\texttt{ntpgauss()} for\n  modifying the truncation radius)\n\n\\item Support for trees complete (use \\texttt{options(np.tree=TRUE)})\n  which when used in conjunction with bounded kernels (i.e.\\\n  \\texttt{\"epanechnikov\"}/\\texttt{\"truncated gaussian\"}) can reduce\n  the computational burden for certain problems\n\n\\item Optimizers that make use of Powell's direction set method now accept\n  additional arguments that can be used to potentially improve\n  default settings\n\n\\item Default search settings for optimizers that make use of Powell's\n  direction set method should better scale to the range of variables\n\n\\item Added mean absolute deviation/1.4826 to mix of robust scale elements\n\n\\item Corrected error in order of conditional density/distribution manual\n  bandwidths pointed out by Decet Romain\n\n\\item Figure in vignette not displaying properly, needed\n  \\texttt{png=TRUE} reported by Christophe Bontemps\n\n\\item Using \\texttt{chol2inv}/\\texttt{chol} rather than solve\n  throughout R routines that rely on inversion\n\n\\item Fixed glitch in \\texttt{npindexbw()} to stop maxit from blowing\n  up every time convergence fails\n\n\\item Fixed issue with summary reporting incorrect value of objective function\n  in certain bandwidth objects\n\n\\item When \\texttt{nmulti} $>$ 1, the full multi-starting search\n  history is now returned in a vector named fval.history\n\n\\item Added \\texttt{na.action} for consistency with other R functions\n  such as \\texttt{lm()}\n\n\\item New function \\texttt{npquantile()} that returns smooth\n  univariate quantiles\n\n\\item \\texttt{npksum()} explicitly only uses raw bandwidths now (and\n  will emit an error if passed numeric scale factors, bandwidth\n  objects are still OK)\n\n\\item Fixed regression in \\texttt{npindex()} with bootstrapped\n  standard errors\n\n\\item Code makes use of one call to \\texttt{npksum()} in\n  \\texttt{npindex()} and \\texttt{npscoef()} where possible rather than\n  two separate calls\n\n\\item Updated \\texttt{npsigtest()} for addition of power and added\n  joint test to the mix\n\n\\item Changed \\texttt{ceiling()} to \\texttt{max(1,round())} in\n  \\texttt{b.star()} per Dimitris Politis's suggestion\n\n\\item Reworked the interface for \\texttt{npcopula()} to avoid two\n  bandwidths and \\texttt{density=TRUE} but exploiting passing of\n  either \\texttt{npudistbw()} (copula) or \\texttt{npudensbw} (copula\n  density)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-13 to 0.50-1 [13-Mar-2013]}\n\n\\begin{itemize}\n\n\\item The functions \\texttt{npudist()} and \\texttt{npudistbw()} are\n  now uncoupled from \\texttt{npudens()} and \\texttt{npudensbw()}\n  (previously they relied on unconditional PDF bandwidths due to the\n  lack of a multivariate mixed-data method for selecting bandwidths\n  for CDFs - now with Li \\& Racine (2013) we have a robust method\n  hence the split)\n\n\\item The use of \\texttt{cdf=TRUE} is deprecated for\n  \\texttt{npudens()} and \\texttt{npudensbw()} objects due to the\n  uncoupling described above\n\n\\item Fixed glitch in \\texttt{gradient} standard errors in\n  \\texttt{npindex()} where identical standard errors were output in\n  \\texttt{model\\$gerr}\n\n\\item Fixed glitch in covariance matrix in \\texttt{npindex()}\n  (\\texttt{\"ichimura\"}) where covariance matrix was not symmetric\n\n\\item Fixed glitch in npksum where use of\n  \\texttt{bwtype=\"adaptive\\_nn\"} and \\texttt{operator=\"integral\"}\n  produced the survivor function rather than the cumulative\n  distribution function\n\n\\item Cleaned up internals for \\texttt{npcmstest()}\n\n\\item Using .onUnload rather than .Last.lib in zzz.R\n\n\\item Fixed glitch in \\texttt{npreg()} summary where `Residual\n  standard error' was reporting residual variance\n\n\\item \\texttt{npksum()} functionality extended\n\n\\begin{itemize}\n\n\\item \\texttt{npksum()} can now return the matrix of kernel weights\n  (which can be useful for constrained estimation, by way of\n  illustration, or constructing novel kernel-based procedures without\n  the need to write extensive code)\n\n\\item \\texttt{npksum()} can now accept different operators on a\n  product kernel, for example,\n\\begin{verbatim}\n  npksum(txdat=data.frame(x1,x2),operator=c(\"derivative\",\"normal\"),bws=c(1,1))\n\\end{verbatim}\n  will use the derivative kernel for x1 (i.e. the derivative of the\n  gaussian kernel) and the default kernel for x2 (i.e. a standard\n  kernel such as the gaussian) thereby allowing the user to program a\n  number of estimators such as conditional CDFs etc. that were\n  previously not available via \\texttt{npksum()}\n\n\\end{itemize}\n\n\\item Fixed glitch with variable scope where certain objects could not be\n  found in the environment\n\n\\item Added function \\texttt{npcopula()} for d-dimensional copula\n  estimation via inversion\n\n\\item Modified stopping rules in \\texttt{npregiv()} and\n  \\texttt{npregivderiv()}\n\n\\item Added reference to $R^2$ measure (Doksum and Samarov (1995))\n\n\\item Startup message points to the faq, faq is now a vignette\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-12 to 0.40-13 [05-Mar-2012]}\n\n\\begin{itemize}\n\n\\item Added new function \\texttt{npregivderiv()} that implements the\n  IV derivative method of Florens and Racine (2012)\n\n\\item Added more passable parameters to \\texttt{npregiv()}\n  (multistarting, parameters passed to \\texttt{optim()} for\n  cross-validation)\n\n\\item Changes to code to improve compliance with R `Writing portable\n  packages' guidelines and correct partial argument matches\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-11 to 0.40-12 [24-Nov-2011]}\n\n\\begin{itemize}\n\n\\item Added option (user request) to hold the bandwidth fixed but\n  optimize the parameters in the single index model\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-10 to 0.40-11 [24-Oct-2011]}\n\n\\begin{itemize}\n\n\\item Corrected code regression in single index errors introduced\n  inadvertently in 0.40-10\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-9 to 0.40-10 [24-Oct-2011]}\n\n\\begin{itemize}\n\n\\item Modified Silverman's adaptive measure of spread to reflect\n  changes in \\texttt{sd()} (sd on matrix deprecated)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-8 to 0.40-9 [30-July-2011]}\n\n\\begin{itemize}\n\n\\item Renamed COPYING file to COPYRIGHTS\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-7 to 0.40-8 [29-July-2011]}\n\n\\begin{itemize}\n\n\\item Fixed issue where calling \\texttt{npplot} resets system seed\n\n\\item Updated examples in docs so that plot is recommended throughout\n  (and not \\texttt{npplot} that is invoked by \\texttt{plot})\n\n\\item Fixed regression in \\texttt{npindex} when \\texttt{gradients=TRUE} and\n  \\texttt{errors=TRUE}\n\n\\item Function \\texttt{npindex/npindexbw} now accepts additional\n  arguments and implements this properly (i.e. proper implementation\n  by Tristen of Version 0.30-8/0.30-9 change for \\texttt{npindex})\n\n\\item Function \\texttt{npplreg} now supports factors in the parametric\n  part just like \\texttt{lm()} does\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-6 to 0.40-7 [8-Jun-2011]}\n\n\\begin{itemize}\n\n\\item Function \\texttt{npregiv} now supports exogenous $X$ and multivariate\n  $Z$ and $W$.\n\n\\item \\texttt{demo(npregiv)} provides a useful illustration.\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-5 to 0.40-6 [1-Jun-2011]}\n\n\\begin{itemize}\n\n\\item Added a new function \\texttt{npregiv} that conducts nonparametric\n  instrumental regression a la Darolles, Fan, Florens and Renault\n  (2011, Econometrica) and Horowitz (2011, Econometrica). Note that\n  this function currently returns the fitted $\\varphi(z)$ (i.e.\\ lacks\n  much functionality relative to other \\texttt{np} functions) and is in\n  `beta status' until further notice.\n\n\\item Added a new dataset \\texttt{Engel95} that allows one to estimate\n  Engel curves using the new nonparametric instrumental regression\n  function \\texttt{npregiv}.\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-4 to 0.40-5 [26-Apr-2011]}\n\n\\begin{itemize}\n\n\\item Fixed issue with \\texttt{npindexbw} where, for certain problems,\n  starting values needed refinement otherwise convergence would fail\n  (we now use an improved normalization for the starting values)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-3 to 0.40-4 [21-Jan-2011]}\n\n\\begin{itemize}\n\n\\item Fixed issue with \\texttt{ckertype} and \\texttt{ckerorder} not\n  being propagated in \\texttt{np.singleindex.bw.R}\n\n\\item Fixed issue with negative penalties being returned by\n  \\texttt{bwmethod=\"cv.aic\"} in \\texttt{npregbw} (ought to have been\n  +infinity)\n\n\\item Error being thrown by \\texttt{system(..., intern=TRUE)} when\n  \\texttt{mpi.quit()} is called, changed to FALSE (change to\n  \\texttt{system()} behaviour detailed in R CHANGELOG 2.12.0)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-1 to 0.40-3 [23-Jul-2010]}\n\n\\begin{itemize}\n\n\\item Added random seed (defaults to 42) to \\texttt{npscoefbw} to ensure\n  consistent values for optimization for successive invocations on the\n  same data\n\n\\item Fixed glitch in multistarting in \\texttt{npscoefbw} whereby\n  multistarting was not working (always returned last computed\n  function value and not the minimum)\n\n\\item Fixed issue for \\texttt{npRmpi} where the \\texttt{C} code\n  underlying regression cross-validation (code in \\texttt{jksum.c})\n  differs between np and npRmpi (both are correct with the latter\n  being a tad slower)\n\n\\item Fixed a scope issue whereby a user would write a function that\n  calls an \\texttt{np/npRmpi} command, however, objects passed to the\n  user's function and called by the \\texttt{np/npRmpi} command\n  (i.e. such as \\texttt{newdata}) cannot be found in the environment\n  yet they exist outside of the function\n\n\\item Fixed issue with \\texttt{bwscaling=TRUE} and\n  \\texttt{bwmethod=\"cv.aic\"} in npreg\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-0 to 0.40-1 [4-Jun-2010]}\n\n\\begin{itemize}\n\n\\item Added asymptotic standard errors to \\texttt{npindex} for the\n  Klein and Spady and Ichimura parameter estimates which, when\n  \\texttt{gradients=TRUE}, can be extracted via \\texttt{vcov(foo)}\n  where \\texttt{foo} is a npsingleindex object (the Z-scores can be\n  obtained via \\texttt{Z <- coef(foo)[-1]/sqrt(diag(vcov(foo)))[-1]})\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-9 to 0.40-0 [25-May-2010]}\n\n\\begin{itemize}\n\n\\item Modified codebase to enable dynamic spawning for\n  interactive sessions in \\texttt{npRmpi}\n\n\\item Interactive examples supported in \\texttt{npRmpi}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-8 to 0.30-9 [17-May-2010]}\n\n\\begin{itemize}\n\n\\item Fixed issue where \\texttt{ukertype} and \\texttt{okertype} were being\n  ignored by \\texttt{npscoef}\n\n\\item Fixed code regression (dating to version 0.30-4) where\n  \\texttt{random.seed=42} was not initialized in functions\n  \\texttt{npcmstest}, \\texttt{npdeneqtest}, \\texttt{npindexbw},\n  \\texttt{npsdeptest}, \\texttt{npqcmstest}, \\texttt{npsigtest},\n  \\texttt{npsymtest}, \\texttt{npunitest}, and \\texttt{npplot}\n\n\\item Fixed issue with saving and restoring random seed in \\texttt{npdeptest}\n\n\\item Changes to codebase to modify method used to prevent division by\n  zero\n\n\\item New vignette for the \\texttt{npRmpi} package\n  \\texttt{(vignette(\"npRmpi\",package=\"npRmpi\"))}\n\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-7 to 0.30-8 [20-Apr-2010]}\n\n\\begin{itemize}\n\n\\item Implemented moment version of metric entropy in \\texttt{npsymtest}\n  and \\texttt{npunitest} with warnings about their use documented\n  carefully and exceptions trapped and warnings issuing when detected\n \n\\item Cleaned up print/summary output formatting of some functions\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-6 to 0.30-7 [15-Feb-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npunitest} for entropy-based testing of\n  equality of univariate densities as described in Maasoumi and Racine\n  (2002)\n\n\\item Updated vignette to reflect new functions from 0.30-4 upwards\n  (Table 1: np functions)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-5 to 0.30-6 [3-Feb-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npsdeptest} for entropy-based testing of\n  nonlinear serial dependence described in Granger, Maasoumi and\n  Racine (2004)\n\n\\item Added function \\texttt{npdeptest} for entropy-based testing of\n  nonlinear pairwise dependence described in Maasoumi and Racine\n  (2002)\n\n\\item Added more bootstrap options to \\texttt{npsymtest} (now both iid\n  and time-series bootstrapping are supported)\n\n\\item Cleaned up summary formatting in the vignette by adding\n  \\verb+\\usepackage[utf8x]{inputenc}+ to the Sweave file np.Rnw\n\n\\item Fixed issue with saving and restoring random seed when there was\n  none in the environment\n  \n\\end{itemize}\n\n\\section*{Changes from Version 0.30-4 to 0.30-5 [29-Jan-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npdeneqtest} for integrated squared\n  difference testing of equality of densities as described in\n  Maasoumi, Li, and Racine (2009), Journal of Econometrics\n\n\\item Save random seed prior to setting seed in certain functions,\n  then restore seed after function completes\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-3 to 0.30-4 [27-Jan-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npsymtest} for entropy-based testing of\n  symmetry described in Maasoumi and Racine (2009), Econometric\n  Reviews\n\n\\item Added function \\texttt{b.star} that automates block length\n  selection for the stationary and circular bootstrap\n\n\\item Cleaned up docs\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-2 to 0.30-3 [28-May-2009]}\n\n\\begin{itemize}\n\n\\item Corrected error in Epanechnikov convolution kernels for fixed\n  and generalized bandwidth objects\n\n\\item Changed default example in \\texttt{npscoef}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-1 to 0.30-2 [19-Apr-2009]}\n\n\\begin{itemize}\n\n\\item \\texttt{min(std,IQR/1.348)} is the adaptive measure of spread. We\n  now test for the pathological case where \\texttt{IQR=0} but \\texttt{std>0}\n  and return \\texttt{std} in this instance\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-0 to 0.30-1 [29-Jan-2009]}\n\n\\begin{itemize}\n\n\\item \\texttt{predict()} now supports bandwidth, density, distribution,\n  conbandwidth, condensity, and condistribution objects\n\n\\item Consistently allow predictions for categorical values outside of\n  support of training data\n\n  Note that predictions based upon unconditional density objects\n  defined over categorical variables that lie outside the support of\n  the training data may no longer be true probabilities (i.e., as\n  defined over the training data and the extended/augmented support --\n  their sum may exceed one) and may therefore require renormalization\n  by the user\n\n\\item Fixed a numerical issue which could hinder \\texttt{npregbw()}'s\n  cross validation with higher-order kernels\n\n\\item Default \\texttt{nmulti} in \\texttt{npplregbw()} is now set correctly\n\n\\item Fixed a bug with the ridging routine in \\texttt{npscoefbw()}, added\n  ridging to \\texttt{npscoef()}\n\n\\item Fixed minor i/o issue with \\texttt{Multistart 1 of...} using\n  \\texttt{npscoefbw()}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-4 to 0.30-0 [15-Jan-2009]}\n\n\\begin{itemize}\n\n\\item Added basic user-interrupt checking for all underlying C code so\n  that either \\verb+<Ctrl-C>+ (Rterm) or the `STOP' icon (Rgui) will\n  interrupt all running processes. This has a number of desirable side\n  effects in addition to being able to interrupt C-based processes\n  including i) R no longer showing up as `not responding' under the\n  task manager (Windows) or the activity monitor (Mac OS X) and ii)\n  buffered output now being correctly displayed when using Rgui under\n  Windows and Mac OS X\n\n  Note that repeated interruption of large jobs can reduce available\n  memory under R - if this becomes an issue (i.e., you get a `cannot\n  allocate...' error under R) simply restart R (i.e., exit then run a\n  fresh R session)\n\n\\item Added a function \\texttt{npseed()} that allows the user to\n  set/reset the random seed for all underlying C routines\n\n\\item Fixed a bug that caused \\texttt{npplregbw()} to ignore any\n  kernel options for the regression of $y$ on $z$\n\n\\item Refined certain constants used in the normal-reference density\n  bandwidth rule for increased accuracy\n\n\\item Moved from using the maximum likelihood estimate of variance\n  throughout to the degrees of freedom corrected estimate (all\n  variance estimates now change by the factor (n-1)/n)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-3 to 0.20-4 [19-Nov-2008]}\n\n\\begin{itemize}\n\n\\item Using an adaptive measure of spread throughout.  The scale\n  factor reported for a bandwidth can appear to be small when the\n  sample standard deviation of the associated variable is inflated due\n  to the presence of outliers.  Furthermore, supplying a scale factor\n  of, say, 1.06 for density estimation when there are outliers that\n  inflate the standard deviation may oversmooth rather dramatically in\n  the presence of outliers.  We now use the measure found in Silverman\n  (1986, equation (3.30)) which is min(standard deviation,\n  interquartile range/1.349). This robust choice produces expected\n  results for scale factors in the presence of outliers\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-2 to 0.20-3[14-Nov-2008] }\n\n\\begin{itemize}\n\n\\item Fixed a typo which caused \\texttt{predict()} and \\texttt{plot()} to\n  abort when called on \\texttt{plregression} objects, and which also\n  prevented \\texttt{print()} and \\texttt{summary()} from printing\n  information about the kernels used when called on \\texttt{plregression}\n  objects\n\n\\item Fixed a typo which caused partially linear regressions to crash\n  when out-of-sample responses were provided with evaluation data\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-1 to 0.20-2 [02-Nov-2008]}\n\n\\begin{itemize}\n\n\\item Allow for evaluation outside of discrete support of factors in\n  \\texttt{npksum()} and fixed a warning in jksum\n\n\\item Fixed a bug which lead to unpredictable behavior when there were\n  more categorical values for the training data than realisations\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-0 to 0.20-1 [13-Aug-2008]}\n\n\\begin{itemize}\n\n\\item Work-around for scale-factor issues during \\texttt{npregbw()} cv\n  when changing the training data\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.14-3 to 0.20-0 [28-Jul-2008]}\n\n\\begin{itemize}\n\n\\item \\texttt{npksum()} now supports an expanded set of kernels\n  (including convolution, derivative and integral), which can be\n  selected via the \\texttt{operator =} argument\n\n\\item Automatic bandwidth searches are now performed when attempting\n  to evaluate on data without bandwidths. This allows users to combine\n  bandwidth selection and estimation in one step\n\n\\item The \\texttt{npsigtest()} interface is brought in line with other\n  functions (S3)\n\n\\item Significance tests can now be performed on \\texttt{npreg()}\n  outputs, so \\texttt{npsigtest(modelname)} is now supported\n\n\\item Added a vignette and faq. To see the vignette try\n  \\texttt{vignette(\"np\",package=\"np\")}\n\n\\item \\texttt{summary()} on \\texttt{npconmode()} now properly\n  retrieves names from bandwidth objects\n\n\\item Fixed the 6th and 8th order epanechnikov kernels\n\n\\item Fixed some quietness issues\n\n\\item \\texttt{npplot()} now returns data upon request for conditional\n  densities\n\n\\item \\texttt{npreg()} and \\texttt{npcdens()} now take the appropriate\n  limits in some pathological cases\n\n\\item User supplied bandwidths now operate seamlessly with the formula\n  interface\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.14-2 to 0.14-3 [02-May-2008]}\n\n\\begin{itemize}\n\n\\item Fixed a glitch that only arose when using the \\texttt{liracine}\n  unordered kernel in the presence of irrelevant variables. The upper\n  bound for numerical search was constrained to be (c-1)/c [that for\n  the \\texttt{aitchisonaitken} unordered kernel] but ought to have been\n  1. The summary output would therefore show a value of lambda hitting\n  the (smaller) upper bound (c-1)/1 when it may have hit the (larger)\n  upper bound 1\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.14-1 to 0.14-2 [11-Jan-2008]}\n\n\\begin{itemize}\n\n\\item Relaxed checking tolerances slightly to prevent spurious\n  'invalid bandwidth' errors\n\n\\item Empty sections were removed from help files\n\n\\item \\texttt{example(foobar)} now works again. This was disabled in\n  0.14-1 at the request of the \\texttt{R} maintainers in order to shorten\n  the duration of \\texttt{R CMD check}. All examples remained in the help\n  files but due to the presence of `dontrun' they were not run when\n  \\texttt{example(foobar)} is requested. Now a limited subset is run\n  while the full set of examples remain in the documents\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.13-1 to 0.14-1 [18-Dec-2007] }\n\n\\begin{itemize}\n\n\\item Now use \\texttt{optim()} for minimisation in single index and\n  smooth coefficient models\n\n\\item Fixed bug in klein-spady objective function\n\n\\item Standard errors are now available in the case of no continuous\n  variables\n\n\\item Summary should look prettier, print additional information\n\n\\item Tidied up lingering issues with out-of-sample data and\n  conditional modes\n\n\\item Fixed error when plotting asymptotic errors with conditional\n  densities\n\n\\item Fixed a bug in \\texttt{npplot()} with partially linear regressions\n  and \\texttt{plot.behavior=\"data\"} or \\texttt{\"plot-data\"}\n\n\\item Maximum default number of multistarts is 5\n\n\\item Least-squares cross-validation of conditional densities uses a\n  new, much faster algorithm\n\n\\item New, faster algorithm for least-squares cross-validation for\n  both local-constant and local linear regressions\n\n  Note that the estimator has changed somewhat: both cross-validation\n  and the estimator itself use a method of shrinking towards the local\n  constant estimator when singularity would otherwise lead to the\n  breakdown of the estimator. This arises in sparse data settings in\n  conjunction with small bandwidths for one or more regressor\n\n\\item Optimised smooth coefficient code, added ridging\n\n\\item Fixed bug in uniform CDF kernel\n\n\\item Fixed bug where \\texttt{npindexbw()} would ignore\n  \\texttt{bandwidth.compute = FALSE} and compute bandwidths when\n  supplied with a preexisting bw object\n\n\\item Now can handle estimation out of discrete support\n\n\\item Summary would misreport the values of discrete scale factors\n  which were computed with \\texttt{bwscaling = TRUE}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.12-1 to 0.13-1  [03-May-2007]}\n\n\\begin{itemize}\n\n\\item Bandwidths are now checked for validity based on their variable\n  and kernel types\n\n\\item np now does a better job of preserving names of some 'y' data\n\n\\item Names of coefficients returned from \\texttt{coef()} now match\n  variable names\n\n\\item Fixed some corner cases in \\texttt{npksum()} involving the\n  dimensionality of outputs\n\n\\item Fixed deprecation warnings in \\texttt{R} 2.5.0 caused by use of \\$\n  on atomic objects\n\n\\item Various and sundry bug fixes in \\texttt{npscoef()}\n\n\\item \\texttt{npscoef()} now handles discrete 'z' data\n\n\\item Predict now accepts the argument 'se.fit', like predict.lm\n\n\\item Fixed bug where incorrect asymptotic standard errors of\n  gradients for regression objects were being displayed in \\texttt{npplot()}\n\n\\item Fixed bug where errors of gradients of regression objects were\n  not being returned in matrix form\n\n\\item \\texttt{vcov()} now works with partially linear regression objects\n\n\\item Fixed detection of evaluation responses when using the formula\n  interface\n\n\\item Pre-computed bandwidth objects are now provided for some of the\n  more computationally burdensome examples\n\n\\item Added Jeffrey Wooldridge's WAGE1 dataset with qualitative\n  variables (married, female, nonwhite)\n\n\\item Predictions outside of discrete support for regressions and\n  conditional densities are now allowed\n\n\\item Fixed sign issue with scaling of standard errors in the single\n  index model\n\n\\item Fixed error when calculating some bandwidths/scale factors for\n  display purposes\n\n\\item Bug in passing certain arguments to \\texttt{npcdensbw()} fixed\n\n\\item Added predict method for qregression objects\n\n\\item Proper normalisation for liracine kernel shown in summary\n\n\\item Fixed output bug ($\\hat ~$H) in summary method for sigtest\n  objects\n\n\\item Fixed regression with plotting of bootstrapped errors in\n  perspective plots\n\n\\item \\texttt{npcdist()} no longer incorrectly calls \\texttt{npcdens()}\n\n\\item Fixed spacing between var name and p-value in significance test\n  summaries\n\n\\end{itemize}\n\n\\section*{Version 0.12-1 [19-Nov-2006]}\n\n\\begin{itemize}\n\n\\item Initial release of the np package on CRAN\n\n\\end{itemize}\n\n\n\\end{document}\n",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/vignettes/np_faq.Rnw": "%% $Id: np_faq.tex,v 1.90 2014/06/27 14:22:06 jracine Exp jracine $\n%\\VignetteIndexEntry{Frequently Asked Questions (np)}\n%\\VignetteDepends{np}\n%\\VignetteKeywords{nonparametric, kernel, categorical}\n%\\VignettePackage{np}\n\n\\documentclass[12pt]{amsart}\n\n\\tolerance=5000\n\n\\usepackage{setspace,hyperref}\n\n\\newcommand{\\field}[1]{\\mathbb{#1}} \\newcommand{\\R}{\\field{R}}\n\n%% Change the default page sizes.\n\n\\setlength{\\topmargin}{-0.25in} \\setlength{\\textheight}{8.5in}\n\\setlength{\\oddsidemargin}{.0in} \\setlength{\\evensidemargin}{.0in}\n\\setlength{\\textwidth}{6.5in} \\setlength{\\footskip}{.5in}\n\n\\title{Package \\texttt{np} FAQ} \\date{\\today} \\author{Jeffrey S.~Racine}\n\n\\begin{document}\n\n\\maketitle\n\n\\tableofcontents\n\n\\onehalfspacing\n\n\\section{Overview and Current Version}\n\nThis set of frequently asked questions is intended to help users who\nare encountering unexpected or undesired behavior when trying to use\nthe \\texttt{np} package.\n\nMany of the underlying C routines have been extensively tested over\nthe past two decades. However, the \\texttt{R} `hooks' needed to call\nthese routines along with processing of the data required for a\nseamless user experience may produce unexpected or undesired results\nin some settings.\n\nIf you encounter any issues with the \\texttt{np} package, kindly first\nensure that you have the most recent version of \\texttt{np},\n\\texttt{R}, and \\texttt{RStudio} (if appropriate) installed. Sometimes\nissues encountered using outdated versions of software have been\nresolved in the current versions, so this is the first thing one ought\nto investigate when the unexpected occurs.\n\nHaving ensured that the problem persists with the most recently\navailable versions of all software involved, kindly report any issues\nyou encounter to me, and \\textsl{please include} your code, data,\nversion of the \\texttt{np} package, version of \\texttt{R}, and\noperating system (with version number) used so that I can help track\ndown any such issues\n(\\href{mailto:racinej@mcmaster.ca}{racinej@mcmaster.ca}). And, of\ncourse, if you encounter an issue that you think might be of interest\nto others, kindly email me the relevant information and I will\nincorporate it into this FAQ.\n\nThis FAQ refers to the most recent version, which as of this writing\nis 0.60-2. Kindly update your version should you not be using the most\ncurrent (from within R, \\texttt{update.packages()} ought to do it,\nthough also see \\ref{update} below.). See the appendix in this file\nfor cumulative changes between this and previous versions of the\n\\texttt{np} package.\n\n\\section{Frequently Asked Questions}\n\n\\subsection{How do I cite the \\texttt{np} package?}\n\nOnce you have installed the \\texttt{np} package\n(\\texttt{install.packages(\"np\")}), if you load the \\texttt{np} package\n(\\texttt{library(\"np\")}) and type \\texttt{citation(\"np\")}\nyou will be presented with the following information.\n\n  \\begin{singlespacing}\n\\begin{verbatim}\n> citation(\"np\")\n\nTo cite np in publications use:\n\n  Tristen Hayfield and Jeffrey S. Racine (2008). Nonparametric\n  Econometrics: The np Package. Journal of Statistical Software\n  27(5). URL http://www.jstatsoft.org/v27/i05/.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Nonparametric Econometrics: The np Package},\n    author = {Tristen Hayfield and Jeffrey S. Racine},\n    journal = {Journal of Statistical Software},\n    year = {2008},\n    volume = {27},\n    number = {5},\n    url = {http://www.jstatsoft.org/v27/i05/},\n  }\n\\end{verbatim}\n  \\end{singlespacing}\n\n\\subsection{I have never used \\texttt{R} before. Can you direct me to some\n    introductory material that will guide me through the basics?}\n\n  There are many excellent introductions to the \\texttt{R} environment\n  with more on the way. First, I would recommend going directly to the\n  \\texttt{R} website (\\url{http://www.r-project.org}) and looking under\n  Documentation/Manuals (\\url{http://cran.r-project.org/manuals.html})\n  where you will discover a wealth of documentation for \\texttt{R} users\n  of all levels. See also the \\texttt{R} task views summary page\n  (\\url{http://cran.nedmirror.nl/web/views/index.html}) for\n  information grouped under field of interest. A few documents that I\n  mention to my students which are tailored to econometricians include\n  \\url{http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf},\n  Cribari-Neto \\& Zarkos (1999) \\cite{CRIBARI_NETO_ZARKOS:1999},\n  Racine \\& Hyndman (2002) \\cite{RACINE_HYNDMAN:2002} and Farnsworth\n  (2006) \\cite{FARNSWORTH:2006}, to name but a few.\n\n  Those looking for exemplar data sets outside of those contained in\n  the \\texttt{np} package are directed to the \\texttt{Ecdat} \\cite{Ecdat}\n  and \\texttt{AER} \\cite{AER} packages.\n\n  I maintain a `Gallery' to provide a forum for users to share code\n  and discover examples and illustrations which can be found at\n  \\url{http://socserv.mcmaster.ca/racinej/Gallery/Home.html}.\n\n  Often the best resource is right down the hall. Ask a colleague\n  whether they use or know anyone who uses R, then offer to buy that\n  person a coffee and along the way drop something like ``I keep\n  hearing about the \\texttt{R} project\\dots I feel like such a\n  Luddite\\dots''\n\n  \\subsection{\\label{update}How do I keep all \\texttt{R} packages on my\n    system current?}\n\n  Run the command \\texttt{update.packages(checkBuilt=TRUE,ask=FALSE)},\n  which will not only update all packages that are no longer current,\n  but will also update all packages built under outdated installed\n  versions of R, if appropriate.\n\n\\subsection{It seems that there are a lot of packages that must be\n    installed in order to conduct econometric analysis (tseries,\n    lmtest, np, etc.). Is there a way to avoid having to individually\n    install each package individually?}\n\n  Certainly. The Comprehensive R Archive Network (CRAN) is a network\n  of ftp and web servers around the world that store identical,\n  up-to-date, versions of code and documentation for \\texttt{R}.  The\n  CRAN `task view' for computational econometrics might be of\n  particular interest to econometricians.  The econometric task view\n  provides an excellent summary of both parametric and nonparametric\n  econometric packages that exist for the R environment and provides\n  one-stop installation for these packages.\n\nSee\n\\href{http://cran.r-project.org/web/views/Econometrics.html}{cran.r-project.org/web/views/Econometrics.html} for further information.\n\nTo automatically install a task view, the \\texttt{ctv} package first\nneeds to be installed and loaded, i.e.,\n\\begin{verbatim}\ninstall.packages(\"ctv\")\nlibrary(\"ctv\")\n\\end{verbatim}\n\nThe econometric task view can then be installed via\n\\texttt{install.views()} and updated via \\texttt{update.views()}\n(which first assesses which of the packages are already installed and\nup-to-date), i.e.,\n\\begin{verbatim}\ninstall.views(\"Econometrics\")\n\\end{verbatim}\nor\n\\begin{verbatim}\nupdate.views(\"Econometrics\")\n\\end{verbatim}\n\n\\subsection{Is there a `gentle guide' to the \\texttt{np} package that\n    contains some easy to follow examples?}\n\n  Perhaps the most gentle introduction is contained in the \\texttt{np}\n  package itself in the form of a `vignette'. To view the vignette run\n  R, install the \\texttt{np} package (\\texttt{install.packages(\"np\")}), then\n  type \\texttt{vignette(\"np\",package=\"np\")} to view or print the vignette\n  (this vignette is essentially the article that appeared in the\n  Journal of Statistical Software that describes the \\texttt{np} package\n  (Hayfield \\& Racine \\cite{HAYFIELD_RACINE:2008})).\n\n  See also \\texttt{vignette(\"entropy\\_np\",package=\"np\")} for a\n  vignette on the entropy-based functions and procedures introduced to\n  the \\texttt{np} package in versions 0.30-4 through 0.30-8.\n\n  In addition, you might be interested in the nonparametric\n  econometrics primer (Racine \\cite{RACINE:2008}) which is available\n  for download from my website as is the code that will generate the\n  examples contained therein.\n\n  For a listing of all routines in the \\texttt{np} package type:\n  \\texttt{library(help=\"np\")}.\n\n\\subsection{I noticed you have placed a new version of the \\texttt{np}\n    package on CRAN. How can I determine what has been changed,\n    modified, fixed etc?}\n\n  See the CHANGELOG on the CRAN site\n  (\\url{http://cran.r-project.org/web/packages/np/ChangeLog}), or go\n  to the end of this document where the CHANGELOG is provided for your\n  convenience.\n\n\\subsection{What is the difference between the \\texttt{np} package and the\n    previous stand-alone programs you wrote, namely, N \\copyright,\n    NPREG \\copyright, and NPDEN \\copyright?}\n\n  The \\texttt{np} package is built from the same C library that underlies\n  its predecessors N \\copyright, NPREG \\copyright, and NPDEN\n  \\copyright. In fact, \\texttt{R} calls the compiled C code that\n  underlies its predecessors (one of the beauties of \\texttt{R} is that\n  you can obtain the benefits of compiled code (i.e., speed) yet have\n  access to the rich superset of \\texttt{R} routines and \\texttt{R} packages\n  built by others). Therefore, there is no penalty in run-time when\n  using R versus the stand alone precompiled binary programs N\n  \\copyright, NPREG \\copyright, and NPDEN \\copyright\\, (unless of\n  course the compiler or compiler flags differ from those used to\n  build its predecessors).\n\n\\subsection{How can I read data stored in various formats such as\n    Stata, SAS, Minitab, SPSS etc.~into the R program?}\n\n  Install the foreign library via \\texttt{install.packages(\"foreign\")}\n  then do something like\n  \\begin{singlespacing}\n\\begin{verbatim}\nmydat <- read.dta(\"datafile.dta\"),\n\\end{verbatim}\n  \\end{singlespacing}\n  where \\texttt{datafile.dta} is the name of your Stata data\n  file. Note that, as of version 0.8-34, the foreign package function\n  \\texttt{read.dta} supports reading files directly over the Internet\n  making for more portable code. For instance, one could do something\n  like\n\\begin{verbatim}\nmydat <- read.dta(file=\"http://www.principlesofeconometrics.com/stata/mroz.dta\")\n\\end{verbatim}\nas one could always do with, say, \\texttt{read.table()}.\n\n\\subsection{I want to use so-and-so's semiparametric/nonparametric\n    method, however, the \\texttt{np} package does not include this\n    particular method\\dots}\n\n  This is why we have included the function \\texttt{npksum()}, which\n  exists so that you can create your own kernel objects and take\n  advantage of underlying kernel methods implemented in the \\texttt{np}\n  package without having to write, say, C or Fortran code.\n\n  With the options available, you could create new nonparametric tests\n  or even new kernel estimators. For instance, the convolution kernel\n  option would allow you to replicate, say, the least squares\n  cross-validation function for kernel density estimation found in\n  \\texttt{npudensbw()}. The function \\texttt{npksum()} uses highly-optimized\n  C code that strives to minimize its memory footprint, while there is\n  low overhead involved when using repeated calls to this function.\n\n  See, by way of illustration, the example in the \\texttt{npksum()}\n  help file that conducts leave-one-out cross-validation for a local\n  constant regression estimator via calls to the \\texttt{R} function\n  \\texttt{nlm()}, and compares this to the \\texttt{npregbw()}\n  function.\n\n  If you wish to have a method incorporated into a future version of\n  the \\texttt{np} package, the best way to achieve this is to\n  successfully code up the method using \\texttt{npksum()}, briefly\n  document it and write up an example, then send it to us. We will\n  then, at our discretion, do our best to adapt and incorporate this\n  into a future version and of course give credit where credit is due.\n\n\\subsection{Cross-validation takes forever, and I can't wait that\n    long\\dots}\n\n  This is the most common complaint from frustrated users coming to\n  terms with numerically demanding statistical methods.  I am fond of\n  saying `if you want the wrong answer, I can give it to you right\n  away', but this wears thin quickly.\n\n  \\begin{enumerate}\n\n  \\item Some background may be in order. Cross-validation methods have\n    run times that are proportional to the square of the number of\n    observations (of computational order $n^2$ hence a doubling of the\n    sample size will increase run time by a factor of four). The\n    solution I favor is to run the code in a parallel computing\n    environment. The underlying C code for \\texttt{np} is MPI-aware\n    (MPI denotes the `message passing interface', a popular parallel\n    programming library that is an international standard), and a\n    version of the \\texttt{np} package titled `\\texttt{npRmpi}' exists\n    for running jobs in a parallel environment that leverages the\n    \\texttt{Rmpi} package (\\cite{Rmpi}).\\footnote{We are extremely\n      grateful to Hao Yu for providing the MPI functionality contained\n      in the \\texttt{Rmpi} package.}\n\n    With respect to the \\texttt{npRmpi} package, kindly note that I\n    cannot assist with issues surrounding installation and setup due\n    to the vast heterogeneity of MPI implementations and methods for\n    executing such programs. You are instead strongly advised to seek\n    local advice from your sysadmin or others. The document\n    \\texttt{npRmpi.pdf} may contain some useful information for such\n    issues. You might also want to first get the \\texttt{Rmpi} package\n    installed and running properly as once you are at this stage it\n    ought to then be trivial to install the \\texttt{npRmpi}\n    package. Also note that running programs in a parallel environment\n    requires minor differences in how \\texttt{np} functions are\n    called, and you would be wise to examine the examples in the demo\n    directory (download and unpack the source code and look in this\n    directory) and the overview file \\texttt{npRmpi.pdf} in the\n    inst/doc/ directory of the \\texttt{npRmpi} package you downloaded\n    and unpacked.\n\n  \\item Alternatively, you can use the method outlined in Racine\n    (1993) \\cite{RACINE:1993}. The method is based on the fact that\n    the unknown constant $c_j$ (the `scale factor') in the formula\n    $c_j\\sigma_j n^{-1/(2p+r)}$ is independent of the sample size, so\n    one can conduct bandwidth selection on random subsets and do this\n    for a large number of subsets then take the mean/median over these\n    subsets and feed the scale factor into the final routine for the\n    entire sample. Below you will find simple \\texttt{R} code that\n    replicates the method using numerical search and resampling\n    without replacement rather than the grid method outlined in\n    \\cite{RACINE:1993} (both have equivalent properties but this is\n    perhaps simpler to implement using the \\texttt{np} package).\n\n    \\begin{singlespacing}\n\\begin{verbatim}\n## Regression example\n## Generate a moderately large data set\n\nset.seed(12345)\nn <- 100000\nx1 <- runif(n)\nx2 <- runif(n)\n\ny <- 1 + x1 + sin(pi*x2) + rnorm(n,sd=.1)\n\n## Set the number of resamples and the subsample size\n\nnum.res <- 50\nn.sub <- 250\n\n## Create a storage matrix\n\nbw.mat <- matrix(NA,nrow=num.res,ncol=2)\n\n## Get the scale factors for resamples from the full sample of size n.sub\n\noptions(np.messages=FALSE)\n\nfor(i in 1:num.res)  {\n\n  cat(paste(\" Replication\", i, \"of\", num.res, \"...\\r\"))\n\n  bw.mat[i,] <- npregbw(y~x1+x2,regtype=\"ll\",\n                        subset=sample(n,n.sub))$sfactor$x\n\n}\n\n## A function to compute the median of the columns of a matrix\n\ncolMedians <- function(data) {\n  colmed <- numeric(ncol(data))\n  for(i in 1:ncol(data)) {\n    colmed[i] <- median(data[,i])\n  }\n  return(colmed)\n}\n\n## Take the median scale factors\n\nbw <- colMedians(bw.mat)\n\n## The final model for the full dataset\n\nmodel.res <- npreg(y~x1+x2,bws=bw,regtype=\"ll\",bwscaling=TRUE)\n\\end{verbatim}\n    \\end{singlespacing}\n\n\\begin{singlespacing}\n\\begin{verbatim}\n## Hat tip to Yoshi Fujiwara <yoshi.fujiwara@gmail.com> for this \n## nice example\n\nn <- 100000\n\nlibrary(MASS)\nrho <- 0.25\nSigma <- matrix(c(1,rho,rho,1),2,2)\nmydat <- mvrnorm(n=n, rep(0, 2), Sigma)\nx <- mydat[,1]\ny <- mydat[,2]\nrm(mydat)\n\nnum.res <- 100\nn.sub <- 100\n\nbw.mat <- matrix(NA,nrow=num.res,ncol=2)\noptions(np.messages=FALSE)\nfor(i in 1:num.res) {\n  bw <- npcdensbw(y ~ x,subset=sample(n,n.sub))\n  bw.mat[i,] <- c(bw$sfactor$y,bw$sfactor$x)\n}\n\ncolMedians <- function(data) {\n colmed <- numeric(ncol(data))\n for(i in 1:ncol(data)) {\n   colmed[i] <- median(data[,i])\n }\n return(colmed)\n}\n\nbw <- colMedians(bw.mat)\nbw <- npcdensbw(y ~ x, bws=bw, bwscaling=TRUE, bandwidth.compute=FALSE)\nsummary(bw)\nplot(bw,xtrim=.01)\n\\end{verbatim}\n\\end{singlespacing}\n\n\\item Barring this, you can set the search tolerances to be a bit less\n  terse (at the expense of potential accuracy, i.e., becoming trapped\n  in local minima) by setting, say, \\texttt{tol=0.1} and\n  \\texttt{ftol=0.1} in the respective bandwidth routine (see the docs\n  for examples). Also, you can set \\texttt{nmulti=1} which overrides\n  the number of times the search procedure restarts from different\n  random starting values (the default is to restart $k$ times where\n  $k$ is the number of variables). Be warned, however, that this is\n  {\\em definitely not recommended} and should {\\em be avoided at all\n    costs} for all but the most casual examination of a\n  relationship. One ought to use multistarting for any final results\n  and never override default search tolerances {\\em unless increasing\n    multistarts beyond the default}. Results based upon exhaustive\n  search {\\em often differ dramatically} from that based on limited\n  search achieved by overriding default search tolerances.\n\n  \\item For those who like to tinker and who work on a *NIX system\n    with the gcc compiler suite, you can change the default compiler\n    switches used for building \\texttt{R} packages which may generate\n    some modest improvements in run time. The default compiler\n    switches are\n    \\begin{singlespacing}\n\\begin{verbatim}\n -g -O2\n\\end{verbatim}\n    \\end{singlespacing}\n    and are set in the R-*.*.*/etc/Makeconf file (where *.*.* refers\n    to your \\texttt{R} version number, e.g. R-2.10.1).  You can edit this\n    file and change these defaults to\n    \\begin{singlespacing}\n\\begin{verbatim}\n-O3 -ffast-math -fexpensive-optimizations -fomit-frame-pointer\n\\end{verbatim}\n    \\end{singlespacing}\n    then reinstall the \\texttt{np} package and you may experience some\n    improvements in run time. Note that the -g flag turns on\n    production of debugging information which can involve some\n    overhead, so we are disabling this feature. This is not a feature\n    used by the typical applied researcher but if you envision\n    requiring this it is clearly trivial to re-enable debugging. I\n    typically experience in the neighborhood of a 0-5\\% reduction in\n    run time for data-driven bandwidth selection on a variety of\n    systems depending on the method being used, though mileage will of\n    course vary.\n\n  \\item Finally, I have recently been working on extending regression\n    splines (as opposed to smoothing splines\\footnote{Unlike\n      regression splines, a smoothing spline places a knot at each\n      data point and penalizes the fit for lack of smoothness as\n      defined by the second derivative (typically cubic splines are\n      used). When the penalty is zero this produces a function that\n      interpolates the data points. When the penalty is infinite, this\n      delivers the linear OLS fit to the data.}) to admit categorical\n    predictors (see references for papers with Yang and Ma in the\n    \\texttt{crs} package). Because regression splines involve simple\n    least-squares fits, they scale much better with respect to the\n    sample size so for large sample sizes you may wish to instead\n    consider this approach (kindly see my webpage for further\n    details). You can run cross-validation with hundreds of thousands\n    of observations on a decent desktop in far less time than that\n    required for cross-validated kernel estimates.\n\n  \\end{enumerate}\n\n\\subsection{I estimated a parametric model using the\n  \\texttt{lm()} function. How can I compare the cross-validation score\n  from the \\texttt{npreg()} approach with that for the parametric\n  model?}\n\nThis can be readily achieved for the parametric model as follows:\n\\begin{verbatim}\ndata(wage1)\nmodel.lm <- lm(lwage ~ married + female + nonwhite + educ +\n                       exper + tenure, data = wage1)\ncv.lm <-mean(residuals(model.lm)^2/(1-hatvalues(model.lm))^2)                                      \ncv.lm\n\\end{verbatim}\nYou can then compare \\verb+cv.lm+ with the cross-validation score for\nthe \\verb+npreg+ object as follows: if your model is named \\verb+foo+\n(i.e.\\ you ran something like \\verb^foo <- npreg(y~x1+x2)^) then\n\\verb+summary(foo$bws)+ will give you a summary of the model's\nbandwidth object (the value of the objective function is the\ncross-validation score), while \\verb+foo$bws$fval+ will give you the\nfunction value, i.e.\\ the value of the cross-validation function, {\\em\n  provided} that the model was estimated via least-squares\ncross-validation (default, \\verb+bwmethod=\"cv.ls\"+). If the\ncross-validation score is lower for one model, that indicates that the\nmodel possessing the lowest score is to be preferred.\n\n\\subsection{Where can I get some examples of R code for the np\n  package in addition to the examples in the help files?}\n\nStart \\texttt{R} then type \\verb+demo(package=\"np\")+ and you will be\npresented with a list of demos for constrained estimation, inference,\nand so forth. To run one of these demos type, for example,\n\\verb+demo(npregiv)+ (note that you must first install the \\texttt{rgl} package to run this particular demo).\n\nTo find the location of a demo type\n\\verb+system.file(\"demo\",\"npregiv.R\",package=\"np\")+ for example,\nthen you can take the source code for this demo and modify it for your\nparticular application.\n\nYou can also find examples and illustrations at the `Gallery' located\nat \\url{http://socserv.mcmaster.ca/racinej/Gallery/Home.html}.\n\n\\subsection{Is\n    there a way to figure out roughly how long\n    cross-validation will take on a large sample?}\n\n  Certainly. You can run cross-validation on subsets of your data of\n  increasing size and time each run, then estimate a double-log model\n  of sample size on run time (run time can be approximated by a linear\n  log-log model) and then use this to predict approximate\n  run-time. The following example demonstrates this for a simple\n  model, but you can modify it trivially for your data. Note that the\n  larger is \\texttt{n.max} the more accurate it will likely be. Note that\n  we presume your data is in no particular order (if it is, you\n  perhaps ought to shuffle it first). We plot the log-log model fit\n  and prediction along with that expressed in hours.\n\n  \\begin{singlespacing}\n\\begin{verbatim}\n## Set the upper bound (n.max > 100) for the sub-samples on which you \n## will run cross-validation (perhaps n.max = 1000 (or 2000) ought to \n## suffice). For your application, n will be your sample size\n\nn <- 2000\nn.max <- 1000\n\nx <- runif(n)\ny <- 1 + x + rnorm(n)\n\nn.seq <- seq(100,n.max,by=100)\ntime.seq <- numeric(length(n.seq))\n\nfor(i in 1:length(n.seq)) {\n  time.seq[i] <- system.time(npregbw(y~x,subset=seq(1:n.seq[i])))[3]\n}\n\n## Now fit a double-log model and generate/plot actual values plus\n## prediction for n (i.e., approximate run time in hours)\n\nlog.time <- log(time.seq)\nlog.n <- log(n.seq)\n\nmodel <- lm(log.time~log.n)\n\nn.seq.aug <- c(n.seq,n)\ntime.fcst <- exp(predict(model,newdata=data.frame(log.n=log(n.seq.aug))))\n\npar(mfrow=c(2,1))\n\nplot(log(n.seq.aug),log(time.fcst),type=\"b\",\n     xlab=\"log(Sample Size)\",\n     ylab=\"log(Run Time)\",\n     main=\"Approximate Run Time (log seconds)\")\n\nplot(n.seq.aug,time.fcst/3600,type=\"b\",\n     xlab=\"Sample Size (n)\",\n     ylab=\"Hours\",\n     main=\"Approximate Run Time (hours)\",\n     sub=paste(\"Predicted run time for n =\", n, \"observations:\",\n     signif(time.fcst[length(time.fcst)]/3600, digits=2),\n     \"hours\"))\n\\end{verbatim}\n  \\end{singlespacing}\n\n\\subsection{I need to apply a boundary correction for a univariate\n  density estimate - how can I do this?}\n\nYou could do this using `data-reflection' (see the description in\n\\cite[Page 30]{SILVERMAN:1986}, as the following illustration\ndemonstrates.\n\n\\begin{verbatim}\n## Use trees to speed up estimation \noptions(np.tree=TRUE)\n\n## Generate data where there is an asymptote at the left boundary\n\nset.seed(42)\nn <- 10000\nx <- sort(rexp(n))\n\n## Compute the bandwidth for the original data\n\nbw <- npudensbw(~x,ckertype=\"epanechnikov\")\n\n## Construct the standard (i.e. no boundary correction) kernel estimate\n\nf.hat <- fitted(npudens(~x,bws=bw$bw,ckertype=\"epanechnikov\"))\n\n## Reflect the data, compute the reflected density then take the estimator which\n## is 2 times the reflected density estimate for x >= 0 (see Silverman (1986,\n## page 30))\n\nx.reflect <- c(x,-x)\nf.reflect <- npudens(~x.reflect,bws=bw$bw,ckertype=\"epanechnikov\")\nf.boundary <- 2*predict(f.reflect,newdata=data.frame(x.reflect=x))\n\n## Plot the nonsmooth histogram, the standard estimator, then the\n## boundary-corrected estimator\n\nhist(x,breaks=100,lty=3,freq=FALSE,ylim=c(0,1),main=\"\")\nrug(x)\nlines(x,f.hat,col=1,lwd=2,lty=1)\nlines(x,f.boundary,col=2,lwd=2,lty=2)\nlegend(\"topright\",\n       c(\"Standard Kernel Estimator (no boundary correction)\",\n         \"Data-reflected Kernel Estimator (boundary correction)\"),\n       lty=1:2,\n       col=1:2,\n       bty=\"n\")\n\\end{verbatim}\n\n\\subsection{I notice your density estimator supports manual bandwidths,\n    the normal-reference rule-of-thumb, likelihood cross-validation\n    and least squares cross-validation, but not plug-in rules. How can\n    I use various plug-in rules for estimating a univariate density?}\n\n  For the univariate case this is straightforward as the default R\n  installation supports a variety of univariate plug-in bandwidths\n  (see \\texttt{?bw.nrd} for details). For example, \\texttt{bs.SJ} computes\n  the bandwidths outlined in\n\n\\begin{verbatim}\n     Sheather, S. J. and Jones, M. C. (1991) A reliable data-based\n     bandwidth selection method for kernel density estimation.\n     _Journal of the Royal Statistical Society series B_, *53*,\n     683-690.\n\\end{verbatim}\n\n  Incorporating these univariate plug-in bandwidth selectors into the\n  univariate density estimation routines in \\texttt{np} is\n  straightforward as the following code snippet demonstrates. They\n  will certainly be faster than the likelihood and least-squares\n  cross-validation approaches. Also, you may not wish the density\n  estimate for all sample realizations, but rather for a shorter\n  grid. The following example demonstrates both and is closer in\n  spirit to the \\texttt{density()} function in base R.\n\n\\begin{verbatim}\nx <- rnorm(10000)\nxeval <- seq(min(x),max(x),length=100)\nf <- npudens(tdat=x,edat=xeval,bws=bw.SJ(x))\n\\end{verbatim}\n\n  You might even be tempted to use these in the multivariate case via\n  \\texttt{bws=c(bw.SJ(x1),bw.SJ(x2),...)} though these would be optimal\n  for univariate densities and most certainly not for a multivariate\n  density. However, for exploratory purposes these may be of interest\n  to some users.\n\n\\subsection{\\label{code_nowork}I wrote a program using \\texttt{np} and it\n    does not work as I expected\\dots}\n\n  There exist a rather extensive set of examples contained in the\n  docs. You can run these examples by typing\n  \\texttt{example(\"npfunctionname\")} where npfunctionname is, say,\n  \\texttt{w}, as in \\texttt{example(\"w\")}.\\footnote{For a listing of\n    all routines in the np package type: \\texttt{library(help=\"np\")}.}\n  These examples all pass quality control and produce the expected\n  results, so first see whether your problem fits into an existing\n  example, and if not, carefully follow the examples listed in a given\n  function for syntax issues etc.\n\n  If you are convinced that the problem lies with \\texttt{np} (there\n  certainly will be undiscovered `features', i.e., bugs), then kindly\n  send me your code and data so that I can replicate and help resolve\n  the issue.\n\n\\subsection{Under Mac OS X, when I run a command no progress is\n    displayed\\dots}\n\n  This should no longer occur for np versions 0.30-0 and up. For\n  previous versions, this reflected a peculiarity of console\n  input/output (I/O) under Mac OS X.  Note, however, that if you run {\n    \\tt R} in a terminal rather than \\texttt{Rgui} you will get the full\n  *NIX\\footnote{*NIX is often used to describe UNIX and other\n    UNIX-like platforms (i.e., UNIX, BSD, and GNU/Linux\n    distributions). I harbour strong preferences for *NIX computing\n    platforms.}  experience. \n\n  But also note that there is a platform-independent interface that\n  does not suffer from this limitation called `RStudio' that you might\n  prefer to existing interfaces (\\url{http://www.rstudio.org}).\n\n\\subsection{When some routines are running under MS Windows, \\texttt{R}\n    appears to be `not responding.' It appears that the program is not\n    `hung', rather is simply computing. The previous stand-alone\n    program (N \\copyright) always displayed useful information\\dots}\n\n  This should no longer occur for np versions 0.30-0 and up. For\n  previous versions, this reflected a peculiarity of the R Windows\n  GUI, and was not specific to the \\texttt{np} package.\n\n  From the \\texttt{R} Windows FAQ\\dots\n\n  ``When using Rgui the output to the console seems to be delayed.\n  This is deliberate: the console output is buffered and re-written in\n  chunks to be faster and less distracting. You can turn buffering off\n  or on from the `Misc' menu or the right-click menu: \\verb+<Ctrl-W>+\n  toggles the setting.''\n\n\\subsection{Some results take a while, and my MS Windows computer is\n    sluggish while \\texttt{R} is running\\dots}\n\n  You can easily change the priority of your \\texttt{R} job on the\n  fly, just as you might under *NIX.  Pull up the task manager\n  (\\verb+<Ctrl>-<Alt>-<Del>+), go to the process list, and find the\n  process \\texttt{Rgui.exe} (or \\texttt{R.exe} if you are running\n  \\texttt{Rterm}), select this process by left clicking on it, then\n  right clicking will bring up a menu, select \\texttt{Set priority},\n  then change priority to \\texttt{low} and hit $<$\\texttt{ok}$>$. For\n  lengthy jobs this will make your life much smoother, and you can,\n  say, run multiple jobs in low priority with no sluggishness\n  whatsoever for your other applications (useful for queuing a number\n  of long jobs). Alternatively, you could permanently change the\n  default priority of \\texttt{R} under MS Windows by modifying the\n  properties of your \\texttt{R} desktop icon.\n\n\\subsection{\\label{factor}A variable must be cast as, say, a factor in\n    order for \\texttt{np} to recognize this as an unordered factor. How\n    can I determine whether my data is already cast as a factor?}\n\n  Use the \\texttt{class()} function. For example, define \\texttt{x <-\n    factor(c(\"male\",\"female\"))}, then type \\texttt{class(x)}.\n\n\\subsection{When I use \\texttt{plot()} \\texttt{(npplot())} existing graphics\n    windows are overwritten. How can I display multiple graphics plots\n    in separate graphics windows so that I can see results from\n    previous runs and compare that to my new run?}\n\n  Use the \\texttt{dev.new()} command in between each call to \\texttt{plot()}. This will leave the existing graphics window open and\n  start a new one. The command \\texttt{dev.list()} will list all graphics\n  windows, and the command \\texttt{dev.set(integer.foo)} will allow you\n  to switch from one to another and overwrite existing graphics\n  windows should you so choose.\n\n  Alternately, you can use RStudio (\\url{http://www.rstudio.org})\n  where the plot window is such that previous plots can be recalled on\n  the fly, resized, saved in various formats etc.\n\n\\subsection{\\label{plot names}Sometimes \\texttt{plot()} fails to use my\n    variable names\\dots}\n\n  This should not occur unless you are using the data frame method and\n  not naming your variables (e.g., you are doing something like\n  data.frame(ordered(year))). To correct this, name your variables in\n  the respective data frame, as in\n\n  \\texttt{data <- data.frame(year=ordered(year),gdp)}\n\n  \\noindent so that the ordered factor appears as `year' and not\n  `ordered.year'\n\n\\subsection{Sometimes \\texttt{plot()} appends my variable names with\n    .ordered or .factor\\dots}\n\n  See also \\ref{plot names} above.\n\n\\subsection{I specify a variable as \\texttt{factor()} or \\texttt{ordered()}\n    in a data frame, then call this when I conduct bandwidth\n    selection. However, when I try to plot the resulting object, it\n    complains with the following message:}\n\n    \\texttt{Error in eval(expr, envir, enclos) : object \"variable\" not\n      found\\dots}\n\n  This arises because \\texttt{plot()} (\\texttt{npplot()}) tries to retrieve\n  the variable from the environment but you have changed the\n  definition when you called the bandwidth selection routine (e.g.,\n  \\texttt{npregbw(y\\textasciitilde x,data=dataframe)}).\n\n  To correct this, simply call \\texttt{plot()} with the argument\n  \\texttt{data=dataframe} where \\texttt{dataframe} is the name of your\n  data frame.\n\n\\subsection{My \\texttt{np} code produces errors when I attempt to run\n    it\\dots}\n\n  First, it is good practice to name all arguments (see the docs for\n  examples) as in \\texttt{npregbw(formula=y\\textasciitilde x)} (i.e.,\n  explicitly call formula for functions that use named formulas). This\n  will help the code return a potentially helpful error message.\n\n  Next, follow the examples listed at the end of each function help\n  page closely (i.e., \\texttt{?npreg} then scroll down to\n  \\texttt{Examples:}). See also \\ref{code_nowork} above.\n\n\\subsection{\\label{0/1}I have (numeric) 0/1 dummy variable regressors\n    in my parametric model. Can I just pass them to the \\texttt{np}\n    functions as I would in a parametric setting?}\n\n  In general, definitely not -- you need to correctly classify each\n  variable as type \\texttt{factor} and treat it as one variable only. By\n  way of example, suppose in your data you have created dummy\n  variables for year, for example, dummy06 which equals 1 for 2006, 0\n  otherwise, dummy07 which equals 1 for 2007, 0 otherwise etc. We\n  create these by habit for parametric models. But, the underlying\n  variable is simply year, which equals 2006, 2007, and so forth.\n\n  In \\texttt{np} (and \\texttt{R} in general), you get to economize by\n  just telling the function that the variable `year' is ordered, as in\n  ordered(year), where year is a vector containing elements 2006, 2007\n  etc. Of course, seasoned R users would appreciate that this is in\n  fact the simple way to do it with a parametric model as well.\n\n  You would {\\em never}, therefore, just pass dummy variables to an\n  \\texttt{np} function as you would for linear parametric models. The\n  {\\em only} exception is where you have only one 0/1 dummy for one\n  variable, say `\\texttt{sex}', and in this case you still would have\n  to enter this as \\texttt{factor(sex)} so that the \\texttt{np}\n  function recognizes this as a factor (otherwise it would treat it as\n  continuous and use a kernel function that is inappropriate for a\n  factor).\n\n\\subsection{\\label{terminates}I have a categorical variable, ignored\n    the advice in items \\ref{factor} and \\ref{0/1}, and R terminates\n    with the following error.}\n\n\\begin{verbatim}\n ** Fatal Error in routine kernel_bandwidth() ** variable 0 appears to be constant!\n ** Program terminated abnormally!\n\\end{verbatim}\n\n  Presuming your variable is not in fact a constant (i.e., is not in\n  fact a `variable'), this can only occur in the case where a variable\n  is `pathological' in that it has \\texttt{IQR=0} but \\texttt{std>0}\n  (see the section titled `Changes from Version 0.30-1 to 0.30-2\n  [19-Apr-2009]' for further details).  This should no longer occur\n  for np versions 0.30-2 and up.\n\n\\subsection{Rgui appears to crash. There must be a bug in the program.}\n\n  Try running your code using the terminal (i.e., \\texttt{Rterm} in\n  Windows or \\texttt{R} in a Mac OS X terminal) and see whether you get\n  the message in item \\ref{terminates}.\n\n\\subsection{Can I skip creating a bandwidth object and enter a\n    bandwidth directly?}\n\n  Certainly, though I would advise doing so for exploratory data\n  analysis only.  For example, attach a dataset via\n  \\begin{singlespacing}\n\\begin{verbatim}\ndata(cps71)\nattach(cps71)\n\\end{verbatim}\n  \\end{singlespacing}\n  then enter, say,\n  \\begin{singlespacing}\n\\begin{verbatim}\nplot(age,logwage,main=\"Manual Bandwidth Example\")\nlines(age,fitted(npreg(logwage~age,bws=1)),col=\"blue\",lty=1)\nlines(age,fitted(npreg(logwage~age,bws=2)),col=\"red\",lty=2)\nlines(age,fitted(npreg(logwage~age,bws=3)),col=\"green\",lty=3)\nlegend(20,15,\n       c(\"h=1\",\"h=2\",\"h=3\"),\n       col=c(\"blue\",\"red\",\"green\"),\n       lty=c(1,2,3))\n\\end{verbatim}\n  \\end{singlespacing}\n  to plot the local constant estimator with bandwidths of 1, 2, and 3\n  years. Note that the age variable is already sorted in this\n  dataset. If your data is not sorted you will need to do so prior to\n  plotting so that your \\texttt{lines} command works properly. Or see\n  \\ref{partial grad q} below for a multivariate example.\n\n\\subsection{\\label{partial grad q}When I estimate my gradients and\n    there are two or more covariates and then extract them with the\n    \\texttt{gradients()} function, they are not `smooth', though if I\n    plot a model with the \\texttt{gradients=TRUE} option, they are. The\n    \\texttt{gradients()} function must be broken\\dots}\n\n  The function \\texttt{plot()} (\\texttt{npplot()}) plots `partial'\n  means and gradients. In other words, it plots $x_1$ versus $\\hat\n  g(x_1,\\bar x_2)$ for the partial mean, where $\\bar x_2$ is, say, the\n  median/modal value of $x_2$. It also plots $x_1$ versus\n  $\\partial\\hat g(x_1,\\bar x_2)/\\partial x_1$ for the gradient. Note\n  that we are controlling for the values of the other\n  covariate(s). This is in effect what people expect when they play\n  with linear parametric models of the form $y=\\beta_0+\\beta_1\n  x_1+\\beta_2x_2+\\epsilon$ since, given the additive nature of the\n  model, $\\partial y/\\partial x_1=\\beta_1$ (i.e., does not vary with\n  $x_2$).\n\n  The example below shows how you could manually generate the partial\n  gradients (and means) for your data where the sample realizations\n  form the evaluation data for $x_1$ (unlike \\texttt{npplot()} which\n  uses an evenly spaced grid). Note we use the function\n  \\texttt{uocquantile()} to generate a vector that holds $x_2$\n  constant at is median/modal value (i.e., the 0.5 quantile) in the\n  evaluation data. The function \\texttt{uocquantile()} can compute\n  quantiles for ordered, unordered, and continuous data (see\n  \\texttt{?uocquantile} for details).\n\n\\begin{singlespacing}\n\\begin{verbatim}\nn <- 100\n\nx1 <- runif(n)\nx2 <- runif(n)\ny <- x1^2+x2^2 + rnorm(n,sd=.1)\n\ndata.train <- data.frame(x1,x2,y)\n\nbw <- npregbw(y~x1+x2,\n              data=data.train,\n              regtype=\"ll\",\n              bwmethod=\"cv.aic\")\n\ndata.eval <-  data.frame(x1 = sort(x1),\n                    x2 = rep(uocquantile(x2,.5),n))\n\nmodel <- npreg(bws=bw,\n               data=data.train, \n               newdata=data.eval,\n               gradients=TRUE)\n\nplot(data.eval[,1],model$grad[,1],xlab=\"X1\",ylab=\"Gradient\",type=\"l\")\n\\end{verbatim}\n\\end{singlespacing}\n\nNote that this uses the sorted sample realizations for \\texttt{x1}\nwhich is perfectly acceptable. If you want to mimic \\texttt{plot()}\nexactly you will see that by default plot uses a grid of equally\nspaced values of length \\texttt{neval=50} as per below. Both are\nperfectly acceptable and the point is they control for the level of\nthe non-axis variables.\n\\begin{singlespacing}\n\\begin{verbatim} \ndata.eval <-  data.frame(x1 = seq(min(x1),max(x1),length=50),\n                         x2 = rep(uocquantile(x2,.5),50))  \n\\end{verbatim}\n\\end{singlespacing}\n\n\\subsection{I use \\texttt{plot()} \\texttt{(npplot())} to plot, say, a density\n    and the resulting plot looks like an inverted density rather than\n    a density\\dots}\n\n  This can occur when the data-driven bandwidth is dramatically\n  undersmoothed. Data-driven (i.e., automatic) bandwidth selection\n  procedures are not guaranteed always to produce good results due to\n  perhaps the presence of outliers or the rounding/discretization of\n  continuous data, among others. By default, \\texttt{npplot()} takes\n  the two extremes of the data (minimum, maximum i.e., actual data\n  points) then creates an equally spaced grid of evaluation data\n  (i.e., not actual data points in general) and computes the density\n  for these points. Since the bandwidth is extremely small, the\n  density estimate at these evaluation points is correctly zero, while\n  those for the sample realizations (in this case only two, the min\n  and max) are non-zero, hence we get two peaks at the edges of the\n  plot and a flat bowl equal to zero everywhere else.\n\n  This can also happen when your data is heavily discretized and you\n  treat it as continuous. In such cases, treating the data as ordered\n  may result in more sensible estimates.\n\n\\subsection{Can \\texttt{npksum()} compute analytical derivatives with\n    respect to a continuous variable?}\n\n  As of version 0.20-0 and up, yes it can, using the \\texttt{operator\n    = \"derivative\"} argument, which is put to its paces in the\n  following code snippet (this supports multiple arguments including\n  \\texttt{\"integral\"} and \\texttt{\"convolution\"} in addition to\n  \\texttt{\"normal\"}, the default).\n\n  \\begin{singlespacing}\n\\begin{verbatim}\nZ <- seq(-2.23,2.23,length=100)\nZc <- seq(-4.47,4.47,length=100)\n\npar(mfrow=c(2,2))\n\nplot(Z,main=\"Kernel\",ylab=\"K()\",npksum(txdat=0,exdat=Z,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"normal\")$ksum,\n         col=\"blue\",type=\"l\")\n\nplot(Z,main=\"Kernel Derivative\",ylab=\"K()\",npksum(txdat=0,exdat=Z,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"derivative\")$ksum,\n         col=\"blue\",type=\"l\")\n\nplot(Z,main=\"Kernel Integral\",ylab=\"K()\",npksum(txdat=0,exdat=Z,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"integral\")$ksum,\n         col=\"blue\",type=\"l\")\n\nplot(Zc,main=\"Kernel Convolution\",ylab=\"K()\",npksum(txdat=0,exdat=Zc,bws=1,\n         ckertype=\"epanechnikov\",ckerorder=2,operator=\"convolution\")$ksum,\n         col=\"blue\",type=\"l\")\n\\end{verbatim}\n  \\end{singlespacing}\n\n  An alternative to computing analytical derivatives is to compute\n  them numerically using finite-differences. One simply computes the\n  kernel sum evaluating the sum with variable $j$ set at $x_j-h_j/2$\n  and calls this, say, $ksum_{j1}$, then again set at $x_j+h_j/2$ and\n  call this $ksum_{j}2$, then compute\n  $\\nabla=(ksum_{j2}-ksum_{j1})/h_j$. This method has been used for\n  both theoretical and applied work and produces consistent estimates\n  of the derivatives, as of course do the analytical derivatives,\n  providing that $h\\to0$ as $n\\to\\infty$ (which will {\\em not} be the\n  case in some settings, i.e., in the presence of irrelevant\n  covariates and so forth). The following example provides a simple\n  demonstration. See \\ref{partial grad q} above for multivariate\n  partial regression when using this method.\n\n  \\begin{singlespacing}\n    \\begin{singlespacing}\n\\begin{verbatim}\n## In this example we consider the local constant estimator computed\n## using npksum, and then use npksum to compute numerical derivatives\n## using finite-difference methods, then finally compare them with the\n## analytical ones.\n\ndata(cps71)\nattach(cps71)\n\n## Grab the cross-validated bandwidth\n\nbw <- npregbw(logwage~age)\nh <- bw$bw[1]\n\n## Evaluate the local constant regression at x-h/2, x+h/2...\n\nksum.1 <- npksum(txdat=age, exdat=age-h/2,tydat=logwage,bws=bw)$ksum/\n  npksum(txdat=age,exdat=age-h/2,bws=bw)$ksum\n\nksum.2 <- npksum(txdat=age, exdat=age+h/2,tydat=logwage,bws=bw)$ksum/\n  npksum(txdat=age,exdat=age+h/2,bws=bw)$ksum\n\n## Compute the numerical gradient...\n\ngrad.numerical <- (ksum.2-ksum.1)/h\n\n## Compare with the analytical gradient...\n\ngrad.analytical <- gradients(npreg(bws=bw,gradient=TRUE))\n\n## Plot the resulting estimates...\n\nplot(age,grad.numerical,type=\"l\",col=\"blue\",lty=1,ylab=\"gradient\")\nlines(age,grad.analytical,type=\"l\",col=\"red\",lty=2)\nlegend(20,-0.05,c(\"Numerical\",\"Analytic\"),col=c(\"blue\",\"red\"),lty=c(1,2))\n\\end{verbatim}\n    \\end{singlespacing}\n  \\end{singlespacing}\n\n\\subsection{Can I use the \\texttt{npcmstest()} function that implements\n    the consistent test for correct specification of parametric\n    regression models as described in Hsiao, Li, \\& Racine (2007)\n    \\cite{HSIAO_LI_RACINE:2007} to test for correct specification of\n    the semiparametric partially linear model?}\n\n  As Brennan Thompson points out, yes, you can.\n\n  To test a parametric linear specification against a semiparametric\n  partially linear alternative, i.e.,\n  \\begin{align*}\n    H_0: y &= X'\\beta + Z'\\gamma + u\\\\\n    H_1: y &= X'\\beta + g(Z) + u,\n  \\end{align*}\n  you could use \\texttt{npcmstest()} as follows:\n  \\begin{singlespacing}\n\\begin{verbatim}\n  lmodel <- lm(y~X+Z,y=TRUE,x=TRUE)\n  uhat <- resid(lmodel)\n  npcmstest(xdat=Z,ydat=uhat,model=lmodel)\n\\end{verbatim}\n  \\end{singlespacing}\n\n  A slightly better way (as discussed in Li \\& Wang (1998)\n  \\cite{LI_WANG:1998}) would be to use a `mixed' residual, i.e., $\\hat\n  u_i = y_i - X_i'\\tilde \\beta - Z_i'\\hat \\gamma$ in the test, where\n  $\\tilde\\beta$ is the semiparametric estimator of $\\beta$ (based on\n  the semiparametric partially linear model), and $\\hat \\gamma$ is the\n  OLS estimator of $\\gamma$ based on the linear model. This could lead\n  to potential power gains due to the improved efficiency of\n  $\\hat\\beta$ under the alternative.\n\n\\subsection{I am using \\texttt{npcmstest()} on a \\texttt{glm()} object (the\n    document says glm() objects are supported and I am estimating a\n    Logit model) but it returns an error saying}\n\\begin{verbatim}\n Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1.\n\\end{verbatim}\n\n  \\texttt{npcmstest()} supports conditional mean models with continuous\n  outcomes (\\texttt{glm()} objects are supported so that models that are\n  nonlinear in parameters can be estimated). The test is based on\n  residual bootstrapping to generate resamples for $Y$. In particular,\n  a resample for the residual vector ($\\hat\\epsilon^*$) is added to\n  the models' fit (i.e., $Y^*=\\hat Y + \\hat\\epsilon^*$) to generate a\n  resample under the null. This excludes binary outcome models and the\n  like because you would have generated a resample for $Y$ that no\n  longer contains zeros and ones, hence the error message.\n\n  Note that \\texttt{npcmstest} supports regression objects generated\n  by \\texttt{lm} and uses features specific to objects of type\n  \\texttt{lm} hence if you attempt to pass objects of a different type\n  the function cannot be expected to work.\n\n\\subsection{I want to plot the kernel function itself. How can I do\n    this?}\n\n  Use the \\texttt{npksum()} function and switch the evaluation and\n  training roles as in the following example that plots the 2nd, 4th,\n  6th and 8th order Epanechnikov kernels.\n  \\begin{singlespacing}\n\\begin{verbatim}\nZ <- seq(-sqrt(5),sqrt(5),length=100)\npar(mfrow=c(2,2))\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=2)$ksum,type=\"l\",main=\"Epanechnikov [order = 2]\")\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=4)$ksum,type=\"l\",main=\"Epanechnikov [order = 4]\")\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=6)$ksum,type=\"l\",main=\"Epanechnikov [order = 6]\")\nplot(Z,ylab=\"kernel\",npksum(txdat=0,exdat=Z,bws=1,ckertype=\"epanechnikov\",\nckerorder=8)$ksum,type=\"l\",main=\"Epanechnikov [order = 8]\")\n\\end{verbatim}\n  \\end{singlespacing}\n\n\\subsection{In version 0.20-0 and up I can `combine' steps such as\n    bandwidth selection and estimation. But when I do \\texttt{summary(model)} I don't get the same summary that I would\n    get from, say, \\texttt{summary(bw)} and then \\texttt{summary(model)}. How do I get bandwidth object summaries\n    when combining steps?}\n\n  Don't worry, the bandwidth object exists when you do the combined\n  steps and is easily accessed via \\texttt{summary(model\\$bws)} or\n  extracted via \\texttt{bw <- model\\$bws} where \\texttt{model} is\n  the name of your model.\n\n\\subsection{I estimated a semiparametric index model via \\texttt{model <-\n      npindex(y\\textasciitilde x1+x2) } but \\texttt{se(model)} returns NULL}.\n\n  Perhaps you want \\texttt{vcov(model)} instead (i.e., the asymptotic\n  variance-covariance matrix)? This is supported as of version 0.40-1\n  provided that you set \\texttt{gradients=TRUE} as the following snippet\n  demonstrates:\n\\begin{verbatim}\nset.seed(42)\nn <- 250\nx1 <- runif(n, min=-1, max=1)\nx2 <- runif(n, min=-1, max=1)\ny <- ifelse(x1 + x2 + rnorm(n) > 0, 1, 0)\n## Note that the first element of the vector beta is normalized to one\n## for identification purposes hence the first row and column of the\n## covariance matrix will contain zeros.\nmodel <- npindex(y~x1+x2, method=\"kleinspady\", gradients=TRUE)\nvcov(model)\nZ <- coef(model)[-1]/sqrt(diag(vcov(model)))[-1]\nZ\n\\end{verbatim}\n\n  Note that, alternatively, you can get robust bootstrapped standard\n  errors for the estimated model and gradients by adding the argument\n  \\texttt{errors=TRUE} to your call to npindex so that\n  \\texttt{se(model)} returns the vector of standard errors for the\n  estimated conditional mean where \\texttt{model} is the name of your\n  model.  Note that \\texttt{model\\$merr } will contain the standard\n  errors returned by \\texttt{se(model)} while \\texttt{model\\$gerr}\n  will return the matrix of standard errors for the gradients {\\em\n    provided} you have set \\texttt{gradients=TRUE} (furthermore,\n  \\texttt{model\\$mean.grad} and \\texttt{model\\$mean.gerr} will give\n  the average derivative and its bootstrapped standard errors). See\n  the documentation of npindex for further details. The following code\n  snippet demonstrates how one could do this for a simulated dataset.\n\\begin{verbatim}\n  n <- 100\n  x1 <- runif(n, min=-1, max=1)\n  x2 <- runif(n, min=-1, max=1)\n  y <- x1 - x2 + rnorm(n)\n\n  bw <- npindexbw(formula=y~x1+x2)\n  model <- npindex(bws=bw,errors=TRUE,gradients=TRUE)\n\n  se.mean <- model$merr\n  se.grad <- model$gerr\n\\end{verbatim}\n\n\\subsection{How do I interpret gradients from the conditional density\n    estimator?}\n\n  If you plot a conditional density $f(y|x)$ when $x$ is a scalar,\n  with gradients, by default you will get the following:\n\n  \\begin{enumerate}\n\n  \\item A plot of $\\partial f(y = \\text{median} | x)/\\partial x$\n    (admittedly not the most useful plot).  (If $y$ is discrete the\n    only difference is that you get a plot of $\\partial f(y =\n    \\text{(unconditional) mode} | x)/\\partial x$).\n\n  \\item A plot of $\\partial f(y | x = \\text{median})/\\partial x$.\n\n  \\end{enumerate}\n\n  If $x$ is multivariate (for example, 2D) you get:\n\n  \\begin{enumerate}\n\n  \\item A plot of $\\partial f(y = \\text{median} | x1, x2 =\n    \\text{median})/\\partial x_1$\n  \\item A plot of $\\partial f(y = \\text{median} | x1, x2 =\n    \\text{median})/\\partial x_2$\n\n  \\item A plot of $\\partial f(y = \\text{median} | x1 = \\text{median},\n    x2)/\\partial x_1$\n  \\item A plot of $\\partial f(y = \\text{median} | x1 = \\text{median},\n    x2)/\\partial x_2$\n\n  \\item A plot of $\\partial f(y | x1 = \\text{median}, x2 =\n    \\text{median})/\\partial x_1$\n  \\item A plot of $\\partial f(y | x1 = \\text{median}, x2 =\n    \\text{median})/\\partial x_2$\n\n  \\end{enumerate}\n\n\\subsection{When I run \\texttt{R} in batch mode via \\texttt{R CMD BATCH\n      filename.R} unwanted status messages (e.g., ```Multistart 1 of\n    10''') crowd out desired output. How can I turn off these unwanted\n    status messages?}\n\n  After loading the np library add the line\n  \\texttt{options(np.messages=FALSE)} and all such messages will be\n  disabled.\n\n\\subsection{I am getting an `unable to allocate...' message after\n    repeatedly interrupting large jobs.}\n\n  Repeated interruption of large jobs can reduce available memory\n  under R. This occurs because memory is allocated dynamically, and\n  memory that has been allocated is not freed when you interrupt the\n  job (the routine will clean up after itself only if it is allowed to\n  complete all computations - when you interrupt you never reach the\n  point in the code where the memory is freed). If this becomes an\n  issue simply restart R (i.e., exit then run a fresh R session).\n\n\\subsection{I have a large number of variables, and when using the\n    formula interface I get an error message stating `invoked with\n    improper formula'. I have double checked and everything looks\n    fine. Furthermore, it works fine with the data frame interface.}\n\n  The issue is that converting formulas into character strings in\n  \\texttt{R} appears to be limited to 500 characters. We are not aware\n  of a simple workaround so we simply advise that you use the data\n  frame interface when this occurs.\n\n\\subsection{How can I estimate additive semiparametric models?}\n\nGeneralized additive semiparametric models (see Hastie and Tibshirani\n(1990) \\cite{HASTIE_TIBSHIRANI:1990}) are implemented in the\n\\texttt{gam} package (though they do not support categorical\nvariables). The \\texttt{gam} package function \\texttt{gam()} (the\n\\texttt{mgcv} package also contains a similar function by the same\nname) uses iteratively reweighted least squares and either smoothing\nsplines (\\texttt{s($\\cdot$)}) or local polynomial regression fitting\n(\\texttt{`loess', lo($\\cdot$)}, with default manual `span' of 0.75,\nthe parameter which controls the degree of smoothing). The following\ncode snippet demonstrates the capabilities of the \\texttt{gam()}\nfunction via the \\verb+wage1+ dataset included in the \\texttt{np}\npackages using three numeric regressors.\n\n\\begin{verbatim}\nlibrary(gam)\ndata(wage1)\nattach(wage1)\nmodel.gam <- gam(lwage~s(educ)+s(exper)+s(tenure))\npar(mfrow=c(2,2))\nplot(model.gam,se=T)\ndetach(wage1)\n\\end{verbatim}\n\nThe \\texttt{mgcv} package also has an implementation of generalized\nadditive models via the identical function name, \\texttt{gam()}. The\nbiggest difference is that this uses generalized cross validation to\nselect the `span' (rather than manually) hence the degree of smoothing\nbecomes part of the method (as is the case for all functions in the\n\\texttt{np} package). The following code snippet demonstrates the\n\\texttt{mgcv} implementation of the \\texttt{gam()} function.\n\n\\begin{verbatim}\nlibrary(mgcv)\ndata(wage1)\nattach(wage1)\nmodel.gam <- gam(lwage~s(educ)+s(exper)+s(tenure))\npar(mfrow=c(2,2))\nplot(model.gam,se=T)\ndetach(wage1)\n\\end{verbatim}\n\n  \\subsection{I am using \\texttt{npRmpi} and am getting an error message\n  involving \\texttt{dlopen()}}\n\nOne of the most common problems experienced by users attempting to\ninstall and run MPI-aware programs is to first correctly identify the\nlocation of libraries and headers for the local MPI installation so\nthat installation can proceed.\n\nBy way of illustration, the following environment variables need to be\nset for the MacPorts version of OpenMPI (www.macports.org) running on\nMac OS X 10.8.3 (the environment commands listed below are for those\nusing the `bash' shell):\n\n\\begin{verbatim}\nexport LD_LIBRARY_PATH=/opt/local/lib\nexport RMPI_LIB_PATH=/opt/local/lib\nexport RMPI_TYPE=OPENMPI\nexport RMPI_INCLUDE=/opt/local/include/openmpi\n\nlaunchctl setenv LD_LIBRARY_PATH $LD_LIBRARY_PATH\nlaunchctl setenv RMPI_LIB_PATH $RMPI_LIB_PATH\nlaunchctl setenv RMPI_TYPE $RMPI_TYPE\nlaunchctl setenv RMPI_INCLUDE $RMPI_INCLUDE\n\\end{verbatim}\n\nOnce set, R (and optionally RStudio) ought to function as\nexpected. However, problems encountered during this phase are best\nresolved by someone with familiarity of the local installation.\n    \n  \\subsection{I am using the \\texttt{npRmpi} package but when I launch\n    one of the demo parallel jobs I get the error \\texttt{Error: could\n      not find function \"mpi.bcast.cmd\"}}\n\nWhen your demo program stops at the following point in your file\n\\begin{verbatim}\n> mpi.bcast.cmd(np.mpi.initialize(),\n+               caller.execute=TRUE)\nError: could not find function \"mpi.bcast.cmd\"\n\\end{verbatim}\nthis likely means that either you have failed to place the\n\\texttt{.Rprofile} file in the current or root directory as directed,\nor the \\texttt{.Rprofile} initialization code is not being loaded as\nexpected.\n\n\\begin{enumerate}\n\n\\item Make sure a copy of the initialization file \\texttt{Rprofile}\n  exists in your working or root directory and is named\n  \\texttt{.Rprofile}\n\n\\item Make sure you did not run \\texttt{R} with either the\n  \\texttt{--no-init-file} or \\texttt{--vanilla} option (this combines\n  a number of options including \\texttt{--no-init-file} which will\n  disable reading of \\texttt{.Rprofile})\n\n\\end{enumerate}\n\n\\subsection{I have estimated a partially linear model and want to\n  extract the gradient/fitted values of the nonparametric component}\n\nPeople use partially linear models because they focus on the\nparametric component and treat the nonparametric component as a\nnuisance. In fact, the partially linear model is estimated by\ncarefully getting rid of the nonparametric component $g(Z)$ prior to\nestimation, and then estimating a set of conditional moments\nnonparametrically.\n\nHowever, suppose after getting rid of this nonparametric nuisance\ncomponent we then wished to construct a consistent estimator of\n$g(Z)$, the nonparametric component for a partially linear model $y =\nX\\beta + g(Z) + u$. We might proceed as follows.\n\n\\begin{verbatim}\nlibrary(np)\n\nset.seed(42)\n\nn <- 250\nx1 <- rnorm(n)\nx2 <- rbinom(n, 1, .5)\n\nz1 <- rbinom(n, 1, .5)\nz2 <- rnorm(n)\n\ny <- 1 + x1 + x2 + z1 + sin(z2) + rnorm(n)\n\n## First, construct the partially linear model using local linear \n## regression.\n\nmodel <- npplreg(y~x1+factor(x2)|factor(z1)+z2,regtype=\"ll\")\n\n## Next, subtract the fitted parametric component from y so that we\n## have y-xbetahat. Since we can have factors we need to create the\n## `model matrix' but make sure we don't keep the intercept generated\n## by model.matrix hence the [,-1]. This gives us the numeric matrix X\n## which we multiply by the coefficient vector to obtain xbetahat which\n## we can subtract from y.\n\ny.sub <- y-model.matrix(~x1+factor(x2))[,-1]%*%coef(model)\n\n## Finally, regress this on the nonparametric components Z using npreg.\n\nmodel.sub <- npreg(y.sub~factor(z1)+z2,regtype=\"ll\",gradients=TRUE)\n\n## Now we can obtain derivatives etc. However, note that this is the\n## model containing the transformed y with respect to the nonparametric\n## components. We can use gradients(model.sub) etc. or plot them and\n## so forth.\n\nplot(model.sub,gradients=TRUE)\n\\end{verbatim}\n\n\\subsection{The R function `lag()' does not work as I expect it\n  to. How can I create the $l$th lag of a numeric variable in R to be\n  fed to functions in the \\texttt{np} package?}\n\nAs of version 0.60-2, time series objects are supported. However, if\nyou prefer you can use the function \\texttt{ts.intersect}, which can\nexploit R's lag function but return a suitable data frame, as per the\nfollowing illustration:\n\\begin{verbatim}\ndata(lynx)\nloglynx <- log(lynx)\nlynxdata <- ts.intersect(loglynx,\n                         loglynxlag1=lag(loglynx,-1),\n                         dframe=TRUE)\nmodel <- npregbw(loglynx~loglynxlag1,\n                 data=lynxdata)\nplot(model)\n\\end{verbatim}\nNote that in order for the above to work, the common argument fed to\n\\texttt{ts.intersect} must be a \\texttt{ts} object, so first cast it\nas such if it is not already (\\texttt{log(lynx)} is a \\texttt{ts}\nobject since \\texttt{lynx} was itself a \\texttt{ts} object).\n\nOr, you can use the \\texttt{embed} function to accomplish\nthis task. Here is a simple function that might work more along the\nlines that you expect. By default we `pad' the vector with NAs but you\ncan switch this to \\texttt{FALSE} if you prefer. The function will\nreturn a vector of the same length as the original vector with NA's\npadded for the missing values.\n\n\\begin{verbatim}\nlag.numeric <- function(x,l=1,pad.NA=TRUE) {\n  if(!is.numeric(x)) stop(\"x must be numeric\")\n  if(l < 1) stop(\"l (lag) must be a positive integer\")\n  if(pad.NA) x <- c(rep(NA,l),x)\n  return(embed(x,l+1)[,l+1])\n}\nx <- 1:10\nx.lag.1 <- lag.numeric(x,1)\n\\end{verbatim}\n\n\\subsection{Can I provide sample weights to be used by functions in\n  the \\texttt{np} package?}\n\nUnfortunately, at this stage the answer is `no', at least not directly\nwith many of the functions as they stand. However, the function\n\\texttt{npksum} underlies many of the functions in the \\texttt{np}\npackage and it supports passing of weights, so you may be able to use\nthis function and do a bit of coding to fulfill your needs. Kindly see\n\\texttt{?npksum} for illustrations.\n\nAlternatively, if reweighting of sample data is sufficient for your\nneeds then you can feed the weighted sample realizations directly to\nexisting functions.\n\n\\subsection{Quantile regression is slow, particularly for very\n  small/large values of tau. Can this be sped up?}\n\nYes, indeed this can be the case for extreme values of tau (and not so\nextreme values as well). The reason for this is because numerical\nmethods are used to invert the CDF. This must be done for each\npredictor observation requiring the solution to $n$ (or $neval$)\noptimization problems. An alternative is to compute the\n`pseudo-inverse' via a `lookup method'. In essence, one computes the\nconditional CDF for a range of $y$ values, and then computes the\npseudo-inverse which is defined as\n\\begin{align*}\n  q_{\\tau}(x) &= \\inf \\{ y: \\, F(y|x) \\geq \\tau \\}\\\\\n  & = \\sup \\{ y: \\, F(y|x) \\le \\tau \\}\n\\end{align*}\n\nThe following code demonstrates this approach for the example used in\nthe help file for \\texttt{npqreg} (see \\texttt{?npqreg}).\n\n\\begin{verbatim}\ndata(\"Italy\")\nattach(Italy)\n     \nbw <- npcdensbw(gdp~year)\n\n## Set a grid of values for which the conditional CDF will be computed\n## with a range that extends well beyond the range of the data\n\nn.eval <- 1000\ngdp.er <- extendrange(gdp,f=2)\ngdp.q <- quantile(gdp,seq(0,1,length=n.eval))\ngdp.eval <- sort(c(seq(gdp.er[1],gdp.er[2],length=n.eval),gdp.q))\nn.q <- length(gdp.eval)\n\n## We only need to compute the conditional quantiles for each unique\n## value of year\n\nyear.unique <- unique(year)\n\n## Consider a range of values for tau\n\ntau.seq <- c(0.01,0.05,0.25,0.5,0.75,0.95,0.99)\ngdp.tau <- matrix(NA,length(year.unique),length(tau.seq))\n\nfor(j in 1:length(tau.seq)) {\n  cat(\"\\r\",j,\"of\",length(tau.seq))\n  tau <- tau.seq[j]\n  for(i in 1:length(year.unique)) {\n    F <- fitted(npcdist(bws=c(bw$ybw,bw$xbw),\n                        txdat = year,\n                        tydat = gdp,\n                        exdat = rep(year.unique[i],n.q),\n                        eydat = gdp.eval))\n    ## For each realization of the predictor, compute the \n    ## the pseudo-inverse\n    gdp.tau[i,j] <- ifelse(tau>=0.5, max(gdp.eval[F<=tau]), \n                                     min(gdp.eval[F>=tau]))\n  }\n}\n\n## Plot the results\n\nplot(year,gdp,ylim=c(min(gdp.tau,gdp),max(gdp.tau,gdp)))     \nfor(j in 1:length(tau.seq)) {\n  lines(year.unique,gdp.tau[,j],col=j+1,lty=j,lwd=2)\n}\n\nlegend(min(year),max(gdp.tau,gdp),\n       paste(\"tau=\",tau.seq),\n       col=1:length(tau.seq)+1,\n       lty=1:length(tau.seq),\n       lwd=2)\n\\end{verbatim}\n\n\\subsection{How can I generate resamples from the unknown distribution\n  of a set of data based on my smooth kernel density estimate?}\n\nThis can be accomplished by picking a sample realization, uniformly at\nrandom, then drawing from the kernel distribution centered on that\ntraining point with scale equal to the bandwidth. Below is a\ndemonstration for the `Old Faithful' data where we draw a random\nsample of size $n=1,000$ where a Gaussian kernel was used for the\ndensity estimator.\n\n\\begin{verbatim}\ndata(faithful)\nn <- nrow(faithful)\n\nx1 <- faithful$eruptions\nx2 <- faithful$waiting\n\n## First compute the bandwidth vector\n\nbw <- npudensbw(~x1+x2,ckertype=\"gaussian\")\n\n## Next generate draws from the kernel density (Gaussian)\n\nn.boot <- 1000\n\ni.boot <- sample(1:n,n.boot,replace=TRUE)\nx1.boot <- rnorm(n.boot,x1[i.boot],bw$bw[1])\nx2.boot <- rnorm(n.boot,x2[i.boot],bw$bw[2])  \n\n## Plot the density for the bootstrap sample using the original \n## bandwidths\n  \nplot(npudens(~x1.boot+x2.boot,bws=bw$bw),view=\"fixed\",xtrim=-.2,neval=100)\n\\end{verbatim}\n\n\\subsection{Some of my variables are measured with an unusually large\n  number of digits\\dots should I rescale?}\n\nIn general there should be no need to rescale data for one's\nstatistical analysis. However, occasionally one can encounter issues\nwith numerical accuracy (e.g.\\ numerical `overflow') regardless of the\nmethod used. It is therefore prudent to be aware of this issue. For\ninstance, if your dependent variable represents housing prices in a\nparticular currency and entries are recorded as e.g.\\\n145,000,000,000,000,000,000 foounits, then it might be prudent to\ndeflate this variable by $10^{20}$ (i.e.\\ 1.45) so that e.g.\\ sums of\nsquares are numerically stable (say when using least squares\ncross-validation). Of course you can run your analysis with and\nwithout the adjustment and see whether it matters or not. But it is\nsometimes surprising that such things can in fact make a\ndifference. See \\texttt{?scale} for a function whose default method\ncenters and/or scales the columns of a numeric matrix.\n\n\\subsection{The local linear gradient estimates appear to be somewhat\n  `off' in that they do not correspond to the first derivative of the\n  estimated regression function.}\n\nIt is perhaps not a widely known fact that the local linear partial\nderivatives obtained from the coefficients of the Taylor\napproximation, $\\hat b(x)$, are not the analytical derivative of the\nestimated regression $\\hat g(x)$ with respect to $x$.  The local\nlinear estimator is obtained by minimizing the following weighted\nleast squares function (consider the one-predictor case to fix ideas)\n\\begin{equation*}\n\\sum_i(Y_i - a - b (x-X_i))^2 K\\left(\\frac{x-X_i}{h}\\right)\n\\end{equation*}\nand the estimated regression function $\\hat g(x)$ is given by\n\\begin{equation*}\n\\hat g(x) =\\hat a \n\\end{equation*}\nwhile the gradient is given by\n\\begin{equation*}\n\\hat \\beta(x)=\\hat b.\n\\end{equation*}\nBut the analytical gradient (i.e.\\ the partial derivative of $\\hat\ng(x)=\\hat a$ with respect to $x$) is \\textsl{not} $\\hat b$ unless the\nbandwidth is very large (i.e.\\ $h=\\infty$) in which case\n$K((x-X_i)/h)=K(0)$ and one gets the standard linear least squares\nestimates for $\\hat g(x)$ and $\\hat\\beta(x)$ (if you compute the\npartial algebraically you will see they are not the same).\n\nSo, the derivative estimates arising directly from the local linear\nestimator will differ from the analytical derivatives, even though\nthey are asymptotically equivalent under standard conditions required\nfor consistency. Thus, if economic constraints are imposed on the\ndirect derivatives, this may produce an estimated surface which is not\nconsistent with the constraints.  This can be avoided by imposing the\nconstraints on the analytical derivatives of the local polynomial\nestimator being used.\n\n\\subsection{How can I turn off all console I/O?}\n\nTo disable all console I/O, set \\texttt{options(np.messages=FALSE)}\nand wrap the function call in \\texttt{suppressWarnings()} to disable any\nwarnings printed to the console. For instance\n\\begin{verbatim}\nlibrary(np)\noptions(np.messages=FALSE)\nset.seed(42)\nn <- 100\nx <- sort(rnorm(n))\nz <- factor(rbinom(n,1,.5))\ny <- x^3 + rnorm(n)\nmodel.np <- suppressWarnings(npreg(y~x+z))\n\\end{verbatim}\nought to produce no console I/O whatsoever in the call to\n\\texttt{npreg}.\n\n\\bibliographystyle{plain} \n\n\\bibliography{np_faq}\n\n\\clearpage\n\n\\appendix\n\n\\section*{Changes from Version 0.60-1 to 0.60-2 [27-Jun-2014]}\n\n\\begin{itemize}\n\n\\item added timeseries support for all relevant objects (i.e.\\ for a\n  \\texttt{ts()} vector data object \\texttt{x},\n  \\verb^npreg(x~lag(x,-1)+lag(x,-2))^ is now supported)\n\n\\item added total time to summary for bandwidth objects\n\n\\item \\texttt{npqreg()} no longer accepts \\texttt{gradients=TRUE} when\n  gradients are in fact not supported\n\n\\item \\texttt{npqreg()} fails with an informative message when passed\n  a conditional density bandwidth object\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.60-0 to 0.60-1 [6-Jun-2014]}\n\n\\begin{itemize}\n\n\\item Fixed glitch in adaptive\\_nn/generalized\\_nn bandwidths that\n  affected all routines that rely on non-fixed bandwidths\n\n\\item Tweaks to search for adaptive\\_nn/generalized\\_nn initial search values\n\n\\item Fixed glitch in local linear estimation with adaptive\\_nn bandwidths\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.50-1 to 0.60-0 [1-Jun-2014]}\n\n\\begin{itemize}\n\n\\item Ordered kernel types now default to liracine/liracine\n  (normalized) for conditional/unconditional objects, respectively\n  (the previous default, i.e.\\ the Wang van Ryzin kernel, is poorly\n  behaved when smoothing out of ordered predictors is appropriate)\n\n\\item Added analytical ordered CDF kernels, resulting in significant\n  speedups for cross validation with ordered variables\n\n\\item Added analytical ordered convolution kernels, resulting in significant\n  speedups for least-squares cross validation with ordered variables\n\n\\item The entire C backend has been rewritten and improved in almost every\n  regard\n\n\\item The Rmpi backend has been updated to Rmpi version 0.6-5\n\n\\item Glitch in adaptive convolution kernels corrected\n\n\\item Added truncated gaussian kernel (see \\texttt{ntpgauss()} for\n  modifying the truncation radius)\n\n\\item Support for trees complete (use \\texttt{options(np.tree=TRUE)})\n  which when used in conjunction with bounded kernels (i.e.\\\n  \\texttt{\"epanechnikov\"}/\\texttt{\"truncated gaussian\"}) can reduce\n  the computational burden for certain problems\n\n\\item Optimizers that make use of Powell's direction set method now accept\n  additional arguments that can be used to potentially improve\n  default settings\n\n\\item Default search settings for optimizers that make use of Powell's\n  direction set method should better scale to the range of variables\n\n\\item Added mean absolute deviation/1.4826 to mix of robust scale elements\n\n\\item Corrected error in order of conditional density/distribution manual\n  bandwidths pointed out by Decet Romain\n\n\\item Figure in vignette not displaying properly, needed\n  \\texttt{png=TRUE} reported by Christophe Bontemps\n\n\\item Using \\texttt{chol2inv}/\\texttt{chol} rather than solve\n  throughout R routines that rely on inversion\n\n\\item Fixed glitch in \\texttt{npindexbw()} to stop maxit from blowing\n  up every time convergence fails\n\n\\item Fixed issue with summary reporting incorrect value of objective function\n  in certain bandwidth objects\n\n\\item When \\texttt{nmulti} $>$ 1, the full multi-starting search\n  history is now returned in a vector named fval.history\n\n\\item Added \\texttt{na.action} for consistency with other R functions\n  such as \\texttt{lm()}\n\n\\item New function \\texttt{npquantile()} that returns smooth\n  univariate quantiles\n\n\\item \\texttt{npksum()} explicitly only uses raw bandwidths now (and\n  will emit an error if passed numeric scale factors, bandwidth\n  objects are still OK)\n\n\\item Fixed regression in \\texttt{npindex()} with bootstrapped\n  standard errors\n\n\\item Code makes use of one call to \\texttt{npksum()} in\n  \\texttt{npindex()} and \\texttt{npscoef()} where possible rather than\n  two separate calls\n\n\\item Updated \\texttt{npsigtest()} for addition of power and added\n  joint test to the mix\n\n\\item Changed \\texttt{ceiling()} to \\texttt{max(1,round())} in\n  \\texttt{b.star()} per Dimitris Politis's suggestion\n\n\\item Reworked the interface for \\texttt{npcopula()} to avoid two\n  bandwidths and \\texttt{density=TRUE} but exploiting passing of\n  either \\texttt{npudistbw()} (copula) or \\texttt{npudensbw} (copula\n  density)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-13 to 0.50-1 [13-Mar-2013]}\n\n\\begin{itemize}\n\n\\item The functions \\texttt{npudist()} and \\texttt{npudistbw()} are\n  now uncoupled from \\texttt{npudens()} and \\texttt{npudensbw()}\n  (previously they relied on unconditional PDF bandwidths due to the\n  lack of a multivariate mixed-data method for selecting bandwidths\n  for CDFs - now with Li \\& Racine (2013) we have a robust method\n  hence the split)\n\n\\item The use of \\texttt{cdf=TRUE} is deprecated for\n  \\texttt{npudens()} and \\texttt{npudensbw()} objects due to the\n  uncoupling described above\n\n\\item Fixed glitch in \\texttt{gradient} standard errors in\n  \\texttt{npindex()} where identical standard errors were output in\n  \\texttt{model\\$gerr}\n\n\\item Fixed glitch in covariance matrix in \\texttt{npindex()}\n  (\\texttt{\"ichimura\"}) where covariance matrix was not symmetric\n\n\\item Fixed glitch in npksum where use of\n  \\texttt{bwtype=\"adaptive\\_nn\"} and \\texttt{operator=\"integral\"}\n  produced the survivor function rather than the cumulative\n  distribution function\n\n\\item Cleaned up internals for \\texttt{npcmstest()}\n\n\\item Using .onUnload rather than .Last.lib in zzz.R\n\n\\item Fixed glitch in \\texttt{npreg()} summary where `Residual\n  standard error' was reporting residual variance\n\n\\item \\texttt{npksum()} functionality extended\n\n\\begin{itemize}\n\n\\item \\texttt{npksum()} can now return the matrix of kernel weights\n  (which can be useful for constrained estimation, by way of\n  illustration, or constructing novel kernel-based procedures without\n  the need to write extensive code)\n\n\\item \\texttt{npksum()} can now accept different operators on a\n  product kernel, for example,\n\\begin{verbatim}\n  npksum(txdat=data.frame(x1,x2),operator=c(\"derivative\",\"normal\"),bws=c(1,1))\n\\end{verbatim}\n  will use the derivative kernel for x1 (i.e. the derivative of the\n  gaussian kernel) and the default kernel for x2 (i.e. a standard\n  kernel such as the gaussian) thereby allowing the user to program a\n  number of estimators such as conditional CDFs etc. that were\n  previously not available via \\texttt{npksum()}\n\n\\end{itemize}\n\n\\item Fixed glitch with variable scope where certain objects could not be\n  found in the environment\n\n\\item Added function \\texttt{npcopula()} for d-dimensional copula\n  estimation via inversion\n\n\\item Modified stopping rules in \\texttt{npregiv()} and\n  \\texttt{npregivderiv()}\n\n\\item Added reference to $R^2$ measure (Doksum and Samarov (1995))\n\n\\item Startup message points to the faq, faq is now a vignette\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-12 to 0.40-13 [05-Mar-2012]}\n\n\\begin{itemize}\n\n\\item Added new function \\texttt{npregivderiv()} that implements the\n  IV derivative method of Florens and Racine (2012)\n\n\\item Added more passable parameters to \\texttt{npregiv()}\n  (multistarting, parameters passed to \\texttt{optim()} for\n  cross-validation)\n\n\\item Changes to code to improve compliance with R `Writing portable\n  packages' guidelines and correct partial argument matches\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-11 to 0.40-12 [24-Nov-2011]}\n\n\\begin{itemize}\n\n\\item Added option (user request) to hold the bandwidth fixed but\n  optimize the parameters in the single index model\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-10 to 0.40-11 [24-Oct-2011]}\n\n\\begin{itemize}\n\n\\item Corrected code regression in single index errors introduced\n  inadvertently in 0.40-10\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-9 to 0.40-10 [24-Oct-2011]}\n\n\\begin{itemize}\n\n\\item Modified Silverman's adaptive measure of spread to reflect\n  changes in \\texttt{sd()} (sd on matrix deprecated)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-8 to 0.40-9 [30-July-2011]}\n\n\\begin{itemize}\n\n\\item Renamed COPYING file to COPYRIGHTS\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-7 to 0.40-8 [29-July-2011]}\n\n\\begin{itemize}\n\n\\item Fixed issue where calling \\texttt{npplot} resets system seed\n\n\\item Updated examples in docs so that plot is recommended throughout\n  (and not \\texttt{npplot} that is invoked by \\texttt{plot})\n\n\\item Fixed regression in \\texttt{npindex} when \\texttt{gradients=TRUE} and\n  \\texttt{errors=TRUE}\n\n\\item Function \\texttt{npindex/npindexbw} now accepts additional\n  arguments and implements this properly (i.e. proper implementation\n  by Tristen of Version 0.30-8/0.30-9 change for \\texttt{npindex})\n\n\\item Function \\texttt{npplreg} now supports factors in the parametric\n  part just like \\texttt{lm()} does\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-6 to 0.40-7 [8-Jun-2011]}\n\n\\begin{itemize}\n\n\\item Function \\texttt{npregiv} now supports exogenous $X$ and multivariate\n  $Z$ and $W$.\n\n\\item \\texttt{demo(npregiv)} provides a useful illustration.\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-5 to 0.40-6 [1-Jun-2011]}\n\n\\begin{itemize}\n\n\\item Added a new function \\texttt{npregiv} that conducts nonparametric\n  instrumental regression a la Darolles, Fan, Florens and Renault\n  (2011, Econometrica) and Horowitz (2011, Econometrica). Note that\n  this function currently returns the fitted $\\varphi(z)$ (i.e.\\ lacks\n  much functionality relative to other \\texttt{np} functions) and is in\n  `beta status' until further notice.\n\n\\item Added a new dataset \\texttt{Engel95} that allows one to estimate\n  Engel curves using the new nonparametric instrumental regression\n  function \\texttt{npregiv}.\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-4 to 0.40-5 [26-Apr-2011]}\n\n\\begin{itemize}\n\n\\item Fixed issue with \\texttt{npindexbw} where, for certain problems,\n  starting values needed refinement otherwise convergence would fail\n  (we now use an improved normalization for the starting values)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-3 to 0.40-4 [21-Jan-2011]}\n\n\\begin{itemize}\n\n\\item Fixed issue with \\texttt{ckertype} and \\texttt{ckerorder} not\n  being propagated in \\texttt{np.singleindex.bw.R}\n\n\\item Fixed issue with negative penalties being returned by\n  \\texttt{bwmethod=\"cv.aic\"} in \\texttt{npregbw} (ought to have been\n  +infinity)\n\n\\item Error being thrown by \\texttt{system(..., intern=TRUE)} when\n  \\texttt{mpi.quit()} is called, changed to FALSE (change to\n  \\texttt{system()} behaviour detailed in R CHANGELOG 2.12.0)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-1 to 0.40-3 [23-Jul-2010]}\n\n\\begin{itemize}\n\n\\item Added random seed (defaults to 42) to \\texttt{npscoefbw} to ensure\n  consistent values for optimization for successive invocations on the\n  same data\n\n\\item Fixed glitch in multistarting in \\texttt{npscoefbw} whereby\n  multistarting was not working (always returned last computed\n  function value and not the minimum)\n\n\\item Fixed issue for \\texttt{npRmpi} where the \\texttt{C} code\n  underlying regression cross-validation (code in \\texttt{jksum.c})\n  differs between np and npRmpi (both are correct with the latter\n  being a tad slower)\n\n\\item Fixed a scope issue whereby a user would write a function that\n  calls an \\texttt{np/npRmpi} command, however, objects passed to the\n  user's function and called by the \\texttt{np/npRmpi} command\n  (i.e. such as \\texttt{newdata}) cannot be found in the environment\n  yet they exist outside of the function\n\n\\item Fixed issue with \\texttt{bwscaling=TRUE} and\n  \\texttt{bwmethod=\"cv.aic\"} in npreg\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.40-0 to 0.40-1 [4-Jun-2010]}\n\n\\begin{itemize}\n\n\\item Added asymptotic standard errors to \\texttt{npindex} for the\n  Klein and Spady and Ichimura parameter estimates which, when\n  \\texttt{gradients=TRUE}, can be extracted via \\texttt{vcov(foo)}\n  where \\texttt{foo} is a npsingleindex object (the Z-scores can be\n  obtained via \\texttt{Z <- coef(foo)[-1]/sqrt(diag(vcov(foo)))[-1]})\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-9 to 0.40-0 [25-May-2010]}\n\n\\begin{itemize}\n\n\\item Modified codebase to enable dynamic spawning for\n  interactive sessions in \\texttt{npRmpi}\n\n\\item Interactive examples supported in \\texttt{npRmpi}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-8 to 0.30-9 [17-May-2010]}\n\n\\begin{itemize}\n\n\\item Fixed issue where \\texttt{ukertype} and \\texttt{okertype} were being\n  ignored by \\texttt{npscoef}\n\n\\item Fixed code regression (dating to version 0.30-4) where\n  \\texttt{random.seed=42} was not initialized in functions\n  \\texttt{npcmstest}, \\texttt{npdeneqtest}, \\texttt{npindexbw},\n  \\texttt{npsdeptest}, \\texttt{npqcmstest}, \\texttt{npsigtest},\n  \\texttt{npsymtest}, \\texttt{npunitest}, and \\texttt{npplot}\n\n\\item Fixed issue with saving and restoring random seed in \\texttt{npdeptest}\n\n\\item Changes to codebase to modify method used to prevent division by\n  zero\n\n\\item New vignette for the \\texttt{npRmpi} package\n  \\texttt{(vignette(\"npRmpi\",package=\"npRmpi\"))}\n\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-7 to 0.30-8 [20-Apr-2010]}\n\n\\begin{itemize}\n\n\\item Implemented moment version of metric entropy in \\texttt{npsymtest}\n  and \\texttt{npunitest} with warnings about their use documented\n  carefully and exceptions trapped and warnings issuing when detected\n \n\\item Cleaned up print/summary output formatting of some functions\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-6 to 0.30-7 [15-Feb-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npunitest} for entropy-based testing of\n  equality of univariate densities as described in Maasoumi and Racine\n  (2002)\n\n\\item Updated vignette to reflect new functions from 0.30-4 upwards\n  (Table 1: np functions)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-5 to 0.30-6 [3-Feb-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npsdeptest} for entropy-based testing of\n  nonlinear serial dependence described in Granger, Maasoumi and\n  Racine (2004)\n\n\\item Added function \\texttt{npdeptest} for entropy-based testing of\n  nonlinear pairwise dependence described in Maasoumi and Racine\n  (2002)\n\n\\item Added more bootstrap options to \\texttt{npsymtest} (now both iid\n  and time-series bootstrapping are supported)\n\n\\item Cleaned up summary formatting in the vignette by adding\n  \\verb+\\usepackage[utf8x]{inputenc}+ to the Sweave file np.Rnw\n\n\\item Fixed issue with saving and restoring random seed when there was\n  none in the environment\n  \n\\end{itemize}\n\n\\section*{Changes from Version 0.30-4 to 0.30-5 [29-Jan-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npdeneqtest} for integrated squared\n  difference testing of equality of densities as described in\n  Maasoumi, Li, and Racine (2009), Journal of Econometrics\n\n\\item Save random seed prior to setting seed in certain functions,\n  then restore seed after function completes\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-3 to 0.30-4 [27-Jan-2010]}\n\n\\begin{itemize}\n\n\\item Added function \\texttt{npsymtest} for entropy-based testing of\n  symmetry described in Maasoumi and Racine (2009), Econometric\n  Reviews\n\n\\item Added function \\texttt{b.star} that automates block length\n  selection for the stationary and circular bootstrap\n\n\\item Cleaned up docs\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-2 to 0.30-3 [28-May-2009]}\n\n\\begin{itemize}\n\n\\item Corrected error in Epanechnikov convolution kernels for fixed\n  and generalized bandwidth objects\n\n\\item Changed default example in \\texttt{npscoef}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-1 to 0.30-2 [19-Apr-2009]}\n\n\\begin{itemize}\n\n\\item \\texttt{min(std,IQR/1.348)} is the adaptive measure of spread. We\n  now test for the pathological case where \\texttt{IQR=0} but \\texttt{std>0}\n  and return \\texttt{std} in this instance\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.30-0 to 0.30-1 [29-Jan-2009]}\n\n\\begin{itemize}\n\n\\item \\texttt{predict()} now supports bandwidth, density, distribution,\n  conbandwidth, condensity, and condistribution objects\n\n\\item Consistently allow predictions for categorical values outside of\n  support of training data\n\n  Note that predictions based upon unconditional density objects\n  defined over categorical variables that lie outside the support of\n  the training data may no longer be true probabilities (i.e., as\n  defined over the training data and the extended/augmented support --\n  their sum may exceed one) and may therefore require renormalization\n  by the user\n\n\\item Fixed a numerical issue which could hinder \\texttt{npregbw()}'s\n  cross validation with higher-order kernels\n\n\\item Default \\texttt{nmulti} in \\texttt{npplregbw()} is now set correctly\n\n\\item Fixed a bug with the ridging routine in \\texttt{npscoefbw()}, added\n  ridging to \\texttt{npscoef()}\n\n\\item Fixed minor i/o issue with \\texttt{Multistart 1 of...} using\n  \\texttt{npscoefbw()}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-4 to 0.30-0 [15-Jan-2009]}\n\n\\begin{itemize}\n\n\\item Added basic user-interrupt checking for all underlying C code so\n  that either \\verb+<Ctrl-C>+ (Rterm) or the `STOP' icon (Rgui) will\n  interrupt all running processes. This has a number of desirable side\n  effects in addition to being able to interrupt C-based processes\n  including i) R no longer showing up as `not responding' under the\n  task manager (Windows) or the activity monitor (Mac OS X) and ii)\n  buffered output now being correctly displayed when using Rgui under\n  Windows and Mac OS X\n\n  Note that repeated interruption of large jobs can reduce available\n  memory under R - if this becomes an issue (i.e., you get a `cannot\n  allocate...' error under R) simply restart R (i.e., exit then run a\n  fresh R session)\n\n\\item Added a function \\texttt{npseed()} that allows the user to\n  set/reset the random seed for all underlying C routines\n\n\\item Fixed a bug that caused \\texttt{npplregbw()} to ignore any\n  kernel options for the regression of $y$ on $z$\n\n\\item Refined certain constants used in the normal-reference density\n  bandwidth rule for increased accuracy\n\n\\item Moved from using the maximum likelihood estimate of variance\n  throughout to the degrees of freedom corrected estimate (all\n  variance estimates now change by the factor (n-1)/n)\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-3 to 0.20-4 [19-Nov-2008]}\n\n\\begin{itemize}\n\n\\item Using an adaptive measure of spread throughout.  The scale\n  factor reported for a bandwidth can appear to be small when the\n  sample standard deviation of the associated variable is inflated due\n  to the presence of outliers.  Furthermore, supplying a scale factor\n  of, say, 1.06 for density estimation when there are outliers that\n  inflate the standard deviation may oversmooth rather dramatically in\n  the presence of outliers.  We now use the measure found in Silverman\n  (1986, equation (3.30)) which is min(standard deviation,\n  interquartile range/1.349). This robust choice produces expected\n  results for scale factors in the presence of outliers\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-2 to 0.20-3[14-Nov-2008] }\n\n\\begin{itemize}\n\n\\item Fixed a typo which caused \\texttt{predict()} and \\texttt{plot()} to\n  abort when called on \\texttt{plregression} objects, and which also\n  prevented \\texttt{print()} and \\texttt{summary()} from printing\n  information about the kernels used when called on \\texttt{plregression}\n  objects\n\n\\item Fixed a typo which caused partially linear regressions to crash\n  when out-of-sample responses were provided with evaluation data\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-1 to 0.20-2 [02-Nov-2008]}\n\n\\begin{itemize}\n\n\\item Allow for evaluation outside of discrete support of factors in\n  \\texttt{npksum()} and fixed a warning in jksum\n\n\\item Fixed a bug which lead to unpredictable behavior when there were\n  more categorical values for the training data than realisations\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.20-0 to 0.20-1 [13-Aug-2008]}\n\n\\begin{itemize}\n\n\\item Work-around for scale-factor issues during \\texttt{npregbw()} cv\n  when changing the training data\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.14-3 to 0.20-0 [28-Jul-2008]}\n\n\\begin{itemize}\n\n\\item \\texttt{npksum()} now supports an expanded set of kernels\n  (including convolution, derivative and integral), which can be\n  selected via the \\texttt{operator =} argument\n\n\\item Automatic bandwidth searches are now performed when attempting\n  to evaluate on data without bandwidths. This allows users to combine\n  bandwidth selection and estimation in one step\n\n\\item The \\texttt{npsigtest()} interface is brought in line with other\n  functions (S3)\n\n\\item Significance tests can now be performed on \\texttt{npreg()}\n  outputs, so \\texttt{npsigtest(modelname)} is now supported\n\n\\item Added a vignette and faq. To see the vignette try\n  \\texttt{vignette(\"np\",package=\"np\")}\n\n\\item \\texttt{summary()} on \\texttt{npconmode()} now properly\n  retrieves names from bandwidth objects\n\n\\item Fixed the 6th and 8th order epanechnikov kernels\n\n\\item Fixed some quietness issues\n\n\\item \\texttt{npplot()} now returns data upon request for conditional\n  densities\n\n\\item \\texttt{npreg()} and \\texttt{npcdens()} now take the appropriate\n  limits in some pathological cases\n\n\\item User supplied bandwidths now operate seamlessly with the formula\n  interface\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.14-2 to 0.14-3 [02-May-2008]}\n\n\\begin{itemize}\n\n\\item Fixed a glitch that only arose when using the \\texttt{liracine}\n  unordered kernel in the presence of irrelevant variables. The upper\n  bound for numerical search was constrained to be (c-1)/c [that for\n  the \\texttt{aitchisonaitken} unordered kernel] but ought to have been\n  1. The summary output would therefore show a value of lambda hitting\n  the (smaller) upper bound (c-1)/1 when it may have hit the (larger)\n  upper bound 1\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.14-1 to 0.14-2 [11-Jan-2008]}\n\n\\begin{itemize}\n\n\\item Relaxed checking tolerances slightly to prevent spurious\n  'invalid bandwidth' errors\n\n\\item Empty sections were removed from help files\n\n\\item \\texttt{example(foobar)} now works again. This was disabled in\n  0.14-1 at the request of the \\texttt{R} maintainers in order to shorten\n  the duration of \\texttt{R CMD check}. All examples remained in the help\n  files but due to the presence of `dontrun' they were not run when\n  \\texttt{example(foobar)} is requested. Now a limited subset is run\n  while the full set of examples remain in the documents\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.13-1 to 0.14-1 [18-Dec-2007] }\n\n\\begin{itemize}\n\n\\item Now use \\texttt{optim()} for minimisation in single index and\n  smooth coefficient models\n\n\\item Fixed bug in klein-spady objective function\n\n\\item Standard errors are now available in the case of no continuous\n  variables\n\n\\item Summary should look prettier, print additional information\n\n\\item Tidied up lingering issues with out-of-sample data and\n  conditional modes\n\n\\item Fixed error when plotting asymptotic errors with conditional\n  densities\n\n\\item Fixed a bug in \\texttt{npplot()} with partially linear regressions\n  and \\texttt{plot.behavior=\"data\"} or \\texttt{\"plot-data\"}\n\n\\item Maximum default number of multistarts is 5\n\n\\item Least-squares cross-validation of conditional densities uses a\n  new, much faster algorithm\n\n\\item New, faster algorithm for least-squares cross-validation for\n  both local-constant and local linear regressions\n\n  Note that the estimator has changed somewhat: both cross-validation\n  and the estimator itself use a method of shrinking towards the local\n  constant estimator when singularity would otherwise lead to the\n  breakdown of the estimator. This arises in sparse data settings in\n  conjunction with small bandwidths for one or more regressor\n\n\\item Optimised smooth coefficient code, added ridging\n\n\\item Fixed bug in uniform CDF kernel\n\n\\item Fixed bug where \\texttt{npindexbw()} would ignore\n  \\texttt{bandwidth.compute = FALSE} and compute bandwidths when\n  supplied with a preexisting bw object\n\n\\item Now can handle estimation out of discrete support\n\n\\item Summary would misreport the values of discrete scale factors\n  which were computed with \\texttt{bwscaling = TRUE}\n\n\\end{itemize}\n\n\\section*{Changes from Version 0.12-1 to 0.13-1  [03-May-2007]}\n\n\\begin{itemize}\n\n\\item Bandwidths are now checked for validity based on their variable\n  and kernel types\n\n\\item np now does a better job of preserving names of some 'y' data\n\n\\item Names of coefficients returned from \\texttt{coef()} now match\n  variable names\n\n\\item Fixed some corner cases in \\texttt{npksum()} involving the\n  dimensionality of outputs\n\n\\item Fixed deprecation warnings in \\texttt{R} 2.5.0 caused by use of \\$\n  on atomic objects\n\n\\item Various and sundry bug fixes in \\texttt{npscoef()}\n\n\\item \\texttt{npscoef()} now handles discrete 'z' data\n\n\\item Predict now accepts the argument 'se.fit', like predict.lm\n\n\\item Fixed bug where incorrect asymptotic standard errors of\n  gradients for regression objects were being displayed in \\texttt{npplot()}\n\n\\item Fixed bug where errors of gradients of regression objects were\n  not being returned in matrix form\n\n\\item \\texttt{vcov()} now works with partially linear regression objects\n\n\\item Fixed detection of evaluation responses when using the formula\n  interface\n\n\\item Pre-computed bandwidth objects are now provided for some of the\n  more computationally burdensome examples\n\n\\item Added Jeffrey Wooldridge's WAGE1 dataset with qualitative\n  variables (married, female, nonwhite)\n\n\\item Predictions outside of discrete support for regressions and\n  conditional densities are now allowed\n\n\\item Fixed sign issue with scaling of standard errors in the single\n  index model\n\n\\item Fixed error when calculating some bandwidths/scale factors for\n  display purposes\n\n\\item Bug in passing certain arguments to \\texttt{npcdensbw()} fixed\n\n\\item Added predict method for qregression objects\n\n\\item Proper normalisation for liracine kernel shown in summary\n\n\\item Fixed output bug ($\\hat ~$H) in summary method for sigtest\n  objects\n\n\\item Fixed regression with plotting of bootstrapped errors in\n  perspective plots\n\n\\item \\texttt{npcdist()} no longer incorrectly calls \\texttt{npcdens()}\n\n\\item Fixed spacing between var name and p-value in significance test\n  summaries\n\n\\end{itemize}\n\n\\section*{Version 0.12-1 [19-Nov-2006]}\n\n\\begin{itemize}\n\n\\item Initial release of the np package on CRAN\n\n\\end{itemize}\n\n\n\\end{document}\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/build/vignette.rds",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/src/kernel.c",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/inst/doc/entropy_np.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/inst/doc/np_faq.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/inst/doc/np.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/data/Italy.rda",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/data/oecdpanel.rda",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/data/wage1.rda",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/data/Engel95.rda",
        "/tmp/vanessa/spack-stage/spack-stage-r-np-0.60-2-43ovjz7dcv6eb4vgvanmpzn6swtg3anv/spack-src/data/cps71.rda"
    ],
    "total_files": 151
}