{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/include/multiverso/net/mpi_net.h": "#ifndef MULTIVERSO_NET_MPI_NET_H_\n#define MULTIVERSO_NET_MPI_NET_H_\n\n#ifdef MULTIVERSO_USE_MPI\n\n#include \"multiverso/net.h\"\n\n#include <limits>\n#include <mutex>\n#include <queue>\n\n#include \"multiverso/message.h\"\n#include \"multiverso/dashboard.h\"\n#include \"multiverso/util/log.h\"\n#include \"multiverso/util/mt_queue.h\"\n\n#include <mpi.h>\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#endif\n\n\n#ifdef _MSC_VER\n#undef max\n#endif\n\nnamespace multiverso {\n\n#define MV_MPI_CALL(mpi_return) CHECK((mpi_return) == MPI_SUCCESS)\n\nnamespace {\n  static MPI_Datatype GetDataType(char*)   { return MPI_CHAR; }\n  static MPI_Datatype GetDataType(int*)    { return MPI_INT; }\n  static MPI_Datatype GetDataType(float*)  { return MPI_FLOAT; }\n  static MPI_Datatype GetDataType(double*) { return MPI_DOUBLE; }\n\n  static void dlopen_libmpi()\n  {\n  #ifndef _WIN32\n    void *handle = 0;\n    int mode = RTLD_NOW | RTLD_GLOBAL;\n  #if defined(__CYGWIN__)\n    /* TODO: Windows */\n  #elif defined(__APPLE__)\n    /* TODO: Mac OS X */\n  #elif defined(__linux__)\n    /* GNU/Linux and others */\n    #ifdef RTLD_NOLOAD\n    mode |= RTLD_NOLOAD;\n    #endif\n    if (!handle) handle = dlopen(\"libmpi_cxx.so\",   mode);\n  #endif\n  #endif\n}\n}\n\nclass MPINetWrapper : public NetInterface {\npublic:\n  MPINetWrapper() : /* more_(std::numeric_limits<char>::max()) */ \n   kover_(std::numeric_limits<size_t>::max()) {\n  }\n\n  class MPIMsgHandle {\n  public:\n    void add_handle(MPI_Request handle) {\n      handles_.push_back(handle);\n    }\n\n    void set_msg(MessagePtr& msg) { msg_ = std::move(msg); }\n    const MessagePtr& msg() const { return msg_; }\n    void set_size(size_t size) { size_ = size; }\n    size_t size() const { return size_; }\n\n    void Wait() {\n      // CHECK_NOTNULL(msg_.get());\n      int count = static_cast<int>(handles_.size());\n      MPI_Status* status = new MPI_Status[count];\n      MV_MPI_CALL(MPI_Waitall(count, handles_.data(), status));\n      delete[] status;\n    }\n\n    int Test() {\n      // CHECK_NOTNULL(msg_.get());\n      int count = static_cast<int>(handles_.size());\n      MPI_Status* status = new MPI_Status[count];\n      int flag;\n      MV_MPI_CALL(MPI_Testall(count, handles_.data(), &flag, status));\n      delete[] status;\n      return flag;\n    }\n  private:\n    std::vector<MPI_Request> handles_;\n    MessagePtr msg_;\n    size_t size_;\n  };\n\n  void Init(int* argc, char** argv) override {\n    // MPI_Init(argc, &argv);\n    MV_MPI_CALL(MPI_Initialized(&inited_));\n    if (!inited_) {\n      // NOTICE: Preload libmpi with the right mode. Otherwise python will load it in \n      // a private which will cause errors\n      dlopen_libmpi();\n      if (argc && *argc == 0) {\n        // When using multithread, giving MPI_Init_thread argv with zero length will cause errors.\n        MV_MPI_CALL(MPI_Init_thread(NULL, NULL, MPI_THREAD_SERIALIZED, &thread_provided_));\n      } else {\n        MV_MPI_CALL(MPI_Init_thread(argc, &argv, MPI_THREAD_SERIALIZED, &thread_provided_));\n      }\n      MV_MPI_CALL(MPI_Initialized(&inited_));\n    }\n    MV_MPI_CALL(MPI_Query_thread(&thread_provided_));\n    if (thread_provided_ < MPI_THREAD_SERIALIZED) {\n      Log::Fatal(\"At least MPI_THREAD_SERIALIZED supported is needed by multiverso.\\n\");\n    }\n    else if (thread_provided_ == MPI_THREAD_SERIALIZED) {\n      Log::Info(\"multiverso MPI-Net is initialized under MPI_THREAD_SERIALIZED mode.\\n\");\n    }\n    else if (thread_provided_ == MPI_THREAD_MULTIPLE) {\n      Log::Debug(\"multiverso MPI-Net is initialized under MPI_THREAD_MULTIPLE mode.\\n\");\n    }\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank_);\n    MPI_Comm_size(MPI_COMM_WORLD, &size_);\n    MPI_Barrier(MPI_COMM_WORLD);\n    Log::Debug(\"%s net util inited, rank = %d, size = %d\\n\",\n      name().c_str(), rank(), size());\n  }\n\n  void Finalize() override { inited_ = 0; MPI_Finalize(); }\n\n  int Bind(int, char*) override { \n    Log::Fatal(\"Shouldn't call this in MPI Net\\n\"); \n  return -1;\n  }\n\n  int Connect(int*, char* [], int) override { \n    Log::Fatal(\"Shouldn't call this in MPI Net\\n\"); \n  return -1;\n  }\n  \n  bool active() const { return inited_ != 0; }\n  int rank() const override { return rank_; }\n  int size() const override { return size_; }\n  std::string name() const override { return \"MPI\"; }\n\n  template <typename ElemType>\n  static void Allreduce(ElemType* data, size_t elem_count) {\n    MPI_Allreduce(MPI_IN_PLACE, data, (int)elem_count,\n      GetDataType(data), MPI_SUM, MPI_COMM_WORLD);\n  }\n\n  //size_t Send(MessagePtr& msg) override {\n  //  while (!msg_handles_.empty()) {\n  //    MPIMsgHandle* prev = msg_handles_.front();\n  //    if (prev->Test()) {\n  //      delete prev;\n  //      prev = nullptr;\n  //      msg_handles_.pop();\n  //    } else {\n  //      break;\n  //    }\n  //  }\n  //  MPIMsgHandle* handle = new MPIMsgHandle();\n  //  handle->set_msg(msg);\n  //  size_t size = SendAsync(handle->msg(), handle);\n  //  handle->set_size(size);\n  //  msg_handles_.push(handle);\n  //  return size;\n  //}\n\n  //size_t Send(MessagePtr& msg) override {\n  //  if (msg.get()) { send_queue_.Push(msg); }\n  //  \n  //  if (last_handle_.get() != nullptr && !last_handle_->Test()) {\n  //    // Last msg is still on the air\n  //    return 0;\n  //  }\n\n  //  // send over, free the last msg\n  //  last_handle_.reset();\n\n  //  // if there is more msg to send\n  //  if (send_queue_.Empty()) return 0;\n  //  \n  //  // Send a front msg of send queue\n  //  last_handle_.reset(new MPIMsgHandle()); \n  //  MessagePtr sending_msg;\n  //  CHECK(send_queue_.TryPop(sending_msg));\n  //  last_handle_->set_msg(sending_msg);\n  //  size_t size = SendAsync(last_handle_->msg(), last_handle_.get());\n  //  return size;\n  //}\n\n  int Send(MessagePtr& msg) override {\n    if (msg.get()) { send_queue_.Push(msg); }\n    \n    if (last_handle_.get() != nullptr && !last_handle_->Test()) {\n      // Last msg is still on the air\n      return 0;\n    }\n\n    // send over, free the last msg\n    last_handle_.reset();\n\n    // if there is more msg to send\n    if (send_queue_.Empty()) return 0;\n    \n    // Send a front msg of send queue\n    last_handle_.reset(new MPIMsgHandle()); \n    MessagePtr sending_msg;\n    CHECK(send_queue_.TryPop(sending_msg));\n\n    int size = SerializeAndSend(sending_msg, last_handle_.get());\n    return size;\n  }\n\n  //size_t Recv(MessagePtr* msg) override {\n  //  MPI_Status status;\n  //  int flag;\n  //  // non-blocking probe whether message comes\n  //  MV_MPI_CALL(MPI_Iprobe(MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &flag, &status));\n  //  int count;\n  //  MV_MPI_CALL(MPI_Get_count(&status, MPI_BYTE, &count));\n  //  if (!flag) return 0;\n  //  CHECK(count == Message::kHeaderSize);\n  //  return RecvMsgFrom(status.MPI_SOURCE, msg);\n  //}\n\n  int Recv(MessagePtr* msg) override {\n    MPI_Status status;\n    int flag;\n    // non-blocking probe whether message comes\n    MV_MPI_CALL(MPI_Iprobe(MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &flag, &status));\n    if (!flag) return 0;\n    int count;\n    MV_MPI_CALL(MPI_Get_count(&status, MPI_BYTE, &count));\n    if (count > recv_size_) {\n      recv_buffer_ = (char*)realloc(recv_buffer_, count);\n      recv_size_ = count;\n    }\n    // CHECK(count == Message::kHeaderSize);\n    return RecvAndDeserialize(status.MPI_SOURCE, count, msg);\n  }\n\n  void SendTo(int rank, char* buf, int len) const override {\n    if (len <= 0) {\n      return;\n    }\n    MPI_Request send_request;\n    MPI_Status status;\n    MV_MPI_CALL(MPI_Isend(buf, len, MPI_BYTE, rank, 0, \n                          MPI_COMM_WORLD, &send_request));\n    MV_MPI_CALL(MPI_Wait(&send_request, &status));\n  }\n\n  void RecvFrom(int rank, char* buf, int len) const override {\n    MPI_Status status;\n    int read_cnt = 0;\n    while (read_cnt < len) {\n      MV_MPI_CALL(MPI_Recv(buf + read_cnt, len - read_cnt, MPI_BYTE, \n                           rank, 0, MPI_COMM_WORLD, &status));\n      int cur_cnt;\n      MV_MPI_CALL(MPI_Get_count(&status, MPI_BYTE, &cur_cnt));\n      read_cnt += cur_cnt;\n    }\n  }\n\n  void SendRecv(int send_rank, char* send_data, int send_len,\n    int recv_rank, char* recv_data, int recv_len) const {\n    MPI_Request send_request;\n    // send first, non-blocking\n    MV_MPI_CALL(MPI_Isend(send_data, send_len, MPI_BYTE, send_rank, \n                          0, MPI_COMM_WORLD, &send_request));\n    // then receive, blocking\n    MPI_Status status;\n    int read_cnt = 0;\n    while (read_cnt < recv_len) {\n      MV_MPI_CALL(MPI_Recv(recv_data + read_cnt, recv_len - read_cnt, MPI_BYTE,\n                           recv_rank, 0, MPI_COMM_WORLD, &status));\n      int cur_cnt;\n      MV_MPI_CALL(MPI_Get_count(&status, MPI_BYTE, &cur_cnt));\n      read_cnt += cur_cnt;\n    }\n    // wait for send complete\n    MV_MPI_CALL(MPI_Wait(&send_request, &status));\n  }\n\n  int SerializeAndSend(MessagePtr& msg, MPIMsgHandle* msg_handle) {\n\n    CHECK_NOTNULL(msg_handle);\n    MONITOR_BEGIN(MPI_NET_SEND_SERIALIZE);\n    int size = sizeof(size_t) + Message::kHeaderSize;\n    for (auto& data : msg->data()) \n      size += static_cast<int>(sizeof(size_t) + data.size());\n    if (size > send_size_) {\n      send_buffer_ = (char*)realloc(send_buffer_, size);\n      send_size_ = size;\n    }\n    memcpy(send_buffer_, msg->header(), Message::kHeaderSize);\n    char* p = send_buffer_ + Message::kHeaderSize;\n    for (auto& data : msg->data()) {\n      size_t s = data.size();\n      memcpy(p, &s, sizeof(size_t));\n      p += sizeof(size_t);\n      memcpy(p, data.data(), s);\n      p += s;\n    }\n    size_t over = kover_; // std::numeric_limits<size_t>::max(); -1;\n    memcpy(p, &over, sizeof(size_t));\n    MONITOR_END(MPI_NET_SEND_SERIALIZE);\n\n    MPI_Request handle;\n    MV_MPI_CALL(MPI_Isend(send_buffer_, static_cast<int>(size), MPI_BYTE, msg->dst(), 0, MPI_COMM_WORLD, &handle));\n    msg_handle->add_handle(handle);\n    return size;\n  }\n\n  int RecvAndDeserialize(int src, int count, MessagePtr* msg_ptr) {\n    if (!msg_ptr->get()) msg_ptr->reset(new Message());\n    MessagePtr& msg = *msg_ptr;\n    msg->data().clear();\n    MPI_Status status;\n    MV_MPI_CALL(MPI_Recv(recv_buffer_, count,\n      MPI_BYTE, src, 0, MPI_COMM_WORLD, &status));\n\n    MONITOR_BEGIN(MPI_NET_RECV_DESERIALIZE)\n    char* p = recv_buffer_;\n    size_t s;\n    memcpy(msg->header(), p, Message::kHeaderSize);\n    p += Message::kHeaderSize;\n    memcpy(&s, p, sizeof(size_t));\n    p += sizeof(size_t);\n    while (s != kover_) {\n      Blob data(s);\n      memcpy(data.data(), p, data.size());\n      msg->Push(data);\n      p += data.size();\n      memcpy(&s, p, sizeof(size_t));\n      p += sizeof(size_t);\n    }\n    MONITOR_END(MPI_NET_RECV_DESERIALIZE)\n    return count;\n  }\n\n  int thread_level_support() override { \n    if (thread_provided_ == MPI_THREAD_MULTIPLE) \n      return NetThreadLevel::THREAD_MULTIPLE;\n    return NetThreadLevel::THREAD_SERIALIZED; \n  }\n\nprivate:\n  //size_t SendAsync(const MessagePtr& msg, \n  //                 MPIMsgHandle* msg_handle) {\n  //  CHECK_NOTNULL(msg_handle);\n  //  size_t size = Message::kHeaderSize;\n  //  MPI_Request handle;\n  //  CHECK_NOTNULL(msg->header());\n  //  MV_MPI_CALL(MPI_Isend(msg->header(), Message::kHeaderSize, MPI_BYTE,\n  //    msg->dst(), 0, MPI_COMM_WORLD, &handle));\n  //  msg_handle->add_handle(handle);\n  //  // Send multiple msg \n  //  for (auto& blob : msg->data()) {\n  //    CHECK_NOTNULL(blob.data());\n  //    MV_MPI_CALL(MPI_Isend(blob.data(), static_cast<int>(blob.size()),\n  //      MPI_BYTE, msg->dst(),\n  //      0, MPI_COMM_WORLD, &handle));\n  //    size += blob.size();\n  //    msg_handle->add_handle(handle);\n  //  }\n  //  // Send an extra over tag indicating the finish of this Message\n  //  MV_MPI_CALL(MPI_Isend(&more_, sizeof(char), MPI_BYTE, msg->dst(),\n  //    0, MPI_COMM_WORLD, &handle));\n  //  // Log::Debug(\"MPI-Net: rank %d send msg size = %d\\n\", rank(), size+4);\n  //  msg_handle->add_handle(handle);\n  //  return size + sizeof(char);\n  //}\n\n  //size_t RecvMsgFrom(int source, MessagePtr* msg_ptr) {\n  //  if (!msg_ptr->get()) msg_ptr->reset(new Message());\n  //  MessagePtr& msg = *msg_ptr;\n  //  msg->data().clear();\n  //  MPI_Status status;\n  //  CHECK_NOTNULL(msg->header());\n  //  MV_MPI_CALL(MPI_Recv(msg->header(), Message::kHeaderSize,\n  //    MPI_BYTE, source, 0, MPI_COMM_WORLD, &status));\n  //  size_t size = Message::kHeaderSize;\n  //  bool has_more = true;\n  //  while (has_more) {\n  //    int count;\n  //    MV_MPI_CALL(MPI_Probe(source, 0, MPI_COMM_WORLD, &status));\n  //    MV_MPI_CALL(MPI_Get_count(&status, MPI_BYTE, &count));\n  //    Blob blob(count);\n  //    // We only receive from msg->src() until we recv the overtag msg\n  //    MV_MPI_CALL(MPI_Recv(blob.data(), count, MPI_BYTE, source,\n  //      0, MPI_COMM_WORLD, &status));\n  //    size += count;\n  //    if (count == sizeof(char)) {\n  //      if (blob.As<char>() == more_) {\n  //        has_more = false; \n  //        break;\n  //      }\n  //      Log::Fatal(\"Unexpected msg format\\n\");\n  //    }\n  //    msg->Push(blob);\n  //  }\n  //  return size;\n  //}\n\nprivate:\n  // const char more_;\n  const size_t kover_;\n  std::mutex mutex_;\n  int thread_provided_;\n  int inited_;\n  int rank_;\n  int size_;\n  // std::queue<MPIMsgHandle *> msg_handles_;\n  std::unique_ptr<MPIMsgHandle> last_handle_;\n  MtQueue<MessagePtr> send_queue_;\n  char* send_buffer_;\n  long long send_size_;\n  char* recv_buffer_;\n  long long recv_size_;\n};\n\n}\n\n#endif // MULTIVERSO_USE_MPI\n\n#endif // MULTIVERSO_NET_MPI_NET_H_\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/Applications/WordEmbedding/example/imges/WS 353 google vs dmtk.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/Applications/WordEmbedding/example/imges/Analogical Reasoning google vs dmtk.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/C#/MultiversoCLR/multiverso.snk",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/python/docs/imgs/accuracy_time.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/python/docs/imgs/accuracy_epoch.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/lua/docs/imgs/top1error_vs_runningtime.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/lua/docs/imgs/top5error_vs_epoch.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/lua/docs/imgs/top1error_vs_epoch.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/binding/lua/docs/imgs/top5error_vs_runningtime.png",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/.git/objects/pack/pack-888a40fb24ab42a724e22b7243b4775a78174bdb.pack",
        "/tmp/vanessa/spack-stage/spack-stage-multiverso-master-tjcrkxvxouagib3gtfpcrzaccrwsntxd/spack-src/.git/objects/pack/pack-888a40fb24ab42a724e22b7243b4775a78174bdb.idx"
    ],
    "total_files": 255
}