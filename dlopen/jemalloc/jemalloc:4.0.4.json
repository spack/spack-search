{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-jemalloc-4.0.4-4naklqrlm2ridywfvjnrevi7trlrivuw/spack-src/src/jemalloc.c": "#define\tJEMALLOC_C_\n#include \"jemalloc/internal/jemalloc_internal.h\"\n\n/******************************************************************************/\n/* Data. */\n\n/* Runtime configuration options. */\nconst char\t*je_malloc_conf JEMALLOC_ATTR(weak);\nbool\topt_abort =\n#ifdef JEMALLOC_DEBUG\n    true\n#else\n    false\n#endif\n    ;\nconst char\t*opt_junk =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    \"true\"\n#else\n    \"false\"\n#endif\n    ;\nbool\topt_junk_alloc =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\nbool\topt_junk_free =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\n\nsize_t\topt_quarantine = ZU(0);\nbool\topt_redzone = false;\nbool\topt_utrace = false;\nbool\topt_xmalloc = false;\nbool\topt_zero = false;\nsize_t\topt_narenas = 0;\n\n/* Initialized to true if the process is running inside Valgrind. */\nbool\tin_valgrind;\n\nunsigned\tncpus;\n\n/* Protects arenas initialization (arenas, narenas_total). */\nstatic malloc_mutex_t\tarenas_lock;\n/*\n * Arenas that are used to service external requests.  Not all elements of the\n * arenas array are necessarily used; arenas are created lazily as needed.\n *\n * arenas[0..narenas_auto) are used for automatic multiplexing of threads and\n * arenas.  arenas[narenas_auto..narenas_total) are only used if the application\n * takes some action to create them and allocate from them.\n */\nstatic arena_t\t\t**arenas;\nstatic unsigned\t\tnarenas_total;\nstatic arena_t\t\t*a0; /* arenas[0]; read-only after initialization. */\nstatic unsigned\t\tnarenas_auto; /* Read-only after initialization. */\n\ntypedef enum {\n\tmalloc_init_uninitialized\t= 3,\n\tmalloc_init_a0_initialized\t= 2,\n\tmalloc_init_recursible\t\t= 1,\n\tmalloc_init_initialized\t\t= 0 /* Common case --> jnz. */\n} malloc_init_t;\nstatic malloc_init_t\tmalloc_init_state = malloc_init_uninitialized;\n\nJEMALLOC_ALIGNED(CACHELINE)\nconst size_t\tindex2size_tab[NSIZES] = {\n#define\tSC(index, lg_grp, lg_delta, ndelta, bin, lg_delta_lookup) \\\n\t((ZU(1)<<lg_grp) + (ZU(ndelta)<<lg_delta)),\n\tSIZE_CLASSES\n#undef SC\n};\n\nJEMALLOC_ALIGNED(CACHELINE)\nconst uint8_t\tsize2index_tab[] = {\n#if LG_TINY_MIN == 0\n#warning \"Dangerous LG_TINY_MIN\"\n#define\tS2B_0(i)\ti,\n#elif LG_TINY_MIN == 1\n#warning \"Dangerous LG_TINY_MIN\"\n#define\tS2B_1(i)\ti,\n#elif LG_TINY_MIN == 2\n#warning \"Dangerous LG_TINY_MIN\"\n#define\tS2B_2(i)\ti,\n#elif LG_TINY_MIN == 3\n#define\tS2B_3(i)\ti,\n#elif LG_TINY_MIN == 4\n#define\tS2B_4(i)\ti,\n#elif LG_TINY_MIN == 5\n#define\tS2B_5(i)\ti,\n#elif LG_TINY_MIN == 6\n#define\tS2B_6(i)\ti,\n#elif LG_TINY_MIN == 7\n#define\tS2B_7(i)\ti,\n#elif LG_TINY_MIN == 8\n#define\tS2B_8(i)\ti,\n#elif LG_TINY_MIN == 9\n#define\tS2B_9(i)\ti,\n#elif LG_TINY_MIN == 10\n#define\tS2B_10(i)\ti,\n#elif LG_TINY_MIN == 11\n#define\tS2B_11(i)\ti,\n#else\n#error \"Unsupported LG_TINY_MIN\"\n#endif\n#if LG_TINY_MIN < 1\n#define\tS2B_1(i)\tS2B_0(i) S2B_0(i)\n#endif\n#if LG_TINY_MIN < 2\n#define\tS2B_2(i)\tS2B_1(i) S2B_1(i)\n#endif\n#if LG_TINY_MIN < 3\n#define\tS2B_3(i)\tS2B_2(i) S2B_2(i)\n#endif\n#if LG_TINY_MIN < 4\n#define\tS2B_4(i)\tS2B_3(i) S2B_3(i)\n#endif\n#if LG_TINY_MIN < 5\n#define\tS2B_5(i)\tS2B_4(i) S2B_4(i)\n#endif\n#if LG_TINY_MIN < 6\n#define\tS2B_6(i)\tS2B_5(i) S2B_5(i)\n#endif\n#if LG_TINY_MIN < 7\n#define\tS2B_7(i)\tS2B_6(i) S2B_6(i)\n#endif\n#if LG_TINY_MIN < 8\n#define\tS2B_8(i)\tS2B_7(i) S2B_7(i)\n#endif\n#if LG_TINY_MIN < 9\n#define\tS2B_9(i)\tS2B_8(i) S2B_8(i)\n#endif\n#if LG_TINY_MIN < 10\n#define\tS2B_10(i)\tS2B_9(i) S2B_9(i)\n#endif\n#if LG_TINY_MIN < 11\n#define\tS2B_11(i)\tS2B_10(i) S2B_10(i)\n#endif\n#define\tS2B_no(i)\n#define\tSC(index, lg_grp, lg_delta, ndelta, bin, lg_delta_lookup) \\\n\tS2B_##lg_delta_lookup(index)\n\tSIZE_CLASSES\n#undef S2B_3\n#undef S2B_4\n#undef S2B_5\n#undef S2B_6\n#undef S2B_7\n#undef S2B_8\n#undef S2B_9\n#undef S2B_10\n#undef S2B_11\n#undef S2B_no\n#undef SC\n};\n\n#ifdef JEMALLOC_THREADED_INIT\n/* Used to let the initializing thread recursively allocate. */\n#  define NO_INITIALIZER\t((unsigned long)0)\n#  define INITIALIZER\t\tpthread_self()\n#  define IS_INITIALIZER\t(malloc_initializer == pthread_self())\nstatic pthread_t\t\tmalloc_initializer = NO_INITIALIZER;\n#else\n#  define NO_INITIALIZER\tfalse\n#  define INITIALIZER\t\ttrue\n#  define IS_INITIALIZER\tmalloc_initializer\nstatic bool\t\t\tmalloc_initializer = NO_INITIALIZER;\n#endif\n\n/* Used to avoid initialization races. */\n#ifdef _WIN32\n#if _WIN32_WINNT >= 0x0600\nstatic malloc_mutex_t\tinit_lock = SRWLOCK_INIT;\n#else\nstatic malloc_mutex_t\tinit_lock;\nstatic bool init_lock_initialized = false;\n\nJEMALLOC_ATTR(constructor)\nstatic void WINAPI\n_init_init_lock(void)\n{\n\n\t/* If another constructor in the same binary is using mallctl to\n\t * e.g. setup chunk hooks, it may end up running before this one,\n\t * and malloc_init_hard will crash trying to lock the uninitialized\n\t * lock. So we force an initialization of the lock in\n\t * malloc_init_hard as well. We don't try to care about atomicity\n\t * of the accessed to the init_lock_initialized boolean, since it\n\t * really only matters early in the process creation, before any\n\t * separate thread normally starts doing anything. */\n\tif (!init_lock_initialized)\n\t\tmalloc_mutex_init(&init_lock);\n\tinit_lock_initialized = true;\n}\n\n#ifdef _MSC_VER\n#  pragma section(\".CRT$XCU\", read)\nJEMALLOC_SECTION(\".CRT$XCU\") JEMALLOC_ATTR(used)\nstatic const void (WINAPI *init_init_lock)(void) = _init_init_lock;\n#endif\n#endif\n#else\nstatic malloc_mutex_t\tinit_lock = MALLOC_MUTEX_INITIALIZER;\n#endif\n\ntypedef struct {\n\tvoid\t*p;\t/* Input pointer (as in realloc(p, s)). */\n\tsize_t\ts;\t/* Request size. */\n\tvoid\t*r;\t/* Result pointer. */\n} malloc_utrace_t;\n\n#ifdef JEMALLOC_UTRACE\n#  define UTRACE(a, b, c) do {\t\t\t\t\t\t\\\n\tif (unlikely(opt_utrace)) {\t\t\t\t\t\\\n\t\tint utrace_serrno = errno;\t\t\t\t\\\n\t\tmalloc_utrace_t ut;\t\t\t\t\t\\\n\t\tut.p = (a);\t\t\t\t\t\t\\\n\t\tut.s = (b);\t\t\t\t\t\t\\\n\t\tut.r = (c);\t\t\t\t\t\t\\\n\t\tutrace(&ut, sizeof(ut));\t\t\t\t\\\n\t\terrno = utrace_serrno;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n#else\n#  define UTRACE(a, b, c)\n#endif\n\n/******************************************************************************/\n/*\n * Function prototypes for static functions that are referenced prior to\n * definition.\n */\n\nstatic bool\tmalloc_init_hard_a0(void);\nstatic bool\tmalloc_init_hard(void);\n\n/******************************************************************************/\n/*\n * Begin miscellaneous support functions.\n */\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_initialized(void)\n{\n\n\treturn (malloc_init_state == malloc_init_initialized);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void\nmalloc_thread_init(void)\n{\n\n\t/*\n\t * TSD initialization can't be safely done as a side effect of\n\t * deallocation, because it is possible for a thread to do nothing but\n\t * deallocate its TLS data via free(), in which case writing to TLS\n\t * would cause write-after-free memory corruption.  The quarantine\n\t * facility *only* gets used as a side effect of deallocation, so make\n\t * a best effort attempt at initializing its TSD by hooking all\n\t * allocation events.\n\t */\n\tif (config_fill && unlikely(opt_quarantine))\n\t\tquarantine_alloc_hook();\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_init_a0(void)\n{\n\n\tif (unlikely(malloc_init_state == malloc_init_uninitialized))\n\t\treturn (malloc_init_hard_a0());\n\treturn (false);\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_init(void)\n{\n\n\tif (unlikely(!malloc_initialized()) && malloc_init_hard())\n\t\treturn (true);\n\tmalloc_thread_init();\n\n\treturn (false);\n}\n\n/*\n * The a0*() functions are used instead of i[mcd]alloc() in situations that\n * cannot tolerate TLS variable access.\n */\n\narena_t *\na0get(void)\n{\n\n\tassert(a0 != NULL);\n\treturn (a0);\n}\n\nstatic void *\na0ialloc(size_t size, bool zero, bool is_metadata)\n{\n\n\tif (unlikely(malloc_init_a0()))\n\t\treturn (NULL);\n\n\treturn (iallocztm(NULL, size, zero, false, is_metadata, a0get()));\n}\n\nstatic void\na0idalloc(void *ptr, bool is_metadata)\n{\n\n\tidalloctm(NULL, ptr, false, is_metadata);\n}\n\nvoid *\na0malloc(size_t size)\n{\n\n\treturn (a0ialloc(size, false, true));\n}\n\nvoid\na0dalloc(void *ptr)\n{\n\n\ta0idalloc(ptr, true);\n}\n\n/*\n * FreeBSD's libc uses the bootstrap_*() functions in bootstrap-senstive\n * situations that cannot tolerate TLS variable access (TLS allocation and very\n * early internal data structure initialization).\n */\n\nvoid *\nbootstrap_malloc(size_t size)\n{\n\n\tif (unlikely(size == 0))\n\t\tsize = 1;\n\n\treturn (a0ialloc(size, false, false));\n}\n\nvoid *\nbootstrap_calloc(size_t num, size_t size)\n{\n\tsize_t num_size;\n\n\tnum_size = num * size;\n\tif (unlikely(num_size == 0)) {\n\t\tassert(num == 0 || size == 0);\n\t\tnum_size = 1;\n\t}\n\n\treturn (a0ialloc(num_size, true, false));\n}\n\nvoid\nbootstrap_free(void *ptr)\n{\n\n\tif (unlikely(ptr == NULL))\n\t\treturn;\n\n\ta0idalloc(ptr, false);\n}\n\n/* Create a new arena and insert it into the arenas array at index ind. */\nstatic arena_t *\narena_init_locked(unsigned ind)\n{\n\tarena_t *arena;\n\n\t/* Expand arenas if necessary. */\n\tassert(ind <= narenas_total);\n\tif (ind > MALLOCX_ARENA_MAX)\n\t\treturn (NULL);\n\tif (ind == narenas_total) {\n\t\tunsigned narenas_new = narenas_total + 1;\n\t\tarena_t **arenas_new =\n\t\t    (arena_t **)a0malloc(CACHELINE_CEILING(narenas_new *\n\t\t    sizeof(arena_t *)));\n\t\tif (arenas_new == NULL)\n\t\t\treturn (NULL);\n\t\tmemcpy(arenas_new, arenas, narenas_total * sizeof(arena_t *));\n\t\tarenas_new[ind] = NULL;\n\t\t/*\n\t\t * Deallocate only if arenas came from a0malloc() (not\n\t\t * base_alloc()).\n\t\t */\n\t\tif (narenas_total != narenas_auto)\n\t\t\ta0dalloc(arenas);\n\t\tarenas = arenas_new;\n\t\tnarenas_total = narenas_new;\n\t}\n\n\t/*\n\t * Another thread may have already initialized arenas[ind] if it's an\n\t * auto arena.\n\t */\n\tarena = arenas[ind];\n\tif (arena != NULL) {\n\t\tassert(ind < narenas_auto);\n\t\treturn (arena);\n\t}\n\n\t/* Actually initialize the arena. */\n\tarena = arenas[ind] = arena_new(ind);\n\treturn (arena);\n}\n\narena_t *\narena_init(unsigned ind)\n{\n\tarena_t *arena;\n\n\tmalloc_mutex_lock(&arenas_lock);\n\tarena = arena_init_locked(ind);\n\tmalloc_mutex_unlock(&arenas_lock);\n\treturn (arena);\n}\n\nunsigned\nnarenas_total_get(void)\n{\n\tunsigned narenas;\n\n\tmalloc_mutex_lock(&arenas_lock);\n\tnarenas = narenas_total;\n\tmalloc_mutex_unlock(&arenas_lock);\n\n\treturn (narenas);\n}\n\nstatic void\narena_bind_locked(tsd_t *tsd, unsigned ind)\n{\n\tarena_t *arena;\n\n\tarena = arenas[ind];\n\tarena->nthreads++;\n\n\tif (tsd_nominal(tsd))\n\t\ttsd_arena_set(tsd, arena);\n}\n\nstatic void\narena_bind(tsd_t *tsd, unsigned ind)\n{\n\n\tmalloc_mutex_lock(&arenas_lock);\n\tarena_bind_locked(tsd, ind);\n\tmalloc_mutex_unlock(&arenas_lock);\n}\n\nvoid\narena_migrate(tsd_t *tsd, unsigned oldind, unsigned newind)\n{\n\tarena_t *oldarena, *newarena;\n\n\tmalloc_mutex_lock(&arenas_lock);\n\toldarena = arenas[oldind];\n\tnewarena = arenas[newind];\n\toldarena->nthreads--;\n\tnewarena->nthreads++;\n\tmalloc_mutex_unlock(&arenas_lock);\n\ttsd_arena_set(tsd, newarena);\n}\n\nunsigned\narena_nbound(unsigned ind)\n{\n\tunsigned nthreads;\n\n\tmalloc_mutex_lock(&arenas_lock);\n\tnthreads = arenas[ind]->nthreads;\n\tmalloc_mutex_unlock(&arenas_lock);\n\treturn (nthreads);\n}\n\nstatic void\narena_unbind(tsd_t *tsd, unsigned ind)\n{\n\tarena_t *arena;\n\n\tmalloc_mutex_lock(&arenas_lock);\n\tarena = arenas[ind];\n\tarena->nthreads--;\n\tmalloc_mutex_unlock(&arenas_lock);\n\ttsd_arena_set(tsd, NULL);\n}\n\narena_t *\narena_get_hard(tsd_t *tsd, unsigned ind, bool init_if_missing)\n{\n\tarena_t *arena;\n\tarena_t **arenas_cache = tsd_arenas_cache_get(tsd);\n\tunsigned narenas_cache = tsd_narenas_cache_get(tsd);\n\tunsigned narenas_actual = narenas_total_get();\n\n\t/* Deallocate old cache if it's too small. */\n\tif (arenas_cache != NULL && narenas_cache < narenas_actual) {\n\t\ta0dalloc(arenas_cache);\n\t\tarenas_cache = NULL;\n\t\tnarenas_cache = 0;\n\t\ttsd_arenas_cache_set(tsd, arenas_cache);\n\t\ttsd_narenas_cache_set(tsd, narenas_cache);\n\t}\n\n\t/* Allocate cache if it's missing. */\n\tif (arenas_cache == NULL) {\n\t\tbool *arenas_cache_bypassp = tsd_arenas_cache_bypassp_get(tsd);\n\t\tassert(ind < narenas_actual || !init_if_missing);\n\t\tnarenas_cache = (ind < narenas_actual) ? narenas_actual : ind+1;\n\n\t\tif (tsd_nominal(tsd) && !*arenas_cache_bypassp) {\n\t\t\t*arenas_cache_bypassp = true;\n\t\t\tarenas_cache = (arena_t **)a0malloc(sizeof(arena_t *) *\n\t\t\t    narenas_cache);\n\t\t\t*arenas_cache_bypassp = false;\n\t\t}\n\t\tif (arenas_cache == NULL) {\n\t\t\t/*\n\t\t\t * This function must always tell the truth, even if\n\t\t\t * it's slow, so don't let OOM, thread cleanup (note\n\t\t\t * tsd_nominal check), nor recursive allocation\n\t\t\t * avoidance (note arenas_cache_bypass check) get in the\n\t\t\t * way.\n\t\t\t */\n\t\t\tif (ind >= narenas_actual)\n\t\t\t\treturn (NULL);\n\t\t\tmalloc_mutex_lock(&arenas_lock);\n\t\t\tarena = arenas[ind];\n\t\t\tmalloc_mutex_unlock(&arenas_lock);\n\t\t\treturn (arena);\n\t\t}\n\t\tassert(tsd_nominal(tsd) && !*arenas_cache_bypassp);\n\t\ttsd_arenas_cache_set(tsd, arenas_cache);\n\t\ttsd_narenas_cache_set(tsd, narenas_cache);\n\t}\n\n\t/*\n\t * Copy to cache.  It's possible that the actual number of arenas has\n\t * increased since narenas_total_get() was called above, but that causes\n\t * no correctness issues unless two threads concurrently execute the\n\t * arenas.extend mallctl, which we trust mallctl synchronization to\n\t * prevent.\n\t */\n\tmalloc_mutex_lock(&arenas_lock);\n\tmemcpy(arenas_cache, arenas, sizeof(arena_t *) * narenas_actual);\n\tmalloc_mutex_unlock(&arenas_lock);\n\tif (narenas_cache > narenas_actual) {\n\t\tmemset(&arenas_cache[narenas_actual], 0, sizeof(arena_t *) *\n\t\t    (narenas_cache - narenas_actual));\n\t}\n\n\t/* Read the refreshed cache, and init the arena if necessary. */\n\tarena = arenas_cache[ind];\n\tif (init_if_missing && arena == NULL)\n\t\tarena = arenas_cache[ind] = arena_init(ind);\n\treturn (arena);\n}\n\n/* Slow path, called only by arena_choose(). */\narena_t *\narena_choose_hard(tsd_t *tsd)\n{\n\tarena_t *ret;\n\n\tif (narenas_auto > 1) {\n\t\tunsigned i, choose, first_null;\n\n\t\tchoose = 0;\n\t\tfirst_null = narenas_auto;\n\t\tmalloc_mutex_lock(&arenas_lock);\n\t\tassert(a0get() != NULL);\n\t\tfor (i = 1; i < narenas_auto; i++) {\n\t\t\tif (arenas[i] != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Choose the first arena that has the lowest\n\t\t\t\t * number of threads assigned to it.\n\t\t\t\t */\n\t\t\t\tif (arenas[i]->nthreads <\n\t\t\t\t    arenas[choose]->nthreads)\n\t\t\t\t\tchoose = i;\n\t\t\t} else if (first_null == narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Record the index of the first uninitialized\n\t\t\t\t * arena, in case all extant arenas are in use.\n\t\t\t\t *\n\t\t\t\t * NB: It is possible for there to be\n\t\t\t\t * discontinuities in terms of initialized\n\t\t\t\t * versus uninitialized arenas, due to the\n\t\t\t\t * \"thread.arena\" mallctl.\n\t\t\t\t */\n\t\t\t\tfirst_null = i;\n\t\t\t}\n\t\t}\n\n\t\tif (arenas[choose]->nthreads == 0\n\t\t    || first_null == narenas_auto) {\n\t\t\t/*\n\t\t\t * Use an unloaded arena, or the least loaded arena if\n\t\t\t * all arenas are already initialized.\n\t\t\t */\n\t\t\tret = arenas[choose];\n\t\t} else {\n\t\t\t/* Initialize a new arena. */\n\t\t\tchoose = first_null;\n\t\t\tret = arena_init_locked(choose);\n\t\t\tif (ret == NULL) {\n\t\t\t\tmalloc_mutex_unlock(&arenas_lock);\n\t\t\t\treturn (NULL);\n\t\t\t}\n\t\t}\n\t\tarena_bind_locked(tsd, choose);\n\t\tmalloc_mutex_unlock(&arenas_lock);\n\t} else {\n\t\tret = a0get();\n\t\tarena_bind(tsd, 0);\n\t}\n\n\treturn (ret);\n}\n\nvoid\nthread_allocated_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nvoid\nthread_deallocated_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nvoid\narena_cleanup(tsd_t *tsd)\n{\n\tarena_t *arena;\n\n\tarena = tsd_arena_get(tsd);\n\tif (arena != NULL)\n\t\tarena_unbind(tsd, arena->ind);\n}\n\nvoid\narenas_cache_cleanup(tsd_t *tsd)\n{\n\tarena_t **arenas_cache;\n\n\tarenas_cache = tsd_arenas_cache_get(tsd);\n\tif (arenas_cache != NULL) {\n\t\ttsd_arenas_cache_set(tsd, NULL);\n\t\ta0dalloc(arenas_cache);\n\t}\n}\n\nvoid\nnarenas_cache_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nvoid\narenas_cache_bypass_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nstatic void\nstats_print_atexit(void)\n{\n\n\tif (config_tcache && config_stats) {\n\t\tunsigned narenas, i;\n\n\t\t/*\n\t\t * Merge stats from extant threads.  This is racy, since\n\t\t * individual threads do not lock when recording tcache stats\n\t\t * events.  As a consequence, the final stats may be slightly\n\t\t * out of date by the time they are reported, if other threads\n\t\t * continue to allocate.\n\t\t */\n\t\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\t\tarena_t *arena = arenas[i];\n\t\t\tif (arena != NULL) {\n\t\t\t\ttcache_t *tcache;\n\n\t\t\t\t/*\n\t\t\t\t * tcache_stats_merge() locks bins, so if any\n\t\t\t\t * code is introduced that acquires both arena\n\t\t\t\t * and bin locks in the opposite order,\n\t\t\t\t * deadlocks may result.\n\t\t\t\t */\n\t\t\t\tmalloc_mutex_lock(&arena->lock);\n\t\t\t\tql_foreach(tcache, &arena->tcache_ql, link) {\n\t\t\t\t\ttcache_stats_merge(tcache, arena);\n\t\t\t\t}\n\t\t\t\tmalloc_mutex_unlock(&arena->lock);\n\t\t\t}\n\t\t}\n\t}\n\tje_malloc_stats_print(NULL, NULL, NULL);\n}\n\n/*\n * End miscellaneous support functions.\n */\n/******************************************************************************/\n/*\n * Begin initialization functions.\n */\n\n#ifndef JEMALLOC_HAVE_SECURE_GETENV\nstatic char *\nsecure_getenv(const char *name)\n{\n\n#  ifdef JEMALLOC_HAVE_ISSETUGID\n\tif (issetugid() != 0)\n\t\treturn (NULL);\n#  endif\n\treturn (getenv(name));\n}\n#endif\n\nstatic unsigned\nmalloc_ncpus(void)\n{\n\tlong result;\n\n#ifdef _WIN32\n\tSYSTEM_INFO si;\n\tGetSystemInfo(&si);\n\tresult = si.dwNumberOfProcessors;\n#else\n\tresult = sysconf(_SC_NPROCESSORS_ONLN);\n#endif\n\treturn ((result == -1) ? 1 : (unsigned)result);\n}\n\nstatic bool\nmalloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,\n    char const **v_p, size_t *vlen_p)\n{\n\tbool accept;\n\tconst char *opts = *opts_p;\n\n\t*k_p = opts;\n\n\tfor (accept = false; !accept;) {\n\t\tswitch (*opts) {\n\t\tcase 'A': case 'B': case 'C': case 'D': case 'E': case 'F':\n\t\tcase 'G': case 'H': case 'I': case 'J': case 'K': case 'L':\n\t\tcase 'M': case 'N': case 'O': case 'P': case 'Q': case 'R':\n\t\tcase 'S': case 'T': case 'U': case 'V': case 'W': case 'X':\n\t\tcase 'Y': case 'Z':\n\t\tcase 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n\t\tcase 'g': case 'h': case 'i': case 'j': case 'k': case 'l':\n\t\tcase 'm': case 'n': case 'o': case 'p': case 'q': case 'r':\n\t\tcase 's': case 't': case 'u': case 'v': case 'w': case 'x':\n\t\tcase 'y': case 'z':\n\t\tcase '0': case '1': case '2': case '3': case '4': case '5':\n\t\tcase '6': case '7': case '8': case '9':\n\t\tcase '_':\n\t\t\topts++;\n\t\t\tbreak;\n\t\tcase ':':\n\t\t\topts++;\n\t\t\t*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;\n\t\t\t*v_p = opts;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\tif (opts != *opts_p) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with key\\n\");\n\t\t\t}\n\t\t\treturn (true);\n\t\tdefault:\n\t\t\tmalloc_write(\"<jemalloc>: Malformed conf string\\n\");\n\t\t\treturn (true);\n\t\t}\n\t}\n\n\tfor (accept = false; !accept;) {\n\t\tswitch (*opts) {\n\t\tcase ',':\n\t\t\topts++;\n\t\t\t/*\n\t\t\t * Look ahead one character here, because the next time\n\t\t\t * this function is called, it will assume that end of\n\t\t\t * input has been cleanly reached if no input remains,\n\t\t\t * but we have optimistically already consumed the\n\t\t\t * comma if one exists.\n\t\t\t */\n\t\t\tif (*opts == '\\0') {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with comma\\n\");\n\t\t\t}\n\t\t\t*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\t*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\topts++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t*opts_p = opts;\n\treturn (false);\n}\n\nstatic void\nmalloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,\n    size_t vlen)\n{\n\n\tmalloc_printf(\"<jemalloc>: %s: %.*s:%.*s\\n\", msg, (int)klen, k,\n\t    (int)vlen, v);\n}\n\nstatic void\nmalloc_conf_init(void)\n{\n\tunsigned i;\n\tchar buf[PATH_MAX + 1];\n\tconst char *opts, *k, *v;\n\tsize_t klen, vlen;\n\n\t/*\n\t * Automatically configure valgrind before processing options.  The\n\t * valgrind option remains in jemalloc 3.x for compatibility reasons.\n\t */\n\tif (config_valgrind) {\n\t\tin_valgrind = (RUNNING_ON_VALGRIND != 0) ? true : false;\n\t\tif (config_fill && unlikely(in_valgrind)) {\n\t\t\topt_junk = \"false\";\n\t\t\topt_junk_alloc = false;\n\t\t\topt_junk_free = false;\n\t\t\tassert(!opt_zero);\n\t\t\topt_quarantine = JEMALLOC_VALGRIND_QUARANTINE_DEFAULT;\n\t\t\topt_redzone = true;\n\t\t}\n\t\tif (config_tcache && unlikely(in_valgrind))\n\t\t\topt_tcache = false;\n\t}\n\n\tfor (i = 0; i < 3; i++) {\n\t\t/* Get runtime configuration. */\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tif (je_malloc_conf != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Use options that were compiled into the\n\t\t\t\t * program.\n\t\t\t\t */\n\t\t\t\topts = je_malloc_conf;\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 1: {\n\t\t\tint linklen = 0;\n#ifndef _WIN32\n\t\t\tint saved_errno = errno;\n\t\t\tconst char *linkname =\n#  ifdef JEMALLOC_PREFIX\n\t\t\t    \"/etc/\"JEMALLOC_PREFIX\"malloc.conf\"\n#  else\n\t\t\t    \"/etc/malloc.conf\"\n#  endif\n\t\t\t    ;\n\n\t\t\t/*\n\t\t\t * Try to use the contents of the \"/etc/malloc.conf\"\n\t\t\t * symbolic link's name.\n\t\t\t */\n\t\t\tlinklen = readlink(linkname, buf, sizeof(buf) - 1);\n\t\t\tif (linklen == -1) {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tlinklen = 0;\n\t\t\t\t/* Restore errno. */\n\t\t\t\tset_errno(saved_errno);\n\t\t\t}\n#endif\n\t\t\tbuf[linklen] = '\\0';\n\t\t\topts = buf;\n\t\t\tbreak;\n\t\t} case 2: {\n\t\t\tconst char *envname =\n#ifdef JEMALLOC_PREFIX\n\t\t\t    JEMALLOC_CPREFIX\"MALLOC_CONF\"\n#else\n\t\t\t    \"MALLOC_CONF\"\n#endif\n\t\t\t    ;\n\n\t\t\tif ((opts = secure_getenv(envname)) != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Do nothing; opts is already initialized to\n\t\t\t\t * the value of the MALLOC_CONF environment\n\t\t\t\t * variable.\n\t\t\t\t */\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\t} default:\n\t\t\tnot_reached();\n\t\t\tbuf[0] = '\\0';\n\t\t\topts = buf;\n\t\t}\n\n\t\twhile (*opts != '\\0' && !malloc_conf_next(&opts, &k, &klen, &v,\n\t\t    &vlen)) {\n#define\tCONF_MATCH(n)\t\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == klen && strncmp(n, k, klen) == 0)\n#define\tCONF_MATCH_VALUE(n)\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == vlen && strncmp(n, v, vlen) == 0)\n#define\tCONF_HANDLE_BOOL(o, n, cont)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tif (CONF_MATCH_VALUE(\"true\"))\t\t\\\n\t\t\t\t\to = true;\t\t\t\\\n\t\t\t\telse if (CONF_MATCH_VALUE(\"false\"))\t\\\n\t\t\t\t\to = false;\t\t\t\\\n\t\t\t\telse {\t\t\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tif (cont)\t\t\t\t\\\n\t\t\t\t\tcontinue;\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_SIZE_T(o, n, min, max, clip)\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tuintmax_t um;\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tum = malloc_strtoumax(v, &end, 0);\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (clip) {\t\t\t\\\n\t\t\t\t\tif ((min) != 0 && um < (min))\t\\\n\t\t\t\t\t\to = (min);\t\t\\\n\t\t\t\t\telse if (um > (max))\t\t\\\n\t\t\t\t\t\to = (max);\t\t\\\n\t\t\t\t\telse\t\t\t\t\\\n\t\t\t\t\t\to = um;\t\t\t\\\n\t\t\t\t} else {\t\t\t\t\\\n\t\t\t\t\tif (((min) != 0 && um < (min))\t\\\n\t\t\t\t\t    || um > (max)) {\t\t\\\n\t\t\t\t\t\tmalloc_conf_error(\t\\\n\t\t\t\t\t\t    \"Out-of-range \"\t\\\n\t\t\t\t\t\t    \"conf value\",\t\\\n\t\t\t\t\t\t    k, klen, v, vlen);\t\\\n\t\t\t\t\t} else\t\t\t\t\\\n\t\t\t\t\t\to = um;\t\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_SSIZE_T(o, n, min, max)\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tlong l;\t\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tl = strtol(v, &end, 0);\t\t\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (l < (ssize_t)(min) || l >\t\\\n\t\t\t\t    (ssize_t)(max)) {\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Out-of-range conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else\t\t\t\t\t\\\n\t\t\t\t\to = l;\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_CHAR_P(o, n, d)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tsize_t cpylen = (vlen <=\t\t\\\n\t\t\t\t    sizeof(o)-1) ? vlen :\t\t\\\n\t\t\t\t    sizeof(o)-1;\t\t\t\\\n\t\t\t\tstrncpy(o, v, cpylen);\t\t\t\\\n\t\t\t\to[cpylen] = '\\0';\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n\n\t\t\tCONF_HANDLE_BOOL(opt_abort, \"abort\", true)\n\t\t\t/*\n\t\t\t * Chunks always require at least one header page,\n\t\t\t * as many as 2^(LG_SIZE_CLASS_GROUP+1) data pages, and\n\t\t\t * possibly an additional page in the presence of\n\t\t\t * redzones.  In order to simplify options processing,\n\t\t\t * use a conservative bound that accommodates all these\n\t\t\t * constraints.\n\t\t\t */\n\t\t\tCONF_HANDLE_SIZE_T(opt_lg_chunk, \"lg_chunk\", LG_PAGE +\n\t\t\t    LG_SIZE_CLASS_GROUP + (config_fill ? 2 : 1),\n\t\t\t    (sizeof(size_t) << 3) - 1, true)\n\t\t\tif (strncmp(\"dss\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < dss_prec_limit; i++) {\n\t\t\t\t\tif (strncmp(dss_prec_names[i], v, vlen)\n\t\t\t\t\t    == 0) {\n\t\t\t\t\t\tif (chunk_dss_prec_set(i)) {\n\t\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\t    \"Error setting dss\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_dss =\n\t\t\t\t\t\t\t    dss_prec_names[i];\n\t\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tmalloc_conf_error(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tCONF_HANDLE_SIZE_T(opt_narenas, \"narenas\", 1,\n\t\t\t    SIZE_T_MAX, false)\n\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_dirty_mult, \"lg_dirty_mult\",\n\t\t\t    -1, (sizeof(size_t) << 3) - 1)\n\t\t\tCONF_HANDLE_BOOL(opt_stats_print, \"stats_print\", true)\n\t\t\tif (config_fill) {\n\t\t\t\tif (CONF_MATCH(\"junk\")) {\n\t\t\t\t\tif (CONF_MATCH_VALUE(\"true\")) {\n\t\t\t\t\t\topt_junk = \"true\";\n\t\t\t\t\t\topt_junk_alloc = opt_junk_free =\n\t\t\t\t\t\t    true;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"false\")) {\n\t\t\t\t\t\topt_junk = \"false\";\n\t\t\t\t\t\topt_junk_alloc = opt_junk_free =\n\t\t\t\t\t\t    false;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"alloc\")) {\n\t\t\t\t\t\topt_junk = \"alloc\";\n\t\t\t\t\t\topt_junk_alloc = true;\n\t\t\t\t\t\topt_junk_free = false;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"free\")) {\n\t\t\t\t\t\topt_junk = \"free\";\n\t\t\t\t\t\topt_junk_alloc = false;\n\t\t\t\t\t\topt_junk_free = true;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t    \"Invalid conf value\", k,\n\t\t\t\t\t\t    klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_quarantine, \"quarantine\",\n\t\t\t\t    0, SIZE_T_MAX, false)\n\t\t\t\tCONF_HANDLE_BOOL(opt_redzone, \"redzone\", true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_zero, \"zero\", true)\n\t\t\t}\n\t\t\tif (config_utrace) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_utrace, \"utrace\", true)\n\t\t\t}\n\t\t\tif (config_xmalloc) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_xmalloc, \"xmalloc\", true)\n\t\t\t}\n\t\t\tif (config_tcache) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_tcache, \"tcache\",\n\t\t\t\t    !config_valgrind || !in_valgrind)\n\t\t\t\tif (CONF_MATCH(\"tcache\")) {\n\t\t\t\t\tassert(config_valgrind && in_valgrind);\n\t\t\t\t\tif (opt_tcache) {\n\t\t\t\t\t\topt_tcache = false;\n\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\"tcache cannot be enabled \"\n\t\t\t\t\t\t\"while running inside Valgrind\",\n\t\t\t\t\t\tk, klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_tcache_max,\n\t\t\t\t    \"lg_tcache_max\", -1,\n\t\t\t\t    (sizeof(size_t) << 3) - 1)\n\t\t\t}\n\t\t\tif (config_prof) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof, \"prof\", true)\n\t\t\t\tCONF_HANDLE_CHAR_P(opt_prof_prefix,\n\t\t\t\t    \"prof_prefix\", \"jeprof\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_active, \"prof_active\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_thread_active_init,\n\t\t\t\t    \"prof_thread_active_init\", true)\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_lg_prof_sample,\n\t\t\t\t    \"lg_prof_sample\", 0,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1, true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_accum, \"prof_accum\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_interval,\n\t\t\t\t    \"lg_prof_interval\", -1,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_gdump, \"prof_gdump\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_final, \"prof_final\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_leak, \"prof_leak\",\n\t\t\t\t    true)\n\t\t\t}\n\t\t\tmalloc_conf_error(\"Invalid conf pair\", k, klen, v,\n\t\t\t    vlen);\n#undef CONF_MATCH\n#undef CONF_HANDLE_BOOL\n#undef CONF_HANDLE_SIZE_T\n#undef CONF_HANDLE_SSIZE_T\n#undef CONF_HANDLE_CHAR_P\n\t\t}\n\t}\n}\n\n/* init_lock must be held. */\nstatic bool\nmalloc_init_hard_needed(void)\n{\n\n\tif (malloc_initialized() || (IS_INITIALIZER && malloc_init_state ==\n\t    malloc_init_recursible)) {\n\t\t/*\n\t\t * Another thread initialized the allocator before this one\n\t\t * acquired init_lock, or this thread is the initializing\n\t\t * thread, and it is recursively allocating.\n\t\t */\n\t\treturn (false);\n\t}\n#ifdef JEMALLOC_THREADED_INIT\n\tif (malloc_initializer != NO_INITIALIZER && !IS_INITIALIZER) {\n\t\t/* Busy-wait until the initializing thread completes. */\n\t\tdo {\n\t\t\tmalloc_mutex_unlock(&init_lock);\n\t\t\tCPU_SPINWAIT;\n\t\t\tmalloc_mutex_lock(&init_lock);\n\t\t} while (!malloc_initialized());\n\t\treturn (false);\n\t}\n#endif\n\treturn (true);\n}\n\n/* init_lock must be held. */\nstatic bool\nmalloc_init_hard_a0_locked(void)\n{\n\n\tmalloc_initializer = INITIALIZER;\n\n\tif (config_prof)\n\t\tprof_boot0();\n\tmalloc_conf_init();\n\tif (opt_stats_print) {\n\t\t/* Print statistics at exit. */\n\t\tif (atexit(stats_print_atexit) != 0) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in atexit()\\n\");\n\t\t\tif (opt_abort)\n\t\t\t\tabort();\n\t\t}\n\t}\n\tif (base_boot())\n\t\treturn (true);\n\tif (chunk_boot())\n\t\treturn (true);\n\tif (ctl_boot())\n\t\treturn (true);\n\tif (config_prof)\n\t\tprof_boot1();\n\tif (arena_boot())\n\t\treturn (true);\n\tif (config_tcache && tcache_boot())\n\t\treturn (true);\n\tif (malloc_mutex_init(&arenas_lock))\n\t\treturn (true);\n\t/*\n\t * Create enough scaffolding to allow recursive allocation in\n\t * malloc_ncpus().\n\t */\n\tnarenas_total = narenas_auto = 1;\n\tarenas = &a0;\n\tmemset(arenas, 0, sizeof(arena_t *) * narenas_auto);\n\t/*\n\t * Initialize one arena here.  The rest are lazily created in\n\t * arena_choose_hard().\n\t */\n\tif (arena_init(0) == NULL)\n\t\treturn (true);\n\tmalloc_init_state = malloc_init_a0_initialized;\n\treturn (false);\n}\n\nstatic bool\nmalloc_init_hard_a0(void)\n{\n\tbool ret;\n\n\tmalloc_mutex_lock(&init_lock);\n\tret = malloc_init_hard_a0_locked();\n\tmalloc_mutex_unlock(&init_lock);\n\treturn (ret);\n}\n\n/*\n * Initialize data structures which may trigger recursive allocation.\n *\n * init_lock must be held.\n */\nstatic void\nmalloc_init_hard_recursible(void)\n{\n\n\tmalloc_init_state = malloc_init_recursible;\n\tmalloc_mutex_unlock(&init_lock);\n\n\tncpus = malloc_ncpus();\n\n#if (!defined(JEMALLOC_MUTEX_INIT_CB) && !defined(JEMALLOC_ZONE) \\\n    && !defined(_WIN32) && !defined(__native_client__))\n\t/* LinuxThreads's pthread_atfork() allocates. */\n\tif (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,\n\t    jemalloc_postfork_child) != 0) {\n\t\tmalloc_write(\"<jemalloc>: Error in pthread_atfork()\\n\");\n\t\tif (opt_abort)\n\t\t\tabort();\n\t}\n#endif\n\tmalloc_mutex_lock(&init_lock);\n}\n\n/* init_lock must be held. */\nstatic bool\nmalloc_init_hard_finish(void)\n{\n\n\tif (mutex_boot())\n\t\treturn (true);\n\n\tif (opt_narenas == 0) {\n\t\t/*\n\t\t * For SMP systems, create more than one arena per CPU by\n\t\t * default.\n\t\t */\n\t\tif (ncpus > 1)\n\t\t\topt_narenas = ncpus << 2;\n\t\telse\n\t\t\topt_narenas = 1;\n\t}\n\tnarenas_auto = opt_narenas;\n\t/*\n\t * Make sure that the arenas array can be allocated.  In practice, this\n\t * limit is enough to allow the allocator to function, but the ctl\n\t * machinery will fail to allocate memory at far lower limits.\n\t */\n\tif (narenas_auto > chunksize / sizeof(arena_t *)) {\n\t\tnarenas_auto = chunksize / sizeof(arena_t *);\n\t\tmalloc_printf(\"<jemalloc>: Reducing narenas to limit (%d)\\n\",\n\t\t    narenas_auto);\n\t}\n\tnarenas_total = narenas_auto;\n\n\t/* Allocate and initialize arenas. */\n\tarenas = (arena_t **)base_alloc(sizeof(arena_t *) * narenas_total);\n\tif (arenas == NULL)\n\t\treturn (true);\n\t/*\n\t * Zero the array.  In practice, this should always be pre-zeroed,\n\t * since it was just mmap()ed, but let's be sure.\n\t */\n\tmemset(arenas, 0, sizeof(arena_t *) * narenas_total);\n\t/* Copy the pointer to the one arena that was already initialized. */\n\tarenas[0] = a0;\n\n\tmalloc_init_state = malloc_init_initialized;\n\treturn (false);\n}\n\nstatic bool\nmalloc_init_hard(void)\n{\n\n#if defined(_WIN32) && _WIN32_WINNT < 0x0600\n\t_init_init_lock();\n#endif\n\tmalloc_mutex_lock(&init_lock);\n\tif (!malloc_init_hard_needed()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (false);\n\t}\n\n\tif (malloc_init_state != malloc_init_a0_initialized &&\n\t    malloc_init_hard_a0_locked()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\tif (malloc_tsd_boot0()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\tif (config_prof && prof_boot2()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tmalloc_init_hard_recursible();\n\n\tif (malloc_init_hard_finish()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tmalloc_mutex_unlock(&init_lock);\n\tmalloc_tsd_boot1();\n\treturn (false);\n}\n\n/*\n * End initialization functions.\n */\n/******************************************************************************/\n/*\n * Begin malloc(3)-compatible functions.\n */\n\nstatic void *\nimalloc_prof_sample(tsd_t *tsd, size_t usize, prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = imalloc(tsd, LARGE_MINCLASS);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = imalloc(tsd, usize);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimalloc_prof(tsd_t *tsd, size_t usize)\n{\n\tvoid *p;\n\tprof_tctx_t *tctx;\n\n\ttctx = prof_alloc_prep(tsd, usize, prof_active_get_unlocked(), true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = imalloc_prof_sample(tsd, usize, tctx);\n\telse\n\t\tp = imalloc(tsd, usize);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(p, usize, tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimalloc_body(size_t size, tsd_t **tsd, size_t *usize)\n{\n\n\tif (unlikely(malloc_init()))\n\t\treturn (NULL);\n\t*tsd = tsd_fetch();\n\n\tif (config_prof && opt_prof) {\n\t\t*usize = s2u(size);\n\t\tif (unlikely(*usize == 0))\n\t\t\treturn (NULL);\n\t\treturn (imalloc_prof(*tsd, *usize));\n\t}\n\n\tif (config_stats || (config_valgrind && unlikely(in_valgrind)))\n\t\t*usize = s2u(size);\n\treturn (imalloc(*tsd, size));\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)\nje_malloc(size_t size)\n{\n\tvoid *ret;\n\ttsd_t *tsd;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tret = imalloc_body(size, &tsd, &usize);\n\tif (unlikely(ret == NULL)) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in malloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && likely(ret != NULL)) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t}\n\tUTRACE(0, size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, false);\n\treturn (ret);\n}\n\nstatic void *\nimemalign_prof_sample(tsd_t *tsd, size_t alignment, size_t usize,\n    prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tassert(sa2u(LARGE_MINCLASS, alignment) == LARGE_MINCLASS);\n\t\tp = ipalloc(tsd, LARGE_MINCLASS, alignment, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = ipalloc(tsd, usize, alignment, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimemalign_prof(tsd_t *tsd, size_t alignment, size_t usize)\n{\n\tvoid *p;\n\tprof_tctx_t *tctx;\n\n\ttctx = prof_alloc_prep(tsd, usize, prof_active_get_unlocked(), true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = imemalign_prof_sample(tsd, alignment, usize, tctx);\n\telse\n\t\tp = ipalloc(tsd, usize, alignment, false);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(p, usize, tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_ATTR(nonnull(1))\nstatic int\nimemalign(void **memptr, size_t alignment, size_t size, size_t min_alignment)\n{\n\tint ret;\n\ttsd_t *tsd;\n\tsize_t usize;\n\tvoid *result;\n\n\tassert(min_alignment != 0);\n\n\tif (unlikely(malloc_init())) {\n\t\tresult = NULL;\n\t\tgoto label_oom;\n\t}\n\ttsd = tsd_fetch();\n\tif (size == 0)\n\t\tsize = 1;\n\n\t/* Make sure that alignment is a large enough power of 2. */\n\tif (unlikely(((alignment - 1) & alignment) != 0\n\t    || (alignment < min_alignment))) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error allocating \"\n\t\t\t    \"aligned memory: invalid alignment\\n\");\n\t\t\tabort();\n\t\t}\n\t\tresult = NULL;\n\t\tret = EINVAL;\n\t\tgoto label_return;\n\t}\n\n\tusize = sa2u(size, alignment);\n\tif (unlikely(usize == 0)) {\n\t\tresult = NULL;\n\t\tgoto label_oom;\n\t}\n\n\tif (config_prof && opt_prof)\n\t\tresult = imemalign_prof(tsd, alignment, usize);\n\telse\n\t\tresult = ipalloc(tsd, usize, alignment, false);\n\tif (unlikely(result == NULL))\n\t\tgoto label_oom;\n\tassert(((uintptr_t)result & (alignment - 1)) == ZU(0));\n\n\t*memptr = result;\n\tret = 0;\nlabel_return:\n\tif (config_stats && likely(result != NULL)) {\n\t\tassert(usize == isalloc(result, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t}\n\tUTRACE(0, size, result);\n\treturn (ret);\nlabel_oom:\n\tassert(result == NULL);\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(\"<jemalloc>: Error allocating aligned memory: \"\n\t\t    \"out of memory\\n\");\n\t\tabort();\n\t}\n\tret = ENOMEM;\n\tgoto label_return;\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nJEMALLOC_ATTR(nonnull(1))\nje_posix_memalign(void **memptr, size_t alignment, size_t size)\n{\n\tint ret = imemalign(memptr, alignment, size, sizeof(void *));\n\tJEMALLOC_VALGRIND_MALLOC(ret == 0, *memptr, isalloc(*memptr,\n\t    config_prof), false);\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(2)\nje_aligned_alloc(size_t alignment, size_t size)\n{\n\tvoid *ret;\n\tint err;\n\n\tif (unlikely((err = imemalign(&ret, alignment, size, 1)) != 0)) {\n\t\tret = NULL;\n\t\tset_errno(err);\n\t}\n\tJEMALLOC_VALGRIND_MALLOC(err == 0, ret, isalloc(ret, config_prof),\n\t    false);\n\treturn (ret);\n}\n\nstatic void *\nicalloc_prof_sample(tsd_t *tsd, size_t usize, prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = icalloc(tsd, LARGE_MINCLASS);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = icalloc(tsd, usize);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nicalloc_prof(tsd_t *tsd, size_t usize)\n{\n\tvoid *p;\n\tprof_tctx_t *tctx;\n\n\ttctx = prof_alloc_prep(tsd, usize, prof_active_get_unlocked(), true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = icalloc_prof_sample(tsd, usize, tctx);\n\telse\n\t\tp = icalloc(tsd, usize);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(p, usize, tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE2(1, 2)\nje_calloc(size_t num, size_t size)\n{\n\tvoid *ret;\n\ttsd_t *tsd;\n\tsize_t num_size;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (unlikely(malloc_init())) {\n\t\tnum_size = 0;\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\ttsd = tsd_fetch();\n\n\tnum_size = num * size;\n\tif (unlikely(num_size == 0)) {\n\t\tif (num == 0 || size == 0)\n\t\t\tnum_size = 1;\n\t\telse {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t/*\n\t * Try to avoid division here.  We know that it isn't possible to\n\t * overflow during multiplication if neither operand uses any of the\n\t * most significant half of the bits in a size_t.\n\t */\n\t} else if (unlikely(((num | size) & (SIZE_T_MAX << (sizeof(size_t) <<\n\t    2))) && (num_size / size != num))) {\n\t\t/* size_t overflow. */\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tif (config_prof && opt_prof) {\n\t\tusize = s2u(num_size);\n\t\tif (unlikely(usize == 0)) {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tret = icalloc_prof(tsd, usize);\n\t} else {\n\t\tif (config_stats || (config_valgrind && unlikely(in_valgrind)))\n\t\t\tusize = s2u(num_size);\n\t\tret = icalloc(tsd, num_size);\n\t}\n\nlabel_return:\n\tif (unlikely(ret == NULL)) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in calloc(): out of \"\n\t\t\t    \"memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && likely(ret != NULL)) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t}\n\tUTRACE(0, num_size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, true);\n\treturn (ret);\n}\n\nstatic void *\nirealloc_prof_sample(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize,\n    prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = iralloc(tsd, old_ptr, old_usize, LARGE_MINCLASS, 0, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = iralloc(tsd, old_ptr, old_usize, usize, 0, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nirealloc_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize)\n{\n\tvoid *p;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(old_ptr);\n\ttctx = prof_alloc_prep(tsd, usize, prof_active, true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = irealloc_prof_sample(tsd, old_ptr, old_usize, usize, tctx);\n\telse\n\t\tp = iralloc(tsd, old_ptr, old_usize, usize, 0, false);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_realloc(tsd, p, usize, tctx, prof_active, true, old_ptr, old_usize,\n\t    old_tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_INLINE_C void\nifree(tsd_t *tsd, void *ptr, tcache_t *tcache)\n{\n\tsize_t usize;\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = isalloc(ptr, config_prof);\n\t\tprof_free(tsd, ptr, usize);\n\t} else if (config_stats || config_valgrind)\n\t\tusize = isalloc(ptr, config_prof);\n\tif (config_stats)\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\trzsize = p2rz(ptr);\n\tiqalloc(tsd, ptr, tcache);\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n}\n\nJEMALLOC_INLINE_C void\nisfree(tsd_t *tsd, void *ptr, size_t usize, tcache_t *tcache)\n{\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\tif (config_prof && opt_prof)\n\t\tprof_free(tsd, ptr, usize);\n\tif (config_stats)\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\trzsize = p2rz(ptr);\n\tisqalloc(tsd, ptr, usize, tcache);\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ALLOC_SIZE(2)\nje_realloc(void *ptr, size_t size)\n{\n\tvoid *ret;\n\ttsd_t *tsd JEMALLOC_CC_SILENCE_INIT(NULL);\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t old_usize = 0;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (unlikely(size == 0)) {\n\t\tif (ptr != NULL) {\n\t\t\t/* realloc(ptr, 0) is equivalent to free(ptr). */\n\t\t\tUTRACE(ptr, 0, 0);\n\t\t\ttsd = tsd_fetch();\n\t\t\tifree(tsd, ptr, tcache_get(tsd, false));\n\t\t\treturn (NULL);\n\t\t}\n\t\tsize = 1;\n\t}\n\n\tif (likely(ptr != NULL)) {\n\t\tassert(malloc_initialized() || IS_INITIALIZER);\n\t\tmalloc_thread_init();\n\t\ttsd = tsd_fetch();\n\n\t\told_usize = isalloc(ptr, config_prof);\n\t\tif (config_valgrind && unlikely(in_valgrind))\n\t\t\told_rzsize = config_prof ? p2rz(ptr) : u2rz(old_usize);\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tusize = s2u(size);\n\t\t\tret = unlikely(usize == 0) ? NULL : irealloc_prof(tsd,\n\t\t\t    ptr, old_usize, usize);\n\t\t} else {\n\t\t\tif (config_stats || (config_valgrind &&\n\t\t\t    unlikely(in_valgrind)))\n\t\t\t\tusize = s2u(size);\n\t\t\tret = iralloc(tsd, ptr, old_usize, size, 0, false);\n\t\t}\n\t} else {\n\t\t/* realloc(NULL, size) is equivalent to malloc(size). */\n\t\tret = imalloc_body(size, &tsd, &usize);\n\t}\n\n\tif (unlikely(ret == NULL)) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in realloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && likely(ret != NULL)) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tUTRACE(ptr, size, ret);\n\tJEMALLOC_VALGRIND_REALLOC(true, ret, usize, true, ptr, old_usize,\n\t    old_rzsize, true, false);\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_free(void *ptr)\n{\n\n\tUTRACE(ptr, 0, 0);\n\tif (likely(ptr != NULL)) {\n\t\ttsd_t *tsd = tsd_fetch();\n\t\tifree(tsd, ptr, tcache_get(tsd, false));\n\t}\n}\n\n/*\n * End malloc(3)-compatible functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard override functions.\n */\n\n#ifdef JEMALLOC_OVERRIDE_MEMALIGN\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc)\nje_memalign(size_t alignment, size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\tif (unlikely(imemalign(&ret, alignment, size, 1) != 0))\n\t\tret = NULL;\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, size, false);\n\treturn (ret);\n}\n#endif\n\n#ifdef JEMALLOC_OVERRIDE_VALLOC\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc)\nje_valloc(size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\tif (unlikely(imemalign(&ret, PAGE, size, 1) != 0))\n\t\tret = NULL;\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, size, false);\n\treturn (ret);\n}\n#endif\n\n/*\n * is_malloc(je_malloc) is some macro magic to detect if jemalloc_defs.h has\n * #define je_malloc malloc\n */\n#define\tmalloc_is_malloc 1\n#define\tis_malloc_(a) malloc_is_ ## a\n#define\tis_malloc(a) is_malloc_(a)\n\n#if ((is_malloc(je_malloc) == 1) && defined(JEMALLOC_GLIBC_MALLOC_HOOK))\n/*\n * glibc provides the RTLD_DEEPBIND flag for dlopen which can make it possible\n * to inconsistently reference libc's malloc(3)-compatible functions\n * (https://bugzilla.mozilla.org/show_bug.cgi?id=493541).\n *\n * These definitions interpose hooks in glibc.  The functions are actually\n * passed an extra argument for the caller return address, which will be\n * ignored.\n */\nJEMALLOC_EXPORT void (*__free_hook)(void *ptr) = je_free;\nJEMALLOC_EXPORT void *(*__malloc_hook)(size_t size) = je_malloc;\nJEMALLOC_EXPORT void *(*__realloc_hook)(void *ptr, size_t size) = je_realloc;\n# ifdef JEMALLOC_GLIBC_MEMALIGN_HOOK\nJEMALLOC_EXPORT void *(*__memalign_hook)(size_t alignment, size_t size) =\n    je_memalign;\n# endif\n#endif\n\n/*\n * End non-standard override functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard functions.\n */\n\nJEMALLOC_ALWAYS_INLINE_C bool\nimallocx_flags_decode_hard(tsd_t *tsd, size_t size, int flags, size_t *usize,\n    size_t *alignment, bool *zero, tcache_t **tcache, arena_t **arena)\n{\n\n\tif ((flags & MALLOCX_LG_ALIGN_MASK) == 0) {\n\t\t*alignment = 0;\n\t\t*usize = s2u(size);\n\t} else {\n\t\t*alignment = MALLOCX_ALIGN_GET_SPECIFIED(flags);\n\t\t*usize = sa2u(size, *alignment);\n\t}\n\tassert(*usize != 0);\n\t*zero = MALLOCX_ZERO_GET(flags);\n\tif ((flags & MALLOCX_TCACHE_MASK) != 0) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\t*tcache = NULL;\n\t\telse\n\t\t\t*tcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\t*tcache = tcache_get(tsd, true);\n\tif ((flags & MALLOCX_ARENA_MASK) != 0) {\n\t\tunsigned arena_ind = MALLOCX_ARENA_GET(flags);\n\t\t*arena = arena_get(tsd, arena_ind, true, true);\n\t\tif (unlikely(*arena == NULL))\n\t\t\treturn (true);\n\t} else\n\t\t*arena = NULL;\n\treturn (false);\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nimallocx_flags_decode(tsd_t *tsd, size_t size, int flags, size_t *usize,\n    size_t *alignment, bool *zero, tcache_t **tcache, arena_t **arena)\n{\n\n\tif (likely(flags == 0)) {\n\t\t*usize = s2u(size);\n\t\tassert(*usize != 0);\n\t\t*alignment = 0;\n\t\t*zero = false;\n\t\t*tcache = tcache_get(tsd, true);\n\t\t*arena = NULL;\n\t\treturn (false);\n\t} else {\n\t\treturn (imallocx_flags_decode_hard(tsd, size, flags, usize,\n\t\t    alignment, zero, tcache, arena));\n\t}\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_flags(tsd_t *tsd, size_t usize, size_t alignment, bool zero,\n    tcache_t *tcache, arena_t *arena)\n{\n\n\tif (unlikely(alignment != 0))\n\t\treturn (ipalloct(tsd, usize, alignment, zero, tcache, arena));\n\tif (unlikely(zero))\n\t\treturn (icalloct(tsd, usize, tcache, arena));\n\treturn (imalloct(tsd, usize, tcache, arena));\n}\n\nstatic void *\nimallocx_prof_sample(tsd_t *tsd, size_t usize, size_t alignment, bool zero,\n    tcache_t *tcache, arena_t *arena)\n{\n\tvoid *p;\n\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tassert(((alignment == 0) ? s2u(LARGE_MINCLASS) :\n\t\t    sa2u(LARGE_MINCLASS, alignment)) == LARGE_MINCLASS);\n\t\tp = imallocx_flags(tsd, LARGE_MINCLASS, alignment, zero, tcache,\n\t\t    arena);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = imallocx_flags(tsd, usize, alignment, zero, tcache, arena);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_prof(tsd_t *tsd, size_t size, int flags, size_t *usize)\n{\n\tvoid *p;\n\tsize_t alignment;\n\tbool zero;\n\ttcache_t *tcache;\n\tarena_t *arena;\n\tprof_tctx_t *tctx;\n\n\tif (unlikely(imallocx_flags_decode(tsd, size, flags, usize, &alignment,\n\t    &zero, &tcache, &arena)))\n\t\treturn (NULL);\n\ttctx = prof_alloc_prep(tsd, *usize, prof_active_get_unlocked(), true);\n\tif (likely((uintptr_t)tctx == (uintptr_t)1U))\n\t\tp = imallocx_flags(tsd, *usize, alignment, zero, tcache, arena);\n\telse if ((uintptr_t)tctx > (uintptr_t)1U) {\n\t\tp = imallocx_prof_sample(tsd, *usize, alignment, zero, tcache,\n\t\t    arena);\n\t} else\n\t\tp = NULL;\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(p, *usize, tctx);\n\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_no_prof(tsd_t *tsd, size_t size, int flags, size_t *usize)\n{\n\tvoid *p;\n\tsize_t alignment;\n\tbool zero;\n\ttcache_t *tcache;\n\tarena_t *arena;\n\n\tif (likely(flags == 0)) {\n\t\tif (config_stats || (config_valgrind && unlikely(in_valgrind)))\n\t\t\t*usize = s2u(size);\n\t\treturn (imalloc(tsd, size));\n\t}\n\n\tif (unlikely(imallocx_flags_decode_hard(tsd, size, flags, usize,\n\t    &alignment, &zero, &tcache, &arena)))\n\t\treturn (NULL);\n\tp = imallocx_flags(tsd, *usize, alignment, zero, tcache, arena);\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\treturn (p);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)\nje_mallocx(size_t size, int flags)\n{\n\ttsd_t *tsd;\n\tvoid *p;\n\tsize_t usize;\n\n\tassert(size != 0);\n\n\tif (unlikely(malloc_init()))\n\t\tgoto label_oom;\n\ttsd = tsd_fetch();\n\n\tif (config_prof && opt_prof)\n\t\tp = imallocx_prof(tsd, size, flags, &usize);\n\telse\n\t\tp = imallocx_no_prof(tsd, size, flags, &usize);\n\tif (unlikely(p == NULL))\n\t\tgoto label_oom;\n\n\tif (config_stats) {\n\t\tassert(usize == isalloc(p, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t}\n\tUTRACE(0, size, p);\n\tJEMALLOC_VALGRIND_MALLOC(true, p, usize, MALLOCX_ZERO_GET(flags));\n\treturn (p);\nlabel_oom:\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(\"<jemalloc>: Error in mallocx(): out of memory\\n\");\n\t\tabort();\n\t}\n\tUTRACE(0, size, 0);\n\treturn (NULL);\n}\n\nstatic void *\nirallocx_prof_sample(tsd_t *tsd, void *old_ptr, size_t old_usize,\n    size_t usize, size_t alignment, bool zero, tcache_t *tcache, arena_t *arena,\n    prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = iralloct(tsd, old_ptr, old_usize, LARGE_MINCLASS, alignment,\n\t\t    zero, tcache, arena);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else {\n\t\tp = iralloct(tsd, old_ptr, old_usize, usize, alignment, zero,\n\t\t    tcache, arena);\n\t}\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nirallocx_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t size,\n    size_t alignment, size_t *usize, bool zero, tcache_t *tcache,\n    arena_t *arena)\n{\n\tvoid *p;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(old_ptr);\n\ttctx = prof_alloc_prep(tsd, *usize, prof_active, true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tp = irallocx_prof_sample(tsd, old_ptr, old_usize, *usize,\n\t\t    alignment, zero, tcache, arena, tctx);\n\t} else {\n\t\tp = iralloct(tsd, old_ptr, old_usize, size, alignment, zero,\n\t\t    tcache, arena);\n\t}\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\n\tif (p == old_ptr && alignment != 0) {\n\t\t/*\n\t\t * The allocation did not move, so it is possible that the size\n\t\t * class is smaller than would guarantee the requested\n\t\t * alignment, and that the alignment constraint was\n\t\t * serendipitously satisfied.  Additionally, old_usize may not\n\t\t * be the same as the current usize because of in-place large\n\t\t * reallocation.  Therefore, query the actual value of usize.\n\t\t */\n\t\t*usize = isalloc(p, config_prof);\n\t}\n\tprof_realloc(tsd, p, *usize, tctx, prof_active, true, old_ptr,\n\t    old_usize, old_tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ALLOC_SIZE(2)\nje_rallocx(void *ptr, size_t size, int flags)\n{\n\tvoid *p;\n\ttsd_t *tsd;\n\tsize_t usize;\n\tsize_t old_usize;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = MALLOCX_ALIGN_GET(flags);\n\tbool zero = flags & MALLOCX_ZERO;\n\tarena_t *arena;\n\ttcache_t *tcache;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\ttsd = tsd_fetch();\n\n\tif (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {\n\t\tunsigned arena_ind = MALLOCX_ARENA_GET(flags);\n\t\tarena = arena_get(tsd, arena_ind, true, true);\n\t\tif (unlikely(arena == NULL))\n\t\t\tgoto label_oom;\n\t} else\n\t\tarena = NULL;\n\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\ttcache = NULL;\n\t\telse\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\ttcache = tcache_get(tsd, true);\n\n\told_usize = isalloc(ptr, config_prof);\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\told_rzsize = u2rz(old_usize);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\t\tassert(usize != 0);\n\t\tp = irallocx_prof(tsd, ptr, old_usize, size, alignment, &usize,\n\t\t    zero, tcache, arena);\n\t\tif (unlikely(p == NULL))\n\t\t\tgoto label_oom;\n\t} else {\n\t\tp = iralloct(tsd, ptr, old_usize, size, alignment, zero,\n\t\t     tcache, arena);\n\t\tif (unlikely(p == NULL))\n\t\t\tgoto label_oom;\n\t\tif (config_stats || (config_valgrind && unlikely(in_valgrind)))\n\t\t\tusize = isalloc(p, config_prof);\n\t}\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\n\tif (config_stats) {\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tUTRACE(ptr, size, p);\n\tJEMALLOC_VALGRIND_REALLOC(true, p, usize, false, ptr, old_usize,\n\t    old_rzsize, false, zero);\n\treturn (p);\nlabel_oom:\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(\"<jemalloc>: Error in rallocx(): out of memory\\n\");\n\t\tabort();\n\t}\n\tUTRACE(ptr, size, 0);\n\treturn (NULL);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\nixallocx_helper(void *ptr, size_t old_usize, size_t size, size_t extra,\n    size_t alignment, bool zero)\n{\n\tsize_t usize;\n\n\tif (ixalloc(ptr, old_usize, size, extra, alignment, zero))\n\t\treturn (old_usize);\n\tusize = isalloc(ptr, config_prof);\n\n\treturn (usize);\n}\n\nstatic size_t\nixallocx_prof_sample(void *ptr, size_t old_usize, size_t size, size_t extra,\n    size_t alignment, bool zero, prof_tctx_t *tctx)\n{\n\tsize_t usize;\n\n\tif (tctx == NULL)\n\t\treturn (old_usize);\n\tusize = ixallocx_helper(ptr, old_usize, size, extra, alignment, zero);\n\n\treturn (usize);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\nixallocx_prof(tsd_t *tsd, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero)\n{\n\tsize_t usize_max, usize;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(ptr);\n\t/*\n\t * usize isn't knowable before ixalloc() returns when extra is non-zero.\n\t * Therefore, compute its maximum possible value and use that in\n\t * prof_alloc_prep() to decide whether to capture a backtrace.\n\t * prof_realloc() will use the actual usize to decide whether to sample.\n\t */\n\tusize_max = (alignment == 0) ? s2u(size+extra) : sa2u(size+extra,\n\t    alignment);\n\tassert(usize_max != 0);\n\ttctx = prof_alloc_prep(tsd, usize_max, prof_active, false);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tusize = ixallocx_prof_sample(ptr, old_usize, size, extra,\n\t\t    alignment, zero, tctx);\n\t} else {\n\t\tusize = ixallocx_helper(ptr, old_usize, size, extra, alignment,\n\t\t    zero);\n\t}\n\tif (usize == old_usize) {\n\t\tprof_alloc_rollback(tsd, tctx, false);\n\t\treturn (usize);\n\t}\n\tprof_realloc(tsd, ptr, usize, tctx, prof_active, false, ptr, old_usize,\n\t    old_tctx);\n\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nje_xallocx(void *ptr, size_t size, size_t extra, int flags)\n{\n\ttsd_t *tsd;\n\tsize_t usize, old_usize;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = MALLOCX_ALIGN_GET(flags);\n\tbool zero = flags & MALLOCX_ZERO;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(SIZE_T_MAX - size >= extra);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\ttsd = tsd_fetch();\n\n\told_usize = isalloc(ptr, config_prof);\n\n\t/* Clamp extra if necessary to avoid (size + extra) overflow. */\n\tif (unlikely(size + extra > HUGE_MAXCLASS)) {\n\t\t/* Check for size overflow. */\n\t\tif (unlikely(size > HUGE_MAXCLASS)) {\n\t\t\tusize = old_usize;\n\t\t\tgoto label_not_resized;\n\t\t}\n\t\textra = HUGE_MAXCLASS - size;\n\t}\n\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\told_rzsize = u2rz(old_usize);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = ixallocx_prof(tsd, ptr, old_usize, size, extra,\n\t\t    alignment, zero);\n\t} else {\n\t\tusize = ixallocx_helper(ptr, old_usize, size, extra, alignment,\n\t\t    zero);\n\t}\n\tif (unlikely(usize == old_usize))\n\t\tgoto label_not_resized;\n\n\tif (config_stats) {\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tJEMALLOC_VALGRIND_REALLOC(false, ptr, usize, false, ptr, old_usize,\n\t    old_rzsize, false, zero);\nlabel_not_resized:\n\tUTRACE(ptr, size, ptr);\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nJEMALLOC_ATTR(pure)\nje_sallocx(const void *ptr, int flags)\n{\n\tsize_t usize;\n\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\n\tif (config_ivsalloc)\n\t\tusize = ivsalloc(ptr, config_prof);\n\telse\n\t\tusize = isalloc(ptr, config_prof);\n\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_dallocx(void *ptr, int flags)\n{\n\ttsd_t *tsd;\n\ttcache_t *tcache;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\ttsd = tsd_fetch();\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\ttcache = NULL;\n\t\telse\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\ttcache = tcache_get(tsd, false);\n\n\tUTRACE(ptr, 0, 0);\n\tifree(tsd_fetch(), ptr, tcache);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\ninallocx(size_t size, int flags)\n{\n\tsize_t usize;\n\n\tif (likely((flags & MALLOCX_LG_ALIGN_MASK) == 0))\n\t\tusize = s2u(size);\n\telse\n\t\tusize = sa2u(size, MALLOCX_ALIGN_GET_SPECIFIED(flags));\n\tassert(usize != 0);\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_sdallocx(void *ptr, size_t size, int flags)\n{\n\ttsd_t *tsd;\n\ttcache_t *tcache;\n\tsize_t usize;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tusize = inallocx(size, flags);\n\tassert(usize == isalloc(ptr, config_prof));\n\n\ttsd = tsd_fetch();\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\ttcache = NULL;\n\t\telse\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\ttcache = tcache_get(tsd, false);\n\n\tUTRACE(ptr, 0, 0);\n\tisfree(tsd, ptr, usize, tcache);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nJEMALLOC_ATTR(pure)\nje_nallocx(size_t size, int flags)\n{\n\n\tassert(size != 0);\n\n\tif (unlikely(malloc_init()))\n\t\treturn (0);\n\n\treturn (inallocx(size, flags));\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,\n    size_t newlen)\n{\n\n\tif (unlikely(malloc_init()))\n\t\treturn (EAGAIN);\n\n\treturn (ctl_byname(name, oldp, oldlenp, newp, newlen));\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp)\n{\n\n\tif (unlikely(malloc_init()))\n\t\treturn (EAGAIN);\n\n\treturn (ctl_nametomib(name, mibp, miblenp));\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,\n  void *newp, size_t newlen)\n{\n\n\tif (unlikely(malloc_init()))\n\t\treturn (EAGAIN);\n\n\treturn (ctl_bymib(mib, miblen, oldp, oldlenp, newp, newlen));\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,\n    const char *opts)\n{\n\n\tstats_print(write_cb, cbopaque, opts);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nje_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr)\n{\n\tsize_t ret;\n\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\n\tif (config_ivsalloc)\n\t\tret = ivsalloc(ptr, config_prof);\n\telse\n\t\tret = (ptr == NULL) ? 0 : isalloc(ptr, config_prof);\n\n\treturn (ret);\n}\n\n/*\n * End non-standard functions.\n */\n/******************************************************************************/\n/*\n * The following functions are used by threading libraries for protection of\n * malloc during fork().\n */\n\n/*\n * If an application creates a thread before doing any allocation in the main\n * thread, then calls fork(2) in the main thread followed by memory allocation\n * in the child process, a race can occur that results in deadlock within the\n * child: the main thread may have forked while the created thread had\n * partially initialized the allocator.  Ordinarily jemalloc prevents\n * fork/malloc races via the following functions it registers during\n * initialization using pthread_atfork(), but of course that does no good if\n * the allocator isn't fully initialized at fork time.  The following library\n * constructor is a partial solution to this problem.  It may still be possible\n * to trigger the deadlock described above, but doing so would involve forking\n * via a library constructor that runs before jemalloc's runs.\n */\nJEMALLOC_ATTR(constructor)\nstatic void\njemalloc_constructor(void)\n{\n\n\tmalloc_init();\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_prefork(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_prefork(void)\n#endif\n{\n\tunsigned i;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (!malloc_initialized())\n\t\treturn;\n#endif\n\tassert(malloc_initialized());\n\n\t/* Acquire all mutexes in a safe order. */\n\tctl_prefork();\n\tprof_prefork();\n\tmalloc_mutex_prefork(&arenas_lock);\n\tfor (i = 0; i < narenas_total; i++) {\n\t\tif (arenas[i] != NULL)\n\t\t\tarena_prefork(arenas[i]);\n\t}\n\tchunk_prefork();\n\tbase_prefork();\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_postfork_parent(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_postfork(void)\n#endif\n{\n\tunsigned i;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (!malloc_initialized())\n\t\treturn;\n#endif\n\tassert(malloc_initialized());\n\n\t/* Release all mutexes, now that fork() has completed. */\n\tbase_postfork_parent();\n\tchunk_postfork_parent();\n\tfor (i = 0; i < narenas_total; i++) {\n\t\tif (arenas[i] != NULL)\n\t\t\tarena_postfork_parent(arenas[i]);\n\t}\n\tmalloc_mutex_postfork_parent(&arenas_lock);\n\tprof_postfork_parent();\n\tctl_postfork_parent();\n}\n\nvoid\njemalloc_postfork_child(void)\n{\n\tunsigned i;\n\n\tassert(malloc_initialized());\n\n\t/* Release all mutexes, now that fork() has completed. */\n\tbase_postfork_child();\n\tchunk_postfork_child();\n\tfor (i = 0; i < narenas_total; i++) {\n\t\tif (arenas[i] != NULL)\n\t\t\tarena_postfork_child(arenas[i]);\n\t}\n\tmalloc_mutex_postfork_child(&arenas_lock);\n\tprof_postfork_child();\n\tctl_postfork_child();\n}\n\n/******************************************************************************/\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-jemalloc-4.0.4-4naklqrlm2ridywfvjnrevi7trlrivuw/spack-src/doc/jemalloc.html"
    ],
    "total_files": 169
}