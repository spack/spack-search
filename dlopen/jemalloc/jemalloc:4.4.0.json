{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-jemalloc-4.4.0-jtxkltrmeme2pslxabjhwgyzz35svfcw/spack-src/src/jemalloc.c": "#define\tJEMALLOC_C_\n#include \"jemalloc/internal/jemalloc_internal.h\"\n\n/******************************************************************************/\n/* Data. */\n\n/* Runtime configuration options. */\nconst char\t*je_malloc_conf\n#ifndef _WIN32\n    JEMALLOC_ATTR(weak)\n#endif\n    ;\nbool\topt_abort =\n#ifdef JEMALLOC_DEBUG\n    true\n#else\n    false\n#endif\n    ;\nconst char\t*opt_junk =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    \"true\"\n#else\n    \"false\"\n#endif\n    ;\nbool\topt_junk_alloc =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\nbool\topt_junk_free =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\n\nsize_t\topt_quarantine = ZU(0);\nbool\topt_redzone = false;\nbool\topt_utrace = false;\nbool\topt_xmalloc = false;\nbool\topt_zero = false;\nunsigned\topt_narenas = 0;\n\n/* Initialized to true if the process is running inside Valgrind. */\nbool\tin_valgrind;\n\nunsigned\tncpus;\n\n/* Protects arenas initialization. */\nstatic malloc_mutex_t\tarenas_lock;\n/*\n * Arenas that are used to service external requests.  Not all elements of the\n * arenas array are necessarily used; arenas are created lazily as needed.\n *\n * arenas[0..narenas_auto) are used for automatic multiplexing of threads and\n * arenas.  arenas[narenas_auto..narenas_total) are only used if the application\n * takes some action to create them and allocate from them.\n */\narena_t\t\t\t**arenas;\nstatic unsigned\t\tnarenas_total; /* Use narenas_total_*(). */\nstatic arena_t\t\t*a0; /* arenas[0]; read-only after initialization. */\nunsigned\t\tnarenas_auto; /* Read-only after initialization. */\n\ntypedef enum {\n\tmalloc_init_uninitialized\t= 3,\n\tmalloc_init_a0_initialized\t= 2,\n\tmalloc_init_recursible\t\t= 1,\n\tmalloc_init_initialized\t\t= 0 /* Common case --> jnz. */\n} malloc_init_t;\nstatic malloc_init_t\tmalloc_init_state = malloc_init_uninitialized;\n\n/* False should be the common case.  Set to true to trigger initialization. */\nstatic bool\tmalloc_slow = true;\n\n/* When malloc_slow is true, set the corresponding bits for sanity check. */\nenum {\n\tflag_opt_junk_alloc\t= (1U),\n\tflag_opt_junk_free\t= (1U << 1),\n\tflag_opt_quarantine\t= (1U << 2),\n\tflag_opt_zero\t\t= (1U << 3),\n\tflag_opt_utrace\t\t= (1U << 4),\n\tflag_in_valgrind\t= (1U << 5),\n\tflag_opt_xmalloc\t= (1U << 6)\n};\nstatic uint8_t\tmalloc_slow_flags;\n\nJEMALLOC_ALIGNED(CACHELINE)\nconst size_t\tpind2sz_tab[NPSIZES] = {\n#define\tPSZ_yes(lg_grp, ndelta, lg_delta)\t\t\t\t\\\n\t(((ZU(1)<<lg_grp) + (ZU(ndelta)<<lg_delta))),\n#define\tPSZ_no(lg_grp, ndelta, lg_delta)\n#define\tSC(index, lg_grp, lg_delta, ndelta, psz, bin, lg_delta_lookup)\t\\\n\tPSZ_##psz(lg_grp, ndelta, lg_delta)\n\tSIZE_CLASSES\n#undef PSZ_yes\n#undef PSZ_no\n#undef SC\n};\n\nJEMALLOC_ALIGNED(CACHELINE)\nconst size_t\tindex2size_tab[NSIZES] = {\n#define\tSC(index, lg_grp, lg_delta, ndelta, psz, bin, lg_delta_lookup)\t\\\n\t((ZU(1)<<lg_grp) + (ZU(ndelta)<<lg_delta)),\n\tSIZE_CLASSES\n#undef SC\n};\n\nJEMALLOC_ALIGNED(CACHELINE)\nconst uint8_t\tsize2index_tab[] = {\n#if LG_TINY_MIN == 0\n#warning \"Dangerous LG_TINY_MIN\"\n#define\tS2B_0(i)\ti,\n#elif LG_TINY_MIN == 1\n#warning \"Dangerous LG_TINY_MIN\"\n#define\tS2B_1(i)\ti,\n#elif LG_TINY_MIN == 2\n#warning \"Dangerous LG_TINY_MIN\"\n#define\tS2B_2(i)\ti,\n#elif LG_TINY_MIN == 3\n#define\tS2B_3(i)\ti,\n#elif LG_TINY_MIN == 4\n#define\tS2B_4(i)\ti,\n#elif LG_TINY_MIN == 5\n#define\tS2B_5(i)\ti,\n#elif LG_TINY_MIN == 6\n#define\tS2B_6(i)\ti,\n#elif LG_TINY_MIN == 7\n#define\tS2B_7(i)\ti,\n#elif LG_TINY_MIN == 8\n#define\tS2B_8(i)\ti,\n#elif LG_TINY_MIN == 9\n#define\tS2B_9(i)\ti,\n#elif LG_TINY_MIN == 10\n#define\tS2B_10(i)\ti,\n#elif LG_TINY_MIN == 11\n#define\tS2B_11(i)\ti,\n#else\n#error \"Unsupported LG_TINY_MIN\"\n#endif\n#if LG_TINY_MIN < 1\n#define\tS2B_1(i)\tS2B_0(i) S2B_0(i)\n#endif\n#if LG_TINY_MIN < 2\n#define\tS2B_2(i)\tS2B_1(i) S2B_1(i)\n#endif\n#if LG_TINY_MIN < 3\n#define\tS2B_3(i)\tS2B_2(i) S2B_2(i)\n#endif\n#if LG_TINY_MIN < 4\n#define\tS2B_4(i)\tS2B_3(i) S2B_3(i)\n#endif\n#if LG_TINY_MIN < 5\n#define\tS2B_5(i)\tS2B_4(i) S2B_4(i)\n#endif\n#if LG_TINY_MIN < 6\n#define\tS2B_6(i)\tS2B_5(i) S2B_5(i)\n#endif\n#if LG_TINY_MIN < 7\n#define\tS2B_7(i)\tS2B_6(i) S2B_6(i)\n#endif\n#if LG_TINY_MIN < 8\n#define\tS2B_8(i)\tS2B_7(i) S2B_7(i)\n#endif\n#if LG_TINY_MIN < 9\n#define\tS2B_9(i)\tS2B_8(i) S2B_8(i)\n#endif\n#if LG_TINY_MIN < 10\n#define\tS2B_10(i)\tS2B_9(i) S2B_9(i)\n#endif\n#if LG_TINY_MIN < 11\n#define\tS2B_11(i)\tS2B_10(i) S2B_10(i)\n#endif\n#define\tS2B_no(i)\n#define\tSC(index, lg_grp, lg_delta, ndelta, psz, bin, lg_delta_lookup)\t\\\n\tS2B_##lg_delta_lookup(index)\n\tSIZE_CLASSES\n#undef S2B_3\n#undef S2B_4\n#undef S2B_5\n#undef S2B_6\n#undef S2B_7\n#undef S2B_8\n#undef S2B_9\n#undef S2B_10\n#undef S2B_11\n#undef S2B_no\n#undef SC\n};\n\n#ifdef JEMALLOC_THREADED_INIT\n/* Used to let the initializing thread recursively allocate. */\n#  define NO_INITIALIZER\t((unsigned long)0)\n#  define INITIALIZER\t\tpthread_self()\n#  define IS_INITIALIZER\t(malloc_initializer == pthread_self())\nstatic pthread_t\t\tmalloc_initializer = NO_INITIALIZER;\n#else\n#  define NO_INITIALIZER\tfalse\n#  define INITIALIZER\t\ttrue\n#  define IS_INITIALIZER\tmalloc_initializer\nstatic bool\t\t\tmalloc_initializer = NO_INITIALIZER;\n#endif\n\n/* Used to avoid initialization races. */\n#ifdef _WIN32\n#if _WIN32_WINNT >= 0x0600\nstatic malloc_mutex_t\tinit_lock = SRWLOCK_INIT;\n#else\nstatic malloc_mutex_t\tinit_lock;\nstatic bool init_lock_initialized = false;\n\nJEMALLOC_ATTR(constructor)\nstatic void WINAPI\n_init_init_lock(void)\n{\n\n\t/* If another constructor in the same binary is using mallctl to\n\t * e.g. setup chunk hooks, it may end up running before this one,\n\t * and malloc_init_hard will crash trying to lock the uninitialized\n\t * lock. So we force an initialization of the lock in\n\t * malloc_init_hard as well. We don't try to care about atomicity\n\t * of the accessed to the init_lock_initialized boolean, since it\n\t * really only matters early in the process creation, before any\n\t * separate thread normally starts doing anything. */\n\tif (!init_lock_initialized)\n\t\tmalloc_mutex_init(&init_lock, \"init\", WITNESS_RANK_INIT);\n\tinit_lock_initialized = true;\n}\n\n#ifdef _MSC_VER\n#  pragma section(\".CRT$XCU\", read)\nJEMALLOC_SECTION(\".CRT$XCU\") JEMALLOC_ATTR(used)\nstatic const void (WINAPI *init_init_lock)(void) = _init_init_lock;\n#endif\n#endif\n#else\nstatic malloc_mutex_t\tinit_lock = MALLOC_MUTEX_INITIALIZER;\n#endif\n\ntypedef struct {\n\tvoid\t*p;\t/* Input pointer (as in realloc(p, s)). */\n\tsize_t\ts;\t/* Request size. */\n\tvoid\t*r;\t/* Result pointer. */\n} malloc_utrace_t;\n\n#ifdef JEMALLOC_UTRACE\n#  define UTRACE(a, b, c) do {\t\t\t\t\t\t\\\n\tif (unlikely(opt_utrace)) {\t\t\t\t\t\\\n\t\tint utrace_serrno = errno;\t\t\t\t\\\n\t\tmalloc_utrace_t ut;\t\t\t\t\t\\\n\t\tut.p = (a);\t\t\t\t\t\t\\\n\t\tut.s = (b);\t\t\t\t\t\t\\\n\t\tut.r = (c);\t\t\t\t\t\t\\\n\t\tutrace(&ut, sizeof(ut));\t\t\t\t\\\n\t\terrno = utrace_serrno;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n#else\n#  define UTRACE(a, b, c)\n#endif\n\n/******************************************************************************/\n/*\n * Function prototypes for static functions that are referenced prior to\n * definition.\n */\n\nstatic bool\tmalloc_init_hard_a0(void);\nstatic bool\tmalloc_init_hard(void);\n\n/******************************************************************************/\n/*\n * Begin miscellaneous support functions.\n */\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_initialized(void)\n{\n\n\treturn (malloc_init_state == malloc_init_initialized);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void\nmalloc_thread_init(void)\n{\n\n\t/*\n\t * TSD initialization can't be safely done as a side effect of\n\t * deallocation, because it is possible for a thread to do nothing but\n\t * deallocate its TLS data via free(), in which case writing to TLS\n\t * would cause write-after-free memory corruption.  The quarantine\n\t * facility *only* gets used as a side effect of deallocation, so make\n\t * a best effort attempt at initializing its TSD by hooking all\n\t * allocation events.\n\t */\n\tif (config_fill && unlikely(opt_quarantine))\n\t\tquarantine_alloc_hook();\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_init_a0(void)\n{\n\n\tif (unlikely(malloc_init_state == malloc_init_uninitialized))\n\t\treturn (malloc_init_hard_a0());\n\treturn (false);\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_init(void)\n{\n\n\tif (unlikely(!malloc_initialized()) && malloc_init_hard())\n\t\treturn (true);\n\tmalloc_thread_init();\n\n\treturn (false);\n}\n\n/*\n * The a0*() functions are used instead of i{d,}alloc() in situations that\n * cannot tolerate TLS variable access.\n */\n\nstatic void *\na0ialloc(size_t size, bool zero, bool is_metadata)\n{\n\n\tif (unlikely(malloc_init_a0()))\n\t\treturn (NULL);\n\n\treturn (iallocztm(TSDN_NULL, size, size2index(size), zero, NULL,\n\t    is_metadata, arena_get(TSDN_NULL, 0, true), true));\n}\n\nstatic void\na0idalloc(void *ptr, bool is_metadata)\n{\n\n\tidalloctm(TSDN_NULL, ptr, false, is_metadata, true);\n}\n\narena_t *\na0get(void)\n{\n\n\treturn (a0);\n}\n\nvoid *\na0malloc(size_t size)\n{\n\n\treturn (a0ialloc(size, false, true));\n}\n\nvoid\na0dalloc(void *ptr)\n{\n\n\ta0idalloc(ptr, true);\n}\n\n/*\n * FreeBSD's libc uses the bootstrap_*() functions in bootstrap-senstive\n * situations that cannot tolerate TLS variable access (TLS allocation and very\n * early internal data structure initialization).\n */\n\nvoid *\nbootstrap_malloc(size_t size)\n{\n\n\tif (unlikely(size == 0))\n\t\tsize = 1;\n\n\treturn (a0ialloc(size, false, false));\n}\n\nvoid *\nbootstrap_calloc(size_t num, size_t size)\n{\n\tsize_t num_size;\n\n\tnum_size = num * size;\n\tif (unlikely(num_size == 0)) {\n\t\tassert(num == 0 || size == 0);\n\t\tnum_size = 1;\n\t}\n\n\treturn (a0ialloc(num_size, true, false));\n}\n\nvoid\nbootstrap_free(void *ptr)\n{\n\n\tif (unlikely(ptr == NULL))\n\t\treturn;\n\n\ta0idalloc(ptr, false);\n}\n\nstatic void\narena_set(unsigned ind, arena_t *arena)\n{\n\n\tatomic_write_p((void **)&arenas[ind], arena);\n}\n\nstatic void\nnarenas_total_set(unsigned narenas)\n{\n\n\tatomic_write_u(&narenas_total, narenas);\n}\n\nstatic void\nnarenas_total_inc(void)\n{\n\n\tatomic_add_u(&narenas_total, 1);\n}\n\nunsigned\nnarenas_total_get(void)\n{\n\n\treturn (atomic_read_u(&narenas_total));\n}\n\n/* Create a new arena and insert it into the arenas array at index ind. */\nstatic arena_t *\narena_init_locked(tsdn_t *tsdn, unsigned ind)\n{\n\tarena_t *arena;\n\n\tassert(ind <= narenas_total_get());\n\tif (ind > MALLOCX_ARENA_MAX)\n\t\treturn (NULL);\n\tif (ind == narenas_total_get())\n\t\tnarenas_total_inc();\n\n\t/*\n\t * Another thread may have already initialized arenas[ind] if it's an\n\t * auto arena.\n\t */\n\tarena = arena_get(tsdn, ind, false);\n\tif (arena != NULL) {\n\t\tassert(ind < narenas_auto);\n\t\treturn (arena);\n\t}\n\n\t/* Actually initialize the arena. */\n\tarena = arena_new(tsdn, ind);\n\tarena_set(ind, arena);\n\treturn (arena);\n}\n\narena_t *\narena_init(tsdn_t *tsdn, unsigned ind)\n{\n\tarena_t *arena;\n\n\tmalloc_mutex_lock(tsdn, &arenas_lock);\n\tarena = arena_init_locked(tsdn, ind);\n\tmalloc_mutex_unlock(tsdn, &arenas_lock);\n\treturn (arena);\n}\n\nstatic void\narena_bind(tsd_t *tsd, unsigned ind, bool internal)\n{\n\tarena_t *arena;\n\n\tif (!tsd_nominal(tsd))\n\t\treturn;\n\n\tarena = arena_get(tsd_tsdn(tsd), ind, false);\n\tarena_nthreads_inc(arena, internal);\n\n\tif (internal)\n\t\ttsd_iarena_set(tsd, arena);\n\telse\n\t\ttsd_arena_set(tsd, arena);\n}\n\nvoid\narena_migrate(tsd_t *tsd, unsigned oldind, unsigned newind)\n{\n\tarena_t *oldarena, *newarena;\n\n\toldarena = arena_get(tsd_tsdn(tsd), oldind, false);\n\tnewarena = arena_get(tsd_tsdn(tsd), newind, false);\n\tarena_nthreads_dec(oldarena, false);\n\tarena_nthreads_inc(newarena, false);\n\ttsd_arena_set(tsd, newarena);\n}\n\nstatic void\narena_unbind(tsd_t *tsd, unsigned ind, bool internal)\n{\n\tarena_t *arena;\n\n\tarena = arena_get(tsd_tsdn(tsd), ind, false);\n\tarena_nthreads_dec(arena, internal);\n\tif (internal)\n\t\ttsd_iarena_set(tsd, NULL);\n\telse\n\t\ttsd_arena_set(tsd, NULL);\n}\n\narena_tdata_t *\narena_tdata_get_hard(tsd_t *tsd, unsigned ind)\n{\n\tarena_tdata_t *tdata, *arenas_tdata_old;\n\tarena_tdata_t *arenas_tdata = tsd_arenas_tdata_get(tsd);\n\tunsigned narenas_tdata_old, i;\n\tunsigned narenas_tdata = tsd_narenas_tdata_get(tsd);\n\tunsigned narenas_actual = narenas_total_get();\n\n\t/*\n\t * Dissociate old tdata array (and set up for deallocation upon return)\n\t * if it's too small.\n\t */\n\tif (arenas_tdata != NULL && narenas_tdata < narenas_actual) {\n\t\tarenas_tdata_old = arenas_tdata;\n\t\tnarenas_tdata_old = narenas_tdata;\n\t\tarenas_tdata = NULL;\n\t\tnarenas_tdata = 0;\n\t\ttsd_arenas_tdata_set(tsd, arenas_tdata);\n\t\ttsd_narenas_tdata_set(tsd, narenas_tdata);\n\t} else {\n\t\tarenas_tdata_old = NULL;\n\t\tnarenas_tdata_old = 0;\n\t}\n\n\t/* Allocate tdata array if it's missing. */\n\tif (arenas_tdata == NULL) {\n\t\tbool *arenas_tdata_bypassp = tsd_arenas_tdata_bypassp_get(tsd);\n\t\tnarenas_tdata = (ind < narenas_actual) ? narenas_actual : ind+1;\n\n\t\tif (tsd_nominal(tsd) && !*arenas_tdata_bypassp) {\n\t\t\t*arenas_tdata_bypassp = true;\n\t\t\tarenas_tdata = (arena_tdata_t *)a0malloc(\n\t\t\t    sizeof(arena_tdata_t) * narenas_tdata);\n\t\t\t*arenas_tdata_bypassp = false;\n\t\t}\n\t\tif (arenas_tdata == NULL) {\n\t\t\ttdata = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tassert(tsd_nominal(tsd) && !*arenas_tdata_bypassp);\n\t\ttsd_arenas_tdata_set(tsd, arenas_tdata);\n\t\ttsd_narenas_tdata_set(tsd, narenas_tdata);\n\t}\n\n\t/*\n\t * Copy to tdata array.  It's possible that the actual number of arenas\n\t * has increased since narenas_total_get() was called above, but that\n\t * causes no correctness issues unless two threads concurrently execute\n\t * the arenas.extend mallctl, which we trust mallctl synchronization to\n\t * prevent.\n\t */\n\n\t/* Copy/initialize tickers. */\n\tfor (i = 0; i < narenas_actual; i++) {\n\t\tif (i < narenas_tdata_old) {\n\t\t\tticker_copy(&arenas_tdata[i].decay_ticker,\n\t\t\t    &arenas_tdata_old[i].decay_ticker);\n\t\t} else {\n\t\t\tticker_init(&arenas_tdata[i].decay_ticker,\n\t\t\t    DECAY_NTICKS_PER_UPDATE);\n\t\t}\n\t}\n\tif (narenas_tdata > narenas_actual) {\n\t\tmemset(&arenas_tdata[narenas_actual], 0, sizeof(arena_tdata_t)\n\t\t    * (narenas_tdata - narenas_actual));\n\t}\n\n\t/* Read the refreshed tdata array. */\n\ttdata = &arenas_tdata[ind];\nlabel_return:\n\tif (arenas_tdata_old != NULL)\n\t\ta0dalloc(arenas_tdata_old);\n\treturn (tdata);\n}\n\n/* Slow path, called only by arena_choose(). */\narena_t *\narena_choose_hard(tsd_t *tsd, bool internal)\n{\n\tarena_t *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\n\tif (narenas_auto > 1) {\n\t\tunsigned i, j, choose[2], first_null;\n\n\t\t/*\n\t\t * Determine binding for both non-internal and internal\n\t\t * allocation.\n\t\t *\n\t\t *   choose[0]: For application allocation.\n\t\t *   choose[1]: For internal metadata allocation.\n\t\t */\n\n\t\tfor (j = 0; j < 2; j++)\n\t\t\tchoose[j] = 0;\n\n\t\tfirst_null = narenas_auto;\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &arenas_lock);\n\t\tassert(arena_get(tsd_tsdn(tsd), 0, false) != NULL);\n\t\tfor (i = 1; i < narenas_auto; i++) {\n\t\t\tif (arena_get(tsd_tsdn(tsd), i, false) != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Choose the first arena that has the lowest\n\t\t\t\t * number of threads assigned to it.\n\t\t\t\t */\n\t\t\t\tfor (j = 0; j < 2; j++) {\n\t\t\t\t\tif (arena_nthreads_get(arena_get(\n\t\t\t\t\t    tsd_tsdn(tsd), i, false), !!j) <\n\t\t\t\t\t    arena_nthreads_get(arena_get(\n\t\t\t\t\t    tsd_tsdn(tsd), choose[j], false),\n\t\t\t\t\t    !!j))\n\t\t\t\t\t\tchoose[j] = i;\n\t\t\t\t}\n\t\t\t} else if (first_null == narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Record the index of the first uninitialized\n\t\t\t\t * arena, in case all extant arenas are in use.\n\t\t\t\t *\n\t\t\t\t * NB: It is possible for there to be\n\t\t\t\t * discontinuities in terms of initialized\n\t\t\t\t * versus uninitialized arenas, due to the\n\t\t\t\t * \"thread.arena\" mallctl.\n\t\t\t\t */\n\t\t\t\tfirst_null = i;\n\t\t\t}\n\t\t}\n\n\t\tfor (j = 0; j < 2; j++) {\n\t\t\tif (arena_nthreads_get(arena_get(tsd_tsdn(tsd),\n\t\t\t    choose[j], false), !!j) == 0 || first_null ==\n\t\t\t    narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Use an unloaded arena, or the least loaded\n\t\t\t\t * arena if all arenas are already initialized.\n\t\t\t\t */\n\t\t\t\tif (!!j == internal) {\n\t\t\t\t\tret = arena_get(tsd_tsdn(tsd),\n\t\t\t\t\t    choose[j], false);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tarena_t *arena;\n\n\t\t\t\t/* Initialize a new arena. */\n\t\t\t\tchoose[j] = first_null;\n\t\t\t\tarena = arena_init_locked(tsd_tsdn(tsd),\n\t\t\t\t    choose[j]);\n\t\t\t\tif (arena == NULL) {\n\t\t\t\t\tmalloc_mutex_unlock(tsd_tsdn(tsd),\n\t\t\t\t\t    &arenas_lock);\n\t\t\t\t\treturn (NULL);\n\t\t\t\t}\n\t\t\t\tif (!!j == internal)\n\t\t\t\t\tret = arena;\n\t\t\t}\n\t\t\tarena_bind(tsd, choose[j], !!j);\n\t\t}\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &arenas_lock);\n\t} else {\n\t\tret = arena_get(tsd_tsdn(tsd), 0, false);\n\t\tarena_bind(tsd, 0, false);\n\t\tarena_bind(tsd, 0, true);\n\t}\n\n\treturn (ret);\n}\n\nvoid\nthread_allocated_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nvoid\nthread_deallocated_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nvoid\niarena_cleanup(tsd_t *tsd)\n{\n\tarena_t *iarena;\n\n\tiarena = tsd_iarena_get(tsd);\n\tif (iarena != NULL)\n\t\tarena_unbind(tsd, iarena->ind, true);\n}\n\nvoid\narena_cleanup(tsd_t *tsd)\n{\n\tarena_t *arena;\n\n\tarena = tsd_arena_get(tsd);\n\tif (arena != NULL)\n\t\tarena_unbind(tsd, arena->ind, false);\n}\n\nvoid\narenas_tdata_cleanup(tsd_t *tsd)\n{\n\tarena_tdata_t *arenas_tdata;\n\n\t/* Prevent tsd->arenas_tdata from being (re)created. */\n\t*tsd_arenas_tdata_bypassp_get(tsd) = true;\n\n\tarenas_tdata = tsd_arenas_tdata_get(tsd);\n\tif (arenas_tdata != NULL) {\n\t\ttsd_arenas_tdata_set(tsd, NULL);\n\t\ta0dalloc(arenas_tdata);\n\t}\n}\n\nvoid\nnarenas_tdata_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nvoid\narenas_tdata_bypass_cleanup(tsd_t *tsd)\n{\n\n\t/* Do nothing. */\n}\n\nstatic void\nstats_print_atexit(void)\n{\n\n\tif (config_tcache && config_stats) {\n\t\ttsdn_t *tsdn;\n\t\tunsigned narenas, i;\n\n\t\ttsdn = tsdn_fetch();\n\n\t\t/*\n\t\t * Merge stats from extant threads.  This is racy, since\n\t\t * individual threads do not lock when recording tcache stats\n\t\t * events.  As a consequence, the final stats may be slightly\n\t\t * out of date by the time they are reported, if other threads\n\t\t * continue to allocate.\n\t\t */\n\t\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\t\tarena_t *arena = arena_get(tsdn, i, false);\n\t\t\tif (arena != NULL) {\n\t\t\t\ttcache_t *tcache;\n\n\t\t\t\t/*\n\t\t\t\t * tcache_stats_merge() locks bins, so if any\n\t\t\t\t * code is introduced that acquires both arena\n\t\t\t\t * and bin locks in the opposite order,\n\t\t\t\t * deadlocks may result.\n\t\t\t\t */\n\t\t\t\tmalloc_mutex_lock(tsdn, &arena->lock);\n\t\t\t\tql_foreach(tcache, &arena->tcache_ql, link) {\n\t\t\t\t\ttcache_stats_merge(tsdn, tcache, arena);\n\t\t\t\t}\n\t\t\t\tmalloc_mutex_unlock(tsdn, &arena->lock);\n\t\t\t}\n\t\t}\n\t}\n\tje_malloc_stats_print(NULL, NULL, NULL);\n}\n\n/*\n * End miscellaneous support functions.\n */\n/******************************************************************************/\n/*\n * Begin initialization functions.\n */\n\n#ifndef JEMALLOC_HAVE_SECURE_GETENV\nstatic char *\nsecure_getenv(const char *name)\n{\n\n#  ifdef JEMALLOC_HAVE_ISSETUGID\n\tif (issetugid() != 0)\n\t\treturn (NULL);\n#  endif\n\treturn (getenv(name));\n}\n#endif\n\nstatic unsigned\nmalloc_ncpus(void)\n{\n\tlong result;\n\n#ifdef _WIN32\n\tSYSTEM_INFO si;\n\tGetSystemInfo(&si);\n\tresult = si.dwNumberOfProcessors;\n#elif defined(JEMALLOC_GLIBC_MALLOC_HOOK) && defined(CPU_COUNT)\n\t/*\n\t * glibc >= 2.6 has the CPU_COUNT macro.\n\t *\n\t * glibc's sysconf() uses isspace().  glibc allocates for the first time\n\t * *before* setting up the isspace tables.  Therefore we need a\n\t * different method to get the number of CPUs.\n\t */\n\t{\n\t\tcpu_set_t set;\n\n\t\tpthread_getaffinity_np(pthread_self(), sizeof(set), &set);\n\t\tresult = CPU_COUNT(&set);\n\t}\n#else\n\tresult = sysconf(_SC_NPROCESSORS_ONLN);\n#endif\n\treturn ((result == -1) ? 1 : (unsigned)result);\n}\n\nstatic bool\nmalloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,\n    char const **v_p, size_t *vlen_p)\n{\n\tbool accept;\n\tconst char *opts = *opts_p;\n\n\t*k_p = opts;\n\n\tfor (accept = false; !accept;) {\n\t\tswitch (*opts) {\n\t\tcase 'A': case 'B': case 'C': case 'D': case 'E': case 'F':\n\t\tcase 'G': case 'H': case 'I': case 'J': case 'K': case 'L':\n\t\tcase 'M': case 'N': case 'O': case 'P': case 'Q': case 'R':\n\t\tcase 'S': case 'T': case 'U': case 'V': case 'W': case 'X':\n\t\tcase 'Y': case 'Z':\n\t\tcase 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n\t\tcase 'g': case 'h': case 'i': case 'j': case 'k': case 'l':\n\t\tcase 'm': case 'n': case 'o': case 'p': case 'q': case 'r':\n\t\tcase 's': case 't': case 'u': case 'v': case 'w': case 'x':\n\t\tcase 'y': case 'z':\n\t\tcase '0': case '1': case '2': case '3': case '4': case '5':\n\t\tcase '6': case '7': case '8': case '9':\n\t\tcase '_':\n\t\t\topts++;\n\t\t\tbreak;\n\t\tcase ':':\n\t\t\topts++;\n\t\t\t*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;\n\t\t\t*v_p = opts;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\tif (opts != *opts_p) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with key\\n\");\n\t\t\t}\n\t\t\treturn (true);\n\t\tdefault:\n\t\t\tmalloc_write(\"<jemalloc>: Malformed conf string\\n\");\n\t\t\treturn (true);\n\t\t}\n\t}\n\n\tfor (accept = false; !accept;) {\n\t\tswitch (*opts) {\n\t\tcase ',':\n\t\t\topts++;\n\t\t\t/*\n\t\t\t * Look ahead one character here, because the next time\n\t\t\t * this function is called, it will assume that end of\n\t\t\t * input has been cleanly reached if no input remains,\n\t\t\t * but we have optimistically already consumed the\n\t\t\t * comma if one exists.\n\t\t\t */\n\t\t\tif (*opts == '\\0') {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with comma\\n\");\n\t\t\t}\n\t\t\t*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\t*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\topts++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t*opts_p = opts;\n\treturn (false);\n}\n\nstatic void\nmalloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,\n    size_t vlen)\n{\n\n\tmalloc_printf(\"<jemalloc>: %s: %.*s:%.*s\\n\", msg, (int)klen, k,\n\t    (int)vlen, v);\n}\n\nstatic void\nmalloc_slow_flag_init(void)\n{\n\t/*\n\t * Combine the runtime options into malloc_slow for fast path.  Called\n\t * after processing all the options.\n\t */\n\tmalloc_slow_flags |= (opt_junk_alloc ? flag_opt_junk_alloc : 0)\n\t    | (opt_junk_free ? flag_opt_junk_free : 0)\n\t    | (opt_quarantine ? flag_opt_quarantine : 0)\n\t    | (opt_zero ? flag_opt_zero : 0)\n\t    | (opt_utrace ? flag_opt_utrace : 0)\n\t    | (opt_xmalloc ? flag_opt_xmalloc : 0);\n\n\tif (config_valgrind)\n\t\tmalloc_slow_flags |= (in_valgrind ? flag_in_valgrind : 0);\n\n\tmalloc_slow = (malloc_slow_flags != 0);\n}\n\nstatic void\nmalloc_conf_init(void)\n{\n\tunsigned i;\n\tchar buf[PATH_MAX + 1];\n\tconst char *opts, *k, *v;\n\tsize_t klen, vlen;\n\n\t/*\n\t * Automatically configure valgrind before processing options.  The\n\t * valgrind option remains in jemalloc 3.x for compatibility reasons.\n\t */\n\tif (config_valgrind) {\n\t\tin_valgrind = (RUNNING_ON_VALGRIND != 0) ? true : false;\n\t\tif (config_fill && unlikely(in_valgrind)) {\n\t\t\topt_junk = \"false\";\n\t\t\topt_junk_alloc = false;\n\t\t\topt_junk_free = false;\n\t\t\tassert(!opt_zero);\n\t\t\topt_quarantine = JEMALLOC_VALGRIND_QUARANTINE_DEFAULT;\n\t\t\topt_redzone = true;\n\t\t}\n\t\tif (config_tcache && unlikely(in_valgrind))\n\t\t\topt_tcache = false;\n\t}\n\n\tfor (i = 0; i < 4; i++) {\n\t\t/* Get runtime configuration. */\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\topts = config_malloc_conf;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tif (je_malloc_conf != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Use options that were compiled into the\n\t\t\t\t * program.\n\t\t\t\t */\n\t\t\t\topts = je_malloc_conf;\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2: {\n\t\t\tssize_t linklen = 0;\n#ifndef _WIN32\n\t\t\tint saved_errno = errno;\n\t\t\tconst char *linkname =\n#  ifdef JEMALLOC_PREFIX\n\t\t\t    \"/etc/\"JEMALLOC_PREFIX\"malloc.conf\"\n#  else\n\t\t\t    \"/etc/malloc.conf\"\n#  endif\n\t\t\t    ;\n\n\t\t\t/*\n\t\t\t * Try to use the contents of the \"/etc/malloc.conf\"\n\t\t\t * symbolic link's name.\n\t\t\t */\n\t\t\tlinklen = readlink(linkname, buf, sizeof(buf) - 1);\n\t\t\tif (linklen == -1) {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tlinklen = 0;\n\t\t\t\t/* Restore errno. */\n\t\t\t\tset_errno(saved_errno);\n\t\t\t}\n#endif\n\t\t\tbuf[linklen] = '\\0';\n\t\t\topts = buf;\n\t\t\tbreak;\n\t\t} case 3: {\n\t\t\tconst char *envname =\n#ifdef JEMALLOC_PREFIX\n\t\t\t    JEMALLOC_CPREFIX\"MALLOC_CONF\"\n#else\n\t\t\t    \"MALLOC_CONF\"\n#endif\n\t\t\t    ;\n\n\t\t\tif ((opts = secure_getenv(envname)) != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Do nothing; opts is already initialized to\n\t\t\t\t * the value of the MALLOC_CONF environment\n\t\t\t\t * variable.\n\t\t\t\t */\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\t} default:\n\t\t\tnot_reached();\n\t\t\tbuf[0] = '\\0';\n\t\t\topts = buf;\n\t\t}\n\n\t\twhile (*opts != '\\0' && !malloc_conf_next(&opts, &k, &klen, &v,\n\t\t    &vlen)) {\n#define\tCONF_MATCH(n)\t\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == klen && strncmp(n, k, klen) == 0)\n#define\tCONF_MATCH_VALUE(n)\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == vlen && strncmp(n, v, vlen) == 0)\n#define\tCONF_HANDLE_BOOL(o, n, cont)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tif (CONF_MATCH_VALUE(\"true\"))\t\t\\\n\t\t\t\t\to = true;\t\t\t\\\n\t\t\t\telse if (CONF_MATCH_VALUE(\"false\"))\t\\\n\t\t\t\t\to = false;\t\t\t\\\n\t\t\t\telse {\t\t\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tif (cont)\t\t\t\t\\\n\t\t\t\t\tcontinue;\t\t\t\\\n\t\t\t}\n#define\tCONF_MIN_no(um, min)\tfalse\n#define\tCONF_MIN_yes(um, min)\t((um) < (min))\n#define\tCONF_MAX_no(um, max)\tfalse\n#define\tCONF_MAX_yes(um, max)\t((um) > (max))\n#define\tCONF_HANDLE_T_U(t, o, n, min, max, check_min, check_max, clip)\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tuintmax_t um;\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tum = malloc_strtoumax(v, &end, 0);\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (clip) {\t\t\t\\\n\t\t\t\t\tif (CONF_MIN_##check_min(um,\t\\\n\t\t\t\t\t    (min)))\t\t\t\\\n\t\t\t\t\t\to = (t)(min);\t\t\\\n\t\t\t\t\telse if (CONF_MAX_##check_max(\t\\\n\t\t\t\t\t    um, (max)))\t\t\t\\\n\t\t\t\t\t\to = (t)(max);\t\t\\\n\t\t\t\t\telse\t\t\t\t\\\n\t\t\t\t\t\to = (t)um;\t\t\\\n\t\t\t\t} else {\t\t\t\t\\\n\t\t\t\t\tif (CONF_MIN_##check_min(um,\t\\\n\t\t\t\t\t    (min)) ||\t\t\t\\\n\t\t\t\t\t    CONF_MAX_##check_max(um,\t\\\n\t\t\t\t\t    (max))) {\t\t\t\\\n\t\t\t\t\t\tmalloc_conf_error(\t\\\n\t\t\t\t\t\t    \"Out-of-range \"\t\\\n\t\t\t\t\t\t    \"conf value\",\t\\\n\t\t\t\t\t\t    k, klen, v, vlen);\t\\\n\t\t\t\t\t} else\t\t\t\t\\\n\t\t\t\t\t\to = (t)um;\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_UNSIGNED(o, n, min, max, check_min, check_max,\t\\\n    clip)\t\t\t\t\t\t\t\t\\\n\t\t\tCONF_HANDLE_T_U(unsigned, o, n, min, max,\t\\\n\t\t\t    check_min, check_max, clip)\n#define\tCONF_HANDLE_SIZE_T(o, n, min, max, check_min, check_max, clip)\t\\\n\t\t\tCONF_HANDLE_T_U(size_t, o, n, min, max,\t\t\\\n\t\t\t    check_min, check_max, clip)\n#define\tCONF_HANDLE_SSIZE_T(o, n, min, max)\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tlong l;\t\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tl = strtol(v, &end, 0);\t\t\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (l < (ssize_t)(min) || l >\t\\\n\t\t\t\t    (ssize_t)(max)) {\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Out-of-range conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else\t\t\t\t\t\\\n\t\t\t\t\to = l;\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_CHAR_P(o, n, d)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tsize_t cpylen = (vlen <=\t\t\\\n\t\t\t\t    sizeof(o)-1) ? vlen :\t\t\\\n\t\t\t\t    sizeof(o)-1;\t\t\t\\\n\t\t\t\tstrncpy(o, v, cpylen);\t\t\t\\\n\t\t\t\to[cpylen] = '\\0';\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n\n\t\t\tCONF_HANDLE_BOOL(opt_abort, \"abort\", true)\n\t\t\t/*\n\t\t\t * Chunks always require at least one header page,\n\t\t\t * as many as 2^(LG_SIZE_CLASS_GROUP+1) data pages, and\n\t\t\t * possibly an additional page in the presence of\n\t\t\t * redzones.  In order to simplify options processing,\n\t\t\t * use a conservative bound that accommodates all these\n\t\t\t * constraints.\n\t\t\t */\n\t\t\tCONF_HANDLE_SIZE_T(opt_lg_chunk, \"lg_chunk\", LG_PAGE +\n\t\t\t    LG_SIZE_CLASS_GROUP + (config_fill ? 2 : 1),\n\t\t\t    (sizeof(size_t) << 3) - 1, yes, yes, true)\n\t\t\tif (strncmp(\"dss\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < dss_prec_limit; i++) {\n\t\t\t\t\tif (strncmp(dss_prec_names[i], v, vlen)\n\t\t\t\t\t    == 0) {\n\t\t\t\t\t\tif (chunk_dss_prec_set(i)) {\n\t\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\t    \"Error setting dss\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_dss =\n\t\t\t\t\t\t\t    dss_prec_names[i];\n\t\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tmalloc_conf_error(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tCONF_HANDLE_UNSIGNED(opt_narenas, \"narenas\", 1,\n\t\t\t    UINT_MAX, yes, no, false)\n\t\t\tif (strncmp(\"purge\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < purge_mode_limit; i++) {\n\t\t\t\t\tif (strncmp(purge_mode_names[i], v,\n\t\t\t\t\t    vlen) == 0) {\n\t\t\t\t\t\topt_purge = (purge_mode_t)i;\n\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tmalloc_conf_error(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_dirty_mult, \"lg_dirty_mult\",\n\t\t\t    -1, (sizeof(size_t) << 3) - 1)\n\t\t\tCONF_HANDLE_SSIZE_T(opt_decay_time, \"decay_time\", -1,\n\t\t\t    NSTIME_SEC_MAX);\n\t\t\tCONF_HANDLE_BOOL(opt_stats_print, \"stats_print\", true)\n\t\t\tif (config_fill) {\n\t\t\t\tif (CONF_MATCH(\"junk\")) {\n\t\t\t\t\tif (CONF_MATCH_VALUE(\"true\")) {\n\t\t\t\t\t\tif (config_valgrind &&\n\t\t\t\t\t\t    unlikely(in_valgrind)) {\n\t\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\t\"Deallocation-time \"\n\t\t\t\t\t\t\t\"junk filling cannot \"\n\t\t\t\t\t\t\t\"be enabled while \"\n\t\t\t\t\t\t\t\"running inside \"\n\t\t\t\t\t\t\t\"Valgrind\", k, klen, v,\n\t\t\t\t\t\t\tvlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_junk = \"true\";\n\t\t\t\t\t\t\topt_junk_alloc = true;\n\t\t\t\t\t\t\topt_junk_free = true;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"false\")) {\n\t\t\t\t\t\topt_junk = \"false\";\n\t\t\t\t\t\topt_junk_alloc = opt_junk_free =\n\t\t\t\t\t\t    false;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"alloc\")) {\n\t\t\t\t\t\topt_junk = \"alloc\";\n\t\t\t\t\t\topt_junk_alloc = true;\n\t\t\t\t\t\topt_junk_free = false;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"free\")) {\n\t\t\t\t\t\tif (config_valgrind &&\n\t\t\t\t\t\t    unlikely(in_valgrind)) {\n\t\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\t\"Deallocation-time \"\n\t\t\t\t\t\t\t\"junk filling cannot \"\n\t\t\t\t\t\t\t\"be enabled while \"\n\t\t\t\t\t\t\t\"running inside \"\n\t\t\t\t\t\t\t\"Valgrind\", k, klen, v,\n\t\t\t\t\t\t\tvlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_junk = \"free\";\n\t\t\t\t\t\t\topt_junk_alloc = false;\n\t\t\t\t\t\t\topt_junk_free = true;\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t    \"Invalid conf value\", k,\n\t\t\t\t\t\t    klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_quarantine, \"quarantine\",\n\t\t\t\t    0, SIZE_T_MAX, no, no, false)\n\t\t\t\tCONF_HANDLE_BOOL(opt_redzone, \"redzone\", true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_zero, \"zero\", true)\n\t\t\t}\n\t\t\tif (config_utrace) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_utrace, \"utrace\", true)\n\t\t\t}\n\t\t\tif (config_xmalloc) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_xmalloc, \"xmalloc\", true)\n\t\t\t}\n\t\t\tif (config_tcache) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_tcache, \"tcache\",\n\t\t\t\t    !config_valgrind || !in_valgrind)\n\t\t\t\tif (CONF_MATCH(\"tcache\")) {\n\t\t\t\t\tassert(config_valgrind && in_valgrind);\n\t\t\t\t\tif (opt_tcache) {\n\t\t\t\t\t\topt_tcache = false;\n\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\"tcache cannot be enabled \"\n\t\t\t\t\t\t\"while running inside Valgrind\",\n\t\t\t\t\t\tk, klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_tcache_max,\n\t\t\t\t    \"lg_tcache_max\", -1,\n\t\t\t\t    (sizeof(size_t) << 3) - 1)\n\t\t\t}\n\t\t\tif (config_prof) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof, \"prof\", true)\n\t\t\t\tCONF_HANDLE_CHAR_P(opt_prof_prefix,\n\t\t\t\t    \"prof_prefix\", \"jeprof\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_active, \"prof_active\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_thread_active_init,\n\t\t\t\t    \"prof_thread_active_init\", true)\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_lg_prof_sample,\n\t\t\t\t    \"lg_prof_sample\", 0, (sizeof(uint64_t) << 3)\n\t\t\t\t    - 1, no, yes, true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_accum, \"prof_accum\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_interval,\n\t\t\t\t    \"lg_prof_interval\", -1,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_gdump, \"prof_gdump\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_final, \"prof_final\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_leak, \"prof_leak\",\n\t\t\t\t    true)\n\t\t\t}\n\t\t\tmalloc_conf_error(\"Invalid conf pair\", k, klen, v,\n\t\t\t    vlen);\n#undef CONF_MATCH\n#undef CONF_MATCH_VALUE\n#undef CONF_HANDLE_BOOL\n#undef CONF_MIN_no\n#undef CONF_MIN_yes\n#undef CONF_MAX_no\n#undef CONF_MAX_yes\n#undef CONF_HANDLE_T_U\n#undef CONF_HANDLE_UNSIGNED\n#undef CONF_HANDLE_SIZE_T\n#undef CONF_HANDLE_SSIZE_T\n#undef CONF_HANDLE_CHAR_P\n\t\t}\n\t}\n}\n\nstatic bool\nmalloc_init_hard_needed(void)\n{\n\n\tif (malloc_initialized() || (IS_INITIALIZER && malloc_init_state ==\n\t    malloc_init_recursible)) {\n\t\t/*\n\t\t * Another thread initialized the allocator before this one\n\t\t * acquired init_lock, or this thread is the initializing\n\t\t * thread, and it is recursively allocating.\n\t\t */\n\t\treturn (false);\n\t}\n#ifdef JEMALLOC_THREADED_INIT\n\tif (malloc_initializer != NO_INITIALIZER && !IS_INITIALIZER) {\n\t\tspin_t spinner;\n\n\t\t/* Busy-wait until the initializing thread completes. */\n\t\tspin_init(&spinner);\n\t\tdo {\n\t\t\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\t\t\tspin_adaptive(&spinner);\n\t\t\tmalloc_mutex_lock(TSDN_NULL, &init_lock);\n\t\t} while (!malloc_initialized());\n\t\treturn (false);\n\t}\n#endif\n\treturn (true);\n}\n\nstatic bool\nmalloc_init_hard_a0_locked()\n{\n\n\tmalloc_initializer = INITIALIZER;\n\n\tif (config_prof)\n\t\tprof_boot0();\n\tmalloc_conf_init();\n\tif (opt_stats_print) {\n\t\t/* Print statistics at exit. */\n\t\tif (atexit(stats_print_atexit) != 0) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in atexit()\\n\");\n\t\t\tif (opt_abort)\n\t\t\t\tabort();\n\t\t}\n\t}\n\tpages_boot();\n\tif (base_boot())\n\t\treturn (true);\n\tif (chunk_boot())\n\t\treturn (true);\n\tif (ctl_boot())\n\t\treturn (true);\n\tif (config_prof)\n\t\tprof_boot1();\n\tarena_boot();\n\tif (config_tcache && tcache_boot(TSDN_NULL))\n\t\treturn (true);\n\tif (malloc_mutex_init(&arenas_lock, \"arenas\", WITNESS_RANK_ARENAS))\n\t\treturn (true);\n\t/*\n\t * Create enough scaffolding to allow recursive allocation in\n\t * malloc_ncpus().\n\t */\n\tnarenas_auto = 1;\n\tnarenas_total_set(narenas_auto);\n\tarenas = &a0;\n\tmemset(arenas, 0, sizeof(arena_t *) * narenas_auto);\n\t/*\n\t * Initialize one arena here.  The rest are lazily created in\n\t * arena_choose_hard().\n\t */\n\tif (arena_init(TSDN_NULL, 0) == NULL)\n\t\treturn (true);\n\n\tmalloc_init_state = malloc_init_a0_initialized;\n\n\treturn (false);\n}\n\nstatic bool\nmalloc_init_hard_a0(void)\n{\n\tbool ret;\n\n\tmalloc_mutex_lock(TSDN_NULL, &init_lock);\n\tret = malloc_init_hard_a0_locked();\n\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\treturn (ret);\n}\n\n/* Initialize data structures which may trigger recursive allocation. */\nstatic bool\nmalloc_init_hard_recursible(void)\n{\n\n\tmalloc_init_state = malloc_init_recursible;\n\n\tncpus = malloc_ncpus();\n\n#if (defined(JEMALLOC_HAVE_PTHREAD_ATFORK) && !defined(JEMALLOC_MUTEX_INIT_CB) \\\n    && !defined(JEMALLOC_ZONE) && !defined(_WIN32) && \\\n    !defined(__native_client__))\n\t/* LinuxThreads' pthread_atfork() allocates. */\n\tif (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,\n\t    jemalloc_postfork_child) != 0) {\n\t\tmalloc_write(\"<jemalloc>: Error in pthread_atfork()\\n\");\n\t\tif (opt_abort)\n\t\t\tabort();\n\t\treturn (true);\n\t}\n#endif\n\n\treturn (false);\n}\n\nstatic bool\nmalloc_init_hard_finish(tsdn_t *tsdn)\n{\n\n\tif (malloc_mutex_boot())\n\t\treturn (true);\n\n\tif (opt_narenas == 0) {\n\t\t/*\n\t\t * For SMP systems, create more than one arena per CPU by\n\t\t * default.\n\t\t */\n\t\tif (ncpus > 1)\n\t\t\topt_narenas = ncpus << 2;\n\t\telse\n\t\t\topt_narenas = 1;\n\t}\n\tnarenas_auto = opt_narenas;\n\t/*\n\t * Limit the number of arenas to the indexing range of MALLOCX_ARENA().\n\t */\n\tif (narenas_auto > MALLOCX_ARENA_MAX) {\n\t\tnarenas_auto = MALLOCX_ARENA_MAX;\n\t\tmalloc_printf(\"<jemalloc>: Reducing narenas to limit (%d)\\n\",\n\t\t    narenas_auto);\n\t}\n\tnarenas_total_set(narenas_auto);\n\n\t/* Allocate and initialize arenas. */\n\tarenas = (arena_t **)base_alloc(tsdn, sizeof(arena_t *) *\n\t    (MALLOCX_ARENA_MAX+1));\n\tif (arenas == NULL)\n\t\treturn (true);\n\t/* Copy the pointer to the one arena that was already initialized. */\n\tarena_set(0, a0);\n\n\tmalloc_init_state = malloc_init_initialized;\n\tmalloc_slow_flag_init();\n\n\treturn (false);\n}\n\nstatic bool\nmalloc_init_hard(void)\n{\n\ttsd_t *tsd;\n\n#if defined(_WIN32) && _WIN32_WINNT < 0x0600\n\t_init_init_lock();\n#endif\n\tmalloc_mutex_lock(TSDN_NULL, &init_lock);\n\tif (!malloc_init_hard_needed()) {\n\t\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\t\treturn (false);\n\t}\n\n\tif (malloc_init_state != malloc_init_a0_initialized &&\n\t    malloc_init_hard_a0_locked()) {\n\t\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\t\treturn (true);\n\t}\n\n\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\t/* Recursive allocation relies on functional tsd. */\n\ttsd = malloc_tsd_boot0();\n\tif (tsd == NULL)\n\t\treturn (true);\n\tif (malloc_init_hard_recursible())\n\t\treturn (true);\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &init_lock);\n\n\tif (config_prof && prof_boot2(tsd)) {\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);\n\t\treturn (true);\n\t}\n\n\tif (malloc_init_hard_finish(tsd_tsdn(tsd))) {\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);\n\t\treturn (true);\n\t}\n\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);\n\tmalloc_tsd_boot1();\n\treturn (false);\n}\n\n/*\n * End initialization functions.\n */\n/******************************************************************************/\n/*\n * Begin malloc(3)-compatible functions.\n */\n\nstatic void *\nialloc_prof_sample(tsd_t *tsd, size_t usize, szind_t ind, bool zero,\n    prof_tctx_t *tctx, bool slow_path)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tszind_t ind_large = size2index(LARGE_MINCLASS);\n\t\tp = ialloc(tsd, LARGE_MINCLASS, ind_large, zero, slow_path);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(tsd_tsdn(tsd), p, usize);\n\t} else\n\t\tp = ialloc(tsd, usize, ind, zero, slow_path);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nialloc_prof(tsd_t *tsd, size_t usize, szind_t ind, bool zero, bool slow_path)\n{\n\tvoid *p;\n\tprof_tctx_t *tctx;\n\n\ttctx = prof_alloc_prep(tsd, usize, prof_active_get_unlocked(), true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = ialloc_prof_sample(tsd, usize, ind, zero, tctx, slow_path);\n\telse\n\t\tp = ialloc(tsd, usize, ind, zero, slow_path);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(tsd_tsdn(tsd), p, usize, tctx);\n\n\treturn (p);\n}\n\n/*\n * ialloc_body() is inlined so that fast and slow paths are generated separately\n * with statically known slow_path.\n *\n * This function guarantees that *tsdn is non-NULL on success.\n */\nJEMALLOC_ALWAYS_INLINE_C void *\nialloc_body(size_t size, bool zero, tsdn_t **tsdn, size_t *usize,\n    bool slow_path)\n{\n\ttsd_t *tsd;\n\tszind_t ind;\n\n\tif (slow_path && unlikely(malloc_init())) {\n\t\t*tsdn = NULL;\n\t\treturn (NULL);\n\t}\n\n\ttsd = tsd_fetch();\n\t*tsdn = tsd_tsdn(tsd);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\tind = size2index(size);\n\tif (unlikely(ind >= NSIZES))\n\t\treturn (NULL);\n\n\tif (config_stats || (config_prof && opt_prof) || (slow_path &&\n\t    config_valgrind && unlikely(in_valgrind))) {\n\t\t*usize = index2size(ind);\n\t\tassert(*usize > 0 && *usize <= HUGE_MAXCLASS);\n\t}\n\n\tif (config_prof && opt_prof)\n\t\treturn (ialloc_prof(tsd, *usize, ind, zero, slow_path));\n\n\treturn (ialloc(tsd, size, ind, zero, slow_path));\n}\n\nJEMALLOC_ALWAYS_INLINE_C void\nialloc_post_check(void *ret, tsdn_t *tsdn, size_t usize, const char *func,\n    bool update_errno, bool slow_path)\n{\n\n\tassert(!tsdn_null(tsdn) || ret == NULL);\n\n\tif (unlikely(ret == NULL)) {\n\t\tif (slow_path && config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_printf(\"<jemalloc>: Error in %s(): out of \"\n\t\t\t    \"memory\\n\", func);\n\t\t\tabort();\n\t\t}\n\t\tif (update_errno)\n\t\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && likely(ret != NULL)) {\n\t\tassert(usize == isalloc(tsdn, ret, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsdn_tsd(tsdn)) += usize;\n\t}\n\twitness_assert_lockless(tsdn);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)\nje_malloc(size_t size)\n{\n\tvoid *ret;\n\ttsdn_t *tsdn;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tif (likely(!malloc_slow)) {\n\t\tret = ialloc_body(size, false, &tsdn, &usize, false);\n\t\tialloc_post_check(ret, tsdn, usize, \"malloc\", true, false);\n\t} else {\n\t\tret = ialloc_body(size, false, &tsdn, &usize, true);\n\t\tialloc_post_check(ret, tsdn, usize, \"malloc\", true, true);\n\t\tUTRACE(0, size, ret);\n\t\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, tsdn, ret, usize, false);\n\t}\n\n\treturn (ret);\n}\n\nstatic void *\nimemalign_prof_sample(tsd_t *tsd, size_t alignment, size_t usize,\n    prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tassert(sa2u(LARGE_MINCLASS, alignment) == LARGE_MINCLASS);\n\t\tp = ipalloc(tsd, LARGE_MINCLASS, alignment, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(tsd_tsdn(tsd), p, usize);\n\t} else\n\t\tp = ipalloc(tsd, usize, alignment, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimemalign_prof(tsd_t *tsd, size_t alignment, size_t usize)\n{\n\tvoid *p;\n\tprof_tctx_t *tctx;\n\n\ttctx = prof_alloc_prep(tsd, usize, prof_active_get_unlocked(), true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = imemalign_prof_sample(tsd, alignment, usize, tctx);\n\telse\n\t\tp = ipalloc(tsd, usize, alignment, false);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(tsd_tsdn(tsd), p, usize, tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_ATTR(nonnull(1))\nstatic int\nimemalign(void **memptr, size_t alignment, size_t size, size_t min_alignment)\n{\n\tint ret;\n\ttsd_t *tsd;\n\tsize_t usize;\n\tvoid *result;\n\n\tassert(min_alignment != 0);\n\n\tif (unlikely(malloc_init())) {\n\t\ttsd = NULL;\n\t\tresult = NULL;\n\t\tgoto label_oom;\n\t}\n\ttsd = tsd_fetch();\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\tif (size == 0)\n\t\tsize = 1;\n\n\t/* Make sure that alignment is a large enough power of 2. */\n\tif (unlikely(((alignment - 1) & alignment) != 0\n\t    || (alignment < min_alignment))) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error allocating \"\n\t\t\t    \"aligned memory: invalid alignment\\n\");\n\t\t\tabort();\n\t\t}\n\t\tresult = NULL;\n\t\tret = EINVAL;\n\t\tgoto label_return;\n\t}\n\n\tusize = sa2u(size, alignment);\n\tif (unlikely(usize == 0 || usize > HUGE_MAXCLASS)) {\n\t\tresult = NULL;\n\t\tgoto label_oom;\n\t}\n\n\tif (config_prof && opt_prof)\n\t\tresult = imemalign_prof(tsd, alignment, usize);\n\telse\n\t\tresult = ipalloc(tsd, usize, alignment, false);\n\tif (unlikely(result == NULL))\n\t\tgoto label_oom;\n\tassert(((uintptr_t)result & (alignment - 1)) == ZU(0));\n\n\t*memptr = result;\n\tret = 0;\nlabel_return:\n\tif (config_stats && likely(result != NULL)) {\n\t\tassert(usize == isalloc(tsd_tsdn(tsd), result, config_prof));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t}\n\tUTRACE(0, size, result);\n\tJEMALLOC_VALGRIND_MALLOC(result != NULL, tsd_tsdn(tsd), result, usize,\n\t    false);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\treturn (ret);\nlabel_oom:\n\tassert(result == NULL);\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(\"<jemalloc>: Error allocating aligned memory: \"\n\t\t    \"out of memory\\n\");\n\t\tabort();\n\t}\n\tret = ENOMEM;\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\tgoto label_return;\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nJEMALLOC_ATTR(nonnull(1))\nje_posix_memalign(void **memptr, size_t alignment, size_t size)\n{\n\tint ret;\n\n\tret = imemalign(memptr, alignment, size, sizeof(void *));\n\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(2)\nje_aligned_alloc(size_t alignment, size_t size)\n{\n\tvoid *ret;\n\tint err;\n\n\tif (unlikely((err = imemalign(&ret, alignment, size, 1)) != 0)) {\n\t\tret = NULL;\n\t\tset_errno(err);\n\t}\n\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE2(1, 2)\nje_calloc(size_t num, size_t size)\n{\n\tvoid *ret;\n\ttsdn_t *tsdn;\n\tsize_t num_size;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tnum_size = num * size;\n\tif (unlikely(num_size == 0)) {\n\t\tif (num == 0 || size == 0)\n\t\t\tnum_size = 1;\n\t\telse\n\t\t\tnum_size = HUGE_MAXCLASS + 1; /* Trigger OOM. */\n\t/*\n\t * Try to avoid division here.  We know that it isn't possible to\n\t * overflow during multiplication if neither operand uses any of the\n\t * most significant half of the bits in a size_t.\n\t */\n\t} else if (unlikely(((num | size) & (SIZE_T_MAX << (sizeof(size_t) <<\n\t    2))) && (num_size / size != num)))\n\t\tnum_size = HUGE_MAXCLASS + 1; /* size_t overflow. */\n\n\tif (likely(!malloc_slow)) {\n\t\tret = ialloc_body(num_size, true, &tsdn, &usize, false);\n\t\tialloc_post_check(ret, tsdn, usize, \"calloc\", true, false);\n\t} else {\n\t\tret = ialloc_body(num_size, true, &tsdn, &usize, true);\n\t\tialloc_post_check(ret, tsdn, usize, \"calloc\", true, true);\n\t\tUTRACE(0, num_size, ret);\n\t\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, tsdn, ret, usize, true);\n\t}\n\n\treturn (ret);\n}\n\nstatic void *\nirealloc_prof_sample(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize,\n    prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = iralloc(tsd, old_ptr, old_usize, LARGE_MINCLASS, 0, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(tsd_tsdn(tsd), p, usize);\n\t} else\n\t\tp = iralloc(tsd, old_ptr, old_usize, usize, 0, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nirealloc_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize)\n{\n\tvoid *p;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(tsd_tsdn(tsd), old_ptr);\n\ttctx = prof_alloc_prep(tsd, usize, prof_active, true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U))\n\t\tp = irealloc_prof_sample(tsd, old_ptr, old_usize, usize, tctx);\n\telse\n\t\tp = iralloc(tsd, old_ptr, old_usize, usize, 0, false);\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_realloc(tsd, p, usize, tctx, prof_active, true, old_ptr, old_usize,\n\t    old_tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_INLINE_C void\nifree(tsd_t *tsd, void *ptr, tcache_t *tcache, bool slow_path)\n{\n\tsize_t usize;\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = isalloc(tsd_tsdn(tsd), ptr, config_prof);\n\t\tprof_free(tsd, ptr, usize);\n\t} else if (config_stats || config_valgrind)\n\t\tusize = isalloc(tsd_tsdn(tsd), ptr, config_prof);\n\tif (config_stats)\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\n\tif (likely(!slow_path))\n\t\tiqalloc(tsd, ptr, tcache, false);\n\telse {\n\t\tif (config_valgrind && unlikely(in_valgrind))\n\t\t\trzsize = p2rz(tsd_tsdn(tsd), ptr);\n\t\tiqalloc(tsd, ptr, tcache, true);\n\t\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n\t}\n}\n\nJEMALLOC_INLINE_C void\nisfree(tsd_t *tsd, void *ptr, size_t usize, tcache_t *tcache, bool slow_path)\n{\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\tif (config_prof && opt_prof)\n\t\tprof_free(tsd, ptr, usize);\n\tif (config_stats)\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\trzsize = p2rz(tsd_tsdn(tsd), ptr);\n\tisqalloc(tsd, ptr, usize, tcache, slow_path);\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ALLOC_SIZE(2)\nje_realloc(void *ptr, size_t size)\n{\n\tvoid *ret;\n\ttsdn_t *tsdn JEMALLOC_CC_SILENCE_INIT(NULL);\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t old_usize = 0;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (unlikely(size == 0)) {\n\t\tif (ptr != NULL) {\n\t\t\ttsd_t *tsd;\n\n\t\t\t/* realloc(ptr, 0) is equivalent to free(ptr). */\n\t\t\tUTRACE(ptr, 0, 0);\n\t\t\ttsd = tsd_fetch();\n\t\t\tifree(tsd, ptr, tcache_get(tsd, false), true);\n\t\t\treturn (NULL);\n\t\t}\n\t\tsize = 1;\n\t}\n\n\tif (likely(ptr != NULL)) {\n\t\ttsd_t *tsd;\n\n\t\tassert(malloc_initialized() || IS_INITIALIZER);\n\t\tmalloc_thread_init();\n\t\ttsd = tsd_fetch();\n\n\t\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\t\told_usize = isalloc(tsd_tsdn(tsd), ptr, config_prof);\n\t\tif (config_valgrind && unlikely(in_valgrind)) {\n\t\t\told_rzsize = config_prof ? p2rz(tsd_tsdn(tsd), ptr) :\n\t\t\t    u2rz(old_usize);\n\t\t}\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tusize = s2u(size);\n\t\t\tret = unlikely(usize == 0 || usize > HUGE_MAXCLASS) ?\n\t\t\t    NULL : irealloc_prof(tsd, ptr, old_usize, usize);\n\t\t} else {\n\t\t\tif (config_stats || (config_valgrind &&\n\t\t\t    unlikely(in_valgrind)))\n\t\t\t\tusize = s2u(size);\n\t\t\tret = iralloc(tsd, ptr, old_usize, size, 0, false);\n\t\t}\n\t\ttsdn = tsd_tsdn(tsd);\n\t} else {\n\t\t/* realloc(NULL, size) is equivalent to malloc(size). */\n\t\tif (likely(!malloc_slow))\n\t\t\tret = ialloc_body(size, false, &tsdn, &usize, false);\n\t\telse\n\t\t\tret = ialloc_body(size, false, &tsdn, &usize, true);\n\t\tassert(!tsdn_null(tsdn) || ret == NULL);\n\t}\n\n\tif (unlikely(ret == NULL)) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in realloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && likely(ret != NULL)) {\n\t\ttsd_t *tsd;\n\n\t\tassert(usize == isalloc(tsdn, ret, config_prof));\n\t\ttsd = tsdn_tsd(tsdn);\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tUTRACE(ptr, size, ret);\n\tJEMALLOC_VALGRIND_REALLOC(maybe, tsdn, ret, usize, maybe, ptr,\n\t    old_usize, old_rzsize, maybe, false);\n\twitness_assert_lockless(tsdn);\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_free(void *ptr)\n{\n\n\tUTRACE(ptr, 0, 0);\n\tif (likely(ptr != NULL)) {\n\t\ttsd_t *tsd = tsd_fetch();\n\t\twitness_assert_lockless(tsd_tsdn(tsd));\n\t\tif (likely(!malloc_slow))\n\t\t\tifree(tsd, ptr, tcache_get(tsd, false), false);\n\t\telse\n\t\t\tifree(tsd, ptr, tcache_get(tsd, false), true);\n\t\twitness_assert_lockless(tsd_tsdn(tsd));\n\t}\n}\n\n/*\n * End malloc(3)-compatible functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard override functions.\n */\n\n#ifdef JEMALLOC_OVERRIDE_MEMALIGN\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc)\nje_memalign(size_t alignment, size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\tif (unlikely(imemalign(&ret, alignment, size, 1) != 0))\n\t\tret = NULL;\n\treturn (ret);\n}\n#endif\n\n#ifdef JEMALLOC_OVERRIDE_VALLOC\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc)\nje_valloc(size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\tif (unlikely(imemalign(&ret, PAGE, size, 1) != 0))\n\t\tret = NULL;\n\treturn (ret);\n}\n#endif\n\n/*\n * is_malloc(je_malloc) is some macro magic to detect if jemalloc_defs.h has\n * #define je_malloc malloc\n */\n#define\tmalloc_is_malloc 1\n#define\tis_malloc_(a) malloc_is_ ## a\n#define\tis_malloc(a) is_malloc_(a)\n\n#if ((is_malloc(je_malloc) == 1) && defined(JEMALLOC_GLIBC_MALLOC_HOOK))\n/*\n * glibc provides the RTLD_DEEPBIND flag for dlopen which can make it possible\n * to inconsistently reference libc's malloc(3)-compatible functions\n * (https://bugzilla.mozilla.org/show_bug.cgi?id=493541).\n *\n * These definitions interpose hooks in glibc.  The functions are actually\n * passed an extra argument for the caller return address, which will be\n * ignored.\n */\nJEMALLOC_EXPORT void (*__free_hook)(void *ptr) = je_free;\nJEMALLOC_EXPORT void *(*__malloc_hook)(size_t size) = je_malloc;\nJEMALLOC_EXPORT void *(*__realloc_hook)(void *ptr, size_t size) = je_realloc;\n# ifdef JEMALLOC_GLIBC_MEMALIGN_HOOK\nJEMALLOC_EXPORT void *(*__memalign_hook)(size_t alignment, size_t size) =\n    je_memalign;\n# endif\n\n#ifdef CPU_COUNT\n/*\n * To enable static linking with glibc, the libc specific malloc interface must\n * be implemented also, so none of glibc's malloc.o functions are added to the\n * link.\n */\n#define\tALIAS(je_fn)\t__attribute__((alias (#je_fn), used))\n/* To force macro expansion of je_ prefix before stringification. */\n#define\tPREALIAS(je_fn)  ALIAS(je_fn)\nvoid\t*__libc_malloc(size_t size) PREALIAS(je_malloc);\nvoid\t__libc_free(void* ptr) PREALIAS(je_free);\nvoid\t*__libc_realloc(void* ptr, size_t size) PREALIAS(je_realloc);\nvoid\t*__libc_calloc(size_t n, size_t size) PREALIAS(je_calloc);\nvoid\t*__libc_memalign(size_t align, size_t s) PREALIAS(je_memalign);\nvoid\t*__libc_valloc(size_t size) PREALIAS(je_valloc);\nint\t__posix_memalign(void** r, size_t a, size_t s)\n    PREALIAS(je_posix_memalign);\n#undef PREALIAS\n#undef ALIAS\n\n#endif\n\n#endif\n\n/*\n * End non-standard override functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard functions.\n */\n\nJEMALLOC_ALWAYS_INLINE_C bool\nimallocx_flags_decode(tsd_t *tsd, size_t size, int flags, size_t *usize,\n    size_t *alignment, bool *zero, tcache_t **tcache, arena_t **arena)\n{\n\n\tif ((flags & MALLOCX_LG_ALIGN_MASK) == 0) {\n\t\t*alignment = 0;\n\t\t*usize = s2u(size);\n\t} else {\n\t\t*alignment = MALLOCX_ALIGN_GET_SPECIFIED(flags);\n\t\t*usize = sa2u(size, *alignment);\n\t}\n\tif (unlikely(*usize == 0 || *usize > HUGE_MAXCLASS))\n\t\treturn (true);\n\t*zero = MALLOCX_ZERO_GET(flags);\n\tif ((flags & MALLOCX_TCACHE_MASK) != 0) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\t*tcache = NULL;\n\t\telse\n\t\t\t*tcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\t*tcache = tcache_get(tsd, true);\n\tif ((flags & MALLOCX_ARENA_MASK) != 0) {\n\t\tunsigned arena_ind = MALLOCX_ARENA_GET(flags);\n\t\t*arena = arena_get(tsd_tsdn(tsd), arena_ind, true);\n\t\tif (unlikely(*arena == NULL))\n\t\t\treturn (true);\n\t} else\n\t\t*arena = NULL;\n\treturn (false);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_flags(tsdn_t *tsdn, size_t usize, size_t alignment, bool zero,\n    tcache_t *tcache, arena_t *arena, bool slow_path)\n{\n\tszind_t ind;\n\n\tif (unlikely(alignment != 0))\n\t\treturn (ipalloct(tsdn, usize, alignment, zero, tcache, arena));\n\tind = size2index(usize);\n\tassert(ind < NSIZES);\n\treturn (iallocztm(tsdn, usize, ind, zero, tcache, false, arena,\n\t    slow_path));\n}\n\nstatic void *\nimallocx_prof_sample(tsdn_t *tsdn, size_t usize, size_t alignment, bool zero,\n    tcache_t *tcache, arena_t *arena, bool slow_path)\n{\n\tvoid *p;\n\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tassert(((alignment == 0) ? s2u(LARGE_MINCLASS) :\n\t\t    sa2u(LARGE_MINCLASS, alignment)) == LARGE_MINCLASS);\n\t\tp = imallocx_flags(tsdn, LARGE_MINCLASS, alignment, zero,\n\t\t    tcache, arena, slow_path);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(tsdn, p, usize);\n\t} else {\n\t\tp = imallocx_flags(tsdn, usize, alignment, zero, tcache, arena,\n\t\t    slow_path);\n\t}\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_prof(tsd_t *tsd, size_t size, int flags, size_t *usize, bool slow_path)\n{\n\tvoid *p;\n\tsize_t alignment;\n\tbool zero;\n\ttcache_t *tcache;\n\tarena_t *arena;\n\tprof_tctx_t *tctx;\n\n\tif (unlikely(imallocx_flags_decode(tsd, size, flags, usize, &alignment,\n\t    &zero, &tcache, &arena)))\n\t\treturn (NULL);\n\ttctx = prof_alloc_prep(tsd, *usize, prof_active_get_unlocked(), true);\n\tif (likely((uintptr_t)tctx == (uintptr_t)1U)) {\n\t\tp = imallocx_flags(tsd_tsdn(tsd), *usize, alignment, zero,\n\t\t    tcache, arena, slow_path);\n\t} else if ((uintptr_t)tctx > (uintptr_t)1U) {\n\t\tp = imallocx_prof_sample(tsd_tsdn(tsd), *usize, alignment, zero,\n\t\t    tcache, arena, slow_path);\n\t} else\n\t\tp = NULL;\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn (NULL);\n\t}\n\tprof_malloc(tsd_tsdn(tsd), p, *usize, tctx);\n\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_no_prof(tsd_t *tsd, size_t size, int flags, size_t *usize,\n    bool slow_path)\n{\n\tvoid *p;\n\tsize_t alignment;\n\tbool zero;\n\ttcache_t *tcache;\n\tarena_t *arena;\n\n\tif (unlikely(imallocx_flags_decode(tsd, size, flags, usize, &alignment,\n\t    &zero, &tcache, &arena)))\n\t\treturn (NULL);\n\tp = imallocx_flags(tsd_tsdn(tsd), *usize, alignment, zero, tcache,\n\t    arena, slow_path);\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\treturn (p);\n}\n\n/* This function guarantees that *tsdn is non-NULL on success. */\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_body(size_t size, int flags, tsdn_t **tsdn, size_t *usize,\n    bool slow_path)\n{\n\ttsd_t *tsd;\n\n\tif (slow_path && unlikely(malloc_init())) {\n\t\t*tsdn = NULL;\n\t\treturn (NULL);\n\t}\n\n\ttsd = tsd_fetch();\n\t*tsdn = tsd_tsdn(tsd);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\tif (likely(flags == 0)) {\n\t\tszind_t ind = size2index(size);\n\t\tif (unlikely(ind >= NSIZES))\n\t\t\treturn (NULL);\n\t\tif (config_stats || (config_prof && opt_prof) || (slow_path &&\n\t\t    config_valgrind && unlikely(in_valgrind))) {\n\t\t\t*usize = index2size(ind);\n\t\t\tassert(*usize > 0 && *usize <= HUGE_MAXCLASS);\n\t\t}\n\n\t\tif (config_prof && opt_prof) {\n\t\t\treturn (ialloc_prof(tsd, *usize, ind, false,\n\t\t\t    slow_path));\n\t\t}\n\n\t\treturn (ialloc(tsd, size, ind, false, slow_path));\n\t}\n\n\tif (config_prof && opt_prof)\n\t\treturn (imallocx_prof(tsd, size, flags, usize, slow_path));\n\n\treturn (imallocx_no_prof(tsd, size, flags, usize, slow_path));\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)\nje_mallocx(size_t size, int flags)\n{\n\ttsdn_t *tsdn;\n\tvoid *p;\n\tsize_t usize;\n\n\tassert(size != 0);\n\n\tif (likely(!malloc_slow)) {\n\t\tp = imallocx_body(size, flags, &tsdn, &usize, false);\n\t\tialloc_post_check(p, tsdn, usize, \"mallocx\", false, false);\n\t} else {\n\t\tp = imallocx_body(size, flags, &tsdn, &usize, true);\n\t\tialloc_post_check(p, tsdn, usize, \"mallocx\", false, true);\n\t\tUTRACE(0, size, p);\n\t\tJEMALLOC_VALGRIND_MALLOC(p != NULL, tsdn, p, usize,\n\t\t    MALLOCX_ZERO_GET(flags));\n\t}\n\n\treturn (p);\n}\n\nstatic void *\nirallocx_prof_sample(tsd_t *tsd, void *old_ptr, size_t old_usize,\n    size_t usize, size_t alignment, bool zero, tcache_t *tcache, arena_t *arena,\n    prof_tctx_t *tctx)\n{\n\tvoid *p;\n\n\tif (tctx == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = iralloct(tsd, old_ptr, old_usize, LARGE_MINCLASS, alignment,\n\t\t    zero, tcache, arena);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(tsd_tsdn(tsd), p, usize);\n\t} else {\n\t\tp = iralloct(tsd, old_ptr, old_usize, usize, alignment, zero,\n\t\t    tcache, arena);\n\t}\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nirallocx_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t size,\n    size_t alignment, size_t *usize, bool zero, tcache_t *tcache,\n    arena_t *arena)\n{\n\tvoid *p;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(tsd_tsdn(tsd), old_ptr);\n\ttctx = prof_alloc_prep(tsd, *usize, prof_active, false);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tp = irallocx_prof_sample(tsd, old_ptr, old_usize, *usize,\n\t\t    alignment, zero, tcache, arena, tctx);\n\t} else {\n\t\tp = iralloct(tsd, old_ptr, old_usize, size, alignment, zero,\n\t\t    tcache, arena);\n\t}\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, false);\n\t\treturn (NULL);\n\t}\n\n\tif (p == old_ptr && alignment != 0) {\n\t\t/*\n\t\t * The allocation did not move, so it is possible that the size\n\t\t * class is smaller than would guarantee the requested\n\t\t * alignment, and that the alignment constraint was\n\t\t * serendipitously satisfied.  Additionally, old_usize may not\n\t\t * be the same as the current usize because of in-place large\n\t\t * reallocation.  Therefore, query the actual value of usize.\n\t\t */\n\t\t*usize = isalloc(tsd_tsdn(tsd), p, config_prof);\n\t}\n\tprof_realloc(tsd, p, *usize, tctx, prof_active, false, old_ptr,\n\t    old_usize, old_tctx);\n\n\treturn (p);\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ALLOC_SIZE(2)\nje_rallocx(void *ptr, size_t size, int flags)\n{\n\tvoid *p;\n\ttsd_t *tsd;\n\tsize_t usize;\n\tsize_t old_usize;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = MALLOCX_ALIGN_GET(flags);\n\tbool zero = flags & MALLOCX_ZERO;\n\tarena_t *arena;\n\ttcache_t *tcache;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\ttsd = tsd_fetch();\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\tif (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {\n\t\tunsigned arena_ind = MALLOCX_ARENA_GET(flags);\n\t\tarena = arena_get(tsd_tsdn(tsd), arena_ind, true);\n\t\tif (unlikely(arena == NULL))\n\t\t\tgoto label_oom;\n\t} else\n\t\tarena = NULL;\n\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\ttcache = NULL;\n\t\telse\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\ttcache = tcache_get(tsd, true);\n\n\told_usize = isalloc(tsd_tsdn(tsd), ptr, config_prof);\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\told_rzsize = u2rz(old_usize);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\t\tif (unlikely(usize == 0 || usize > HUGE_MAXCLASS))\n\t\t\tgoto label_oom;\n\t\tp = irallocx_prof(tsd, ptr, old_usize, size, alignment, &usize,\n\t\t    zero, tcache, arena);\n\t\tif (unlikely(p == NULL))\n\t\t\tgoto label_oom;\n\t} else {\n\t\tp = iralloct(tsd, ptr, old_usize, size, alignment, zero,\n\t\t     tcache, arena);\n\t\tif (unlikely(p == NULL))\n\t\t\tgoto label_oom;\n\t\tif (config_stats || (config_valgrind && unlikely(in_valgrind)))\n\t\t\tusize = isalloc(tsd_tsdn(tsd), p, config_prof);\n\t}\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\n\tif (config_stats) {\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tUTRACE(ptr, size, p);\n\tJEMALLOC_VALGRIND_REALLOC(maybe, tsd_tsdn(tsd), p, usize, no, ptr,\n\t    old_usize, old_rzsize, no, zero);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\treturn (p);\nlabel_oom:\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(\"<jemalloc>: Error in rallocx(): out of memory\\n\");\n\t\tabort();\n\t}\n\tUTRACE(ptr, size, 0);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\treturn (NULL);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\nixallocx_helper(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero)\n{\n\tsize_t usize;\n\n\tif (ixalloc(tsdn, ptr, old_usize, size, extra, alignment, zero))\n\t\treturn (old_usize);\n\tusize = isalloc(tsdn, ptr, config_prof);\n\n\treturn (usize);\n}\n\nstatic size_t\nixallocx_prof_sample(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero, prof_tctx_t *tctx)\n{\n\tsize_t usize;\n\n\tif (tctx == NULL)\n\t\treturn (old_usize);\n\tusize = ixallocx_helper(tsdn, ptr, old_usize, size, extra, alignment,\n\t    zero);\n\n\treturn (usize);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\nixallocx_prof(tsd_t *tsd, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero)\n{\n\tsize_t usize_max, usize;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(tsd_tsdn(tsd), ptr);\n\t/*\n\t * usize isn't knowable before ixalloc() returns when extra is non-zero.\n\t * Therefore, compute its maximum possible value and use that in\n\t * prof_alloc_prep() to decide whether to capture a backtrace.\n\t * prof_realloc() will use the actual usize to decide whether to sample.\n\t */\n\tif (alignment == 0) {\n\t\tusize_max = s2u(size+extra);\n\t\tassert(usize_max > 0 && usize_max <= HUGE_MAXCLASS);\n\t} else {\n\t\tusize_max = sa2u(size+extra, alignment);\n\t\tif (unlikely(usize_max == 0 || usize_max > HUGE_MAXCLASS)) {\n\t\t\t/*\n\t\t\t * usize_max is out of range, and chances are that\n\t\t\t * allocation will fail, but use the maximum possible\n\t\t\t * value and carry on with prof_alloc_prep(), just in\n\t\t\t * case allocation succeeds.\n\t\t\t */\n\t\t\tusize_max = HUGE_MAXCLASS;\n\t\t}\n\t}\n\ttctx = prof_alloc_prep(tsd, usize_max, prof_active, false);\n\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tusize = ixallocx_prof_sample(tsd_tsdn(tsd), ptr, old_usize,\n\t\t    size, extra, alignment, zero, tctx);\n\t} else {\n\t\tusize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,\n\t\t    extra, alignment, zero);\n\t}\n\tif (usize == old_usize) {\n\t\tprof_alloc_rollback(tsd, tctx, false);\n\t\treturn (usize);\n\t}\n\tprof_realloc(tsd, ptr, usize, tctx, prof_active, false, ptr, old_usize,\n\t    old_tctx);\n\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nje_xallocx(void *ptr, size_t size, size_t extra, int flags)\n{\n\ttsd_t *tsd;\n\tsize_t usize, old_usize;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = MALLOCX_ALIGN_GET(flags);\n\tbool zero = flags & MALLOCX_ZERO;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(SIZE_T_MAX - size >= extra);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\ttsd = tsd_fetch();\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\n\told_usize = isalloc(tsd_tsdn(tsd), ptr, config_prof);\n\n\t/*\n\t * The API explicitly absolves itself of protecting against (size +\n\t * extra) numerical overflow, but we may need to clamp extra to avoid\n\t * exceeding HUGE_MAXCLASS.\n\t *\n\t * Ordinarily, size limit checking is handled deeper down, but here we\n\t * have to check as part of (size + extra) clamping, since we need the\n\t * clamped value in the above helper functions.\n\t */\n\tif (unlikely(size > HUGE_MAXCLASS)) {\n\t\tusize = old_usize;\n\t\tgoto label_not_resized;\n\t}\n\tif (unlikely(HUGE_MAXCLASS - size < extra))\n\t\textra = HUGE_MAXCLASS - size;\n\n\tif (config_valgrind && unlikely(in_valgrind))\n\t\told_rzsize = u2rz(old_usize);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = ixallocx_prof(tsd, ptr, old_usize, size, extra,\n\t\t    alignment, zero);\n\t} else {\n\t\tusize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,\n\t\t    extra, alignment, zero);\n\t}\n\tif (unlikely(usize == old_usize))\n\t\tgoto label_not_resized;\n\n\tif (config_stats) {\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tJEMALLOC_VALGRIND_REALLOC(no, tsd_tsdn(tsd), ptr, usize, no, ptr,\n\t    old_usize, old_rzsize, no, zero);\nlabel_not_resized:\n\tUTRACE(ptr, size, ptr);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nJEMALLOC_ATTR(pure)\nje_sallocx(const void *ptr, int flags)\n{\n\tsize_t usize;\n\ttsdn_t *tsdn;\n\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\n\ttsdn = tsdn_fetch();\n\twitness_assert_lockless(tsdn);\n\n\tif (config_ivsalloc)\n\t\tusize = ivsalloc(tsdn, ptr, config_prof);\n\telse\n\t\tusize = isalloc(tsdn, ptr, config_prof);\n\n\twitness_assert_lockless(tsdn);\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_dallocx(void *ptr, int flags)\n{\n\ttsd_t *tsd;\n\ttcache_t *tcache;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\ttsd = tsd_fetch();\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\ttcache = NULL;\n\t\telse\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\ttcache = tcache_get(tsd, false);\n\n\tUTRACE(ptr, 0, 0);\n\tif (likely(!malloc_slow))\n\t\tifree(tsd, ptr, tcache, false);\n\telse\n\t\tifree(tsd, ptr, tcache, true);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\ninallocx(tsdn_t *tsdn, size_t size, int flags)\n{\n\tsize_t usize;\n\n\twitness_assert_lockless(tsdn);\n\n\tif (likely((flags & MALLOCX_LG_ALIGN_MASK) == 0))\n\t\tusize = s2u(size);\n\telse\n\t\tusize = sa2u(size, MALLOCX_ALIGN_GET_SPECIFIED(flags));\n\twitness_assert_lockless(tsdn);\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_sdallocx(void *ptr, size_t size, int flags)\n{\n\ttsd_t *tsd;\n\ttcache_t *tcache;\n\tsize_t usize;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\ttsd = tsd_fetch();\n\tusize = inallocx(tsd_tsdn(tsd), size, flags);\n\tassert(usize == isalloc(tsd_tsdn(tsd), ptr, config_prof));\n\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE)\n\t\t\ttcache = NULL;\n\t\telse\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t} else\n\t\ttcache = tcache_get(tsd, false);\n\n\tUTRACE(ptr, 0, 0);\n\tif (likely(!malloc_slow))\n\t\tisfree(tsd, ptr, usize, tcache, false);\n\telse\n\t\tisfree(tsd, ptr, usize, tcache, true);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nJEMALLOC_ATTR(pure)\nje_nallocx(size_t size, int flags)\n{\n\tsize_t usize;\n\ttsdn_t *tsdn;\n\n\tassert(size != 0);\n\n\tif (unlikely(malloc_init()))\n\t\treturn (0);\n\n\ttsdn = tsdn_fetch();\n\twitness_assert_lockless(tsdn);\n\n\tusize = inallocx(tsdn, size, flags);\n\tif (unlikely(usize > HUGE_MAXCLASS))\n\t\treturn (0);\n\n\twitness_assert_lockless(tsdn);\n\treturn (usize);\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,\n    size_t newlen)\n{\n\tint ret;\n\ttsd_t *tsd;\n\n\tif (unlikely(malloc_init()))\n\t\treturn (EAGAIN);\n\n\ttsd = tsd_fetch();\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\tret = ctl_byname(tsd, name, oldp, oldlenp, newp, newlen);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp)\n{\n\tint ret;\n\ttsdn_t *tsdn;\n\n\tif (unlikely(malloc_init()))\n\t\treturn (EAGAIN);\n\n\ttsdn = tsdn_fetch();\n\twitness_assert_lockless(tsdn);\n\tret = ctl_nametomib(tsdn, name, mibp, miblenp);\n\twitness_assert_lockless(tsdn);\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,\n  void *newp, size_t newlen)\n{\n\tint ret;\n\ttsd_t *tsd;\n\n\tif (unlikely(malloc_init()))\n\t\treturn (EAGAIN);\n\n\ttsd = tsd_fetch();\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\tret = ctl_bymib(tsd, mib, miblen, oldp, oldlenp, newp, newlen);\n\twitness_assert_lockless(tsd_tsdn(tsd));\n\treturn (ret);\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,\n    const char *opts)\n{\n\ttsdn_t *tsdn;\n\n\ttsdn = tsdn_fetch();\n\twitness_assert_lockless(tsdn);\n\tstats_print(write_cb, cbopaque, opts);\n\twitness_assert_lockless(tsdn);\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nje_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr)\n{\n\tsize_t ret;\n\ttsdn_t *tsdn;\n\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tmalloc_thread_init();\n\n\ttsdn = tsdn_fetch();\n\twitness_assert_lockless(tsdn);\n\n\tif (config_ivsalloc)\n\t\tret = ivsalloc(tsdn, ptr, config_prof);\n\telse\n\t\tret = (ptr == NULL) ? 0 : isalloc(tsdn, ptr, config_prof);\n\n\twitness_assert_lockless(tsdn);\n\treturn (ret);\n}\n\n/*\n * End non-standard functions.\n */\n/******************************************************************************/\n/*\n * The following functions are used by threading libraries for protection of\n * malloc during fork().\n */\n\n/*\n * If an application creates a thread before doing any allocation in the main\n * thread, then calls fork(2) in the main thread followed by memory allocation\n * in the child process, a race can occur that results in deadlock within the\n * child: the main thread may have forked while the created thread had\n * partially initialized the allocator.  Ordinarily jemalloc prevents\n * fork/malloc races via the following functions it registers during\n * initialization using pthread_atfork(), but of course that does no good if\n * the allocator isn't fully initialized at fork time.  The following library\n * constructor is a partial solution to this problem.  It may still be possible\n * to trigger the deadlock described above, but doing so would involve forking\n * via a library constructor that runs before jemalloc's runs.\n */\n#ifndef JEMALLOC_JET\nJEMALLOC_ATTR(constructor)\nstatic void\njemalloc_constructor(void)\n{\n\n\tmalloc_init();\n}\n#endif\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_prefork(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_prefork(void)\n#endif\n{\n\ttsd_t *tsd;\n\tunsigned i, j, narenas;\n\tarena_t *arena;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (!malloc_initialized())\n\t\treturn;\n#endif\n\tassert(malloc_initialized());\n\n\ttsd = tsd_fetch();\n\n\tnarenas = narenas_total_get();\n\n\twitness_prefork(tsd);\n\t/* Acquire all mutexes in a safe order. */\n\tctl_prefork(tsd_tsdn(tsd));\n\tmalloc_mutex_prefork(tsd_tsdn(tsd), &arenas_lock);\n\tprof_prefork0(tsd_tsdn(tsd));\n\tfor (i = 0; i < 3; i++) {\n\t\tfor (j = 0; j < narenas; j++) {\n\t\t\tif ((arena = arena_get(tsd_tsdn(tsd), j, false)) !=\n\t\t\t    NULL) {\n\t\t\t\tswitch (i) {\n\t\t\t\tcase 0:\n\t\t\t\t\tarena_prefork0(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 1:\n\t\t\t\t\tarena_prefork1(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 2:\n\t\t\t\t\tarena_prefork2(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault: not_reached();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tbase_prefork(tsd_tsdn(tsd));\n\tfor (i = 0; i < narenas; i++) {\n\t\tif ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL)\n\t\t\tarena_prefork3(tsd_tsdn(tsd), arena);\n\t}\n\tprof_prefork1(tsd_tsdn(tsd));\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_postfork_parent(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_postfork(void)\n#endif\n{\n\ttsd_t *tsd;\n\tunsigned i, narenas;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (!malloc_initialized())\n\t\treturn;\n#endif\n\tassert(malloc_initialized());\n\n\ttsd = tsd_fetch();\n\n\twitness_postfork_parent(tsd);\n\t/* Release all mutexes, now that fork() has completed. */\n\tbase_postfork_parent(tsd_tsdn(tsd));\n\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\tarena_t *arena;\n\n\t\tif ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL)\n\t\t\tarena_postfork_parent(tsd_tsdn(tsd), arena);\n\t}\n\tprof_postfork_parent(tsd_tsdn(tsd));\n\tmalloc_mutex_postfork_parent(tsd_tsdn(tsd), &arenas_lock);\n\tctl_postfork_parent(tsd_tsdn(tsd));\n}\n\nvoid\njemalloc_postfork_child(void)\n{\n\ttsd_t *tsd;\n\tunsigned i, narenas;\n\n\tassert(malloc_initialized());\n\n\ttsd = tsd_fetch();\n\n\twitness_postfork_child(tsd);\n\t/* Release all mutexes, now that fork() has completed. */\n\tbase_postfork_child(tsd_tsdn(tsd));\n\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\tarena_t *arena;\n\n\t\tif ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL)\n\t\t\tarena_postfork_child(tsd_tsdn(tsd), arena);\n\t}\n\tprof_postfork_child(tsd_tsdn(tsd));\n\tmalloc_mutex_postfork_child(tsd_tsdn(tsd), &arenas_lock);\n\tctl_postfork_child(tsd_tsdn(tsd));\n}\n\n/******************************************************************************/\n"
    },
    "skipped": [],
    "total_files": 206
}