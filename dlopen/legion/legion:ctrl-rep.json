{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/bindings/python/legion_cffi.py.in": "#!/usr/bin/env python3\n\n# Copyright 2021 Stanford University\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# IMPORTANT:\n#   * legion_cffi.py.in is used as an input to string.format()\n#   * legion_cffi.py is a generated file and should not be modified by hand\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport cffi\n\nheader = {header}\n\nffi = cffi.FFI()\nffi.cdef(header)\nlib = ffi.dlopen(None)\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/runtime/realm/codedesc.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// constructs for describing code blobs to Realm\n\n#include \"realm/codedesc.h\"\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n#include \"realm/logging.h\"\n#include \"realm/utils.h\"\n\nnamespace Realm {\n\n  Logger log_codetrans(\"codetrans\");\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Type\n\n  std::ostream& operator<<(std::ostream& os, const Type& t)\n  {\n    switch(t.f_common.kind) {\n    case Type::InvalidKind: os << \"INVALIDTYPE\"; break;\n    case Type::OpaqueKind:\n      {\n\tif(t.size_bits() == 0)\n\t  os << \"void\";\n\telse\n\t  os << \"opaque(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::IntegerKind:\n      {\n\tos << (t.f_integer.is_signed ? 's' : 'u') << \"int(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::FloatingPointKind: os << \"float(\" << t.size_bits() << \")\"; break;\n    case Type::PointerKind:\n      {\n\tos << *t.f_pointer.base_type;\n\tif(t.f_pointer.is_const) os << \" const\";\n\tos << \" *\";\n\tbreak;\n      }\n    case Type::FunctionPointerKind:\n      {\n\tos << *t.f_funcptr.return_type << \"(*)(\";\n\tconst std::vector<Type>& p = *t.f_funcptr.param_types;\n\tif(p.size()) {\n\t  for(size_t i = 0; i < p.size(); i++) {\n\t    if(i) os << \", \";\n\t    os << p[i];\n\t  }\n\t} else\n\t  os << \"void\";\n\tos << \")\";\n\tbreak;\n      }\n    }\n    return os;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeDescriptor\n\n  CodeDescriptor::CodeDescriptor(void)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const Type& _t)\n    : m_type(_t)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const CodeDescriptor& rhs)\n  {\n    copy_from(rhs);\n  }\n\n  CodeDescriptor& CodeDescriptor::operator=(const CodeDescriptor& rhs)\n  {\n    if(this != &rhs) {\n      clear();\n      copy_from(rhs);\n    }\n    return *this;\n  }\n\n  CodeDescriptor::~CodeDescriptor(void)\n  {\n    clear();\n  }\n\n  void CodeDescriptor::clear(void)\n  {\n    m_type = Type();\n    delete_container_contents(m_impls);\n    delete_container_contents(m_props);\n  }\n\n  void CodeDescriptor::copy_from(const CodeDescriptor& rhs)\n  {\n    m_type = rhs.m_type;\n    {\n      size_t s = rhs.m_impls.size();\n      m_impls.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_impls[i] = rhs.m_impls[i]->clone();\n    }\n    {\n      size_t s = rhs.m_props.size();\n      m_props.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_props[i] = rhs.m_props[i]->clone();\n    }\n  }\n\n  // are any of the code implementations marked as \"portable\" (i.e.\n  //  usable in another process/address space)?\n  bool CodeDescriptor::has_portable_implementations(void) const\n  {\n    for(std::vector<CodeImplementation *>::const_iterator it = m_impls.begin();\n\tit != m_impls.end();\n\tit++)\n      if((*it)->is_portable())\n\treturn true;\n    return false;\n  }\n\n  // attempt to make a portable implementation from what we have\n  bool CodeDescriptor::create_portable_implementation(void)\n  {\n    // TODO: actually have translators registered where we can find them\n#if defined(REALM_USE_DLFCN) && defined(REALM_USE_DLADDR)\n    const FunctionPointerImplementation *fpi = find_impl<FunctionPointerImplementation>();\n    if(fpi) {\n      DSOReferenceImplementation *dsoref = DSOReferenceImplementation::cvt_fnptr_to_dsoref(fpi, true /*quiet*/);\n      if(dsoref) {\n\tm_impls.push_back(dsoref);\n\treturn true;\n      }\n    }\n#endif\n\n    return false;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class FunctionPointerImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      FunctionPointerImplementation> FunctionPointerImplementation::serdez_subclass;\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void)\n    : fnptr(0)\n  {}\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void (*_fnptr)())\n    : fnptr(_fnptr)\n  {}\n\n  FunctionPointerImplementation::~FunctionPointerImplementation(void)\n  {}\n\n  CodeImplementation *FunctionPointerImplementation::clone(void) const\n  {\n    return new FunctionPointerImplementation(fnptr);\n  }\n\n  bool FunctionPointerImplementation::is_portable(void) const\n  {\n    return false;\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOReferenceImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      DSOReferenceImplementation> DSOReferenceImplementation::serdez_subclass;\n\n  DSOReferenceImplementation::DSOReferenceImplementation(void)\n  {}\n\n  DSOReferenceImplementation::DSOReferenceImplementation(const std::string& _dso_name,\n\t\t\t\t\t\t\t const std::string& _symbol_name)\n    : dso_name(_dso_name), symbol_name(_symbol_name)\n  {}\n\n  DSOReferenceImplementation::~DSOReferenceImplementation(void)\n  {}\n\n  CodeImplementation *DSOReferenceImplementation::clone(void) const\n  {\n    return new DSOReferenceImplementation(dso_name, symbol_name);\n  }\n\n  bool DSOReferenceImplementation::is_portable(void) const\n  {\n    return true;\n  }\n\n#ifdef REALM_USE_DLADDR\n  namespace {\n    // pgcc doesn't let us declare a weak 'main'\n#ifndef __PGI\n    extern \"C\" { int main(int argc, const char *argv[]) __attribute__((weak)); };\n#endif\n\n    DSOReferenceImplementation *dladdr_helper(void *ptr, bool quiet)\n    {\n      // if dladdr() gives us something with the same base pointer, assume that's portable\n      // note: return code is not-POSIX-y (i.e. 0 == failure)\n      Dl_info inf;\n      int ret = dladdr(ptr, &inf);\n      if(ret == 0) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"couldn't map fnptr \" << ptr << \" to a dynamic symbol\";\n\treturn 0;\n      }\n\n      if(inf.dli_saddr != ptr) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"pointer \" << ptr << \" in middle of symbol '\" << inf.dli_sname << \" (\" << inf.dli_saddr << \")?\";\n\treturn 0;\n      }\n\n      // try to detect symbols that are in the base executable and change the filename to \"\"\n      // only do this if the weak 'main' reference found an actual main\n      const char *fname = inf.dli_fname;\n#ifndef __PGI\n      if(((void *)main) != 0) {\n\tstatic std::string local_fname;\n\tif(local_fname.empty()) {\n\t  Dl_info inf2;\n\t  ret = dladdr((void *)main, &inf2);\n\t  assert(ret != 0);\n\t  local_fname = inf2.dli_fname;\n\t}\n\tif(local_fname.compare(fname) == 0)\n\t  fname = \"\";\n      }\n#endif\n      return new DSOReferenceImplementation(fname, inf.dli_sname);\n    }\n  };\n\n  /*static*/ DSOReferenceImplementation *DSOReferenceImplementation::cvt_fnptr_to_dsoref(const FunctionPointerImplementation *fpi,\n\t\t\t\t\t\t\t\t\t\t\t bool quiet /*= false*/)\n  {\n    return dladdr_helper((void *)(fpi->fnptr), quiet);\n  } \n#endif\n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeTranslator\n\n  CodeTranslator::CodeTranslator(const std::string& _name)\n    : name(_name)\n  {}\n\n  CodeTranslator::~CodeTranslator(void)\n  {}\n\n  // default version just iterates over all the implementations in the source\n  bool CodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t     const std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn true;\n    }\n\n    return false;\n  }\n\n  // default version just iterates over all the implementations in the source\n  CodeImplementation *CodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn translate(*it, target_impl_type);\n    }\n\n    return 0;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOCodeTranslator\n\n#ifdef REALM_USE_DLFCN\n  DSOCodeTranslator::DSOCodeTranslator(void)\n    : CodeTranslator(\"dso\")\n  {}\n\n  DSOCodeTranslator::~DSOCodeTranslator(void)\n  {\n    // unload any modules we have loaded\n    for(std::map<std::string, void *>::iterator it = modules_loaded.begin();\n\tit != modules_loaded.end();\n\tit++) {\n      int ret = dlclose(it->second);\n      if(ret != 0)\n\tlog_codetrans.warning() << \"error on dlclose of '\" << it->first << \"': \" << dlerror();\n    }\n  }\n\n  bool DSOCodeTranslator::can_translate(const std::type_info& source_impl_type,\n\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    // DSO ref -> function pointer\n    if((source_impl_type == typeid(DSOReferenceImplementation)) &&\n       (target_impl_type == typeid(FunctionPointerImplementation)))\n      return true;\n\n#ifdef REALM_USE_DLADDR\n    if((source_impl_type == typeid(FunctionPointerImplementation)) &&\n       (target_impl_type == typeid(DSOReferenceImplementation)))\n      return true;\n#endif\n\n      return false;\n    }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeImplementation *source,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    if(target_impl_type == typeid(FunctionPointerImplementation)) {\n      const DSOReferenceImplementation *dsoref = dynamic_cast<const DSOReferenceImplementation *>(source);\n      assert(dsoref != 0);\n\n      void *handle = 0;\n      // check to see if we've already loaded the module?\n      std::map<std::string, void *>::iterator it = modules_loaded.find(dsoref->dso_name);\n      if(it != modules_loaded.end()) {\n\thandle = it->second;\n      } else {\n\t// try to load it - empty string for dso_name means the main executable\n\tconst char *dso_name = dsoref->dso_name.c_str();\n\thandle = dlopen(*dso_name ? dso_name : 0, RTLD_NOW | RTLD_LOCAL);\n\tif(!handle) {\n\t  log_codetrans.warning() << \"could not open DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\t  return 0;\n\t}\n\tmodules_loaded[dsoref->dso_name] = handle;\n      }\n\n      void *ptr = dlsym(handle, dsoref->symbol_name.c_str());\n      if(!ptr) {\n\tlog_codetrans.warning() << \"could not find symbol '\" << dsoref->symbol_name << \"' in  DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\treturn 0;\n      }\n\n      return new FunctionPointerImplementation((void(*)())ptr);\n    }\n\n#ifdef REALM_USE_DLADDR\n    if(target_impl_type == typeid(DSOReferenceImplementation)) {\n      const FunctionPointerImplementation *fpi = dynamic_cast<const FunctionPointerImplementation *>(source);\n      assert(fpi != 0);\n\n      return dladdr_helper((void *)(fpi->fnptr), false /*!quiet*/);\n    }\n#endif\n\n    return 0;\n  }\n\n  // these pass through to CodeTranslator's definitions\n  bool DSOCodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    return CodeTranslator::can_translate(source_codedesc, target_impl_type);\n  }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    return CodeTranslator::translate(source_codedesc, target_impl_type);\n  }\n#endif\n\n\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/runtime/realm/module.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Realm modules\n\n#include \"realm/realm_config.h\"\n\n#define REALM_MODULE_REGISTRATION_STATIC\n#include \"realm/module.h\"\n\n#include \"realm/logging.h\"\n\n#include <assert.h>\n#include <string.h>\n#include <stdlib.h>\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n// TODO: replace this with Makefile (or maybe cmake) magic that adapts automatically\n//  to the build-system-controlled list of statically-linked Realm modules\n#include \"realm/runtime_impl.h\"\n#include \"realm/numa/numa_module.h\"\n#ifdef REALM_USE_OPENMP\n#include \"realm/openmp/openmp_module.h\"\n#endif\n#include \"realm/procset/procset_module.h\"\n#ifdef REALM_USE_PYTHON\n#include \"realm/python/python_module.h\"\n#endif\n#ifdef REALM_USE_CUDA\n#include \"realm/cuda/cuda_module.h\"\n#endif\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit_module.h\"\n#endif\n#ifdef REALM_USE_HDF5\n#include \"realm/hdf5/hdf5_module.h\"\n#endif\n#ifdef REALM_USE_GASNET1\n#include \"realm/gasnet1/gasnet1_module.h\"\n#endif\n#ifdef REALM_USE_GASNETEX\n#include \"realm/gasnetex/gasnetex_module.h\"\n#endif\n#if defined REALM_USE_MPI\n#include \"realm/mpi/mpi_module.h\"\n#endif\n\nnamespace Realm {\n\n  Logger log_module(\"module\");\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Module\n  //\n\n  Module::Module(const std::string& _name)\n    : name(_name)\n  {\n    log_module.debug() << \"module \" << name << \" created\";\n  }\n\n  Module::~Module(void)\n  {\n    log_module.debug() << \"module \" << name << \" destroyed\";\n  }\n\n  const std::string& Module::get_name(void) const\n  {\n    return name;\n  }\n\n  void Module::initialize(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" initialize\";\n  }\n\n  void Module::create_memories(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_memories\";\n  }\n\n  void Module::create_processors(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_processors\";\n  }\n  \n  void Module::create_dma_channels(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_dma_channels\";\n  }\n  \n  void Module::create_code_translators(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_code_translators\";\n  }\n\n  void Module::cleanup(void)\n  {\n    log_module.debug() << \"module \" << name << \" cleanup\";\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class ModuleRegistrar\n  //\n\n  namespace {\n    ModuleRegistrar::StaticRegistrationBase *static_modules_head = 0;\n    ModuleRegistrar::StaticRegistrationBase **static_modules_tail = &static_modules_head;\n  };\n\n  ModuleRegistrar::ModuleRegistrar(RuntimeImpl *_runtime)\n    : runtime(_runtime)\n  {}\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_static_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t      std::vector<Module *>& modules)\n  {\n    // just iterate over the static module list, trying to create each module\n    for(const StaticRegistrationBase *sreg = static_modules_head;\n\tsreg;\n\tsreg = sreg->next) {\n      Module *m = sreg->create_module(runtime, cmdline);\n      if(m)\n\tmodules.push_back(m);\n    }\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  // accepts a colon-separated list of so files to try to load\n  static void load_module_list(const char *sonames,\n\t\t\t       RuntimeImpl *runtime,\n\t\t\t       std::vector<std::string>& cmdline,\n\t\t\t       std::vector<void *>& handles,\n\t\t\t       std::vector<Module *>& modules)\n  {\n    // null/empty strings are nops\n    if(!sonames || !*sonames) return;\n\n    const char *p1 = sonames;\n    while(true) {\n      // skip leading colons\n      while(*p1 == ':') p1++;\n      if(!*p1) break;\n\n      const char *p2 = p1 + 1;\n      while(*p2 && (*p2 != ':')) p2++;\n\n      char filename[1024];\n      strncpy(filename, p1, p2 - p1);\n\n      // no leftover errors from anybody else please...\n      assert(dlerror() == 0);\n\n      // open so file, resolving all symbols but not polluting global namespace\n      void *handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n\n      if(handle != 0) {\n\t// this file should have a \"create_realm_module\" symbol\n\tvoid *sym = dlsym(handle, \"create_realm_module\");\n\n\tif(sym != 0) {\n\t  // TODO: hold onto the handle even if it doesn't create a module?\n\t  handles.push_back(handle);\n\n\t  Module *m = ((Module *(*)(RuntimeImpl *, std::vector<std::string>&))dlsym)(runtime, cmdline);\n\t  if(m)\n\t    modules.push_back(m);\n\t} else {\n\t  log_module.error() << \"symbol 'create_realm_module' not found in \" << filename;\n#ifndef NDEBUG\n\t  int ret =\n#endif\n\t    dlclose(handle);\n\t  assert(ret == 0);\n\t}\n      } else {\n\tlog_module.error() << \"could not load \" << filename << \": \" << dlerror();\n      }\n\n      if(!*p2) break;\n      p1 = p2 + 1;\n    }\n  }\n#endif\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_dynamic_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t       std::vector<Module *>& modules)\n  {\n    // dynamic modules are requested in one of two ways:\n    // 1) REALM_DYNAMIC_MODULES=sonames environment variable\n    // 2) \"-ll:module sonames\" on command line\n    // in both cases, 'sonames' is a colon-separate listed of .so files that should be\n\n    // loading modules can also monkey with the cmdline, so do a pass first where we pull\n    //  out all the name we want to load\n    std::vector<std::string> sonames_list;\n\n    {\n      const char *e = getenv(\"REALM_DYNAMIC_MODULES\");\n      if(e)\n\tsonames_list.push_back(std::string(e));\n    }\n\n    {\n      std::vector<std::string>::iterator it = cmdline.begin();\n      while(it != cmdline.end()) {\n\tif(*it != \"-ll:module\") {\n\t  it++;\n\t  continue;\n\t}\n\n\t// eat this argument and move the next one to sonames_list\n\tit = cmdline.erase(it);\n\tassert(it != cmdline.end());\n\tsonames_list.push_back(*it);\n\tit = cmdline.erase(it);\n      }\n    }\n\n#ifdef REALM_USE_DLFCN\n    for(std::vector<std::string>::const_iterator it = sonames_list.begin();\n\tit != sonames_list.end();\n\tit++)\n      load_module_list(it->c_str(),\n\t\t       runtime, cmdline, sofile_handles, modules);\n#else\n    if(!sonames_list.empty()) {\n      log_module.error() << \"loading of dynamic Realm modules requested, but REALM_USE_DLFCN=0!\";\n      exit(1);\n    }\n#endif\n  }\n\n  // called by runtime after all modules have been cleaned up\n  void ModuleRegistrar::unload_module_sofiles(void)\n  {\n#ifdef REALM_USE_DLFCN\n    while(!sofile_handles.empty()) {\n      void *handle = sofile_handles.back();\n      sofile_handles.pop_back();\n\n#ifndef NDEBUG\n      int ret =\n#endif\n\tdlclose(handle);\n      assert(ret == 0);\n    }\n#endif\n  }\n\n  // called by the module registration helpers\n  /*static*/ void ModuleRegistrar::add_static_registration(StaticRegistrationBase *reg)\n  {\n    // done during init, so single-threaded\n    *static_modules_tail = reg;\n    static_modules_tail = &(reg->next);\n  }\n  \n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/runtime/realm/python/python_module.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"realm/python/python_module.h\"\n#include \"realm/python/python_internal.h\"\n\n#include \"realm/numa/numasysif.h\"\n#include \"realm/logging.h\"\n#include \"realm/cmdline.h\"\n#include \"realm/proc_impl.h\"\n#include \"realm/mem_impl.h\"\n#include \"realm/threads.h\"\n#include \"realm/runtime_impl.h\"\n#include \"realm/utils.h\"\n\n#include <dlfcn.h>\n#ifdef REALM_USE_DLMOPEN\n#include <link.h>\n#endif // REALM_USE_DLMOPEN\n\n#include <list>\n\nnamespace Realm {\n\n  Logger log_py(\"python\");\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonAPI\n\n  PythonAPI::PythonAPI(void *_handle)\n    : handle(_handle)\n  {\n    get_symbol(this->Py_DecRef, \"Py_DecRef\");\n    get_symbol(this->Py_Finalize, \"Py_Finalize\");\n    get_symbol(this->Py_InitializeEx, \"Py_InitializeEx\");\n\n    get_symbol(this->PyByteArray_FromStringAndSize, \"PyByteArray_FromStringAndSize\");\n\n    get_symbol(this->PyEval_InitThreads, \"PyEval_InitThreads\");\n\n#ifdef USE_PYGILSTATE_CALLS\n    get_symbol(this->PyGILState_Ensure, \"PyGILState_Ensure\");\n    get_symbol(this->PyGILState_Release, \"PyGILState_Release\");\n#else\n    get_symbol(this->PyThreadState_New, \"PyThreadState_New\");\n    get_symbol(this->PyThreadState_Clear, \"PyThreadState_Clear\");\n    get_symbol(this->PyThreadState_Delete, \"PyThreadState_Delete\");\n#endif\n    get_symbol(this->PyEval_RestoreThread, \"PyEval_RestoreThread\");\n    get_symbol(this->PyEval_SaveThread, \"PyEval_SaveThread\");\n\n    get_symbol(this->PyThreadState_Swap, \"PyThreadState_Swap\");\n    get_symbol(this->PyThreadState_Get, \"PyThreadState_Get\");\n\n    get_symbol(this->PyErr_PrintEx, \"PyErr_PrintEx\");\n\n    get_symbol(this->PyImport_ImportModule, \"PyImport_ImportModule\");\n    get_symbol(this->PyModule_GetDict, \"PyModule_GetDict\");\n\n    get_symbol(this->PyLong_FromUnsignedLong, \"PyLong_FromUnsignedLong\");\n\n    get_symbol(this->PyObject_CallFunction, \"PyObject_CallFunction\");\n    get_symbol(this->PyObject_CallObject, \"PyObject_CallObject\");\n    get_symbol(this->PyObject_GetAttrString, \"PyObject_GetAttrString\");\n    get_symbol(this->PyObject_Print, \"PyObject_Print\");\n\n    get_symbol(this->PyRun_SimpleString, \"PyRun_SimpleString\");\n    get_symbol(this->PyRun_String, \"PyRun_String\");\n\n    get_symbol(this->PyTuple_New, \"PyTuple_New\");\n    get_symbol(this->PyTuple_SetItem, \"PyTuple_SetItem\");\n  }\n\n  template<typename T>\n  void PythonAPI::get_symbol(T &fn, const char *symbol,\n                             bool missing_ok /*= false*/)\n  {\n    fn = reinterpret_cast<T>(dlsym(handle, symbol));\n    if(!fn && !missing_ok) {\n      const char *error = dlerror();\n      log_py.fatal() << \"failed to find symbol '\" << symbol << \"': \" << error;\n      assert(false);\n    }\n  }\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonInterpreter\n\n#ifdef REALM_USE_DLMOPEN\n  // dlmproxy symbol lookups have to happen in a function we define so that\n  //  dl[v]sym searches in the right place\n  static void *dlmproxy_lookup(const char *symname, const char *symver)\n  {\n    \n    void *handle = 0;\n    void *sym = (symver ?\n\t\t   dlvsym(handle, symname, symver) :\n\t\t   dlsym(handle, symname));\n    if(sym)\n      log_py.debug() << \"found symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\") << \" ptr=\" << sym;\n    else\n      log_py.warning() << \"missing symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\");\n    return sym;\n  }\n#endif\n\n  PythonInterpreter::PythonInterpreter() \n  {\n#ifdef REALM_PYTHON_LIB\n    const char *python_lib = REALM_PYTHON_LIB;\n#else\n    const char *python_lib = \"libpython2.7.so\";\n#endif\n\n#ifdef REALM_USE_DLMOPEN\n    // loading libpython into its own namespace will cause it to try to bring\n    //   in a second copy of libpthread.so.0, which is fairly disastrous\n    // we deal with it by loading a \"dlmproxy\" of pthreads that tunnels all \n    //   pthreads calls back to the (only) version in the main executable\n    const char *dlmproxy_filename = getenv(\"DLMPROXY_LIBPTHREAD\");\n    if(!dlmproxy_filename)\n      dlmproxy_filename = \"dlmproxy_libpthread.so.0\";\n    dlmproxy_handle = dlmopen(LM_ID_NEWLM,\n\t\t\t      dlmproxy_filename,\n\t\t\t      RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_LAZY);\n    if(!dlmproxy_handle) {\n      const char *error = dlerror();\n      log_py.fatal() << \"HELP!  Use of dlmopen for python requires dlmproxy for pthreads!  Failed to\\n\"\n\t\t     << \"  load: \" << dlmproxy_filename << \"\\n\"\n\t\t     << \"  error: \" << error;\n      assert(false);\n    }\n\n    // now that the proxy is loaded, we need to tell it where the real\n    //  libpthreads functions are\n    {\n      void *sym = dlsym(dlmproxy_handle, \"dlmproxy_load_symbols\");\n      assert(sym != 0);\n      ((void (*)(void *(*)(const char *, const char *)))sym)(dlmproxy_lookup);\n    }\n\n    // now we can load libpython, but make sure we do it in the new namespace\n    Lmid_t lmid;\n    int ret = dlinfo(dlmproxy_handle, RTLD_DI_LMID, &lmid);\n    assert(ret == 0);\n\n    handle = dlmopen(lmid, python_lib, RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_NOW);\n#else\n    // life is so much easier if we use dlopen (but we only get one copy then)\n    handle = dlopen(python_lib, RTLD_GLOBAL | RTLD_LAZY);\n#endif\n    if (!handle) {\n      const char *error = dlerror();\n      log_py.fatal() << error;\n      assert(false);\n    }\n\n    api = new PythonAPI(handle);\n\n    (api->Py_InitializeEx)(0 /*!initsigs*/);\n    (api->PyEval_InitThreads)();\n    //(api->Py_Finalize)();\n\n    //PyThreadState *state;\n    //state = (api->PyEval_SaveThread)();\n    //(api->PyEval_RestoreThread)(state);\n\n    //(api->PyRun_SimpleString)(\"print 'hello Python world!'\");\n\n    //PythonSourceImplementation psi(\"taskreg_helper\", \"task1\");\n    //find_or_import_function(&psi);\n  }\n\n  PythonInterpreter::~PythonInterpreter()\n  {\n    (api->Py_Finalize)();\n\n    delete api;\n\n    if (dlclose(handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"libpython dlclose error: \" << error;\n      assert(false);\n    }\n\n#ifdef REALM_USE_DLMOPEN\n    if (dlclose(dlmproxy_handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"dlmproxy dlclose error: \" << error;\n      assert(false);\n    }\n#endif\n  }\n\n  PyObject *PythonInterpreter::find_or_import_function(const PythonSourceImplementation *psi)\n  {\n    //log_py.print() << \"attempting to acquire python lock\";\n    //(api->PyEval_AcquireLock)();\n    //log_py.print() << \"lock acquired\";\n\n    // not calling PythonInterpreter::import_module here because we want the\n    //  PyObject result\n    log_py.debug() << \"attempting to import module: \" << psi->module_name;\n    PyObject *module = (api->PyImport_ImportModule)(psi->module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(module, stdout, 0); printf(\"\\n\");\n\n    PyObject *function = module;\n    for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n           ie = psi->function_name.end(); function && it != ie; ++it) {\n      function = (api->PyObject_GetAttrString)(function, it->c_str());\n    }\n    if (!function) {\n      {\n        LoggerMessage m = log_py.fatal();\n        m << \"unable to import Python function \";\n        for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n               ie = psi->function_name.begin(); it != ie; ++it) {\n          m << *it;\n          if (it + 1 != ie) {\n            m << \".\";\n          }\n        }\n        m << \" from module \" << psi->module_name;\n      }\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(function, stdout, 0); printf(\"\\n\");\n\n    //(api->PyObject_CallFunction)(function, \"iii\", 1, 2, 3);\n\n    (api->Py_DecRef)(module);\n\n    return function;\n  }\n\n  void PythonInterpreter::import_module(const std::string& module_name)\n  {\n    log_py.debug() << \"attempting to import module: \" << module_name;\n    PyObject *module = (api->PyImport_ImportModule)(module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(module);\n  }\n\n  void PythonInterpreter::run_string(const std::string& script_text)\n  {\n    // from Python.h\n    const int Py_file_input = 257;\n\n    log_py.debug() << \"running python string: \" << script_text;\n    PyObject *mainmod = (api->PyImport_ImportModule)(\"__main__\");\n    assert(mainmod != 0);\n    PyObject *globals = (api->PyModule_GetDict)(mainmod);\n    assert(globals != 0);\n    PyObject *res = (api->PyRun_String)(script_text.c_str(),\n\t\t\t\t\tPy_file_input,\n\t\t\t\t\tglobals,\n\t\t\t\t\tglobals);\n    if(!res) {\n      log_py.fatal() << \"unable to run python string:\" << script_text;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(res);\n    (api->Py_DecRef)(globals);\n    (api->Py_DecRef)(mainmod);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonThreadTaskScheduler\n\n  PythonThreadTaskScheduler::PythonThreadTaskScheduler(LocalPythonProcessor *_pyproc,\n\t\t\t\t\t\t       CoreReservation& _core_rsrv)\n    : KernelThreadTaskScheduler(_pyproc->me, _core_rsrv)\n    , pyproc(_pyproc)\n    , interpreter_ready(false)\n  {}\n\n  // both real and internal tasks need to be wrapped with acquires of the GIL\n  bool PythonThreadTaskScheduler::execute_task(Task *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    bool ok = KernelThreadTaskScheduler::execute_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n\n    return ok;\n  }\n  \n  void PythonThreadTaskScheduler::execute_internal_task(InternalTask *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    KernelThreadTaskScheduler::execute_internal_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n  }\n    \n  void PythonThreadTaskScheduler::python_scheduler_loop(void)\n  {\n    // global startup of python interpreter if needed\n    if(!interpreter_ready) {\n      log_py.info() << \"creating interpreter\";\n      pyproc->create_interpreter();\n      interpreter_ready = true;\n    }\n\n#ifdef REALM_USE_OPENMP\n    // associate with an OpenMP thread pool if one is available\n    if(pyproc->omp_threadpool != 0)\n      pyproc->omp_threadpool->associate_as_master();\n#endif\n\n#ifdef USE_PYGILSTATE_CALLS\n    // our PyThreadState is implicit when using the PyGILState calls\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = 0;\n#else\n    // always create and remember our own python thread - does NOT require GIL\n    PyThreadState *pythread = (pyproc->interpreter->api->PyThreadState_New)(pyproc->master_thread->interp);\n    log_py.debug() << \"created python thread: \" << pythread;\n    \n    assert(pythread != 0);\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = pythread;\n#endif\n\n    // take lock and go into normal task scheduler loop\n    {\n      AutoLock<> al(lock);\n      KernelThreadTaskScheduler::scheduler_loop();\n    }\n#if 0\n    // now go into main scheduler loop, holding scheduler lock for whole thing\n    AutoLock<> al(lock);\n    while(true) {\n      // remember the work counter value before we start so that we don't iterate\n      //   unnecessarily\n      long long old_work_counter = work_counter.read_counter();\n\n      // first priority - task registration\n      while(!taskreg_queue.empty()) {\n\tLocalPythonProcessor::TaskRegistration *treg = taskreg_queue.front();\n\ttaskreg_queue.pop_front();\n\t\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\t\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = TaskQueue::PRI_POS_INF;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  pyproc->perform_task_registration(treg);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      }\n\n      // if we have both resumable and new ready tasks, we want the one that\n      //  is the highest priority, with ties going to resumable tasks - we\n      //  can do this cleanly by taking advantage of the fact that the\n      //  resumable_workers queue uses the scheduler lock, so can't change\n      //  during this call\n      // peek at the top thing (if any) in that queue, and then try to find\n      //  a ready task with higher priority\n      int resumable_priority = ResumableQueue::PRI_NEG_INF;\n      resumable_workers.peek(&resumable_priority);\n\n      // try to get a new task then\n      int task_priority = resumable_priority;\n      Task *task = TaskQueue::get_best_task(task_queues, task_priority);\n\n      // did we find work to do?\n      if(task) {\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = task_priority;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  execute_task(task);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n\tcontinue;\n      }\n\n      // having checked for higher-priority ready tasks, we can always\n      //  take the highest-priority resumable task, if any, and run it\n      if(!resumable_workers.empty()) {\n\tThread *yield_to = resumable_workers.get(0); // priority is irrelevant\n\tassert(yield_to != Thread::self());\n\n\t// this should only happen if we're at the max active worker count (otherwise\n\t//  somebody should have just woken this guy up earlier), and reduces the \n\t// unassigned worker count by one\n\tupdate_worker_count(0, -1);\n\n\tidle_workers.push_back(Thread::self());\n\tworker_sleep(yield_to);\n\n\t// loop around and check both queues again\n\tcontinue;\n      }\n\n      {\n\t// no ready or resumable tasks?  thumb twiddling time\n\n\t// are we shutting down?\n\tif(shutdown_flag.load()) {\n\t  // yes, we can terminate - wake up an idler (if any) first though\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t  } else {\n\t    // nobody to wake, so -1 active/unassigned worker\n\t    update_worker_count(-1, -1, false); // ok to drop below mins\n\t    worker_terminate(0);\n\t  }\n\t  return;\n\t}\n\n\t// do we have more unassigned and idle tasks than we need?\n\tint total_idle_count = (unassigned_worker_count +\n\t\t\t\t(int)(idle_workers.size()));\n\tif(total_idle_count > cfg_max_idle_workers) {\n\t  // if there are sleeping idlers, terminate in favor of one of those - keeps\n\t  //  worker counts constant\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    assert(to_wake != Thread::self());\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t    return;\n\t  }\n\t}\n\n\t// no, stay awake but suspend until there's a chance that the next iteration\n\t//  of this loop would turn out different\n\twait_for_work(old_work_counter);\n      }\n    }\n    // should never get here\n    assert(0);\n#endif\n  }\n\n  Thread *PythonThreadTaskScheduler::worker_create(bool make_active)\n  {\n    // lock is held by caller\n    ThreadLaunchParameters tlp;\n    Thread *t = Thread::create_kernel_thread<PythonThreadTaskScheduler,\n\t\t\t\t\t     &PythonThreadTaskScheduler::python_scheduler_loop>(this,\n\t\t\t\t\t\t\t\t\t\t\t\ttlp,\n\t\t\t\t\t\t\t\t\t\t\t\tcore_rsrv,\n\t\t\t\t\t\t\t\t\t\t\t\tthis);\n    all_workers.insert(t);\n    if(make_active)\n      active_workers.insert(t);\n    return t;\n  }\n \n  // called by a worker thread when it needs to wait for something (and we\n  //   should release the GIL)\n  void PythonThreadTaskScheduler::thread_blocking(Thread *thread)\n  {\n    // if this gets called before we're done initializing the interpreter,\n    //  we need a simple blocking wait\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n\n      log_py.debug() << \"waiting during initialization\";\n      bool really_blocked = try_update_thread_state(thread,\n\t\t\t\t\t\t    Thread::STATE_BLOCKING,\n\t\t\t\t\t\t    Thread::STATE_BLOCKED);\n      if(!really_blocked) return;\n\n      while(true) {\n\tlong long old_work_counter = work_counter.read_counter();\n\n\tif(!resumable_workers.empty()) {\n\t  Thread *t = resumable_workers.get(0);\n\t  assert(t == thread);\n\t  log_py.debug() << \"awake again\";\n\t  return;\n\t}\n\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // if we got here through a cffi call, the GIL has already been released,\n    //  so try to handle that case here - a call PyEval_SaveThread\n    //  if the GIL is not held will assert-fail, and while a call to\n    //  PyThreadState_Swap is technically illegal (and unsafe if python-created\n    //  threads exist), it does what we want for now\n    // NOTE: we use PyEval_{Save,Restore}Thread here even if USE_PYGILSTATE_CALLS\n    //  is defined, as a call to PyGILState_Release will destroy a thread\n    //  context - the Save/Restore take care of the actual lock, and since we\n    //  restore each python thread on the OS thread that owned it intially, the\n    //  PyGILState TLS stuff should remain consistent\n    PyThreadState *saved = (pyproc->interpreter->api->PyThreadState_Swap)(0);\n    if(saved != 0) {\n      log_py.info() << \"python worker sleeping - releasing GIL\";\n      // put it back so we can save it properly\n      (pyproc->interpreter->api->PyThreadState_Swap)(saved);\n      // would like to sanity-check that this returns the expected thread state,\n      //  but that would require taking the PythonThreadTaskScheduler's lock\n      (pyproc->interpreter->api->PyEval_SaveThread)();\n      log_py.debug() << \"SaveThread -> \" << saved;\n    } else\n      log_py.info() << \"python worker sleeping - GIL already released\";\n    \n    KernelThreadTaskScheduler::thread_blocking(thread);\n\n    if(saved) {\n      log_py.info() << \"python worker awake - acquiring GIL\";\n      log_py.debug() << \"RestoreThread <- \" << saved;\n      (pyproc->interpreter->api->PyEval_RestoreThread)(saved);\n    } else\n      log_py.info() << \"python worker awake - not acquiring GIL\";\n  }\n\n  void PythonThreadTaskScheduler::thread_ready(Thread *thread)\n  {\n    // handle the wakening of the initialization thread specially\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n      resumable_workers.put(thread, 0);\n    } else {\n      KernelThreadTaskScheduler::thread_ready(thread);\n    }\n  }\n\n  void PythonThreadTaskScheduler::worker_terminate(Thread *switch_to)\n  {\n#ifdef USE_PYGILSTATE_CALLS\n    // nothing to do?  pythreads entry was a placeholder\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    pythreads.erase(it);\n\n#else\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    PyThreadState *pythread = it->second;\n    pythreads.erase(it);\n\n    log_py.debug() << \"destroying python thread: \" << pythread;\n    \n    // our thread should not be active\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\n    // switch to the master thread, retaining the GIL\n    log_py.debug() << \"RestoreThread <- \" << pyproc->master_thread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pyproc->master_thread);\n\n    // clear and delete the worker thread\n    (pyproc->interpreter->api->PyThreadState_Clear)(pythread);\n    (pyproc->interpreter->api->PyThreadState_Delete)(pythread);\n\n    // release the GIL\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pyproc->master_thread);\n#endif\n\n    // TODO: tear down interpreter if last thread\n    if(shutdown_flag.load() && pythreads.empty())\n      pyproc->destroy_interpreter();\n\n    KernelThreadTaskScheduler::worker_terminate(switch_to);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class LocalPythonProcessor\n\n  LocalPythonProcessor::LocalPythonProcessor(Processor _me, int _numa_node,\n                                             CoreReservationSet& crs,\n                                             size_t _stack_size,\n#ifdef REALM_USE_OPENMP\n\t\t\t\t\t     int _omp_workers,\n#endif\n\t\t\t\t\t     const std::vector<std::string>& _import_modules,\n\t\t\t\t\t     const std::vector<std::string>& _init_scripts)\n    : ProcessorImpl(_me, Processor::PY_PROC)\n    , numa_node(_numa_node)\n    , import_modules(_import_modules)\n    , init_scripts(_init_scripts)\n    , interpreter(0)\n    , ready_task_count(stringbuilder() << \"realm/proc \" << me << \"/ready tasks\")\n  {\n    task_queue.set_gauge(&ready_task_count);\n    deferred_spawn_cache.clear();\n\n    CoreReservationParameters params;\n    params.set_num_cores(1);\n    params.set_numa_domain(numa_node);\n    params.set_alu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_fpu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_ldst_usage(params.CORE_USAGE_SHARED);\n    params.set_max_stack_size(_stack_size);\n\n    std::string name = stringbuilder() << \"Python\" << numa_node << \" proc \" << _me;\n\n    core_rsrv = new CoreReservation(name, crs, params);\n\n#ifdef REALM_USE_OPENMP\n    if(_omp_workers > 0) {\n      // create a pool (except for one thread, which is the main task thread)\n      omp_threadpool = new ThreadPool(_omp_workers - 1,\n\t\t\t\t      name, -1 /*numa_node*/, _stack_size, crs);\n    } else\n      omp_threadpool = 0;\n#endif\n\n    sched = new PythonThreadTaskScheduler(this, *core_rsrv);\n    sched->add_task_queue(&task_queue);\n  }\n\n  LocalPythonProcessor::~LocalPythonProcessor(void)\n  {\n    delete core_rsrv;\n    delete sched;\n#ifdef REALM_USE_OPENMP\n    if(omp_threadpool != 0)\n      delete omp_threadpool;\n#endif\n  }\n\n  // starts worker threads and performs any per-processor initialization\n  void LocalPythonProcessor::start_threads(void)\n  {\n    // finally, fire up the scheduler\n    sched->start();\n  }\n\n  void LocalPythonProcessor::shutdown(void)\n  {\n    log_py.info() << \"shutting down\";\n\n    sched->shutdown();\n#ifdef REALM_USE_OPENMP\n    if(omp_threadpool != 0)\n      omp_threadpool->stop_worker_threads();\n#endif\n    deferred_spawn_cache.flush();\n  }\n\n  void LocalPythonProcessor::create_interpreter(void)\n  {\n    assert(interpreter == 0);\n  \n    // create a python interpreter that stays entirely within this thread\n    interpreter = new PythonInterpreter;\n    // the call to PyEval_InitThreads in the PythonInterpreter constructor\n    //  acquired the GIL on our behalf already\n    master_thread = (interpreter->api->PyThreadState_Get)();\n\n    // always need the python threading module\n    interpreter->import_module(\"threading\");\n    \n    // perform requested initialization\n    for(std::vector<std::string>::const_iterator it = import_modules.begin();\n\tit != import_modules.end();\n\t++it)\n      interpreter->import_module(*it);\n\n    for(std::vector<std::string>::const_iterator it = init_scripts.begin();\n\tit != init_scripts.end();\n\t++it)\n      interpreter->run_string(*it);\n\n    // default state is GIL _released_ - even if using PyGILState_* calls,\n    //  use PyEval_SaveThread here to release the lock without decrementing\n    //  the use count of our master thread\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n  }\n\n  void LocalPythonProcessor::destroy_interpreter(void)\n  {\n    assert(interpreter != 0);\n\n    log_py.info() << \"destroying interpreter\";\n\n    // take GIL with master thread\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (interpreter->api->PyGILState_Ensure)();\n    assert(gilstate == PyGILState_UNLOCKED);\n#else\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n#endif\n\n    // during shutdown, the threading module tries to remove the Thread object\n    //  associated with this kernel thread - if that doesn't exist (because we're\n    //  shutting down from a different thread that we initialized the interpreter\n    //  _and_ nobody called threading.current_thread() from this kernel thread),\n    //  we'll get a KeyError in threading.py\n    // resolve this by calling threading.current_thread() here, using __import__\n    //  to deal with the case where 'import threading' never got called\n    (interpreter->api->PyRun_SimpleString)(\"__import__('threading').current_thread()\");\n\n    delete interpreter;\n    interpreter = 0;\n    master_thread = 0;\n  }\n  \n  bool LocalPythonProcessor::perform_task_registration(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(treg->func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << treg->func_id;\n      assert(0);\n    }\n\n    // this can run arbitrary python code, which might ask which processor it's\n    //  on\n    ThreadLocal::current_processor = me;\n\n    // we'll take either a python function or a cpp function\n    PyObject *python_fnptr = 0;\n    Processor::TaskFuncPtr cpp_fnptr = 0;\n\n    do {\n      // prefer a python function, if it's available\n      {\n\tconst PythonSourceImplementation *psi = treg->codedesc->find_impl<PythonSourceImplementation>();\n\tif(psi) {\n\t  python_fnptr = interpreter->find_or_import_function(psi);\n\t  assert(python_fnptr != 0);\n\t  break;\n\t}\n      }\n\n      // take a function pointer, if that's available\n      {\n\tconst FunctionPointerImplementation *fpi = treg->codedesc->find_impl<FunctionPointerImplementation>();\n\tif(fpi) {\n\t  cpp_fnptr = (Processor::TaskFuncPtr)(fpi->fnptr);\n\t  break;\n\t}\n      }\n\n      // last try: can we convert something to a function pointer?\n      {\n\tconst std::vector<CodeTranslator *>& translators = get_runtime()->get_code_translators();\n\tbool ok = false;\n\tfor(std::vector<CodeTranslator *>::const_iterator it = translators.begin();\n\t    it != translators.end();\n\t    it++)\n\t  if((*it)->can_translate<FunctionPointerImplementation>(*(treg->codedesc))) {\n\t    FunctionPointerImplementation *fpi = (*it)->translate<FunctionPointerImplementation>(*(treg->codedesc));\n\t    if(fpi) {\n\t      cpp_fnptr = (Processor::TaskFuncPtr)(fpi->fnptr);\n\t      ok = true;\n\t      break;\n\t    }\n\t  }\n\tif(ok) break;\n      }\n\n      log_py.fatal() << \"invalid code descriptor for python proc: \" << *(treg->codedesc);\n      assert(0);\n    } while(0);\n\n    log_py.info() << \"task \" << treg->func_id << \" registered on \" << me << \": \" << *(treg->codedesc);\n\n    TaskTableEntry &tte = task_table[treg->func_id];\n    tte.python_fnptr = python_fnptr;\n    tte.cpp_fnptr = cpp_fnptr;\n    tte.user_data.swap(treg->user_data);\n\n    delete treg->codedesc;\n    delete treg;\n\n    return true;\n  }\n\n  void LocalPythonProcessor::enqueue_task(Task *task)\n  {\n    task_queue.enqueue_task(task);\n  }\n\n  void LocalPythonProcessor::enqueue_tasks(Task::TaskList& tasks, size_t num_tasks)\n  {\n    task_queue.enqueue_tasks(tasks, num_tasks);\n  }\n\n  void LocalPythonProcessor::spawn_task(Processor::TaskFuncID func_id,\n\t\t\t\t\tconst void *args, size_t arglen,\n\t\t\t\t\tconst ProfilingRequestSet &reqs,\n\t\t\t\t\tEvent start_event,\n\t\t\t\t\tGenEventImpl *finish_event,\n\t\t\t\t\tEventImpl::gen_t finish_gen,\n\t\t\t\t\tint priority)\n  {\n    // create a task object for this\n    Task *task = new Task(me, func_id, args, arglen, reqs,\n\t\t\t  start_event, finish_event, finish_gen, priority);\n    get_runtime()->optable.add_local_operation(finish_event->make_event(finish_gen), task);\n\n    enqueue_or_defer_task(task, start_event, &deferred_spawn_cache);\n  }\n\n  void LocalPythonProcessor::add_to_group(ProcessorGroupImpl *group)\n  {\n    // add the group's task queue to our scheduler too\n    sched->add_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::remove_from_group(ProcessorGroupImpl *group)\n  {\n    // remove the group's task queue from our scheduler\n    sched->remove_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::register_task(Processor::TaskFuncID func_id,\n                                           CodeDescriptor& codedesc,\n                                           const ByteArrayRef& user_data)\n  {\n    TaskRegistration *treg = new TaskRegistration;\n    treg->proc = this;\n    treg->func_id = func_id;\n    treg->codedesc = new CodeDescriptor(codedesc);\n    treg->user_data = user_data;\n    sched->add_internal_task(treg);\n  }\n\n  void LocalPythonProcessor::execute_task(Processor::TaskFuncID func_id,\n\t\t\t\t\t  const ByteArrayRef& task_args)\n  {\n    std::map<Processor::TaskFuncID, TaskTableEntry>::const_iterator it = task_table.find(func_id);\n    if(it == task_table.end()) {\n      // TODO: remove this hack once the tools are available to the HLR to call these directly\n      if(func_id < Processor::TASK_ID_FIRST_AVAILABLE) {\n\tlog_py.info() << \"task \" << func_id << \" not registered on \" << me << \": ignoring missing legacy setup/shutdown task\";\n\treturn;\n      }\n      log_py.fatal() << \"task \" << func_id << \" not registered on \" << me;\n      assert(0);\n    }\n\n    const TaskTableEntry& tte = it->second;\n\n    if(tte.python_fnptr != 0) {\n      // task is a python function - wrap arguments in python objects and call\n      log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": python function \" << ((void *)(tte.python_fnptr));\n\n      PyObject *arg1 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)task_args.base(),\n\t\t\t\t\t\t   task_args.size());\n      assert(arg1 != 0);\n      PyObject *arg2 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)tte.user_data.base(),\n\t\t\t\t\t\t   tte.user_data.size());\n      assert(arg2 != 0);\n      // TODO: make into a Python realm.Processor object\n      PyObject *arg3 = (interpreter->api->PyLong_FromUnsignedLong)(me.id);\n      assert(arg3 != 0);\n\n      PyObject *args = (interpreter->api->PyTuple_New)(3);\n      assert(args != 0);\n      (interpreter->api->PyTuple_SetItem)(args, 0, arg1);\n      (interpreter->api->PyTuple_SetItem)(args, 1, arg2);\n      (interpreter->api->PyTuple_SetItem)(args, 2, arg3);\n\n      //printf(\"args = \"); (interpreter->api->PyObject_Print)(args, stdout, 0); printf(\"\\n\");\n\n      PyObject *res = (interpreter->api->PyObject_CallObject)(tte.python_fnptr, args);\n\n      (interpreter->api->Py_DecRef)(args);\n\n      //printf(\"res = \"); PyObject_Print(res, stdout, 0); printf(\"\\n\");\n      if(res != 0) {\n\t(interpreter->api->Py_DecRef)(res);\n      } else {\n\tlog_py.fatal() << \"python exception occurred within task:\";\n\t(interpreter->api->PyErr_PrintEx)(0);\n\t(interpreter->api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n\tassert(0);\n      }\n    } else {\n      // no python function - better have a cpp function\n      assert(tte.cpp_fnptr != 0);\n\n      log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": cpp function \" << ((void *)(tte.cpp_fnptr));\n\n      (tte.cpp_fnptr)(task_args.base(), task_args.size(),\n\t\t      tte.user_data.base(), tte.user_data.size(),\n\t\t      me);\n    }\n  }\n\n  namespace Python {\n\n    ////////////////////////////////////////////////////////////////////////\n    //\n    // class PythonModule\n\n    /*static*/ std::vector<std::string> PythonModule::extra_import_modules;\n\n    PythonModule::PythonModule(void)\n      : Module(\"python\")\n      , cfg_num_python_cpus(0)\n      , cfg_use_numa(false)\n      , cfg_stack_size(2 << 20)\n#ifdef REALM_USE_OPENMP\n      , cfg_pyomp_threads(0)\n#endif\n    {\n    }\n\n    PythonModule::~PythonModule(void)\n    {}\n\n    /*static*/ void PythonModule::import_python_module(const char *module_name)\n    {\n      extra_import_modules.push_back(module_name);\n    }\n\n    /*static*/ Module *PythonModule::create_module(RuntimeImpl *runtime,\n                                                 std::vector<std::string>& cmdline)\n    {\n      // create a module to fill in with stuff - we'll delete it if numa is\n      //  disabled\n      PythonModule *m = new PythonModule;\n\n      // first order of business - read command line parameters\n      {\n        CommandLineParser cp;\n\n        cp.add_option_int(\"-ll:py\", m->cfg_num_python_cpus)\n\t  .add_option_int(\"-ll:pynuma\", m->cfg_use_numa)\n\t  .add_option_int_units(\"-ll:pystack\", m->cfg_stack_size, 'm')\n\t  .add_option_stringlist(\"-ll:pyimport\", m->cfg_import_modules)\n\t  .add_option_stringlist(\"-ll:pyinit\", m->cfg_init_scripts);\n#ifdef REALM_USE_OPENMP\n\tcp.add_option_int(\"-ll:pyomp\", m->cfg_pyomp_threads);\n#endif\n\n        bool ok = cp.parse_command_line(cmdline);\n        if(!ok) {\n          log_py.fatal() << \"error reading Python command line parameters\";\n          assert(false);\n        }\n      }\n\n      // add extra module imports requested by the application\n      m->cfg_import_modules.insert(m->cfg_import_modules.end(),\n                                   extra_import_modules.begin(),\n                                   extra_import_modules.end());\n\n      // if no cpus were requested, there's no point\n      if(m->cfg_num_python_cpus == 0) {\n        log_py.debug() << \"no Python cpus requested\";\n        delete m;\n        return 0;\n      }\n\n#ifndef REALM_USE_DLMOPEN\n      // Multiple CPUs are only allowed if we're using dlmopen.\n      if(m->cfg_num_python_cpus > 1) {\n        log_py.fatal() << \"support for multiple Python CPUs is not available: recompile with USE_DLMOPEN\";\n        assert(false);\n      }\n#endif\n\n      // get number/sizes of NUMA nodes -\n      //   disable (with a warning) numa binding if support not found\n      if(m->cfg_use_numa) {\n        std::map<int, NumaNodeCpuInfo> cpuinfo;\n        if(numasysif_numa_available() &&\n           numasysif_get_cpu_info(cpuinfo) &&\n           !cpuinfo.empty()) {\n          // filter out any numa domains with insufficient core counts\n          int cores_needed = m->cfg_num_python_cpus;\n          for(std::map<int, NumaNodeCpuInfo>::const_iterator it = cpuinfo.begin();\n              it != cpuinfo.end();\n              ++it) {\n            const NumaNodeCpuInfo& ci = it->second;\n            if(ci.cores_available >= cores_needed) {\n              m->active_numa_domains.insert(ci.node_id);\n            } else {\n              log_py.warning() << \"not enough cores in NUMA domain \" << ci.node_id << \" (\" << ci.cores_available << \" < \" << cores_needed << \")\";\n            }\n          }\n        } else {\n          log_py.warning() << \"numa support not found (or not working)\";\n          m->cfg_use_numa = false;\n        }\n      }\n\n      // if we don't end up with any active numa domains,\n      //  use NUMA_DOMAIN_DONTCARE\n      // actually, use the value (-1) since it seems to cause link errors!?\n      if(m->active_numa_domains.empty())\n        m->active_numa_domains.insert(-1 /*CoreReservationParameters::NUMA_DOMAIN_DONTCARE*/);\n\n      return m;\n    }\n\n    // do any general initialization - this is called after all configuration is\n    //  complete\n    void PythonModule::initialize(RuntimeImpl *runtime)\n    {\n      Module::initialize(runtime);\n    }\n\n    // create any processors provided by the module (default == do nothing)\n    //  (each new ProcessorImpl should use a Processor from\n    //   RuntimeImpl::next_local_processor_id)\n    void PythonModule::create_processors(RuntimeImpl *runtime)\n    {\n      Module::create_processors(runtime);\n\n      for(std::set<int>::const_iterator it = active_numa_domains.begin();\n          it != active_numa_domains.end();\n          ++it) {\n        int cpu_node = *it;\n        for(int i = 0; i < cfg_num_python_cpus; i++) {\n          Processor p = runtime->next_local_processor_id();\n          ProcessorImpl *pi = new LocalPythonProcessor(p, cpu_node,\n                                                       runtime->core_reservation_set(),\n                                                       cfg_stack_size,\n#ifdef REALM_USE_OPENMP\n\t\t\t\t\t\t       cfg_pyomp_threads,\n#endif\n\t\t\t\t\t\t       cfg_import_modules,\n\t\t\t\t\t\t       cfg_init_scripts);\n          runtime->add_processor(pi);\n\n          // create affinities between this processor and system/reg memories\n          // if the memory is one we created, use the kernel-reported distance\n          // to adjust the answer\n          std::vector<MemoryImpl *>& local_mems = runtime->nodes[Network::my_node_id].memories;\n          for(std::vector<MemoryImpl *>::iterator it2 = local_mems.begin();\n              it2 != local_mems.end();\n              ++it2) {\n            Memory::Kind kind = (*it2)->get_kind();\n            if((kind != Memory::SYSTEM_MEM) && (kind != Memory::REGDMA_MEM))\n              continue;\n\n            Machine::ProcessorMemoryAffinity pma;\n            pma.p = p;\n            pma.m = (*it2)->me;\n\n            // use the same made-up numbers as in\n            //  runtime_impl.cc\n            if(kind == Memory::SYSTEM_MEM) {\n              pma.bandwidth = 100;  // \"large\"\n              pma.latency = 5;      // \"small\"\n            } else {\n              pma.bandwidth = 80;   // \"large\"\n              pma.latency = 10;     // \"small\"\n            }\n\n            runtime->add_proc_mem_affinity(pma);\n          }\n        }\n      }\n    }\n\n    // clean up any common resources created by the module - this will be called\n    //  after all memories/processors/etc. have been shut down and destroyed\n    void PythonModule::cleanup(void)\n    {\n      Module::cleanup();\n    }\n\n  }; // namespace Python\n\n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/runtime/legion/runtime.cc": "/* Copyright 2021 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"legion.h\"\n#include \"legion/runtime.h\"\n#include \"legion/legion_ops.h\"\n#include \"legion/legion_tasks.h\"\n#include \"legion/legion_trace.h\"\n#include \"legion/legion_utilities.h\"\n#include \"legion/region_tree.h\"\n#include \"legion/legion_spy.h\"\n#include \"legion/legion_profiling.h\"\n#include \"legion/legion_instances.h\"\n#include \"legion/legion_views.h\"\n#include \"legion/legion_context.h\"\n#include \"legion/legion_replication.h\"\n#include \"legion/mapper_manager.h\"\n#include \"legion/garbage_collection.h\"\n#include \"mappers/default_mapper.h\"\n#include \"mappers/test_mapper.h\"\n#include \"mappers/replay_mapper.h\"\n#include \"mappers/debug_mapper.h\"\n#include \"realm/cmdline.h\"\n\n#include <unistd.h> // sleep for warnings\n\n#include <sys/mman.h>\n#ifdef LEGION_USE_CUDA\n#include <cuda.h>\n#endif\n\n#define REPORT_DUMMY_CONTEXT(message)                        \\\n  REPORT_LEGION_ERROR(ERROR_DUMMY_CONTEXT_OPERATION,  message)\n\nnamespace Legion {\n  namespace Internal {\n\n    // If you add a logger, update the LEGION_EXTERN_LOGGER_DECLARATIONS\n    // macro in legion_types.h\n    Realm::Logger log_run(\"runtime\");\n    Realm::Logger log_task(\"tasks\");\n    Realm::Logger log_index(\"index_spaces\");\n    Realm::Logger log_field(\"field_spaces\");\n    Realm::Logger log_region(\"regions\");\n    Realm::Logger log_inst(\"instances\");\n    Realm::Logger log_variant(\"variants\");\n    Realm::Logger log_allocation(\"allocation\");\n    Realm::Logger log_migration(\"migration\");\n    Realm::Logger log_prof(\"legion_prof\");\n    Realm::Logger log_garbage(\"legion_gc\");\n    Realm::Logger log_shutdown(\"shutdown\");\n    Realm::Logger log_tracing(\"tracing\");\n    Realm::Logger log_eager(\"eager\");\n    namespace LegionSpy {\n      Realm::Logger log_spy(\"legion_spy\");\n    };\n\n    __thread TaskContext *implicit_context = NULL;\n    __thread Runtime *implicit_runtime = NULL;\n    __thread AutoLock *local_lock_list = NULL;\n    __thread UniqueID implicit_provenance = 0;\n    __thread unsigned inside_registration_callback = NO_REGISTRATION_CALLBACK;\n#ifdef DEBUG_LEGION_WAITS\n    __thread int meta_task_id = -1;\n#endif\n\n    const LgEvent LgEvent::NO_LG_EVENT = LgEvent();\n    const ApEvent ApEvent::NO_AP_EVENT = ApEvent();\n    const ApUserEvent ApUserEvent::NO_AP_USER_EVENT = ApUserEvent();\n    const ApBarrier ApBarrier::NO_AP_BARRIER = ApBarrier();\n    const RtEvent RtEvent::NO_RT_EVENT = RtEvent();\n    const RtUserEvent RtUserEvent::NO_RT_USER_EVENT = RtUserEvent();\n    const RtBarrier RtBarrier::NO_RT_BARRIER = RtBarrier();\n    const PredEvent PredEvent::NO_PRED_EVENT = PredEvent();\n\n    /////////////////////////////////////////////////////////////\n    // Argument Map Impl\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ArgumentMapImpl::ArgumentMapImpl(void)\n      : Collectable(), runtime(implicit_runtime),\n        future_map(NULL), point_set(Domain::NO_DOMAIN), dimensionality(0), \n        dependent_futures(0), update_point_set(false), own_point_set(false), \n        equivalent(false)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ArgumentMapImpl::ArgumentMapImpl(const FutureMap &rhs)\n      : Collectable(), runtime(implicit_runtime), future_map(rhs.impl), \n        dependent_futures(0), update_point_set(false), own_point_set(false), \n        equivalent(false)\n    //--------------------------------------------------------------------------\n    {\n      if (future_map.impl != NULL)\n      {\n        point_set = future_map.impl->get_domain();\n        dimensionality = point_set.get_dim();\n      }\n      else\n      {\n        point_set = Domain::NO_DOMAIN;\n        dimensionality = 0;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    ArgumentMapImpl::ArgumentMapImpl(const ArgumentMapImpl &impl)\n      : Collectable(), runtime(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // This should never ever be called\n      assert(false);\n    }\n    \n    //--------------------------------------------------------------------------\n    ArgumentMapImpl::~ArgumentMapImpl(void)\n    //--------------------------------------------------------------------------\n    {\n      if (own_point_set)\n        free_point_set(); \n    }\n\n    //--------------------------------------------------------------------------\n    ArgumentMapImpl& ArgumentMapImpl::operator=(const ArgumentMapImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // This should never ever be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    bool ArgumentMapImpl::has_point(const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      if (dimensionality > 0)\n      {\n        const unsigned point_dim = point.get_dim();\n        if (point_dim != dimensionality)\n          REPORT_LEGION_ERROR(ERROR_ARGUMENT_MAP_DIMENSIONALITY,\n              \"Mismatch in dimensionality in 'has_point' on ArgumentMap \"\n              \"with %d dimensions and point with %d dimensions. ArgumentMaps \"\n              \"must always contain points of the same dimensionality.\",\n              dimensionality, point_dim)\n      }\n      if (point_set.exists() && !update_point_set && point_set.contains(point))\n        return true;\n      if (future_map.impl != NULL)\n        unfreeze();\n      return (arguments.find(point) != arguments.end());\n    }\n\n    //--------------------------------------------------------------------------\n    void ArgumentMapImpl::set_point(const DomainPoint &point, \n                                const TaskArgument &arg,\n                                bool replace)\n    //--------------------------------------------------------------------------\n    {\n      if (dimensionality > 0)\n      {\n        const unsigned point_dim = point.get_dim();\n        if (point_dim != dimensionality)\n          REPORT_LEGION_ERROR(ERROR_ARGUMENT_MAP_DIMENSIONALITY,\n              \"Mismatch in dimensionality in 'set_point' on ArgumentMap \"\n              \"with %d dimensions and point with %d dimensions. ArgumentMaps \"\n              \"must always contain points of the same dimensionality.\",\n              dimensionality, point_dim)\n      }\n      else\n      {\n        dimensionality = point.get_dim();\n#ifdef DEBUG_LEGION\n        assert(dimensionality > 0);\n#endif\n      }\n      if (!replace and point_set.exists() && !update_point_set &&\n          point_set.contains(point))\n        return;\n      if (future_map.impl != NULL)\n        unfreeze();\n      std::map<DomainPoint,Future>::iterator finder = arguments.find(point);\n      if (finder != arguments.end())\n      {\n        // If it already exists and we're not replacing it then we're done\n        if (!replace)\n          return;\n        if (finder->second.impl->producer_op != NULL)\n        {\n#ifdef DEBUG_LEGION\n          assert(dependent_futures > 0);\n#endif\n          dependent_futures--;\n        }\n        if (arg.get_size() > 0)\n          finder->second = \n            Future::from_untyped_pointer(runtime->external,\n                                         arg.get_ptr(), arg.get_size());\n        else\n          finder->second = Future();\n      }\n      else\n      {\n        if (arg.get_size() > 0)\n          arguments[point] = \n            Future::from_untyped_pointer(runtime->external,\n                                         arg.get_ptr(), arg.get_size());\n        else\n          arguments[point] = Future();\n        // Had to add a new point so the point set is no longer value\n        update_point_set = true;\n      }\n      // If we modified things then they are no longer equivalent\n      if (future_map.impl != NULL)\n      {\n        equivalent = false;\n        future_map = FutureMap();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ArgumentMapImpl::set_point(const DomainPoint &point, \n                                    const Future &f, bool replace)\n    //--------------------------------------------------------------------------\n    {\n      if (dimensionality > 0)\n      {\n        const unsigned point_dim = point.get_dim();\n        if (point_dim != dimensionality)\n          REPORT_LEGION_ERROR(ERROR_ARGUMENT_MAP_DIMENSIONALITY,\n              \"Mismatch in dimensionality in 'set_point' on ArgumentMap \"\n              \"with %d dimensions and point with %d dimensions. ArgumentMaps \"\n              \"must always contain points of the same dimensionality.\",\n              dimensionality, point_dim)\n      }\n      else\n      {\n        dimensionality = point.get_dim();\n#ifdef DEBUG_LEGION\n        assert(dimensionality > 0);\n#endif\n      }\n      if (!replace and point_set.exists() && !update_point_set &&\n          point_set.contains(point))\n        return;\n      if (future_map.impl != NULL)\n        unfreeze();\n      std::map<DomainPoint,Future>::iterator finder = arguments.find(point);\n      if (finder != arguments.end())\n      {\n        // If it already exists and we're not replacing it then we're done\n        if (!replace)\n          return;\n        if (finder->second.impl->producer_op != NULL)\n        {\n#ifdef DEBUG_LEGION\n          assert(dependent_futures > 0);\n#endif\n          dependent_futures--;\n        }\n        finder->second = f; \n        \n      }\n      else\n      {\n        arguments[point] = f;\n        // Had to add a new point so the point set is no longer valid\n        update_point_set = true;\n      }\n      if (f.impl->producer_op != NULL)\n          dependent_futures++;\n      // If we modified things then they are no longer equivalent\n      if (future_map.impl != NULL)\n      {\n        equivalent = false;\n        future_map = FutureMap();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool ArgumentMapImpl::remove_point(const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      if (dimensionality > 0)\n      {\n        const unsigned point_dim = point.get_dim();\n        if (point_dim != dimensionality)\n          REPORT_LEGION_ERROR(ERROR_ARGUMENT_MAP_DIMENSIONALITY,\n              \"Mismatch in dimensionality in 'remove_point' on ArgumentMap \"\n              \"with %d dimensions and point with %d dimensions. ArgumentMaps \"\n              \"must always contain points of the same dimensionality.\",\n              dimensionality, point_dim)\n      }\n      else\n      {\n        dimensionality = point.get_dim();\n#ifdef DEBUG_LEGION\n        assert(dimensionality > 0);\n#endif\n      }\n      if (point_set.exists() && !update_point_set && !point_set.contains(point))\n        return false;\n      if (future_map.impl != NULL)\n        unfreeze();\n      std::map<DomainPoint,Future>::iterator finder = arguments.find(point);\n      if (finder != arguments.end())\n      {\n        if (finder->second.impl->producer_op != NULL)\n        {\n#ifdef DEBUG_LEGION\n          assert(dependent_futures > 0);\n#endif\n          dependent_futures--;\n        }\n        arguments.erase(finder);\n        // If we modified things then they are no longer equivalent\n        if (future_map.impl != NULL)\n        {\n          equivalent = false;\n          future_map = FutureMap();\n        }\n        // We removed a point so the point set is no longer valid\n        update_point_set = true;\n        return true;\n      }\n      return false;\n    }\n\n    //--------------------------------------------------------------------------\n    TaskArgument ArgumentMapImpl::get_point(const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      if (dimensionality > 0)\n      {\n        const unsigned point_dim = point.get_dim();\n        if (point_dim != dimensionality)\n          REPORT_LEGION_ERROR(ERROR_ARGUMENT_MAP_DIMENSIONALITY,\n              \"Mismatch in dimensionality in 'get_point' on ArgumentMap \"\n              \"with %d dimensions and point with %d dimensions. ArgumentMaps \"\n              \"must always contain points of the same dimensionality.\",\n              dimensionality, point_dim)\n      }\n      if (point_set.exists() && !update_point_set && !point_set.contains(point))\n        return TaskArgument();\n      if (future_map.impl != NULL)\n        unfreeze();\n      std::map<DomainPoint,Future>::const_iterator finder=arguments.find(point);\n      if ((finder == arguments.end()) || (finder->second.impl == NULL))\n        return TaskArgument();\n      return TaskArgument(finder->second.impl->get_untyped_result(),\n                          finder->second.impl->get_untyped_size());\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMap ArgumentMapImpl::freeze(TaskContext *ctx)\n    //--------------------------------------------------------------------------\n    {\n      // If we already have a future map then we are good\n      if (future_map.impl != NULL)\n        return future_map;\n      // If we have no futures then we can return an empty map\n      if (arguments.empty())\n        return FutureMap();\n      // Compute the point set if needed\n      if (update_point_set)\n      {\n        // Free the existing point set if we're going to make a new one\n        if (own_point_set)\n          free_point_set();\n        if (!arguments.empty())\n        {\n          switch (dimensionality)\n          {\n#define DIMFUNC(DIM) \\\n            case DIM: \\\n            { \\\n              std::vector<Realm::Point<DIM,coord_t> > points(arguments.size());\\\n              unsigned index = 0; \\\n              for (std::map<DomainPoint,Future>::const_iterator it = \\\n                    arguments.begin(); it != arguments.end(); it++) \\\n              { \\\n                const Point<DIM,coord_t> point = it->first; \\\n                points[index++] = point; \\\n              } \\\n              const Realm::IndexSpace<DIM,coord_t> space(points); \\\n              const DomainT<DIM,coord_t> domaint(space); \\\n              point_set = domaint; \\\n              break; \\\n            }\n            LEGION_FOREACH_N(DIMFUNC)\n#undef DIMFUNC\n            default:\n              assert(false);\n          }\n          // We only need to count as owning this if it is not dense\n          own_point_set = !point_set.dense();\n        }\n        else\n        {\n          point_set = Domain::NO_DOMAIN;\n          own_point_set = false;\n        }\n        update_point_set = false;\n      }\n      RtUserEvent deletion_precondition;\n      // If we own the point set then we need to know when everyone is\n      // done using it so we can delete it\n      if (own_point_set)\n      {\n        deletion_precondition = Runtime::create_rt_user_event();\n        point_set_deletion_preconditions.insert(deletion_precondition);\n      }\n      // See if we have any dependent future points, if we do then we need\n      // to launch an explicit creation operation to ensure we get the right\n      // mapping dependences for this future map\n      if (dependent_futures == 0 && !runtime->safe_control_replication)\n      {\n        // Otherwise we have to make a future map and set all the futures\n        // We know that they are already completed \n        DistributedID did = runtime->get_available_distributed_id();\n        future_map = FutureMap(new FutureMapImpl(ctx, runtime, point_set, did,\n          0/*index*/, runtime->address_space, RtEvent::NO_RT_EVENT, \n          true/*reg now*/, deletion_precondition));\n        future_map.impl->set_all_futures(arguments);\n      }\n      else\n        future_map = ctx->construct_future_map(point_set, arguments,\n                           deletion_precondition, true/*internal*/);\n      equivalent = true; // mark that these are equivalent\n      dependent_futures = 0; // reset this for the next unpack \n      return future_map;\n    }\n\n    //--------------------------------------------------------------------------\n    void ArgumentMapImpl::unfreeze(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(future_map.impl != NULL);\n#endif\n      // If they are already equivalent then we're done\n      if (equivalent)\n        return;\n      // Otherwise we need to make them equivalent\n      future_map.impl->get_all_futures(arguments);\n      point_set = future_map.impl->get_domain();\n      update_point_set = false;\n      own_point_set = false;\n      // Count how many dependent futures we have\n#ifdef DEBUG_LEGION\n      assert(dependent_futures == 0);\n#endif\n      for (std::map<DomainPoint,Future>::const_iterator it = \n            arguments.begin(); it != arguments.end(); it++)\n        if (it->second.impl->producer_op != NULL)\n          dependent_futures++;\n      equivalent = true;\n    }\n\n    //--------------------------------------------------------------------------\n    void ArgumentMapImpl::free_point_set(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(own_point_set);\n      assert(point_set.exists());\n#endif\n      RtEvent precondition;\n      if (!point_set_deletion_preconditions.empty())\n      {\n        precondition = Runtime::merge_events(point_set_deletion_preconditions);\n        point_set_deletion_preconditions.clear();\n      }\n      switch (dimensionality)\n      {\n#define DIMFUNC(DIM) \\\n        case DIM: \\\n        { \\\n          DomainT<DIM,coord_t> is = point_set; \\\n          is.destroy(precondition); \\\n          break; \\\n        }\n        LEGION_FOREACH_N(DIMFUNC)\n#undef DIMFUNC\n        default:\n          assert(false);\n      }\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Field Allocator Impl\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    FieldAllocatorImpl::FieldAllocatorImpl(FieldSpaceNode *n, TaskContext *ctx,\n                                           RtEvent ready)\n      : field_space(n->handle), node(n), context(ctx), ready_event(ready)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(node != NULL);\n      assert(context != NULL);\n#endif\n      context->add_reference();\n      node->add_base_resource_ref(FIELD_ALLOCATOR_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    FieldAllocatorImpl::FieldAllocatorImpl(const FieldAllocatorImpl &rhs)\n      : field_space(rhs.field_space), node(rhs.node), context(rhs.context), \n        ready_event(rhs.ready_event)\n    //--------------------------------------------------------------------------\n    {\n      // Should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    FieldAllocatorImpl::~FieldAllocatorImpl(void)\n    //--------------------------------------------------------------------------\n    {\n      context->destroy_field_allocator(node);\n      if (context->remove_reference())\n        delete context;\n      if (node->remove_base_resource_ref(FIELD_ALLOCATOR_REF))\n        delete node;\n    }\n\n    //--------------------------------------------------------------------------\n    FieldAllocatorImpl& FieldAllocatorImpl::operator=(\n                                                  const FieldAllocatorImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // Should never be called\n      assert(false);\n      return *this;\n    }\n    \n    //--------------------------------------------------------------------------\n    FieldID FieldAllocatorImpl::allocate_field(size_t field_size,\n                                               FieldID desired_fieldid,\n                                               CustomSerdezID serdez_id, \n                                               bool local)\n    //--------------------------------------------------------------------------\n    {\n      // Need to wait for this allocator to be ready\n      if (ready_event.exists() && !ready_event.has_triggered())\n        ready_event.wait();\n      return context->allocate_field(field_space, field_size, desired_fieldid,\n                                     local, serdez_id);\n    }\n\n    //--------------------------------------------------------------------------\n    FieldID FieldAllocatorImpl::allocate_field(const Future &field_size,\n                                               FieldID desired_fieldid,\n                                               CustomSerdezID serdez_id, \n                                               bool local)\n    //--------------------------------------------------------------------------\n    {\n      // Need to wait for this allocator to be ready\n      if (ready_event.exists() && !ready_event.has_triggered())\n        ready_event.wait();\n      return context->allocate_field(field_space, field_size, desired_fieldid, \n                                     local, serdez_id);\n    }\n\n    //--------------------------------------------------------------------------\n    void FieldAllocatorImpl::free_field(FieldID fid, const bool unordered)\n    //--------------------------------------------------------------------------\n    {\n      // Don't need to wait here since deletion operations catch\n      // dependences on the allocator themselves\n      context->free_field(this, field_space, fid, unordered);\n    }\n\n    //--------------------------------------------------------------------------\n    void FieldAllocatorImpl::allocate_fields(\n                                        const std::vector<size_t> &field_sizes,\n                                        std::vector<FieldID> &resulting_fields,\n                                        CustomSerdezID serdez_id, bool local)\n    //--------------------------------------------------------------------------\n    {\n      // Need to wait for this allocator to be ready\n      if (ready_event.exists() && !ready_event.has_triggered())\n        ready_event.wait();\n      context->allocate_fields(field_space, field_sizes, resulting_fields,\n                               local, serdez_id);\n    }\n\n    //--------------------------------------------------------------------------\n    void FieldAllocatorImpl::allocate_fields(\n                                        const std::vector<Future> &field_sizes,\n                                        std::vector<FieldID> &resulting_fields,\n                                        CustomSerdezID serdez_id, bool local)\n    //--------------------------------------------------------------------------\n    {\n      // Need to wait for this allocator to be ready\n      if (ready_event.exists() && !ready_event.has_triggered())\n        ready_event.wait();\n      context->allocate_fields(field_space, field_sizes, resulting_fields, \n                               local, serdez_id);\n    }\n\n    //--------------------------------------------------------------------------\n    void FieldAllocatorImpl::free_fields(const std::set<FieldID> &to_free,\n                                         const bool unordered)\n    //--------------------------------------------------------------------------\n    {\n      // Don't need to wait here since deletion operations catch\n      // dependences on the allocator themselves\n      context->free_fields(this, field_space, to_free, unordered);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Future Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    FutureImpl::FutureImpl(Runtime *rt, bool register_now, DistributedID did,\n            AddressSpaceID own_space, ApEvent complete, Operation *o /*= NULL*/,\n            bool compute_coordinates)\n      : DistributedCollectable(rt, \n          LEGION_DISTRIBUTED_HELP_ENCODE(did, FUTURE_DC), \n          own_space, register_now),\n        producer_op(o), op_gen((o == NULL) ? 0 : o->get_generation()),\n        producer_depth((o == NULL) ? -1 : o->get_context()->get_depth()),\n#ifdef LEGION_SPY\n        producer_uid((o == NULL) ? 0 : o->get_unique_op_id()),\n#endif\n        future_complete(complete), result(NULL), result_size(0), \n        result_set_space(local_space), callback_functor(NULL),\n        own_callback_functor(false), empty(true), sampled(false)\n    //--------------------------------------------------------------------------\n    {\n      if (producer_op != NULL)\n      {\n        producer_op->add_mapping_reference(op_gen);\n        if (compute_coordinates && runtime->safe_control_replication)\n          producer_op->compute_future_coordinates(coordinates);\n      }\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Future %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl::FutureImpl(Runtime *rt, bool register_now, DistributedID did,\n                       AddressSpaceID own_space, ApEvent complete,\n                       Operation *o, GenerationID gen,\n#ifdef LEGION_SPY\n                       UniqueID uid,\n#endif\n                       int depth)\n      : DistributedCollectable(rt, \n          LEGION_DISTRIBUTED_HELP_ENCODE(did, FUTURE_DC), \n          own_space, register_now),\n        producer_op(o), op_gen(gen), producer_depth(depth),\n#ifdef LEGION_SPY\n        producer_uid(uid),\n#endif\n        future_complete(complete), result(NULL), result_size(0), \n        result_set_space(local_space), callback_functor(NULL),\n        own_callback_functor(false), empty(true), sampled(false)\n    //--------------------------------------------------------------------------\n    {\n      if (producer_op != NULL)\n        producer_op->add_mapping_reference(op_gen);\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Future %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl::FutureImpl(const FutureImpl &rhs)\n      : DistributedCollectable(NULL, 0, 0), producer_op(NULL), op_gen(0),\n        producer_depth(0)\n#ifdef LEGION_SPY\n        , producer_uid(0)\n#endif\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl::~FutureImpl(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!subscription_event.exists());\n#endif\n      // Remove the extra reference on a remote set future if there is one\n      if (empty && (result_set_space != local_space))\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(did);\n          rez.serialize<size_t>(0);\n        }\n        runtime->send_future_broadcast(result_set_space, rez);\n      }\n      if (result != NULL)\n      {\n        free(result);\n        result = NULL;\n        result_size = 0;\n      }\n      if (producer_op != NULL)\n        producer_op->remove_mapping_reference(op_gen);\n      if (callback_functor != NULL)\n        callback_functor->callback_release_future();\n      if (own_callback_functor)\n        delete callback_functor;\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl& FutureImpl::operator=(const FutureImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::wait(bool silence_warnings, const char *warning_string)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime->runtime_warnings && !silence_warnings && \n          (implicit_context != NULL))\n      {\n        if (!implicit_context->is_leaf_context())\n          REPORT_LEGION_WARNING(LEGION_WARNING_WAITING_FUTURE_NONLEAF, \n             \"Waiting on a future in non-leaf task %s \"\n             \"(UID %lld) is a violation of Legion's deferred execution model \"\n             \"best practices. You may notice a severe performance \"\n             \"degradation. Warning string: %s\",\n             implicit_context->get_task_name(), \n             implicit_context->get_unique_id(),\n             (warning_string == NULL) ? \"\" : warning_string)\n      }\n      if ((implicit_context != NULL) && !runtime->separate_runtime_instances)\n        implicit_context->record_blocking_call();\n      bool poisoned = false;\n      if (!future_complete.has_triggered_faultaware(poisoned))\n      {\n        TaskContext *context = implicit_context;\n        if (context != NULL)\n        {\n          context->begin_task_wait(false/*from runtime*/);\n          future_complete.wait_faultaware(poisoned);\n          context->end_task_wait();\n        }\n        else\n          future_complete.wait_faultaware(poisoned);\n      }\n      if (poisoned)\n        implicit_context->raise_poison_exception();\n      mark_sampled();\n    }\n    \n    //--------------------------------------------------------------------------\n    void* FutureImpl::get_untyped_result(bool silence_warnings,\n                                      const char *warning_string, bool internal, \n                                      bool check_size, size_t future_size)\n    //--------------------------------------------------------------------------\n    {\n      if (!internal)\n      {\n        if (runtime->runtime_warnings && !silence_warnings && \n            (implicit_context != NULL))\n        {\n          if (!implicit_context->is_leaf_context())\n            REPORT_LEGION_WARNING(LEGION_WARNING_WAITING_FUTURE_NONLEAF, \n               \"Waiting on a future in non-leaf task %s \"\n               \"(UID %lld) is a violation of Legion's deferred execution model \"\n               \"best practices. You may notice a severe performance \"\n               \"degradation. Warning string: %s\",\n               implicit_context->get_task_name(), \n               implicit_context->get_unique_id(),\n               (warning_string == NULL) ? \"\" : warning_string)\n        }\n        if ((implicit_context != NULL) && !runtime->separate_runtime_instances)\n          implicit_context->record_blocking_call();\n      }\n      if (internal)\n      {\n        const RtEvent ready_event = subscribe_internal();\n        if (!ready_event.has_triggered())\n        {\n          TaskContext *context = implicit_context;\n          if (context != NULL)\n          {\n            context->begin_task_wait(false/*from runtime*/);\n            ready_event.wait();\n            context->end_task_wait();\n          }\n          else\n            ready_event.wait();\n        }\n      }\n      else\n      {\n        const ApEvent ready_event = subscribe();\n        bool poisoned = false;\n        if (!ready_event.has_triggered_faultaware(poisoned))\n        {\n          TaskContext *context = implicit_context;\n          if (context != NULL)\n          {\n            context->begin_task_wait(false/*from runtime*/);\n            ready_event.wait_faultaware(poisoned);\n            context->end_task_wait();\n          }\n          else\n            ready_event.wait_faultaware(poisoned);\n        }\n        if (poisoned)\n          implicit_context->raise_poison_exception();\n      }\n      if (check_size)\n      {\n        if (empty)\n          REPORT_LEGION_ERROR(ERROR_REQUEST_FOR_EMPTY_FUTURE, \n                              \"Accessing empty future! (UID %lld)\",\n                              (producer_op == NULL) ? 0 :\n                                producer_op->get_unique_op_id())\n        else if (future_size != result_size)\n          REPORT_LEGION_ERROR(ERROR_FUTURE_SIZE_MISMATCH,\n              \"Future size mismatch! Expected type of %zd bytes but \"\n              \"requested type is %zd bytes. (UID %lld)\", \n              result_size, future_size, (producer_op == NULL) ? 0 : \n              producer_op->get_unique_op_id())\n      }\n      mark_sampled();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    size_t FutureImpl::get_untyped_size(bool internal)\n    //--------------------------------------------------------------------------\n    {\n      // Call this first to make sure the future is ready\n      get_untyped_result(true, NULL, internal);\n      return result_size;\n    }\n\n    //--------------------------------------------------------------------------\n    bool FutureImpl::is_empty(bool block, bool silence_warnings,\n                              const char *warning_string, bool internal)\n    //--------------------------------------------------------------------------\n    {\n      if (!internal)\n      {\n        if (runtime->runtime_warnings && !silence_warnings && \n            (producer_op != NULL))\n        {\n          TaskContext *context = producer_op->get_context();\n          if (!context->is_leaf_context())\n            REPORT_LEGION_WARNING(LEGION_WARNING_BLOCKING_EMPTY, \n                \"Performing a blocking is_empty test on a \"\n                \"in non-leaf task %s (UID %lld) is a violation of Legion's \"\n                \"deferred execution model best practices. You may notice a \"\n                \"severe performance degradation. Warning string: %s\", \n                context->get_task_name(), \n                context->get_unique_id(),\n                (warning_string == NULL) ? \"\" : warning_string)\n        }\n        if (block && producer_op != NULL && Internal::implicit_context != NULL)\n          Internal::implicit_context->record_blocking_call();\n      }\n      if (block)\n      {\n        const ApEvent ready_event = subscribe();\n        bool poisoned = false;\n        if (!ready_event.has_triggered_faultaware(poisoned))\n        {\n          TaskContext *context =\n            (producer_op == NULL) ? NULL : producer_op->get_context();\n          if (context != NULL)\n          {\n            context->begin_task_wait(false/*from runtime*/);\n            ready_event.wait_faultaware(poisoned);\n            context->end_task_wait();\n          }\n          else\n            ready_event.wait_faultaware(poisoned);\n        }\n        if (poisoned)\n          implicit_context->raise_poison_exception();\n        mark_sampled();\n      }\n      return empty;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::set_result(const void *args, size_t arglen, bool own)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(future_lock);\n      if (!empty || (callback_functor != NULL))\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_FUTURE_SET,\n            \"Duplicate future set! This can be either a runtime bug or a \"\n            \"user error. If you have a must epoch launch in this program \"\n            \"please check that all of the point tasks that it creates have \"\n            \"unique index points. If your program has no must epoch launches \"\n            \"then this is likely a runtime bug.\")\n      if (own)\n      {\n        result = const_cast<void*>(args);\n        result_size = arglen;\n      }\n      else\n      {\n        result_size = arglen;\n        result = malloc(result_size);\n        memcpy(result,args,result_size);\n      }\n      finish_set_future();\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::set_result(FutureFunctor *functor, bool own,Processor proc)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(proc.kind() != Processor::UTIL_PROC);\n#endif\n      AutoLock f_lock(future_lock);\n      if (!empty || (callback_functor != NULL))\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_FUTURE_SET,\n            \"Duplicate future set! This can be either a runtime bug or a \"\n            \"user error. If you have a must epoch launch in this program \"\n            \"please check that all of the point tasks that it creates have \"\n            \"unique index points. If your program has no must epoch launches \"\n            \"then this is likely a runtime bug.\")\n      callback_functor = functor;\n      own_callback_functor = own;\n      callback_proc = proc;\n      finish_set_future();\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::finish_set_future(void)\n    //--------------------------------------------------------------------------\n    {\n      // must be called while we are already holding the lock\n      empty = false; \n      if (!is_owner())\n      {\n        // Add an extra reference to prevent this from being collected\n        // until the owner is also deleted, the owner will notify us\n        // they are deleted with a broadcast of size 0 when they are deleted\n        add_base_resource_ref(RUNTIME_REF);\n        // If we're the first set then we need to tell the owner\n        // that we are the ones with the value\n        // This is literally an empty message\n        Serializer rez;\n        rez.serialize(did);\n        runtime->send_future_notification(owner_space, rez); \n      }\n      else if (!subscribers.empty())\n        broadcast_result(subscribers, future_complete, false/*need lock*/);\n      if (subscription_internal.exists())\n      {\n        Runtime::trigger_event(subscription_internal);\n        if (!subscription_event.exists() && \n            remove_base_resource_ref(RUNTIME_REF))\n          assert(false); // should always hold reference from caller\n      }\n      if (subscription_event.exists())\n      {\n        // Be very careful here, it might look like you can trigger the\n        // subscription event immediately on the owner node but you can't\n        // because we still rely on futures to propagate privileges when\n        // return region tree types\n        if (future_complete != subscription_event)\n          Runtime::trigger_event(NULL, subscription_event, future_complete);\n        else\n          Runtime::trigger_event(NULL, subscription_event);\n        subscription_event = ApUserEvent::NO_AP_USER_EVENT;\n        if (remove_base_resource_ref(RUNTIME_REF))\n          assert(false); // should always hold a reference from caller\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    ApEvent FutureImpl::invoke_callback(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(callback_functor != NULL);\n#endif\n      if (!callback_ready.exists())\n      {\n        callback_ready = Runtime::create_ap_user_event(NULL);\n        FutureCallbackArgs args(this);    \n        runtime->issue_application_processor_task(args, \n            LG_LATENCY_WORK_PRIORITY, callback_proc);\n      }\n      return callback_ready;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::perform_callback(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(callback_functor != NULL);\n      assert(callback_ready.exists());\n#endif\n      result_size = callback_functor->callback_get_future_size();\n      if (result_size > 0)\n      {\n        result = malloc(result_size);\n        callback_functor->callback_pack_future(result, result_size);\n      }\n      callback_functor->callback_release_future();\n      if (own_callback_functor)\n        delete callback_functor;\n      // Retake the lock and remove the guards\n      AutoLock f_lock(future_lock);\n      callback_functor = NULL;\n      Runtime::trigger_event(NULL, callback_ready, future_complete);\n      // Check for any subscribers that we need to tell about the result\n      if (!subscribers.empty())\n        broadcast_result(subscribers, future_complete, false/*need lock*/);\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl::FutureCallbackArgs::FutureCallbackArgs(FutureImpl *i)\n      : LgTaskArgs<FutureCallbackArgs>(implicit_provenance), impl(i)\n    //--------------------------------------------------------------------------\n    {\n      impl->add_base_gc_ref(DEFERRED_TASK_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureImpl::handle_callback(const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const FutureCallbackArgs *fargs = (const FutureCallbackArgs*)args;\n      fargs->impl->perform_callback();\n      if (fargs->impl->remove_base_gc_ref(DEFERRED_TASK_REF))\n        delete fargs->impl;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::unpack_future(Deserializer &derez)\n    //-------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      AutoLock f_lock(future_lock);\n#ifdef DEBUG_LEGION\n      assert(empty);\n      assert(subscription_event.exists() || subscription_internal.exists());\n#endif\n      derez.deserialize(result_size);\n      if (result_size > 0)\n      {\n        result = malloc(result_size);\n        derez.deserialize(result,result_size);\n      }\n      empty = false;\n      ApEvent complete;\n      derez.deserialize(complete);\n      if (subscription_event.exists())\n      {\n        Runtime::trigger_event(NULL, subscription_event, complete);\n        subscription_event = ApUserEvent::NO_AP_USER_EVENT;\n      }\n      if (subscription_internal.exists())\n        Runtime::trigger_event(subscription_internal);\n      if (is_owner())\n      {\n#ifdef DEBUG_LEGION\n        assert(result_set_space != local_space);\n#endif\n        // Send a message to the result set space future to remove its\n        // reference now that we no longer need it\n        Serializer rez;\n        {\n          RezCheck z2(rez);\n          rez.serialize(did);\n          rez.serialize<size_t>(0);\n        }\n        runtime->send_future_broadcast(result_set_space, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool FutureImpl::reset_future(void)\n    //--------------------------------------------------------------------------\n    {\n      // TODO: update this for resilience\n      assert(false);\n      bool was_sampled = sampled;\n      sampled = false;\n      return was_sampled;\n    }\n\n    //--------------------------------------------------------------------------\n    bool FutureImpl::get_boolean_value(bool &valid)\n    //--------------------------------------------------------------------------\n    {\n      if (!empty)\n      {\n        valid = !subscription_internal.exists() || \n                  subscription_internal.has_triggered();\n#ifdef DEBUG_LEGION\n        assert(callback_functor == NULL);\n#endif\n        return *((const bool*)result); \n      }\n      else\n      {\n        valid = false;\n        return false; \n      }\n    }\n\n    //--------------------------------------------------------------------------\n    ApEvent FutureImpl::subscribe(bool need_local_data)\n    //--------------------------------------------------------------------------\n    {\n      if (!empty && (callback_functor == NULL))\n        return future_complete;\n      AutoLock f_lock(future_lock);\n      // See if we lost the race\n      if (empty)\n      {\n        if (!subscription_event.exists())\n        {\n          subscription_event = Runtime::create_ap_user_event(NULL);\n          if (!is_owner())\n          {\n#ifdef DEBUG_LEGION\n            assert(!future_complete.exists());\n#endif\n            future_complete = subscription_event;\n          }\n          if (!subscription_internal.exists())\n          {\n            // Add a reference to prevent us from being collected\n            // until we get the result of the subscription\n            add_base_resource_ref(RUNTIME_REF);\n            if (!is_owner())\n            {\n              // Send a request to the owner node to subscribe\n              Serializer rez;\n              rez.serialize(did);\n              runtime->send_future_subscription(owner_space, rez);\n            }\n            else\n              record_subscription(local_space, false/*need lock*/);\n          }\n        }\n        return subscription_event;\n      }\n      else\n      {\n        if ((callback_functor != NULL) && need_local_data)\n          return invoke_callback();\n        return future_complete;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent FutureImpl::subscribe_internal(bool need_local_data)\n    //--------------------------------------------------------------------------\n    {\n      if (!empty && (callback_functor == NULL))\n        return RtEvent::NO_RT_EVENT;\n      AutoLock f_lock(future_lock);\n      // See if we lost the race\n      if (empty)\n      {\n        if (!subscription_internal.exists())\n        {\n          subscription_internal = Runtime::create_rt_user_event();\n          if (!subscription_event.exists())\n          {\n            // Add a reference to prevent us from being collected\n            // until we get the result of the subscription\n            add_base_resource_ref(RUNTIME_REF);\n            if (!is_owner())\n            {\n              // Send a request to the owner node to subscribe\n              Serializer rez;\n              rez.serialize(did);\n              runtime->send_future_subscription(owner_space, rez);\n            }\n            else if (need_local_data)\n              record_subscription(local_space, false/*need lock*/);\n          }\n        }\n        return subscription_internal;\n      }\n      else\n      {\n        if ((callback_functor != NULL) && need_local_data)\n        {\n          const ApEvent ready = invoke_callback();\n          if (ready.exists() && !ready.has_triggered())\n            return Runtime::protect_event(ready);\n        }\n        return RtEvent::NO_RT_EVENT;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::set_future_coordinates(\n                            std::vector<std::pair<size_t,DomainPoint> > &coords)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(coordinates.empty());\n#endif\n      coordinates.swap(coords);\n    }\n\n    //--------------------------------------------------------------------------\n    const std::vector<std::pair<size_t,DomainPoint> >& \n                                  FutureImpl::get_future_coordinates(void) const\n    //--------------------------------------------------------------------------\n    {\n      return coordinates;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::pack_future(Serializer &rez) const\n    //--------------------------------------------------------------------------\n    {\n      rez.serialize<DistributedID>(did);\n      if (runtime->safe_control_replication)\n      {\n        rez.serialize<size_t>(coordinates.size());\n        for (std::vector<std::pair<size_t,DomainPoint> >::const_iterator it =\n              coordinates.begin(); it != coordinates.end(); it++)\n        {\n          rez.serialize(it->first);\n          rez.serialize(it->second);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ FutureImpl* FutureImpl::unpack_future(Runtime *runtime, \n                                Deserializer &derez, ReferenceMutator *mutator,\n                                Operation *op, GenerationID op_gen,\n#ifdef LEGION_SPY\n                                UniqueID op_uid,\n#endif\n                                int op_depth)\n    //--------------------------------------------------------------------------\n    {\n      DistributedID future_did;\n      derez.deserialize(future_did);\n      if (future_did == 0)\n        return NULL;\n      std::vector<std::pair<size_t,DomainPoint> > coordinates;\n      if (runtime->safe_control_replication)\n      {\n        size_t num_coordinates;\n        derez.deserialize(num_coordinates);\n        coordinates.resize(num_coordinates);\n        for (unsigned idx = 0; idx < num_coordinates; idx++)\n        {\n          std::pair<size_t,DomainPoint> &coord = coordinates[idx]; \n          derez.deserialize(coord.first);\n          derez.deserialize(coord.second);\n        }\n      }\n      return runtime->find_or_create_future(future_did, mutator, coordinates,\n                                            op, op_gen,\n#ifdef LEGION_SPY\n                                            op_uid,\n#endif\n                                            op_depth);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::notify_active(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // If we are not the owner, send a gc reference back to the owner\n      if (!is_owner())\n        send_remote_gc_increment(owner_space, mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::notify_valid(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::notify_invalid(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::notify_inactive(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // If we are not the owner, remove our gc reference\n      if (!is_owner())\n        send_remote_gc_decrement(owner_space, mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::register_dependence(Operation *consumer_op)\n    //--------------------------------------------------------------------------\n    {\n      if (producer_op != NULL)\n      {\n        // Only record dependences on things from the same context\n        // We know futures can never flow up the task tree so the\n        // only way they have the same depth is if they are from \n        // the same parent context\n        TaskContext *context = consumer_op->get_context();\n        const int consumer_depth = context->get_depth();\n#ifdef DEBUG_LEGION\n        assert(consumer_depth >= producer_depth);\n#endif\n        if (consumer_depth == producer_depth)\n        {\n          consumer_op->register_dependence(producer_op, op_gen);\n#ifdef LEGION_SPY\n          LegionSpy::log_mapping_dependence(\n              context->get_unique_id(), producer_uid, 0,\n              consumer_op->get_unique_op_id(), 0, TRUE_DEPENDENCE);\n#endif\n        }\n      }\n#ifdef DEBUG_LEGION\n      else\n        assert(!empty); // better not be empty if it doesn't have an op\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::mark_sampled(void)\n    //--------------------------------------------------------------------------\n    {\n      sampled = true;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::broadcast_result(std::set<AddressSpaceID> &targets,\n                                      ApEvent complete, const bool need_lock)\n    //--------------------------------------------------------------------------\n    {\n      if (targets.empty())\n        return;\n      if (need_lock)\n      {\n        AutoLock f_lock(future_lock,1,false/*exclusive*/);\n        broadcast_result(targets, complete, false/*need lock*/);\n        return;\n      }\n#ifdef DEBUG_LEGION\n      assert(!empty);\n#endif\n      if (callback_functor != NULL)\n      {\n        // Handle the special case where the only subscriber is the local\n        // node so we can lazily defer this until later and the user\n        // actually asks us for the result\n        if (!targets.empty() && ((targets.size() > 1) ||\n              (targets.find(local_space) == targets.end())))\n        {\n          // If we still have a callback to perform do\n          // that now to get it in flight, it will send\n          // out any updates to subscribers\n          invoke_callback();\n          // Make sure these targets are all in the set of subscribers\n          // so that the callback will broadcast them later\n          subscribers.insert(targets.begin(), targets.end());\n        }\n        return;\n      }\n      for (std::set<AddressSpaceID>::const_iterator it = \n            targets.begin(); it != targets.end(); it++)\n      {\n        if ((*it) == local_space)\n          continue;\n        Serializer rez;\n        {\n          rez.serialize(did);\n          RezCheck z(rez);\n          rez.serialize(result_size);\n          if (result_size > 0)\n            rez.serialize(result,result_size);\n          rez.serialize(complete);\n        }\n        runtime->send_future_result(*it, rez);\n      }\n      targets.clear();\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::record_subscription(AddressSpaceID subscriber, \n                                         bool need_lock)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n#endif\n      if (need_lock)\n      {\n        AutoLock f_lock(future_lock);\n        record_subscription(subscriber, false/*need lock*/);\n        return;\n      }\n      if (empty)\n      {\n        // See if we know who has the result\n        if (result_set_space != local_space)\n        {\n          // We don't have the result, but we know who does so \n          // request that they send it out to the target\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(did);\n            rez.serialize<size_t>(1); // size\n            rez.serialize(subscriber);\n            rez.serialize(future_complete);\n          }\n          runtime->send_future_broadcast(result_set_space, rez);\n        }\n        else\n        {\n          // We don't know yet, so save this for later\n#ifdef DEBUG_LEGION\n          assert(subscribers.find(subscriber) == subscribers.end());\n#endif\n          subscribers.insert(subscriber);\n        }\n      }\n      else\n      {\n        if (callback_functor != NULL)\n        {\n          invoke_callback();\n          // If we still have a callback to be done, make sure\n          // it is in flight and that the subscriber is there\n          // for it to be messaged when the callback is done\n          subscribers.insert(subscriber);\n          return;\n        }\n        // We've got the result so we can't send it back right away\n        Serializer rez;\n        {\n          rez.serialize(did);\n          RezCheck z(rez);\n          rez.serialize(result_size);\n          if (result_size > 0)\n            rez.serialize(result,result_size);\n          rez.serialize(future_complete);\n        }\n        runtime->send_future_result(subscriber, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::notify_remote_set(AddressSpaceID remote_space)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(future_lock);\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n      assert(result_set_space == local_space);\n      assert(result_set_space != remote_space);\n#endif\n      result_set_space = remote_space;\n      if (!subscribers.empty())\n      {\n        // Pack these up and send them to the remote space\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(did);\n          rez.serialize<size_t>(subscribers.size());\n          for (std::set<AddressSpaceID>::const_iterator it = \n               subscribers.begin(); it != subscribers.end(); it++)\n            rez.serialize(*it);\n          rez.serialize(future_complete);\n        }\n        runtime->send_future_broadcast(remote_space, rez);\n        subscribers.clear();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::record_future_registered(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // Similar to DistributedCollectable::register_with_runtime but\n      // we don't actually need to do the registration since we know\n      // it has already been done\n#ifdef DEBUG_LEGION\n      assert(!registered_with_runtime);\n#endif\n      registered_with_runtime = true;\n      if (!is_owner())\n      {\n#ifdef DEBUG_LEGION\n        assert(mutator != NULL);\n#endif\n        send_remote_registration(mutator);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureImpl::handle_future_result(Deserializer &derez,\n                                                 Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      DistributedID did;\n      derez.deserialize(did);\n      DistributedCollectable *dc = runtime->find_distributed_collectable(did);\n#ifdef DEBUG_LEGION\n      FutureImpl *future = dynamic_cast<FutureImpl*>(dc);\n      assert(future != NULL);\n#else\n      FutureImpl *future = static_cast<FutureImpl*>(dc);\n#endif\n      future->unpack_future(derez);\n      // Now we can remove the reference that we added from before we\n      // sent the subscription message\n      if (future->remove_base_resource_ref(RUNTIME_REF))\n        delete future;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureImpl::handle_future_subscription(\n                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DistributedID did;\n      derez.deserialize(did);\n      DistributedCollectable *dc = runtime->find_distributed_collectable(did);\n#ifdef DEBUG_LEGION\n      FutureImpl *future = dynamic_cast<FutureImpl*>(dc);\n      assert(future != NULL);\n#else\n      FutureImpl *future = static_cast<FutureImpl*>(dc);\n#endif\n      future->record_subscription(source, true/*need lock*/);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureImpl::handle_future_notification(\n                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DistributedID did;\n      derez.deserialize(did);\n      DistributedCollectable *dc = runtime->find_distributed_collectable(did);\n#ifdef DEBUG_LEGION\n      FutureImpl *future = dynamic_cast<FutureImpl*>(dc);\n      assert(future != NULL);\n#else\n      FutureImpl *future = static_cast<FutureImpl*>(dc);\n#endif\n      future->notify_remote_set(source);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureImpl::handle_future_broadcast(\n                                          Deserializer &derez, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      DistributedID did;\n      derez.deserialize(did);\n      DistributedCollectable *dc = runtime->find_distributed_collectable(did);\n#ifdef DEBUG_LEGION\n      FutureImpl *future = dynamic_cast<FutureImpl*>(dc);\n      assert(future != NULL);\n#else\n      FutureImpl *future = static_cast<FutureImpl*>(dc);\n#endif\n      size_t num_subscribers;\n      derez.deserialize(num_subscribers);\n      // Special case for removing our final reference\n      if (num_subscribers == 0)\n      {\n        if (future->remove_base_resource_ref(RUNTIME_REF))\n          delete future;\n        return;\n      }\n      std::set<AddressSpaceID> subscribers;\n      for (unsigned idx = 0; idx < num_subscribers; idx++)\n      {\n        AddressSpaceID subscriber;\n        derez.deserialize(subscriber);\n        subscribers.insert(subscriber);\n      }\n      ApEvent complete_event;\n      derez.deserialize(complete_event);\n      future->broadcast_result(subscribers, complete_event, true/*need lock*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureImpl::contribute_to_collective(const DynamicCollective &dc, \n                                              unsigned count)\n    //--------------------------------------------------------------------------\n    {\n      const ApEvent ready = subscribe();\n      if (!ready.has_triggered_faultignorant())\n      {\n        // If we're not done then defer the operation until we are triggerd\n        // First add a garbage collection reference so we don't get\n        // collected while we are waiting for the contribution task to run\n        add_base_gc_ref(PENDING_COLLECTIVE_REF);\n        ContributeCollectiveArgs args(this, dc, count);\n        // Spawn the task dependent on the future being ready\n        runtime->issue_runtime_meta_task(args, LG_LATENCY_WORK_PRIORITY,\n                                         Runtime::protect_event(ready));\n      }\n      else // If we've already triggered, then we can do the arrival now\n        Runtime::phase_barrier_arrive(dc, count, ApEvent::NO_AP_EVENT,\n                                      result, result_size);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureImpl::handle_contribute_to_collective(\n                                                               const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const ContributeCollectiveArgs *cargs = (ContributeCollectiveArgs*)args;\n      cargs->impl->contribute_to_collective(cargs->dc, cargs->count);\n      // Now remote the garbage collection reference and see if we can \n      // reclaim the future\n      if (cargs->impl->remove_base_gc_ref(PENDING_COLLECTIVE_REF))\n        delete cargs->impl;\n    }\n      \n    /////////////////////////////////////////////////////////////\n    // Future Map Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    FutureMapImpl::FutureMapImpl(TaskContext *ctx, Operation *o, RtEvent ready,\n                                 const Domain &domain, Runtime *rt,\n                                 DistributedID did, AddressSpaceID owner_space,\n                                 RtUserEvent deleted)\n      : DistributedCollectable(rt, \n          LEGION_DISTRIBUTED_HELP_ENCODE(did, FUTURE_MAP_DC),  owner_space), \n        context(ctx), op(o), op_ctx_index(o->get_ctx_index()),\n        op_gen(o->get_generation()), op_depth(o->get_context()->get_depth()),\n#ifdef LEGION_SPY\n        op_uid(o->get_unique_op_id()),\n#endif\n        future_map_domain(domain), ready_event(ready), delete_event(deleted)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(future_map_domain.exists());\n#endif\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Future Map %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMapImpl::FutureMapImpl(TaskContext *ctx, Runtime *rt, const Domain &d,\n                                 DistributedID did, size_t index,\n                                 AddressSpaceID owner_space, RtEvent ready, \n                                 bool register_now, RtUserEvent deleted)\n      : DistributedCollectable(rt, \n          LEGION_DISTRIBUTED_HELP_ENCODE(did, FUTURE_MAP_DC), \n          owner_space, register_now), \n        context(ctx), op(NULL), op_ctx_index(index), op_gen(0), op_depth(0),\n#ifdef LEGION_SPY\n        op_uid(0),\n#endif\n        future_map_domain(d), ready_event(ready), delete_event(deleted)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(future_map_domain.exists());\n#endif\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Future Map %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMapImpl::FutureMapImpl(const FutureMapImpl &rhs)\n      : DistributedCollectable(rhs), context(NULL), op(NULL), op_ctx_index(0),\n        op_gen(0), op_depth(0)\n#ifdef LEGION_SPY\n        , op_uid(0)\n#endif\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMapImpl::~FutureMapImpl(void)\n    //--------------------------------------------------------------------------\n    {\n      futures.clear();\n      if (delete_event.exists())\n        Runtime::trigger_event(delete_event);\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMapImpl& FutureMapImpl::operator=(const FutureMapImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::notify_active(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // If we are not the owner, send a gc reference back to the owner\n      if (!is_owner())\n        send_remote_gc_increment(owner_space, mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::notify_valid(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::notify_invalid(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::notify_inactive(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // If we are not the owner, remove our gc reference\n      if (!is_owner())\n        send_remote_gc_decrement(owner_space, mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    Future FutureMapImpl::get_future(const DomainPoint &point, \n                                     bool internal, RtEvent *wait_on)\n    //--------------------------------------------------------------------------\n    {\n#ifndef DEBUG_LEGION\n      if (!internal)\n#endif\n      {\n        if (!future_map_domain.contains(point))\n          REPORT_LEGION_ERROR(ERROR_INVALID_FUTURE_MAP_POINT,\n              \"Invalid request for a point not contained in the \"\n              \"domain of a future map in task %s (UID %lld).\",\n              context->get_task_name(), context->get_unique_id())\n      }\n      if (!is_owner())\n      {\n        // See if we already have it\n        {\n          AutoLock fm_lock(future_map_lock,1,false/*exlusive*/);\n          std::map<DomainPoint,Future>::const_iterator finder = \n                                                futures.find(point);\n          if (finder != futures.end())\n            return finder->second;\n        }\n        // Make an event for when we have the answer\n        RtUserEvent future_ready_event = Runtime::create_rt_user_event();\n        // If not send a message to get it\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(did);\n          rez.serialize(point);\n          rez.serialize(future_ready_event);\n          rez.serialize<bool>(internal);\n        }\n        runtime->send_future_map_request_future(owner_space, rez);\n        if (wait_on != NULL)\n        {\n          *wait_on = future_ready_event;\n          return Future();\n        }\n        future_ready_event.wait(); \n        // When we wake up it should be here\n        AutoLock fm_lock(future_map_lock,1,false/*exlusive*/);\n        std::map<DomainPoint,Future>::const_iterator finder = \n                                              futures.find(point);\n#ifdef DEBUG_LEGION\n        assert(finder != futures.end());\n#endif\n        return finder->second;\n      }\n      else\n      {\n        AutoLock fm_lock(future_map_lock);\n        // Check to see if we already have a future for the point\n        std::map<DomainPoint,Future>::const_iterator finder = \n                                              futures.find(point);\n        if (finder != futures.end())\n          return finder->second;\n        // Otherwise we need a future from the context to use for\n        // the point that we will fill in later\n        Future result = \n          runtime->help_create_future(ApEvent::NO_AP_EVENT, op);\n        if (runtime->safe_control_replication)\n        {\n          std::vector<std::pair<size_t,DomainPoint> > new_coords(coordinates);\n          new_coords.push_back(std::make_pair(op_ctx_index, point));\n          result.impl->set_future_coordinates(new_coords);\n        }\n        futures[point] = result;\n        if (runtime->legion_spy_enabled)\n          LegionSpy::log_future_creation(op->get_unique_op_id(),\n                                   ApEvent::NO_AP_EVENT, point);\n        return result;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::set_future(const DomainPoint &point, FutureImpl *impl,\n                                   ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // Add the reference first and then set the future\n      impl->add_base_gc_ref(FUTURE_HANDLE_REF, mutator);\n      AutoLock fm_lock(future_map_lock);\n      futures[point] = Future(impl, false/*need reference*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::get_void_result(const DomainPoint &point,\n                                        bool silence_warnings,\n                                        const char *warning_string)\n    //--------------------------------------------------------------------------\n    {\n      Future f = get_future(point, false/*internal*/);\n      f.get_void_result(silence_warnings, warning_string);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::wait_all_results(bool silence_warnings,\n                                         const char *warning_string)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n#endif\n      if (runtime->runtime_warnings && !silence_warnings && \n          (context != NULL) && !context->is_leaf_context())\n        REPORT_LEGION_WARNING(LEGION_WARNING_WAITING_ALL_FUTURES, \n            \"Waiting for all futures in a future map in \"\n            \"non-leaf task %s (UID %lld) is a violation of Legion's deferred \"\n            \"execution model best practices. You may notice a severe \"\n            \"performance degredation. Warning string: %s\", \n            context->get_task_name(),\n            context->get_unique_id(),\n            (warning_string == NULL) ? \"\" : warning_string)\n      if ((op != NULL) && (Internal::implicit_context != NULL))\n        Internal::implicit_context->record_blocking_call();\n      // Wait on the event that indicates the entire task has finished\n      if (!ready_event.has_triggered())\n      {\n        if (context != NULL)\n        {\n          context->begin_task_wait(false/*from runtime*/);\n          ready_event.wait();\n          context->end_task_wait();\n        }\n        else\n          ready_event.wait();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool FutureMapImpl::reset_all_futures(RtEvent new_ready_event)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n#endif\n      // TODO: send messages to all the remote copies of this\n      assert(false);\n      bool result = false;\n      AutoLock fm_lock(future_map_lock);\n      for (std::map<DomainPoint,Future>::const_iterator it = \n            futures.begin(); it != futures.end(); it++)\n      {\n        bool restart = runtime->help_reset_future(it->second);\n        if (restart)\n          result = true;\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::set_future_map_coordinates(\n                            std::vector<std::pair<size_t,DomainPoint> > &coords)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(coordinates.empty());\n#endif\n      coordinates.swap(coords);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::pack_future_map(Serializer &rez) const\n    //--------------------------------------------------------------------------\n    {\n      rez.serialize(did);\n      rez.serialize(future_map_domain);\n      rez.serialize(get_ready_event());\n      rez.serialize(op_ctx_index);\n      if (runtime->safe_control_replication)\n      {\n        rez.serialize<size_t>(coordinates.size());\n        for (std::vector<std::pair<size_t,DomainPoint> >::const_iterator it =\n              coordinates.begin(); it != coordinates.end(); it++)\n        {\n          rez.serialize(it->first);\n          rez.serialize(it->second);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ FutureMapImpl* FutureMapImpl::unpack_future_map(Runtime *runtime,\n               Deserializer &derez, ReferenceMutator *mutator, TaskContext *ctx)\n    //--------------------------------------------------------------------------\n    {\n      DistributedID future_map_did;\n      derez.deserialize(future_map_did);\n      if (future_map_did == 0)\n        return NULL;\n      Domain future_map_domain;\n      derez.deserialize(future_map_domain);\n      RtEvent ready_event;\n      derez.deserialize(ready_event);\n      size_t index;\n      derez.deserialize(index);\n      std::vector<std::pair<size_t,DomainPoint> > coordinates;\n      if (runtime->safe_control_replication)\n      {\n        size_t num_coordinates;\n        derez.deserialize(num_coordinates);\n        coordinates.resize(num_coordinates);\n        for (unsigned idx = 0; idx < num_coordinates; idx++)\n        {\n          std::pair<size_t,DomainPoint> &coord = coordinates[idx];\n          derez.deserialize(coord.first);\n          derez.deserialize(coord.second);\n        }\n      }\n      return runtime->find_or_create_future_map(future_map_did, ctx, index,\n                      future_map_domain, ready_event, mutator, coordinates);\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::get_all_futures(std::map<DomainPoint,Future> &others)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n#endif\n      if (op != NULL && Internal::implicit_context != NULL)\n        Internal::implicit_context->record_blocking_call();\n      if (!ready_event.has_triggered())\n      {\n        if (context != NULL)\n        {\n          context->begin_task_wait(false/*from runtime*/);\n          ready_event.wait();\n          context->end_task_wait();\n        }\n        else\n          ready_event.wait();\n      }\n      // No need for the lock since the map should be fixed at this point\n      others = futures;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::set_all_futures(\n                                     const std::map<DomainPoint,Future> &others)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n#endif\n      // No need for the lock here since we're initializing\n      futures = others;\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl* FutureMapImpl::find_shard_local_future(const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      // Wait for all the futures to be ready\n      if (!ready_event.has_triggered())\n        ready_event.wait();\n      // No need for the lock since the map should be fixed now\n      std::map<DomainPoint,Future>::const_iterator finder = futures.find(point);\n      if (finder != futures.end())\n        return finder->second.impl;\n      else\n        return NULL;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::get_shard_local_futures(\n                                      std::map<DomainPoint,FutureImpl*> &others)\n    //--------------------------------------------------------------------------\n    {\n      // Wait for all the futures to be ready\n      if (!ready_event.has_triggered())\n        ready_event.wait();\n      for (std::map<DomainPoint,Future>::const_iterator it = \n            futures.begin(); it != futures.end(); it++)\n        others[it->first] = it->second.impl;\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::register_dependence(Operation *consumer_op)\n    //--------------------------------------------------------------------------\n    {\n      if (op == NULL)\n        return;\n      // Only record dependences on things from the same context\n      // We know futures can never flow up the task tree so the\n      // only way they have the same depth is if they are from \n      // the same parent context\n      TaskContext *context = consumer_op->get_context();\n      const int consumer_depth = context->get_depth();\n#ifdef DEBUG_LEGION\n      assert(consumer_depth >= op_depth);\n#endif\n      if (consumer_depth == op_depth)\n      {\n        consumer_op->register_dependence(op, op_gen);\n#ifdef LEGION_SPY\n        LegionSpy::log_mapping_dependence(\n            context->get_unique_id(), op_uid, 0,\n            consumer_op->get_unique_op_id(), 0, TRUE_DEPENDENCE);\n#endif\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void FutureMapImpl::record_future_map_registered(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // Similar to DistributedCollectable::register_with_runtime but\n      // we don't actually need to do the registration since we know\n      // it has already been done\n#ifdef DEBUG_LEGION\n      assert(!registered_with_runtime);\n#endif\n      registered_with_runtime = true;\n      if (!is_owner())\n        // Send the remote registration notice\n        send_remote_registration(mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureMapImpl::handle_future_map_future_request(\n                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      DistributedID did;\n      derez.deserialize(did);\n      DomainPoint point;\n      derez.deserialize(point);\n      RtUserEvent done;\n      derez.deserialize(done);\n      bool internal;\n      derez.deserialize(internal);\n      \n      // Should always find it since this is the owner node\n      DistributedCollectable *dc = runtime->find_distributed_collectable(did);\n#ifdef DEBUG_LEGION\n      FutureMapImpl *impl = dynamic_cast<FutureMapImpl*>(dc);\n      assert(impl != NULL);\n#else\n      FutureMapImpl *impl = static_cast<FutureMapImpl*>(dc);\n#endif\n      Future f = impl->get_future(point, internal);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(did);\n        rez.serialize(point);\n        f.impl->pack_future(rez);\n        rez.serialize(done);\n      }\n      runtime->send_future_map_response_future(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void FutureMapImpl::handle_future_map_future_response(\n                                          Deserializer &derez, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      DistributedID did;\n      derez.deserialize(did);\n      DomainPoint point;\n      derez.deserialize(point);\n      \n      // Should always find it since this is the source node\n      DistributedCollectable *dc = runtime->find_distributed_collectable(did);\n#ifdef DEBUG_LEGION\n      FutureMapImpl *impl = dynamic_cast<FutureMapImpl*>(dc);\n      assert(impl != NULL);\n#else\n      FutureMapImpl *impl = static_cast<FutureMapImpl*>(dc);\n#endif\n      std::set<RtEvent> done_events;\n      WrapperReferenceMutator mutator(done_events);\n      FutureImpl *future = FutureImpl::unpack_future(runtime, derez, &mutator);\n      // Add it to the map\n      impl->set_future(point, future, &mutator);\n      // Trigger the done event\n      RtUserEvent done;\n      derez.deserialize(done);\n      if (!done_events.empty())\n        Runtime::trigger_event(done, Runtime::merge_events(done_events));\n      else\n        Runtime::trigger_event(done);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Repl Future Map Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ReplFutureMapImpl::ReplFutureMapImpl(ReplicateContext *ctx, Operation *op,\n                                         RtEvent ready, const Domain &domain,\n                                         const Domain &shard_dom, Runtime *rt, \n                                         DistributedID did,AddressSpaceID owner,\n                                         RtUserEvent deletion_trigger)\n      : FutureMapImpl(ctx, op, ready, domain, rt, did, owner, deletion_trigger),\n        repl_ctx(ctx), shard_domain(shard_dom),\n        future_map_barrier_index(ctx->peek_next_future_map_barrier_index()),\n        future_map_barrier(ctx->get_next_future_map_barrier()),\n        collective_index(ctx->get_next_collective_index(COLLECTIVE_LOC_32)),\n        op_depth(repl_ctx->get_depth()), op_uid(op->get_unique_op_id()),\n        sharding_function_ready(Runtime::create_rt_user_event()), \n        sharding_function(NULL), collective_performed(false), \n        has_non_trivial_call(false)\n    //--------------------------------------------------------------------------\n    {\n      repl_ctx->add_reference();\n      // Now register ourselves with the context\n      repl_ctx->register_future_map(this);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplFutureMapImpl::ReplFutureMapImpl(const ReplFutureMapImpl &rhs)\n      : FutureMapImpl(rhs), repl_ctx(NULL), shard_domain(Domain::NO_DOMAIN), \n        future_map_barrier_index(0), collective_index(0), op_depth(0), op_uid(0)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplFutureMapImpl::~ReplFutureMapImpl(void)\n    //--------------------------------------------------------------------------\n    {\n      if (repl_ctx->remove_reference())\n        delete repl_ctx;\n    }\n\n    //--------------------------------------------------------------------------\n    ReplFutureMapImpl& ReplFutureMapImpl::operator=(\n                                                   const ReplFutureMapImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::notify_inactive(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner());\n#endif\n      // Do the base version, then arrive on our barrier\n      FutureMapImpl::notify_inactive(mutator);\n      // Decide what to do here about our future map barrier depending\n      // on whether we saw any non-trivial calls on this shard. If we \n      // did not see any non-trivial calls then neither should any of\n      // the other shards and we don't have to use the barrier to guide\n      // reclamation of this future map\n      if (has_non_trivial_call)\n      {\n        if (!exchange_events.empty())\n          Runtime::phase_barrier_arrive(future_map_barrier, 1/*count*/,\n              Runtime::merge_events(exchange_events));\n        else\n          Runtime::phase_barrier_arrive(future_map_barrier, 1/*count*/);\n        if (!future_map_barrier.has_triggered())\n        {\n          // Add a reference to this to prevent it being collected\n          add_base_resource_ref(DEFERRED_TASK_REF);\n          // Launch a task to do the reclaim once everyone is done\n          ReclaimFutureMapArgs args(repl_ctx, this, op_uid);\n          runtime->issue_runtime_meta_task(args, \n              LG_LATENCY_WORK_PRIORITY, future_map_barrier);\n        }\n        else\n          repl_ctx->unregister_future_map(this);\n      }\n      else\n      {\n        // No non-trivial call so we can unregister ourselves now\n        repl_ctx->unregister_future_map(this);\n        // If we're the owner shard of the barrier then do the arrival\n        // for all the shards so that the barrier generation triggers\n        // without needing to do any communication\n        const size_t total_shards = repl_ctx->total_shards;\n        if ((future_map_barrier_index % total_shards) ==  \n            repl_ctx->owner_shard->shard_id)\n          Runtime::phase_barrier_arrive(future_map_barrier, total_shards);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    Future ReplFutureMapImpl::get_future(const DomainPoint &point,\n                                         bool internal, RtEvent *wait_on)\n    //--------------------------------------------------------------------------\n    {\n      if (!internal)\n        has_non_trivial_call = true;\n      // Do a quick check to see if we've already got it\n      {\n        AutoLock f_lock(future_map_lock,1,false/*exclusive*/);\n        std::map<DomainPoint,Future>::const_iterator finder = \n          futures.find(point);\n        if (finder != futures.end())\n          return finder->second;\n      }\n      // Now we need to figure out which shard we're on, see if we know\n      // the sharding function yet, if not we have to wait\n      if (!sharding_function_ready.has_triggered())\n        sharding_function_ready.wait();\n      const ShardID owner_shard = \n        sharding_function->find_owner(point, shard_domain);\n      // If we're the owner shard we can just do the normal thing\n      if (owner_shard != repl_ctx->owner_shard->shard_id)\n      {\n        // We have to figure out the name of the future from the owner shard\n        RtUserEvent done_event = Runtime::create_rt_user_event();\n        Serializer rez;\n        rez.serialize(repl_ctx->shard_manager->repl_id);\n        rez.serialize(owner_shard);\n        rez.serialize<RtEvent>(future_map_barrier);\n        rez.serialize(point);\n        rez.serialize(did);\n        rez.serialize(done_event);\n        rez.serialize<bool>(internal);\n        repl_ctx->shard_manager->send_future_map_request(owner_shard, rez);\n        if (wait_on != NULL)\n        {\n          *wait_on = done_event;\n          return Future();\n        }\n        // Wait for the event\n        done_event.wait();\n        // Now we can wake up see if we found it\n        AutoLock f_lock(future_map_lock,1,false/*exclusive*/);\n        std::map<DomainPoint,Future>::const_iterator finder = \n          futures.find(point);\n#ifdef DEBUG_LEGION\n        assert(finder != futures.end());\n#endif\n        return finder->second;\n      }\n      else // If we're the owner shard we can just do the normal thing\n        return FutureMapImpl::get_future(point, internal, wait_on);\n    }\n\n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::get_all_futures(\n                                           std::map<DomainPoint,Future> &others)\n    //--------------------------------------------------------------------------\n    {\n      has_non_trivial_call = true;\n      // We know this call only comes from the application so we don't\n      // need to worry about thread safety\n      if (collective_performed)\n      {\n        // No need for the lock, we know we have all the futures\n        others = futures;\n        return;\n      }\n      // Wait for all the local futures to be completed\n      if (!ready_event.has_triggered())\n        ready_event.wait();\n      // Now we've got all our local futures so we can do the exchange\n      // Have to hold the lock when doing this as there might be\n      // other requests for the future map\n      WrapperReferenceMutator mutator(exchange_events);\n      AutoLock f_lock(future_map_lock);\n      if (!collective_performed)\n      {\n        FutureNameExchange collective(repl_ctx, collective_index,this,&mutator);\n        collective.exchange_future_names(futures);\n        // When the collective is done we can mark that we've done it\n        // and then copy the results\n        collective_performed = true;\n      }\n      others = futures;\n    }\n\n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::wait_all_results(bool silence_warnings,\n                                             const char *warning_string)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime->runtime_warnings && !silence_warnings && \n          (context != NULL) && !context->is_leaf_context())\n        REPORT_LEGION_WARNING(LEGION_WARNING_WAITING_ALL_FUTURES, \n            \"Waiting for all futures in a future map in \"\n            \"non-leaf task %s (UID %lld) is a violation of Legion's deferred \"\n            \"execution model best practices. You may notice a severe \"\n            \"performance degredation. Warning string: %s\", \n            context->get_task_name(), context->get_unique_id(),\n            (warning_string == NULL) ? \"\" : warning_string)\n      // As a proxy for this, we will get the names of all the futures\n      // needed for this future map in case we need them in the future\n      // The process of doing this will wait on both our ready event\n      // as well as on the ready events of all other shards\n      std::map<DomainPoint,Future> dummy_others;\n      get_all_futures(dummy_others);\n    }\n\n    //--------------------------------------------------------------------------\n    FutureImpl* ReplFutureMapImpl::find_shard_local_future(\n                                                       const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      if (!sharding_function_ready.has_triggered())\n        sharding_function_ready.wait();\n      // Check to see if we own this point or not\n      const ShardID shard = sharding_function->find_owner(point, shard_domain);\n      if (shard != repl_ctx->owner_shard->shard_id)\n        return NULL;\n      return FutureMapImpl::find_shard_local_future(point);\n    }\n    \n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::get_shard_local_futures(\n                                      std::map<DomainPoint,FutureImpl*> &others)\n    //--------------------------------------------------------------------------\n    {\n      FutureMapImpl::get_shard_local_futures(others);\n      const ShardID local_shard = repl_ctx->owner_shard->shard_id;\n      if (!sharding_function_ready.has_triggered())\n        sharding_function_ready.wait();\n      for (std::map<DomainPoint,FutureImpl*>::iterator it = \n            others.begin(); it != others.end(); /*nothing*/)\n      {\n        const ShardID shard = \n          sharding_function->find_owner(it->first, shard_domain);\n        if (shard != local_shard)\n        {\n          std::map<DomainPoint,FutureImpl*>::iterator to_delete = it++;\n          others.erase(to_delete);\n        }\n        else\n          it++;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::set_sharding_function(ShardingFunction *function)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(sharding_function == NULL);\n#endif\n      std::vector<PendingRequest> to_perform;\n      {\n        AutoLock fm_lock(future_map_lock);\n        sharding_function = function;\n        if (!pending_future_map_requests.empty())\n          to_perform.swap(pending_future_map_requests);\n      }\n      Runtime::trigger_event(sharding_function_ready);\n      if (!to_perform.empty())\n      {\n        for (std::vector<PendingRequest>::const_iterator it = \n              to_perform.begin(); it != to_perform.end(); it++)\n          process_future_map_request(it->point, it->src_did, \n                                     it->internal, it->done_event);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::handle_future_map_request(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DomainPoint point;\n      derez.deserialize(point);\n      DistributedID src_did;\n      derez.deserialize(src_did);\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n      bool internal;\n      derez.deserialize(internal);\n      // We can't actually process this until we get our sharding function\n      if (sharding_function == NULL)\n      {\n        // Take the lock and see if we lost the race\n        AutoLock fm_lock(future_map_lock);\n        if (sharding_function == NULL)\n        {\n          pending_future_map_requests.push_back(\n              PendingRequest(point, src_did, done_event, internal));\n          return;\n        }\n        // If we have a sharding function now we can fall through and continue\n      }\n      process_future_map_request(point, src_did, internal, done_event);\n    }\n\n    //--------------------------------------------------------------------------\n    void ReplFutureMapImpl::process_future_map_request(const DomainPoint &point,\n                                                       DistributedID src_did,\n                                                       const bool internal,\n                                                       RtUserEvent done_event)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(sharding_function != NULL);\n#endif\n      const AddressSpaceID source = runtime->determine_owner(src_did);\n      Future result = get_future(point, internal);\n      if (source != runtime->address_space)\n      {\n        // Remote future map so send the answer back\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(src_did);\n          rez.serialize(point);\n          if (result.impl != NULL)\n            result.impl->pack_future(rez);\n          else\n            rez.serialize<DistributedID>(0);\n          rez.serialize(done_event);\n        }\n        runtime->send_control_replicate_future_map_response(source, rez);\n      }\n      else\n      {\n        // Local future map so we should be able to find it and set it\n        DistributedCollectable *dc = \n          runtime->find_distributed_collectable(src_did);\n#ifdef DEBUG_LEGION\n        ReplFutureMapImpl *target = dynamic_cast<ReplFutureMapImpl*>(dc);\n        assert(target != NULL);\n#else\n        ReplFutureMapImpl *target = static_cast<ReplFutureMapImpl*>(dc);\n#endif\n        std::set<RtEvent> preconditions;\n        WrapperReferenceMutator mutator(preconditions);\n        target->set_future(point, result.impl, &mutator);\n        if (!preconditions.empty())\n          Runtime::trigger_event(done_event,\n              Runtime::merge_events(preconditions));\n        else\n          Runtime::trigger_event(done_event);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ReplFutureMapImpl::handle_future_map_response(\n                                          Deserializer &derez, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      DistributedID map_did;\n      derez.deserialize(map_did);\n      DomainPoint point;\n      derez.deserialize(point);\n\n      // It should already exist so we're just finding it\n      DistributedCollectable *dc = \n          runtime->find_distributed_collectable(map_did);\n#ifdef DEBUG_LEGION\n      ReplFutureMapImpl *target = dynamic_cast<ReplFutureMapImpl*>(dc);\n      assert(target != NULL);\n#else\n      ReplFutureMapImpl *target = static_cast<ReplFutureMapImpl*>(dc);\n#endif\n      std::set<RtEvent> done_events;\n      WrapperReferenceMutator mutator(done_events);\n      FutureImpl *impl = FutureImpl::unpack_future(runtime, derez, &mutator,\n                                                   target->op, target->op_gen,\n#ifdef LEGION_SPY\n                                                   target->op_uid,\n#endif\n                                                   target->op_depth);\n      target->set_future(point, impl, &mutator);\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n      if (!done_events.empty())\n        Runtime::trigger_event(done_event, Runtime::merge_events(done_events));\n      else\n        Runtime::trigger_event(done_event);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/void ReplFutureMapImpl::handle_future_map_reclaim(const void *arg)\n    //--------------------------------------------------------------------------\n    {\n      const ReclaimFutureMapArgs *recl_args = (const ReclaimFutureMapArgs*)arg;\n      recl_args->ctx->unregister_future_map(recl_args->impl);\n      if (recl_args->impl->remove_base_resource_ref(DEFERRED_TASK_REF))\n        delete recl_args->impl;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Physical Region Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    PhysicalRegionImpl::PhysicalRegionImpl(const RegionRequirement &r, \n      RtEvent mapped, ApEvent ready, ApUserEvent term, bool m, TaskContext *ctx, \n      MapperID mid, MappingTagID t, bool leaf, bool virt, Runtime *rt)\n      : Collectable(), runtime(rt), context(ctx), map_id(mid), tag(t),\n        leaf_region(leaf), virtual_mapped(virt), \n        replaying((ctx != NULL) ? ctx->owner_task->is_replaying() : false),\n        req(r),mapped_event(mapped),ready_event(ready),termination_event(term),\n        sharded_view(NULL), mapped(m), valid(false), made_accessor(false)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalRegionImpl::PhysicalRegionImpl(const PhysicalRegionImpl &rhs)\n      : Collectable(), runtime(NULL), context(NULL), map_id(0), tag(0),\n        leaf_region(false), virtual_mapped(false), replaying(false),\n        req(rhs.req), mapped_event(RtEvent::NO_RT_EVENT)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalRegionImpl::~PhysicalRegionImpl(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!termination_event.exists());\n#endif\n      if (!references.empty() && !replaying)\n        references.remove_resource_references(PHYSICAL_REGION_REF);\n      if ((sharded_view != NULL) && \n          sharded_view->remove_base_resource_ref(PHYSICAL_REGION_REF))\n        delete sharded_view;\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalRegionImpl& PhysicalRegionImpl::operator=(\n                                                  const PhysicalRegionImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::set_sharded_view(ShardedView *view)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(sharded_view == NULL);\n      assert(view != NULL);\n#endif\n      sharded_view = view;\n      sharded_view->add_base_resource_ref(PHYSICAL_REGION_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::wait_until_valid(bool silence_warnings, \n                                              const char *warning_string,\n                                              bool warn, const char *source)\n    //--------------------------------------------------------------------------\n    {\n      if (context != NULL)\n        context->record_blocking_call();\n      if (runtime->runtime_warnings && !silence_warnings &&\n          (context != NULL) && !context->is_leaf_context())\n      {\n        if (source != NULL)\n          REPORT_LEGION_WARNING(LEGION_WARNING_WAITING_REGION, \n              \"Waiting for a physical region to be valid \"\n              \"for call %s in non-leaf task %s (UID %lld) is a violation of \"\n              \"Legion's deferred execution model best practices. You may \"\n              \"notice a severe performance degradation. Warning string: %s\", \n              source, context->get_task_name(), context->get_unique_id(),\n              (warning_string == NULL) ? \"\" : warning_string)\n        else\n          REPORT_LEGION_WARNING(LEGION_WARNING_WAITING_REGION, \n              \"Waiting for a physical region to be valid \"\n              \"in non-leaf task %s (UID %lld) is a violation of Legion's \"\n              \"deferred execution model best practices. You may notice a \"\n              \"severe performance degradation. Warning string: %s\", \n              context->get_task_name(), context->get_unique_id(),\n              (warning_string == NULL) ? \"\" : warning_string)\n      }\n      if (mapped_event.exists() && !mapped_event.has_triggered())\n      {\n        if (warn && !silence_warnings && (source != NULL))\n          REPORT_LEGION_WARNING(LEGION_WARNING_MISSING_REGION_WAIT, \n              \"Request for %s was performed on a \"\n              \"physical region in task %s (ID %lld) without first waiting \"\n              \"for the physical region to be valid. Legion is performing \"\n              \"the wait for you. Warning string: %s\", source, \n              context->get_task_name(), context->get_unique_id(),\n              (warning_string == NULL) ? \"\" : warning_string)\n        if (context != NULL)\n          context->begin_task_wait(false/*from runtime*/);\n        mapped_event.wait();\n        if (context != NULL)\n          context->end_task_wait();\n      }\n      // If we've already gone through this process we're good\n      if (valid)\n        return;\n      // Now wait for the reference to be ready\n      bool poisoned = false;\n      if (!ready_event.has_triggered_faultaware(poisoned))\n      {\n        if (!poisoned)\n        {\n          if (context != NULL)\n            context->begin_task_wait(false/*from runtime*/);\n          ready_event.wait_faultaware(poisoned);\n          if (context != NULL)\n            context->end_task_wait();\n        }\n      }\n      valid = true;\n    }\n\n    //--------------------------------------------------------------------------\n    bool PhysicalRegionImpl::is_valid(void) const\n    //--------------------------------------------------------------------------\n    {\n      if (valid)\n        return true;\n      if (!mapped_event.exists() || mapped_event.has_triggered())\n      {\n        bool poisoned = false;\n        if (ready_event.has_triggered_faultaware(poisoned))\n          return true;\n        if (poisoned)\n          implicit_context->raise_poison_exception();\n      }\n      return false;\n    }\n\n    //--------------------------------------------------------------------------\n    bool PhysicalRegionImpl::is_mapped(void) const\n    //--------------------------------------------------------------------------\n    {\n      return mapped;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion PhysicalRegionImpl::get_logical_region(void) const\n    //--------------------------------------------------------------------------\n    {\n      return req.region;\n    }\n\n    //--------------------------------------------------------------------------\n    PrivilegeMode PhysicalRegionImpl::get_privilege(void) const\n    //--------------------------------------------------------------------------\n    {\n      return req.privilege;\n    }\n\n    //--------------------------------------------------------------------------\n    LegionRuntime::Accessor::RegionAccessor<\n      LegionRuntime::Accessor::AccessorType::Generic>\n        PhysicalRegionImpl::get_accessor(bool silence_warnings)\n    //--------------------------------------------------------------------------\n    {\n      if (context != NULL)\n      {\n        if (context->is_inner_context())\n          REPORT_LEGION_ERROR(ERROR_INNER_TASK_VIOLATION, \n            \"Illegal call to 'get_accessor' inside task \"\n            \"%s (UID %lld) for a variant that was labeled as an 'inner' \"\n            \"variant.\", context->get_task_name(), context->get_unique_id())\n        else if (runtime->runtime_warnings && !silence_warnings &&\n                  !context->is_leaf_context())\n          REPORT_LEGION_WARNING(LEGION_WARNING_NONLEAF_ACCESSOR, \n              \"Call to 'get_accessor' in non-leaf task %s \"\n              \"(UID %lld) is a blocking operation in violation of Legion's \"\n              \"deferred execution model best practices. You may notice a \"\n              \"severe performance degradation.\", context->get_task_name(),\n              context->get_unique_id())\n      }\n      // If this physical region isn't mapped, then we have to\n      // map it before we can return an accessor\n      if (!mapped)\n      {\n        if (virtual_mapped)\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_IMPLICIT_MAPPING, \n                        \"Illegal implicit mapping of a virtual mapped region \"\n                        \"in task %s (UID %lld)\", context->get_task_name(),\n                        context->get_unique_id())\n        if (runtime->runtime_warnings && !silence_warnings)\n          REPORT_LEGION_WARNING(LEGION_WARNING_UNMAPPED_ACCESSOR, \n                          \"Request for 'get_accessor' was \"\n                          \"performed on an unmapped region in task %s \"\n                          \"(UID %lld). Legion is mapping it for you. \"\n                          \"Please try to be more careful.\",\n                          context->get_task_name(), context->get_unique_id())\n        runtime->remap_region(context, PhysicalRegion(this));\n        // At this point we should have a new ready event\n        // and be mapped\n#ifdef DEBUG_LEGION\n        assert(mapped);\n#endif\n      }\n      // Wait until we are valid before returning the accessor\n      wait_until_valid(silence_warnings, NULL, \n                       runtime->runtime_warnings, \"get_accessor\");\n      // You can only legally invoke this method when you have one instance\n      if (references.size() > 1)\n        REPORT_LEGION_ERROR(ERROR_DEPRECATED_METHOD_USE, \n                      \"Illegal invocation of deprecated 'get_accessor' method \"\n                      \"in task %s (ID %lld) on a PhysicalRegion containing \"\n                      \"multiple internal instances. Use of this deprecated \"\n                      \"method is only supported if the PhysicalRegion contains \"\n                      \"a single physical instance.\", context->get_task_name(),\n                      context->get_unique_id())\n      made_accessor = true;\n#if defined(LEGION_PRIVILEGE_CHECKS) || defined(LEGION_BOUNDS_CHECKS)\n      LegionRuntime::Accessor::RegionAccessor<\n        LegionRuntime::Accessor::AccessorType::Generic>\n          result = references[0].get_accessor();\n      result.set_region_untyped(this);\n#ifdef LEGION_PRIVILEGE_CHECKS\n      result.set_privileges_untyped(\n          (LegionRuntime::AccessorPrivilege)req.get_accessor_privilege()); \n#endif\n      return result;\n#else // privilege or bounds checks\n      return references[0].get_accessor();\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    LegionRuntime::Accessor::RegionAccessor<\n        LegionRuntime::Accessor::AccessorType::Generic>\n          PhysicalRegionImpl::get_field_accessor(FieldID fid, \n                                                 bool silence_warnings)\n    //--------------------------------------------------------------------------\n    {\n      if (context != NULL)\n      {\n        if (context->is_inner_context())\n          REPORT_LEGION_ERROR(ERROR_INNER_TASK_VIOLATION, \n            \"Illegal call to 'get_field_accessor' inside \"\n            \"task %s (UID %lld) for a variant that was labeled as an 'inner' \"\n            \"variant.\", context->get_task_name(), context->get_unique_id())\n        else if (runtime->runtime_warnings && !silence_warnings &&\n                  !context->is_leaf_context())\n          REPORT_LEGION_WARNING(LEGION_WARNING_NONLEAF_ACCESSOR, \n              \"Call to 'get_field_accessor' in non-leaf \"\n              \"task %s (UID %lld) is a blocking operation in violation of \"\n              \"Legion's deferred execution model best practices. You may \"\n              \"notice a severe performance degradation.\", \n              context->get_task_name(), context->get_unique_id())\n      }\n      // If this physical region isn't mapped, then we have to\n      // map it before we can return an accessor\n      if (!mapped)\n      {\n        if (virtual_mapped)\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_IMPLICIT_MAPPING, \n                        \"Illegal implicit mapping of a virtual mapped region \"\n                        \"in task %s (UID %lld)\", context->get_task_name(),\n                        context->get_unique_id())\n        if (runtime->runtime_warnings && !silence_warnings)\n          REPORT_LEGION_WARNING(LEGION_WARNING_UNMAPPED_ACCESSOR, \n                          \"Request for 'get_field_accessor' was \"\n                          \"performed on an unmapped region in task %s \"\n                          \"(UID %lld). Legion is mapping it for you. \"\n                          \"Please try to be more careful.\",\n                          context->get_task_name(), context->get_unique_id())\n        runtime->remap_region(context, PhysicalRegion(this));\n        // At this point we should have a new ready event\n        // and be mapped\n#ifdef DEBUG_LEGION\n        assert(mapped);\n#endif \n      }\n      // Wait until we are valid before returning the accessor\n      wait_until_valid(silence_warnings, NULL, \n                       runtime->runtime_warnings, \"get_field_acessor\");\n#ifdef DEBUG_LEGION\n      if (req.privilege_fields.find(fid) == req.privilege_fields.end())\n        REPORT_LEGION_ERROR(ERROR_INVALID_FIELD_PRIVILEGES, \n            \"Requested field accessor for field %d without privileges!\", fid)\n#endif\n      made_accessor = true;\n#if defined(LEGION_PRIVILEGE_CHECKS) || defined(LEGION_BOUNDS_CHECKS)\n      LegionRuntime::Accessor::RegionAccessor<\n        LegionRuntime::Accessor::AccessorType::Generic>\n          result = references.get_field_accessor(fid);\n      result.set_region_untyped(this);\n#ifdef LEGION_PRIVILEGE_CHECKS\n      result.set_privileges_untyped(\n          (LegionRuntime::AccessorPrivilege)req.get_accessor_privilege());\n#endif\n      return result;\n#else // privilege or bounds checks\n      return references.get_field_accessor(fid);\n#endif\n    } \n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::unmap_region(void)\n    //--------------------------------------------------------------------------\n    {\n      if (!mapped)\n        return;\n#ifdef DEBUG_LEGION\n      assert(termination_event.exists());\n#endif\n      // trigger the termination event conditional upon the ready event\n      Runtime::trigger_event(NULL, termination_event, ready_event);\n#ifdef DEBUG_LEGION\n      termination_event = ApUserEvent::NO_AP_USER_EVENT;\n#endif\n      mapped = false;\n      valid = false;\n    }\n\n    //--------------------------------------------------------------------------\n    ApEvent PhysicalRegionImpl::remap_region(ApEvent new_ready)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!mapped);\n      assert(!termination_event.exists());\n#endif\n      termination_event = Runtime::create_ap_user_event(NULL);\n      ready_event = new_ready;\n      mapped = true;\n      return termination_event;\n    }\n\n    //--------------------------------------------------------------------------\n    const RegionRequirement& PhysicalRegionImpl::get_requirement(void) const\n    //--------------------------------------------------------------------------\n    {\n      return req;\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::set_reference(const InstanceRef &ref, bool safe)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(ref.has_ref());\n      assert(references.empty());\n      assert(safe || (mapped_event.exists() && !mapped_event.has_triggered()));\n#endif\n      references.add_instance(ref);\n      ref.add_resource_reference(PHYSICAL_REGION_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::set_references(const InstanceSet &refs, bool safe)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(references.empty());\n      assert(safe || (mapped_event.exists() && !mapped_event.has_triggered()));\n#endif\n      references = refs;\n      if (!references.empty())\n        references.add_resource_references(PHYSICAL_REGION_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    bool PhysicalRegionImpl::has_references(void) const\n    //--------------------------------------------------------------------------\n    {\n      return !references.empty();\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::get_references(InstanceSet &instances) const\n    //--------------------------------------------------------------------------\n    {\n      if (mapped_event.exists() && !mapped_event.has_triggered())\n        mapped_event.wait();\n      instances = references;\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::get_memories(std::set<Memory>& memories,\n                        bool silence_warnings, const char *warning_string) const\n    //--------------------------------------------------------------------------\n    {\n      if (mapped_event.exists() && !mapped_event.has_triggered())\n      {\n        if (runtime->runtime_warnings && !silence_warnings)\n          REPORT_LEGION_WARNING(LEGION_WARNING_MISSING_REGION_WAIT, \n              \"Request for 'get_memories' was performed on a \"\n              \"physical region in task %s (ID %lld) without first waiting \"\n              \"for the physical region to be valid. Legion is performing \"\n              \"the wait for you. Warning string: %s\", context->get_task_name(), \n              context->get_unique_id(), (warning_string == NULL) ? \n              \"\" : warning_string)\n        if (context != NULL)\n          context->begin_task_wait(false/*from runtime*/);\n        mapped_event.wait();\n        if (context != NULL)\n          context->end_task_wait();\n      }\n      for (unsigned idx = 0; idx < references.size(); idx++)\n        memories.insert(references[idx].get_memory());\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::get_fields(std::vector<FieldID>& fields) const\n    //--------------------------------------------------------------------------\n    {\n      // Just get these from the region requirement\n      fields.insert(fields.end(), req.privilege_fields.begin(),\n                    req.privilege_fields.end());\n    }\n\n\n#if defined(LEGION_PRIVILEGE_CHECKS) || defined(LEGION_BOUNDS_CHECKS)\n    //--------------------------------------------------------------------------\n    const char* PhysicalRegionImpl::get_task_name(void) const\n    //--------------------------------------------------------------------------\n    {\n      return context->get_task_name();\n    }\n#endif\n\n#ifdef LEGION_BOUNDS_CHECKS \n    //--------------------------------------------------------------------------\n    bool PhysicalRegionImpl::contains_ptr(ptr_t ptr)\n    //--------------------------------------------------------------------------\n    {\n      if (!bounds.exists())\n        bounds = runtime->forest->get_node(req.region.get_index_space())->\n                    get_color_space_domain();\n      DomainPoint dp(ptr.value);\n      return bounds.contains(dp);\n    }\n    \n    //--------------------------------------------------------------------------\n    bool PhysicalRegionImpl::contains_point(const DomainPoint &dp)\n    //--------------------------------------------------------------------------\n    {\n      if (!bounds.exists())\n        bounds = runtime->forest->get_node(req.region.get_index_space())->\n                    get_color_space_domain();\n      return bounds.contains(dp);\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::get_bounds(void *realm_is, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      runtime->get_index_space_domain(req.region.get_index_space(),\n                                      realm_is, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    PieceIteratorImpl* PhysicalRegionImpl::get_piece_iterator(FieldID fid,\n         bool privilege_only, bool silence_warnings, const char *warning_string)\n    //--------------------------------------------------------------------------\n    {\n      if (req.privilege_fields.find(fid) == req.privilege_fields.end())\n        REPORT_LEGION_ERROR(ERROR_INVALID_FIELD_PRIVILEGES, \n                       \"Piece iterator construction in task %s on \"\n                       \"PhysicalRegion that does not contain field %d!\", \n                       context->get_task_name(), fid)\n      if (mapped_event.exists() && !mapped_event.has_triggered())\n      {\n        if (runtime->runtime_warnings && !silence_warnings)\n          REPORT_LEGION_WARNING(LEGION_WARNING_MISSING_REGION_WAIT, \n              \"Request for 'get_piece_iterator' was performed on a \"\n              \"physical region in task %s (ID %lld) without first waiting \"\n              \"for the physical region to be valid. Legion is performing \"\n              \"the wait for you. Warning string: %s\", context->get_task_name(), \n              context->get_unique_id(), (warning_string == NULL) ? \n              \"\" : warning_string)\n        if (context != NULL)\n          context->begin_task_wait(false/*from runtime*/);\n        mapped_event.wait();\n        if (context != NULL)\n          context->end_task_wait();\n      }\n      for (unsigned idx = 0; idx < references.size(); idx++)\n      {\n        const InstanceRef &ref = references[idx];\n        if (ref.is_field_set(fid))\n        {\n          PhysicalManager *manager = ref.get_instance_manager();\n          if (privilege_only)\n          {\n            IndexSpaceNode *privilege_node =\n              runtime->forest->get_node(req.region.get_index_space());\n            return manager->create_piece_iterator(privilege_node);\n          }\n          else\n            return manager->create_piece_iterator(NULL);\n        }\n      }\n      assert(false);\n      return NULL;\n    }\n    \n    //--------------------------------------------------------------------------\n    PhysicalInstance PhysicalRegionImpl::get_instance_info(PrivilegeMode mode, \n                                              FieldID fid, size_t field_size, \n                                              void *realm_is, TypeTag type_tag,\n                                              const char *warning_string,\n                                              bool silence_warnings, \n                                              bool generic_accessor,\n                                              bool check_field_size,\n                                              ReductionOpID redop)\n    //--------------------------------------------------------------------------\n    { \n      // Check the privilege mode first\n      switch (mode)\n      {\n        case LEGION_READ_ONLY:\n          {\n            if (!(LEGION_READ_ONLY & req.privilege))\n              REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                            \"Error creating read-only field accessor without \"\n                            \"read-only privileges on field %d in task %s\",\n                            fid, context->get_task_name())\n            break;\n          }\n        case LEGION_READ_WRITE:\n          {\n            if (req.privilege == LEGION_WRITE_DISCARD)\n            {\n              if (!silence_warnings)\n                REPORT_LEGION_WARNING(LEGION_WARNING_READ_DISCARD, \n                                \"creating read-write accessor for \"\n                                \"field %d in task %s which only has \"\n                                \"WRITE_DISCARD privileges. You may be \"\n                                \"accessing uninitialized data. \"\n                                \"Warning string: %s\",\n                                fid, context->get_task_name(),\n                                (warning_string == NULL) ? \"\" : warning_string)\n            }\n            else if (req.privilege != LEGION_READ_WRITE)\n              REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                            \"Error creating read-write field accessor without \"\n                            \"read-write privileges on field %d in task %s\",\n                            fid, context->get_task_name())\n            break;\n          }\n        case LEGION_WRITE_ONLY:\n        case LEGION_WRITE_DISCARD:\n          {\n            if (!(LEGION_WRITE_DISCARD & req.privilege))\n              REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                            \"Error creating write-discard field accessor \"\n                            \"without write privileges on field %d in task %s\",\n                            fid, context->get_task_name())\n            break;\n          }\n        case LEGION_REDUCE:\n          {\n            if ((LEGION_REDUCE != req.privilege) || (redop != req.redop))\n            {\n              if (!(LEGION_REDUCE & req.privilege))\n                REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                              \"Error creating reduction field accessor \"\n                              \"without reduction privileges on field %d in \"\n                              \"task %s\", fid, context->get_task_name())\n              else if (redop != req.redop)\n                REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                              \"Error creating reduction field accessor \"\n                              \"with mismatched reduction operators %d and %d \"\n                              \"on field %d in task %s\", redop, req.redop,\n                              fid, context->get_task_name())\n              else\n                REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                              \"Error creating reduction-only field accessor \"\n                              \"for a region requirement with more than \"\n                              \"reduction-only privileges for field %d in task \"\n                              \"%s. Please use a read-write accessor instead.\",\n                              fid, context->get_task_name())\n            }\n            break;\n          }\n        default: // rest of the privileges don't matter\n          break;\n      }\n      if (context != NULL)\n      {\n        if (context->is_inner_context())\n          REPORT_LEGION_ERROR(ERROR_INNER_TASK_VIOLATION, \n            \"Illegal accessor construction inside \"\n            \"task %s (UID %lld) for a variant that was labeled as an 'inner' \"\n            \"variant.\", context->get_task_name(), context->get_unique_id())\n        else if (runtime->runtime_warnings && !silence_warnings &&\n                  !context->is_leaf_context())\n          REPORT_LEGION_WARNING(LEGION_WARNING_NONLEAF_ACCESSOR, \n              \"Accessor construction in non-leaf \"\n              \"task %s (UID %lld) is a blocking operation in violation of \"\n              \"Legion's deferred execution model best practices. You may \"\n              \"notice a severe performance degradation. Warning string: %s\",\n              context->get_task_name(), context->get_unique_id(),\n              (warning_string == NULL) ? \"\" : warning_string)\n      }\n      // If this physical region isn't mapped, then we have to\n      // map it before we can return an accessor\n      if (!mapped)\n      {\n        if (virtual_mapped)\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_IMPLICIT_MAPPING, \n                        \"Illegal implicit mapping of a virtual mapped region \"\n                        \"in task %s (UID %lld)\", context->get_task_name(),\n                        context->get_unique_id())\n        if (runtime->runtime_warnings && !silence_warnings)\n          REPORT_LEGION_WARNING(LEGION_WARNING_UNMAPPED_ACCESSOR, \n                          \"Accessor construction was \"\n                          \"performed on an unmapped region in task %s \"\n                          \"(UID %lld). Legion is mapping it for you. \"\n                          \"Please try to be more careful. Warning string: %s\",\n                          context->get_task_name(), context->get_unique_id(),\n                          (warning_string == NULL) ? \"\" : warning_string)\n        runtime->remap_region(context, PhysicalRegion(this));\n        // At this point we should have a new ready event\n        // and be mapped\n#ifdef DEBUG_LEGION\n        assert(mapped);\n#endif \n      }\n      if (req.privilege_fields.find(fid) == req.privilege_fields.end())\n        REPORT_LEGION_ERROR(ERROR_INVALID_FIELD_PRIVILEGES, \n                       \"Accessor construction for field %d in task %s \"\n                       \"without privileges!\", fid, context->get_task_name())\n      if (generic_accessor && runtime->runtime_warnings && !silence_warnings)\n        REPORT_LEGION_WARNING(LEGION_WARNING_GENERIC_ACCESSOR,\n                              \"Using a generic accessor for accessing a \"\n                              \"physical instance of task %s (UID %lld). \"\n                              \"Generic accessors are very slow and are \"\n                              \"strongly discouraged for use in high \"\n                              \"performance code. Warning string: %s\", \n                              context->get_task_name(),\n                              context->get_unique_id(),\n                              (warning_string == NULL) ? \"\" : warning_string)\n      // Get the index space to use for the accessor\n      runtime->get_index_space_domain(req.region.get_index_space(),\n                                      realm_is, type_tag);\n      // Wait until we are valid before returning the accessor\n      wait_until_valid(silence_warnings, warning_string,\n                       runtime->runtime_warnings, \"Accessor Construction\");\n      made_accessor = true;\n      for (unsigned idx = 0; idx < references.size(); idx++)\n      {\n        const InstanceRef &ref = references[idx];\n        if (ref.is_field_set(fid))\n        {\n          PhysicalManager *manager = ref.get_instance_manager();\n          if (check_field_size)\n          {\n            const size_t actual_size = \n              manager->field_space_node->get_field_size(fid);\n            if (actual_size != field_size)\n              REPORT_LEGION_ERROR(ERROR_ACCESSOR_FIELD_SIZE_CHECK,\n                            \"Error creating accessor for field %d with a \"\n                            \"type of size %zd bytes when the field was \"\n                            \"originally allocated with a size of %zd bytes \"\n                            \"in task %s (UID %lld)\",\n                            fid, field_size, actual_size, \n                            context->get_task_name(), context->get_unique_id()) \n          }\n          return manager->get_instance(context->owner_task->index_point);\n        }\n      }\n      // should never get here at worst there should have been an\n      // error raised earlier in this function\n      assert(false);\n      return PhysicalInstance::NO_INST;\n    } \n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::report_incompatible_accessor(\n              const char *accessor_kind, PhysicalInstance instance, FieldID fid)\n    //--------------------------------------------------------------------------\n    {\n      REPORT_LEGION_ERROR(ERROR_ACCESSOR_COMPATIBILITY_CHECK,\n          \"Unable to create Realm %s for field %d of instance %llx in task %s\",\n          accessor_kind, fid, instance.id, context->get_task_name())\n    }\n\n    //--------------------------------------------------------------------------\n    void PhysicalRegionImpl::report_incompatible_multi_accessor(unsigned index,\n                    FieldID fid, PhysicalInstance inst1, PhysicalInstance inst2)\n    //--------------------------------------------------------------------------\n    {\n      REPORT_LEGION_ERROR(ERROR_ACCESSOR_COMPATIBILITY_CHECK,\n          \"Unable to create multi-region accessor for field %d because \"\n          \"instances \" IDFMT \" (index 0) and \" IDFMT \" (index %d) are \"\n          \"differnt. Multi-region accessors must always be for region \"\n          \"requirements with the same physical instance.\", \n          fid, inst1.id, inst2.id, index)\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void PhysicalRegionImpl::fail_bounds_check(DomainPoint p, \n                                    FieldID fid, PrivilegeMode mode, bool multi)\n    //--------------------------------------------------------------------------\n    {\n      char point_string[128];\n      sprintf(point_string,\" (\");\n      for (int d = 0; d < p.get_dim(); d++)\n      {\n        char buffer[32];\n        if (d == 0)\n          sprintf(buffer,\"%lld\", p[0]);\n        else\n          sprintf(buffer,\",%lld\", p[d]);\n        strcat(point_string, buffer);\n      }\n      strcat(point_string,\")\");\n      switch (mode)\n      {\n        case LEGION_READ_ONLY:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_BOUNDS_CHECK, \n                          \"Bounds check failure reading point %s from \"\n                          \"field %d in task %s%s\\n\", point_string, fid,\n                          implicit_context->get_task_name(),\n                          multi ? \" for multi-region accessor\" : \"\")\n            break;\n          }\n        case LEGION_READ_WRITE:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_BOUNDS_CHECK, \n                          \"Bounds check failure geting a reference to point %s \"\n                          \"from field %d in task %s%s\\n\", point_string, fid,\n                          implicit_context->get_task_name(),\n                          multi ? \" for multi-region accessor\" : \"\")\n            break;\n          }\n        case LEGION_WRITE_ONLY:\n        case LEGION_WRITE_DISCARD:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_BOUNDS_CHECK, \n                          \"Bounds check failure writing to point %s in \"\n                          \"field %d in task %s%s\\n\", point_string, fid,\n                          implicit_context->get_task_name(),\n                          multi ? \" for multi-region accessor\" : \"\")\n            break;\n          }\n        case LEGION_REDUCE:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_BOUNDS_CHECK, \n                          \"Bounds check failure reducing to point %s in \"\n                          \"field %d in task %s%s\\n\", point_string, fid,\n                          implicit_context->get_task_name(),\n                          multi ? \" for multi-region accessor\" : \"\")\n            break;\n          }\n        default:\n          assert(false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void PhysicalRegionImpl::fail_bounds_check(Domain dom, \n                                    FieldID fid, PrivilegeMode mode, bool multi)\n    //--------------------------------------------------------------------------\n    {\n      char rect_string[256];\n      sprintf(rect_string,\" (\");\n      for (int d = 0; d < dom.get_dim(); d++)\n      {\n        char buffer[32];\n        if (d == 0)\n          sprintf(buffer,\"%lld\", dom.lo()[0]);\n        else\n          sprintf(buffer,\",%lld\", dom.lo()[d]);\n        strcat(rect_string, buffer);\n      }\n      strcat(rect_string,\") - (\");\n      for (int d = 0; d < dom.get_dim(); d++)\n      {\n        char buffer[32];\n        if (d == 0)\n          sprintf(buffer,\"%lld\", dom.hi()[0]);\n        else\n          sprintf(buffer,\",%lld\", dom.hi()[d]);\n        strcat(rect_string, buffer);\n      }\n      strcat(rect_string,\")\");\n      switch (mode)\n      {\n        case LEGION_READ_ONLY:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_BOUNDS_CHECK, \n                          \"Bounds check failure getting a read-only reference \"\n                          \"to rect %s from field %d in task %s%s\\n\", \n                          rect_string, fid, implicit_context->get_task_name(),\n                          multi ? \" for multi-region accessor\" : \"\")\n            break;\n          }\n        case LEGION_READ_WRITE:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_BOUNDS_CHECK, \n                          \"Bounds check failure geting a reference to rect %s \"\n                          \"from field %d in task %s%s\\n\", rect_string, fid,\n                          implicit_context->get_task_name(),\n                          multi ? \" for multi-region accessor\" : \"\")\n            break;\n          }\n        default:\n          assert(false);\n      }\n    } \n\n    //--------------------------------------------------------------------------\n    /*static*/ void PhysicalRegionImpl::fail_privilege_check(DomainPoint p, \n                                                FieldID fid, PrivilegeMode mode)\n    //--------------------------------------------------------------------------\n    {\n      char point_string[128];\n      sprintf(point_string,\" (\");\n      for (int d = 0; d < p.get_dim(); d++)\n      {\n        char buffer[32];\n        if (d == 0)\n          sprintf(buffer,\"%lld\", p[0]);\n        else\n          sprintf(buffer,\",%lld\", p[d]);\n        strcat(point_string, buffer);\n      }\n      strcat(point_string,\")\");\n      switch (mode)\n      {\n        case LEGION_READ_ONLY:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                          \"Privilege check failure reading point %s from \"\n                          \"field %d in task %s\\n\", point_string, fid,\n                          implicit_context->get_task_name())\n            break;\n          }\n        case LEGION_READ_WRITE:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                          \"Privilege check failure geting a reference to point \"\n                          \"%s from field %d in task %s\\n\", point_string, fid,\n                          implicit_context->get_task_name())\n            break;\n          }\n        case LEGION_WRITE_ONLY:\n        case LEGION_WRITE_DISCARD:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                          \"Privilege check failure writing to point %s in \"\n                          \"field %d in task %s\\n\", point_string, fid,\n                          implicit_context->get_task_name())\n            break;\n          }\n        case LEGION_REDUCE:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                          \"Privilege check failure reducing to point %s in \"\n                          \"field %d in task %s\\n\", point_string, fid,\n                          implicit_context->get_task_name())\n            break;\n          }\n        default:\n          assert(false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void PhysicalRegionImpl::fail_privilege_check(Domain dom, \n                                                FieldID fid, PrivilegeMode mode)\n    //--------------------------------------------------------------------------\n    {\n      char rect_string[256];\n      sprintf(rect_string,\" (\");\n      for (int d = 0; d < dom.get_dim(); d++)\n      {\n        char buffer[32];\n        if (d == 0)\n          sprintf(buffer,\"%lld\", dom.lo()[0]);\n        else\n          sprintf(buffer,\",%lld\", dom.lo()[d]);\n        strcat(rect_string, buffer);\n      }\n      strcat(rect_string,\") - (\");\n      for (int d = 0; d < dom.get_dim(); d++)\n      {\n        char buffer[32];\n        if (d == 0)\n          sprintf(buffer,\"%lld\", dom.hi()[0]);\n        else\n          sprintf(buffer,\",%lld\", dom.hi()[d]);\n        strcat(rect_string, buffer);\n      }\n      strcat(rect_string,\")\");\n      switch (mode)\n      {\n        case LEGION_READ_ONLY:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                          \"Privilege check failure getting a read-only \"\n                          \"reference to rect %s from field %d in task %s\\n\", \n                          rect_string, fid, implicit_context->get_task_name())\n            break;\n          }\n        case LEGION_READ_WRITE:\n          {\n            REPORT_LEGION_ERROR(ERROR_ACCESSOR_PRIVILEGE_CHECK, \n                          \"Privilege check failure geting a reference to rect \"\n                          \"%s from field %d in task %s\\n\", rect_string, fid,\n                          implicit_context->get_task_name())\n            break;\n          }\n        default:\n          assert(false);\n      }\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Output Region Impl\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    OutputRegionImpl::OutputRegionImpl(unsigned i,\n                                       const RegionRequirement &r,\n                                       InstanceSet is,\n                                       TaskContext *ctx,\n                                       Runtime *rt, const bool global,\n                                       const bool valid)\n      : Collectable(), runtime(rt), context(ctx),\n        req(r), instance_set(is), num_elements(-1LU), index(i), \n        created_region(\n          (req.flags & LEGION_CREATED_OUTPUT_REQUIREMENT_FLAG) && !valid),\n        global_indexing(global)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    OutputRegionImpl::OutputRegionImpl(const OutputRegionImpl &rhs)\n      : Collectable(), runtime(NULL), context(NULL),\n        req(), instance_set(), num_elements(-1LU), index(-1U), \n        created_region(false), global_indexing(false)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    OutputRegionImpl::~OutputRegionImpl(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    OutputRegionImpl& OutputRegionImpl::operator=(\n                                                  const OutputRegionImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    Memory OutputRegionImpl::target_memory(void) const\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(instance_set.size() > 0);\n#endif\n      InstanceRef ref = instance_set[0];\n      return ref.get_manager()->as_individual_manager()->get_memory();\n    }\n\n    //--------------------------------------------------------------------------\n    void OutputRegionImpl::return_data(size_t new_num_elements,\n                                       FieldID field_id,\n                                       uintptr_t ptr,\n                                       size_t alignment,\n                                       bool eager_pool /*= false */)\n    //--------------------------------------------------------------------------\n    {\n      std::map<FieldID,ExternalInstanceInfo>::iterator finder =\n        returned_instances.find(field_id);\n      if (finder != returned_instances.end())\n      {\n        REPORT_LEGION_ERROR(ERROR_INVALID_OUTPUT_SIZE,\n          \"Data has already been set to field %u of output region %u of \"\n          \"task %s (UID: %lld). You can return data for each field of an \"\n          \"output region only once.\",\n          field_id, index, context->owner_task->get_task_name(),\n          context->owner_task->get_unique_op_id());\n      }\n\n      if (num_elements != -1LU && new_num_elements != num_elements)\n      {\n          REPORT_LEGION_ERROR(ERROR_INVALID_OUTPUT_SIZE,\n            \"Output region %u of task %s (UID: %lld) has already been \"\n            \"initialized to have %zd elements, but the new output data \"\n            \"holds %zd elements. You must return the same number of \"\n            \"elements to all the fields in the same output region.\",\n            index, context->owner_task->get_task_name(),\n            context->owner_task->get_unique_op_id(),\n            num_elements, new_num_elements);\n      }\n      else\n        num_elements = new_num_elements;\n\n      if (req.privilege_fields.find(field_id) == req.privilege_fields.end())\n      {\n          REPORT_LEGION_ERROR(ERROR_INVALID_OUTPUT_FIELD,\n            \"Output region %u of task %s (UID: %lld) does not have privilege \"\n            \"on field %u.\", index, context->owner_task->get_task_name(),\n            context->owner_task->get_unique_op_id(), field_id);\n      }\n\n      // Here we simply queue up the output data, rather than eagerly\n      // creating and setting an instance to the output region.\n      ExternalInstanceInfo &info = returned_instances[field_id];\n      info.eager_pool = eager_pool;\n      // Sanitize the pointer when the size is 0\n      info.ptr = num_elements != 0 ? ptr : 0;\n      info.alignment = alignment;\n    }\n\n    //--------------------------------------------------------------------------\n    void OutputRegionImpl::return_data(size_t num_elements,\n                                       std::map<FieldID,void*> ptrs,\n                                       std::map<FieldID,size_t> *_alignments)\n    //--------------------------------------------------------------------------\n    {\n      std::map<FieldID,size_t> dummy_alignments;\n      std::map<FieldID,size_t> &alignments =\n        _alignments != NULL ?  *_alignments : dummy_alignments;\n\n      for (std::map<FieldID,void*>::iterator it = ptrs.begin();\n           it != ptrs.end(); ++it)\n      {\n        std::map<FieldID,size_t>::iterator finder = alignments.find(it->first);\n        size_t alignment = finder != alignments.end() ? finder->second : 0;\n        return_data(num_elements,\n                    it->first,\n                    reinterpret_cast<uintptr_t>(it->second),\n                    alignment,\n                    false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void OutputRegionImpl::return_data(FieldID field_id,\n                                       PhysicalInstance instance,\n                                       size_t field_size,\n                                       const size_t *pnum_elements)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode *fspace_node = \n        runtime->forest->get_node(req.region.get_field_space());\n      size_t alloc_size = fspace_node->get_field_size(field_id);\n\n      if (alloc_size != field_size)\n        REPORT_LEGION_ERROR(ERROR_INVALID_OUTPUT_SIZE,\n          \"Field %u of output region %u of task %s (UID: %lld) has a type of \"\n          \"size %zd, but the returned deferred buffer is allocaited with a \"\n          \"type of size %zd.\",\n          field_id, index, context->owner_task->get_task_name(),\n          context->owner_task->get_unique_op_id(), alloc_size, field_size);\n\n      IndividualManager *manager = get_manager(field_id);\n      if (instance.get_location() != manager->get_memory())\n        REPORT_LEGION_ERROR(ERROR_INVALID_OUTPUT_SIZE,\n          \"Field %u of output region %u of task %s (UID: %lld) is requested \"\n          \"to have an instance on memory \" IDFMT \", but the returned instance \"\n          \"is allocated on memory \" IDFMT \".\",\n          field_id, index, context->owner_task->get_task_name(),\n          context->owner_task->get_unique_op_id(),\n          instance.get_location().id, manager->get_memory().id);\n\n      // The realm instance backing a deferred buffer is currently tagged as\n      // a task local instance, so we need to tell the runtime that the instance\n      // now escapes the context.\n      uintptr_t ptr = context->escape_task_local_instance(instance);\n\n      // This is safe to do as we require the deferred buffer to be 1-D.\n      const Realm::InstanceLayout<1,coord_t> *layout =\n        reinterpret_cast<const Realm::InstanceLayout<1,coord_t>*>(\n            instance.get_layout());\n\n      size_t num_elements = pnum_elements != NULL\n                          ? *pnum_elements\n                          : layout->space.bounds.volume();\n\n      return_data(num_elements, field_id, ptr, layout->alignment_reqd, true);\n      // This instance was escaped so the context is no longer responsible\n      // for destroying it when the task is done, we take that responsibility\n      escaped_instances.push_back(instance);\n    }\n\n    //--------------------------------------------------------------------------\n    void OutputRegionImpl::finalize(bool defer /*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      Domain domain;\n      RegionNode *node = runtime->forest->get_node(req.region);\n      if (created_region)\n      {\n        IndexSpaceNode *index_node =\n          node->get_row_source()->as_index_space_node();\n        // Subregions of a globally indexed output region cannot be finalized \n        // in the first round because their sizes are yet to be determined.\n        if (defer && req.partition.exists() && global_indexing)\n        {\n          add_reference();\n          FinalizeOutputArgs args(this);\n          runtime->issue_runtime_meta_task(\n              args, LG_THROUGHPUT_DEFERRED_PRIORITY,\n              Runtime::protect_event(index_node->index_space_ready));\n          return;\n        }\n\n        // Initialize the index space domain\n        if (req.partition.exists())\n        {\n          if (!global_indexing)\n          {\n            DomainPoint index_point = context->owner_task->index_point;\n            domain.dim = index_point.get_dim() + 1;\n#ifdef DEBUG_LEGION\n            assert(domain.dim <= LEGION_MAX_DIM);\n#endif\n            for (int idx = 0; idx < index_point.dim; ++idx)\n            {\n              domain.rect_data[idx] = index_point[idx];\n              domain.rect_data[idx + domain.dim] = index_point[idx];\n            }\n            if (num_elements > 0)\n            {\n              domain.rect_data[domain.dim-1] = 0;\n              domain.rect_data[2*domain.dim-1] = num_elements - 1;\n            }\n            else\n            {\n              domain.rect_data[domain.dim-1] = 1;\n              domain.rect_data[2*domain.dim-1] = 0;\n            }\n\n            runtime->forest->set_pending_space_domain(\n                index_node->handle, domain, runtime->address_space);\n          }\n          else\n          {\n            // For a globally indexed output region, the domain has\n            // already been initialized once we reach here, so\n            // we just retrieve it.\n            ApEvent ready = ApEvent::NO_AP_EVENT;\n            domain = index_node->get_domain(ready, true);\n            if (ready.exists())\n              ready.wait();\n          }\n        }\n        else\n        {\n          domain =\n            num_elements > 0 ? Rect<1>(0, num_elements - 1) : Rect<1>(0, -1);\n          index_node->set_domain(domain, runtime->address_space);\n        }\n      }\n      else\n        node->row_source->get_launch_space_domain(domain);\n\n      FieldSpaceNode *fspace_node = node->get_column_source();\n\n      // Create a Realm instance and update the physical manager\n      // for each output field\n      for (std::map<FieldID,ExternalInstanceInfo>::iterator it =\n           returned_instances.begin(); it !=\n           returned_instances.end(); ++it)\n      {\n        FieldID field_id = it->first;\n        IndividualManager *manager = get_manager(field_id);\n\n        // Create a Realm layout\n        std::map<Realm::FieldID,size_t> field_sizes;\n        size_t field_size = fspace_node->get_field_size(field_id);\n        field_sizes[field_id] = field_size;\n        Realm::InstanceLayoutConstraints constraints(field_sizes,\n                                                     0 /*block_size*/);\n        Realm::InstanceLayoutGeneric *layout = NULL;\n        switch (domain.get_dim())\n        {\n#define DIMFUNC(DIM)                                                         \\\n          case DIM:                                                          \\\n            {                                                                \\\n              int dim_order[DIM];                                            \\\n              for (unsigned idx = 0; idx < DIM; ++idx)                       \\\n                dim_order[idx] = idx;                                        \\\n              const DomainT<DIM,coord_t> bounds = Rect<DIM,coord_t>(domain); \\\n              layout =                                                       \\\n                Realm::InstanceLayoutGeneric::choose_instance_layout(        \\\n                    bounds, constraints, dim_order);                         \\\n              break;                                                         \\\n            }\n          LEGION_FOREACH_N(DIMFUNC)\n#undef DIMFUNC\n          default:\n            assert(false);\n        }\n#ifdef DEBUG_LEGION\n        assert(layout != NULL);\n#endif\n\n        LayoutConstraints *manager_cons = manager->layout->constraints;\n\n        size_t alignment = 0;\n        if (!manager_cons->alignment_constraints.empty())\n        {\n#ifdef DEBUG_LEGION\n          assert(manager_cons->alignment_constraints.size() == 1);\n          assert(manager_cons->alignment_constraints[0].fid == field_id);\n#endif\n          alignment = manager_cons->alignment_constraints[0].alignment;\n        }\n        // If no alignment is given, set it to the field size\n        if (alignment == 0)\n          alignment = field_size;\n        size_t bytes_used =\n          field_size > 0\n          ? (num_elements * field_size + alignment - 1) / alignment * alignment\n          : 0;\n        layout->bytes_used = bytes_used;\n\n        if (!manager_cons->offset_constraints.empty())\n        {\n#ifdef DEBUG_LEGION\n          assert(manager_cons->offset_constraints.size() == 1);\n          assert(manager_cons->offset_constraints[0].fid == field_id);\n#endif\n          Realm::InstanceLayoutGeneric::FieldLayout &fl =\n            layout->fields[field_id];\n          fl.rel_offset = manager_cons->offset_constraints[0].offset;\n        }\n\n        // Create an external Realm instance\n        Realm::RegionInstance instance;\n        Realm::ProfilingRequestSet no_requests;\n        ExternalInstanceInfo &info = it->second;\n        const Realm::ExternalMemoryResource resource(info.ptr, \n                      layout->bytes_used, false/*read only*/);\n        RtEvent wait_on(Realm::RegionInstance::create_external_instance(\n          instance, manager->get_memory(), layout, resource, no_requests));\n        if (wait_on.exists())\n          wait_on.wait();\n#ifdef DEBUG_LEGION\n        assert(instance.exists());\n#endif\n        // Finally we set the instance to the physical manager\n        const bool delete_now = manager->update_physical_instance(instance,\n                                          info.eager_pool ? \n                                          PhysicalManager::EAGER_INSTANCE_KIND :\n                                  PhysicalManager::EXTERNAL_OWNED_INSTANCE_KIND,\n                                          bytes_used,\n                                          info.ptr);\n        if (delete_now)\n          delete manager;\n      }\n      // Lasty destroy our physical instance objects since the task is done\n      for (std::vector<PhysicalInstance>::const_iterator it =\n            escaped_instances.begin(); it != escaped_instances.end(); it++)\n        it->destroy();\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void OutputRegionImpl::handle_finalize_output(const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const FinalizeOutputArgs *finalize_args = (const FinalizeOutputArgs*)args;\n      OutputRegionImpl *region = finalize_args->region;\n      region->finalize(false);\n      if (region->remove_reference())\n        delete region;\n    }\n\n    //--------------------------------------------------------------------------\n    bool OutputRegionImpl::is_complete(FieldID &unbound_field) const\n    //--------------------------------------------------------------------------\n    {\n      for (std::vector<FieldID>::const_iterator it =\n           req.instance_fields.begin(); it !=\n           req.instance_fields.end(); ++it)\n      {\n        if (returned_instances.find(*it) == returned_instances.end())\n        {\n          unbound_field = *it;\n          return false;\n        }\n      }\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    IndividualManager *OutputRegionImpl::get_manager(FieldID field_id)\n    //--------------------------------------------------------------------------\n    {\n      RegionNode *node = runtime->forest->get_node(req.region);\n      FieldSpaceNode *fspace_node = node->get_column_source();\n\n      std::set<FieldID> fields; fields.insert(field_id);\n      FieldMask mask = fspace_node->get_field_mask(fields);\n\n      // Find the right physical manager by checking against\n      // the field mask of the instance ref\n      IndividualManager *manager = NULL;\n      for (unsigned idx = 0; idx < instance_set.size(); ++idx)\n      {\n        const InstanceRef &instance = instance_set[idx];\n        if (!!(instance.get_valid_fields() & mask))\n        {\n          manager =\n            instance.get_instance_manager()->as_individual_manager();\n          break;\n        }\n      }\n#ifdef DEBUG_LEGION\n      assert(manager != NULL);\n#endif\n      return manager;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Grant Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    GrantImpl::GrantImpl(void)\n      : acquired(false)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    GrantImpl::GrantImpl(const std::vector<ReservationRequest> &reqs)\n      : requests(reqs), acquired(false)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    GrantImpl::GrantImpl(const GrantImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    GrantImpl::~GrantImpl(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    GrantImpl& GrantImpl::operator=(const GrantImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void GrantImpl::register_operation(ApEvent completion_event)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock g_lock(grant_lock);\n      completion_events.insert(completion_event);\n    }\n\n    //--------------------------------------------------------------------------\n    ApEvent GrantImpl::acquire_grant(void)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock g_lock(grant_lock);\n      if (!acquired)\n      {\n        grant_event = ApEvent::NO_AP_EVENT;\n        for (std::vector<ReservationRequest>::const_iterator it = \n              requests.begin(); it != requests.end(); it++)\n        {\n          grant_event = ApEvent(it->reservation.acquire(it->mode, \n                                                it->exclusive, grant_event));\n        }\n        acquired = true;\n      }\n      return grant_event;\n    }\n\n    //--------------------------------------------------------------------------\n    void GrantImpl::release_grant(void)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock g_lock(grant_lock);\n      ApEvent deferred_release = Runtime::merge_events(NULL, completion_events);\n      for (std::vector<ReservationRequest>::const_iterator it = \n            requests.begin(); it != requests.end(); it++)\n      {\n        it->reservation.release(deferred_release);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void GrantImpl::pack_grant(Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      ApEvent pack_event = acquire_grant();\n      rez.serialize(pack_event);\n    }\n\n    //--------------------------------------------------------------------------\n    void GrantImpl::unpack_grant(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ApEvent unpack_event;\n      derez.deserialize(unpack_event);\n      AutoLock g_lock(grant_lock);\n#ifdef DEBUG_LEGION\n      assert(!acquired);\n#endif\n      grant_event = unpack_event;\n      acquired = true;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Legion Handshake Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    LegionHandshakeImpl::LegionHandshakeImpl(bool init_ext, int ext_parts,\n                                                   int legion_parts)\n      : init_in_ext(init_ext), ext_participants(ext_parts), \n        legion_participants(legion_parts)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    LegionHandshakeImpl::LegionHandshakeImpl(const LegionHandshakeImpl &rhs)\n      : init_in_ext(false), ext_participants(-1), legion_participants(-1)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    LegionHandshakeImpl::~LegionHandshakeImpl(void)\n    //--------------------------------------------------------------------------\n    {\n      ext_wait_barrier.get_barrier().destroy_barrier();\n      legion_wait_barrier.get_barrier().destroy_barrier();\n    }\n\n    //--------------------------------------------------------------------------\n    LegionHandshakeImpl& LegionHandshakeImpl::operator=(\n                                                 const LegionHandshakeImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void LegionHandshakeImpl::initialize(void)\n    //--------------------------------------------------------------------------\n    {\n      ext_wait_barrier = PhaseBarrier(ApBarrier(\n            Realm::Barrier::create_barrier(legion_participants)));\n      legion_wait_barrier = PhaseBarrier(ApBarrier(\n            Realm::Barrier::create_barrier(ext_participants)));\n      ext_arrive_barrier = legion_wait_barrier;\n      legion_arrive_barrier = ext_wait_barrier;\n      // Advance the two wait barriers\n      Runtime::advance_barrier(ext_wait_barrier);\n      Runtime::advance_barrier(legion_wait_barrier);\n      // Whoever is waiting first, we have to advance their arrive barriers\n      if (init_in_ext)\n      {\n        Runtime::phase_barrier_arrive(legion_arrive_barrier, legion_participants);\n        Runtime::advance_barrier(ext_wait_barrier);\n      }\n      else\n      {\n        Runtime::phase_barrier_arrive(ext_arrive_barrier, ext_participants);\n        Runtime::advance_barrier(legion_wait_barrier);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void LegionHandshakeImpl::ext_handoff_to_legion(void)\n    //--------------------------------------------------------------------------\n    {\n      // Just have to do our arrival\n      Runtime::phase_barrier_arrive(ext_arrive_barrier, 1);\n    }\n\n    //--------------------------------------------------------------------------\n    void LegionHandshakeImpl::ext_wait_on_legion(void)\n    //--------------------------------------------------------------------------\n    {\n      // When we get this call, we know we have done \n      // all the arrivals so we can advance it\n      Runtime::advance_barrier(ext_arrive_barrier);\n      // Wait for ext  to be ready to run\n      // Note we use the external wait to be sure \n      // we don't get drafted by the Realm runtime\n      ApBarrier previous = Runtime::get_previous_phase(ext_wait_barrier);\n      if (!previous.has_triggered_faultignorant())\n      {\n        // We can't call external wait directly on the barrier\n        // right now, so as a work-around we'll make an event\n        // and then wait on that\n        ApUserEvent wait_on = Runtime::create_ap_user_event(NULL);\n        Runtime::trigger_event(NULL, wait_on, previous);\n        wait_on.external_wait();\n      }\n      // Now we can advance our wait barrier\n      Runtime::advance_barrier(ext_wait_barrier);\n    }\n\n    //--------------------------------------------------------------------------\n    void LegionHandshakeImpl::legion_handoff_to_ext(void)\n    //--------------------------------------------------------------------------\n    {\n      // Just have to do our arrival\n      Runtime::phase_barrier_arrive(legion_arrive_barrier, 1);\n    }\n\n    //--------------------------------------------------------------------------\n    void LegionHandshakeImpl::legion_wait_on_ext(void)\n    //--------------------------------------------------------------------------\n    {\n      Runtime::advance_barrier(legion_arrive_barrier);\n      // Wait for Legion to be ready to run\n      // No need to avoid being drafted by the\n      // Realm runtime here\n      legion_wait_barrier.wait();\n      // Now we can advance our wait barrier\n      Runtime::advance_barrier(legion_wait_barrier);\n    }\n\n    //--------------------------------------------------------------------------\n    PhaseBarrier LegionHandshakeImpl::get_legion_wait_phase_barrier(void)\n    //--------------------------------------------------------------------------\n    {\n      return legion_wait_barrier;\n    }\n\n    //--------------------------------------------------------------------------\n    PhaseBarrier LegionHandshakeImpl::get_legion_arrive_phase_barrier(void)\n    //--------------------------------------------------------------------------\n    {\n      return legion_arrive_barrier;\n    }\n\n    //--------------------------------------------------------------------------\n    void LegionHandshakeImpl::advance_legion_handshake(void)\n    //--------------------------------------------------------------------------\n    {\n      Runtime::advance_barrier(legion_wait_barrier);\n      Runtime::advance_barrier(legion_arrive_barrier);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // MPI Rank Table\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    MPIRankTable::MPIRankTable(Runtime *rt)\n      : runtime(rt), collective_radix(rt->legion_collective_radix),\n        done_triggered(false)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime->total_address_spaces > 1)\n      {\n        configure_collective_settings(runtime->total_address_spaces,\n            runtime->address_space, collective_radix, collective_log_radix,\n            collective_stages, collective_participating_spaces, \n            collective_last_radix);\n        participating = \n          (int(runtime->address_space) < collective_participating_spaces);\n        // We already have our contributions for each stage so\n        // we can set the inditial participants to 1\n        if (participating)\n        {\n          sent_stages.resize(collective_stages, false);\n#ifdef DEBUG_LEGION\n          assert(collective_stages > 0);\n#endif\n          stage_notifications.resize(collective_stages, 1);\n          // Stage 0 always starts with 0 notifications since we'll \n          // explictcly arrive on it\n          stage_notifications[0] = 0;\n        }\n        done_event = Runtime::create_rt_user_event();\n      }\n      // Add ourselves to the set before any exchanges start\n#ifdef DEBUG_LEGION\n      assert(Runtime::mpi_rank >= 0);\n#endif\n      forward_mapping[Runtime::mpi_rank] = runtime->address_space;\n    }\n    \n    //--------------------------------------------------------------------------\n    MPIRankTable::MPIRankTable(const MPIRankTable &rhs)\n      : runtime(NULL), participating(false)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    MPIRankTable::~MPIRankTable(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    MPIRankTable& MPIRankTable::operator=(const MPIRankTable &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void MPIRankTable::perform_rank_exchange(void)\n    //--------------------------------------------------------------------------\n    {\n      // We can skip this part if there are not multiple nodes\n      if (runtime->total_address_spaces > 1)\n      {\n        // See if we are participating node or not\n        if (participating)\n        {\n          // We are a participating node\n          // See if we are waiting for an initial notification\n          // if not we can just send our message now\n          if ((int(runtime->total_address_spaces) ==\n                collective_participating_spaces) ||\n              (runtime->address_space >= (runtime->total_address_spaces -\n                collective_participating_spaces)))\n          {\n            const bool all_stages_done = initiate_exchange();\n            if (all_stages_done)\n              complete_exchange();\n          }\n        }\n        else\n        {\n          // We are not a participating node\n          // so we just have to send notification to one node\n          send_remainder_stage();\n        }\n        // Wait for our done event to be ready\n        done_event.wait();\n      }\n#ifdef DEBUG_LEGION\n      assert(forward_mapping.size() == runtime->total_address_spaces);\n#endif\n      // Reverse the mapping\n      for (std::map<int,AddressSpace>::const_iterator it = \n            forward_mapping.begin(); it != forward_mapping.end(); it++)\n        reverse_mapping[it->second] = it->first;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MPIRankTable::initiate_exchange(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(participating); // should only get this for participating shards\n#endif\n      {\n        AutoLock r_lock(reservation);\n#ifdef DEBUG_LEGION\n        assert(!sent_stages.empty());\n        assert(!sent_stages[0]); // stage 0 shouldn't be sent yet\n        assert(!stage_notifications.empty());\n        if (collective_stages == 1)\n          assert(stage_notifications[0] < collective_last_radix); \n        else\n          assert(stage_notifications[0] < collective_radix);\n#endif\n        stage_notifications[0]++;\n      }\n      return send_ready_stages(0/*start stage*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void MPIRankTable::send_remainder_stage(void)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(-1);\n        AutoLock r_lock(reservation, 1, false/*exclusive*/);\n        rez.serialize<size_t>(forward_mapping.size());\n        for (std::map<int,AddressSpace>::const_iterator it = \n              forward_mapping.begin(); it != forward_mapping.end(); it++)\n        {\n          rez.serialize(it->first);\n          rez.serialize(it->second);\n        }\n      }\n      if (participating)\n      {\n        // Send back to the nodes that are not participating\n        AddressSpaceID target = runtime->address_space +\n          collective_participating_spaces;\n#ifdef DEBUG_LEGION\n        assert(target < runtime->total_address_spaces);\n#endif\n        runtime->send_mpi_rank_exchange(target, rez);\n      }\n      else\n      {\n        // Sent to a node that is participating\n        AddressSpaceID target = runtime->address_space % \n          collective_participating_spaces;\n        runtime->send_mpi_rank_exchange(target, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool MPIRankTable::send_ready_stages(const int start_stage) \n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(participating);\n#endif\n      // Iterate through the stages and send any that are ready\n      // Remember that stages have to be done in order\n      for (int stage = start_stage; stage < collective_stages; stage++)\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(stage);\n          AutoLock r_lock(reservation);\n          // If this stage has already been sent then we can keep going\n          if (sent_stages[stage])\n            continue;\n          // Check to see if we're sending this stage\n          // We need all the notifications from the previous stage before\n          // we can send this stage\n          if ((stage > 0) && (stage_notifications[stage-1] < collective_radix))\n            return false;\n          // If we get here then we can send the stage\n          sent_stages[stage] = true;\n#ifdef DEBUG_LEGION\n          {\n            size_t expected_size = 1;\n            for (int idx = 0; idx < stage; idx++)\n              expected_size *= collective_radix;\n            assert(expected_size <= forward_mapping.size());\n          }\n#endif\n          rez.serialize<size_t>(forward_mapping.size());\n          for (std::map<int,AddressSpace>::const_iterator it = \n                forward_mapping.begin(); it != forward_mapping.end(); it++)\n          {\n            rez.serialize(it->first);\n            rez.serialize(it->second);\n          }\n        }\n        // Now we can do the send\n        if (stage == (collective_stages-1))\n        {\n          for (int r = 1; r < collective_last_radix; r++)\n          {\n            AddressSpaceID target = runtime->address_space ^\n              (r << (stage * collective_log_radix));\n#ifdef DEBUG_LEGION\n            assert(int(target) < collective_participating_spaces);\n#endif\n            runtime->send_mpi_rank_exchange(target, rez);\n          }\n        }\n        else\n        {\n          for (int r = 1; r < collective_radix; r++)\n          {\n            AddressSpaceID target = runtime->address_space ^\n              (r << (stage * collective_log_radix));\n#ifdef DEBUG_LEGION\n            assert(int(target) < collective_participating_spaces);\n#endif\n            runtime->send_mpi_rank_exchange(target, rez);\n          }\n        }\n      }\n      // If we make it here, then we sent the last stage, check to see\n      // if we've seen all the notifications for it\n      AutoLock r_lock(reservation);\n      if ((stage_notifications.back() == collective_last_radix)\n          && !done_triggered)\n      {\n        done_triggered = true;\n        return true;\n      }\n      else\n        return false;\n    }\n\n    //--------------------------------------------------------------------------\n    void MPIRankTable::handle_mpi_rank_exchange(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      int stage;\n      derez.deserialize(stage);\n#ifdef DEBUG_LEGION\n      assert(participating || (stage == -1));\n#endif\n      unpack_exchange(stage, derez);\n      bool all_stages_done = false;\n      if (stage == -1)\n      {\n        if (!participating)\n          all_stages_done = true;\n        else // we can now send our stage 0\n          all_stages_done = initiate_exchange();\n      }\n      else\n        all_stages_done = send_ready_stages();\n      if (all_stages_done)\n        complete_exchange();\n    }\n\n    //--------------------------------------------------------------------------\n    void MPIRankTable::unpack_exchange(int stage, Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      size_t num_entries;\n      derez.deserialize(num_entries);\n      AutoLock r_lock(reservation);\n      for (unsigned idx = 0; idx < num_entries; idx++)\n      {\n        int rank;\n        derez.deserialize(rank);\n\tunsigned space;\n\tderez.deserialize(space);\n#ifdef DEBUG_LEGION\n\t// Duplicates are possible because later messages aren't \"held\", but\n\t// they should be exact matches\n\tassert ((forward_mapping.count(rank) == 0) ||\n\t\t(forward_mapping[rank] == space));\n#endif\n\tforward_mapping[rank] = space;\n      }\n      if (stage >= 0)\n      {\n#ifdef DEBUG_LEGION\n\tassert(stage < int(stage_notifications.size()));\n        if (stage < (collective_stages-1))\n          assert(stage_notifications[stage] < collective_radix);\n        else\n          assert(stage_notifications[stage] < collective_last_radix);\n#endif\n        stage_notifications[stage]++;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void MPIRankTable::complete_exchange(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(forward_mapping.size() == runtime->total_address_spaces);\n#endif\n      // See if we have to send a message back to a\n      // non-participating node\n      if ((int(runtime->total_address_spaces) > \n           collective_participating_spaces) &&\n          (int(runtime->address_space) < int(runtime->total_address_spaces -\n            collective_participating_spaces)))\n        send_remainder_stage();\n      // We are done\n      Runtime::trigger_event(done_event);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Implicit Shard Manager\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ImplicitShardManager::ImplicitShardManager(Runtime *rt, TaskID tid,\n                     MapperID mid, Processor::Kind k, unsigned shards_per_space)\n      : Collectable(), runtime(rt), task_id(tid), mapper_id(mid), kind(k), \n        shards_per_address_space(shards_per_space), \n        expected_local_arrivals(shards_per_space), expected_remote_arrivals(0),\n        local_shard_id(0), top_context(NULL), shard_manager(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // If we're the owner node, we also expect one arrival from\n      // every remote node as well\n      if (runtime->address_space == 0)\n        expected_remote_arrivals = (runtime->total_address_spaces - 1);\n    }\n\n    //--------------------------------------------------------------------------\n    ImplicitShardManager::ImplicitShardManager(const ImplicitShardManager &rhs)\n      : Collectable(), runtime(rhs.runtime), task_id(rhs.task_id), \n        mapper_id(rhs.mapper_id), kind(rhs.kind), \n        shards_per_address_space(rhs.shards_per_address_space)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    ImplicitShardManager::~ImplicitShardManager(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ImplicitShardManager& ImplicitShardManager::operator=(\n                                                const ImplicitShardManager &rhs)\n    //--------------------------------------------------------------------------\n    {\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    bool ImplicitShardManager::record_arrival(bool local)\n    //--------------------------------------------------------------------------\n    {\n      // No need for the lock here, we're always protected by the shard_lock\n      // when this is called\n      if (local)\n      {\n#ifdef DEBUG_LEGION\n        assert(expected_local_arrivals > 0);\n#endif\n        return ((--expected_local_arrivals == 0) && \n                  (expected_remote_arrivals == 0));\n      }\n      else\n      {\n#ifdef DEBUG_LEGION\n        assert(expected_remote_arrivals > 0);\n#endif\n        return ((--expected_remote_arrivals == 0) &&\n                (expected_local_arrivals == 0));\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    ShardTask* ImplicitShardManager::create_shard(int shard_id, Processor proxy,\n                                                  const char *task_name)\n    //--------------------------------------------------------------------------\n    {\n      ShardTask *result = NULL;\n      if (runtime->address_space == 0)\n      {\n        AutoLock m_lock(manager_lock);\n        if (shard_manager == NULL)\n          create_shard_manager(proxy, task_name);\n#ifdef DEBUG_LEGION\n        assert(local_shard_id < shards_per_address_space);\n#endif\n        const ShardID shard = (shard_id < 0) ? local_shard_id++ : shard_id;\n        result = shard_manager->create_shard(shard, proxy);\n      }\n      else\n      {\n        RtEvent wait_on;\n        if (shard_manager == NULL)\n        {\n          AutoLock m_lock(manager_lock); \n          if (shard_manager == NULL)\n          {\n            if (!manager_ready.exists())\n              request_shard_manager();\n            wait_on = manager_ready;\n          }\n        }\n        if (wait_on.exists())\n          wait_on.wait();\n        AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n        assert(local_shard_id < shards_per_address_space);\n#endif\n        const ShardID shard = (shard_id < 0) ? (runtime->address_space * \n          shards_per_address_space + local_shard_id++) : shard_id; \n        result = shard_manager->create_shard(shard, proxy);\n      }\n#ifdef DEBUG_LEGION\n      assert(top_context != NULL);\n#endif\n      top_context->increment_pending();\n      result->initialize_implicit_task(top_context, task_id, mapper_id, proxy);\n      result->complete_mapping();\n      result->resolve_speculation();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void ImplicitShardManager::create_shard_manager(Processor proxy,\n                                                    const char *task_name)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(top_context == NULL);\n      assert(shard_manager == NULL);\n#endif\n      IndividualTask *implicit_top = \n       runtime->create_implicit_top_level(task_id, mapper_id, proxy, task_name);\n      top_context = implicit_top->get_context();\n      // Now we need to make the shard manager\n      const ReplicationID repl_context = runtime->get_unique_replication_id();\n      const size_t total_shards = \n        runtime->total_address_spaces * shards_per_address_space;\n      // We also need a shard \n      shard_manager = new ShardManager(runtime, repl_context, true/*cr*/,\n        true/*top level*/, total_shards, runtime->address_space, implicit_top);\n      implicit_top->set_shard_manager(shard_manager);\n      // This is a dummy shard_mapping for now since we won't actually need\n      // a real one, this just needs to make sure all the checks pass\n      std::vector<Processor> shard_mapping(total_shards, Processor::NO_PROC);\n      shard_manager->set_shard_mapping(shard_mapping);\n      std::vector<AddressSpaceID> address_spaces(total_shards);\n      for (AddressSpaceID space = 0; \n            space < runtime->total_address_spaces; space++)\n      {\n        for (unsigned idx = 0; idx < shards_per_address_space; idx++)\n          address_spaces[space * shards_per_address_space + idx] = space;\n      }\n      shard_manager->set_address_spaces(address_spaces);\n      // We also need to make the callback barrier here, but its easy here\n      // because we know that this has to contain all address spaces\n      shard_manager->create_callback_barrier(runtime->total_address_spaces);\n      if (runtime->legion_spy_enabled)\n        LegionSpy::log_replication(implicit_top->get_unique_id(), repl_context,\n                                   true/*control replication*/);\n      // Distribute the shard manager to all the remove nodes\n      std::vector<ShardTask*> empty_shards;\n      for (AddressSpaceID space = 1; \n            space < runtime->total_address_spaces; space++)\n        shard_manager->distribute_shards(space, empty_shards);\n      // Then send any pending responses\n      if (!remote_spaces.empty())\n      {\n        for (std::vector<std::pair<AddressSpaceID,void*> >::const_iterator it = \n              remote_spaces.begin(); it != remote_spaces.end(); it++)\n        {\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(it->second);\n            rez.serialize(top_context->get_context_uid());\n            rez.serialize(repl_context);\n          }\n          runtime->send_control_replicate_implicit_response(it->first, rez);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ImplicitShardManager::request_shard_manager(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(shard_manager == NULL);\n      assert(!manager_ready.exists());\n#endif\n      manager_ready = Runtime::create_rt_user_event();\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(task_id);\n        rez.serialize(mapper_id);\n        rez.serialize(kind);\n        rez.serialize(shards_per_address_space);\n        rez.serialize(this);\n      }\n      runtime->send_control_replicate_implicit_request(0/*owner*/, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void ImplicitShardManager::process_implicit_request(void *remote,\n                                                        AddressSpaceID space)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(manager_lock);\n      if (shard_manager != NULL)\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(remote);\n          rez.serialize(top_context->get_context_uid());\n          rez.serialize(shard_manager->repl_id);\n        }\n        runtime->send_control_replicate_implicit_response(space, rez);\n      }\n      else\n        remote_spaces.push_back(std::pair<AddressSpaceID,void*>(space, remote));\n    }\n    \n    //--------------------------------------------------------------------------\n    RtUserEvent ImplicitShardManager::process_implicit_response(ShardManager *m,\n                                                                InnerContext *c)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n      assert(top_context == NULL);\n      assert(shard_manager == NULL);\n      assert(manager_ready.exists());\n#endif\n      top_context = c;\n      shard_manager = m;\n      RtUserEvent to_trigger = manager_ready;\n      manager_ready = RtUserEvent::NO_RT_USER_EVENT;\n      return to_trigger;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ImplicitShardManager::handle_remote_request(\n             Deserializer &derez, Runtime *runtime, AddressSpaceID remote_space)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      TaskID task_id;\n      derez.deserialize(task_id);\n      MapperID mapper_id;\n      derez.deserialize(mapper_id);\n      Processor::Kind kind;\n      derez.deserialize(kind);\n      unsigned shards_per_address_space;\n      derez.deserialize(shards_per_address_space);\n      void *remote;\n      derez.deserialize(remote);\n      ImplicitShardManager *manager = runtime->find_implicit_shard_manager(\n          task_id, mapper_id, kind, shards_per_address_space, false/*local*/);\n      manager->process_implicit_request(remote, remote_space);\n      if (manager->remove_reference())\n        delete manager;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ImplicitShardManager::handle_remote_response(\n                                          Deserializer &derez, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      ImplicitShardManager *manager;\n      derez.deserialize(manager);\n      UniqueID context_uid;\n      derez.deserialize(context_uid);\n      ReplicationID repl_id;\n      derez.deserialize(repl_id);\n      ShardManager *shard_manager = runtime->find_shard_manager(repl_id);\n      RtEvent context_ready;\n      InnerContext *context = \n        runtime->find_context(context_uid, false, &context_ready);\n      RtUserEvent to_trigger = \n        manager->process_implicit_response(shard_manager, context);\n      Runtime::trigger_event(to_trigger, context_ready);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Processor Manager \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ProcessorManager::ProcessorManager(Processor proc, Processor::Kind kind,\n                                       Runtime *rt, unsigned def_mappers,\n                                       bool no_steal, bool replay)\n      : runtime(rt), local_proc(proc), proc_kind(kind), \n        stealing_disabled(no_steal), replay_execution(replay), \n        next_local_index(0), task_scheduler_enabled(false), \n        outstanding_task_scheduler(false),\n        total_active_contexts(0), total_active_mappers(0)\n    //--------------------------------------------------------------------------\n    {\n      context_states.resize(LEGION_DEFAULT_CONTEXTS);\n      // Find our set of visible memories\n      Machine::MemoryQuery vis_mems(runtime->machine);\n      vis_mems.has_affinity_to(proc);\n      for (Machine::MemoryQuery::iterator it = vis_mems.begin();\n            it != vis_mems.end(); it++)\n        visible_memories.insert(*it);\n    }\n\n    //--------------------------------------------------------------------------\n    ProcessorManager::ProcessorManager(const ProcessorManager &rhs)\n      : runtime(NULL), local_proc(Processor::NO_PROC),\n        proc_kind(Processor::LOC_PROC), stealing_disabled(false), \n        replay_execution(false)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    ProcessorManager::~ProcessorManager(void)\n    //--------------------------------------------------------------------------\n    {\n      mapper_states.clear();\n    }\n\n    //--------------------------------------------------------------------------\n    ProcessorManager& ProcessorManager::operator=(const ProcessorManager &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::prepare_for_shutdown(void)\n    //--------------------------------------------------------------------------\n    {\n      for (std::map<MapperID,std::pair<MapperManager*,bool> >::iterator it = \n            mappers.begin(); it != mappers.end(); it++)\n      {\n        if (it->second.second)\n          delete it->second.first;\n      }\n      mappers.clear();\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::startup_mappers(void)\n    //--------------------------------------------------------------------------\n    {\n      // No one can be modifying the mapper set here so \n      // there is no to hold the lock\n      std::multimap<Processor,MapperID> stealing_targets;\n      // See what if any stealing we should perform\n      for (std::map<MapperID,std::pair<MapperManager*,bool> >::const_iterator\n            it = mappers.begin(); it != mappers.end(); it++)\n        it->second.first->perform_stealing(stealing_targets);\n      if (!stealing_targets.empty())\n        runtime->send_steal_request(stealing_targets, local_proc);\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::add_mapper(MapperID mid, MapperManager *m, \n                                      bool check, bool own, bool skip_replay)\n    //--------------------------------------------------------------------------\n    {\n      // Don't do this if we are doing replay execution\n      if (!skip_replay && replay_execution)\n        return;\n      log_run.spew(\"Adding mapper %d on processor \" IDFMT \"\", \n                          mid, local_proc.id);\n      if (check && (mid == 0))\n        REPORT_LEGION_ERROR(ERROR_RESERVED_MAPPING_ID, \n                            \"Invalid mapping ID. ID 0 is reserved.\");\n      if (check && !inside_registration_callback)\n          REPORT_LEGION_WARNING(LEGION_WARNING_NON_CALLBACK_REGISTRATION,\n            \"Mapper %s (ID %d) was dynamically registered outside of a \"\n            \"registration callback invocation. In the near future this will \" \n            \"become an error in order to support task subprocesses. Please \"\n            \"use 'perform_registration_callback' to generate a callback \"\n            \"where it will be safe to perform dynamic registrations.\", \n            m->get_mapper_name(), mid)\n      AutoLock m_lock(mapper_lock);\n      std::map<MapperID,std::pair<MapperManager*,bool> >::iterator finder = \n        mappers.find(mid);\n      if (finder != mappers.end())\n      {\n        if (finder->second.second)\n          delete finder->second.first;\n        finder->second = std::pair<MapperManager*,bool>(m, own);\n      }\n      else\n      {\n        mappers[mid] = std::pair<MapperManager*,bool>(m, own); \n        AutoLock q_lock(queue_lock);\n        mapper_states[mid] = MapperState();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::replace_default_mapper(MapperManager *m, bool own)\n    //--------------------------------------------------------------------------\n    {\n      // Don't do this if we are doing replay execution\n      if (replay_execution)\n        return;\n      if (!inside_registration_callback)\n          REPORT_LEGION_WARNING(LEGION_WARNING_NON_CALLBACK_REGISTRATION,\n            \"Replacing default mapper with %s was dynamically performed \"\n            \"outside of a registration callback invocation. In the near \"\n            \"future this will become an error in order to support task \"\n            \"subprocesses. Please use 'perform_registration_callback' to \"\n            \"generate a callback where it will be safe to perform dynamic \" \n            \"registrations.\", m->get_mapper_name())\n      AutoLock m_lock(mapper_lock);\n      std::map<MapperID,std::pair<MapperManager*,bool> >::iterator finder = \n        mappers.find(0);\n      if (finder != mappers.end())\n      {\n        if (finder->second.second)\n          delete finder->second.first;\n        finder->second = std::pair<MapperManager*,bool>(m, own);\n      }\n      else\n      {\n        mappers[0] = std::pair<MapperManager*,bool>(m, own);\n        AutoLock q_lock(queue_lock);\n        mapper_states[0] = MapperState();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    MapperManager* ProcessorManager::find_mapper(MapperID mid) const \n    //--------------------------------------------------------------------------\n    {\n      // Easy case if we are doing replay execution\n      if (replay_execution)\n      {\n        std::map<MapperID,std::pair<MapperManager*,bool> >::const_iterator\n          finder = mappers.find(0);\n#ifdef DEBUG_LEGION\n        assert(finder != mappers.end());\n#endif\n        return finder->second.first;\n      }\n      AutoLock m_lock(mapper_lock, 0/*mode*/, false/*exclusive*/);\n      MapperManager *result = NULL;\n      // We've got the lock, so do the operation\n      std::map<MapperID,std::pair<MapperManager*,bool> >::const_iterator\n        finder = mappers.find(mid);\n      if (finder != mappers.end())\n        result = finder->second.first;\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::perform_scheduling(void)\n    //--------------------------------------------------------------------------\n    {\n      perform_mapping_operations(); \n      // Now re-take the lock and re-check the condition to see \n      // if the next scheduling task should be launched\n      AutoLock q_lock(queue_lock);\n#ifdef DEBUG_LEGION\n      assert(outstanding_task_scheduler);\n#endif\n      // If the task scheduler is enabled launch ourselves again\n      if (task_scheduler_enabled)\n      {\n        SchedulerArgs sched_args(local_proc);\n        runtime->issue_runtime_meta_task(sched_args, LG_LATENCY_WORK_PRIORITY);\n      }\n      else\n        outstanding_task_scheduler = false;\n    } \n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::launch_task_scheduler(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!outstanding_task_scheduler);\n#endif\n      outstanding_task_scheduler = true;\n      SchedulerArgs sched_args(local_proc);\n      runtime->issue_runtime_meta_task(sched_args, LG_LATENCY_WORK_PRIORITY);\n    } \n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::notify_deferred_mapper(MapperID map_id,\n                                                  RtEvent deferred_event)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock q_lock(queue_lock);\n      MapperState &state = mapper_states[map_id];\n      // Check to see if the deferral event matches the one that we have\n      if (state.deferral_event == deferred_event)\n      {\n        // Now we can clear it\n        state.deferral_event = RtEvent::NO_RT_EVENT;\n        // And if we still have tasks, reactivate the mapper\n        if (!state.ready_queue.empty())\n          increment_active_mappers();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ProcessorManager::handle_defer_mapper(const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const DeferMapperSchedulerArgs *dargs = \n        (const DeferMapperSchedulerArgs*)args; \n      dargs->proxy_this->notify_deferred_mapper(dargs->map_id, \n                                                dargs->deferral_event);\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::activate_context(InnerContext *context)\n    //--------------------------------------------------------------------------\n    {\n      ContextID ctx_id = context->get_context_id();\n      AutoLock q_lock(queue_lock); \n      ContextState &state = context_states[ctx_id];\n#ifdef DEBUG_LEGION\n      assert(!state.active);\n#endif\n      state.active = true;\n      if (state.owned_tasks > 0)\n        increment_active_contexts();\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::deactivate_context(InnerContext *context)\n    //--------------------------------------------------------------------------\n    {\n      ContextID ctx_id = context->get_context_id();\n      // We can do this without holding the lock because we know\n      // the size of this vector is fixed\n      AutoLock q_lock(queue_lock); \n      ContextState &state = context_states[ctx_id];\n#ifdef DEBUG_LEGION\n      assert(state.active);\n#endif\n      state.active = false;\n      if (state.owned_tasks > 0)\n        decrement_active_contexts();\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::update_max_context_count(unsigned max_contexts)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock q_lock(queue_lock);\n      context_states.resize(max_contexts);\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::increment_active_contexts(void)\n    //--------------------------------------------------------------------------\n    {\n      // Better be called while holding the queue lock\n      if (!task_scheduler_enabled && (total_active_contexts == 0) &&\n          (total_active_mappers > 0))\n      {\n        task_scheduler_enabled = true;\n        if (!outstanding_task_scheduler)\n          launch_task_scheduler();\n      }\n      total_active_contexts++;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::decrement_active_contexts(void)\n    //--------------------------------------------------------------------------\n    {\n      // Better be called while holding the queue lock\n#ifdef DEBUG_LEGION\n      assert(total_active_contexts > 0);\n#endif\n      total_active_contexts--;\n      if (total_active_contexts == 0)\n        task_scheduler_enabled = false;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::increment_active_mappers(void)\n    //--------------------------------------------------------------------------\n    {\n      // Better be called while holding the queue lock\n      if (!task_scheduler_enabled && (total_active_mappers == 0) &&\n          (total_active_contexts > 0))\n      {\n        task_scheduler_enabled = true;\n        if (!outstanding_task_scheduler)\n          launch_task_scheduler();\n      }\n      total_active_mappers++;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::decrement_active_mappers(void)\n    //--------------------------------------------------------------------------\n    {\n      // Better be called while holding the queue lock\n#ifdef DEBUG_LEGION\n      assert(total_active_mappers > 0);\n#endif\n      total_active_mappers--;\n      if (total_active_mappers == 0)\n        task_scheduler_enabled = false;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::process_steal_request(Processor thief,\n                                           const std::vector<MapperID> &thieves)\n    //--------------------------------------------------------------------------\n    {\n      log_run.spew(\"handling a steal request on processor \" IDFMT \" \"\n                         \"from processor \" IDFMT \"\", local_proc.id,thief.id);\n      // Iterate over the task descriptions, asking the appropriate mapper\n      // whether we can steal the task\n      std::set<TaskOp*> stolen;\n      std::vector<MapperID> successful_thiefs;\n      for (std::vector<MapperID>::const_iterator steal_it = thieves.begin();\n            steal_it != thieves.end(); steal_it++)\n      {\n        const MapperID stealer = *steal_it;\n        // Handle a race condition here where some processors can \n        // issue steal requests to another processor before the mappers \n        // have been initialized on that processor.  There's no \n        // correctness problem for ignoring a steal request so just do that.\n        MapperManager *mapper = find_mapper(stealer);\n        if (mapper == NULL)\n          continue;\n        // Wait until we can exclusive access to the ready queue\n        std::list<TaskOp*> queue_copy;\n        RtEvent queue_copy_ready;\n        // Pull out the current tasks for this mapping operation\n        // Need to iterate until we get access to the queue\n        do\n        {\n          if (queue_copy_ready.exists() && !queue_copy_ready.has_triggered())\n          {\n            queue_copy_ready.wait();\n            queue_copy_ready = RtEvent::NO_RT_EVENT;\n          }\n          AutoLock q_lock(queue_lock);\n          MapperState &map_state = mapper_states[*steal_it];\n          if (!map_state.queue_guard)\n          {\n            // If we don't have a deferral event then grab our\n            // ready queue of tasks so we can try to map them\n            // this will also prevent them from being stolen\n            if (!map_state.ready_queue.empty())\n            {\n              map_state.ready_queue.swap(queue_copy);\n              // Set the queue guard so no one else tries to\n              // read the ready queue while we've checked it out\n              map_state.queue_guard = true;\n            }\n          }\n          else\n          {\n            // Make an event if necessary\n            if (!map_state.queue_waiter.exists())\n              map_state.queue_waiter = Runtime::create_rt_user_event();\n            // Record that we need to wait on it\n            queue_copy_ready = map_state.queue_waiter;\n          }\n        } while (queue_copy_ready.exists());\n        if (queue_copy.empty())\n          continue;\n        Mapper::StealRequestInput input;\n        input.thief_proc = thief;\n        for (std::list<TaskOp*>::const_iterator it = \n              queue_copy.begin(); it != queue_copy.end(); it++)\n        {\n          if ((*it)->is_stealable() && !(*it)->is_origin_mapped())\n            input.stealable_tasks.push_back(*it);\n        }\n        Mapper::StealRequestOutput output;\n        // Ask the mapper what it wants to allow be stolen\n        if (!input.stealable_tasks.empty())\n          mapper->invoke_permit_steal_request(&input, &output);\n        // See which tasks we can succesfully steal\n        std::vector<TaskOp*> local_stolen;\n        if (!output.stolen_tasks.empty())\n        {\n          std::set<const Task*> to_steal(output.stolen_tasks.begin(), \n                                         output.stolen_tasks.end());\n          // Remove any tasks that are going to be stolen\n          for (std::list<TaskOp*>::iterator it = \n                queue_copy.begin(); it != queue_copy.end(); /*nothing*/)\n          {\n            if ((to_steal.find(*it) != to_steal.end()) && \n                (*it)->prepare_steal())\n            {\n              // Mark this as stolen and update the target processor\n              (*it)->mark_stolen();\n              local_stolen.push_back(*it);\n              it = queue_copy.erase(it);\n            }\n            else\n              it++;\n          }\n        }\n        {\n          // Retake the lock, put any tasks still in the ready queue\n          // back into the queue and remove the queue guard\n          AutoLock q_lock(queue_lock);\n          MapperState &map_state = mapper_states[*steal_it];\n#ifdef DEBUG_LEGION\n          assert(map_state.queue_guard);\n#endif\n          std::list<TaskOp*> &rqueue = map_state.ready_queue;\n          if (!queue_copy.empty())\n          {\n            // Put any new items on the back of the queue\n            if (!rqueue.empty())\n            {\n              for (std::list<TaskOp*>::const_iterator it = \n                    rqueue.begin(); it != rqueue.end(); it++)\n                queue_copy.push_back(*it);\n            }\n            rqueue.swap(queue_copy);\n          }\n          else if (rqueue.empty())\n          {\n            if (map_state.deferral_event.exists())\n              map_state.deferral_event = RtEvent::NO_RT_EVENT;\n            else\n              decrement_active_mappers();\n          }\n          if (!local_stolen.empty())\n          {\n            for (std::vector<TaskOp*>::const_iterator it = \n                  local_stolen.begin(); it != local_stolen.end(); it++)\n            {\n              // Wait until we are no longer holding the lock\n              // to mark that this is no longer an outstanding task\n              ContextID ctx_id = (*it)->get_context()->get_context_id();\n              ContextState &state = context_states[ctx_id];\n#ifdef DEBUG_LEGION\n              assert(state.owned_tasks > 0);\n#endif\n              state.owned_tasks--;\n              if (state.active && (state.owned_tasks == 0))\n                decrement_active_contexts();\n            }\n          }\n          // Remove the queue guard\n          map_state.queue_guard = false;\n          if (map_state.queue_waiter.exists())\n          {\n            Runtime::trigger_event(map_state.queue_waiter);\n            map_state.queue_waiter = RtUserEvent::NO_RT_USER_EVENT;\n          }\n        }\n        if (!local_stolen.empty())\n        {\n          successful_thiefs.push_back(stealer);\n          for (std::vector<TaskOp*>::const_iterator it = \n                local_stolen.begin(); it != local_stolen.end(); it++)\n          {\n            (*it)->deactivate_outstanding_task();\n            stolen.insert(*it);\n          }\n        }\n        else\n          mapper->process_failed_steal(thief);\n      }\n      if (!stolen.empty())\n      {\n#ifdef DEBUG_LEGION\n        for (std::set<TaskOp*>::const_iterator it = stolen.begin();\n              it != stolen.end(); it++)\n        {\n          log_task.debug(\"task %s (ID %lld) stolen from processor \" IDFMT\n                         \" by processor \" IDFMT \"\", (*it)->get_task_name(), \n                         (*it)->get_unique_id(), local_proc.id, thief.id);\n        }\n#endif\n        runtime->send_tasks(thief, stolen);\n        // Also have to send advertisements to the mappers that \n        // successfully stole so they know that they can try again\n        std::set<Processor> thief_set;\n        thief_set.insert(thief);\n        for (std::vector<MapperID>::const_iterator it = \n              successful_thiefs.begin(); it != successful_thiefs.end(); it++)\n          runtime->send_advertisements(thief_set, *it, local_proc);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::process_advertisement(Processor advertiser,\n                                                 MapperID mid)\n    //--------------------------------------------------------------------------\n    {\n      MapperManager *mapper = find_mapper(mid);\n      mapper->process_advertisement(advertiser);\n      // See if this mapper would like to try stealing again\n      std::multimap<Processor,MapperID> stealing_targets;\n      mapper->perform_stealing(stealing_targets);\n      if (!stealing_targets.empty())\n        runtime->send_steal_request(stealing_targets, local_proc);\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::add_to_ready_queue(TaskOp *task)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(task != NULL);\n#endif\n      // have to do this when we are not holding the lock\n      task->activate_outstanding_task();\n      // We can do this without holding the lock because the\n      // vector is of a fixed size\n      ContextID ctx_id = task->get_context()->get_context_id();\n      AutoLock q_lock(queue_lock);\n#ifdef DEBUG_LEGION\n      assert(mapper_states.find(task->map_id) != mapper_states.end());\n#endif\n      // Update the state for the context\n      ContextState &state = context_states[ctx_id];\n      if (state.active && (state.owned_tasks == 0))\n        increment_active_contexts();\n      state.owned_tasks++;\n      // Also update the queue for the mapper\n      MapperState &map_state = mapper_states[task->map_id];\n      if (map_state.ready_queue.empty() || map_state.deferral_event.exists())\n      {\n        // Clear our deferral event since we are changing state\n        map_state.deferral_event = RtEvent::NO_RT_EVENT;\n        increment_active_mappers();\n      }\n      map_state.ready_queue.push_back(task);\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::add_to_local_ready_queue(Operation *op, \n                                           LgPriority priority, RtEvent wait_on) \n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(op != NULL);\n#endif\n      Operation::TriggerOpArgs args(op);\n      runtime->issue_runtime_meta_task(args, priority, wait_on); \n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::perform_mapping_operations(void)\n    //--------------------------------------------------------------------------\n    {\n      std::multimap<Processor,MapperID> stealing_targets;\n      std::vector<MapperID> mappers_with_stealable_work;\n      std::vector<std::pair<MapperID,MapperManager*> > current_mappers;\n      // Take a snapshot of our current mappers\n      {\n        AutoLock m_lock(mapper_lock,1,false/*exclusive*/);\n        // Fast path for no deferred mappers\n        current_mappers.resize(mappers.size());\n        unsigned idx = 0;\n        for (std::map<MapperID,std::pair<MapperManager*,bool> >::\n              const_iterator it = mappers.begin(); it != \n              mappers.end(); it++, idx++)\n          current_mappers[idx] = \n            std::pair<MapperID,MapperManager*>(it->first, it->second.first);\n      }\n      for (std::vector<std::pair<MapperID,MapperManager*> >::const_iterator\n            it = current_mappers.begin(); it != current_mappers.end(); it++)\n      {\n        const MapperID map_id = it->first;\n        MapperManager *const mapper = it->second;\n        std::list<TaskOp*> queue_copy;\n        RtEvent queue_copy_ready;\n        // Pull out the current tasks for this mapping operation\n        // Need to iterate until we get access to the queue\n        do\n        {\n          if (queue_copy_ready.exists() && !queue_copy_ready.has_triggered())\n          {\n            queue_copy_ready.wait();\n            queue_copy_ready = RtEvent::NO_RT_EVENT;\n          }\n          AutoLock q_lock(queue_lock);\n          MapperState &map_state = mapper_states[map_id];\n          if (!map_state.queue_guard)\n          {\n            // If we don't have a deferral event then grab our\n            // ready queue of tasks so we can try to map them\n            // this will also prevent them from being stolen\n            if (!map_state.deferral_event.exists() &&\n                !map_state.ready_queue.empty())\n            {\n              map_state.ready_queue.swap(queue_copy);\n              // Set the queue guard so no one else tries to\n              // read the ready queue while we've checked it out\n              map_state.queue_guard = true;\n            }\n          }\n          else\n          {\n            // Make an event if necessary\n            if (!map_state.queue_waiter.exists())\n              map_state.queue_waiter = Runtime::create_rt_user_event();\n            // Record that we need to wait on it\n            queue_copy_ready = map_state.queue_waiter;\n          }\n        } while (queue_copy_ready.exists());\n        // Do this before anything else in case we don't have any tasks\n        if (!stealing_disabled)\n          mapper->perform_stealing(stealing_targets);\n        // Nothing to do if there are no tasks on the queue\n        if (queue_copy.empty())\n          continue;\n        // Ask the mapper which tasks it would like to schedule\n        Mapper::SelectMappingInput input;\n        Mapper::SelectMappingOutput output;\n        for (std::list<TaskOp*>::const_iterator it = \n              queue_copy.begin(); it != queue_copy.end(); it++)\n          input.ready_tasks.push_back(*it);\n        mapper->invoke_select_tasks_to_map(&input, &output);\n        // If we had no entry then we better have gotten a mapper event\n        std::vector<TaskOp*> to_trigger;\n        if (output.map_tasks.empty() && output.relocate_tasks.empty())\n        {\n          const RtEvent wait_on = output.deferral_event.impl;\n          if (wait_on.exists())\n          {\n            // Put this on the list of the deferred mappers\n            AutoLock q_lock(queue_lock);\n            MapperState &map_state = mapper_states[map_id];\n            // We have to check to see if any new tasks were added to \n            // the ready queue while we were doing our mapper call, if \n            // they were then we need to invoke select_tasks_to_map again\n            if (map_state.ready_queue.empty())\n            {\n#ifdef DEBUG_LEGION\n              assert(!map_state.deferral_event.exists());\n              assert(map_state.queue_guard);\n#endif\n              map_state.deferral_event = wait_on;\n              // Decrement the number of active mappers\n              decrement_active_mappers();\n              // Put our tasks back on the queue\n              map_state.ready_queue.swap(queue_copy);\n              // Clear the queue guard\n              map_state.queue_guard = false;\n              if (map_state.queue_waiter.exists())\n              {\n                Runtime::trigger_event(map_state.queue_waiter);\n                map_state.queue_waiter = RtUserEvent::NO_RT_USER_EVENT;\n              }\n              // Launch a task to remove the deferred mapper \n              // event when it triggers\n              DeferMapperSchedulerArgs args(this, map_id, wait_on);\n              runtime->issue_runtime_meta_task(args, \n                  LG_LATENCY_DEFERRED_PRIORITY, wait_on);\n              // We can continue because there is nothing \n              // left to do for this mapper\n              continue;\n            }\n            // Otherwise we fall through to put our tasks back on the queue \n            // which will lead to select_tasks_to_map being called again\n          }\n          else // Very bad, error message\n            REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,\n                          \"Mapper %s failed to specify an output MapperEvent \"\n                          \"when returning from a call to 'select_tasks_to_map' \"\n                          \"that performed no other actions. Specifying a \"\n                          \"MapperEvent in such situation is necessary to avoid \"\n                          \"livelock conditions. Please return a \"\n                          \"'deferral_event' in the 'output' struct.\",\n                          mapper->get_mapper_name())\n        }\n        else\n        {\n          // Figure out which tasks are to be triggered\n          std::set<const Task*> selected;\n          if (!output.map_tasks.empty())\n            selected.insert(output.map_tasks.begin(), output.map_tasks.end());\n          if (!output.relocate_tasks.empty())\n          {\n            for (std::map<const Task*,Processor>::const_iterator it = \n                  output.relocate_tasks.begin(); it != \n                  output.relocate_tasks.end(); it++)\n            {\n              if (it->second.kind() == Processor::UTIL_PROC)\n                REPORT_LEGION_ERROR(ERROR_INVALID_MAPPER_OUTPUT,\n                    \"Invalid mapper output. Mapper %s requested that task %s \"\n                    \"(UID %lld) be relocated to a utility processor in \"\n                    \"'select_tasks_to_map.' Only application processor kinds \"\n                    \"are permitted to be the target processor for tasks.\",\n                    mapper->get_mapper_name(), it->first->get_task_name(),\n                    it->first->get_unique_id())\n              selected.insert(it->first);\n            }\n          }\n          // Remove any tasks that are going to be triggered\n          for (std::list<TaskOp*>::iterator it = \n                queue_copy.begin(); it != queue_copy.end(); /*nothing*/)\n          {\n            if (selected.find(*it) != selected.end())\n            {\n              to_trigger.push_back(*it);\n              it = queue_copy.erase(it);\n            }\n            else\n              it++;\n          }\n        }\n        {\n          // Retake the lock, put any tasks that the mapper didn't select\n          // back on the queue and update the context states for any\n          // that were selected \n          AutoLock q_lock(queue_lock);\n          MapperState &map_state = mapper_states[map_id];\n#ifdef DEBUG_LEGION\n          assert(map_state.queue_guard);\n#endif\n          std::list<TaskOp*> &rqueue = map_state.ready_queue;\n          if (!queue_copy.empty())\n          {\n            // Put any new items on the back of the queue\n            if (!rqueue.empty())\n            {\n              for (std::list<TaskOp*>::const_iterator it = \n                    rqueue.begin(); it != rqueue.end(); it++)\n                queue_copy.push_back(*it);\n            }\n            rqueue.swap(queue_copy);\n          }\n          else if (rqueue.empty())\n          {\n            if (map_state.deferral_event.exists())\n              map_state.deferral_event = RtEvent::NO_RT_EVENT;\n            else\n              decrement_active_mappers();\n          }\n          if (!to_trigger.empty())\n          {\n            for (std::vector<TaskOp*>::const_iterator it = \n                  to_trigger.begin(); it != to_trigger.end(); it++)\n            {\n              ContextID ctx_id = (*it)->get_context()->get_context_id(); \n              ContextState &state = context_states[ctx_id];\n#ifdef DEBUG_LEGION\n              assert(state.owned_tasks > 0);\n#endif\n              state.owned_tasks--;\n              if (state.active && (state.owned_tasks == 0))\n                decrement_active_contexts();\n            }\n          }\n          if (!stealing_disabled && !rqueue.empty())\n          {\n            for (std::list<TaskOp*>::const_iterator it =\n                  rqueue.begin(); it != rqueue.end(); it++)\n            {\n              if ((*it)->is_stealable())\n              {\n                mappers_with_stealable_work.push_back(map_id);\n                break;\n              }\n            }\n          }\n          // Remove the queue guard\n          map_state.queue_guard = false;\n          if (map_state.queue_waiter.exists())\n          {\n            Runtime::trigger_event(map_state.queue_waiter);\n            map_state.queue_waiter = RtUserEvent::NO_RT_USER_EVENT;\n          }\n        }\n        // Now we can trigger our tasks that the mapper selected\n        for (std::vector<TaskOp*>::const_iterator it = \n              to_trigger.begin(); it != to_trigger.end(); it++)\n        {\n          // Update the target processor for this task if necessary\n          std::map<const Task*,Processor>::const_iterator finder = \n            output.relocate_tasks.find(*it);\n          const bool send_remotely = (finder != output.relocate_tasks.end());\n          if (send_remotely)\n            (*it)->set_target_proc(finder->second);\n          // Mark that this task is no longer outstanding\n          (*it)->deactivate_outstanding_task();\n          TaskOp::TriggerTaskArgs trigger_args(*it);\n          runtime->issue_runtime_meta_task(trigger_args,\n                                           LG_THROUGHPUT_WORK_PRIORITY);\n        }\n      }\n\n      // Advertise any work that we have\n      if (!stealing_disabled && !mappers_with_stealable_work.empty())\n      {\n        for (std::vector<MapperID>::const_iterator it = \n              mappers_with_stealable_work.begin(); it !=\n              mappers_with_stealable_work.end(); it++)\n          issue_advertisements(*it);\n      }\n\n      // Finally issue any steal requeusts\n      if (!stealing_disabled && !stealing_targets.empty())\n        runtime->send_steal_request(stealing_targets, local_proc);\n    }\n\n    //--------------------------------------------------------------------------\n    void ProcessorManager::issue_advertisements(MapperID map_id)\n    //--------------------------------------------------------------------------\n    {\n      // Create a clone of the processors we want to advertise so that\n      // we don't call into the high level runtime holding a lock\n      std::set<Processor> failed_waiters;\n      MapperManager *mapper = find_mapper(map_id);\n      mapper->perform_advertisements(failed_waiters);\n      if (!failed_waiters.empty())\n        runtime->send_advertisements(failed_waiters, map_id, local_proc);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Memory Manager \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    MemoryManager::MemoryManager(Memory m, Runtime *rt)\n      : memory(m), owner_space(m.address_space()), \n        is_owner(m.address_space() == rt->address_space),\n        capacity(m.capacity()), remaining_capacity(capacity), runtime(rt),\n        eager_pool_instance(PhysicalInstance::NO_INST), eager_pool(0),\n        eager_allocator(NULL), eager_remaining_capacity(0),next_allocation_id(0)\n    //--------------------------------------------------------------------------\n    {\n#ifdef LEGION_USE_CUDA\n      if (memory.kind() == Memory::GPU_FB_MEM)\n      {\n        Machine::ProcessorQuery finder(runtime->machine);\n        finder.best_affinity_to(memory);\n        finder.only_kind(Processor::TOC_PROC);\n        assert(finder.count() > 0);\n        local_gpu = finder.first();\n      }\n      else if (memory.kind() == Memory::Z_COPY_MEM)\n      {\n        Machine::ProcessorQuery finder(runtime->machine);\n        finder.has_affinity_to(memory);\n        finder.only_kind(Processor::TOC_PROC);\n        assert(finder.count() > 0);\n        local_gpu = finder.first();\n      }\n#endif\n      // We do not make eager pool instances if we are not the owner or if the\n      // memory has capacity zero (e.g. disk memory) where the creation of any \n      // instances that are not external instances are disallowed\n      if (!is_owner || (capacity == 0)) \n        return;\n\n      // Allocate eager pool\n      const coord_t eager_pool_size = \n        capacity * runtime->eager_alloc_percentage / 100;\n      log_eager.info(\"create an eager pool of size %lld on memory \" IDFMT,\n                     eager_pool_size, memory.id);\n      const DomainT<1,coord_t> bounds(Rect<1>(0,Point<1>(eager_pool_size - 1)));\n      const std::vector<size_t> field_sizes(1,sizeof(char));\n      Realm::InstanceLayoutConstraints constraints(field_sizes, 0/*blocking*/);\n      int dim_order[] = {0};\n      Realm::InstanceLayoutGeneric *layout =\n        Realm::InstanceLayoutGeneric::choose_instance_layout(bounds,\n                                                             constraints,\n                                                             dim_order);\n      Realm::ProfilingRequestSet no_requests;\n      Realm::RegionInstance::create_instance(eager_pool_instance,\n                                             memory,\n                                             layout,\n                                             no_requests);\n\n      if (eager_pool_size > 0)\n      {\n        Realm::AffineAccessor<char,1,coord_t> accessor(eager_pool_instance,\n                                                       0/*field id*/);\n        eager_pool = reinterpret_cast<uintptr_t>(accessor.ptr(0));\n\n        eager_allocator = new EagerAllocator();\n        eager_allocator->add_range(0, eager_pool_size - 1);\n        eager_remaining_capacity = eager_pool_size;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    MemoryManager::MemoryManager(const MemoryManager &rhs)\n      : memory(Memory::NO_MEMORY), owner_space(0), \n        is_owner(false), capacity(0), runtime(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);   \n    }\n\n    //--------------------------------------------------------------------------\n    MemoryManager::~MemoryManager(void)\n    //--------------------------------------------------------------------------\n    {\n      if (eager_pool_instance.exists())\n        eager_pool_instance.destroy();\n      delete eager_allocator;\n    }\n\n    //--------------------------------------------------------------------------\n    MemoryManager& MemoryManager::operator=(const MemoryManager &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::find_shutdown_preconditions(\n                                               std::set<ApEvent> &preconditions)\n    //--------------------------------------------------------------------------\n    {\n      std::vector<PhysicalManager*> to_check;\n      {\n        AutoLock m_lock(manager_lock,1,false/*exclusive*/);\n        for (std::map<RegionTreeID,TreeInstances>::const_iterator cit = \n              current_instances.begin(); cit != current_instances.end(); cit++)\n          for (TreeInstances::const_iterator it = \n                cit->second.begin(); it != cit->second.end(); it++)\n          {\n            // We only need to check this on the owner node instances and \n            // in fact it's only safe for us to do it on the owner node\n            // instance because we only are guaranteed to have references\n            // to the owner node objects\n            if (!it->first->is_owner())\n              continue;\n            it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n            to_check.push_back(it->first);\n          }\n      }\n      for (std::vector<PhysicalManager*>::const_iterator it = \n            to_check.begin(); it != to_check.end(); it++)\n      {\n        (*it)->find_shutdown_preconditions(preconditions);\n        if ((*it)->remove_base_resource_ref(MEMORY_MANAGER_REF))\n          delete (*it);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::prepare_for_shutdown(void)\n    //--------------------------------------------------------------------------\n    {\n      // Only need to do things if we are the owner memory\n      if (!is_owner)\n        return;\n      std::map<PhysicalManager*,RtEvent> to_delete;\n      {\n        AutoLock m_lock(manager_lock);\n        std::vector<PhysicalManager*> to_remove;\n        for (std::map<RegionTreeID,TreeInstances>::iterator cit = \n              current_instances.begin(); cit != current_instances.end(); cit++)\n          for (TreeInstances::iterator it = \n                cit->second.begin(); it != cit->second.end(); it++)\n          {\n            if (it->second.current_state == PENDING_COLLECTED_STATE)\n              continue;\n#ifdef DEBUG_LEGION\n            assert(it->second.current_state != PENDING_COLLECTED_STATE);\n            assert(it->second.current_state != PENDING_ACQUIRE_STATE);\n#endif\n            if (it->second.current_state != COLLECTABLE_STATE)\n            {\n              RtUserEvent deferred_collect = Runtime::create_rt_user_event();\n              it->second.current_state = PENDING_COLLECTED_STATE;\n              it->second.deferred_collect = deferred_collect;\n              to_delete[it->first] = deferred_collect;\n              it->first->add_base_resource_ref(MEMORY_MANAGER_REF);   \n#ifdef LEGION_MALLOC_INSTANCES\n              pending_collectables[deferred_collect] = 0; \n#endif\n            }\n            else // reference flows out since we're deleting this\n            {\n              to_delete[it->first] = RtEvent::NO_RT_EVENT;\n              to_remove.push_back(it->first);\n            }\n          }\n        if (!to_remove.empty())\n        {\n          for (std::vector<PhysicalManager*>::const_iterator it = \n                to_remove.begin(); it != to_remove.end(); it++)\n          {\n            std::map<RegionTreeID,TreeInstances>::iterator finder = \n              current_instances.find((*it)->tree_id);\n#ifdef DEBUG_LEGION\n            assert(finder != current_instances.end());\n#endif\n            finder->second.erase(*it);\n            if (finder->second.empty())\n              current_instances.erase(finder);\n          }\n        }\n      }\n      for (std::map<PhysicalManager*,RtEvent>::const_iterator it = \n            to_delete.begin(); it != to_delete.end(); it++)\n      {\n        it->first->perform_deletion(it->second);\n        // Remove our base resource reference\n        if (it->first->remove_base_resource_ref(MEMORY_MANAGER_REF))\n          delete (it->first);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::finalize(void)\n    //--------------------------------------------------------------------------\n    {\n      if (!is_owner)\n        return;\n      // No need for the lock, no one should be doing anything at this point\n      for (std::map<RegionTreeID,TreeInstances>::const_iterator cit = \n            current_instances.begin(); cit != current_instances.end(); cit++)\n        for (std::map<PhysicalManager*,InstanceInfo>::const_iterator it = \n              cit->second.begin(); it != cit->second.end(); it++)\n        {\n          if (it->second.current_state == PENDING_COLLECTED_STATE)\n            Runtime::trigger_event(it->second.deferred_collect);\n          else\n            it->first->force_deletion();\n        }\n#ifdef LEGION_MALLOC_INSTANCES\n      for (std::map<RtEvent,uintptr_t>::const_iterator it = \n            pending_collectables.begin(); it != \n            pending_collectables.end(); it++)\n        if (it->second > 0)\n          free_legion_instance(it->first, it->second);\n      pending_collectables.clear();\n#endif\n    }\n    \n    //--------------------------------------------------------------------------\n    void MemoryManager::register_remote_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      const size_t inst_size = manager->get_instance_size();\n      AutoLock m_lock(manager_lock);\n      TreeInstances &insts = current_instances[manager->tree_id];\n#ifdef DEBUG_LEGION\n      assert(insts.find(manager) == insts.end());\n#endif\n      // Make it valid to start since we know when we were created\n      // that we were made valid to begin with\n      InstanceInfo &info = insts[manager];\n      info.instance_size = inst_size;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::unregister_remote_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(manager_lock);\n      std::map<RegionTreeID,TreeInstances>::iterator finder = \n        current_instances.find(manager->tree_id);\n #ifdef DEBUG_LEGION\n      assert(finder != current_instances.end());\n      assert(finder->second.find(manager) != finder->second.end());\n#endif     \n      finder->second.erase(manager);\n      if (finder->second.empty())\n        current_instances.erase(finder);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::activate_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n      assert(current_instances.find(manager->tree_id) != \n              current_instances.end());\n#endif\n      TreeInstances::iterator finder = \n        current_instances[manager->tree_id].find(manager);\n#ifdef DEBUG_LEGION\n      assert(finder != current_instances[manager->tree_id].end());\n      // This can be a valid state too if we just made the instance\n      // and we marked it valid to prevent GC from claiming it before\n      // it can be used for the first time\n      assert((finder->second.current_state == COLLECTABLE_STATE) ||\n             (finder->second.current_state == PENDING_ACQUIRE_STATE) ||\n             (finder->second.current_state == VALID_STATE));\n#endif\n      if (finder->second.current_state == COLLECTABLE_STATE)\n        finder->second.current_state = ACTIVE_STATE;\n      // Otherwise stay in our current state\n#ifdef DEBUG_LEGION\n#ifndef NDEBUG\n      else if (finder->second.current_state != VALID_STATE)\n        assert(finder->second.pending_acquires > 0);\n#endif\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::deactivate_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      bool remove_reference = false;\n#ifdef LEGION_MALLOC_INSTANCES\n      std::pair<RtEvent,uintptr_t> to_free(RtEvent::NO_RT_EVENT, 0);\n#endif\n      {\n        AutoLock m_lock(manager_lock);\n        std::map<RegionTreeID,TreeInstances>::iterator tree_finder = \n          current_instances.find(manager->tree_id);\n#ifdef DEBUG_LEGION\n        assert(tree_finder != current_instances.end());\n#endif\n        TreeInstances::iterator finder = tree_finder->second.find(manager);\n#ifdef DEBUG_LEGION\n        assert(finder != tree_finder->second.end());\n        assert((finder->second.current_state == ACTIVE_STATE) ||\n               (finder->second.current_state == PENDING_COLLECTED_STATE) ||\n               (finder->second.current_state == PENDING_ACQUIRE_STATE));\n#endif\n        InstanceInfo &info = finder->second;\n        // See if we deleted this yet\n        if (finder->second.current_state == PENDING_COLLECTED_STATE)\n        {\n          // already deferred collected this, so we can trigger \n          // the deletion now this should only happen on the owner node\n#ifdef DEBUG_LEGION\n          assert(is_owner);\n          assert(info.deferred_collect.exists());\n#endif\n          Runtime::trigger_event(info.deferred_collect);\n#ifdef LEGION_MALLOC_INSTANCES\n          std::map<RtEvent,uintptr_t>::iterator free_finder = \n            pending_collectables.find(info.deferred_collect);\n          if (free_finder != pending_collectables.end())\n          {\n            to_free = *free_finder;\n            pending_collectables.erase(free_finder); \n          }\n#endif\n          // Now we can delete our entry because it has been deleted\n          tree_finder->second.erase(finder);\n          if (tree_finder->second.empty())\n            current_instances.erase(tree_finder);\n          remove_reference = true;\n        }\n        else if (finder->second.current_state == PENDING_ACQUIRE_STATE)\n        {\n          // We'll stay in this state until our pending acquires are done\n#ifdef DEBUG_LEGION\n          assert(finder->second.pending_acquires > 0);\n#endif\n        }\n        else // didn't collect it yet\n          info.current_state = COLLECTABLE_STATE;\n      }\n      if (remove_reference)\n      {\n        if (manager->remove_base_resource_ref(MEMORY_MANAGER_REF))\n          delete manager;\n      }\n#ifdef LEGION_MALLOC_INSTANCES\n      if (to_free.second > 0)\n        free_legion_instance(to_free.first, to_free.second);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::validate_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(manager_lock);\n      TreeInstances::iterator finder = \n        current_instances[manager->tree_id].find(manager);\n#ifdef DEBUG_LEGION\n      assert(finder != current_instances[manager->tree_id].end());\n      assert((finder->second.current_state == ACTIVE_STATE) ||\n             (finder->second.current_state == PENDING_ACQUIRE_STATE) ||\n             (finder->second.current_state == VALID_STATE));\n#endif\n      if (finder->second.current_state == ACTIVE_STATE)\n        finder->second.current_state = VALID_STATE;\n      // Otherwise we stay in the state we are currently in\n#ifdef DEBUG_LEGION\n#ifndef NDEBUG\n      else if (finder->second.current_state == PENDING_ACQUIRE_STATE)\n        assert(finder->second.pending_acquires > 0);\n#endif\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::invalidate_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(manager_lock);\n      TreeInstances::iterator finder = \n        current_instances[manager->tree_id].find(manager);\n#ifdef DEBUG_LEGION\n      assert(finder != current_instances[manager->tree_id].end());\n      assert((finder->second.current_state == VALID_STATE) ||\n             (finder->second.current_state == PENDING_ACQUIRE_STATE) ||\n             (finder->second.current_state == PENDING_COLLECTED_STATE));\n#endif\n      if (finder->second.current_state == VALID_STATE)\n        finder->second.current_state = ACTIVE_STATE;\n      // Otherwise we stay in whatever state we should be in\n#ifdef DEBUG_LEGION\n#ifndef NDEBUG\n      else if (finder->second.current_state == PENDING_ACQUIRE_STATE)\n        assert(finder->second.pending_acquires > 0);\n#endif\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::attempt_acquire(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      AutoLock m_lock(manager_lock);\n      std::map<RegionTreeID,TreeInstances>::iterator tree_finder = \n        current_instances.find(manager->tree_id);\n      if (tree_finder == current_instances.end())\n        return false;\n      TreeInstances::iterator finder = tree_finder->second.find(manager);\n      // If we can't even find it then it was deleted\n      if (finder == tree_finder->second.end())\n        return false;\n      // If it's going to be deleted that is not going to work\n      if (finder->second.current_state == PENDING_COLLECTED_STATE)\n        return false;\n#ifdef DEBUG_LEGION\n      if (finder->second.current_state != PENDING_ACQUIRE_STATE)\n        assert(finder->second.pending_acquires == 0);\n#endif\n      finder->second.current_state = PENDING_ACQUIRE_STATE;\n      finder->second.pending_acquires++;\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::complete_acquire(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n      assert(current_instances.find(manager->tree_id) != \n              current_instances.end());\n#endif\n      std::map<PhysicalManager*,InstanceInfo>::iterator finder = \n        current_instances[manager->tree_id].find(manager);\n#ifdef DEBUG_LEGION\n      assert(finder != current_instances[manager->tree_id].end());\n      assert(finder->second.current_state == PENDING_ACQUIRE_STATE);\n      assert(finder->second.pending_acquires > 0);\n#endif\n      finder->second.pending_acquires--;\n      // If all our pending acquires are done then we are in the valid state\n      if (finder->second.pending_acquires == 0)\n        finder->second.current_state = VALID_STATE;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::create_physical_instance(\n                                const LayoutConstraintSet &constraints,\n                                const std::vector<LogicalRegion> &regions,\n                                MappingInstance &result, MapperID mapper_id, \n                                Processor processor, bool acquire, \n                                GCPriority priority, bool tight_bounds,\n                                LayoutConstraintKind *unsat_kind,\n                                unsigned *unsat_index, size_t *footprint, \n                                CollectiveManager *target, DomainPoint *point,\n                                UniqueID creator_id, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      volatile bool success = false;\n      if (!is_owner)\n      {\n        // Not the owner, send a meessage to the owner to request the creation\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(CREATE_INSTANCE_CONSTRAINTS);\n          rez.serialize(ready_event);\n          rez.serialize<size_t>(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          if (target != NULL)\n          {\n            rez.serialize(target->did);\n            rez.serialize(*point);\n          }\n          else\n            rez.serialize<DistributedID>(0);\n          constraints.serialize(rez);\n          rez.serialize(mapper_id);\n          rez.serialize(processor);\n          rez.serialize(priority);\n          rez.serialize<bool>(tight_bounds);\n          rez.serialize(unsat_kind);\n          rez.serialize(unsat_index);\n          rez.serialize(footprint);\n          rez.serialize(creator_id);\n          rez.serialize(&success);\n          rez.serialize(&result);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled in\n      }\n      else\n      {\n        // Create the builder and initialize it before getting\n        // the allocation privilege to avoid deadlock scenario\n        InstanceBuilder builder(regions, constraints, runtime, this,creator_id);\n        builder.initialize(runtime->forest);\n        // Acquire allocation privilege before doing anything\n        const RtEvent wait_on = acquire_allocation_privilege();\n        if (wait_on.exists())\n          wait_on.wait();\n        // Try to make the result\n        PhysicalManager *manager = allocate_physical_instance(builder, \n            footprint, unsat_kind, unsat_index, target, point);\n        if (manager != NULL)\n        {\n          if (runtime->legion_spy_enabled)\n            manager->log_instance_creation(creator_id, processor, regions);\n          record_created_instance(manager, acquire, mapper_id, processor,\n                                  priority, remote);\n          result = MappingInstance(manager);\n          success = true;\n        }\n        // Release our allocation privilege after doing the record\n        release_allocation_privilege();\n      }\n      return success;\n    }\n    \n    //--------------------------------------------------------------------------\n    bool MemoryManager::create_physical_instance(LayoutConstraints *constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result,MapperID mapper_id,\n                                     Processor processor, bool acquire, \n                                     GCPriority priority, bool tight_bounds,\n                                     LayoutConstraintKind *unsat_kind,\n                                     unsigned *unsat_index, size_t *footprint, \n                                     CollectiveManager *target, DomainPoint *p,\n                                     UniqueID creator_id, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      volatile bool success = false;\n      if (!is_owner)\n      {\n        // Not the owner, send a meessage to the owner to request the creation\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(CREATE_INSTANCE_LAYOUT);\n          rez.serialize(ready_event);\n          rez.serialize<size_t>(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          if (target != NULL)\n          {\n            rez.serialize(target->did);\n            rez.serialize(*p);\n          }\n          else\n            rez.serialize<DistributedID>(0);\n          rez.serialize(constraints->layout_id);\n          rez.serialize(mapper_id);\n          rez.serialize(processor);\n          rez.serialize(priority);\n          rez.serialize<bool>(tight_bounds);\n          rez.serialize(unsat_kind);\n          rez.serialize(unsat_index);\n          rez.serialize(footprint);\n          rez.serialize(creator_id);\n          rez.serialize(&success);\n          rez.serialize(&result);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled in\n      }\n      else\n      {\n        // Create the builder and initialize it before getting\n        // the allocation privilege to avoid deadlock scenario\n        InstanceBuilder builder(regions,*constraints, runtime, this,creator_id);\n        builder.initialize(runtime->forest);\n        // Acquire allocation privilege before doing anything\n        const RtEvent wait_on = acquire_allocation_privilege();\n        if (wait_on.exists())\n          wait_on.wait();\n        // Try to make the instance\n        PhysicalManager *manager = allocate_physical_instance(builder, \n            footprint, unsat_kind, unsat_index, target, p);\n        if (manager != NULL)\n        {\n          if (runtime->legion_spy_enabled)\n            manager->log_instance_creation(creator_id, processor, regions);\n          record_created_instance(manager, acquire, mapper_id, processor,\n                                  priority, remote);\n          result = MappingInstance(manager);\n          success = true;\n        }\n        // Release our allocation privilege after doing the record\n        release_allocation_privilege();\n      }\n      return success;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_or_create_physical_instance(\n                                  const LayoutConstraintSet &constraints,\n                                  const std::vector<LogicalRegion> &regions,\n                                  MappingInstance &result, bool &created, \n                                  MapperID mapper_id, Processor processor,\n                                  bool acquire, GCPriority priority,\n                                  bool tight_region_bounds, \n                                  LayoutConstraintKind *unsat_kind,\n                                  unsigned *unsat_index, size_t *footprint, \n                                  UniqueID creator_id, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      volatile bool success = false;\n      // Set created to default to false\n      created = false;\n      if (!is_owner)\n      {\n        // See if we can find a locally valid instance first\n        success = find_valid_instance(constraints, regions, result, \n                                      acquire, tight_region_bounds, remote);\n        if (success)\n          return true;\n        // Not the owner, send a message to the owner to request creation\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(FIND_OR_CREATE_CONSTRAINTS);\n          rez.serialize(ready_event);\n          rez.serialize<size_t>(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          constraints.serialize(rez);\n          rez.serialize(mapper_id);\n          rez.serialize(processor);\n          rez.serialize(priority);\n          rez.serialize<bool>(tight_region_bounds);\n          rez.serialize(unsat_kind);\n          rez.serialize(unsat_index);\n          rez.serialize(footprint);\n          rez.serialize(creator_id);\n          rez.serialize(&success);\n          rez.serialize(&result);\n          rez.serialize(&created);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled in\n      }\n      else\n      {\n        // Create the builder and initialize it before getting\n        // the allocation privilege to avoid deadlock scenario\n        InstanceBuilder builder(regions, constraints, runtime, this,creator_id);\n        builder.initialize(runtime->forest);\n        // First get our allocation privileges so we're the only\n        // one trying to do any allocations\n        const RtEvent wait_on = acquire_allocation_privilege();\n        if (wait_on.exists())\n          wait_on.wait();\n        // Since this is find or acquire, first see if we can find\n        // an instance that has already been makde that satisfies \n        // our layout constraints\n        success = find_satisfying_instance(constraints, regions, \n                   result, acquire, tight_region_bounds, remote);\n        if (!success)\n        {\n          // If we couldn't find it, we have to make it\n          PhysicalManager *manager = allocate_physical_instance(builder, \n              footprint, unsat_kind, unsat_index);\n          if (manager != NULL)\n          {\n            success = true;\n            if (runtime->legion_spy_enabled)\n              manager->log_instance_creation(creator_id, processor, regions);\n            record_created_instance(manager, acquire, mapper_id, processor,\n                                    priority, remote);\n            result = MappingInstance(manager);\n            // We made this instance so mark that it was created\n            created = true;\n          }\n        }\n        else if (footprint != NULL)\n          *footprint = result.get_instance_size();\n        // Release our allocation privilege after doing the record\n        release_allocation_privilege();\n      }\n      return success;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_or_create_physical_instance(\n                                LayoutConstraints *constraints, \n                                const std::vector<LogicalRegion> &regions,\n                                MappingInstance &result, bool &created,\n                                MapperID mapper_id, Processor processor,\n                                bool acquire, GCPriority priority, \n                                bool tight_region_bounds, \n                                LayoutConstraintKind *unsat_kind,\n                                unsigned *unsat_index, size_t *footprint, \n                                UniqueID creator_id, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      volatile bool success = false;\n      // Set created to false in case we fail\n      created = false;\n      if (!is_owner)\n      {\n        // See if we can find it locally\n        success = find_valid_instance(constraints, regions, result, \n                                      acquire, tight_region_bounds, remote);\n        if (success)\n          return true;\n        // Not the owner, send a message to the owner to request creation\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(FIND_OR_CREATE_LAYOUT);\n          rez.serialize(ready_event);\n          rez.serialize<size_t>(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          rez.serialize(constraints->layout_id);\n          rez.serialize(mapper_id);\n          rez.serialize(processor);\n          rez.serialize(priority);\n          rez.serialize<bool>(tight_region_bounds);\n          rez.serialize(unsat_kind);\n          rez.serialize(unsat_index);\n          rez.serialize(footprint);\n          rez.serialize(creator_id);\n          rez.serialize(&success);\n          rez.serialize(&result);\n          rez.serialize(&created);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled\n      }\n      else\n      {\n        // Create the builder and initialize it before getting\n        // the allocation privilege to avoid deadlock scenario\n        InstanceBuilder builder(regions,*constraints, runtime, this,creator_id);\n        builder.initialize(runtime->forest);\n        // First get our allocation privileges so we're the only\n        // one trying to do any allocations\n        const RtEvent wait_on = acquire_allocation_privilege();\n        if (wait_on.exists())\n          wait_on.wait();\n        // Since this is find or acquire, first see if we can find\n        // an instance that has already been makde that satisfies \n        // our layout constraints\n        // Try to find an instance first and then make one\n        success = find_satisfying_instance(constraints, regions, \n                   result, acquire, tight_region_bounds, remote);\n        if (!success)\n        {\n          // If we couldn't find it, we have to make it\n          PhysicalManager *manager = allocate_physical_instance(builder, \n              footprint, unsat_kind, unsat_index);\n          if (manager != NULL)\n          {\n            success = true;\n            if (runtime->legion_spy_enabled)\n              manager->log_instance_creation(creator_id, processor, regions);\n            record_created_instance(manager, acquire, mapper_id, processor,\n                                    priority, remote);\n            result = MappingInstance(manager);\n            // We made this instance so mark that it was created\n            created = true;\n          }\n        }\n        else if (footprint != NULL)\n          *footprint = result.get_instance_size();\n        // Release our allocation privilege after doing the record\n        release_allocation_privilege();\n      }\n      return success;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_physical_instance(\n                                     const LayoutConstraintSet &constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result, bool acquire, \n                                     bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      volatile bool success = false;\n      if (!is_owner)\n      {\n        // See if we can find it locally \n        success = find_valid_instance(constraints, regions, result, \n                                      acquire, tight_region_bounds, remote);\n        if (success)\n          return true;\n        // Not the owner, send a message to the owner to try and find it\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(FIND_ONLY_CONSTRAINTS);\n          rez.serialize(ready_event);\n          rez.serialize(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          constraints.serialize(rez);\n          rez.serialize<bool>(tight_region_bounds);\n          rez.serialize(&success);\n          rez.serialize(&result);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled\n      }\n      else\n      {\n        // Try to find an instance\n        success = find_satisfying_instance(constraints, regions, result, \n                                  acquire, tight_region_bounds, remote);\n      }\n      return success;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_physical_instance(LayoutConstraints *constraints,\n                                      const std::vector<LogicalRegion> &regions,\n                                      MappingInstance &result, bool acquire, \n                                      bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      volatile bool success = false;\n      if (!is_owner)\n      {\n        // See if we can find a persistent instance\n        success = find_valid_instance(constraints, regions, result, \n                                      acquire, tight_region_bounds, remote);\n        if (success)\n          return true;\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(FIND_ONLY_LAYOUT);\n          rez.serialize(ready_event);\n          rez.serialize<size_t>(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          rez.serialize(constraints->layout_id);\n          rez.serialize<bool>(tight_region_bounds);\n          rez.serialize(&success);\n          rez.serialize(&result);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled\n      }\n      else\n      {\n        // Try to find an instance\n        success = find_satisfying_instance(constraints, regions, result,\n                                   acquire, tight_region_bounds, remote);\n      }\n      return success;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::find_physical_instances(\n                            const LayoutConstraintSet &constraints,\n                            const std::vector<LogicalRegion> &regions,\n                            std::vector<MappingInstance> &results, \n                            bool acquire, bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (!is_owner)\n      {\n        // Not the owner, send a message to the owner to try and find it\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(FIND_MANY_CONSTRAINTS);\n          rez.serialize(ready_event);\n          rez.serialize(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          constraints.serialize(rez);\n          rez.serialize<bool>(tight_region_bounds);\n          rez.serialize(&results);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled\n      }\n      else\n        find_satisfying_instances(constraints, regions, results,\n                                  acquire, tight_region_bounds, remote);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::find_physical_instances(LayoutConstraints *constraints,\n                            const std::vector<LogicalRegion> &regions,\n                            std::vector<MappingInstance> &results, \n                            bool acquire, bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (!is_owner)\n      {\n        Serializer rez;\n        RtUserEvent ready_event = Runtime::create_rt_user_event();\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(FIND_MANY_LAYOUT);\n          rez.serialize(ready_event);\n          rez.serialize<size_t>(regions.size());\n          for (unsigned idx = 0; idx < regions.size(); idx++)\n            rez.serialize(regions[idx]);\n          rez.serialize<bool>(acquire);\n          rez.serialize(constraints->layout_id);\n          rez.serialize<bool>(tight_region_bounds);\n          rez.serialize(&results);\n        }\n        runtime->send_instance_request(owner_space, rez);\n        ready_event.wait();\n        // When the event is triggered, everything will be filled\n      }\n      else\n        find_satisfying_instances(constraints, regions, results,\n                                  acquire, tight_region_bounds, remote);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::release_tree_instances(RegionTreeID tree_id)\n    //--------------------------------------------------------------------------\n    {\n      // If we're not the owner, then there is nothing to do\n      if (!is_owner)\n        return;\n      // Take the manager lock and see if there are any managers\n      // we can release now\n      std::map<PhysicalManager*,std::pair<RtEvent,bool> > to_release;\n      do \n      {\n        std::vector<PhysicalManager*> to_remove;\n        AutoLock m_lock(manager_lock);\n        std::map<RegionTreeID,TreeInstances>::iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (TreeInstances::iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          // If the region for the instance is not for the tree then\n          // we get to skip it\n          if (it->first->tree_id != tree_id)\n            continue;\n          // If it's already been deleted, then there is nothing to do\n          if (it->second.current_state == PENDING_COLLECTED_STATE)\n            continue;\n#ifdef DEBUG_LEGION\n          assert(it->second.current_state != PENDING_ACQUIRE_STATE);\n#endif\n          if (it->second.current_state != COLLECTABLE_STATE)\n          {\n#ifdef DEBUG_LEGION\n            // We might have lost a race with adding LEGION_NEVER_GC_REF\n            // after release the manager lock if we hit this assertion\n            if (it->second.min_priority == LEGION_GC_NEVER_PRIORITY)\n              assert(it->second.current_state == VALID_STATE);\n#endif\n            bool remove_valid_ref = false;\n            it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n            // Remove any NEVER GC references if necessary\n            if (it->second.min_priority == LEGION_GC_NEVER_PRIORITY)\n              remove_valid_ref = true;\n            it->second.mapper_priorities.clear();\n            it->second.min_priority = LEGION_GC_MAX_PRIORITY;\n            // Go to the pending collectable state\n            RtUserEvent deferred_collect = Runtime::create_rt_user_event();\n            it->second.current_state = PENDING_COLLECTED_STATE;\n            it->second.deferred_collect = deferred_collect;\n            to_release[it->first] = std::pair<RtEvent,bool>(\n                                      deferred_collect, remove_valid_ref);\n#ifdef LEGION_MALLOC_INSTANCES\n            pending_collectables[deferred_collect] = 0; \n#endif\n          }\n          else\n          {\n            to_release[it->first] = std::pair<RtEvent,bool>(\n                   RtEvent::NO_RT_EVENT, false/*remove valid ref*/);\n            to_remove.push_back(it->first);\n          }\n        }\n        if (!to_remove.empty())\n        {\n          for (std::vector<PhysicalManager*>::const_iterator it = \n                to_remove.begin(); it != to_remove.end(); it++)\n            finder->second.erase(*it);\n          if (finder->second.empty())\n            current_instances.erase(finder);\n        }\n      } while (false);\n      for (std::map<PhysicalManager*,std::pair<RtEvent,bool> >::\n            const_iterator it = to_release.begin(); it != to_release.end();it++)\n      {\n        it->first->perform_deletion(it->second.first);\n        if (it->second.second)\n          it->first->remove_base_valid_ref(NEVER_GC_REF);\n        // Now we can release our resource reference\n        if (it->first->remove_base_resource_ref(MEMORY_MANAGER_REF))\n          delete (it->first);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::set_garbage_collection_priority(\n                                PhysicalManager *manager, MapperID mapper_id, \n                                Processor processor, GCPriority priority)\n    //--------------------------------------------------------------------------\n    {\n      // Ignore garbage collection priorities on external instances\n      if (manager->is_external_instance())\n      {\n        MapperManager *manager = runtime->find_mapper(processor, mapper_id);\n        REPORT_LEGION_WARNING(LEGION_WARNING_EXTERNAL_GARBAGE_PRIORITY,\n            \"Ignoring request for mapper %s to set garbage collection \"\n            \"priority on an external instance\", manager->get_mapper_name())\n        return;\n      }\n      bool remove_min_reference = false;\n      IgnoreReferenceMutator mutator;\n      if (!is_owner)\n      {\n        RtUserEvent never_gc_wait;\n        bool remove_never_gc_ref = false;\n        std::pair<MapperID,Processor> key(mapper_id,processor);\n        // Check to see if this is or is going to be a max priority instance\n        if (priority == LEGION_GC_NEVER_PRIORITY)\n        {\n          // See if we need a handback\n          AutoLock m_lock(manager_lock,1,false);\n          std::map<RegionTreeID,TreeInstances>::const_iterator tree_finder =\n            current_instances.find(manager->tree_id);\n          if (tree_finder != current_instances.end())\n          {\n            TreeInstances::const_iterator finder = \n              tree_finder->second.find(manager);\n            if (finder != tree_finder->second.end())\n            {\n              // If priority is already max priority, then we are done\n              if (finder->second.min_priority == priority)\n                return;\n              // Make an event for a callback\n              never_gc_wait = Runtime::create_rt_user_event();\n            }\n          }\n        }\n        else\n        {\n          AutoLock m_lock(manager_lock);\n          std::map<RegionTreeID,TreeInstances>::iterator tree_finder =\n            current_instances.find(manager->tree_id);\n          if (tree_finder != current_instances.end())\n          {\n            TreeInstances::iterator finder = \n              tree_finder->second.find(manager);\n            if (finder != tree_finder->second.end())\n            {\n              if (finder->second.min_priority == LEGION_GC_NEVER_PRIORITY)\n              {\n                finder->second.mapper_priorities.erase(key);\n                if (finder->second.mapper_priorities.empty())\n                {\n                  finder->second.min_priority = 0;\n                  remove_never_gc_ref = true;\n                }\n              }\n            }\n          }\n        }\n        // Won't delete the whole manager because we still hold\n        // a resource reference\n        if (remove_never_gc_ref)\n          manager->remove_base_valid_ref(NEVER_GC_REF);\n        // We are not the owner so send a message to the owner\n        // to update the priority, no need to send the manager\n        // since we know we are sending to the owner node\n        volatile bool success = true;\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(manager->did);\n          rez.serialize(mapper_id);\n          rez.serialize(processor);\n          rez.serialize(priority);\n          rez.serialize(never_gc_wait);\n          if (never_gc_wait.exists())\n            rez.serialize(&success);\n        }\n        runtime->send_gc_priority_update(owner_space, rez);\n        // In most cases, we will fire and forget, the one exception\n        // is if we are waiting for a confirmation of setting max priority\n        if (never_gc_wait.exists())\n        {\n          never_gc_wait.wait();\n          bool remove_duplicate = false;\n          if (success)\n          {\n            LocalReferenceMutator local_mutator;\n            // Add our local reference\n            manager->add_base_valid_ref(NEVER_GC_REF, &local_mutator);\n            const RtEvent reference_effects = local_mutator.get_done_event();\n            manager->send_remote_valid_decrement(owner_space, NULL,\n                                                 reference_effects);\n            if (reference_effects.exists())\n              mutator.record_reference_mutation_effect(reference_effects);\n            // Then record it\n            AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n            assert(current_instances.find(manager->tree_id) !=\n                    current_instances.end());\n            assert(current_instances[manager->tree_id].find(manager) != \n                    current_instances[manager->tree_id].end());\n#endif\n            InstanceInfo &info = current_instances[manager->tree_id][manager];\n            if (info.min_priority == LEGION_GC_NEVER_PRIORITY)\n              remove_duplicate = true; // lost the race\n            else\n              info.min_priority = LEGION_GC_NEVER_PRIORITY;\n            info.mapper_priorities[key] = LEGION_GC_NEVER_PRIORITY;\n          }\n          if (remove_duplicate && \n              manager->remove_base_valid_ref(NEVER_GC_REF, &mutator))\n            delete manager; \n        }\n      }\n      else\n      {\n        // If this a max priority, try adding the reference beforehand, if\n        // it fails then we know the instance is already deleted so whatever\n        if ((priority == LEGION_GC_NEVER_PRIORITY) &&\n            !manager->acquire_instance(NEVER_GC_REF, &mutator))\n          return;\n        // Do the update locally \n        AutoLock m_lock(manager_lock);\n        std::map<RegionTreeID,TreeInstances>::iterator tree_finder = \n          current_instances.find(manager->tree_id);\n        if (tree_finder != current_instances.end())\n        {\n          std::map<PhysicalManager*,InstanceInfo>::iterator finder = \n            tree_finder->second.find(manager);\n          if (finder != tree_finder->second.end())\n          {\n            std::map<std::pair<MapperID,Processor>,GCPriority> \n              &mapper_priorities = finder->second.mapper_priorities;\n            std::pair<MapperID,Processor> key(mapper_id,processor);\n            // If the new priority is NEVER_GC and we were already at NEVER_GC\n            // then we need to remove the redundant reference when we are done\n            if ((priority == LEGION_GC_NEVER_PRIORITY) && \n                (finder->second.min_priority == LEGION_GC_NEVER_PRIORITY))\n              remove_min_reference = true;\n            // See if we can find the current priority  \n            std::map<std::pair<MapperID,Processor>,GCPriority>::iterator \n              priority_finder = mapper_priorities.find(key);\n            if (priority_finder != mapper_priorities.end())\n            {\n              // See if it changed\n              if (priority_finder->second != priority)\n              {\n                // Update the min if necessary\n                if (priority < finder->second.min_priority)\n                {\n                  // It decreased \n                  finder->second.min_priority = priority;\n                }\n                // It might go up if this was (one of) the min priorities\n                else if ((priority > finder->second.min_priority) &&\n                       (finder->second.min_priority == priority_finder->second))\n                {\n                  // This was (one of) the min priorities, but it \n                  // is about to go up so compute the new min\n                  GCPriority new_min = priority;\n                  for (std::map<std::pair<MapperID,Processor>,GCPriority>::\n                        const_iterator it = mapper_priorities.begin(); it != \n                        mapper_priorities.end(); it++)\n                  {\n                    if (it->first == key)\n                      continue;\n                    // If we find another one with the same as the current \n                    // min then we know we are just going to stay the same\n                    if (it->second == finder->second.min_priority)\n                    {\n                      new_min = it->second;\n                      break;\n                    }\n                    if (it->second < new_min)\n                      new_min = it->second;\n                  }\n                  if ((finder->second.min_priority == LEGION_GC_NEVER_PRIORITY)\n                        && (new_min > LEGION_GC_NEVER_PRIORITY))\n                    remove_min_reference = true;\n                  finder->second.min_priority = new_min;\n                }\n                // Finally update the priority\n                priority_finder->second = priority;\n              }\n            }\n            else // previous priority was zero, see if we need to update it\n            {\n              mapper_priorities[key] = priority;\n              if (priority < finder->second.min_priority)\n                finder->second.min_priority = priority;\n            }\n          }\n        }\n      }\n      if (remove_min_reference && \n          manager->remove_base_valid_ref(NEVER_GC_REF, &mutator))\n        delete manager;\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent MemoryManager::acquire_instances(\n                                     const std::set<PhysicalManager*> &managers,\n                                     std::vector<bool> &results)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!is_owner); // should never be called on the owner\n      assert(results.empty());\n#endif\n      results.resize(managers.size(), false/*assume everything fails*/);\n      // Package everything up and send the request \n      RtUserEvent done = Runtime::create_rt_user_event();\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(memory);\n        rez.serialize<size_t>(managers.size());\n        for (std::set<PhysicalManager*>::const_iterator it = \n              managers.begin(); it != managers.end(); it++)\n        {\n          rez.serialize((*it)->did);\n          rez.serialize(*it);\n        }\n        rez.serialize(&results);\n        rez.serialize(done);\n      }\n      runtime->send_acquire_request(owner_space, rez);\n      return done;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::process_instance_request(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      RequestKind kind;\n      derez.deserialize(kind);\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      size_t num_regions;\n      derez.deserialize(num_regions);\n      std::vector<LogicalRegion> regions(num_regions);\n      for (unsigned idx = 0; idx < num_regions; idx++)\n        derez.deserialize(regions[idx]);\n      bool acquire;\n      derez.deserialize(acquire);\n      switch (kind)\n      {\n        case CREATE_INSTANCE_CONSTRAINTS:\n          {\n            DistributedID collective_did;\n            derez.deserialize(collective_did);\n            RtEvent collective_ready;\n            DomainPoint point;\n            CollectiveManager *collective = NULL;\n            if (collective_did > 0)\n            {\n              collective = static_cast<CollectiveManager*>(\n                  runtime->find_or_request_instance_manager(collective_did, \n                                                            collective_ready));\n              derez.deserialize(point);\n            }\n            LayoutConstraintSet constraints;\n            constraints.deserialize(derez);\n            MapperID mapper_id;\n            derez.deserialize(mapper_id);\n            Processor processor;\n            derez.deserialize(processor);\n            GCPriority priority;\n            derez.deserialize(priority);\n            bool tight_region_bounds;\n            derez.deserialize<bool>(tight_region_bounds);\n            LayoutConstraintKind *remote_kind;\n            derez.deserialize(remote_kind);\n            unsigned *remote_index;\n            derez.deserialize(remote_index);\n            size_t *remote_footprint; // warning: remote pointer\n            derez.deserialize(remote_footprint);\n            UniqueID creator_id;\n            derez.deserialize(creator_id);\n            bool *remote_success;\n            derez.deserialize(remote_success);\n            MappingInstance *remote_target;\n            derez.deserialize(remote_target);\n            MappingInstance result;\n            size_t local_footprint;\n            LayoutConstraintKind local_kind;\n            unsigned local_index;\n            if (collective_ready.exists() && !collective_ready.has_triggered())\n              collective_ready.wait();\n            bool success = create_physical_instance(constraints, regions, \n                                 result, mapper_id, processor, acquire, \n                                 priority, tight_region_bounds,\n                                 &local_kind, &local_index, &local_footprint,\n                                 collective, &point, creator_id,true/*remote*/);\n            if (success || (remote_footprint != NULL) || \n                (remote_kind != NULL) || (remote_index != NULL))\n            {\n              // Send back the response starting with the instance\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(success);\n                if (success)\n                {\n                  InstanceManager *manager = result.impl;\n                  rez.serialize(manager->did);\n                  rez.serialize<bool>(acquire);\n                  rez.serialize(remote_target);\n                  rez.serialize(remote_success);\n                  rez.serialize(kind);\n                  bool min_priority = (priority == LEGION_GC_NEVER_PRIORITY);\n                  rez.serialize<bool>(min_priority);\n                  if (min_priority)\n                  {\n                    rez.serialize(mapper_id);\n                    rez.serialize(processor);\n                  }\n                }\n                else\n                  rez.serialize(kind);\n                rez.serialize(remote_kind);\n                rez.serialize(local_kind);\n                rez.serialize(remote_index);\n                rez.serialize(local_index);\n                rez.serialize(remote_footprint);\n                rez.serialize(local_footprint);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // we can just trigger the done event since we failed\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case CREATE_INSTANCE_LAYOUT:\n          {\n            DistributedID collective_did;\n            derez.deserialize(collective_did);\n            RtEvent collective_ready;\n            CollectiveManager *collective = NULL;\n            DomainPoint point;\n            if (collective_did > 0)\n            {\n              collective = static_cast<CollectiveManager*>(\n                  runtime->find_or_request_instance_manager(collective_did, \n                                                            collective_ready));\n              derez.deserialize(point);\n            }\n            LayoutConstraintID layout_id;\n            derez.deserialize(layout_id);\n            MapperID mapper_id;\n            derez.deserialize(mapper_id);\n            Processor processor;\n            derez.deserialize(processor);\n            GCPriority priority;\n            derez.deserialize(priority);\n            bool tight_region_bounds;\n            derez.deserialize<bool>(tight_region_bounds);\n            LayoutConstraintKind *remote_kind;\n            derez.deserialize(remote_kind);\n            unsigned *remote_index;\n            derez.deserialize(remote_index);\n            size_t *remote_footprint; // warning: remote pointer\n            derez.deserialize(remote_footprint);\n            UniqueID creator_id;\n            derez.deserialize(creator_id);\n            bool *remote_success;\n            derez.deserialize(remote_success);\n            MappingInstance *remote_target;\n            derez.deserialize(remote_target);\n            LayoutConstraints *constraints = \n              runtime->find_layout_constraints(layout_id);\n            MappingInstance result;\n            size_t local_footprint;\n            LayoutConstraintKind local_kind;\n            unsigned local_index;\n            if (collective_ready.exists() && !collective_ready.has_triggered())\n              collective_ready.wait();\n            bool success = create_physical_instance(constraints, regions, \n                                 result, mapper_id, processor, acquire, \n                                 priority, tight_region_bounds,\n                                 &local_kind, &local_index, &local_footprint,\n                                 collective, &point, creator_id,true/*remote*/);\n            if (success || (remote_footprint != NULL) ||\n                (remote_kind != NULL) || (remote_index != NULL))\n            {\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(success);\n                if (success)\n                {\n                  InstanceManager *manager = result.impl;\n                  rez.serialize(manager->did);\n                  rez.serialize<bool>(acquire);\n                  rez.serialize(remote_target);\n                  rez.serialize(remote_success);\n                  rez.serialize(kind);\n                  bool min_priority = (priority == LEGION_GC_NEVER_PRIORITY);\n                  rez.serialize<bool>(min_priority);\n                  if (min_priority)\n                  {\n                    rez.serialize(mapper_id);\n                    rez.serialize(processor);\n                  }\n                }\n                else\n                  rez.serialize(kind);\n                rez.serialize(remote_kind);\n                rez.serialize(local_kind);\n                rez.serialize(remote_index);\n                rez.serialize(local_index);\n                rez.serialize(remote_footprint);\n                rez.serialize(local_footprint);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // if we failed, we can just trigger the response\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case FIND_OR_CREATE_CONSTRAINTS:\n          {\n            LayoutConstraintSet constraints;\n            constraints.deserialize(derez);\n            MapperID mapper_id;\n            derez.deserialize(mapper_id);\n            Processor processor;\n            derez.deserialize(processor);\n            GCPriority priority;\n            derez.deserialize(priority);\n            bool tight_bounds;\n            derez.deserialize(tight_bounds);\n            LayoutConstraintKind *remote_kind;\n            derez.deserialize(remote_kind);\n            unsigned *remote_index;\n            derez.deserialize(remote_index);\n            size_t *remote_footprint; // warning: remote pointer\n            derez.deserialize(remote_footprint);\n            UniqueID creator_id;\n            derez.deserialize(creator_id);\n            bool *remote_success, *remote_created;\n            derez.deserialize(remote_success);\n            MappingInstance *remote_target;\n            derez.deserialize(remote_target);\n            derez.deserialize(remote_created);\n            MappingInstance result;\n            size_t local_footprint;\n            LayoutConstraintKind local_kind;\n            unsigned local_index;\n            bool created;\n            bool success = find_or_create_physical_instance(constraints, \n                                regions, result, created, mapper_id, \n                                processor, acquire, priority, tight_bounds,\n                                &local_kind, &local_index,\n                                &local_footprint, creator_id, true/*remote*/);\n            if (success || (remote_footprint != NULL) ||\n                (remote_kind != NULL) || (remote_index != NULL))\n            {\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(success);\n                if (success)\n                {\n                  InstanceManager *manager = result.impl;\n                  rez.serialize(manager->did);\n                  rez.serialize<bool>(acquire);\n                  rez.serialize(remote_target);\n                  rez.serialize(remote_success);\n                  rez.serialize(kind);\n                  rez.serialize(remote_created);\n                  rez.serialize<bool>(created);\n                  if (created)\n                  {\n                    bool min_priority = (priority == LEGION_GC_NEVER_PRIORITY);\n                    rez.serialize<bool>(min_priority);\n                    if (min_priority)\n                    {\n                      rez.serialize(mapper_id);\n                      rez.serialize(processor);\n                    }\n                  }\n                }\n                else\n                  rez.serialize(kind);\n                rez.serialize(remote_kind);\n                rez.serialize(local_kind);\n                rez.serialize(remote_index);\n                rez.serialize(local_index);\n                rez.serialize(remote_footprint);\n                rez.serialize(local_footprint);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // if we failed, we can just trigger the response\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case FIND_OR_CREATE_LAYOUT:\n          {\n            LayoutConstraintID layout_id;\n            derez.deserialize(layout_id);\n            MapperID mapper_id;\n            derez.deserialize(mapper_id);\n            Processor processor;\n            derez.deserialize(processor);\n            GCPriority priority;\n            derez.deserialize(priority);\n            bool tight_bounds;\n            derez.deserialize(tight_bounds);\n            LayoutConstraintKind *remote_kind;\n            derez.deserialize(remote_kind);\n            unsigned *remote_index;\n            derez.deserialize(remote_index);\n            size_t *remote_footprint; // warning: remote pointer\n            derez.deserialize(remote_footprint);\n            UniqueID creator_id;\n            derez.deserialize(creator_id);\n            bool *remote_success, *remote_created;\n            derez.deserialize(remote_success);\n            MappingInstance *remote_target;\n            derez.deserialize(remote_target);\n            derez.deserialize(remote_created);\n            LayoutConstraints *constraints = \n              runtime->find_layout_constraints(layout_id);\n            MappingInstance result;\n            size_t local_footprint;\n            LayoutConstraintKind local_kind;\n            unsigned local_index;\n            bool created;\n            bool success = find_or_create_physical_instance(constraints, \n                                 regions, result, created, mapper_id, \n                                 processor, acquire, priority, tight_bounds,\n                                 &local_kind, &local_index,\n                                 &local_footprint, creator_id, true/*remote*/);\n            if (success || (remote_footprint != NULL) ||\n                (remote_kind != NULL) || (remote_index != NULL))\n            {\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(success);\n                if (success)\n                {\n                  InstanceManager *manager = result.impl;\n                  rez.serialize(manager->did);\n                  rez.serialize<bool>(acquire);\n                  rez.serialize(remote_target);\n                  rez.serialize(remote_success);\n                  rez.serialize(kind);\n                  rez.serialize(remote_created);\n                  rez.serialize<bool>(created);\n                  if (created)\n                  {\n                    bool min_priority = (priority == LEGION_GC_NEVER_PRIORITY);\n                    rez.serialize<bool>(min_priority);\n                    if (min_priority)\n                    {\n                      rez.serialize(mapper_id);\n                      rez.serialize(processor);\n                    }\n                  }\n                }\n                else\n                  rez.serialize(kind);\n                rez.serialize(remote_kind);\n                rez.serialize(local_kind);\n                rez.serialize(remote_index);\n                rez.serialize(local_index);\n                rez.serialize(remote_footprint);\n                rez.serialize(local_footprint);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // we failed so just trigger the response\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case FIND_ONLY_CONSTRAINTS:\n          {\n            LayoutConstraintSet constraints; \n            constraints.deserialize(derez);\n            bool tight_bounds;\n            derez.deserialize(tight_bounds);\n            bool *remote_success;\n            derez.deserialize(remote_success);\n            MappingInstance *remote_target;\n            derez.deserialize(remote_target);\n            MappingInstance result;\n            bool success = find_physical_instance(constraints, regions,\n                        result, acquire, tight_bounds, true/*remote*/);\n            if (success)\n            {\n              InstanceManager *manager = result.impl;\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(true); // success\n                rez.serialize(manager->did);\n                rez.serialize<bool>(acquire);\n                rez.serialize(remote_target);\n                rez.serialize(remote_success);\n                rez.serialize(kind);\n                // No things for us to pass back here\n                rez.serialize<LayoutConstraintKind*>(NULL);\n                rez.serialize(LEGION_SPECIALIZED_CONSTRAINT);\n                rez.serialize<unsigned*>(NULL);\n                rez.serialize<unsigned>(0);\n                rez.serialize<size_t*>(NULL);\n                rez.serialize<size_t>(0);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // we failed so we can just trigger the response\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case FIND_ONLY_LAYOUT:\n          {\n            LayoutConstraintID layout_id;\n            derez.deserialize(layout_id);\n            bool tight_bounds;\n            derez.deserialize(tight_bounds);\n            bool *remote_success;\n            derez.deserialize(remote_success);\n            MappingInstance *remote_target;\n            derez.deserialize(remote_target);\n            LayoutConstraints *constraints = \n              runtime->find_layout_constraints(layout_id);\n            MappingInstance result;\n            bool success = find_physical_instance(constraints, regions, \n                        result, acquire, tight_bounds, true/*remote*/);\n            if (success)\n            {\n              InstanceManager *manager = result.impl;\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(true); // success\n                rez.serialize(manager->did);\n                rez.serialize<bool>(acquire);\n                rez.serialize(remote_target);\n                rez.serialize(remote_success);\n                rez.serialize(kind);\n                // No things for us to pass back here\n                rez.serialize<LayoutConstraintKind*>(NULL);\n                rez.serialize(LEGION_SPECIALIZED_CONSTRAINT);\n                rez.serialize<unsigned*>(NULL);\n                rez.serialize<unsigned>(0);\n                rez.serialize<size_t*>(NULL);\n                rez.serialize<size_t>(0);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // we failed so just trigger\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case FIND_MANY_CONSTRAINTS:\n          {\n            LayoutConstraintSet constraints; \n            constraints.deserialize(derez);\n            bool tight_bounds;\n            derez.deserialize(tight_bounds);\n            bool *remote_success;\n            derez.deserialize(remote_success);\n            std::vector<MappingInstance> *remote_target;\n            derez.deserialize(remote_target);\n            std::vector<MappingInstance> results;\n            find_physical_instances(constraints, regions, results, acquire, \n                                    tight_bounds, true/*remote*/);\n            if (!results.empty())\n            {\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(false); // success\n                rez.serialize(kind);\n                rez.serialize(remote_target);\n                rez.serialize<bool>(acquire);\n                rez.serialize<size_t>(results.size());\n                for (unsigned idx = 0; idx < results.size(); idx++)\n                {\n                  InstanceManager *manager = results[idx].impl;\n                  rez.serialize(manager->did);\n                }\n                // No things for us to pass back here\n                rez.serialize<LayoutConstraintKind*>(NULL);\n                rez.serialize(LEGION_SPECIALIZED_CONSTRAINT);\n                rez.serialize<unsigned*>(NULL);\n                rez.serialize<unsigned>(0);\n                rez.serialize<size_t*>(NULL);\n                rez.serialize<size_t>(0);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // we failed so we can just trigger the response\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        case FIND_MANY_LAYOUT:\n          {\n            LayoutConstraintID layout_id;\n            derez.deserialize(layout_id);\n            bool tight_bounds;\n            derez.deserialize(tight_bounds);\n            bool *remote_success;\n            derez.deserialize(remote_success);\n            std::vector<MappingInstance> *remote_target;\n            derez.deserialize(remote_target);\n            LayoutConstraints *constraints = \n              runtime->find_layout_constraints(layout_id);\n            std::vector<MappingInstance> results;\n            find_physical_instances(constraints, regions, results, acquire, \n                                    tight_bounds, true/*remote*/);\n            if (!results.empty())\n            {\n              Serializer rez;\n              {\n                RezCheck z(rez);\n                rez.serialize(memory);\n                rez.serialize(to_trigger);\n                rez.serialize<bool>(false); // success\n                rez.serialize(kind);\n                rez.serialize(remote_target);\n                rez.serialize<bool>(acquire);\n                rez.serialize<size_t>(results.size());\n                for (unsigned idx = 0; idx < results.size(); idx++)\n                {\n                  InstanceManager *manager = results[idx].impl;\n                  rez.serialize(manager->did);\n                }\n                // No things for us to pass back here\n                rez.serialize<LayoutConstraintKind*>(NULL);\n                rez.serialize(LEGION_SPECIALIZED_CONSTRAINT);\n                rez.serialize<unsigned*>(NULL);\n                rez.serialize<unsigned>(0);\n                rez.serialize<size_t*>(NULL);\n                rez.serialize<size_t>(0);\n              }\n              runtime->send_instance_response(source, rez);\n            }\n            else // we failed so just trigger\n              Runtime::trigger_event(to_trigger);\n            break;\n          }\n        default:\n          assert(false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::process_instance_response(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      bool success;\n      derez.deserialize<bool>(success);\n      std::set<RtEvent> preconditions;\n      if (success)\n      {\n        DistributedID did;\n        derez.deserialize(did);\n        bool acquire;\n        derez.deserialize(acquire);\n        MappingInstance *target;\n        derez.deserialize(target);\n        bool *success_ptr;\n        derez.deserialize(success_ptr);\n        RequestKind kind;\n        derez.deserialize(kind);\n#ifdef DEBUG_LEGION\n        assert((CREATE_INSTANCE_CONSTRAINTS <= kind) &&\n               (kind <= FIND_ONLY_LAYOUT));\n#endif\n        RtEvent manager_ready = RtEvent::NO_RT_EVENT;\n        PhysicalManager *manager = \n          runtime->find_or_request_instance_manager(did, manager_ready);\n        WrapperReferenceMutator mutator(preconditions);\n        // If the manager isn't ready yet, then we need to wait for it\n        if (manager_ready.exists())\n          manager_ready.wait();\n        // If we acquired on the owner node, add our own local reference\n        // and then remove the remote DID\n        if (acquire)\n        {\n          LocalReferenceMutator local_mutator;\n          manager->add_base_valid_ref(MAPPING_ACQUIRE_REF, &local_mutator);\n          const RtEvent reference_effects = local_mutator.get_done_event();\n          manager->send_remote_valid_decrement(source, NULL,\n                                               reference_effects);\n          if (reference_effects.exists())\n            mutator.record_reference_mutation_effect(reference_effects);\n        }\n        *target = MappingInstance(manager);\n        *success_ptr = true;\n        if ((kind == FIND_OR_CREATE_CONSTRAINTS) || \n            (kind == FIND_OR_CREATE_LAYOUT))\n        {\n          bool *created_ptr;\n          derez.deserialize(created_ptr);\n          bool created;\n          derez.deserialize(created);\n          *created_ptr = created;\n          bool min_priority = false;\n          MapperID mapper_id = 0;\n          Processor processor = Processor::NO_PROC;\n          if (created)\n          {\n            derez.deserialize(min_priority);\n            if (min_priority)\n            {\n              derez.deserialize(mapper_id);\n              derez.deserialize(processor);\n            }\n          }\n          // Record the instance as a max priority instance\n          bool remove_duplicate_valid = false;\n          // No need to be safe here, we have a valid reference\n          if (created && min_priority)\n            manager->add_base_valid_ref(NEVER_GC_REF, &mutator);\n          {\n            AutoLock m_lock(manager_lock);\n            std::map<RegionTreeID,TreeInstances>::iterator tree_finder = \n              current_instances.find(manager->tree_id);\n            if (tree_finder != current_instances.end())\n            {\n              TreeInstances::const_iterator finder = \n                tree_finder->second.find(manager);\n            if (finder == tree_finder->second.end())\n              tree_finder->second[manager] = InstanceInfo();  \n            }\n            else\n              current_instances[manager->tree_id][manager] = InstanceInfo();\n            if (created && min_priority)\n            {\n              std::pair<MapperID,Processor> key(mapper_id,processor);\n              InstanceInfo &info = current_instances[manager->tree_id][manager];\n              if (info.min_priority == LEGION_GC_NEVER_PRIORITY)\n                remove_duplicate_valid = true;\n              else\n                info.min_priority = LEGION_GC_NEVER_PRIORITY;\n              info.mapper_priorities[key] = LEGION_GC_NEVER_PRIORITY;\n            }\n          }\n          if (remove_duplicate_valid && \n              manager->remove_base_valid_ref(NEVER_GC_REF, &mutator))\n            delete manager;\n        }\n        else if ((kind == CREATE_INSTANCE_CONSTRAINTS) ||\n                 (kind == CREATE_INSTANCE_LAYOUT))\n        {\n          bool min_priority;\n          derez.deserialize(min_priority);\n          MapperID mapper_id = 0;\n          Processor processor = Processor::NO_PROC;\n          if (min_priority)\n          {\n            derez.deserialize(mapper_id);\n            derez.deserialize(processor);\n          }\n          bool remove_duplicate_valid = false;\n          if (min_priority)\n            manager->add_base_valid_ref(NEVER_GC_REF, &mutator);\n          {\n            std::pair<MapperID,Processor> key(mapper_id,processor);\n            AutoLock m_lock(manager_lock);\n            std::map<RegionTreeID,TreeInstances>::iterator tree_finder = \n              current_instances.find(manager->tree_id);\n            if (tree_finder != current_instances.end())\n            {\n              TreeInstances::const_iterator finder = \n                tree_finder->second.find(manager);\n            if (finder == tree_finder->second.end())\n              tree_finder->second[manager] = InstanceInfo();  \n            }\n            else\n              current_instances[manager->tree_id][manager] = InstanceInfo();\n            if (min_priority)\n            {\n              InstanceInfo &info = current_instances[manager->tree_id][manager];\n              if (info.min_priority == LEGION_GC_NEVER_PRIORITY)\n                remove_duplicate_valid = true;\n              else\n                info.min_priority = LEGION_GC_NEVER_PRIORITY;\n              info.mapper_priorities[key] = LEGION_GC_NEVER_PRIORITY;\n            }\n          }\n          if (remove_duplicate_valid && \n              manager->remove_base_valid_ref(NEVER_GC_REF, &mutator))\n            delete manager;\n        }\n      }\n      else\n      {\n        RequestKind kind;\n        derez.deserialize(kind);\n        if ((kind == FIND_MANY_CONSTRAINTS) || (kind == FIND_MANY_LAYOUT))\n        {\n          std::vector<MappingInstance> *target;\n          derez.deserialize(target);\n          bool acquire;\n          derez.deserialize<bool>(acquire);\n          size_t num_insts;\n          derez.deserialize(num_insts);\n          WrapperReferenceMutator mutator(preconditions);\n          for (unsigned idx = 0; idx < num_insts; idx++)\n          {\n            DistributedID did;\n            derez.deserialize(did);\n            RtEvent manager_ready = RtEvent::NO_RT_EVENT;\n            PhysicalManager *manager = \n              runtime->find_or_request_instance_manager(did, manager_ready);\n            // If the manager isn't ready yet, then we need to wait for it\n            if (manager_ready.exists())\n              manager_ready.wait();\n            // If we acquired on the owner node, add our own local reference\n            // and then remove the remote DID\n            if (acquire)\n            {\n              LocalReferenceMutator local_mutator;\n              manager->add_base_valid_ref(MAPPING_ACQUIRE_REF, &local_mutator);\n              const RtEvent reference_effects = local_mutator.get_done_event();\n              manager->send_remote_valid_decrement(source, NULL,\n                                                   reference_effects);\n              if (reference_effects.exists())\n                mutator.record_reference_mutation_effect(reference_effects);\n            }\n            target->push_back(MappingInstance(manager));\n          }\n        }\n      }\n      // Unpack the constraint responses\n      LayoutConstraintKind *local_kind;\n      derez.deserialize(local_kind);\n      LayoutConstraintKind kind;\n      derez.deserialize(kind);\n      if (local_kind != NULL)\n        *local_kind = kind;\n      unsigned *local_index;\n      derez.deserialize(local_index);\n      unsigned index;\n      derez.deserialize(index);\n      if (local_index != NULL)\n        *local_index = index;\n      // Unpack the footprint and asign it if necessary\n      size_t *local_footprint;\n      derez.deserialize(local_footprint);\n      size_t footprint;\n      derez.deserialize(footprint);\n      if (local_footprint != NULL)\n        *local_footprint = footprint;\n      // Trigger that we are done\n      if (!preconditions.empty())\n        Runtime::trigger_event(to_trigger,Runtime::merge_events(preconditions));\n      else\n        Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::process_gc_priority_update(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DistributedID did;\n      derez.deserialize(did);\n      MapperID mapper_id;\n      derez.deserialize(mapper_id);\n      Processor processor;\n      derez.deserialize(processor);\n      GCPriority priority;\n      derez.deserialize(priority);\n      RtUserEvent never_gc_event;\n      derez.deserialize(never_gc_event);\n      // Hold our lock to make sure our allocation doesn't change\n      // when getting the reference\n      PhysicalManager *manager = NULL;\n      {\n        AutoLock m_lock(manager_lock,1,false/*exclusive*/);\n        DistributedCollectable *dc = \n          runtime->weak_find_distributed_collectable(did);\n        if (dc != NULL)\n        {\n#ifdef DEBUG_LEGION\n          manager = dynamic_cast<PhysicalManager*>(dc);\n#else\n          manager = static_cast<PhysicalManager*>(dc);\n#endif\n          manager->add_base_resource_ref(MEMORY_MANAGER_REF);\n        }\n      }\n      // If the instance was already collected, there is nothing to do\n      if (manager == NULL)\n      {\n        if (never_gc_event.exists())\n        {\n          bool *success;\n          derez.deserialize(success);\n          // Only have to send the message back when we fail\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(memory);\n            rez.serialize(success);\n            rez.serialize(never_gc_event);\n          }\n          runtime->send_never_gc_response(source, rez);\n        }\n        return;\n      }\n      set_garbage_collection_priority(manager, mapper_id, processor, priority);\n      if (never_gc_event.exists())\n      {\n        bool *success;\n        derez.deserialize(success);\n        // If we succeed we can trigger immediately, otherwise we\n        // have to send back the response to fail\n        if (!manager->acquire_instance(REMOTE_DID_REF, NULL))\n        {\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(memory);\n            rez.serialize(success);\n            rez.serialize(never_gc_event);\n          }\n          runtime->send_never_gc_response(source, rez);\n        }\n        else\n          Runtime::trigger_event(never_gc_event);\n      }\n      // Remote our reference\n      if (manager->remove_base_resource_ref(MEMORY_MANAGER_REF))\n        delete manager;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::process_never_gc_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      bool *success;\n      derez.deserialize(success);\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      *success = false;\n      Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::process_acquire_request(Deserializer &derez,\n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      std::vector<std::pair<unsigned,PhysicalManager*> > successes;\n      size_t num_managers;\n      derez.deserialize(num_managers);\n      for (unsigned idx = 0; idx < num_managers; idx++)\n      {\n        DistributedID did;\n        derez.deserialize(did);\n        PhysicalManager *remote_manager; // remote pointer, never use!\n        derez.deserialize(remote_manager);\n        PhysicalManager *manager = NULL;\n        // Prevent changes until we can get a resource reference\n        {\n          AutoLock m_lock(manager_lock,1,false/*exclusive*/);\n          DistributedCollectable *dc = \n            runtime->weak_find_distributed_collectable(did);\n          if (dc != NULL)\n          {\n#ifdef DEBUG_LEGION\n            manager = dynamic_cast<PhysicalManager*>(dc);\n#else\n            manager = static_cast<PhysicalManager*>(dc);\n#endif\n            manager->add_base_resource_ref(MEMORY_MANAGER_REF);\n          }\n        }\n        if (manager == NULL)\n          continue;\n        // Otherwise try to acquire it locally\n        if (!manager->acquire_instance(REMOTE_DID_REF, NULL))\n        {\n          // Failed to acquire so this is not helpful\n          if (manager->remove_base_resource_ref(MEMORY_MANAGER_REF))\n            delete manager;\n        }\n        else // just remove our reference since we succeeded\n        {\n          successes.push_back(\n              std::pair<unsigned,PhysicalManager*>(idx, remote_manager));\n          manager->remove_base_resource_ref(MEMORY_MANAGER_REF);\n        }\n      }\n      std::vector<bool> *target;\n      derez.deserialize(target);\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      // See if we had any failures\n      if (!successes.empty())\n      {\n        // Send back the failures\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(target);\n          rez.serialize<size_t>(successes.size());\n          for (std::vector<std::pair<unsigned,PhysicalManager*> >::\n                const_iterator it = successes.begin(); \n                it != successes.end(); it++)\n          {\n            rez.serialize(it->first);\n            rez.serialize(it->second);\n          }\n          rez.serialize(to_trigger);\n        }\n        runtime->send_acquire_response(source, rez);\n      }\n      else // if everything failed, this easy, just trigger\n        Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::process_acquire_response(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      std::vector<bool> *target;\n      derez.deserialize(target);\n      size_t num_successes;\n      derez.deserialize(num_successes);\n      std::set<RtEvent> preconditions;\n      for (unsigned idx = 0; idx < num_successes; idx++)\n      {\n        unsigned index;\n        derez.deserialize(index);\n        (*target)[index] = true;\n        PhysicalManager *manager;\n        derez.deserialize(manager);\n        LocalReferenceMutator local_mutator;\n        manager->add_base_valid_ref(MAPPING_ACQUIRE_REF, &local_mutator);\n        const RtEvent reference_effects = local_mutator.get_done_event();\n        manager->send_remote_valid_decrement(source, NULL, reference_effects);\n        if (reference_effects.exists())\n          preconditions.insert(reference_effects);\n      }\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      if (!preconditions.empty())\n        Runtime::trigger_event(to_trigger,Runtime::merge_events(preconditions));\n      else\n        Runtime::trigger_event(to_trigger);\n    }\n    \n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_satisfying_instance(\n                                const LayoutConstraintSet &constraints,\n                                const std::vector<LogicalRegion> &regions,\n                                MappingInstance &result, bool acquire, \n                                bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (regions.empty())\n        return false;\n      std::deque<PhysicalManager*> candidates;\n      const RegionTreeID tree_id = regions[0].get_tree_id(); \n      do \n      {\n        // Hold the lock while iterating here\n        AutoLock m_lock(manager_lock, 1, false/*exclusive*/);\n        std::map<RegionTreeID,TreeInstances>::const_iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (TreeInstances::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          // Skip it if has already been collected\n          if (it->second.current_state == PENDING_COLLECTED_STATE)\n            continue;\n          it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n          candidates.push_back(it->first);\n        }\n      } while (false);\n      // If we have any candidates check their constraints\n      bool found = false;\n      if (!candidates.empty())\n      {\n        std::set<IndexSpaceExpression*> region_exprs;\n        RegionTreeForest *forest = runtime->forest;\n        for (std::vector<LogicalRegion>::const_iterator it = \n              regions.begin(); it != regions.end(); it++)\n        {\n          // If the region tree IDs don't match that is bad\n          if (tree_id != it->get_tree_id())\n            return false;\n          RegionNode *node = forest->get_node(*it);\n          region_exprs.insert(node->row_source);\n        }\n        IndexSpaceExpression *space_expr = (region_exprs.size() == 1) ?\n          *(region_exprs.begin()) : forest->union_index_spaces(region_exprs);\n        for (std::deque<PhysicalManager*>::const_iterator it = \n              candidates.begin(); it != candidates.end(); it++)\n        {\n          if (!(*it)->meets_expression(space_expr, tight_region_bounds))\n            continue;\n          if ((*it)->entails(constraints, DomainPoint(), NULL))\n          {\n            // Check to see if we need to acquire\n            // If we fail to acquire then keep going\n            if (acquire && !(*it)->acquire_instance(\n                    remote ? REMOTE_DID_REF : MAPPING_ACQUIRE_REF, NULL))\n              continue;\n            // If we make it here, we succeeded\n            result = MappingInstance(*it);\n            found = true;\n            break;\n          }\n        }\n        release_candidate_references(candidates);\n      }\n      return found;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_satisfying_instance(LayoutConstraints *constraints,\n                                      const std::vector<LogicalRegion> &regions,\n                                      MappingInstance &result, bool acquire, \n                                      bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (regions.empty())\n        return false;\n      std::deque<PhysicalManager*> candidates;\n      const RegionTreeID tree_id = regions[0].get_tree_id();\n      do\n      {\n        // Hold the lock while iterating here\n        AutoLock m_lock(manager_lock, 1, false/*exclusive*/);\n        std::map<RegionTreeID,TreeInstances>::const_iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (TreeInstances::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          // Skip it if has already been collected\n          if (it->second.current_state == PENDING_COLLECTED_STATE)\n            continue;\n          it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n          candidates.push_back(it->first);\n        }\n      } while (false);\n      // If we have any candidates check their constraints\n      bool found = false;\n      if (!candidates.empty())\n      {\n        std::set<IndexSpaceExpression*> region_exprs;\n        RegionTreeForest *forest = runtime->forest;\n        for (std::vector<LogicalRegion>::const_iterator it = \n              regions.begin(); it != regions.end(); it++)\n        {\n          // If the region tree IDs don't match that is bad\n          if (tree_id != it->get_tree_id())\n            return false;\n          RegionNode *node = forest->get_node(*it);\n          region_exprs.insert(node->row_source);\n        }\n        IndexSpaceExpression *space_expr = (region_exprs.size() == 1) ?\n          *(region_exprs.begin()) : forest->union_index_spaces(region_exprs);\n        for (std::deque<PhysicalManager*>::const_iterator it = \n              candidates.begin(); it != candidates.end(); it++)\n        {\n          if (!(*it)->meets_expression(space_expr, tight_region_bounds))\n            continue;\n          if ((*it)->entails(constraints, DomainPoint(), NULL))\n          {\n            // Check to see if we need to acquire\n            // If we fail to acquire then keep going\n            if (acquire && !(*it)->acquire_instance(\n                    remote ? REMOTE_DID_REF : MAPPING_ACQUIRE_REF, NULL))\n              continue;\n            // If we make it here, we succeeded\n            result = MappingInstance(*it);\n            found = true;\n            break;\n          }\n        }\n        release_candidate_references(candidates);\n      }\n      return found;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::find_satisfying_instances(\n                            const LayoutConstraintSet &constraints,\n                            const std::vector<LogicalRegion> &regions,\n                            std::vector<MappingInstance> &results, \n                            bool acquire, bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (regions.empty())\n        return;\n      std::deque<PhysicalManager*> candidates;\n      const RegionTreeID tree_id = regions[0].get_tree_id(); \n      do \n      {\n        // Hold the lock while iterating here\n        AutoLock m_lock(manager_lock, 1, false/*exclusive*/);\n        std::map<RegionTreeID,TreeInstances>::const_iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (TreeInstances::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          // Skip it if has already been collected\n          if (it->second.current_state == PENDING_COLLECTED_STATE)\n            continue;\n          it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n          candidates.push_back(it->first);\n        }\n      } while (false);\n      // If we have any candidates check their constraints\n      if (!candidates.empty())\n      {\n        std::set<IndexSpaceExpression*> region_exprs;\n        RegionTreeForest *forest = runtime->forest;\n        for (std::vector<LogicalRegion>::const_iterator it = \n              regions.begin(); it != regions.end(); it++)\n        {\n          // If the region tree IDs don't match that is bad\n          if (tree_id != it->get_tree_id())\n            return;\n          RegionNode *node = forest->get_node(*it);\n          region_exprs.insert(node->row_source);\n        }\n        IndexSpaceExpression *space_expr = (region_exprs.size() == 1) ?\n          *(region_exprs.begin()) : forest->union_index_spaces(region_exprs);\n        for (std::deque<PhysicalManager*>::const_iterator it = \n              candidates.begin(); it != candidates.end(); it++)\n        {\n          if (!(*it)->meets_expression(space_expr, tight_region_bounds))\n            continue;\n          if ((*it)->entails(constraints, DomainPoint(), NULL))\n          {\n            // Check to see if we need to acquire\n            // If we fail to acquire then keep going\n            if (acquire && !(*it)->acquire_instance(\n                    remote ? REMOTE_DID_REF : MAPPING_ACQUIRE_REF, NULL))\n              continue;\n            // If we make it here, we succeeded\n            results.push_back(MappingInstance(*it));\n          }\n        }\n        release_candidate_references(candidates);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::find_satisfying_instances(\n                            LayoutConstraints *constraints,\n                            const std::vector<LogicalRegion> &regions,\n                            std::vector<MappingInstance> &results, \n                            bool acquire, bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (regions.empty())\n        return;\n      std::deque<PhysicalManager*> candidates;\n      const RegionTreeID tree_id = regions[0].get_tree_id();\n      do\n      {\n        // Hold the lock while iterating here\n        AutoLock m_lock(manager_lock, 1, false/*exclusive*/);\n        std::map<RegionTreeID,TreeInstances>::const_iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (TreeInstances::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          // Skip it if has already been collected\n          if (it->second.current_state == PENDING_COLLECTED_STATE)\n            continue;\n          it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n          candidates.push_back(it->first);\n        }\n      } while (false);\n      // If we have any candidates check their constraints\n      if (!candidates.empty())\n      {\n        std::set<IndexSpaceExpression*> region_exprs;\n        RegionTreeForest *forest = runtime->forest;\n        for (std::vector<LogicalRegion>::const_iterator it = \n              regions.begin(); it != regions.end(); it++)\n        {\n          // If the region tree IDs don't match that is bad\n          if (tree_id != it->get_tree_id())\n            return;\n          RegionNode *node = forest->get_node(*it);\n          region_exprs.insert(node->row_source);\n        }\n        IndexSpaceExpression *space_expr = (region_exprs.size() == 1) ?\n          *(region_exprs.begin()) : forest->union_index_spaces(region_exprs);\n        for (std::deque<PhysicalManager*>::const_iterator it = \n              candidates.begin(); it != candidates.end(); it++)\n        {\n          if (!(*it)->meets_expression(space_expr, tight_region_bounds))\n            continue;\n          if ((*it)->entails(constraints, DomainPoint(), NULL))\n          {\n            // Check to see if we need to acquire\n            // If we fail to acquire then keep going\n            if (acquire && !(*it)->acquire_instance(\n                    remote ? REMOTE_DID_REF : MAPPING_ACQUIRE_REF, NULL))\n              continue;\n            // If we make it here, we succeeded\n            results.push_back(MappingInstance(*it));\n          }\n        }\n        release_candidate_references(candidates);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_valid_instance(\n                                     const LayoutConstraintSet &constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result, bool acquire, \n                                     bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (regions.empty())\n        return false;\n      std::deque<PhysicalManager*> candidates;\n      const RegionTreeID tree_id = regions[0].get_tree_id();\n      do\n      {\n        // Hold the lock while iterating here\n        AutoLock m_lock(manager_lock, 1, false/*exclusive*/);\n        std::map<RegionTreeID,TreeInstances>::const_iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (TreeInstances::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n\n          // Only consider ones that are currently valid\n          if (it->second.current_state != VALID_STATE)\n            continue;\n          it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n          candidates.push_back(it->first);\n        }\n      } while (false);\n      // If we have any candidates check their constraints\n      bool found = false;\n      if (!candidates.empty())\n      {\n        std::set<IndexSpaceExpression*> region_exprs;\n        RegionTreeForest *forest = runtime->forest;\n        for (std::vector<LogicalRegion>::const_iterator it = \n              regions.begin(); it != regions.end(); it++)\n        {\n          // If the region tree IDs don't match that is bad\n          if (tree_id != it->get_tree_id())\n            return false;\n          RegionNode *node = forest->get_node(*it);\n          region_exprs.insert(node->row_source);\n        }\n        IndexSpaceExpression *space_expr = (region_exprs.size() == 1) ?\n          *(region_exprs.begin()) : forest->union_index_spaces(region_exprs);\n        for (std::deque<PhysicalManager*>::const_iterator it = \n              candidates.begin(); it != candidates.end(); it++)\n        {\n          if (!(*it)->meets_expression(space_expr, tight_region_bounds))\n            continue;\n          if ((*it)->entails(constraints, DomainPoint(), NULL))\n          {\n            // Check to see if we need to acquire\n            // If we fail to acquire then keep going\n            if (acquire && !(*it)->acquire_instance(\n                    remote ? REMOTE_DID_REF : MAPPING_ACQUIRE_REF, NULL))\n              continue;\n            // If we make it here, we succeeded\n            result = MappingInstance(*it);\n            found = true;\n            break;\n          }\n        }\n        release_candidate_references(candidates);\n      }\n      return found;\n    }\n    \n    //--------------------------------------------------------------------------\n    bool MemoryManager::find_valid_instance(\n                                     LayoutConstraints *constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result, bool acquire, \n                                     bool tight_region_bounds, bool remote)\n    //--------------------------------------------------------------------------\n    {\n      if (regions.empty())\n        return false;\n      std::deque<PhysicalManager*> candidates;\n      const RegionTreeID tree_id = regions[0].get_tree_id();\n      do\n      {\n        // Hold the lock while iterating here\n        AutoLock m_lock(manager_lock, 1, false/*exclusive*/);\n        std::map<RegionTreeID,TreeInstances>::const_iterator finder = \n          current_instances.find(tree_id);\n        if (finder == current_instances.end())\n          break;\n        for (std::map<PhysicalManager*,InstanceInfo>::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          // Only consider ones that are currently valid\n          if (it->second.current_state != VALID_STATE)\n            continue;\n          it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n          candidates.push_back(it->first);\n        }\n      } while (false);\n      // If we have any candidates check their constraints\n      bool found = false;\n      if (!candidates.empty())\n      {\n        std::set<IndexSpaceExpression*> region_exprs;\n        RegionTreeForest *forest = runtime->forest;\n        for (std::vector<LogicalRegion>::const_iterator it = \n              regions.begin(); it != regions.end(); it++)\n        {\n          // If the region tree IDs don't match that is bad\n          if (tree_id != it->get_tree_id())\n            return false;\n          RegionNode *node = forest->get_node(*it);\n          region_exprs.insert(node->row_source);\n        }\n        IndexSpaceExpression *space_expr = (region_exprs.size() == 1) ?\n          *(region_exprs.begin()) : forest->union_index_spaces(region_exprs);\n        for (std::deque<PhysicalManager*>::const_iterator it = \n              candidates.begin(); it != candidates.end(); it++)\n        {\n          if (!(*it)->meets_expression(space_expr, tight_region_bounds))\n            continue;\n          if ((*it)->entails(constraints, DomainPoint(), NULL))\n          {\n            // Check to see if we need to acquire\n            // If we fail to acquire then keep going\n            if (acquire && !(*it)->acquire_instance(\n                    remote ? REMOTE_DID_REF : MAPPING_ACQUIRE_REF, NULL))\n              continue;\n            // If we make it here, we succeeded\n            result = MappingInstance(*it);\n            found = true;\n            break;\n          }\n        }\n        release_candidate_references(candidates);\n      }\n      return found;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::release_candidate_references(\n                           const std::deque<PhysicalManager*> &candidates) const\n    //--------------------------------------------------------------------------\n    {\n      for (std::deque<PhysicalManager*>::const_iterator it = \n            candidates.begin(); it != candidates.end(); it++)\n      {\n        if ((*it)->remove_base_resource_ref(MEMORY_MANAGER_REF))\n          delete (*it);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalManager* MemoryManager::create_shadow_instance(\n                                                       InstanceBuilder &builder)\n    //--------------------------------------------------------------------------\n    {\n      // Acquire allocation privilege before doing anything\n      const RtEvent wait_on = acquire_allocation_privilege();\n      if (wait_on.exists())\n        wait_on.wait();\n      // Try to make the result\n      PhysicalManager *manager = allocate_physical_instance(builder, \n          NULL/*footprint*/, NULL/*unsat kind*/, NULL/*unsat index*/);\n      // Release our allocation privilege after doing the record\n      release_allocation_privilege();\n      return manager;\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalManager* MemoryManager::create_unbound_instance(\n                                               LogicalRegion region,\n                                               LayoutConstraintSet &constraints,\n                                               ApEvent ready_event,\n                                               MapperID mapper_id,\n                                               Processor target_proc,\n                                               GCPriority priority)\n    //--------------------------------------------------------------------------\n    {\n      // We don't need to acquire allocation privilege as this function\n      // doesn't eagerly perform any instance collections.\n\n      RegionNode *node = runtime->forest->get_node(region);\n      FieldSpaceNode *fspace_node = node->get_column_source();\n\n      const std::vector<FieldID> &fields =\n        constraints.field_constraint.field_set;\n      FieldMask instance_mask;\n      std::vector<size_t> field_sizes(fields.size());\n      std::vector<unsigned> mask_index_map(fields.size());\n      std::vector<CustomSerdezID> serdez(fields.size());\n      fspace_node->compute_field_layout(\n          fields, field_sizes, mask_index_map, serdez, instance_mask);\n\n      LayoutDescription *layout =\n        fspace_node->find_layout_description(instance_mask, 1, constraints);\n      if (layout == NULL)\n      {\n        LayoutConstraints *internal_constraints =\n          runtime->register_layout(\n              fspace_node->handle, constraints, true/*internal*/);\n        layout = fspace_node->create_layout_description(\n            instance_mask, 1, internal_constraints,\n            mask_index_map, fields, field_sizes, serdez);\n      }\n\n      // Create an individual manager with a null instance\n      DistributedID did = runtime->get_available_distributed_id();\n      IndividualManager *manager =\n        new IndividualManager(runtime->forest, did,\n                              runtime->address_space,\n                              this,\n                              PhysicalInstance::NO_INST,\n                              node->get_row_source()->as_index_space_node(),\n                              NULL/*piece_list*/,\n                              0/*piece_list_size*/,\n                              fspace_node,\n                              region.get_tree_id(),\n                              layout,\n                              0/*redop id*/, true/*register now*/,\n                              -1U/*instance_footprint*/,\n                              ready_event,\n                              PhysicalManager::UNBOUND_INSTANCE_KIND,\n                              NULL/*op*/,\n                              false/*shadow_instance*/);\n\n      // Register the instance to make it visible to downstream tasks\n      record_created_instance(manager,\n                              true/*acquire*/,\n                              mapper_id,\n                              target_proc,\n                              priority,\n                              false/*remote*/,\n                              true/*eager*/);\n\n      return manager;\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent MemoryManager::acquire_allocation_privilege(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner); // should only happen on the owner\n#endif\n      const RtUserEvent our_event = Runtime::create_rt_user_event();\n      AutoLock m_lock(manager_lock);\n      // Wait for the previous allocation if there is one\n      const RtEvent wait_on = pending_allocation_attempts.empty() ? \n        RtEvent::NO_RT_EVENT : pending_allocation_attempts.back();\n      pending_allocation_attempts.push_back(our_event);\n      return wait_on;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::release_allocation_privilege(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner); // should only happen on the owner\n#endif\n      RtUserEvent to_trigger;\n      {\n        AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n        assert(!pending_allocation_attempts.empty());\n#endif\n        to_trigger = pending_allocation_attempts.front();\n        pending_allocation_attempts.pop_front();\n      }\n      Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalManager* MemoryManager::allocate_physical_instance(\n                        InstanceBuilder &builder, size_t *footprint,\n                        LayoutConstraintKind *unsat_kind, unsigned *unsat_index,\n                        CollectiveManager *collective, DomainPoint *point)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      // First, just try to make the instance as is, if it works we are done \n      size_t needed_size;\n      PhysicalManager *manager = builder.create_physical_instance(\n          runtime->forest,collective,point,unsat_kind,unsat_index,&needed_size);\n      if (footprint != NULL)\n        *footprint = needed_size;\n      if ((manager != NULL) || (needed_size == 0))\n        return manager;\n      // If that didn't work then we're going to try to delete some instances\n      // from this memory to make space. We do this in four separate passes:\n      // 1. Delete immediately collectable objects larger than what we need\n      // 2. Delete immediately collectable objects smaller than what we need\n      // 3. Delete deferred collectable objects larger than what we need\n      // 4. Delete deferred collectable objects smaller than what we need\n      // If we get through all these and still can't collect then we're screwed\n      // Keep trying to delete large collectable instances first\n      while (!delete_by_size_and_state(needed_size, COLLECTABLE_STATE, \n                                       true/*large only*/))\n      {\n        // See if we can make the instance\n        PhysicalManager *result = builder.create_physical_instance(\n            runtime->forest, collective, point, unsat_kind, unsat_index);\n        if (result != NULL)\n          return result;\n      }\n      // Then try deleting as many small collectable instances next\n      while (!delete_by_size_and_state(needed_size, COLLECTABLE_STATE,\n                                       false/*large only*/))\n      {\n        // See if we can make the instance\n        PhysicalManager *result = builder.create_physical_instance(\n            runtime->forest, collective, point, unsat_kind, unsat_index);\n        if (result != NULL)\n          return result;\n      }\n      // Now switch to large objects still in the active state\n      while (!delete_by_size_and_state(needed_size, ACTIVE_STATE,\n                                       true/*large only*/))\n      {\n        // See if we can make the instance\n        PhysicalManager *result = builder.create_physical_instance(\n            runtime->forest, collective, point, unsat_kind, unsat_index);\n        if (result != NULL)\n          return result;\n      }\n      // Finally switch to doing small objects in the active state\n      while (!delete_by_size_and_state(needed_size, ACTIVE_STATE,\n                                       false/*large only*/))\n      {\n        // See if we can make the instance\n        PhysicalManager *result = builder.create_physical_instance(\n            runtime->forest, collective, point, unsat_kind, unsat_index);\n        if (result != NULL)\n          return result;\n      }\n      // If we made it here well then we failed \n      return NULL;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::record_created_instance(PhysicalManager *manager,\n                           bool acquire, MapperID mapper_id, Processor p, \n                           GCPriority priority, bool remote, bool eager)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      // First do the insertion\n      // If we're going to add a valid reference, mark this valid early\n      // to avoid races with deletions\n      bool early_valid = acquire || (priority == LEGION_GC_NEVER_PRIORITY);\n      size_t instance_size = manager->get_instance_size();\n      bool external = manager->is_external_instance();\n      // Since we're going to put this in the table add a reference\n      manager->add_base_resource_ref(MEMORY_MANAGER_REF);\n      {\n        AutoLock m_lock(manager_lock);\n        TreeInstances &insts = current_instances[manager->tree_id];\n#ifdef DEBUG_LEGION\n        assert(insts.find(manager) == insts.end());\n#endif\n        InstanceInfo &info = insts[manager];\n        if (early_valid)\n          info.current_state = VALID_STATE;\n        info.min_priority = priority;\n        info.instance_size = instance_size;\n        info.external = external;\n        info.eager = eager;\n        info.mapper_priorities[\n          std::pair<MapperID,Processor>(mapper_id,p)] = priority;\n      }\n      // Now we can add any references that we need to\n      if (acquire)\n      {\n        if (remote)\n          manager->add_base_valid_ref(REMOTE_DID_REF);\n        else\n          manager->add_base_valid_ref(MAPPING_ACQUIRE_REF);\n      }\n      if (priority == LEGION_GC_NEVER_PRIORITY)\n        manager->add_base_valid_ref(NEVER_GC_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent MemoryManager::attach_external_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(manager->is_external_instance());\n#endif\n      if (!manager->is_owner())\n      {\n        // Send a message to the owner node to do the record\n        RtUserEvent result = Runtime::create_rt_user_event();\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(manager->did);\n          rez.serialize(result);\n        }\n        runtime->send_external_attach(manager->owner_space, rez);\n        return result;\n      }\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      // First do the insertion\n      // If we're going to add a valid reference, mark this valid early\n      // to avoid races with deletions\n      size_t instance_size = manager->get_instance_size();\n      // Since we're going to put this in the table add a reference\n      manager->add_base_resource_ref(MEMORY_MANAGER_REF);\n      {\n        AutoLock m_lock(manager_lock);\n        TreeInstances &insts = current_instances[manager->tree_id];\n#ifdef DEBUG_LEGION\n        assert(insts.find(manager) == insts.end());\n#endif\n        InstanceInfo &info = insts[manager];\n        info.instance_size = instance_size;\n      }\n      return RtEvent::NO_RT_EVENT;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::delete_by_size_and_state(const size_t needed_size,\n                                                 const InstanceState state,\n                                                 const bool larger_only,\n                                                 const bool eager)\n    //--------------------------------------------------------------------------\n    {\n      bool pass_complete = true;\n      size_t total_deleted = 0;\n      std::map<PhysicalManager*,RtEvent> to_delete;\n      {\n        AutoLock m_lock(manager_lock);\n        if (state == COLLECTABLE_STATE)\n        {\n          for (std::map<RegionTreeID,TreeInstances>::const_iterator cit = \n               current_instances.begin(); cit != current_instances.end(); cit++)\n          {\n            for (TreeInstances::const_iterator it = \n                  cit->second.begin(); it != cit->second.end(); it++)\n            {\n              if ((it->second.current_state != COLLECTABLE_STATE) ||\n                  it->second.external || (it->second.eager != eager))\n                continue;\n              const size_t inst_size = it->first->get_instance_size();\n              if ((inst_size >= needed_size) || !larger_only)\n              {\n                // Resource references will flow out\n                to_delete[it->first] = RtEvent::NO_RT_EVENT;\n                total_deleted += inst_size;\n                if (total_deleted >= needed_size)\n                {\n                  // If we exit early we are not done with this pass\n                  pass_complete = false;\n                  break;\n                }\n              }\n            }\n            if (!pass_complete)\n              break;\n          }\n          if (!to_delete.empty())\n          {\n            for (std::map<PhysicalManager*,RtEvent>::const_iterator it = \n                  to_delete.begin(); it != to_delete.end(); it++)\n            {\n              std::map<RegionTreeID,TreeInstances>::iterator finder = \n                current_instances.find(it->first->tree_id);\n#ifdef DEBUG_LEGION\n              assert(finder != current_instances.end());\n#endif\n              finder->second.erase(it->first);\n              if (finder->second.empty())\n                current_instances.erase(finder);\n            }\n          }\n        }\n        else\n        {\n#ifdef DEBUG_LEGION\n          assert(state == ACTIVE_STATE);\n#endif\n          for (std::map<RegionTreeID,TreeInstances>::iterator cit = \n               current_instances.begin(); cit != current_instances.end(); cit++)\n          {\n            for (TreeInstances::iterator it = \n                  cit->second.begin(); it != cit->second.end(); it++)\n            {\n              if ((it->second.current_state != ACTIVE_STATE) ||\n                  it->second.external || (it->second.eager != eager))\n                continue;\n              const size_t inst_size = it->first->get_instance_size();\n              if ((inst_size >= needed_size) || !larger_only)\n              {\n                RtUserEvent deferred_collect = Runtime::create_rt_user_event();\n                to_delete[it->first] = deferred_collect;\n                // Add our own reference here as this flows out\n                it->first->add_base_resource_ref(MEMORY_MANAGER_REF);\n                // Update the state information\n                it->second.current_state = PENDING_COLLECTED_STATE;\n                it->second.deferred_collect = deferred_collect;\n#ifdef LEGION_MALLOC_INSTANCES\n                pending_collectables[deferred_collect] = 0; \n#endif\n                total_deleted += inst_size;\n                if (total_deleted >= needed_size)\n                {\n                  // If we exit early we are not done with this pass\n                  pass_complete = false;\n                  break;\n                }\n              }\n            }\n            if (!pass_complete)\n              break;\n          }\n        }\n      }\n      // Now that we've release the lock we can do the deletions\n      // and remove any references that we are holding\n      if (!to_delete.empty())\n      {\n        for (std::map<PhysicalManager*,RtEvent>::const_iterator it = \n              to_delete.begin(); it != to_delete.end(); it++)\n        {\n          it->first->perform_deletion(it->second);\n          if (it->first->remove_base_resource_ref(MEMORY_MANAGER_REF))\n            delete it->first;\n        }\n      }\n      return pass_complete;\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent MemoryManager::detach_external_instance(PhysicalManager *manager)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(manager->is_external_instance());\n#endif\n      if (!manager->is_owner())\n      {\n        // Send a message to the owner node to do the deletion\n        RtUserEvent result = Runtime::create_rt_user_event();\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(memory);\n          rez.serialize(manager->did);\n          rez.serialize(result);\n        }\n        runtime->send_external_detach(manager->owner_space, rez);\n        return result;\n      }\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      // Either delete the instance now or do a deferred deltion\n      // that will delete the instance once all operations are\n      // done using it\n      RtEvent deferred_collect = RtEvent::NO_RT_EVENT;\n      {\n        AutoLock m_lock(manager_lock);\n        std::map<RegionTreeID,TreeInstances>::iterator tree_finder = \n          current_instances.find(manager->tree_id);\n#ifdef DEBUG_LEGION\n        assert(tree_finder != current_instances.end());\n#endif\n        std::map<PhysicalManager*,InstanceInfo>::iterator finder = \n          tree_finder->second.find(manager);\n#ifdef DEBUG_LEGION\n        assert(finder != tree_finder->second.end());\n        assert(finder->second.current_state != PENDING_COLLECTED_STATE);\n        assert(finder->second.current_state != PENDING_ACQUIRE_STATE);\n#endif\n        if (finder->second.current_state != COLLECTABLE_STATE)\n        {\n          finder->second.current_state = PENDING_COLLECTED_STATE;\n          finder->second.deferred_collect = Runtime::create_rt_user_event();\n          deferred_collect = finder->second.deferred_collect;\n          manager->add_base_resource_ref(MEMORY_MANAGER_REF);\n#ifdef LEGION_MALLOC_INSTANCES\n          pending_collectables[deferred_collect] = 0; \n#endif\n        }\n        else // Reference will flow out\n        {\n          tree_finder->second.erase(finder);\n          if (tree_finder->second.empty())\n            current_instances.erase(tree_finder);\n        }\n      }\n      // Perform the deletion contingent on references being removed\n      manager->perform_deletion(deferred_collect);\n      if (manager->remove_base_resource_ref(MEMORY_MANAGER_REF))\n        delete manager;\n      // No conditions on being done with this now\n      return RtEvent::NO_RT_EVENT;\n    }\n\n    //--------------------------------------------------------------------------\n    bool MemoryManager::is_visible_memory(Memory other)\n    //--------------------------------------------------------------------------\n    {\n      if (other == memory)\n        return true;\n      {\n        AutoLock m_lock(manager_lock,1,false);\n        if (!visible_memories.empty())\n          return (visible_memories.find(other) != visible_memories.end());\n      }\n      // Do the query while not holding the lock\n      Machine::MemoryQuery vis_mems(runtime->machine);\n      vis_mems.has_affinity_to(memory);\n      AutoLock m_lock(manager_lock);\n      if (visible_memories.empty())\n        for (Machine::MemoryQuery::iterator it = vis_mems.begin();\n              it != vis_mems.end(); it++)\n          visible_memories.insert(*it);\n      return (visible_memories.find(other) != visible_memories.end());\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent MemoryManager::create_eager_instance(\n               PhysicalInstance &instance, Realm::InstanceLayoutGeneric *layout)\n    //--------------------------------------------------------------------------\n    {\n      RtEvent wait_on(RtEvent::NO_EVENT);\n      instance = PhysicalInstance::NO_INST;\n      if (eager_allocator == NULL) \n        return wait_on;\n      AutoLock lock(manager_lock);\n      size_t allocation_id = next_allocation_id++;\n\n      bool allocated = false;\n      const size_t size = layout->bytes_used;\n      size_t offset = 0;\n\n      while (!allocated)\n      {\n        allocated = eager_allocator->allocate(\n            allocation_id, size, layout->alignment_reqd, offset);\n        if (allocated) break;\n        else {\n          lock.release();\n          if (delete_by_size_and_state(size,\n                                       COLLECTABLE_STATE,\n                                       false/*larger only*/,\n                                       true/*external*/))\n          {\n            lock.reacquire();\n            break;\n          }\n          lock.reacquire();\n        }\n      }\n\n      if (allocated)\n      {\n        eager_remaining_capacity -= size;\n        uintptr_t ptr = eager_pool + offset;\n        Realm::ProfilingRequestSet no_requests;\n        const Realm::ExternalMemoryResource resource(ptr, \n                    layout->bytes_used, false/*read only*/);\n        wait_on = RtEvent(Realm::RegionInstance::create_external_instance(\n              instance, memory, layout, resource, no_requests));\n#ifdef DEBUG_LEGION\n        assert(eager_allocations.find(ptr) == eager_allocations.end());\n#endif\n        eager_allocations[ptr] = allocation_id;\n        log_eager.debug(\"allocate instance \" IDFMT\n                        \" (%p+%zd, %zd) on memory \" IDFMT \", %zd bytes left\",\n                        instance.id,\n                        reinterpret_cast<void*>(eager_pool),\n                        offset,\n                        size,\n                        memory.id,\n                        eager_remaining_capacity);\n      }\n      else\n      {\n        log_eager.debug(\"failed to allocate an instance of size %zd on memory \"\n                        IDFMT \" (%zd bytes left)\",\n                        size, memory.id, eager_remaining_capacity);\n        if (runtime->dump_free_ranges)\n          eager_allocator->dump_all_free_ranges(log_eager);\n      }\n\n      return wait_on;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::free_eager_instance(\n                                       PhysicalInstance instance, RtEvent defer)\n    //--------------------------------------------------------------------------\n    {\n      if (defer.exists() && !defer.has_triggered())\n      {\n        log_eager.debug(\"defer deallocation of instance \" IDFMT\n                        \" on memory \" IDFMT \": wait for \" IDFMT,\n                        instance.id, memory.id, defer.id);\n        FreeEagerInstanceArgs args(this, instance);\n        runtime->issue_runtime_meta_task(args, LG_LOW_PRIORITY, defer);\n      }\n      else\n      {\n        // Technically realm could return us a null pointer here if the \n        // instance is not directly accessible on this node, but that\n        // should never happen because all eager allocations are done\n        // locally and to memories for which loads and stores are safe\n        void *base = instance.pointer_untyped(0,0);\n#ifdef DEBUG_LEGION\n        assert(base != NULL);\n#endif\n        const uintptr_t ptr = reinterpret_cast<uintptr_t>(base);\n        {\n          AutoLock lock(manager_lock);\n          std::map<uintptr_t,size_t>::iterator finder = \n            eager_allocations.find(ptr);\n#ifdef DEBUG_LEGION\n          assert(finder != eager_allocations.end());\n#endif\n          const size_t size = eager_allocator->get_size(finder->second);\n          eager_remaining_capacity += size;\n          eager_allocator->deallocate(finder->second);\n          eager_allocations.erase(finder);\n          log_eager.debug(\n            \"deallocate instance \" IDFMT \" of size %zd on memory \" IDFMT\n            \", %zd bytes left\", instance.id, size, memory.id,\n            eager_remaining_capacity);\n        }\n        instance.destroy();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void MemoryManager::handle_free_eager_instance(const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const FreeEagerInstanceArgs *fargs = (const FreeEagerInstanceArgs*)args;\n      fargs->manager->free_eager_instance(fargs->inst, RtEvent::NO_RT_EVENT);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::free_external_allocation(uintptr_t ptr, size_t size)\n    //--------------------------------------------------------------------------\n    {\n      switch (memory.kind())\n      {\n        case Memory::SYSTEM_MEM:\n        case Memory::SOCKET_MEM:\n          {\n            free((void*)ptr);\n            break;\n          }\n        case Memory::REGDMA_MEM:\n          {\n            munlock((void*)ptr, size);\n            free((void*)ptr);\n            break;\n          }\n#ifdef LEGION_USE_CUDA\n        case Memory::GPU_FB_MEM:\n          {\n            cuMemFree((CUdeviceptr)ptr);\n            break;\n          }\n        case Memory::Z_COPY_MEM:\n          {\n            cuMemFreeHost((void*)ptr);\n            break;\n          }\n#endif\n        default:\n          REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,\n              \"Unsupported memory kind %d\", memory.kind())\n      }\n    }\n\n#ifdef LEGION_MALLOC_INSTANCES\n    //--------------------------------------------------------------------------\n    uintptr_t MemoryManager::allocate_legion_instance(size_t footprint,\n                                                      bool needs_deferral)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n      assert(footprint > 0);\n#endif\n      uintptr_t result = 0;\n      switch (memory.kind())\n      {\n        case Memory::SYSTEM_MEM:\n        case Memory::SOCKET_MEM:\n          {\n            void *ptr = NULL;\n            if (posix_memalign(&ptr, 32/*alignment*/, footprint))\n              result = 0;\n            else\n              result = (uintptr_t)ptr;\n            break;\n          }\n        case Memory::REGDMA_MEM:\n          {\n            void *ptr = NULL;\n            if (posix_memalign(&ptr, 32/*alignment*/, footprint))\n              result = 0;\n            else\n              result = (uintptr_t)ptr;\n            mlock((void*)result, footprint);\n            break;\n          }\n#ifdef LEGION_USE_CUDA\n        case Memory::Z_COPY_MEM:\n        case Memory::GPU_FB_MEM:\n          {\n            if (needs_deferral)\n            {\n              MallocInstanceArgs args(this, footprint, &result);\n              const RtEvent wait_on = \n                runtime->issue_application_processor_task(args,\n                  LG_LATENCY_WORK_PRIORITY, local_gpu);\n              if (wait_on.exists() && !wait_on.has_triggered())\n                wait_on.wait();\n              return result;\n            }\n            else\n            {\n              // Use the driver API here to avoid the CUDA hijack\n              if (memory.kind() == Memory::GPU_FB_MEM)\n              {\n                CUdeviceptr ptr;\n                if (cuMemAlloc(&ptr, footprint) == CUDA_SUCCESS)\n                  result = (uintptr_t)ptr;\n                else\n                  result = 0;\n              }\n              else\n              {\n                void *ptr = NULL;\n                if (cuMemHostAlloc(&ptr, footprint, CU_MEMHOSTALLOC_PORTABLE |\n                      CU_MEMHOSTALLOC_DEVICEMAP) == CUDA_SUCCESS)\n                {\n                  result = (uintptr_t)ptr;\n                  // Check that the device pointer is the same as the host\n                  CUdeviceptr gpuptr;\n                  if (cuMemHostGetDevicePointer(&gpuptr,ptr,0) == CUDA_SUCCESS)\n                  {\n                    if (ptr != (void*)gpuptr)\n                      result = 0;\n                  }\n                  else\n                    result = 0;\n                }\n                else\n                  result = 0;\n              }\n            }\n            break;\n          }\n#endif\n        default:\n          REPORT_LEGION_FATAL(LEGION_FATAL_UNIMPLEMENTED_FEATURE,\n              \"Unsupported memory kind for LEGION_MALLOC_INSTANCES %d\",\n              memory.kind())\n      }\n      if (result > 0)\n      {\n        AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n        assert(allocations.find(result) == allocations.end());\n#endif\n        allocations[result] = footprint;\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::record_legion_instance(InstanceManager *man,uintptr_t p)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n      assert(legion_instances.find(man) == legion_instances.end());\n#endif\n      legion_instances[man] = p;\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::free_legion_instance(InstanceManager *man,RtEvent defer)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      uintptr_t ptr;\n      {\n        AutoLock m_lock(manager_lock);\n        std::map<InstanceManager*,uintptr_t>::iterator finder = \n          legion_instances.find(man);\n#ifdef DEBUG_LEGION\n        assert(finder != legion_instances.end());\n#endif\n        ptr = finder->second;\n        legion_instances.erase(finder);\n      }\n      free_legion_instance(defer, ptr);\n    }\n\n    //--------------------------------------------------------------------------\n    void MemoryManager::free_legion_instance(RtEvent defer, uintptr_t ptr,\n                                             bool needs_defer)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(is_owner);\n#endif\n      if (ptr == 0)\n        return;\n      size_t size;\n      {\n        AutoLock m_lock(manager_lock);\n        if (defer.exists() && !defer.has_triggered())\n        {\n          std::map<RtEvent,uintptr_t>::iterator finder = \n            pending_collectables.find(defer);\n          if (finder == pending_collectables.end())\n          {\n            FreeInstanceArgs args(this, ptr);\n#ifdef LEGION_USE_CUDA\n            if (local_gpu.exists())\n              runtime->issue_application_processor_task(args, LG_LOW_PRIORITY, \n                                                        local_gpu, defer);\n            else\n              runtime->issue_runtime_meta_task(args, LG_LOW_PRIORITY, defer);\n#else\n            runtime->issue_runtime_meta_task(args, LG_LOW_PRIORITY, defer);\n#endif\n          }\n          else\n            finder->second = ptr;\n          return;\n        }\n        std::map<uintptr_t,size_t>::iterator finder = allocations.find(ptr);\n#ifdef DEBUG_LEGION\n        assert(finder != allocations.end());\n#endif\n        size = finder->second;\n        allocations.erase(finder);\n      }\n#ifdef LEGION_USE_CUDA\n      if (needs_defer &&\n          ((memory.kind() == Z_COPY_MEM) || (memory.kind() == GPU_FB_MEM)))\n      {\n        // Put the allocation back in for when we go to look\n        // for it on the second pass\n        {\n          AutoLock m_lock(manager_lock);\n#ifdef DEBUG_LEGION\n          assert(allocations.find(ptr) == allocations.end());\n#endif\n          allocations[ptr] = size;\n        }\n        FreeInstanceArgs args(this, ptr);\n        runtime->issue_application_processor_task(args, LG_LOW_PRIORITY, \n                                                  local_gpu, defer);\n        return;\n      }\n#endif\n      free_external_allocation(ptr, size);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void MemoryManager::handle_malloc_instance(const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const MallocInstanceArgs *margs = (const MallocInstanceArgs*)args;\n      *(margs->ptr) = margs->manager->allocate_legion_instance(margs->size, \n                                                     false/*nneds defer*/);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void MemoryManager::handle_free_instance(const void *args)\n    //--------------------------------------------------------------------------\n    {\n      const FreeInstanceArgs *fargs = (const FreeInstanceArgs*)args;\n      fargs->manager->free_legion_instance(RtEvent::NO_RT_EVENT, fargs->ptr, \n                                                      false/*needs defer*/);\n    }\n#endif\n\n    /////////////////////////////////////////////////////////////\n    // Virtual Channel \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    VirtualChannel::VirtualChannel(VirtualChannelKind kind, \n        AddressSpaceID local_address_space, size_t max_message_size, \n        bool profile_outgoing, LegionProfiler *prof)\n      : sending_buffer((char*)malloc(max_message_size)), \n        sending_buffer_size(max_message_size), \n        ordered_channel((kind != DEFAULT_VIRTUAL_CHANNEL) &&\n                        (kind != THROUGHPUT_VIRTUAL_CHANNEL)), \n        profile_outgoing_messages(profile_outgoing),\n        request_priority((kind == THROUGHPUT_VIRTUAL_CHANNEL) ?\n            LG_THROUGHPUT_MESSAGE_PRIORITY : (kind == UPDATE_VIRTUAL_CHANNEL) ?\n            LG_LATENCY_DEFERRED_PRIORITY : LG_LATENCY_MESSAGE_PRIORITY),\n        response_priority((kind == THROUGHPUT_VIRTUAL_CHANNEL) ?\n            LG_THROUGHPUT_RESPONSE_PRIORITY : (kind == UPDATE_VIRTUAL_CHANNEL) ?\n            LG_LATENCY_MESSAGE_PRIORITY : LG_LATENCY_RESPONSE_PRIORITY),\n        partial_messages(0), observed_recent(true), profiler(prof)\n    //--------------------------------------------------------------------------\n    //\n    {\n      receiving_buffer_size = max_message_size;\n      receiving_buffer = (char*)legion_malloc(MESSAGE_BUFFER_ALLOC,\n                                              receiving_buffer_size);\n#ifdef DEBUG_LEGION\n      assert(sending_buffer != NULL);\n      assert(receiving_buffer != NULL);\n#endif\n      // Use a dummy implicit provenance at the front for the message\n      // to comply with the requirements of the meta-task handler which\n      // expects this before the task ID. We'll actually have individual\n      // implicit provenances that will override this when handling the\n      // messages so we can just set this to zero.\n      *((UniqueID*)sending_buffer) = 0;\n      sending_index = sizeof(UniqueID);\n      // Set up the buffer for sending the first batch of messages\n      // Only need to write the processor once\n      *((LgTaskID*)(((char*)sending_buffer)+sending_index))= LG_MESSAGE_ID;\n      sending_index += sizeof(LgTaskID);\n      *((AddressSpaceID*)\n          (((char*)sending_buffer)+sending_index)) = local_address_space;\n      sending_index += sizeof(local_address_space);\n      *((VirtualChannelKind*)\n          (((char*)sending_buffer)+sending_index)) = kind;\n      sending_index += sizeof(kind);\n      header = FULL_MESSAGE;\n      sending_index += sizeof(header);\n      packaged_messages = 0;\n      sending_index += sizeof(packaged_messages);\n      last_message_event = RtEvent::NO_RT_EVENT;\n      partial_message_id = 0;\n      partial_assembly = NULL;\n      partial = false;\n      // Set up the receiving buffer\n      received_messages = 0;\n      receiving_index = 0;\n    }\n\n    //--------------------------------------------------------------------------\n    VirtualChannel::VirtualChannel(const VirtualChannel &rhs)\n      : sending_buffer(NULL), sending_buffer_size(0), \n        ordered_channel(false), profile_outgoing_messages(false),\n        request_priority(rhs.request_priority),\n        response_priority(rhs.response_priority), profiler(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    VirtualChannel::~VirtualChannel(void)\n    //--------------------------------------------------------------------------\n    {\n      free(sending_buffer);\n      free(receiving_buffer);\n      receiving_buffer = NULL;\n      receiving_buffer_size = 0;\n      if (partial_assembly != NULL)\n        delete partial_assembly;\n    }\n\n    //--------------------------------------------------------------------------\n    void VirtualChannel::package_message(Serializer &rez, MessageKind k,\n                         bool flush, Runtime *runtime, Processor target, \n                         bool response, bool shutdown)\n    //--------------------------------------------------------------------------\n    {\n      // First check to see if the message fits in the current buffer    \n      // including the overhead for the message: kind and size\n      size_t buffer_size = rez.get_used_bytes();\n      const char *buffer = (const char*)rez.get_buffer();\n      const size_t header_size = \n        sizeof(k) + sizeof(implicit_provenance) + sizeof(buffer_size);\n      // Need to hold the lock when manipulating the buffer\n      AutoLock c_lock(channel_lock);\n      if ((sending_index+header_size+buffer_size) > sending_buffer_size)\n      {\n        // Make sure we can at least get the meta-data into the buffer\n        // Since there is no partial data we can fake the flush\n        if ((sending_buffer_size - sending_index) <= header_size)\n          send_message(true/*complete*/, runtime, target, k, response,shutdown);\n        // Now can package up the meta data\n        packaged_messages++;\n        *((MessageKind*)(sending_buffer+sending_index)) = k;\n        sending_index += sizeof(k);\n        *((UniqueID*)(sending_buffer+sending_index)) = implicit_provenance;\n        sending_index += sizeof(implicit_provenance);\n        *((size_t*)(sending_buffer+sending_index)) = buffer_size;\n        sending_index += sizeof(buffer_size);\n        while (buffer_size > 0)\n        {\n          unsigned remaining = sending_buffer_size - sending_index;\n          if (remaining == 0)\n            send_message(false/*complete*/, runtime, \n                         target, k, response, shutdown);\n          remaining = sending_buffer_size - sending_index;\n#ifdef DEBUG_LEGION\n          assert(remaining > 0); // should be space after the send\n#endif\n          // Figure out how much to copy into the buffer\n          unsigned to_copy = (remaining < buffer_size) ? \n                                            remaining : buffer_size;\n          memcpy(sending_buffer+sending_index,buffer,to_copy);\n          buffer_size -= to_copy;\n          buffer += to_copy;\n          sending_index += to_copy;\n        } \n      }\n      else\n      {\n        packaged_messages++;\n        // Package up the kind and the size first\n        *((MessageKind*)(sending_buffer+sending_index)) = k;\n        sending_index += sizeof(k);\n        *((UniqueID*)(sending_buffer+sending_index)) = implicit_provenance;\n        sending_index += sizeof(implicit_provenance);\n        *((size_t*)(sending_buffer+sending_index)) = buffer_size;\n        sending_index += sizeof(buffer_size);\n        // Then copy over the buffer\n        memcpy(sending_buffer+sending_index,buffer,buffer_size); \n        sending_index += buffer_size;\n      }\n      if (flush)\n        send_message(true/*complete*/, runtime, target, k, response, shutdown);\n    }\n\n    //--------------------------------------------------------------------------\n    void VirtualChannel::send_message(bool complete, Runtime *runtime,\n               Processor target, MessageKind kind, bool response, bool shutdown)\n    //--------------------------------------------------------------------------\n    {\n      // See if we need to switch the header file\n      // and update the state of partial\n      bool first_partial = false;\n      if (!complete)\n      {\n        header = PARTIAL_MESSAGE;\n        // If this is an unordered virtual channel, then embed our partial\n        // message id in the high-order bits\n        if (!ordered_channel)\n          header = (MessageHeader)\n            (((unsigned)header) | (partial_message_id << 2));\n        if (!partial)\n        {\n          partial = true;\n          first_partial = true;\n        }\n      }\n      else if (partial)\n      {\n        header = FINAL_MESSAGE;\n        // If this is an unordered virtual channel, then embed our partial\n        // message id in the high-order bits\n        if (!ordered_channel)\n          // Also increment the partial message id for the next message\n          // This can overflow safely since it's an unsigned integer\n          header = (MessageHeader)\n            (((unsigned)header) | (partial_message_id++ << 2));\n        partial = false;\n      }\n      // Save the header and the number of messages into the buffer\n      const size_t base_size = sizeof(UniqueID) + sizeof(LgTaskID) + \n        sizeof(AddressSpaceID) + sizeof(VirtualChannelKind);\n      *((MessageHeader*)(sending_buffer + base_size)) = header;\n      *((unsigned*)(sending_buffer + base_size + sizeof(header))) = \n                                                            packaged_messages;\n      // Send the message directly there, don't go through the\n      // runtime interface to avoid being counted, still include\n      // a profiling request though if necessary in order to \n      // see waits on message handlers\n      // Note that we don't profile on shutdown messages or we would \n      // never actually finish running\n      if (profile_outgoing_messages && !shutdown)\n      {\n        Realm::ProfilingRequestSet requests;\n        LegionProfiler::add_message_request(requests, kind, target);\n        last_message_event = RtEvent(target.spawn(\n#ifdef LEGION_SEPARATE_META_TASKS\n              LG_TASK_ID + LG_MESSAGE_ID + kind,\n#else\n              LG_TASK_ID, \n#endif\n              sending_buffer, sending_index, requests, \n              (ordered_channel || \n               ((header != FULL_MESSAGE) && !first_partial)) ?\n               last_message_event : RtEvent::NO_RT_EVENT, \n              response ? response_priority : request_priority));\n        if (!ordered_channel && (header != PARTIAL_MESSAGE))\n        {\n          unordered_events.insert(last_message_event);\n          if (unordered_events.size() >= MAX_UNORDERED_EVENTS)\n            filter_unordered_events();\n        }\n      }\n      else\n      {\n        last_message_event = RtEvent(target.spawn(\n#ifdef LEGION_SEPARATE_META_TASKS\n                LG_TASK_ID + LG_MESSAGE_ID + kind,\n#else\n                LG_TASK_ID, \n#endif\n                sending_buffer, sending_index, \n                (ordered_channel || \n                 ((header != FULL_MESSAGE) && !first_partial)) ?\n                  last_message_event : RtEvent::NO_RT_EVENT, \n                response ? response_priority : request_priority));\n        if (!ordered_channel && (header != PARTIAL_MESSAGE))\n        {\n          unordered_events.insert(last_message_event);\n          if (unordered_events.size() >= MAX_UNORDERED_EVENTS)\n            filter_unordered_events();\n        }\n      }\n      // Reset the state of the buffer\n      sending_index = base_size + sizeof(header) + sizeof(unsigned);\n      if (partial)\n        header = PARTIAL_MESSAGE;\n      else\n        header = FULL_MESSAGE;\n      packaged_messages = 0;\n    }\n\n    //--------------------------------------------------------------------------\n    void VirtualChannel::filter_unordered_events(void)\n    //--------------------------------------------------------------------------\n    {\n      // Lock held from caller\n#ifdef DEBUG_LEGION\n      assert(!ordered_channel);\n      assert(unordered_events.size() >= MAX_UNORDERED_EVENTS);\n#endif\n      // Prune out any triggered events\n      for (std::set<RtEvent>::iterator it = unordered_events.begin();\n            it != unordered_events.end(); /*nothing*/)\n      {\n        if (it->has_triggered())\n        {\n          std::set<RtEvent>::iterator to_delete = it++;\n          unordered_events.erase(to_delete);\n        }\n        else\n          it++;\n      }\n      // If we still have too many events, collapse them down\n      if (unordered_events.size() >= MAX_UNORDERED_EVENTS)\n      {\n        const RtEvent summary = Runtime::merge_events(unordered_events);\n        unordered_events.clear();\n        unordered_events.insert(summary);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void VirtualChannel::confirm_shutdown(ShutdownManager *shutdown_manager,\n                                          bool phase_one)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(channel_lock);\n      if (phase_one)\n      {\n        if (packaged_messages > 0)\n          shutdown_manager->record_recent_message();\n        if (ordered_channel)\n        {\n          if (!last_message_event.has_triggered())\n          {\n            // Subscribe to make sure we see this trigger\n            last_message_event.subscribe();\n            // A little hack here for slow gasnet conduits\n            // If the event didn't trigger yet, make sure its just\n            // because we haven't gotten the return message yet\n            usleep(1000);\n            if (!last_message_event.has_triggered())\n              shutdown_manager->record_pending_message(last_message_event);\n            else\n              observed_recent = false;\n          }\n          else\n            observed_recent = false;\n        }\n        else\n        {\n          observed_recent = false;\n          for (std::set<RtEvent>::const_iterator it = \n                unordered_events.begin(); it != unordered_events.end(); it++)\n          {\n            if (!it->has_triggered())\n            {\n              // Subscribe to make sure we see this trigger\n              it->subscribe();\n              // A little hack here for slow gasnet conduits\n              // If the event didn't trigger yet, make sure its just\n              // because we haven't gotten the return message yet\n              usleep(1000);\n              if (!it->has_triggered())\n              {\n                shutdown_manager->record_pending_message(*it); \n                observed_recent = true;\n                break;\n              }\n            }\n          }\n        }\n      }\n      else\n      {\n        if (observed_recent || (packaged_messages > 0)) \n          shutdown_manager->record_recent_message(); \n        else\n        {\n          if (ordered_channel)\n          {\n            if (!last_message_event.has_triggered())\n            {\n              // Subscribe to make sure we see this trigger\n              last_message_event.subscribe();\n              // A little hack here for slow gasnet conduits\n              // If the event didn't trigger yet, make sure its just\n              // because we haven't gotten the return message yet\n              usleep(1000);\n              if (!last_message_event.has_triggered())\n                shutdown_manager->record_recent_message();\n            }\n          }\n          else\n          {\n            for (std::set<RtEvent>::const_iterator it = \n                  unordered_events.begin(); it != unordered_events.end(); it++)\n            {\n              if (!it->has_triggered())\n              {\n                // Subscribe to make sure we see this trigger\n                it->subscribe();\n                // A little hack here for slow gasnet conduits\n                // If the event didn't trigger yet, make sure its just\n                // because we haven't gotten the return message yet\n                usleep(1000);\n                if (!it->has_triggered())\n                {\n                  shutdown_manager->record_recent_message();\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void VirtualChannel::process_message(const void *args, size_t arglen,\n                         Runtime *runtime, AddressSpaceID remote_address_space)\n    //--------------------------------------------------------------------------\n    {\n      // If we have a profiler we need to increment our requests count\n      if (profiler != NULL)\n#ifdef DEBUG_LEGION\n        profiler->increment_total_outstanding_requests(\n                    LegionProfiler::LEGION_PROF_MESSAGE);\n#else\n        profiler->increment_total_outstanding_requests();\n#endif\n      // Strip off our header and the number of messages, the \n      // processor part was already stipped off by the Legion runtime\n      const char *buffer = (const char*)args;\n      MessageHeader head = *((const MessageHeader*)buffer);\n      buffer += sizeof(head);\n      arglen -= sizeof(head);\n      unsigned num_messages = *((const unsigned*)buffer);\n      buffer += sizeof(num_messages);\n      arglen -= sizeof(num_messages);\n      unsigned incoming_message_id = 0;\n      if (!ordered_channel)\n      {\n        incoming_message_id = ((unsigned)head) >> 2; \n        head = (MessageHeader)(((unsigned)head) & 0x3);\n      }\n      switch (head)\n      {\n        case FULL_MESSAGE:\n          {\n            // Can handle these messages directly\n            if (handle_messages(num_messages, runtime, \n                                remote_address_space, buffer, arglen) &&\n                // If we had a shutdown message and a profiler then we\n                // shouldn't have incremented the outstanding profiling\n                // count because we don't actually do profiling requests\n                // on any shutdown messages\n                (profiler != NULL))\n            {\n#ifdef DEBUG_LEGION\n              profiler->decrement_total_outstanding_requests(\n                          LegionProfiler::LEGION_PROF_MESSAGE);\n#else\n              profiler->decrement_total_outstanding_requests();\n#endif\n            }\n            break;\n          }\n        case PARTIAL_MESSAGE:\n          {\n            // Save these messages onto the receiving buffer\n            // but do not handle them\n            if (!ordered_channel)\n            {\n              AutoLock c_lock(channel_lock);\n              if (partial_assembly == NULL)\n                partial_assembly = new std::map<unsigned,PartialMessage>();\n              PartialMessage &message = \n                (*partial_assembly)[incoming_message_id];\n              // Allocate the buffer on the first pass\n              if (message.buffer == NULL)\n              {\n                // Same as max message size\n                message.size = sending_buffer_size;\n                message.buffer = \n                  (char*)legion_malloc(MESSAGE_BUFFER_ALLOC, message.size);\n              }\n              buffer_messages(num_messages, buffer, arglen,\n                              message.buffer, message.size,\n                              message.index, message.messages, message.total);\n            }\n            else\n              // Ordered channels don't need the lock\n              buffer_messages(num_messages, buffer, arglen, receiving_buffer, \n                              receiving_buffer_size, receiving_index, \n                              received_messages, partial_messages);\n            break;\n          }\n        case FINAL_MESSAGE:\n          {\n            // Save the remaining messages onto the receiving\n            // buffer, then handle them and reset the state.\n            char *final_buffer = NULL;\n            size_t final_index = 0;\n            unsigned final_messages = 0, final_total = 0;\n            bool free_buffer = false;\n            if (!ordered_channel)\n            {\n              AutoLock c_lock(channel_lock);\n#ifdef DEBUG_LEGION\n              assert(partial_assembly != NULL);\n#endif\n              std::map<unsigned,PartialMessage>::iterator finder = \n                partial_assembly->find(incoming_message_id);\n#ifdef DEBUG_LEGION\n              assert(finder != partial_assembly->end());\n              assert(finder->second.buffer != NULL);\n#endif\n              buffer_messages(num_messages, buffer, arglen,\n                              finder->second.buffer, finder->second.size,\n                              finder->second.index, finder->second.messages,\n                              finder->second.total);\n              final_index = finder->second.index;\n              final_buffer = finder->second.buffer;\n              final_messages = finder->second.messages;\n              final_total = finder->second.total;\n              free_buffer = true;\n              partial_assembly->erase(finder);\n            }\n            else\n            {\n              buffer_messages(num_messages, buffer, arglen, receiving_buffer,\n                              receiving_buffer_size, receiving_index, \n                              received_messages, partial_messages);\n              final_index = receiving_index;\n              final_buffer = receiving_buffer;\n              final_messages = received_messages;\n              final_total = partial_messages;\n              receiving_index = 0;\n              received_messages = 0;\n              partial_messages = 0;\n            }\n            if (handle_messages(final_messages, runtime, remote_address_space,\n                                final_buffer, final_index) &&\n                // If we had a shutdown message and a profiler then we\n                // shouldn't have incremented the outstanding profiling\n                // count because we don't actually do profiling requests\n                // on any shutdown messages\n                (profiler != NULL))\n            {\n#ifdef DEBUG_LEGION\n              profiler->decrement_total_outstanding_requests(\n                          LegionProfiler::LEGION_PROF_MESSAGE, final_total);\n#else\n              profiler->decrement_total_outstanding_requests(final_total);\n#endif\n            }\n            if (free_buffer)\n              free(final_buffer);\n            break;\n          }\n        default:\n          assert(false); // should never get here\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool VirtualChannel::handle_messages(unsigned num_messages,\n                                         Runtime *runtime,\n                                         AddressSpaceID remote_address_space,\n                                         const char *args, size_t arglen) const\n    //--------------------------------------------------------------------------\n    {\n      bool has_shutdown = false;\n      for (unsigned idx = 0; idx < num_messages; idx++)\n      {\n        // Pull off the message kind and the size of the message\n#ifdef DEBUG_LEGION\n        assert(arglen >= (sizeof(MessageKind)+sizeof(size_t)));\n#endif\n        MessageKind kind = *((const MessageKind*)args);\n        // Any message that is not a shutdown message needs to be recorded\n        if (!observed_recent && (kind != SEND_SHUTDOWN_NOTIFICATION) &&\n            (kind != SEND_SHUTDOWN_RESPONSE))\n          observed_recent = true;\n        args += sizeof(kind);\n        arglen -= sizeof(kind);\n        implicit_provenance = *((const UniqueID*)args);\n        args += sizeof(implicit_provenance);\n        arglen -= sizeof(implicit_provenance);\n        size_t message_size = *((const size_t*)args);\n        args += sizeof(message_size);\n        arglen -= sizeof(message_size);\n#ifdef DEBUG_LEGION\n        if (idx == (num_messages-1))\n          assert(message_size == arglen);\n#endif\n        // Build the deserializer\n        Deserializer derez(args,message_size);\n        switch (kind)\n        {\n          case TASK_MESSAGE:\n            {\n              runtime->handle_task(derez);\n              break;\n            }\n          case STEAL_MESSAGE:\n            {\n              runtime->handle_steal(derez);\n              break;\n            }\n          case ADVERTISEMENT_MESSAGE:\n            {\n              runtime->handle_advertisement(derez);\n              break;\n            }\n#ifdef LEGION_USE_LIBDL\n          case SEND_REGISTRATION_CALLBACK:\n            {\n              runtime->handle_registration_callback(derez);\n              break;\n            }\n#endif\n          case SEND_REMOTE_TASK_REPLAY:\n            {\n              runtime->handle_remote_task_replay(derez);\n              break;\n            }\n          case SEND_REMOTE_TASK_PROFILING_RESPONSE:\n            {\n              runtime->handle_remote_task_profiling_response(derez);\n              break;\n            }\n          case SEND_SHARED_OWNERSHIP:\n            {\n              runtime->handle_shared_ownership(derez);\n              break;\n            }\n          case SEND_INDEX_SPACE_NODE:\n            {\n              runtime->handle_index_space_node(derez, remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_REQUEST:\n            {\n              runtime->handle_index_space_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_RETURN:\n            {\n              runtime->handle_index_space_return(derez);\n              break;\n            }\n          case SEND_INDEX_SPACE_SET:\n            {\n              runtime->handle_index_space_set(derez, remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_CHILD_REQUEST:\n            {\n              runtime->handle_index_space_child_request(derez, \n                                                        remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_CHILD_RESPONSE:\n            {\n              runtime->handle_index_space_child_response(derez);\n              break;\n            }\n          case SEND_INDEX_SPACE_COLORS_REQUEST:\n            {\n              runtime->handle_index_space_colors_request(derez,\n                                                         remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_COLORS_RESPONSE:\n            {\n              runtime->handle_index_space_colors_response(derez);\n              break;\n            }\n          case SEND_INDEX_SPACE_REMOTE_EXPRESSION_REQUEST:\n            {\n              runtime->handle_index_space_remote_expression_request(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_REMOTE_EXPRESSION_RESPONSE:\n            {\n              runtime->handle_index_space_remote_expression_response(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_REMOTE_EXPRESSION_INVALIDATION:\n            {\n              runtime->handle_index_space_remote_expression_invalidation(derez);\n              break;\n            }\n          case SEND_INDEX_SPACE_GENERATE_COLOR_REQUEST:\n            {\n              runtime->handle_index_space_generate_color_request(derez,\n                                                  remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_GENERATE_COLOR_RESPONSE:\n            {\n              runtime->handle_index_space_generate_color_response(derez);\n              break;\n            }\n          case SEND_INDEX_SPACE_RELEASE_COLOR:\n            {\n              runtime->handle_index_space_release_color(derez);\n              break;\n            }\n          case SEND_INDEX_PARTITION_NOTIFICATION:\n            {\n              runtime->handle_index_partition_notification(derez);\n              break;\n            }\n          case SEND_INDEX_PARTITION_NODE:\n            {\n              runtime->handle_index_partition_node(derez, remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_REQUEST:\n            {\n              runtime->handle_index_partition_request(derez, \n                                                      remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_RETURN:\n            {\n              runtime->handle_index_partition_return(derez);\n              break;\n            }\n          case SEND_INDEX_PARTITION_CHILD_REQUEST:\n            {\n              runtime->handle_index_partition_child_request(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_CHILD_RESPONSE:\n            {\n              runtime->handle_index_partition_child_response(derez);\n              break;\n            }\n          case SEND_INDEX_PARTITION_DISJOINT_UPDATE:\n            {\n              runtime->handle_index_partition_disjoint_update(derez);\n              break;\n            }\n          case SEND_INDEX_PARTITION_SHARD_RECTS_REQUEST:\n            {\n              runtime->handle_index_partition_shard_rects_request(derez);\n              break;\n            }\n          case SEND_INDEX_PARTITION_SHARD_RECTS_RESPONSE:\n            {\n              runtime->handle_index_partition_shard_rects_response(derez,\n                                                    remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_REMOTE_INTERFERENCE_REQUEST:\n            {\n              runtime->handle_index_partition_remote_interference_request(\n                                              derez, remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_REMOTE_INTERFERENCE_RESPONSE:\n            {\n              runtime->handle_index_partition_remote_interference_response(\n                                                                    derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_NODE:\n            {\n              runtime->handle_field_space_node(derez, remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SPACE_REQUEST:\n            {\n              runtime->handle_field_space_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SPACE_RETURN:\n            {\n              runtime->handle_field_space_return(derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_ALLOCATOR_REQUEST:\n            {\n              runtime->handle_field_space_allocator_request(derez,\n                                            remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SPACE_ALLOCATOR_RESPONSE:\n            {\n              runtime->handle_field_space_allocator_response(derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_ALLOCATOR_INVALIDATION:\n            {\n              runtime->handle_field_space_allocator_invalidation(derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_ALLOCATOR_FLUSH:\n            {\n              runtime->handle_field_space_allocator_flush(derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_ALLOCATOR_FREE:\n            {\n              runtime->handle_field_space_allocator_free(derez, \n                                          remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SPACE_INFOS_REQUEST:\n            {\n              runtime->handle_field_space_infos_request(derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_INFOS_RESPONSE:\n            {\n              runtime->handle_field_space_infos_response(derez);\n              break;\n            }\n          case SEND_FIELD_ALLOC_REQUEST:\n            {\n              runtime->handle_field_alloc_request(derez);\n              break;\n            }\n          case SEND_FIELD_SIZE_UPDATE:\n            {\n              runtime->handle_field_size_update(derez, remote_address_space);\n              break;\n            }\n          case SEND_FIELD_FREE:\n            {\n              runtime->handle_field_free(derez, remote_address_space);\n              break;\n            }\n          case SEND_FIELD_FREE_INDEXES:\n            {\n              runtime->handle_field_free_indexes(derez);\n              break;\n            }\n          case SEND_FIELD_SPACE_LAYOUT_INVALIDATION:\n            {\n              runtime->handle_field_space_layout_invalidation(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_LOCAL_FIELD_ALLOC_REQUEST:\n            {\n              runtime->handle_local_field_alloc_request(derez, \n                                                        remote_address_space);\n              break;\n            }\n          case SEND_LOCAL_FIELD_ALLOC_RESPONSE:\n            {\n              runtime->handle_local_field_alloc_response(derez);\n              break;\n            }\n          case SEND_LOCAL_FIELD_FREE:\n            {\n              runtime->handle_local_field_free(derez);\n              break;\n            }\n          case SEND_LOCAL_FIELD_UPDATE:\n            {\n              runtime->handle_local_field_update(derez);\n              break;\n            }\n          case SEND_TOP_LEVEL_REGION_REQUEST:\n            {\n              runtime->handle_top_level_region_request(derez, \n                                                       remote_address_space);\n              break;\n            }\n          case SEND_TOP_LEVEL_REGION_RETURN:\n            {\n              runtime->handle_top_level_region_return(derez);\n              break;\n            }\n          case SEND_LOGICAL_REGION_NODE:\n            {\n              runtime->handle_logical_region_node(derez, remote_address_space);\n              break;\n            }\n          case INDEX_SPACE_DESTRUCTION_MESSAGE:\n            {\n              runtime->handle_index_space_destruction(derez); \n              break;\n            }\n          case INDEX_PARTITION_DESTRUCTION_MESSAGE:\n            {\n              runtime->handle_index_partition_destruction(derez); \n              break;\n            }\n          case FIELD_SPACE_DESTRUCTION_MESSAGE:\n            {\n              runtime->handle_field_space_destruction(derez); \n              break;\n            }\n          case LOGICAL_REGION_DESTRUCTION_MESSAGE:\n            {\n              runtime->handle_logical_region_destruction(derez); \n              break;\n            }\n          case INDIVIDUAL_REMOTE_COMPLETE:\n            {\n              runtime->handle_individual_remote_complete(derez);\n              break;\n            }\n          case INDIVIDUAL_REMOTE_COMMIT:\n            {\n              runtime->handle_individual_remote_commit(derez);\n              break;\n            }\n          case SLICE_REMOTE_MAPPED:\n            {\n              runtime->handle_slice_remote_mapped(derez, remote_address_space);\n              break;\n            }\n          case SLICE_REMOTE_COMPLETE:\n            {\n              runtime->handle_slice_remote_complete(derez);\n              break;\n            }\n          case SLICE_REMOTE_COMMIT:\n            {\n              runtime->handle_slice_remote_commit(derez);\n              break;\n            }\n          case SLICE_FIND_INTRA_DEP:\n            {\n              runtime->handle_slice_find_intra_dependence(derez);\n              break;\n            }\n          case SLICE_RECORD_INTRA_DEP:\n            {\n              runtime->handle_slice_record_intra_dependence(derez);\n              break;\n            }\n          case SLICE_COLLECTIVE_REQUEST:\n            {\n              runtime->handle_slice_collective_request(derez,\n                                                       remote_address_space);\n              break;\n            }\n          case SLICE_COLLECTIVE_RESPONSE:\n            {\n              runtime->handle_slice_collective_response(derez);\n              break;\n            }\n          case DISTRIBUTED_REMOTE_REGISTRATION:\n            {\n              runtime->handle_did_remote_registration(derez, \n                                                      remote_address_space);\n              break;\n            }\n          case DISTRIBUTED_VALID_UPDATE:\n            {\n              runtime->handle_did_remote_valid_update(derez);\n              break;\n            }\n          case DISTRIBUTED_GC_UPDATE:\n            {\n              runtime->handle_did_remote_gc_update(derez); \n              break;\n            }\n          case DISTRIBUTED_CREATE_ADD:\n            {\n              runtime->handle_did_create_add(derez);\n              break;\n            }\n          case DISTRIBUTED_CREATE_REMOVE:\n            {\n              runtime->handle_did_create_remove(derez);\n              break;\n            }\n          case DISTRIBUTED_UNREGISTER:\n            {\n              runtime->handle_did_remote_unregister(derez);\n              break;\n            }\n          case SEND_ATOMIC_RESERVATION_REQUEST:\n            {\n              runtime->handle_send_atomic_reservation_request(derez,\n                                                      remote_address_space);\n              break;\n            }\n          case SEND_ATOMIC_RESERVATION_RESPONSE:\n            {\n              runtime->handle_send_atomic_reservation_response(derez);\n              break;\n            }\n          case SEND_CREATED_REGION_CONTEXTS:\n            {\n              runtime->handle_created_region_contexts(derez,\n                                                      remote_address_space);\n              break;\n            }\n          case SEND_MATERIALIZED_VIEW:\n            {\n              runtime->handle_send_materialized_view(derez, \n                                                     remote_address_space);\n              break;\n            }\n          case SEND_FILL_VIEW:\n            {\n              runtime->handle_send_fill_view(derez, remote_address_space);\n              break;\n            }\n          case SEND_PHI_VIEW:\n            {\n              runtime->handle_send_phi_view(derez, remote_address_space);\n              break;\n            }\n          case SEND_SHARDED_VIEW:\n            {\n              runtime->handle_send_sharded_view(derez, remote_address_space);\n              break;\n            }\n          case SEND_REDUCTION_VIEW:\n            {\n              runtime->handle_send_reduction_view(derez, remote_address_space);\n              break;\n            }\n          case SEND_INSTANCE_MANAGER:\n            {\n              runtime->handle_send_instance_manager(derez, \n                                                    remote_address_space);\n              break;\n            }\n          case SEND_MANAGER_UPDATE:\n            {\n              runtime->handle_send_manager_update(derez,\n                                                  remote_address_space);\n              break;\n            }\n          case SEND_COLLECTIVE_MANAGER:\n            {\n              runtime->handle_collective_instance_manager(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_COLLECTIVE_MESSAGE:\n            {\n              runtime->handle_collective_instance_message(derez);\n              break;\n            }\n#ifdef LEGION_GPU_REDUCTIONS\n          case SEND_CREATE_SHADOW_REQUEST:\n            {\n              runtime->handle_create_shadow_reduction_request(derez, \n                                              remote_address_space);\n              break;\n            }\n          case SEND_CREATE_SHADOW_RESPONSE:\n            {\n              runtime->handle_create_shadow_reduction_response(derez);\n              break;\n            }\n#endif\n          case SEND_CREATE_TOP_VIEW_REQUEST:\n            {\n              runtime->handle_create_top_view_request(derez,\n                                                      remote_address_space);\n              break;\n            }\n          case SEND_CREATE_TOP_VIEW_RESPONSE:\n            {\n              runtime->handle_create_top_view_response(derez);\n              break;\n            }\n          case SEND_VIEW_REQUEST:\n            {\n              runtime->handle_view_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_VIEW_REGISTER_USER:\n            {\n              runtime->handle_view_register_user(derez, remote_address_space);\n              break;\n            }\n          case SEND_VIEW_FIND_COPY_PRE_REQUEST:\n            {\n              runtime->handle_view_copy_pre_request(derez,remote_address_space);\n              break;\n            }\n          case SEND_VIEW_FIND_COPY_PRE_RESPONSE:\n            {\n              runtime->handle_view_copy_pre_response(derez,\n                                                    remote_address_space);\n              break;\n            }\n          case SEND_VIEW_ADD_COPY_USER:\n            {\n              runtime->handle_view_add_copy_user(derez, remote_address_space);\n              break;\n            }\n#ifdef ENABLE_VIEW_REPLICATION\n          case SEND_VIEW_REPLICATION_REQUEST:\n            {\n              runtime->handle_view_replication_request(derez, \n                                                       remote_address_space);\n              break;\n            }\n          case SEND_VIEW_REPLICATION_RESPONSE:\n            {\n              runtime->handle_view_replication_response(derez);\n              break;\n            }\n          case SEND_VIEW_REPLICATION_REMOVAL:\n            {\n              runtime->handle_view_replication_removal(derez, \n                                                       remote_address_space);\n              break;\n            }\n#endif\n          case SEND_MANAGER_REQUEST:\n            {\n              runtime->handle_manager_request(derez, remote_address_space);\n              break;\n            } \n          case SEND_FUTURE_RESULT:\n            {\n              runtime->handle_future_result(derez);\n              break;\n            }\n          case SEND_FUTURE_SUBSCRIPTION:\n            {\n              runtime->handle_future_subscription(derez, remote_address_space);\n              break;\n            }\n          case SEND_FUTURE_NOTIFICATION:\n            {\n              runtime->handle_future_notification(derez, remote_address_space);\n              break;\n            }\n          case SEND_FUTURE_BROADCAST:\n            {\n              runtime->handle_future_broadcast(derez);\n              break;\n            }\n          case SEND_FUTURE_MAP_REQUEST:\n            {\n              runtime->handle_future_map_future_request(derez, \n                                        remote_address_space);\n              break;\n            }\n          case SEND_FUTURE_MAP_RESPONSE:\n            {\n              runtime->handle_future_map_future_response(derez);\n              break;\n            }\n          case SEND_REPL_FUTURE_MAP_REQUEST:\n            {\n              runtime->handle_control_replicate_future_map_request(derez); \n              break;\n            }\n          case SEND_REPL_FUTURE_MAP_RESPONSE:\n            {\n              runtime->handle_control_replicate_future_map_response(derez);\n              break;\n            }\n          case SEND_REPL_TOP_VIEW_REQUEST:\n            {\n              runtime->handle_control_replicate_top_view_request(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_REPL_TOP_VIEW_RESPONSE:\n            {\n              runtime->handle_control_replicate_top_view_response(derez);\n              break;\n            }\n          case SEND_REPL_DISJOINT_COMPLETE_REQUEST:\n            {\n              runtime->handle_control_replicate_disjoint_complete_request(\n                                                                    derez);\n              break;\n            }\n          case SEND_REPL_DISJOINT_COMPLETE_RESPONSE:\n            {\n              runtime->handle_control_replicate_disjoint_complete_response(\n                                                                      derez);\n              break;\n            }\n          case SEND_REPL_INTRA_SPACE_DEP:\n            {\n              runtime->handle_control_replicate_intra_space_dependence(derez);\n              break;\n            }\n          case SEND_REPL_BROADCAST_UPDATE:\n            {\n              runtime->handle_control_replicate_broadcast_update(derez);\n              break;\n            }\n          case SEND_REPL_TRACE_EVENT_REQUEST:\n            {\n              runtime->handle_control_replicate_trace_event_request(derez,\n                                                    remote_address_space);\n              break;\n            }\n          case SEND_REPL_TRACE_EVENT_RESPONSE:\n            {\n              runtime->handle_control_replicate_trace_event_response(derez);\n              break;\n            }\n          case SEND_REPL_TRACE_UPDATE:\n            {\n              runtime->handle_control_replicate_trace_update(derez,\n                                                    remote_address_space);\n              break;\n            }\n          case SEND_REPL_IMPLICIT_REQUEST:\n            {\n              runtime->handle_control_replicate_implicit_request(derez,\n                                                    remote_address_space);\n              break;\n            }\n          case SEND_REPL_IMPLICIT_RESPONSE:\n            {\n              runtime->handle_control_replicate_implicit_response(derez);\n              break;\n            }\n          case SEND_MAPPER_MESSAGE:\n            {\n              runtime->handle_mapper_message(derez);\n              break;\n            }\n          case SEND_MAPPER_BROADCAST:\n            {\n              runtime->handle_mapper_broadcast(derez);\n              break;\n            }\n          case SEND_TASK_IMPL_SEMANTIC_REQ:\n            {\n              runtime->handle_task_impl_semantic_request(derez, \n                                                        remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_SEMANTIC_REQ:\n            {\n              runtime->handle_index_space_semantic_request(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_SEMANTIC_REQ:\n            {\n              runtime->handle_index_partition_semantic_request(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SPACE_SEMANTIC_REQ:\n            {\n              runtime->handle_field_space_semantic_request(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SEMANTIC_REQ:\n            {\n              runtime->handle_field_semantic_request(derez, \n                                                     remote_address_space);\n              break;\n            }\n          case SEND_LOGICAL_REGION_SEMANTIC_REQ:\n            {\n              runtime->handle_logical_region_semantic_request(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_LOGICAL_PARTITION_SEMANTIC_REQ:\n            {\n              runtime->handle_logical_partition_semantic_request(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_TASK_IMPL_SEMANTIC_INFO:\n            {\n              runtime->handle_task_impl_semantic_info(derez,\n                                                      remote_address_space);\n              break;\n            }\n          case SEND_INDEX_SPACE_SEMANTIC_INFO:\n            {\n              runtime->handle_index_space_semantic_info(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_INDEX_PARTITION_SEMANTIC_INFO:\n            {\n              runtime->handle_index_partition_semantic_info(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SPACE_SEMANTIC_INFO:\n            {\n              runtime->handle_field_space_semantic_info(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_FIELD_SEMANTIC_INFO:\n            {\n              runtime->handle_field_semantic_info(derez, remote_address_space);\n              break;\n            }\n          case SEND_LOGICAL_REGION_SEMANTIC_INFO:\n            {\n              runtime->handle_logical_region_semantic_info(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_LOGICAL_PARTITION_SEMANTIC_INFO:\n            {\n              runtime->handle_logical_partition_semantic_info(derez,\n                                                          remote_address_space);\n              break;\n            }\n          case SEND_REMOTE_CONTEXT_REQUEST:\n            {\n              runtime->handle_remote_context_request(derez, \n                                                     remote_address_space);\n              break;\n            }\n          case SEND_REMOTE_CONTEXT_RESPONSE:\n            {\n              runtime->handle_remote_context_response(derez);\n              break;\n            }\n          case SEND_REMOTE_CONTEXT_FREE:\n            {\n              runtime->handle_remote_context_free(derez);\n              break;\n            }\n          case SEND_REMOTE_CONTEXT_PHYSICAL_REQUEST:\n            {\n              runtime->handle_remote_context_physical_request(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_REMOTE_CONTEXT_PHYSICAL_RESPONSE:\n            {\n              runtime->handle_remote_context_physical_response(derez);\n              break;\n            }\n          case SEND_COMPUTE_EQUIVALENCE_SETS_REQUEST: \n            {\n              runtime->handle_compute_equivalence_sets_request(derez,\n                                               remote_address_space);\n              break;\n            }\n          case SEND_COMPUTE_EQUIVALENCE_SETS_RESPONSE:\n            {\n              runtime->handle_compute_equivalence_sets_response(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REQUEST:\n            {\n              runtime->handle_equivalence_set_request(derez, \n                                      remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_RESPONSE:\n            {\n              runtime->handle_equivalence_set_response(derez,\n                                                       remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_INVALIDATE_TRACKERS:\n            {\n              runtime->handle_equivalence_set_invalidate_trackers(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REPLICATION_REQUEST:\n            {\n              runtime->handle_equivalence_set_replication_request(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REPLICATION_RESPONSE:\n            {\n              runtime->handle_equivalence_set_replication_response(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REPLICATION_UPDATE:\n            {\n              runtime->handle_equivalence_set_replication_update(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_MIGRATION:\n            {\n              runtime->handle_equivalence_set_migration(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_OWNER_UPDATE:\n            {\n              runtime->handle_equivalence_set_owner_update(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_MAKE_OWNER:\n            {\n              runtime->handle_equivalence_set_make_owner(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_CLONE_REQUEST:\n            {\n              runtime->handle_equivalence_set_clone_request(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_CLONE_RESPONSE:\n            {\n              runtime->handle_equivalence_set_clone_response(derez);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_CAPTURE_REQUEST:\n            {\n              runtime->handle_equivalence_set_capture_request(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_CAPTURE_RESPONSE:\n            {\n              runtime->handle_equivalence_set_capture_response(derez,\n                                                remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_REQUEST_INSTANCES:\n            {\n              runtime->handle_equivalence_set_remote_request_instances(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_REQUEST_INVALID:\n            {\n              runtime->handle_equivalence_set_remote_request_invalid(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_REQUEST_ANTIVALID:\n            {\n              runtime->handle_equivalence_set_remote_request_antivalid(derez,\n                                                        remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_UPDATES:\n            {\n              runtime->handle_equivalence_set_remote_updates(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_ACQUIRES:\n            {\n              runtime->handle_equivalence_set_remote_acquires(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_RELEASES:\n            {\n              runtime->handle_equivalence_set_remote_releases(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_COPIES_ACROSS:\n            {\n              runtime->handle_equivalence_set_remote_copies_across(derez,\n                                                    remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_OVERWRITES:\n            {\n              runtime->handle_equivalence_set_remote_overwrites(derez,\n                                                remote_address_space);\n            break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_FILTERS:\n            {\n              runtime->handle_equivalence_set_remote_filters(derez,\n                                              remote_address_space);\n              break;\n            }\n          case SEND_EQUIVALENCE_SET_REMOTE_INSTANCES:\n            {\n              runtime->handle_equivalence_set_remote_instances(derez);\n              break;\n            }\n          case SEND_INSTANCE_REQUEST:\n            {\n              runtime->handle_instance_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_INSTANCE_RESPONSE:\n            {\n              runtime->handle_instance_response(derez, remote_address_space);\n              break;\n            }\n          case SEND_EXTERNAL_CREATE_REQUEST:\n            {\n              runtime->handle_external_create_request(derez, \n                                                      remote_address_space);\n              break;\n            }\n          case SEND_EXTERNAL_CREATE_RESPONSE:\n            {\n              runtime->handle_external_create_response(derez);\n              break;\n            }\n          case SEND_EXTERNAL_ATTACH:\n            {\n              runtime->handle_external_attach(derez);\n              break;\n            }\n          case SEND_EXTERNAL_DETACH:\n            {\n              runtime->handle_external_detach(derez);\n              break;\n            }\n          case SEND_GC_PRIORITY_UPDATE:\n            {\n              runtime->handle_gc_priority_update(derez, remote_address_space);\n              break;\n            }\n          case SEND_NEVER_GC_RESPONSE:\n            {\n              runtime->handle_never_gc_response(derez);\n              break;\n            }\n          case SEND_ACQUIRE_REQUEST:\n            {\n              runtime->handle_acquire_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_ACQUIRE_RESPONSE:\n            {\n              runtime->handle_acquire_response(derez, remote_address_space);\n              break;\n            }\n          case SEND_VARIANT_BROADCAST:\n            {\n              runtime->handle_variant_broadcast(derez);\n              break;\n            }\n          case SEND_CONSTRAINT_REQUEST:\n            {\n              runtime->handle_constraint_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_CONSTRAINT_RESPONSE:\n            {\n              runtime->handle_constraint_response(derez, remote_address_space);\n              break;\n            }\n          case SEND_CONSTRAINT_RELEASE:\n            {\n              runtime->handle_constraint_release(derez);\n              break;\n            }\n          case SEND_TOP_LEVEL_TASK_REQUEST:\n            {\n              runtime->handle_top_level_task_request(derez);\n              break;\n            }\n          case SEND_TOP_LEVEL_TASK_COMPLETE:\n            {\n              runtime->handle_top_level_task_complete(derez);\n              break;\n            }\n          case SEND_MPI_RANK_EXCHANGE:\n            {\n              runtime->handle_mpi_rank_exchange(derez);\n              break;\n            }\n          case SEND_REPLICATE_LAUNCH:\n            {\n              runtime->handle_replicate_launch(derez, remote_address_space);\n              break;\n            }\n          case SEND_REPLICATE_DELETE:\n            {\n              runtime->handle_replicate_delete(derez);\n              break;\n            }\n          case SEND_REPLICATE_POST_MAPPED:\n            {\n              runtime->handle_replicate_post_mapped(derez);\n              break;\n            }\n          case SEND_REPLICATE_POST_EXECUTION:\n            {\n              runtime->handle_replicate_post_execution(derez);\n              break;\n            }\n          case SEND_REPLICATE_TRIGGER_COMPLETE:\n            {\n              runtime->handle_replicate_trigger_complete(derez);\n              break;\n            }\n          case SEND_REPLICATE_TRIGGER_COMMIT:\n            {\n              runtime->handle_replicate_trigger_commit(derez);\n              break;\n            }\n          case SEND_CONTROL_REPLICATE_COLLECTIVE_MESSAGE:\n            {\n              runtime->handle_control_replicate_collective_message(derez);\n              break;\n            }\n          case SEND_LIBRARY_MAPPER_REQUEST:\n            {\n              runtime->handle_library_mapper_request(derez, \n                                      remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_MAPPER_RESPONSE:\n            {\n              runtime->handle_library_mapper_response(derez);\n              break;\n            }\n          case SEND_LIBRARY_TRACE_REQUEST:\n            {\n              runtime->handle_library_trace_request(derez,remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_TRACE_RESPONSE:\n            {\n              runtime->handle_library_trace_response(derez);\n              break;\n            }\n          case SEND_LIBRARY_PROJECTION_REQUEST:\n            {\n              runtime->handle_library_projection_request(derez,\n                                          remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_PROJECTION_RESPONSE:\n            {\n              runtime->handle_library_projection_response(derez);\n              break;\n            }\n          case SEND_LIBRARY_SHARDING_REQUEST:\n            {\n              runtime->handle_library_sharding_request(derez, \n                                                       remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_SHARDING_RESPONSE:\n            {\n              runtime->handle_library_sharding_response(derez);\n              break;\n            }\n          case SEND_LIBRARY_TASK_REQUEST:\n            {\n              runtime->handle_library_task_request(derez, remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_TASK_RESPONSE:\n            {\n              runtime->handle_library_task_response(derez);\n              break;\n            }\n          case SEND_LIBRARY_REDOP_REQUEST:\n            {\n              runtime->handle_library_redop_request(derez,remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_REDOP_RESPONSE:\n            {\n              runtime->handle_library_redop_response(derez);\n              break;\n            }\n          case SEND_LIBRARY_SERDEZ_REQUEST:\n            {\n              runtime->handle_library_serdez_request(derez,\n                                      remote_address_space);\n              break;\n            }\n          case SEND_LIBRARY_SERDEZ_RESPONSE:\n            {\n              runtime->handle_library_serdez_response(derez);\n              break;\n            }\n          case SEND_REMOTE_OP_REPORT_UNINIT:\n            {\n              runtime->handle_remote_op_report_uninitialized(derez);\n              break;\n            }\n          case SEND_REMOTE_OP_PROFILING_COUNT_UPDATE:\n            {\n              runtime->handle_remote_op_profiling_count_update(derez);\n              break;\n            }\n          case SEND_REMOTE_TRACE_UPDATE:\n            {\n              runtime->handle_remote_tracing_update(derez,remote_address_space);\n              break;\n            }\n          case SEND_REMOTE_TRACE_RESPONSE:\n            {\n              runtime->handle_remote_tracing_response(derez);\n              break;\n            }\n          case SEND_SHUTDOWN_NOTIFICATION:\n            {\n#ifdef DEBUG_LEGION\n              assert(!has_shutdown); // should only be one per message\n#endif\n              has_shutdown = true; \n              runtime->handle_shutdown_notification(derez,remote_address_space);\n              break;\n            }\n          case SEND_SHUTDOWN_RESPONSE:\n            {\n#ifdef DEBUG_LEGION\n              assert(!has_shutdown); // should only be one per message\n#endif\n              has_shutdown = true;\n              runtime->handle_shutdown_response(derez);\n              break;\n            }\n          default:\n            assert(false); // should never get here\n        }\n        // Update the args and arglen\n        args += message_size;\n        arglen -= message_size;\n      }\n#ifdef DEBUG_LEGION\n      assert(arglen == 0); // make sure we processed everything\n#endif\n      return has_shutdown;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void VirtualChannel::buffer_messages(unsigned num_messages,\n                                         const void *args, size_t arglen,\n                                         char *&receiving_buffer,\n                                         size_t &receiving_buffer_size,\n                                         size_t &receiving_index,\n                                         unsigned &received_messages,\n                                         unsigned &partial_messages)\n    //--------------------------------------------------------------------------\n    {\n      received_messages += num_messages;\n      partial_messages += 1; // up the number of partial messages received\n      // Check to see if it fits\n      if (receiving_buffer_size < (receiving_index+arglen))\n      {\n        // Figure out what the new size should be\n        // Keep doubling until it's larger\n        size_t new_buffer_size = receiving_buffer_size;\n        while (new_buffer_size < (receiving_index+arglen))\n          new_buffer_size *= 2;\n#ifdef DEBUG_LEGION\n        assert(new_buffer_size != 0); // would cause deallocation\n#endif\n        // Now realloc the memory\n        void *new_ptr = legion_realloc(MESSAGE_BUFFER_ALLOC, receiving_buffer,\n                                       receiving_buffer_size, new_buffer_size);\n        receiving_buffer_size = new_buffer_size;\n#ifdef DEBUG_LEGION\n        assert(new_ptr != NULL);\n#endif\n        receiving_buffer = (char*)new_ptr;\n      }\n      // Copy the data in\n      memcpy(receiving_buffer+receiving_index,args,arglen);\n      receiving_index += arglen;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Message Manager \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    MessageManager::MessageManager(AddressSpaceID remote,\n                                   Runtime *rt, size_t max_message_size,\n                                   const Processor remote_util_group)\n      : channels((VirtualChannel*)\n                  malloc(MAX_NUM_VIRTUAL_CHANNELS*sizeof(VirtualChannel))), \n        runtime(rt), remote_address_space(remote), target(remote_util_group), \n        always_flush(remote < rt->num_profiling_nodes)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(remote != runtime->address_space);\n#endif\n      // Initialize our virtual channels \n      for (unsigned idx = 0; idx < MAX_NUM_VIRTUAL_CHANNELS; idx++)\n      {\n        new (channels+idx) VirtualChannel((VirtualChannelKind)idx,\n          rt->address_space, max_message_size, always_flush, runtime->profiler);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    MessageManager::MessageManager(const MessageManager &rhs)\n      : channels(NULL), runtime(NULL), remote_address_space(0), \n        target(rhs.target), always_flush(false)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    MessageManager::~MessageManager(void)\n    //--------------------------------------------------------------------------\n    {\n      for (unsigned idx = 0; idx < MAX_NUM_VIRTUAL_CHANNELS; idx++)\n      {\n        channels[idx].~VirtualChannel();\n      }\n      free(channels);\n    }\n\n    //--------------------------------------------------------------------------\n    MessageManager& MessageManager::operator=(const MessageManager &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void MessageManager::send_message(Serializer &rez, MessageKind kind,\n           VirtualChannelKind channel, bool flush, bool response, bool shutdown)\n    //--------------------------------------------------------------------------\n    {\n      // Always flush for the profiler if we're doing that\n      if (!flush && always_flush)\n        flush = true;\n      channels[channel].package_message(rez, kind, flush, runtime, \n                                        target, response, shutdown);\n    }\n\n    //--------------------------------------------------------------------------\n    void MessageManager::receive_message(const void *args, size_t arglen)\n    //--------------------------------------------------------------------------\n    {\n      // Pull the channel off to do the receiving\n      const char *buffer = (const char*)args;\n      VirtualChannelKind channel = *((const VirtualChannelKind*)buffer);\n      buffer += sizeof(channel);\n      arglen -= sizeof(channel);\n      channels[channel].process_message(buffer, arglen, runtime, \n                                        remote_address_space);\n    }\n\n    //--------------------------------------------------------------------------\n    void MessageManager::confirm_shutdown(ShutdownManager *shutdown_manager, \n                                          bool phase_one)\n    //--------------------------------------------------------------------------\n    {\n      for (unsigned idx = 0; idx < MAX_NUM_VIRTUAL_CHANNELS; idx++)\n        channels[idx].confirm_shutdown(shutdown_manager, phase_one);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Shutdown Manager \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ShutdownManager::ShutdownManager(ShutdownPhase p, Runtime *rt, \n                                     AddressSpaceID s, unsigned r, \n                                     ShutdownManager *own)\n      : phase(p), runtime(rt), source(s), radix(r), owner(own),\n        needed_responses(0), return_code(rt->return_code), result(true)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ShutdownManager::ShutdownManager(const ShutdownManager &rhs)\n      : phase(rhs.phase), runtime(NULL), source(0), radix(0), owner(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    ShutdownManager::~ShutdownManager(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ShutdownManager& ShutdownManager::operator=(const ShutdownManager &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    bool ShutdownManager::attempt_shutdown(void)\n    //--------------------------------------------------------------------------\n    {\n      // Do the broadcast tree to the other nodes\n      // Figure out who we have to send messages to\n      std::vector<AddressSpaceID> targets;\n      const AddressSpaceID local_space = runtime->address_space;\n      const AddressSpaceID start = local_space * radix + 1;\n      for (unsigned idx = 0; idx < radix; idx++)\n      {\n        AddressSpaceID next = start+idx;\n        if (next < runtime->total_address_spaces)\n          targets.push_back(next);\n        else\n          break;\n      }\n      \n      if (!targets.empty())\n      {\n        // Set the number of needed_responses\n        needed_responses = targets.size();\n        Serializer rez;\n        rez.serialize(this);\n        rez.serialize(phase);\n        for (std::vector<AddressSpaceID>::const_iterator it = \n              targets.begin(); it != targets.end(); it++)\n          runtime->send_shutdown_notification(*it, rez); \n        return false;\n      }\n      else // no messages means we can finalize right now\n      {\n        finalize();\n        return true;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    bool ShutdownManager::handle_response(int code, bool success,\n                                          const std::set<RtEvent> &to_add)\n    //--------------------------------------------------------------------------\n    {\n      bool done = false;\n      {\n        AutoLock s_lock(shutdown_lock);\n        if ((return_code == 0) && (code != 0))\n          return_code = code;\n        if (result && !success)\n          result = false;\n        wait_for.insert(to_add.begin(), to_add.end()); \n#ifdef DEBUG_LEGION\n        assert(needed_responses > 0);\n#endif\n        needed_responses--;\n        done = (needed_responses == 0);\n      }\n      if (done)\n      {\n        finalize();\n        return true;\n      }\n      return false;\n    }\n\n    //--------------------------------------------------------------------------\n    void ShutdownManager::finalize(void)\n    //--------------------------------------------------------------------------\n    {\n      // Do our local check\n      runtime->confirm_runtime_shutdown(this, phase);\n#ifdef DEBUG_SHUTDOWN_HANG\n      if (!result)\n      {\n        LG_TASK_DESCRIPTIONS(task_descs);\n        // Only need to see tasks less than this \n        for (unsigned idx = 0; idx < LG_BEGIN_SHUTDOWN_TASK_IDS; idx++)\n        {\n          if (runtime->outstanding_counts[idx] == 0)\n            continue;\n          log_shutdown.info(\"Meta-Task %s: %d outstanding\",\n                task_descs[idx], runtime->outstanding_counts[idx]);\n        }\n      }\n#endif\n      if (result && (runtime->address_space == source))\n      {\n        log_shutdown.info(\"SHUTDOWN PHASE %d SUCCESS!\", phase);\n        if (phase != CONFIRM_SHUTDOWN)\n        {\n          if (phase == CONFIRM_TERMINATION)\n            runtime->prepare_runtime_shutdown();\n          // Do the next phase\n          runtime->initiate_runtime_shutdown(source, (ShutdownPhase)(phase+1));\n        }\n        else\n        {\n          log_shutdown.info(\"SHUTDOWN SUCCEEDED!\");\n          runtime->finalize_runtime_shutdown(return_code);\n        }\n      }\n      else if (runtime->address_space != source)\n      {\n#ifdef DEBUG_LEGION\n        assert(owner != NULL);\n#endif\n        // Send the message back\n        Serializer rez;\n        rez.serialize(owner);\n        rez.serialize(return_code);\n        rez.serialize<bool>(result);\n        rez.serialize<size_t>(wait_for.size());\n        for (std::set<RtEvent>::const_iterator it = \n              wait_for.begin(); it != wait_for.end(); it++)\n          rez.serialize(*it);\n        runtime->send_shutdown_response(source, rez);\n      }\n      else\n      {\n#ifdef DEBUG_LEGION\n        assert(!result);\n#endif\n        log_shutdown.info(\"FAILED SHUTDOWN PHASE %d! Trying again...\", phase);\n        RtEvent precondition;\n        if (!wait_for.empty())\n          precondition = Runtime::merge_events(wait_for);\n        // If we failed an even phase we go back to the one before it\n        RetryShutdownArgs args(((phase % 2) == 0) ?\n            (ShutdownPhase)(phase-1) : phase);\n        runtime->issue_runtime_meta_task(args, LG_LOW_PRIORITY,\n                                         precondition);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_tracing_update(Deserializer &derez,\n                                               AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RemoteTraceRecorder::handle_remote_update(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_tracing_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      RemoteTraceRecorder::handle_remote_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ShutdownManager::handle_shutdown_notification(\n                   Deserializer &derez, Runtime *runtime, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShutdownManager *owner;\n      derez.deserialize(owner);\n      ShutdownPhase phase;\n      derez.deserialize(phase);\n      runtime->initiate_runtime_shutdown(source, phase, owner);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ShutdownManager::handle_shutdown_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShutdownManager *shutdown_manager;\n      derez.deserialize(shutdown_manager);\n      int return_code;\n      derez.deserialize(return_code);\n      bool success;\n      derez.deserialize(success);\n      size_t num_events;\n      derez.deserialize(num_events);\n      std::set<RtEvent> wait_for;\n      for (unsigned idx = 0; idx < num_events; idx++)\n      {\n        RtEvent event;\n        derez.deserialize(event);\n        wait_for.insert(event);\n      }\n      if (shutdown_manager->handle_response(return_code, success, wait_for))\n        delete shutdown_manager;\n    }\n\n    //--------------------------------------------------------------------------\n    void ShutdownManager::record_outstanding_tasks(void)\n    //--------------------------------------------------------------------------\n    {\n      // Instant death\n      result = false;\n      log_shutdown.info(\"Outstanding tasks on node %d\", runtime->address_space);\n    }\n\n    //--------------------------------------------------------------------------\n    void ShutdownManager::record_recent_message(void)\n    //--------------------------------------------------------------------------\n    {\n      // Instant death\n      result = false;\n      log_shutdown.info(\"Outstanding message on node %d\", \n                        runtime->address_space);\n    }\n\n    //--------------------------------------------------------------------------\n    void ShutdownManager::record_pending_message(RtEvent pending_event)\n    //--------------------------------------------------------------------------\n    {\n      // Instant death\n      result = false;\n      wait_for.insert(pending_event);\n      log_shutdown.info(\"Pending message on node %d\", runtime->address_space);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Pending Registrations \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    PendingVariantRegistration::PendingVariantRegistration(VariantID v,\n                                  bool has_ret, const TaskVariantRegistrar &reg,\n                                  const void *udata, size_t udata_size,\n                                  const CodeDescriptor &realm, \n                                  const char *task_name)\n      : vid(v), has_return(has_ret), registrar(reg), \n        realm_desc(realm), logical_task_name(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // If we're doing a pending registration, this is a static\n      // registration so we don't have to register it globally\n      registrar.global_registration = false;\n      // Make sure we own the task variant name\n      if (reg.task_variant_name != NULL)\n        registrar.task_variant_name = strdup(reg.task_variant_name);\n      // We need to own the user data too\n      if (udata != NULL)\n      {\n        user_data_size = udata_size;\n        user_data = malloc(user_data_size);\n        memcpy(user_data,udata,user_data_size);\n      }\n      else\n      {\n        user_data_size = 0;\n        user_data = NULL;\n      }\n      if (task_name != NULL)\n        logical_task_name = strdup(task_name);\n    }\n\n    //--------------------------------------------------------------------------\n    PendingVariantRegistration::PendingVariantRegistration(\n                                          const PendingVariantRegistration &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    PendingVariantRegistration::~PendingVariantRegistration(void)\n    //--------------------------------------------------------------------------\n    {\n      if (registrar.task_variant_name != NULL)\n        free(const_cast<char*>(registrar.task_variant_name));\n      if (user_data != NULL)\n        free(user_data);\n      if (logical_task_name != NULL)\n        free(logical_task_name);\n    }\n\n    //--------------------------------------------------------------------------\n    PendingVariantRegistration& PendingVariantRegistration::operator=(\n                                          const PendingVariantRegistration &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void PendingVariantRegistration::perform_registration(Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      // If we have a logical task name, attach the name info\n      // Do this first before any logging for the variant\n      if (logical_task_name != NULL)\n        runtime->attach_semantic_information(registrar.task_id, \n                          LEGION_NAME_SEMANTIC_TAG, logical_task_name, \n                          strlen(logical_task_name)+1, \n                          false/*mutable*/, false/*send to owner*/);\n      runtime->register_variant(registrar, user_data, user_data_size,\n                    realm_desc, has_return, vid, false/*check task*/,\n                    true/*check context*/, true/*preregistered*/);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Task Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    TaskImpl::TaskImpl(TaskID tid, Runtime *rt, const char *name/*=NULL*/)\n      : task_id(tid), runtime(rt), initial_name(static_cast<char*>(\n          malloc(((name == NULL) ? 64 : strlen(name) + 1) * sizeof(char)))),\n        has_return_type(false), all_idempotent(false)\n    //--------------------------------------------------------------------------\n    {\n      // Always fill in semantic info 0 with a name for the task\n      if (name != NULL)\n      {\n        const size_t name_size = strlen(name) + 1; // for \\0\n        char *name_copy = (char*)legion_malloc(SEMANTIC_INFO_ALLOC, name_size);\n        memcpy(name_copy, name, name_size);\n        semantic_infos[LEGION_NAME_SEMANTIC_TAG] = \n          SemanticInfo(name_copy, name_size, false/*mutable*/);\n        if (runtime->legion_spy_enabled)\n          LegionSpy::log_task_name(task_id, name);\n        // Also set the initial name to be safe\n        memcpy(initial_name, name, name_size);\n        // Register this task with the profiler if necessary\n        if (runtime->profiler != NULL)\n          runtime->profiler->register_task_kind(task_id, name, false);\n      }\n      else // Just set the initial name\n      {\n        snprintf(initial_name,64,\"unnamed_task_%d\", task_id);\n        // Register this task with the profiler if necessary\n        if (runtime->profiler != NULL)\n          runtime->profiler->register_task_kind(task_id, initial_name, false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    TaskImpl::TaskImpl(const TaskImpl &rhs)\n      : task_id(rhs.task_id), runtime(rhs.runtime), initial_name(NULL)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    TaskImpl::~TaskImpl(void)\n    //-------------------------------------------------------------------------\n    {\n      for (std::map<SemanticTag,SemanticInfo>::const_iterator it = \n            semantic_infos.begin(); it != semantic_infos.end(); it++)\n      {\n        legion_free(SEMANTIC_INFO_ALLOC, it->second.buffer,\n                    it->second.size);\n      }\n      semantic_infos.clear();\n      free(initial_name);\n    }\n\n    //--------------------------------------------------------------------------\n    TaskImpl& TaskImpl::operator=(const TaskImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    VariantID TaskImpl::get_unique_variant_id(void)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(task_lock);\n      // VariantIDs have to uniquely identify our node so start at our\n      // current runtime name and stride by the number of nodes\n      VariantID result = runtime->address_space;\n      if (result == 0) // Never use VariantID 0\n        result = runtime->runtime_stride;\n      for ( ; result <= (UINT_MAX - runtime->runtime_stride); \n            result += runtime->runtime_stride)\n      {\n        if (variants.find(result) != variants.end())\n          continue;\n        if (pending_variants.find(result) != pending_variants.end())\n          continue;\n        pending_variants.insert(result);\n        return result;\n      }\n      assert(false);\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void TaskImpl::add_variant(VariantImpl *impl)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(impl->owner == this);\n#endif\n      AutoLock t_lock(task_lock);\n      if (!variants.empty())\n      {\n        // Make sure that all the variants agree whether there is \n        // a return type or not\n        if (has_return_type != impl->returns_value())\n          REPORT_LEGION_ERROR(ERROR_RETURN_SIZE_MISMATCH, \n                        \"Variants of task %s (ID %d) disagree on whether \"\n                        \"there is a return type or not. All variants \"\n                        \"of a task must agree on whether there is a \"\n                        \"return type.\", get_name(false/*need lock*/), task_id)\n        if (all_idempotent != impl->is_idempotent())\n          REPORT_LEGION_ERROR(ERROR_IDEMPOTENT_MISMATCH, \n                        \"Variants of task %s (ID %d) have different idempotent \"\n                        \"options.  All variants of the same task must \"\n                        \"all be either idempotent or non-idempotent.\",\n                        get_name(false/*need lock*/), task_id)\n      }\n      else\n      {\n        has_return_type = impl->returns_value();\n        all_idempotent  = impl->is_idempotent();\n      }\n      // Check to see if this variant has already been registered\n      if (variants.find(impl->vid) != variants.end())\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_VARIANT_REGISTRATION,\n                      \"Duplicate variant ID %d registered for task %s (ID %d)\",\n                      impl->vid, get_name(false/*need lock*/), task_id)\n      variants[impl->vid] = impl;\n      // Erase the pending VariantID if there is one\n      pending_variants.erase(impl->vid);\n    }\n\n    //--------------------------------------------------------------------------\n    VariantImpl* TaskImpl::find_variant_impl(VariantID variant_id,bool can_fail)\n    //--------------------------------------------------------------------------\n    {\n      // See if we already have the variant\n      {\n        AutoLock t_lock(task_lock,1,false/*exclusive*/);\n        std::map<VariantID,VariantImpl*>::const_iterator finder = \n          variants.find(variant_id);\n        if (finder != variants.end())\n          return finder->second;\n      }\n      if (!can_fail)\n        REPORT_LEGION_ERROR(ERROR_UNREGISTERED_VARIANT, \n                            \"Unable to find variant %d of task %s!\",\n                            variant_id, get_name())\n      return NULL;\n    }\n\n    //--------------------------------------------------------------------------\n    void TaskImpl::find_valid_variants(std::vector<VariantID> &valid_variants,\n                                       Processor::Kind kind) const\n    //--------------------------------------------------------------------------\n    {\n      if (kind == Processor::NO_KIND)\n      {\n        AutoLock t_lock(task_lock,1,false/*exclusive*/);\n        valid_variants.resize(variants.size());\n        unsigned idx = 0;\n        for (std::map<VariantID,VariantImpl*>::const_iterator it = \n              variants.begin(); it != variants.end(); it++, idx++)\n        {\n          valid_variants[idx] = it->first; \n        }\n      }\n      else\n      {\n        AutoLock t_lock(task_lock,1,false/*exclusive*/);\n        for (std::map<VariantID,VariantImpl*>::const_iterator it = \n              variants.begin(); it != variants.end(); it++)\n        {\n          if (it->second->can_use(kind, true/*warn*/))\n            valid_variants.push_back(it->first);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    const char* TaskImpl::get_name(bool needs_lock /*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (needs_lock)\n      {\n        // Do the request through the semantic information\n        const void *result = NULL; size_t dummy_size;\n        if (retrieve_semantic_information(LEGION_NAME_SEMANTIC_TAG, result, \n              dummy_size, true/*can fail*/, false/*wait until*/))\n          return reinterpret_cast<const char*>(result);\n      }\n      else\n      {\n        // If we're already holding the lock then we can just do\n        // the local look-up regardless of if we're the owner or not\n        std::map<SemanticTag,SemanticInfo>::const_iterator finder = \n          semantic_infos.find(LEGION_NAME_SEMANTIC_TAG);\n        if (finder != semantic_infos.end())\n          return reinterpret_cast<const char*>(finder->second.buffer);\n      }\n      // Couldn't find it so use the initial name\n      return initial_name;\n    }\n\n    //--------------------------------------------------------------------------\n    void TaskImpl::attach_semantic_information(SemanticTag tag,\n                                               AddressSpaceID source,\n                                               const void *buffer, size_t size,\n                                            bool is_mutable, bool send_to_owner)\n    //--------------------------------------------------------------------------\n    {\n      if ((tag == LEGION_NAME_SEMANTIC_TAG) && (runtime->profiler != NULL))\n        runtime->profiler->register_task_kind(task_id,(const char*)buffer,true);\n\n      void *local = legion_malloc(SEMANTIC_INFO_ALLOC, size);\n      memcpy(local, buffer, size);\n      bool added = true;\n      RtUserEvent to_trigger;\n      {\n        AutoLock t_lock(task_lock);\n        std::map<SemanticTag,SemanticInfo>::iterator finder = \n          semantic_infos.find(tag);\n        if (finder != semantic_infos.end())\n        {\n          // Check to see if it is valid\n          if (finder->second.is_valid())\n          {\n            // See if it is mutable or not\n            if (!finder->second.is_mutable)\n            {\n              // Note mutable so check to make sure that the bits are the same\n              if (size != finder->second.size)\n                REPORT_LEGION_ERROR(ERROR_INCONSISTENT_SEMANTIC_TAG,\n                              \"Inconsistent Semantic Tag value \"\n                              \"for tag %ld with different sizes of %zd\"\n                              \" and %zd for task impl\", \n                              tag, size, finder->second.size)\n              // Otherwise do a bitwise comparison\n              {\n                const char *orig = (const char*)finder->second.buffer;\n                const char *next = (const char*)buffer;\n                for (unsigned idx = 0; idx < size; idx++)\n                {\n                  char diff = orig[idx] ^ next[idx];\n                  if (diff)\n                    REPORT_LEGION_ERROR(ERROR_INCONSISTENT_SEMANTIC_TAG,\n                         \"Inconsistent Semantic Tag value \"\n                                     \"for tag %ld with different values at\"\n                                     \"byte %d for task impl, %x != %x\",\n                                     tag, idx, orig[idx], next[idx])\n                }\n              }\n              added = false;\n            }\n            else\n            {\n              // It is mutable so just overwrite it\n              legion_free(SEMANTIC_INFO_ALLOC, \n                          finder->second.buffer, finder->second.size);\n              finder->second.buffer = local;\n              finder->second.size = size;\n              finder->second.ready_event = RtUserEvent::NO_RT_USER_EVENT;\n              finder->second.is_mutable = is_mutable;\n            }\n          }\n          else\n          {\n            finder->second.buffer = local;\n            finder->second.size = size;\n            to_trigger = finder->second.ready_event;\n            finder->second.ready_event = RtUserEvent::NO_RT_USER_EVENT;\n            finder->second.is_mutable = is_mutable;\n          }\n        }\n        else\n          semantic_infos[tag] = SemanticInfo(local, size, is_mutable);\n      }\n      if (to_trigger.exists())\n        Runtime::trigger_event(to_trigger);\n      if (added)\n      {\n        if (send_to_owner)\n        {\n          AddressSpaceID owner_space = get_owner_space();\n          // if we are not the owner and the message didn't come\n          // from the owner, then send it\n          if ((owner_space != runtime->address_space) && \n              (source != owner_space))\n          {\n            if (tag == LEGION_NAME_SEMANTIC_TAG)\n            {\n              // Special case here for task names, the user can reasonably\n              // expect all tasks to have an initial name so we have to \n              // guarantee that this update is propagated before continuing\n              // because otherwise we can't distinguish the case where a \n              // name hasn't propagated from one where it was never set\n              RtUserEvent wait_on = Runtime::create_rt_user_event();\n              send_semantic_info(owner_space, tag, buffer, size, \n                                 is_mutable, wait_on);\n              wait_on.wait();\n            }\n            else\n              send_semantic_info(owner_space, tag, buffer, size, is_mutable);\n          }\n        }\n      }\n      else\n        legion_free(SEMANTIC_INFO_ALLOC, local, size);\n    }\n\n    //--------------------------------------------------------------------------\n    bool TaskImpl::retrieve_semantic_information(SemanticTag tag,\n              const void *&result, size_t &size, bool can_fail, bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      RtEvent wait_on;\n      RtUserEvent request;\n      const AddressSpaceID owner_space = get_owner_space();\n      const bool is_remote = (owner_space != runtime->address_space);\n      {\n        AutoLock t_lock(task_lock);\n        std::map<SemanticTag,SemanticInfo>::const_iterator finder = \n          semantic_infos.find(tag);\n        if (finder != semantic_infos.end())\n        {\n          // Already have the data so we are done\n          if (finder->second.is_valid())\n          {\n            result = finder->second.buffer;\n            size = finder->second.size;\n            return true;\n          }\n          else if (is_remote)\n          {\n            if (can_fail)\n            {\n              // Have to make our own event\n              request = Runtime::create_rt_user_event();\n              wait_on = request;\n            }\n            else // can use the canonical event\n              wait_on = finder->second.ready_event; \n          }\n          else if (wait_until) // local so use the canonical event\n            wait_on = finder->second.ready_event;\n        }\n        else\n        {\n          // Otherwise we make an event to wait on\n          if (!can_fail && wait_until)\n          {\n            // Make a canonical ready event\n            request = Runtime::create_rt_user_event();\n            semantic_infos[tag] = SemanticInfo(request);\n            wait_on = request;\n          }\n          else if (is_remote)\n          {\n            // Make an event just for us to use\n            request = Runtime::create_rt_user_event();\n            wait_on = request;\n          }\n        }\n      }\n      // We didn't find it yet, see if we have something to wait on\n      if (!wait_on.exists())\n      {\n        // Nothing to wait on so we have to do something\n        if (can_fail)\n          return false;\n        REPORT_LEGION_ERROR(ERROR_INVALID_SEMANTIC_TAG, \n                      \"Invalid semantic tag %ld for task implementation\", tag)\n      }\n      else\n      {\n        // Send a request if necessary\n        if (is_remote && request.exists())\n          send_semantic_request(owner_space, tag, can_fail, wait_until,request);\n        wait_on.wait();\n      }\n      // When we wake up, we should be able to find everything\n      AutoLock t_lock(task_lock,1,false/*exclusive*/);\n      std::map<SemanticTag,SemanticInfo>::const_iterator finder = \n        semantic_infos.find(tag);\n      if (finder == semantic_infos.end())\n      {\n        if (can_fail)\n          return false;\n        REPORT_LEGION_ERROR(ERROR_INVALID_SEMANTIC_TAG, \n            \"invalid semantic tag %ld for task implementation\", tag)\n      }\n      result = finder->second.buffer;\n      size = finder->second.size;\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    void TaskImpl::send_semantic_info(AddressSpaceID target, SemanticTag tag,\n                                      const void *buffer, size_t size, \n                                      bool is_mutable, RtUserEvent to_trigger)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(task_id);\n        rez.serialize(tag);\n        rez.serialize(size);\n        rez.serialize(buffer, size);\n        rez.serialize(is_mutable);\n        rez.serialize(to_trigger);\n      }\n      runtime->send_task_impl_semantic_info(target, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void TaskImpl::send_semantic_request(AddressSpaceID target, \n             SemanticTag tag, bool can_fail, bool wait_until, RtUserEvent ready)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(task_id);\n        rez.serialize(tag);\n        rez.serialize(can_fail);\n        rez.serialize(wait_until);\n        rez.serialize(ready);\n      }\n      runtime->send_task_impl_semantic_request(target, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void TaskImpl::process_semantic_request(SemanticTag tag, \n       AddressSpaceID target, bool can_fail, bool wait_until, RtUserEvent ready)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(get_owner_space() == runtime->address_space);\n#endif\n      RtEvent precondition;\n      void *result = NULL;\n      size_t size = 0;\n      bool is_mutable = false;\n      {\n        AutoLock t_lock(task_lock);\n        // See if we already have the data\n        std::map<SemanticTag,SemanticInfo>::iterator finder = \n          semantic_infos.find(tag);\n        if (finder != semantic_infos.end())\n        {\n          if (finder->second.is_valid())\n          {\n            result = finder->second.buffer;\n            size = finder->second.size;\n            is_mutable = finder->second.is_mutable;\n          }\n          else if (!can_fail && wait_until)\n            precondition = finder->second.ready_event;\n        }\n        else if (!can_fail && wait_until)\n        {\n          // Don't have it yet, make a condition and hope that one comes\n          RtUserEvent ready_event = Runtime::create_rt_user_event();\n          precondition = ready_event;\n          semantic_infos[tag] = SemanticInfo(ready_event);\n        }\n      }\n      if (result == NULL)\n      {\n        // this will cause a failure on the original node\n        if (can_fail || !wait_until)\n          Runtime::trigger_event(ready);  \n        else\n        {\n          // Defer this until the semantic condition is ready\n          SemanticRequestArgs args(this, tag, target);\n          runtime->issue_runtime_meta_task(args, LG_LATENCY_WORK_PRIORITY, \n                                           precondition);\n        }\n      }\n      else\n        send_semantic_info(target, tag, result, size, is_mutable, ready);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void TaskImpl::handle_semantic_request(Runtime *runtime,\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      TaskID task_id;\n      derez.deserialize(task_id);\n      SemanticTag tag;\n      derez.deserialize(tag);\n      bool can_fail;\n      derez.deserialize(can_fail);\n      bool wait_until;\n      derez.deserialize(wait_until);\n      RtUserEvent ready;\n      derez.deserialize(ready);\n      TaskImpl *impl = runtime->find_or_create_task_impl(task_id);\n      impl->process_semantic_request(tag, source, can_fail, wait_until, ready);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void TaskImpl::handle_semantic_info(Runtime *runtime,\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      TaskID task_id;\n      derez.deserialize(task_id);\n      SemanticTag tag;\n      derez.deserialize(tag);\n      size_t size;\n      derez.deserialize(size);\n      const void *buffer = derez.get_current_pointer();\n      derez.advance_pointer(size);\n      bool is_mutable;\n      derez.deserialize(is_mutable);\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      TaskImpl *impl = runtime->find_or_create_task_impl(task_id);\n      impl->attach_semantic_information(tag, source, buffer, size, \n                                        is_mutable, false/*send to owner*/);\n      if (to_trigger.exists())\n        Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ AddressSpaceID TaskImpl::get_owner_space(TaskID task_id,\n                                                        Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      return (task_id % runtime->total_address_spaces);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Variant Impl \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    VariantImpl::VariantImpl(Runtime *rt, VariantID v, TaskImpl *own, \n                           const TaskVariantRegistrar &registrar, bool ret,\n                           const CodeDescriptor &realm,\n                           const void *udata /*=NULL*/, size_t udata_size/*=0*/)\n      : vid(v), owner(own), runtime(rt), global(registrar.global_registration),\n        has_return_value(ret), \n        descriptor_id(runtime->get_unique_code_descriptor_id()),\n        realm_descriptor(realm),\n        execution_constraints(registrar.execution_constraints),\n        layout_constraints(registrar.layout_constraints),\n        user_data_size(udata_size), leaf_variant(registrar.leaf_variant), \n        inner_variant(registrar.inner_variant),\n        idempotent_variant(registrar.idempotent_variant),\n        replicable_variant(registrar.replicable_variant)\n    //--------------------------------------------------------------------------\n    { \n      if (udata != NULL)\n      {\n        user_data = malloc(user_data_size);\n        memcpy(user_data, udata, user_data_size);\n      }\n      else\n        user_data = NULL;\n      // If we have a variant name, then record it\n      if (registrar.task_variant_name == NULL)\n      {\n        variant_name = (char*)malloc(64*sizeof(char));\n        snprintf(variant_name,64,\"unnamed_variant_%d\", vid);\n      }\n      else\n        variant_name = strdup(registrar.task_variant_name);\n      // If a global registration was requested, but the code descriptor\n      // provided does not have portable implementations, try to make one\n      // (if it fails, we'll complain below)\n      if (global && !realm_descriptor.has_portable_implementations())\n\trealm_descriptor.create_portable_implementation();\n      // Perform the registration, the normal case is not to have separate\n      // runtime instances, but if we do have them, we only register on\n      // the local processor\n      if (!runtime->separate_runtime_instances)\n      {\n        Realm::ProfilingRequestSet profiling_requests;\n        const ProcessorConstraint &proc_constraint = \n          execution_constraints.processor_constraint;\n        if (proc_constraint.valid_kinds.empty())\n        {\n          REPORT_LEGION_WARNING(LEGION_WARNING_MISSING_PROC_CONSTRAINT, \n                     \"NO PROCESSOR CONSTRAINT SPECIFIED FOR VARIANT\"\n                     \" %s (ID %d) OF TASK %s (ID %d)! ASSUMING LOC_PROC!\",\n                     variant_name, vid, owner->get_name(false), owner->task_id)\n          ready_event = ApEvent(Processor::register_task_by_kind(\n                Processor::LOC_PROC, false/*global*/, descriptor_id, \n                realm_descriptor, profiling_requests, user_data, user_data_size));\n        }\n        else if (proc_constraint.valid_kinds.size() > 1)\n        {\n          std::set<ApEvent> ready_events;\n          for (std::vector<Processor::Kind>::const_iterator it = \n                proc_constraint.valid_kinds.begin(); it !=\n                proc_constraint.valid_kinds.end(); it++)\n            ready_events.insert(ApEvent(Processor::register_task_by_kind(*it,\n                false/*global*/, descriptor_id, realm_descriptor, \n                profiling_requests, user_data, user_data_size)));\n          ready_event = Runtime::merge_events(NULL, ready_events);\n        }\n        else\n          ready_event = ApEvent(Processor::register_task_by_kind(\n              proc_constraint.valid_kinds[0], false/*global*/, descriptor_id, \n              realm_descriptor, profiling_requests, user_data, user_data_size));\n      }\n      else\n      {\n        // This is a debug case for when we have one runtime instance\n        // for each processor\n        std::set<Processor::Kind> handled_kinds;\n        Machine::ProcessorQuery local_procs(runtime->machine);\n        local_procs.local_address_space();\n        std::set<ApEvent> ready_events;\n        for (Machine::ProcessorQuery::iterator it = \n              local_procs.begin(); it != local_procs.end(); it++)\n        {\n          const Processor::Kind kind = it->kind();\n          if (handled_kinds.find(kind) != handled_kinds.end())\n            continue;\n          Realm::ProfilingRequestSet profiling_requests;\n          ready_events.insert(ApEvent(Processor::register_task_by_kind(kind,\n                          false/*global*/, descriptor_id, realm_descriptor, \n                          profiling_requests, user_data, user_data_size)));\n          handled_kinds.insert(kind);\n        }\n        if (!ready_events.empty())\n          ready_event = Runtime::merge_events(NULL, ready_events);\n      }\n      // register this with the runtime profiler if we have to\n      if (runtime->profiler != NULL)\n        runtime->profiler->register_task_variant(own->task_id, vid,\n            variant_name);\n      // Check that global registration has portable implementations\n      if (global && (!realm_descriptor.has_portable_implementations()))\n        REPORT_LEGION_ERROR(ERROR_ILLEGAL_GLOBAL_VARIANT_REGISTRATION, \n             \"Variant %s requested global registration without \"\n                         \"a portable implementation.\", variant_name)\n      if (leaf_variant && inner_variant)\n        REPORT_LEGION_ERROR(ERROR_INNER_LEAF_MISMATCH, \n                      \"Task variant %s (ID %d) of task %s (ID %d) is not \"\n                      \"permitted to be both inner and leaf tasks \"\n                      \"simultaneously.\", variant_name, vid,\n                      owner->get_name(), owner->task_id)\n      if (runtime->record_registration)\n        log_run.print(\"Task variant %s of task %s (ID %d) has Realm ID %ld\",\n              variant_name, owner->get_name(), owner->task_id, descriptor_id);\n    }\n\n    //--------------------------------------------------------------------------\n    VariantImpl::VariantImpl(const VariantImpl &rhs) \n      : vid(rhs.vid), owner(rhs.owner), runtime(rhs.runtime), \n        global(rhs.global), has_return_value(rhs.has_return_value),\n        descriptor_id(rhs.descriptor_id), realm_descriptor(rhs.realm_descriptor) \n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    VariantImpl::~VariantImpl(void)\n    //--------------------------------------------------------------------------\n    {\n      if (user_data != NULL)\n        free(user_data);\n      if (variant_name != NULL)\n        free(variant_name);\n    }\n\n    //--------------------------------------------------------------------------\n    VariantImpl& VariantImpl::operator=(const VariantImpl &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    bool VariantImpl::is_no_access_region(unsigned idx) const\n    //--------------------------------------------------------------------------\n    {\n      bool result = false;\n      for (std::multimap<unsigned,LayoutConstraintID>::const_iterator it = \n            layout_constraints.layouts.lower_bound(idx); it !=\n            layout_constraints.layouts.upper_bound(idx); it++)\n      {\n        result = true;\n        LayoutConstraints *constraints = \n          runtime->find_layout_constraints(it->second);\n        if (!constraints->specialized_constraint.is_no_access())\n        {\n          result = false;\n          break;\n        }\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    ApEvent VariantImpl::dispatch_task(Processor target, SingleTask *task,\n                                       TaskContext *ctx, ApEvent precondition,\n                                       PredEvent predicate_guard, int priority, \n                                       Realm::ProfilingRequestSet &requests)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      // Either it is local or it is a group that we made\n      assert(runtime->is_local(target) || runtime->separate_runtime_instances ||\n              (target.kind() == Processor::PROC_GROUP));\n#endif\n      // Add any profiling requests\n      if (runtime->profiler != NULL)\n        runtime->profiler->add_task_request(requests, owner->task_id, vid,\n                                            task->get_unique_op_id(), target);\n      // Increment the number of outstanding tasks\n#ifdef DEBUG_LEGION\n      runtime->increment_total_outstanding_tasks(task->task_id, false/*meta*/);\n#else\n      runtime->increment_total_outstanding_tasks();\n#endif\n      DETAILED_PROFILER(runtime, REALM_SPAWN_TASK_CALL);\n      // If our ready event hasn't triggered, include it in the precondition\n      if (predicate_guard.exists())\n      {\n        // Merge in the predicate guard\n        ApEvent pre = Runtime::merge_events(NULL, precondition, ready_event, \n                                            ApEvent(predicate_guard));\n        // Have to protect the result in case it misspeculates\n        return Runtime::ignorefaults(target.spawn(descriptor_id, \n                    &ctx, sizeof(ctx), requests, pre, priority));\n      }\n      else\n      {\n        // No predicate guard\n        if (ready_event.exists())\n          return ApEvent(target.spawn(descriptor_id, &ctx, sizeof(ctx),requests,\n             Runtime::merge_events(NULL, precondition, ready_event), priority));\n        return ApEvent(target.spawn(descriptor_id, &ctx, sizeof(ctx), requests, \n                                    precondition, priority));\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void VariantImpl::dispatch_inline(Processor current, TaskContext *ctx)\n    //--------------------------------------------------------------------------\n    {\n      const Realm::FunctionPointerImplementation *fp_impl = \n        realm_descriptor.find_impl<Realm::FunctionPointerImplementation>();\n#ifdef DEBUG_LEGION\n      assert(fp_impl != NULL);\n      assert(implicit_context != NULL);\n#endif\n      // Save the implicit context here on the stack so we can restore it\n      TaskContext *previous_context = implicit_context;\n      RealmFnptr inline_ptr = fp_impl->get_impl<RealmFnptr>();\n      (*inline_ptr)(&ctx, sizeof(ctx), user_data, user_data_size, current);\n      // Restore the implicit context back to the previous context\n      implicit_context = previous_context;\n    }\n\n    //--------------------------------------------------------------------------\n    bool VariantImpl::can_use(Processor::Kind kind, bool warn) const\n    //--------------------------------------------------------------------------\n    {\n      const ProcessorConstraint &constraint = \n                                  execution_constraints.processor_constraint;\n      if (constraint.is_valid())\n        return constraint.can_use(kind);\n      if (warn)\n        REPORT_LEGION_WARNING(LEGION_WARNING_MISSING_PROC_CONSTRAINT, \n           \"NO PROCESSOR CONSTRAINT SPECIFIED FOR VARIANT\"\n                        \" %s (ID %d) OF TASK %s (ID %d)! ASSUMING LOC_PROC!\",\n                      variant_name, vid, owner->get_name(false),owner->task_id)\n      return (Processor::LOC_PROC == kind);\n    }\n\n    //--------------------------------------------------------------------------\n    void VariantImpl::broadcast_variant(RtUserEvent done, AddressSpaceID origin,\n                                        AddressSpaceID local)\n    //--------------------------------------------------------------------------\n    {\n      std::vector<AddressSpaceID> targets;\n      std::vector<AddressSpaceID> locals;\n      const AddressSpaceID start = local * runtime->legion_collective_radix + 1;\n      for (int idx = 0; idx < runtime->legion_collective_radix; idx++)\n      {\n        AddressSpaceID next = start+idx;\n        if (next >= runtime->total_address_spaces)\n          break;\n        locals.push_back(next);\n        // Convert from relative to actual address space\n        AddressSpaceID actual = (origin + next) % runtime->total_address_spaces;\n        targets.push_back(actual);\n      }\n      if (!targets.empty())\n      {\n        std::set<RtEvent> local_done;\n        for (unsigned idx = 0; idx < targets.size(); idx++)\n        {\n          RtUserEvent next_done = Runtime::create_rt_user_event();\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(owner->task_id);\n            rez.serialize(vid);\n            // Extra padding to fix a realm bug for now\n            rez.serialize(vid);\n            rez.serialize(next_done);\n            rez.serialize(has_return_value);\n            // pack the code descriptors \n            Realm::Serialization::ByteCountSerializer counter;\n            realm_descriptor.serialize(counter, true/*portable*/);\n            const size_t impl_size = counter.bytes_used();\n            rez.serialize(impl_size);\n            {\n              Realm::Serialization::FixedBufferSerializer \n                serializer(rez.reserve_bytes(impl_size), impl_size);\n              realm_descriptor.serialize(serializer, true/*portable*/);\n            }\n            rez.serialize(user_data_size);\n            if (user_data_size > 0)\n              rez.serialize(user_data, user_data_size);\n            rez.serialize(leaf_variant);\n            rez.serialize(inner_variant);\n            rez.serialize(idempotent_variant);\n            rez.serialize(replicable_variant);\n            size_t name_size = strlen(variant_name)+1;\n            rez.serialize(variant_name, name_size);\n            // Pack the constraints\n            execution_constraints.serialize(rez);\n            layout_constraints.serialize(rez);\n            rez.serialize(origin);\n            rez.serialize(locals[idx]);\n          }\n          runtime->send_variant_broadcast(targets[idx], rez);\n          local_done.insert(next_done);\n        }\n        Runtime::trigger_event(done, Runtime::merge_events(local_done));\n      }\n      else\n        Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void VariantImpl::handle_variant_broadcast(Runtime *runtime,\n                                                          Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      TaskID task_id;\n      derez.deserialize(task_id);\n      TaskVariantRegistrar registrar(task_id, false/*global*/);\n      VariantID variant_id;\n      derez.deserialize(variant_id);\n      // Extra padding to fix a realm bug for now\n      derez.deserialize(variant_id); \n      RtUserEvent done;\n      derez.deserialize(done);\n      bool has_return;\n      derez.deserialize(has_return);\n      size_t impl_size;\n      derez.deserialize(impl_size);\n      CodeDescriptor realm_desc;\n      {\n        // Realm's serializers assume properly aligned buffers, so\n        // malloc a temporary buffer here and copy the data to ensure\n        // alignment.\n        void *impl_buffer = malloc(impl_size);\n#ifdef DEBUG_LEGION\n        assert(impl_buffer);\n#endif\n        memcpy(impl_buffer, derez.get_current_pointer(), impl_size);\n        derez.advance_pointer(impl_size);\n        Realm::Serialization::FixedBufferDeserializer\n          deserializer(impl_buffer, impl_size);\n#ifdef DEBUG_LEGION\n#ifndef NDEBUG\n        bool ok =\n#endif\n                  realm_desc.deserialize(deserializer);\n        assert(ok);\n#else\n        realm_desc.deserialize(deserializer);\n#endif\n        free(impl_buffer);\n      }\n      size_t user_data_size;\n      derez.deserialize(user_data_size);\n      const void *user_data = derez.get_current_pointer();\n      derez.advance_pointer(user_data_size);\n      derez.deserialize(registrar.leaf_variant);\n      derez.deserialize(registrar.inner_variant);\n      derez.deserialize(registrar.idempotent_variant);\n      derez.deserialize(registrar.replicable_variant);\n      // The last thing will be the name\n      registrar.task_variant_name = (const char*)derez.get_current_pointer();\n      size_t name_size = strlen(registrar.task_variant_name)+1;\n      derez.advance_pointer(name_size);\n      // Unpack the constraints\n      registrar.execution_constraints.deserialize(derez);\n      registrar.layout_constraints.deserialize(derez);\n      // Ask the runtime to perform the registration \n      // Can lie about preregistration since the user would already have\n      // gotten there error message on the owner node\n      runtime->register_variant(registrar, user_data, user_data_size,\n              realm_desc, has_return, variant_id, false/*check task*/,\n              false/*check context*/, true/*preregistered*/);\n      AddressSpaceID origin;\n      derez.deserialize(origin);\n      AddressSpaceID local;\n      derez.deserialize(local);\n      VariantImpl *impl = runtime->find_variant_impl(task_id, variant_id);\n      impl->broadcast_variant(done, origin, local);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Layout Constraints \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints::LayoutConstraints(LayoutConstraintID lay_id,FieldSpace h,\n                                     Runtime *rt, bool inter, DistributedID did)\n      : LayoutConstraintSet(), DistributedCollectable(rt, (did > 0) ? did : \n          rt->get_available_distributed_id(), get_owner_space(lay_id, rt), \n          false/*register*/), layout_id(lay_id), handle(h), internal(inter), \n        constraints_name(NULL)\n    //--------------------------------------------------------------------------\n    {\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Constraints %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints::LayoutConstraints(LayoutConstraintID lay_id, Runtime *rt,\n      const LayoutConstraintRegistrar &registrar, bool inter, DistributedID did)\n      : LayoutConstraintSet(registrar.layout_constraints), \n        DistributedCollectable(rt, (did > 0) ? did : \n            rt->get_available_distributed_id(), get_owner_space(lay_id, rt),\n            false/*register with runtime*/), \n        layout_id(lay_id), handle(registrar.handle), internal(inter)\n    //--------------------------------------------------------------------------\n    {\n      if (registrar.layout_name == NULL)\n      {\n        constraints_name = (char*)malloc(64*sizeof(char));\n        snprintf(constraints_name,64,\"layout constraints %ld\", layout_id);\n      }\n      else\n        constraints_name = strdup(registrar.layout_name);\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Constraints %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints::LayoutConstraints(LayoutConstraintID lay_id, Runtime *rt,\n                                         const LayoutConstraintSet &cons,\n                                         FieldSpace h, bool inter)\n      : LayoutConstraintSet(cons), DistributedCollectable(rt,\n          rt->get_available_distributed_id(), get_owner_space(lay_id, rt),\n          false/*register with runtime*/), \n        layout_id(lay_id), handle(h), internal(inter)\n    //--------------------------------------------------------------------------\n    {\n      constraints_name = (char*)malloc(64*sizeof(char));\n      snprintf(constraints_name,64,\"layout constraints %ld\", layout_id);\n#ifdef LEGION_GC\n      log_garbage.info(\"GC Constraints %lld %d\", \n          LEGION_DISTRIBUTED_ID_FILTER(this->did), local_space);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints::LayoutConstraints(const LayoutConstraints &rhs)\n      : LayoutConstraintSet(rhs), DistributedCollectable(NULL, 0, 0), \n        layout_id(rhs.layout_id), handle(rhs.handle), internal(false)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints::~LayoutConstraints(void)\n    //--------------------------------------------------------------------------\n    {\n      if (constraints_name != NULL)\n        free(constraints_name);\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints& LayoutConstraints::operator=(const LayoutConstraints &rh)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::operator==(const LayoutConstraints &rhs) const\n    //--------------------------------------------------------------------------\n    {\n      // We check equalities only on the members of LayoutConstraintSet\n      return field_constraint == rhs.field_constraint\n             && specialized_constraint == rhs.specialized_constraint\n             && memory_constraint == rhs.memory_constraint\n             && ordering_constraint == rhs.ordering_constraint\n             && alignment_constraints == rhs.alignment_constraints\n             && dimension_constraints == rhs.dimension_constraints\n             && splitting_constraints == rhs.splitting_constraints\n             && offset_constraints == rhs.offset_constraints\n             && pointer_constraint == rhs.pointer_constraint;\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::operator==(const LayoutConstraintSet &rhs) const\n    //--------------------------------------------------------------------------\n    {\n      // We check equalities only on the members of LayoutConstraintSet\n      return field_constraint == rhs.field_constraint\n             && specialized_constraint == rhs.specialized_constraint\n             && memory_constraint == rhs.memory_constraint\n             && ordering_constraint == rhs.ordering_constraint\n             && alignment_constraints == rhs.alignment_constraints\n             && dimension_constraints == rhs.dimension_constraints\n             && splitting_constraints == rhs.splitting_constraints\n             && offset_constraints == rhs.offset_constraints\n             && pointer_constraint == rhs.pointer_constraint;\n    }\n\n    //--------------------------------------------------------------------------\n    void LayoutConstraints::notify_active(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // If we're not the owner add a remote reference\n      if (!is_owner())\n        send_remote_gc_increment(owner_space, mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    void LayoutConstraints::notify_inactive(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      if (is_owner())\n        runtime->unregister_layout(layout_id);\n      else\n        send_remote_gc_decrement(owner_space, mutator);\n    }\n\n    //--------------------------------------------------------------------------\n    void LayoutConstraints::notify_valid(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    void LayoutConstraints::notify_invalid(ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    void LayoutConstraints::send_constraint_response(AddressSpaceID target,\n                                                     RtUserEvent done_event)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(layout_id);\n        rez.serialize(did);\n        rez.serialize(handle);\n        rez.serialize<bool>(internal);\n        size_t name_len = strlen(constraints_name)+1;\n        rez.serialize(name_len);\n        rez.serialize(constraints_name, name_len);\n        // pack the constraints\n        serialize(rez);   \n        // pack the done events\n        rez.serialize(done_event);\n      }\n      runtime->send_constraint_response(target, rez);\n      update_remote_instances(target);\n    }\n\n    //--------------------------------------------------------------------------\n    void LayoutConstraints::update_constraints(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(constraints_name == NULL);\n#endif\n      size_t name_len;\n      derez.deserialize(name_len);\n      constraints_name = (char*)malloc(name_len);\n      derez.deserialize(constraints_name, name_len);\n      // unpack the constraints\n      deserialize(derez); \n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::entails(LayoutConstraints *constraints,\n                unsigned total_dims, const LayoutConstraint **failed_constraint)\n    //--------------------------------------------------------------------------\n    {\n      const std::pair<LayoutConstraintID,unsigned> \n        key(constraints->layout_id, total_dims);\n      // Check to see if the result is in the cache\n      {\n        AutoLock lay(layout_lock,1,false/*exclusive*/);\n        std::map<std::pair<LayoutConstraintID,unsigned>,\n                  const LayoutConstraint*>::const_iterator finder = \n            entailment_cache.find(key);\n        if (finder != entailment_cache.end())\n        {\n          if (finder->second != NULL)\n          {\n            if (failed_constraint != NULL)\n              *failed_constraint = finder->second;\n            return false;\n          }\n          else\n            return true;\n        }\n      }\n      // Didn't find it, so do the test for real\n      const LayoutConstraint *result = NULL;\n      const bool entailment = entails(*constraints, total_dims, &result);\n#ifdef DEBUG_LEGION\n      assert(entailment ^ (result != NULL)); // only one should be true\n#endif\n      // Save the result in the cache\n      AutoLock lay(layout_lock);\n      entailment_cache[key] = result;\n      if (!entailment && (failed_constraint != NULL))\n        *failed_constraint = result;\n      return entailment;\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::entails(const LayoutConstraintSet &other,\n          unsigned total_dims, const LayoutConstraint **failed_constraint) const\n    //--------------------------------------------------------------------------\n    {\n      return LayoutConstraintSet::entails(other, total_dims, failed_constraint);\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::conflicts(LayoutConstraints *constraints,\n              unsigned total_dims, const LayoutConstraint **conflict_constraint)\n    //--------------------------------------------------------------------------\n    {\n      const std::pair<LayoutConstraintID,unsigned> \n        key(constraints->layout_id, total_dims);\n      // Check to see if the result is in the cache\n      {\n        AutoLock lay(layout_lock,1,false/*exclusive*/);\n        std::map<std::pair<LayoutConstraintID,unsigned>,\n                  const LayoutConstraint*>::const_iterator finder = \n          conflict_cache.find(key);\n        if (finder != conflict_cache.end())\n        {\n          if (finder->second != NULL)\n          {\n            if (conflict_constraint != NULL)\n              *conflict_constraint = finder->second;\n            return true;\n          }\n          else\n            return false;\n        }\n      }\n      // Didn't find it, so do the test for real\n      const LayoutConstraint *result = NULL;\n      const bool conflicted = conflicts(*constraints, total_dims, &result);\n#ifdef DEBUG_LEGION\n      assert(conflicted ^ (result == NULL)); // only one should be true\n#endif\n      // Save the result in the cache\n      AutoLock lay(layout_lock);\n      conflict_cache[key] = result;\n      if (conflicted && (conflict_constraint != NULL))\n        *conflict_constraint = result;\n      return conflicted;\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::conflicts(const LayoutConstraintSet &other,\n        unsigned total_dims, const LayoutConstraint **conflict_constraint) const\n    //--------------------------------------------------------------------------\n    {\n      return LayoutConstraintSet::conflicts(other, total_dims, \n                                            conflict_constraint);\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::entails_without_pointer(\n                            LayoutConstraints *constraints, unsigned total_dims,\n                            const LayoutConstraint **failed_constraint)\n    //--------------------------------------------------------------------------\n    {\n      const std::pair<LayoutConstraintID,unsigned> \n        key(constraints->layout_id, total_dims);\n      // See if we have it in the cache\n      {\n        AutoLock lay(layout_lock,1,false/*exclusive*/);\n        std::map<std::pair<LayoutConstraintID,unsigned>,\n                  const LayoutConstraint*>::const_iterator finder = \n            no_pointer_entailment_cache.find(key);\n        if (finder != no_pointer_entailment_cache.end())\n        {\n          if (finder->second != NULL)\n          {\n            if (failed_constraint != NULL)\n              *failed_constraint = finder->second;\n            return false;\n          }\n          else\n            return true;\n        }\n      }\n      // Didn't find it so do the test for real\n      const LayoutConstraint *result = NULL;\n      const bool entailment = \n        entails_without_pointer(*constraints, total_dims, &result);\n      // Save the result in the cache\n      AutoLock lay(layout_lock);\n      no_pointer_entailment_cache[key] = result;\n      if (!entailment && (failed_constraint != NULL))\n        *failed_constraint = result;\n      return entailment;\n    }\n\n    //--------------------------------------------------------------------------\n    bool LayoutConstraints::entails_without_pointer(\n                          const LayoutConstraintSet &other, unsigned total_dims,\n                          const LayoutConstraint **failed_constraint) const\n    //--------------------------------------------------------------------------\n    {\n      // Do all the normal entailment but don't check the pointer constraint \n      if (!specialized_constraint.entails(other.specialized_constraint))\n      {\n        if (failed_constraint != NULL)\n          *failed_constraint = &other.specialized_constraint; \n        return false;\n      }\n      if (!field_constraint.entails(other.field_constraint))\n      {\n        if (failed_constraint != NULL)\n          *failed_constraint = &other.field_constraint;\n        return false;\n      }\n      if (!memory_constraint.entails(other.memory_constraint))\n      {\n        if (failed_constraint != NULL)\n          *failed_constraint = &other.memory_constraint;\n        return false;\n      }\n      if (!ordering_constraint.entails(other.ordering_constraint, total_dims))\n        return false;\n      for (std::vector<SplittingConstraint>::const_iterator it = \n            other.splitting_constraints.begin(); it !=\n            other.splitting_constraints.end(); it++)\n      {\n        bool entailed = false;\n        for (unsigned idx = 0; idx < splitting_constraints.size(); idx++)\n        {\n          if (splitting_constraints[idx].entails(*it))\n          {\n            entailed = true;\n            break;\n          }\n        }\n        if (!entailed)\n        {\n          if (failed_constraint != NULL)\n            *failed_constraint = &(*it);\n          return false;\n        }\n      }\n      for (std::vector<DimensionConstraint>::const_iterator it = \n            other.dimension_constraints.begin(); it != \n            other.dimension_constraints.end(); it++)\n      {\n        bool entailed = false;\n        for (unsigned idx = 0; idx < dimension_constraints.size(); idx++)\n        {\n          if (dimension_constraints[idx].entails(*it))\n          {\n            entailed = true;\n            break;\n          }\n        }\n        if (!entailed)\n        {\n          if (failed_constraint != NULL)\n            *failed_constraint = &(*it);\n          return false;\n        }\n      }\n      for (std::vector<AlignmentConstraint>::const_iterator it = \n            other.alignment_constraints.begin(); it != \n            other.alignment_constraints.end(); it++)\n      {\n        bool entailed = false;\n        for (unsigned idx = 0; idx < alignment_constraints.size(); idx++)\n        {\n          if (alignment_constraints[idx].entails(*it))\n          {\n            entailed = true;\n            break;\n          }\n        }\n        if (!entailed)\n        {\n          if (failed_constraint != NULL)\n            *failed_constraint = &(*it);\n          return false;\n        }\n      }\n      for (std::vector<OffsetConstraint>::const_iterator it = \n            other.offset_constraints.begin(); it != \n            other.offset_constraints.end(); it++)\n      {\n        bool entailed = false;\n        for (unsigned idx = 0; idx < offset_constraints.size(); idx++)\n        {\n          if (offset_constraints[idx].entails(*it))\n          {\n            entailed = true;\n            break;\n          }\n        }\n        if (!entailed)\n        {\n          if (failed_constraint != NULL)\n            *failed_constraint = &(*it);\n          return false;\n        }\n      }\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ AddressSpaceID LayoutConstraints::get_owner_space(\n                            LayoutConstraintID layout_id, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      return (layout_id % runtime->total_address_spaces);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void LayoutConstraints::process_request(Runtime *runtime,\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      LayoutConstraintID lay_id;\n      derez.deserialize(lay_id);\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n      bool can_fail;\n      derez.deserialize(can_fail);\n      LayoutConstraints *constraints = \n        runtime->find_layout_constraints(lay_id, can_fail);\n      if (can_fail && (constraints == NULL))\n        Runtime::trigger_event(done_event);\n      else\n        constraints->send_constraint_response(source, done_event);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void LayoutConstraints::process_response(\n                   Runtime *runtime, Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      LayoutConstraintID lay_id;\n      derez.deserialize(lay_id);\n      DistributedID did;\n      derez.deserialize(did);\n      FieldSpace handle;\n      derez.deserialize(handle);\n      bool internal;\n      derez.deserialize(internal);\n      // Make it an unpack it, then try to register it \n      LayoutConstraints *new_constraints = \n        new LayoutConstraints(lay_id, handle, runtime, internal, did);\n      new_constraints->update_constraints(derez);\n      std::set<RtEvent> preconditions;\n      WrapperReferenceMutator mutator(preconditions);\n      // Now try to register this with the runtime\n      if (!runtime->register_layout(new_constraints, &mutator))\n        delete new_constraints;\n      // Trigger our done event and then return it\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n      if (!preconditions.empty())\n        Runtime::trigger_event(done_event,Runtime::merge_events(preconditions));\n      else\n        Runtime::trigger_event(done_event);\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Identity Projection Functor\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    IdentityProjectionFunctor::IdentityProjectionFunctor(Legion::Runtime *rt)\n      : ProjectionFunctor(rt)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    IdentityProjectionFunctor::~IdentityProjectionFunctor(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion IdentityProjectionFunctor::project(const Mappable *mappable,\n            unsigned index, LogicalRegion upper_bound, const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      // We know we don't use the domain so we can fake it\n      Domain launch_domain;\n      return project(upper_bound, point, launch_domain);\n    }\n    \n    //--------------------------------------------------------------------------\n    LogicalRegion IdentityProjectionFunctor::project(const Mappable *mappable,\n         unsigned index, LogicalPartition upper_bound, const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      // We know we don't use the domain so we can fake it\n      Domain launch_domain;\n      return project(upper_bound, point, launch_domain);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion IdentityProjectionFunctor::project(LogicalRegion upper_bound,\n                          const DomainPoint &point, const Domain &launch_domain)\n    //--------------------------------------------------------------------------\n    {\n      return upper_bound;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion IdentityProjectionFunctor::project(LogicalPartition up_bound, \n                          const DomainPoint &point, const Domain &launch_domain)\n    //--------------------------------------------------------------------------\n    {\n      return runtime->get_logical_subregion_by_color(up_bound, point);\n    }\n\n    //--------------------------------------------------------------------------\n    bool IdentityProjectionFunctor::is_functional(void) const\n    //--------------------------------------------------------------------------\n    {\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    bool IdentityProjectionFunctor::is_exclusive(void) const\n    //--------------------------------------------------------------------------\n    {\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    unsigned IdentityProjectionFunctor::get_depth(void) const\n    //--------------------------------------------------------------------------\n    {\n      return 0;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Projection Function \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ProjectionFunction::ProjectionFunction(ProjectionID pid, \n                                           ProjectionFunctor *func)\n      : depth(func->get_depth()), is_exclusive(func->is_exclusive()),\n        is_functional(func->is_functional()), \n        is_invertible(func->is_invertible()), projection_id(pid), functor(func)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionFunction::ProjectionFunction(const ProjectionFunction &rhs)\n      : depth(rhs.depth), is_exclusive(rhs.is_exclusive), \n        is_functional(rhs.is_functional), is_invertible(rhs.is_invertible), \n        projection_id(rhs.projection_id), functor(rhs.functor)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionFunction::~ProjectionFunction(void)\n    //--------------------------------------------------------------------------\n    {\n      // These can be shared in the case of multiple runtime instances\n      if (!implicit_runtime->separate_runtime_instances)\n        delete functor;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::prepare_for_shutdown(void)\n    //--------------------------------------------------------------------------\n    {\n      // This will remove the index space references we are holding\n      elide_close_results.clear();\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion ProjectionFunction::project_point(Task *task, unsigned idx, \n        Runtime *runtime, const Domain &launch_domain, const DomainPoint &point)\n    //--------------------------------------------------------------------------\n    {\n      const RegionRequirement &req = task->regions[idx];\n#ifdef DEBUG_LEGION\n      assert(req.handle_type != LEGION_SINGULAR_PROJECTION);\n#endif\n      if (!is_exclusive)\n      {\n        AutoLock p_lock(projection_reservation);\n        if (req.handle_type == LEGION_PARTITION_PROJECTION)\n        {\n          LogicalRegion result = is_functional ?\n            functor->project(req.partition, point, launch_domain) : \n            functor->project(task, idx, req.partition, point); \n          check_projection_partition_result(req, task, idx, result, runtime);\n          return result;\n        }\n        else\n        {\n          LogicalRegion result = is_functional ?\n            functor->project(req.region, point, launch_domain) : \n            functor->project(task, idx, req.region, point);\n          check_projection_region_result(req, task, idx, result, runtime);\n          return result;\n        }\n      }\n      else\n      {\n        if (req.handle_type == LEGION_PARTITION_PROJECTION)\n        {\n          LogicalRegion result = is_functional ?\n            functor->project(req.partition, point, launch_domain) : \n            functor->project(task, idx, req.partition, point);\n          check_projection_partition_result(req, task, idx, result, runtime);\n          return result;\n        }\n        else\n        {\n          LogicalRegion result = is_functional ?\n            functor->project(req.region, point, launch_domain) :\n            functor->project(task, idx, req.region, point);\n          check_projection_region_result(req, task, idx, result, runtime);\n          return result;\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::project_points(const RegionRequirement &req, \n                  unsigned idx, Runtime *runtime, const Domain &launch_domain,\n                  const std::vector<PointTask*> &point_tasks)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(req.handle_type != LEGION_SINGULAR_PROJECTION);\n#endif\n      std::map<LogicalRegion,std::vector<DomainPoint> > dependences;\n      const bool find_dependences = is_invertible && IS_WRITE(req);\n      if (!is_exclusive)\n      {\n        AutoLock p_lock(projection_reservation);\n        if (req.handle_type == LEGION_PARTITION_PROJECTION)\n        {\n          for (std::vector<PointTask*>::const_iterator it = \n                point_tasks.begin(); it != point_tasks.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n             functor->project(req.partition, \n                 (*it)->get_domain_point(), launch_domain) :\n             functor->project(*it, idx, req.partition, \n                              (*it)->get_domain_point());\n            check_projection_partition_result(req, static_cast<Task*>(*it),\n                                              idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n            if (find_dependences)\n            {\n              std::vector<DomainPoint> &region_deps = dependences[result];\n              if (region_deps.empty())\n              {\n                functor->invert(result,req.partition,launch_domain,region_deps);\n                check_inversion((*it), idx, region_deps);\n              }\n              else\n                check_containment((*it), idx, region_deps);\n              (*it)->record_intra_space_dependences(idx, region_deps);\n            }\n          }\n        }\n        else\n        {\n          for (std::vector<PointTask*>::const_iterator it = \n                point_tasks.begin(); it != point_tasks.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n              functor->project(req.region, \n                  (*it)->get_domain_point(), launch_domain) :\n              functor->project(*it, idx, req.region,(*it)->get_domain_point());\n            check_projection_region_result(req, static_cast<Task*>(*it), \n                                           idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n            if (find_dependences)\n            {\n              std::vector<DomainPoint> &region_deps = dependences[result];\n              if (region_deps.empty())\n              {\n                functor->invert(result, req.region, launch_domain, region_deps);\n                check_inversion((*it), idx, region_deps);\n              }\n              else\n                check_containment((*it), idx, region_deps);\n              (*it)->record_intra_space_dependences(idx, region_deps);\n            }\n          }\n        }\n      }\n      else\n      {\n        if (req.handle_type == LEGION_PARTITION_PROJECTION)\n        {\n          for (std::vector<PointTask*>::const_iterator it = \n                point_tasks.begin(); it != point_tasks.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n             functor->project(req.partition, \n                 (*it)->get_domain_point(), launch_domain) :\n             functor->project(*it, idx, req.partition, \n                              (*it)->get_domain_point());\n            check_projection_partition_result(req, static_cast<Task*>(*it),\n                                              idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n            if (find_dependences)\n            {\n              std::vector<DomainPoint> &region_deps = dependences[result];\n              if (region_deps.empty())\n              {\n                functor->invert(result,req.partition,launch_domain,region_deps);\n                check_inversion((*it), idx, region_deps);\n              }\n              else\n                check_containment((*it), idx, region_deps);\n              (*it)->record_intra_space_dependences(idx, region_deps);\n            }\n          }\n        }\n        else\n        {\n          for (std::vector<PointTask*>::const_iterator it = \n                point_tasks.begin(); it != point_tasks.end(); it++)\n          {\n            LogicalRegion result = is_functional ? \n              functor->project(req.region, \n                  (*it)->get_domain_point(), launch_domain) :\n              functor->project(*it, idx, req.region,(*it)->get_domain_point());\n            check_projection_region_result(req, static_cast<Task*>(*it),\n                                           idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n            if (find_dependences)\n            {\n              std::vector<DomainPoint> &region_deps = dependences[result];\n              if (region_deps.empty())\n              {\n                functor->invert(result, req.region, launch_domain, region_deps);\n                check_inversion((*it), idx, region_deps);\n              }\n              else\n                check_containment((*it), idx, region_deps);\n              (*it)->record_intra_space_dependences(idx, region_deps);\n            }\n          }\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::project_points(Operation *op, unsigned idx,\n                         const RegionRequirement &req, \n                         Runtime *runtime, const Domain &launch_domain, \n                         const std::vector<ProjectionPoint*> &points)\n    //--------------------------------------------------------------------------\n    {\n      Mappable *mappable = op->get_mappable();\n#ifdef DEBUG_LEGION\n      assert(req.handle_type != LEGION_SINGULAR_PROJECTION);\n      assert(mappable != NULL);\n#endif\n      // TODO: support for invertible point operations\n      if (is_invertible && (req.privilege == LEGION_READ_WRITE))\n        assert(false);\n      if (!is_exclusive)\n      {\n        AutoLock p_lock(projection_reservation);\n        if (req.handle_type == LEGION_PARTITION_PROJECTION)\n        {\n          for (std::vector<ProjectionPoint*>::const_iterator it = \n                points.begin(); it != points.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n              functor->project(req.partition, \n                  (*it)->get_domain_point(), launch_domain) :\n              functor->project(mappable, idx, req.partition, \n                                (*it)->get_domain_point());\n            check_projection_partition_result(req, op, idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n          }\n        }\n        else\n        {\n          for (std::vector<ProjectionPoint*>::const_iterator it = \n                points.begin(); it != points.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n              functor->project(req.region, \n                  (*it)->get_domain_point(), launch_domain) :\n              functor->project(mappable, idx, req.region,\n                               (*it)->get_domain_point());\n            check_projection_region_result(req, op, idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n          }\n        }\n      }\n      else\n      {\n        if (req.handle_type == LEGION_PARTITION_PROJECTION)\n        {\n          for (std::vector<ProjectionPoint*>::const_iterator it = \n                points.begin(); it != points.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n              functor->project(req.partition, \n                  (*it)->get_domain_point(), launch_domain) :\n              functor->project(mappable, idx, req.partition, \n                               (*it)->get_domain_point());\n            check_projection_partition_result(req, op, idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n          }\n        }\n        else\n        {\n          for (std::vector<ProjectionPoint*>::const_iterator it = \n                points.begin(); it != points.end(); it++)\n          {\n            LogicalRegion result = is_functional ?\n              functor->project(req.region, \n                  (*it)->get_domain_point(), launch_domain) :\n              functor->project(mappable, idx, req.region,\n                               (*it)->get_domain_point());\n            check_projection_region_result(req, op, idx, result, runtime);\n            (*it)->set_projection_result(idx, result);\n          }\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::project_refinement(IndexSpaceNode *domain,\n                  RegionTreeNode *node, std::vector<RegionNode*> &regions) const\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      // should only get this call on functional projection functors\n      assert(is_functional);\n#endif\n      Domain launch_domain;\n      domain->get_launch_space_domain(launch_domain);\n      // If we're exclusive, we'll store the handles until after\n      // we release the lock to go look them up in the region tree\n      std::vector<LogicalRegion> handles;\n      if (is_exclusive)\n        handles.reserve(domain->get_volume());\n      else\n        regions.reserve(domain->get_volume());\n      RegionTreeForest *forest = node->context;\n      // No need to bother error checking here, this projection functor\n      // was already invoked in the same way by an actual operation so\n      // any errors will be handled automatically\n      if (node->is_region())\n      {\n        RegionNode *root = node->as_region_node();\n        if (is_exclusive)\n        {\n          AutoLock p_lock(projection_reservation);\n          for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n          {\n            const LogicalRegion handle = \n              functor->project(root->handle, itr.p, launch_domain);\n            if (handle.exists())\n              handles.push_back(handle);\n          }\n        }\n        else\n        {\n          for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n          {\n            const LogicalRegion handle = \n              functor->project(root->handle, itr.p, launch_domain);\n            if (handle.exists())\n              regions.push_back(forest->get_node(handle));\n          }\n        }\n      }\n      else\n      {\n        PartitionNode *root = node->as_partition_node();\n        if (is_exclusive)\n        {\n          AutoLock p_lock(projection_reservation);\n          for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n          {\n            const LogicalRegion handle = \n              functor->project(root->handle, itr.p, launch_domain);\n            if (handle.exists())\n              handles.push_back(handle);\n          }\n        }\n        else\n        {\n          for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n          {\n            const LogicalRegion handle = \n              functor->project(root->handle, itr.p, launch_domain);\n            if (handle.exists())\n              regions.push_back(forest->get_node(handle));\n          }\n        }\n      }\n      if (is_exclusive)\n      {\n        regions.reserve(handles.size());\n        for (std::vector<LogicalRegion>::const_iterator it =\n              handles.begin(); it != handles.end(); it++)\n          regions.push_back(forest->get_node(*it));\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::check_projection_region_result(\n        const RegionRequirement &req, const Task *task, unsigned idx,\n        LogicalRegion result, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      // NO_REGION is always an acceptable answer\n      if (result == LogicalRegion::NO_REGION)\n        return;\n      if (result.get_tree_id() != req.region.get_tree_id())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion of tree ID %d for region requirement %d \"\n            \"of task %s (UID %lld) which is different from the upper \"\n            \"bound node of tree ID %d\", projection_id, \n            result.get_tree_id(), idx, task->get_task_name(), \n            task->get_unique_id(), req.region.get_tree_id())\n#ifdef DEBUG_LEGION\n      if (!runtime->forest->is_subregion(result, req.region))\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which is not a subregion of the \"\n            \"upper bound region for region requirement %d of \"\n            \"task %s (UID %lld)\", projection_id, idx,\n            task->get_task_name(), task->get_unique_id())\n      const unsigned projection_depth = \n        runtime->forest->get_projection_depth(result, req.region);\n      if (projection_depth != functor->get_depth())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which has projection depth %d which \"\n            \"is different from stated projection depth of the functor \"\n            \"which is %d for region requirement %d of task %s (ID %lld)\",\n            projection_id, projection_depth, functor->get_depth(),\n            idx, task->get_task_name(), task->get_unique_id())\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::check_projection_partition_result(\n        const RegionRequirement &req, const Task *task, unsigned idx,\n        LogicalRegion result, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      // NO_REGION is always an acceptable answer\n      if (result == LogicalRegion::NO_REGION)\n        return;\n      if (result.get_tree_id() != req.partition.get_tree_id())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion of tree ID %d for region requirement %d \"\n            \"of task %s (UID %lld) which is different from the upper \"\n            \"bound node of tree ID %d\", projection_id, \n            result.get_tree_id(), idx, task->get_task_name(), \n            task->get_unique_id(), req.partition.get_tree_id())\n#ifdef DEBUG_LEGION\n      if (!runtime->forest->is_subregion(result, req.partition))\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which is not a subregion of the \"\n            \"upper bound region for region requirement %d of \"\n            \"task %s (UID %lld)\", projection_id, idx,\n            task->get_task_name(), task->get_unique_id())\n      const unsigned projection_depth = \n        runtime->forest->get_projection_depth(result, req.partition);\n      if (projection_depth != functor->get_depth())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which has projection depth %d which \"\n            \"is different from stated projection depth of the functor \"\n            \"which is %d for region requirement %d of task %s (ID %lld)\",\n            projection_id, projection_depth, functor->get_depth(),\n            idx, task->get_task_name(), task->get_unique_id())\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::check_projection_region_result(\n        const RegionRequirement &req, Operation *op, unsigned idx,\n        LogicalRegion result, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      // NO_REGION is always an acceptable answer\n      if (result == LogicalRegion::NO_REGION)\n        return;\n      if (result.get_tree_id() != req.region.get_tree_id())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion of tree ID %d for region requirement %d \"\n            \"of operation %s (UID %lld) which is different from the upper \"\n            \"bound node of tree ID %d\", projection_id, \n            result.get_tree_id(), idx, op->get_logging_name(), \n            op->get_unique_op_id(), req.region.get_tree_id())\n#ifdef DEBUG_LEGION\n      if (!runtime->forest->is_subregion(result, req.region))\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which is not a subregion of the \"\n            \"upper bound region for region requirement %d of \"\n            \"operation %s (UID %lld)\", projection_id, idx,\n            op->get_logging_name(), op->get_unique_op_id())\n      const unsigned projection_depth = \n        runtime->forest->get_projection_depth(result, req.region);\n      if (projection_depth != functor->get_depth())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which has projection depth %d which \"\n            \"is different from stated projection depth of the functor \"\n            \"which is %d for region requirement %d of operation %s (ID %lld)\",\n            projection_id, projection_depth, functor->get_depth(),\n            idx, op->get_logging_name(), op->get_unique_op_id())\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::check_projection_partition_result(\n        const RegionRequirement &req, Operation *op, unsigned idx,\n        LogicalRegion result, Runtime *runtime)\n    //--------------------------------------------------------------------------\n    {\n      // NO_REGION is always an acceptable answer\n      if (result == LogicalRegion::NO_REGION)\n        return;\n      if (result.get_tree_id() != req.partition.get_tree_id())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion of tree ID %d for region requirement %d \"\n            \"of operation %s (UID %lld) which is different from the upper \"\n            \"bound node of tree ID %d\", projection_id, \n            result.get_tree_id(), idx, op->get_logging_name(), \n            op->get_unique_op_id(), req.partition.get_tree_id())\n#ifdef DEBUG_LEGION\n      if (!runtime->forest->is_subregion(result, req.partition))\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which is not a subregion of the \"\n            \"upper bound region for region requirement %d of \"\n            \"operation %s (UID %lld)\", projection_id, idx,\n            op->get_logging_name(), op->get_unique_op_id())\n      const unsigned projection_depth = \n        runtime->forest->get_projection_depth(result, req.partition);\n      if (projection_depth != functor->get_depth())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT, \n            \"Projection functor %d produced an invalid \"\n            \"logical subregion which has projection depth %d which \"\n            \"is different from stated projection depth of the functor \"\n            \"which is %d for region requirement %d of operation %s (ID %lld)\",\n            projection_id, projection_depth, functor->get_depth(),\n            idx, op->get_logging_name(), op->get_unique_op_id())\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::check_inversion(const Task *task, unsigned index,\n                                         const std::vector<DomainPoint> &points)\n    //--------------------------------------------------------------------------\n    {\n      if (points.empty())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT,\n            \"Projection functor %d produced an empty inversion result \"\n            \"while inverting region requirement %d of task %s (UID %lld). \"\n            \"Empty inversions are never legal because the point task that \"\n            \"produced the region must always be included.\",\n            projection_id, index, task->get_task_name(), task->get_unique_id())\n#ifdef DEBUG_LEGION\n      std::set<DomainPoint> unique_points(points.begin(), points.end());\n      if (unique_points.size() != points.size())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT,\n            \"Projection functor %d produced an invalid inversion result \"\n            \"containing duplicate points for region requirement %d of \"\n            \"task %s (UID %lld). Each point is only permitted to \"\n            \"appear once in an inversion.\", projection_id, index,\n            task->get_task_name(), task->get_unique_id())\n      if (unique_points.find(task->index_point) == unique_points.end())\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT,\n            \"Projection functor %d produced an invalid inversion result \"\n            \"that does not contain the original point for region requirement \"\n            \"%d of task %s (UID %lld).\", projection_id, index,\n            task->get_task_name(), task->get_unique_id())\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::check_containment(const Task *task, unsigned index,\n                                         const std::vector<DomainPoint> &points)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      for (std::vector<DomainPoint>::const_iterator it = \n            points.begin(); it != points.end(); it++)\n      {\n        if ((*it) == task->index_point)\n          return;\n      }\n      REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_RESULT,\n          \"Projection functor %d produced an invalid inversion result \"\n          \"that does not contain the original point for region requirement \"\n          \"%d of task %s (UID %lld).\", projection_id, index,\n          task->get_task_name(), task->get_unique_id())\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    bool ProjectionFunction::is_complete(RegionTreeNode *node, Operation *op,\n                         unsigned index, IndexSpaceNode *projection_space) const\n    //--------------------------------------------------------------------------\n    {\n      Domain launch_domain;\n      projection_space->get_launch_space_domain(launch_domain);\n      if (node->is_region())\n      {\n        RegionNode *region = node->as_region_node();\n        if (is_functional)\n        {\n          if (is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            return functor->is_complete(region->handle, launch_domain);\n          }\n          else\n            return functor->is_complete(region->handle, launch_domain);\n        }\n        else\n        {\n          Mappable *mappable = op->get_mappable();\n          if (is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            return functor->is_complete(mappable, index,\n                                        region->handle, launch_domain);\n          }\n          else\n            return functor->is_complete(mappable, index, \n                                        region->handle, launch_domain);\n        }\n      }\n      else\n      {\n        PartitionNode *partition = node->as_partition_node();\n        if (is_functional)\n        {\n          if (is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            return functor->is_complete(partition->handle, launch_domain);\n          }\n          else\n            return functor->is_complete(partition->handle, launch_domain);\n        }\n        else\n        {\n          Mappable *mappable = op->get_mappable();\n          if (is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            return functor->is_complete(mappable, index,\n                                        partition->handle, launch_domain);\n          }\n          else\n            return functor->is_complete(mappable, index,\n                                        partition->handle, launch_domain);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionFunction::ElideCloseResult::ElideCloseResult(void)\n      : node(NULL), result(false)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionFunction::ElideCloseResult::ElideCloseResult(IndexTreeNode *n,\n        const std::set<ProjectionSummary> &proj, bool res)\n      : node(n), projections(proj), result(res)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    bool ProjectionFunction::ElideCloseResult::matches(IndexTreeNode *other,\n                     const std::set<ProjectionSummary> &other_projections) const\n    //--------------------------------------------------------------------------\n    {\n      if (node != other)\n        return false;\n      if (projections.size() != other_projections.size())\n        return false;\n      std::set<ProjectionSummary>::const_iterator it1 = projections.begin();\n      std::set<ProjectionSummary>::const_iterator it2 = \n        other_projections.begin();\n      while (it1 != projections.end())\n      {\n        if (it1 != it2)\n          return false;\n        it1++; it2++;\n      }\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    bool ProjectionFunction::find_elide_close_result(const ProjectionInfo &info,\n                                 const std::set<ProjectionSummary> &projections, \n                                 RegionTreeNode *node, bool &result,\n                                 std::set<RtEvent> &applied_events) const\n    //--------------------------------------------------------------------------\n    {\n      // No memoizing if we're not functional\n      if (!is_functional)\n        return false;\n      ProjectionSummary key(info, applied_events);\n      IndexTreeNode *row_source = node->get_row_source();\n      AutoLock p_lock(projection_reservation,1,false/*exclusive*/);\n      std::map<ProjectionSummary,std::vector<ElideCloseResult> >::const_iterator\n        finder = elide_close_results.find(key);\n      if (finder == elide_close_results.end())\n        return false;\n      for (std::vector<ElideCloseResult>::const_iterator it = \n            finder->second.begin(); it != finder->second.end(); it++)\n      {\n        if (it->matches(row_source, projections))\n        {\n          result = it->result;\n          return true;\n        }\n      }\n      return false;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::record_elide_close_result(\n                                const ProjectionInfo &info,\n                                const std::set<ProjectionSummary> &projections,\n                                RegionTreeNode *node, bool result,\n                                std::set<RtEvent> &applied_events)\n    //--------------------------------------------------------------------------\n    {\n      if (!is_functional)\n        return;\n      ProjectionSummary key(info, applied_events);\n      IndexTreeNode *row_source = node->get_row_source();\n      AutoLock p_lock(projection_reservation);\n      std::vector<ElideCloseResult> &close_results = elide_close_results[key];\n      // See if someone else saved the result in between to avoid duplicates\n      for (std::vector<ElideCloseResult>::const_iterator it = \n            close_results.begin(); it != close_results.end(); it++)\n        if (it->matches(row_source, projections))\n          return;\n      close_results.push_back(ElideCloseResult(row_source, projections,result));\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionTree* ProjectionFunction::construct_projection_tree(Operation *op,\n                            unsigned index, RegionTreeNode *root, \n                            IndexSpaceNode *launch_space,\n                            ShardingFunction *sharding_function, \n                            IndexSpaceNode *sharding_space) const\n    //--------------------------------------------------------------------------\n    {\n      Mappable *mappable = is_functional ? NULL : op->get_mappable();\n      IndexTreeNode *row_source = root->get_row_source();\n      RegionTreeForest *context = root->context;\n      ProjectionTree *result = new ProjectionTree(row_source);\n      std::map<IndexTreeNode*,ProjectionTree*> node_map;\n      node_map[row_source] = result;\n      // Iterate over the points, compute the projections, and build the tree   \n      Domain launch_domain, sharding_domain;\n      launch_space->get_launch_space_domain(launch_domain);\n      if ((sharding_function != NULL) && (launch_space != sharding_space))\n        sharding_space->get_launch_space_domain(sharding_domain);\n      else\n        sharding_domain = launch_domain;\n      if (root->is_region())\n      {\n        RegionNode *region = root->as_region_node();\n        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n        {\n          LogicalRegion result;\n          if (!is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            if (is_functional)\n              result = functor->project(region->handle, itr.p, launch_domain);\n            else\n              result = functor->project(mappable, index, region->handle, itr.p);\n          }\n          else\n          {\n            if (is_functional)\n              result = functor->project(region->handle, itr.p, launch_domain);\n            else\n              result = functor->project(mappable, index, region->handle, itr.p);\n          }\n          if (!result.exists())\n            continue;\n          if (sharding_function != NULL)\n          {\n            ShardID own = sharding_function->find_owner(itr.p, sharding_domain);\n            add_to_projection_tree(result, row_source, context, node_map, own);\n          }\n          else\n            add_to_projection_tree(result, row_source, context, node_map);\n        }\n      }\n      else\n      {\n        PartitionNode *partition = root->as_partition_node();\n        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n        {\n          LogicalRegion result;\n          if (!is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            if (is_functional)\n              result = functor->project(partition->handle, itr.p,launch_domain);\n            else\n              result = functor->project(mappable,index,partition->handle,itr.p);\n          }\n          else\n          {\n            if (is_functional)\n              result = functor->project(partition->handle, itr.p,launch_domain);\n            else\n              result = functor->project(mappable,index,partition->handle,itr.p);\n          }\n          if (!result.exists())\n            continue;\n          if (sharding_function != NULL)\n          {\n            ShardID own = sharding_function->find_owner(itr.p, sharding_domain);\n            add_to_projection_tree(result, row_source, context, node_map, own);\n          }\n          else\n            add_to_projection_tree(result, row_source, context, node_map);\n        }\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void ProjectionFunction::construct_projection_tree(Operation *op,\n          unsigned index, RegionTreeNode *root, IndexSpaceNode *launch_space, \n          ShardingFunction *sharding_function, IndexSpaceNode *sharding_space, \n          std::map<IndexTreeNode*,ProjectionTree*> &node_map) const\n    //--------------------------------------------------------------------------\n    {\n      Mappable *mappable = is_functional ? NULL : op->get_mappable();\n      IndexTreeNode *row_source = root->get_row_source();\n      RegionTreeForest *context = root->context;\n      // Iterate over the points, compute the projections, and build the tree   \n      Domain launch_domain, sharding_domain;\n      launch_space->get_launch_space_domain(launch_domain);\n      if ((sharding_function != NULL) && (launch_space != sharding_space))\n        sharding_space->get_launch_space_domain(sharding_domain);\n      else\n        sharding_domain = launch_domain;\n      if (root->is_region())\n      {\n        RegionNode *region = root->as_region_node();\n        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n        {\n          LogicalRegion result;\n          if (!is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            if (is_functional)\n              result = functor->project(region->handle, itr.p, launch_domain);\n            else\n              result = functor->project(mappable, index, region->handle, itr.p);\n          }\n          else\n          {\n            if (is_functional)\n              result = functor->project(region->handle, itr.p, launch_domain);\n            else\n              result = functor->project(mappable, index, region->handle, itr.p);\n          }\n          if (!result.exists())\n            continue;\n          if (sharding_function != NULL)\n          {\n            ShardID own = sharding_function->find_owner(itr.p, sharding_domain);\n            add_to_projection_tree(result, row_source, context, node_map, own);\n          }\n          else\n            add_to_projection_tree(result, row_source, context, node_map);\n        }\n      }\n      else\n      {\n        PartitionNode *partition = root->as_partition_node();\n        for (Domain::DomainPointIterator itr(launch_domain); itr; itr++)\n        {\n          LogicalRegion result;\n          if (!is_exclusive)\n          {\n            AutoLock p_lock(projection_reservation);\n            if (is_functional)\n              result = functor->project(partition->handle, itr.p,launch_domain);\n            else\n              result = functor->project(mappable,index,partition->handle,itr.p);\n          }\n          else\n          {\n            if (is_functional)\n              result = functor->project(partition->handle, itr.p,launch_domain);\n            else\n              result = functor->project(mappable,index,partition->handle,itr.p);\n          }\n          if (!result.exists())\n            continue;\n          if (sharding_function != NULL)\n          {\n            ShardID own = sharding_function->find_owner(itr.p, sharding_domain);\n            add_to_projection_tree(result, row_source, context, node_map, own);\n          }\n          else\n            add_to_projection_tree(result, row_source, context, node_map);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void ProjectionFunction::add_to_projection_tree(LogicalRegion r,\n                             IndexTreeNode *root, RegionTreeForest *context,\n                             std::map<IndexTreeNode*,ProjectionTree*> &node_map,\n                             ShardID owner_shard)\n    //--------------------------------------------------------------------------\n    {\n      IndexTreeNode *child = context->get_node(r)->row_source;\n      std::map<IndexTreeNode*,ProjectionTree*>::const_iterator finder = \n        node_map.find(child);\n      ProjectionTree *current = NULL;\n      if (finder == node_map.end())\n      {\n        current = new ProjectionTree(child, owner_shard);\n        node_map[child] = current;\n      }\n      else\n        current = finder->second;\n      while (child != root)\n      {\n        // Find the next one to add this to\n        IndexTreeNode *parent = child->get_parent();\n        finder = node_map.find(parent);\n        ProjectionTree *next = NULL;\n        if (finder == node_map.end())\n        {\n          next = new ProjectionTree(parent);\n          node_map[parent] = next;\n        }\n        else\n          next = finder->second;\n        next->add_child(current);\n        // Now we can walk up the tree\n        child = parent;\n        current = next;\n      }\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Cyclic Sharding Functor\n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    CyclicShardingFunctor::CyclicShardingFunctor(void)\n      : ShardingFunctor()\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    CyclicShardingFunctor::CyclicShardingFunctor(\n                                               const CyclicShardingFunctor &rhs)\n      : ShardingFunctor()\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    CyclicShardingFunctor::~CyclicShardingFunctor(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    CyclicShardingFunctor& CyclicShardingFunctor::operator=(\n                                               const CyclicShardingFunctor &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    template<int DIM>\n    size_t CyclicShardingFunctor::linearize_point(\n                                   const Realm::IndexSpace<DIM,coord_t> &is,\n                                   const Realm::Point<DIM,coord_t> &point) const\n    //--------------------------------------------------------------------------\n    {\n      if (is.dense())\n      {\n        Realm::AffineLinearizedIndexSpace<DIM,coord_t> linearizer(is);\n        return linearizer.linearize(point);\n      }\n      else\n      {\n        size_t offset = 0;\n        for (Realm::IndexSpaceIterator<DIM,coord_t> it(is); it.valid; it.step())\n        {\n          if (it.rect.contains(point))\n          {\n            Realm::AffineLinearizedIndexSpace<DIM,coord_t> \n              linearizer(Realm::IndexSpace<DIM,coord_t>(it.rect));\n            return offset + linearizer.linearize(point);\n          }\n          else\n            offset += it.rect.volume();\n        }\n        return offset;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    ShardID CyclicShardingFunctor::shard(const DomainPoint &point,\n                                         const Domain &full_space,\n                                         const size_t total_shards)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(point.get_dim() == full_space.get_dim());\n#endif\n      switch (point.get_dim())\n      {\n#define DIMFUNC(DIM) \\\n        case DIM: \\\n          { \\\n            const DomainT<DIM,coord_t> is = full_space; \\\n            const Point<DIM,coord_t> p1 = point; \\\n            return (linearize_point<DIM>(is, p1) % total_shards); \\\n          }\n        LEGION_FOREACH_N(DIMFUNC)\n#undef DIMFUNC\n        default:\n          assert(false);\n      }\n      return 0;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Sharding Function \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    ShardingFunction::ShardingFunction(ShardingFunctor *func, \n              RegionTreeForest *f, ShardingID id, size_t total)\n      : functor(func), forest(f), sharding_id(id), total_shards(total)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ShardingFunction::ShardingFunction(const ShardingFunction &rhs)\n      : functor(NULL), forest(NULL), sharding_id(0), total_shards(0)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    ShardingFunction::~ShardingFunction(void)\n    //--------------------------------------------------------------------------\n    {\n    }\n\n    //--------------------------------------------------------------------------\n    ShardingFunction& ShardingFunction::operator=(const ShardingFunction &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    ShardID ShardingFunction::find_owner(const DomainPoint &point,\n                                         const Domain &sharding_space)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(sharding_space.contains(point));\n#endif\n      ShardID result = functor->shard(point, sharding_space, total_shards);\n      if (total_shards <= result)\n        REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARDING_FUNCTOR_OUTPUT,\n                            \"Illegal output shard %d from sharding functor %d. \"\n                            \"Shards for this index space launch must be \"\n                            \"between 0 and %zd (exclusive).\", result,\n                            sharding_id, total_shards)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace ShardingFunction::find_shard_space(ShardID shard,\n                             IndexSpaceNode *full_space, IndexSpace shard_space)\n    //--------------------------------------------------------------------------\n    {\n      const ShardKey key(shard, full_space->handle, shard_space);\n      // Check to see if we already have it\n      {\n        AutoLock s_lock(sharding_lock,1,false/*exclusive*/);\n        std::map<ShardKey,IndexSpace>::const_iterator \n          finder = shard_index_spaces.find(key);\n        if (finder != shard_index_spaces.end())\n          return finder->second;\n      }\n      // Otherwise we need to make it\n      IndexSpace result = \n        full_space->create_shard_space(this, shard, shard_space);\n      AutoLock s_lock(sharding_lock);\n      shard_index_spaces[key] = result;\n      return result;\n    }\n\n    /////////////////////////////////////////////////////////////\n    // Legion Runtime \n    /////////////////////////////////////////////////////////////\n\n    //--------------------------------------------------------------------------\n    Runtime::Runtime(Machine m, const LegionConfiguration &config, \n                     bool background, InputArgs args, AddressSpaceID unique,\n                     const std::set<Processor> &locals,\n                     const std::set<Processor> &local_utilities,\n                     const std::set<AddressSpaceID> &address_spaces,\n                     const std::map<Processor,AddressSpaceID> &processor_spaces,\n                     bool default_mapper)\n      : external(new Legion::Runtime(this)),\n        mapper_runtime(new Legion::Mapping::MapperRuntime()),\n        machine(m), address_space(unique), \n        total_address_spaces(address_spaces.size()),\n        runtime_stride(address_spaces.size()), profiler(NULL),\n        forest(new RegionTreeForest(this)), virtual_manager(NULL), \n        num_utility_procs(local_utilities.empty() ? locals.size() : \n                          local_utilities.size()), input_args(args),\n        initial_task_window_size(config.initial_task_window_size),\n        initial_task_window_hysteresis(config.initial_task_window_hysteresis),\n        initial_tasks_to_schedule(config.initial_tasks_to_schedule),\n        initial_meta_task_vector_width(config.initial_meta_task_vector_width),\n        eager_alloc_percentage(config.eager_alloc_percentage),\n        max_message_size(config.max_message_size),\n        gc_epoch_size(config.gc_epoch_size),\n        max_control_replication_contexts(\n                      config.max_control_replication_contexts),\n        max_local_fields(config.max_local_fields),\n        max_replay_parallelism(config.max_replay_parallelism),\n        safe_control_replication(config.safe_control_replication),\n        program_order_execution(config.program_order_execution),\n        dump_physical_traces(config.dump_physical_traces),\n        no_tracing(config.no_tracing),\n        no_physical_tracing(config.no_physical_tracing),\n        no_trace_optimization(config.no_trace_optimization),\n        no_fence_elision(config.no_fence_elision),\n        replay_on_cpus(config.replay_on_cpus),\n        verify_partitions(config.verify_partitions),\n        runtime_warnings(config.runtime_warnings),\n        warnings_backtrace(config.warnings_backtrace),\n        report_leaks(config.report_leaks),\n        separate_runtime_instances(config.separate_runtime_instances),\n        record_registration(config.record_registration),\n        stealing_disabled(config.stealing_disabled),\n        resilient_mode(config.resilient_mode),\n        unsafe_launch(config.unsafe_launch),\n#ifdef DEBUG_LEGION\n        unsafe_mapper(config.unsafe_mapper),\n#else\n        unsafe_mapper(!config.safe_mapper),\n#endif\n        disable_independence_tests(config.disable_independence_tests),\n#ifdef LEGION_SPY\n        legion_spy_enabled(true),\n#else\n        legion_spy_enabled(config.legion_spy_enabled),\n#endif\n        supply_default_mapper(default_mapper),\n        enable_test_mapper(config.enable_test_mapper),\n        legion_ldb_enabled(!config.ldb_file.empty()),\n        replay_file(legion_ldb_enabled ? config.ldb_file : config.replay_file),\n#ifdef DEBUG_LEGION\n        logging_region_tree_state(config.logging_region_tree_state),\n        verbose_logging(config.verbose_logging),\n        logical_logging_only(config.logical_logging_only),\n        physical_logging_only(config.physical_logging_only),\n#endif\n        check_privileges(config.check_privileges),\n        dump_free_ranges(config.dump_free_ranges),\n        num_profiling_nodes(config.num_profiling_nodes),\n        legion_collective_radix(config.legion_collective_radix),\n        mpi_rank_table((mpi_rank >= 0) ? new MPIRankTable(this) : NULL),\n        prepared_for_shutdown(false), total_outstanding_tasks(0), \n        // In the case where the runtime is backgrounded, have node 0 keep\n        // a reference for each node so that we wait until we see all wait\n        // call from each node before we start trying to perform a shutdown\n        outstanding_top_level_tasks(\n            ((unique == 0) && background) ? total_address_spaces : 0),\n        local_procs(locals), local_utils(local_utilities),\n        proc_spaces(processor_spaces),\n        unique_index_space_id((unique == 0) ? runtime_stride : unique),\n        unique_index_partition_id((unique == 0) ? runtime_stride : unique), \n        unique_field_space_id((unique == 0) ? runtime_stride : unique),\n        unique_index_tree_id((unique == 0) ? runtime_stride : unique),\n        unique_region_tree_id((unique == 0) ? runtime_stride : unique),\n        unique_operation_id((unique == 0) ? runtime_stride : unique),\n        unique_field_id(LEGION_MAX_APPLICATION_FIELD_ID + \n                        ((unique == 0) ? runtime_stride : unique)),\n        unique_code_descriptor_id(LG_TASK_ID_AVAILABLE +\n#ifdef LEGION_GPU_REDUCTIONS\n                        get_gpu_reduction_table().size() + \n#endif\n                        ((unique == 0) ? runtime_stride : unique)),\n        unique_constraint_id((unique == 0) ? runtime_stride : unique),\n        unique_is_expr_id((unique == 0) ? runtime_stride : unique),\n        unique_control_replication_id((unique == 0) ? runtime_stride : unique),\n#ifdef LEGION_SPY\n        unique_indirections_id((unique == 0) ? runtime_stride : unique),\n#endif\n        unique_task_id(get_current_static_task_id()+unique),\n        unique_mapper_id(get_current_static_mapper_id()+unique),\n        unique_trace_id(get_current_static_trace_id()+unique),\n        unique_projection_id(get_current_static_projection_id()+unique),\n        unique_sharding_id(get_current_static_sharding_id()+unique),\n        unique_redop_id(get_current_static_reduction_id()+unique),\n        unique_serdez_id(get_current_static_serdez_id()+unique),\n        unique_library_mapper_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_library_trace_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_library_projection_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_library_sharding_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_library_task_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_library_redop_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_library_serdez_id(LEGION_INITIAL_LIBRARY_ID_OFFSET),\n        unique_distributed_id((unique == 0) ? runtime_stride : unique)\n    //--------------------------------------------------------------------------\n    {\n      log_run.debug(\"Initializing Legion runtime in address space %x\",\n                            address_space);\n      // Construct a local utility processor group\n      if (local_utils.empty())\n      {\n        // make the utility group the set of all the local processors\n#ifdef DEBUG_LEGION\n        assert(!locals.empty());\n#endif\n        if (locals.size() == 1)\n          utility_group = *(locals.begin());\n        else\n        {\n          std::vector<Processor> util_group(locals.begin(), locals.end());\n          utility_group = ProcessorGroup::create_group(util_group);\n        }\n      }\n      else if (local_utils.size() == 1)\n        utility_group = *(local_utils.begin());\n      else\n      {\n        std::vector<Processor> util_g(local_utils.begin(), local_utils.end());\n        utility_group = ProcessorGroup::create_group(util_g);\n      }\n#ifdef DEBUG_LEGION\n      assert(utility_group.exists());\n#endif\n      Machine::ProcessorQuery all_procs(machine); \n      // For each of the processors in our local set construct a manager\n      for (std::set<Processor>::const_iterator it = local_procs.begin();\n            it != local_procs.end(); it++)\n      {\n#ifdef DEBUG_LEGION\n        assert((*it).kind() != Processor::UTIL_PROC);\n#endif\n        ProcessorManager *manager = new ProcessorManager(*it,\n\t\t\t\t    (*it).kind(), this,\n                                    LEGION_DEFAULT_MAPPER_SLOTS, \n                                    stealing_disabled,\n                                    !replay_file.empty());\n        proc_managers[*it] = manager;\n      }\n      // Initialize the message manager array so that we can construct\n      // message managers lazily as they are needed\n      for (unsigned idx = 0; idx < LEGION_MAX_NUM_NODES; idx++)\n        message_managers[idx] = NULL;\n      \n      // Make the default number of contexts\n      // No need to hold the lock yet because nothing is running\n      for (total_contexts = 0; total_contexts < LEGION_DEFAULT_CONTEXTS; \n            total_contexts++)\n      {\n        available_contexts.push_back(RegionTreeContext(total_contexts)); \n      }\n      // Initialize our random number generator state\n      random_state[0] = address_space & 0xFFFF; // low-order bits of node ID \n      random_state[1] = (address_space >> 16) & 0xFFFF; // high-order bits\n      random_state[2] = LEGION_INIT_SEED;\n      // Do some mixing\n      for (int i = 0; i < 256; i++)\n        nrand48(random_state);\n      // Initialize our profiling instance\n      if (address_space < num_profiling_nodes)\n        initialize_legion_prof(config);\n#ifdef TRACE_ALLOCATION\n      allocation_tracing_count = 0;\n      // Instantiate all the kinds of allocations\n      for (unsigned idx = ARGUMENT_MAP_ALLOC; idx < LAST_ALLOC; idx++)\n        allocation_manager[((AllocationType)idx)] = AllocationTracker();\n#endif\n#ifdef LEGION_GC\n      {\n        REFERENCE_NAMES_ARRAY(reference_names);\n        for (unsigned idx = 0; idx < LAST_SOURCE_REF; idx++)\n        {\n          log_garbage.info(\"GC Source Kind %d %s\", idx, reference_names[idx]);\n        }\n      }\n#endif \n#ifdef DEBUG_LEGION\n      if (logging_region_tree_state)\n      {\n\ttree_state_logger = new TreeStateLogger(address_space, \n                                                verbose_logging,\n                                                logical_logging_only,\n                                                physical_logging_only);\n\tassert(tree_state_logger != NULL);\n      } else {\n\ttree_state_logger = NULL;\n      }\n#endif\n#ifdef DEBUG_SHUTDOWN_HANG\n      outstanding_counts.resize(LG_LAST_TASK_ID, 0);\n#endif\n      // Attach any accessor debug hooks for privilege or bounds checks\n#ifdef LEGION_PRIVILEGE_CHECKS\n      LegionRuntime::Accessor::DebugHooks::find_privilege_task_name =\n\t&Legion::Internal::Runtime::find_privilege_task_name;\n#endif\n#ifdef LEGION_BOUNDS_CHECKS\n      LegionRuntime::Accessor::DebugHooks::check_bounds_ptr =\n\t&Legion::Internal::Runtime::check_bounds;\n      LegionRuntime::Accessor::DebugHooks::check_bounds_dpoint =\n\t&Legion::Internal::Runtime::check_bounds;\n#endif \n    }\n\n    //--------------------------------------------------------------------------\n    Runtime::Runtime(const Runtime &rhs)\n      : external(NULL), mapper_runtime(NULL), machine(rhs.machine), \n        address_space(0), total_address_spaces(0), runtime_stride(0), \n        profiler(NULL), forest(NULL), \n        num_utility_procs(rhs.num_utility_procs), input_args(rhs.input_args),\n        initial_task_window_size(rhs.initial_task_window_size),\n        initial_task_window_hysteresis(rhs.initial_task_window_hysteresis),\n        initial_tasks_to_schedule(rhs.initial_tasks_to_schedule),\n        initial_meta_task_vector_width(rhs.initial_meta_task_vector_width),\n        eager_alloc_percentage(rhs.eager_alloc_percentage),\n        max_message_size(rhs.max_message_size),\n        gc_epoch_size(rhs.gc_epoch_size), \n        max_control_replication_contexts(rhs.max_control_replication_contexts),\n        max_local_fields(rhs.max_local_fields),\n        max_replay_parallelism(rhs.max_replay_parallelism),\n        safe_control_replication(rhs.safe_control_replication),\n        program_order_execution(rhs.program_order_execution),\n        dump_physical_traces(rhs.dump_physical_traces),\n        no_tracing(rhs.no_tracing),\n        no_physical_tracing(rhs.no_physical_tracing),\n        no_trace_optimization(rhs.no_trace_optimization),\n        no_fence_elision(rhs.no_fence_elision),\n        replay_on_cpus(rhs.replay_on_cpus),\n        verify_partitions(rhs.verify_partitions),\n        runtime_warnings(rhs.runtime_warnings),\n        warnings_backtrace(rhs.warnings_backtrace),\n        report_leaks(rhs.report_leaks),\n        separate_runtime_instances(rhs.separate_runtime_instances),\n        record_registration(rhs.record_registration),\n        stealing_disabled(rhs.stealing_disabled),\n        resilient_mode(rhs.resilient_mode),\n        unsafe_launch(rhs.unsafe_launch),\n        unsafe_mapper(rhs.unsafe_mapper),\n        disable_independence_tests(rhs.disable_independence_tests),\n        legion_spy_enabled(rhs.legion_spy_enabled),\n        supply_default_mapper(rhs.supply_default_mapper),\n        enable_test_mapper(rhs.enable_test_mapper),\n        legion_ldb_enabled(rhs.legion_ldb_enabled),\n        replay_file(rhs.replay_file),\n#ifdef DEBUG_LEGION\n        logging_region_tree_state(rhs.logging_region_tree_state),\n        verbose_logging(rhs.verbose_logging),\n        logical_logging_only(rhs.logical_logging_only),\n        physical_logging_only(rhs.physical_logging_only),\n#endif\n        check_privileges(rhs.check_privileges),\n        dump_free_ranges(rhs.dump_free_ranges),\n        num_profiling_nodes(rhs.num_profiling_nodes),\n        legion_collective_radix(rhs.legion_collective_radix),\n        mpi_rank_table(NULL), local_procs(rhs.local_procs), \n        local_utils(rhs.local_utils), proc_spaces(rhs.proc_spaces)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n    }\n\n    //--------------------------------------------------------------------------\n    Runtime::~Runtime(void)\n    //--------------------------------------------------------------------------\n    {\n      // Make sure we don't send anymore messages\n      for (unsigned idx = 0; idx < LEGION_MAX_NUM_NODES; idx++)\n      {\n        if (message_managers[idx] != NULL)\n        {\n          delete message_managers[idx];\n          message_managers[idx] = NULL;\n        }\n      }\n      if (profiler != NULL)\n      {\n        delete profiler;\n        profiler = NULL;\n      } \n      delete forest;\n      delete external;\n      delete mapper_runtime;\n      // Avoid duplicate deletions on these for separate runtime\n      // instances by just leaking them for now\n      if (!separate_runtime_instances)\n      {\n        for (std::map<ProjectionID,ProjectionFunction*>::\n              iterator it = projection_functions.begin(); \n              it != projection_functions.end(); it++)\n        {\n          delete it->second;\n        } \n        projection_functions.clear();\n        for (std::map<ShardingID,ShardingFunctor*>::iterator it = \n              sharding_functors.begin(); it != \n              sharding_functors.end(); it++)\n        {\n          delete it->second;\n        }\n        sharding_functors.clear();\n      }\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        delete it->second;\n      }\n      proc_managers.clear(); \n      for (std::deque<IndividualTask*>::const_iterator it = \n            available_individual_tasks.begin(); \n            it != available_individual_tasks.end(); it++)\n      {\n        delete (*it);\n      }\n      available_individual_tasks.clear();\n      for (std::deque<PointTask*>::const_iterator it = \n            available_point_tasks.begin(); it != \n            available_point_tasks.end(); it++)\n      {\n        delete (*it);\n      }\n      available_point_tasks.clear();\n      for (std::deque<IndexTask*>::const_iterator it = \n            available_index_tasks.begin(); it != \n            available_index_tasks.end(); it++)\n      {\n        delete (*it);\n      }\n      available_index_tasks.clear();\n      for (std::deque<SliceTask*>::const_iterator it = \n            available_slice_tasks.begin(); it != \n            available_slice_tasks.end(); it++)\n      {\n        delete (*it);\n      }\n      available_slice_tasks.clear();\n      for (std::deque<MapOp*>::const_iterator it = \n            available_map_ops.begin(); it != \n            available_map_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_map_ops.clear();\n      for (std::deque<CopyOp*>::const_iterator it = \n            available_copy_ops.begin(); it != \n            available_copy_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_copy_ops.clear();\n      for (std::deque<FenceOp*>::const_iterator it = \n            available_fence_ops.begin(); it != \n            available_fence_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_fence_ops.clear();\n      for (std::deque<FrameOp*>::const_iterator it = \n            available_frame_ops.begin(); it !=\n            available_frame_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_frame_ops.clear();\n      for (std::deque<CreationOp*>::const_iterator it = \n            available_creation_ops.begin(); it != \n            available_creation_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_creation_ops.clear();\n      for (std::deque<DeletionOp*>::const_iterator it = \n            available_deletion_ops.begin(); it != \n            available_deletion_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_deletion_ops.clear();\n      for (std::deque<MergeCloseOp*>::const_iterator it = \n            available_merge_close_ops.begin(); it !=\n            available_merge_close_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_merge_close_ops.clear();\n      for (std::deque<PostCloseOp*>::const_iterator it = \n            available_post_close_ops.begin(); it !=\n            available_post_close_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_post_close_ops.clear();\n      for (std::deque<VirtualCloseOp*>::const_iterator it = \n            available_virtual_close_ops.begin(); it !=\n            available_virtual_close_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_virtual_close_ops.clear();\n      for (std::deque<RefinementOp*>::const_iterator it =\n            available_refinement_ops.begin(); it !=\n            available_refinement_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_refinement_ops.clear();\n      for (std::deque<AdvisementOp*>::const_iterator it =\n            available_advisement_ops.begin(); it !=\n            available_advisement_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_advisement_ops.clear();\n      for (std::deque<DynamicCollectiveOp*>::const_iterator it = \n            available_dynamic_collective_ops.begin(); it !=\n            available_dynamic_collective_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_dynamic_collective_ops.end();\n      for (std::deque<FuturePredOp*>::const_iterator it = \n            available_future_pred_ops.begin(); it !=\n            available_future_pred_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_future_pred_ops.clear();\n      for (std::deque<NotPredOp*>::const_iterator it = \n            available_not_pred_ops.begin(); it !=\n            available_not_pred_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_not_pred_ops.clear();\n      for (std::deque<AndPredOp*>::const_iterator it = \n            available_and_pred_ops.begin(); it !=\n            available_and_pred_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_and_pred_ops.clear();\n      for (std::deque<OrPredOp*>::const_iterator it = \n            available_or_pred_ops.begin(); it !=\n            available_or_pred_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_or_pred_ops.clear();\n      for (std::deque<AcquireOp*>::const_iterator it = \n            available_acquire_ops.begin(); it !=\n            available_acquire_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_acquire_ops.clear();\n      for (std::deque<ReleaseOp*>::const_iterator it = \n            available_release_ops.begin(); it !=\n            available_release_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_release_ops.clear();\n      for (std::deque<TraceCaptureOp*>::const_iterator it = \n            available_capture_ops.begin(); it !=\n            available_capture_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_capture_ops.clear();\n      for (std::deque<TraceCompleteOp*>::const_iterator it = \n            available_trace_ops.begin(); it !=\n            available_trace_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_trace_ops.clear();\n      for (std::deque<TraceReplayOp*>::const_iterator it = \n            available_replay_ops.begin(); it !=\n            available_replay_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_replay_ops.clear();\n      for (std::deque<TraceBeginOp*>::const_iterator it = \n            available_begin_ops.begin(); it !=\n            available_begin_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_begin_ops.clear();\n      for (std::deque<TraceSummaryOp*>::const_iterator it = \n            available_summary_ops.begin(); it !=\n            available_summary_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_summary_ops.clear();\n      for (std::deque<MustEpochOp*>::const_iterator it = \n            available_epoch_ops.begin(); it !=\n            available_epoch_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_epoch_ops.clear();\n      for (std::deque<PendingPartitionOp*>::const_iterator it = \n            available_pending_partition_ops.begin(); it !=\n            available_pending_partition_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_pending_partition_ops.clear();\n      for (std::deque<DependentPartitionOp*>::const_iterator it = \n            available_dependent_partition_ops.begin(); it !=\n            available_dependent_partition_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_dependent_partition_ops.clear();\n      for (std::deque<FillOp*>::const_iterator it = \n            available_fill_ops.begin(); it !=\n            available_fill_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_fill_ops.clear();\n      for (std::deque<AttachOp*>::const_iterator it = \n            available_attach_ops.begin(); it !=\n            available_attach_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_attach_ops.clear();\n      for (std::deque<DetachOp*>::const_iterator it = \n            available_detach_ops.begin(); it !=\n            available_detach_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_detach_ops.clear();\n      for (std::deque<TimingOp*>::const_iterator it = \n            available_timing_ops.begin(); it != \n            available_timing_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_timing_ops.clear();\n      for (std::deque<AllReduceOp*>::const_iterator it = \n            available_all_reduce_ops.begin(); it !=\n            available_all_reduce_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_all_reduce_ops.clear();\n      for (std::deque<ReplIndividualTask*>::const_iterator it = \n            available_repl_individual_tasks.begin(); it !=\n            available_repl_individual_tasks.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_individual_tasks.clear();\n      for (std::deque<ReplIndexTask*>::const_iterator it = \n            available_repl_index_tasks.begin(); it !=\n            available_repl_index_tasks.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_index_tasks.clear();\n      for (std::deque<ReplMergeCloseOp*>::const_iterator it = \n            available_repl_merge_close_ops.begin(); it !=\n            available_repl_merge_close_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_merge_close_ops.clear();\n      for (std::deque<ReplRefinementOp*>::const_iterator it =\n            available_repl_refinement_ops.begin(); it !=\n            available_repl_refinement_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_refinement_ops.clear();\n      for (std::deque<ReplFillOp*>::const_iterator it = \n            available_repl_fill_ops.begin(); it !=\n            available_repl_fill_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_fill_ops.clear();\n      for (std::deque<ReplIndexFillOp*>::const_iterator it = \n            available_repl_index_fill_ops.begin(); it !=\n            available_repl_index_fill_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_index_fill_ops.clear();\n      for (std::deque<ReplCopyOp*>::const_iterator it = \n            available_repl_copy_ops.begin(); it !=\n            available_repl_copy_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_copy_ops.clear();\n      for (std::deque<ReplIndexCopyOp*>::const_iterator it = \n            available_repl_index_copy_ops.begin(); it !=\n            available_repl_index_copy_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_index_copy_ops.clear();\n      for (std::deque<ReplDeletionOp*>::const_iterator it = \n            available_repl_deletion_ops.begin(); it !=\n            available_repl_deletion_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_deletion_ops.clear();\n      for (std::deque<ReplPendingPartitionOp*>::const_iterator it = \n            available_repl_pending_partition_ops.begin(); it !=\n            available_repl_pending_partition_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_pending_partition_ops.clear();\n      for (std::deque<ReplDependentPartitionOp*>::const_iterator it = \n            available_repl_dependent_partition_ops.begin(); it !=\n            available_repl_dependent_partition_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_dependent_partition_ops.clear();\n      for (std::deque<ReplMustEpochOp*>::const_iterator it = \n            available_repl_must_epoch_ops.begin(); it !=\n            available_repl_must_epoch_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_must_epoch_ops.clear();\n      for (std::deque<ReplTimingOp*>::const_iterator it = \n            available_repl_timing_ops.begin(); it !=\n            available_repl_timing_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_timing_ops.clear();\n      for (std::deque<ReplAllReduceOp*>::const_iterator it = \n            available_repl_all_reduce_ops.begin(); it !=\n            available_repl_all_reduce_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_all_reduce_ops.clear();\n      for (std::deque<ReplFenceOp*>::const_iterator it = \n            available_repl_fence_ops.begin(); it !=\n            available_repl_fence_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_fence_ops.clear();\n      for (std::deque<ReplMapOp*>::const_iterator it = \n            available_repl_map_ops.begin(); it != \n            available_repl_map_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_map_ops.clear();\n      for (std::deque<ReplAttachOp*>::const_iterator it = \n            available_repl_attach_ops.begin(); it !=\n            available_repl_attach_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_attach_ops.clear();\n      for (std::deque<ReplDetachOp*>::const_iterator it = \n            available_repl_detach_ops.begin(); it !=\n            available_repl_detach_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_detach_ops.clear();\n      for (std::deque<ReplTraceCaptureOp*>::const_iterator it = \n            available_repl_capture_ops.begin(); it !=\n            available_repl_capture_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_capture_ops.clear();\n      for (std::deque<ReplTraceCompleteOp*>::const_iterator it = \n            available_repl_trace_ops.begin(); it !=\n            available_repl_trace_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_trace_ops.clear();\n      for (std::deque<ReplTraceReplayOp*>::const_iterator it = \n            available_repl_replay_ops.begin(); it !=\n            available_repl_replay_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_replay_ops.clear();\n      for (std::deque<ReplTraceBeginOp*>::const_iterator it = \n            available_repl_begin_ops.begin(); it !=\n            available_repl_begin_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_begin_ops.clear();\n      for (std::deque<ReplTraceSummaryOp*>::const_iterator it = \n            available_repl_summary_ops.begin(); it !=\n            available_repl_summary_ops.end(); it++)\n      {\n        delete (*it);\n      }\n      available_repl_summary_ops.clear();\n      for (std::map<TaskID,TaskImpl*>::const_iterator it = \n            task_table.begin(); it != task_table.end(); it++)\n      {\n        delete (it->second);\n      }\n      task_table.clear();\n      // Skip this if we are in separate runtime mode\n      if (!separate_runtime_instances)\n      {\n        for (std::deque<VariantImpl*>::const_iterator it = \n              variant_table.begin(); it != variant_table.end(); it++)\n        {\n          delete (*it);\n        }\n      }\n      variant_table.clear();\n      // Skip this if we are in separate runtime mode\n      if (!separate_runtime_instances)\n      {\n        while (!layout_constraints_table.empty())\n        {\n          std::map<LayoutConstraintID,LayoutConstraints*>::iterator next_it = \n            layout_constraints_table.begin();\n          LayoutConstraints *next = next_it->second;\n          layout_constraints_table.erase(next_it);\n          if (next->remove_base_resource_ref(RUNTIME_REF))\n            delete (next);\n        }\n        while (!layout_constraints_table.empty())\n        {\n          std::map<LayoutConstraintID,LayoutConstraints*>::iterator next_it = \n            layout_constraints_table.begin();\n          LayoutConstraints *next = next_it->second;\n          layout_constraints_table.erase(next_it);\n          if (next->remove_base_resource_ref(RUNTIME_REF))\n            delete (next);\n        }\n        // We can also delete all of our reduction operators\n        ReductionOpTable &redop_table = get_reduction_table(true/*safe*/);\n        while (!redop_table.empty())\n        {\n          ReductionOpTable::iterator it = redop_table.begin();\n          delete it->second;\n          redop_table.erase(it);\n        }\n      }\n      for (LegionMap<uint64_t,LegionDeque<ProcessorGroupInfo>::aligned,\n            PROCESSOR_GROUP_ALLOC>::aligned::const_iterator git = \n            processor_groups.begin(); git != processor_groups.end(); git++)\n        for (LegionDeque<ProcessorGroupInfo>::aligned::const_iterator it = \n              git->second.begin(); it != git->second.end(); it++)\n          it->processor_group.destroy();\n      for (std::map<Memory,MemoryManager*>::const_iterator it =\n            memory_managers.begin(); it != memory_managers.end(); it++)\n      {\n        delete it->second;\n      }\n      memory_managers.clear();\n#ifdef DEBUG_LEGION\n      if (logging_region_tree_state)\n\tdelete tree_state_logger;\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    Runtime& Runtime::operator=(const Runtime &rhs)\n    //--------------------------------------------------------------------------\n    {\n      // should never be called\n      assert(false);\n      return *this;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_static_variants(void)\n    //--------------------------------------------------------------------------\n    {\n      std::deque<PendingVariantRegistration*> &pending_variants = \n        get_pending_variant_table();\n      if (!pending_variants.empty())\n      {\n        for (std::deque<PendingVariantRegistration*>::const_iterator it =\n              pending_variants.begin(); it != pending_variants.end(); it++)\n        {\n          (*it)->perform_registration(this);\n          // avoid races on separate runtime instances\n          if (!separate_runtime_instances)\n            delete *it;\n        }\n        // avoid races on separate runtime instances\n        if (!separate_runtime_instances)\n          pending_variants.clear();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_static_constraints(void)\n    //--------------------------------------------------------------------------\n    {\n      // Register any pending constraint sets\n      std::map<LayoutConstraintID,LayoutConstraintRegistrar> \n        &pending_constraints = get_pending_constraint_table();\n      if (!pending_constraints.empty())\n      {\n        // Update the next available constraint\n        while (pending_constraints.find(unique_constraint_id) !=\n                pending_constraints.end())\n          unique_constraint_id += runtime_stride;\n        // Now do the registrations\n        std::map<AddressSpaceID,unsigned> address_counts;\n        for (std::map<LayoutConstraintID,LayoutConstraintRegistrar>::\n              const_iterator it = pending_constraints.begin(); \n              it != pending_constraints.end(); it++)\n        {\n          // Figure out the distributed ID that we expect and then\n          // check against what we expect on the owner node. This\n          // is slightly brittle, but we'll always catch it when\n          // we break the invariant.\n          const AddressSpaceID owner_space = \n            LayoutConstraints::get_owner_space(it->first, this);\n          // Compute the expected DID\n          DistributedID expected_did;\n          std::map<AddressSpaceID,unsigned>::iterator finder = \n            address_counts.find(owner_space);\n          if (finder != address_counts.end())\n          {\n            if (owner_space == 0)\n              expected_did = (finder->second+1) * runtime_stride;\n            else\n              expected_did = owner_space + (finder->second * runtime_stride);\n            finder->second++;\n          }\n          else\n          {\n            if (owner_space == 0)\n              expected_did = runtime_stride;\n            else\n              expected_did = owner_space;\n            address_counts[owner_space] = 1;\n          }\n          // Now if we're the owner we have to actually bump the distributed ID\n          // number to reflect that we allocated, we'll also confirm that it\n          // is what we expected\n          if (owner_space == address_space)\n          {\n            const DistributedID did = get_available_distributed_id();\n            if (did != expected_did)\n              assert(false);\n          }\n          register_layout(it->second, it->first, expected_did);\n        }\n        // avoid races if we are doing separate runtime creation\n        if (!separate_runtime_instances)\n          pending_constraints.clear();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_static_projections(void)\n    //--------------------------------------------------------------------------\n    {\n      std::map<ProjectionID,ProjectionFunctor*> &pending_projection_functors =\n        get_pending_projection_table();\n      for (std::map<ProjectionID,ProjectionFunctor*>::const_iterator it =\n            pending_projection_functors.begin(); it !=\n            pending_projection_functors.end(); it++)\n      {\n        it->second->set_runtime(external);\n        register_projection_functor(it->first, it->second, true/*need check*/,\n                          true/*was preregistered*/, NULL, true/*pregistered*/);\n      }\n      register_projection_functor(0, \n          new IdentityProjectionFunctor(this->external), false/*need check*/,\n                        true/*was preregistered*/, NULL, true/*preregistered*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_static_sharding_functors(void)\n    //--------------------------------------------------------------------------\n    {\n      std::map<ShardingID,ShardingFunctor*> &pending_sharding_functors = \n        get_pending_sharding_table();\n      for (std::map<ShardingID,ShardingFunctor*>::const_iterator it = \n            pending_sharding_functors.begin(); it !=\n            pending_sharding_functors.end(); it++)\n        register_sharding_functor(it->first, it->second, true/*zero check*/,\n                    true/*was preregistered*/, NULL, true/*preregistered*/);\n      register_sharding_functor(0,\n          new CyclicShardingFunctor(), false/*need check*/, \n          true/*was preregistered*/, NULL, true/*preregistered*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::initialize_legion_prof(const LegionConfiguration &config)\n    //--------------------------------------------------------------------------\n    {\n      // For the profiler we want to find as many \"holes\" in the execution\n      // as possible in which to run profiler tasks so we can minimize the\n      // overhead on the application. To do this we want profiler tasks to\n      // run on any processor that has a dedicated core which is either any\n      // CPU processor a utility processor. There's no need to use GPU or\n      // I/O processors since they share the same cores as the utility cores. \n      std::vector<Processor> prof_procs(local_utils.begin(), local_utils.end());\n      for (std::set<Processor>::const_iterator it = local_procs.begin();\n            it != local_procs.end(); it++)\n      {\n        if (it->kind() == Processor::LOC_PROC)\n          prof_procs.push_back(*it);\n      }\n#ifdef DEBUG_LEGION\n      assert(!prof_procs.empty());\n#endif\n      const Processor target_proc_for_profiler = prof_procs.size() > 1 ?\n        ProcessorGroup::create_group(prof_procs) : prof_procs.front();\n      LG_TASK_DESCRIPTIONS(lg_task_descriptions);\n      LG_MESSAGE_DESCRIPTIONS(lg_message_descriptions);\n      LEGION_STATIC_ASSERT((LG_MESSAGE_ID+1) == LG_LAST_TASK_ID,\n          \"LG_MESSAGE_ID must always be the last meta-task ID\");\n      profiler = new LegionProfiler(target_proc_for_profiler,\n                                    machine, this, LG_MESSAGE_ID,\n                                    lg_task_descriptions, LAST_SEND_KIND, \n                                    lg_message_descriptions,\n                                    Operation::LAST_OP_KIND,\n                                    Operation::op_names,\n                                    config.serializer_type.c_str(),\n                                    config.prof_logfile.c_str(),\n                                    total_address_spaces,\n                                    config.prof_footprint_threshold << 20,\n                                    config.prof_target_latency);\n      MAPPER_CALL_NAMES(lg_mapper_calls);\n      profiler->record_mapper_call_kinds(lg_mapper_calls, LAST_MAPPER_CALL);\n#ifdef DETAILED_LEGION_PROF\n      RUNTIME_CALL_DESCRIPTIONS(lg_runtime_calls);\n      profiler->record_runtime_call_kinds(lg_runtime_calls, \n                                          LAST_RUNTIME_CALL_KIND);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::log_machine(Machine machine) const\n    //--------------------------------------------------------------------------\n    {\n      if (!legion_spy_enabled)\n        return;\n      std::set<Processor::Kind> proc_kinds;\n      Machine::ProcessorQuery all_procs(machine);\n      // Log processors\n      for (Machine::ProcessorQuery::iterator it = all_procs.begin();\n            it != all_procs.end(); it++)\n      {\n        Processor::Kind kind = it->kind();\n        if (proc_kinds.find(kind) == proc_kinds.end())\n        {\n          switch (kind)\n          {\n            case Processor::NO_KIND:\n              {\n                LegionSpy::log_processor_kind(kind, \"NoProc\");\n                break;\n              }\n            case Processor::TOC_PROC:\n              {\n                LegionSpy::log_processor_kind(kind, \"GPU\");\n                break;\n              }\n            case Processor::LOC_PROC:\n              {\n                LegionSpy::log_processor_kind(kind, \"CPU\");\n                break;\n              }\n            case Processor::UTIL_PROC:\n              {\n                LegionSpy::log_processor_kind(kind, \"Utility\");\n                break;\n              }\n            case Processor::IO_PROC:\n              {\n                LegionSpy::log_processor_kind(kind, \"IO\");\n                break;\n              }\n            case Processor::PROC_GROUP:\n              {\n                LegionSpy::log_processor_kind(kind, \"ProcGroup\");\n                break;\n              }\n            case Processor::PROC_SET:\n              {\n                LegionSpy::log_processor_kind(kind, \"ProcSet\");\n                break;\n              }\n            case Processor::OMP_PROC:\n              {\n                LegionSpy::log_processor_kind(kind, \"OpenMP\");\n                break;\n              }\n            case Processor::PY_PROC:\n              {\n                LegionSpy::log_processor_kind(kind, \"Python\");\n                break;\n              }\n            default:\n              assert(false); // unknown processor kind\n          }\n          proc_kinds.insert(kind);\n        }\n        LegionSpy::log_processor(it->id, kind);\n      }\n      // Log memories\n      std::set<Memory::Kind> mem_kinds;\n      Machine::MemoryQuery all_mems(machine);\n      for (Machine::MemoryQuery::iterator it = all_mems.begin();\n            it != all_mems.end(); it++)\n      {\n        Memory::Kind kind = it->kind();\n        if (mem_kinds.find(kind) == mem_kinds.end())\n        {\n          switch (kind)\n          {\n\t    case Memory::GLOBAL_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"GASNet\");\n                break;\n              }\n\t    case Memory::SYSTEM_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"System\");\n                break;\n              }\n\t    case Memory::REGDMA_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"Registered\");\n                break;\n              }\n\t    case Memory::SOCKET_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"NUMA\");\n                break;\n              }\n\t    case Memory::Z_COPY_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"Zero-Copy\");\n                break;\n              }\n\t    case Memory::GPU_FB_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"Framebuffer\");\n                break;\n              }\n\t    case Memory::DISK_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"Disk\");\n                break;\n              }\n\t    case Memory::HDF_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"HDF\");\n                break;\n              }\n\t    case Memory::FILE_MEM:\n              {\n                LegionSpy::log_memory_kind(kind, \"File\");\n                break;\n              }\n\t    case Memory::LEVEL3_CACHE:\n              {\n                LegionSpy::log_memory_kind(kind, \"L3\");\n                break;\n              }\n\t    case Memory::LEVEL2_CACHE:\n              {\n                LegionSpy::log_memory_kind(kind, \"L2\");\n                break;\n              }\n\t    case Memory::LEVEL1_CACHE:\n              {\n                LegionSpy::log_memory_kind(kind, \"L1\");\n                break;\n              }\n            default:\n              assert(false); // unknown memory kind\n          }\n        }\n        LegionSpy::log_memory(it->id, it->capacity(), it->kind());\n      }\n      // Log Proc-Mem Affinity\n      Machine::ProcessorQuery all_procs2(machine);\n      for (Machine::ProcessorQuery::iterator pit = all_procs2.begin();\n            pit != all_procs2.end(); pit++)\n      {\n        std::vector<ProcessorMemoryAffinity> affinities;\n        machine.get_proc_mem_affinity(affinities, *pit);\n        for (std::vector<ProcessorMemoryAffinity>::const_iterator it = \n              affinities.begin(); it != affinities.end(); it++)\n        {\n          LegionSpy::log_proc_mem_affinity(pit->id, it->m.id, \n                                           it->bandwidth, it->latency);\n        }\n      }\n      // Log Mem-Mem Affinity\n      Machine::MemoryQuery all_mems2(machine);\n      for (Machine::MemoryQuery::iterator mit = all_mems2.begin();\n            mit != all_mems2.begin(); mit++)\n      {\n        std::vector<MemoryMemoryAffinity> affinities;\n        machine.get_mem_mem_affinity(affinities, *mit);\n        for (std::vector<MemoryMemoryAffinity>::const_iterator it = \n              affinities.begin(); it != affinities.end(); it++)\n        {\n          LegionSpy::log_mem_mem_affinity(it->m1.id, it->m2.id, \n                                          it->bandwidth, it->latency);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::initialize_mappers(void)\n    //--------------------------------------------------------------------------\n    {\n      if (replay_file.empty()) // This is the normal path\n      {\n        if (enable_test_mapper)\n        {\n          // Make test mappers for everyone\n          for (std::map<Processor,ProcessorManager*>::const_iterator it = \n                proc_managers.begin(); it != proc_managers.end(); it++)\n          {\n            Mapper *mapper = \n              new Mapping::TestMapper(mapper_runtime, machine, it->first);\n            MapperManager *wrapper = wrap_mapper(this, mapper, 0, it->first);\n            it->second->add_mapper(0, wrapper, false/*check*/, true/*owns*/);\n          }\n        }\n        else if (supply_default_mapper)\n        {\n          // Make default mappers for everyone\n          for (std::map<Processor,ProcessorManager*>::const_iterator it = \n                proc_managers.begin(); it != proc_managers.end(); it++)\n          {\n            Mapper *mapper = \n              new Mapping::DefaultMapper(mapper_runtime, machine, it->first);\n            MapperManager *wrapper = wrap_mapper(this, mapper, 0, it->first);\n            it->second->add_mapper(0, wrapper, false/*check*/, true/*owns*/);\n          } \n        }\n      }\n      else // This is the replay/debug path\n      {\n        if (legion_ldb_enabled)\n        {\n          // This path is not quite ready yet\n          assert(false);\n          for (std::map<Processor,ProcessorManager*>::const_iterator it = \n                proc_managers.begin(); it != proc_managers.end(); it++)\n          {\n            Mapper *mapper = new Mapping::DebugMapper(mapper_runtime, \n                                    machine, it->first, replay_file.c_str());\n            MapperManager *wrapper = wrap_mapper(this, mapper, 0, it->first);\n            it->second->add_mapper(0, wrapper, false/*check*/, true/*owns*/, \n                                    true/*skip replay*/);\n          }\n        }\n        else\n        {\n          for (std::map<Processor,ProcessorManager*>::const_iterator it =\n                proc_managers.begin(); it != proc_managers.end(); it++)\n          {\n            Mapper *mapper = new Mapping::ReplayMapper(mapper_runtime, \n                                    machine, it->first, replay_file.c_str());\n            MapperManager *wrapper = wrap_mapper(this, mapper, 0, it->first);\n            it->second->add_mapper(0, wrapper, false/*check*/, true/*owns*/,\n                                    true/*skip replay*/);\n          }\n        }\n      }\n      \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::initialize_virtual_manager(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(virtual_manager == NULL);\n#endif\n      // make a layout constraints\n      LayoutConstraintSet constraint_set;\n      constraint_set.add_constraint(\n          SpecializedConstraint(LEGION_VIRTUAL_SPECIALIZE));\n      LayoutConstraints *constraints = \n        register_layout(FieldSpace::NO_SPACE, constraint_set, true/*internal*/);\n      FieldMask all_ones(LEGION_FIELD_MASK_FIELD_ALL_ONES);\n      std::vector<unsigned> mask_index_map;\n      std::vector<CustomSerdezID> serdez;\n      std::vector<std::pair<FieldID,size_t> > field_sizes;\n      LayoutDescription *layout = new LayoutDescription(all_ones, constraints);\n      virtual_manager = new VirtualManager(this, 0/*did*/, layout);\n      virtual_manager->add_base_resource_ref(NEVER_GC_REF);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::initialize_runtime(void)\n    //--------------------------------------------------------------------------\n    {  \n      // If we have an MPI rank table do the exchanges before initializing\n      // the mappers as they may want to look at the rank table\n      if (mpi_rank_table != NULL)\n        mpi_rank_table->perform_rank_exchange();\n      initialize_mappers(); \n      // Pull in any static registrations that were done\n      register_static_variants();\n      register_static_constraints();\n      register_static_projections();\n      register_static_sharding_functors();\n      // Initialize our virtual manager and our mappers\n      initialize_virtual_manager();\n      // Finally perform the registration callback methods\n      const std::vector<RegistrationCallbackFnptr> &registration_callbacks\n        = get_pending_registration_callbacks();\n      if (!registration_callbacks.empty())\n      {\n        log_run.info(\"Invoking registration callback functions...\");\n        for (std::vector<RegistrationCallbackFnptr>::const_iterator it = \n              registration_callbacks.begin(); it !=\n              registration_callbacks.end(); it++)\n          perform_registration_callback(*it, \n              false/*global*/, true/*preregistered*/);\n        log_run.info(\"Finished execution of registration callbacks\");\n      }\n    }\n\n#ifdef LEGION_USE_LIBDL\n    //--------------------------------------------------------------------------\n    void Runtime::send_registration_callback(AddressSpaceID target,\n                                         Realm::DSOReferenceImplementation *dso,\n                                         RtEvent global_done_event,\n                                         std::set<RtEvent> &applied_events)\n    //--------------------------------------------------------------------------\n    {\n      const RtUserEvent done_event = Runtime::create_rt_user_event();\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        const size_t dso_size = dso->dso_name.size() + 1;\n        const size_t sym_size = dso->symbol_name.size() + 1;\n        rez.serialize(dso_size);\n        rez.serialize(dso->dso_name.c_str(), dso_size);\n        rez.serialize(sym_size);\n        rez.serialize(dso->symbol_name.c_str(), sym_size);\n        rez.serialize(global_done_event);\n        rez.serialize(done_event);\n      }\n      find_messenger(target)->send_message(rez, SEND_REGISTRATION_CALLBACK,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n      applied_events.insert(done_event);\n    }\n#endif // LEGION_USE_LIBDL\n\n    //--------------------------------------------------------------------------\n    RtEvent Runtime::perform_registration_callback(\n            RegistrationCallbackFnptr callback, bool global, bool preregistered)\n    //--------------------------------------------------------------------------\n    { \n      if (inside_registration_callback)\n        REPORT_LEGION_ERROR(ERROR_NESTED_REGISTRATION_CALLBACKS,\n            \"Nested registration callbacks are not permitted in Legion\")\n      std::pair<std::string,std::string> global_key;\n#ifdef LEGION_USE_LIBDL\n      Realm::DSOReferenceImplementation *dso = NULL;\n      if (global)\n      {\n        // No such thing as global registration if there's only one addres space\n        if (total_address_spaces > 1)\n        {\n          // Convert this to it's portable representation or raise an error\n          // This is a little scary, we could still be inside of dlopen when\n          // we get this call as part of the constructor for a shared object\n          // and yet we're about to do a call to dladdr. This seems to work\n          // but there is no documentation anywhere about whether this is \n          // legal or safe to do...\n          Realm::FunctionPointerImplementation impl((void (*)(void))callback);\n#ifdef DEBUG_LEGION\n          assert(callback_translator.can_translate(\n                typeid(Realm::FunctionPointerImplementation),\n                typeid(Realm::DSOReferenceImplementation)));\n#endif\n          dso = static_cast<Realm::DSOReferenceImplementation*>(\n              callback_translator.translate(&impl, \n                typeid(Realm::DSOReferenceImplementation)));\n          if (dso == NULL)\n            REPORT_LEGION_FATAL(LEGION_FATAL_CALLBACK_NOT_PORTABLE,\n                \"Global registration callback function pointer %p is not \"\n                \"portable. All registration callbacks requesting to be \"\n                \"performed 'globally' must be able to be recognized by \"\n                \"a call to 'dladdr'. This requires that they come from a \"\n                \"shared object or the binary is linked with the '-rdynamic' \"\n                \"flag.\", callback)\n          global_key = \n            std::pair<std::string,std::string>(dso->dso_name, dso->symbol_name);\n        }\n        else\n          global = false;\n      }\n#else\n      assert(!global);\n#endif\n      RtEvent local_done, global_done;\n      RtUserEvent local_perform, global_perform;\n      {\n        AutoLock c_lock(callback_lock); \n        if (global)\n        {\n          // See if we're going to perform this or not\n          std::map<std::pair<std::string,std::string>,RtEvent>::const_iterator\n            local_finder = global_local_done.find(global_key);\n          if (local_finder == global_local_done.end())\n          {\n            local_perform = Runtime::create_rt_user_event();\n            global_local_done[global_key] = local_perform;\n            // Check to see if we have any pending global callbacks to \n            // notify about being done locally\n            std::map<std::pair<std::string,std::string>,\n                      std::set<RtUserEvent> >::iterator\n              pending_finder = pending_remote_callbacks.find(global_key);\n            if (pending_finder != pending_remote_callbacks.end())\n            {\n              for (std::set<RtUserEvent>::const_iterator it = \n                    pending_finder->second.begin(); it != \n                    pending_finder->second.end(); it++)\n                Runtime::trigger_event(*it, local_perform);\n              pending_remote_callbacks.erase(pending_finder);\n            }\n          }\n          else\n            local_done = local_finder->second;\n          // Now see if we need to do our global registration callbacks\n          std::map<std::pair<std::string,std::string>,RtEvent>::const_iterator\n            global_finder = global_callbacks_done.find(global_key);\n          if (global_finder == global_callbacks_done.end())\n          {\n            global_perform = Runtime::create_rt_user_event();\n            global_callbacks_done[global_key] = global_perform;\n          }\n          else\n            global_done = global_finder->second;\n        }\n        else\n        {\n          std::map<RegistrationCallbackFnptr,RtEvent>::const_iterator\n            local_finder = local_callbacks_done.find(callback);\n          if (local_finder == local_callbacks_done.end())\n          {\n            local_perform = Runtime::create_rt_user_event();\n            local_callbacks_done[callback] = local_perform;\n          }\n          else\n            return local_finder->second;\n        }\n      }\n      // Do the local callback and record it now \n      if (local_perform.exists())\n      {\n        // All the pregistered cases are effectively global too\n        if (global || preregistered)\n          inside_registration_callback = GLOBAL_REGISTRATION_CALLBACK;\n        else\n          inside_registration_callback = LOCAL_REGISTRATION_CALLBACK;\n        (*callback)(machine, external, local_procs);\n        inside_registration_callback = NO_REGISTRATION_CALLBACK;\n        Runtime::trigger_event(local_perform);\n        if (!global)\n          return local_perform;\n      }\n#ifdef LEGION_USE_LIBDL\n#ifdef DEBUG_LEGION\n      assert(global);\n#endif\n      if (global_done.exists())\n      {\n        delete dso;\n        return global_done;\n      }\n#ifdef DEBUG_LEGION\n      assert(global_perform.exists());\n#endif\n      // See if we're inside of a task and can use that to help do the \n      // global invocations of this registration callback\n      if (implicit_context == NULL)\n      {\n#ifdef DEBUG_LEGION\n        assert(implicit_runtime == NULL);\n#endif\n        // This means we're in an external thread asking for us to\n        // perform a global registration so just send out messages\n        // to all the nodes asking them to do the registration\n        std::set<RtEvent> preconditions;\n        for (AddressSpaceID space = 0; space < total_address_spaces; space++)\n        {\n          if (space == address_space)\n            continue;\n          send_registration_callback(space, dso, global_perform, preconditions);\n        }\n        if (!preconditions.empty())\n          Runtime::trigger_event(global_perform,\n              Runtime::merge_events(preconditions));\n        else\n          Runtime::trigger_event(global_perform);\n      }\n      else\n      {\n        std::set<RtEvent> preconditions;\n        implicit_context->perform_global_registration_callbacks(\n            dso, local_done, global_perform, preconditions);\n        if (!preconditions.empty())\n          Runtime::trigger_event(global_perform,\n              Runtime::merge_events(preconditions));\n        else\n          Runtime::trigger_event(global_perform);\n      }\n      delete dso;\n#endif // LEGION_USE_LIBDL\n      return global_perform;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::startup_runtime(void)\n    //--------------------------------------------------------------------------\n    {\n      // If stealing is not disabled then startup our mappers\n      if (!stealing_disabled)\n      {\n        for (std::map<Processor,ProcessorManager*>::const_iterator it = \n              proc_managers.begin(); it != proc_managers.end(); it++)\n          it->second->startup_mappers();\n      }\n      if (address_space == 0)\n      {\n        if (legion_spy_enabled)\n            log_machine(machine);\n        // If we are runtime 0 then we launch the top-level task\n        if (legion_main_set)\n        {\n          TaskLauncher launcher(Runtime::legion_main_id, \n                                TaskArgument(&input_args, sizeof(InputArgs)),\n                                Predicate::TRUE_PRED, legion_main_mapper_id);\n          launch_top_level_task(launcher); \n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::finalize_runtime(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(virtual_manager != NULL);\n#endif\n      if (virtual_manager->remove_base_resource_ref(NEVER_GC_REF))\n      {\n        delete virtual_manager;\n        virtual_manager = NULL;\n      }\n      // Have the memory managers for deletion of all their instances\n      for (std::map<Memory,MemoryManager*>::const_iterator it =\n           memory_managers.begin(); it != memory_managers.end(); it++)\n        it->second->finalize(); \n      if (profiler != NULL)\n        profiler->finalize();\n    }\n    \n    //--------------------------------------------------------------------------\n    ApEvent Runtime::launch_mapper_task(Mapper *mapper, Processor proc, \n                                        TaskID tid, const TaskArgument &arg,\n                                        MapperID map_id)\n    //--------------------------------------------------------------------------\n    {\n      // Get an individual task to be the top-level task\n      IndividualTask *mapper_task = get_available_individual_task();\n      // Get a remote task to serve as the top of the top-level task\n      TopLevelContext *map_context = \n        new TopLevelContext(this, get_unique_operation_id());\n      map_context->add_reference();\n      map_context->set_executing_processor(proc);\n      TaskLauncher launcher(tid, arg, Predicate::TRUE_PRED, map_id);\n      Future f = mapper_task->initialize_task(map_context, launcher, \n                                              false/*track parent*/);\n      mapper_task->set_current_proc(proc);\n      mapper_task->select_task_options(false/*prioritize*/);\n      // Create a temporary event to name the result since we \n      // have to pack it in the task that runs, but it also depends\n      // on the task being reported back to the mapper\n      ApUserEvent result = Runtime::create_ap_user_event(NULL);\n      // Add a reference to the future impl to prevent it being collected\n      f.impl->add_base_gc_ref(FUTURE_HANDLE_REF);\n      // Create a meta-task to return the results to the mapper\n      MapperTaskArgs args(f.impl, map_id, proc, result, map_context);\n      ApEvent pre = f.impl->get_ready_event();\n      ApEvent post(issue_runtime_meta_task(args, LG_LATENCY_WORK_PRIORITY,\n                                           Runtime::protect_event(pre)));\n      // Chain the events properly\n      Runtime::trigger_event(NULL, result, post);\n      // Mark that we have another outstanding top level task\n      increment_outstanding_top_level_tasks();\n      // Now we can put it on the queue\n      add_to_ready_queue(proc, mapper_task);\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::process_mapper_task_result(const MapperTaskArgs *args)\n    //--------------------------------------------------------------------------\n    {\n#if 0\n      MapperManager *mapper = find_mapper(args->proc, args->map_id);\n      Mapper::MapperTaskResult result;\n      result.mapper_event = args->event;\n      result.result = args->future->get_untyped_result();\n      result.result_size = args->future->get_untyped_size();\n      mapper->invoke_handle_task_result(&result);\n#else\n      assert(false); // update this\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::create_shared_ownership(IndexSpace handle,\n                                          const bool total_sharding_collective)\n    //--------------------------------------------------------------------------\n    {\n      const AddressSpaceID owner_space = \n        IndexSpaceNode::get_owner_space(handle, this);\n      if (owner_space == address_space)\n      {\n        IndexSpaceNode *node = forest->get_node(handle);\n        if (!node->check_valid_and_increment(APPLICATION_REF))\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,\n              \"Illegal call to add shared ownership to index space %x \"\n              \"which has already been deleted\", handle.get_id())\n      }\n      else if (!total_sharding_collective)\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<int>(0);\n          rez.serialize(handle);\n        }\n        send_shared_ownership(owner_space, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::create_shared_ownership(IndexPartition handle,\n                                          const bool total_sharding_collective)\n    //--------------------------------------------------------------------------\n    {\n      const AddressSpaceID owner_space = \n        IndexPartNode::get_owner_space(handle, this);\n      if (owner_space == address_space)\n      {\n        IndexPartNode *node = forest->get_node(handle);\n        if (!node->check_valid_and_increment(APPLICATION_REF))\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,\n              \"Illegal call to add shared ownership to index partition %x \"\n              \"which has already been deleted\", handle.get_id())\n      }\n      else if (!total_sharding_collective)\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<int>(1);\n          rez.serialize(handle);\n        }\n        send_shared_ownership(owner_space, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::create_shared_ownership(FieldSpace handle,\n                                          const bool total_sharding_collective)\n    //--------------------------------------------------------------------------\n    {\n      const AddressSpaceID owner_space = \n        FieldSpaceNode::get_owner_space(handle, this);\n      if (owner_space == address_space)\n      {\n        FieldSpaceNode *node = forest->get_node(handle);\n        if (!node->check_valid_and_increment(APPLICATION_REF))\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,\n              \"Illegal call to add shared ownership to field space %x \"\n              \"which has already been deleted\", handle.get_id())\n      }\n      else if (!total_sharding_collective)\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<int>(2);\n          rez.serialize(handle);\n        }\n        send_shared_ownership(owner_space, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::create_shared_ownership(LogicalRegion handle,\n                                          const bool total_sharding_collective)\n    //--------------------------------------------------------------------------\n    {\n      const AddressSpaceID owner_space = \n        RegionNode::get_owner_space(handle, this);\n      if (owner_space == address_space)\n      {\n        RegionNode *node = forest->get_node(handle);\n        if (!node->check_valid_and_increment(APPLICATION_REF))\n          REPORT_LEGION_ERROR(ERROR_ILLEGAL_SHARED_OWNERSHIP,\n              \"Illegal call to add shared ownership to logical region \"\n              \"(%x,%x,%x) which has already been deleted\", \n              handle.index_space.get_id(), handle.field_space.get_id(),\n              handle.tree_id)\n      }\n      else if (!total_sharding_collective)\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<int>(3);\n          rez.serialize(handle);\n        }\n        send_shared_ownership(owner_space, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    IndexPartition Runtime::get_index_partition(Context ctx, \n                                                IndexSpace parent, Color color)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexPartition result = get_index_partition(parent, color);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexPartition Runtime::get_index_partition(IndexSpace parent, Color color)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartition result = forest->get_index_partition(parent, color);\n#ifdef DEBUG_LEGION\n      if (!result.exists())\n        REPORT_LEGION_ERROR(ERROR_INVALID_INDEX_SPACE_COLOR, \n            \"Invalid color %d for get index partitions\", color);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_index_partition(Context ctx, IndexSpace parent, \n                                      Color color)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = has_index_partition(parent, color);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_index_partition(IndexSpace parent, Color color)\n    //--------------------------------------------------------------------------\n    {\n      return forest->has_index_partition(parent, color);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::get_index_subspace(Context ctx, IndexPartition p, \n                                           const void *realm_color,\n                                           TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexSpace result = get_index_subspace(p, realm_color, type_tag); \n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::get_index_subspace(IndexPartition p, \n                                           const void *realm_color,\n                                           TypeTag type_tag) \n    //--------------------------------------------------------------------------\n    {\n      return forest->get_index_subspace(p, realm_color, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_index_subspace(Context ctx, IndexPartition p,\n                                     const void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = has_index_subspace(p, realm_color, type_tag); \n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_index_subspace(IndexPartition p,\n                                     const void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      return forest->has_index_subspace(p, realm_color, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_space_domain(Context ctx, IndexSpace handle,\n                                         void *realm_is, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      get_index_space_domain(handle, realm_is, type_tag);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_space_domain(IndexSpace handle, \n                                         void *realm_is, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      forest->get_index_space_domain(handle, realm_is, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    Domain Runtime::get_index_partition_color_space(Context ctx,\n                                                    IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      Domain result = get_index_partition_color_space(p); \n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    Domain Runtime::get_index_partition_color_space(IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode *part = forest->get_node(p);\n      const IndexSpace color_space = part->color_space->handle;\n      switch (NT_TemplateHelper::get_dim(color_space.get_type_tag()))\n      {\n#define DIMFUNC(DIM) \\\n        case DIM: \\\n          { \\\n            DomainT<DIM,coord_t> color_index_space; \\\n            forest->get_index_space_domain(color_space, &color_index_space, \\\n                                           color_space.get_type_tag()); \\\n            return Domain(color_index_space); \\\n          }\n        LEGION_FOREACH_N(DIMFUNC)\n#undef DIMFUNC\n        default:\n          assert(false);\n      }\n      return Domain::NO_DOMAIN;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_partition_color_space(IndexPartition p,\n                                               void *realm_is, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode *part = forest->get_node(p);\n      const IndexSpace color_space = part->color_space->handle;\n      forest->get_index_space_domain(color_space, realm_is, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::get_index_partition_color_space_name(Context ctx,\n                                                             IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexSpace result = get_index_partition_color_space_name(p);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::get_index_partition_color_space_name(IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_index_partition_color_space(p);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_space_partition_colors(Context ctx, IndexSpace sp,\n                                                   std::set<Color> &colors)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      get_index_space_partition_colors(sp, colors);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_space_partition_colors(IndexSpace handle,\n                                                   std::set<Color> &colors)\n    //--------------------------------------------------------------------------\n    {\n      forest->get_index_space_partition_colors(handle, colors);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_index_partition_disjoint(Context ctx, IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = forest->is_index_partition_disjoint(p);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_index_partition_disjoint(IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      return forest->is_index_partition_disjoint(p);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_index_partition_complete(Context ctx, IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = forest->is_index_partition_complete(p);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_index_partition_complete(IndexPartition p)\n    //--------------------------------------------------------------------------\n    {\n      return forest->is_index_partition_complete(p);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_space_color_point(Context ctx, IndexSpace handle,\n                                            void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      forest->get_index_space_color(handle, realm_color, type_tag);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_index_space_color_point(IndexSpace handle,\n                                            void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      forest->get_index_space_color(handle, realm_color, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    DomainPoint Runtime::get_index_space_color_point(Context ctx, \n                                                     IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexSpaceNode *node = forest->get_node(handle);\n      DomainPoint result = node->get_domain_point_color();\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    DomainPoint Runtime::get_index_space_color_point(IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode *node = forest->get_node(handle);\n      return node->get_domain_point_color();\n    }\n\n    //--------------------------------------------------------------------------\n    Color Runtime::get_index_partition_color(Context ctx, \n                                                   IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      Color result = forest->get_index_partition_color(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    Color Runtime::get_index_partition_color(IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_index_partition_color(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::get_parent_index_space(Context ctx,   \n                                               IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexSpace result = forest->get_parent_index_space(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::get_parent_index_space(IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_parent_index_space(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_parent_index_partition(Context ctx, IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = forest->has_parent_index_partition(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_parent_index_partition(IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->has_parent_index_partition(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexPartition Runtime::get_parent_index_partition(Context ctx,\n                                                       IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexPartition result = forest->get_parent_index_partition(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexPartition Runtime::get_parent_index_partition(IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_parent_index_partition(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    unsigned Runtime::get_index_space_depth(Context ctx, IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      unsigned result = forest->get_index_space_depth(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    unsigned Runtime::get_index_space_depth(IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_index_space_depth(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    unsigned Runtime::get_index_partition_depth(Context ctx, \n                                                IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      unsigned result = forest->get_index_partition_depth(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    unsigned Runtime::get_index_partition_depth(IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_index_partition_depth(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::safe_cast(Context ctx, LogicalRegion region,\n                            const void *realm_point, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context safe cast!\");\n      return ctx->safe_cast(forest, region.get_index_space(), \n                            realm_point, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    size_t Runtime::get_field_size(Context ctx, FieldSpace handle, FieldID fid)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      size_t result = forest->get_field_size(handle, fid);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    size_t Runtime::get_field_size(FieldSpace handle, FieldID fid)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_field_size(handle, fid);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_field_space_fields(Context ctx, FieldSpace handle,\n                                         std::vector<FieldID> &fields)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      forest->get_field_space_fields(handle, fields);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_field_space_fields(FieldSpace handle, \n                                         std::vector<FieldID> &fields)\n    //--------------------------------------------------------------------------\n    {\n      forest->get_field_space_fields(handle, fields);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::create_logical_region(Context ctx, \n                IndexSpace index_space, FieldSpace field_space, bool task_local)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context create logical region!\");\n      return ctx->create_logical_region(forest, index_space, field_space,\n                                        task_local); \n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_logical_partition(Context ctx, \n                                    LogicalRegion parent, IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalPartition result = forest->get_logical_partition(parent, handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_logical_partition(LogicalRegion parent,\n                                                    IndexPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_partition(parent, handle);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_logical_partition_by_color(\n                                    Context ctx, LogicalRegion parent, Color c)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalPartition result = \n        forest->get_logical_partition_by_color(parent, c);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_logical_partition_by_color(LogicalRegion par,\n                                                             Color c)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_partition_by_color(par, c);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_logical_partition_by_color(Context ctx, \n                                              LogicalRegion parent, Color color)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = forest->has_logical_partition_by_color(parent, color);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_logical_partition_by_color(LogicalRegion parent, \n                                                 Color color)\n    //--------------------------------------------------------------------------\n    {\n      return forest->has_logical_partition_by_color(parent, color);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_logical_partition_by_tree(\n                                            Context ctx, IndexPartition handle, \n                                            FieldSpace fspace, RegionTreeID tid)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalPartition result = \n        forest->get_logical_partition_by_tree(handle, fspace, tid);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_logical_partition_by_tree(\n                                                            IndexPartition part,\n                                                            FieldSpace fspace,\n                                                            RegionTreeID tid)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_partition_by_tree(part, fspace, tid);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_logical_subregion(Context ctx, \n                                    LogicalPartition parent, IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalRegion result = forest->get_logical_subregion(parent, handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_logical_subregion(LogicalPartition parent,\n                                                 IndexSpace handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_subregion(parent, handle);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_logical_subregion_by_color(Context ctx,\n             LogicalPartition parent, const void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalRegion result = \n        forest->get_logical_subregion_by_color(parent, realm_color, type_tag);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_logical_subregion_by_color(LogicalPartition par,\n                                      const void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_subregion_by_color(par, realm_color, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_logical_subregion_by_color(Context ctx,\n             LogicalPartition parent, const void *realm_point, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = \n        forest->has_logical_subregion_by_color(parent, realm_point, type_tag);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_logical_subregion_by_color(LogicalPartition parent, \n                                      const void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      return forest->has_logical_subregion_by_color(parent, \n                                                    realm_color, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_logical_subregion_by_tree(Context ctx, \n                        IndexSpace handle, FieldSpace fspace, RegionTreeID tid)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalRegion result = \n        forest->get_logical_subregion_by_tree(handle, fspace, tid);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_logical_subregion_by_tree(IndexSpace handle,\n                                                         FieldSpace fspace,\n                                                         RegionTreeID tid)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_subregion_by_tree(handle, fspace, tid);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_logical_region_color(Context ctx, LogicalRegion handle, \n                                           void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      forest->get_logical_region_color(handle, realm_color, type_tag);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_logical_region_color(LogicalRegion handle,\n                                            void *realm_color, TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      forest->get_logical_region_color(handle, realm_color, type_tag);\n    }\n\n    //--------------------------------------------------------------------------\n    DomainPoint Runtime::get_logical_region_color_point(Context ctx,\n                                                        LogicalRegion handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      IndexSpaceNode *node = forest->get_node(handle.get_index_space());\n      DomainPoint result = node->get_domain_point_color();\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    DomainPoint Runtime::get_logical_region_color_point(LogicalRegion handle)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode *node = forest->get_node(handle.get_index_space());\n      return node->get_domain_point_color();\n    }\n\n    //--------------------------------------------------------------------------\n    Color Runtime::get_logical_partition_color(Context ctx,\n                                                     LogicalPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      Color result = forest->get_logical_partition_color(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    Color Runtime::get_logical_partition_color(LogicalPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_logical_partition_color(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_parent_logical_region(Context ctx, \n                                                     LogicalPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalRegion result = forest->get_parent_logical_region(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalRegion Runtime::get_parent_logical_region(LogicalPartition handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_parent_logical_region(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_parent_logical_partition(Context ctx, \n                                               LogicalRegion handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      bool result = forest->has_parent_logical_partition(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_parent_logical_partition(LogicalRegion handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->has_parent_logical_partition(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_parent_logical_partition(Context ctx,\n                                                           LogicalRegion handle)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      LogicalPartition result = forest->get_parent_logical_partition(handle);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalPartition Runtime::get_parent_logical_partition(\n                                                           LogicalRegion handle)\n    //--------------------------------------------------------------------------\n    {\n      return forest->get_parent_logical_partition(handle);\n    }\n\n    //--------------------------------------------------------------------------\n    ArgumentMap Runtime::create_argument_map(void)\n    //--------------------------------------------------------------------------\n    {\n      ArgumentMapImpl *impl = new ArgumentMapImpl();\n#ifdef DEBUG_LEGION\n      assert(impl != NULL);\n#endif\n      return ArgumentMap(impl);\n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::execute_task(Context ctx, const TaskLauncher &launcher,\n                                 std::vector<OutputRequirement> *outputs)\n    //--------------------------------------------------------------------------\n    { \n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context execute task!\");\n      return ctx->execute_task(launcher, outputs);\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMap Runtime::execute_index_space(\n                                  Context ctx, const IndexTaskLauncher &launcher,\n                                  std::vector<OutputRequirement> *outputs)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context execute index space!\");\n      return ctx->execute_index_space(launcher, outputs);\n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::execute_index_space(\n                                 Context ctx, const IndexTaskLauncher &launcher,\n                                 ReductionOpID redop, bool deterministic,\n                                 std::vector<OutputRequirement> *outputs)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context execute index space!\");\n      return ctx->execute_index_space(launcher, redop, deterministic, outputs);\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalRegion Runtime::map_region(Context ctx, \n                                                const InlineLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context map region!\");\n      return ctx->map_region(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalRegion Runtime::map_region(Context ctx, unsigned idx, \n                                                  MapperID id, MappingTagID tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context map region!\");\n      PhysicalRegion result = ctx->get_physical_region(idx);\n      // Check to see if we are already mapped, if not, then remap it\n      if (!result.impl->is_mapped())\n        remap_region(ctx, result);\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::remap_region(Context ctx, PhysicalRegion region)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context remap region!\");\n      ctx->remap_region(region); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::unmap_region(Context ctx, PhysicalRegion region)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context unmap region!\");\n      ctx->unmap_region(region); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::fill_fields(Context ctx, const FillLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context fill operation!\");\n      ctx->fill_fields(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::fill_fields(Context ctx, const IndexFillLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context fill operation!\");\n      ctx->fill_fields(launcher);\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalRegion Runtime::attach_external_resource(Context ctx, \n                                                 const AttachLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context attach external resource!\");\n      return ctx->attach_resource(launcher);\n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::detach_external_resource(Context ctx, PhysicalRegion region,\n                                         const bool flush, const bool unordered)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context detach external resource!\");\n      return ctx->detach_resource(region, flush, unordered);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::progress_unordered_operations(Context ctx)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context progress unordered ops\")\n      return ctx->progress_unordered_operations();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::issue_copy_operation(Context ctx,const CopyLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context issue copy operation!\");\n      ctx->issue_copy(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::issue_copy_operation(Context ctx,\n                                       const IndexCopyLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context issue copy operation!\");\n      ctx->issue_copy(launcher);\n    }\n\n    //--------------------------------------------------------------------------\n    Predicate Runtime::create_predicate(Context ctx, const Future &f) \n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context create predicate!\");\n      return ctx->create_predicate(f);\n    }\n\n    //--------------------------------------------------------------------------\n    Predicate Runtime::predicate_not(Context ctx, const Predicate &p) \n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context create predicate not!\");\n      return ctx->predicate_not(p); \n    }\n\n    //--------------------------------------------------------------------------\n    Predicate Runtime::create_predicate(Context ctx, \n                                        const PredicateLauncher &launcher) \n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context create predicate!\");\n      return ctx->create_predicate(launcher);\n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::get_predicate_future(Context ctx, const Predicate &p)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context get predicate future!\");\n      return ctx->get_predicate_future(p);\n    }\n\n    //--------------------------------------------------------------------------\n    Lock Runtime::create_lock(Context ctx)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      Lock result(Reservation::create_reservation());\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::destroy_lock(Context ctx, Lock l)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      ctx->destroy_user_lock(l.reservation_lock);\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    Grant Runtime::acquire_grant(Context ctx, \n                                 const std::vector<LockRequest> &requests)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      // Kind of annoying, but we need to unpack and repack the\n      // Lock type here to build new requests because the C++\n      // type system is dumb with nested classes.\n      std::vector<GrantImpl::ReservationRequest> \n        unpack_requests(requests.size());\n      for (unsigned idx = 0; idx < requests.size(); idx++)\n      {\n        unpack_requests[idx] = \n          GrantImpl::ReservationRequest(requests[idx].lock.reservation_lock,\n                                        requests[idx].mode,\n                                        requests[idx].exclusive);\n      }\n      Grant result(new GrantImpl(unpack_requests));\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::release_grant(Context ctx, Grant grant)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      grant.impl->release_grant();\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n    }\n\n    //--------------------------------------------------------------------------\n    PhaseBarrier Runtime::create_phase_barrier(Context ctx, unsigned arrivals) \n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context create phase barrier!\");\n      return PhaseBarrier(ctx->create_phase_barrier(arrivals));\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::destroy_phase_barrier(Context ctx, PhaseBarrier pb)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context destroy phase barrier!\");\n      ctx->destroy_phase_barrier(pb.phase_barrier);\n    }\n\n    //--------------------------------------------------------------------------\n    PhaseBarrier Runtime::advance_phase_barrier(Context ctx, PhaseBarrier pb)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context advance phase barrier!\");\n      return ctx->advance_phase_barrier(pb);\n    }\n\n    //--------------------------------------------------------------------------\n    DynamicCollective Runtime::create_dynamic_collective(Context ctx,\n                                                         unsigned arrivals,\n                                                         ReductionOpID redop,\n                                                         const void *init_value,\n                                                         size_t init_size)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context create dynamic collective!\");\n      return DynamicCollective(ctx->create_phase_barrier(arrivals, redop,\n                                             init_value, init_size), redop);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::destroy_dynamic_collective(Context ctx, DynamicCollective dc)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context destroy dynamic collective!\");\n      ctx->destroy_phase_barrier(dc.phase_barrier);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::arrive_dynamic_collective(Context ctx, DynamicCollective dc,\n                                            const void *buffer, size_t size,\n                                            unsigned count)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context arrive dynamic collective!\");\n      ctx->arrive_dynamic_collective(dc, buffer, size, count);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::defer_dynamic_collective_arrival(Context ctx, \n                                                   DynamicCollective dc,\n                                                   const Future &f, \n                                                   unsigned count)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context defer dynamic collective arrival!\");\n      ctx->defer_dynamic_collective_arrival(dc, f, count);\n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::get_dynamic_collective_result(Context ctx, \n                                                  DynamicCollective dc)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context get dynamic collective result!\");\n      return ctx->get_dynamic_collective_result(dc);\n    }\n\n    //--------------------------------------------------------------------------\n    DynamicCollective Runtime::advance_dynamic_collective(Context ctx,\n                                                          DynamicCollective dc)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context advance dynamic collective!\");\n      return ctx->advance_dynamic_collective(dc);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::issue_acquire(Context ctx, const AcquireLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context issue acquire!\");\n      ctx->issue_acquire(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::issue_release(Context ctx, const ReleaseLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context issue release!\");\n      ctx->issue_release(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::issue_mapping_fence(Context ctx)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context issue mapping fence!\");\n      return ctx->issue_mapping_fence(); \n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::issue_execution_fence(Context ctx)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context issue execution fence!\");\n      return ctx->issue_execution_fence(); \n    }\n\n    //--------------------------------------------------------------------------\n    TraceID Runtime::generate_dynamic_trace_id(bool check_context/*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_trace_id();\n      TraceID result = __sync_fetch_and_add(&unique_trace_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Trace IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n    \n    //--------------------------------------------------------------------------\n    TraceID Runtime::generate_library_trace_ids(const char *name, size_t count)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (count == 0)\n        return LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibraryTraceIDs>::const_iterator finder = \n          library_trace_ids.find(library_name);\n        if (finder != library_trace_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != count)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"TraceID generation counts %zd and %zd differ for library %s\",\n                finder->second.count, count, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibraryTraceIDs>::const_iterator finder = \n          library_trace_ids.find(library_name);\n        if (finder != library_trace_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != count)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"TraceID generation counts %zd and %zd differ for library %s\",\n                finder->second.count, count, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibraryTraceIDs &record = library_trace_ids[library_name];\n          record.count = count;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_trace_id;\n            unique_library_trace_id += count;\n#ifdef DEBUG_LEGION\n            assert(unique_library_trace_id > record.result);\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(count);\n          rez.serialize(request_event);\n        }\n        send_library_trace_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibraryTraceIDs>::const_iterator finder = \n          library_trace_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_trace_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ TraceID& Runtime::get_current_static_trace_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static TraceID next_trace_id = LEGION_MAX_APPLICATION_TRACE_ID;\n      return next_trace_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ TraceID Runtime::generate_static_trace_id(void)\n    //--------------------------------------------------------------------------\n    {\n      TraceID &next_trace = get_current_static_trace_id();\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'generate_static_trace_id' after \"\n                      \"the runtime has been started!\")\n      return next_trace++;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::complete_frame(Context ctx)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context issue frame!\");\n      ctx->complete_frame(); \n    }\n\n    //--------------------------------------------------------------------------\n    FutureMap Runtime::execute_must_epoch(Context ctx, \n                                          const MustEpochLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context issue must epoch!\");\n      return ctx->execute_must_epoch(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::issue_timing_measurement(Context ctx,\n                                             const TimingLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context in timing measurement!\");\n      return ctx->issue_timing_measurement(launcher); \n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::select_tunable_value(Context ctx, TunableID tid,\n                                         MapperID mid, MappingTagID tag,\n                                         const void *args, size_t argsize)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context select tunable value!\");\n      ctx->begin_runtime_call();\n#ifdef DEBUG_LEGION\n      log_run.debug(\"Getting a value for tunable variable %d in \"\n                    \"task %s (ID %lld)\", tid, ctx->get_task_name(),\n                    ctx->get_unique_id());\n#endif\n      const ApUserEvent to_trigger = Runtime::create_ap_user_event(NULL);\n      FutureImpl *result = new FutureImpl(this, true/*register*/,\n                              get_available_distributed_id(),\n                              address_space, to_trigger,\n                              ctx->get_owner_task());\n      // Make this here to get a local reference on it now\n      Future result_future(result);\n      result->add_base_gc_ref(FUTURE_HANDLE_REF);\n      SelectTunableArgs task_args(ctx->get_owner_task()->get_unique_op_id(),\n          mid, tag, tid, args, argsize, ctx, result, to_trigger);\n      if (legion_spy_enabled)\n        task_args.tunable_index = ctx->get_tunable_index();\n      issue_runtime_meta_task(task_args, LG_LATENCY_WORK_PRIORITY); \n      ctx->end_runtime_call();\n      return result_future;\n    }\n\n    //--------------------------------------------------------------------------\n    int Runtime::get_tunable_value(Context ctx, TunableID tid,\n                                   MapperID mid, MappingTagID tag)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context get tunable value!\");\n      ctx->begin_runtime_call();\n      Future f = select_tunable_value(ctx, tid, mid, tag, NULL, 0);\n      int result = f.get_result<int>();\n      if (legion_spy_enabled)\n      {\n        unsigned index = ctx->get_tunable_index();\n        LegionSpy::log_tunable_value(ctx->get_unique_id(), index,\n                                     &result, sizeof(result));\n      }\n      ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::perform_tunable_selection(const SelectTunableArgs *args)\n    //--------------------------------------------------------------------------\n    {\n      // Get the mapper first\n      MapperManager *mapper = find_mapper(args->ctx->get_executing_processor(),\n                                          args->mapper_id);\n      Mapper::SelectTunableInput input;\n      Mapper::SelectTunableOutput output;\n      input.tunable_id = args->tunable_id;\n      input.mapping_tag = args->tag;\n      input.args = args->args;\n      input.size = args->argsize;\n      output.value = NULL;\n      output.size = 0;\n      output.take_ownership = true;\n      mapper->invoke_select_tunable_value(args->ctx->get_owner_task(), \n                                          &input, &output);\n      if (legion_spy_enabled)\n        LegionSpy::log_tunable_value(args->ctx->get_unique_id(), \n            args->tunable_index, output.value, output.size);\n      // Set and complete the future\n      if ((output.value != NULL) && (output.size > 0))\n        args->result->set_result(output.value, output.size, \n                                 output.take_ownership);\n      Runtime::trigger_event(NULL, args->to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void* Runtime::get_local_task_variable(Context ctx, LocalVariableID id)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context get local task variable!\");\n      return ctx->get_local_task_variable(id);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::set_local_task_variable(Context ctx, LocalVariableID id,\n                                   const void *value, void (*destructor)(void*))\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\n            \"Illegal dummy context set local task variable!\");\n      ctx->set_local_task_variable(id, value, destructor);\n    }\n\n    //--------------------------------------------------------------------------\n    Mapper* Runtime::get_mapper(Context ctx, MapperID id, Processor target)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      if (!target.exists())\n      {\n        Processor proc = ctx->get_executing_processor();\n#ifdef DEBUG_LEGION\n        assert(proc_managers.find(proc) != proc_managers.end());\n#endif\n        if (ctx != DUMMY_CONTEXT)\n          ctx->end_runtime_call();\n        return proc_managers[proc]->find_mapper(id)->mapper;\n      }\n      else\n      {\n        std::map<Processor,ProcessorManager*>::const_iterator finder = \n          proc_managers.find(target);\n        if (finder == proc_managers.end())\n          REPORT_LEGION_ERROR(ERROR_INVALID_PROCESSOR_NAME, \n           \"Invalid processor \" IDFMT \" passed to get mapper call.\", target.id);\n        if (ctx != DUMMY_CONTEXT)\n          ctx->end_runtime_call();\n        return finder->second->find_mapper(id)->mapper;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    MappingCallInfo* Runtime::begin_mapper_call(Context ctx, MapperID id, \n                                                Processor target)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx != DUMMY_CONTEXT)\n        ctx->begin_runtime_call();\n      MapperManager *manager;\n      if (!target.exists())\n      {\n        Processor proc = ctx->get_executing_processor();\n#ifdef DEBUG_LEGION\n        assert(proc_managers.find(proc) != proc_managers.end());\n#endif\n        manager = proc_managers[proc]->find_mapper(id);\n      }\n      else\n      {\n        std::map<Processor,ProcessorManager*>::const_iterator finder = \n          proc_managers.find(target);\n        if (finder == proc_managers.end())\n          REPORT_LEGION_ERROR(ERROR_INVALID_PROCESSOR_NAME, \"Invalid processor \"\n                              IDFMT \" passed to begin mapper call.\", target.id)\n        manager = finder->second->find_mapper(id);\n      }\n      RtEvent ready;\n      MappingCallInfo *result = manager->begin_mapper_call(\n          APPLICATION_MAPPER_CALL, NULL, ready);\n      if (ready.exists() && !ready.has_triggered())\n        ready.wait();\n      if (ctx != DUMMY_CONTEXT)\n        ctx->end_runtime_call();\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::end_mapper_call(MappingCallInfo *info)\n    //--------------------------------------------------------------------------\n    {\n      info->manager->finish_mapper_call(info);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::print_once(Context ctx, FILE *f, const char *message)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context print once!\");\n      ctx->print_once(f, message);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::log_once(Context ctx, Realm::LoggerMessage &message)\n    //--------------------------------------------------------------------------\n    {\n      if (ctx == DUMMY_CONTEXT)\n        REPORT_DUMMY_CONTEXT(\"Illegal dummy context log once!\");\n      ctx->log_once(message);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_MPI_interop_configured(void)\n    //--------------------------------------------------------------------------\n    {\n      return (mpi_rank_table != NULL);\n    }\n\n    //--------------------------------------------------------------------------\n    const std::map<int,AddressSpace>& Runtime::find_forward_MPI_mapping(void)\n    //--------------------------------------------------------------------------\n    {\n      if (mpi_rank_table == NULL)\n        REPORT_LEGION_ERROR(ERROR_MPI_INTEROPERABILITY_NOT_CONFIGURED, \n             \"Forward MPI mapping call not supported without \"\n                      \"calling configure_MPI_interoperability during \"\n                      \"start up\")\n#ifdef DEBUG_LEGION\n      assert(!mpi_rank_table->forward_mapping.empty());\n#endif\n      return mpi_rank_table->forward_mapping;\n    }\n\n    //--------------------------------------------------------------------------\n    const std::map<AddressSpace,int>& Runtime::find_reverse_MPI_mapping(void)\n    //--------------------------------------------------------------------------\n    {\n      if (mpi_rank_table == NULL)\n        REPORT_LEGION_ERROR(ERROR_MPI_INTEROPERABILITY_NOT_CONFIGURED,\n             \"Reverse MPI mapping call not supported without \"\n                      \"calling configure_MPI_interoperability during \"\n                      \"start up\")\n#ifdef DEBUG_LEGION\n      assert(!mpi_rank_table->reverse_mapping.empty());\n#endif\n      return mpi_rank_table->reverse_mapping;\n    }\n\n    //--------------------------------------------------------------------------\n    int Runtime::find_local_MPI_rank(void)\n    //-------------------------------------------------------------------------\n    {\n      if (mpi_rank_table == NULL)\n        REPORT_LEGION_ERROR(ERROR_MPI_INTEROPERABILITY_NOT_CONFIGURED,\n             \"Findling local MPI rank not supported without \"\n                      \"calling configure_MPI_interoperability during \"\n                      \"start up\")\n      return mpi_rank;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::add_mapper(MapperID map_id, Mapper *mapper, Processor proc)\n    //--------------------------------------------------------------------------\n    {\n      // If we have a custom mapper then silently ignore this\n      if (!replay_file.empty() || enable_test_mapper)\n      {\n        // We take ownership of these things so delete it now\n        delete mapper;\n        return;\n      }\n      // First, wrap this mapper in a mapper manager\n      MapperManager *manager = wrap_mapper(this, mapper, map_id, proc);\n      if (!proc.exists())\n      {\n        bool own = true;\n        // Save it to all the managers\n        for (std::map<Processor,ProcessorManager*>::const_iterator it = \n              proc_managers.begin(); it != proc_managers.end(); it++)\n        {\n          it->second->add_mapper(map_id, manager, true/*check*/, own);\n          own = false;\n        }\n      }\n      else\n      {\n#ifdef DEBUG_LEGION\n        assert(proc_managers.find(proc) != proc_managers.end());\n#endif\n        proc_managers[proc]->add_mapper(map_id, manager, \n                                        true/*check*/, true/*own*/);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    Mapping::MapperRuntime* Runtime::get_mapper_runtime(void)\n    //--------------------------------------------------------------------------\n    {\n      return mapper_runtime;\n    }\n\n    //--------------------------------------------------------------------------\n    MapperID Runtime::generate_dynamic_mapper_id(bool check_context/*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_mapper_id();\n      MapperID result = __sync_fetch_and_add(&unique_mapper_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Mapper IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    MapperID Runtime::generate_library_mapper_ids(const char *name, size_t cnt)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (cnt == 0)\n        return LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibraryMapperIDs>::const_iterator finder = \n          library_mapper_ids.find(library_name);\n        if (finder != library_mapper_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"MapperID generation counts %zd and %zd differ for library %s\",\n                finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibraryMapperIDs>::const_iterator finder = \n          library_mapper_ids.find(library_name);\n        if (finder != library_mapper_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"MapperID generation counts %zd and %zd differ for library %s\",\n                finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibraryMapperIDs &record = library_mapper_ids[library_name];\n          record.count = cnt;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_mapper_id;\n            unique_library_mapper_id += cnt;\n#ifdef DEBUG_LEGION\n            assert(unique_library_mapper_id > record.result);\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(cnt);\n          rez.serialize(request_event);\n        }\n        send_library_mapper_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibraryMapperIDs>::const_iterator finder = \n          library_mapper_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_mapper_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ MapperID& Runtime::get_current_static_mapper_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static MapperID current_mapper_id = LEGION_MAX_APPLICATION_MAPPER_ID;\n      return current_mapper_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ MapperID Runtime::generate_static_mapper_id(void)\n    //--------------------------------------------------------------------------\n    {\n      MapperID &next_mapper = get_current_static_mapper_id(); \n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'generate_static_mapper_id' after \"\n                      \"the runtime has been started!\")\n      return next_mapper++;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::replace_default_mapper(Mapper *mapper, Processor proc)\n    //--------------------------------------------------------------------------\n    {\n      // If we have a custom mapper then silently ignore this\n      if (!replay_file.empty() || enable_test_mapper)\n      {\n        // We take ownership of mapper so delete it now\n        delete mapper;\n        return;\n      }\n      // First, wrap this mapper in a mapper manager\n      MapperManager *manager = wrap_mapper(this, mapper, 0, proc); \n      if (!proc.exists())\n      {\n        bool own = true;\n        // Save it to all the managers\n        for (std::map<Processor,ProcessorManager*>::const_iterator it = \n              proc_managers.begin(); it != proc_managers.end(); it++)\n        {\n          it->second->replace_default_mapper(manager, own);\n          own = false;\n        }\n      }\n      else\n      {\n#ifdef DEBUG_LEGION\n        assert(proc_managers.find(proc) != proc_managers.end());\n#endif\n        proc_managers[proc]->replace_default_mapper(manager, true/*own*/);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ MapperManager* Runtime::wrap_mapper(Runtime *rt, Mapper *mapper,\n                                                   MapperID map_id, Processor p)\n    //--------------------------------------------------------------------------\n    {\n      MapperManager *manager = NULL;\n      switch (mapper->get_mapper_sync_model())\n      {\n        case Mapper::CONCURRENT_MAPPER_MODEL:\n          {\n            manager = new ConcurrentManager(rt, mapper, map_id, p);\n            break;\n          }\n        case Mapper::SERIALIZED_REENTRANT_MAPPER_MODEL:\n          {\n            manager = new SerializingManager(rt, mapper, \n                                             map_id, p, true/*reentrant*/);\n            break;\n          }\n        case Mapper::SERIALIZED_NON_REENTRANT_MAPPER_MODEL:\n          {\n            manager = new SerializingManager(rt, mapper, \n                                             map_id, p, false/*reentrant*/);\n            break;\n          }\n        default:\n          assert(false);\n      }\n      return manager;\n    }\n\n    //--------------------------------------------------------------------------\n    MapperManager* Runtime::find_mapper(MapperID map_id)\n    //--------------------------------------------------------------------------\n    {\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        MapperManager *result = it->second->find_mapper(map_id);\n        if (result != NULL)\n          return result;\n      }\n      return NULL;\n    }\n\n    //--------------------------------------------------------------------------\n    MapperManager* Runtime::find_mapper(Processor target, MapperID map_id)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(target.exists());\n#endif\n      std::map<Processor,ProcessorManager*>::const_iterator finder = \n        proc_managers.find(target);\n#ifdef DEBUG_LEGION\n      assert(finder != proc_managers.end());\n#endif\n      return finder->second->find_mapper(map_id);\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionID Runtime::generate_dynamic_projection_id(\n                                                   bool check_context/*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_projection_id();\n      ProjectionID result = \n        __sync_fetch_and_add(&unique_projection_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Projection IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionID Runtime::generate_library_projection_ids(const char *name, \n                                                          size_t cnt)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (cnt == 0)\n        return LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibraryProjectionIDs>::const_iterator finder = \n          library_projection_ids.find(library_name);\n        if (finder != library_projection_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"ProjectionID generation counts %zd and %zd differ for \"\n                \"library %s\", finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibraryProjectionIDs>::const_iterator finder = \n          library_projection_ids.find(library_name);\n        if (finder != library_projection_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"ProjectionID generation counts %zd and %zd differ for \"\n                \"library %s\", finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibraryProjectionIDs &record = library_projection_ids[library_name];\n          record.count = cnt;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_projection_id;\n            unique_library_projection_id += cnt;\n#ifdef DEBUG_LEGION\n            assert(unique_library_projection_id > record.result);\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(cnt);\n          rez.serialize(request_event);\n        }\n        send_library_projection_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibraryProjectionIDs>::const_iterator finder = \n          library_projection_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_projection_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n    \n    //--------------------------------------------------------------------------\n    /*static*/ ProjectionID& Runtime::get_current_static_projection_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static ProjectionID current_projection_id = \n        LEGION_MAX_APPLICATION_PROJECTION_ID;\n      return current_projection_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ProjectionID Runtime::generate_static_projection_id(void)\n    //--------------------------------------------------------------------------\n    {\n      ProjectionID &next_projection = get_current_static_projection_id();\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'generate_static_projection_id' after \"\n                      \"the runtime has been started!\");\n      return next_projection++;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_projection_functor(ProjectionID pid,\n                                              ProjectionFunctor *functor,\n                                              bool need_zero_check,\n                                              bool silence_warnings,\n                                              const char *warning_string,\n                                              bool preregistered)\n    //--------------------------------------------------------------------------\n    {\n      if (need_zero_check && (pid == 0))\n        REPORT_LEGION_ERROR(ERROR_RESERVED_PROJECTION_ID, \n                            \"ProjectionID zero is reserved.\\n\");\n      if (!preregistered && !inside_registration_callback && !silence_warnings)\n        REPORT_LEGION_WARNING(LEGION_WARNING_NON_CALLBACK_REGISTRATION,\n            \"Projection functor %d was dynamically registered outside of a \"\n            \"registration callback invocation. In the near future this will \"\n            \"become an error in order to support task subprocesses. Please \"\n            \"use 'perform_registration_callback' to generate a callback where \"\n            \"it will be safe to perform dynamic registrations.\", pid)\n      if (!silence_warnings && (total_address_spaces > 1) &&\n          (inside_registration_callback != GLOBAL_REGISTRATION_CALLBACK))\n        REPORT_LEGION_WARNING(LEGION_WARNING_DYNAMIC_PROJECTION_REG,\n                        \"Projection functor %d is being dynamically \"\n                        \"registered for a multi-node run with %d nodes. It is \"\n                        \"currently the responsibility of the application to \"\n                        \"ensure that this projection functor is registered on \"\n                        \"all nodes where it will be required. \"\n                        \"Warning string: %s\", pid, total_address_spaces,\n                        (warning_string == NULL) ? \"\" : warning_string)\n      ProjectionFunction *function = new ProjectionFunction(pid, functor);\n      AutoLock p_lock(projection_lock);\n      std::map<ProjectionID,ProjectionFunction*>::\n        const_iterator finder = projection_functions.find(pid);\n      if (finder != projection_functions.end())\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_PROJECTION_ID, \n                      \"ProjectionID %d has already been used in \"\n                      \"the region projection table\\n\", pid)\n      projection_functions[pid] = function;\n      if (legion_spy_enabled)\n        LegionSpy::log_projection_function(pid, function->depth, \n                                           function->is_invertible);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::preregister_projection_functor(ProjectionID pid,\n                                                     ProjectionFunctor *functor)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'preregister_projection_functor' after \"\n                      \"the runtime has started!\")\n      if (pid == 0)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_PROJECTION_ID, \n                            \"ProjectionID zero is reserved.\\n\");\n      std::map<ProjectionID,ProjectionFunctor*> &pending_projection_functors =\n        get_pending_projection_table();\n      std::map<ProjectionID,ProjectionFunctor*>::const_iterator finder = \n        pending_projection_functors.find(pid);\n      if (finder != pending_projection_functors.end())\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_PROJECTION_ID, \n                      \"ProjectionID %d has already been used in \"\n                      \"the region projection table\\n\", pid)\n      pending_projection_functors[pid] = functor;\n    }\n\n    //--------------------------------------------------------------------------\n    ProjectionFunction* Runtime::find_projection_function(ProjectionID pid,\n                                                          bool can_fail)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(projection_lock,1,false/*exclusive*/);\n      std::map<ProjectionID,ProjectionFunction*>::\n        const_iterator finder = projection_functions.find(pid);\n      if (finder == projection_functions.end())\n      {\n        if (can_fail)\n          return NULL;\n        REPORT_LEGION_ERROR(ERROR_INVALID_PROJECTION_ID, \n                        \"Unable to find registered region projection ID %d. \"\n                        \"Please upgrade to using projection functors!\", pid);\n      }\n      return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ProjectionFunctor* Runtime::get_projection_functor(\n                                                               ProjectionID pid) \n    //--------------------------------------------------------------------------\n    {\n      if (runtime_started)\n      {\n        ProjectionFunction *func = \n          the_runtime->find_projection_function(pid, true/*can fail*/);\n        if (func != NULL)\n          return func->functor;\n      }\n      else\n      {\n        std::map<ProjectionID,ProjectionFunctor*> &pending_projection_functors =\n          get_pending_projection_table();\n        std::map<ProjectionID,ProjectionFunctor*>::const_iterator finder = \n          pending_projection_functors.find(pid);\n        if (finder != pending_projection_functors.end())\n          return finder->second;\n      }\n      return NULL;\n    }\n\n    //--------------------------------------------------------------------------\n    ShardingID Runtime::generate_dynamic_sharding_id(bool check_context/*true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_sharding_id();\n      ShardingID result = \n        __sync_fetch_and_add(&unique_sharding_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Shardinging IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    ShardingID Runtime::generate_library_sharding_ids(const char *name,\n                                                      size_t cnt)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (cnt == 0)\n        return LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibraryShardingIDs>::const_iterator finder = \n          library_sharding_ids.find(library_name);\n        if (finder != library_sharding_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n               \"ShardingID generation counts %zd and %zd differ for library %s\",\n               finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibraryShardingIDs>::const_iterator finder = \n          library_sharding_ids.find(library_name);\n        if (finder != library_sharding_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n               \"ShardingID generation counts %zd and %zd differ for library %s\",\n               finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibraryShardingIDs &record = library_sharding_ids[library_name];\n          record.count = cnt;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_sharding_id;\n            unique_library_sharding_id += cnt;\n#ifdef DEBUG_LEGION\n            assert(unique_library_sharding_id > record.result);\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(cnt);\n          rez.serialize(request_event);\n        }\n        send_library_sharding_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibraryShardingIDs>::const_iterator finder = \n          library_sharding_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_sharding_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ShardingID& Runtime::get_current_static_sharding_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static ShardingID current_sharding_id = \n        LEGION_MAX_APPLICATION_SHARDING_ID;\n      return current_sharding_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ShardingID Runtime::generate_static_sharding_id(void)\n    //--------------------------------------------------------------------------\n    {\n      ShardingID &next_sharding = get_current_static_sharding_id();\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START,\n                      \"Illegal call to 'generate_static_sharding_id' after \"\n                      \"the runtime has been started!\")\n      return next_sharding++;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_sharding_functor(ShardingID sid,\n                                            ShardingFunctor *functor,\n                                            bool need_zero_check,\n                                            bool silence_warnings,\n                                            const char *warning_string,\n                                            bool preregistered)\n    //--------------------------------------------------------------------------\n    {\n      if (sid == UINT_MAX)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_SHARDING_ID,\n            \"ERROR: %d (UINT_MAX) is a reserved sharding ID.\", UINT_MAX)\n      else if (need_zero_check && (sid == 0))\n        REPORT_LEGION_ERROR(ERROR_RESERVED_SHARDING_ID,\n                            \"ERROR: ShardingID zero is reserved.\")\n      if (!preregistered && !inside_registration_callback && !silence_warnings)\n        REPORT_LEGION_WARNING(LEGION_WARNING_NON_CALLBACK_REGISTRATION,\n            \"Sharding functor %d was dynamically registered outside of a \"\n            \"registration callback invocation. In the near future this will \"\n            \"become an error in order to support task subprocesses. Please \"\n            \"use 'perform_registration_callback' to generate a callback where \"\n            \"it will be safe to perform dynamic registrations.\", sid)\n      if (!silence_warnings && (total_address_spaces > 1) &&\n          (inside_registration_callback != GLOBAL_REGISTRATION_CALLBACK))\n        REPORT_LEGION_WARNING(LEGION_WARNING_DYNAMIC_SHARDING_REG,\n                        \"WARNING: Sharding functor %d is being dynamically \"\n                        \"registered for a multi-node run with %d nodes. It is \"\n                        \"currently the responsibility of the application to \"\n                        \"ensure that this sharding functor is registered on \"\n                        \"all nodes where it will be required. \"\n                        \"Warning string: %s\", sid, total_address_spaces,\n                        (warning_string == NULL) ? \"\" : warning_string)\n      AutoLock s_lock(sharding_lock);\n      std::map<ShardingID,ShardingFunctor*>::const_iterator finder = \n        sharding_functors.find(sid);\n      if (finder != sharding_functors.end())\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_SHARDING_ID,\n                      \"ERROR: ShardingID %d has already been used by another \"\n                      \"sharding functor.\", sid)\n      sharding_functors[sid] = functor;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::preregister_sharding_functor(ShardingID sid,\n                                                       ShardingFunctor *functor)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START,\n                      \"Illegal call to 'preregister_sharding_functor' after \"\n                      \"the runtime has started!\");\n      if (sid == UINT_MAX)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_SHARDING_ID,\n            \"ERROR: %d (UINT_MAX) is a reserved sharding ID.\", UINT_MAX)\n      else if (sid == 0)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_SHARDING_ID,\n                            \"ERROR: ShardingID zero is reserved.\")\n      std::map<ShardingID,ShardingFunctor*> &pending_sharding_functors = \n        get_pending_sharding_table();\n      std::map<ShardID,ShardingFunctor*>::const_iterator finder = \n        pending_sharding_functors.find(sid);\n      if (finder != pending_sharding_functors.end())\n        REPORT_LEGION_ERROR(ERROR_DUPLICATE_SHARDING_ID,\n                      \"ERROR: ShardingID %d has already been used by another \"\n                      \"sharding functor.\", sid)\n      pending_sharding_functors[sid] = functor;\n    }\n\n    //--------------------------------------------------------------------------\n    ShardingFunctor* Runtime::find_sharding_functor(ShardingID sid, \n                                                    bool can_fail)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(sharding_lock,1,false/*exclusive*/);\n      std::map<ShardingID,ShardingFunctor*>::const_iterator finder = \n        sharding_functors.find(sid);\n      if (finder == sharding_functors.end())\n      {\n        if (can_fail)\n          return NULL;\n        REPORT_LEGION_ERROR(ERROR_INVALID_SHARDING_ID,\n                    \"Unable to find registered sharding functor ID %d.\", sid)\n      }\n      return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ShardingFunctor* Runtime::get_sharding_functor(ShardingID sid)\n    //--------------------------------------------------------------------------\n    {\n      if (!runtime_started)\n      {\n        std::map<ShardingID,ShardingFunctor*> &pending_sharding_functors = \n          get_pending_sharding_table();\n        std::map<ShardID,ShardingFunctor*>::const_iterator finder = \n          pending_sharding_functors.find(sid);\n        if (finder == pending_sharding_functors.end())\n          return NULL;\n        else\n          return finder->second;\n      }\n      else\n        return the_runtime->find_sharding_functor(sid, true/*can fail*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(TaskID task_id, SemanticTag tag,\n           const void *buffer, size_t size, bool is_mutable, bool send_to_owner)\n    //--------------------------------------------------------------------------\n    {\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(send_to_owner))\n        return;\n      if ((tag == LEGION_NAME_SEMANTIC_TAG) && legion_spy_enabled)\n        LegionSpy::log_task_name(task_id, static_cast<const char*>(buffer));\n      TaskImpl *impl = find_or_create_task_impl(task_id);\n      impl->attach_semantic_information(tag, address_space, buffer, size, \n                                        is_mutable, send_to_owner);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(IndexSpace handle, \n                                              SemanticTag tag,\n                                              const void *buffer, size_t size,\n                                              bool is_mutable)\n    //--------------------------------------------------------------------------\n    {\n      bool global = true;\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(global))\n        return;\n      forest->attach_semantic_information(handle, tag, address_space, \n                                          buffer, size, is_mutable, !global);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(IndexPartition handle, \n                                              SemanticTag tag,\n                                              const void *buffer, size_t size,\n                                              bool is_mutable)\n    //--------------------------------------------------------------------------\n    {\n      bool global = true;\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(global))\n        return;\n      forest->attach_semantic_information(handle, tag, address_space, \n                                          buffer, size, is_mutable, !global);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(FieldSpace handle, \n                                              SemanticTag tag,\n                                              const void *buffer, size_t size,\n                                              bool is_mutable)\n    //--------------------------------------------------------------------------\n    {\n      bool global = true;\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(global))\n        return;\n      forest->attach_semantic_information(handle, tag, address_space, \n                                          buffer, size, is_mutable, !global);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(FieldSpace handle, FieldID fid,\n                                              SemanticTag tag,\n                                              const void *buffer, size_t size,\n                                              bool is_mutable)\n    //--------------------------------------------------------------------------\n    {\n      bool global = true;\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(global))\n        return;\n      forest->attach_semantic_information(handle, fid, tag, address_space, \n                                          buffer, size, is_mutable, !global);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(LogicalRegion handle, \n                                              SemanticTag tag,\n                                              const void *buffer, size_t size,\n                                              bool is_mutable)\n    //--------------------------------------------------------------------------\n    {\n      bool global = true;\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(global))\n        return;\n      forest->attach_semantic_information(handle, tag, address_space, \n                                          buffer, size, is_mutable, !global);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::attach_semantic_information(LogicalPartition handle, \n                                              SemanticTag tag,\n                                              const void *buffer, size_t size,\n                                              bool is_mutable)\n    //--------------------------------------------------------------------------\n    {\n      bool global = true;\n      if ((implicit_context != NULL) && \n          !implicit_context->perform_semantic_attach(global))\n        return;\n      forest->attach_semantic_information(handle, tag, address_space, \n                                          buffer, size, is_mutable, !global);\n      if (implicit_context != NULL)\n        implicit_context->post_semantic_attach();\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(TaskID task_id,SemanticTag tag,\n              const void *&result, size_t &size, bool can_fail, bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      TaskImpl *impl = find_or_create_task_impl(task_id);\n      return impl->retrieve_semantic_information(tag, result, size, \n                                                 can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(IndexSpace handle,\n                                                SemanticTag tag,\n                                                const void *&result, \n                                                size_t &size, bool can_fail,\n                                                bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      return forest->retrieve_semantic_information(handle, tag, result, size,\n                                                   can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(IndexPartition handle,\n                                                SemanticTag tag,\n                                                const void *&result, \n                                                size_t &size, bool can_fail,\n                                                bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      return forest->retrieve_semantic_information(handle, tag, result, size, \n                                                   can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(FieldSpace handle,\n                                                SemanticTag tag,\n                                                const void *&result, \n                                                size_t &size, bool can_fail,\n                                                bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      return forest->retrieve_semantic_information(handle, tag, result, size,\n                                                   can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(FieldSpace handle, FieldID fid,\n                                                SemanticTag tag,\n                                                const void *&result, \n                                                size_t &size, bool can_fail,\n                                                bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      return forest->retrieve_semantic_information(handle, fid, tag, result, \n                                                   size, can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(LogicalRegion handle,\n                                                SemanticTag tag,\n                                                const void *&result, \n                                                size_t &size, bool can_fail,\n                                                bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      return forest->retrieve_semantic_information(handle, tag, result, size,\n                                                   can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::retrieve_semantic_information(LogicalPartition handle,\n                                                SemanticTag tag,\n                                                const void *&result, \n                                                size_t &size, bool can_fail,\n                                                bool wait_until)\n    //--------------------------------------------------------------------------\n    {\n      return forest->retrieve_semantic_information(handle, tag, result, size,\n                                                   can_fail, wait_until);\n    }\n\n    //--------------------------------------------------------------------------\n    TaskID Runtime::generate_dynamic_task_id(bool check_context/*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_task_id();\n      TaskID result = __sync_fetch_and_add(&unique_task_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Task IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    TaskID Runtime::generate_library_task_ids(const char *name, size_t cnt)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (cnt == 0)\n        return LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibraryTaskIDs>::const_iterator finder = \n          library_task_ids.find(library_name);\n        if (finder != library_task_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"TaskID generation counts %zd and %zd differ for library %s\",\n                finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibraryTaskIDs>::const_iterator finder = \n          library_task_ids.find(library_name);\n        if (finder != library_task_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != cnt)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"TaskID generation counts %zd and %zd differ for library %s\",\n                finder->second.count, cnt, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibraryTaskIDs &record = library_task_ids[library_name];\n          record.count = cnt;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_task_id;\n            unique_library_task_id += cnt;\n#ifdef DEBUG_LEGION\n            assert(unique_library_task_id > record.result);\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(cnt);\n          rez.serialize(request_event);\n        }\n        send_library_task_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibraryTaskIDs>::const_iterator finder = \n          library_task_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_task_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n\n    //--------------------------------------------------------------------------\n    VariantID Runtime::register_variant(const TaskVariantRegistrar &registrar,\n                                  const void *user_data, size_t user_data_size,\n                                  const CodeDescriptor &realm_code_desc,\n                                  bool ret,VariantID vid /*= AUTO_GENERATE_ID*/,\n                                  bool check_task_id /*= true*/,\n                                  bool check_context /*= true*/,\n                                  bool preregistered /*= false*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->register_variant(registrar, user_data,\n            user_data_size, realm_code_desc, ret, vid, check_task_id);\n      // TODO: figure out a way to make this check safe with dynamic generation\n#if 0\n      if (check_task_id && \n          (registrar.task_id >= LEGION_MAX_APPLICATION_TASK_ID))\n        REPORT_LEGION_ERROR(ERROR_MAX_APPLICATION_TASK_ID_EXCEEDED, \n                      \"Error registering task with ID %d. Exceeds the \"\n                      \"statically set bounds on application task IDs of %d. \"\n                      \"See %s in legion_config.h.\", \n                      registrar.task_id, LEGION_MAX_APPLICATION_TASK_ID, \n                      LEGION_MACRO_TO_STRING(LEGION_MAX_APPLICATION_TASK_ID))\n#endif  \n      // First find the task implementation\n      TaskImpl *task_impl = find_or_create_task_impl(registrar.task_id);\n      // See if we need to make a new variant ID\n      if (vid == LEGION_AUTO_GENERATE_ID) // Make a variant ID to use\n        vid = task_impl->get_unique_variant_id();\n      else if (vid == 0)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_VARIANT_ID,\n                      \"Error registering variant for task ID %d with \"\n                      \"variant ID 0. Variant ID 0 is reserved for task \"\n                      \"generators.\", registrar.task_id)\n      // Make our variant and add it to the set of variants\n      VariantImpl *impl = new VariantImpl(this, vid, task_impl, \n                                          registrar, ret, realm_code_desc,\n                                          user_data, user_data_size);\n      // Add this variant to the owner\n      task_impl->add_variant(impl);\n      {\n        AutoLock tv_lock(task_variant_lock);\n        variant_table.push_back(impl);\n      }\n      // If this is a global registration we need to broadcast the variant\n      if (registrar.global_registration && (total_address_spaces > 1))\n      {\n        RtUserEvent done_event = Runtime::create_rt_user_event();\n        impl->broadcast_variant(done_event, address_space, 0);\n        done_event.wait();\n      }\n      if (legion_spy_enabled)\n        LegionSpy::log_task_variant(registrar.task_id, vid, \n                                    impl->is_inner(), impl->is_leaf(),\n                                    impl->is_idempotent(), impl->get_name());\n      return vid;\n    }\n\n    //--------------------------------------------------------------------------\n    TaskImpl* Runtime::find_or_create_task_impl(TaskID task_id)\n    //--------------------------------------------------------------------------\n    {\n      {\n        AutoLock tv_lock(task_variant_lock,1,false/*exclusive*/);\n        std::map<TaskID,TaskImpl*>::const_iterator finder = \n          task_table.find(task_id);\n        if (finder != task_table.end())\n          return finder->second;\n      }\n      AutoLock tv_lock(task_variant_lock);\n      std::map<TaskID,TaskImpl*>::const_iterator finder = \n        task_table.find(task_id);\n      // Check to see if we lost the race\n      if (finder == task_table.end())\n      {\n        TaskImpl *result = new TaskImpl(task_id, this);\n        task_table[task_id] = result;\n        return result;\n      }\n      else // Lost the race as it already exists\n        return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    TaskImpl* Runtime::find_task_impl(TaskID task_id)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock tv_lock(task_variant_lock,1,false/*exclusive*/);\n      std::map<TaskID,TaskImpl*>::const_iterator finder = \n        task_table.find(task_id);\n#ifdef DEBUG_LEGION\n      assert(finder != task_table.end());\n#endif\n      return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    VariantImpl* Runtime::find_variant_impl(TaskID task_id, \n                                             VariantID variant_id,bool can_fail)\n    //--------------------------------------------------------------------------\n    {\n      TaskImpl *owner = find_or_create_task_impl(task_id);\n      return owner->find_variant_impl(variant_id, can_fail);\n    }\n\n    //--------------------------------------------------------------------------\n    ReductionOpID Runtime::generate_dynamic_reduction_id(\n                                                   bool check_context/*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_reduction_id();\n      ReductionOpID result = \n        __sync_fetch_and_add(&unique_redop_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Reduction IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    ReductionOpID Runtime::generate_library_reduction_ids(const char *name,\n                                                          size_t count)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (count == 0)\n        return (ReductionOpID)LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibraryRedopIDs>::const_iterator finder = \n          library_redop_ids.find(library_name);\n        if (finder != library_redop_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != count)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"ReductionOpID generation counts %zd and %zd differ for \"\n                \"library %s\", finder->second.count, count, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibraryRedopIDs>::const_iterator finder = \n          library_redop_ids.find(library_name);\n        if (finder != library_redop_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != count)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"ReductionOpID generation counts %zd and %zd differ for \"\n                \"library %s\", finder->second.count, count, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibraryRedopIDs &record = library_redop_ids[library_name];\n          record.count = count;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_redop_id;\n            unique_library_redop_id += count;\n#ifdef DEBUG_LEGION\n            assert(unique_library_redop_id > unsigned(record.result));\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(count);\n          rez.serialize(request_event);\n        }\n        send_library_redop_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibraryRedopIDs>::const_iterator finder = \n          library_redop_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_redop_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n\n    //--------------------------------------------------------------------------\n    CustomSerdezID Runtime::generate_dynamic_serdez_id(\n                                                   bool check_context/*= true*/)\n    //--------------------------------------------------------------------------\n    {\n      if (check_context && (implicit_context != NULL))\n        return implicit_context->generate_dynamic_serdez_id();\n      CustomSerdezID result = \n        __sync_fetch_and_add(&unique_serdez_id, runtime_stride);\n      // Check for hitting the library limit\n      if (result >= LEGION_INITIAL_LIBRARY_ID_OFFSET)\n        REPORT_LEGION_FATAL(LEGION_FATAL_EXCEEDED_LIBRARY_ID_OFFSET,\n            \"Dynamic Custom Serdez IDs exceeded library ID offset %d\",\n            LEGION_INITIAL_LIBRARY_ID_OFFSET)\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    CustomSerdezID Runtime::generate_library_serdez_ids(const char *name,\n                                                        size_t count)\n    //--------------------------------------------------------------------------\n    {\n      // Easy case if the user asks for no IDs\n      if (count == 0)\n        return (CustomSerdezID)LEGION_AUTO_GENERATE_ID;\n      const std::string library_name(name); \n      // Take the lock in read only mode and see if we can find the result\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(library_lock,1,false/*exclusive*/);\n        std::map<std::string,LibrarySerdezIDs>::const_iterator finder = \n          library_serdez_ids.find(library_name);\n        if (finder != library_serdez_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != count)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"CustomSerdezID generation counts %zd and %zd differ for \"\n                \"library %s\", finder->second.count, count, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n      }\n      RtUserEvent request_event;\n      if (!wait_on.exists())\n      {\n        AutoLock l_lock(library_lock);\n        // Check to make sure we didn't lose the race\n        std::map<std::string,LibrarySerdezIDs>::const_iterator finder = \n          library_serdez_ids.find(library_name);\n        if (finder != library_serdez_ids.end())\n        {\n          // First do a check to see if the counts match\n          if (finder->second.count != count)\n            REPORT_LEGION_ERROR(ERROR_LIBRARY_COUNT_MISMATCH,\n                \"CustomSerdezID generation counts %zd and %zd differ for \"\n                \"library %s\", finder->second.count, count, name)\n          if (finder->second.result_set)\n            return finder->second.result;\n          // This should never happen unless we are on a node other than 0\n#ifdef DEBUG_LEGION\n          assert(address_space > 0);\n#endif\n          wait_on = finder->second.ready;\n        }\n        if (!wait_on.exists())\n        {\n          LibrarySerdezIDs &record = library_serdez_ids[library_name];\n          record.count = count;\n          if (address_space == 0)\n          {\n            // We're going to make the result\n            record.result = unique_library_serdez_id;\n            unique_library_serdez_id += count;\n#ifdef DEBUG_LEGION\n            assert(unique_library_serdez_id > unsigned(record.result));\n#endif\n            record.result_set = true;\n            return record.result;\n          }\n          else\n          {\n            // We're going to request the result\n            request_event = Runtime::create_rt_user_event();\n            record.ready = request_event;\n            record.result_set = false;\n            wait_on = request_event;\n          }\n        }\n      }\n      // Should only get here on nodes other than 0\n#ifdef DEBUG_LEGION\n      assert(address_space > 0);\n      assert(wait_on.exists());\n#endif\n      if (request_event.exists())\n      {\n        // Include the null terminator in length\n        const size_t string_length = strlen(name) + 1;\n        // Send the request to node 0 for the result\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize<size_t>(string_length);\n          rez.serialize(name, string_length);\n          rez.serialize<size_t>(count);\n          rez.serialize(request_event);\n        }\n        send_library_serdez_request(0/*target*/, rez);\n      }\n      wait_on.wait();\n      // When we wake up we should be able to find the result\n      AutoLock l_lock(library_lock,1,false/*exclusive*/);\n      std::map<std::string,LibrarySerdezIDs>::const_iterator finder = \n          library_serdez_ids.find(library_name);\n#ifdef DEBUG_LEGION\n      assert(finder != library_serdez_ids.end());\n      assert(finder->second.result_set);\n#endif\n      return finder->second.result;\n    }\n\n    //--------------------------------------------------------------------------\n    MemoryManager* Runtime::find_memory_manager(Memory mem)\n    //--------------------------------------------------------------------------\n    {\n      {\n        AutoLock m_lock(memory_manager_lock,1,false/*exclusive*/);\n        std::map<Memory,MemoryManager*>::const_iterator finder = \n          memory_managers.find(mem);\n        if (finder != memory_managers.end())\n          return finder->second;\n      }\n      // Not there?  Take exclusive lock and check again, create if needed\n      AutoLock m_lock(memory_manager_lock);\n      std::map<Memory,MemoryManager*>::const_iterator finder =\n        memory_managers.find(mem);\n      if (finder != memory_managers.end())\n        return finder->second;\n      // Really do need to create it (and put it in the map)\n      MemoryManager *result = new MemoryManager(mem, this);\n      memory_managers[mem] = result;\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    AddressSpaceID Runtime::find_address_space(Memory handle) const\n    //--------------------------------------------------------------------------\n    {\n      // Just use the standard translation for now\n      AddressSpaceID result = handle.address_space();\n      return result;\n    }\n\n#ifdef LEGION_MALLOC_INSTANCES\n    //--------------------------------------------------------------------------\n    uintptr_t Runtime::allocate_deferred_instance(Memory memory, size_t size,\n                                                  bool free)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(memory);\n      // Note that we don't need to defer this because this call had to \n      // come from an application processor where we can do the call\n      // to allocate directly (e.g. CUDA contexts are already here)\n      uintptr_t result = manager->allocate_legion_instance(size,false/*defer*/);\n      if (free)\n        manager->free_legion_instance(\n            RtEvent(Processor::get_current_finish_event()), result, false);\n      return result;\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    MessageManager* Runtime::find_messenger(AddressSpaceID sid)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(sid < LEGION_MAX_NUM_NODES);\n      assert(sid != address_space); // shouldn't be sending messages to ourself\n#endif\n      MessageManager *result = message_managers[sid];\n      if (result != NULL)\n        return result;\n      // If we made it here, then we don't have a message manager yet\n      // re-take the lock and re-check to see if we don't have a manager\n      // If we still don't then we need to make one\n      RtEvent wait_on;\n      bool send_request = false;\n      {\n        AutoLock m_lock(message_manager_lock);\n        // Re-check to see if we lost the race, force the compiler\n        // to re-load the value here\n        result = *(((MessageManager**)message_managers)+sid);\n        if (result != NULL)\n          return result;\n        // Figure out if there is an event to wait on yet\n        std::map<AddressSpace,RtUserEvent>::const_iterator finder = \n          pending_endpoint_requests.find(sid);\n        if (finder == pending_endpoint_requests.end())\n        {\n          RtUserEvent done = Runtime::create_rt_user_event();\n          pending_endpoint_requests[sid] = done;\n          wait_on = done;\n          send_request = true;\n        }\n        else\n          wait_on = finder->second;\n      }\n      if (send_request)\n      {\n#ifdef DEBUG_LEGION\n        bool found = false;\n#endif\n        // Find a processor on which to send the task\n        for (std::map<Processor,AddressSpaceID>::const_iterator it = \n              proc_spaces.begin(); it != proc_spaces.end(); it++)\n        {\n          if (it->second != sid)\n            continue;\n#ifdef DEBUG_LEGION\n          found = true;\n#endif\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize<bool>(true); // request\n            rez.serialize(utility_group);\n          }\n          const Realm::ProfilingRequestSet empty_requests;\n          it->first.spawn(LG_ENDPOINT_TASK_ID, rez.get_buffer(),\n              rez.get_used_bytes(), empty_requests);\n          break;\n        }\n#ifdef DEBUG_LEGION\n        assert(found);\n#endif\n      }\n#ifdef DEBUG_LEGION\n      assert(wait_on.exists());\n#endif\n      if (!wait_on.has_triggered())\n        wait_on.wait();\n      // When we wake up there should be a result\n      result = *(((MessageManager**)message_managers)+sid);\n#ifdef DEBUG_LEGION\n      assert(result != NULL);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    MessageManager* Runtime::find_messenger(Processor target)\n    //--------------------------------------------------------------------------\n    {\n      return find_messenger(find_address_space(target));\n    }\n\n    //--------------------------------------------------------------------------\n    AddressSpaceID Runtime::find_address_space(Processor target) const\n    //--------------------------------------------------------------------------\n    {\n      std::map<Processor,AddressSpaceID>::const_iterator finder = \n        proc_spaces.find(target);\n      if (finder != proc_spaces.end())\n        return finder->second;\n#ifdef DEBUG_LEGION\n      // If we get here then this better be a processor group\n      assert(target.kind() == Processor::PROC_GROUP);\n#endif\n      AutoLock m_lock(message_manager_lock,1,false/*exclusive*/);\n      finder = endpoint_spaces.find(target);\n#ifdef DEBUG_LEGION\n      assert(finder != endpoint_spaces.end());\n#endif\n      return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_endpoint_creation(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      bool request;\n      derez.deserialize(request);\n      Processor remote_utility_group;\n      derez.deserialize(remote_utility_group);\n      if (request)\n      {\n        Serializer rez;\n        {\n          RezCheck z2(rez);\n          rez.serialize<bool>(false/*request*/);\n          rez.serialize(utility_group);\n          rez.serialize(address_space);\n        }\n        const Realm::ProfilingRequestSet empty_requests;\n        remote_utility_group.spawn(LG_ENDPOINT_TASK_ID, rez.get_buffer(),\n            rez.get_used_bytes(), empty_requests); \n      }\n      else\n      {\n        AddressSpaceID remote_space;\n        derez.deserialize(remote_space);\n        AutoLock m_lock(message_manager_lock);\n        message_managers[remote_space] = new MessageManager(remote_space, \n                            this, max_message_size, remote_utility_group);\n        // Also update the endpoint spaces\n        endpoint_spaces[remote_utility_group] = remote_space;\n        std::map<AddressSpaceID,RtUserEvent>::iterator finder = \n          pending_endpoint_requests.find(remote_space);\n#ifdef DEBUG_LEGION\n        assert(finder != pending_endpoint_requests.end());\n#endif\n        Runtime::trigger_event(finder->second);\n        pending_endpoint_requests.erase(finder);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::process_mapper_message(Processor target, MapperID map_id,\n                                     Processor source, const void *message,\n                                     size_t message_size, unsigned message_kind)\n    //--------------------------------------------------------------------------\n    {\n      if (is_local(target))\n      {\n        Mapper::MapperMessage message_args;\n        message_args.sender = source;\n        message_args.kind = message_kind;\n        message_args.message = message;\n        message_args.size = message_size;\n        message_args.broadcast = false;\n        MapperManager *mapper = find_mapper(target, map_id);\n        mapper->invoke_handle_message(&message_args);\n      }\n      else\n      {\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(target);\n          rez.serialize(map_id);\n          rez.serialize(source);\n          rez.serialize(message_kind);\n          rez.serialize(message_size);\n          rez.serialize(message, message_size);\n        }\n        send_mapper_message(find_address_space(target), rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::process_mapper_broadcast(MapperID map_id, Processor source, \n                                    const void *message, size_t message_size, \n                                    unsigned message_kind, int radix, int index)\n    //--------------------------------------------------------------------------\n    {\n      // First forward the message onto any remote nodes\n      int base = index * radix;\n      int init;\n      if (separate_runtime_instances)\n      {\n        std::map<Processor,AddressSpaceID>::const_iterator finder = \n          proc_spaces.find(source); \n#ifdef DEBUG_LEGION\n        // only works with a single process\n        assert(finder != proc_spaces.end()); \n#endif\n        init = finder->second;\n      }\n      else\n        init = source.address_space();\n      // The runtime stride is the same as the number of nodes\n      const int total_nodes = total_address_spaces;\n      for (int r = 1; r <= radix; r++)\n      {\n        int offset = base + r;\n        // If we've handled all of our nodes then we are done\n        if (offset >= total_nodes)\n          break;\n        AddressSpaceID target = (init + offset) % total_nodes;\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(map_id);\n          rez.serialize(source);\n          rez.serialize(message_kind);\n          rez.serialize(radix);\n          rez.serialize(offset);\n          rez.serialize(message_size);\n          rez.serialize(message, message_size);\n        }\n        send_mapper_broadcast(target, rez);\n      }\n      // Then send it to all our local mappers, set will deduplicate\n      std::set<MapperManager*> managers;\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        managers.insert(it->second->find_mapper(map_id));\n      }\n      Mapper::MapperMessage message_args;\n      message_args.sender = source;\n      message_args.kind = message_kind;\n      message_args.message = message;\n      message_args.size = message_size;\n      message_args.broadcast = true;\n      for (std::set<MapperManager*>::const_iterator it = \n            managers.begin(); it != managers.end(); it++)\n        (*it)->invoke_handle_message(&message_args);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_task(TaskOp *task)\n    //--------------------------------------------------------------------------\n    {\n      Processor target = task->target_proc;\n      if (!target.exists())\n        REPORT_LEGION_ERROR(ERROR_INVALID_TARGET_PROC, \n                      \"Mapper requested invalid NO_PROC as target proc!\");\n      // Check to see if the target processor is still local \n      std::map<Processor,ProcessorManager*>::const_iterator finder = \n        proc_managers.find(target);\n      if (finder != proc_managers.end())\n      {\n        // Update the current processor\n        task->set_current_proc(target);\n        finder->second->add_to_ready_queue(task);\n      }\n      else\n      {\n        MessageManager *manager = find_messenger(target);\n        Serializer rez;\n        bool deactivate_task;\n        const AddressSpaceID target_addr = find_address_space(target);\n        {\n          RezCheck z(rez);\n          rez.serialize(target);\n          rez.serialize(task->get_task_kind());\n          deactivate_task = task->pack_task(rez, target_addr);\n        }\n        manager->send_message(rez, TASK_MESSAGE, \n                              TASK_VIRTUAL_CHANNEL, true/*flush*/);\n        if (deactivate_task)\n          task->deactivate();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_tasks(Processor target, const std::set<TaskOp*> &tasks)\n    //--------------------------------------------------------------------------\n    {\n      if (!target.exists())\n        REPORT_LEGION_ERROR(ERROR_INVALID_TARGET_PROC, \n                      \"Mapper requested invalid NO_PROC as target proc!\");\n      // Check to see if the target processor is still local \n      std::map<Processor,ProcessorManager*>::const_iterator finder = \n        proc_managers.find(target);\n      if (finder != proc_managers.end())\n      {\n        // Still local\n        for (std::set<TaskOp*>::const_iterator it = tasks.begin();\n              it != tasks.end(); it++)\n        {\n          // Update the current processor\n          (*it)->set_current_proc(target);\n          finder->second->add_to_ready_queue(*it);\n        }\n      }\n      else\n      {\n        // Otherwise we need to send it remotely\n        MessageManager *manager = find_messenger(target);\n        unsigned idx = 1;\n        const AddressSpaceID target_addr = find_address_space(target);\n        for (std::set<TaskOp*>::const_iterator it = tasks.begin();\n              it != tasks.end(); it++,idx++)\n        {\n          Serializer rez;\n          bool deactivate_task;\n          {\n            RezCheck z(rez);\n            rez.serialize(target);\n            rez.serialize((*it)->get_task_kind());\n            deactivate_task = (*it)->pack_task(rez, target_addr);\n          }\n          // Put it in the queue, flush the last task\n          manager->send_message(rez, TASK_MESSAGE,\n                                TASK_VIRTUAL_CHANNEL, (idx == tasks.size()));\n          // Deactivate the task if it is remote\n          if (deactivate_task)\n            (*it)->deactivate();\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_steal_request(\n              const std::multimap<Processor,MapperID> &targets, Processor thief)\n    //--------------------------------------------------------------------------\n    {\n      for (std::multimap<Processor,MapperID>::const_iterator it = \n            targets.begin(); it != targets.end(); it++)\n      {\n        Processor target = it->first;\n        std::map<Processor,ProcessorManager*>::const_iterator finder = \n          proc_managers.find(target);\n        if (finder == proc_managers.end())\n        {\n          // Need to send remotely\n          MessageManager *manager = find_messenger(target);\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(target);\n            rez.serialize(thief);\n            int num_mappers = targets.count(target);\n            rez.serialize(num_mappers);\n            for ( ; it != targets.upper_bound(target); it++)\n              rez.serialize(it->second);\n          }\n          manager->send_message(rez, STEAL_MESSAGE,\n                                MAPPER_VIRTUAL_CHANNEL, true/*flush*/);\n        }\n        else\n        {\n          // Still local, so notify the processor manager\n          std::vector<MapperID> thieves;\n          for ( ; it != targets.upper_bound(target); it++)\n            thieves.push_back(it->second);\n          finder->second->process_steal_request(thief, thieves);\n          \n        }\n        if (it == targets.end())\n          break;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_advertisements(const std::set<Processor> &targets,\n                                            MapperID map_id, Processor source)\n    //--------------------------------------------------------------------------\n    {\n      std::set<MessageManager*> already_sent;\n      for (std::set<Processor>::const_iterator it = targets.begin();\n            it != targets.end(); it++)\n      {\n        std::map<Processor,ProcessorManager*>::const_iterator finder = \n          proc_managers.find(*it);\n        if (finder != proc_managers.end())\n        {\n          // still local\n          finder->second->process_advertisement(source, map_id);\n        }\n        else\n        {\n          // otherwise remote, check to see if we already sent it\n          MessageManager *messenger = find_messenger(*it);\n          if (already_sent.find(messenger) != already_sent.end())\n            continue;\n          Serializer rez;\n          {\n            RezCheck z(rez);\n            rez.serialize(source);\n            rez.serialize(map_id);\n          }\n          messenger->send_message(rez, ADVERTISEMENT_MESSAGE, \n                                  MAPPER_VIRTUAL_CHANNEL, true/*flush*/);\n          already_sent.insert(messenger);\n        }\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_task_replay(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REMOTE_TASK_REPLAY,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_task_profiling_response(Processor target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_REMOTE_TASK_PROFILING_RESPONSE, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_shared_ownership(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_SHARED_OWNERSHIP,\n          REFERENCE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_node(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Will be flushed by index space return\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_NODE,\n                               INDEX_SPACE_VIRTUAL_CHANNEL, false/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_request(AddressSpaceID target,\n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_REQUEST, \n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_return(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_RETURN,\n            INDEX_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_set(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_SET,\n              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*return*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_child_request(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_CHILD_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_child_response(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_CHILD_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_colors_request(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_COLORS_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_colors_response(AddressSpaceID target,\n                                                   Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,SEND_INDEX_SPACE_COLORS_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_remote_expression_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_INDEX_SPACE_REMOTE_EXPRESSION_REQUEST,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_remote_expression_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_INDEX_SPACE_REMOTE_EXPRESSION_RESPONSE,\n          EXPRESSION_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_remote_expression_invalidation(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_INDEX_SPACE_REMOTE_EXPRESSION_INVALIDATION,\n          EXPRESSION_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_generate_color_request(AddressSpaceID target,\n                                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_INDEX_SPACE_GENERATE_COLOR_REQUEST, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_generate_color_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_INDEX_SPACE_GENERATE_COLOR_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_release_color(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // This has to go on the reference virtual channel so that it is \n      // handled before the owner node is deleted\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_RELEASE_COLOR,\n                                      REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_notification(AddressSpaceID target,\n                                                    Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n                                  SEND_INDEX_PARTITION_NOTIFICATION, \n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_node(AddressSpaceID target, \n                                            Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Will be flushed by the return\n      find_messenger(target)->send_message(rez, SEND_INDEX_PARTITION_NODE,\n                               INDEX_SPACE_VIRTUAL_CHANNEL, false/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_request(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_PARTITION_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_return(AddressSpaceID target,\n                                              Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_PARTITION_RETURN,\n              INDEX_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_child_request(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n                                SEND_INDEX_PARTITION_CHILD_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_child_response(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n                                SEND_INDEX_PARTITION_CHILD_RESPONSE, \n                                DEFAULT_VIRTUAL_CHANNEL, \n                                true/*flush*/, true/*response*/); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_disjoint_update(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // This has to go on the index space virtual channel so that it is\n      // ordered with respect to the index_partition_node messages\n      find_messenger(target)->send_message(rez, \n                                SEND_INDEX_PARTITION_DISJOINT_UPDATE, \n                                INDEX_SPACE_VIRTUAL_CHANNEL,\n                                true/*flush*/, true/*response*/); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_shard_rects_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_INDEX_PARTITION_SHARD_RECTS_REQUEST, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_shard_rects_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_INDEX_PARTITION_SHARD_RECTS_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_remote_interference_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_INDEX_PARTITION_REMOTE_INTERFERENCE_REQUEST,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_remote_interference_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_INDEX_PARTITION_REMOTE_INTERFERENCE_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_node(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Will be flushed by return\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_NODE,\n                               FIELD_SPACE_VIRTUAL_CHANNEL, false/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_request(AddressSpaceID target,\n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_return(AddressSpaceID target,\n                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_RETURN,\n            FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_allocator_request(AddressSpaceID target,\n                                                     Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_FIELD_SPACE_ALLOCATOR_REQUEST,\n          FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_allocator_response(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_FIELD_SPACE_ALLOCATOR_RESPONSE,\n          FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_allocator_invalidation(AddressSpaceID target,\n                                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_FIELD_SPACE_ALLOCATOR_INVALIDATION,\n          FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_allocator_flush(AddressSpaceID target,\n                                                   Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,SEND_FIELD_SPACE_ALLOCATOR_FLUSH,\n          FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_allocator_free(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_ALLOCATOR_FREE,\n                                    FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_infos_request(AddressSpaceID target, \n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_INFOS_REQUEST,\n          FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_infos_response(AddressSpaceID target, \n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_INFOS_RESPONSE,\n          FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_alloc_request(AddressSpaceID target,\n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_ALLOC_REQUEST,\n                              FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_size_update(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // put this on the reference virtual channel since it has no effects\n      // tracking and we need to make sure it is handled before references\n      // are removed from the remote copies\n      find_messenger(target)->send_message(rez, SEND_FIELD_SIZE_UPDATE,\n                REFERENCE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_free(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_FREE,\n                    FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_free_indexes(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_FREE_INDEXES,\n                            FIELD_SPACE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_layout_invalidation(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Send this on the reference virtual channel since it's effects\n      // are not being tracked and we need to know it is handled before\n      // the remote objects have their references removed\n      find_messenger(target)->send_message(rez, \n          SEND_FIELD_SPACE_LAYOUT_INVALIDATION, \n          REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_local_field_alloc_request(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LOCAL_FIELD_ALLOC_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_local_field_alloc_response(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LOCAL_FIELD_ALLOC_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_local_field_free(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LOCAL_FIELD_FREE,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_local_field_update(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LOCAL_FIELD_UPDATE,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_top_level_region_request(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_TOP_LEVEL_REGION_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_top_level_region_return(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_TOP_LEVEL_REGION_RETURN,\n                LOGICAL_TREE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_logical_region_node(AddressSpaceID target, \n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // flushed by return\n      find_messenger(target)->send_message(rez, SEND_LOGICAL_REGION_NODE,\n                                  LOGICAL_TREE_VIRTUAL_CHANNEL, false/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_destruction(IndexSpace handle, \n                                               AddressSpaceID target,\n                                               std::set<RtEvent> &applied)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(handle);\n        const RtUserEvent done = create_rt_user_event();\n        rez.serialize(done);\n        applied.insert(done);\n      }\n      // Put this message on the same virtual channel as the unregister\n      // messages for distributed collectables to make sure that they \n      // are properly ordered\n      find_messenger(target)->send_message(rez, INDEX_SPACE_DESTRUCTION_MESSAGE,\n                                      REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_destruction(IndexPartition handle, \n                                                   AddressSpaceID target,\n                                                   std::set<RtEvent> &applied)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(handle);\n        const RtUserEvent done = create_rt_user_event();\n        rez.serialize(done);\n        applied.insert(done);\n      }\n      // Put this message on the same virtual channel as the unregister\n      // messages for distributed collectables to make sure that they \n      // are properly ordered\n      find_messenger(target)->send_message(rez, \n        INDEX_PARTITION_DESTRUCTION_MESSAGE, REFERENCE_VIRTUAL_CHANNEL,\n                                                             true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_destruction(FieldSpace handle, \n                                               AddressSpaceID target,\n                                               std::set<RtEvent> &applied)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(handle);\n        const RtUserEvent done = create_rt_user_event();\n        rez.serialize(done);\n        applied.insert(done);\n      }\n      // Put this message on the same virtual channel as the unregister\n      // messages for distributed collectables to make sure that they \n      // are properly ordered\n      find_messenger(target)->send_message(rez, \n          FIELD_SPACE_DESTRUCTION_MESSAGE, REFERENCE_VIRTUAL_CHANNEL,\n                                                              true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_logical_region_destruction(LogicalRegion handle, \n                                                  AddressSpaceID target,\n                                                  std::set<RtEvent> &applied)\n    //--------------------------------------------------------------------------\n    {\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(handle);\n        const RtUserEvent done = create_rt_user_event();\n        rez.serialize(done);\n        applied.insert(done);\n      }\n      // Put this message on the same virtual channel as the unregister\n      // messages for distributed collectables to make sure that they \n      // are properly ordered\n      find_messenger(target)->send_message(rez, \n          LOGICAL_REGION_DESTRUCTION_MESSAGE, REFERENCE_VIRTUAL_CHANNEL,\n                                                              true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_individual_remote_complete(Processor target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, INDIVIDUAL_REMOTE_COMPLETE,\n                  TASK_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_individual_remote_commit(Processor target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, INDIVIDUAL_REMOTE_COMMIT,\n                TASK_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_remote_mapped(Processor target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_REMOTE_MAPPED,\n                TASK_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_remote_complete(Processor target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_REMOTE_COMPLETE,\n                TASK_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_remote_commit(Processor target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_REMOTE_COMMIT,\n                TASK_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_find_intra_space_dependence(Processor target,\n                                                         Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_FIND_INTRA_DEP,\n                              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_record_intra_space_dependence(Processor target,\n                                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_RECORD_INTRA_DEP,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_collective_instance_request(Processor target,\n                                                         Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_COLLECTIVE_REQUEST,\n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_slice_collective_instance_response(AddressSpaceID target,\n                                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SLICE_COLLECTIVE_RESPONSE,\n                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_did_remote_registration(AddressSpaceID target, \n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, DISTRIBUTED_REMOTE_REGISTRATION,\n                    REFERENCE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_did_remote_valid_update(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, DISTRIBUTED_VALID_UPDATE,\n                                    REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_did_remote_gc_update(AddressSpaceID target,\n                                            Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, DISTRIBUTED_GC_UPDATE,\n                                    REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_did_add_create_reference(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, DISTRIBUTED_CREATE_ADD,\n                                    REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_did_remove_create_reference(AddressSpaceID target,\n                                                    Serializer &rez, bool flush)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, DISTRIBUTED_CREATE_REMOVE,\n                                           REFERENCE_VIRTUAL_CHANNEL, flush);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_did_remote_unregister(AddressSpaceID target, \n                                         Serializer &rez, VirtualChannelKind vc)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, DISTRIBUTED_UNREGISTER,\n                                           vc, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_created_region_contexts(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_CREATED_REGION_CONTEXTS,\n                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_atomic_reservation_request(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_ATOMIC_RESERVATION_REQUEST,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_atomic_reservation_response(AddressSpaceID target,\n                                                   Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,SEND_ATOMIC_RESERVATION_RESPONSE,\n                         DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_materialized_view(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_MATERIALIZED_VIEW,\n              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_fill_view(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FILL_VIEW,\n                 DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_phi_view(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_PHI_VIEW,\n                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_sharded_view(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_SHARDED_VIEW,\n                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_reduction_view(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REDUCTION_VIEW,\n                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_instance_manager(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INSTANCE_MANAGER,\n                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_manager_update(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_MANAGER_UPDATE,\n                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_collective_instance_manager(AddressSpaceID target, \n                                                   Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_COLLECTIVE_MANAGER,\n                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_collective_instance_message(AddressSpaceID target,\n                                                   Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Put all these messages on the reference virtual channel to keep them\n      // all in order and make sure that we never send any of this messages\n      // once the references on the collective instance are removed\n      find_messenger(target)->send_message(rez, SEND_COLLECTIVE_MESSAGE,\n                                   REFERENCE_VIRTUAL_CHANNEL, true/*flush*/); \n    }\n\n#ifdef LEGION_GPU_REDUCTIONS\n    //--------------------------------------------------------------------------\n    void Runtime::send_create_shadow_reduction_request(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_CREATE_SHADOW_REQUEST,\n                                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_create_shadow_reduction_response(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_CREATE_SHADOW_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n#endif // LEGION_GPU_REDUCTIONS\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_create_top_view_request(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_CREATE_TOP_VIEW_REQUEST,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_create_top_view_response(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_CREATE_TOP_VIEW_RESPONSE,\n                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_view_register_user(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VIEW_REGISTER_USER,\n                                         UPDATE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_view_find_copy_preconditions_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VIEW_FIND_COPY_PRE_REQUEST,\n                                         UPDATE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_view_find_copy_preconditions_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,SEND_VIEW_FIND_COPY_PRE_RESPONSE,\n                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::send_view_add_copy_user(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VIEW_ADD_COPY_USER,\n                                         UPDATE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n#ifdef ENABLE_VIEW_REPLICATION\n    //--------------------------------------------------------------------------\n    void Runtime::send_view_replication_request(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VIEW_REPLICATION_REQUEST,\n                                       UPDATE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_view_replication_response(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VIEW_REPLICATION_RESPONSE,\n                       UPDATE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_view_replication_removal(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VIEW_REPLICATION_REMOVAL,\n                                       UPDATE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_future_result(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FUTURE_RESULT,\n            DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_future_subscription(AddressSpaceID target,\n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Since this message is fused with doing the remote registration for\n      // the future it also needs to go on the same virtual channel as \n      // send_did_remote_registration which is the REFERENCE_VIRTUAL_CHANNEL \n      find_messenger(target)->send_message(rez, SEND_FUTURE_SUBSCRIPTION,\n                                REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_future_notification(AddressSpaceID target,\n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // This also has to happen on the reference virtual channel to prevent\n      // the owner from being deleted before its references are removed\n      find_messenger(target)->send_message(rez, SEND_FUTURE_NOTIFICATION,\n              REFERENCE_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_future_broadcast(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // We need all these to be ordered, preferably with respect to \n      // reference removals too so put them on the reference virtual channel\n      find_messenger(target)->send_message(rez, SEND_FUTURE_BROADCAST,\n                            REFERENCE_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_future_map_request_future(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FUTURE_MAP_REQUEST,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_future_map_response_future(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FUTURE_MAP_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_future_map_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_FUTURE_MAP_REQUEST,\n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_future_map_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_FUTURE_MAP_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_top_view_request(AddressSpaceID target,\n                                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_TOP_VIEW_REQUEST,\n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_top_view_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_TOP_VIEW_RESPONSE,\n                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_disjoint_complete_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_REPL_DISJOINT_COMPLETE_REQUEST,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_disjoint_complete_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_REPL_DISJOINT_COMPLETE_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_intra_space_dependence(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_INTRA_SPACE_DEP,\n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_broadcast_update(AddressSpaceID target,\n                                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_BROADCAST_UPDATE,\n                                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_trace_event_request(\n                                         AddressSpaceID target, Serializer &rez) \n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_TRACE_EVENT_REQUEST,\n                                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_trace_event_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_TRACE_EVENT_RESPONSE,\n                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_trace_update(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_TRACE_UPDATE,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_implicit_request(AddressSpaceID target,\n                                                          Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPL_IMPLICIT_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_implicit_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // This has to go on the task virtual channel so that it is ordered\n      // with respect to any distributions\n      // See Runtime::send_replicate_launch\n      find_messenger(target)->send_message(rez, SEND_REPL_IMPLICIT_RESPONSE,\n                                        TASK_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_mapper_message(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_MAPPER_MESSAGE,\n                                        MAPPER_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_mapper_broadcast(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_MAPPER_BROADCAST,\n                                         MAPPER_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_task_impl_semantic_request(AddressSpaceID target,\n                                                   Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_TASK_IMPL_SEMANTIC_REQ,\n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_semantic_request(AddressSpaceID target,\n                                                    Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_SEMANTIC_REQ,\n                                 DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_semantic_request(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_INDEX_PARTITION_SEMANTIC_REQ, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_semantic_request(AddressSpaceID target,\n                                                    Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_SEMANTIC_REQ,\n                                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_semantic_request(AddressSpaceID target,\n                                              Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SEMANTIC_REQ,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_logical_region_semantic_request(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n              SEND_LOGICAL_REGION_SEMANTIC_REQ, \n              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_logical_partition_semantic_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n            SEND_LOGICAL_PARTITION_SEMANTIC_REQ, \n            DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_task_impl_semantic_info(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_TASK_IMPL_SEMANTIC_INFO,\n              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_space_semantic_info(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INDEX_SPACE_SEMANTIC_INFO,\n               DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_index_partition_semantic_info(AddressSpaceID target,\n                                                     Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_INDEX_PARTITION_SEMANTIC_INFO, DEFAULT_VIRTUAL_CHANNEL,\n                                             true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_space_semantic_info(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SPACE_SEMANTIC_INFO,\n                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_field_semantic_info(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_FIELD_SEMANTIC_INFO,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_logical_region_semantic_info(AddressSpaceID target,\n                                                    Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n              SEND_LOGICAL_REGION_SEMANTIC_INFO, DEFAULT_VIRTUAL_CHANNEL,\n                                              true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_logical_partition_semantic_info(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n            SEND_LOGICAL_PARTITION_SEMANTIC_INFO, \n            DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_context_request(AddressSpaceID target,\n                                              Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REMOTE_CONTEXT_REQUEST, \n                                        CONTEXT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_context_response(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REMOTE_CONTEXT_RESPONSE, \n                    CONTEXT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_context_free(AddressSpaceID target, \n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REMOTE_CONTEXT_FREE,\n                                        CONTEXT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_context_physical_request(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_REMOTE_CONTEXT_PHYSICAL_REQUEST, \n          CONTEXT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_context_physical_response(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_REMOTE_CONTEXT_PHYSICAL_RESPONSE,\n          CONTEXT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_compute_equivalence_sets_request(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_COMPUTE_EQUIVALENCE_SETS_REQUEST,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_compute_equivalence_sets_response(AddressSpaceID target,\n                                                         Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_COMPUTE_EQUIVALENCE_SETS_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_response(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_EQUIVALENCE_SET_RESPONSE,\n                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_invalidate_trackers(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    { \n      find_messenger(target)->send_message(rez, \n          SEND_EQUIVALENCE_SET_INVALIDATE_TRACKERS, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_replication_request(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_EQUIVALENCE_SET_REPLICATION_REQUEST, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_replication_response(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_EQUIVALENCE_SET_REPLICATION_RESPONSE, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_replication_update(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_EQUIVALENCE_SET_REPLICATION_UPDATE, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_migration(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_EQUIVALENCE_SET_MIGRATION,\n          MIGRATION_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_owner_update(AddressSpaceID target,\n                                                    Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_EQUIVALENCE_SET_OWNER_UPDATE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_make_owner(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_MAKE_OWNER, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_clone_request(AddressSpaceID target,\n                                                     Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_CLONE_REQUEST,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_clone_response(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_CLONE_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_capture_request(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_CAPTURE_REQUEST,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_capture_response(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_CAPTURE_RESPONSE,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_request_instances(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_REQUEST_INSTANCES, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_request_invalid(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_REQUEST_INVALID,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_request_antivalid(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_REQUEST_ANTIVALID,\n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_updates(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_UPDATES, \n          THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_acquires(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_ACQUIRES, \n          THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_releases(AddressSpaceID target,\n                                                       Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_RELEASES, \n          THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_copies_across(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_COPIES_ACROSS, \n          THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_overwrites(AddressSpaceID target,\n                                                         Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_OVERWRITES, \n          THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_filters(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_FILTERS, \n          THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_equivalence_set_remote_instances(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,\n          SEND_EQUIVALENCE_SET_REMOTE_INSTANCES, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*return*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_instance_request(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INSTANCE_REQUEST,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_instance_response(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_INSTANCE_RESPONSE,\n              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_external_create_request(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_EXTERNAL_CREATE_REQUEST,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_external_create_response(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_EXTERNAL_CREATE_RESPONSE,\n                    DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_external_attach(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_EXTERNAL_ATTACH,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_external_detach(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_EXTERNAL_DETACH,\n                                DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_gc_priority_update(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_GC_PRIORITY_UPDATE,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_never_gc_response(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_NEVER_GC_RESPONSE,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_acquire_request(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_ACQUIRE_REQUEST,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::send_acquire_response(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_ACQUIRE_RESPONSE,\n              DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_variant_broadcast(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_VARIANT_BROADCAST,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_constraint_request(AddressSpaceID target, \n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // This is paging in constraints so it needs its own virtual channel\n      find_messenger(target)->send_message(rez, SEND_CONSTRAINT_REQUEST,\n                              LAYOUT_CONSTRAINT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_constraint_response(AddressSpaceID target, \n                                            Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // This is paging in constraints so it needs its own virtual channel\n      find_messenger(target)->send_message(rez, SEND_CONSTRAINT_RESPONSE,\n        LAYOUT_CONSTRAINT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_constraint_release(AddressSpaceID target,\n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_CONSTRAINT_RELEASE,\n                        LAYOUT_CONSTRAINT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_mpi_rank_exchange(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_MPI_RANK_EXCHANGE,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_replicate_launch(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // Put this on the task virtual channel so it can be ordered with\n      // respect to requests for shard managers in implicit cases. \n      // See ImplicitShardManager::create_shard_manager\n      // See Runtime::send_control_replicate_implicit_response\n      find_messenger(target)->send_message(rez, SEND_REPLICATE_LAUNCH,\n                                           TASK_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_replicate_delete(AddressSpaceID target,Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPLICATE_DELETE,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_replicate_post_mapped(AddressSpaceID target, \n                                             Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPLICATE_POST_MAPPED,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_replicate_post_execution(AddressSpaceID target, \n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPLICATE_POST_EXECUTION,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_replicate_trigger_complete(AddressSpaceID target,\n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPLICATE_TRIGGER_COMPLETE,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_replicate_trigger_commit(AddressSpaceID target,\n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REPLICATE_TRIGGER_COMMIT,\n                                        DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_control_replicate_collective_message(\n                                         AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_CONTROL_REPLICATE_COLLECTIVE_MESSAGE, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_mapper_request(AddressSpaceID target, \n                                              Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_MAPPER_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_mapper_response(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_MAPPER_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_trace_request(AddressSpaceID target, \n                                             Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_TRACE_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_trace_response(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_TRACE_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_projection_request(AddressSpaceID target, \n                                                  Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_PROJECTION_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_projection_response(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez,SEND_LIBRARY_PROJECTION_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_sharding_request(AddressSpaceID target, \n                                                Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_SHARDING_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_sharding_response(AddressSpaceID target,\n                                                 Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_SHARDING_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_task_request(AddressSpaceID target, \n                                            Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_TASK_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_task_response(AddressSpaceID target,\n                                             Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_TASK_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_redop_request(AddressSpaceID target, \n                                             Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_REDOP_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_redop_response(AddressSpaceID target,\n                                              Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_REDOP_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_serdez_request(AddressSpaceID target, \n                                              Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_SERDEZ_REQUEST,\n                                     DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_library_serdez_response(AddressSpaceID target,\n                                               Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_LIBRARY_SERDEZ_RESPONSE,\n                   DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    } \n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_op_report_uninitialized(AddressSpaceID target,\n                                                      Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_REMOTE_OP_REPORT_UNINIT,\n                                      DEFAULT_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_op_profiling_count_update(AddressSpaceID target,\n                                                        Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, \n          SEND_REMOTE_OP_PROFILING_COUNT_UPDATE, \n          DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_trace_update(AddressSpaceID target, \n                                           Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // All these messages must be on the same ordered virtual channel\n      // so that they are ordered in their program order and handled on\n      // the target node in this order as they would have been if they\n      // were being handled directly on the owner node\n      find_messenger(target)->send_message(rez, SEND_REMOTE_TRACE_UPDATE,\n                                  TRACING_VIRTUAL_CHANNEL, true/*flush*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_remote_trace_response(AddressSpaceID target, \n                                             Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      // No need for responses to be ordered so they can be handled on\n      // the default virtual channel in whatever order\n      find_messenger(target)->send_message(rez, SEND_REMOTE_TRACE_RESPONSE,\n                  DEFAULT_VIRTUAL_CHANNEL, true/*flush*/, true/*response*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_shutdown_notification(AddressSpaceID target, \n                                             Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_SHUTDOWN_NOTIFICATION,\n                                    THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/, \n                                    false/*response*/, true/*shutdown*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::send_shutdown_response(AddressSpaceID target, Serializer &rez)\n    //--------------------------------------------------------------------------\n    {\n      find_messenger(target)->send_message(rez, SEND_SHUTDOWN_RESPONSE,\n                                THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/,\n                                false/*response*/, true/*shutdown*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_task(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      TaskOp::process_unpack_task(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_steal(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Processor target;\n      derez.deserialize(target);\n      Processor thief;\n      derez.deserialize(thief);\n      int num_mappers;\n      derez.deserialize(num_mappers);\n      std::vector<MapperID> thieves(num_mappers);\n      for (int idx = 0; idx < num_mappers; idx++)\n        derez.deserialize(thieves[idx]);\n#ifdef DEBUG_LEGION\n      assert(proc_managers.find(target) != proc_managers.end());\n#endif\n      proc_managers[target]->process_steal_request(thief, thieves);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_advertisement(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Processor source;\n      derez.deserialize(source);\n      MapperID map_id;\n      derez.deserialize(map_id);\n      // Just advertise it to all the managers\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        it->second->process_advertisement(source, map_id);\n      }\n    }\n\n#ifdef LEGION_USE_LIBDL\n    //--------------------------------------------------------------------------\n    void Runtime::handle_registration_callback(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(implicit_context == NULL);\n      assert(implicit_runtime != NULL);\n#endif\n      DerezCheck z(derez);\n      size_t dso_size;\n      derez.deserialize(dso_size);\n      const std::string dso_name((const char*)derez.get_current_pointer());\n      derez.advance_pointer(dso_size);\n      size_t sym_size;\n      derez.deserialize(sym_size);\n      const std::string sym_name((const char*)derez.get_current_pointer());\n      derez.advance_pointer(sym_size);\n      RtEvent global_done_event;\n      derez.deserialize(global_done_event);\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n\n      // Converting the DSO reference could call dlopen and might block\n      // us if the constructor for that shared object requests its own\n      // global registration callback, so register our guards first\n      const std::pair<std::string,std::string> key(dso_name, sym_name);\n      {\n        AutoLock c_lock(callback_lock);\n        // First see if the local case has already been done in which case\n        // we know that we are done also when it is done\n        std::map<std::pair<std::string,std::string>,RtEvent>::const_iterator\n          finder = global_local_done.find(key);\n        if (finder != global_local_done.end())\n        {\n          Runtime::trigger_event(done_event, finder->second);\n          return;\n        }\n        // No one has attempted a global registration callback here yet\n        // Record that we are pending and put in a guard for all the\n        // of the global registrations being done\n        if (global_callbacks_done.find(key) == global_callbacks_done.end())\n          global_callbacks_done[key] = global_done_event;\n        pending_remote_callbacks[key].insert(done_event);\n      }\n\n      // Now we can do the translation of ourselves to get the function pointer\n      Realm::DSOReferenceImplementation dso(dso_name, sym_name);\n#ifdef DEBUG_LEGION\n      assert(callback_translator.can_translate(\n            typeid(Realm::DSOReferenceImplementation),\n            typeid(Realm::FunctionPointerImplementation)));\n#endif\n      Realm::FunctionPointerImplementation *impl = \n        static_cast<Realm::FunctionPointerImplementation*>(\n            callback_translator.translate(&dso, \n              typeid(Realm::FunctionPointerImplementation)));\n#ifdef DEBUG_LEGION\n      assert(impl != NULL);\n#endif\n      RegistrationCallbackFnptr callback = \n        impl->get_impl<RegistrationCallbackFnptr>();\n      RtEvent precondition;\n      // Now take the lock and see if we need to perform anything\n      {\n        AutoLock c_lock(callback_lock);\n        std::map<std::pair<std::string,std::string>,\n          std::set<RtUserEvent> >::iterator finder = \n            pending_remote_callbacks.find(key);\n        // If someone already handled everything then we are done\n        if (finder != pending_remote_callbacks.end())\n        {\n          // We should still be in there\n#ifdef DEBUG_LEGION\n          assert(finder->second.find(done_event) != finder->second.end());\n#endif\n          finder->second.erase(done_event);\n          if (finder->second.empty())\n            pending_remote_callbacks.erase(finder);\n          // Now see if anyone else has done the local registration\n          std::map<RegistrationCallbackFnptr,RtEvent>::const_iterator\n            finder = local_callbacks_done.find(callback);\n          if (finder != local_callbacks_done.end())\n          {\n#ifdef DEBUG_LEGION\n            assert(finder->second.exists());\n#endif\n            precondition = finder->second;\n          }\n          else\n          {\n            local_callbacks_done[callback] = done_event;\n            global_local_done[key] = done_event; \n          }\n        }\n        else // We were already handled so nothing to do\n          done_event = RtUserEvent::NO_RT_USER_EVENT;\n      }\n      if (done_event.exists())\n      {\n        // This is the signal that we need to do the callback\n        if (!precondition.exists())\n        {\n          inside_registration_callback = GLOBAL_REGISTRATION_CALLBACK;\n          (*callback)(machine, external, local_procs);\n          inside_registration_callback = NO_REGISTRATION_CALLBACK;\n        }\n        Runtime::trigger_event(done_event, precondition);\n      }\n      // Delete our resources that we allocated\n      delete impl;\n    }\n#endif // LEGION_USE_LIBDL\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_task_replay(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      TaskOp::process_remote_replay(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_task_profiling_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      SingleTask::process_remote_profiling_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_shared_ownership(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      int kind;\n      derez.deserialize(kind);\n      switch (kind)\n      {\n        case 0:\n          {\n            IndexSpace handle;\n            derez.deserialize(handle);\n            create_shared_ownership(handle);\n            break;\n          }\n        case 1:\n          {\n            IndexPartition handle;\n            derez.deserialize(handle);\n            create_shared_ownership(handle);\n            break;\n          }\n        case 2:\n          {\n            FieldSpace handle;\n            derez.deserialize(handle);\n            create_shared_ownership(handle);\n            break;\n          }\n        case 3:\n          {\n            LogicalRegion handle;\n            derez.deserialize(handle);\n            create_shared_ownership(handle);\n            break;\n          }\n        default:\n          assert(false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_node(Deserializer &derez, \n                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_node_creation(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_request(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_node_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_return(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_node_return(derez); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_set(Deserializer &derez, \n                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_index_space_set(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_child_request(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_node_child_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_child_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_node_child_response(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_colors_request(Deserializer &derez,\n                                                    AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_colors_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_colors_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_colors_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_remote_expression_request(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      forest->handle_remote_expression_request(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_remote_expression_response(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      forest->handle_remote_expression_response(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_remote_expression_invalidation(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      forest->handle_remote_expression_invalidation(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_generate_color_request(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_generate_color_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_generate_color_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_generate_color_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_release_color(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_release_color(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_notification(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_notification(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_node(Deserializer &derez,\n                                              AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_node_creation(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_request(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_node_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_return(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_node_return(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_child_request(Deserializer &derez,\n                                                       AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_node_child_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_child_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_node_child_response(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_disjoint_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_node_disjoint_update(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_shard_rects_request(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_shard_rects_request(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_shard_rects_response(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_shard_rects_response(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_remote_interference_request(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_remote_interference_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_remote_interference_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_remote_interference_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_node(Deserializer &derez, \n                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_node_creation(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_request(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_node_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_return(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_node_return(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_allocator_request(Deserializer &derez,\n                                                       AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_allocator_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_allocator_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_allocator_response(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_allocator_invalidation(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_allocator_invalidation(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_allocator_flush(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_allocator_flush(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_allocator_free(Deserializer &derez,\n                                                    AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_allocator_free(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_infos_request(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_infos_request(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_infos_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_infos_response(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_alloc_request(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_alloc_request(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_size_update(Deserializer &derez,\n                                           AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_field_size_update(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_free(Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_field_free(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_free_indexes(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_field_free_indexes(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_layout_invalidation(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_layout_invalidation(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_local_field_alloc_request(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_local_alloc_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_local_field_alloc_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_local_alloc_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_local_field_free(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_local_free(forest, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_local_field_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      RemoteContext::handle_local_field_update(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_top_level_region_request(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RegionNode::handle_top_level_request(forest, derez, source); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_top_level_region_return(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      RegionNode::handle_top_level_return(derez);   \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_logical_region_node(Deserializer &derez, \n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RegionNode::handle_node_creation(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_destruction(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      IndexSpace handle;\n      derez.deserialize(handle);\n      RtUserEvent done;\n      derez.deserialize(done);\n#ifdef DEBUG_LEGION\n      assert(done.exists());\n#endif\n      std::set<RtEvent> applied;\n      forest->destroy_index_space(handle, applied);\n      if (!applied.empty())\n        Runtime::trigger_event(done, Runtime::merge_events(applied));\n      else\n        Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_destruction(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      IndexPartition handle;\n      derez.deserialize(handle);\n      RtUserEvent done;\n      derez.deserialize(done);\n#ifdef DEBUG_LEGION\n      assert(done.exists());\n#endif\n      std::set<RtEvent> applied;\n      forest->destroy_index_partition(handle, applied);\n      if (!applied.empty())\n        Runtime::trigger_event(done, Runtime::merge_events(applied));\n      else\n        Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_destruction(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      FieldSpace handle;\n      derez.deserialize(handle);\n      RtUserEvent done;\n      derez.deserialize(done);\n#ifdef DEBUG_LEGION\n      assert(done.exists());\n#endif\n      std::set<RtEvent> applied;\n      forest->destroy_field_space(handle, applied);\n      if (!applied.empty())\n        Runtime::trigger_event(done, Runtime::merge_events(applied));\n      else\n        Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_logical_region_destruction(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      LogicalRegion handle;\n      derez.deserialize(handle);\n      RtUserEvent done;\n      derez.deserialize(done);\n      std::set<RtEvent> applied;\n      forest->destroy_logical_region(handle, applied);\n      if (done.exists())\n      {\n        if (!applied.empty())\n          Runtime::trigger_event(done, Runtime::merge_events(applied));\n        else\n          Runtime::trigger_event(done);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_individual_remote_complete(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndividualTask::process_unpack_remote_complete(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_individual_remote_commit(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndividualTask::process_unpack_remote_commit(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_remote_mapped(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexTask::process_slice_mapped(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_remote_complete(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexTask::process_slice_complete(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_remote_commit(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexTask::process_slice_commit(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_find_intra_dependence(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexTask::process_slice_find_intra_dependence(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_record_intra_dependence(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndexTask::process_slice_record_intra_dependence(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_collective_request(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      SliceTask::handle_collective_instance_request(derez, source, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_slice_collective_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      SliceTask::handle_collective_instance_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_did_remote_registration(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable::handle_did_remote_registration(this,derez,source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_did_remote_valid_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable::handle_did_remote_valid_update(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_did_remote_gc_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable::handle_did_remote_gc_update(this, derez); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_did_create_add(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable::handle_did_add_create(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_did_create_remove(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable::handle_did_remove_create(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_did_remote_unregister(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable::handle_unregister_collectable(this, derez);\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::handle_created_region_contexts(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RemoteContext::handle_created_region_contexts(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_atomic_reservation_request(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_send_atomic_reservation_request(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_atomic_reservation_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_send_atomic_reservation_response(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_materialized_view(Deserializer &derez, \n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      MaterializedView::handle_send_materialized_view(this, derez, source); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_fill_view(Deserializer &derez, \n                                        AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FillView::handle_send_fill_view(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_phi_view(Deserializer &derez,\n                                       AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      PhiView::handle_send_phi_view(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_sharded_view(Deserializer &derez,\n                                           AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShardedView::handle_send_sharded_view(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_reduction_view(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ReductionView::handle_send_reduction_view(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_instance_manager(Deserializer &derez,\n                                               AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndividualManager::handle_send_manager(this, source, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_send_manager_update(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndividualManager::handle_send_manager_update(this, source, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_collective_instance_manager(Deserializer &derez,\n                                                     AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      CollectiveManager::handle_send_manager(this, source, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_collective_instance_message(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      CollectiveManager::handle_collective_message(derez, this);\n    }\n\n#ifdef LEGION_GPU_REDUCTIONS\n    //--------------------------------------------------------------------------\n    void Runtime::handle_create_shadow_reduction_request(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndividualManager::handle_create_shadow_request(this, source, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_create_shadow_reduction_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      IndividualManager::handle_create_shadow_response(this, derez);\n    }\n#endif // LEGION_GPU_REDUCTIONS\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_create_top_view_request(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InnerContext::handle_create_top_view_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_create_top_view_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      InnerContext::handle_create_top_view_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_request(Deserializer &derez, \n                                      AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      LogicalView::handle_view_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_manager_request(Deserializer &derez, \n                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      PhysicalManager::handle_manager_request(derez, this, source);\n    }\n\n#ifdef ENABLE_VIEW_REPLICATION\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_replication_request(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_replication_request(derez, this, source);\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_replication_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_replication_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_replication_removal(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_replication_removal(derez, this, source);\n    }\n#endif // ENABLE_VIEW_REPLICATION\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_future_result(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FutureImpl::handle_future_result(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_future_subscription(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FutureImpl::handle_future_subscription(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_future_map_future_request(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FutureMapImpl::handle_future_map_future_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_future_map_future_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FutureMapImpl::handle_future_map_future_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_future_map_request(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_future_map_request(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_future_map_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ReplFutureMapImpl::handle_future_map_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_future_notification(Deserializer &derez, \n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FutureImpl::handle_future_notification(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_future_broadcast(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FutureImpl::handle_future_broadcast(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_top_view_request(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_top_view_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_top_view_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_top_view_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_disjoint_complete_request(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_disjoint_complete_request(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_disjoint_complete_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ReplicateContext::handle_disjoint_complete_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_intra_space_dependence(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_intra_space_dependence(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_broadcast_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_broadcast_update(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_trace_event_request(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_trace_event_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_trace_event_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_trace_event_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_trace_update(Deserializer &derez,\n                                                        AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_trace_update(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_implicit_request(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ImplicitShardManager::handle_remote_request(derez, this, source);  \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_implicit_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ImplicitShardManager::handle_remote_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_register_user(Deserializer &derez,\n                                            AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_register_user(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_copy_pre_request(Deserializer &derez,\n                                               AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_find_copy_pre_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_copy_pre_response(Deserializer &derez,\n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_find_copy_pre_response(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_view_add_copy_user(Deserializer &derez,\n                                            AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InstanceView::handle_view_add_copy_user(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_mapper_message(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Processor target;\n      derez.deserialize(target);\n      MapperID map_id;\n      derez.deserialize(map_id);\n      Processor source;\n      derez.deserialize(source);\n      unsigned message_kind;\n      derez.deserialize(message_kind);\n      size_t message_size;\n      derez.deserialize(message_size);\n      const void *message = derez.get_current_pointer();\n      derez.advance_pointer(message_size);\n      process_mapper_message(target, map_id, source, message, \n                             message_size, message_kind);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_mapper_broadcast(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      MapperID map_id;\n      derez.deserialize(map_id);\n      Processor source;\n      derez.deserialize(source);\n      unsigned message_kind;\n      derez.deserialize(message_kind);\n      int radix;\n      derez.deserialize(radix);\n      int index;\n      derez.deserialize(index);\n      size_t message_size;\n      derez.deserialize(message_size);\n      const void *message = derez.get_current_pointer();\n      derez.advance_pointer(message_size);\n      process_mapper_broadcast(map_id, source, message, \n                               message_size, message_kind, radix, index);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_task_impl_semantic_request(Deserializer &derez,\n                                                     AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      TaskImpl::handle_semantic_request(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_semantic_request(Deserializer &derez,\n                                                      AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_semantic_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_semantic_request(Deserializer &derez, \n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_semantic_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_semantic_request(Deserializer &derez,\n                                                      AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_semantic_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_semantic_request(Deserializer &derez,\n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_field_semantic_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_logical_region_semantic_request(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RegionNode::handle_semantic_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_logical_partition_semantic_request(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      PartitionNode::handle_semantic_request(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_task_impl_semantic_info(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      TaskImpl::handle_semantic_info(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_space_semantic_info(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceNode::handle_semantic_info(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_index_partition_semantic_info(Deserializer &derez, \n                                                       AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartNode::handle_semantic_info(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_space_semantic_info(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_semantic_info(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_field_semantic_info(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_field_semantic_info(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_logical_region_semantic_info(Deserializer &derez,\n                                                      AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RegionNode::handle_semantic_info(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_logical_partition_semantic_info(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      PartitionNode::handle_semantic_info(forest, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_context_request(Deserializer &derez,\n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      UniqueID context_uid;\n      derez.deserialize(context_uid);\n      RemoteContext *target;\n      derez.deserialize(target);\n      InnerContext *context = find_context(context_uid);\n      context->send_remote_context(source, target);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_context_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      RemoteContext *context;\n      derez.deserialize(context);\n      // Unpack the result\n      std::set<RtEvent> preconditions;\n      context->unpack_remote_context(derez, preconditions);\n      // Then register it\n      UniqueID context_uid = context->get_context_uid();\n      register_remote_context(context_uid, context, preconditions);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_context_free(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      UniqueID remote_owner_uid;\n      derez.deserialize(remote_owner_uid);\n      unregister_remote_context(remote_owner_uid);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_context_physical_request(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      RemoteContext::handle_physical_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_context_physical_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      RemoteContext::handle_physical_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_compute_equivalence_sets_request(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InnerContext::handle_compute_equivalence_sets_request(derez, this,source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_compute_equivalence_sets_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      VersionManager::handle_compute_equivalence_sets_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_request(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_equivalence_set_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_response(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_equivalence_set_response(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_invalidate_trackers(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_invalidate_trackers(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_replication_request(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_replication_request(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_replication_response(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_replication_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_replication_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_replication_update(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_migration(Deserializer &derez,\n                                                   AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_migration(derez, this, source); \n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_owner_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_owner_update(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_make_owner(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_make_owner(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_clone_request(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_clone_request(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_clone_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_clone_response(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_capture_request(Deserializer &derez,\n                                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_capture_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_capture_response(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      EquivalenceSet::handle_capture_response(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_request_instances(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ValidInstAnalysis::handle_remote_request_instances(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_request_invalid(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      InvalidInstAnalysis::handle_remote_request_invalid(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_request_antivalid(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      AntivalidInstAnalysis::handle_remote_request_antivalid(derez,this,source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_updates(Deserializer &derez,\n                                                        AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      UpdateAnalysis::handle_remote_updates(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_acquires(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      AcquireAnalysis::handle_remote_acquires(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_releases(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ReleaseAnalysis::handle_remote_releases(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_copies_across(\n                                     Deserializer &derez, AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      CopyAcrossAnalysis::handle_remote_copies_across(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_overwrites(Deserializer &derez,\n                                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      OverwriteAnalysis::handle_remote_overwrites(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_filters(Deserializer &derez,\n                                                        AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FilterAnalysis::handle_remote_filters(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_equivalence_set_remote_instances(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      PhysicalAnalysis::handle_remote_instances(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_instance_request(Deserializer &derez, \n                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      MemoryManager *manager = find_memory_manager(target_memory);\n      manager->process_instance_request(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_instance_response(Deserializer &derez,\n                                           AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      MemoryManager *manager = find_memory_manager(target_memory);\n      manager->process_instance_response(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_external_create_request(Deserializer &derez,\n                                                 AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_external_create_request(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_external_create_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceNode::handle_external_create_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_external_attach(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      DistributedID did;\n      derez.deserialize(did);\n      RtEvent manager_ready;\n      PhysicalManager *manager = \n        find_or_request_instance_manager(did, manager_ready);\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n      MemoryManager *memory_manager = find_memory_manager(target_memory);\n      if (manager_ready.exists() && !manager_ready.has_triggered())\n        manager_ready.wait();\n      RtEvent local_done = memory_manager->attach_external_instance(manager);\n      Runtime::trigger_event(done_event, local_done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_external_detach(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      DistributedID did;\n      derez.deserialize(did);\n      RtEvent manager_ready;\n      PhysicalManager *manager = \n        find_or_request_instance_manager(did, manager_ready);\n      RtUserEvent done_event;\n      derez.deserialize(done_event);\n      MemoryManager *memory_manager = find_memory_manager(target_memory);\n      if (manager_ready.exists() && !manager_ready.has_triggered())\n        manager_ready.wait();\n      RtEvent local_done = memory_manager->detach_external_instance(manager);\n      Runtime::trigger_event(done_event, local_done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_gc_priority_update(Deserializer &derez,\n                                            AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      MemoryManager *manager = find_memory_manager(target_memory);\n      manager->process_gc_priority_update(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_never_gc_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      MemoryManager *manager = find_memory_manager(target_memory);\n      manager->process_never_gc_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_acquire_request(Deserializer &derez, \n                                         AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      MemoryManager *manager = find_memory_manager(target_memory);\n      manager->process_acquire_request(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_acquire_response(Deserializer &derez, \n                                          AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      Memory target_memory;\n      derez.deserialize(target_memory);\n      MemoryManager *manager = find_memory_manager(target_memory);\n      manager->process_acquire_response(derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_variant_broadcast(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      VariantImpl::handle_variant_broadcast(this, derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_constraint_request(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints::process_request(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_constraint_response(Deserializer &derez,\n                                             AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints::process_response(this, derez, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_constraint_release(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      LayoutConstraintID layout_id;\n      derez.deserialize(layout_id);\n      release_layout(layout_id);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_top_level_task_request(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(address_space == 0); // should only happen on node 0\n#endif\n      RtUserEvent to_trigger;\n      derez.deserialize(to_trigger);\n      increment_outstanding_top_level_tasks();\n      Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_top_level_task_complete(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(address_space == 0); // should only happen on node 0\n#endif\n      decrement_outstanding_top_level_tasks();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_mpi_rank_exchange(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(Runtime::mpi_rank_table != NULL);\n#endif\n      Runtime::mpi_rank_table->handle_mpi_rank_exchange(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_replicate_launch(Deserializer &derez, \n                                            AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_launch(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_replicate_delete(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_delete(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_replicate_post_mapped(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_post_mapped(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_replicate_post_execution(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_post_execution(derez, this);\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::handle_replicate_trigger_complete(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_trigger_complete(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_replicate_trigger_commit(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_trigger_commit(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_control_replicate_collective_message(\n                                                            Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      ShardManager::handle_collective_message(derez, this);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_mapper_request(Deserializer &derez,\n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      MapperID result = generate_library_mapper_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_mapper_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_mapper_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      MapperID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibraryMapperIDs>::iterator finder = \n          library_mapper_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_mapper_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_trace_request(Deserializer &derez,\n                                               AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      TraceID result = generate_library_trace_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_trace_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_trace_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      TraceID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibraryTraceIDs>::iterator finder = \n          library_trace_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_trace_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_projection_request(Deserializer &derez,\n                                                    AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      ProjectionID result = generate_library_projection_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_projection_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_projection_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      ProjectionID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibraryProjectionIDs>::iterator finder = \n          library_projection_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_projection_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_sharding_request(Deserializer &derez,\n                                                  AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      ShardingID result = generate_library_sharding_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_sharding_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_sharding_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      ShardingID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibraryShardingIDs>::iterator finder = \n          library_sharding_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_sharding_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_task_request(Deserializer &derez,\n                                              AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      TaskID result = generate_library_task_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_task_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_task_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      TaskID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibraryTaskIDs>::iterator finder = \n          library_task_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_task_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_redop_request(Deserializer &derez,\n                                               AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      ReductionOpID result = generate_library_reduction_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_redop_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_redop_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      ReductionOpID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibraryRedopIDs>::iterator finder = \n          library_redop_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_redop_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_serdez_request(Deserializer &derez,\n                                                AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      size_t count;\n      derez.deserialize(count);\n      RtUserEvent done;\n      derez.deserialize(done);\n      \n      CustomSerdezID result = generate_library_serdez_ids(name, count);\n      Serializer rez;\n      {\n        RezCheck z2(rez);\n        rez.serialize(string_length);\n        rez.serialize(name, string_length);\n        rez.serialize(result);\n        rez.serialize(done);\n      }\n      send_library_serdez_response(source, rez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_library_serdez_response(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      DerezCheck z(derez);\n      size_t string_length;\n      derez.deserialize(string_length);\n      const char *name = (const char*)derez.get_current_pointer();\n      derez.advance_pointer(string_length);\n      CustomSerdezID result;\n      derez.deserialize(result);\n      RtUserEvent done;\n      derez.deserialize(done);\n\n      const std::string library_name(name);\n      {\n        AutoLock l_lock(library_lock); \n        std::map<std::string,LibrarySerdezIDs>::iterator finder = \n          library_serdez_ids.find(library_name);\n#ifdef DEBUG_LEGION\n        assert(finder != library_serdez_ids.end());\n        assert(!finder->second.result_set);\n        assert(finder->second.ready == done);\n#endif\n        finder->second.result = result;\n        finder->second.result_set = true;\n      }\n      Runtime::trigger_event(done);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_op_report_uninitialized(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      RemoteOp::handle_report_uninitialized(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_remote_op_profiling_count_update(Deserializer &derez)\n    //--------------------------------------------------------------------------\n    {\n      RemoteOp::handle_report_profiling_count_update(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_shutdown_notification(Deserializer &derez,\n                                               AddressSpaceID source)\n    //--------------------------------------------------------------------------\n    {\n      ShutdownManager::handle_shutdown_notification(derez, this, source);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::handle_shutdown_response(Deserializer &derez) \n    //--------------------------------------------------------------------------\n    {\n      ShutdownManager::handle_shutdown_response(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::create_physical_instance(Memory target_memory,\n                                     const LayoutConstraintSet &constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result,\n                                     MapperID mapper_id, Processor processor, \n                                     bool acquire, GCPriority priority,\n                                     bool tight_bounds, \n                                     const LayoutConstraint **unsat,\n                                     size_t *footprint, UniqueID creator_id,\n                                     CollectiveManager *target, DomainPoint *p)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(target_memory);\n      if (unsat != NULL)\n      {\n        LayoutConstraintKind unsat_kind = LEGION_SPECIALIZED_CONSTRAINT;\n        unsigned unsat_index = 0;\n        if (!manager->create_physical_instance(constraints, regions, result,\n                         mapper_id, processor, acquire, priority, tight_bounds,\n                         &unsat_kind, &unsat_index, footprint, target, p,\n                         creator_id))\n        {\n          *unsat = constraints.convert_unsatisfied(unsat_kind, unsat_index);\n          return false;\n        }\n        else\n          return true;\n      }\n      else\n        return manager->create_physical_instance(constraints, regions, result,\n                         mapper_id, processor, acquire, priority, tight_bounds,\n                         NULL, NULL, footprint, target, p, creator_id);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::create_physical_instance(Memory target_memory,\n                                     LayoutConstraints *constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result,\n                                     MapperID mapper_id, Processor processor,\n                                     bool acquire, GCPriority priority,\n                                     bool tight_bounds, \n                                     const LayoutConstraint **unsat,\n                                     size_t *footprint, UniqueID creator_id,\n                                     CollectiveManager *target, DomainPoint *p)\n    //--------------------------------------------------------------------------\n    { \n      MemoryManager *manager = find_memory_manager(target_memory);\n      if (unsat != NULL)\n      {\n        LayoutConstraintKind unsat_kind = LEGION_SPECIALIZED_CONSTRAINT;\n        unsigned unsat_index = 0;\n        if (!manager->create_physical_instance(constraints, regions, result,\n                         mapper_id, processor, acquire, priority, tight_bounds,\n                         &unsat_kind, &unsat_index, footprint, target, p,\n                         creator_id))\n        {\n          *unsat = constraints->convert_unsatisfied(unsat_kind, unsat_index);\n          return false;\n        }\n        else\n          return true;\n      }\n      else\n        return manager->create_physical_instance(constraints, regions, result,\n                         mapper_id, processor, acquire, priority, tight_bounds,\n                         NULL, NULL, footprint, target, p, creator_id);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::find_or_create_physical_instance(Memory target_memory,\n                                     const LayoutConstraintSet &constraints,\n                                     const std::vector<LogicalRegion> &regions,\n                                     MappingInstance &result, bool &created, \n                                     MapperID mapper_id, Processor processor,\n                                     bool acquire, GCPriority priority,\n                                     bool tight_bounds, \n                                     const LayoutConstraint **unsat,\n                                     size_t *footprint, UniqueID creator_id)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(target_memory);\n      if (unsat != NULL)\n      {\n        LayoutConstraintKind unsat_kind = LEGION_SPECIALIZED_CONSTRAINT;\n        unsigned unsat_index = 0;\n        if (!manager->find_or_create_physical_instance(constraints, regions, \n                         result, created, mapper_id, processor, acquire, \n                         priority, tight_bounds, &unsat_kind, &unsat_index,\n                         footprint, creator_id))\n        {\n          *unsat = constraints.convert_unsatisfied(unsat_kind, unsat_index);\n          return false;\n        }\n        else\n          return true;\n      }\n      else\n        return manager->find_or_create_physical_instance(constraints, regions, \n                         result, created, mapper_id, processor, acquire, \n                         priority, tight_bounds,NULL,NULL,footprint,creator_id);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::find_or_create_physical_instance(Memory target_memory,\n                                    LayoutConstraints *constraints,\n                                    const std::vector<LogicalRegion> &regions,\n                                    MappingInstance &result, bool &created, \n                                    MapperID mapper_id, Processor processor,\n                                    bool acquire, GCPriority priority,\n                                    bool tight_bounds, \n                                    const LayoutConstraint **unsat,\n                                    size_t *footprint, UniqueID creator_id)\n    //--------------------------------------------------------------------------\n    { \n      MemoryManager *manager = find_memory_manager(target_memory);\n      if (unsat != NULL)\n      {\n        LayoutConstraintKind unsat_kind = LEGION_SPECIALIZED_CONSTRAINT;\n        unsigned unsat_index = 0;\n        if (!manager->find_or_create_physical_instance(constraints, regions,\n                           result, created, mapper_id, processor, acquire, \n                           priority, tight_bounds, &unsat_kind, &unsat_index,\n                           footprint, creator_id))\n        {\n          *unsat = constraints->convert_unsatisfied(unsat_kind, unsat_index);\n          return false;\n        }\n        else\n          return true;\n      }\n      else\n        return manager->find_or_create_physical_instance(constraints, regions,\n                     result, created, mapper_id, processor, acquire, \n                     priority, tight_bounds, NULL, NULL, footprint, creator_id);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::find_physical_instance(Memory target_memory,\n                                      const LayoutConstraintSet &constraints,\n                                      const std::vector<LogicalRegion> &regions,\n                                      MappingInstance &result, bool acquire,\n                                      bool tight_region_bounds)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(target_memory);\n      return manager->find_physical_instance(constraints, regions, \n                             result, acquire, tight_region_bounds);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::find_physical_instance(Memory target_memory,\n                                      LayoutConstraints *constraints,\n                                      const std::vector<LogicalRegion> &regions,\n                                      MappingInstance &result, bool acquire,\n                                      bool tight_region_bounds)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(target_memory);\n      return manager->find_physical_instance(constraints, regions, \n                                     result, acquire, tight_region_bounds);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::find_physical_instances(Memory target_memory,\n                                      const LayoutConstraintSet &constraints,\n                                      const std::vector<LogicalRegion> &regions,\n                                      std::vector<MappingInstance> &results, \n                                      bool acquire, bool tight_region_bounds)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(target_memory);\n      return manager->find_physical_instances(constraints, regions, \n                             results, acquire, tight_region_bounds);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::find_physical_instances(Memory target_memory,\n                                      LayoutConstraints *constraints,\n                                      const std::vector<LogicalRegion> &regions,\n                                      std::vector<MappingInstance> &results, \n                                      bool acquire, bool tight_region_bounds)\n    //--------------------------------------------------------------------------\n    {\n      MemoryManager *manager = find_memory_manager(target_memory);\n      return manager->find_physical_instances(constraints, regions, \n                                     results, acquire, tight_region_bounds);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::release_tree_instances(RegionTreeID tid)\n    //--------------------------------------------------------------------------\n    {\n      std::map<Memory,MemoryManager*> copy_managers;\n      {\n        AutoLock m_lock(memory_manager_lock,1,false/*exclusive*/);\n        copy_managers = memory_managers;\n      }\n      for (std::map<Memory,MemoryManager*>::const_iterator it = \n            copy_managers.begin(); it != copy_managers.end(); it++)\n        it->second->release_tree_instances(tid);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::process_schedule_request(Processor proc)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(local_procs.find(proc) != local_procs.end());\n#endif\n      log_run.debug(\"Running scheduler on processor \" IDFMT \"\", proc.id);\n      ProcessorManager *manager = proc_managers[proc];\n      manager->perform_scheduling();\n#ifdef TRACE_ALLOCATION\n      unsigned long long trace_count = \n        __sync_fetch_and_add(&allocation_tracing_count,1); \n      if ((trace_count % LEGION_TRACE_ALLOCATION_FREQUENCY) == 0)\n        dump_allocation_info();\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::process_message_task(const void *args, size_t arglen)\n    //--------------------------------------------------------------------------\n    {\n      const char *buffer = (const char*)args;\n      AddressSpaceID sender = *((const AddressSpaceID*)buffer);\n      buffer += sizeof(sender);\n      arglen -= sizeof(sender);\n      find_messenger(sender)->receive_message(buffer, arglen);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::activate_context(InnerContext *context)\n    //--------------------------------------------------------------------------\n    {\n      for (std::map<Processor,ProcessorManager*>::const_iterator it =\n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        it->second->activate_context(context);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::deactivate_context(InnerContext *context)\n    //--------------------------------------------------------------------------\n    {\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        it->second->deactivate_context(context);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::add_to_ready_queue(Processor p, TaskOp *op, RtEvent wait_on)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(p.kind() != Processor::UTIL_PROC);\n      assert(proc_managers.find(p) != proc_managers.end());\n#endif\n      if (wait_on.exists() && !wait_on.has_triggered())\n      {\n        TaskOp::DeferredEnqueueArgs args(proc_managers[p], op);\n        issue_runtime_meta_task(args, LG_LATENCY_DEFERRED_PRIORITY, wait_on);\n      }\n      else\n        proc_managers[p]->add_to_ready_queue(op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::add_to_local_queue(Processor p, Operation *op, \n                                     LgPriority priority, RtEvent wait_on)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(p.kind() != Processor::UTIL_PROC);\n      assert(proc_managers.find(p) != proc_managers.end());\n#endif\n      proc_managers[p]->add_to_local_ready_queue(op, priority, wait_on);\n    }\n\n    //--------------------------------------------------------------------------\n    Processor Runtime::find_processor_group(const std::vector<Processor> &procs)\n    //--------------------------------------------------------------------------\n    {\n      // Compute a hash of all the processor ids to avoid testing all sets \n      // Only need to worry about local IDs since all processors are\n      // in this address space.\n      ProcessorMask local_mask = find_processor_mask(procs);\n      uint64_t hash = local_mask.get_hash_key();\n      AutoLock g_lock(group_lock);\n      std::map<uint64_t,LegionDeque<ProcessorGroupInfo>::aligned >::iterator \n        finder = processor_groups.find(hash);\n      if (finder != processor_groups.end())\n      {\n        for (LegionDeque<ProcessorGroupInfo>::aligned::const_iterator it = \n              finder->second.begin(); it != finder->second.end(); it++)\n        {\n          if (local_mask == it->processor_mask)\n            return it->processor_group;\n        }\n      }\n      // If we make it here create a new processor group and add it\n      ProcessorGroup group = ProcessorGroup::create_group(procs);\n      if (finder != processor_groups.end())\n        finder->second.push_back(ProcessorGroupInfo(group, local_mask));\n      else\n        processor_groups[hash].push_back(ProcessorGroupInfo(group, local_mask));\n      return group;\n    }\n\n    //--------------------------------------------------------------------------\n    ProcessorMask Runtime::find_processor_mask(\n                                            const std::vector<Processor> &procs)\n    //--------------------------------------------------------------------------\n    {\n      ProcessorMask result;\n      std::vector<Processor> need_allocation;\n      {\n        AutoLock p_lock(processor_mapping_lock,1,false/*exclusive*/);\n        for (std::vector<Processor>::const_iterator it = procs.begin();\n              it != procs.end(); it++)\n        {\n          std::map<Processor,unsigned>::const_iterator finder = \n            processor_mapping.find(*it);\n          if (finder == processor_mapping.end())\n          {\n            need_allocation.push_back(*it);\n            continue;\n          }\n          result.set_bit(finder->second);\n        }\n      }\n      if (need_allocation.empty())\n        return result;\n      AutoLock p_lock(processor_mapping_lock);\n      for (std::vector<Processor>::const_iterator it = \n            need_allocation.begin(); it != need_allocation.end(); it++)\n      {\n        // Check to make sure we didn't lose the race\n        std::map<Processor,unsigned>::const_iterator finder = \n            processor_mapping.find(*it);\n        if (finder != processor_mapping.end())\n        {\n          result.set_bit(finder->second);\n          continue;\n        }\n        unsigned next_index = processor_mapping.size();\n#ifdef DEBUG_LEGION\n        assert(next_index < LEGION_MAX_NUM_PROCS);\n#endif\n        processor_mapping[*it] = next_index;\n        result.set_bit(next_index);\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    DistributedID Runtime::get_available_distributed_id(void)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(distributed_id_lock);\n      if (!available_distributed_ids.empty())\n      {\n        DistributedID result = available_distributed_ids.front();\n        available_distributed_ids.pop_front();\n        return result;\n      }\n      DistributedID result = unique_distributed_id;\n      unique_distributed_id += total_address_spaces;\n#ifdef DEBUG_LEGION\n      assert(result < LEGION_DISTRIBUTED_ID_MASK);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_distributed_id(DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      // Don't recycle distributed IDs if we're doing LegionSpy or LegionGC\n#ifndef LEGION_GC\n#ifndef LEGION_SPY\n      AutoLock d_lock(distributed_id_lock);\n      available_distributed_ids.push_back(did);\n#endif\n#endif\n#ifdef DEBUG_LEGION\n      AutoLock dist_lock(distributed_collectable_lock,1,false/*exclusive*/);\n      assert(dist_collectables.find(did) == dist_collectables.end());\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    RtEvent Runtime::recycle_distributed_id(DistributedID did,\n                                            RtEvent recycle_event)\n    //--------------------------------------------------------------------------\n    {\n      // Special case for did 0 on shutdown\n      if (did == 0)\n        return RtEvent::NO_RT_EVENT;\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n#ifdef DEBUG_LEGION\n      // Should only be getting back our own DIDs\n      assert(determine_owner(did) == address_space);\n#endif\n      if (!recycle_event.has_triggered())\n      {\n        DeferredRecycleArgs deferred_recycle_args(did);\n        return issue_runtime_meta_task(deferred_recycle_args, \n                LG_THROUGHPUT_WORK_PRIORITY, recycle_event);\n      }\n      else\n      {\n        free_distributed_id(did);\n        return RtEvent::NO_RT_EVENT;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    AddressSpaceID Runtime::determine_owner(DistributedID did) const\n    //--------------------------------------------------------------------------\n    {\n      return ((did & LEGION_DISTRIBUTED_ID_MASK) % total_address_spaces);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_distributed_collectable(DistributedID did,\n                                                   DistributedCollectable *dc)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n      RtUserEvent to_trigger;\n      {\n        AutoLock dc_lock(distributed_collectable_lock);\n        // If we make it here then we have the lock\n#ifdef DEBUG_LEGION\n        assert(dist_collectables.find(did) == dist_collectables.end());\n#endif\n        dist_collectables[did] = dc;\n        // See if this was a pending collectable\n        std::map<DistributedID,\n                 std::pair<DistributedCollectable*,RtUserEvent> >::iterator \n            finder = pending_collectables.find(did);\n        if (finder != pending_collectables.end())\n        {\n#ifdef DEBUG_LEGION\n          assert((finder->second.first == dc) || \n                 (finder->second.first == NULL));\n#endif\n          to_trigger = finder->second.second;\n          pending_collectables.erase(finder);\n        }\n      }\n      if (to_trigger.exists())\n        Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::unregister_distributed_collectable(DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n      AutoLock d_lock(distributed_collectable_lock);\n#ifdef DEBUG_LEGION\n      assert(dist_collectables.find(did) != dist_collectables.end());\n#endif\n      dist_collectables.erase(did);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_distributed_collectable(DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n      AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n      return (dist_collectables.find(did) != dist_collectables.end());\n    }\n\n    //--------------------------------------------------------------------------\n    DistributedCollectable* Runtime::find_distributed_collectable(\n                                                              DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      RtEvent wait_on;\n      DistributedCollectable *dc = find_distributed_collectable(did, wait_on);\n      if (wait_on.exists() && !wait_on.has_triggered())\n        wait_on.wait();\n      return dc;\n    }\n\n    //--------------------------------------------------------------------------\n    DistributedCollectable* Runtime::find_distributed_collectable(\n                                              DistributedID did, RtEvent &ready)\n    //--------------------------------------------------------------------------\n    {\n      bool found = false;\n      const DistributedID to_find = LEGION_DISTRIBUTED_ID_FILTER(did);\n      {\n        AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n        std::map<DistributedID,DistributedCollectable*>::const_iterator finder =\n          dist_collectables.find(to_find);\n        if (finder == dist_collectables.end())\n        {\n          // Check to see if it is in the pending set too\n          std::map<DistributedID,\n            std::pair<DistributedCollectable*,RtUserEvent> >::const_iterator\n              pending_finder = pending_collectables.find(to_find);\n          if (pending_finder != pending_collectables.end())\n          {\n            found = true;\n            ready = pending_finder->second.second;\n            if (pending_finder->second.first != NULL)\n              return pending_finder->second.first;\n          }\n        }\n        else\n          return finder->second;\n      }\n      if (!found)\n        log_run.error(\"Unable to find distributed collectable %llx \"\n                    \"with type %lld\", did, LEGION_DISTRIBUTED_HELP_DECODE(did));\n      // Wait for it to be ready\n      ready.wait();\n      AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n      std::map<DistributedID,DistributedCollectable*>::const_iterator finder =\n          dist_collectables.find(to_find);\n#ifdef DEBUG_LEGION\n      assert(finder != dist_collectables.end());\n#endif\n      return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    DistributedCollectable* Runtime::weak_find_distributed_collectable(\n                                                              DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n      AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n      std::map<DistributedID,DistributedCollectable*>::const_iterator finder = \n        dist_collectables.find(did);\n      if (finder == dist_collectables.end())\n        return NULL;\n      return finder->second;\n    } \n\n    //--------------------------------------------------------------------------\n    bool Runtime::find_pending_collectable_location(DistributedID did,\n                                                    void *&location)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n      AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n#ifdef DEBUG_LEGION\n      assert(dist_collectables.find(did) == dist_collectables.end());\n#endif\n      std::map<DistributedID,std::pair<DistributedCollectable*,RtUserEvent> >::\n        const_iterator finder = pending_collectables.find(did);\n      if (finder != pending_collectables.end())\n      {\n        location = finder->second.first;\n        return true;\n      }\n      return false;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::record_pending_distributed_collectable(DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      const RtUserEvent registered = Runtime::create_rt_user_event();\n      AutoLock d_lock(distributed_collectable_lock);\n#ifdef DEBUG_LEGION\n      assert(dist_collectables.find(did) == dist_collectables.end());\n      assert(pending_collectables.find(did) == pending_collectables.end());\n#endif\n      pending_collectables[did] = \n        std::pair<DistributedCollectable*,RtUserEvent>(NULL, registered);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::revoke_pending_distributed_collectable(DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      RtUserEvent to_trigger;\n      {\n        AutoLock d_lock(distributed_collectable_lock);\n        std::map<DistributedID,\n          std::pair<DistributedCollectable*,RtUserEvent> >::iterator finder =\n            pending_collectables.find(did);\n        if (finder != pending_collectables.end())\n        {\n          to_trigger = finder->second.second;\n          pending_collectables.erase(finder);\n        }\n      }\n      if (to_trigger.exists())\n        Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    LogicalView* Runtime::find_or_request_logical_view(DistributedID did,\n                                                       RtEvent &ready)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable *dc = NULL;\n      if (LogicalView::is_materialized_did(did))\n        dc = find_or_request_distributed_collectable<\n         MaterializedView,SEND_VIEW_REQUEST,DEFAULT_VIRTUAL_CHANNEL>(did,ready);\n      else if (LogicalView::is_reduction_did(did))\n        dc = find_or_request_distributed_collectable<\n          ReductionView, SEND_VIEW_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(did,ready);\n      else if (LogicalView::is_fill_did(did))\n        dc = find_or_request_distributed_collectable<\n          FillView, SEND_VIEW_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(did, ready);\n      else if (LogicalView::is_phi_did(did))\n        dc = find_or_request_distributed_collectable<\n          PhiView, SEND_VIEW_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(did, ready);\n      else if (LogicalView::is_sharded_did(did))\n        dc = find_or_request_distributed_collectable<\n          ShardedView, SEND_VIEW_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(did, ready);\n      else\n        assert(false);\n      // Have to static cast since the memory might not have been initialized\n      return static_cast<LogicalView*>(dc);\n    }\n\n    //--------------------------------------------------------------------------\n    PhysicalManager* Runtime::find_or_request_instance_manager(\n                                              DistributedID did, RtEvent &ready)\n    //--------------------------------------------------------------------------\n    {\n      DistributedCollectable *dc = NULL;\n      if (InstanceManager::is_collective_did(did))\n        dc = find_or_request_distributed_collectable<\n          CollectiveManager, SEND_MANAGER_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(\n                                                                    did, ready);\n      else if (InstanceManager::is_instance_did(did))\n        dc = find_or_request_distributed_collectable<\n          IndividualManager, SEND_MANAGER_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(\n                                                                    did, ready);\n      else\n        assert(false);\n      // Have to static cast since the memory might not have been initialized\n      return static_cast<PhysicalManager*>(dc);\n    }\n\n    //--------------------------------------------------------------------------\n    EquivalenceSet* Runtime::find_or_request_equivalence_set(DistributedID did,\n                                                             RtEvent &ready)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(LEGION_DISTRIBUTED_HELP_DECODE(did) == EQUIVALENCE_SET_DC);\n#endif\n      DistributedCollectable *dc = find_or_request_distributed_collectable<\n        EquivalenceSet, SEND_EQUIVALENCE_SET_REQUEST, DEFAULT_VIRTUAL_CHANNEL>(\n                                                                    did, ready);\n      // Have to static cast since the memory might not have been initialized\n      return static_cast<EquivalenceSet*>(dc);\n    }\n\n    //--------------------------------------------------------------------------\n    template<typename T, MessageKind MK, VirtualChannelKind VC>\n    DistributedCollectable* Runtime::find_or_request_distributed_collectable(\n                                          DistributedID to_find, RtEvent &ready)\n    //--------------------------------------------------------------------------\n    {\n      const DistributedID did = LEGION_DISTRIBUTED_ID_FILTER(to_find);\n      DistributedCollectable *result = NULL;\n      {\n        AutoLock d_lock(distributed_collectable_lock);\n        std::map<DistributedID,DistributedCollectable*>::const_iterator finder =\n          dist_collectables.find(did);\n        // If we've already got it, then we are done\n        if (finder != dist_collectables.end())\n        {\n          ready = RtEvent::NO_RT_EVENT;\n          return finder->second;\n        }\n        // If it is already pending, we can just return the ready event\n        std::map<DistributedID,std::pair<DistributedCollectable*,RtUserEvent> \n          >::const_iterator pending_finder = pending_collectables.find(did);\n        if (pending_finder != pending_collectables.end())\n        {\n          ready = pending_finder->second.second;\n          return pending_finder->second.first;\n        }\n        // This is the first request we've seen for this did, make it now\n        // Allocate space for the result and type case\n        result = (T*)legion_alloc_aligned<T,false/*bytes*/>(1/*count*/);  \n        RtUserEvent to_trigger = Runtime::create_rt_user_event();\n        pending_collectables[did] = \n          std::pair<DistributedCollectable*,RtUserEvent>(result, to_trigger);\n        ready = to_trigger;\n      }\n      AddressSpaceID target = determine_owner(did);\n#ifdef DEBUG_LEGION\n      assert(target != address_space); // shouldn't be sending to ourself\n#endif\n      // Now send the message\n      Serializer rez;\n      {\n        RezCheck z(rez);\n        rez.serialize(to_find);\n      }\n      find_messenger(target)->send_message(rez, MK, VC, true/*flush*/);\n      return result;\n    }\n    \n    //--------------------------------------------------------------------------\n    FutureImpl* Runtime::find_or_create_future(DistributedID did,\n                                               ReferenceMutator *mutator,\n                       std::vector<std::pair<size_t,DomainPoint> > &coordinates,\n                                               Operation *op, GenerationID gen,\n#ifdef LEGION_SPY\n                                               UniqueID op_uid,\n#endif\n                                               int op_depth)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK; \n      {\n        AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n        std::map<DistributedID,DistributedCollectable*>::const_iterator \n          finder = dist_collectables.find(did);\n        if (finder != dist_collectables.end())\n        {\n#ifdef DEBUG_LEGION\n          FutureImpl *result = dynamic_cast<FutureImpl*>(finder->second);\n          assert(result != NULL);\n#else\n          FutureImpl *result = static_cast<FutureImpl*>(finder->second);\n#endif\n          return result;\n        }\n      }\n      const AddressSpaceID owner_space = determine_owner(did);\n#ifdef DEBUG_LEGION\n      assert(owner_space != address_space);\n#endif\n      FutureImpl *result = (op == NULL) ? \n        new FutureImpl(this, false/*register*/, did, owner_space,\n                       ApEvent::NO_AP_EVENT) : \n        new FutureImpl(this, false/*register*/, did, owner_space, \n                       ApEvent::NO_AP_EVENT, op, gen,\n#ifdef LEGION_SPY\n                       op_uid,\n#endif\n                       op_depth);\n      // Retake the lock and see if we lost the race\n      {\n        AutoLock d_lock(distributed_collectable_lock);\n        std::map<DistributedID,DistributedCollectable*>::const_iterator \n          finder = dist_collectables.find(did);\n        if (finder != dist_collectables.end())\n        {\n          // We lost the race\n          if (!result->is_owner() && \n              result->remove_base_resource_ref(REMOTE_DID_REF))\n            delete (result);\n#ifdef DEBUG_LEGION\n          result = dynamic_cast<FutureImpl*>(finder->second);\n          assert(result != NULL);\n#else\n          result = static_cast<FutureImpl*>(finder->second);\n#endif\n          return result;\n        }\n        result->record_future_registered(mutator);\n        if (!coordinates.empty())\n          result->set_future_coordinates(coordinates);\n        dist_collectables[did] = result;\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    FutureMapImpl* Runtime::find_or_create_future_map(DistributedID did,\n                          TaskContext *ctx, size_t index, const Domain &domain,\n                          RtEvent complete, ReferenceMutator *mutator,\n                          std::vector<std::pair<size_t,DomainPoint> > &coords)\n    //--------------------------------------------------------------------------\n    {\n      did &= LEGION_DISTRIBUTED_ID_MASK;\n      {\n        AutoLock d_lock(distributed_collectable_lock,1,false/*exclusive*/);\n        std::map<DistributedID,DistributedCollectable*>::const_iterator \n          finder = dist_collectables.find(did);\n        if (finder != dist_collectables.end())\n        {\n#ifdef DEBUG_LEGION\n          FutureMapImpl *result = dynamic_cast<FutureMapImpl*>(finder->second);\n          assert(result != NULL);\n#else\n          FutureMapImpl *result = static_cast<FutureMapImpl*>(finder->second);\n#endif\n          return result;\n        }\n      }\n      // Check to see if we need to prefetch sparsity map data to this node\n      if (!domain.dense())\n      {\n        switch (domain.get_dim())\n        {\n#define DIMFUNC(DIM) \\\n          case DIM: \\\n            { \\\n              const DomainT<DIM,coord_t> domaint = domain; \\\n              const RtEvent wait_on(domaint.make_valid()); \\\n              if (wait_on.exists() && !wait_on.has_triggered()) \\\n                wait_on.wait(); \\\n              break; \\\n            }\n          LEGION_FOREACH_N(DIMFUNC)\n#undef DIMFUNC\n          default:\n            assert(false);\n        }\n      }\n      const AddressSpaceID owner_space = determine_owner(did);\n#ifdef DEBUG_LEGION\n      assert(owner_space != address_space);\n#endif\n      FutureMapImpl *result = new FutureMapImpl(ctx, this, domain, did, index,\n                                owner_space, complete, false/*register now */);\n      // Retake the lock and see if we lost the race\n      {\n        AutoLock d_lock(distributed_collectable_lock);\n        std::map<DistributedID,DistributedCollectable*>::const_iterator \n          finder = dist_collectables.find(did);\n        if (finder != dist_collectables.end())\n        {\n          // We lost the race\n          if (!result->is_owner() &&\n              result->remove_base_resource_ref(REMOTE_DID_REF))\n            delete (result);\n#ifdef DEBUG_LEGION\n          result = dynamic_cast<FutureMapImpl*>(finder->second);\n          assert(result != NULL);\n#else\n          result = static_cast<FutureMapImpl*>(finder->second);\n#endif\n          return result;\n        }\n        result->record_future_map_registered(mutator);\n        if (!coords.empty())\n          result->set_future_map_coordinates(coords);\n        dist_collectables[did] = result;\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::find_or_create_index_slice_space(const Domain &domain,\n                                                         TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(type_tag != 0);\n#endif\n      const std::pair<Domain,TypeTag> key(domain, type_tag);\n      {\n        AutoLock is_lock(is_slice_lock,1,false/*exclusive*/);\n        std::map<std::pair<Domain,TypeTag>,IndexSpace>::const_iterator finder =\n          index_slice_spaces.find(key);\n        if (finder != index_slice_spaces.end())\n          return finder->second;\n      }\n      const IndexSpace result(get_unique_index_space_id(),\n                              get_unique_index_tree_id(), type_tag);\n      const DistributedID did = get_available_distributed_id();\n      forest->create_index_space(result, &domain, did);\n      if (legion_spy_enabled)\n        LegionSpy::log_top_index_space(result.id);\n      // Overwrite and leak for now, don't care too much as this \n      // should occur infrequently\n      AutoLock is_lock(is_slice_lock);\n      index_slice_spaces[key] = result;\n      return result;\n    } \n\n    //--------------------------------------------------------------------------\n    void Runtime::increment_outstanding_top_level_tasks(void)\n    //--------------------------------------------------------------------------\n    {\n      // Check to see if we are on node 0 or not\n      if (address_space != 0)\n      {\n        // Send a message to node 0 requesting permission to \n        // lauch a new top-level task and wait on an event\n        // to signal that permission has been granted\n        RtUserEvent grant_event = Runtime::create_rt_user_event();\n        Serializer rez;\n        rez.serialize(grant_event);\n        find_messenger(0)->send_message(rez, SEND_TOP_LEVEL_TASK_REQUEST,\n                                THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n        grant_event.wait();\n      }\n      else\n      {\n        __sync_fetch_and_add(&outstanding_top_level_tasks,1);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::decrement_outstanding_top_level_tasks(void)\n    //--------------------------------------------------------------------------\n    {\n      // Check to see if we are on node 0 or not\n      if (address_space != 0)\n      {\n        // Send a message to node 0 indicating that we finished\n        // executing a top-level task\n        Serializer rez;\n        find_messenger(0)->send_message(rez, SEND_TOP_LEVEL_TASK_COMPLETE,\n                                THROUGHPUT_VIRTUAL_CHANNEL, true/*flush*/);\n      }\n      else\n      {\n        unsigned prev = __sync_fetch_and_sub(&outstanding_top_level_tasks,1);\n#ifdef DEBUG_LEGION\n        assert(prev > 0);\n#endif\n        // Check to see if we have no more outstanding top-level tasks\n        // If we don't launch a task to handle the try to shutdown the runtime \n        if (prev == 1)\n          issue_runtime_shutdown_attempt();\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::issue_runtime_shutdown_attempt(void)\n    //--------------------------------------------------------------------------\n    {\n      ShutdownManager::RetryShutdownArgs args(\n            ShutdownManager::CHECK_TERMINATION);\n      // Issue this with a low priority so that other meta-tasks\n      // have an opportunity to run\n      issue_runtime_meta_task(args, LG_LOW_PRIORITY);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::initiate_runtime_shutdown(AddressSpaceID source,\n                                           ShutdownManager::ShutdownPhase phase,\n                                           ShutdownManager *owner)\n    //--------------------------------------------------------------------------\n    {\n      log_shutdown.info(\"Received notification on node %d for phase %d\",\n                        address_space, phase);\n      // If this is the first phase, do all our normal stuff\n      if (phase == ShutdownManager::CHECK_TERMINATION)\n      {\n        // Get the preconditions for any outstanding operations still\n        // available for garabage collection and wait on them to \n        // try and get close to when there are no more outstanding tasks\n        std::map<Memory,MemoryManager*> copy_managers;\n        {\n          AutoLock m_lock(memory_manager_lock,1,false/*exclusive*/);\n          copy_managers = memory_managers;\n        }\n        std::set<ApEvent> wait_events;\n        for (std::map<Memory,MemoryManager*>::const_iterator it = \n              copy_managers.begin(); it != copy_managers.end(); it++)\n          it->second->find_shutdown_preconditions(wait_events);\n        if (!wait_events.empty())\n        {\n          RtEvent wait_on = Runtime::protect_merge_events(wait_events);\n          wait_on.wait();\n        }\n      }\n      else if ((phase == ShutdownManager::CHECK_SHUTDOWN) && \n                !prepared_for_shutdown)\n      {\n        // First time we check for shutdown we do the prepare for shutdown\n        prepare_runtime_shutdown();  \n      }\n      ShutdownManager *shutdown_manager = \n        new ShutdownManager(phase, this, source, \n                            LEGION_SHUTDOWN_RADIX, owner);\n      if (shutdown_manager->attempt_shutdown())\n        delete shutdown_manager;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::confirm_runtime_shutdown(ShutdownManager *shutdown_manager, \n                                           bool phase_one)\n    //--------------------------------------------------------------------------\n    {\n      if (has_outstanding_tasks())\n      {\n        shutdown_manager->record_outstanding_tasks();\n#ifdef DEBUG_LEGION\n        LG_TASK_DESCRIPTIONS(meta_task_names);\n        AutoLock out_lock(outstanding_task_lock,1,false/*exclusive*/);\n        for (std::map<std::pair<unsigned,bool>,unsigned>::const_iterator it =\n              outstanding_task_counts.begin(); it != \n              outstanding_task_counts.end(); it++)\n        {\n          if (it->second == 0)\n            continue;\n          if (it->first.second)\n            log_shutdown.info(\"RT %d: %d outstanding meta task(s) %s\",\n                              address_space, it->second, \n                              meta_task_names[it->first.first]);\n          else                \n            log_shutdown.info(\"RT %d: %d outstanding application task(s) %d\",\n                              address_space, it->second, it->first.first);\n        }\n#endif\n      }\n      // Check all our message managers for outstanding messages\n      for (unsigned idx = 0; idx < LEGION_MAX_NUM_NODES; idx++)\n      {\n        if (message_managers[idx] != NULL)\n          message_managers[idx]->confirm_shutdown(shutdown_manager, phase_one);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::prepare_runtime_shutdown(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!prepared_for_shutdown);\n#endif\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n        it->second->prepare_for_shutdown();\n      for (std::map<Memory,MemoryManager*>::const_iterator it = \n            memory_managers.begin(); it != memory_managers.end(); it++)\n        it->second->prepare_for_shutdown();\n      // Destroy any index slice spaces that we made during execution\n      std::set<RtEvent> applied;\n      for (std::map<std::pair<Domain,TypeTag>,IndexSpace>::const_iterator it =\n            index_slice_spaces.begin(); it != index_slice_spaces.end(); it++)\n        forest->destroy_index_space(it->second, applied);\n      for (std::map<ProjectionID,ProjectionFunction*>::const_iterator it =\n           projection_functions.begin(); it != projection_functions.end(); it++)\n        it->second->prepare_for_shutdown();\n      // If there are still any layout constraints that the application\n      // failed to remove its references to then we can remove the reference\n      // for them and make sure it's effects propagate\n      if (!separate_runtime_instances)\n      {\n        std::vector<LayoutConstraints*> to_remove;\n        {\n          AutoLock l_lock(layout_constraints_lock,1,false/*exclusive*/);\n          for (std::map<LayoutConstraintID,LayoutConstraints*>::const_iterator\n                it = layout_constraints_table.begin(); it !=\n                layout_constraints_table.end(); it++)\n            if (it->second->is_owner() && !it->second->internal)\n              to_remove.push_back(it->second);\n        }\n        if (!to_remove.empty())\n        {\n          WrapperReferenceMutator mutator(applied); \n          for (std::vector<LayoutConstraints*>::const_iterator it = \n                to_remove.begin(); it != to_remove.end(); it++)\n            if ((*it)->remove_base_gc_ref(APPLICATION_REF, &mutator))\n              delete (*it);\n        }\n      }\n      if (!redop_fill_views.empty())\n      {\n        WrapperReferenceMutator mutator(applied);\n        for (std::map<ReductionOpID,FillView*>::const_iterator it = \n              redop_fill_views.begin(); it != redop_fill_views.end(); it++)\n          if (it->second->remove_base_valid_ref(RUNTIME_REF, &mutator))\n            delete it->second;\n        redop_fill_views.clear();\n      }\n      if (!applied.empty())\n      {\n        const RtEvent wait_on = Runtime::merge_events(applied);\n        if (wait_on.exists() && !wait_on.has_triggered())\n          wait_on.wait();\n      }\n      prepared_for_shutdown = true;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::finalize_runtime_shutdown(int exit_code)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(address_space == 0); // only happens on node 0\n#endif\n      std::set<RtEvent> shutdown_events;\n      // Launch tasks to shutdown all the runtime instances\n      Machine::ProcessorQuery all_procs(machine);\n      Realm::ProfilingRequestSet empty_requests;\n      if (Runtime::separate_runtime_instances)\n      {\n        // If we are doing separate runtime instances, run it once on every\n        // processor since we have separate runtimes for every processor\n        for (Machine::ProcessorQuery::iterator it = all_procs.begin();\n              it != all_procs.end(); it++)\n        {\n          shutdown_events.insert(\n              RtEvent(it->spawn(LG_SHUTDOWN_TASK_ID, NULL, 0, empty_requests)));\n        }\n      }\n      else\n      {\n        // In the normal case we just have to run this once on every node\n        std::set<AddressSpace> shutdown_spaces;\n        for (Machine::ProcessorQuery::iterator it = all_procs.begin();\n              it != all_procs.end(); it++)\n        {\n          AddressSpace space = it->address_space();\n          if (shutdown_spaces.find(space) == shutdown_spaces.end())\n          {\n            shutdown_events.insert(\n                RtEvent(it->spawn(LG_SHUTDOWN_TASK_ID,NULL,0,empty_requests)));\n            shutdown_spaces.insert(space);\n          }\n        }\n      }\n      // One last really crazy precondition on shutdown, we actually need to\n      // make sure that this task itself is done executing before trying to\n      // shutdown so add our own completion event as a precondition\n      shutdown_events.insert(RtEvent(Processor::get_current_finish_event()));\n      // Then tell Realm to shutdown when they are all done\n      RtEvent shutdown_precondition = Runtime::merge_events(shutdown_events);\n      RealmRuntime realm = RealmRuntime::get_runtime();\n      realm.shutdown(shutdown_precondition, exit_code);\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::has_outstanding_tasks(void)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      AutoLock out_lock(outstanding_task_lock);\n      return (total_outstanding_tasks > 0);\n#else\n      return (__sync_fetch_and_add(&total_outstanding_tasks,0) != 0);\n#endif\n    }\n\n#ifdef DEBUG_LEGION\n    //--------------------------------------------------------------------------\n    void Runtime::increment_total_outstanding_tasks(unsigned tid, bool meta)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock out_lock(outstanding_task_lock); \n      total_outstanding_tasks++;\n      std::pair<unsigned,bool> key(tid,meta);\n      std::map<std::pair<unsigned,bool>,unsigned>::iterator finder = \n        outstanding_task_counts.find(key);\n      if (finder == outstanding_task_counts.end())\n        outstanding_task_counts[key] = 1;\n      else\n        finder->second++;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::decrement_total_outstanding_tasks(unsigned tid, bool meta)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock out_lock(outstanding_task_lock);\n      assert(total_outstanding_tasks > 0);\n      total_outstanding_tasks--;\n      std::pair<unsigned,bool> key(tid,meta);\n      std::map<std::pair<unsigned,bool>,unsigned>::iterator finder = \n        outstanding_task_counts.find(key);\n      assert(finder != outstanding_task_counts.end());\n      assert(finder->second > 0);\n      finder->second--;\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    IndividualTask* Runtime::get_available_individual_task(void)\n    //--------------------------------------------------------------------------\n    {\n      IndividualTask *result = get_available(individual_task_lock, \n                                         available_individual_tasks);\n#ifdef DEBUG_LEGION\n      AutoLock i_lock(individual_task_lock);\n      out_individual_tasks.insert(result);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    PointTask* Runtime::get_available_point_task(void)\n    //--------------------------------------------------------------------------\n    {\n      PointTask *result = get_available(point_task_lock, \n                                        available_point_tasks);\n#ifdef DEBUG_LEGION\n      AutoLock p_lock(point_task_lock);\n      out_point_tasks.insert(result);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexTask* Runtime::get_available_index_task(void)\n    //--------------------------------------------------------------------------\n    {\n      IndexTask *result = get_available(index_task_lock, \n                                       available_index_tasks);\n#ifdef DEBUG_LEGION\n      AutoLock i_lock(index_task_lock);\n      out_index_tasks.insert(result);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    SliceTask* Runtime::get_available_slice_task(void)\n    //--------------------------------------------------------------------------\n    {\n      SliceTask *result = get_available(slice_task_lock,\n                                       available_slice_tasks);\n#ifdef DEBUG_LEGION\n      AutoLock s_lock(slice_task_lock);\n      out_slice_tasks.insert(result);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    MapOp* Runtime::get_available_map_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(map_op_lock, available_map_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    CopyOp* Runtime::get_available_copy_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(copy_op_lock, available_copy_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexCopyOp* Runtime::get_available_index_copy_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(copy_op_lock, available_index_copy_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    PointCopyOp* Runtime::get_available_point_copy_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(copy_op_lock, available_point_copy_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    FenceOp* Runtime::get_available_fence_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(fence_op_lock, available_fence_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    FrameOp* Runtime::get_available_frame_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(frame_op_lock, available_frame_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    CreationOp* Runtime::get_available_creation_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(creation_op_lock, available_creation_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    DeletionOp* Runtime::get_available_deletion_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(deletion_op_lock, available_deletion_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    MergeCloseOp* Runtime::get_available_merge_close_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(merge_close_op_lock, available_merge_close_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    PostCloseOp* Runtime::get_available_post_close_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(post_close_op_lock, available_post_close_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    VirtualCloseOp* Runtime::get_available_virtual_close_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(virtual_close_op_lock, available_virtual_close_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    RefinementOp* Runtime::get_available_refinement_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(refinement_op_lock, available_refinement_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    AdvisementOp* Runtime::get_available_advisement_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(advisement_op_lock, available_advisement_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    DynamicCollectiveOp* Runtime::get_available_dynamic_collective_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(dynamic_collective_op_lock, \n                           available_dynamic_collective_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    FuturePredOp* Runtime::get_available_future_pred_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(future_pred_op_lock, available_future_pred_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    NotPredOp* Runtime::get_available_not_pred_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(not_pred_op_lock, available_not_pred_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    AndPredOp* Runtime::get_available_and_pred_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(and_pred_op_lock, available_and_pred_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    OrPredOp* Runtime::get_available_or_pred_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(or_pred_op_lock, available_or_pred_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    AcquireOp* Runtime::get_available_acquire_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(acquire_op_lock, available_acquire_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReleaseOp* Runtime::get_available_release_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(release_op_lock, available_release_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    TraceCaptureOp* Runtime::get_available_capture_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(capture_op_lock, available_capture_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    TraceCompleteOp* Runtime::get_available_trace_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(trace_op_lock, available_trace_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    TraceReplayOp* Runtime::get_available_replay_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(replay_op_lock, available_replay_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    TraceBeginOp* Runtime::get_available_begin_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(begin_op_lock, available_begin_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    TraceSummaryOp* Runtime::get_available_summary_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(summary_op_lock, available_summary_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    MustEpochOp* Runtime::get_available_epoch_op(void)\n    //--------------------------------------------------------------------------\n    {\n      MustEpochOp *result = get_available(epoch_op_lock, available_epoch_ops);\n#ifdef DEBUG_LEGION\n      AutoLock e_lock(epoch_op_lock);\n      out_must_epoch.insert(result);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    PendingPartitionOp* Runtime::get_available_pending_partition_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(pending_partition_op_lock, \n                           available_pending_partition_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    DependentPartitionOp* Runtime::get_available_dependent_partition_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(dependent_partition_op_lock, \n                           available_dependent_partition_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    PointDepPartOp* Runtime::get_available_point_dep_part_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(dependent_partition_op_lock,\n                           available_point_dep_part_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    FillOp* Runtime::get_available_fill_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(fill_op_lock, available_fill_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexFillOp* Runtime::get_available_index_fill_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(fill_op_lock, available_index_fill_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    PointFillOp* Runtime::get_available_point_fill_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(fill_op_lock, available_point_fill_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    AttachOp* Runtime::get_available_attach_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(attach_op_lock, available_attach_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    DetachOp* Runtime::get_available_detach_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(detach_op_lock, available_detach_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    TimingOp* Runtime::get_available_timing_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(timing_op_lock, available_timing_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    AllReduceOp* Runtime::get_available_all_reduce_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(all_reduce_op_lock, available_all_reduce_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplIndividualTask* Runtime::get_available_repl_individual_task(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(individual_task_lock, \n                           available_repl_individual_tasks);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplIndexTask* Runtime::get_available_repl_index_task(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(index_task_lock, available_repl_index_tasks);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplMergeCloseOp* Runtime::get_available_repl_merge_close_op(void)\n    //-------------------------------------------------------------------------- \n    {\n      return get_available(merge_close_op_lock, available_repl_merge_close_ops);\n    }\n    \n    //--------------------------------------------------------------------------\n    ReplRefinementOp* Runtime::get_available_repl_refinement_op(void)\n    //-------------------------------------------------------------------------- \n    {\n      return get_available(refinement_op_lock, available_repl_refinement_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplFillOp* Runtime::get_available_repl_fill_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(fill_op_lock, available_repl_fill_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplIndexFillOp* Runtime::get_available_repl_index_fill_op(void) \n    //--------------------------------------------------------------------------\n    {\n      return get_available(fill_op_lock, available_repl_index_fill_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplCopyOp* Runtime::get_available_repl_copy_op(void) \n    //--------------------------------------------------------------------------\n    {\n      return get_available(copy_op_lock, available_repl_copy_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplIndexCopyOp* Runtime::get_available_repl_index_copy_op(void) \n    //--------------------------------------------------------------------------\n    {\n      return get_available(copy_op_lock, available_repl_index_copy_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplDeletionOp* Runtime::get_available_repl_deletion_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(deletion_op_lock, available_repl_deletion_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplPendingPartitionOp* \n                          Runtime::get_available_repl_pending_partition_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(pending_partition_op_lock,\n                           available_repl_pending_partition_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplDependentPartitionOp* \n                        Runtime::get_available_repl_dependent_partition_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(dependent_partition_op_lock,\n                           available_repl_dependent_partition_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplMustEpochOp* Runtime::get_available_repl_epoch_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(epoch_op_lock, available_repl_must_epoch_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplTimingOp* Runtime::get_available_repl_timing_op(void) \n    //--------------------------------------------------------------------------\n    {\n      return get_available(timing_op_lock, available_repl_timing_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplAllReduceOp* Runtime::get_available_repl_all_reduce_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(all_reduce_op_lock, available_repl_all_reduce_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplFenceOp* Runtime::get_available_repl_fence_op(void) \n    //--------------------------------------------------------------------------\n    {\n      return get_available(fence_op_lock, available_repl_fence_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplMapOp* Runtime::get_available_repl_map_op(void) \n    //--------------------------------------------------------------------------\n    {\n      return get_available(map_op_lock, available_repl_map_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplAttachOp* Runtime::get_available_repl_attach_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(attach_op_lock, available_repl_attach_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplDetachOp* Runtime::get_available_repl_detach_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(detach_op_lock, available_repl_detach_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplTraceCaptureOp* Runtime::get_available_repl_capture_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(capture_op_lock, available_repl_capture_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplTraceCompleteOp* Runtime::get_available_repl_trace_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(trace_op_lock, available_repl_trace_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplTraceReplayOp* Runtime::get_available_repl_replay_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(replay_op_lock, available_repl_replay_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplTraceBeginOp* Runtime::get_available_repl_begin_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(begin_op_lock, available_repl_begin_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    ReplTraceSummaryOp* Runtime::get_available_repl_summary_op(void)\n    //--------------------------------------------------------------------------\n    {\n      return get_available(summary_op_lock, available_repl_summary_ops);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_individual_task(IndividualTask *task)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock i_lock(individual_task_lock);\n      release_operation<false>(available_individual_tasks, task);\n#ifdef DEBUG_LEGION\n      out_individual_tasks.erase(task);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_point_task(PointTask *task)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(point_task_lock);\n#ifdef DEBUG_LEGION\n      out_point_tasks.erase(task);\n#endif\n      // Note that we can safely delete point tasks because they are\n      // never registered in the logical state of the region tree\n      // as part of the dependence analysis. This does not apply\n      // to all operation objects.\n      release_operation<true>(available_point_tasks, task);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_index_task(IndexTask *task)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock i_lock(index_task_lock);\n      release_operation<false>(available_index_tasks, task);\n#ifdef DEBUG_LEGION\n      out_index_tasks.erase(task);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_slice_task(SliceTask *task)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(slice_task_lock);\n#ifdef DEBUG_LEGION\n      out_slice_tasks.erase(task);\n#endif\n      // Note that we can safely delete slice tasks because they are\n      // never registered in the logical state of the region tree\n      // as part of the dependence analysis. This does not apply\n      // to all operation objects.\n      release_operation<true>(available_slice_tasks, task);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_map_op(MapOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(map_op_lock);\n      release_operation<false>(available_map_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_copy_op(CopyOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(copy_op_lock);\n      release_operation<false>(available_copy_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_index_copy_op(IndexCopyOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(copy_op_lock);\n      release_operation<false>(available_index_copy_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_point_copy_op(PointCopyOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(copy_op_lock);\n      release_operation<true>(available_point_copy_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_fence_op(FenceOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(fence_op_lock);\n      release_operation<false>(available_fence_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_frame_op(FrameOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(frame_op_lock);\n      release_operation<false>(available_frame_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_creation_op(CreationOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(creation_op_lock);\n      release_operation<false>(available_creation_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_deletion_op(DeletionOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(deletion_op_lock);\n      release_operation<false>(available_deletion_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_merge_close_op(MergeCloseOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock i_lock(merge_close_op_lock);\n      release_operation<false>(available_merge_close_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_post_close_op(PostCloseOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(post_close_op_lock);\n      release_operation<false>(available_post_close_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_virtual_close_op(VirtualCloseOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock v_lock(virtual_close_op_lock);\n      release_operation<false>(available_virtual_close_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_refinement_op(RefinementOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock r_lock(refinement_op_lock);\n      release_operation<false>(available_refinement_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_advisement_op(AdvisementOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(advisement_op_lock);\n      release_operation<false>(available_advisement_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_dynamic_collective_op(DynamicCollectiveOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock dc_lock(dynamic_collective_op_lock);\n      release_operation<false>(available_dynamic_collective_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_future_predicate_op(FuturePredOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(future_pred_op_lock);\n      release_operation<false>(available_future_pred_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_not_predicate_op(NotPredOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock n_lock(not_pred_op_lock);\n      release_operation<false>(available_not_pred_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_and_predicate_op(AndPredOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(and_pred_op_lock);\n      release_operation<false>(available_and_pred_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_or_predicate_op(OrPredOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock o_lock(or_pred_op_lock);\n      release_operation<false>(available_or_pred_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_acquire_op(AcquireOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(acquire_op_lock);\n      release_operation<false>(available_acquire_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_release_op(ReleaseOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock r_lock(release_op_lock);\n      release_operation<false>(available_release_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_capture_op(TraceCaptureOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(capture_op_lock);\n      release_operation<false>(available_capture_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_trace_op(TraceCompleteOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(trace_op_lock);\n      release_operation<false>(available_trace_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_replay_op(TraceReplayOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(replay_op_lock);\n      release_operation<false>(available_replay_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_begin_op(TraceBeginOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(begin_op_lock);\n      release_operation<false>(available_begin_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_summary_op(TraceSummaryOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(summary_op_lock);\n      release_operation<false>(available_summary_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_epoch_op(MustEpochOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock e_lock(epoch_op_lock);\n      release_operation<false>(available_epoch_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_pending_partition_op(PendingPartitionOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(pending_partition_op_lock);\n      release_operation<false>(available_pending_partition_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_dependent_partition_op(DependentPartitionOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(dependent_partition_op_lock);\n      release_operation<false>(available_dependent_partition_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_point_dep_part_op(PointDepPartOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(dependent_partition_op_lock);\n      release_operation<true>(available_point_dep_part_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_fill_op(FillOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(fill_op_lock);\n      release_operation<false>(available_fill_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_index_fill_op(IndexFillOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(fill_op_lock);\n      release_operation<false>(available_index_fill_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_point_fill_op(PointFillOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(fill_op_lock);\n      release_operation<true>(available_point_fill_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_attach_op(AttachOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(attach_op_lock);\n      release_operation<false>(available_attach_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_detach_op(DetachOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(detach_op_lock);\n      release_operation<false>(available_detach_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_timing_op(TimingOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(timing_op_lock);\n      release_operation<false>(available_timing_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_individual_task(ReplIndividualTask *task)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock i_lock(individual_task_lock);\n      release_operation<false>(available_repl_individual_tasks, task);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_index_task(ReplIndexTask *task)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock i_lock(index_task_lock);\n      release_operation<false>(available_repl_index_tasks, task);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_merge_close_op(ReplMergeCloseOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(merge_close_op_lock);\n      release_operation<false>(available_repl_merge_close_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_refinement_op(ReplRefinementOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(refinement_op_lock);\n      release_operation<false>(available_repl_refinement_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_fill_op(ReplFillOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(fill_op_lock);\n      release_operation<false>(available_repl_fill_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_index_fill_op(ReplIndexFillOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock f_lock(fill_op_lock);\n      release_operation<false>(available_repl_index_fill_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_copy_op(ReplCopyOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(copy_op_lock);\n      release_operation<false>(available_repl_copy_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_index_copy_op(ReplIndexCopyOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(copy_op_lock);\n      release_operation<false>(available_repl_index_copy_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_deletion_op(ReplDeletionOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(deletion_op_lock);\n      release_operation<false>(available_repl_deletion_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_pending_partition_op(ReplPendingPartitionOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock p_lock(pending_partition_op_lock);\n      release_operation<false>(available_repl_pending_partition_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_dependent_partition_op(ReplDependentPartitionOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(dependent_partition_op_lock);\n      release_operation<false>(available_repl_dependent_partition_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_epoch_op(ReplMustEpochOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(epoch_op_lock);\n      release_operation<false>(available_repl_must_epoch_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_timing_op(ReplTimingOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(timing_op_lock);\n      release_operation<false>(available_repl_timing_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_all_reduce_op(ReplAllReduceOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(all_reduce_op_lock);\n      release_operation<false>(available_repl_all_reduce_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_fence_op(ReplFenceOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(fence_op_lock);\n      release_operation<false>(available_repl_fence_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_map_op(ReplMapOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock m_lock(map_op_lock);\n      release_operation<false>(available_repl_map_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_attach_op(ReplAttachOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(attach_op_lock);\n      release_operation<false>(available_repl_attach_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_detach_op(ReplDetachOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock d_lock(detach_op_lock);\n      release_operation<false>(available_repl_detach_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_capture_op(ReplTraceCaptureOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock c_lock(capture_op_lock);\n      release_operation<false>(available_repl_capture_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_trace_op(ReplTraceCompleteOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(trace_op_lock);\n      release_operation<false>(available_repl_trace_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_replay_op(ReplTraceReplayOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(replay_op_lock);\n      release_operation<false>(available_repl_replay_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_begin_op(ReplTraceBeginOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(begin_op_lock);\n      release_operation<false>(available_repl_begin_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_repl_summary_op(ReplTraceSummaryOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock t_lock(summary_op_lock);\n      release_operation<false>(available_repl_summary_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_all_reduce_op(AllReduceOp *op)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(all_reduce_op_lock);\n      release_operation<false>(available_all_reduce_ops, op);\n    }\n\n    //--------------------------------------------------------------------------\n    RegionTreeContext Runtime::allocate_region_tree_context(void)\n    //--------------------------------------------------------------------------\n    {\n      // Try getting something off the list of available contexts\n      AutoLock ctx_lock(context_lock);\n      if (!available_contexts.empty())\n      {\n        RegionTreeContext result = available_contexts.front();\n        available_contexts.pop_front();\n        return result;\n      }\n      // If we failed to get a context, double the number of total \n      // contexts and then update the forest nodes to have the right\n      // number of contexts available\n      RegionTreeContext result(total_contexts);\n      for (unsigned idx = 1; idx < total_contexts; idx++)\n        available_contexts.push_back(RegionTreeContext(total_contexts+idx));\n      // Mark that we doubled the total number of contexts\n      // Very important that we do this before calling the\n      // RegionTreeForest's resize method!\n      total_contexts *= 2;\n#ifdef DEBUG_LEGION\n      assert(!available_contexts.empty());\n#endif\n      // Tell all the processor managers about the additional contexts\n      for (std::map<Processor,ProcessorManager*>::const_iterator it = \n            proc_managers.begin(); it != proc_managers.end(); it++)\n      {\n        it->second->update_max_context_count(total_contexts); \n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::free_region_tree_context(RegionTreeContext context)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(context.exists());\n      forest->check_context_state(context);\n#endif\n      AutoLock ctx_lock(context_lock);\n      available_contexts.push_back(context);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_local_context(UniqueID context_uid,InnerContext *ctx)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      // sanity check\n      assert((context_uid % total_address_spaces) == address_space); \n#endif\n      AutoLock ctx_lock(context_lock);\n#ifdef DEBUG_LEGION\n      assert(local_contexts.find(context_uid) == local_contexts.end());\n#endif\n      local_contexts[context_uid] = ctx;\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::unregister_local_context(UniqueID context_uid)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      // sanity check\n      assert((context_uid % total_address_spaces) == address_space); \n#endif\n      AutoLock ctx_lock(context_lock);\n      std::map<UniqueID,InnerContext*>::iterator finder = \n        local_contexts.find(context_uid);\n#ifdef DEBUG_LEGION\n      assert(finder != local_contexts.end());\n#endif\n      local_contexts.erase(finder);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_remote_context(UniqueID context_uid, \n                       RemoteContext *context, std::set<RtEvent> &preconditions)\n    //--------------------------------------------------------------------------\n    {\n      RtUserEvent to_trigger;\n      {\n        AutoLock ctx_lock(context_lock);\n        std::map<UniqueID,std::pair<RtUserEvent,RemoteContext*> >::iterator \n          finder = pending_remote_contexts.find(context_uid);\n#ifdef DEBUG_LEGION\n        assert(remote_contexts.find(context_uid) == remote_contexts.end());\n        assert(finder != pending_remote_contexts.end());\n#endif\n        to_trigger = finder->second.first;\n        pending_remote_contexts.erase(finder);\n        remote_contexts[context_uid] = context; \n      }\n#ifdef DEBUG_LEGION\n      assert(to_trigger.exists());\n#endif\n      if (!preconditions.empty())\n        Runtime::trigger_event(to_trigger,Runtime::merge_events(preconditions));\n      else\n        Runtime::trigger_event(to_trigger);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::unregister_remote_context(UniqueID context_uid)\n    //--------------------------------------------------------------------------\n    {\n      RemoteContext *context = NULL;\n      {\n        AutoLock ctx_lock(context_lock);\n        std::map<UniqueID,RemoteContext*>::iterator finder = \n          remote_contexts.find(context_uid);\n#ifdef DEBUG_LEGION\n        assert(finder != remote_contexts.end());\n#endif\n        context = finder->second;\n        remote_contexts.erase(finder);\n      }\n      // Remove our reference and delete it if we're done with it\n      if (context->remove_reference())\n        delete context;\n    }\n\n    //--------------------------------------------------------------------------\n    InnerContext* Runtime::find_context(UniqueID context_uid,\n                                      bool return_null_if_not_found /*=false*/,\n                                      RtEvent *wait_for /*=NULL*/)\n    //--------------------------------------------------------------------------\n    {\n      RtEvent wait_on;\n      RtUserEvent ready_event;\n      RemoteContext *result = NULL;\n      {\n        // Need exclusive permission since we might mutate stuff\n        AutoLock ctx_lock(context_lock,1,false/*exclusive*/);\n        // See if it is local first\n        std::map<UniqueID,InnerContext*>::const_iterator\n          local_finder = local_contexts.find(context_uid);\n        if (local_finder != local_contexts.end())\n          return local_finder->second;\n        // Now see if it is remote\n        std::map<UniqueID,RemoteContext*>::const_iterator\n          remote_finder = remote_contexts.find(context_uid);\n        if (remote_finder != remote_contexts.end())\n          return remote_finder->second;\n        // If we don't have it, see if we should send the response or not\n        std::map<UniqueID,\n                 std::pair<RtUserEvent,RemoteContext*> >::const_iterator \n          pending_finder = pending_remote_contexts.find(context_uid);\n        if (pending_finder != pending_remote_contexts.end())\n        {\n          if (wait_for != NULL)\n          {\n            *wait_for = pending_finder->second.first;\n            return pending_finder->second.second;\n          }\n          else\n          {\n            wait_on = pending_finder->second.first;\n            result = pending_finder->second.second;\n          }\n        } else if (return_null_if_not_found)\n          // If its not here and we are supposed to return null do that\n          return NULL;\n      }\n      if (result == NULL)\n      {\n        // Make a remote context here in case we need to request it, \n        // we can't make it while holding the lock\n        RemoteContext *temp = new RemoteContext(this, context_uid);\n        // Add a reference to the newly created context\n        temp->add_reference();\n        InnerContext *local_result = NULL;\n        // Use a do while (false) loop here for easy breaks\n        do \n        { \n          // Retake the lock in exclusive mode and see if we lost the race\n          AutoLock ctx_lock(context_lock);\n          // See if it is local first\n          std::map<UniqueID,InnerContext*>::const_iterator\n            local_finder = local_contexts.find(context_uid);\n          if (local_finder != local_contexts.end())\n          {\n            // Need to jump to end to avoid leaking memory with temp\n            local_result = local_finder->second;\n            break;\n          }\n          // Now see if it is remote\n          std::map<UniqueID,RemoteContext*>::const_iterator\n            remote_finder = remote_contexts.find(context_uid);\n          if (remote_finder != remote_contexts.end())\n          {\n            // Need to jump to end to avoid leaking memory with temp\n            local_result = remote_finder->second;\n            break;\n          }\n          // If we don't have it, see if we should send the response or not\n          std::map<UniqueID,\n                   std::pair<RtUserEvent,RemoteContext*> >::const_iterator \n            pending_finder = pending_remote_contexts.find(context_uid);\n          if (pending_finder == pending_remote_contexts.end())\n          {\n#ifdef DEBUG_LEGION\n            assert(!return_null_if_not_found);\n#endif\n            // Make an event to trigger for when we are done\n            ready_event = Runtime::create_rt_user_event();\n            pending_remote_contexts[context_uid] = \n              std::pair<RtUserEvent,RemoteContext*>(ready_event, temp); \n            result = temp;\n            // Add a result that will be removed when the response\n            // message comes back from the owner, this also prevents\n            // temp from being deleted at the end of this block\n            result->add_reference();\n          }\n          else // if we're going to have it we might as well wait\n          {\n            if (wait_for != NULL)\n            {\n              *wait_for = pending_finder->second.first;\n              local_result = pending_finder->second.second;\n              // Need to continue to end to avoid leaking memory with temp\n            }\n            else\n            {\n              wait_on = pending_finder->second.first;\n              result = pending_finder->second.second;\n            }\n          }\n        } while (false); // only go through this block once\n        if (temp->remove_reference())\n          delete temp;\n        if (local_result != NULL)\n          return local_result;\n      }\n#ifdef DEBUG_LEGION\n      assert(result != NULL);\n#endif\n      // If there is no wait event, we have to send the message\n      if (!wait_on.exists())\n      {\n#ifdef DEBUG_LEGION\n        assert(ready_event.exists());\n#endif\n        // We have to send the message\n        // Figure out the target\n        const AddressSpaceID target = get_runtime_owner(context_uid);\n#ifdef DEBUG_LEGION\n        assert(target != address_space);\n#endif\n        // Send the message\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(context_uid);\n          rez.serialize(result);\n        }\n        send_remote_context_request(target, rez); \n        if (wait_for != NULL)\n        {\n          *wait_for = ready_event;\n          return result;\n        }\n        else\n        {\n          // Wait for it to be ready\n          ready_event.wait();\n          // We already know the answer cause we sent the message\n          return result;\n        }\n      }\n      else\n      {\n        // Can't wait in some cases\n        if (return_null_if_not_found && !wait_on.has_triggered())\n          return NULL;\n        // We wait for the results to be ready\n        wait_on.wait();\n        return result;\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_shard_manager(ReplicationID repl_id, \n                                         ShardManager *manager)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(shard_lock);\n#ifdef DEBUG_LEGION\n      assert(shard_managers.find(repl_id) == shard_managers.end());\n#endif\n      shard_managers[repl_id] = manager;\n    }\n    \n    //--------------------------------------------------------------------------\n    void Runtime::unregister_shard_manager(\n                                         ReplicationID repl_id, bool reclaim_id)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(shard_lock);\n      std::map<ReplicationID,ShardManager*>::iterator\n        finder = shard_managers.find(repl_id);\n#ifdef DEBUG_LEGION\n      assert(finder != shard_managers.end());\n#endif\n      shard_managers.erase(finder);\n    }\n\n    //--------------------------------------------------------------------------\n    ShardManager* Runtime::find_shard_manager(ReplicationID repl_id, \n                                              bool can_fail)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(shard_lock,1,false/*exclusive*/);\n      std::map<ReplicationID,ShardManager*>::const_iterator\n        finder = shard_managers.find(repl_id);\n      if (finder == shard_managers.end())\n      {\n        if (can_fail)\n          return NULL;\n        assert(false); // Should never get here\n      }\n      return finder->second;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_local(Processor proc) const\n    //--------------------------------------------------------------------------\n    {\n      return (local_procs.find(proc) != local_procs.end());\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::is_visible_memory(Processor proc, Memory memory)\n    //--------------------------------------------------------------------------\n    {\n      // If we cached it locally for our processors, then just go\n      // ahead and get the result\n      std::map<Processor,ProcessorManager*>::const_iterator finder = \n        proc_managers.find(proc);\n      if (finder != proc_managers.end())\n        return finder->second->is_visible_memory(memory);\n      // Otherwise look up the result\n      Machine::MemoryQuery visible_memories(machine);\n      // Have to handle the case where this is a processor group\n      if (proc.kind() == Processor::PROC_GROUP)\n      {\n        std::vector<Processor> group_members;\n        proc.get_group_members(group_members);\n        for (std::vector<Processor>::const_iterator it =\n              group_members.begin(); it != group_members.end(); it++)\n          visible_memories.has_affinity_to(*it);\n      }\n      else\n        visible_memories.has_affinity_to(proc);\n      for (Machine::MemoryQuery::iterator it =\n            visible_memories.begin(); it != visible_memories.end(); it++)\n        if ((*it) == memory)\n          return true;\n      return false;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::find_visible_memories(Processor proc, \n                                        std::set<Memory> &visible)\n    //--------------------------------------------------------------------------\n    {\n      // If we cached it locally for our processors, then just go\n      // ahead and get the result\n      std::map<Processor,ProcessorManager*>::const_iterator finder = \n        proc_managers.find(proc);\n      if (finder != proc_managers.end())\n      {\n        finder->second->find_visible_memories(visible);\n        return;\n      }\n      // Otherwise look up the result\n      Machine::MemoryQuery visible_memories(machine);\n      // Have to handle the case where this is a processor group\n      if (proc.kind() == Processor::PROC_GROUP)\n      {\n        std::vector<Processor> group_members;\n        proc.get_group_members(group_members);\n        for (std::vector<Processor>::const_iterator it = \n              group_members.begin(); it != group_members.end(); it++)\n          visible_memories.has_affinity_to(*it);\n      }\n      else\n        visible_memories.has_affinity_to(proc);\n      for (Machine::MemoryQuery::iterator it = visible_memories.begin();\n            it != visible_memories.end(); it++)\n        visible.insert(*it);\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpaceID Runtime::get_unique_index_space_id(void)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceID result = __sync_fetch_and_add(&unique_index_space_id,\n                                                 runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      // If we have overflow on the number of partitions created\n      // then we are really in a bad place.\n      assert(result <= unique_index_space_id); \n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexPartitionID Runtime::get_unique_index_partition_id(void)\n    //--------------------------------------------------------------------------\n    {\n      IndexPartitionID result = __sync_fetch_and_add(&unique_index_partition_id,\n                                                     runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      // If we have overflow on the number of partitions created\n      // then we are really in a bad place.\n      assert(result <= unique_index_partition_id); \n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    FieldSpaceID Runtime::get_unique_field_space_id(void)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpaceID result = __sync_fetch_and_add(&unique_field_space_id,\n                                                 runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      // If we have overflow on the number of field spaces\n      // created then we are really in a bad place.\n      assert(result <= unique_field_space_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexTreeID Runtime::get_unique_index_tree_id(void)\n    //--------------------------------------------------------------------------\n    {\n      IndexTreeID result = __sync_fetch_and_add(&unique_index_tree_id,\n                                                runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      // If we have overflow on the number of region trees\n      // created then we are really in a bad place.\n      assert(result <= unique_index_tree_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    RegionTreeID Runtime::get_unique_region_tree_id(void)\n    //--------------------------------------------------------------------------\n    {\n      RegionTreeID result = __sync_fetch_and_add(&unique_region_tree_id,\n                                                 runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      // If we have overflow on the number of region trees\n      // created then we are really in a bad place.\n      assert(result <= unique_region_tree_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    UniqueID Runtime::get_unique_operation_id(void)\n    //--------------------------------------------------------------------------\n    {\n      UniqueID result = __sync_fetch_and_add(&unique_operation_id,\n                                             runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      assert(result <= unique_operation_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    FieldID Runtime::get_unique_field_id(void)\n    //--------------------------------------------------------------------------\n    {\n      FieldID result = __sync_fetch_and_add(&unique_field_id,\n                                            runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      assert(result <= unique_field_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    CodeDescriptorID Runtime::get_unique_code_descriptor_id(void)\n    //--------------------------------------------------------------------------\n    {\n      CodeDescriptorID result = __sync_fetch_and_add(&unique_code_descriptor_id,\n                                                     runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      assert(result <= unique_code_descriptor_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraintID Runtime::get_unique_constraint_id(void)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraintID result = __sync_fetch_and_add(&unique_constraint_id,\n                                                       runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      assert(result <= unique_constraint_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpaceExprID Runtime::get_unique_index_space_expr_id(void)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpaceExprID result = __sync_fetch_and_add(&unique_is_expr_id,\n                                                     runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      assert(result <= unique_is_expr_id);\n#endif\n      return result;\n    }\n\n#ifdef LEGION_SPY\n    //--------------------------------------------------------------------------\n    unsigned Runtime::get_unique_indirections_id(void)\n    //--------------------------------------------------------------------------\n    {\n      unsigned result = __sync_fetch_and_add(&unique_indirections_id,\n                                             runtime_stride);\n#ifdef DEBUG_LEGION\n      // check for overflow\n      assert(result <= unique_indirections_id);\n#endif\n      return result;\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    ReplicationID Runtime::get_unique_replication_id(void)\n    //--------------------------------------------------------------------------\n    {\n      ReplicationID result = \n        __sync_fetch_and_add(&unique_control_replication_id, runtime_stride);\n#ifdef DEBUG_LEGION\n      assert(result <= unique_control_replication_id);\n#endif\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    LegionErrorType Runtime::verify_requirement(\n                               const RegionRequirement &req, FieldID &bad_field)\n    //--------------------------------------------------------------------------\n    {\n      FieldSpace sp = (req.handle_type == LEGION_SINGULAR_PROJECTION) \n                      || (req.handle_type == LEGION_REGION_PROJECTION)\n                        ? req.region.field_space : req.partition.field_space;\n      // First make sure that all the privilege fields are valid for\n      // the given field space of the region or partition\n      for (std::set<FieldID>::const_iterator it = req.privilege_fields.begin();\n            it != req.privilege_fields.end(); it++)\n      {\n        if (!forest->has_field(sp, *it))\n        {\n          bad_field = *it;\n          return ERROR_FIELD_SPACE_FIELD_MISMATCH;\n        }\n      }\n      // Make sure that the requested node is a valid request\n      if ((req.handle_type == LEGION_SINGULAR_PROJECTION) || \n          (req.handle_type == LEGION_REGION_PROJECTION))\n      {\n        if (!forest->has_node(req.region))\n          return ERROR_INVALID_REGION_HANDLE;\n        if (req.region.get_tree_id() != req.parent.get_tree_id())\n          return ERROR_INVALID_REGION_HANDLE;\n      }\n      else\n      {\n        if (!forest->has_node(req.partition))\n          return ERROR_INVALID_PARTITION_HANDLE;\n        if (req.partition.get_tree_id() != req.parent.get_tree_id())\n          return ERROR_INVALID_PARTITION_HANDLE;\n      }\n\n      // Then check that any instance fields are included in the privilege \n      // fields.  Make sure that there are no duplicates in the instance fields\n      std::set<FieldID> inst_duplicates;\n      for (std::vector<FieldID>::const_iterator it = \n            req.instance_fields.begin(); it != \n            req.instance_fields.end(); it++)\n      {\n        if (req.privilege_fields.find(*it) == req.privilege_fields.end())\n        {\n          bad_field = *it;\n          return ERROR_INVALID_INSTANCE_FIELD;\n        }\n        if (inst_duplicates.find(*it) != inst_duplicates.end())\n        {\n          bad_field = *it;\n          return ERROR_DUPLICATE_INSTANCE_FIELD;\n        }\n        inst_duplicates.insert(*it);\n      }\n\n      // If this is a projection requirement and the child region selected will \n      // need to be in exclusive mode then the partition must be disjoint\n      if ((req.handle_type == LEGION_PARTITION_PROJECTION) && \n          (IS_WRITE(req)))\n      {\n        if (!forest->is_disjoint(req.partition))\n          return ERROR_NON_DISJOINT_PARTITION;\n      }\n\n      // Made it here, then there is no error\n      return NO_ERROR;\n    }\n\n    //--------------------------------------------------------------------------\n    Future Runtime::help_create_future(ApEvent complete_event, \n                                       Operation *op /*= NULL*/)\n    //--------------------------------------------------------------------------\n    {\n      return Future(new FutureImpl(this, true/*register*/,\n                                   get_available_distributed_id(),address_space, \n                                   complete_event,op,false/*get coordinates*/));\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::help_reset_future(const Future &f)\n    //--------------------------------------------------------------------------\n    {\n      return f.impl->reset_future();\n    }\n\n    //--------------------------------------------------------------------------\n    IndexSpace Runtime::help_create_index_space_handle(TypeTag type_tag)\n    //--------------------------------------------------------------------------\n    {\n      IndexSpace handle(get_unique_index_space_id(),\n                        get_unique_index_tree_id(), type_tag);\n      return handle;\n    }\n\n    //--------------------------------------------------------------------------\n    unsigned Runtime::generate_random_integer(void)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock r_lock(random_lock);\n      unsigned result = nrand48(random_state);\n      return result;\n    }\n\n#ifdef TRACE_ALLOCATION \n    //--------------------------------------------------------------------------\n    void Runtime::trace_allocation(AllocationType type, size_t size, int elems)\n    //--------------------------------------------------------------------------\n    {\n      if (prepared_for_shutdown)\n        return;\n      AutoLock a_lock(allocation_lock);\n      std::map<AllocationType,AllocationTracker>::iterator finder = \n        allocation_manager.find(type);\n      size_t alloc_size = size * elems;\n      finder->second.total_allocations += elems;\n      finder->second.total_bytes += alloc_size;\n      finder->second.diff_allocations += elems;\n      finder->second.diff_bytes += alloc_size;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::trace_free(AllocationType type, size_t size, int elems)\n    //--------------------------------------------------------------------------\n    {\n      if (prepared_for_shutdown)\n        return;\n      AutoLock a_lock(allocation_lock);\n      std::map<AllocationType,AllocationTracker>::iterator finder = \n        allocation_manager.find(type);\n      size_t free_size = size * elems;\n      finder->second.total_allocations -= elems;\n      finder->second.total_bytes -= free_size;\n      finder->second.diff_allocations -= elems;\n      finder->second.diff_bytes -= free_size;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::dump_allocation_info(void)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock a_lock(allocation_lock);\n      for (std::map<AllocationType,AllocationTracker>::iterator it = \n            allocation_manager.begin(); it != allocation_manager.end(); it++)\n      {\n        // Skip anything that is empty\n        if (it->second.total_allocations == 0)\n          continue;\n        // Skip anything that hasn't changed\n        if (it->second.diff_allocations == 0)\n          continue;\n        log_allocation.info(\"%s on %d: \"\n            \"total=%d total_bytes=%ld diff=%d diff_bytes=%lld\",\n            get_allocation_name(it->first), address_space,\n            it->second.total_allocations, it->second.total_bytes,\n            it->second.diff_allocations, (long long int)it->second.diff_bytes);\n        it->second.diff_allocations = 0;\n        it->second.diff_bytes = 0;\n      }\n      log_allocation.info(\" \");\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ const char* Runtime::get_allocation_name(AllocationType type)\n    //--------------------------------------------------------------------------\n    {\n      switch (type)\n      {\n        case ARGUMENT_MAP_ALLOC:\n          return \"Argument Map\";\n        case ARGUMENT_MAP_STORE_ALLOC:\n          return \"Argument Map Store\";\n        case STORE_ARGUMENT_ALLOC:\n          return \"Store Argument\";\n        case MPI_HANDSHAKE_ALLOC:\n          return \"MPI Handshake\";\n        case GRANT_ALLOC:\n          return \"Grant\";\n        case FUTURE_ALLOC:\n          return \"Future\";\n        case FUTURE_MAP_ALLOC:\n          return \"Future Map\";\n        case PHYSICAL_REGION_ALLOC:\n          return \"Physical Region\";\n        case STATIC_TRACE_ALLOC:\n          return \"Static Trace\";\n        case DYNAMIC_TRACE_ALLOC:\n          return \"Dynamic Trace\";\n        case ALLOC_MANAGER_ALLOC:\n          return \"Allocation Manager\";\n        case ALLOC_INTERNAL_ALLOC:\n          return \"Allocation Internal\";\n        case TASK_ARGS_ALLOC:\n          return \"Task Arguments\";\n        case REDUCTION_ALLOC:\n          return \"Reduction Result\"; \n        case PREDICATE_ALLOC:\n          return \"Default Predicate\";\n        case FUTURE_RESULT_ALLOC:\n          return \"Future Result\";\n        case INDIVIDUAL_INST_MANAGER_ALLOC:\n          return \"Individual Manager\";\n        case COLLECTIVE_INST_MANAGER_ALLOC:\n          return \"Collective Manager\";\n        case TREE_CLOSE_ALLOC:\n          return \"Tree Close List\";\n        case TREE_CLOSE_IMPL_ALLOC:\n          return \"Tree Close Impl\";\n        case MATERIALIZED_VIEW_ALLOC:\n          return \"Materialized View\";\n        case REDUCTION_VIEW_ALLOC:\n          return \"Reduction View\";\n        case FILL_VIEW_ALLOC:\n          return \"Fill View\";\n        case PHI_VIEW_ALLOC:\n          return \"Phi View\";\n        case INDIVIDUAL_TASK_ALLOC:\n          return \"Individual Task\";\n        case POINT_TASK_ALLOC:\n          return \"Point Task\";\n        case INDEX_TASK_ALLOC:\n          return \"Index Task\";\n        case SLICE_TASK_ALLOC:\n          return \"Slice Task\";\n        case TOP_TASK_ALLOC:\n          return \"Top Level Task\";\n        case REMOTE_TASK_ALLOC:\n          return \"Remote Task\";\n        case INLINE_TASK_ALLOC:\n          return \"Inline Task\";\n        case MAP_OP_ALLOC:\n          return \"Map Op\";\n        case COPY_OP_ALLOC:\n          return \"Copy Op\";\n        case FENCE_OP_ALLOC:\n          return \"Fence Op\";\n        case FRAME_OP_ALLOC:\n          return \"Frame Op\";\n        case CREATION_OP_ALLOC:\n          return \"Creation Op\";\n        case DELETION_OP_ALLOC:\n          return \"Deletion Op\";\n        case CLOSE_OP_ALLOC:\n          return \"Close Op\";\n        case REFINEMENT_OP_ALLOC:\n          return \"Refinement Op\";\n        case DYNAMIC_COLLECTIVE_OP_ALLOC:\n          return \"Dynamic Collective Op\";\n        case FUTURE_PRED_OP_ALLOC:\n          return \"Future Pred Op\";\n        case NOT_PRED_OP_ALLOC:\n          return \"Not Pred Op\";\n        case AND_PRED_OP_ALLOC:\n          return \"And Pred Op\";\n        case OR_PRED_OP_ALLOC:\n          return \"Or Pred Op\";\n        case ACQUIRE_OP_ALLOC:\n          return \"Acquire Op\";\n        case RELEASE_OP_ALLOC:\n          return \"Release Op\";\n        case TRACE_CAPTURE_OP_ALLOC:\n          return \"Trace Capture Op\";\n        case TRACE_COMPLETE_OP_ALLOC:\n          return \"Trace Complete Op\";\n        case MUST_EPOCH_OP_ALLOC:\n          return \"Must Epoch Op\";\n        case PENDING_PARTITION_OP_ALLOC:\n          return \"Pending Partition Op\";\n        case DEPENDENT_PARTITION_OP_ALLOC:\n          return \"Dependent Partition Op\";\n        case FILL_OP_ALLOC:\n          return \"Fill Op\";\n        case ATTACH_OP_ALLOC:\n          return \"Attach Op\";\n        case DETACH_OP_ALLOC:\n          return \"Detach Op\";\n        case MESSAGE_BUFFER_ALLOC:\n          return \"Message Buffer\";\n        case EXECUTING_CHILD_ALLOC:\n          return \"Executing Children\";\n        case EXECUTED_CHILD_ALLOC:\n          return \"Executed Children\";\n        case COMPLETE_CHILD_ALLOC:\n          return \"Complete Children\";\n        case PHYSICAL_MANAGER_ALLOC:\n          return \"Physical Managers\";\n        case LOGICAL_VIEW_ALLOC:\n          return \"Logical Views\";\n        case LOGICAL_FIELD_VERSIONS_ALLOC:\n          return \"Logical Field Versions\";\n        case LOGICAL_FIELD_STATE_ALLOC:\n          return \"Logical Field States\";\n        case CURR_LOGICAL_ALLOC:\n          return \"Current Logical Users\";\n        case PREV_LOGICAL_ALLOC:\n          return \"Previous Logical Users\";\n        case VERSION_ID_ALLOC:\n          return \"Version IDs\";\n        case LOGICAL_REC_ALLOC:\n          return \"Recorded Logical Users\";\n        case CLOSE_LOGICAL_ALLOC:\n          return \"Close Logical Users\";\n        case VALID_VIEW_ALLOC:\n          return \"Valid Instance Views\";\n        case VALID_REDUCTION_ALLOC:\n          return \"Valid Reduction Views\";\n        case PENDING_UPDATES_ALLOC:\n          return \"Pending Updates\";\n        case LAYOUT_DESCRIPTION_ALLOC:\n          return \"Layout Description\";\n        case PHYSICAL_USER_ALLOC:\n          return \"Physical Users\";\n        case PHYSICAL_VERSION_ALLOC:\n          return \"Physical Versions\";\n        case MEMORY_INSTANCES_ALLOC:\n          return \"Memory Manager Instances\";\n        case MEMORY_GARBAGE_ALLOC:\n          return \"Memory Garbage Instances\";\n        case PROCESSOR_GROUP_ALLOC:\n          return \"Processor Groups\";\n        case RUNTIME_DISTRIBUTED_ALLOC:\n          return \"Runtime Distributed IDs\";\n        case RUNTIME_DIST_COLLECT_ALLOC:\n          return \"Distributed Collectables\";\n        case RUNTIME_GC_EPOCH_ALLOC:\n          return \"Runtime Garbage Collection Epochs\";\n        case RUNTIME_FUTURE_ALLOC:\n          return \"Runtime Futures\";\n        case RUNTIME_REMOTE_ALLOC:\n          return \"Runtime Remote Contexts\";\n        case TASK_INLINE_REGION_ALLOC:\n          return \"Task Inline Regions\";\n        case TASK_TRACES_ALLOC:\n          return \"Task Traces\";\n        case TASK_RESERVATION_ALLOC:\n          return \"Task Reservations\";\n        case TASK_BARRIER_ALLOC:\n          return \"Task Barriers\";\n        case TASK_LOCAL_FIELD_ALLOC:\n          return \"Task Local Fields\";\n        case SEMANTIC_INFO_ALLOC:\n          return \"Semantic Information\";\n        case DIRECTORY_ALLOC:\n          return \"State Directory\";\n        case DENSE_INDEX_ALLOC:\n          return \"Dense Index Set\";\n        case CURRENT_STATE_ALLOC:\n          return \"Current State\";\n        case VERSION_MANAGER_ALLOC:\n          return \"Version Manager\";\n        case PHYSICAL_STATE_ALLOC:\n          return \"Physical State\";\n        case EQUIVALENCE_SET_ALLOC:\n          return \"Equivalence Set\";\n        case AGGREGATE_VERSION_ALLOC:\n          return \"Aggregate Version\";\n        case TASK_IMPL_ALLOC:\n          return \"Task Implementation\";\n        case VARIANT_IMPL_ALLOC:\n          return \"Variant Implementation\";\n        case LAYOUT_CONSTRAINTS_ALLOC:\n          return \"Layout Constraints\";\n        case COPY_FILL_AGGREGATOR_ALLOC:\n          return \"Copy Fill Aggregator\";\n        default:\n          assert(false); // should never get here\n      }\n      return NULL;\n    }\n#endif\n\n#ifdef DEBUG_LEGION\n    //--------------------------------------------------------------------------\n    void Runtime::print_out_individual_tasks(FILE *f, int cnt /*= -1*/)\n    //--------------------------------------------------------------------------\n    {\n      // Build a map of the tasks based on their task IDs\n      // so we can print them out in the order that they were created.\n      // No need to hold the lock because we'll only ever call this\n      // in the debugger.\n      std::map<UniqueID,IndividualTask*> out_tasks;\n      for (std::set<IndividualTask*>::const_iterator it = \n            out_individual_tasks.begin(); it !=\n            out_individual_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::map<UniqueID,IndividualTask*>::const_iterator it = \n            out_tasks.begin(); (it != out_tasks.end()); it++)\n      {\n        ApEvent completion = it->second->get_completion_event();\n        fprintf(f,\"Outstanding Individual Task %lld: %p %s (\" IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id); \n        if (cnt > 0)\n          cnt--;\n        else if (cnt == 0)\n          break;\n      }\n      fflush(f);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::print_out_index_tasks(FILE *f, int cnt /*= -1*/)\n    //--------------------------------------------------------------------------\n    {\n      // Build a map of the tasks based on their task IDs\n      // so we can print them out in the order that they were created.\n      // No need to hold the lock because we'll only ever call this\n      // in the debugger.\n      std::map<UniqueID,IndexTask*> out_tasks;\n      for (std::set<IndexTask*>::const_iterator it = \n            out_index_tasks.begin(); it !=\n            out_index_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::map<UniqueID,IndexTask*>::const_iterator it = \n            out_tasks.begin(); (it != out_tasks.end()); it++)\n      {\n        ApEvent completion = it->second->get_completion_event();\n        fprintf(f,\"Outstanding Index Task %lld: %p %s (\" IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id); \n        if (cnt > 0)\n          cnt--;\n        else if (cnt == 0)\n          break;\n      }\n      fflush(f);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::print_out_slice_tasks(FILE *f, int cnt /*= -1*/)\n    //--------------------------------------------------------------------------\n    {\n      // Build a map of the tasks based on their task IDs\n      // so we can print them out in the order that they were created.\n      // No need to hold the lock because we'll only ever call this\n      // in the debugger.\n      std::map<UniqueID,SliceTask*> out_tasks;\n      for (std::set<SliceTask*>::const_iterator it = \n            out_slice_tasks.begin(); it !=\n            out_slice_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::map<UniqueID,SliceTask*>::const_iterator it = \n            out_tasks.begin(); (it != out_tasks.end()); it++)\n      {\n        ApEvent completion = it->second->get_completion_event();\n        fprintf(f,\"Outstanding Slice Task %lld: %p %s (\" IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id); \n        if (cnt > 0)\n          cnt--;\n        else if (cnt == 0)\n          break;\n      }\n      fflush(f);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::print_out_point_tasks(FILE *f, int cnt /*= -1*/)\n    //--------------------------------------------------------------------------\n    {\n      // Build a map of the tasks based on their task IDs\n      // so we can print them out in the order that they were created.\n      // No need to hold the lock because we'll only ever call this\n      // in the debugger.\n      std::map<UniqueID,PointTask*> out_tasks;\n      for (std::set<PointTask*>::const_iterator it = \n            out_point_tasks.begin(); it !=\n            out_point_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::map<UniqueID,PointTask*>::const_iterator it = \n            out_tasks.begin(); (it != out_tasks.end()); it++)\n      {\n        ApEvent completion = it->second->get_completion_event();\n        fprintf(f,\"Outstanding Point Task %lld: %p %s (\" IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id); \n        if (cnt > 0)\n          cnt--;\n        else if (cnt == 0)\n          break;\n      }\n      fflush(f);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::print_outstanding_tasks(FILE *f, int cnt /*= -1*/)\n    //--------------------------------------------------------------------------\n    {\n      std::map<UniqueID,TaskOp*> out_tasks;\n      for (std::set<IndividualTask*>::const_iterator it = \n            out_individual_tasks.begin(); it !=\n            out_individual_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::set<IndexTask*>::const_iterator it = \n            out_index_tasks.begin(); it !=\n            out_index_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::set<SliceTask*>::const_iterator it = \n            out_slice_tasks.begin(); it !=\n            out_slice_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::set<PointTask*>::const_iterator it = \n            out_point_tasks.begin(); it !=\n            out_point_tasks.end(); it++)\n      {\n        out_tasks[(*it)->get_unique_id()] = *it;\n      }\n      for (std::map<UniqueID,TaskOp*>::const_iterator it = \n            out_tasks.begin(); it != out_tasks.end(); it++)\n      {\n        ApEvent completion = it->second->get_completion_event();\n        switch (it->second->get_task_kind())\n        {\n          case TaskOp::INDIVIDUAL_TASK_KIND:\n            {\n              fprintf(f,\"Outstanding Individual Task %lld: %p %s (\" \n                        IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id);\n              break;\n            }\n          case TaskOp::POINT_TASK_KIND:\n            {\n              fprintf(f,\"Outstanding Point Task %lld: %p %s (\" \n                        IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id);\n              break;\n            }\n          case TaskOp::INDEX_TASK_KIND:\n            {\n              fprintf(f,\"Outstanding Index Task %lld: %p %s (\" \n                        IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id);\n              break;\n            }\n          case TaskOp::SLICE_TASK_KIND:\n            {\n              fprintf(f,\"Outstanding Slice Task %lld: %p %s (\" \n                        IDFMT \")\\n\",\n                it->first, it->second, it->second->get_task_name(),\n                completion.id);\n              break;\n            }\n          default:\n            assert(false);\n        }\n        if (cnt > 0)\n          cnt--;\n        else if (cnt == 0)\n          break;\n      }\n      fflush(f);\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    LayoutConstraintID Runtime::register_layout(\n                                const LayoutConstraintRegistrar &registrar,\n                                LayoutConstraintID layout_id, DistributedID did)\n    //--------------------------------------------------------------------------\n    {\n      if (layout_id == LEGION_AUTO_GENERATE_ID)\n        layout_id = get_unique_constraint_id();\n      // Now make our entry and then return the result\n      LayoutConstraints *constraints = \n        new LayoutConstraints(layout_id, this, registrar,false/*internal*/,did);\n      if (register_layout(constraints, NULL/*mutator*/))\n      {\n        // These constraints are available on all the nodes so if we own\n        // them then record that we have remote instances for everything else\n        if ((did > 0) && constraints->is_owner())\n        {\n          for (AddressSpaceID space = 0; space < total_address_spaces; space++)\n            if (space != address_space)\n              constraints->update_remote_instances(space);\n        }\n      }\n      else\n        // If someone else already registered this ID then we delete our object\n        delete constraints;\n      return layout_id;\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints* Runtime::register_layout(FieldSpace handle,\n                                 const LayoutConstraintSet &cons, bool internal)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints *constraints = new LayoutConstraints(\n          get_unique_constraint_id(), this, cons, handle, internal);\n      register_layout(constraints, NULL/*mutator*/);\n      return constraints;\n    }\n\n    //--------------------------------------------------------------------------\n    bool Runtime::register_layout(LayoutConstraints *new_constraints,\n                                  ReferenceMutator *mutator)\n    //--------------------------------------------------------------------------\n    {\n      new_constraints->add_base_resource_ref(RUNTIME_REF);\n      // If we're not internal and we're the owner then we also\n      // add an application reference to prevent early collection\n      if (!new_constraints->internal && new_constraints->is_owner())\n        new_constraints->add_base_gc_ref(APPLICATION_REF);\n      AutoLock l_lock(layout_constraints_lock);\n      std::map<LayoutConstraintID,LayoutConstraints*>::const_iterator finder =\n        layout_constraints_table.find(new_constraints->layout_id);\n      if (finder != layout_constraints_table.end())\n        return false;\n      layout_constraints_table[new_constraints->layout_id] = new_constraints;\n      // Remove any pending requests\n      pending_constraint_requests.erase(new_constraints->layout_id);\n      // Now we can do the registration with the runtime\n      new_constraints->register_with_runtime(mutator);\n      return true;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::release_layout(LayoutConstraintID layout_id)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints *constraints = find_layout_constraints(layout_id);\n#ifdef DEBUG_LEGION\n      assert(!constraints->internal);\n#endif\n      // Check to see if this is the owner\n      if (constraints->is_owner())\n      {\n        if (constraints->remove_base_gc_ref(APPLICATION_REF))\n          delete constraints;\n      }\n      else\n      {\n        // Send a message to the owner asking it to do the release\n        Serializer rez;\n        {\n          RezCheck z(rez);\n          rez.serialize(layout_id);\n        }\n        send_constraint_release(constraints->owner_space, rez);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::unregister_layout(LayoutConstraintID layout_id)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints *constraints = NULL;\n      {\n        AutoLock l_lock(layout_constraints_lock);\n        std::map<LayoutConstraintID,LayoutConstraints*>::iterator finder = \n          layout_constraints_table.find(layout_id);\n        if (finder != layout_constraints_table.end())\n        {\n          constraints = finder->second;\n          layout_constraints_table.erase(finder);\n        }\n      }\n      if ((constraints != NULL) && \n          constraints->remove_base_resource_ref(RUNTIME_REF))\n        delete (constraints);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ LayoutConstraintID Runtime::preregister_layout(\n                                     const LayoutConstraintRegistrar &registrar,\n                                     LayoutConstraintID layout_id)\n    //--------------------------------------------------------------------------\n    { \n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'preregister_layout' after \"\n                      \"the runtime has started!\");\n      std::map<LayoutConstraintID,LayoutConstraintRegistrar> \n        &pending_constraints = get_pending_constraint_table();\n      // See if we have to generate an ID\n      if (layout_id == LEGION_AUTO_GENERATE_ID)\n      {\n        // Find the first available layout ID\n        layout_id = 1;\n        for (std::map<LayoutConstraintID,LayoutConstraintRegistrar>::\n              const_iterator it = pending_constraints.begin(); \n              it != pending_constraints.end(); it++)\n        {\n          if (layout_id != it->first)\n          {\n            // We've found a free one, so we can use it\n            break;\n          }\n          else\n            layout_id++;\n        }\n      }\n      else\n      {\n        if (layout_id == 0)\n          REPORT_LEGION_ERROR(ERROR_RESERVED_CONSTRAINT_ID, \n                        \"Illegal use of reserved constraint ID 0\");\n        // Check to make sure it is not already used\n        std::map<LayoutConstraintID,LayoutConstraintRegistrar>::const_iterator\n          finder = pending_constraints.find(layout_id);\n        if (finder != pending_constraints.end())\n          REPORT_LEGION_ERROR(ERROR_DUPLICATE_CONSTRAINT_ID, \n                        \"Duplicate use of constraint ID %ld\", layout_id);\n      }\n      pending_constraints[layout_id] = registrar;\n      return layout_id;\n    }\n\n    //--------------------------------------------------------------------------\n    FieldSpace Runtime::get_layout_constraint_field_space(\n                                                   LayoutConstraintID layout_id)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints *constraints = find_layout_constraints(layout_id);\n      return constraints->get_field_space();\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::get_layout_constraints(LayoutConstraintID layout_id,\n                                        LayoutConstraintSet &layout_constraints)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints *constraints = find_layout_constraints(layout_id);\n      layout_constraints = *constraints;\n    }\n\n    //--------------------------------------------------------------------------\n    const char* Runtime::get_layout_constraints_name(\n                                                   LayoutConstraintID layout_id)\n    //--------------------------------------------------------------------------\n    {\n      LayoutConstraints *constraints = find_layout_constraints(layout_id);\n      return constraints->get_name();\n    }\n\n    //--------------------------------------------------------------------------\n    LayoutConstraints* Runtime::find_layout_constraints(\n                      LayoutConstraintID layout_id, bool can_fail /*= false*/, \n                      RtEvent *wait_for /*=NULL*/)\n    //--------------------------------------------------------------------------\n    {\n      // See if we can find it first\n      RtEvent wait_on;\n      {\n        AutoLock l_lock(layout_constraints_lock);\n        std::map<LayoutConstraintID,LayoutConstraints*>::const_iterator\n          finder = layout_constraints_table.find(layout_id);\n        if (finder != layout_constraints_table.end())\n        {\n          return finder->second;\n        }\n        else\n        {\n          // See if a request has already been issued\n          std::map<LayoutConstraintID,RtEvent>::const_iterator\n            wait_on_finder = pending_constraint_requests.find(layout_id);\n          if (can_fail || \n              (wait_on_finder == pending_constraint_requests.end()))\n          {\n            // Ask for the constraints\n            AddressSpaceID target = \n              LayoutConstraints::get_owner_space(layout_id, this); \n            RtUserEvent to_trigger = Runtime::create_rt_user_event();\n            Serializer rez;\n            {\n              RezCheck z(rez);\n              rez.serialize(layout_id);\n              rez.serialize(to_trigger);\n              rez.serialize(can_fail);\n            }\n            // Send the message\n            send_constraint_request(target, rez);\n            // Only save the event to wait on if this can't fail\n            if (!can_fail)\n              pending_constraint_requests[layout_id] = to_trigger;\n            wait_on = to_trigger;\n          }\n          else\n            wait_on = wait_on_finder->second;\n        }\n      }\n      // If we want the wait event, just return\n      if (wait_for != NULL)\n      {\n        *wait_for = wait_on;\n        return NULL;\n      }\n      // If we didn't find it send a remote request for the constraints\n      wait_on.wait();\n      // When we wake up, the result should be there\n      AutoLock l_lock(layout_constraints_lock);\n      std::map<LayoutConstraintID,LayoutConstraints*>::const_iterator\n          finder = layout_constraints_table.find(layout_id);\n      if (finder == layout_constraints_table.end())\n      {\n        if (can_fail)\n          return NULL;\n#ifdef DEBUG_LEGION\n        assert(finder != layout_constraints_table.end());\n#endif\n      }\n      return finder->second;\n    }\n\n    /*static*/ TaskID Runtime::legion_main_id = 0;\n    /*static*/ MapperID Runtime::legion_main_mapper_id = 0;\n    /*static*/ bool Runtime::legion_main_set = false;\n    /*static*/ bool Runtime::runtime_initialized = false;\n    /*static*/ bool Runtime::runtime_started = false;\n    /*static*/ bool Runtime::runtime_backgrounded = false;\n    /*static*/ Runtime* Runtime::the_runtime = NULL;\n    /*static*/ RtUserEvent Runtime::runtime_started_event = \n                                              RtUserEvent::NO_RT_USER_EVENT;\n    /*static*/ int Runtime::background_waits = 0;\n    /*static*/ int Runtime::return_code = 0;\n    /*static*/ int Runtime::mpi_rank = -1;\n\n    //--------------------------------------------------------------------------\n    /*static*/ int Runtime::start(int argc, char **argv, bool background,\n                                  bool supply_default_mapper)\n    //--------------------------------------------------------------------------\n    {\n      // Some static asserts that need to hold true for the runtime to work\n      LEGION_STATIC_ASSERT(LEGION_MAX_RETURN_SIZE > 0, \n          \"Need a positive and non-zero value for LEGION_MAX_RETURN_SIZE\");\n      LEGION_STATIC_ASSERT((1 << LEGION_FIELD_LOG2) == LEGION_MAX_FIELDS,\n          \"LEGION_MAX_FIELDS must be a pwoer of 2\");\n      LEGION_STATIC_ASSERT(LEGION_MAX_NUM_NODES > 0,\n          \"Need a positive and non-zero value for LEGION_MAX_NUM_NODES\");\n      LEGION_STATIC_ASSERT(LEGION_MAX_NUM_PROCS > 0,\n          \"Need a positive and non-zero value for LEGION_MAX_NUM_PROCS\");\n      LEGION_STATIC_ASSERT(LEGION_DEFAULT_MAX_TASK_WINDOW > 0,\n          \"Need a positive and non-zero value for \"\n          \"LEGION_DEFAULT_MAX_TASK_WINDOW\");\n      LEGION_STATIC_ASSERT(LEGION_DEFAULT_MIN_TASKS_TO_SCHEDULE > 0,\n          \"Need a positive and non-zero value for \"\n          \"LEGION_DEFAULT_MIN_TASKS_TO_SCHEDULE\");\n      LEGION_STATIC_ASSERT(LEGION_DEFAULT_MAX_MESSAGE_SIZE > 0,\n          \"Need a positive and non-zero value for \"\n          \"LEGION_DEFAULT_MAX_MESSAGE_SIZE\"); \n#ifdef LEGION_SPY\n      LEGION_STATIC_ASSERT(\n          Realm::Logger::REALM_LOGGING_MIN_LEVEL <= Realm::Logger::LEVEL_INFO,\n        \"Legion Spy requires a COMPILE_TIME_MIN_LEVEL of at most LEVEL_INFO.\");\n#endif\n#ifdef LEGION_GC\n      LEGION_STATIC_ASSERT(\n          Realm::Logger::REALM_LOGGING_MIN_LEVEL <= Realm::Logger::LEVEL_INFO,\n          \"Legion GC requires a COMPILE_TIME_MIN_LEVEL of at most LEVEL_INFO.\");\n#endif\n#ifdef DEBUG_SHUTDOWN_HANG\n      LEGION_STATIC_ASSERT(\n          Realm::Logger::REALM_LOGGING_MIN_LEVEL <= Realm::Logger::LEVEL_INFO,\n          \"DEBUG_SHUTDOWN_HANG requires a COMPILE_TIME_MIN_LEVEL \"\n          \"of at most LEVEL_INFO.\");\n#endif\n\n      // Register builtin reduction operators\n      register_builtin_reduction_operators();\n\n      // Need to pass argc and argv to low-level runtime before we can record \n      // their values as they might be changed by GASNet or MPI or whatever.\n      // Note that the logger isn't initialized until after this call returns \n      // which means any logging that occurs before this has undefined behavior.\n      const LegionConfiguration &config = initialize(&argc, &argv, false);\n      RealmRuntime realm = RealmRuntime::get_runtime();\n\n      // Perform any waits that the user requested before starting\n      if (config.delay_start > 0)\n          sleep(config.delay_start);\n      // Check for any slow configurations\n      if (!config.slow_config_ok)\n        perform_slow_config_checks(config);\n      // Configure legion spy if necessary\n      if (config.legion_spy_enabled)\n        LegionSpy::log_legion_spy_config();\n      // Configure MPI Interoperability\n      const std::vector<LegionHandshake> &pending_handshakes =\n        get_pending_handshake_table();\n      if ((mpi_rank >= 0) || (!pending_handshakes.empty()))\n        configure_interoperability(config.separate_runtime_instances);\n      // Construct our runtime objects \n      Processor::Kind startup_kind = Processor::NO_KIND;\n      const RtEvent tasks_registered = configure_runtime(argc, argv,\n          config, realm, startup_kind, background, supply_default_mapper);\n#ifdef DEBUG_LEGION\n      // Startup kind should be a CPU or a Utility processor\n      assert((startup_kind == Processor::LOC_PROC) ||\n              (startup_kind == Processor::UTIL_PROC));\n#endif\n      // We have to set these prior to starting Realm as once we start\n      // Realm it might fork child processes so they all need to see\n      // the same values for these static variables\n      runtime_started = true;\n      runtime_backgrounded = background;\n      // Make a user event that we will trigger once we the \n      // startup task is done. If we're node 0 then we will use this\n      // as the precondition for launching the top-level task\n      runtime_started_event = Runtime::create_rt_user_event();\n\n      // Now that we have everything setup we can tell Realm to\n      // start the processors. It is at this point which fork\n      // can be called to spawn subprocesses.\n      realm.start();\n\n      // First we issue a \"barrier\" NOP task that runs on all the\n      // Realm processors to make sure that Realm is initialized\n      const RtEvent realm_initialized(realm.collective_spawn_by_kind(\n            Processor::NO_KIND, 0/*NOP*/, NULL, 0, false/*one per node*/));\n\n      // Now we initialize all the runtimes so that they are ready\n      // to begin execution. Note this also acts as a barrier across\n      // the machine to ensure that nobody does anything related to\n      // startup until all the runtimes are initialized everywhere\n      const RtEvent legion_initialized(realm.collective_spawn_by_kind(\n            (config.separate_runtime_instances ? Processor::NO_KIND :\n             startup_kind), LG_INITIALIZE_TASK_ID, NULL, 0,\n            !config.separate_runtime_instances, tasks_registered)); \n      // Now we can do one more spawn call to startup the runtime \n      // across the machine since we know everything is initialized\n      const RtEvent runtime_started(realm.collective_spawn_by_kind(\n              (config.separate_runtime_instances ? Processor::NO_KIND : \n               startup_kind), LG_STARTUP_TASK_ID, NULL, 0, \n              !config.separate_runtime_instances, \n              Runtime::merge_events(realm_initialized, legion_initialized)));\n      // Trigger the start event when the runtime is ready\n      Runtime::trigger_event(runtime_started_event, runtime_started);\n      // If we are supposed to background this thread, then we wait\n      // for the runtime to shutdown, otherwise we can now return\n      if (!background)\n        return realm.wait_for_shutdown();\n      return 0;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ const Runtime::LegionConfiguration& Runtime::initialize(\n                                           int *argc, char ***argv, bool filter)\n    //--------------------------------------------------------------------------\n    {\n      static LegionConfiguration config;\n      if (runtime_initialized)\n        return config;\n      RealmRuntime realm;\n#ifndef NDEBUG\n      bool ok = \n#endif\n        realm.network_init(argc, argv);\n      assert(ok);\n\n      const int num_args = *argc;\n      // Next we configure the realm runtime after which we can access the\n      // machine model and make events and reservations and do reigstrations\n      std::vector<std::string> cmdline(num_args-1);\n      for (int i = 1; i < num_args; i++)\n        cmdline[i-1] = (*argv)[i];\n#ifndef NDEBUG\n      ok = \n#endif\n        realm.configure_from_command_line(cmdline, filter);\n      assert(ok);\n      Realm::CommandLineParser cp; \n      cp.add_option_bool(\"-lg:warn_backtrace\",\n                         config.warnings_backtrace, !filter)\n        .add_option_bool(\"-lg:warn\", config.runtime_warnings, !filter)\n        .add_option_bool(\"-lg:leaks\", config.report_leaks, !filter)\n        .add_option_bool(\"-lg:separate\",\n                         config.separate_runtime_instances, !filter)\n        .add_option_bool(\"-lg:registration\",config.record_registration,!filter)\n        .add_option_bool(\"-lg:nosteal\",config.stealing_disabled,!filter)\n        .add_option_bool(\"-lg:resilient\",config.resilient_mode,!filter)\n        .add_option_bool(\"-lg:unsafe_launch\",config.unsafe_launch,!filter)\n        .add_option_bool(\"-lg:unsafe_mapper\",config.unsafe_mapper,!filter)\n        .add_option_bool(\"-lg:safe_mapper\",config.safe_mapper,!filter)\n        .add_option_int(\"-lg:safe_ctrlrepl\",\n                         config.safe_control_replication, !filter)\n        .add_option_bool(\"-lg:inorder\",config.program_order_execution,!filter)\n        .add_option_bool(\"-lg:dump_physical_traces\",\n                         config.dump_physical_traces, !filter)\n        .add_option_bool(\"-lg:no_tracing\",config.no_tracing, !filter)\n        .add_option_bool(\"-lg:no_physical_tracing\",\n                         config.no_physical_tracing, !filter)\n        .add_option_bool(\"-lg:no_trace_optimization\",\n                         config.no_trace_optimization, !filter)\n        .add_option_bool(\"-lg:no_fence_elision\",\n                         config.no_fence_elision, !filter)\n        .add_option_bool(\"-lg:replay_on_cpus\",\n                         config.replay_on_cpus, !filter)\n        .add_option_bool(\"-lg:disjointness\",\n                         config.verify_partitions, !filter)\n        .add_option_bool(\"-lg:partcheck\",\n                         config.verify_partitions, !filter)\n        .add_option_int(\"-lg:window\", config.initial_task_window_size, !filter)\n        .add_option_int(\"-lg:hysteresis\", \n                        config.initial_task_window_hysteresis, !filter)\n        .add_option_int(\"-lg:sched\", \n                        config.initial_tasks_to_schedule, !filter)\n        .add_option_int(\"-lg:vector\", \n                        config.initial_meta_task_vector_width, !filter)\n        .add_option_int(\"-lg:eager_alloc_percentage\",\n                        config.eager_alloc_percentage, !filter)\n        .add_option_bool(\"-lg:dump_free_ranges\",\n                         config.dump_free_ranges, !filter)\n        .add_option_int(\"-lg:message\",config.max_message_size, !filter)\n        .add_option_int(\"-lg:epoch\", config.gc_epoch_size, !filter)\n        .add_option_int(\"-lg:local\", config.max_local_fields, !filter)\n        .add_option_int(\"-lg:parallel_replay\", \n                        config.max_replay_parallelism, !filter)\n        .add_option_bool(\"-lg:no_dyn\",config.disable_independence_tests,!filter)\n        .add_option_bool(\"-lg:spy\",config.legion_spy_enabled, !filter)\n        .add_option_bool(\"-lg:test\",config.enable_test_mapper, !filter)\n        .add_option_int(\"-lg:delay\", config.delay_start, !filter)\n        .add_option_string(\"-lg:replay\", config.replay_file, !filter)\n        .add_option_string(\"-lg:ldb\", config.ldb_file, !filter)\n#ifdef DEBUG_LEGION\n        .add_option_bool(\"-lg:tree\",config.logging_region_tree_state, !filter)\n        .add_option_bool(\"-lg:verbose\",config.verbose_logging, !filter)\n        .add_option_bool(\"-lg:logical_only\",config.logical_logging_only,!filter)\n        .add_option_bool(\"-lg:physical_only\",\n                         config.physical_logging_only,!filter)\n#endif\n        .add_option_int(\"-lg:prof\", config.num_profiling_nodes, !filter)\n        .add_option_string(\"-lg:serializer\", config.serializer_type, !filter)\n        .add_option_string(\"-lg:prof_logfile\", config.prof_logfile, !filter)\n        .add_option_int(\"-lg:prof_footprint\", \n                        config.prof_footprint_threshold, !filter)\n        .add_option_int(\"-lg:prof_latency\",config.prof_target_latency, !filter)\n        .add_option_bool(\"-lg:debug_ok\",config.slow_config_ok, !filter)\n        // These are all the deprecated versions of these flag\n        .add_option_bool(\"-hl:separate\",\n                         config.separate_runtime_instances, !filter)\n        .add_option_bool(\"-hl:registration\",config.record_registration, !filter)\n        .add_option_bool(\"-hl:nosteal\",config.stealing_disabled, !filter)\n        .add_option_bool(\"-hl:resilient\",config.resilient_mode, !filter)\n        .add_option_bool(\"-hl:unsafe_launch\",config.unsafe_launch, !filter)\n        .add_option_bool(\"-hl:unsafe_mapper\",config.unsafe_mapper, !filter)\n        .add_option_bool(\"-hl:safe_mapper\",config.safe_mapper, !filter)\n        .add_option_bool(\"-hl:inorder\",config.program_order_execution, !filter)\n        .add_option_bool(\"-hl:disjointness\",config.verify_partitions, !filter)\n        .add_option_int(\"-hl:window\", config.initial_task_window_size, !filter)\n        .add_option_int(\"-hl:hysteresis\", \n                        config.initial_task_window_hysteresis, !filter)\n        .add_option_int(\"-hl:sched\", config.initial_tasks_to_schedule, !filter)\n        .add_option_int(\"-hl:message\",config.max_message_size, !filter)\n        .add_option_int(\"-hl:epoch\", config.gc_epoch_size, !filter)\n        .add_option_bool(\"-hl:no_dyn\",config.disable_independence_tests,!filter)\n        .add_option_bool(\"-hl:spy\",config.legion_spy_enabled, !filter)\n        .add_option_bool(\"-hl:test\",config.enable_test_mapper, !filter)\n        .add_option_int(\"-hl:delay\", config.delay_start, !filter)\n        .add_option_string(\"-hl:replay\", config.replay_file, !filter)\n        .add_option_string(\"-hl:ldb\", config.ldb_file, !filter)\n#ifdef DEBUG_LEGION\n        .add_option_bool(\"-hl:tree\",config.logging_region_tree_state,!filter)\n        .add_option_bool(\"-hl:verbose\",config.verbose_logging,!filter)\n        .add_option_bool(\"-hl:logical_only\",config.logical_logging_only,!filter)\n        .add_option_bool(\"-hl:physical_only\",\n                         config.physical_logging_only,!filter)\n#endif\n        .add_option_int(\"-hl:prof\", config.num_profiling_nodes, !filter)\n        .add_option_string(\"-hl:serializer\", config.serializer_type, !filter)\n        .add_option_string(\"-hl:prof_logfile\", config.prof_logfile, !filter)\n        .parse_command_line(cmdline);\n      // If we asked to filter the arguments, now we need to go back in\n      // and update the arguments so that they reflect the pruned data\n      if (filter)\n      {\n        if (!cmdline.empty())\n        {\n          int arg_index = 1;\n          for (unsigned idx = 0; idx < cmdline.size(); idx++)\n          {\n            const char *str = cmdline[idx].c_str();\n            // Find the location of this string in the original\n            // arguments to so that we can get its original pointer \n            assert(arg_index < num_args);\n            while (strcmp(str, (*argv)[arg_index]) != 0)\n            {\n              arg_index++;\n              assert(arg_index < num_args);\n            }\n            // Now that we've got it's original pointer we can move\n            // it to the new location in the outputs\n            if (arg_index == int(idx+1))\n              arg_index++; // already in the right place \n            else\n              (*argv)[idx+1] = (*argv)[arg_index++];\n          }\n          *argc = (1 + cmdline.size());\n        }\n        else\n          *argc = 1;\n      }\n#ifdef DEBUG_LEGION\n      if (config.logging_region_tree_state)\n        REPORT_LEGION_WARNING(LEGION_WARNING_REGION_TREE_STATE_LOGGING,\n            \"Region tree state logging is disabled.  To enable region \"\n            \"tree state logging compile in debug mode.\")\n#endif\n      if (config.initial_task_window_hysteresis > 100)\n        REPORT_LEGION_ERROR(ERROR_LEGION_CONFIGURATION,\n            \"Illegal task window hysteresis value of %d which is not a value \"\n            \"between 0 and 100.\", config.initial_task_window_hysteresis)\n      if (config.max_local_fields > LEGION_MAX_FIELDS)\n        REPORT_LEGION_ERROR(ERROR_LEGION_CONFIGURATION,\n            \"Illegal max local fields value %d which is larger than the \"\n            \"value of LEGION_MAX_FIELDS (%d).\", config.max_local_fields,\n            LEGION_MAX_FIELDS)\n      const Realm::Logger::LoggingLevel compile_time_min_level =\n            Realm::Logger::REALM_LOGGING_MIN_LEVEL;\n      if (config.legion_spy_enabled && \n          (Realm::Logger::LEVEL_INFO < compile_time_min_level))\n        REPORT_LEGION_ERROR(ERROR_LEGION_CONFIGURATION,\n            \"Legion Spy logging requires a COMPILE_TIME_MIN_LEVEL \"\n            \"of at most LEVEL_INFO, but current setting is %s\",\n            (compile_time_min_level == Realm::Logger::LEVEL_PRINT) ? \n              \"LEVEL_PRINT\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_WARNING) ?\n              \"LEVEL_WARNING\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_ERROR) ?\n              \"LEVEL_ERROR\" :\n            (compile_time_min_level == Realm::Logger::LEVEL_FATAL) ?\n              \"LEVEL_FATAL\" : \"LEVEL_NONE\")\n      if ((config.num_profiling_nodes > 0) &&\n          (strcmp(config.serializer_type.c_str(), \"ascii\") == 0) &&\n          (Realm::Logger::LEVEL_INFO < compile_time_min_level))\n        REPORT_LEGION_ERROR(ERROR_LEGION_CONFIGURATION,\n            \"Legion Prof 'ascii' logging requires a COMPILE_TIME_MIN_LEVEL \"\n            \"of at most LEVEL_INFO, but current setting is %s\",\n            (compile_time_min_level == Realm::Logger::LEVEL_PRINT) ? \n              \"LEVEL_PRINT\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_WARNING) ?\n              \"LEVEL_WARNING\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_ERROR) ?\n              \"LEVEL_ERROR\" :\n            (compile_time_min_level == Realm::Logger::LEVEL_FATAL) ?\n              \"LEVEL_FATAL\" : \"LEVEL_NONE\")\n      if (config.record_registration &&\n          (Realm::Logger::LEVEL_PRINT < compile_time_min_level))\n        REPORT_LEGION_ERROR(ERROR_LEGION_CONFIGURATION,\n            \"Legion registration logging requires a COMPILE_TIME_MIN_LEVEL \"\n            \"of at most LEVEL_PRINT, but current setting is %s\",\n            (compile_time_min_level == Realm::Logger::LEVEL_WARNING) ?\n              \"LEVEL_WARNING\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_ERROR) ?\n              \"LEVEL_ERROR\" :\n            (compile_time_min_level == Realm::Logger::LEVEL_FATAL) ?\n              \"LEVEL_FATAL\" : \"LEVEL_NONE\")\n      if (config.dump_physical_traces &&\n          (Realm::Logger::LEVEL_INFO < compile_time_min_level))\n        REPORT_LEGION_ERROR(ERROR_LEGION_CONFIGURATION,\n            \"Legion physical trace logging requires a COMPILE_TIME_MIN_LEVEL \"\n            \"of at most LEVEL_INFO, but current setting is %s\",\n            (compile_time_min_level == Realm::Logger::LEVEL_PRINT) ? \n              \"LEVEL_PRINT\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_WARNING) ?\n              \"LEVEL_WARNING\" : \n            (compile_time_min_level == Realm::Logger::LEVEL_ERROR) ?\n              \"LEVEL_ERROR\" :\n            (compile_time_min_level == Realm::Logger::LEVEL_FATAL) ?\n              \"LEVEL_FATAL\" : \"LEVEL_NONE\")\n      runtime_initialized = true;\n      return config;\n    } \n\n    //--------------------------------------------------------------------------\n    Future Runtime::launch_top_level_task(const TaskLauncher &launcher)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(!local_procs.empty());\n#endif \n      // Find a target processor, we'll prefer a CPU processor for\n      // backwards compatibility, but will take anything we get\n      Processor target = Processor::NO_PROC;\n      for (std::set<Processor>::const_iterator it = \n            local_procs.begin(); it != local_procs.end(); it++)\n      {\n        if (it->kind() == Processor::LOC_PROC)\n        {\n          target = *it;\n          break;\n        }\n        else if (!target.exists())\n          target = *it;\n      }\n#ifdef DEBUG_LEGION\n      assert(target.exists());\n#endif\n      // Get an individual task to be the top-level task\n      IndividualTask *top_task = get_available_individual_task();\n      // Get a remote task to serve as the top of the top-level task\n      TopLevelContext *top_context = \n        new TopLevelContext(this, get_unique_operation_id());\n      // Add a reference to the top level context\n      top_context->add_reference();\n      // Set the executing processor\n      top_context->set_executing_processor(target);\n      // Mark that this task is the top-level task\n      Future result = top_task->initialize_task(top_context, launcher, \n                                false/*track parent*/,true/*top level task*/);\n      // Set this to be the current processor\n      top_task->set_current_proc(target);\n      top_task->select_task_options(false/*prioritize*/);\n      increment_outstanding_top_level_tasks();\n      // Launch a task to deactivate the top-level context\n      // when the top-level task is done\n      TopFinishArgs args(top_context);\n      ApEvent pre = top_task->get_completion_event();\n      issue_runtime_meta_task(args, LG_LATENCY_WORK_PRIORITY,\n                              Runtime::protect_event(pre));\n      // Put the task in the ready queue, make sure that the runtime is all\n      // set up across the machine before we launch it as well\n      add_to_ready_queue(target, top_task, runtime_started_event);\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    IndividualTask* Runtime::create_implicit_top_level(TaskID top_task_id,\n                 MapperID top_mapper_id, Processor proxy, const char *task_name)\n    //--------------------------------------------------------------------------\n    {\n      // Save the top-level task name if necessary\n      if (task_name != NULL)\n        attach_semantic_information(top_task_id, \n            LEGION_NAME_SEMANTIC_TAG, task_name, \n            strlen(task_name) + 1, true/*mutable*/);\n      // Get an individual task to be the top-level task\n      IndividualTask *top_task = get_available_individual_task();\n      // Get a remote task to serve as the top of the top-level task\n      TopLevelContext *top_context = \n        new TopLevelContext(this, get_unique_operation_id());\n      // Save the context in the implicit context\n      implicit_context = top_context;\n      // Add a reference to the top level context\n      top_context->add_reference();\n      // Set the executing processor\n      top_context->set_executing_processor(proxy);\n      TaskLauncher launcher(top_task_id, TaskArgument(),\n                            Predicate::TRUE_PRED, top_mapper_id);\n      // Mark that this task is the top-level task\n      top_task->initialize_task(top_context, launcher, false/*track parent*/,\n                    true/*top level task*/, true/*implicit top level task*/);\n      increment_outstanding_top_level_tasks();\n      // Launch a task to deactivate the top-level context\n      // when the top-level task is done\n      TopFinishArgs args(top_context);\n      ApEvent pre = top_task->get_completion_event();\n      issue_runtime_meta_task(args, LG_LATENCY_WORK_PRIORITY,\n                              Runtime::protect_event(pre));\n      return top_task;\n    }\n\n    //--------------------------------------------------------------------------\n    ImplicitShardManager* Runtime::find_implicit_shard_manager(\n                  TaskID top_task_id, MapperID mapper_id, Processor::Kind kind,\n                  unsigned shards_per_address_space, bool local)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(shard_lock);\n      std::map<TaskID,ImplicitShardManager*>::iterator finder = \n        implicit_shard_managers.find(top_task_id);\n      ImplicitShardManager *result = NULL;\n      if (finder == implicit_shard_managers.end())\n      {\n        result = new ImplicitShardManager(this, top_task_id, mapper_id, \n                                          kind, shards_per_address_space);\n        result->add_reference();\n        implicit_shard_managers[top_task_id] = result;\n        finder = implicit_shard_managers.find(top_task_id);\n      }\n      else\n        result = finder->second;\n      result->add_reference();\n      if (result->record_arrival(local))\n      {\n        if (finder->second->remove_reference())\n          assert(false); // should never hit this assertion\n        implicit_shard_managers.erase(finder);\n      }\n      return result;\n    }\n\n    //--------------------------------------------------------------------------\n    Context Runtime::begin_implicit_task(TaskID top_task_id,\n                                         MapperID top_mapper_id,\n                                         Processor::Kind proc_kind,\n                                         const char *task_name,\n                                         bool control_replicable,\n                                         unsigned shards_per_address_space,\n                                         int shard_id)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(runtime_started);\n#endif\n      // Check that we're on an external thread\n      const Processor p = Processor::get_executing_processor();\n      if (p.exists())\n        REPORT_LEGION_ERROR(ERROR_ILLEGAL_IMPLICIT_TOP_LEVEL_TASK,\n            \"Implicit top-level tasks are not allowed to be started on \"\n            \"processors managed by Legion. They can only be started on \"\n            \"external threads that Legion does not control.\")\n#ifdef DEBUG_LEGION\n      assert(!local_procs.empty());\n#endif \n      // Find a proxy processor, we'll prefer a CPU processor for\n      // backwards compatibility, but will take anything we get\n      Processor proxy = Processor::NO_PROC;\n      for (std::set<Processor>::const_iterator it =\n            local_procs.begin(); it != local_procs.end(); it++)\n      {\n        if (it->kind() == proc_kind)\n        {\n          proxy = *it;\n          break;\n        }\n      }\n#ifdef DEBUG_LEGION\n      // TODO: remove this once realm supports drafting this thread\n      // as a new kind of processor to use\n      assert(proxy.exists());\n#endif\n      // Wait for the runtime to have started if necessary\n      if (!runtime_started_event.has_triggered())\n        runtime_started_event.external_wait();\n      SingleTask *local_task = NULL;\n      // Now that the runtime is started we can make our context\n      if (control_replicable && (total_address_spaces > 1))\n      {\n        // Either find or make an implicit shard manager for hooking up\n        ImplicitShardManager *implicit_shard_manager = \n          find_implicit_shard_manager(top_task_id, top_mapper_id, proc_kind, \n                                      shards_per_address_space, true/*local*/);\n        local_task = \n          implicit_shard_manager->create_shard(shard_id, proxy, task_name);\n        if (implicit_shard_manager->remove_reference())\n          delete implicit_shard_manager;\n      }\n      else\n      {\n        local_task = create_implicit_top_level(top_task_id, top_mapper_id,\n                                               proxy, task_name);\n        // Increment the pending count here\n        local_task->get_context()->increment_pending();\n      }\n#ifdef DEBUG_LEGION\n      increment_total_outstanding_tasks(top_task_id, false);\n#else\n      increment_total_outstanding_tasks();\n#endif\n      InnerContext *execution_context = local_task->create_implicit_context();\n      implicit_context = execution_context;\n      implicit_runtime = this;\n      Legion::Runtime *dummy_rt;\n      execution_context->begin_task(dummy_rt);\n      execution_context->set_executing_processor(proxy);\n      return execution_context;\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::finish_implicit_task(TaskContext *ctx)\n    //--------------------------------------------------------------------------\n    {\n      // this is just a normal finish operation\n      ctx->end_task(NULL, 0, false/*owned*/, PhysicalInstance::NO_INST, NULL);\n      // Record that this is no longer an implicit external task\n      implicit_runtime = NULL;\n      implicit_context = NULL;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::perform_slow_config_checks(\n                                              const LegionConfiguration &config)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      if (config.num_profiling_nodes > 0)\n      {\n        // Give a massive warning about profiling with debug enabled\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"!!! YOU ARE PROFILING IN DEBUG MODE           !!!\\n\");\n        fprintf(stderr,\"!!! SERIOUS PERFORMANCE DEGRADATION WILL OCCUR!!!\\n\");\n        fprintf(stderr,\"!!! COMPILE WITH DEBUG=0 FOR PROFILING        !!!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"\\n\");\n        fprintf(stderr,\"SLEEPING FOR 5 SECONDS SO YOU READ THIS WARNING...\\n\");\n        fflush(stderr);\n        sleep(5);\n      }\n#endif\n#ifdef LEGION_SPY\n      if (config.num_profiling_nodes > 0)\n      {\n        // Give a massive warning about profiling with Legion Spy enabled\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"!!! YOU ARE PROFILING WITH LegionSpy ENABLED  !!!\\n\");\n        fprintf(stderr,\"!!! SERIOUS PERFORMANCE DEGRADATION WILL OCCUR!!!\\n\");\n        fprintf(stderr,\"!!! COMPILE WITHOUT -DLEGION_SPY FOR PROFILING!!!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"\\n\");\n        fprintf(stderr,\"SLEEPING FOR 5 SECONDS SO YOU READ THIS WARNING...\\n\");\n        fflush(stderr);\n        sleep(5);\n      }\n#else\n      if (config.legion_spy_enabled && (config.num_profiling_nodes > 0))\n      {\n        // Give a massive warning about profiling with Legion Spy enabled\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"!!! YOU ARE PROFILING WITH LegionSpy ENABLED  !!!\\n\");\n        fprintf(stderr,\"!!! SERIOUS PERFORMANCE DEGRADATION WILL OCCUR!!!\\n\");\n        fprintf(stderr,\"!!! RUN WITHOUT -lg:spy flag FOR PROFILING    !!!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"\\n\");\n        fprintf(stderr,\"SLEEPING FOR 5 SECONDS SO YOU READ THIS WARNING...\\n\");\n        fflush(stderr);\n        sleep(5);\n      }\n#endif\n#ifdef LEGION_BOUNDS_CHECKS\n      if (config.num_profiling_nodes > 0)\n      {\n        // Give a massive warning about profiling with bounds checks enabled\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"!!! YOU ARE PROFILING WITH LEGION_BOUNDS_CHECKS!!!\\n\");\n        fprintf(stderr,\"!!! SERIOUS PERFORMANCE DEGRADATION WILL OCCUR !!!\\n\");\n        fprintf(stderr,\"!!! PLEASE COMPILE WITHOUT LEGION_BOUNDS_CHECKS!!!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"\\n\");\n        fprintf(stderr,\"SLEEPING FOR 5 SECONDS SO YOU READ THIS WARNING...\\n\");\n        fflush(stderr);\n        sleep(5);\n      }\n#endif\n#ifdef LEGION_PRIVILEGE_CHECKS\n      if (config.num_profiling_nodes > 0)\n      {\n        // Give a massive warning about profiling with privilege checks enabled\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"!!!YOU ARE PROFILING WITH LEGION_PRIVILEGE_CHECKS!!\\n\");\n        fprintf(stderr,\"!!!SERIOUS PERFORMANCE DEGRADATION WILL OCCUR!!!\\n\");\n        fprintf(stderr,\"!!!PLEASE COMPILE WITHOUT LEGION_PRIVILEGE_CHECKS!!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"\\n\");\n        fprintf(stderr,\"SLEEPING FOR 5 SECONDS SO YOU READ THIS WARNING...\\n\");\n        fflush(stderr);\n        sleep(5);\n      }\n#endif\n      if (config.verify_partitions && (config.num_profiling_nodes > 0))\n      {\n        // Give a massive warning about profiling with partition checks enabled\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"!!! YOU ARE PROFILING WITH PARTITION CHECKS ON!!!\\n\");\n        fprintf(stderr,\"!!! SERIOUS PERFORMANCE DEGRADATION WILL OCCUR!!!\\n\");\n        fprintf(stderr,\"!!! DO NOT USE -lg:partcheck WITH PROFILING   !!!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        for (int i = 0; i < 4; i++)\n          fprintf(stderr,\"!WARNING WARNING WARNING WARNING WARNING WARNING!\\n\");\n        for (int i = 0; i < 2; i++)\n          fprintf(stderr,\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\");\n        fprintf(stderr,\"\\n\");\n        fprintf(stderr,\"SLEEPING FOR 5 SECONDS SO YOU READ THIS WARNING...\\n\");\n        fflush(stderr);\n        sleep(5);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::configure_interoperability(\n                                                bool separate_runtime_instances)\n    //--------------------------------------------------------------------------\n    {\n      if (separate_runtime_instances && (mpi_rank > 0))\n        REPORT_LEGION_ERROR(ERROR_MPI_INTEROP_MISCONFIGURATION,\n            \"Legion-MPI Interoperability is not supported when running \"\n            \"with separate runtime instances for each processor\")\n      const std::vector<LegionHandshake> &pending_handshakes = \n        get_pending_handshake_table();\n      if (!pending_handshakes.empty())\n      {\n        for (std::vector<LegionHandshake>::const_iterator it = \n              pending_handshakes.begin(); it != pending_handshakes.end(); it++)\n          it->impl->initialize();\n      }\n    }\n\n#ifdef LEGION_GPU_REDUCTIONS\n    extern void register_builtin_gpu_reduction_tasks(\n      GPUReductionTable &gpu_reductions, std::set<RtEvent> &registered_events);\n#endif\n\n    //--------------------------------------------------------------------------\n    /*static*/ RtEvent Runtime::configure_runtime(int argc, char **argv,\n                         const LegionConfiguration &config, RealmRuntime &realm,\n                         Processor::Kind &startup_kind, bool background,\n                         bool supply_default_mapper)\n    //--------------------------------------------------------------------------\n    {\n      // Do some error checking in case we are running with separate instances\n      Machine machine = Machine::get_machine();\n      // Compute the data structures necessary for constructing a runtime \n      std::set<Processor> local_procs;\n      std::set<Processor> local_util_procs;\n      // First we find all our local processors\n      {\n        Machine::ProcessorQuery local_proc_query(machine);\n        local_proc_query.local_address_space();\n        // Check for exceeding the local number of processors\n        if (local_proc_query.count() > LEGION_MAX_NUM_PROCS)\n          REPORT_LEGION_ERROR(ERROR_MAXIMUM_PROCS_EXCEEDED, \n                        \"Maximum number of local processors %zd exceeds \"\n                        \"compile-time maximum of %d.  Change the value \"\n                        \"LEGION_MAX_NUM_PROCS in legion_config.h and recompile.\"\n                        , local_proc_query.count(), LEGION_MAX_NUM_PROCS)\n        for (Machine::ProcessorQuery::iterator it = \n              local_proc_query.begin(); it != local_proc_query.end(); it++)\n        {\n          if (it->kind() == Processor::UTIL_PROC)\n          {\n            local_util_procs.insert(*it);\n            // Startup can also be a utility processor if nothing else\n            if (startup_kind == Processor::NO_KIND)\n              startup_kind = Processor::UTIL_PROC;\n          }\n          else\n          {\n            local_procs.insert(*it);\n            // Prefer CPUs for the startup kind\n            if (it->kind() == Processor::LOC_PROC)\n              startup_kind = Processor::LOC_PROC;\n          }\n        }\n        if (local_procs.empty())\n          REPORT_LEGION_ERROR(ERROR_NO_PROCESSORS, \n                        \"Machine model contains no local processors!\")\n      }\n      // Check to make sure we have something to do startup\n      if (startup_kind == Processor::NO_KIND)\n        REPORT_LEGION_ERROR(ERROR_NO_PROCESSORS, \"Machine model contains \"\n            \"no CPU processors and no utility processors! At least one \"\n            \"CPU or one utility processor is required for Legion.\")\n      Realm::ProfilingRequestSet no_requests;\n      // Keep track of all the registration events\n      std::set<RtEvent> registered_events;\n#ifdef LEGION_GPU_REDUCTIONS\n      // Do this here to make sure we get the gpu reduction table\n      // setup before we make the runtime object\n      register_builtin_gpu_reduction_tasks(get_gpu_reduction_table(),\n                                           registered_events);\n      GPUReductionTable &gpu_reduction_table = get_gpu_reduction_table();\n      const std::map<ReductionOpID,CodeDescriptor> &pending_gpu_reductions =\n        get_pending_gpu_reduction_table();\n      for (std::map<ReductionOpID,CodeDescriptor>::const_iterator it = \n            pending_gpu_reductions.begin(); it != \n            pending_gpu_reductions.end(); it++)\n      {\n        const TaskID task_id = \n          LG_TASK_ID_AVAILABLE + gpu_reduction_table.size();\n        registered_events.insert(RtEvent(Processor::register_task_by_kind(\n                Processor::TOC_PROC, false/*global*/, task_id, it->second,\n                no_requests, NULL, 0)));\n        gpu_reduction_table[it->first] = task_id;\n      }\n#endif\n      // Now build the data structures for all processors \n      std::map<Processor,Runtime*> processor_mapping;\n      if (config.separate_runtime_instances)\n      {\n#ifdef TRACE_ALLOCATION\n        REPORT_LEGION_FATAL(LEGION_FATAL_SEPARATE_RUNTIME_INSTANCES, \n                      \"Memory tracing not supported with \"\n                      \"separate runtime instances.\")\n#endif\n        if (!local_util_procs.empty())\n          REPORT_LEGION_FATAL(LEGION_FATAL_SEPARATE_RUNTIME_INSTANCES, \n                        \"Separate runtime instances are not \"\n                        \"supported when running with explicit \"\n                        \"utility processors\")\n        std::set<AddressSpaceID> address_spaces;\n        std::map<Processor,AddressSpaceID> proc_spaces;\n        // If we are doing separate runtime instances then each\n        // processor effectively gets its own address space\n        Machine::ProcessorQuery all_procs(machine);\n        AddressSpaceID sid = 0;\n        for (Machine::ProcessorQuery::iterator it = \n              all_procs.begin(); it != all_procs.end(); it++,sid++)\n        {\n          address_spaces.insert(sid);\n          proc_spaces[*it] = sid;\n        }\n        InputArgs input_args;\n        input_args.argc = argc;\n        input_args.argv = argv;\n        // Now we make runtime instances for each of the local processors\n        for (std::set<Processor>::const_iterator it =\n              local_procs.begin(); it != local_procs.end(); it++)\n        {\n          const AddressSpace local_space = proc_spaces[*it];\n          // Only one local processor here\n          std::set<Processor> fake_local_procs;\n          fake_local_procs.insert(*it);\n          Runtime *runtime = new Runtime(machine, config, background,\n                                         input_args, local_space,\n                                         fake_local_procs, local_util_procs,\n                                         address_spaces, proc_spaces,\n                                         supply_default_mapper);\n          processor_mapping[*it] = runtime;\n          // Save the the_runtime as the first one we make\n          // just so that things will work in the multi-processor case\n          if (the_runtime == NULL)\n            the_runtime = runtime;\n        }\n      }\n      else\n      {\n        // The normal path\n        std::set<AddressSpaceID> address_spaces;\n        std::map<Processor,AddressSpaceID> proc_spaces;\n        Machine::ProcessorQuery all_procs(machine);\n        for (Machine::ProcessorQuery::iterator it = \n              all_procs.begin(); it != all_procs.end(); it++)\n        {\n          AddressSpaceID sid = it->address_space();\n          address_spaces.insert(sid);\n          proc_spaces[*it] = sid;\n        }\n        // Make one runtime instance and record it with all the processors\n        const AddressSpace local_space = local_procs.begin()->address_space();\n        InputArgs input_args;\n        input_args.argc = argc;\n        input_args.argv = argv;\n        Runtime *runtime = new Runtime(machine, config, background,\n                                       input_args, local_space,\n                                       local_procs, local_util_procs,\n                                       address_spaces, proc_spaces,\n                                       supply_default_mapper);\n        // Save THE runtime \n        the_runtime = runtime;\n        for (std::set<Processor>::const_iterator it = \n              local_procs.begin(); it != local_procs.end(); it++)\n          processor_mapping[*it] = runtime;\n        for (std::set<Processor>::const_iterator it = \n              local_util_procs.begin(); it != local_util_procs.end(); it++)\n          processor_mapping[*it] = runtime;\n      }\n      // Make the code descriptors for our tasks\n      CodeDescriptor initialize_task(Runtime::initialize_runtime_task);\n      CodeDescriptor shutdown_task(Runtime::shutdown_runtime_task);\n      CodeDescriptor lg_task(Runtime::legion_runtime_task);\n      CodeDescriptor rt_profiling_task(Runtime::profiling_runtime_task);\n      CodeDescriptor startup_task(Runtime::startup_runtime_task);\n      CodeDescriptor endpoint_task(Runtime::endpoint_runtime_task); \n      CodeDescriptor app_proc_task(Runtime::application_processor_runtime_task);\n      for (std::map<Processor,Runtime*>::const_iterator it = \n            processor_mapping.begin(); it != processor_mapping.end(); it++)\n      {\n        // These tasks get registered on startup_kind processors\n        if (it->first.kind() == startup_kind)\n        {\n          registered_events.insert(RtEvent(\n                it->first.register_task(LG_INITIALIZE_TASK_ID, initialize_task,\n                  no_requests, &it->second, sizeof(it->second))));\n          registered_events.insert(RtEvent(\n              it->first.register_task(LG_STARTUP_TASK_ID, startup_task,\n                no_requests, &it->second, sizeof(it->second))));\n        }\n        // Register these tasks on utility processors if we have\n        // them otherwise register them on all the processor kinds\n        if (local_util_procs.empty() || \n            (it->first.kind() == Processor::UTIL_PROC))\n        {\n          registered_events.insert(RtEvent(\n                it->first.register_task(LG_SHUTDOWN_TASK_ID, shutdown_task,\n                  no_requests, &it->second, sizeof(it->second))));\n#ifdef LEGION_SEPARATE_META_TASKS\n          for (unsigned idx = 0; idx < LG_LAST_TASK_ID; idx++)\n          {\n            if (idx == LG_MESSAGE_ID)\n            {\n              for (unsigned msg = 0; msg < LAST_SEND_KIND; msg++)\n                registered_events.insert(RtEvent(\n                    it->first.register_task(LG_TASK_ID+idx+msg, lg_task,\n                        no_requests, &it->second, sizeof(it->second))));\n            }\n            else\n              registered_events.insert(RtEvent(\n                    it->first.register_task(LG_TASK_ID+idx, lg_task,\n                      no_requests, &it->second, sizeof(it->second))));\n          }\n#else\n          registered_events.insert(RtEvent(\n                it->first.register_task(LG_TASK_ID, lg_task,\n                  no_requests, &it->second, sizeof(it->second))));\n#endif\n          registered_events.insert(RtEvent(\n                it->first.register_task(LG_ENDPOINT_TASK_ID, endpoint_task,\n                  no_requests, &it->second, sizeof(it->second))));\n        }\n        // Register profiling return meta-task on all processor kinds\n        registered_events.insert(RtEvent(\n            it->first.register_task(LG_LEGION_PROFILING_ID, rt_profiling_task,\n              no_requests, &it->second, sizeof(it->second))));\n        // Application processor tasks get registered on all\n        // processors which are not utility processors\n#ifdef LEGION_SEPARATE_META_TASKS\n        if (it->first.kind() != Processor::UTIL_PROC)\n        {\n          for (unsigned idx = 0; idx < LG_LAST_TASK_ID; idx++)\n            registered_events.insert(RtEvent(\n                it->first.register_task(LG_APP_PROC_TASK_ID+idx, app_proc_task,\n                      no_requests, &it->second, sizeof(it->second))));\n        }\n#else\n        if (it->first.kind() != Processor::UTIL_PROC)\n          registered_events.insert(RtEvent(\n                it->first.register_task(LG_APP_PROC_TASK_ID, app_proc_task,\n                  no_requests, &it->second, sizeof(it->second))));\n#endif\n      }\n      // Lastly do any other registrations we might have\n#ifdef DEBUG_LEGION_COLLECTIVES\n      ReductionOpTable& red_table = get_reduction_table(true/*safe*/);\n      red_table[CollectiveCheckReduction::REDOP] =\n        Realm::ReductionOpUntyped::create_reduction_op<\n                                CollectiveCheckReduction>();\n      red_table[CloseCheckReduction::REDOP]=\n        Realm::ReductionOpUntyped::create_reduction_op<\n                                CloseCheckReduction>();\n#else\n      const ReductionOpTable& red_table = get_reduction_table(true/*safe*/);\n#endif\n      for(ReductionOpTable::const_iterator it = red_table.begin();\n          it != red_table.end();\n          it++)\n        realm.register_reduction(it->first, it->second);\n\n      const SerdezOpTable &serdez_table = get_serdez_table(true/*safe*/);\n      for (SerdezOpTable::const_iterator it = serdez_table.begin();\n            it != serdez_table.end(); it++)\n        realm.register_custom_serdez(it->first, it->second);\n      \n      if (config.record_registration)\n      {\n        log_run.print(\"Legion runtime initialize task has Realm ID %d\",\n                      LG_INITIALIZE_TASK_ID);\n        log_run.print(\"Legion runtime shutdown task has Realm ID %d\", \n                      LG_SHUTDOWN_TASK_ID);\n        log_run.print(\"Legion runtime meta-task has Realm ID %d\", \n                      LG_TASK_ID);\n        log_run.print(\"Legion runtime profiling task Realm ID %d\",\n                      LG_LEGION_PROFILING_ID);\n        log_run.print(\"Legion startup task has Realm ID %d\",\n                      LG_STARTUP_TASK_ID);\n        log_run.print(\"Legion endpoint task has Realm ID %d\",\n                      LG_ENDPOINT_TASK_ID);\n      }\n      return Runtime::merge_events(registered_events);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ int Runtime::wait_for_shutdown(void)\n    //--------------------------------------------------------------------------\n    {\n      if (!runtime_backgrounded)\n        REPORT_LEGION_ERROR(ERROR_ILLEGAL_WAIT_FOR_SHUTDOWN, \n                      \"Illegal call to wait_for_shutdown when runtime was \"\n                      \"not launched in background mode!\");\n      // If this is the first time we've called this on this node then \n      // we need to remove our reference to allow shutdown to proceed\n      if (__sync_fetch_and_add(&background_waits, 1) == 0)\n        the_runtime->decrement_outstanding_top_level_tasks();\n      return RealmRuntime::get_runtime().wait_for_shutdown();\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::set_return_code(int code)\n    //--------------------------------------------------------------------------\n    {\n      return_code = code;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::set_top_level_task_id(TaskID top_id)\n    //--------------------------------------------------------------------------\n    {\n      legion_main_id = top_id;\n      legion_main_set = true;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::set_top_level_task_mapper_id(MapperID mapper_id)\n    //--------------------------------------------------------------------------\n    {\n      legion_main_mapper_id = mapper_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::configure_MPI_interoperability(int rank)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'configure_MPI_interoperability' after \"\n                      \"the runtime has been started!\");\n#ifdef DEBUG_LEGION\n      assert(rank >= 0);\n#endif\n      // Check to see if it was already set\n      if (mpi_rank >= 0)\n      {\n        if (rank != mpi_rank)\n          REPORT_LEGION_ERROR(ERROR_DUPLICATE_MPI_CONFIG, \n              \"multiple calls to \"\n              \"configure_MPI_interoperability with different ranks \"\n              \"%d and %d on the same Legion runtime!\", mpi_rank, rank)\n        else\n          REPORT_LEGION_WARNING(LEGION_WARNING_DUPLICATE_MPI_CONFIG,\n                                \"duplicate calls to configure_\"\n                                \"MPI_interoperability on rank %d!\", rank);\n      }\n      mpi_rank = rank;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::register_handshake(LegionHandshake &handshake)\n    //--------------------------------------------------------------------------\n    {\n      // See if the runtime is started or not\n      if (runtime_started)\n      {\n        // If it's started, we can just do the initialization now\n        handshake.impl->initialize();\n      }\n      else\n      {\n        std::vector<LegionHandshake> &pending_handshakes = \n          get_pending_handshake_table();\n        pending_handshakes.push_back(handshake);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ const ReductionOp* Runtime::get_reduction_op(\n                                                        ReductionOpID redop_id,\n                                                        bool has_lock/*=false*/)\n    //--------------------------------------------------------------------------\n    {\n      if (redop_id == 0)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_REDOP_ID, \n                      \"ReductionOpID zero is reserved.\")\n      if (!runtime_started || has_lock)\n      {\n        ReductionOpTable &red_table = \n          Runtime::get_reduction_table(true/*safe*/);\n#ifdef DEBUG_LEGION\n        if (red_table.find(redop_id) == red_table.end())\n          REPORT_LEGION_ERROR(ERROR_INVALID_REDOP_ID, \n                        \"Invalid ReductionOpID %d\",redop_id)\n#endif\n        return red_table[redop_id];\n      }\n      else\n        return the_runtime->get_reduction(redop_id);\n    }\n\n    //--------------------------------------------------------------------------\n    const ReductionOp* Runtime::get_reduction(ReductionOpID redop_id)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock r_lock(redop_lock);\n      return get_reduction_op(redop_id, true/*has lock*/); \n    }\n\n    //--------------------------------------------------------------------------\n    FillView* Runtime::find_or_create_reduction_fill_view(ReductionOpID redop)\n    //--------------------------------------------------------------------------\n    {\n      {\n        AutoLock r_lock(redop_lock,1,false/*exclusive*/);\n        std::map<ReductionOpID,FillView*>::const_iterator finder = \n          redop_fill_views.find(redop);\n        if (finder != redop_fill_views.end())\n          return finder->second;\n      }\n      AutoLock r_lock(redop_lock);\n      // Check to see if we lost the race\n      std::map<ReductionOpID,FillView*>::const_iterator finder = \n        redop_fill_views.find(redop);\n      if (finder != redop_fill_views.end())\n        return finder->second;\n      const ReductionOp *reduction_op = \n        get_reduction_op(redop, true/*has lock*/);\n      void *fill_buffer = malloc(reduction_op->sizeof_rhs);\n      reduction_op->init(fill_buffer, 1);\n      FillView::FillViewValue *fill_value = \n        new FillView::FillViewValue(fill_buffer, reduction_op->sizeof_rhs);\n      FillView *fill_view = new FillView(forest, get_available_distributed_id(),\n                                 address_space, fill_value, true/*register now*/\n#ifdef LEGION_SPY\n                                 , 0/*no creator*/\n#endif\n                                 );\n      fill_view->add_base_valid_ref(RUNTIME_REF);\n      redop_fill_views[redop] = fill_view;\n      return fill_view;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ const SerdezOp* Runtime::get_serdez_op(CustomSerdezID serdez_id,\n                                                      bool has_lock/*=false*/)\n    //--------------------------------------------------------------------------\n    {\n      if (serdez_id == 0)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_SERDEZ_ID, \n                      \"CustomSerdezID zero is reserved.\")\n      if (!runtime_started || has_lock)\n      {\n        SerdezOpTable &serdez_table = Runtime::get_serdez_table(true/*safe*/);\n#ifdef DEBUG_LEGION\n        if (serdez_table.find(serdez_id) == serdez_table.end())\n          REPORT_LEGION_ERROR(ERROR_INVALID_SERDEZ_ID, \n                        \"Invalid CustomSerdezOpID %d\", serdez_id)\n#endif\n        return serdez_table[serdez_id];\n      }\n      else\n        return the_runtime->get_serdez(serdez_id);\n    }\n\n    //--------------------------------------------------------------------------\n    const SerdezOp* Runtime::get_serdez(CustomSerdezID serdez_id)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock s_lock(serdez_lock);\n      return get_serdez_op(serdez_id, true/*has lock*/);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ const SerdezRedopFns* Runtime::get_serdez_redop_fns(\n                                                       ReductionOpID redop_id,\n                                                       bool has_lock/*= false*/)\n    //--------------------------------------------------------------------------\n    {\n      if (!runtime_started || has_lock)\n      {\n        SerdezRedopTable &serdez_table = get_serdez_redop_table(true/*safe*/); \n        SerdezRedopTable::const_iterator finder = serdez_table.find(redop_id);\n        if (finder != serdez_table.end())\n          return &(finder->second);\n        return NULL;\n      }\n      else\n        return the_runtime->get_serdez_redop(redop_id);\n    }\n\n    //--------------------------------------------------------------------------\n    const SerdezRedopFns* Runtime::get_serdez_redop(ReductionOpID redop_id)\n    //--------------------------------------------------------------------------\n    {\n      AutoLock r_lock(redop_lock);\n      return get_serdez_redop_fns(redop_id, true/*has lock*/);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::add_registration_callback(\n                                             RegistrationCallbackFnptr callback)\n    //--------------------------------------------------------------------------\n    {\n      if (!runtime_started)\n      {\n        std::vector<RegistrationCallbackFnptr> &registration_callbacks = \n          get_pending_registration_callbacks();\n        registration_callbacks.push_back(callback);\n      }\n      else\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'add_registration_callback' after \"\n                      \"the runtime has been started! Please use \"\n                      \"'perform_registration_callback' for registration \"\n                      \"calls to be done after the runtime has started.\")\n    }\n\n#ifdef LEGION_USE_LIBDL\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::perform_dynamic_registration_callback(\n                                RegistrationCallbackFnptr callback, bool global)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime_started)\n      {\n        // Wait for the runtime to be started everywhere\n        if (!runtime_started_event.has_triggered())\n          // If we're here this has to be an external thread\n          runtime_started_event.external_wait();\n        if (the_runtime->separate_runtime_instances)\n            REPORT_LEGION_FATAL(LEGION_FATAL_SEPARATE_RUNTIME_INSTANCES,\n                \"Dynamic registration callbacks cannot be registered after \"\n                \"the runtime has been started with multiple runtime instances.\") \n        const RtEvent done_event = \n          the_runtime->perform_registration_callback(callback, global);\n        if (done_event.exists())\n        {\n          // If we have a context then record that no operations are \n          // allowed to be executed until after this registration is done\n          if (implicit_context != NULL)\n            implicit_context->handle_registration_callback_effects(done_event);\n          else if (!done_event.has_triggered())\n          {\n            if (Processor::get_executing_processor().exists())\n              done_event.wait();\n            else\n              done_event.external_wait();\n          }\n        }\n      }\n      else // can safely ignore global as this call must be done everywhere\n        add_registration_callback(callback);\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    /*static*/ ReductionOpTable& Runtime::get_reduction_table(bool safe)\n    //--------------------------------------------------------------------------\n    {\n      static ReductionOpTable table;\n      if (!safe && runtime_started)\n        assert(false);\n      return table;\n    }\n\n#ifdef LEGION_GPU_REDUCTIONS\n    //--------------------------------------------------------------------------\n    /*static*/ GPUReductionTable& Runtime::get_gpu_reduction_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static GPUReductionTable table;\n      return table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::map<ReductionOpID,CodeDescriptor>&\n                                  Runtime::get_pending_gpu_reduction_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::map<ReductionOpID,CodeDescriptor> pending_table;\n      return pending_table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::preregister_gpu_reduction_op(ReductionOpID redop,\n                                                     const CodeDescriptor &desc)\n    //--------------------------------------------------------------------------\n    {\n      std::map<ReductionOpID,CodeDescriptor> &pending_table = \n        get_pending_gpu_reduction_table();\n      pending_table[redop] = desc;\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    /*static*/ SerdezOpTable& Runtime::get_serdez_table(bool safe)\n    //--------------------------------------------------------------------------\n    {\n      static SerdezOpTable table;\n      if (!safe && runtime_started)\n        assert(false);\n      return table;\n    }\n    \n    //--------------------------------------------------------------------------\n    /*static*/ SerdezRedopTable& Runtime::get_serdez_redop_table(bool safe)\n    //--------------------------------------------------------------------------\n    {\n      static SerdezRedopTable table;\n      if (!safe && runtime_started)\n        assert(false);\n      return table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::register_reduction_op(ReductionOpID redop_id,\n                                                   ReductionOp *redop,\n                                                   SerdezInitFnptr init_fnptr,\n                                                   SerdezFoldFnptr fold_fnptr,\n                                                   bool permit_duplicates,\n                                                   bool has_lock/*= false*/)\n    //--------------------------------------------------------------------------\n    {\n      if (!runtime_started || has_lock)\n      {\n        if (redop_id == 0)\n          REPORT_LEGION_ERROR(ERROR_RESERVED_REDOP_ID, \n                              \"ERROR: ReductionOpID zero is reserved.\")\n        // TODO: figure out a way to make this safe with dynamic registration\n#if 0\n        if (redop_id >= LEGION_MAX_APPLICATION_REDOP_ID)\n          REPORT_LEGION_ERROR(ERROR_RESERVED_REDOP_ID,\n                         \"ERROR: ReductionOpID %d is greater than or equal \"\n                         \"to the LEGION_MAX_APPLICATION_REDOP_ID of %d \"\n                         \"set in legion_config.h.\", redop_id, \n                         LEGION_MAX_APPLICATION_REDOP_ID)\n#endif\n        ReductionOpTable &red_table = \n          Runtime::get_reduction_table(true/*safe*/);\n        // Check to make sure we're not overwriting a prior reduction op \n        if (!permit_duplicates &&\n            (red_table.find(redop_id) != red_table.end()))\n          REPORT_LEGION_ERROR(ERROR_DUPLICATE_REDOP_ID, \"ERROR: ReductionOpID \"\n                    \"%d has already been used in the reduction table\",redop_id)\n        red_table[redop_id] = redop;\n        if ((init_fnptr != NULL) || (fold_fnptr != NULL))\n        {\n#ifdef DEBUG_LEGION\n          assert((init_fnptr != NULL) && (fold_fnptr != NULL));\n#endif\n          SerdezRedopTable &serdez_red_table = \n            Runtime::get_serdez_redop_table(true/*safe*/);\n          SerdezRedopFns &fns = serdez_red_table[redop_id];\n          fns.init_fn = init_fnptr;\n          fns.fold_fn = fold_fnptr;\n        }\n      }\n      else\n        the_runtime->register_reduction(redop_id, redop, init_fnptr,\n              fold_fnptr, permit_duplicates, false/*preregistered*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_reduction(ReductionOpID redop_id,\n                                     ReductionOp *redop,\n                                     SerdezInitFnptr init_fnptr,\n                                     SerdezFoldFnptr fold_fnptr,\n                                     bool permit_duplicates,\n                                     bool preregistered)\n    //--------------------------------------------------------------------------\n    {\n      if (!preregistered && !inside_registration_callback)\n        REPORT_LEGION_WARNING(LEGION_WARNING_NON_CALLBACK_REGISTRATION,\n            \"Reduction operator %d was dynamically registered outside of a \"\n            \"registration callback invocation. In the near future this will \"\n            \"become an error in order to support task subprocesses. Please \"\n            \"use 'perform_registration_callback' to generate a callback where \"\n            \"it will be safe to perform dynamic registrations.\", redop_id)\n      // Dynamic registration so do it with realm too\n      RealmRuntime realm = RealmRuntime::get_runtime();\n      realm.register_reduction(redop_id, redop);\n      AutoLock r_lock(redop_lock);\n      Runtime::register_reduction_op(redop_id, redop, init_fnptr,\n                fold_fnptr, permit_duplicates, true/*has locks*/);\n    }\n\n    //--------------------------------------------------------------------------\n    void Runtime::register_serdez(CustomSerdezID serdez_id, SerdezOp *serdez_op, \n                                  bool permit_duplicates, bool preregistered)\n    //--------------------------------------------------------------------------\n    {\n      if (!preregistered && !inside_registration_callback)\n        REPORT_LEGION_WARNING(LEGION_WARNING_NON_CALLBACK_REGISTRATION,\n            \"Custom serdez operator %d was dynamically registered outside of a \"\n            \"registration callback invocation. In the near future this will \"\n            \"become an error in order to support task subprocesses. Please \"\n            \"use 'perform_registration_callback' to generate a callback where \"\n            \"it will be safe to perform dynamic registrations.\", serdez_id)\n      // Dynamic registration so do it with realm too\n      RealmRuntime realm = RealmRuntime::get_runtime();\n      realm.register_custom_serdez(serdez_id, serdez_op);\n      AutoLock s_lock(serdez_lock);\n      Runtime::register_serdez_op(serdez_id, serdez_op, \n                                  permit_duplicates, true/*has lock*/);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::register_serdez_op(CustomSerdezID serdez_id,\n                                                SerdezOp *serdez_op,\n                                                bool permit_duplicates,\n                                                bool has_lock/*= false*/)\n    //--------------------------------------------------------------------------\n    {\n      if (!runtime_started || has_lock)\n      {\n        if (serdez_id == 0)\n          REPORT_LEGION_ERROR(ERROR_RESERVED_SERDEZ_ID,\n                              \"ERROR: Custom Serdez ID zero is reserved.\\n\")\n        // TODO: figure out a way to make this safe with dynamic registration\n#if 0\n        if (serdez_id >= LEGION_MAX_APPLICATION_SERDEZ_ID)\n          REPORT_LEGION_ERROR(ERROR_RESERVED_SERDEZ_ID,\n                         \"ERROR: ReductionOpID %d is greater than or equal \"\n                         \"to the LEGION_MAX_APPLICATION_SERDEZ_ID of %d set \"\n                         \"in legion_config.h.\", serdez_id, \n                         LEGION_MAX_APPLICATION_SERDEZ_ID)\n#endif\n        SerdezOpTable &serdez_table = Runtime::get_serdez_table(true/*safe*/);\n        // Check to make sure we're not overwriting a prior serdez op\n        if (!permit_duplicates &&\n            (serdez_table.find(serdez_id) != serdez_table.end()))\n          REPORT_LEGION_ERROR(ERROR_DUPLICATE_SERDEZ_ID,\n                         \"ERROR: CustomSerdezID %d has already been used \"\n                         \"in the serdez operation table\", serdez_id)\n        serdez_table[serdez_id] = serdez_op;\n      }\n      else\n        the_runtime->register_serdez(serdez_id, serdez_op, \n                                     permit_duplicates, false/*preregistered*/);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::deque<PendingVariantRegistration*>& \n                                       Runtime::get_pending_variant_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::deque<PendingVariantRegistration*> pending_variant_table;\n      return pending_variant_table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::map<LayoutConstraintID,LayoutConstraintRegistrar>&\n                                    Runtime::get_pending_constraint_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::map<LayoutConstraintID,LayoutConstraintRegistrar>\n                                                    pending_constraint_table;\n      return pending_constraint_table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::map<ProjectionID,ProjectionFunctor*>&\n                                     Runtime::get_pending_projection_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::map<ProjectionID,ProjectionFunctor*> pending_projection_table;\n      return pending_projection_table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::map<ShardingID,ShardingFunctor*>&\n                                       Runtime::get_pending_sharding_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::map<ShardingID,ShardingFunctor*> pending_sharding_table;\n      return pending_sharding_table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::vector<LegionHandshake>& \n                                      Runtime::get_pending_handshake_table(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::vector<LegionHandshake> pending_handshakes_table;\n      return pending_handshakes_table;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ std::vector<RegistrationCallbackFnptr>&\n                               Runtime::get_pending_registration_callbacks(void)\n    //--------------------------------------------------------------------------\n    {\n      static std::vector<RegistrationCallbackFnptr> pending_callbacks;\n      return pending_callbacks;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ TaskID& Runtime::get_current_static_task_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static TaskID current_task_id = LEGION_MAX_APPLICATION_TASK_ID;\n      return current_task_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ TaskID Runtime::generate_static_task_id(void)\n    //--------------------------------------------------------------------------\n    {\n      TaskID &next_task = get_current_static_task_id(); \n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'generate_static_task_id' after \"\n                      \"the runtime has been started!\")\n      return next_task++;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ReductionOpID& Runtime::get_current_static_reduction_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static ReductionOpID current_redop_id = LEGION_MAX_APPLICATION_REDOP_ID;\n      return current_redop_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ ReductionOpID Runtime::generate_static_reduction_id(void)\n    //--------------------------------------------------------------------------\n    {\n      ReductionOpID &next_redop = get_current_static_reduction_id();\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'generate_static_reduction_id' after \"\n                      \"the runtime has been started!\")\n      return next_redop++;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ CustomSerdezID& Runtime::get_current_static_serdez_id(void)\n    //--------------------------------------------------------------------------\n    {\n      static CustomSerdezID current_serdez_id =LEGION_MAX_APPLICATION_SERDEZ_ID;\n      return current_serdez_id;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ CustomSerdezID Runtime::generate_static_serdez_id(void)\n    //--------------------------------------------------------------------------\n    {\n      CustomSerdezID &next_serdez = get_current_static_serdez_id();\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'generate_static_serdez_id' after \"\n                      \"the runtime has been started!\")\n      return next_serdez++;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ VariantID Runtime::preregister_variant(\n                          const TaskVariantRegistrar &registrar,\n                          const void *user_data, size_t user_data_size,\n                          const CodeDescriptor &code_desc, bool has_ret, \n                          const char *task_name, VariantID vid, bool check_id)\n    //--------------------------------------------------------------------------\n    {\n      // Report an error if the runtime has already started\n      if (runtime_started)\n        REPORT_LEGION_ERROR(ERROR_STATIC_CALL_POST_RUNTIME_START, \n                      \"Illegal call to 'preregister_task_variant' after \"\n                      \"the runtime has been started!\")\n      if (check_id && (registrar.task_id >= get_current_static_task_id()))\n        REPORT_LEGION_ERROR(ERROR_MAX_APPLICATION_TASK_ID_EXCEEDED, \n                      \"Error preregistering task with ID %d. Exceeds the \"\n                      \"statically set bounds on application task IDs of %d. \"\n                      \"See %s in legion_config.h.\", \n                      registrar.task_id, LEGION_MAX_APPLICATION_TASK_ID, \n                      LEGION_MACRO_TO_STRING(LEGION_MAX_APPLICATION_TASK_ID))\n      std::deque<PendingVariantRegistration*> &pending_table = \n        get_pending_variant_table();\n      // See if we need to pick a variant\n      if (vid == LEGION_AUTO_GENERATE_ID)\n        vid = pending_table.size() + 1;\n      else if (vid == 0)\n        REPORT_LEGION_ERROR(ERROR_RESERVED_VARIANT_ID,\n                      \"Error preregistering variant for task ID %d with \"\n                      \"variant ID 0. Variant ID 0 is reserved for task \"\n                      \"generators.\", registrar.task_id)\n      // Offset by the runtime tasks\n      pending_table.push_back(new PendingVariantRegistration(vid, has_ret,\n                              registrar, user_data, user_data_size, \n                              code_desc, task_name));\n      return vid;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::report_fatal_message(int id,\n                                          const char *file_name, const int line,\n                                                  const char *message)\n    //--------------------------------------------------------------------------\n    {\n      log_run.fatal(id, \"LEGION FATAL: %s (from file %s:%d)\",\n                    message, file_name, line);\n      abort();\n    }\n    \n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::report_error_message(int id,\n                                          const char *file_name, const int line,\n                                                  const char *message)\n    //--------------------------------------------------------------------------\n    {\n      log_run.error(id, \"LEGION ERROR: %s (from file %s:%d)\",\n                    message, file_name, line);\n      abort();\n    }\n    \n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::report_warning_message(\n                                         int id,\n                                         const char *file_name, const int line,\n                                         const char *message)\n    //--------------------------------------------------------------------------\n    {\n      log_run.warning(id, \"LEGION WARNING: %s (from file %s:%d)\",\n                      message, file_name, line);\n      if (Runtime::the_runtime && Runtime::the_runtime->warnings_backtrace)\n      {\n        Realm::Backtrace bt;\n        bt.capture_backtrace();\n        bt.lookup_symbols();\n        log_run.warning() << bt;\n      }\n#ifdef LEGION_WARNINGS_FATAL\n      abort();\n#endif\n    }\n\n#if defined(LEGION_PRIVILEGE_CHECKS) || defined(LEGION_BOUNDS_CHECKS)\n    //--------------------------------------------------------------------------\n    /*static*/ const char* Runtime::find_privilege_task_name(void *impl)\n    //--------------------------------------------------------------------------\n    {\n      PhysicalRegionImpl *region = static_cast<PhysicalRegionImpl*>(impl);\n      return region->get_task_name();\n    }\n#endif\n\n#ifdef LEGION_BOUNDS_CHECKS\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::check_bounds(void *impl, ptr_t ptr)\n    //--------------------------------------------------------------------------\n    {\n      PhysicalRegionImpl *region = static_cast<PhysicalRegionImpl*>(impl);\n      if (!region->contains_ptr(ptr))\n      {\n        fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                       \"pointer %lld\\n\", region->get_task_name(), ptr.value);\n        assert(false);\n      }\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::check_bounds(void *impl, \n                                          const DomainPoint &dp)\n    //--------------------------------------------------------------------------\n    {\n      PhysicalRegionImpl *region = static_cast<PhysicalRegionImpl*>(impl);\n      if (!region->contains_point(dp))\n      {\n        switch(dp.get_dim())\n        {\n          case 1:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                           \"1D point (%lld)\\n\", region->get_task_name(),\n                            dp.point_data[0]);\n            break;\n#if LEGION_MAX_DIM >= 2\n          case 2:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                           \"2D point (%lld,%lld)\\n\", region->get_task_name(),\n                            dp.point_data[0], dp.point_data[1]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 3\n          case 3:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                         \"3D point (%lld,%lld,%lld)\\n\", region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 4\n          case 4:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                         \"4D point (%lld,%lld,%lld,%lld)\\n\", \n                          region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2],\n                          dp.point_data[3]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 5\n          case 5:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                         \"5D point (%lld,%lld,%lld,%lld,%lld)\\n\", \n                          region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2],\n                          dp.point_data[3], dp.point_data[4]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 6\n          case 6:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                         \"6D point (%lld,%lld,%lld,%lld,%lld,%lld)\\n\", \n                          region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2],\n                          dp.point_data[3], dp.point_data[4], dp.point_data[5]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 7\n          case 7:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                         \"7D point (%lld,%lld,%lld,%lld,%lld,%lld,%lld)\\n\", \n                          region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2],\n                          dp.point_data[3], dp.point_data[4], dp.point_data[5],\n                          dp.point_data[6]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 8\n          case 8:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                         \"8D point (%lld,%lld,%lld,%lld,%lld,%lld,%lld,%lld)\\n\",\n                          region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2],\n                          dp.point_data[3], dp.point_data[4], dp.point_data[5],\n                          dp.point_data[6], dp.point_data[7]);\n            break;\n#endif\n#if LEGION_MAX_DIM >= 9\n          case 9:\n            fprintf(stderr,\"BOUNDS CHECK ERROR IN TASK %s: Accessing invalid \"\n                   \"9D point (%lld,%lld,%lld,%lld,%lld,%lld,%lld,%lld,%lld)\\n\",\n                          region->get_task_name(),\n                          dp.point_data[0], dp.point_data[1], dp.point_data[2],\n                          dp.point_data[3], dp.point_data[4], dp.point_data[5],\n                          dp.point_data[6], dp.point_data[7], dp.point_data[8]);\n            break;\n#endif\n          default:\n            assert(false);\n        }\n        assert(false);\n      }\n    }\n#endif\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::initialize_runtime_task(const void *args, \n               size_t arglen, const void *userdata, size_t userlen, Processor p)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#endif\n      Runtime *runtime = *((Runtime**)userdata); \n      implicit_runtime = runtime;\n      runtime->initialize_runtime();\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::shutdown_runtime_task(const void *args, \n               size_t arglen, const void *userdata, size_t userlen, Processor p)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#endif\n      Runtime *runtime = *((Runtime**)userdata); \n      implicit_runtime = runtime;\n      // Finalize the runtime and then delete it\n      runtime->finalize_runtime();\n      delete runtime;\n      // Handle a little shutdown race condition here where the \n      // runtime_startup_event on nodes other than zero may not \n      // have triggered yet before shutdown\n      if (!runtime_started_event.has_triggered())\n        runtime_started_event.wait();\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::legion_runtime_task(\n                                  const void *args, size_t arglen, \n\t\t\t\t  const void *userdata, size_t userlen,\n\t\t\t\t  Processor p)\n    //--------------------------------------------------------------------------\n    {\n      Runtime *runtime = *((Runtime**)userdata);\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#if !defined(LEGION_MALLOC_INSTANCES) && !defined(LEGION_USE_CUDA)\n      // Meta-tasks can run on application processors only when there\n      // are no utility processors for us to use\n      if (!runtime->local_utils.empty())\n        assert(implicit_context == NULL); // this better hold\n#endif\n#endif\n      implicit_runtime = runtime;\n      // We immediately bump the priority of all meta-tasks once they start\n      // up to the highest level to ensure that they drain once they begin\n      Processor::set_current_task_priority(LG_RUNNING_PRIORITY);\n      const char *data = (const char*)args;\n      implicit_provenance = *((const UniqueID*)data);\n      data += sizeof(implicit_provenance);\n      arglen -= sizeof(implicit_provenance);\n      LgTaskID tid = *((const LgTaskID*)data);\n#ifdef DEBUG_LEGION_WAITS\n      meta_task_id = tid;\n#endif\n      data += sizeof(tid);\n      arglen -= sizeof(tid);\n      switch (tid)\n      {\n        case LG_SCHEDULER_ID:\n          {\n            const ProcessorManager::SchedulerArgs *sched_args = \n              (const ProcessorManager::SchedulerArgs*)args;\n            runtime->process_schedule_request(sched_args->proc);\n            break;\n          }\n        case LG_MESSAGE_ID:\n          {\n            runtime->process_message_task(data, arglen);\n            break;\n          }\n        case LG_POST_END_ID:\n          {\n            InnerContext::handle_post_end_task(args); \n            break;\n          }\n        case LG_DEFERRED_READY_TRIGGER_ID:\n          {\n            const Operation::DeferredReadyArgs *deferred_ready_args = \n              (const Operation::DeferredReadyArgs*)args;\n            deferred_ready_args->proxy_this->trigger_ready();\n            break;\n          }\n        case LG_DEFERRED_RESOLUTION_TRIGGER_ID:\n          {\n            const Operation::DeferredResolutionArgs *deferred_resolution_args =\n              (const Operation::DeferredResolutionArgs*)args;\n            deferred_resolution_args->proxy_this->trigger_resolution();\n            break;\n          }\n        case LG_DEFERRED_COMMIT_TRIGGER_ID:\n          {\n            const Operation::DeferredCommitTriggerArgs *deferred_commit_args =\n              (const Operation::DeferredCommitTriggerArgs*)args;\n            deferred_commit_args->proxy_this->deferred_commit_trigger(\n                deferred_commit_args->gen);\n            break;\n          }\n        case LG_DEFERRED_EXECUTE_ID:\n          {\n            const Operation::DeferredExecArgs *deferred_exec_args = \n              (const Operation::DeferredExecArgs*)args;\n            deferred_exec_args->proxy_this->complete_execution();\n            break;\n          }\n        case LG_DEFERRED_EXECUTION_TRIGGER_ID:\n          {\n            const Operation::DeferredExecuteArgs *deferred_mapping_args = \n              (const Operation::DeferredExecuteArgs*)args;\n            deferred_mapping_args->proxy_this->deferred_execute();\n            break;\n          }\n        case LG_DEFERRED_COMPLETE_ID:\n          {\n            const Operation::DeferredCompleteArgs *deferred_complete_args =\n              (const Operation::DeferredCompleteArgs*)args;\n            deferred_complete_args->proxy_this->complete_operation();\n            break;\n          } \n        case LG_DEFERRED_COMMIT_ID:\n          {\n            const Operation::DeferredCommitArgs *deferred_commit_args = \n              (const Operation::DeferredCommitArgs*)args;\n            deferred_commit_args->proxy_this->commit_operation(\n                deferred_commit_args->deactivate);\n            break;\n          }\n        case LG_DEFERRED_COLLECT_ID:\n          {\n            const PhysicalManager::GarbageCollectionArgs *collect_args =\n              (const PhysicalManager::GarbageCollectionArgs*)args;\n            CollectableView::handle_deferred_collect(collect_args->view,\n                                                    *collect_args->to_collect);\n            delete collect_args->to_collect;\n            break;\n          }\n        case LG_PRE_PIPELINE_ID:\n          {\n            InnerContext::handle_prepipeline_stage(args);\n            break;\n          }\n        case LG_TRIGGER_DEPENDENCE_ID:\n          {\n            InnerContext::handle_dependence_stage(args);\n            break;\n          }\n        case LG_TRIGGER_COMPLETE_ID:\n          {\n            const Operation::TriggerCompleteArgs *trigger_complete_args =\n              (const Operation::TriggerCompleteArgs*)args;\n            trigger_complete_args->proxy_this->trigger_complete();\n            break;\n          }\n        case LG_TRIGGER_OP_ID:\n          {\n            // Key off of args here instead of data\n            const Operation::TriggerOpArgs *trigger_args = \n                            (const Operation::TriggerOpArgs*)args;\n            trigger_args->op->trigger_mapping();\n            break;\n          }\n        case LG_TRIGGER_TASK_ID:\n          {\n            // Key off of args here instead of data\n            const TaskOp::TriggerTaskArgs *trigger_args = \n                          (const TaskOp::TriggerTaskArgs*)args;\n            trigger_args->op->trigger_mapping(); \n            break;\n          }\n        case LG_DEFER_MAPPER_SCHEDULER_TASK_ID:\n          {\n            ProcessorManager::handle_defer_mapper(args);\n            break;\n          }\n        case LG_DEFERRED_RECYCLE_ID:\n          {\n            const DeferredRecycleArgs *deferred_recycle_args = \n              (const DeferredRecycleArgs*)args;\n            runtime->free_distributed_id(deferred_recycle_args->did);\n            break;\n          }\n        case LG_MUST_INDIV_ID:\n          {\n            MustEpochOp::handle_trigger_individual(args);\n            break;\n          }\n        case LG_MUST_INDEX_ID:\n          {\n            MustEpochOp::handle_trigger_index(args);\n            break;\n          }\n        case LG_MUST_MAP_ID:\n          {\n            MustEpochOp::handle_map_task(args);\n            break;\n          }\n        case LG_MUST_DIST_ID:\n          {\n            MustEpochOp::handle_distribute_task(args);\n            break;\n          }\n        case LG_MUST_LAUNCH_ID:\n          {\n            MustEpochOp::handle_launch_task(args);\n            break;\n          }\n        case LG_DEFERRED_FUTURE_SET_ID:\n          {\n            TaskOp::DeferredFutureSetArgs *future_args =  \n              (TaskOp::DeferredFutureSetArgs*)args;\n            const size_t result_size = \n              future_args->task_op->check_future_size(future_args->result);\n            if (result_size > 0)\n              future_args->target->set_result(\n                  future_args->result->get_untyped_result(),\n                  result_size, false/*own*/);\n            if (future_args->target->remove_base_gc_ref(DEFERRED_TASK_REF))\n              delete (future_args->target);\n            if (future_args->result->remove_base_gc_ref(DEFERRED_TASK_REF)) \n              delete (future_args->result);\n            break;\n          }\n        case LG_DEFERRED_FUTURE_MAP_SET_ID:\n          {\n            TaskOp::DeferredFutureMapSetArgs *future_args = \n              (TaskOp::DeferredFutureMapSetArgs*)args;\n            const size_t result_size = \n              future_args->task_op->check_future_size(future_args->result);\n            const void *result = future_args->result->get_untyped_result();\n            for (Domain::DomainPointIterator itr(future_args->domain); \n                  itr; itr++)\n            {\n              Future f = \n                future_args->future_map->get_future(itr.p, true/*internal*/);\n              if (result_size > 0)\n                f.impl->set_result(result, result_size, false/*own*/);\n            }\n            if (future_args->future_map->remove_base_gc_ref(\n                                                        DEFERRED_TASK_REF))\n              delete (future_args->future_map);\n            if (future_args->result->remove_base_gc_ref(FUTURE_HANDLE_REF))\n              delete (future_args->result);\n            future_args->task_op->complete_execution();\n            break;\n          }\n        case LG_RESOLVE_FUTURE_PRED_ID:\n          {\n            FuturePredOp::ResolveFuturePredArgs *resolve_args = \n              (FuturePredOp::ResolveFuturePredArgs*)args;\n            resolve_args->future_pred_op->resolve_future_predicate();\n            resolve_args->future_pred_op->remove_predicate_reference();\n            break;\n          }\n        case LG_CONTRIBUTE_COLLECTIVE_ID:\n          {\n            FutureImpl::handle_contribute_to_collective(args);\n            break;\n          }\n        case LG_TOP_FINISH_TASK_ID:\n          {\n            TopFinishArgs *fargs = (TopFinishArgs*)args; \n            fargs->ctx->free_remote_contexts();\n            if (fargs->ctx->remove_reference())\n              delete fargs->ctx;\n            // Finally tell the runtime that we have one less top level task\n            runtime->decrement_outstanding_top_level_tasks();\n            break;\n          }\n        case LG_MAPPER_TASK_ID:\n          {\n            MapperTaskArgs *margs = (MapperTaskArgs*)args;\n            runtime->process_mapper_task_result(margs);\n            // Now indicate that we are done with the future\n            if (margs->future->remove_base_gc_ref(FUTURE_HANDLE_REF))\n              delete margs->future;\n            // We can also deactivate the enclosing context \n            if (margs->ctx->remove_reference())\n              delete margs->ctx;\n            // Finally tell the runtime we have one less top level task\n            runtime->decrement_outstanding_top_level_tasks();\n            break;\n          }\n        case LG_DISJOINTNESS_TASK_ID:\n          {\n            RegionTreeForest *forest = runtime->forest;\n            IndexPartNode::handle_disjointness_computation(args, forest);\n            break;\n          }\n        case LG_DEFER_PHYSICAL_REGISTRATION_TASK_ID:\n          {\n            runtime->forest->handle_defer_registration(args);\n            break;\n          }\n        case LG_PART_INDEPENDENCE_TASK_ID:\n          {\n            IndexSpaceNode::DynamicIndependenceArgs *dargs = \n              (IndexSpaceNode::DynamicIndependenceArgs*)args;\n            IndexSpaceNode::handle_disjointness_test(\n                dargs->parent, dargs->left, dargs->right);\n            break;\n          }\n        case LG_SPACE_INDEPENDENCE_TASK_ID:\n          {\n            IndexPartNode::DynamicIndependenceArgs *dargs = \n              (IndexPartNode::DynamicIndependenceArgs*)args;\n            IndexPartNode::handle_disjointness_test(\n                dargs->parent, dargs->left, dargs->right);\n            break;\n          }\n        case LG_POST_DECREMENT_TASK_ID:\n          {\n            InnerContext::PostDecrementArgs *dargs = \n              (InnerContext::PostDecrementArgs*)args;\n            runtime->activate_context(dargs->parent_ctx);\n            break;\n          }\n        case LG_ISSUE_FRAME_TASK_ID:\n          {\n            InnerContext::IssueFrameArgs *fargs = \n              (InnerContext::IssueFrameArgs*)args;\n            fargs->parent_ctx->perform_frame_issue(fargs->frame, \n                                                   fargs->frame_termination);\n            break;\n          }\n        case LG_MAPPER_CONTINUATION_TASK_ID:\n          {\n            MapperContinuation::handle_continuation(args);\n            break;\n          }\n        case LG_TASK_IMPL_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            TaskImpl::SemanticRequestArgs *req_args = \n              (TaskImpl::SemanticRequestArgs*)args;\n            req_args->proxy_this->process_semantic_request(\n                          req_args->tag, req_args->source, \n                          false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_INDEX_SPACE_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            IndexSpaceNode::SemanticRequestArgs *req_args = \n              (IndexSpaceNode::SemanticRequestArgs*)args;\n            req_args->proxy_this->process_semantic_request(\n                          req_args->tag, req_args->source, \n                          false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_INDEX_PART_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            IndexPartNode::SemanticRequestArgs *req_args = \n              (IndexPartNode::SemanticRequestArgs*)args;\n            req_args->proxy_this->process_semantic_request(\n                          req_args->tag, req_args->source, \n                          false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_FIELD_SPACE_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            FieldSpaceNode::SemanticRequestArgs *req_args = \n              (FieldSpaceNode::SemanticRequestArgs*)args;\n            req_args->proxy_this->process_semantic_request(\n                          req_args->tag, req_args->source, \n                          false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_FIELD_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            FieldSpaceNode::SemanticFieldRequestArgs *req_args = \n              (FieldSpaceNode::SemanticFieldRequestArgs*)args;\n            req_args->proxy_this->process_semantic_field_request(\n                  req_args->fid, req_args->tag, req_args->source, \n                  false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_DEFER_FIELD_INFOS_TASK_ID:\n          {\n            FieldSpaceNode::handle_defer_infos_request(args);\n            break;\n          }\n        case LG_DEFER_COMPUTE_EQ_SETS_TASK_ID:\n          {\n            RegionNode::handle_deferred_compute_equivalence_sets(args);\n            break;\n          }\n        case LG_REGION_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            RegionNode::SemanticRequestArgs *req_args = \n              (RegionNode::SemanticRequestArgs*)args;\n            req_args->proxy_this->process_semantic_request(\n                          req_args->tag, req_args->source, \n                          false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_PARTITION_SEMANTIC_INFO_REQ_TASK_ID:\n          {\n            PartitionNode::SemanticRequestArgs *req_args = \n              (PartitionNode::SemanticRequestArgs*)args;\n            req_args->proxy_this->process_semantic_request(\n                          req_args->tag, req_args->source, \n                          false, false, RtUserEvent::NO_RT_USER_EVENT);\n            break;\n          }\n        case LG_INDEX_SPACE_DEFER_CHILD_TASK_ID:\n          {\n            IndexSpaceNode::defer_node_child_request(args);\n            break;\n          }\n        case LG_INDEX_PART_DEFER_CHILD_TASK_ID:\n          {\n            IndexPartNode::defer_node_child_request(args);\n            break;\n          }\n        case LG_SELECT_TUNABLE_TASK_ID:\n          {\n            const SelectTunableArgs *tunable_args = \n              (const SelectTunableArgs*)args;\n            runtime->perform_tunable_selection(tunable_args);\n            // Remove the reference that we added\n            if (tunable_args->result->remove_base_gc_ref(FUTURE_HANDLE_REF)) \n              delete (tunable_args->result);\n            if (tunable_args->args != NULL)\n              free(tunable_args->args);\n            break;\n          }\n        case LG_DEFERRED_ENQUEUE_OP_ID:\n          {\n            const Operation::DeferredEnqueueArgs *deferred_enqueue_args = \n              (const Operation::DeferredEnqueueArgs*)args;\n            deferred_enqueue_args->proxy_this->enqueue_ready_operation(\n                RtEvent::NO_RT_EVENT, deferred_enqueue_args->priority);\n            break;\n          }\n        case LG_DEFERRED_ENQUEUE_TASK_ID:\n          {\n            const TaskOp::DeferredEnqueueArgs *enqueue_args = \n              (const TaskOp::DeferredEnqueueArgs*)args;\n            enqueue_args->manager->add_to_ready_queue(enqueue_args->task);\n            break;\n          }\n        case LG_DEFER_MAPPER_MESSAGE_TASK_ID:\n          {\n            MapperManager::handle_deferred_message(args);\n            break;\n          }\n        case LG_REMOTE_VIEW_CREATION_TASK_ID:\n          {\n            InnerContext::handle_remote_view_creation(args);\n            break;\n          }\n        case LG_DEFER_DISTRIBUTE_TASK_ID:\n          {\n            const TaskOp::DeferDistributeArgs *dargs = \n              (const TaskOp::DeferDistributeArgs*)args;\n            if (dargs->proxy_this->distribute_task())\n              dargs->proxy_this->launch_task();\n            break;\n          }\n        case LG_DEFER_PERFORM_MAPPING_TASK_ID:\n          {\n            const TaskOp::DeferMappingArgs *margs = \n              (const TaskOp::DeferMappingArgs*)args;\n            const RtEvent deferred = \n              margs->proxy_this->perform_mapping(margs->must_op, margs);\n            // Once we've no longer been deferred then we can trigger\n            // the done event to signal we are done\n            if (!deferred.exists())\n              Runtime::trigger_event(margs->done_event);\n            break;\n          }\n        case LG_DEFER_LAUNCH_TASK_ID:\n          {\n            const TaskOp::DeferLaunchArgs *largs = \n              (const TaskOp::DeferLaunchArgs*)args;\n            largs->proxy_this->launch_task();\n            break;\n          }\n        case LG_MISSPECULATE_TASK_ID:\n          {\n            const SingleTask::MisspeculationTaskArgs *targs = \n              (const SingleTask::MisspeculationTaskArgs*)args;\n            targs->task->handle_misspeculation();\n            break;\n          }\n        case LG_DEFER_TRIGGER_TASK_COMPLETE_TASK_ID:\n          {\n            SingleTask::handle_deferred_task_complete(args);\n            break;\n          }\n        case LG_DEFER_FIND_COPY_PRE_TASK_ID:\n          {\n            InstanceView::handle_view_find_copy_pre_request(args, runtime);\n            break;\n          }\n        case LG_DEFER_MATERIALIZED_VIEW_TASK_ID:\n          {\n            MaterializedView::handle_defer_materialized_view(args, runtime);\n            break;\n          }\n        case LG_DEFER_REDUCTION_VIEW_TASK_ID:\n          {\n            ReductionView::handle_defer_reduction_view(args, runtime);\n            break;\n          }\n        case LG_DEFER_PHI_VIEW_REF_TASK_ID:\n          {\n            PhiView::handle_deferred_view_ref(args);\n            break;\n          }\n        case LG_DEFER_PHI_VIEW_REGISTRATION_TASK_ID:\n          {\n            PhiView::handle_deferred_view_registration(args);\n            break;\n          }\n        case LG_CONTROL_REP_LAUNCH_TASK_ID:\n          {\n            ShardManager::handle_launch(args);\n            break;\n          }\n        case LG_CONTROL_REP_DELETE_TASK_ID:\n          {\n            ShardManager::handle_delete(args);\n            break;\n          }\n        case LG_RECLAIM_FUTURE_MAP_TASK_ID:\n          {\n            ReplFutureMapImpl::handle_future_map_reclaim(args);\n            break;\n          }\n        case LG_TIGHTEN_INDEX_SPACE_TASK_ID:\n          {\n            IndexSpaceExpression::handle_tighten_index_space(args);\n            break;\n          }\n        case LG_REMOTE_PHYSICAL_REQUEST_TASK_ID:\n          {\n            RemoteContext::defer_physical_request(args, runtime);\n            break;\n          }\n        case LG_REMOTE_PHYSICAL_RESPONSE_TASK_ID:\n          {\n            RemoteContext::defer_physical_response(args);\n            break;\n          }\n        case LG_REPLAY_SLICE_ID:\n          {\n            PhysicalTemplate::handle_replay_slice(args);\n            break;\n          }\n        case LG_DELETE_TEMPLATE_ID:\n          {\n            PhysicalTemplate::handle_delete_template(args);\n            break;\n          }\n        case LG_DEFER_MAKE_OWNER_TASK_ID:\n          {\n            EquivalenceSet::handle_make_owner(args);\n            break;\n          }\n        case LG_DEFER_PENDING_REPLICATION_TASK_ID:\n          {\n            EquivalenceSet::handle_pending_replication(args);\n            break;\n          }\n        case LG_DEFER_APPLY_STATE_TASK_ID:\n          {\n            EquivalenceSet::handle_apply_state(args);\n            break;\n          }\n        case LG_DEFER_RELEASE_REF_TASK_ID:\n          {\n            EquivalenceSet::handle_remove_refs(args);\n            break;\n          }\n        case LG_DEFER_REMOTE_REF_UPDATE_TASK_ID:\n          {\n            DistributedCollectable::handle_defer_remote_reference_update(\n                                                            runtime, args);\n            break;\n          }\n        case LG_COPY_FILL_AGGREGATION_TASK_ID:\n          {\n            CopyFillAggregator::handle_aggregation(args);\n            break;\n          }\n        case LG_COPY_FILL_DELETION_TASK_ID:\n          {\n            CopyFillGuard::handle_deletion(args);\n            break;\n          }\n        case LG_FINALIZE_EQ_SETS_TASK_ID:\n          {\n            VersionManager::handle_finalize_eq_sets(args);\n            break;\n          }\n        case LG_DEFERRED_COPY_ACROSS_TASK_ID:\n          {\n            CopyOp::handle_deferred_across(args);\n            break;\n          }\n        case LG_DEFER_REMOTE_OP_DELETION_TASK_ID:\n          {\n            RemoteOp::handle_deferred_deletion(args);\n            break;\n          }\n        case LG_DEFER_PERFORM_TRAVERSAL_TASK_ID:\n          {\n            PhysicalAnalysis::handle_deferred_traversal(args);\n            break;\n          }\n        case LG_DEFER_PERFORM_REMOTE_TASK_ID:\n          {\n            PhysicalAnalysis::handle_deferred_remote(args);\n            break;\n          }\n        case LG_DEFER_PERFORM_UPDATE_TASK_ID:\n          {\n            PhysicalAnalysis::handle_deferred_update(args);\n            break;\n          }\n        case LG_DEFER_PERFORM_OUTPUT_TASK_ID:\n          {\n            PhysicalAnalysis::handle_deferred_output(args);\n            break;\n          }\n        case LG_DEFER_INDIVIDUAL_MANAGER_TASK_ID:\n          {\n            IndividualManager::handle_defer_manager(args, runtime);\n            break;\n          }\n        case LG_DEFER_DELETE_INDIVIDUAL_MANAGER_TASK_ID:\n          {\n            IndividualManager::handle_defer_perform_deletion(args, runtime);\n            break;\n          }\n        case LG_DEFER_COLLECTIVE_MANAGER_TASK_ID:\n          {\n            CollectiveManager::handle_defer_manager(args, runtime);\n            break;\n          }\n        case LG_DEFER_VERIFY_PARTITION_TASK_ID:\n          {\n            InnerContext::handle_partition_verification(args);\n            break;\n          }\n        case LG_DEFER_REMOVE_REMOTE_REFS_TASK_ID:\n          {\n            InnerContext::handle_remove_remote_references(args);\n            break;\n          }\n        case LG_DEFER_RELEASE_ACQUIRED_TASK_ID:\n          {\n            Operation::handle_deferred_release(args);\n            break;\n          }\n        case LG_DEFER_DISJOINT_COMPLETE_TASK_ID:\n          {\n            ReplicateContext::handle_defer_disjoint_complete_response(runtime,\n                                                                      args);\n            break;\n          }\n        case LG_DEFER_FINALIZE_PENDING_SET_TASK_ID:\n          {\n            PendingEquivalenceSet::handle_defer_finalize(args);\n            break;\n          }\n        case LG_FREE_EAGER_INSTANCE_TASK_ID:\n          {\n            MemoryManager::handle_free_eager_instance(args);\n            break;\n          }\n        case LG_FINALIZE_OUTPUT_ID:\n          {\n            OutputRegionImpl::handle_finalize_output(args);\n            break;\n          }\n#ifdef LEGION_MALLOC_INSTANCES\n        // LG_MALLOC_INSTANCE_TASK_ID should always run app processor\n        case LG_FREE_INSTANCE_TASK_ID:\n          {\n            MemoryManager::handle_free_instance(args);\n            break;\n          }\n#endif\n        case LG_DEFER_TRACE_PRECONDITION_TASK_ID:\n          {\n            TraceConditionSet::handle_precondition_test(args);\n            break;\n          }\n        case LG_DEFER_TRACE_POSTCONDITION_TASK_ID:\n          {\n            TraceConditionSet::handle_postcondition_test(args);\n            break;\n          }\n        case LG_DEFER_TRACE_FINALIZE_SETS_TASK_ID:\n          {\n            TraceConditionSet::handle_finalize_sets(args);\n            break;\n          }\n        case LG_DEFER_TRACE_UPDATE_TASK_ID:\n          {\n            ShardedPhysicalTemplate::handle_deferred_trace_update(args,runtime);\n            break;\n          }\n        case LG_DEFER_CONSENSUS_MATCH_TASK_ID:\n          {\n            ConsensusMatchBase::handle_consensus_match(args);\n            break;\n          }\n        case LG_YIELD_TASK_ID:\n          break; // nothing to do here\n        case LG_RETRY_SHUTDOWN_TASK_ID:\n          {\n            const ShutdownManager::RetryShutdownArgs *shutdown_args = \n              (const ShutdownManager::RetryShutdownArgs*)args;\n            runtime->initiate_runtime_shutdown(runtime->address_space,\n                                               shutdown_args->phase);\n            break;\n          }\n        default:\n          assert(false); // should never get here\n      }\n#ifdef DEBUG_LEGION\n      if (tid < LG_BEGIN_SHUTDOWN_TASK_IDS)\n        runtime->decrement_total_outstanding_tasks(tid, true/*meta*/);\n#else\n      if (tid < LG_BEGIN_SHUTDOWN_TASK_IDS)\n        runtime->decrement_total_outstanding_tasks();\n#endif\n#ifdef DEBUG_SHUTDOWN_HANG\n      __sync_fetch_and_add(&runtime->outstanding_counts[tid],-1);\n#endif\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::profiling_runtime_task(\n                                   const void *args, size_t arglen, \n\t\t\t\t   const void *userdata, size_t userlen,\n\t\t\t\t   Processor p)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#endif\n      Runtime *runtime = *((Runtime**)userdata);\n      implicit_runtime = runtime;\n      Realm::ProfilingResponse response(args, arglen);\n      const ProfilingResponseBase *base = \n        (const ProfilingResponseBase*)response.user_data();\n      if (base->handler == NULL)\n      {\n        // If we got a NULL let's assume they meant the profiler\n        // this mainly happens with messages that cross nodes\n        runtime->profiler->handle_profiling_response(base,response,args,arglen);\n      }\n      else\n        base->handler->handle_profiling_response(base, response, args, arglen);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::startup_runtime_task(\n                                   const void *args, size_t arglen, \n\t\t\t\t   const void *userdata, size_t userlen,\n\t\t\t\t   Processor p)\n    //--------------------------------------------------------------------------\n    {\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#endif\n      Runtime *runtime = *((Runtime**)userdata);\n      implicit_runtime = runtime;\n      runtime->startup_runtime();\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::endpoint_runtime_task(\n                                   const void *args, size_t arglen, \n\t\t\t\t   const void *userdata, size_t userlen,\n\t\t\t\t   Processor p)\n    //--------------------------------------------------------------------------\n    {\n      Runtime *runtime = *((Runtime**)userdata);\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#endif\n      Deserializer derez(args, arglen);\n      runtime->handle_endpoint_creation(derez);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void Runtime::application_processor_runtime_task(\n                                   const void *args, size_t arglen, \n\t\t\t\t   const void *userdata, size_t userlen,\n\t\t\t\t   Processor p)\n    //--------------------------------------------------------------------------\n    {\n      Runtime *runtime = *((Runtime**)userdata);\n#ifdef DEBUG_LEGION\n      assert(userlen == sizeof(Runtime**));\n#endif\n      implicit_runtime = runtime;\n      // We immediately bump the priority of all meta-tasks once they start\n      // up to the highest level to ensure that they drain once they begin\n      Processor::set_current_task_priority(LG_RUNNING_PRIORITY);\n      const char *data = (const char*)args;\n      implicit_provenance = *((const UniqueID*)data);\n      data += sizeof(implicit_provenance);\n      arglen -= sizeof(implicit_provenance);\n      LgTaskID tid = *((const LgTaskID*)data);\n      data += sizeof(tid);\n      arglen -= sizeof(tid);\n      switch (tid)\n      {\n        case LG_FUTURE_CALLBACK_TASK_ID:\n          {\n            FutureImpl::handle_callback(args);\n            break;\n          }\n        case LG_REPLAY_SLICE_ID:\n          {\n            PhysicalTemplate::handle_replay_slice(args);\n            break;\n          }\n#ifdef LEGION_MALLOC_INSTANCES\n        case LG_MALLOC_INSTANCE_TASK_ID:\n          {\n            MemoryManager::handle_malloc_instance(args);\n            break;\n          }\n        case LG_FREE_INSTANCE_TASK_ID:\n          {\n            MemoryManager::handle_free_instance(args);\n            break;\n          }\n#endif\n        default:\n          assert(false); // should never get here\n      }\n#ifdef DEBUG_LEGION\n      runtime->decrement_total_outstanding_tasks(tid, true/*meta*/);\n#else\n      runtime->decrement_total_outstanding_tasks();\n#endif\n#ifdef DEBUG_SHUTDOWN_HANG\n      __sync_fetch_and_add(&runtime->outstanding_counts[tid],-1);\n#endif\n    }\n\n#ifdef TRACE_ALLOCATION\n    //--------------------------------------------------------------------------\n    /*static*/ void LegionAllocation::trace_allocation(\n                                       AllocationType a, size_t size, int elems)\n    //--------------------------------------------------------------------------\n    {\n      Runtime *rt = Runtime::the_runtime;\n      if (rt != NULL)\n        rt->trace_allocation(a, size, elems);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void LegionAllocation::trace_free(AllocationType a, \n                                                 size_t size, int elems)\n    //--------------------------------------------------------------------------\n    {\n      Runtime *rt = Runtime::the_runtime;\n      if (rt != NULL)\n        rt->trace_free(a, size, elems);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ Runtime* LegionAllocation::find_runtime(void)\n    //--------------------------------------------------------------------------\n    {\n      return Runtime::the_runtime;\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void LegionAllocation::trace_allocation(Runtime *&runtime,\n                                       AllocationType a, size_t size, int elems)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime == NULL)\n      {\n        runtime = LegionAllocation::find_runtime();\n        // Only happens during initialization\n        if (runtime == NULL)\n          return;\n      }\n      runtime->trace_allocation(a, size, elems);\n    }\n\n    //--------------------------------------------------------------------------\n    /*static*/ void LegionAllocation::trace_free(Runtime *&runtime,\n                                       AllocationType a, size_t size, int elems)\n    //--------------------------------------------------------------------------\n    {\n      if (runtime == NULL)\n      {\n        runtime = LegionAllocation::find_runtime();\n        // Only happens during intialization\n        if (runtime == NULL)\n          return;\n      }\n      runtime->trace_free(a, size, elems);\n    }\n#endif \n\n  }; // namespace Internal \n}; // namespace Legion \n\n// EOF\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/language/src/regent/cudahelper.t": "-- Copyright 2021 Stanford University, Los Alamos National Laboratory\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal ast = require(\"regent/ast\")\nlocal base = require(\"regent/std_base\")\nlocal config = require(\"regent/config\").args()\nlocal data = require(\"common/data\")\nlocal report = require(\"common/report\")\n\nlocal cudahelper = {}\n\n-- Exit early if the user turned off CUDA code generation\n\nif config[\"cuda\"] == 0 then\n  function cudahelper.check_cuda_available()\n    return false\n  end\n  return cudahelper\nend\n\nlocal c = base.c\nlocal ef = terralib.externfunction\nlocal externcall_builtin = terralib.externfunction\nlocal cudapaths = { OSX = \"/usr/local/cuda/lib/libcuda.dylib\";\n                    Linux =  \"libcuda.so\";\n                    Windows = \"nvcuda.dll\"; }\n\n-- #####################################\n-- ## CUDA Hijack API\n-- #################\n\nlocal HijackAPI = terralib.includec(\"regent_cudart_hijack.h\")\n\nstruct fat_bin_t {\n  magic : int,\n  versions : int,\n  data : &opaque,\n  filename : &opaque,\n}\n\n-- #####################################\n-- ## CUDA Device API\n-- #################\n\nlocal struct CUctx_st\nlocal struct CUmod_st\nlocal struct CUlinkState_st\nlocal struct CUfunc_st\nlocal CUdevice = int32\nlocal CUjit_option = uint32\nlocal CU_JIT_ERROR_LOG_BUFFER = 5\nlocal CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES = 6\nlocal CU_JIT_INPUT_PTX = 1\nlocal CU_JIT_TARGET = 9\nlocal DriverAPI = {\n  cuInit = ef(\"cuInit\", {uint32} -> uint32);\n  cuCtxGetCurrent = ef(\"cuCtxGetCurrent\", {&&CUctx_st} -> uint32);\n  cuCtxGetDevice = ef(\"cuCtxGetDevice\",{&int32} -> uint32);\n  cuDeviceGet = ef(\"cuDeviceGet\",{&int32,int32} -> uint32);\n  cuCtxCreate_v2 = ef(\"cuCtxCreate_v2\",{&&CUctx_st,uint32,int32} -> uint32);\n  cuCtxDestroy = ef(\"cuCtxDestroy\",{&CUctx_st} -> uint32);\n  cuDeviceComputeCapability = ef(\"cuDeviceComputeCapability\",\n    {&int32,&int32,int32} -> uint32);\n}\n\nlocal RuntimeAPI = false\ndo\n  if not terralib.cudacompile then\n    function cudahelper.check_cuda_available()\n      return false, \"Terra is built without CUDA support\"\n    end\n  else\n    -- Try to load the CUDA runtime header\n    pcall(function() RuntimeAPI = terralib.includec(\"cuda_runtime.h\") end)\n\n    if RuntimeAPI == nil then\n      function cudahelper.check_cuda_available()\n        return false, \"cuda_runtime.h does not exist in INCLUDE_PATH\"\n      end\n    elseif config[\"offline\"] or config[\"cuda-offline\"] then\n      function cudahelper.check_cuda_available()\n        return true\n      end\n    else\n      local dlfcn = terralib.includec(\"dlfcn.h\")\n      local terra has_symbol(symbol : rawstring)\n        var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n        var has_symbol = dlfcn.dlsym(lib, symbol) ~= [&opaque](0)\n        dlfcn.dlclose(lib)\n        return has_symbol\n      end\n\n      if has_symbol(\"cuInit\") then\n        local r = DriverAPI.cuInit(0)\n        if r == 0 then\n          function cudahelper.check_cuda_available()\n            return true\n          end\n        else\n          function cudahelper.check_cuda_available()\n            return false, \"calling cuInit(0) failed for some reason (CUDA devices might not exist)\"\n          end\n        end\n      else\n        function cudahelper.check_cuda_available()\n          return false, \"the cuInit function is missing (Regent might have been installed without CUDA support)\"\n        end\n      end\n    end\n  end\nend\n\ndo\n  local available, error_message = cudahelper.check_cuda_available()\n  if not available then\n    if config[\"cuda\"] == 1 then\n      print(\"CUDA code generation failed since \" .. error_message)\n      os.exit(-1)\n    else\n      return cudahelper\n    end\n  end\nend\n\n-- Declare the API calls that are deprecated in CUDA SDK 10\n-- TODO: We must move on to the new execution control API as these old functions\n--       can be dropped in the future.\nlocal ExecutionAPI = {\n  cudaConfigureCall =\n    ef(\"cudaConfigureCall\", {RuntimeAPI.dim3, RuntimeAPI.dim3, uint64, RuntimeAPI.cudaStream_t} -> uint32);\n  cudaSetupArgument = ef(\"cudaSetupArgument\", {&opaque, uint64, uint64} -> uint32);\n  cudaLaunch = ef(\"cudaLaunch\", {&opaque} -> uint32);\n}\n\ndo\n  local ffi = require('ffi')\n  local cudaruntimelinked = false\n  function cudahelper.link_driver_library()\n    if cudaruntimelinked then return end\n    local path = assert(cudapaths[ffi.os],\"unknown OS?\")\n    base.linklibrary(path)\n    cudaruntimelinked = true\n  end\nend\n\n-- #####################################\n-- ## Printf for CUDA (not exposed to the user for the moment)\n-- #################\n\nlocal vprintf = ef(\"cudart:vprintf\", {&int8,&int8} -> int)\n\nlocal function createbuffer(args)\n  local Buf = terralib.types.newstruct()\n  for i,e in ipairs(args) do\n    local typ = e:gettype()\n    local field = \"_\"..tonumber(i)\n    typ = typ == float and double or typ\n    table.insert(Buf.entries,{field,typ})\n  end\n  return quote\n    var buf : Buf\n    escape\n        for i,e in ipairs(args) do\n            emit quote\n               buf.[\"_\"..tonumber(i)] = e\n            end\n        end\n    end\n  in\n    [&int8](&buf)\n  end\nend\n\nlocal cuda_printf = macro(function(fmt,...)\n  local buf = createbuffer({...})\n  return `vprintf(fmt,buf)\nend)\n\n-- #####################################\n-- ## Supported CUDA compute versions\n-- #################\n\nlocal supported_archs = {\n  [\"fermi\"]   = 20,\n  [\"kepler\"]  = 30,\n  [\"k20\"]     = 35,\n  [\"maxwell\"] = 52,\n  [\"pascal\"]  = 60,\n  [\"volta\"]   = 70,\n}\n\nlocal function parse_cuda_arch(arch)\n  arch = string.lower(arch)\n  local sm = supported_archs[arch]\n  if sm == nil then\n    local archs\n    for k, v in pairs(supported_archs) do\n      archs = (not archs and k) or (archs and archs .. \", \" .. k)\n    end\n    print(\"Error: Unsupported GPU architecture \" .. arch ..\n          \". Supported architectures: \" .. archs)\n    os.exit(1)\n  end\n  return sm\nend\n\n-- #####################################\n-- ## Registration functions\n-- #################\n\nlocal terra register_ptx(ptxc : rawstring, ptxSize : uint32, version : uint64) : &&opaque\n  var fat_bin : &fat_bin_t\n  var fat_size = sizeof(fat_bin_t)\n  -- TODO: this line is leaking memory\n  fat_bin = [&fat_bin_t](c.malloc(fat_size))\n  base.assert(fat_size == 0 or fat_bin ~= nil, \"malloc failed in register_ptx\")\n  fat_bin.magic = 1234\n  fat_bin.versions = 5678\n  var fat_data_size = ptxSize + 1\n  fat_bin.data = c.malloc(fat_data_size)\n  base.assert(fat_data_size == 0 or fat_bin.data ~= nil, \"malloc failed in register_ptx\")\n  fat_bin.data = ptxc\n  var handle = HijackAPI.hijackCudaRegisterFatBinary(fat_bin)\n  return handle\nend\n\nlocal terra register_function(handle : &&opaque, id : int, name : &int8)\n  HijackAPI.hijackCudaRegisterFunction(handle, [&int8](id), name)\nend\n\nlocal function find_device_library(target)\n  local device_lib_dir = terralib.cudahome .. \"/nvvm/libdevice/\"\n  local libdevice = nil\n  for f in io.popen(\"ls \" .. device_lib_dir):lines() do\n    local version = tonumber(string.match(string.match(f, \"[0-9][0-9][.]\"), \"[0-9][0-9]\"))\n    if version <= target then\n      libdevice = device_lib_dir .. f\n    end\n  end\n  assert(libdevice ~= nil, \"Failed to find a device library\")\n  return libdevice\nend\n\nlocal get_cuda_version\ndo\n  local cached_cuda_version = nil\n  local terra get_cuda_version_terra() : uint64\n    var cx : &CUctx_st\n    var cx_created = false\n    var r = DriverAPI.cuCtxGetCurrent(&cx)\n    base.assert(r == 0, \"CUDA error in cuCtxGetCurrent\")\n    var device : int32\n    if cx ~= nil then\n      r = DriverAPI.cuCtxGetDevice(&device)\n      base.assert(r == 0, \"CUDA error in cuCtxGetDevice\")\n    else\n      r = DriverAPI.cuDeviceGet(&device, 0)\n      base.assert(r == 0, \"CUDA error in cuDeviceGet\")\n      r = DriverAPI.cuCtxCreate_v2(&cx, 0, device)\n      base.assert(r == 0, \"CUDA error in cuCtxCreate_v2\")\n      cx_created = true\n    end\n\n    var major : int, minor : int\n    r = DriverAPI.cuDeviceComputeCapability(&major, &minor, device)\n    base.assert(r == 0, \"CUDA error in cuDeviceComputeCapability\")\n    var version = [uint64](major * 10 + minor)\n    if cx_created then\n      DriverAPI.cuCtxDestroy(cx)\n    end\n    return version\n  end\n\n  get_cuda_version = function()\n    if cached_cuda_version ~= nil then\n      return cached_cuda_version\n    end\n    if not (config[\"offline\"] or config[\"cuda-offline\"]) then\n      cached_cuda_version = get_cuda_version_terra()\n    else\n      cached_cuda_version = parse_cuda_arch(config[\"cuda-arch\"])\n    end\n    return cached_cuda_version\n  end\nend\n\nfunction cudahelper.jit_compile_kernels_and_register(kernels)\n  local module = {}\n  for k, v in pairs(kernels) do\n    module[v.name] = v.kernel\n  end\n  local version = get_cuda_version()\n  local libdevice = find_device_library(tonumber(version))\n  local llvmbc = terralib.linkllvm(libdevice)\n  externcall_builtin = function(name, ftype)\n    return llvmbc:extern(name, ftype)\n  end\n  local ptx = cudalib.toptx(module, nil, version)\n\n  local ptxc = terralib.constant(ptx)\n  local handle = terralib.newsymbol(&&opaque, \"handle\")\n  local register = quote\n    var [handle] = register_ptx(ptxc, [ptx:len() + 1], [version])\n  end\n\n  for k, v in pairs(kernels) do\n    register = quote\n      [register]\n      register_function([handle], [k], [v.name])\n    end\n  end\n\n  return register\nend\n\n-- #####################################\n-- ## Primitives\n-- #################\n\nlocal THREAD_BLOCK_SIZE = 128\nlocal NUM_THREAD_X = 16\nlocal NUM_THREAD_Y = THREAD_BLOCK_SIZE / NUM_THREAD_X\nlocal MAX_NUM_BLOCK = 32768\nlocal GLOBAL_RED_BUFFER = 256\nassert(GLOBAL_RED_BUFFER % THREAD_BLOCK_SIZE == 0)\n\nlocal tid_x   = cudalib.nvvm_read_ptx_sreg_tid_x\nlocal n_tid_x = cudalib.nvvm_read_ptx_sreg_ntid_x\nlocal bid_x   = cudalib.nvvm_read_ptx_sreg_ctaid_x\nlocal n_bid_x = cudalib.nvvm_read_ptx_sreg_nctaid_x\n\nlocal tid_y   = cudalib.nvvm_read_ptx_sreg_tid_y\nlocal n_tid_y = cudalib.nvvm_read_ptx_sreg_ntid_y\nlocal bid_y   = cudalib.nvvm_read_ptx_sreg_ctaid_y\nlocal n_bid_y = cudalib.nvvm_read_ptx_sreg_nctaid_y\n\nlocal tid_z   = cudalib.nvvm_read_ptx_sreg_tid_z\nlocal n_tid_z = cudalib.nvvm_read_ptx_sreg_ntid_z\nlocal bid_z   = cudalib.nvvm_read_ptx_sreg_ctaid_z\nlocal n_bid_z = cudalib.nvvm_read_ptx_sreg_nctaid_z\n\nlocal barrier = cudalib.nvvm_barrier0\n\nlocal supported_scalar_red_ops = {\n  [\"+\"]   = true,\n  [\"*\"]   = true,\n  [\"max\"] = true,\n  [\"min\"] = true,\n}\n\nfunction cudahelper.global_thread_id()\n  local bid = `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\n  local num_threads = `(n_tid_x())\n  return `([bid] * [num_threads] + tid_x())\nend\n\nfunction cudahelper.global_block_id()\n  return `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\nend\n\nfunction cudahelper.get_thread_block_size()\n  return THREAD_BLOCK_SIZE\nend\n\nfunction cudahelper.get_num_thread_x()\n  return NUM_THREAD_X\nend\n\nfunction cudahelper.get_num_thread_y()\n  return NUM_THREAD_Y\nend\n\n-- Slow atomic operation implementations (copied and modified from Ebb)\nlocal terra cas_uint64(address : &uint64, compare : uint64, value : uint64)\n  return terralib.asm(terralib.types.uint64,\n                      \"atom.global.cas.b64 $0, [$1], $2, $3;\",\n                      \"=l,l,l,l\", true, address, compare, value)\nend\ncas_uint64:setinlined(true)\n\nlocal terra cas_uint32(address : &uint32, compare : uint32, value : uint32)\n  return terralib.asm(terralib.types.uint32,\n                      \"atom.global.cas.b32 $0, [$1], $2, $3;\",\n                      \"=r,l,r,r\", true, address, compare, value)\nend\ncas_uint32:setinlined(true)\n\nfunction cudahelper.generate_atomic_update(op, typ)\n  if terralib.llvmversion <= 38 then\n    if op == \"+\" and typ == float then\n      return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f32.p0f32\",\n                                {&float,float} -> {float})\n    elseif op == \"+\" and typ == double and get_cuda_version() >= 60 then\n      return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f64.p0f64\",\n                                {&double,double} -> {double})\n    end\n  end\n\n  local cas_type\n  local cas_func\n  if sizeof(typ) == 4 then\n    cas_type = uint32\n    cas_func = cas_uint32\n  else\n    assert(sizeof(typ) == 8)\n    cas_type = uint64\n    cas_func = cas_uint64\n  end\n  local terra atomic_op(address : &typ, operand : typ)\n    var old : typ = @address\n    var assumed : typ\n    var new     : typ\n\n    var new_b     : &cas_type = [&cas_type](&new)\n    var assumed_b : &cas_type = [&cas_type](&assumed)\n    var res       :  cas_type\n\n    var mask = false\n    repeat\n      if not mask then\n        assumed = old\n        new     = [base.quote_binary_op(op, assumed, operand)]\n        res     = cas_func([&cas_type](address), @assumed_b, @new_b)\n        old     = @[&typ](&res)\n        mask    = @assumed_b == @[&cas_type](&old)\n      end\n    until mask\n  end\n  atomic_op:setinlined(true)\n  return atomic_op\nend\n\nlocal function generate_element_reduction(lhs, rhs, op, volatile)\n  if volatile then\n    return quote\n      do\n        var v = [base.quote_binary_op(op, lhs, rhs)]\n        terralib.attrstore(&[lhs], v, { isvolatile = true })\n      end\n    end\n  else\n    return quote\n      [lhs] = [base.quote_binary_op(op, lhs, rhs)]\n    end\n  end\nend\n\nlocal function generate_element_reductions(lhs, rhs, op, type, volatile)\n  local actions = terralib.newlist()\n  if type:isarray() then\n    for k = 1, type.N do -- inclusive!\n      local lhs = `([lhs][ [k - 1] ])\n      local rhs = `([rhs][ [k - 1] ])\n      actions:insert(generate_element_reduction(lhs, rhs, op, volatile))\n    end\n  else\n    assert(type:isprimitive())\n    actions:insert(generate_element_reduction(lhs, rhs, op, volatile))\n  end\n  return quote [actions] end\nend\n\n-- #####################################\n-- ## Code generation for scalar reduction\n-- #################\n\nfunction cudahelper.compute_reduction_buffer_size(cx, node, reductions)\n  local size = 0\n  for k, v in pairs(reductions) do\n    if not supported_scalar_red_ops[v] then\n      report.error(node,\n          \"Scalar reduction with operator \" .. v .. \" is not supported yet\")\n    elseif not (sizeof(k.type) == 4 or sizeof(k.type) == 8) then\n      report.error(node,\n          \"Scalar reduction for type \" .. tostring(k.type) .. \" is not supported yet\")\n    end\n    size = size + THREAD_BLOCK_SIZE * sizeof(k.type)\n  end\n  size = size + cx:compute_reduction_buffer_size()\n  return size\nend\n\nlocal internal_kernel_id = 2 ^ 30\nlocal internal_kernels = {}\nlocal INTERNAL_KERNEL_PREFIX = \"__internal\"\n\nfunction cudahelper.get_internal_kernels()\n  return internal_kernels\nend\n\ncudahelper.generate_buffer_init_kernel = terralib.memoize(function(type, op)\n  local value = base.reduction_op_init[op][type]\n  local op_name = base.reduction_ops[op].name\n  local kernel_id = internal_kernel_id\n  internal_kernel_id = internal_kernel_id - 1\n  local kernel_name =\n    INTERNAL_KERNEL_PREFIX .. \"__init__\" .. tostring(type) ..\n    \"__\" .. tostring(op_name) .. \"__\"\n  local terra init(buffer : &type)\n    var tid = tid_x() + bid_x() * n_tid_x()\n    buffer[tid] = [value]\n  end\n  init:setname(kernel_name)\n  internal_kernels[kernel_id] = {\n    name = kernel_name,\n    kernel = init,\n  }\n  return kernel_id\nend)\n\ncudahelper.generate_buffer_reduction_kernel = terralib.memoize(function(type, op)\n  local value = base.reduction_op_init[op][type]\n  local op_name = base.reduction_ops[op].name\n  local kernel_id = internal_kernel_id\n  internal_kernel_id = internal_kernel_id - 1\n  local kernel_name =\n    INTERNAL_KERNEL_PREFIX .. \"__red__\" .. tostring(type) ..\n    \"__\" .. tostring(op_name) .. \"__\"\n\n  local tid = terralib.newsymbol(c.size_t, \"tid\")\n  local input = terralib.newsymbol(&type, \"input\")\n  local result = terralib.newsymbol(&type, \"result\")\n  local shared_mem_ptr = cudalib.sharedmemory(type, THREAD_BLOCK_SIZE)\n\n  local shared_mem_init = `([input][ [tid] ])\n  for i = 1, (GLOBAL_RED_BUFFER / THREAD_BLOCK_SIZE) - 1 do\n    shared_mem_init =\n      base.quote_binary_op(op, shared_mem_init,\n                           `([input][ [tid] + [i * THREAD_BLOCK_SIZE] ]))\n  end\n  local terra red([input], [result])\n    var [tid] = tid_x()\n    [shared_mem_ptr][ [tid] ] = [shared_mem_init]\n    barrier()\n    [cudahelper.generate_reduction_tree(tid, shared_mem_ptr, THREAD_BLOCK_SIZE, op, type)]\n    barrier()\n    if [tid] == 0 then [result][0] = [shared_mem_ptr][ [tid] ] end\n  end\n\n  red:setname(kernel_name)\n  internal_kernels[kernel_id] = {\n    name = kernel_name,\n    kernel = red,\n  }\n  return kernel_id\nend)\n\nfunction cudahelper.generate_reduction_preamble(cx, reductions)\n  local preamble = terralib.newlist()\n  local device_ptrs = terralib.newlist()\n  local device_ptrs_map = {}\n  local host_ptrs_map = {}\n\n  for red_var, red_op in pairs(reductions) do\n    local device_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local host_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local init_kernel_id = cudahelper.generate_buffer_init_kernel(red_var.type, red_op)\n    local init_args = terralib.newlist({device_ptr})\n    preamble:insert(quote\n      var [device_ptr] = [&red_var.type](nil)\n      var [host_ptr] = [&red_var.type](nil)\n      do\n        var bounds : c.legion_rect_1d_t\n        bounds.lo.x[0] = 0\n        bounds.hi.x[0] = [sizeof(red_var.type) * GLOBAL_RED_BUFFER - 1]\n        var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.GPU_FB_MEM, [&int8](nil))\n        [device_ptr] =\n          [&red_var.type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n        [cudahelper.codegen_kernel_call(cx, init_kernel_id, GLOBAL_RED_BUFFER, init_args, 0, true)]\n      end\n      do\n        var bounds : c.legion_rect_1d_t\n        bounds.lo.x[0] = 0\n        bounds.hi.x[0] = [sizeof(red_var.type) - 1]\n        var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.Z_COPY_MEM, [&int8](nil))\n        [host_ptr] =\n          [&red_var.type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n      end\n    end)\n    device_ptrs:insert(device_ptr)\n    device_ptrs_map[device_ptr] = red_var\n    host_ptrs_map[device_ptr] = host_ptr\n  end\n\n  return device_ptrs, device_ptrs_map, host_ptrs_map, preamble\nend\n\nfunction cudahelper.generate_reduction_tree(tid, shared_mem_ptr, num_threads, red_op, type)\n  local outer_reductions = terralib.newlist()\n  local step = num_threads\n  while step > 64 do\n    step = step / 2\n    outer_reductions:insert(quote\n      if [tid] < step then\n        [generate_element_reductions(`([shared_mem_ptr][ [tid] ]),\n                                     `([shared_mem_ptr][ [tid] + [step] ]),\n                                     red_op, type, false)]\n      end\n      barrier()\n    end)\n  end\n  local unrolled_reductions = terralib.newlist()\n  while step > 1 do\n    step = step / 2\n    unrolled_reductions:insert(quote\n      [generate_element_reductions(`([shared_mem_ptr][ [tid] ]),\n                                   `([shared_mem_ptr][ [tid] + [step] ]),\n                                   red_op, type, false)]\n      barrier()\n    end)\n  end\n  if #outer_reductions > 0 then\n    return quote\n      [outer_reductions]\n      if [tid] < 32 then\n        [unrolled_reductions]\n      end\n    end\n  else\n    return quote\n      [unrolled_reductions]\n    end\n  end\nend\n\nfunction cudahelper.generate_reduction_kernel(cx, reductions, device_ptrs_map)\n  local preamble = terralib.newlist()\n  local postamble = terralib.newlist()\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local shared_mem_ptr =\n      cudalib.sharedmemory(red_var.type, THREAD_BLOCK_SIZE)\n    local init = base.reduction_op_init[red_op][red_var.type]\n    preamble:insert(quote\n      var [red_var] = [init]\n      [shared_mem_ptr][ tid_x() ] = [red_var]\n    end)\n\n    local tid = terralib.newsymbol(c.size_t, \"tid\")\n    local reduction_tree =\n      cudahelper.generate_reduction_tree(tid, shared_mem_ptr, THREAD_BLOCK_SIZE, red_op, red_var.type)\n    postamble:insert(quote\n      do\n        var [tid] = tid_x()\n        var bid = [cudahelper.global_block_id()]\n        [shared_mem_ptr][ [tid] ] = [red_var]\n        barrier()\n        [reduction_tree]\n        if [tid] == 0 then\n          [cudahelper.generate_atomic_update(red_op, red_var.type)](\n            &[device_ptr][bid % [GLOBAL_RED_BUFFER] ], [shared_mem_ptr][ [tid] ])\n        end\n      end\n    end)\n  end\n\n  preamble:insertall(cx:generate_preamble())\n  postamble:insertall(cx:generate_postamble())\n\n  return preamble, postamble\nend\n\nfunction cudahelper.generate_reduction_postamble(cx, reductions, device_ptrs_map, host_ptrs_map)\n  local postamble = quote end\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local red_kernel_id = cudahelper.generate_buffer_reduction_kernel(red_var.type, red_op)\n    local host_ptr = host_ptrs_map[device_ptr]\n    local red_args = terralib.newlist({device_ptr, host_ptr})\n    local shared_mem_size = terralib.sizeof(red_var.type) * THREAD_BLOCK_SIZE\n    postamble = quote\n      [postamble];\n      [cudahelper.codegen_kernel_call(cx, red_kernel_id, THREAD_BLOCK_SIZE, red_args, shared_mem_size, true)]\n    end\n  end\n\n  local needs_sync = true\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    if needs_sync then\n      postamble = quote\n        [postamble];\n        RuntimeAPI.cudaDeviceSynchronize()\n      end\n      needs_sync = false\n    end\n    local red_op = reductions[red_var]\n    local host_ptr = host_ptrs_map[device_ptr]\n    postamble = quote\n      [postamble];\n      [red_var] = [base.quote_binary_op(red_op, red_var, `([host_ptr][0]))]\n    end\n  end\n\n  return postamble\nend\n\n-- #####################################\n-- ## Code generation for parallel prefix operators\n-- #################\n\nlocal NUM_BANKS = 16\nlocal bank_offset = macro(function(e)\n  return `(e / [NUM_BANKS])\nend)\n\nlocal function generate_prefix_op_kernel(shmem, tid, num_leaves, op, init, left_to_right)\n  return quote\n    do\n      var oa = 2 * [tid] + 1\n      var ob = 2 * [tid] + 2 * [left_to_right]\n      var d : int = [num_leaves] >> 1\n      var offset = 1\n      while d > 0 do\n        barrier()\n        if [tid]  < d then\n          var ai : int = offset * oa - [left_to_right]\n          var bi : int = offset * ob - [left_to_right]\n          ai = ai + bank_offset(ai)\n          bi = bi + bank_offset(bi)\n          [shmem][bi] = [base.quote_binary_op(op, `([shmem][ai]), `([shmem][bi]))]\n        end\n        offset = offset << 1\n        d = d >> 1\n      end\n      if [tid] == 0 then\n        var idx = ([num_leaves] - [left_to_right]) % [num_leaves]\n        [shmem][idx + bank_offset(idx)] = [init]\n      end\n      d = 1\n      while d <= [num_leaves] do\n        offset = offset >> 1\n        barrier()\n        if [tid] < d and offset > 0 then\n          var ai = offset * oa - [left_to_right]\n          var bi = offset * ob - [left_to_right]\n          ai = ai + bank_offset(ai)\n          bi = bi + bank_offset(bi)\n          var x = [shmem][ai]\n          [shmem][ai] = [shmem][bi]\n          [shmem][bi] = [base.quote_binary_op(op, x, `([shmem][bi]))]\n        end\n        d = d << 1\n      end\n      barrier()\n    end\n  end\nend\n\nlocal function generate_prefix_op_prescan(shmem, lhs, rhs, lhs_ptr, rhs_ptr, res, idx, dir, op, init)\n  local prescan_full, prescan_arbitrary\n  local NUM_LEAVES = THREAD_BLOCK_SIZE * 2\n\n  local function advance_ptrs(lhs_ptr, rhs_ptr, bid)\n    if lhs_ptr == rhs_ptr then\n      return quote\n        [lhs_ptr] = &([lhs_ptr][ bid * [NUM_LEAVES] ])\n      end\n    else\n      return quote\n        [lhs_ptr] = &([lhs_ptr][ bid * [NUM_LEAVES] ])\n        [rhs_ptr] = &([rhs_ptr][ bid * [NUM_LEAVES] ])\n      end\n    end\n  end\n\n  terra prescan_full([lhs_ptr],\n                     [rhs_ptr],\n                     [dir])\n    var [idx]\n    var t = tid_x()\n    var bid = [cudahelper.global_block_id()]\n    [advance_ptrs(lhs_ptr, rhs_ptr, bid)]\n    var lr = [int]([dir] >= 0)\n\n    [idx].__ptr = t\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n    [idx].__ptr = [idx].__ptr + [THREAD_BLOCK_SIZE]\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n\n    [generate_prefix_op_kernel(shmem, t, NUM_LEAVES, op, init, lr)]\n\n    var [res]\n    [idx].__ptr = t\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n    [idx].__ptr = [idx].__ptr + [THREAD_BLOCK_SIZE]\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n  end\n\n  terra prescan_arbitrary([lhs_ptr],\n                          [rhs_ptr],\n                          num_elmts : c.size_t,\n                          num_leaves : c.size_t,\n                          [dir])\n    var [idx]\n    var t = tid_x()\n    var lr = [int]([dir] >= 0)\n\n    [idx].__ptr = t\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n    [idx].__ptr = [idx].__ptr + (num_leaves / 2)\n    if [idx].__ptr < num_elmts then\n      [rhs.actions]\n      [shmem][ [idx].__ptr + bank_offset([idx].__ptr) ] = [rhs.value]\n    else\n      [shmem][ [idx].__ptr + bank_offset([idx].__ptr) ] = [init]\n    end\n\n    [generate_prefix_op_kernel(shmem, t, num_leaves, op, init, lr)]\n\n    var [res]\n    [idx].__ptr = t\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op,\n        `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n    [idx].__ptr = [idx].__ptr + (num_leaves / 2)\n    if [idx].__ptr < num_elmts then\n      [rhs.actions]\n      [res] = [base.quote_binary_op(op,\n          `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n      [lhs.actions]\n    end\n  end\n\n  return prescan_full, prescan_arbitrary\nend\n\nlocal function generate_prefix_op_scan(shmem, lhs_wr, lhs_rd, lhs_ptr, res, idx, dir, op, init)\n  local scan_full, scan_arbitrary\n  local NUM_LEAVES = THREAD_BLOCK_SIZE * 2\n\n  terra scan_full([lhs_ptr],\n                  offset : uint64,\n                  [dir])\n    var [idx]\n    var t = tid_x()\n    var bid = [cudahelper.global_block_id()]\n    [lhs_ptr] = &([lhs_ptr][ bid * [offset] * [NUM_LEAVES] ])\n    var lr = [int]([dir] >= 0)\n\n    var tidx = t\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n    tidx = tidx + [THREAD_BLOCK_SIZE]\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n\n    [generate_prefix_op_kernel(shmem, t, NUM_LEAVES, op, init, lr)]\n\n    var [res]\n    tidx = t\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n    [lhs_wr.actions]\n    tidx = tidx + [THREAD_BLOCK_SIZE]\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n    [lhs_wr.actions]\n  end\n\n  terra scan_arbitrary([lhs_ptr],\n                       num_elmts : c.size_t,\n                       num_leaves : c.size_t,\n                       offset : c.size_t,\n                       [dir])\n    var [idx]\n    var t = tid_x()\n    var lr = [int]([dir] >= 0)\n\n    if lr == 1 then\n      var tidx = t\n      [idx].__ptr = (tidx + 1) * [offset] - 1\n      [lhs_rd.actions]\n      [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n\n      tidx = t + num_leaves / 2\n      if tidx < [num_elmts] then\n        [idx].__ptr = (tidx + 1) * [offset] - 1\n        [lhs_rd.actions]\n        [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      else\n        [shmem][tidx + bank_offset(tidx)] = [init]\n      end\n    else\n      var tidx = t\n      [idx].__ptr = tidx * [offset]\n      [lhs_rd.actions]\n      [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      tidx = t + num_leaves / 2\n      if tidx < [num_elmts] then\n        [idx].__ptr = tidx * [offset]\n        [lhs_rd.actions]\n        [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      else\n        [shmem][tidx + bank_offset(tidx)] = [init]\n      end\n    end\n\n    [generate_prefix_op_kernel(shmem, t, num_leaves, op, init, lr)]\n\n    var [res]\n    if lr == 1 then\n      var tidx = t\n      [idx].__ptr = (tidx + 1) * [offset] - 1\n      [lhs_rd.actions]\n      [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n      [lhs_wr.actions]\n      tidx = tidx + num_leaves / 2\n      if [tidx] < [num_elmts] then\n        [idx].__ptr = (tidx + 1) * [offset] - 1\n        [lhs_rd.actions]\n        [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n        [lhs_wr.actions]\n      end\n    else\n      var tidx = t\n      [idx].__ptr = tidx * [offset]\n      [lhs_rd.actions]\n      [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n      [lhs_wr.actions]\n      tidx = tidx + num_leaves / 2\n      if [tidx] < [num_elmts] then\n        [idx].__ptr = tidx * [offset]\n        [lhs_rd.actions]\n        [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n        [lhs_wr.actions]\n      end\n    end\n  end\n\n  return scan_full, scan_arbitrary\nend\n\n-- This function expects lhs and rhs to be the values from the following expressions.\n--\n--   * lhs: lhs[idx] = res\n--   * rhs: rhs[idx]\n--\n-- The code generator below captures 'idx' and 'res' to change the meaning of these values\nfunction cudahelper.generate_prefix_op_kernels(lhs_wr, lhs_rd, rhs, lhs_ptr, rhs_ptr,\n                                               res, idx, dir, op, elem_type)\n  local BLOCK_SIZE = THREAD_BLOCK_SIZE * 2\n  local shmem = cudalib.sharedmemory(elem_type, BLOCK_SIZE)\n  local init = base.reduction_op_init[op][elem_type]\n\n  local prescan_full, prescan_arbitrary =\n    generate_prefix_op_prescan(shmem, lhs_wr, rhs, lhs_ptr, rhs_ptr, res, idx, dir, op, init)\n\n  local scan_full, scan_arbitrary =\n    generate_prefix_op_scan(shmem, lhs_wr, lhs_rd, lhs_ptr, res, idx, dir, op, init)\n\n  local terra postscan_full([lhs_ptr],\n                            offset : uint64,\n                            num_elmts : uint64,\n                            [dir])\n    var t = [cudahelper.global_thread_id()]\n    if t >= num_elmts - [BLOCK_SIZE] or t % [BLOCK_SIZE] == [BLOCK_SIZE - 1] then return end\n\n    var sum_loc = t / [BLOCK_SIZE] * [BLOCK_SIZE] + [BLOCK_SIZE - 1]\n    var val_loc = t + [BLOCK_SIZE]\n    var [idx], [res]\n    if [dir] >= 0 then\n      [idx].__ptr = sum_loc * [offset] + ([offset] - 1)\n      [lhs_rd.actions]\n      var v1 = [lhs_rd.value]\n\n      [idx].__ptr = val_loc * [offset] + ([offset] - 1)\n      [lhs_rd.actions]\n      var v2 = [lhs_rd.value]\n\n      [res] = [base.quote_binary_op(op, v1, v2)]\n      [lhs_wr.actions]\n    else\n      var t = [cudahelper.global_thread_id()]\n      if t % [BLOCK_SIZE] == [BLOCK_SIZE - 1] then return end\n\n      [idx].__ptr = (num_elmts - 1 - sum_loc) * [offset]\n      [lhs_rd.actions]\n      var v1 = [lhs_rd.value]\n\n      [idx].__ptr = (num_elmts - 1 - val_loc) * [offset]\n      [lhs_rd.actions]\n      var v2 = [lhs_rd.value]\n\n      [res] = [base.quote_binary_op(op, v1, v2)]\n      [lhs_wr.actions]\n    end\n  end\n\n  return prescan_full, prescan_arbitrary, scan_full, scan_arbitrary, postscan_full\nend\n\nfunction cudahelper.generate_parallel_prefix_op(cx, variant, total, lhs_wr, lhs_rd, rhs, lhs_ptr,\n                                                rhs_ptr, res, idx, dir, op, elem_type)\n  local BLOCK_SIZE = THREAD_BLOCK_SIZE * 2\n  local SHMEM_SIZE = terralib.sizeof(elem_type) * THREAD_BLOCK_SIZE * 2\n\n  local pre_full, pre_arb, scan_full, scan_arb, post_full, post2, post3 =\n    cudahelper.generate_prefix_op_kernels(lhs_wr, lhs_rd, rhs, lhs_ptr, rhs_ptr,\n                                          res, idx, dir, op, elem_type)\n  local prescan_full_id = variant:add_cuda_kernel(pre_full)\n  local prescan_arb_id = variant:add_cuda_kernel(pre_arb)\n  local scan_full_id = variant:add_cuda_kernel(scan_full)\n  local scan_arb_id = variant:add_cuda_kernel(scan_arb)\n  local postscan_full_id = variant:add_cuda_kernel(post_full)\n\n  local num_leaves = terralib.newsymbol(c.size_t, \"num_leaves\")\n  local num_elmts = terralib.newsymbol(c.size_t, \"num_elmts\")\n  local num_threads = terralib.newsymbol(c.size_t, \"num_threads\")\n  local offset = terralib.newsymbol(uint64, \"offset\")\n  local lhs_ptr_arg = terralib.newsymbol(lhs_ptr.type, lhs_ptr.name)\n  local rhs_ptr_arg = terralib.newsymbol(rhs_ptr.type, rhs_ptr.name)\n\n  local prescan_full_args = terralib.newlist()\n  prescan_full_args:insertall({lhs_ptr_arg, rhs_ptr_arg, dir})\n  local call_prescan_full =\n    cudahelper.codegen_kernel_call(cx, prescan_full_id, num_threads, prescan_full_args, SHMEM_SIZE, true)\n\n  local prescan_arb_args = terralib.newlist()\n  prescan_arb_args:insertall({lhs_ptr_arg, rhs_ptr_arg, num_elmts, num_leaves, dir})\n  local call_prescan_arbitrary =\n    cudahelper.codegen_kernel_call(cx, prescan_arb_id, num_threads, prescan_arb_args, SHMEM_SIZE, true)\n\n  local scan_full_args = terralib.newlist()\n  scan_full_args:insertall({lhs_ptr_arg, offset, dir})\n  local call_scan_full =\n    cudahelper.codegen_kernel_call(cx, scan_full_id, num_threads, scan_full_args, SHMEM_SIZE, true)\n\n  local scan_arb_args = terralib.newlist()\n  scan_arb_args:insertall({lhs_ptr_arg, num_elmts, num_leaves, offset, dir})\n  local call_scan_arbitrary =\n    cudahelper.codegen_kernel_call(cx, scan_arb_id, num_threads, scan_arb_args, SHMEM_SIZE, true)\n\n  local postscan_full_args = terralib.newlist()\n  postscan_full_args:insertall({lhs_ptr, offset, num_elmts, dir})\n  local call_postscan_full =\n    cudahelper.codegen_kernel_call(cx, postscan_full_id, num_threads, postscan_full_args, 0, true)\n\n  local terra recursive_scan :: {uint64,uint64,uint64,lhs_ptr.type,dir.type} -> {}\n\n  terra recursive_scan(remaining : uint64,\n                       [offset],\n                       [total],\n                       [lhs_ptr],\n                       [dir])\n    if remaining <= 1 then return end\n\n    var num_blocks : uint64 = remaining / [BLOCK_SIZE]\n\n    if num_blocks > 0 then\n      var [num_threads] = num_blocks * [THREAD_BLOCK_SIZE]\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = [lhs_ptr]\n      else\n        [lhs_ptr_arg] = &[lhs_ptr][(remaining % [BLOCK_SIZE]) * [offset]]\n      end\n      [call_scan_full]\n    end\n    if remaining % [BLOCK_SIZE] > 0 then\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = &[lhs_ptr][ num_blocks * [BLOCK_SIZE] * [offset] ]\n      else\n        [lhs_ptr_arg] = [lhs_ptr]\n      end\n      var [num_elmts] = remaining % [BLOCK_SIZE]\n      var [num_leaves] = [BLOCK_SIZE]\n      while [num_leaves] / 2 > [num_elmts] do\n        [num_leaves] = [num_leaves] / 2\n      end\n      var [num_threads] = [num_leaves] / 2\n      [call_scan_arbitrary]\n    end\n\n    var [lhs_ptr_arg]\n    if [dir] >= 0 then\n      [lhs_ptr_arg] = [lhs_ptr]\n    else\n      [lhs_ptr_arg] = &[lhs_ptr][ (remaining % [BLOCK_SIZE]) * [offset] ]\n    end\n\n    recursive_scan(num_blocks,\n                   [offset] * [BLOCK_SIZE],\n                   [total],\n                   [lhs_ptr_arg],\n                   [dir])\n\n    if [remaining] > [BLOCK_SIZE] then\n      var [num_elmts] = remaining\n      var [num_threads] = remaining - [BLOCK_SIZE]\n      [call_postscan_full]\n    end\n  end\n\n\n  local launch = quote\n    do\n      var num_blocks : uint64 = total / [BLOCK_SIZE]\n      if num_blocks > 0 then\n        var [lhs_ptr_arg]\n        var [rhs_ptr_arg]\n        var [num_threads] = num_blocks * [THREAD_BLOCK_SIZE]\n        if [dir] >= 0 then\n          [lhs_ptr_arg] = [lhs_ptr]\n          [rhs_ptr_arg] = [rhs_ptr]\n        else\n          [lhs_ptr_arg] = &[lhs_ptr][ total % [BLOCK_SIZE] ]\n          [rhs_ptr_arg] = &[rhs_ptr][ total % [BLOCK_SIZE] ]\n        end\n        [call_prescan_full]\n      end\n      if total % [BLOCK_SIZE] > 0 then\n        var [lhs_ptr_arg]\n        var [rhs_ptr_arg]\n        if [dir] >= 0 then\n          [lhs_ptr_arg] = &[lhs_ptr][ num_blocks * [BLOCK_SIZE] ]\n          [rhs_ptr_arg] = &[rhs_ptr][ num_blocks * [BLOCK_SIZE] ]\n        else\n          [lhs_ptr_arg] = [lhs_ptr]\n          [rhs_ptr_arg] = [rhs_ptr]\n        end\n        var [num_elmts] = total % [BLOCK_SIZE]\n        var [num_leaves] = [BLOCK_SIZE]\n        while [num_leaves] / 2 > [num_elmts] do\n          [num_leaves] = [num_leaves] / 2\n        end\n        var [num_threads] = [num_leaves] / 2\n        [call_prescan_arbitrary]\n      end\n\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = [lhs_ptr]\n      else\n        [lhs_ptr_arg] = &[lhs_ptr][ total % [BLOCK_SIZE] ]\n      end\n\n      recursive_scan(total / [BLOCK_SIZE],\n                     [BLOCK_SIZE],\n                     [total],\n                     [lhs_ptr_arg],\n                     [dir])\n\n      if total > [BLOCK_SIZE] then\n        var [offset] = 1\n        var [num_elmts] = total\n        var [num_threads] = total - [BLOCK_SIZE]\n        [call_postscan_full]\n      end\n    end\n  end\n\n  return launch\nend\n\nfunction cudahelper.codegen_kernel_call(cx, kernel_id, count, args, shared_mem_size, tight)\n  local setupArguments = terralib.newlist()\n\n  local offset = 0\n  for i = 1, #args do\n    local arg =  args[i]\n    local size = terralib.sizeof(arg.type)\n    setupArguments:insert(quote\n      ExecutionAPI.cudaSetupArgument(&[arg], size, offset)\n    end)\n    offset = offset + size\n  end\n\n  local grid = terralib.newsymbol(RuntimeAPI.dim3, \"grid\")\n  local block = terralib.newsymbol(RuntimeAPI.dim3, \"block\")\n  local num_blocks = terralib.newsymbol(int64, \"num_blocks\")\n\n  local function round_exp(v, n)\n    return `((v + (n - 1)) / n)\n  end\n\n  local launch_domain_init = nil\n  if not cx.use_2d_launch then\n    launch_domain_init = quote\n      if [count] <= THREAD_BLOCK_SIZE and tight then\n        [block].x, [block].y, [block].z = [count], 1, 1\n      else\n        [block].x, [block].y, [block].z = THREAD_BLOCK_SIZE, 1, 1\n      end\n      var [num_blocks] = [round_exp(count, THREAD_BLOCK_SIZE)]\n    end\n  else\n    launch_domain_init = quote\n      if [count] <= NUM_THREAD_X and tight then\n        [block].x, [block].y, [block].z = [count], NUM_THREAD_Y, 1\n      else\n        [block].x, [block].y, [block].z = NUM_THREAD_X, NUM_THREAD_Y, 1\n      end\n      var [num_blocks] = [round_exp(count, NUM_THREAD_X)]\n    end\n  end\n\n  launch_domain_init = quote\n    [launch_domain_init]\n    if [num_blocks] <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z = [num_blocks], 1, 1\n    elseif [count] / MAX_NUM_BLOCK <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, [round_exp(num_blocks, MAX_NUM_BLOCK)], 1\n    else\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, MAX_NUM_BLOCK,\n        [round_exp(num_blocks, MAX_NUM_BLOCK, MAX_NUM_BLOCK)]\n    end\n  end\n\n  return quote\n    if [count] > 0 then\n      var [grid], [block]\n      [launch_domain_init]\n      ExecutionAPI.cudaConfigureCall([grid], [block], shared_mem_size, nil)\n      [setupArguments]\n      ExecutionAPI.cudaLaunch([&int8](kernel_id))\n    end\n  end\nend\n\nlocal function get_nv_fn_name(name, type)\n  assert(type:isfloat())\n  local nv_name = \"__nv_\" .. name\n\n  -- Okay. a little divergence from the C standard...\n  if name == \"isnan\" or name == \"isinf\" then\n    if type == double then\n      nv_name = nv_name .. \"d\"\n    else\n      nv_name = nv_name .. \"f\"\n    end\n  -- Seriously?\n  elseif name == \"finite\" then\n    if type == double then\n      nv_name = \"__nv_isfinited\"\n    else\n      nv_name = \"__nv_finitef\"\n    end\n  elseif type == float then\n    nv_name = nv_name .. \"f\"\n  end\n  return nv_name\nend\n\nlocal function get_cuda_definition(self)\n  if self:has_variant(\"cuda\") then\n    return self:get_variant(\"cuda\")\n  else\n    local fn_type = self.super:get_definition().type\n    local fn_name = get_nv_fn_name(self:get_name(), self:get_arg_type())\n    assert(fn_name ~= nil)\n    local fn = externcall_builtin(fn_name, fn_type)\n    self:set_variant(\"cuda\", fn)\n    return fn\n  end\nend\n\nfunction cudahelper.get_cuda_variant(math_fn)\n  return math_fn:override(get_cuda_definition)\nend\n\n-- #####################################\n-- ## CUDA Codegen Context\n-- #################\n\nlocal context = {}\n\nfunction context:__index(field)\n  local value = context[field]\n  if value ~= nil then\n    return value\n  end\n  error(\"context has no field '\" .. field .. \"' (in lookup)\", 2)\nend\n\nfunction context:__newindex(field, value)\n  error(\"context has no field '\" .. field .. \"' (in assignment)\", 2)\nend\n\nfunction context.new(use_2d_launch, offset_2d)\n  local offset_2d = offset_2d or false\n  return setmetatable({\n    use_2d_launch = use_2d_launch,\n    offset_2d = offset_2d,\n    buffered_reductions = data.newmap(),\n  }, context)\nend\n\nfunction context:reduction_buffer(ref_type, value_type, op, generator)\n  local tbl = self.buffered_reductions[ref_type]\n  if tbl == nil then\n    tbl = {\n      buffer = cudalib.sharedmemory(value_type, THREAD_BLOCK_SIZE),\n      type = value_type,\n      op = op,\n      generator = generator,\n    }\n    self.buffered_reductions[ref_type] = tbl\n  end\n  return tbl\nend\n\nfunction context:compute_reduction_buffer_size()\n  local size = 0\n  for k, tbl in self.buffered_reductions:items() do\n    size = size + sizeof(tbl.type) * THREAD_BLOCK_SIZE\n  end\n  return size\nend\n\nfunction context:generate_preamble()\n  local preamble = terralib.newlist()\n\n  if self.use_2d_launch then\n    preamble:insert(quote\n      var [self.offset_2d:getsymbol()] = tid_y()\n    end)\n  end\n\n  for k, tbl in self.buffered_reductions:items() do\n    if tbl.type:isarray() then\n      local init = base.reduction_op_init[tbl.op][tbl.type.type]\n      preamble:insert(quote\n        for k = 0, [tbl.type.N] do\n          [tbl.buffer][ tid_y() + tid_x() * [NUM_THREAD_Y] ][k] = [init]\n        end\n      end)\n    else\n      local init = base.reduction_op_init[tbl.op][tbl.type]\n      preamble:insert(quote\n        [tbl.buffer][ tid_y() + tid_x() * [NUM_THREAD_Y] ] = [init]\n      end)\n    end\n  end\n\n  return preamble\nend\n\nfunction context:generate_postamble()\n  local postamble = terralib.newlist()\n\n  for k, tbl in self.buffered_reductions:items() do\n    postamble:insert(quote\n      do\n        var tid = tid_y()\n        var buf = &[tbl.buffer][ tid_x() * [NUM_THREAD_Y] ]\n        barrier()\n        [cudahelper.generate_reduction_tree(tid, buf, NUM_THREAD_Y, tbl.op, tbl.type)]\n        if tid == 0 then [tbl.generator(`(@buf))] end\n      end\n    end)\n  end\n\n  return postamble\nend\n\nlocal function check_2d_launch_profitable(node)\n  if not base.config[\"cuda-2d-launch\"] or not node:is(ast.typed.stat.ForList) then\n    return false, false\n  end\n  -- TODO: This is a very simple heurstic that does not even extend to 3D case.\n  --       At least we need to check if the inner loop has any centered accesses with\n  --       respect to that loop. In the longer term, we need a better algorithm to detect\n  --       cases where multi-dimensional kernel launches are profitable.\n  if #node.block.stats == 1 and node.block.stats[1]:is(ast.typed.stat.ForNum) then\n    local inner_loop = node.block.stats[1]\n    if inner_loop.metadata and inner_loop.metadata.parallelizable then\n      assert(#inner_loop.values == 2)\n      return true, base.newsymbol(inner_loop.symbol:gettype(), \"offset\")\n    end\n  end\n  return false, false\nend\n\nfunction cudahelper.new_kernel_context(node)\n  local use_2d_launch, offset_2d = check_2d_launch_profitable(node)\n  return context.new(use_2d_launch, offset_2d)\nend\n\nfunction cudahelper.optimize_loop(cx, node, block)\n  if cx.use_2d_launch then\n    local inner_loop = block.stats[1]\n    local index_type = inner_loop.symbol:gettype()\n    -- If the inner loop is eligible to a 2D kernel launch, we change the stride of the inner\n    -- loop accordingly.\n    inner_loop = inner_loop {\n      values = terralib.newlist({\n        ast.typed.expr.Binary {\n          op = \"+\",\n          lhs = inner_loop.values[1],\n          rhs = ast.typed.expr.ID {\n            value = cx.offset_2d,\n            expr_type = index_type,\n            annotations = ast.default_annotations(),\n            span = inner_loop.span,\n          },\n          expr_type = inner_loop.values[1].expr_type,\n          annotations = ast.default_annotations(),\n          span = inner_loop.span,\n        },\n        inner_loop.values[2],\n        ast.typed.expr.Constant {\n          value = NUM_THREAD_Y,\n          expr_type = index_type,\n          annotations = ast.default_annotations(),\n          span = inner_loop.span,\n        }\n      })\n    }\n    block = block { stats = terralib.newlist({ inner_loop }) }\n  end\n  return block\nend\n\nfunction cudahelper.generate_region_reduction(cx, loop_symbol, node, rhs, lhs_type, value_type, gen)\n  if cx.use_2d_launch then\n    local needs_buffer = base.types.is_ref(lhs_type) and\n                         (value_type:isprimitive() or value_type:isarray()) and\n                         node.metadata and\n                         node.metadata.centers and\n                         node.metadata.centers:has(loop_symbol)\n    if needs_buffer then\n      local buffer = cx:reduction_buffer(lhs_type, value_type, node.op, gen).buffer\n      return quote\n        do\n          var idx = tid_y() + tid_x() * [NUM_THREAD_Y]\n          [generate_element_reductions(`([buffer][ [idx] ]), rhs, node.op, value_type, false)]\n        end\n      end\n    else\n      return gen(rhs)\n    end\n  else\n    return gen(rhs)\n  end\nend\n\n-- #####################################\n-- ## Code generation for kernel argument spill\n-- #################\n\nlocal MAX_SIZE_INLINE_KERNEL_PARAMS = 1024\n\nfunction cudahelper.check_arguments_need_spill(args)\n  local param_size = 0\n  args:map(function(arg) param_size = param_size + terralib.sizeof(arg.type) end)\n  return param_size > MAX_SIZE_INLINE_KERNEL_PARAMS\nend\n\nfunction cudahelper.generate_argument_spill(args)\n  local arg_type = terralib.types.newstruct(\"cuda_kernel_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(args) do\n    local field_name\n    field_name = \"_arg\" .. tostring(i)\n    arg_type.entries:insert({ field_name, symbol.type })\n    mapping[field_name] = symbol\n  end\n\n  local kernel_arg = terralib.newsymbol(&arg_type)\n  local buffer_size = sizeof(arg_type)\n  buffer_size = (buffer_size + 7) / 8 * 8\n\n  local param_pack = terralib.newlist()\n  local param_unpack = terralib.newlist()\n\n  param_pack:insert(quote\n    var [kernel_arg]\n    do\n      var bounds : c.legion_rect_1d_t\n      bounds.lo.x[0] = 0\n      bounds.hi.x[0] = [buffer_size - 1]\n      var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.Z_COPY_MEM, [&int8](nil))\n      [kernel_arg] =\n        [&arg_type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n    end\n  end)\n  arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local arg = mapping[field_name]\n    param_pack:insert(quote (@[kernel_arg]).[field_name] = [arg] end)\n    param_unpack:insert(quote var [arg] = (@[kernel_arg]).[field_name] end)\n  end)\n\n  return param_pack, param_unpack, kernel_arg\nend\n\nreturn cudahelper\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/language/src/regent/openmphelper.t": "-- Copyright 2021 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal base = require(\"regent/std_base\")\nlocal std = require(\"regent/std\")\n\nlocal omp = {}\n\n-- Exit early if the user turned off OpenMP code generation\n\nif std.config[\"openmp\"] == 0 then\n  function omp.check_openmp_available()\n    return false\n  end\n  return omp\nend\n\nlocal has_openmp = true\nif not (std.config[\"offline\"] or std.config[\"openmp-offline\"]) then\n  local dlfcn = terralib.includec(\"dlfcn.h\")\n  local terra find_openmp_symbols()\n    var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n    var has_openmp =\n      dlfcn.dlsym(lib, \"GOMP_parallel\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_num_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_max_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_thread_num\") ~= [&opaque](0)\n    dlfcn.dlclose(lib)\n    return has_openmp\n  end\n  has_openmp = find_openmp_symbols()\nend\n\nif not has_openmp then\n  function omp.check_openmp_available()\n    return false, \"Regent is installed without OpenMP support\"\n  end\n  if std.config[\"openmp\"] == 1 then\n    local available, message = omp.check_openmp_available()\n    print(\"OpenMP code generation failed since \" .. message)\n    os.exit(-1)\n  end\n\nelse\n  omp.check_openmp_available = function() return true end\n  local omp_abi = terralib.includecstring [[\n    extern int omp_get_num_threads(void);\n    extern int omp_get_max_threads(void);\n    extern int omp_get_thread_num(void);\n    extern void GOMP_parallel(void (*fnptr)(void *data), void *data, int nthreads, unsigned flags);\n  ]]\n\n  omp.get_num_threads = omp_abi.omp_get_num_threads\n  omp.get_max_threads = omp_abi.omp_get_max_threads\n  omp.get_thread_num = omp_abi.omp_get_thread_num\n  omp.launch = omp_abi.GOMP_parallel\nend\n\n-- TODO: This might not be the right size in platforms other than x86\nomp.CACHE_LINE_SIZE = 64\n\nlocal FAST_ATOMICS = {\n  [\"+\"] = \"add\",\n  [\"-\"] = \"sub\",\n}\n\nomp.generate_atomic_update = terralib.memoize(function(op, typ)\n  -- Build a C wrapper to use atomic intrinsics in LLVM\n  local atomic_update = nil\n  local op_name = base.reduction_ops[op].name\n  assert(op_name ~= nil)\n  -- Integer types\n  if typ:isintegral() then\n    local ctype = typ.cachedcstring or typ:cstring()\n    assert(ctype ~= nil)\n    -- If there is a native support for the operation, use it directly\n    if FAST_ATOMICS[op] ~= nil then\n      local fun_name = string.format(\"__atomic_update_%s_%s\", op_name, ctype)\n      local C = terralib.includecstring(string.format([[\n        #include <stdint.h>\n        inline void %s(%s *address, %s val) {\n          __sync_fetch_and_%s(address, val);\n        }\n      ]], fun_name, ctype, ctype, FAST_ATOMICS[op]))\n      terra atomic_update(address : &typ, val : typ)\n        [ C[fun_name] ](address, val)\n      end\n    else\n      local fun_name = string.format(\"__compare_and_swap_%s_%s\", op_name, ctype)\n      local C = terralib.includecstring(string.format([[\n        #include <stdint.h>\n        inline %s %s(%s *address, %s old, %s new) {\n          return __sync_val_compare_and_swap(address, old, new);\n        }\n      ]], ctype, fun_name, ctype, ctype, ctype))\n      terra atomic_update(address : &typ, val : typ)\n        var success = false\n        while not success do\n          var old = @address\n          var new = [std.quote_binary_op(op, old, val)]\n          var res = [ C[fun_name] ](address, old, new)\n          success = res == old\n        end\n      end\n    end\n  else\n    local size = terralib.sizeof(typ) * 8\n    local cas_type = _G[\"uint\" .. tostring(size)]\n    local ctype = typ.cachedcstring or typ:cstring()\n    local cas_ctype = cas_type.cachedcstring or cas_type:cstring()\n    local fun_name = string.format(\"__compare_and_swap_%s_%s\", op_name, ctype)\n    local C = terralib.includecstring(string.format([[\n      #include <stdint.h>\n      inline %s %s(%s *address, %s old, %s new) {\n        return __sync_val_compare_and_swap(address, old, new);\n      }\n    ]], cas_ctype, fun_name, cas_ctype, cas_ctype, cas_ctype))\n    terra atomic_update(address : &typ, val : typ)\n      var success = false\n      while not success do\n        var old = @address\n        var new = [std.quote_binary_op(op, old, val)]\n\n        var address_b : &cas_type = [&cas_type](address)\n        var old_b : &cas_type = [&cas_type](&old)\n        var new_b : &cas_type = [&cas_type](&new)\n        var res : cas_type = [ C[fun_name] ](address_b, @old_b, @new_b)\n        success = res == @old_b\n      end\n    end\n  end\n  assert(atomic_update ~= nil)\n  atomic_update:setinlined(true)\n  return atomic_update\nend)\n\nfunction omp.generate_preamble(rect, idx, start_idx, end_idx)\n  return quote\n    var num_threads = [omp.get_num_threads]()\n    var thread_id = [omp.get_thread_num]()\n    var lo = [rect].lo.x[idx]\n    var hi = [rect].hi.x[idx] + 1\n    var chunk = (hi - lo + num_threads - 1) / num_threads\n    if chunk == 0 then chunk = 1 end\n    var [start_idx] = thread_id * chunk + lo\n    var [end_idx] = (thread_id + 1) * chunk + lo\n    if [end_idx] > hi then [end_idx] = hi end\n  end\nend\n\nfunction omp.generate_argument_type(symbols, reductions)\n  local arg_type = terralib.types.newstruct(\"omp_worker_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(symbols) do\n    local field_name\n    if reductions[symbol] == nil then\n      field_name = \"_arg\" .. tostring(i)\n      arg_type.entries:insert({ field_name, symbol.type })\n    else\n      field_name = \"_red\" .. tostring(i)\n      arg_type.entries:insert({ field_name, &symbol.type })\n    end\n    mapping[field_name] = symbol\n  end\n  return arg_type, mapping\nend\n\nfunction omp.generate_argument_init(arg, arg_type, mapping, can_change, reductions)\n  local worker_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      return quote var [symbol] = [init] end\n    else\n      return quote var [symbol] = [arg].[field_name] end\n    end\n  end)\n\n  local launch_init = terralib.newlist()\n  launch_init:insert(quote\n    var arg_obj : arg_type\n    var [arg] = &arg_obj\n  end)\n  local launch_update = terralib.newlist()\n\n  arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      assert(field_type:ispointer())\n      launch_init:insert(quote\n        var num_threads = [omp.get_max_threads]()\n        -- We don't like false sharing\n        var size = num_threads  * omp.CACHE_LINE_SIZE\n        var data = std.c.malloc(size)\n        std.assert(size == 0 or data ~= nil, \"malloc failed in generate_argument_init\")\n        [arg].[field_name] = [field_type](data)\n        for i = 0, num_threads do\n          @[&symbol.type]([&int8](data) + i * omp.CACHE_LINE_SIZE) = [init]\n        end\n      end)\n    elseif not can_change[symbol] then\n      launch_init:insert(quote [arg].[field_name] = [symbol] end)\n    else\n      launch_update:insert(quote [arg].[field_name] = [symbol] end)\n    end\n  end)\n\n  return worker_init, launch_init, launch_update\nend\n\nfunction omp.generate_worker_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        do\n          var idx = [omp.get_thread_num]() * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [arg].[field_name][idx] = [std.quote_binary_op(op, symbol,\n            `([arg].[field_name][idx]))]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nfunction omp.generate_launcher_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        for i = 0, [omp.get_max_threads]() do\n          var idx = i * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [symbol] = [std.quote_binary_op(op, symbol, `([arg].[field_name][idx]))]\n        end\n        std.c.free([arg].[field_name])\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nreturn omp\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/.git/objects/pack/pack-f76c49113d92bd37c10a8034ca5d27a60e285e45.pack",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/.git/objects/pack/pack-f76c49113d92bd37c10a8034ca5d27a60e285e45.idx",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/language/examples/pagerank/sample.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/language/examples/mssp/small/edges.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/language/examples/mssp/small/result_3.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/doc/arch/persistent/hdf5/figs/high-level-design.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-ctrl-rep-nzc35sh2konio2wyg2hcm4j4475m24oa/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout.pptx"
    ],
    "total_files": 2133
}