{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/bindings/python/legion.py": "#!/usr/bin/env python\n\n# Copyright 2017 Stanford University\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import print_function\n\nimport cffi\nimport cPickle\nimport collections\nimport itertools\nimport numpy\nimport os\nimport re\nimport subprocess\nimport sys\nimport threading\n\n_pickle_version = cPickle.HIGHEST_PROTOCOL # Use latest Pickle protocol\n\nroot_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nruntime_dir = os.path.join(root_dir, 'runtime')\nlegion_dir = os.path.join(runtime_dir, 'legion')\n\nheader = subprocess.check_output(['gcc', '-I', runtime_dir, '-E', '-P', os.path.join(legion_dir, 'legion_c.h')])\n\n# Hack: Fix for Ubuntu 16.04 versions of standard library headers:\nheader = re.sub(r'typedef struct {.+?} max_align_t;', '', header, flags=re.DOTALL)\n\nffi = cffi.FFI()\nffi.cdef(header)\nc = ffi.dlopen(None)\n\n# The Legion context is stored in thread-local storage. This assumes\n# that the Python processor maintains the invariant that every task\n# corresponds to one and only one thread.\n_my = threading.local()\n\nclass Context(object):\n    __slots__ = ['context_root', 'context', 'runtime_root', 'runtime',\n                 'task_root', 'task', 'regions', 'current_launch']\n    def __init__(self, context_root, runtime_root, task_root, regions):\n        self.context_root = context_root\n        self.context = self.context_root[0]\n        self.runtime_root = runtime_root\n        self.runtime = self.runtime_root[0]\n        self.task_root = task_root\n        self.task = self.task_root[0]\n        self.regions = regions\n        self.current_launch = None\n    def begin_launch(self, launch):\n        assert self.current_launch == None\n        self.current_launch = launch\n    def end_launch(self, launch):\n        assert self.current_launch == launch\n        self.current_launch = None\n\nclass DomainPoint(object):\n    __slots__ = ['impl']\n    def __init__(self, value):\n        assert(isinstance(value, _IndexValue))\n        self.impl = ffi.new('legion_domain_point_t *')\n        self.impl[0].dim = 1\n        self.impl[0].point_data[0] = int(value)\n    def raw_value(self):\n        return self.impl[0]\n\nclass Domain(object):\n    __slots__ = ['impl']\n    def __init__(self, extent, start=None):\n        if start is not None:\n            assert len(start) == len(extent)\n        else:\n            start = [0 for _ in extent]\n        assert 1 <= len(extent) <= 3\n        rect = ffi.new('legion_rect_{}d_t *'.format(len(extent)))\n        for i in xrange(len(extent)):\n            rect[0].lo.x[i] = start[i]\n            rect[0].hi.x[i] = start[i] + extent[i] - 1\n        self.impl = getattr(c, 'legion_domain_from_rect_{}d'.format(len(extent)))(rect[0])\n    def raw_value(self):\n        return self.impl\n\nclass Future(object):\n    __slots__ = ['handle', 'value_type']\n    def __init__(self, handle, value_type=None):\n        self.handle = c.legion_future_copy(handle)\n        self.value_type = value_type\n\n    def __del__(self):\n        c.legion_future_destroy(self.handle)\n\n    def get(self):\n        if self.value_type is None:\n            value_ptr = c.legion_future_get_untyped_pointer(self.handle)\n            value_size = c.legion_future_get_untyped_size(self.handle)\n            assert value_size > 0\n            value_str = ffi.unpack(ffi.cast('char *', value_ptr), value_size)\n            value = cPickle.loads(value_str)\n            return value\n        else:\n            expected_size = ffi.sizeof(self.value_type)\n\n            value_ptr = c.legion_future_get_untyped_pointer(self.handle)\n            value_size = c.legion_future_get_untyped_size(self.handle)\n            assert value_size == expected_size\n            value = ffi.cast(ffi.getctype(self.value_type, '*'), value_ptr)[0]\n            return value\n\nclass Type(object):\n    __slots__ = ['numpy_type', 'size']\n\n    def __init__(self, numpy_type):\n        self.numpy_type = numpy_type\n        self.size = numpy.dtype(numpy_type).itemsize\n\n    def __reduce__(self):\n        return (Type, (self.numpy_type,))\n\n# Pre-defined Types\nfloat16 = Type(numpy.float16)\nfloat32 = Type(numpy.float32)\nfloat64 = Type(numpy.float64)\nint16 = Type(numpy.int16)\nint32 = Type(numpy.int32)\nint64 = Type(numpy.int64)\nuint16 = Type(numpy.uint16)\nuint32 = Type(numpy.uint32)\nuint64 = Type(numpy.uint64)\n\nclass Privilege(object):\n    __slots__ = ['read', 'write', 'discard']\n\n    def __init__(self, read=False, write=False, discard=False):\n        self.read = read\n        self.write = write\n        self.discard = discard\n\n    def _fields(self):\n        return (self.read, self.write, self.discard)\n\n    def __eq__(self, other):\n        return isinstance(other, Privilege) and self._fields() == other._fields()\n\n    def __cmp__(self, other):\n        assert isinstance(other, Privilege)\n        return self._fields().__cmp__(other._fields())\n\n    def __hash__(self):\n        return hash(self._fields())\n\n    def __call__(self, fields):\n        return PrivilegeFields(self, fields)\n\n    def _legion_privilege(self):\n        bits = 0\n        if self.discard:\n            assert self.write\n            bits |= 2 # WRITE_DISCARD\n        else:\n            if self.write: bits = 7 # READ_WRITE\n            elif self.read: bits = 1 # READ_ONLY\n        return bits\n\nclass PrivilegeFields(Privilege):\n    __slots__ = ['read', 'write', 'discard', 'fields']\n\n    def __init__(self, privilege, fields):\n        Privilege.__init__(self, privilege.read, privilege.write, privilege.discard)\n        self.fields = fields\n\n# Pre-defined Privileges\nN = Privilege()\nR = Privilege(read=True)\nRO = Privilege(read=True)\nRW = Privilege(read=True, write=True)\nWD = Privilege(write=True, discard=True)\n\n# Hack: Can't pickle static methods.\ndef _Ispace_unpickle(ispace_tid, ispace_id, ispace_type_tag):\n    handle = ffi.new('legion_index_space_t *')\n    handle[0].tid = ispace_tid\n    handle[0].id = ispace_id\n    handle[0].type_tag = ispace_type_tag\n    return Ispace(handle[0])\n\nclass Ispace(object):\n    __slots__ = ['handle']\n\n    def __init__(self, handle):\n        # Important: Copy handle. Do NOT assume ownership.\n        self.handle = ffi.new('legion_index_space_t *', handle)\n\n    def __reduce__(self):\n        return (_Ispace_unpickle,\n                (self.handle[0].tid,\n                 self.handle[0].id,\n                 self.handle[0].type_tag))\n\n    @staticmethod\n    def create(extent, start=None):\n        domain = Domain(extent, start=start).raw_value()\n        handle = c.legion_index_space_create_domain(_my.ctx.runtime, _my.ctx.context, domain)\n        return Ispace(handle)\n\n# Hack: Can't pickle static methods.\ndef _Fspace_unpickle(fspace_id, field_ids, field_types):\n    handle = ffi.new('legion_field_space_t *')\n    handle[0].id = fspace_id\n    return Fspace(handle[0], field_ids, field_types)\n\nclass Fspace(object):\n    __slots__ = ['handle', 'field_ids', 'field_types']\n\n    def __init__(self, handle, field_ids, field_types):\n        # Important: Copy handle. Do NOT assume ownership.\n        self.handle = ffi.new('legion_field_space_t *', handle)\n        self.field_ids = field_ids\n        self.field_types = field_types\n\n    def __reduce__(self):\n        return (_Fspace_unpickle,\n                (self.handle[0].id,\n                 self.field_ids,\n                 self.field_types))\n\n    @staticmethod\n    def create(fields):\n        handle = c.legion_field_space_create(_my.ctx.runtime, _my.ctx.context)\n        alloc = c.legion_field_allocator_create(\n            _my.ctx.runtime, _my.ctx.context, handle)\n        field_ids = {}\n        field_types = {}\n        for field_name, field_entry in fields.items():\n            try:\n                field_type, field_id = field_entry\n            except TypeError:\n                field_type = field_entry\n                field_id = ffi.cast('legion_field_id_t', -1) # AUTO_GENERATE_ID\n            field_id = c.legion_field_allocator_allocate_field(\n                alloc, field_type.size, field_id)\n            c.legion_field_id_attach_name(\n                _my.ctx.runtime, handle, field_id, field_name, False)\n            field_ids[field_name] = field_id\n            field_types[field_name] = field_type\n        c.legion_field_allocator_destroy(alloc)\n        return Fspace(handle, field_ids, field_types)\n\n# Hack: Can't pickle static methods.\ndef _Region_unpickle(tree_id, ispace, fspace):\n    handle = ffi.new('legion_logical_region_t *')\n    handle[0].tree_id = tree_id\n    handle[0].index_space.tid = ispace.handle[0].tid\n    handle[0].index_space.id = ispace.handle[0].id\n    handle[0].field_space.id = fspace.handle[0].id\n\n    return Region(handle[0], ispace, fspace)\n\nclass Region(object):\n    __slots__ = ['handle', 'ispace', 'fspace',\n                 'instances', 'privileges', 'instance_wrappers']\n\n    def __init__(self, handle, ispace, fspace):\n        # Important: Copy handle. Do NOT assume ownership.\n        self.handle = ffi.new('legion_logical_region_t *', handle)\n        self.ispace = ispace\n        self.fspace = fspace\n        self.instances = {}\n        self.privileges = {}\n        self.instance_wrappers = {}\n\n    def __reduce__(self):\n        return (_Region_unpickle,\n                (self.handle[0].tree_id,\n                 self.ispace,\n                 self.fspace))\n\n    @staticmethod\n    def create(ispace, fspace):\n        if not isinstance(ispace, Ispace):\n            ispace = Ispace.create(ispace)\n        if not isinstance(fspace, Fspace):\n            fspace = Fspace.create(fspace)\n        handle = c.legion_logical_region_create(\n            _my.ctx.runtime, _my.ctx.context, ispace.handle[0], fspace.handle[0])\n        result = Region(handle, ispace, fspace)\n        for field_name in fspace.field_ids.keys():\n            result.set_privilege(field_name, RW)\n        return result\n\n    def destroy(self):\n        # This is not something you want to have happen in a\n        # destructor, since regions may outlive the lifetime of the handle.\n        c.legion_logical_region_destroy(\n            _my.ctx.runtime, _my.ctx.context, self.handle[0])\n        # Clear out references. Technically unnecessary but avoids abuse.\n        del self.instance_wrappers\n        del self.instances\n        del self.handle\n        del self.ispace\n        del self.fspace\n\n    def set_privilege(self, field_name, privilege):\n        assert field_name not in self.privileges\n        self.privileges[field_name] = privilege\n\n    def set_instance(self, field_name, instance, privilege=None):\n        assert field_name not in self.instances\n        self.instances[field_name] = instance\n        if privilege is not None:\n            assert field_name not in self.privileges\n            self.privileges[field_name] = privilege\n\n    def map_inline(self):\n        fields_by_privilege = collections.defaultdict(set)\n        for field_name, privilege in self.privileges.iteritems():\n            fields_by_privilege[privilege].add(field_name)\n        for privilege, field_names  in fields_by_privilege.iteritems():\n            launcher = c.legion_inline_launcher_create_logical_region(\n                self.handle[0],\n                privilege._legion_privilege(), 0, # EXCLUSIVE\n                self.handle[0],\n                0, False, 0, 0)\n            for field_name in field_names:\n                c.legion_inline_launcher_add_field(\n                    launcher, self.fspace.field_ids[field_name], True)\n            instance = c.legion_inline_launcher_execute(\n                _my.ctx.runtime, _my.ctx.context, launcher)\n            for field_name in field_names:\n                self.set_instance(field_name, instance)\n\n    def __getattr__(self, field_name):\n        if field_name in self.fspace.field_ids:\n            if field_name not in self.instances:\n                if self.privileges[field_name] is None:\n                    raise Exception('Invalid attempt to access field \"%s\" without privileges' % field_name)\n                self.map_inline()\n            if field_name not in self.instance_wrappers:\n                self.instance_wrappers[field_name] = RegionField(\n                    self, field_name)\n            return self.instance_wrappers[field_name]\n        else:\n            raise AttributeError()\n\nclass RegionField(numpy.ndarray):\n    # NumPy requires us to implement __new__ for subclasses of ndarray:\n    # https://docs.scipy.org/doc/numpy/user/basics.subclassing.html\n    def __new__(cls, region, field_name):\n        accessor = RegionField._get_accessor(region, field_name)\n        initializer = RegionField._get_array_initializer(region, field_name, accessor)\n        obj = numpy.asarray(initializer).view(cls)\n\n        obj.accessor = accessor\n        return obj\n\n    @staticmethod\n    def _get_accessor(region, field_name):\n        # Note: the accessor needs to be kept alive, to make sure to\n        # save the result of this function in an instance variable.\n        instance = region.instances[field_name]\n        domain = c.legion_index_space_get_domain(\n            _my.ctx.runtime, region.ispace.handle[0])\n        dim = domain.dim\n        get_accessor = getattr(c, 'legion_physical_region_get_field_accessor_array_{}d'.format(dim))\n        return get_accessor(instance, region.fspace.field_ids[field_name])\n\n    @staticmethod\n    def _get_base_and_stride(region, field_name, accessor):\n        domain = c.legion_index_space_get_domain(\n            _my.ctx.runtime, region.ispace.handle[0])\n        dim = domain.dim\n        rect = getattr(c, 'legion_domain_get_rect_{}d'.format(dim))(domain)\n        subrect = ffi.new('legion_rect_{}d_t *'.format(dim))\n        offsets = ffi.new('legion_byte_offset_t[]', dim)\n\n        base_ptr = getattr(c, 'legion_accessor_array_{}d_raw_rect_ptr'.format(dim))(\n            accessor, rect, subrect, offsets)\n        assert base_ptr\n        for i in xrange(dim):\n            assert subrect[0].lo.x[i] == rect.lo.x[i]\n            assert subrect[0].hi.x[i] == rect.hi.x[i]\n        assert offsets[0].offset == region.fspace.field_types[field_name].size\n\n        shape = tuple(rect.hi.x[i] - rect.lo.x[i] + 1 for i in xrange(dim))\n        strides = tuple(offsets[i].offset for i in xrange(dim))\n\n        return base_ptr, shape, strides\n\n    @staticmethod\n    def _get_array_initializer(region, field_name, accessor):\n        base_ptr, shape, strides = RegionField._get_base_and_stride(\n            region, field_name, accessor)\n        field_type = region.fspace.field_types[field_name]\n\n        # Numpy doesn't know about CFFI pointers, so we have to cast\n        # this to a Python long before we can hand it off to Numpy.\n        base_ptr = long(ffi.cast(\"size_t\", base_ptr))\n\n        return _RegionNdarray(shape, field_type, base_ptr, strides, False)\n\n# This is a dummy object that is only used as an initializer for the\n# RegionField object above. It is thrown away as soon as the\n# RegionField is constructed.\nclass _RegionNdarray(object):\n    __slots__ = ['__array_interface__']\n    def __init__(self, shape, field_type, base_ptr, strides, read_only):\n        # See: https://docs.scipy.org/doc/numpy/reference/arrays.interface.html\n        self.__array_interface__ = {\n            'version': 3,\n            'shape': shape,\n            'typestr': numpy.dtype(field_type.numpy_type).str,\n            'data': (base_ptr, read_only),\n            'strides': strides,\n        }\n\nclass ExternTask(object):\n    __slots__ = ['privileges', 'calling_convention', 'task_id']\n\n    def __init__(self, task_id, privileges=None):\n        if privileges is not None:\n            privileges = [(x if x is not None else N) for x in privileges]\n        self.privileges = privileges\n        self.calling_convention = None\n        assert isinstance(task_id, int)\n        self.task_id = task_id\n\n    def __call__(self, *args):\n        return self.spawn_task(*args)\n\n    def spawn_task(self, *args):\n        if _my.ctx.current_launch:\n            return _my.ctx.current_launch.spawn_task(self, *args)\n        return TaskLaunch().spawn_task(self, *args)\n\ndef extern_task(**kwargs):\n    return ExternTask(**kwargs)\n\nclass Task (object):\n    __slots__ = ['body', 'privileges', 'leaf', 'inner', 'idempotent', 'calling_convention', 'task_id']\n\n    def __init__(self, body, privileges=None,\n                 leaf=False, inner=False, idempotent=False,\n                 register=True):\n        self.body = body\n        if privileges is not None:\n            privileges = [(x if x is not None else N) for x in privileges]\n        self.privileges = privileges\n        self.leaf = bool(leaf)\n        self.inner = bool(inner)\n        self.idempotent = bool(idempotent)\n        self.calling_convention = 'python'\n        self.task_id = None\n        if register:\n            self.register()\n\n    def __call__(self, *args):\n        # Hack: This entrypoint needs to be able to handle both being\n        # called in user code (to launch a task) and as the task\n        # wrapper when the task itself executes. Unfortunately isn't a\n        # good way to disentangle these. Detect if we're in the task\n        # wrapper case by checking the number and types of arguments.\n        if len(args) == 3 and \\\n           isinstance(args[0], bytearray) and \\\n           isinstance(args[1], bytearray) and \\\n           isinstance(args[2], long):\n            return self.execute_task(*args)\n        else:\n            return self.spawn_task(*args)\n\n    def spawn_task(self, *args):\n        if _my.ctx.current_launch:\n            return _my.ctx.current_launch.spawn_task(self, *args)\n        return TaskLaunch().spawn_task(self, *args)\n\n    def execute_task(self, raw_args, user_data, proc):\n        raw_arg_ptr = ffi.new('char[]', bytes(raw_args))\n        raw_arg_size = len(raw_args)\n\n        # Execute preamble to obtain Legion API context.\n        task = ffi.new('legion_task_t *')\n        raw_regions = ffi.new('legion_physical_region_t **')\n        num_regions = ffi.new('unsigned *')\n        context = ffi.new('legion_context_t *')\n        runtime = ffi.new('legion_runtime_t *')\n        c.legion_task_preamble(\n            raw_arg_ptr, raw_arg_size, proc,\n            task, raw_regions, num_regions, context, runtime)\n\n        # Decode arguments from Pickle format.\n        if c.legion_task_get_is_index_space(task[0]):\n            arg_ptr = ffi.cast('char *', c.legion_task_get_local_args(task[0]))\n            arg_size = c.legion_task_get_local_arglen(task[0])\n        else:\n            arg_ptr = ffi.cast('char *', c.legion_task_get_args(task[0]))\n            arg_size = c.legion_task_get_arglen(task[0])\n\n        if arg_size > 0:\n            args = cPickle.loads(ffi.unpack(arg_ptr, arg_size))\n        else:\n            args = ()\n\n        # Unpack regions.\n        regions = []\n        for i in xrange(num_regions[0]):\n            regions.append(raw_regions[0][i])\n\n        # Unpack physical regions.\n        if self.privileges is not None:\n            req = 0\n            for i, arg in zip(range(len(args)), args):\n                if isinstance(arg, Region):\n                    assert req < num_regions[0] and req < len(self.privileges)\n                    instance = raw_regions[0][req]\n                    req += 1\n\n                    priv = self.privileges[i]\n                    if hasattr(priv, 'fields'):\n                        assert set(priv.fields) <= set(arg.fspace.field_ids.keys())\n                    for name, fid in arg.fspace.field_ids.items():\n                        if not hasattr(priv, 'fields') or name in priv.fields:\n                            arg.set_instance(name, instance, priv)\n            assert req == num_regions[0]\n\n        # Build context.\n        ctx = Context(context, runtime, task, regions)\n\n        # Ensure that we're not getting tangled up in another\n        # thread. There should be exactly one thread per task.\n        try:\n            _my.ctx\n        except AttributeError:\n            pass\n        else:\n            raise Exception('thread-local context already set')\n\n        # Store context in thread-local storage.\n        _my.ctx = ctx\n\n        # Execute task body.\n        result = self.body(*args)\n\n        # Encode result in Pickle format.\n        if result is not None:\n            result_str = cPickle.dumps(result, protocol=_pickle_version)\n            result_size = len(result_str)\n            result_ptr = ffi.new('char[]', result_size)\n            ffi.buffer(result_ptr, result_size)[:] = result_str\n        else:\n            result_size = 0\n            result_ptr = ffi.NULL\n\n        # Execute postamble.\n        c.legion_task_postamble(runtime[0], context[0], result_ptr, result_size)\n\n        # Clear thread-local storage.\n        del _my.ctx\n\n    def register(self):\n        assert(self.task_id is None)\n\n        execution_constraints = c.legion_execution_constraint_set_create()\n        c.legion_execution_constraint_set_add_processor_constraint(\n            execution_constraints, c.PY_PROC)\n\n        layout_constraints = c.legion_task_layout_constraint_set_create()\n        # FIXME: Add layout constraints\n\n        options = ffi.new('legion_task_config_options_t *')\n        options[0].leaf = self.leaf\n        options[0].inner = self.inner\n        options[0].idempotent = self.idempotent\n\n        task_id = c.legion_runtime_preregister_task_variant_python_source(\n            ffi.cast('legion_task_id_t', -1), # AUTO_GENERATE_ID\n            '%s.%s' % (self.body.__module__, self.body.__name__),\n            execution_constraints,\n            layout_constraints,\n            options[0],\n            self.body.__module__,\n            self.body.__name__,\n            ffi.NULL,\n            0)\n\n        c.legion_execution_constraint_set_destroy(execution_constraints)\n        c.legion_task_layout_constraint_set_destroy(layout_constraints)\n\n        self.task_id = task_id\n        return self\n\ndef task(body=None, **kwargs):\n    if body is None:\n        return lambda body: task(body, **kwargs)\n    return Task(body, **kwargs)\n\nclass _TaskLauncher(object):\n    __slots__ = ['task_id', 'privileges', 'calling_convention']\n\n    def __init__(self, task_id, privileges, calling_convention):\n        self.task_id = task_id\n        self.privileges = privileges\n        self.calling_convention = calling_convention\n\n    def preprocess_args(self, *args):\n        return [\n            arg._legion_preprocess_task_argument()\n            if hasattr(arg, '_legion_preprocess_task_argument') else arg\n            for arg in args]\n\n    def encode_args(self, *args):\n        task_args = ffi.new('legion_task_argument_t *')\n        task_args_buffer = None\n        if self.calling_convention == 'python':\n            arg_str = cPickle.dumps(args, protocol=_pickle_version)\n            task_args_buffer = ffi.new('char[]', arg_str)\n            task_args[0].args = task_args_buffer\n            task_args[0].arglen = len(arg_str)\n        else:\n            # FIXME: External tasks need a dedicated calling\n            # convention to permit the passing of task arguments.\n            task_args[0].args = ffi.NULL\n            task_args[0].arglen = 0\n        # WARNING: Need to return the interior buffer or else it will be GC'd\n        return task_args, task_args_buffer\n\n    def spawn_task(self, *args):\n        assert(isinstance(_my.ctx, Context))\n\n        args = self.preprocess_args(*args)\n        task_args, _ = self.encode_args(*args)\n\n        # Construct the task launcher.\n        launcher = c.legion_task_launcher_create(\n            self.task_id, task_args[0], c.legion_predicate_true(), 0, 0)\n        for i, arg in zip(range(len(args)), args):\n            if isinstance(arg, Region):\n                assert i < len(self.privileges)\n                priv = self.privileges[i]\n                req = c.legion_task_launcher_add_region_requirement_logical_region(\n                    launcher, arg.handle[0],\n                    priv._legion_privilege(),\n                    0, # EXCLUSIVE\n                    arg.handle[0], 0, False)\n                if hasattr(priv, 'fields'):\n                    assert set(priv.fields) <= set(arg.fspace.field_ids.keys())\n                for name, fid in arg.fspace.field_ids.items():\n                    if not hasattr(priv, 'fields') or name in priv.fields:\n                        c.legion_task_launcher_add_field(\n                            launcher, req, fid, True)\n            elif self.calling_convention is None:\n                # FIXME: Task arguments aren't being encoded AT ALL;\n                # at least throw an exception so that the user knows\n                raise Exception('External tasks do not support non-region arguments')\n\n        # Launch the task.\n        result = c.legion_task_launcher_execute(\n            _my.ctx.runtime, _my.ctx.context, launcher)\n        c.legion_task_launcher_destroy(launcher)\n\n        # Build future of result.\n        future = Future(result)\n        c.legion_future_destroy(result)\n        return future\n\nclass _IndexLauncher(_TaskLauncher):\n    __slots__ = ['task_id', 'privileges', 'calling_convention',\n                 'domain', 'local_args']\n\n    def __init__(self, task_id, privileges, calling_convention, domain):\n        super(_IndexLauncher, self).__init__(\n            task_id, privileges, calling_convention)\n        self.domain = domain\n        self.local_args = c.legion_argument_map_create()\n\n    def __del__(self):\n        c.legion_argument_map_destroy(self.local_args)\n\n    def spawn_task(self, *args):\n        raise Exception('IndexLaunch does not support spawn_task')\n\n    def attach_local_args(self, index, *args):\n        point = DomainPoint(index)\n        args = self.preprocess_args(*args)\n        task_args, _ = self.encode_args(*args)\n        c.legion_argument_map_set_point(\n            self.local_args, point.raw_value(), task_args[0], False)\n\n    def launch(self):\n        # All arguments are passed as local, so global is NULL.\n        global_args = ffi.new('legion_task_argument_t *')\n        global_args[0].args = ffi.NULL\n        global_args[0].arglen = 0\n\n        # Construct the task launcher.\n        launcher = c.legion_index_launcher_create(\n            self.task_id, self.domain.raw_value(),\n            global_args[0], self.local_args,\n            c.legion_predicate_true(), False, 0, 0)\n\n        # Launch the task.\n        result = c.legion_index_launcher_execute(\n            _my.ctx.runtime, _my.ctx.context, launcher)\n        c.legion_index_launcher_destroy(launcher)\n\n        # TODO: Build future (map) of result.\n        c.legion_future_map_destroy(result)\n\nclass TaskLaunch(object):\n    __slots__ = []\n    def spawn_task(self, task, *args):\n        launcher = _TaskLauncher(\n            task_id=task.task_id,\n            privileges=task.privileges,\n            calling_convention=task.calling_convention)\n        return launcher.spawn_task(*args)\n\nclass _IndexValue(object):\n    __slots__ = ['value']\n    def __init__(self, value):\n        self.value = value\n    def __int__(self):\n        return self.value\n    def __index__(self):\n        return self.value\n    def __str__(self):\n        return str(self.value)\n    def __repr__(self):\n        return repr(self.value)\n    def _legion_preprocess_task_argument(self):\n        return self.value\n\nclass IndexLaunch(object):\n    __slots__ = ['extent', 'domain', 'launcher', 'point',\n                 'saved_task', 'saved_args']\n\n    def __init__(self, extent):\n        assert len(extent) == 1\n        self.extent = extent\n        self.domain = Domain(extent)\n        self.launcher = None\n        self.point = None\n        self.saved_task = None\n        self.saved_args = None\n\n    def __iter__(self):\n        _my.ctx.begin_launch(self)\n        self.point = _IndexValue(None)\n        for i in xrange(self.extent[0]):\n            self.point.value = i\n            yield self.point\n        _my.ctx.end_launch(self)\n        self.launch()\n\n    def ensure_launcher(self, task):\n        if self.launcher is None:\n            self.launcher = _IndexLauncher(\n                task_id=task.task_id,\n                privileges=task.privileges,\n                calling_convention=task.calling_convention,\n                domain=self.domain)\n\n    def check_compatibility(self, task, *args):\n        # The tasks in a launch must conform to the following constraints:\n        #   * Only one task can be launched.\n        #   * The arguments must be compatible:\n        #       * At a given argument position, the value must always\n        #         be a region, or always not.\n        #       * If a region, the value must be symbolic (i.e. able\n        #         to be analyzed as a function of the index expression).\n\n        if self.saved_task is None:\n            self.saved_task = task\n        if task != self.saved_task:\n            raise Exception('An IndexLaunch may contain only one task launch')\n\n        if self.saved_args is None:\n            self.saved_args = args\n        for arg, saved_arg in itertools.izip_longest(args, self.saved_args):\n            # TODO: Add support for region arguments\n            if isinstance(arg, Region) or isinstance(arg, RegionField):\n                raise Exception('TODO: Support region arguments to an IndexLaunch')\n\n    def spawn_task(self, task, *args):\n        self.ensure_launcher(task)\n        self.check_compatibility(task, *args)\n        self.launcher.attach_local_args(self.point, *args)\n        # TODO: attach region args\n        # TODO: attach future args\n\n    def launch(self):\n        self.launcher.launch()\n\n@task(leaf=True)\ndef _dummy_task():\n    return 1\n\ndef execution_fence(block=False):\n    c.legion_runtime_issue_execution_fence(_my.ctx.runtime, _my.ctx.context)\n    if block:\n        _dummy_task().get()\n\nclass Tunable(object):\n    # FIXME: Deduplicate this with DefaultMapper::DefaultTunables\n    NODE_COUNT = 0\n    LOCAL_CPUS = 1\n    LOCAL_GPUS = 2\n    LOCAL_IOS = 3\n    LOCAL_OMPS = 4\n    LOCAL_PYS = 5\n    GLOBAL_CPUS = 6\n    GLOBAL_GPUS = 7\n    GLOBAL_IOS = 8\n    GLOBAL_OMPS = 9\n    GLOBAL_PYS = 10\n\n    @staticmethod\n    def select(tunable_id):\n        result = c.legion_runtime_select_tunable_value(\n            _my.ctx.runtime, _my.ctx.context, tunable_id, 0, 0)\n        future = Future(result, 'size_t')\n        c.legion_future_destroy(result)\n        return future\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/runtime/realm/codedesc.cc": "/* Copyright 2017 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// constructs for describing code blobs to Realm\n\n#include \"realm/codedesc.h\"\n\n#include <dlfcn.h>\n\n#include \"realm/logging.h\"\n#include \"realm/utils.h\"\n\nnamespace Realm {\n\n  Logger log_codetrans(\"codetrans\");\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Type\n\n  std::ostream& operator<<(std::ostream& os, const Type& t)\n  {\n    switch(t.f_common.kind) {\n    case Type::InvalidKind: os << \"INVALIDTYPE\"; break;\n    case Type::OpaqueKind:\n      {\n\tif(t.size_bits() == 0)\n\t  os << \"void\";\n\telse\n\t  os << \"opaque(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::IntegerKind:\n      {\n\tos << (t.f_integer.is_signed ? 's' : 'u') << \"int(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::FloatingPointKind: os << \"float(\" << t.size_bits() << \")\"; break;\n    case Type::PointerKind:\n      {\n\tos << *t.f_pointer.base_type;\n\tif(t.f_pointer.is_const) os << \" const\";\n\tos << \" *\";\n\tbreak;\n      }\n    case Type::FunctionPointerKind:\n      {\n\tos << *t.f_funcptr.return_type << \"(*)(\";\n\tconst std::vector<Type>& p = *t.f_funcptr.param_types;\n\tif(p.size()) {\n\t  for(size_t i = 0; i < p.size(); i++) {\n\t    if(i) os << \", \";\n\t    os << p[i];\n\t  }\n\t} else\n\t  os << \"void\";\n\tos << \")\";\n\tbreak;\n      }\n    }\n    return os;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeDescriptor\n\n  CodeDescriptor::CodeDescriptor(void)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const Type& _t)\n    : m_type(_t)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const CodeDescriptor& rhs)\n  {\n    copy_from(rhs);\n  }\n\n  CodeDescriptor& CodeDescriptor::operator=(const CodeDescriptor& rhs)\n  {\n    if(this != &rhs) {\n      clear();\n      copy_from(rhs);\n    }\n    return *this;\n  }\n\n  CodeDescriptor::~CodeDescriptor(void)\n  {\n    clear();\n  }\n\n  void CodeDescriptor::clear(void)\n  {\n    m_type = Type();\n    delete_container_contents(m_impls);\n    delete_container_contents(m_props);\n  }\n\n  void CodeDescriptor::copy_from(const CodeDescriptor& rhs)\n  {\n    m_type = rhs.m_type;\n    {\n      size_t s = rhs.m_impls.size();\n      m_impls.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_impls[i] = rhs.m_impls[i]->clone();\n    }\n    {\n      size_t s = rhs.m_props.size();\n      m_props.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_props[i] = rhs.m_props[i]->clone();\n    }\n  }\n\n  // are any of the code implementations marked as \"portable\" (i.e.\n  //  usable in another process/address space)?\n  bool CodeDescriptor::has_portable_implementations(void) const\n  {\n    for(std::vector<CodeImplementation *>::const_iterator it = m_impls.begin();\n\tit != m_impls.end();\n\tit++)\n      if((*it)->is_portable())\n\treturn true;\n    return false;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class FunctionPointerImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      FunctionPointerImplementation> FunctionPointerImplementation::serdez_subclass;\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void)\n    : fnptr(0)\n  {}\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void (*_fnptr)())\n    : fnptr(_fnptr)\n  {}\n\n  FunctionPointerImplementation::~FunctionPointerImplementation(void)\n  {}\n\n  CodeImplementation *FunctionPointerImplementation::clone(void) const\n  {\n    return new FunctionPointerImplementation(fnptr);\n  }\n\n  bool FunctionPointerImplementation::is_portable(void) const\n  {\n    return false;\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOReferenceImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      DSOReferenceImplementation> DSOReferenceImplementation::serdez_subclass;\n\n  DSOReferenceImplementation::DSOReferenceImplementation(void)\n  {}\n\n  DSOReferenceImplementation::DSOReferenceImplementation(const std::string& _dso_name,\n\t\t\t\t\t\t\t const std::string& _symbol_name)\n    : dso_name(_dso_name), symbol_name(_symbol_name)\n  {}\n\n  DSOReferenceImplementation::~DSOReferenceImplementation(void)\n  {}\n\n  CodeImplementation *DSOReferenceImplementation::clone(void) const\n  {\n    return new DSOReferenceImplementation(dso_name, symbol_name);\n  }\n\n  bool DSOReferenceImplementation::is_portable(void) const\n  {\n    return true;\n  }\n\n#ifdef REALM_USE_DLADDR\n  namespace {\n    extern \"C\" { int main(int argc, const char *argv[]) __attribute__((weak)); };\n\n    DSOReferenceImplementation *dladdr_helper(void *ptr, bool quiet)\n    {\n      // if dladdr() gives us something with the same base pointer, assume that's portable\n      // note: return code is not-POSIX-y (i.e. 0 == failure)\n      Dl_info inf;\n      int ret = dladdr(ptr, &inf);\n      if(ret == 0) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"couldn't map fnptr \" << ptr << \" to a dynamic symbol\";\n\treturn 0;\n      }\n\n      if(inf.dli_saddr != ptr) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"pointer \" << ptr << \" in middle of symbol '\" << inf.dli_sname << \" (\" << inf.dli_saddr << \")?\";\n\treturn 0;\n      }\n\n      // try to detect symbols that are in the base executable and change the filename to \"\"\n      // only do this if the weak 'main' reference found an actual main\n      if(((void *)main) != 0) {\n\tconst char *fname = inf.dli_fname;\n\t{\n\t  static std::string local_fname;\n\t  if(local_fname.empty()) {\n\t    Dl_info inf2;\n\t    ret = dladdr((void *)main, &inf2);\n\t    assert(ret != 0);\n\t    local_fname = inf2.dli_fname;\n\t  }\n\t  if(local_fname.compare(fname) == 0)\n\t    fname = \"\";\n\t}\n\n\treturn new DSOReferenceImplementation(fname, inf.dli_sname);\n      }\n\n      return 0;\n    }\n  };\n#endif\n\n  /*static*/ DSOReferenceImplementation *DSOReferenceImplementation::cvt_fnptr_to_dsoref(const FunctionPointerImplementation *fpi,\n\t\t\t\t\t\t\t\t\t\t\t bool quiet /*= false*/)\n  {\n    return dladdr_helper((void *)(fpi->fnptr), quiet);\n  } \n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeTranslator\n\n  CodeTranslator::CodeTranslator(const std::string& _name)\n    : name(_name)\n  {}\n\n  CodeTranslator::~CodeTranslator(void)\n  {}\n\n  // default version just iterates over all the implementations in the source\n  bool CodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t     const std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn true;\n    }\n\n    return false;\n  }\n\n  // default version just iterates over all the implementations in the source\n  CodeImplementation *CodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn translate(*it, target_impl_type);\n    }\n\n    return 0;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOCodeTranslator\n\n#ifdef REALM_USE_DLFCN\n  DSOCodeTranslator::DSOCodeTranslator(void)\n    : CodeTranslator(\"dso\")\n  {}\n\n  DSOCodeTranslator::~DSOCodeTranslator(void)\n  {\n    // unload any modules we have loaded\n    for(std::map<std::string, void *>::iterator it = modules_loaded.begin();\n\tit != modules_loaded.end();\n\tit++) {\n      int ret = dlclose(it->second);\n      if(ret != 0)\n\tlog_codetrans.warning() << \"error on dlclose of '\" << it->first << \"': \" << dlerror();\n    }\n  }\n\n  bool DSOCodeTranslator::can_translate(const std::type_info& source_impl_type,\n\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    // DSO ref -> function pointer\n    if((source_impl_type == typeid(DSOReferenceImplementation)) &&\n       (target_impl_type == typeid(FunctionPointerImplementation)))\n      return true;\n\n#ifdef REALM_USE_DLADDR\n    if((source_impl_type == typeid(FunctionPointerImplementation)) &&\n       (target_impl_type == typeid(DSOReferenceImplementation)))\n      return true;\n#endif\n\n      return false;\n    }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeImplementation *source,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    if(target_impl_type == typeid(FunctionPointerImplementation)) {\n      const DSOReferenceImplementation *dsoref = dynamic_cast<const DSOReferenceImplementation *>(source);\n      assert(dsoref != 0);\n\n      void *handle = 0;\n      // check to see if we've already loaded the module?\n      std::map<std::string, void *>::iterator it = modules_loaded.find(dsoref->dso_name);\n      if(it != modules_loaded.end()) {\n\thandle = it->second;\n      } else {\n\t// try to load it - empty string for dso_name means the main executable\n\tconst char *dso_name = dsoref->dso_name.c_str();\n\thandle = dlopen(*dso_name ? dso_name : 0, RTLD_NOW | RTLD_LOCAL);\n\tif(!handle) {\n\t  log_codetrans.warning() << \"could not open DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\t  return 0;\n\t}\n\tmodules_loaded[dsoref->dso_name] = handle;\n      }\n\n      void *ptr = dlsym(handle, dsoref->symbol_name.c_str());\n      if(!ptr) {\n\tlog_codetrans.warning() << \"could not find symbol '\" << dsoref->symbol_name << \"' in  DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\treturn 0;\n      }\n\n      return new FunctionPointerImplementation((void(*)())ptr);\n    }\n\n#ifdef REALM_USE_DLADDR\n    if(target_impl_type == typeid(DSOReferenceImplementation)) {\n      const FunctionPointerImplementation *fpi = dynamic_cast<const FunctionPointerImplementation *>(source);\n      assert(fpi != 0);\n\n      return dladdr_helper((void *)(fpi->fnptr), false /*!quiet*/);\n    }\n#endif\n\n    return 0;\n  }\n\n  // these pass through to CodeTranslator's definitions\n  bool DSOCodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    return CodeTranslator::can_translate(source_codedesc, target_impl_type);\n  }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    return CodeTranslator::translate(source_codedesc, target_impl_type);\n  }\n#endif\n\n\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/runtime/realm/module.cc": "/* Copyright 2017 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Realm modules\n\n#include \"realm/realm_config.h\"\n\n#define REALM_MODULE_REGISTRATION_STATIC\n#include \"realm/module.h\"\n\n#include \"realm/logging.h\"\n\n#include <assert.h>\n#include <string.h>\n#include <stdlib.h>\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n// TODO: replace this with Makefile (or maybe cmake) magic that adapts automatically\n//  to the build-system-controlled list of statically-linked Realm modules\n#include \"realm/runtime_impl.h\"\n#include \"realm/numa/numa_module.h\"\n#ifdef REALM_USE_OPENMP\n#include \"realm/openmp/openmp_module.h\"\n#endif\n#include \"realm/procset/procset_module.h\"\n#ifdef REALM_USE_PYTHON\n#include \"realm/python/python_module.h\"\n#endif\n#ifdef USE_CUDA\n#include \"realm/cuda/cuda_module.h\"\n#endif\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit_module.h\"\n#endif\n#ifdef USE_HDF\n#include \"realm/hdf5/hdf5_module.h\"\n#endif\n\nnamespace Realm {\n\n  Logger log_module(\"module\");\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Module\n  //\n\n  Module::Module(const std::string& _name)\n    : name(_name)\n  {\n    log_module.debug() << \"module \" << name << \" created\";\n  }\n\n  Module::~Module(void)\n  {\n    log_module.debug() << \"module \" << name << \" destroyed\";\n  }\n\n  const std::string& Module::get_name(void) const\n  {\n    return name;\n  }\n\n  void Module::initialize(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" initialize\";\n  }\n\n  void Module::create_memories(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_memories\";\n  }\n\n  void Module::create_processors(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_processors\";\n  }\n  \n  void Module::create_dma_channels(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_dma_channels\";\n  }\n  \n  void Module::create_code_translators(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_code_translators\";\n  }\n\n  void Module::cleanup(void)\n  {\n    log_module.debug() << \"module \" << name << \" cleanup\";\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class ModuleRegistrar\n  //\n\n  std::vector<const ModuleRegistrar::StaticRegistrationBase *>& static_module_registrations(void)\n  {\n    static std::vector<const ModuleRegistrar::StaticRegistrationBase *> data;\n    return data;\n  }\n\n  ModuleRegistrar::ModuleRegistrar(RuntimeImpl *_runtime)\n    : runtime(_runtime)\n  {}\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_static_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t      std::vector<Module *>& modules)\n  {\n    // just iterate over the static module list, trying to create each module\n    for(std::vector<const StaticRegistrationBase *>::const_iterator it = static_module_registrations().begin();\n\tit != static_module_registrations().end();\n\tit++) {\n      Module *m = (*it)->create_module(runtime, cmdline);\n      if(m)\n\tmodules.push_back(m);\n    }\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  // accepts a colon-separated list of so files to try to load\n  static void load_module_list(const char *sonames,\n\t\t\t       RuntimeImpl *runtime,\n\t\t\t       std::vector<std::string>& cmdline,\n\t\t\t       std::vector<void *>& handles,\n\t\t\t       std::vector<Module *>& modules)\n  {\n    // null/empty strings are nops\n    if(!sonames || !*sonames) return;\n\n    const char *p1 = sonames;\n    while(true) {\n      // skip leading colons\n      while(*p1 == ':') p1++;\n      if(!*p1) break;\n\n      const char *p2 = p1 + 1;\n      while(*p2 && (*p2 != ':')) p2++;\n\n      char filename[1024];\n      strncpy(filename, p1, p2 - p1);\n\n      // no leftover errors from anybody else please...\n      assert(dlerror() == 0);\n\n      // open so file, resolving all symbols but not polluting global namespace\n      void *handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n\n      if(handle != 0) {\n\t// this file should have a \"create_realm_module\" symbol\n\tvoid *sym = dlsym(handle, \"create_realm_module\");\n\n\tif(sym != 0) {\n\t  // TODO: hold onto the handle even if it doesn't create a module?\n\t  handles.push_back(handle);\n\n\t  Module *m = ((Module *(*)(RuntimeImpl *, std::vector<std::string>&))dlsym)(runtime, cmdline);\n\t  if(m)\n\t    modules.push_back(m);\n\t} else {\n\t  log_module.error() << \"symbol 'create_realm_module' not found in \" << filename;\n#ifndef NDEBUG\n\t  int ret =\n#endif\n\t    dlclose(handle);\n\t  assert(ret == 0);\n\t}\n      } else {\n\tlog_module.error() << \"could not load \" << filename << \": \" << dlerror();\n      }\n\n      if(!*p2) break;\n      p1 = p2 + 1;\n    }\n  }\n#endif\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_dynamic_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t       std::vector<Module *>& modules)\n  {\n    // dynamic modules are requested in one of two ways:\n    // 1) REALM_DYNAMIC_MODULES=sonames environment variable\n    // 2) \"-ll:module sonames\" on command line\n    // in both cases, 'sonames' is a colon-separate listed of .so files that should be\n\n    // loading modules can also monkey with the cmdline, so do a pass first where we pull\n    //  out all the name we want to load\n    std::vector<std::string> sonames_list;\n\n    {\n      const char *e = getenv(\"REALM_DYNAMIC_MODULES\");\n      if(e)\n\tsonames_list.push_back(std::string(e));\n    }\n\n    {\n      std::vector<std::string>::iterator it = cmdline.begin();\n      while(it != cmdline.end()) {\n\tif(*it != \"-ll:module\") {\n\t  it++;\n\t  continue;\n\t}\n\n\t// eat this argument and move the next one to sonames_list\n\tit = cmdline.erase(it);\n\tassert(it != cmdline.end());\n\tsonames_list.push_back(*it);\n\tit = cmdline.erase(it);\n      }\n    }\n\n#ifdef REALM_USE_DLFCN\n    for(std::vector<std::string>::const_iterator it = sonames_list.begin();\n\tit != sonames_list.end();\n\tit++)\n      load_module_list(it->c_str(),\n\t\t       runtime, cmdline, sofile_handles, modules);\n#else\n    if(!sonames_list.empty()) {\n      log_module.error() << \"loading of dynamic Realm modules requested, but REALM_USE_DLFCN=0!\";\n      exit(1);\n    }\n#endif\n  }\n\n  // called by runtime after all modules have been cleaned up\n  void ModuleRegistrar::unload_module_sofiles(void)\n  {\n#ifdef REALM_USE_DLFCN\n    while(!sofile_handles.empty()) {\n      void *handle = sofile_handles.back();\n      sofile_handles.pop_back();\n\n#ifndef NDEBUG\n      int ret =\n#endif\n\tdlclose(handle);\n      assert(ret == 0);\n    }\n#endif\n  }\n\n  // called by the module registration helpers\n  /*static*/ void ModuleRegistrar::add_static_registration(const StaticRegistrationBase *reg)\n  {\n    // done during init, so single-threaded\n    static_module_registrations().push_back(reg);\n  }\n  \n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/runtime/realm/python/python_module.cc": "/* Copyright 2017 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"realm/python/python_module.h\"\n#include \"realm/python/python_internal.h\"\n\n#include \"realm/numa/numasysif.h\"\n#include \"realm/logging.h\"\n#include \"realm/cmdline.h\"\n#include \"realm/proc_impl.h\"\n#include \"realm/threads.h\"\n#include \"realm/runtime_impl.h\"\n#include \"realm/utils.h\"\n\n#include <dlfcn.h>\n#include <link.h>\n\n#include <list>\n\nnamespace Realm {\n\n  Logger log_py(\"python\");\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonAPI\n\n  PythonAPI::PythonAPI(void *_handle)\n    : handle(_handle)\n  {\n    get_symbol(this->Py_DecRef, \"Py_DecRef\");\n    get_symbol(this->Py_Finalize, \"Py_Finalize\");\n    get_symbol(this->Py_InitializeEx, \"Py_InitializeEx\");\n\n    get_symbol(this->PyByteArray_FromStringAndSize, \"PyByteArray_FromStringAndSize\");\n\n    get_symbol(this->PyEval_InitThreads, \"PyEval_InitThreads\");\n    get_symbol(this->PyThreadState_New, \"PyThreadState_New\");\n    get_symbol(this->PyThreadState_Clear, \"PyThreadState_Clear\");\n    get_symbol(this->PyThreadState_Delete, \"PyThreadState_Delete\");\n    get_symbol(this->PyThreadState_Get, \"PyThreadState_Get\");\n    get_symbol(this->PyThreadState_Swap, \"PyThreadState_Swap\");\n    get_symbol(this->PyEval_RestoreThread, \"PyEval_RestoreThread\");\n    get_symbol(this->PyEval_SaveThread, \"PyEval_SaveThread\");\n\n    get_symbol(this->PyErr_PrintEx, \"PyErr_PrintEx\");\n\n    get_symbol(this->PyImport_ImportModule, \"PyImport_ImportModule\");\n    get_symbol(this->PyModule_GetDict, \"PyModule_GetDict\");\n\n    get_symbol(this->PyLong_FromUnsignedLong, \"PyLong_FromUnsignedLong\");\n\n    get_symbol(this->PyObject_CallFunction, \"PyObject_CallFunction\");\n    get_symbol(this->PyObject_CallObject, \"PyObject_CallObject\");\n    get_symbol(this->PyObject_GetAttrString, \"PyObject_GetAttrString\");\n    get_symbol(this->PyObject_Print, \"PyObject_Print\");\n\n    get_symbol(this->PyRun_SimpleString, \"PyRun_SimpleString\");\n    get_symbol(this->PyRun_String, \"PyRun_String\");\n\n    get_symbol(this->PyTuple_New, \"PyTuple_New\");\n    get_symbol(this->PyTuple_SetItem, \"PyTuple_SetItem\");\n  }\n\n  template<typename T>\n  void PythonAPI::get_symbol(T &fn, const char *symbol,\n                             bool missing_ok /*= false*/)\n  {\n    fn = reinterpret_cast<T>(dlsym(handle, symbol));\n    if(!fn && !missing_ok) {\n      const char *error = dlerror();\n      log_py.fatal() << \"failed to find symbol '\" << symbol << \"': \" << error;\n      assert(false);\n    }\n  }\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonInterpreter\n\n#ifdef REALM_USE_DLMOPEN\n  // dlmproxy symbol lookups have to happen in a function we define so that\n  //  dl[v]sym searches in the right place\n  static void *dlmproxy_lookup(const char *symname, const char *symver)\n  {\n    \n    void *handle = 0;\n    void *sym = (symver ?\n\t\t   dlvsym(handle, symname, symver) :\n\t\t   dlsym(handle, symname));\n    if(sym)\n      log_py.debug() << \"found symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\") << \" ptr=\" << sym;\n    else\n      log_py.warning() << \"missing symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\");\n    return sym;\n  }\n#endif\n\n  PythonInterpreter::PythonInterpreter() \n  {\n#ifdef REALM_PYTHON_LIB\n    const char *python_lib = REALM_PYTHON_LIB;\n#else\n    const char *python_lib = \"libpython2.7.so\";\n#endif\n\n#ifdef REALM_USE_DLMOPEN\n    // loading libpython into its own namespace will cause it to try to bring\n    //   in a second copy of libpthread.so.0, which is fairly disastrous\n    // we deal with it by loading a \"dlmproxy\" of pthreads that tunnels all \n    //   pthreads calls back to the (only) version in the main executable\n    const char *dlmproxy_filename = getenv(\"DLMPROXY_LIBPTHREAD\");\n    if(!dlmproxy_filename)\n      dlmproxy_filename = \"dlmproxy_libpthread.so.0\";\n    dlmproxy_handle = dlmopen(LM_ID_NEWLM,\n\t\t\t      dlmproxy_filename,\n\t\t\t      RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_LAZY);\n    if(!dlmproxy_handle) {\n      const char *error = dlerror();\n      log_py.fatal() << \"HELP!  Use of dlmopen for python requires dlmproxy for pthreads!  Failed to\\n\"\n\t\t     << \"  load: \" << dlmproxy_filename << \"\\n\"\n\t\t     << \"  error: \" << error;\n      assert(false);\n    }\n\n    // now that the proxy is loaded, we need to tell it where the real\n    //  libpthreads functions are\n    {\n      void *sym = dlsym(dlmproxy_handle, \"dlmproxy_load_symbols\");\n      assert(sym != 0);\n      ((void (*)(void *(*)(const char *, const char *)))sym)(dlmproxy_lookup);\n    }\n\n    // now we can load libpython, but make sure we do it in the new namespace\n    Lmid_t lmid;\n    int ret = dlinfo(dlmproxy_handle, RTLD_DI_LMID, &lmid);\n    assert(ret == 0);\n\n    handle = dlmopen(lmid, python_lib, RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_NOW);\n#else\n    // life is so much easier if we use dlopen (but we only get one copy then)\n    handle = dlopen(python_lib, RTLD_GLOBAL | RTLD_LAZY);\n#endif\n    if (!handle) {\n      const char *error = dlerror();\n      log_py.fatal() << error;\n      assert(false);\n    }\n\n    api = new PythonAPI(handle);\n\n    (api->Py_InitializeEx)(0 /*!initsigs*/);\n    (api->PyEval_InitThreads)();\n    //(api->Py_Finalize)();\n\n    //PyThreadState *state;\n    //state = (api->PyEval_SaveThread)();\n    //(api->PyEval_RestoreThread)(state);\n\n    //(api->PyRun_SimpleString)(\"print 'hello Python world!'\");\n\n    //PythonSourceImplementation psi(\"taskreg_helper\", \"task1\");\n    //find_or_import_function(&psi);\n  }\n\n  PythonInterpreter::~PythonInterpreter()\n  {\n    (api->Py_Finalize)();\n\n    delete api;\n\n    if (dlclose(handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"libpython dlclose error: \" << error;\n      assert(false);\n    }\n\n#ifdef REALM_USE_DLMOPEN\n    if (dlclose(dlmproxy_handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"dlmproxy dlclose error: \" << error;\n      assert(false);\n    }\n#endif\n  }\n\n  PyObject *PythonInterpreter::find_or_import_function(const PythonSourceImplementation *psi)\n  {\n    //log_py.print() << \"attempting to acquire python lock\";\n    //(api->PyEval_AcquireLock)();\n    //log_py.print() << \"lock acquired\";\n\n    // not calling PythonInterpreter::import_module here because we want the\n    //  PyObject result\n    log_py.debug() << \"attempting to import module: \" << psi->module_name;\n    PyObject *module = (api->PyImport_ImportModule)(psi->module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    //(api->PyObject_Print)(module, stdout, 0); printf(\"\\n\");\n\n    log_py.debug() << \"finding attribute '\" << psi->function_name << \"' in module '\" << psi->module_name << \"'\";\n    PyObject *function = (api->PyObject_GetAttrString)(module, psi->function_name.c_str());\n    if (!function) {\n      log_py.fatal() << \"unable to import Python function \" << psi->function_name << \" from module\" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    //(api->PyObject_Print)(function, stdout, 0); printf(\"\\n\");\n\n    //(api->PyObject_CallFunction)(function, \"iii\", 1, 2, 3);\n\n    (api->Py_DecRef)(module);\n\n    return function;\n  }\n\n  void PythonInterpreter::import_module(const std::string& module_name)\n  {\n    log_py.debug() << \"attempting to import module: \" << module_name;\n    PyObject *module = (api->PyImport_ImportModule)(module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << module_name;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    (api->Py_DecRef)(module);\n  }\n\n  void PythonInterpreter::run_string(const std::string& script_text)\n  {\n    // from Python.h\n    const int Py_file_input = 257;\n\n    log_py.debug() << \"running python string: \" << script_text;\n    PyObject *mainmod = (api->PyImport_ImportModule)(\"__main__\");\n    assert(mainmod != 0);\n    PyObject *globals = (api->PyModule_GetDict)(mainmod);\n    assert(globals != 0);\n    PyObject *res = (api->PyRun_String)(script_text.c_str(),\n\t\t\t\t\tPy_file_input,\n\t\t\t\t\tglobals,\n\t\t\t\t\tglobals);\n    if(!res) {\n      log_py.fatal() << \"unable to run python string:\" << script_text;\n      (api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n    (api->Py_DecRef)(res);\n    (api->Py_DecRef)(globals);\n    (api->Py_DecRef)(mainmod);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonThreadTaskScheduler\n\n  PythonThreadTaskScheduler::PythonThreadTaskScheduler(LocalPythonProcessor *_pyproc,\n\t\t\t\t\t\t       CoreReservation& _core_rsrv)\n    : KernelThreadTaskScheduler(_pyproc->me, _core_rsrv)\n    , pyproc(_pyproc)\n  {}\n\n  void PythonThreadTaskScheduler::enqueue_taskreg(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    AutoHSLLock al(lock);\n    taskreg_queue.push_back(treg);\n    // we've added work to the system\n    work_counter.increment_counter();\n  }\n\n  void PythonThreadTaskScheduler::python_scheduler_loop(void)\n  {\n    // hold scheduler lock for whole thing\n    AutoHSLLock al(lock);\n\n    // global startup of python interpreter if needed\n    if(pyproc->interpreter == 0) {\n      log_py.info() << \"creating interpreter\";\n      pyproc->create_interpreter();\n    }\n\n    // always create and remember our own python thread - does NOT require GIL\n    PyThreadState *pythread = (pyproc->interpreter->api->PyThreadState_New)(pyproc->master_thread->interp);\n    log_py.debug() << \"created python thread: \" << pythread;\n    \n    assert(pythread != 0);\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = pythread;\n\n    // now go into main scheduler loop\n    while(true) {\n      // remember the work counter value before we start so that we don't iterate\n      //   unnecessarily\n      long long old_work_counter = work_counter.read_counter();\n\n      // first rule - always yield to a resumable worker\n      while(!resumable_workers.empty()) {\n\tThread *yield_to = resumable_workers.get(0); // priority is irrelevant\n\tassert(yield_to != Thread::self());\n\n\t// this should only happen if we're at the max active worker count (otherwise\n\t//  somebody should have just woken this guy up earlier), and reduces the \n\t// unassigned worker count by one\n\tupdate_worker_count(0, -1);\n\n\tidle_workers.push_back(Thread::self());\n\tworker_sleep(yield_to);\n\n\t// we're awake again, but still looking for work...\n\told_work_counter = work_counter.read_counter();  // re-read - may have changed while we slept\n      }\n\n      // next priority - task registration\n      while(!taskreg_queue.empty()) {\n\tLocalPythonProcessor::TaskRegistration *treg = taskreg_queue.front();\n\ttaskreg_queue.pop_front();\n\t\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\t\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = TaskQueue::PRI_POS_INF;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  pyproc->perform_task_registration(treg);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      }\n\n      // try to get a new task then\n      // remember where a task has come from in case we want to put it back\n      Task *task = 0;\n      TaskQueue *task_source = 0;\n      int task_priority = TaskQueue::PRI_NEG_INF;\n      for(std::vector<TaskQueue *>::const_iterator it = task_queues.begin();\n\t  it != task_queues.end();\n\t  it++) {\n\tint new_priority;\n\tTask *new_task = (*it)->get(&new_priority, task_priority);\n\tif(new_task) {\n\t  // if we got something better, put back the old thing (if any)\n\t  if(task)\n\t    task_source->put(task, task_priority, false); // back on front of list\n\t  \n\t  task = new_task;\n\t  task_source = *it;\n\t  task_priority = new_priority;\n\t}\n      }\n\n      // did we find work to do?\n      if(task) {\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = task_priority;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n\t// make our python thread state active, acquiring the GIL\n\tassert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\tlog_py.debug() << \"RestoreThread <- \" << pythread;\n\t(pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  execute_task(task);\n\tassert(ok);  // no fault recovery yet\n\n\t// release the GIL\n\tPyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n\tlog_py.debug() << \"SaveThread -> \" << saved;\n\tassert(saved == pythread);\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      } else {\n\t// no?  thumb twiddling time\n\n\t// are we shutting down?\n\tif(shutdown_flag) {\n\t  // yes, we can terminate - wake up an idler (if any) first though\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t  } else {\n\t    // nobody to wake, so -1 active/unassigned worker\n\t    update_worker_count(-1, -1, false); // ok to drop below mins\n\t    worker_terminate(0);\n\t  }\n\t  return;\n\t}\n\n\t// do we have more unassigned and idle tasks than we need?\n\tint total_idle_count = (unassigned_worker_count +\n\t\t\t\t(int)(idle_workers.size()));\n\tif(total_idle_count > cfg_max_idle_workers) {\n\t  // if there are sleeping idlers, terminate in favor of one of those - keeps\n\t  //  worker counts constant\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    assert(to_wake != Thread::self());\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t    return;\n\t  }\n\t}\n\n\t// no, stay awake but suspend until there's a chance that the next iteration\n\t//  of this loop would turn out different\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // should never get here\n    assert(0);\n  }\n\n  Thread *PythonThreadTaskScheduler::worker_create(bool make_active)\n  {\n    // lock is held by caller\n    ThreadLaunchParameters tlp;\n    Thread *t = Thread::create_kernel_thread<PythonThreadTaskScheduler,\n\t\t\t\t\t     &PythonThreadTaskScheduler::python_scheduler_loop>(this,\n\t\t\t\t\t\t\t\t\t\t\t\ttlp,\n\t\t\t\t\t\t\t\t\t\t\t\tcore_rsrv,\n\t\t\t\t\t\t\t\t\t\t\t\tthis);\n    all_workers.insert(t);\n    if(make_active)\n      active_workers.insert(t);\n    return t;\n  }\n \n  // called by a worker thread when it needs to wait for something (and we\n  //   should release the GIL)\n  void PythonThreadTaskScheduler::thread_blocking(Thread *thread)\n  {\n    // if we got here through a cffi call, the GIL has already been released,\n    //  so try to handle that case here - a call PyEval_SaveThread\n    //  if the GIL is not held will assert-fail, and while a call to\n    //  PyThreadState_Swap is technically illegal (and unsafe if python-created\n    //  threads exist), it does what we want for now\n    PyThreadState *saved = (pyproc->interpreter->api->PyThreadState_Swap)(0);\n    if(saved != 0) {\n      log_py.info() << \"python worker sleeping - releasing GIL\";\n      // put it back so we can save it properly\n      (pyproc->interpreter->api->PyThreadState_Swap)(saved);\n      // would like to sanity-check that this returns the expected thread state,\n      //  but that would require taking the PythonThreadTaskScheduler's lock\n      (pyproc->interpreter->api->PyEval_SaveThread)();\n      log_py.debug() << \"SaveThread -> \" << saved;\n    } else\n      log_py.info() << \"python worker sleeping - GIL already released\";\n    \n    KernelThreadTaskScheduler::thread_blocking(thread);\n\n    if(saved) {\n      log_py.info() << \"python worker awake - acquiring GIL\";\n      log_py.debug() << \"RestoreThread <- \" << saved;\n      (pyproc->interpreter->api->PyEval_RestoreThread)(saved);\n    } else\n      log_py.info() << \"python worker awake - not acquiring GIL\";\n  }\n\n  void PythonThreadTaskScheduler::worker_terminate(Thread *switch_to)\n  {\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    PyThreadState *pythread = it->second;\n    pythreads.erase(it);\n\n    log_py.debug() << \"destroying python thread: \" << pythread;\n    \n    // our thread should not be active\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\n    // switch to the master thread, retaining the GIL\n    log_py.debug() << \"RestoreThread <- \" << pyproc->master_thread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pyproc->master_thread);\n\n    // clear and delete the worker thread\n    (pyproc->interpreter->api->PyThreadState_Clear)(pythread);\n    (pyproc->interpreter->api->PyThreadState_Delete)(pythread);\n\n    // release the GIL\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pyproc->master_thread);\n\n    // TODO: tear down interpreter if last thread\n    if(shutdown_flag && pythreads.empty())\n      pyproc->destroy_interpreter();\n\n    KernelThreadTaskScheduler::worker_terminate(switch_to);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class LocalPythonProcessor\n\n  LocalPythonProcessor::LocalPythonProcessor(Processor _me, int _numa_node,\n                                             CoreReservationSet& crs,\n                                             size_t _stack_size,\n\t\t\t\t\t     const std::vector<std::string>& _import_modules,\n\t\t\t\t\t     const std::vector<std::string>& _init_scripts)\n    : ProcessorImpl(_me, Processor::PY_PROC)\n    , numa_node(_numa_node)\n    , import_modules(_import_modules)\n    , init_scripts(_init_scripts)\n    , interpreter(0)\n    , ready_task_count(stringbuilder() << \"realm/proc \" << me << \"/ready tasks\")\n  {\n    task_queue.set_gauge(&ready_task_count);\n\n    CoreReservationParameters params;\n    params.set_num_cores(1);\n    params.set_numa_domain(numa_node);\n    params.set_alu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_fpu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_ldst_usage(params.CORE_USAGE_SHARED);\n    params.set_max_stack_size(_stack_size);\n\n    std::string name = stringbuilder() << \"Python\" << numa_node << \" proc \" << _me;\n\n    core_rsrv = new CoreReservation(name, crs, params);\n\n    sched = new PythonThreadTaskScheduler(this, *core_rsrv);\n    sched->add_task_queue(&task_queue);\n    sched->start();\n  }\n\n  LocalPythonProcessor::~LocalPythonProcessor(void)\n  {\n    delete core_rsrv;\n    delete sched;\n  }\n\n  void LocalPythonProcessor::shutdown(void)\n  {\n    log_py.info() << \"shutting down\";\n\n    sched->shutdown();\n  }\n\n  void LocalPythonProcessor::create_interpreter(void)\n  {\n    assert(interpreter == 0);\n  \n    // create a python interpreter that stays entirely within this thread\n    interpreter = new PythonInterpreter;\n    master_thread = (interpreter->api->PyThreadState_Get)();\n\n    // always need the python threading module\n    interpreter->import_module(\"threading\");\n    \n    // perform requested initialization\n    for(std::vector<std::string>::const_iterator it = import_modules.begin();\n\tit != import_modules.end();\n\t++it)\n      interpreter->import_module(*it);\n\n    for(std::vector<std::string>::const_iterator it = init_scripts.begin();\n\tit != init_scripts.end();\n\t++it)\n      interpreter->run_string(*it);\n\n    // default state is GIL _released_\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n  }\n\n  void LocalPythonProcessor::destroy_interpreter(void)\n  {\n    assert(interpreter != 0);\n\n    // take GIL with master thread\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n\n    // during shutdown, the threading module tries to remove the Thread object\n    //  associated with this kernel thread - if that doesn't exist (because we're\n    //  shutting down from a different thread that we initialized the interpreter\n    //  _and_ nobody called threading.current_thread() from this kernel thread),\n    //  we'll get a KeyError in threading.py\n    // resolve this by calling threading.current_thread() here, using __import__\n    //  to deal with the case where 'import threading' never got called\n    (interpreter->api->PyRun_SimpleString)(\"__import__('threading').current_thread()\");\n\n    delete interpreter;\n    interpreter = 0;\n    master_thread = 0;\n  }\n  \n  bool LocalPythonProcessor::perform_task_registration(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(treg->func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << treg->func_id;\n      assert(0);\n    }\n\n    // next, see if we have a Python function to register\n    const PythonSourceImplementation *psi = treg->codedesc->find_impl<PythonSourceImplementation>();\n    if(!psi) {\n      log_py.fatal() << \"invalid code descriptor for python proc: \" << *(treg->codedesc);\n      assert(0);\n    }\n\n    // perform import/compile on master thread\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n    \n    PyObject *fnptr = interpreter->find_or_import_function(psi);\n    assert(fnptr != 0);\n\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n\n    log_py.info() << \"task \" << treg->func_id << \" registered on \" << me << \": \" << *(treg->codedesc);\n\n    TaskTableEntry &tte = task_table[treg->func_id];\n    tte.fnptr = fnptr;\n    tte.user_data.swap(treg->user_data);\n\n    delete treg->codedesc;\n    delete treg;\n\n    return true;\n  }\n\n  void LocalPythonProcessor::enqueue_task(Task *task)\n  {\n    // just jam it into the task queue, scheduler will take care of the rest\n    if(task->mark_ready())\n      task_queue.put(task, task->priority);\n    else\n      task->mark_finished(false /*!successful*/);\n  }\n\n  void LocalPythonProcessor::spawn_task(Processor::TaskFuncID func_id,\n\t\t\t\t\tconst void *args, size_t arglen,\n\t\t\t\t\tconst ProfilingRequestSet &reqs,\n\t\t\t\t\tEvent start_event, Event finish_event,\n\t\t\t\t\tint priority)\n  {\n    // create a task object for this\n    Task *task = new Task(me, func_id, args, arglen, reqs,\n\t\t\t  start_event, finish_event, priority);\n    get_runtime()->optable.add_local_operation(finish_event, task);\n\n    // if the start event has already triggered, we can enqueue right away\n    bool poisoned = false;\n    if (start_event.has_triggered_faultaware(poisoned)) {\n      if(poisoned) {\n\tlog_poison.info() << \"cancelling poisoned task - task=\" << task << \" after=\" << task->get_finish_event();\n\ttask->handle_poisoned_precondition(start_event);\n      } else\n\tenqueue_task(task);\n    } else {\n      EventImpl::add_waiter(start_event, new DeferredTaskSpawn(this, task));\n    }\n  }\n\n  void LocalPythonProcessor::add_to_group(ProcessorGroup *group)\n  {\n    // add the group's task queue to our scheduler too\n    sched->add_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::register_task(Processor::TaskFuncID func_id,\n                                           CodeDescriptor& codedesc,\n                                           const ByteArrayRef& user_data)\n  {\n    TaskRegistration *treg = new TaskRegistration;\n    treg->func_id = func_id;\n    treg->codedesc = new CodeDescriptor(codedesc);\n    treg->user_data = user_data;\n    sched->enqueue_taskreg(treg);\n#if 0\n    {\n      AutoHSLLock al(mutex);\n      bool was_empty = taskreg_queue.empty() && task_queue.empty();\n      taskreg_queue.push_back(treg);\n      if(was_empty)\n\tcondvar.signal();\n    }\n#endif\n#if 0\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << func_id;\n      assert(0);\n    }\n\n    // next, get see if we have a Python function to register\n    const PythonSourceImplementation *psi = codedesc.find_impl<PythonSourceImplementation>();\n    assert(psi != 0);\n\n    PyObject *fnptr = interpreter->find_or_import_function(psi);\n\n    log_py.info() << \"task \" << func_id << \" registered on \" << me << \": \" << codedesc;\n\n    TaskTableEntry &tte = task_table[func_id];\n    tte.fnptr = fnptr;\n    tte.user_data = user_data;\n#endif\n  }\n\n  void LocalPythonProcessor::execute_task(Processor::TaskFuncID func_id,\n\t\t\t\t\t  const ByteArrayRef& task_args)\n  {\n    std::map<Processor::TaskFuncID, TaskTableEntry>::const_iterator it = task_table.find(func_id);\n    if(it == task_table.end()) {\n      // TODO: remove this hack once the tools are available to the HLR to call these directly\n      if(func_id < Processor::TASK_ID_FIRST_AVAILABLE) {\n\tlog_py.info() << \"task \" << func_id << \" not registered on \" << me << \": ignoring missing legacy setup/shutdown task\";\n\treturn;\n      }\n      log_py.fatal() << \"task \" << func_id << \" not registered on \" << me;\n      assert(0);\n    }\n\n    const TaskTableEntry& tte = it->second;\n\n    log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": \" << ((void *)(tte.fnptr));\n\n    PyObject *arg1 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)task_args.base(),\n\t\t\t\t\t\t   task_args.size());\n    assert(arg1 != 0);\n    PyObject *arg2 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)tte.user_data.base(),\n\t\t\t\t\t\t   tte.user_data.size());\n    assert(arg2 != 0);\n    // TODO: make into a Python realm.Processor object\n    PyObject *arg3 = (interpreter->api->PyLong_FromUnsignedLong)(me.id);\n    assert(arg3 != 0);\n\n    PyObject *args = (interpreter->api->PyTuple_New)(3);\n    assert(args != 0);\n    (interpreter->api->PyTuple_SetItem)(args, 0, arg1);\n    (interpreter->api->PyTuple_SetItem)(args, 1, arg2);\n    (interpreter->api->PyTuple_SetItem)(args, 2, arg3);\n\n    //printf(\"args = \"); (interpreter->api->PyObject_Print)(args, stdout, 0); printf(\"\\n\");\n\n    PyObject *res = (interpreter->api->PyObject_CallObject)(tte.fnptr, args);\n\n    (interpreter->api->Py_DecRef)(args);\n\n    //printf(\"res = \"); PyObject_Print(res, stdout, 0); printf(\"\\n\");\n    if(res != 0) {\n      (interpreter->api->Py_DecRef)(res);\n    } else {\n      log_py.fatal() << \"python exception occurred within task:\";\n      (interpreter->api->PyErr_PrintEx)(0);\n      assert(0);\n    }\n  }\n\n  namespace Python {\n\n    ////////////////////////////////////////////////////////////////////////\n    //\n    // class PythonModule\n\n    /*static*/ std::vector<std::string> PythonModule::extra_import_modules;\n\n    PythonModule::PythonModule(void)\n      : Module(\"python\")\n      , cfg_num_python_cpus(0)\n      , cfg_use_numa(false)\n      , cfg_stack_size_in_mb(2)\n    {\n    }\n\n    PythonModule::~PythonModule(void)\n    {}\n\n    /*static*/ void PythonModule::import_python_module(const char *module_name)\n    {\n      extra_import_modules.push_back(module_name);\n    }\n\n    /*static*/ Module *PythonModule::create_module(RuntimeImpl *runtime,\n                                                 std::vector<std::string>& cmdline)\n    {\n      // create a module to fill in with stuff - we'll delete it if numa is\n      //  disabled\n      PythonModule *m = new PythonModule;\n\n      // first order of business - read command line parameters\n      {\n        CommandLineParser cp;\n\n        cp.add_option_int(\"-ll:py\", m->cfg_num_python_cpus)\n\t  .add_option_int(\"-ll:pynuma\", m->cfg_use_numa)\n\t  .add_option_int(\"-ll:pystack\", m->cfg_stack_size_in_mb)\n\t  .add_option_stringlist(\"-ll:pyimport\", m->cfg_import_modules)\n\t  .add_option_stringlist(\"-ll:pyinit\", m->cfg_init_scripts);\n\n        bool ok = cp.parse_command_line(cmdline);\n        if(!ok) {\n          log_py.fatal() << \"error reading Python command line parameters\";\n          assert(false);\n        }\n      }\n\n      // add extra module imports requested by the application\n      m->cfg_import_modules.insert(m->cfg_import_modules.end(),\n                                   extra_import_modules.begin(),\n                                   extra_import_modules.end());\n\n      // if no cpus were requested, there's no point\n      if(m->cfg_num_python_cpus == 0) {\n        log_py.debug() << \"no Python cpus requested\";\n        delete m;\n        return 0;\n      }\n\n#ifndef REALM_USE_DLMOPEN\n      // Multiple CPUs are only allowed if we're using dlmopen.\n      if(m->cfg_num_python_cpus > 1) {\n        log_py.fatal() << \"support for multiple Python CPUs is not available: recompile with USE_DLMOPEN\";\n        assert(false);\n      }\n#endif\n\n      // get number/sizes of NUMA nodes -\n      //   disable (with a warning) numa binding if support not found\n      if(m->cfg_use_numa) {\n        std::map<int, NumaNodeCpuInfo> cpuinfo;\n        if(numasysif_numa_available() &&\n           numasysif_get_cpu_info(cpuinfo) &&\n           !cpuinfo.empty()) {\n          // filter out any numa domains with insufficient core counts\n          int cores_needed = m->cfg_num_python_cpus;\n          for(std::map<int, NumaNodeCpuInfo>::const_iterator it = cpuinfo.begin();\n              it != cpuinfo.end();\n              ++it) {\n            const NumaNodeCpuInfo& ci = it->second;\n            if(ci.cores_available >= cores_needed) {\n              m->active_numa_domains.insert(ci.node_id);\n            } else {\n              log_py.warning() << \"not enough cores in NUMA domain \" << ci.node_id << \" (\" << ci.cores_available << \" < \" << cores_needed << \")\";\n            }\n          }\n        } else {\n          log_py.warning() << \"numa support not found (or not working)\";\n          m->cfg_use_numa = false;\n        }\n      }\n\n      // if we don't end up with any active numa domains,\n      //  use NUMA_DOMAIN_DONTCARE\n      // actually, use the value (-1) since it seems to cause link errors!?\n      if(m->active_numa_domains.empty())\n        m->active_numa_domains.insert(-1 /*CoreReservationParameters::NUMA_DOMAIN_DONTCARE*/);\n\n      return m;\n    }\n\n    // do any general initialization - this is called after all configuration is\n    //  complete\n    void PythonModule::initialize(RuntimeImpl *runtime)\n    {\n      Module::initialize(runtime);\n    }\n\n    // create any processors provided by the module (default == do nothing)\n    //  (each new ProcessorImpl should use a Processor from\n    //   RuntimeImpl::next_local_processor_id)\n    void PythonModule::create_processors(RuntimeImpl *runtime)\n    {\n      Module::create_processors(runtime);\n\n      for(std::set<int>::const_iterator it = active_numa_domains.begin();\n          it != active_numa_domains.end();\n          ++it) {\n        int cpu_node = *it;\n        for(int i = 0; i < cfg_num_python_cpus; i++) {\n          Processor p = runtime->next_local_processor_id();\n          ProcessorImpl *pi = new LocalPythonProcessor(p, cpu_node,\n                                                       runtime->core_reservation_set(),\n                                                       cfg_stack_size_in_mb << 20,\n\t\t\t\t\t\t       cfg_import_modules,\n\t\t\t\t\t\t       cfg_init_scripts);\n          runtime->add_processor(pi);\n\n          // create affinities between this processor and system/reg memories\n          // if the memory is one we created, use the kernel-reported distance\n          // to adjust the answer\n          std::vector<MemoryImpl *>& local_mems = runtime->nodes[my_node_id].memories;\n          for(std::vector<MemoryImpl *>::iterator it2 = local_mems.begin();\n              it2 != local_mems.end();\n              ++it2) {\n            Memory::Kind kind = (*it2)->get_kind();\n            if((kind != Memory::SYSTEM_MEM) && (kind != Memory::REGDMA_MEM))\n              continue;\n\n            Machine::ProcessorMemoryAffinity pma;\n            pma.p = p;\n            pma.m = (*it2)->me;\n\n            // use the same made-up numbers as in\n            //  runtime_impl.cc\n            if(kind == Memory::SYSTEM_MEM) {\n              pma.bandwidth = 100;  // \"large\"\n              pma.latency = 5;      // \"small\"\n            } else {\n              pma.bandwidth = 80;   // \"large\"\n              pma.latency = 10;     // \"small\"\n            }\n\n            runtime->add_proc_mem_affinity(pma);\n          }\n        }\n      }\n    }\n\n    // clean up any common resources created by the module - this will be called\n    //  after all memories/processors/etc. have been shut down and destroyed\n    void PythonModule::cleanup(void)\n    {\n      Module::cleanup();\n    }\n\n  }; // namespace Python\n\n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/language/src/regent/cudahelper.t": "-- Copyright 2017 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal config = require(\"regent/config\")\nlocal report = require(\"common/report\")\n\nlocal cudahelper = {}\ncudahelper.check_cuda_available = function() return false end\n\nif not config.args()[\"cuda\"] or not terralib.cudacompile then\n  return cudahelper\nend\n\n-- copied and modified from cudalib.lua in Terra interpreter\n\nlocal ffi = require('ffi')\n\nlocal cudapaths = { OSX = \"/usr/local/cuda/lib/libcuda.dylib\";\n                    Linux =  \"libcuda.so\";\n                    Windows = \"nvcuda.dll\"; }\n\nlocal cudaruntimelinked = false\nfunction cudahelper.link_driver_library()\n    if cudaruntimelinked then return end\n    local path = assert(cudapaths[ffi.os],\"unknown OS?\")\n    terralib.linklibrary(path)\n    cudaruntimelinked = true\nend\n\n--\n\nlocal ef = terralib.externfunction\nlocal externcall_builtin = terralib.externfunction\n\nlocal RuntimeAPI = terralib.includec(\"cuda_runtime.h\")\nlocal HijackAPI = terralib.includec(\"legion_terra_cudart_hijack.h\")\n\nlocal C = terralib.includecstring [[\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n]]\n\nlocal struct CUctx_st\nlocal struct CUmod_st\nlocal struct CUlinkState_st\nlocal struct CUfunc_st\nlocal CUdevice = int32\nlocal CUjit_option = uint32\nlocal CU_JIT_ERROR_LOG_BUFFER = 5\nlocal CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES = 6\nlocal CU_JIT_INPUT_PTX = 1\nlocal CU_JIT_TARGET = 9\nlocal DriverAPI = {\n  cuInit = ef(\"cuInit\", {uint32} -> uint32);\n  cuCtxGetCurrent = ef(\"cuCtxGetCurrent\", {&&CUctx_st} -> uint32);\n  cuCtxGetDevice = ef(\"cuCtxGetDevice\",{&int32} -> uint32);\n  cuDeviceGet = ef(\"cuDeviceGet\",{&int32,int32} -> uint32);\n  cuCtxCreate_v2 = ef(\"cuCtxCreate_v2\",{&&CUctx_st,uint32,int32} -> uint32);\n  cuDeviceComputeCapability = ef(\"cuDeviceComputeCapability\",\n    {&int32,&int32,int32} -> uint32);\n  cuLinkCreate_v2 = ef(\"cuLinkCreate_v2\",\n    {uint32,&uint32,&&opaque,&&CUlinkState_st} -> uint32);\n  cuLinkAddData_v2 = ef(\"cuLinkAddData_v2\",\n    {&CUlinkState_st,uint32,&opaque,uint64,&int8,uint32,&uint32,&&opaque} -> uint32);\n  cuLinkComplete = ef(\"cuLinkComplete\",\n    {&CUlinkState_st,&&opaque,&uint64} -> uint32);\n  cuLinkDestroy = ef(\"cuLinkDestroy\", {&CUlinkState_st} -> uint32);\n}\n\nlocal dlfcn = terralib.includec(\"dlfcn.h\")\nlocal terra has_symbol(symbol : rawstring)\n  var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n  var has_symbol = dlfcn.dlsym(lib, symbol) ~= [&opaque](0)\n  dlfcn.dlclose(lib)\n  return has_symbol\nend\n\ndo\n  if has_symbol(\"cuInit\") then\n    local r = DriverAPI.cuInit(0)\n    assert(r == 0)\n    terra cudahelper.check_cuda_available()\n      return [r] == 0;\n    end\n  else\n    terra cudahelper.check_cuda_available()\n      return false\n    end\n  end\nend\n\n-- copied and modified from cudalib.lua in Terra interpreter\n\nlocal c = terralib.includec(\"unistd.h\")\n\nlocal terra assert(x : bool, message : rawstring)\n  if not x then\n    var stderr = C.fdopen(2, \"w\")\n    C.fprintf(stderr, \"assertion failed: %s\\n\", message)\n    -- Just because it's stderr doesn't mean it's unbuffered...\n    C.fflush(stderr)\n    C.abort()\n  end\nend\n\nlocal terra init_cuda() : int32\n  var cx : &CUctx_st\n  var r = DriverAPI.cuCtxGetCurrent(&cx)\n  assert(r == 0, \"CUDA error in cuCtxGetCurrent\")\n  var d : int32\n  if cx ~= nil then\n    r = DriverAPI.cuCtxGetDevice(&d)\n    assert(r == 0, \"CUDA error in cuCtxGetDevice\")\n  else\n    r = DriverAPI.cuDeviceGet(&d, 0)\n    assert(r == 0, \"CUDA error in cuDeviceGet\")\n    r = DriverAPI.cuCtxCreate_v2(&cx, 0, d)\n    assert(r == 0, \"CUDA error in cuCtxCreate_v2\")\n  end\n\n  return d\nend\n\nlocal terra get_cuda_version(device : int) : uint64\n  var major : int, minor : int\n  var r = DriverAPI.cuDeviceComputeCapability(&major, &minor, device)\n  assert(r == 0, \"CUDA error in cuDeviceComputeCapability\")\n  return [uint64](major * 10 + minor)\nend\n\n--\n\nstruct fat_bin_t {\n  magic : int,\n  versions : int,\n  data : &opaque,\n  filename : &opaque,\n}\n\nlocal terra register_ptx(ptxc : rawstring, ptxSize : uint32, version : uint64) : &&opaque\n  var fat_bin : &fat_bin_t\n  -- TODO: this line is leaking memory\n  fat_bin = [&fat_bin_t](C.malloc(sizeof(fat_bin_t)))\n  fat_bin.magic = 1234\n  fat_bin.versions = 5678\n  fat_bin.data = C.malloc(ptxSize + 1)\n  fat_bin.data = ptxc\n  var handle = HijackAPI.hijackCudaRegisterFatBinary(fat_bin)\n  return handle\nend\n\nlocal terra register_function(handle : &&opaque, id : int, name : &int8)\n  HijackAPI.hijackCudaRegisterFunction(handle, [&int8](id), name)\nend\n\nlocal function find_device_library(target)\n  local device_lib_dir = terralib.cudahome .. \"/nvvm/libdevice/\"\n  local libdevice = nil\n  for f in io.popen(\"ls \" .. device_lib_dir):lines() do\n    local version = tonumber(string.match(string.match(f, \"[0-9][0-9][.]\"), \"[0-9][0-9]\"))\n    if version <= target then\n      libdevice = device_lib_dir .. f\n    end\n  end\n  assert(libdevice ~= nil, \"Failed to find a device library\")\n  return libdevice\nend\n\nfunction cudahelper.jit_compile_kernels_and_register(kernels)\n  local module = {}\n  for k, v in pairs(kernels) do\n    module[v.name] = v.kernel\n  end\n  local device = init_cuda()\n  local version = get_cuda_version(device)\n  local libdevice = find_device_library(tonumber(version))\n  local llvmbc = terralib.linkllvm(libdevice)\n  externcall_builtin = function(name, ftype)\n    return llvmbc:extern(name, ftype)\n  end\n  local ptx = cudalib.toptx(module, nil, version)\n\n  local ptxc = terralib.constant(ptx)\n  local handle = terralib.newsymbol(&&opaque, \"handle\")\n  local register = quote\n    var [handle] = register_ptx(ptxc, [ptx:len() + 1], [version])\n  end\n\n  for k, v in pairs(kernels) do\n    register = quote\n      [register]\n      register_function([handle], [k], [v.name])\n    end\n  end\n\n  return register\nend\n\nfunction cudahelper.codegen_kernel_call(kernel_id, counts, args)\n  local setupArguments = terralib.newlist()\n\n  local offset = 0\n  for i = 1, #args do\n    local arg =  args[i]\n    local size = terralib.sizeof(arg.type)\n    setupArguments:insert(quote\n      RuntimeAPI.cudaSetupArgument(&[arg], size, offset)\n    end)\n    offset = offset + size\n  end\n\n  local grid = terralib.newsymbol(RuntimeAPI.dim3, \"grid\")\n  local block = terralib.newsymbol(RuntimeAPI.dim3, \"block\")\n  local launch_domain_init\n\n  local function round_exp(v, n)\n    return `((v + (n - 1)) / n)\n  end\n\n  -- TODO: Make this handle different thread block sizes and access strides\n  if #counts == 1 then\n    local threadSizeX = 128\n    launch_domain_init = quote\n      [grid].x, [grid].y, [grid].z =\n        [round_exp(counts[1], threadSizeX)], 1, 1\n      [block].x, [block].y, [block].z =\n        threadSizeX, 1, 1\n    end\n  elseif #counts == 2 then\n    local threadSizeX = 16\n    local threadSizeY = 16\n    launch_domain_init = quote\n      [grid].x, [grid].y, [grid].z =\n        [round_exp(counts[1], threadSizeX)],\n        [round_exp(counts[2], threadSizeY)], 1\n      [block].x, [block].y, [block].z =\n        [threadSizeX], [threadSizeY], 1\n    end\n  elseif #counts == 3 then\n    local threadSizeX = 16\n    local threadSizeY = 8\n    local threadSizeZ = 2\n    launch_domain_init = quote\n      [grid].x, [grid].y, [grid].z =\n        [round_exp(counts[1], threadSizeX)],\n        [round_exp(counts[2], threadSizeY)],\n        [round_exp(counts[3], threadSizeZ)]\n      [block].x, [block].y, [block].z =\n        [threadSizeX], [threadSizeY], [threadSizeZ]\n    end\n  else\n    assert(false, \"Indexspaces more than 3 dimensions are not supported\")\n  end\n\n  return quote\n    var [grid], [block]\n    [launch_domain_init]\n    RuntimeAPI.cudaConfigureCall([grid], [block], 0, nil)\n    [setupArguments]\n    RuntimeAPI.cudaLaunch([&int8](kernel_id))\n  end\nend\n\nlocal builtin_gpu_fns = {\n  acos  = externcall_builtin(\"__nv_acos\"  , double -> double),\n  asin  = externcall_builtin(\"__nv_asin\"  , double -> double),\n  atan  = externcall_builtin(\"__nv_atan\"  , double -> double),\n  cbrt  = externcall_builtin(\"__nv_cbrt\"  , double -> double),\n  ceil  = externcall_builtin(\"__nv_ceil\"  , double -> double),\n  cos   = externcall_builtin(\"__nv_cos\"   , double -> double),\n  fabs  = externcall_builtin(\"__nv_fabs\"  , double -> double),\n  floor = externcall_builtin(\"__nv_floor\" , double -> double),\n  fmod  = externcall_builtin(\"__nv_fmod\"  , {double, double} -> double),\n  log   = externcall_builtin(\"__nv_log\"   , double -> double),\n  pow   = externcall_builtin(\"__nv_pow\"   , {double, double} -> double),\n  sin   = externcall_builtin(\"__nv_sin\"   , double -> double),\n  sqrt  = externcall_builtin(\"__nv_sqrt\"  , double -> double),\n  tan   = externcall_builtin(\"__nv_tan\"   , double -> double),\n}\n\nlocal cpu_fn_to_gpu_fn = {}\n\nfunction cudahelper.register_builtin(name, cpu_fn)\n  cpu_fn_to_gpu_fn[cpu_fn] = builtin_gpu_fns[name]\nend\n\nfunction cudahelper.replace_with_builtin(fn)\n  return cpu_fn_to_gpu_fn[fn] or fn\nend\n\nreturn cudahelper\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/language/src/regent/openmphelper.t": "-- Copyright 2017 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal std = require(\"regent/std\")\n\nlocal omp = {}\n\nlocal has_openmp = false\ndo\n  local dlfcn = terralib.includec(\"dlfcn.h\")\n  local terra find_openmp_symbols()\n    var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n    var has_openmp =\n      dlfcn.dlsym(lib, \"GOMP_parallel\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_num_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_max_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_thread_num\") ~= [&opaque](0)\n    dlfcn.dlclose(lib)\n    return has_openmp\n  end\n  has_openmp = find_openmp_symbols()\nend\n\nif not (std.config[\"openmp\"] and has_openmp) then\n  omp.check_openmp_available = function() return false end\n  terra omp.get_num_threads() return 1 end\n  terra omp.get_max_threads() return 1 end\n  terra omp.get_thread_num() return 0 end\n  local omp_worker_type =\n    terralib.types.functype(terralib.newlist({&opaque}), terralib.types.unit, false)\n  terra omp.launch(fnptr : &omp_worker_type, data : &opaque, nthreads : int32, flags : uint32)\n    fnptr(data)\n  end\nelse\n  omp.check_openmp_available = function() return true end\n  local omp_abi = terralib.includecstring [[\n    extern int omp_get_num_threads(void);\n    extern int omp_get_max_threads(void);\n    extern int omp_get_thread_num(void);\n    extern void GOMP_parallel(void (*fnptr)(void *data), void *data, int nthreads, unsigned flags);\n  ]]\n\n  omp.get_num_threads = omp_abi.omp_get_num_threads\n  omp.get_max_threads = omp_abi.omp_get_max_threads\n  omp.get_thread_num = omp_abi.omp_get_thread_num\n  omp.launch = omp_abi.GOMP_parallel\nend\n\n-- TODO: This might not be the right size in platforms other than x86\nomp.CACHE_LINE_SIZE = 64\n\nfunction omp.generate_preamble_structured(rect, idx, start_idx, end_idx)\n  return quote\n    var num_threads = [omp.get_num_threads]()\n    var thread_id = [omp.get_thread_num]()\n    var lo = [rect].lo.x[idx]\n    var hi = [rect].hi.x[idx] + 1\n    var chunk = (hi - lo + num_threads - 1) / num_threads\n    if chunk == 0 then chunk = 1 end\n    var [start_idx] = thread_id * chunk + lo\n    var [end_idx] = (thread_id + 1) * chunk + lo\n    if [end_idx] > hi then [end_idx] = hi end\n  end\nend\n\nfunction omp.generate_argument_type(symbols, reductions)\n  local arg_type = terralib.types.newstruct(\"omp_worker_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(symbols) do\n    local field_name\n    if reductions[symbol] == nil then\n      field_name = \"_arg\" .. tostring(i)\n      arg_type.entries:insert({ field_name, symbol.type })\n    else\n      field_name = \"_red\" .. tostring(i)\n      arg_type.entries:insert({ field_name, &symbol.type })\n    end\n    mapping[field_name] = symbol\n  end\n  return arg_type, mapping\nend\n\nfunction omp.generate_argument_init(arg, arg_type, mapping, reductions)\n  local worker_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      return quote var [symbol] = [init] end\n    else\n      return quote var [symbol] = [arg].[field_name] end\n    end\n  end)\n  local launch_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      assert(field_type:ispointer())\n      return quote\n        -- We don't like false sharing\n        [arg].[field_name] =\n          [field_type](std.c.malloc([omp.get_max_threads]()  * omp.CACHE_LINE_SIZE))\n      end\n    else\n      return quote [arg].[field_name] = [symbol] end\n    end\n  end)\n  return worker_init, launch_init\nend\n\nfunction omp.generate_worker_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      return quote\n        do\n          var idx = [omp.get_thread_num]() * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [arg].[field_name][idx] = [symbol]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nfunction omp.generate_launcher_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        for i = 0, [omp.get_max_threads]() do\n          var idx = i * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [symbol] = [std.quote_binary_op(op, symbol, `([arg].[field_name][idx]))]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nreturn omp\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/language/examples/mssp/small/edges.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/language/examples/mssp/small/result_3.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/doc/arch/persistent/hdf5/figs/high-level-design.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-17.10.0-bdsjurc4y7bxcvilqi3qahqvuptrhcx4/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout.pptx"
    ],
    "total_files": 1491
}