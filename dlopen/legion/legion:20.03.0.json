{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/bindings/python/legion_cffi.py.in": "#!/usr/bin/env python\n\n# Copyright 2020 Stanford University\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# IMPORTANT:\n#   * legion_cffi.py.in is used as an input to string.format()\n#   * legion_cffi.py is a generated file and should not be modified by hand\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport cffi\n\nheader = {header}\n\nffi = cffi.FFI()\nffi.cdef(header)\nlib = ffi.dlopen(None)\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/runtime/realm/codedesc.cc": "/* Copyright 2020 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// constructs for describing code blobs to Realm\n\n#include \"realm/codedesc.h\"\n\n#include <dlfcn.h>\n\n#include \"realm/logging.h\"\n#include \"realm/utils.h\"\n\nnamespace Realm {\n\n  Logger log_codetrans(\"codetrans\");\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Type\n\n  std::ostream& operator<<(std::ostream& os, const Type& t)\n  {\n    switch(t.f_common.kind) {\n    case Type::InvalidKind: os << \"INVALIDTYPE\"; break;\n    case Type::OpaqueKind:\n      {\n\tif(t.size_bits() == 0)\n\t  os << \"void\";\n\telse\n\t  os << \"opaque(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::IntegerKind:\n      {\n\tos << (t.f_integer.is_signed ? 's' : 'u') << \"int(\" << t.size_bits() << \")\";\n\tbreak;\n      }\n    case Type::FloatingPointKind: os << \"float(\" << t.size_bits() << \")\"; break;\n    case Type::PointerKind:\n      {\n\tos << *t.f_pointer.base_type;\n\tif(t.f_pointer.is_const) os << \" const\";\n\tos << \" *\";\n\tbreak;\n      }\n    case Type::FunctionPointerKind:\n      {\n\tos << *t.f_funcptr.return_type << \"(*)(\";\n\tconst std::vector<Type>& p = *t.f_funcptr.param_types;\n\tif(p.size()) {\n\t  for(size_t i = 0; i < p.size(); i++) {\n\t    if(i) os << \", \";\n\t    os << p[i];\n\t  }\n\t} else\n\t  os << \"void\";\n\tos << \")\";\n\tbreak;\n      }\n    }\n    return os;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeDescriptor\n\n  CodeDescriptor::CodeDescriptor(void)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const Type& _t)\n    : m_type(_t)\n  {}\n\n  CodeDescriptor::CodeDescriptor(const CodeDescriptor& rhs)\n  {\n    copy_from(rhs);\n  }\n\n  CodeDescriptor& CodeDescriptor::operator=(const CodeDescriptor& rhs)\n  {\n    if(this != &rhs) {\n      clear();\n      copy_from(rhs);\n    }\n    return *this;\n  }\n\n  CodeDescriptor::~CodeDescriptor(void)\n  {\n    clear();\n  }\n\n  void CodeDescriptor::clear(void)\n  {\n    m_type = Type();\n    delete_container_contents(m_impls);\n    delete_container_contents(m_props);\n  }\n\n  void CodeDescriptor::copy_from(const CodeDescriptor& rhs)\n  {\n    m_type = rhs.m_type;\n    {\n      size_t s = rhs.m_impls.size();\n      m_impls.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_impls[i] = rhs.m_impls[i]->clone();\n    }\n    {\n      size_t s = rhs.m_props.size();\n      m_props.resize(s);\n      for(size_t i = 0; i < s; i++)\n\tm_props[i] = rhs.m_props[i]->clone();\n    }\n  }\n\n  // are any of the code implementations marked as \"portable\" (i.e.\n  //  usable in another process/address space)?\n  bool CodeDescriptor::has_portable_implementations(void) const\n  {\n    for(std::vector<CodeImplementation *>::const_iterator it = m_impls.begin();\n\tit != m_impls.end();\n\tit++)\n      if((*it)->is_portable())\n\treturn true;\n    return false;\n  }\n\n  // attempt to make a portable implementation from what we have\n  bool CodeDescriptor::create_portable_implementation(void)\n  {\n    // TODO: actually have translators registered where we can find them\n#if defined(REALM_USE_DLFCN) && defined(REALM_USE_DLADDR)\n    const FunctionPointerImplementation *fpi = find_impl<FunctionPointerImplementation>();\n    if(fpi) {\n      DSOReferenceImplementation *dsoref = DSOReferenceImplementation::cvt_fnptr_to_dsoref(fpi, true /*quiet*/);\n      if(dsoref) {\n\tm_impls.push_back(dsoref);\n\treturn true;\n      }\n    }\n#endif\n\n    return false;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class FunctionPointerImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      FunctionPointerImplementation> FunctionPointerImplementation::serdez_subclass;\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void)\n    : fnptr(0)\n  {}\n\n  FunctionPointerImplementation::FunctionPointerImplementation(void (*_fnptr)())\n    : fnptr(_fnptr)\n  {}\n\n  FunctionPointerImplementation::~FunctionPointerImplementation(void)\n  {}\n\n  CodeImplementation *FunctionPointerImplementation::clone(void) const\n  {\n    return new FunctionPointerImplementation(fnptr);\n  }\n\n  bool FunctionPointerImplementation::is_portable(void) const\n  {\n    return false;\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOReferenceImplementation\n\n  /*static*/ Serialization::PolymorphicSerdezSubclass<CodeImplementation,\n\t\t\t\t\t\t      DSOReferenceImplementation> DSOReferenceImplementation::serdez_subclass;\n\n  DSOReferenceImplementation::DSOReferenceImplementation(void)\n  {}\n\n  DSOReferenceImplementation::DSOReferenceImplementation(const std::string& _dso_name,\n\t\t\t\t\t\t\t const std::string& _symbol_name)\n    : dso_name(_dso_name), symbol_name(_symbol_name)\n  {}\n\n  DSOReferenceImplementation::~DSOReferenceImplementation(void)\n  {}\n\n  CodeImplementation *DSOReferenceImplementation::clone(void) const\n  {\n    return new DSOReferenceImplementation(dso_name, symbol_name);\n  }\n\n  bool DSOReferenceImplementation::is_portable(void) const\n  {\n    return true;\n  }\n\n#ifdef REALM_USE_DLADDR\n  namespace {\n    extern \"C\" { int main(int argc, const char *argv[]) __attribute__((weak)); };\n\n    DSOReferenceImplementation *dladdr_helper(void *ptr, bool quiet)\n    {\n      // if dladdr() gives us something with the same base pointer, assume that's portable\n      // note: return code is not-POSIX-y (i.e. 0 == failure)\n      Dl_info inf;\n      int ret = dladdr(ptr, &inf);\n      if(ret == 0) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"couldn't map fnptr \" << ptr << \" to a dynamic symbol\";\n\treturn 0;\n      }\n\n      if(inf.dli_saddr != ptr) {\n\tif(!quiet)\n\t  log_codetrans.warning() << \"pointer \" << ptr << \" in middle of symbol '\" << inf.dli_sname << \" (\" << inf.dli_saddr << \")?\";\n\treturn 0;\n      }\n\n      // try to detect symbols that are in the base executable and change the filename to \"\"\n      // only do this if the weak 'main' reference found an actual main\n      if(((void *)main) != 0) {\n\tconst char *fname = inf.dli_fname;\n\t{\n\t  static std::string local_fname;\n\t  if(local_fname.empty()) {\n\t    Dl_info inf2;\n\t    ret = dladdr((void *)main, &inf2);\n\t    assert(ret != 0);\n\t    local_fname = inf2.dli_fname;\n\t  }\n\t  if(local_fname.compare(fname) == 0)\n\t    fname = \"\";\n\t}\n\n\treturn new DSOReferenceImplementation(fname, inf.dli_sname);\n      }\n\n      return 0;\n    }\n  };\n\n  /*static*/ DSOReferenceImplementation *DSOReferenceImplementation::cvt_fnptr_to_dsoref(const FunctionPointerImplementation *fpi,\n\t\t\t\t\t\t\t\t\t\t\t bool quiet /*= false*/)\n  {\n    return dladdr_helper((void *)(fpi->fnptr), quiet);\n  } \n#endif\n#endif\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class CodeTranslator\n\n  CodeTranslator::CodeTranslator(const std::string& _name)\n    : name(_name)\n  {}\n\n  CodeTranslator::~CodeTranslator(void)\n  {}\n\n  // default version just iterates over all the implementations in the source\n  bool CodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t     const std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn true;\n    }\n\n    return false;\n  }\n\n  // default version just iterates over all the implementations in the source\n  CodeImplementation *CodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    const std::vector<CodeImplementation *>& impls = source_codedesc.implementations();\n    for(std::vector<CodeImplementation *>::const_iterator it = impls.begin();\n\tit != impls.end();\n\tit++) {\n      CodeImplementation &impl = **it;\n      if(can_translate(typeid(impl), target_impl_type))\n\treturn translate(*it, target_impl_type);\n    }\n\n    return 0;\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class DSOCodeTranslator\n\n#ifdef REALM_USE_DLFCN\n  DSOCodeTranslator::DSOCodeTranslator(void)\n    : CodeTranslator(\"dso\")\n  {}\n\n  DSOCodeTranslator::~DSOCodeTranslator(void)\n  {\n    // unload any modules we have loaded\n    for(std::map<std::string, void *>::iterator it = modules_loaded.begin();\n\tit != modules_loaded.end();\n\tit++) {\n      int ret = dlclose(it->second);\n      if(ret != 0)\n\tlog_codetrans.warning() << \"error on dlclose of '\" << it->first << \"': \" << dlerror();\n    }\n  }\n\n  bool DSOCodeTranslator::can_translate(const std::type_info& source_impl_type,\n\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    // DSO ref -> function pointer\n    if((source_impl_type == typeid(DSOReferenceImplementation)) &&\n       (target_impl_type == typeid(FunctionPointerImplementation)))\n      return true;\n\n#ifdef REALM_USE_DLADDR\n    if((source_impl_type == typeid(FunctionPointerImplementation)) &&\n       (target_impl_type == typeid(DSOReferenceImplementation)))\n      return true;\n#endif\n\n      return false;\n    }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeImplementation *source,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    if(target_impl_type == typeid(FunctionPointerImplementation)) {\n      const DSOReferenceImplementation *dsoref = dynamic_cast<const DSOReferenceImplementation *>(source);\n      assert(dsoref != 0);\n\n      void *handle = 0;\n      // check to see if we've already loaded the module?\n      std::map<std::string, void *>::iterator it = modules_loaded.find(dsoref->dso_name);\n      if(it != modules_loaded.end()) {\n\thandle = it->second;\n      } else {\n\t// try to load it - empty string for dso_name means the main executable\n\tconst char *dso_name = dsoref->dso_name.c_str();\n\thandle = dlopen(*dso_name ? dso_name : 0, RTLD_NOW | RTLD_LOCAL);\n\tif(!handle) {\n\t  log_codetrans.warning() << \"could not open DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\t  return 0;\n\t}\n\tmodules_loaded[dsoref->dso_name] = handle;\n      }\n\n      void *ptr = dlsym(handle, dsoref->symbol_name.c_str());\n      if(!ptr) {\n\tlog_codetrans.warning() << \"could not find symbol '\" << dsoref->symbol_name << \"' in  DSO '\" << dsoref->dso_name << \"': \" << dlerror();\n\treturn 0;\n      }\n\n      return new FunctionPointerImplementation((void(*)())ptr);\n    }\n\n#ifdef REALM_USE_DLADDR\n    if(target_impl_type == typeid(DSOReferenceImplementation)) {\n      const FunctionPointerImplementation *fpi = dynamic_cast<const FunctionPointerImplementation *>(source);\n      assert(fpi != 0);\n\n      return dladdr_helper((void *)(fpi->fnptr), false /*!quiet*/);\n    }\n#endif\n\n    return 0;\n  }\n\n  // these pass through to CodeTranslator's definitions\n  bool DSOCodeTranslator::can_translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\tconst std::type_info& target_impl_type)\n  {\n    return CodeTranslator::can_translate(source_codedesc, target_impl_type);\n  }\n\n  CodeImplementation *DSOCodeTranslator::translate(const CodeDescriptor& source_codedesc,\n\t\t\t\t\t\t   const std::type_info& target_impl_type)\n  {\n    return CodeTranslator::translate(source_codedesc, target_impl_type);\n  }\n#endif\n\n\n};\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/runtime/realm/module.cc": "/* Copyright 2020 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Realm modules\n\n#include \"realm/realm_config.h\"\n\n#define REALM_MODULE_REGISTRATION_STATIC\n#include \"realm/module.h\"\n\n#include \"realm/logging.h\"\n\n#include <assert.h>\n#include <string.h>\n#include <stdlib.h>\n\n#ifdef REALM_USE_DLFCN\n#include <dlfcn.h>\n#endif\n\n// TODO: replace this with Makefile (or maybe cmake) magic that adapts automatically\n//  to the build-system-controlled list of statically-linked Realm modules\n#include \"realm/runtime_impl.h\"\n#include \"realm/numa/numa_module.h\"\n#ifdef REALM_USE_OPENMP\n#include \"realm/openmp/openmp_module.h\"\n#endif\n#include \"realm/procset/procset_module.h\"\n#ifdef REALM_USE_PYTHON\n#include \"realm/python/python_module.h\"\n#endif\n#ifdef REALM_USE_CUDA\n#include \"realm/cuda/cuda_module.h\"\n#endif\n#ifdef REALM_USE_LLVM\n#include \"realm/llvmjit/llvmjit_module.h\"\n#endif\n#ifdef REALM_USE_HDF5\n#include \"realm/hdf5/hdf5_module.h\"\n#endif\n#ifdef REALM_USE_GASNET1\n#include \"realm/gasnet1/gasnet1_module.h\"\n#endif\n#if defined REALM_USE_MPI\n#include \"realm/mpi/mpi_module.h\"\n#endif\n\nnamespace Realm {\n\n  Logger log_module(\"module\");\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class Module\n  //\n\n  Module::Module(const std::string& _name)\n    : name(_name)\n  {\n    log_module.debug() << \"module \" << name << \" created\";\n  }\n\n  Module::~Module(void)\n  {\n    log_module.debug() << \"module \" << name << \" destroyed\";\n  }\n\n  const std::string& Module::get_name(void) const\n  {\n    return name;\n  }\n\n  void Module::initialize(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" initialize\";\n  }\n\n  void Module::create_memories(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_memories\";\n  }\n\n  void Module::create_processors(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_processors\";\n  }\n  \n  void Module::create_dma_channels(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_dma_channels\";\n  }\n  \n  void Module::create_code_translators(RuntimeImpl *runtime)\n  {\n    log_module.debug() << \"module \" << name << \" create_code_translators\";\n  }\n\n  void Module::cleanup(void)\n  {\n    log_module.debug() << \"module \" << name << \" cleanup\";\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class ModuleRegistrar\n  //\n\n  namespace {\n    ModuleRegistrar::StaticRegistrationBase *static_modules_head = 0;\n    ModuleRegistrar::StaticRegistrationBase **static_modules_tail = &static_modules_head;\n  };\n\n  ModuleRegistrar::ModuleRegistrar(RuntimeImpl *_runtime)\n    : runtime(_runtime)\n  {}\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_static_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t      std::vector<Module *>& modules)\n  {\n    // just iterate over the static module list, trying to create each module\n    for(const StaticRegistrationBase *sreg = static_modules_head;\n\tsreg;\n\tsreg = sreg->next) {\n      Module *m = sreg->create_module(runtime, cmdline);\n      if(m)\n\tmodules.push_back(m);\n    }\n  }\n\n\n#ifdef REALM_USE_DLFCN\n  // accepts a colon-separated list of so files to try to load\n  static void load_module_list(const char *sonames,\n\t\t\t       RuntimeImpl *runtime,\n\t\t\t       std::vector<std::string>& cmdline,\n\t\t\t       std::vector<void *>& handles,\n\t\t\t       std::vector<Module *>& modules)\n  {\n    // null/empty strings are nops\n    if(!sonames || !*sonames) return;\n\n    const char *p1 = sonames;\n    while(true) {\n      // skip leading colons\n      while(*p1 == ':') p1++;\n      if(!*p1) break;\n\n      const char *p2 = p1 + 1;\n      while(*p2 && (*p2 != ':')) p2++;\n\n      char filename[1024];\n      strncpy(filename, p1, p2 - p1);\n\n      // no leftover errors from anybody else please...\n      assert(dlerror() == 0);\n\n      // open so file, resolving all symbols but not polluting global namespace\n      void *handle = dlopen(filename, RTLD_NOW | RTLD_LOCAL);\n\n      if(handle != 0) {\n\t// this file should have a \"create_realm_module\" symbol\n\tvoid *sym = dlsym(handle, \"create_realm_module\");\n\n\tif(sym != 0) {\n\t  // TODO: hold onto the handle even if it doesn't create a module?\n\t  handles.push_back(handle);\n\n\t  Module *m = ((Module *(*)(RuntimeImpl *, std::vector<std::string>&))dlsym)(runtime, cmdline);\n\t  if(m)\n\t    modules.push_back(m);\n\t} else {\n\t  log_module.error() << \"symbol 'create_realm_module' not found in \" << filename;\n#ifndef NDEBUG\n\t  int ret =\n#endif\n\t    dlclose(handle);\n\t  assert(ret == 0);\n\t}\n      } else {\n\tlog_module.error() << \"could not load \" << filename << \": \" << dlerror();\n      }\n\n      if(!*p2) break;\n      p1 = p2 + 1;\n    }\n  }\n#endif\n\n  // called by the runtime during init\n  void ModuleRegistrar::create_dynamic_modules(std::vector<std::string>& cmdline,\n\t\t\t\t\t       std::vector<Module *>& modules)\n  {\n    // dynamic modules are requested in one of two ways:\n    // 1) REALM_DYNAMIC_MODULES=sonames environment variable\n    // 2) \"-ll:module sonames\" on command line\n    // in both cases, 'sonames' is a colon-separate listed of .so files that should be\n\n    // loading modules can also monkey with the cmdline, so do a pass first where we pull\n    //  out all the name we want to load\n    std::vector<std::string> sonames_list;\n\n    {\n      const char *e = getenv(\"REALM_DYNAMIC_MODULES\");\n      if(e)\n\tsonames_list.push_back(std::string(e));\n    }\n\n    {\n      std::vector<std::string>::iterator it = cmdline.begin();\n      while(it != cmdline.end()) {\n\tif(*it != \"-ll:module\") {\n\t  it++;\n\t  continue;\n\t}\n\n\t// eat this argument and move the next one to sonames_list\n\tit = cmdline.erase(it);\n\tassert(it != cmdline.end());\n\tsonames_list.push_back(*it);\n\tit = cmdline.erase(it);\n      }\n    }\n\n#ifdef REALM_USE_DLFCN\n    for(std::vector<std::string>::const_iterator it = sonames_list.begin();\n\tit != sonames_list.end();\n\tit++)\n      load_module_list(it->c_str(),\n\t\t       runtime, cmdline, sofile_handles, modules);\n#else\n    if(!sonames_list.empty()) {\n      log_module.error() << \"loading of dynamic Realm modules requested, but REALM_USE_DLFCN=0!\";\n      exit(1);\n    }\n#endif\n  }\n\n  // called by runtime after all modules have been cleaned up\n  void ModuleRegistrar::unload_module_sofiles(void)\n  {\n#ifdef REALM_USE_DLFCN\n    while(!sofile_handles.empty()) {\n      void *handle = sofile_handles.back();\n      sofile_handles.pop_back();\n\n#ifndef NDEBUG\n      int ret =\n#endif\n\tdlclose(handle);\n      assert(ret == 0);\n    }\n#endif\n  }\n\n  // called by the module registration helpers\n  /*static*/ void ModuleRegistrar::add_static_registration(StaticRegistrationBase *reg)\n  {\n    // done during init, so single-threaded\n    *static_modules_tail = reg;\n    static_modules_tail = &(reg->next);\n  }\n  \n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/runtime/realm/python/python_module.cc": "/* Copyright 2020 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"realm/python/python_module.h\"\n#include \"realm/python/python_internal.h\"\n\n#include \"realm/numa/numasysif.h\"\n#include \"realm/logging.h\"\n#include \"realm/cmdline.h\"\n#include \"realm/proc_impl.h\"\n#include \"realm/threads.h\"\n#include \"realm/runtime_impl.h\"\n#include \"realm/utils.h\"\n\n#include <dlfcn.h>\n#ifdef REALM_USE_DLMOPEN\n#include <link.h>\n#endif // REALM_USE_DLMOPEN\n\n#include <list>\n\nnamespace Realm {\n\n  Logger log_py(\"python\");\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonAPI\n\n  PythonAPI::PythonAPI(void *_handle)\n    : handle(_handle)\n  {\n    get_symbol(this->Py_DecRef, \"Py_DecRef\");\n    get_symbol(this->Py_Finalize, \"Py_Finalize\");\n    get_symbol(this->Py_InitializeEx, \"Py_InitializeEx\");\n\n    get_symbol(this->PyByteArray_FromStringAndSize, \"PyByteArray_FromStringAndSize\");\n\n    get_symbol(this->PyEval_InitThreads, \"PyEval_InitThreads\");\n\n#ifdef USE_PYGILSTATE_CALLS\n    get_symbol(this->PyGILState_Ensure, \"PyGILState_Ensure\");\n    get_symbol(this->PyGILState_Release, \"PyGILState_Release\");\n#else\n    get_symbol(this->PyThreadState_New, \"PyThreadState_New\");\n    get_symbol(this->PyThreadState_Clear, \"PyThreadState_Clear\");\n    get_symbol(this->PyThreadState_Delete, \"PyThreadState_Delete\");\n    get_symbol(this->PyThreadState_Get, \"PyThreadState_Get\");\n#endif\n    get_symbol(this->PyEval_RestoreThread, \"PyEval_RestoreThread\");\n    get_symbol(this->PyEval_SaveThread, \"PyEval_SaveThread\");\n\n    get_symbol(this->PyThreadState_Swap, \"PyThreadState_Swap\");\n\n    get_symbol(this->PyErr_PrintEx, \"PyErr_PrintEx\");\n\n    get_symbol(this->PyImport_ImportModule, \"PyImport_ImportModule\");\n    get_symbol(this->PyModule_GetDict, \"PyModule_GetDict\");\n\n    get_symbol(this->PyLong_FromUnsignedLong, \"PyLong_FromUnsignedLong\");\n\n    get_symbol(this->PyObject_CallFunction, \"PyObject_CallFunction\");\n    get_symbol(this->PyObject_CallObject, \"PyObject_CallObject\");\n    get_symbol(this->PyObject_GetAttrString, \"PyObject_GetAttrString\");\n    get_symbol(this->PyObject_Print, \"PyObject_Print\");\n\n    get_symbol(this->PyRun_SimpleString, \"PyRun_SimpleString\");\n    get_symbol(this->PyRun_String, \"PyRun_String\");\n\n    get_symbol(this->PyTuple_New, \"PyTuple_New\");\n    get_symbol(this->PyTuple_SetItem, \"PyTuple_SetItem\");\n  }\n\n  template<typename T>\n  void PythonAPI::get_symbol(T &fn, const char *symbol,\n                             bool missing_ok /*= false*/)\n  {\n    fn = reinterpret_cast<T>(dlsym(handle, symbol));\n    if(!fn && !missing_ok) {\n      const char *error = dlerror();\n      log_py.fatal() << \"failed to find symbol '\" << symbol << \"': \" << error;\n      assert(false);\n    }\n  }\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonInterpreter\n\n#ifdef REALM_USE_DLMOPEN\n  // dlmproxy symbol lookups have to happen in a function we define so that\n  //  dl[v]sym searches in the right place\n  static void *dlmproxy_lookup(const char *symname, const char *symver)\n  {\n    \n    void *handle = 0;\n    void *sym = (symver ?\n\t\t   dlvsym(handle, symname, symver) :\n\t\t   dlsym(handle, symname));\n    if(sym)\n      log_py.debug() << \"found symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\") << \" ptr=\" << sym;\n    else\n      log_py.warning() << \"missing symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\");\n    return sym;\n  }\n#endif\n\n  PythonInterpreter::PythonInterpreter() \n  {\n#ifdef REALM_PYTHON_LIB\n    const char *python_lib = REALM_PYTHON_LIB;\n#else\n    const char *python_lib = \"libpython2.7.so\";\n#endif\n\n#ifdef REALM_USE_DLMOPEN\n    // loading libpython into its own namespace will cause it to try to bring\n    //   in a second copy of libpthread.so.0, which is fairly disastrous\n    // we deal with it by loading a \"dlmproxy\" of pthreads that tunnels all \n    //   pthreads calls back to the (only) version in the main executable\n    const char *dlmproxy_filename = getenv(\"DLMPROXY_LIBPTHREAD\");\n    if(!dlmproxy_filename)\n      dlmproxy_filename = \"dlmproxy_libpthread.so.0\";\n    dlmproxy_handle = dlmopen(LM_ID_NEWLM,\n\t\t\t      dlmproxy_filename,\n\t\t\t      RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_LAZY);\n    if(!dlmproxy_handle) {\n      const char *error = dlerror();\n      log_py.fatal() << \"HELP!  Use of dlmopen for python requires dlmproxy for pthreads!  Failed to\\n\"\n\t\t     << \"  load: \" << dlmproxy_filename << \"\\n\"\n\t\t     << \"  error: \" << error;\n      assert(false);\n    }\n\n    // now that the proxy is loaded, we need to tell it where the real\n    //  libpthreads functions are\n    {\n      void *sym = dlsym(dlmproxy_handle, \"dlmproxy_load_symbols\");\n      assert(sym != 0);\n      ((void (*)(void *(*)(const char *, const char *)))sym)(dlmproxy_lookup);\n    }\n\n    // now we can load libpython, but make sure we do it in the new namespace\n    Lmid_t lmid;\n    int ret = dlinfo(dlmproxy_handle, RTLD_DI_LMID, &lmid);\n    assert(ret == 0);\n\n    handle = dlmopen(lmid, python_lib, RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_NOW);\n#else\n    // life is so much easier if we use dlopen (but we only get one copy then)\n    handle = dlopen(python_lib, RTLD_GLOBAL | RTLD_LAZY);\n#endif\n    if (!handle) {\n      const char *error = dlerror();\n      log_py.fatal() << error;\n      assert(false);\n    }\n\n    api = new PythonAPI(handle);\n\n    (api->Py_InitializeEx)(0 /*!initsigs*/);\n    (api->PyEval_InitThreads)();\n    //(api->Py_Finalize)();\n\n    //PyThreadState *state;\n    //state = (api->PyEval_SaveThread)();\n    //(api->PyEval_RestoreThread)(state);\n\n    //(api->PyRun_SimpleString)(\"print 'hello Python world!'\");\n\n    //PythonSourceImplementation psi(\"taskreg_helper\", \"task1\");\n    //find_or_import_function(&psi);\n  }\n\n  PythonInterpreter::~PythonInterpreter()\n  {\n    (api->Py_Finalize)();\n\n    delete api;\n\n    if (dlclose(handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"libpython dlclose error: \" << error;\n      assert(false);\n    }\n\n#ifdef REALM_USE_DLMOPEN\n    if (dlclose(dlmproxy_handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"dlmproxy dlclose error: \" << error;\n      assert(false);\n    }\n#endif\n  }\n\n  PyObject *PythonInterpreter::find_or_import_function(const PythonSourceImplementation *psi)\n  {\n    //log_py.print() << \"attempting to acquire python lock\";\n    //(api->PyEval_AcquireLock)();\n    //log_py.print() << \"lock acquired\";\n\n    // not calling PythonInterpreter::import_module here because we want the\n    //  PyObject result\n    log_py.debug() << \"attempting to import module: \" << psi->module_name;\n    PyObject *module = (api->PyImport_ImportModule)(psi->module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(module, stdout, 0); printf(\"\\n\");\n\n    PyObject *function = module;\n    for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n           ie = psi->function_name.end(); function && it != ie; ++it) {\n      function = (api->PyObject_GetAttrString)(function, it->c_str());\n    }\n    if (!function) {\n      {\n        LoggerMessage m = log_py.fatal();\n        m << \"unable to import Python function \";\n        for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n               ie = psi->function_name.begin(); it != ie; ++it) {\n          m << *it;\n          if (it + 1 != ie) {\n            m << \".\";\n          }\n        }\n        m << \" from module \" << psi->module_name;\n      }\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(function, stdout, 0); printf(\"\\n\");\n\n    //(api->PyObject_CallFunction)(function, \"iii\", 1, 2, 3);\n\n    (api->Py_DecRef)(module);\n\n    return function;\n  }\n\n  void PythonInterpreter::import_module(const std::string& module_name)\n  {\n    log_py.debug() << \"attempting to import module: \" << module_name;\n    PyObject *module = (api->PyImport_ImportModule)(module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(module);\n  }\n\n  void PythonInterpreter::run_string(const std::string& script_text)\n  {\n    // from Python.h\n    const int Py_file_input = 257;\n\n    log_py.debug() << \"running python string: \" << script_text;\n    PyObject *mainmod = (api->PyImport_ImportModule)(\"__main__\");\n    assert(mainmod != 0);\n    PyObject *globals = (api->PyModule_GetDict)(mainmod);\n    assert(globals != 0);\n    PyObject *res = (api->PyRun_String)(script_text.c_str(),\n\t\t\t\t\tPy_file_input,\n\t\t\t\t\tglobals,\n\t\t\t\t\tglobals);\n    if(!res) {\n      log_py.fatal() << \"unable to run python string:\" << script_text;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(res);\n    (api->Py_DecRef)(globals);\n    (api->Py_DecRef)(mainmod);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonThreadTaskScheduler\n\n  PythonThreadTaskScheduler::PythonThreadTaskScheduler(LocalPythonProcessor *_pyproc,\n\t\t\t\t\t\t       CoreReservation& _core_rsrv)\n    : KernelThreadTaskScheduler(_pyproc->me, _core_rsrv)\n    , pyproc(_pyproc)\n    , interpreter_ready(false)\n  {}\n\n  // both real and internal tasks need to be wrapped with acquires of the GIL\n  bool PythonThreadTaskScheduler::execute_task(Task *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    bool ok = KernelThreadTaskScheduler::execute_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n\n    return ok;\n  }\n  \n  void PythonThreadTaskScheduler::execute_internal_task(InternalTask *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    KernelThreadTaskScheduler::execute_internal_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n  }\n    \n  void PythonThreadTaskScheduler::python_scheduler_loop(void)\n  {\n    // global startup of python interpreter if needed\n    if(!interpreter_ready) {\n      log_py.info() << \"creating interpreter\";\n      pyproc->create_interpreter();\n      interpreter_ready = true;\n    }\n\n#ifdef USE_PYGILSTATE_CALLS\n    // our PyThreadState is implicit when using the PyGILState calls\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = 0;\n#else\n    // always create and remember our own python thread - does NOT require GIL\n    PyThreadState *pythread = (pyproc->interpreter->api->PyThreadState_New)(pyproc->master_thread->interp);\n    log_py.debug() << \"created python thread: \" << pythread;\n    \n    assert(pythread != 0);\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = pythread;\n#endif\n\n    // take lock and go into normal task scheduler loop\n    {\n      AutoLock<> al(lock);\n      KernelThreadTaskScheduler::scheduler_loop();\n    }\n#if 0\n    // now go into main scheduler loop, holding scheduler lock for whole thing\n    AutoLock<> al(lock);\n    while(true) {\n      // remember the work counter value before we start so that we don't iterate\n      //   unnecessarily\n      long long old_work_counter = work_counter.read_counter();\n\n      // first priority - task registration\n      while(!taskreg_queue.empty()) {\n\tLocalPythonProcessor::TaskRegistration *treg = taskreg_queue.front();\n\ttaskreg_queue.pop_front();\n\t\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\t\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = TaskQueue::PRI_POS_INF;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  pyproc->perform_task_registration(treg);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      }\n\n      // if we have both resumable and new ready tasks, we want the one that\n      //  is the highest priority, with ties going to resumable tasks - we\n      //  can do this cleanly by taking advantage of the fact that the\n      //  resumable_workers queue uses the scheduler lock, so can't change\n      //  during this call\n      // peek at the top thing (if any) in that queue, and then try to find\n      //  a ready task with higher priority\n      int resumable_priority = ResumableQueue::PRI_NEG_INF;\n      resumable_workers.peek(&resumable_priority);\n\n      // try to get a new task then\n      int task_priority = resumable_priority;\n      Task *task = TaskQueue::get_best_task(task_queues, task_priority);\n\n      // did we find work to do?\n      if(task) {\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = task_priority;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  execute_task(task);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n\tcontinue;\n      }\n\n      // having checked for higher-priority ready tasks, we can always\n      //  take the highest-priority resumable task, if any, and run it\n      if(!resumable_workers.empty()) {\n\tThread *yield_to = resumable_workers.get(0); // priority is irrelevant\n\tassert(yield_to != Thread::self());\n\n\t// this should only happen if we're at the max active worker count (otherwise\n\t//  somebody should have just woken this guy up earlier), and reduces the \n\t// unassigned worker count by one\n\tupdate_worker_count(0, -1);\n\n\tidle_workers.push_back(Thread::self());\n\tworker_sleep(yield_to);\n\n\t// loop around and check both queues again\n\tcontinue;\n      }\n\n      {\n\t// no ready or resumable tasks?  thumb twiddling time\n\n\t// are we shutting down?\n\tif(shutdown_flag) {\n\t  // yes, we can terminate - wake up an idler (if any) first though\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t  } else {\n\t    // nobody to wake, so -1 active/unassigned worker\n\t    update_worker_count(-1, -1, false); // ok to drop below mins\n\t    worker_terminate(0);\n\t  }\n\t  return;\n\t}\n\n\t// do we have more unassigned and idle tasks than we need?\n\tint total_idle_count = (unassigned_worker_count +\n\t\t\t\t(int)(idle_workers.size()));\n\tif(total_idle_count > cfg_max_idle_workers) {\n\t  // if there are sleeping idlers, terminate in favor of one of those - keeps\n\t  //  worker counts constant\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    assert(to_wake != Thread::self());\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t    return;\n\t  }\n\t}\n\n\t// no, stay awake but suspend until there's a chance that the next iteration\n\t//  of this loop would turn out different\n\twait_for_work(old_work_counter);\n      }\n    }\n    // should never get here\n    assert(0);\n#endif\n  }\n\n  Thread *PythonThreadTaskScheduler::worker_create(bool make_active)\n  {\n    // lock is held by caller\n    ThreadLaunchParameters tlp;\n    Thread *t = Thread::create_kernel_thread<PythonThreadTaskScheduler,\n\t\t\t\t\t     &PythonThreadTaskScheduler::python_scheduler_loop>(this,\n\t\t\t\t\t\t\t\t\t\t\t\ttlp,\n\t\t\t\t\t\t\t\t\t\t\t\tcore_rsrv,\n\t\t\t\t\t\t\t\t\t\t\t\tthis);\n    all_workers.insert(t);\n    if(make_active)\n      active_workers.insert(t);\n    return t;\n  }\n \n  // called by a worker thread when it needs to wait for something (and we\n  //   should release the GIL)\n  void PythonThreadTaskScheduler::thread_blocking(Thread *thread)\n  {\n    // if this gets called before we're done initializing the interpreter,\n    //  we need a simple blocking wait\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n\n      log_py.debug() << \"waiting during initialization\";\n      bool really_blocked = try_update_thread_state(thread,\n\t\t\t\t\t\t    Thread::STATE_BLOCKING,\n\t\t\t\t\t\t    Thread::STATE_BLOCKED);\n      if(!really_blocked) return;\n\n      while(true) {\n\tlong long old_work_counter = work_counter.read_counter();\n\n\tif(!resumable_workers.empty()) {\n\t  Thread *t = resumable_workers.get(0);\n\t  assert(t == thread);\n\t  log_py.debug() << \"awake again\";\n\t  return;\n\t}\n\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // if we got here through a cffi call, the GIL has already been released,\n    //  so try to handle that case here - a call PyEval_SaveThread\n    //  if the GIL is not held will assert-fail, and while a call to\n    //  PyThreadState_Swap is technically illegal (and unsafe if python-created\n    //  threads exist), it does what we want for now\n    // NOTE: we use PyEval_{Save,Restore}Thread here even if USE_PYGILSTATE_CALLS\n    //  is defined, as a call to PyGILState_Release will destroy a thread\n    //  context - the Save/Restore take care of the actual lock, and since we\n    //  restore each python thread on the OS thread that owned it intially, the\n    //  PyGILState TLS stuff should remain consistent\n    PyThreadState *saved = (pyproc->interpreter->api->PyThreadState_Swap)(0);\n    if(saved != 0) {\n      log_py.info() << \"python worker sleeping - releasing GIL\";\n      // put it back so we can save it properly\n      (pyproc->interpreter->api->PyThreadState_Swap)(saved);\n      // would like to sanity-check that this returns the expected thread state,\n      //  but that would require taking the PythonThreadTaskScheduler's lock\n      (pyproc->interpreter->api->PyEval_SaveThread)();\n      log_py.debug() << \"SaveThread -> \" << saved;\n    } else\n      log_py.info() << \"python worker sleeping - GIL already released\";\n    \n    KernelThreadTaskScheduler::thread_blocking(thread);\n\n    if(saved) {\n      log_py.info() << \"python worker awake - acquiring GIL\";\n      log_py.debug() << \"RestoreThread <- \" << saved;\n      (pyproc->interpreter->api->PyEval_RestoreThread)(saved);\n    } else\n      log_py.info() << \"python worker awake - not acquiring GIL\";\n  }\n\n  void PythonThreadTaskScheduler::thread_ready(Thread *thread)\n  {\n    // handle the wakening of the initialization thread specially\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n      resumable_workers.put(thread, 0);\n    } else {\n      KernelThreadTaskScheduler::thread_ready(thread);\n    }\n  }\n\n  void PythonThreadTaskScheduler::worker_terminate(Thread *switch_to)\n  {\n#ifdef USE_PYGILSTATE_CALLS\n    // nothing to do?  pythreads entry was a placeholder\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    pythreads.erase(it);\n\n#else\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    PyThreadState *pythread = it->second;\n    pythreads.erase(it);\n\n    log_py.debug() << \"destroying python thread: \" << pythread;\n    \n    // our thread should not be active\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\n    // switch to the master thread, retaining the GIL\n    log_py.debug() << \"RestoreThread <- \" << pyproc->master_thread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pyproc->master_thread);\n\n    // clear and delete the worker thread\n    (pyproc->interpreter->api->PyThreadState_Clear)(pythread);\n    (pyproc->interpreter->api->PyThreadState_Delete)(pythread);\n\n    // release the GIL\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pyproc->master_thread);\n#endif\n\n    // TODO: tear down interpreter if last thread\n    if(shutdown_flag && pythreads.empty())\n      pyproc->destroy_interpreter();\n\n    KernelThreadTaskScheduler::worker_terminate(switch_to);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class LocalPythonProcessor\n\n  LocalPythonProcessor::LocalPythonProcessor(Processor _me, int _numa_node,\n                                             CoreReservationSet& crs,\n                                             size_t _stack_size,\n\t\t\t\t\t     const std::vector<std::string>& _import_modules,\n\t\t\t\t\t     const std::vector<std::string>& _init_scripts)\n    : ProcessorImpl(_me, Processor::PY_PROC)\n    , numa_node(_numa_node)\n    , import_modules(_import_modules)\n    , init_scripts(_init_scripts)\n    , interpreter(0)\n    , ready_task_count(stringbuilder() << \"realm/proc \" << me << \"/ready tasks\")\n  {\n    task_queue.set_gauge(&ready_task_count);\n    deferred_spawn_cache.clear();\n\n    CoreReservationParameters params;\n    params.set_num_cores(1);\n    params.set_numa_domain(numa_node);\n    params.set_alu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_fpu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_ldst_usage(params.CORE_USAGE_SHARED);\n    params.set_max_stack_size(_stack_size);\n\n    std::string name = stringbuilder() << \"Python\" << numa_node << \" proc \" << _me;\n\n    core_rsrv = new CoreReservation(name, crs, params);\n\n    sched = new PythonThreadTaskScheduler(this, *core_rsrv);\n    sched->add_task_queue(&task_queue);\n  }\n\n  LocalPythonProcessor::~LocalPythonProcessor(void)\n  {\n    delete core_rsrv;\n    delete sched;\n  }\n\n  // starts worker threads and performs any per-processor initialization\n  void LocalPythonProcessor::start_threads(void)\n  {\n    // finally, fire up the scheduler\n    sched->start();\n  }\n\n  void LocalPythonProcessor::shutdown(void)\n  {\n    log_py.info() << \"shutting down\";\n\n    sched->shutdown();\n    deferred_spawn_cache.flush();\n  }\n\n  void LocalPythonProcessor::create_interpreter(void)\n  {\n    assert(interpreter == 0);\n  \n    // create a python interpreter that stays entirely within this thread\n    interpreter = new PythonInterpreter;\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (interpreter->api->PyGILState_Ensure)();\n    assert(gilstate = PyGILState_UNLOCKED);\n#else\n    master_thread = (interpreter->api->PyThreadState_Get)();\n#endif\n\n    // always need the python threading module\n    interpreter->import_module(\"threading\");\n    \n    // perform requested initialization\n    for(std::vector<std::string>::const_iterator it = import_modules.begin();\n\tit != import_modules.end();\n\t++it)\n      interpreter->import_module(*it);\n\n    for(std::vector<std::string>::const_iterator it = init_scripts.begin();\n\tit != init_scripts.end();\n\t++it)\n      interpreter->run_string(*it);\n\n    // default state is GIL _released_\n#ifdef USE_PYGILSTATE_CALLS\n    (interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n#endif\n  }\n\n  void LocalPythonProcessor::destroy_interpreter(void)\n  {\n    assert(interpreter != 0);\n\n    log_py.info() << \"destroying interpreter\";\n\n    // take GIL with master thread\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (interpreter->api->PyGILState_Ensure)();\n    assert(gilstate == PyGILState_UNLOCKED);\n#else\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n#endif\n\n    // during shutdown, the threading module tries to remove the Thread object\n    //  associated with this kernel thread - if that doesn't exist (because we're\n    //  shutting down from a different thread that we initialized the interpreter\n    //  _and_ nobody called threading.current_thread() from this kernel thread),\n    //  we'll get a KeyError in threading.py\n    // resolve this by calling threading.current_thread() here, using __import__\n    //  to deal with the case where 'import threading' never got called\n    (interpreter->api->PyRun_SimpleString)(\"__import__('threading').current_thread()\");\n\n    delete interpreter;\n    interpreter = 0;\n    master_thread = 0;\n  }\n  \n  bool LocalPythonProcessor::perform_task_registration(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(treg->func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << treg->func_id;\n      assert(0);\n    }\n\n    // next, see if we have a Python function to register\n    const PythonSourceImplementation *psi = treg->codedesc->find_impl<PythonSourceImplementation>();\n    if(!psi) {\n      log_py.fatal() << \"invalid code descriptor for python proc: \" << *(treg->codedesc);\n      assert(0);\n    }\n\n    PyObject *fnptr = interpreter->find_or_import_function(psi);\n    assert(fnptr != 0);\n\n    log_py.info() << \"task \" << treg->func_id << \" registered on \" << me << \": \" << *(treg->codedesc);\n\n    TaskTableEntry &tte = task_table[treg->func_id];\n    tte.fnptr = fnptr;\n    tte.user_data.swap(treg->user_data);\n\n    delete treg->codedesc;\n    delete treg;\n\n    return true;\n  }\n\n  void LocalPythonProcessor::enqueue_task(Task *task)\n  {\n    task_queue.enqueue_task(task);\n  }\n\n  void LocalPythonProcessor::enqueue_tasks(Task::TaskList& tasks)\n  {\n    task_queue.enqueue_tasks(tasks);\n  }\n\n  void LocalPythonProcessor::spawn_task(Processor::TaskFuncID func_id,\n\t\t\t\t\tconst void *args, size_t arglen,\n\t\t\t\t\tconst ProfilingRequestSet &reqs,\n\t\t\t\t\tEvent start_event,\n\t\t\t\t\tGenEventImpl *finish_event,\n\t\t\t\t\tEventImpl::gen_t finish_gen,\n\t\t\t\t\tint priority)\n  {\n    // create a task object for this\n    Task *task = new Task(me, func_id, args, arglen, reqs,\n\t\t\t  start_event, finish_event, finish_gen, priority);\n    get_runtime()->optable.add_local_operation(finish_event->make_event(finish_gen), task);\n\n    enqueue_or_defer_task(task, start_event, &deferred_spawn_cache);\n  }\n\n  void LocalPythonProcessor::add_to_group(ProcessorGroup *group)\n  {\n    // add the group's task queue to our scheduler too\n    sched->add_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::register_task(Processor::TaskFuncID func_id,\n                                           CodeDescriptor& codedesc,\n                                           const ByteArrayRef& user_data)\n  {\n    TaskRegistration *treg = new TaskRegistration;\n    treg->proc = this;\n    treg->func_id = func_id;\n    treg->codedesc = new CodeDescriptor(codedesc);\n    treg->user_data = user_data;\n    sched->add_internal_task(treg);\n  }\n\n  void LocalPythonProcessor::execute_task(Processor::TaskFuncID func_id,\n\t\t\t\t\t  const ByteArrayRef& task_args)\n  {\n    std::map<Processor::TaskFuncID, TaskTableEntry>::const_iterator it = task_table.find(func_id);\n    if(it == task_table.end()) {\n      // TODO: remove this hack once the tools are available to the HLR to call these directly\n      if(func_id < Processor::TASK_ID_FIRST_AVAILABLE) {\n\tlog_py.info() << \"task \" << func_id << \" not registered on \" << me << \": ignoring missing legacy setup/shutdown task\";\n\treturn;\n      }\n      log_py.fatal() << \"task \" << func_id << \" not registered on \" << me;\n      assert(0);\n    }\n\n    const TaskTableEntry& tte = it->second;\n\n    log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": \" << ((void *)(tte.fnptr));\n\n    PyObject *arg1 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)task_args.base(),\n\t\t\t\t\t\t   task_args.size());\n    assert(arg1 != 0);\n    PyObject *arg2 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)tte.user_data.base(),\n\t\t\t\t\t\t   tte.user_data.size());\n    assert(arg2 != 0);\n    // TODO: make into a Python realm.Processor object\n    PyObject *arg3 = (interpreter->api->PyLong_FromUnsignedLong)(me.id);\n    assert(arg3 != 0);\n\n    PyObject *args = (interpreter->api->PyTuple_New)(3);\n    assert(args != 0);\n    (interpreter->api->PyTuple_SetItem)(args, 0, arg1);\n    (interpreter->api->PyTuple_SetItem)(args, 1, arg2);\n    (interpreter->api->PyTuple_SetItem)(args, 2, arg3);\n\n    //printf(\"args = \"); (interpreter->api->PyObject_Print)(args, stdout, 0); printf(\"\\n\");\n\n    PyObject *res = (interpreter->api->PyObject_CallObject)(tte.fnptr, args);\n\n    (interpreter->api->Py_DecRef)(args);\n\n    //printf(\"res = \"); PyObject_Print(res, stdout, 0); printf(\"\\n\");\n    if(res != 0) {\n      (interpreter->api->Py_DecRef)(res);\n    } else {\n      log_py.fatal() << \"python exception occurred within task:\";\n      (interpreter->api->PyErr_PrintEx)(0);\n      (interpreter->api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n  }\n\n  namespace Python {\n\n    ////////////////////////////////////////////////////////////////////////\n    //\n    // class PythonModule\n\n    /*static*/ std::vector<std::string> PythonModule::extra_import_modules;\n\n    PythonModule::PythonModule(void)\n      : Module(\"python\")\n      , cfg_num_python_cpus(0)\n      , cfg_use_numa(false)\n      , cfg_stack_size(2 << 20)\n    {\n    }\n\n    PythonModule::~PythonModule(void)\n    {}\n\n    /*static*/ void PythonModule::import_python_module(const char *module_name)\n    {\n      extra_import_modules.push_back(module_name);\n    }\n\n    /*static*/ Module *PythonModule::create_module(RuntimeImpl *runtime,\n                                                 std::vector<std::string>& cmdline)\n    {\n      // create a module to fill in with stuff - we'll delete it if numa is\n      //  disabled\n      PythonModule *m = new PythonModule;\n\n      // first order of business - read command line parameters\n      {\n        CommandLineParser cp;\n\n        cp.add_option_int(\"-ll:py\", m->cfg_num_python_cpus)\n\t  .add_option_int(\"-ll:pynuma\", m->cfg_use_numa)\n\t  .add_option_int_units(\"-ll:pystack\", m->cfg_stack_size, 'm')\n\t  .add_option_stringlist(\"-ll:pyimport\", m->cfg_import_modules)\n\t  .add_option_stringlist(\"-ll:pyinit\", m->cfg_init_scripts);\n\n        bool ok = cp.parse_command_line(cmdline);\n        if(!ok) {\n          log_py.fatal() << \"error reading Python command line parameters\";\n          assert(false);\n        }\n      }\n\n      // add extra module imports requested by the application\n      m->cfg_import_modules.insert(m->cfg_import_modules.end(),\n                                   extra_import_modules.begin(),\n                                   extra_import_modules.end());\n\n      // if no cpus were requested, there's no point\n      if(m->cfg_num_python_cpus == 0) {\n        log_py.debug() << \"no Python cpus requested\";\n        delete m;\n        return 0;\n      }\n\n#ifndef REALM_USE_DLMOPEN\n      // Multiple CPUs are only allowed if we're using dlmopen.\n      if(m->cfg_num_python_cpus > 1) {\n        log_py.fatal() << \"support for multiple Python CPUs is not available: recompile with USE_DLMOPEN\";\n        assert(false);\n      }\n#endif\n\n      // get number/sizes of NUMA nodes -\n      //   disable (with a warning) numa binding if support not found\n      if(m->cfg_use_numa) {\n        std::map<int, NumaNodeCpuInfo> cpuinfo;\n        if(numasysif_numa_available() &&\n           numasysif_get_cpu_info(cpuinfo) &&\n           !cpuinfo.empty()) {\n          // filter out any numa domains with insufficient core counts\n          int cores_needed = m->cfg_num_python_cpus;\n          for(std::map<int, NumaNodeCpuInfo>::const_iterator it = cpuinfo.begin();\n              it != cpuinfo.end();\n              ++it) {\n            const NumaNodeCpuInfo& ci = it->second;\n            if(ci.cores_available >= cores_needed) {\n              m->active_numa_domains.insert(ci.node_id);\n            } else {\n              log_py.warning() << \"not enough cores in NUMA domain \" << ci.node_id << \" (\" << ci.cores_available << \" < \" << cores_needed << \")\";\n            }\n          }\n        } else {\n          log_py.warning() << \"numa support not found (or not working)\";\n          m->cfg_use_numa = false;\n        }\n      }\n\n      // if we don't end up with any active numa domains,\n      //  use NUMA_DOMAIN_DONTCARE\n      // actually, use the value (-1) since it seems to cause link errors!?\n      if(m->active_numa_domains.empty())\n        m->active_numa_domains.insert(-1 /*CoreReservationParameters::NUMA_DOMAIN_DONTCARE*/);\n\n      return m;\n    }\n\n    // do any general initialization - this is called after all configuration is\n    //  complete\n    void PythonModule::initialize(RuntimeImpl *runtime)\n    {\n      Module::initialize(runtime);\n    }\n\n    // create any processors provided by the module (default == do nothing)\n    //  (each new ProcessorImpl should use a Processor from\n    //   RuntimeImpl::next_local_processor_id)\n    void PythonModule::create_processors(RuntimeImpl *runtime)\n    {\n      Module::create_processors(runtime);\n\n      for(std::set<int>::const_iterator it = active_numa_domains.begin();\n          it != active_numa_domains.end();\n          ++it) {\n        int cpu_node = *it;\n        for(int i = 0; i < cfg_num_python_cpus; i++) {\n          Processor p = runtime->next_local_processor_id();\n          ProcessorImpl *pi = new LocalPythonProcessor(p, cpu_node,\n                                                       runtime->core_reservation_set(),\n                                                       cfg_stack_size,\n\t\t\t\t\t\t       cfg_import_modules,\n\t\t\t\t\t\t       cfg_init_scripts);\n          runtime->add_processor(pi);\n\n          // create affinities between this processor and system/reg memories\n          // if the memory is one we created, use the kernel-reported distance\n          // to adjust the answer\n          std::vector<MemoryImpl *>& local_mems = runtime->nodes[Network::my_node_id].memories;\n          for(std::vector<MemoryImpl *>::iterator it2 = local_mems.begin();\n              it2 != local_mems.end();\n              ++it2) {\n            Memory::Kind kind = (*it2)->get_kind();\n            if((kind != Memory::SYSTEM_MEM) && (kind != Memory::REGDMA_MEM))\n              continue;\n\n            Machine::ProcessorMemoryAffinity pma;\n            pma.p = p;\n            pma.m = (*it2)->me;\n\n            // use the same made-up numbers as in\n            //  runtime_impl.cc\n            if(kind == Memory::SYSTEM_MEM) {\n              pma.bandwidth = 100;  // \"large\"\n              pma.latency = 5;      // \"small\"\n            } else {\n              pma.bandwidth = 80;   // \"large\"\n              pma.latency = 10;     // \"small\"\n            }\n\n            runtime->add_proc_mem_affinity(pma);\n          }\n        }\n      }\n    }\n\n    // clean up any common resources created by the module - this will be called\n    //  after all memories/processors/etc. have been shut down and destroyed\n    void PythonModule::cleanup(void)\n    {\n      Module::cleanup();\n    }\n\n  }; // namespace Python\n\n}; // namespace Realm\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/language/src/regent/cudahelper.t": "-- Copyright 2020 Stanford University, Los Alamos National Laboratory\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal ast = require(\"regent/ast\")\nlocal base = require(\"regent/std_base\")\nlocal config = require(\"regent/config\").args()\nlocal data = require(\"common/data\")\nlocal report = require(\"common/report\")\n\nlocal cudahelper = {}\n\n-- Exit early if the user turned off CUDA code generation\n\nif config[\"cuda\"] == 0 then\n  function cudahelper.check_cuda_available()\n    return false\n  end\n  return cudahelper\nend\n\nlocal c = base.c\nlocal ef = terralib.externfunction\nlocal externcall_builtin = terralib.externfunction\nlocal cudapaths = { OSX = \"/usr/local/cuda/lib/libcuda.dylib\";\n                    Linux =  \"libcuda.so\";\n                    Windows = \"nvcuda.dll\"; }\n\n-- #####################################\n-- ## CUDA Hijack API\n-- #################\n\nlocal HijackAPI = terralib.includec(\"regent_cudart_hijack.h\")\n\nstruct fat_bin_t {\n  magic : int,\n  versions : int,\n  data : &opaque,\n  filename : &opaque,\n}\n\n-- #####################################\n-- ## CUDA Device API\n-- #################\n\nlocal struct CUctx_st\nlocal struct CUmod_st\nlocal struct CUlinkState_st\nlocal struct CUfunc_st\nlocal CUdevice = int32\nlocal CUjit_option = uint32\nlocal CU_JIT_ERROR_LOG_BUFFER = 5\nlocal CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES = 6\nlocal CU_JIT_INPUT_PTX = 1\nlocal CU_JIT_TARGET = 9\nlocal DriverAPI = {\n  cuInit = ef(\"cuInit\", {uint32} -> uint32);\n  cuCtxGetCurrent = ef(\"cuCtxGetCurrent\", {&&CUctx_st} -> uint32);\n  cuCtxGetDevice = ef(\"cuCtxGetDevice\",{&int32} -> uint32);\n  cuDeviceGet = ef(\"cuDeviceGet\",{&int32,int32} -> uint32);\n  cuCtxCreate_v2 = ef(\"cuCtxCreate_v2\",{&&CUctx_st,uint32,int32} -> uint32);\n  cuCtxDestroy = ef(\"cuCtxDestroy\",{&CUctx_st} -> uint32);\n  cuDeviceComputeCapability = ef(\"cuDeviceComputeCapability\",\n    {&int32,&int32,int32} -> uint32);\n}\n\nlocal RuntimeAPI = false\ndo\n  if not terralib.cudacompile then\n    function cudahelper.check_cuda_available()\n      return false, \"Terra is built without CUDA support\"\n    end\n  else\n    -- Try to load the CUDA runtime header\n    pcall(function() RuntimeAPI = terralib.includec(\"cuda_runtime.h\") end)\n\n    if RuntimeAPI == nil then\n      function cudahelper.check_cuda_available()\n        return false, \"cuda_runtime.h does not exist in INCLUDE_PATH\"\n      end\n    elseif config[\"cuda-offline\"] then\n      function cudahelper.check_cuda_available()\n        return true\n      end\n    else\n      local dlfcn = terralib.includec(\"dlfcn.h\")\n      local terra has_symbol(symbol : rawstring)\n        var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n        var has_symbol = dlfcn.dlsym(lib, symbol) ~= [&opaque](0)\n        dlfcn.dlclose(lib)\n        return has_symbol\n      end\n\n      if has_symbol(\"cuInit\") then\n        local r = DriverAPI.cuInit(0)\n        if r == 0 then\n          function cudahelper.check_cuda_available()\n            return true\n          end\n        else\n          function cudahelper.check_cuda_available()\n            return false, \"calling cuInit(0) failed for some reason (CUDA devices might not exist)\"\n          end\n        end\n      else\n        function cudahelper.check_cuda_available()\n          return false, \"the cuInit function is missing (Regent might have been installed without CUDA support)\"\n        end\n      end\n    end\n  end\nend\n\ndo\n  local available, error_message = cudahelper.check_cuda_available()\n  if not available then\n    if config[\"cuda\"] == 1 then\n      print(\"CUDA code generation failed since \" .. error_message)\n      os.exit(-1)\n    else\n      return cudahelper\n    end\n  end\nend\n\n-- Declare the API calls that are deprecated in CUDA SDK 10\n-- TODO: We must move on to the new execution control API as these old functions\n--       can be dropped in the future.\nlocal ExecutionAPI = {\n  cudaConfigureCall =\n    ef(\"cudaConfigureCall\", {RuntimeAPI.dim3, RuntimeAPI.dim3, uint64, RuntimeAPI.cudaStream_t} -> uint32);\n  cudaSetupArgument = ef(\"cudaSetupArgument\", {&opaque, uint64, uint64} -> uint32);\n  cudaLaunch = ef(\"cudaLaunch\", {&opaque} -> uint32);\n}\n\ndo\n  local ffi = require('ffi')\n  local cudaruntimelinked = false\n  function cudahelper.link_driver_library()\n    if cudaruntimelinked then return end\n    local path = assert(cudapaths[ffi.os],\"unknown OS?\")\n    terralib.linklibrary(path)\n    cudaruntimelinked = true\n  end\nend\n\n-- #####################################\n-- ## Printf for CUDA (not exposed to the user for the moment)\n-- #################\n\nlocal vprintf = ef(\"cudart:vprintf\", {&int8,&int8} -> int)\n\nlocal function createbuffer(args)\n  local Buf = terralib.types.newstruct()\n  for i,e in ipairs(args) do\n    local typ = e:gettype()\n    local field = \"_\"..tonumber(i)\n    typ = typ == float and double or typ\n    table.insert(Buf.entries,{field,typ})\n  end\n  return quote\n    var buf : Buf\n    escape\n        for i,e in ipairs(args) do\n            emit quote\n               buf.[\"_\"..tonumber(i)] = e\n            end\n        end\n    end\n  in\n    [&int8](&buf)\n  end\nend\n\nlocal cuda_printf = macro(function(fmt,...)\n  local buf = createbuffer({...})\n  return `vprintf(fmt,buf)\nend)\n\n-- #####################################\n-- ## Supported CUDA compute versions\n-- #################\n\nlocal supported_archs = {\n  [\"fermi\"]   = 20,\n  [\"kepler\"]  = 30,\n  [\"k20\"]     = 35,\n  [\"maxwell\"] = 52,\n  [\"pascal\"]  = 60,\n  [\"volta\"]   = 70,\n}\n\nlocal function parse_cuda_arch(arch)\n  arch = string.lower(arch)\n  local sm = supported_archs[arch]\n  if sm == nil then\n    local archs\n    for k, v in pairs(supported_archs) do\n      archs = (not archs and k) or (archs and archs .. \", \" .. k)\n    end\n    print(\"Error: Unsupported GPU architecture \" .. arch ..\n          \". Supported architectures: \" .. archs)\n    os.exit(1)\n  end\n  return sm\nend\n\n-- #####################################\n-- ## Registration functions\n-- #################\n\nlocal terra register_ptx(ptxc : rawstring, ptxSize : uint32, version : uint64) : &&opaque\n  var fat_bin : &fat_bin_t\n  var fat_size = sizeof(fat_bin_t)\n  -- TODO: this line is leaking memory\n  fat_bin = [&fat_bin_t](c.malloc(fat_size))\n  base.assert(fat_size == 0 or fat_bin ~= nil, \"malloc failed in register_ptx\")\n  fat_bin.magic = 1234\n  fat_bin.versions = 5678\n  var fat_data_size = ptxSize + 1\n  fat_bin.data = c.malloc(fat_data_size)\n  base.assert(fat_data_size == 0 or fat_bin.data ~= nil, \"malloc failed in register_ptx\")\n  fat_bin.data = ptxc\n  var handle = HijackAPI.hijackCudaRegisterFatBinary(fat_bin)\n  return handle\nend\n\nlocal terra register_function(handle : &&opaque, id : int, name : &int8)\n  HijackAPI.hijackCudaRegisterFunction(handle, [&int8](id), name)\nend\n\nlocal function find_device_library(target)\n  local device_lib_dir = terralib.cudahome .. \"/nvvm/libdevice/\"\n  local libdevice = nil\n  for f in io.popen(\"ls \" .. device_lib_dir):lines() do\n    local version = tonumber(string.match(string.match(f, \"[0-9][0-9][.]\"), \"[0-9][0-9]\"))\n    if version <= target then\n      libdevice = device_lib_dir .. f\n    end\n  end\n  assert(libdevice ~= nil, \"Failed to find a device library\")\n  return libdevice\nend\n\nlocal get_cuda_version\ndo\n  local cached_cuda_version = nil\n  local terra get_cuda_version_terra() : uint64\n    var cx : &CUctx_st\n    var cx_created = false\n    var r = DriverAPI.cuCtxGetCurrent(&cx)\n    base.assert(r == 0, \"CUDA error in cuCtxGetCurrent\")\n    var device : int32\n    if cx ~= nil then\n      r = DriverAPI.cuCtxGetDevice(&device)\n      base.assert(r == 0, \"CUDA error in cuCtxGetDevice\")\n    else\n      r = DriverAPI.cuDeviceGet(&device, 0)\n      base.assert(r == 0, \"CUDA error in cuDeviceGet\")\n      r = DriverAPI.cuCtxCreate_v2(&cx, 0, device)\n      base.assert(r == 0, \"CUDA error in cuCtxCreate_v2\")\n      cx_created = true\n    end\n\n    var major : int, minor : int\n    r = DriverAPI.cuDeviceComputeCapability(&major, &minor, device)\n    base.assert(r == 0, \"CUDA error in cuDeviceComputeCapability\")\n    var version = [uint64](major * 10 + minor)\n    if cx_created then\n      DriverAPI.cuCtxDestroy(cx)\n    end\n    return version\n  end\n\n  get_cuda_version = function()\n    if cached_cuda_version ~= nil then\n      return cached_cuda_version\n    end\n    if not config[\"cuda-offline\"] then\n      cached_cuda_version = get_cuda_version_terra()\n    else\n      cached_cuda_version = parse_cuda_arch(config[\"cuda-arch\"])\n    end\n    return cached_cuda_version\n  end\nend\n\nfunction cudahelper.jit_compile_kernels_and_register(kernels)\n  local module = {}\n  for k, v in pairs(kernels) do\n    module[v.name] = v.kernel\n  end\n  local version = get_cuda_version()\n  local libdevice = find_device_library(tonumber(version))\n  local llvmbc = terralib.linkllvm(libdevice)\n  externcall_builtin = function(name, ftype)\n    return llvmbc:extern(name, ftype)\n  end\n  local ptx = cudalib.toptx(module, nil, version)\n\n  local ptxc = terralib.constant(ptx)\n  local handle = terralib.newsymbol(&&opaque, \"handle\")\n  local register = quote\n    var [handle] = register_ptx(ptxc, [ptx:len() + 1], [version])\n  end\n\n  for k, v in pairs(kernels) do\n    register = quote\n      [register]\n      register_function([handle], [k], [v.name])\n    end\n  end\n\n  return register\nend\n\n-- #####################################\n-- ## Primitives\n-- #################\n\nlocal THREAD_BLOCK_SIZE = 128\nlocal NUM_THREAD_X = 16\nlocal NUM_THREAD_Y = THREAD_BLOCK_SIZE / NUM_THREAD_X\nlocal MAX_NUM_BLOCK = 32768\nlocal GLOBAL_RED_BUFFER = 256\nassert(GLOBAL_RED_BUFFER % THREAD_BLOCK_SIZE == 0)\n\nlocal tid_x   = cudalib.nvvm_read_ptx_sreg_tid_x\nlocal n_tid_x = cudalib.nvvm_read_ptx_sreg_ntid_x\nlocal bid_x   = cudalib.nvvm_read_ptx_sreg_ctaid_x\nlocal n_bid_x = cudalib.nvvm_read_ptx_sreg_nctaid_x\n\nlocal tid_y   = cudalib.nvvm_read_ptx_sreg_tid_y\nlocal n_tid_y = cudalib.nvvm_read_ptx_sreg_ntid_y\nlocal bid_y   = cudalib.nvvm_read_ptx_sreg_ctaid_y\nlocal n_bid_y = cudalib.nvvm_read_ptx_sreg_nctaid_y\n\nlocal tid_z   = cudalib.nvvm_read_ptx_sreg_tid_z\nlocal n_tid_z = cudalib.nvvm_read_ptx_sreg_ntid_z\nlocal bid_z   = cudalib.nvvm_read_ptx_sreg_ctaid_z\nlocal n_bid_z = cudalib.nvvm_read_ptx_sreg_nctaid_z\n\nlocal barrier = cudalib.nvvm_barrier0\n\nlocal supported_scalar_red_ops = {\n  [\"+\"]   = true,\n  [\"*\"]   = true,\n  [\"max\"] = true,\n  [\"min\"] = true,\n}\n\nfunction cudahelper.global_thread_id()\n  local bid = `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\n  local num_threads = `(n_tid_x())\n  return `([bid] * [num_threads] + tid_x())\nend\n\nfunction cudahelper.global_block_id()\n  return `(bid_x() + n_bid_x() * bid_y() + n_bid_x() * n_bid_y() * bid_z())\nend\n\nfunction cudahelper.get_thread_block_size()\n  return THREAD_BLOCK_SIZE\nend\n\nfunction cudahelper.get_num_thread_x()\n  return NUM_THREAD_X\nend\n\nfunction cudahelper.get_num_thread_y()\n  return NUM_THREAD_Y\nend\n\n-- Slow atomic operation implementations (copied and modified from Ebb)\nlocal terra cas_uint64(address : &uint64, compare : uint64, value : uint64)\n  return terralib.asm(terralib.types.uint64,\n                      \"atom.global.cas.b64 $0, [$1], $2, $3;\",\n                      \"=l,l,l,l\", true, address, compare, value)\nend\ncas_uint64:setinlined(true)\n\nlocal terra cas_uint32(address : &uint32, compare : uint32, value : uint32)\n  return terralib.asm(terralib.types.uint32,\n                      \"atom.global.cas.b32 $0, [$1], $2, $3;\",\n                      \"=r,l,r,r\", true, address, compare, value)\nend\ncas_uint32:setinlined(true)\n\nfunction cudahelper.generate_atomic_update(op, typ)\n  if op == \"+\" and typ == float then\n    return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f32.p0f32\",\n                              {&float,float} -> {float})\n  elseif op == \"+\" and typ == double and get_cuda_version() >= 60 then\n    return terralib.intrinsic(\"llvm.nvvm.atomic.load.add.f64.p0f64\",\n                              {&double,double} -> {double})\n  end\n\n  local cas_type\n  local cas_func\n  if sizeof(typ) == 4 then\n    cas_type = uint32\n    cas_func = cas_uint32\n  else\n    assert(sizeof(typ) == 8)\n    cas_type = uint64\n    cas_func = cas_uint64\n  end\n  local terra atomic_op(address : &typ, operand : typ)\n    var old : typ = @address\n    var assumed : typ\n    var new     : typ\n\n    var new_b     : &cas_type = [&cas_type](&new)\n    var assumed_b : &cas_type = [&cas_type](&assumed)\n    var res       :  cas_type\n\n    var mask = false\n    repeat\n      if not mask then\n        assumed = old\n        new     = [base.quote_binary_op(op, assumed, operand)]\n        res     = cas_func([&cas_type](address), @assumed_b, @new_b)\n        old     = @[&typ](&res)\n        mask    = @assumed_b == @[&cas_type](&old)\n      end\n    until mask\n  end\n  atomic_op:setinlined(true)\n  return atomic_op\nend\n\nlocal function generate_element_reduction(lhs, rhs, op, volatile)\n  if volatile then\n    return quote\n      do\n        var v = [base.quote_binary_op(op, lhs, rhs)]\n        terralib.attrstore(&[lhs], v, { isvolatile = true })\n      end\n    end\n  else\n    return quote\n      [lhs] = [base.quote_binary_op(op, lhs, rhs)]\n    end\n  end\nend\n\nlocal function generate_element_reductions(lhs, rhs, op, type, volatile)\n  local actions = terralib.newlist()\n  if type:isarray() then\n    for k = 1, type.N do -- inclusive!\n      local lhs = `([lhs][ [k - 1] ])\n      local rhs = `([rhs][ [k - 1] ])\n      actions:insert(generate_element_reduction(lhs, rhs, op, volatile))\n    end\n  else\n    assert(type:isprimitive())\n    actions:insert(generate_element_reduction(lhs, rhs, op, volatile))\n  end\n  return quote [actions] end\nend\n\n-- #####################################\n-- ## Code generation for scalar reduction\n-- #################\n\nfunction cudahelper.compute_reduction_buffer_size(cx, node, reductions)\n  local size = 0\n  for k, v in pairs(reductions) do\n    if size ~= 0 then\n      -- TODO: We assume there is only one scalar reduction for now\n      report.error(node,\n          \"Multiple scalar reductions in a CUDA task are not supported yet\")\n    elseif not supported_scalar_red_ops[v] then\n      report.error(node,\n          \"Scalar reduction with operator \" .. v .. \" is not supported yet\")\n    elseif not (sizeof(k.type) == 4 or sizeof(k.type) == 8) then\n      report.error(node,\n          \"Scalar reduction for type \" .. tostring(k.type) .. \" is not supported yet\")\n    end\n    size = size + THREAD_BLOCK_SIZE * sizeof(k.type)\n  end\n  size = size + cx:compute_reduction_buffer_size()\n  return size\nend\n\nlocal internal_kernel_id = 2 ^ 30\nlocal internal_kernels = {}\nlocal INTERNAL_KERNEL_PREFIX = \"__internal\"\n\nfunction cudahelper.get_internal_kernels()\n  return internal_kernels\nend\n\ncudahelper.generate_buffer_init_kernel = terralib.memoize(function(type, op)\n  local value = base.reduction_op_init[op][type]\n  local op_name = base.reduction_ops[op].name\n  local kernel_id = internal_kernel_id\n  internal_kernel_id = internal_kernel_id - 1\n  local kernel_name =\n    INTERNAL_KERNEL_PREFIX .. \"__init__\" .. tostring(type) ..\n    \"__\" .. tostring(op_name) .. \"__\"\n  local terra init(buffer : &type)\n    var tid = tid_x() + bid_x() * n_tid_x()\n    buffer[tid] = [value]\n  end\n  init:setname(kernel_name)\n  internal_kernels[kernel_id] = {\n    name = kernel_name,\n    kernel = init,\n  }\n  return kernel_id\nend)\n\ncudahelper.generate_buffer_reduction_kernel = terralib.memoize(function(type, op)\n  local value = base.reduction_op_init[op][type]\n  local op_name = base.reduction_ops[op].name\n  local kernel_id = internal_kernel_id\n  internal_kernel_id = internal_kernel_id - 1\n  local kernel_name =\n    INTERNAL_KERNEL_PREFIX .. \"__red__\" .. tostring(type) ..\n    \"__\" .. tostring(op_name) .. \"__\"\n\n  local tid = terralib.newsymbol(c.size_t, \"tid\")\n  local input = terralib.newsymbol(&type, \"input\")\n  local result = terralib.newsymbol(&type, \"result\")\n  local shared_mem_ptr = cudalib.sharedmemory(type, THREAD_BLOCK_SIZE)\n\n  local shared_mem_init = `([input][ [tid] ])\n  for i = 1, (GLOBAL_RED_BUFFER / THREAD_BLOCK_SIZE) - 1 do\n    shared_mem_init =\n      base.quote_binary_op(op, shared_mem_init,\n                           `([input][ [tid] + [i * THREAD_BLOCK_SIZE] ]))\n  end\n  local terra red([input], [result])\n    var [tid] = tid_x()\n    [shared_mem_ptr][ [tid] ] = [shared_mem_init]\n    barrier()\n    [cudahelper.generate_reduction_tree(tid, shared_mem_ptr, THREAD_BLOCK_SIZE, op, type)]\n    barrier()\n    if [tid] == 0 then [result][0] = [shared_mem_ptr][ [tid] ] end\n  end\n\n  red:setname(kernel_name)\n  internal_kernels[kernel_id] = {\n    name = kernel_name,\n    kernel = red,\n  }\n  return kernel_id\nend)\n\nfunction cudahelper.generate_reduction_preamble(cx, reductions)\n  local preamble = terralib.newlist()\n  local device_ptrs = terralib.newlist()\n  local device_ptrs_map = {}\n  local host_ptrs_map = {}\n\n  for red_var, red_op in pairs(reductions) do\n    local device_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local host_ptr = terralib.newsymbol(&red_var.type, red_var.displayname)\n    local init_kernel_id = cudahelper.generate_buffer_init_kernel(red_var.type, red_op)\n    local init_args = terralib.newlist({device_ptr})\n    preamble:insert(quote\n      var [device_ptr] = [&red_var.type](nil)\n      var [host_ptr] = [&red_var.type](nil)\n      do\n        var bounds : c.legion_rect_1d_t\n        bounds.lo.x[0] = 0\n        bounds.hi.x[0] = [sizeof(red_var.type) * GLOBAL_RED_BUFFER - 1]\n        var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.GPU_FB_MEM, [&int8](nil))\n        [device_ptr] =\n          [&red_var.type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n        [cudahelper.codegen_kernel_call(cx, init_kernel_id, GLOBAL_RED_BUFFER, init_args, 0, true)]\n      end\n      do\n        var bounds : c.legion_rect_1d_t\n        bounds.lo.x[0] = 0\n        bounds.hi.x[0] = [sizeof(red_var.type) - 1]\n        var buffer = c.legion_deferred_buffer_char_1d_create(bounds, c.Z_COPY_MEM, [&int8](nil))\n        [host_ptr] =\n          [&red_var.type]([&opaque](c.legion_deferred_buffer_char_1d_ptr(buffer, bounds.lo)))\n      end\n    end)\n    device_ptrs:insert(device_ptr)\n    device_ptrs_map[device_ptr] = red_var\n    host_ptrs_map[device_ptr] = host_ptr\n  end\n\n  return device_ptrs, device_ptrs_map, host_ptrs_map, preamble\nend\n\nfunction cudahelper.generate_reduction_tree(tid, shared_mem_ptr, num_threads, red_op, type)\n  local outer_reductions = terralib.newlist()\n  local step = num_threads\n  while step > 64 do\n    step = step / 2\n    outer_reductions:insert(quote\n      if [tid] < step then\n        [generate_element_reductions(`([shared_mem_ptr][ [tid] ]),\n                                     `([shared_mem_ptr][ [tid] + [step] ]),\n                                     red_op, type, false)]\n      end\n      barrier()\n    end)\n  end\n  local unrolled_reductions = terralib.newlist()\n  while step > 1 do\n    step = step / 2\n    unrolled_reductions:insert(quote\n      [generate_element_reductions(`([shared_mem_ptr][ [tid] ]),\n                                   `([shared_mem_ptr][ [tid] + [step] ]),\n                                   red_op, type, false)]\n      barrier()\n    end)\n  end\n  if #outer_reductions > 0 then\n    return quote\n      [outer_reductions]\n      if [tid] < 32 then\n        [unrolled_reductions]\n      end\n    end\n  else\n    return quote\n      [unrolled_reductions]\n    end\n  end\nend\n\nfunction cudahelper.generate_reduction_kernel(cx, reductions, device_ptrs_map)\n  local preamble = terralib.newlist()\n  local postamble = terralib.newlist()\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local shared_mem_ptr =\n      cudalib.sharedmemory(red_var.type, THREAD_BLOCK_SIZE)\n    local init = base.reduction_op_init[red_op][red_var.type]\n    preamble:insert(quote\n      var [red_var] = [init]\n      [shared_mem_ptr][ tid_x() ] = [red_var]\n    end)\n\n    local tid = terralib.newsymbol(c.size_t, \"tid\")\n    local reduction_tree =\n      cudahelper.generate_reduction_tree(tid, shared_mem_ptr, THREAD_BLOCK_SIZE, red_op, red_var.type)\n    postamble:insert(quote\n      do\n        var [tid] = tid_x()\n        var bid = [cudahelper.global_block_id()]\n        [shared_mem_ptr][ [tid] ] = [red_var]\n        barrier()\n        [reduction_tree]\n        if [tid] == 0 then\n          [cudahelper.generate_atomic_update(red_op, red_var.type)](\n            &[device_ptr][bid % [GLOBAL_RED_BUFFER] ], [shared_mem_ptr][ [tid] ])\n        end\n      end\n    end)\n  end\n\n  preamble:insertall(cx:generate_preamble())\n  postamble:insertall(cx:generate_postamble())\n\n  return preamble, postamble\nend\n\nfunction cudahelper.generate_reduction_postamble(cx, reductions, device_ptrs_map, host_ptrs_map)\n  local postamble = quote end\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    local red_op = reductions[red_var]\n    local red_kernel_id = cudahelper.generate_buffer_reduction_kernel(red_var.type, red_op)\n    local host_ptr = host_ptrs_map[device_ptr]\n    local red_args = terralib.newlist({device_ptr, host_ptr})\n    local shared_mem_size = terralib.sizeof(red_var.type) * THREAD_BLOCK_SIZE\n    postamble = quote\n      [postamble];\n      [cudahelper.codegen_kernel_call(cx, red_kernel_id, THREAD_BLOCK_SIZE, red_args, shared_mem_size, true)]\n    end\n  end\n\n  local needs_sync = true\n  for device_ptr, red_var in pairs(device_ptrs_map) do\n    if needs_sync then\n      postamble = quote\n        [postamble];\n        RuntimeAPI.cudaDeviceSynchronize()\n      end\n      needs_sync = false\n    end\n    local red_op = reductions[red_var]\n    local host_ptr = host_ptrs_map[device_ptr]\n    postamble = quote\n      [postamble];\n      [red_var] = [base.quote_binary_op(red_op, red_var, `([host_ptr][0]))]\n    end\n  end\n\n  return postamble\nend\n\n-- #####################################\n-- ## Code generation for parallel prefix operators\n-- #################\n\nlocal NUM_BANKS = 16\nlocal bank_offset = macro(function(e)\n  return `(e / [NUM_BANKS])\nend)\n\nlocal function generate_prefix_op_kernel(shmem, tid, num_leaves, op, init, left_to_right)\n  return quote\n    do\n      var oa = 2 * [tid] + 1\n      var ob = 2 * [tid] + 2 * [left_to_right]\n      var d : int = [num_leaves] >> 1\n      var offset = 1\n      while d > 0 do\n        barrier()\n        if [tid]  < d then\n          var ai : int = offset * oa - [left_to_right]\n          var bi : int = offset * ob - [left_to_right]\n          ai = ai + bank_offset(ai)\n          bi = bi + bank_offset(bi)\n          [shmem][bi] = [base.quote_binary_op(op, `([shmem][ai]), `([shmem][bi]))]\n        end\n        offset = offset << 1\n        d = d >> 1\n      end\n      if [tid] == 0 then\n        var idx = ([num_leaves] - [left_to_right]) % [num_leaves]\n        [shmem][idx + bank_offset(idx)] = [init]\n      end\n      d = 1\n      while d <= [num_leaves] do\n        offset = offset >> 1\n        barrier()\n        if [tid] < d and offset > 0 then\n          var ai = offset * oa - [left_to_right]\n          var bi = offset * ob - [left_to_right]\n          ai = ai + bank_offset(ai)\n          bi = bi + bank_offset(bi)\n          var x = [shmem][ai]\n          [shmem][ai] = [shmem][bi]\n          [shmem][bi] = [base.quote_binary_op(op, x, `([shmem][bi]))]\n        end\n        d = d << 1\n      end\n      barrier()\n    end\n  end\nend\n\nlocal function generate_prefix_op_prescan(shmem, lhs, rhs, lhs_ptr, rhs_ptr, res, idx, dir, op, init)\n  local prescan_full, prescan_arbitrary\n  local NUM_LEAVES = THREAD_BLOCK_SIZE * 2\n\n  local function advance_ptrs(lhs_ptr, rhs_ptr, bid)\n    if lhs_ptr == rhs_ptr then\n      return quote\n        [lhs_ptr] = &([lhs_ptr][ bid * [NUM_LEAVES] ])\n      end\n    else\n      return quote\n        [lhs_ptr] = &([lhs_ptr][ bid * [NUM_LEAVES] ])\n        [rhs_ptr] = &([rhs_ptr][ bid * [NUM_LEAVES] ])\n      end\n    end\n  end\n\n  terra prescan_full([lhs_ptr],\n                     [rhs_ptr],\n                     [dir])\n    var [idx]\n    var t = tid_x()\n    var bid = [cudahelper.global_block_id()]\n    [advance_ptrs(lhs_ptr, rhs_ptr, bid)]\n    var lr = [int]([dir] >= 0)\n\n    [idx].__ptr = t\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n    [idx].__ptr = [idx].__ptr + [THREAD_BLOCK_SIZE]\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n\n    [generate_prefix_op_kernel(shmem, t, NUM_LEAVES, op, init, lr)]\n\n    var [res]\n    [idx].__ptr = t\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n    [idx].__ptr = [idx].__ptr + [THREAD_BLOCK_SIZE]\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n  end\n\n  terra prescan_arbitrary([lhs_ptr],\n                          [rhs_ptr],\n                          num_elmts : c.size_t,\n                          num_leaves : c.size_t,\n                          [dir])\n    var [idx]\n    var t = tid_x()\n    var lr = [int]([dir] >= 0)\n\n    [idx].__ptr = t\n    [rhs.actions]\n    [shmem][ [idx].__ptr + bank_offset([idx].__ptr)] = [rhs.value]\n    [idx].__ptr = [idx].__ptr + (num_leaves / 2)\n    if [idx].__ptr < num_elmts then\n      [rhs.actions]\n      [shmem][ [idx].__ptr + bank_offset([idx].__ptr) ] = [rhs.value]\n    else\n      [shmem][ [idx].__ptr + bank_offset([idx].__ptr) ] = [init]\n    end\n\n    [generate_prefix_op_kernel(shmem, t, num_leaves, op, init, lr)]\n\n    var [res]\n    [idx].__ptr = t\n    [rhs.actions]\n    [res] = [base.quote_binary_op(op,\n        `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n    [lhs.actions]\n    [idx].__ptr = [idx].__ptr + (num_leaves / 2)\n    if [idx].__ptr < num_elmts then\n      [rhs.actions]\n      [res] = [base.quote_binary_op(op,\n          `([shmem][ [idx].__ptr + bank_offset([idx].__ptr) ]), rhs.value)]\n      [lhs.actions]\n    end\n  end\n\n  return prescan_full, prescan_arbitrary\nend\n\nlocal function generate_prefix_op_scan(shmem, lhs_wr, lhs_rd, lhs_ptr, res, idx, dir, op, init)\n  local scan_full, scan_arbitrary\n  local NUM_LEAVES = THREAD_BLOCK_SIZE * 2\n\n  terra scan_full([lhs_ptr],\n                  offset : uint64,\n                  [dir])\n    var [idx]\n    var t = tid_x()\n    var bid = [cudahelper.global_block_id()]\n    [lhs_ptr] = &([lhs_ptr][ bid * [offset] * [NUM_LEAVES] ])\n    var lr = [int]([dir] >= 0)\n\n    var tidx = t\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n    tidx = tidx + [THREAD_BLOCK_SIZE]\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n\n    [generate_prefix_op_kernel(shmem, t, NUM_LEAVES, op, init, lr)]\n\n    var [res]\n    tidx = t\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n    [lhs_wr.actions]\n    tidx = tidx + [THREAD_BLOCK_SIZE]\n    [idx].__ptr = (tidx + lr) * [offset] - lr\n    [lhs_rd.actions]\n    [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n    [lhs_wr.actions]\n  end\n\n  terra scan_arbitrary([lhs_ptr],\n                       num_elmts : c.size_t,\n                       num_leaves : c.size_t,\n                       offset : c.size_t,\n                       [dir])\n    var [idx]\n    var t = tid_x()\n    var lr = [int]([dir] >= 0)\n\n    if lr == 1 then\n      var tidx = t\n      [idx].__ptr = (tidx + 1) * [offset] - 1\n      [lhs_rd.actions]\n      [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n\n      tidx = t + num_leaves / 2\n      if tidx < [num_elmts] then\n        [idx].__ptr = (tidx + 1) * [offset] - 1\n        [lhs_rd.actions]\n        [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      else\n        [shmem][tidx + bank_offset(tidx)] = [init]\n      end\n    else\n      var tidx = t\n      [idx].__ptr = tidx * [offset]\n      [lhs_rd.actions]\n      [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      tidx = t + num_leaves / 2\n      if tidx < [num_elmts] then\n        [idx].__ptr = tidx * [offset]\n        [lhs_rd.actions]\n        [shmem][tidx + bank_offset(tidx)] = [lhs_rd.value]\n      else\n        [shmem][tidx + bank_offset(tidx)] = [init]\n      end\n    end\n\n    [generate_prefix_op_kernel(shmem, t, num_leaves, op, init, lr)]\n\n    var [res]\n    if lr == 1 then\n      var tidx = t\n      [idx].__ptr = (tidx + 1) * [offset] - 1\n      [lhs_rd.actions]\n      [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n      [lhs_wr.actions]\n      tidx = tidx + num_leaves / 2\n      if [tidx] < [num_elmts] then\n        [idx].__ptr = (tidx + 1) * [offset] - 1\n        [lhs_rd.actions]\n        [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n        [lhs_wr.actions]\n      end\n    else\n      var tidx = t\n      [idx].__ptr = tidx * [offset]\n      [lhs_rd.actions]\n      [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n      [lhs_wr.actions]\n      tidx = tidx + num_leaves / 2\n      if [tidx] < [num_elmts] then\n        [idx].__ptr = tidx * [offset]\n        [lhs_rd.actions]\n        [res] = [base.quote_binary_op(op, `([shmem][tidx + bank_offset(tidx)]), lhs_rd.value)]\n        [lhs_wr.actions]\n      end\n    end\n  end\n\n  return scan_full, scan_arbitrary\nend\n\n-- This function expects lhs and rhs to be the values from the following expressions.\n--\n--   * lhs: lhs[idx] = res\n--   * rhs: rhs[idx]\n--\n-- The code generator below captures 'idx' and 'res' to change the meaning of these values\nfunction cudahelper.generate_prefix_op_kernels(lhs_wr, lhs_rd, rhs, lhs_ptr, rhs_ptr,\n                                               res, idx, dir, op, elem_type)\n  local BLOCK_SIZE = THREAD_BLOCK_SIZE * 2\n  local shmem = cudalib.sharedmemory(elem_type, BLOCK_SIZE)\n  local init = base.reduction_op_init[op][elem_type]\n\n  local prescan_full, prescan_arbitrary =\n    generate_prefix_op_prescan(shmem, lhs_wr, rhs, lhs_ptr, rhs_ptr, res, idx, dir, op, init)\n\n  local scan_full, scan_arbitrary =\n    generate_prefix_op_scan(shmem, lhs_wr, lhs_rd, lhs_ptr, res, idx, dir, op, init)\n\n  local terra postscan_full([lhs_ptr],\n                            offset : uint64,\n                            num_elmts : uint64,\n                            [dir])\n    var t = [cudahelper.global_thread_id()]\n    if t >= num_elmts - [BLOCK_SIZE] or t % [BLOCK_SIZE] == [BLOCK_SIZE - 1] then return end\n\n    var sum_loc = t / [BLOCK_SIZE] * [BLOCK_SIZE] + [BLOCK_SIZE - 1]\n    var val_loc = t + [BLOCK_SIZE]\n    var [idx], [res]\n    if [dir] >= 0 then\n      [idx].__ptr = sum_loc * [offset] + ([offset] - 1)\n      [lhs_rd.actions]\n      var v1 = [lhs_rd.value]\n\n      [idx].__ptr = val_loc * [offset] + ([offset] - 1)\n      [lhs_rd.actions]\n      var v2 = [lhs_rd.value]\n\n      [res] = [base.quote_binary_op(op, v1, v2)]\n      [lhs_wr.actions]\n    else\n      var t = [cudahelper.global_thread_id()]\n      if t % [BLOCK_SIZE] == [BLOCK_SIZE - 1] then return end\n\n      [idx].__ptr = (num_elmts - 1 - sum_loc) * [offset]\n      [lhs_rd.actions]\n      var v1 = [lhs_rd.value]\n\n      [idx].__ptr = (num_elmts - 1 - val_loc) * [offset]\n      [lhs_rd.actions]\n      var v2 = [lhs_rd.value]\n\n      [res] = [base.quote_binary_op(op, v1, v2)]\n      [lhs_wr.actions]\n    end\n  end\n\n  return prescan_full, prescan_arbitrary, scan_full, scan_arbitrary, postscan_full\nend\n\nfunction cudahelper.generate_parallel_prefix_op(cx, variant, total, lhs_wr, lhs_rd, rhs, lhs_ptr,\n                                                rhs_ptr, res, idx, dir, op, elem_type)\n  local BLOCK_SIZE = THREAD_BLOCK_SIZE * 2\n  local SHMEM_SIZE = terralib.sizeof(elem_type) * THREAD_BLOCK_SIZE * 2\n\n  local pre_full, pre_arb, scan_full, scan_arb, post_full, post2, post3 =\n    cudahelper.generate_prefix_op_kernels(lhs_wr, lhs_rd, rhs, lhs_ptr, rhs_ptr,\n                                          res, idx, dir, op, elem_type)\n  local prescan_full_id = variant:add_cuda_kernel(pre_full)\n  local prescan_arb_id = variant:add_cuda_kernel(pre_arb)\n  local scan_full_id = variant:add_cuda_kernel(scan_full)\n  local scan_arb_id = variant:add_cuda_kernel(scan_arb)\n  local postscan_full_id = variant:add_cuda_kernel(post_full)\n\n  local num_leaves = terralib.newsymbol(c.size_t, \"num_leaves\")\n  local num_elmts = terralib.newsymbol(c.size_t, \"num_elmts\")\n  local num_threads = terralib.newsymbol(c.size_t, \"num_threads\")\n  local offset = terralib.newsymbol(uint64, \"offset\")\n  local lhs_ptr_arg = terralib.newsymbol(lhs_ptr.type, lhs_ptr.name)\n  local rhs_ptr_arg = terralib.newsymbol(rhs_ptr.type, rhs_ptr.name)\n\n  local prescan_full_args = terralib.newlist()\n  prescan_full_args:insertall({lhs_ptr_arg, rhs_ptr_arg, dir})\n  local call_prescan_full =\n    cudahelper.codegen_kernel_call(cx, prescan_full_id, num_threads, prescan_full_args, SHMEM_SIZE, true)\n\n  local prescan_arb_args = terralib.newlist()\n  prescan_arb_args:insertall({lhs_ptr_arg, rhs_ptr_arg, num_elmts, num_leaves, dir})\n  local call_prescan_arbitrary =\n    cudahelper.codegen_kernel_call(cx, prescan_arb_id, num_threads, prescan_arb_args, SHMEM_SIZE, true)\n\n  local scan_full_args = terralib.newlist()\n  scan_full_args:insertall({lhs_ptr_arg, offset, dir})\n  local call_scan_full =\n    cudahelper.codegen_kernel_call(cx, scan_full_id, num_threads, scan_full_args, SHMEM_SIZE, true)\n\n  local scan_arb_args = terralib.newlist()\n  scan_arb_args:insertall({lhs_ptr_arg, num_elmts, num_leaves, offset, dir})\n  local call_scan_arbitrary =\n    cudahelper.codegen_kernel_call(cx, scan_arb_id, num_threads, scan_arb_args, SHMEM_SIZE, true)\n\n  local postscan_full_args = terralib.newlist()\n  postscan_full_args:insertall({lhs_ptr, offset, num_elmts, dir})\n  local call_postscan_full =\n    cudahelper.codegen_kernel_call(cx, postscan_full_id, num_threads, postscan_full_args, 0, true)\n\n  local terra recursive_scan :: {uint64,uint64,uint64,lhs_ptr.type,dir.type} -> {}\n\n  terra recursive_scan(remaining : uint64,\n                       [offset],\n                       [total],\n                       [lhs_ptr],\n                       [dir])\n    if remaining <= 1 then return end\n\n    var num_blocks : uint64 = remaining / [BLOCK_SIZE]\n\n    if num_blocks > 0 then\n      var [num_threads] = num_blocks * [THREAD_BLOCK_SIZE]\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = [lhs_ptr]\n      else\n        [lhs_ptr_arg] = &[lhs_ptr][(remaining % [BLOCK_SIZE]) * [offset]]\n      end\n      [call_scan_full]\n    end\n    if remaining % [BLOCK_SIZE] > 0 then\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = &[lhs_ptr][ num_blocks * [BLOCK_SIZE] * [offset] ]\n      else\n        [lhs_ptr_arg] = [lhs_ptr]\n      end\n      var [num_elmts] = remaining % [BLOCK_SIZE]\n      var [num_leaves] = [BLOCK_SIZE]\n      while [num_leaves] / 2 > [num_elmts] do\n        [num_leaves] = [num_leaves] / 2\n      end\n      var [num_threads] = [num_leaves] / 2\n      [call_scan_arbitrary]\n    end\n\n    var [lhs_ptr_arg]\n    if [dir] >= 0 then\n      [lhs_ptr_arg] = [lhs_ptr]\n    else\n      [lhs_ptr_arg] = &[lhs_ptr][ (remaining % [BLOCK_SIZE]) * [offset] ]\n    end\n\n    recursive_scan(num_blocks,\n                   [offset] * [BLOCK_SIZE],\n                   [total],\n                   [lhs_ptr_arg],\n                   [dir])\n\n    if [remaining] > [BLOCK_SIZE] then\n      var [num_elmts] = remaining\n      var [num_threads] = remaining - [BLOCK_SIZE]\n      [call_postscan_full]\n    end\n  end\n\n\n  local launch = quote\n    do\n      var num_blocks : uint64 = total / [BLOCK_SIZE]\n      if num_blocks > 0 then\n        var [lhs_ptr_arg]\n        var [rhs_ptr_arg]\n        var [num_threads] = num_blocks * [THREAD_BLOCK_SIZE]\n        if [dir] >= 0 then\n          [lhs_ptr_arg] = [lhs_ptr]\n          [rhs_ptr_arg] = [rhs_ptr]\n        else\n          [lhs_ptr_arg] = &[lhs_ptr][ total % [BLOCK_SIZE] ]\n          [rhs_ptr_arg] = &[rhs_ptr][ total % [BLOCK_SIZE] ]\n        end\n        [call_prescan_full]\n      end\n      if total % [BLOCK_SIZE] > 0 then\n        var [lhs_ptr_arg]\n        var [rhs_ptr_arg]\n        if [dir] >= 0 then\n          [lhs_ptr_arg] = &[lhs_ptr][ num_blocks * [BLOCK_SIZE] ]\n          [rhs_ptr_arg] = &[rhs_ptr][ num_blocks * [BLOCK_SIZE] ]\n        else\n          [lhs_ptr_arg] = [lhs_ptr]\n          [rhs_ptr_arg] = [rhs_ptr]\n        end\n        var [num_elmts] = total % [BLOCK_SIZE]\n        var [num_leaves] = [BLOCK_SIZE]\n        while [num_leaves] / 2 > [num_elmts] do\n          [num_leaves] = [num_leaves] / 2\n        end\n        var [num_threads] = [num_leaves] / 2\n        [call_prescan_arbitrary]\n      end\n\n      var [lhs_ptr_arg]\n      if [dir] >= 0 then\n        [lhs_ptr_arg] = [lhs_ptr]\n      else\n        [lhs_ptr_arg] = &[lhs_ptr][ total % [BLOCK_SIZE] ]\n      end\n\n      recursive_scan(total / [BLOCK_SIZE],\n                     [BLOCK_SIZE],\n                     [total],\n                     [lhs_ptr_arg],\n                     [dir])\n\n      if total > [BLOCK_SIZE] then\n        var [offset] = 1\n        var [num_elmts] = total\n        var [num_threads] = total - [BLOCK_SIZE]\n        [call_postscan_full]\n      end\n    end\n  end\n\n  return launch\nend\n\nfunction cudahelper.codegen_kernel_call(cx, kernel_id, count, args, shared_mem_size, tight)\n  local setupArguments = terralib.newlist()\n\n  local offset = 0\n  for i = 1, #args do\n    local arg =  args[i]\n    local size = terralib.sizeof(arg.type)\n    setupArguments:insert(quote\n      ExecutionAPI.cudaSetupArgument(&[arg], size, offset)\n    end)\n    offset = offset + size\n  end\n\n  local grid = terralib.newsymbol(RuntimeAPI.dim3, \"grid\")\n  local block = terralib.newsymbol(RuntimeAPI.dim3, \"block\")\n  local num_blocks = terralib.newsymbol(int64, \"num_blocks\")\n\n  local function round_exp(v, n)\n    return `((v + (n - 1)) / n)\n  end\n\n  local launch_domain_init = nil\n  if not cx.use_2d_launch then\n    launch_domain_init = quote\n      if [count] <= THREAD_BLOCK_SIZE and tight then\n        [block].x, [block].y, [block].z = [count], 1, 1\n      else\n        [block].x, [block].y, [block].z = THREAD_BLOCK_SIZE, 1, 1\n      end\n      var [num_blocks] = [round_exp(count, THREAD_BLOCK_SIZE)]\n    end\n  else\n    launch_domain_init = quote\n      if [count] <= NUM_THREAD_X and tight then\n        [block].x, [block].y, [block].z = [count], NUM_THREAD_Y, 1\n      else\n        [block].x, [block].y, [block].z = NUM_THREAD_X, NUM_THREAD_Y, 1\n      end\n      var [num_blocks] = [round_exp(count, NUM_THREAD_X)]\n    end\n  end\n\n  launch_domain_init = quote\n    [launch_domain_init]\n    if [num_blocks] <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z = [num_blocks], 1, 1\n    elseif [count] / MAX_NUM_BLOCK <= MAX_NUM_BLOCK then\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, [round_exp(num_blocks, MAX_NUM_BLOCK)], 1\n    else\n      [grid].x, [grid].y, [grid].z =\n        MAX_NUM_BLOCK, MAX_NUM_BLOCK,\n        [round_exp(num_blocks, MAX_NUM_BLOCK, MAX_NUM_BLOCK)]\n    end\n  end\n\n  return quote\n    var [grid], [block]\n    [launch_domain_init]\n    ExecutionAPI.cudaConfigureCall([grid], [block], shared_mem_size, nil)\n    [setupArguments]\n    ExecutionAPI.cudaLaunch([&int8](kernel_id))\n  end\nend\n\nlocal function get_nv_fn_name(name, type)\n  assert(type:isfloat())\n  local nv_name = \"__nv_\" .. name\n\n  -- Okay. a little divergence from the C standard...\n  if name == \"isnan\" or name == \"isinf\" then\n    if type == double then\n      nv_name = nv_name .. \"d\"\n    else\n      nv_name = nv_name .. \"f\"\n    end\n  -- Seriously?\n  elseif name == \"finite\" then\n    if type == double then\n      nv_name = \"__nv_isfinited\"\n    else\n      nv_name = \"__nv_finitef\"\n    end\n  elseif type == float then\n    nv_name = nv_name .. \"f\"\n  end\n  return nv_name\nend\n\nlocal function get_cuda_definition(self)\n  if self:has_variant(\"cuda\") then\n    return self:get_variant(\"cuda\")\n  else\n    local fn_type = self.super:get_definition().type\n    local fn_name = get_nv_fn_name(self:get_name(), self:get_arg_type())\n    assert(fn_name ~= nil)\n    local fn = externcall_builtin(fn_name, fn_type)\n    self:set_variant(\"cuda\", fn)\n    return fn\n  end\nend\n\nfunction cudahelper.get_cuda_variant(math_fn)\n  return math_fn:override(get_cuda_definition)\nend\n\n-- #####################################\n-- ## CUDA Codegen Context\n-- #################\n\nlocal context = {}\n\nfunction context:__index(field)\n  local value = context[field]\n  if value ~= nil then\n    return value\n  end\n  error(\"context has no field '\" .. field .. \"' (in lookup)\", 2)\nend\n\nfunction context:__newindex(field, value)\n  error(\"context has no field '\" .. field .. \"' (in assignment)\", 2)\nend\n\nfunction context.new(use_2d_launch, offset_2d)\n  local offset_2d = offset_2d or false\n  return setmetatable({\n    use_2d_launch = use_2d_launch,\n    offset_2d = offset_2d,\n    buffered_reductions = data.newmap(),\n  }, context)\nend\n\nfunction context:reduction_buffer(ref_type, value_type, op, generator)\n  local tbl = self.buffered_reductions[ref_type]\n  if tbl == nil then\n    tbl = {\n      buffer = cudalib.sharedmemory(value_type, THREAD_BLOCK_SIZE),\n      type = value_type,\n      op = op,\n      generator = generator,\n    }\n    self.buffered_reductions[ref_type] = tbl\n  end\n  return tbl\nend\n\nfunction context:compute_reduction_buffer_size()\n  local size = 0\n  for k, tbl in self.buffered_reductions:items() do\n    size = size + sizeof(tbl.type) * THREAD_BLOCK_SIZE\n  end\n  return size\nend\n\nfunction context:generate_preamble()\n  local preamble = terralib.newlist()\n\n  if self.use_2d_launch then\n    preamble:insert(quote\n      var [self.offset_2d:getsymbol()] = tid_y()\n    end)\n  end\n\n  for k, tbl in self.buffered_reductions:items() do\n    if tbl.type:isarray() then\n      local init = base.reduction_op_init[tbl.op][tbl.type.type]\n      preamble:insert(quote\n        for k = 0, [tbl.type.N] do\n          [tbl.buffer][ tid_y() + tid_x() * [NUM_THREAD_Y] ][k] = [init]\n        end\n      end)\n    else\n      local init = base.reduction_op_init[tbl.op][tbl.type]\n      preamble:insert(quote\n        [tbl.buffer][ tid_y() + tid_x() * [NUM_THREAD_Y] ] = [init]\n      end)\n    end\n  end\n\n  return preamble\nend\n\nfunction context:generate_postamble()\n  local postamble = terralib.newlist()\n\n  for k, tbl in self.buffered_reductions:items() do\n    postamble:insert(quote\n      do\n        var tid = tid_y()\n        var buf = &[tbl.buffer][ tid_x() * [NUM_THREAD_Y] ]\n        barrier()\n        [cudahelper.generate_reduction_tree(tid, buf, NUM_THREAD_Y, tbl.op, tbl.type)]\n        if tid == 0 then [tbl.generator(`(@buf))] end\n      end\n    end)\n  end\n\n  return postamble\nend\n\nlocal function check_2d_launch_profitable(node)\n  if not base.config[\"cuda-2d-launch\"] or not node:is(ast.typed.stat.ForList) then\n    return false, false\n  end\n  -- TODO: This is a very simple heurstic that does not even extend to 3D case.\n  --       At least we need to check if the inner loop has any centered accesses with\n  --       respect to that loop. In the longer term, we need a better algorithm to detect\n  --       cases where multi-dimensional kernel launches are profitable.\n  if #node.block.stats == 1 and node.block.stats[1]:is(ast.typed.stat.ForNum) then\n    local inner_loop = node.block.stats[1]\n    if inner_loop.metadata and inner_loop.metadata.parallelizable then\n      assert(#inner_loop.values == 2)\n      return true, base.newsymbol(inner_loop.symbol:gettype(), \"offset\")\n    end\n  end\n  return false, false\nend\n\nfunction cudahelper.new_kernel_context(node)\n  local use_2d_launch, offset_2d = check_2d_launch_profitable(node)\n  return context.new(use_2d_launch, offset_2d)\nend\n\nfunction cudahelper.optimize_loop(cx, node, block)\n  if cx.use_2d_launch then\n    local inner_loop = block.stats[1]\n    local index_type = inner_loop.symbol:gettype()\n    -- If the inner loop is eligible to a 2D kernel launch, we change the stride of the inner\n    -- loop accordingly.\n    inner_loop = inner_loop {\n      values = terralib.newlist({\n        ast.typed.expr.Binary {\n          op = \"+\",\n          lhs = inner_loop.values[1],\n          rhs = ast.typed.expr.ID {\n            value = cx.offset_2d,\n            expr_type = index_type,\n            annotations = ast.default_annotations(),\n            span = inner_loop.span,\n          },\n          expr_type = inner_loop.values[1].expr_type,\n          annotations = ast.default_annotations(),\n          span = inner_loop.span,\n        },\n        inner_loop.values[2],\n        ast.typed.expr.Constant {\n          value = NUM_THREAD_Y,\n          expr_type = index_type,\n          annotations = ast.default_annotations(),\n          span = inner_loop.span,\n        }\n      })\n    }\n    block = block { stats = terralib.newlist({ inner_loop }) }\n  end\n  return block\nend\n\nfunction cudahelper.generate_region_reduction(cx, loop_symbol, node, rhs, lhs_type, value_type, gen)\n  if cx.use_2d_launch then\n    local needs_buffer = base.types.is_ref(lhs_type) and\n                         (value_type:isprimitive() or value_type:isarray()) and\n                         node.metadata and\n                         node.metadata.centers and\n                         node.metadata.centers:has(loop_symbol)\n    if needs_buffer then\n      local buffer = cx:reduction_buffer(lhs_type, value_type, node.op, gen).buffer\n      return quote\n        do\n          var idx = tid_y() + tid_x() * [NUM_THREAD_Y]\n          [generate_element_reductions(`([buffer][ [idx] ]), rhs, node.op, value_type, false)]\n        end\n      end\n    else\n      return gen(rhs)\n    end\n  else\n    return gen(rhs)\n  end\nend\n\nreturn cudahelper\n",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/language/src/regent/openmphelper.t": "-- Copyright 2020 Stanford University\n--\n-- Licensed under the Apache License, Version 2.0 (the \"License\");\n-- you may not use this file except in compliance with the License.\n-- You may obtain a copy of the License at\n--\n--     http://www.apache.org/licenses/LICENSE-2.0\n--\n-- Unless required by applicable law or agreed to in writing, software\n-- distributed under the License is distributed on an \"AS IS\" BASIS,\n-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-- See the License for the specific language governing permissions and\n-- limitations under the License.\n\nlocal base = require(\"regent/std_base\")\nlocal std = require(\"regent/std\")\n\nlocal omp = {}\n\n-- Exit early if the user turned off OpenMP code generation\n\nif std.config[\"openmp\"] == 0 then\n  function omp.check_openmp_available()\n    return false\n  end\n  return omp\nend\n\nlocal has_openmp = true\nif not std.config[\"openmp-offline\"] then\n  local dlfcn = terralib.includec(\"dlfcn.h\")\n  local terra find_openmp_symbols()\n    var lib = dlfcn.dlopen([&int8](0), dlfcn.RTLD_LAZY)\n    var has_openmp =\n      dlfcn.dlsym(lib, \"GOMP_parallel\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_num_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_max_threads\") ~= [&opaque](0) and\n      dlfcn.dlsym(lib, \"omp_get_thread_num\") ~= [&opaque](0)\n    dlfcn.dlclose(lib)\n    return has_openmp\n  end\n  has_openmp = find_openmp_symbols()\nend\n\nif not has_openmp then\n  function omp.check_openmp_available()\n    return false, \"Regent is installed without OpenMP support\"\n  end\n  if std.config[\"openmp\"] == 1 then\n    local available, message = omp.check_openmp_available()\n    print(\"OpenMP code generation failed since \" .. message)\n    os.exit(-1)\n  end\n\nelse\n  omp.check_openmp_available = function() return true end\n  local omp_abi = terralib.includecstring [[\n    extern int omp_get_num_threads(void);\n    extern int omp_get_max_threads(void);\n    extern int omp_get_thread_num(void);\n    extern void GOMP_parallel(void (*fnptr)(void *data), void *data, int nthreads, unsigned flags);\n  ]]\n\n  omp.get_num_threads = omp_abi.omp_get_num_threads\n  omp.get_max_threads = omp_abi.omp_get_max_threads\n  omp.get_thread_num = omp_abi.omp_get_thread_num\n  omp.launch = omp_abi.GOMP_parallel\nend\n\n-- TODO: This might not be the right size in platforms other than x86\nomp.CACHE_LINE_SIZE = 64\n\nlocal FAST_ATOMICS = {\n  [\"+\"] = \"add\",\n  [\"-\"] = \"sub\",\n}\n\nomp.generate_atomic_update = terralib.memoize(function(op, typ)\n  -- Build a C wrapper to use atomic intrinsics in LLVM\n  local atomic_update = nil\n  local op_name = base.reduction_ops[op].name\n  assert(op_name ~= nil)\n  -- Integer types\n  if typ:isintegral() then\n    local ctype = typ.cachedcstring or typ:cstring()\n    assert(ctype ~= nil)\n    -- If there is a native support for the operation, use it directly\n    if FAST_ATOMICS[op] ~= nil then\n      local fun_name = string.format(\"__atomic_update_%s_%s\", op_name, ctype)\n      local C = terralib.includecstring(string.format([[\n        #include <stdint.h>\n        inline void %s(%s *address, %s val) {\n          __sync_fetch_and_%s(address, val);\n        }\n      ]], fun_name, ctype, ctype, FAST_ATOMICS[op]))\n      terra atomic_update(address : &typ, val : typ)\n        [ C[fun_name] ](address, val)\n      end\n    else\n      local fun_name = string.format(\"__compare_and_swap_%s_%s\", op_name, ctype)\n      local C = terralib.includecstring(string.format([[\n        #include <stdint.h>\n        inline %s %s(%s *address, %s old, %s new) {\n          return __sync_val_compare_and_swap(address, old, new);\n        }\n      ]], ctype, fun_name, ctype, ctype, ctype))\n      terra atomic_update(address : &typ, val : typ)\n        var success = false\n        while not success do\n          var old = @address\n          var new = [std.quote_binary_op(op, old, val)]\n          var res = [ C[fun_name] ](address, old, new)\n          success = res == old\n        end\n      end\n    end\n  else\n    local size = terralib.sizeof(typ) * 8\n    local cas_type = _G[\"uint\" .. tostring(size)]\n    local ctype = typ.cachedcstring or typ:cstring()\n    local cas_ctype = cas_type.cachedcstring or cas_type:cstring()\n    local fun_name = string.format(\"__compare_and_swap_%s_%s\", op_name, ctype)\n    local C = terralib.includecstring(string.format([[\n      #include <stdint.h>\n      inline %s %s(%s *address, %s old, %s new) {\n        return __sync_val_compare_and_swap(address, old, new);\n      }\n    ]], cas_ctype, fun_name, cas_ctype, cas_ctype, cas_ctype))\n    terra atomic_update(address : &typ, val : typ)\n      var success = false\n      while not success do\n        var old = @address\n        var new = [std.quote_binary_op(op, old, val)]\n\n        var address_b : &cas_type = [&cas_type](address)\n        var old_b : &cas_type = [&cas_type](&old)\n        var new_b : &cas_type = [&cas_type](&new)\n        var res : cas_type = [ C[fun_name] ](address_b, @old_b, @new_b)\n        success = res == @old_b\n      end\n    end\n  end\n  assert(atomic_update ~= nil)\n  atomic_update:setinlined(true)\n  return atomic_update\nend)\n\nfunction omp.generate_preamble(rect, idx, start_idx, end_idx)\n  return quote\n    var num_threads = [omp.get_num_threads]()\n    var thread_id = [omp.get_thread_num]()\n    var lo = [rect].lo.x[idx]\n    var hi = [rect].hi.x[idx] + 1\n    var chunk = (hi - lo + num_threads - 1) / num_threads\n    if chunk == 0 then chunk = 1 end\n    var [start_idx] = thread_id * chunk + lo\n    var [end_idx] = (thread_id + 1) * chunk + lo\n    if [end_idx] > hi then [end_idx] = hi end\n  end\nend\n\nfunction omp.generate_argument_type(symbols, reductions)\n  local arg_type = terralib.types.newstruct(\"omp_worker_arg\")\n  arg_type.entries = terralib.newlist()\n  local mapping = {}\n  for i, symbol in pairs(symbols) do\n    local field_name\n    if reductions[symbol] == nil then\n      field_name = \"_arg\" .. tostring(i)\n      arg_type.entries:insert({ field_name, symbol.type })\n    else\n      field_name = \"_red\" .. tostring(i)\n      arg_type.entries:insert({ field_name, &symbol.type })\n    end\n    mapping[field_name] = symbol\n  end\n  return arg_type, mapping\nend\n\nfunction omp.generate_argument_init(arg, arg_type, mapping, can_change, reductions)\n  local worker_init = arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      return quote var [symbol] = [init] end\n    else\n      return quote var [symbol] = [arg].[field_name] end\n    end\n  end)\n\n  local launch_init = terralib.newlist()\n  launch_init:insert(quote\n    var arg_obj : arg_type\n    var [arg] = &arg_obj\n  end)\n  local launch_update = terralib.newlist()\n\n  arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    if reductions[symbol] ~= nil then\n      local init = std.reduction_op_init[reductions[symbol]][symbol.type]\n      assert(field_type:ispointer())\n      launch_init:insert(quote\n        var num_threads = [omp.get_max_threads]()\n        -- We don't like false sharing\n        var size = num_threads  * omp.CACHE_LINE_SIZE\n        var data = std.c.malloc(size)\n        std.assert(size == 0 or data ~= nil, \"malloc failed in generate_argument_init\")\n        [arg].[field_name] = [field_type](data)\n        for i = 0, num_threads do\n          @[&symbol.type]([&int8](data) + i * omp.CACHE_LINE_SIZE) = [init]\n        end\n      end)\n    elseif not can_change[symbol] then\n      launch_init:insert(quote [arg].[field_name] = [symbol] end)\n    else\n      launch_update:insert(quote [arg].[field_name] = [symbol] end)\n    end\n  end)\n\n  return worker_init, launch_init, launch_update\nend\n\nfunction omp.generate_worker_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        do\n          var idx = [omp.get_thread_num]() * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [arg].[field_name][idx] = [std.quote_binary_op(op, symbol,\n            `([arg].[field_name][idx]))]\n        end\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nfunction omp.generate_launcher_cleanup(arg, arg_type, mapping, reductions)\n  return arg_type.entries:map(function(pair)\n    local field_name, field_type = unpack(pair)\n    local symbol = mapping[field_name]\n    local op = reductions[symbol]\n    if op ~= nil then\n      return quote\n        for i = 0, [omp.get_max_threads]() do\n          var idx = i * (omp.CACHE_LINE_SIZE / [sizeof(symbol.type)])\n          [symbol] = [std.quote_binary_op(op, symbol, `([arg].[field_name][idx]))]\n        end\n        std.c.free([arg].[field_name])\n      end\n    else\n      return quote end\n    end\n  end)\nend\n\nreturn omp\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/language/examples/pagerank/sample.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/language/examples/mssp/small/edges.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/language/examples/mssp/small/result_3.dat",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/doc/arch/persistent/hdf5/figs/high-level-design.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.png",
        "/tmp/vanessa/spack-stage/spack-stage-legion-20.03.0-565njrleujwewpm4jmtjcmi5n3rn45jd/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout.pptx"
    ],
    "total_files": 2039
}