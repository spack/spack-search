{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/io/bmi/bmi.c": "/*\n * (C) 2001 Clemson University and The University of Chicago\n *\n * See COPYING in top-level directory.\n */\n\n/** \\file\n *  \\ingroup bmiint\n *\n *  Top-level BMI network interface routines.\n */\n\n#include <errno.h>\n#include <string.h>\n#include <assert.h>\n#include <time.h>\n#include <sys/time.h>\n#include <stdio.h>\n\n#include \"bmi.h\"\n#include \"bmi-method-support.h\"\n#include \"bmi-method-callback.h\"\n#include \"gossip.h\"\n#include \"reference-list.h\"\n#include \"op-list.h\"\n#include \"gen-locks.h\"\n#include \"str-utils.h\"\n#include \"id-generator.h\"\n#include \"pvfs2-internal.h\"\n#include \"pvfs2-debug.h\"\n\nstatic int bmi_initialized_count = 0;\nstatic gen_mutex_t bmi_initialize_mutex = GEN_MUTEX_INITIALIZER;\n\n/*\n * List of BMI addrs currently managed.\n */\nstatic ref_list_p cur_ref_list = NULL;\n\n/* array to keep up with active contexts */\nstatic int context_array[BMI_MAX_CONTEXTS] = { 0 };\nstatic gen_mutex_t context_mutex = GEN_MUTEX_INITIALIZER;\nstatic gen_mutex_t ref_mutex = GEN_MUTEX_INITIALIZER;\n\nstatic QLIST_HEAD(forget_list);\nstatic gen_mutex_t forget_list_mutex = GEN_MUTEX_INITIALIZER;\n\nstruct forget_item\n{\n    struct qlist_head link;\n    BMI_addr_t addr;\n};\n\n/*\n * BMI trigger to reap all method resources for inactive addresses.\n */\nstatic QLIST_HEAD(bmi_addr_force_drop_list);\nstatic gen_mutex_t bmi_addr_force_drop_list_mutex = GEN_MUTEX_INITIALIZER;\nstruct drop_item\n{\n    struct qlist_head link;\n    char  *method_name;\n};\n\n/*\n * Static list of defined BMI methods.  These are pre-compiled into\n * the client libraries and into the server.\n */\n#ifdef __STATIC_METHOD_BMI_TCP__\nextern struct bmi_method_ops bmi_tcp_ops;\n#endif\n#ifdef __STATIC_METHOD_BMI_GM__\nextern struct bmi_method_ops bmi_gm_ops;\n#endif\n#ifdef __STATIC_METHOD_BMI_MX__\nextern struct bmi_method_ops bmi_mx_ops;\n#endif\n#ifdef __STATIC_METHOD_BMI_IB__\nextern struct bmi_method_ops bmi_ib_ops;\n#endif\n#ifdef __STATIC_METHOD_BMI_PORTALS__\nextern struct bmi_method_ops bmi_portals_ops;\n#endif\n#ifdef __STATIC_METHOD_BMI_ZOID__\nextern struct bmi_method_ops bmi_zoid_ops;\n#endif\n\nstatic struct bmi_method_ops *const static_methods[] = {\n#ifdef __STATIC_METHOD_BMI_TCP__\n    &bmi_tcp_ops,\n#endif\n#ifdef __STATIC_METHOD_BMI_GM__\n    &bmi_gm_ops,\n#endif\n#ifdef __STATIC_METHOD_BMI_MX__\n    &bmi_mx_ops,\n#endif\n#ifdef __STATIC_METHOD_BMI_IB__\n    &bmi_ib_ops,\n#endif\n#ifdef __STATIC_METHOD_BMI_PORTALS__\n    &bmi_portals_ops,\n#endif\n#ifdef __STATIC_METHOD_BMI_ZOID__\n    &bmi_zoid_ops,\n#endif\n    NULL\n};\n\n/*\n * List of \"known\" BMI methods.  This is dynamic, starting with\n * just the static ones above, and perhaps adding more if we turn\n * back on dynamic module loading.\n */\nstatic int known_method_count = 0;\nstatic struct bmi_method_ops **known_method_table = 0;\n\n/*\n * List of active BMI methods.  These are the ones that will be\n * dealt with for a test call, for example.  On a client, known methods\n * become active only when someone calls BMI_addr_lookup().  On\n * a server, all possibly active methods are known at startup time\n * because we listen on them for the duration.\n */\nstatic int active_method_count = 0;\nstatic gen_mutex_t active_method_count_mutex = GEN_MUTEX_INITIALIZER;\n\nstatic struct bmi_method_ops **active_method_table = NULL;\n\nstruct method_usage_t {\n    int iters_polled;  /* how many iterations since this method was polled */\n    int iters_active;  /* how many iterations since this method had action */\n    int plan;\n    int flags;\n};\n\nstatic struct method_usage_t * expected_method_usage = NULL;\nstatic struct method_usage_t * unexpected_method_usage = NULL;\n\nstatic const int usage_iters_starvation = 100000;\nstatic const int usage_iters_active = 10000;\nstatic int global_flags;\n\nstatic int activate_method(const char *name, const char *listen_addr,\n    int flags);\nstatic void bmi_addr_drop(ref_st_p tmp_ref);\nstatic void bmi_addr_force_drop(ref_st_p ref, ref_list_p ref_list);\nstatic void bmi_check_forget_list(void);\nstatic void bmi_check_addr_force_drop (void);\n\n/** Initializes the BMI layer.  Must be called before any other BMI\n *  functions.\n *\n *  \\param method_list a comma separated list of BMI methods to\n *         use\n *  \\param listen_addr a comma separated list of addresses to listen on\n *         for each method (if needed)\n *  \\param flags initialization flags\n *\n *  \\return 0 on success, -errno on failure\n */\nint BMI_initialize(const char *method_list,\n\t\t   const char *listen_addr,\n\t\t   int flags)\n{\n    int ret = -1;\n    int i = 0, j = 0;\n    char **requested_methods = NULL;\n    char **listen_addrs = NULL;\n    char *this_addr = NULL;\n    char *proto = NULL;\n    int addr_count = 0;\n\n    /* server must specify method list at startup, optional for client */\n    if (flags & BMI_INIT_SERVER) {\n\tif (!listen_addr || !method_list)\n\t    return bmi_errno_to_pvfs(-EINVAL);\n    } else {\n\tif (listen_addr)\n\t    return bmi_errno_to_pvfs(-EINVAL);\n\tif (flags) {\n\t    gossip_lerr(\"Warning: flags ignored on client.\\n\");\n\t}\n    }\n\n    gen_mutex_lock(&bmi_initialize_mutex);\n    if(bmi_initialized_count > 0)\n    {\n        /* Already initialized! Just increment ref count and return. */\n\t++bmi_initialized_count;\n\tgen_mutex_unlock(&bmi_initialize_mutex);\n        return 0;\n    }\n    ++bmi_initialized_count;\n    gen_mutex_unlock(&bmi_initialize_mutex);\n\n    global_flags = flags;\n\n    /* make sure that id generator is initialized if not already */\n    ret = id_gen_safe_initialize();\n    if(ret < 0)\n    {\n        gen_mutex_lock(&bmi_initialize_mutex);\n        --bmi_initialized_count;\n        gen_mutex_unlock(&bmi_initialize_mutex);\n        return(ret);\n    }\n\n    /* make a new reference list */\n    cur_ref_list = ref_list_new();\n    if (!cur_ref_list)\n    {\n\tret = bmi_errno_to_pvfs(-ENOMEM);\n\tgoto bmi_initialize_failure;\n    }\n\n    /* initialize the known method list from the null-terminated static list */\n    known_method_count = sizeof(static_methods) / sizeof(static_methods[0]) - 1;\n    known_method_table = malloc(\n\tknown_method_count * sizeof(*known_method_table));\n    if (!known_method_table)\n    {\n        gen_mutex_lock(&bmi_initialize_mutex);\n        --bmi_initialized_count;\n        gen_mutex_unlock(&bmi_initialize_mutex);\n\treturn bmi_errno_to_pvfs(-ENOMEM);\n    }\n    memcpy(known_method_table, static_methods,\n\tknown_method_count * sizeof(*known_method_table));\n\n    gen_mutex_lock(&active_method_count_mutex);\n    if (!method_list) {\n\t/* nothing active until lookup */\n\tactive_method_count = 0;\n    } else {\n\t/* split and initialize the requested method list */\n\tint numreq = PINT_split_string_list(&requested_methods, method_list);\n\tif (numreq < 1)\n\t{\n\t    gossip_lerr(\"Error: bad method list.\\n\");\n\t    ret = bmi_errno_to_pvfs(-EINVAL);\n\t    gen_mutex_unlock(&active_method_count_mutex);\n\t    goto bmi_initialize_failure;\n\t}\n\n\t/* Today is that day! */\n\taddr_count = PINT_split_string_list(&listen_addrs, listen_addr);\n\t\n\tfor (i=0; i<numreq; i++) {\n\n\t    /* assume the method name is bmi_<proto>, and find the <proto>\n\t     * part\n\t     */\n\t    proto = strstr(requested_methods[i], \"bmi_\");\n\t    if(!proto)\n\t    {\n\t        gossip_err(\"%s: Invalid method name: %s.  Method names \"\n\t\t\t   \"must start with 'bmi_'\\n\",\n\t\t\t   __func__, requested_methods[i]);\n\t\tret = -EINVAL;\n\t\tgen_mutex_unlock(&active_method_count_mutex);\n\t\tgoto bmi_initialize_failure;\n\t    }\n\t    proto += 4;\n\n\t    /* match the proper listen addr to the method */\n\t    for(j=0; j<addr_count; ++j)\n\t    {\n\t\t/* we don't want a strstr here in case the addr has\n\t\t * the proto as part of the hostname\n\t\t */\n\t\tif(!strncmp(listen_addrs[j], proto, strlen(proto)))\n\t\t{\n\t\t    /* found the right addr */\n\t\t    this_addr = listen_addrs[j];\n\t\t    break;\n\t\t}\n\t    }\n\t\t\n\t    if(!this_addr)\n\t    {\n\t\t/* couldn't find the right listen addr */\n\t\tgossip_err(\"%s: Failed to find an appropriate listening \"\n\t\t\t   \"address for the bmi method: %s\\n\",\n\t\t\t   __func__, requested_methods[i]);\n\t\tret = -EINVAL;\n\t\tgen_mutex_unlock(&active_method_count_mutex);\n\t\tgoto bmi_initialize_failure;\n\t    }\n\n\t    ret = activate_method(requested_methods[i], this_addr, flags);\n\t    if (ret < 0) {\n\t\tret = bmi_errno_to_pvfs(ret);\n\t\tgen_mutex_unlock(&active_method_count_mutex);\n\t\tgoto bmi_initialize_failure;\n\t    }\n\t    free(requested_methods[i]);\n\t}\n\tfree(requested_methods);\n\tif(listen_addrs)\n\t{\n\t    PINT_free_string_list(listen_addrs, addr_count);\n\t    listen_addrs = NULL;\n\t}\n    }\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    return (0);\n\n  bmi_initialize_failure:\n\n    /* kill reference list */\n    if (cur_ref_list)\n    {\n\tref_list_cleanup(cur_ref_list);\n        cur_ref_list = NULL;\n    }\n\n    gen_mutex_lock(&active_method_count_mutex);\n    /* look for loaded methods and shut down */\n    if (active_method_table)\n    {\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    if (active_method_table[i])\n\t    {\n\t\tactive_method_table[i]->finalize();\n\t    }\n\t}\n\tfree(active_method_table);\n    }\n\n    if (known_method_table) {\n\tfree(known_method_table);\n\tknown_method_count = 0;\n    }\n\n    /* get rid of method string list */\n    if (requested_methods)\n    {\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    if (requested_methods[i])\n\t    {\n\t\tfree(requested_methods[i]);\n\t    }\n\t}\n\tfree(requested_methods);\n    }\n\n    if(listen_addrs)\n    {\n\tPINT_free_string_list(listen_addrs, addr_count);\n    }\n\n    active_method_count = 0;\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    /* shut down id generator */\n    id_gen_safe_finalize();\n\n    gen_mutex_lock(&bmi_initialize_mutex);\n    --bmi_initialized_count;\n    gen_mutex_unlock(&bmi_initialize_mutex);\n\n    return (ret);\n}\n\n/* the following is the old BMI_initialize() function that used dl to\n * pull in method modules dynamically.  Just hanging around as an\n * example...\n */\n#if 0\n/* BMI_initialize()\n * \n * Initializes the BMI layer.  Must be called before any other BMI\n * functions.  module_string is a comma separated list of BMI modules to\n * use, listen_addr is a comma separated list of addresses to listen on\n * for each module (if needed), and flags are initialization flags.\n *\n * returns 0 on success, -errno on failure\n */\nint BMI_initialize(const char *module_string,\n\t\t   const char *listen_addr,\n\t\t   int flags)\n{\n\n    int ret = -1;\n    int i = 0;\n    char **modules = NULL;\n    void *meth_mod = NULL;\n    char *mod_error = NULL;\n    method_addr_p new_addr = NULL;\n    op_list_p olp = NULL;\n\n    /* TODO: this is a hack to make sure we get all of the symbols loaded\n     * into the library... is there a better way?\n     */\n    olp = op_list_new();\n    op_list_cleanup(olp);\n\n    if (((flags & BMI_INIT_SERVER) && (!listen_addr)) || !module_string)\n    {\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n    }\n\n    /* separate out the module list */\n    active_method_count = PINT_split_string_list(\n        &modules, module_string);\n    if (active_method_count < 1)\n    {\n\tgossip_lerr(\"Error: bad module list.\\n\");\n\tret = bmi_errno_to_pvfs(-EINVAL);\n\tgoto bmi_initialize_failure;\n    }\n\n    /* create a table to keep up with the method modules */\n    active_method_table = (struct bmi_method_ops **)malloc(\n        active_method_count * sizeof(struct bmi_method_ops *));\n    if (!active_method_table)\n    {\n\tret = bmi_errno_to_pvfs(-ENOMEM);\n\tgoto bmi_initialize_failure;\n    }\n\n    /* iterate through each method in the list and load its module */\n    for (i = 0; i < active_method_count; i++)\n    {\n\tmeth_mod = dlopen(modules[i], RTLD_NOW);\n\tif (!meth_mod)\n\t{\n\t    gossip_lerr(\"Error: could not open module: %s\\n\", dlerror());\n\t    ret = bmi_errno_to_pvfs(-EINVAL);\n\t    goto bmi_initialize_failure;\n\t}\n\tdlerror();\n\n\tactive_method_table[i] = (struct bmi_method_ops *)\n            dlsym(meth_mod, \"method_interface\");\n\tmod_error = dlerror();\n\tif (mod_error)\n\t{\n\t    gossip_lerr(\"Error: module load: %s\\n\", mod_error);\n\t    ret = bmi_errno_to_pvfs(-EINVAL);\n\t    goto bmi_initialize_failure;\n\t}\n    }\n\n    /* make a new reference list */\n    cur_ref_list = ref_list_new();\n    if (!cur_ref_list)\n    {\n\tret = bmi_errno_to_pvfs(-ENOMEM);\n\tgoto bmi_initialize_failure;\n    }\n\n    /* initialize methods */\n    for (i = 0; i < active_method_count; i++)\n    {\n\tif (flags & BMI_INIT_SERVER)\n\t{\n\t    if ((new_addr =\n\t\t active_method_table[i]->\n\t\t BMI_meth_method_addr_lookup(listen_addr)) != NULL)\n\t    {\n\t\t/* this is a bit of a hack */\n\t\tnew_addr->method_type = i;\n\t\tret = active_method_table[i]->BMI_meth_initialize(\n                    new_addr, i, flags);\n\t    }\n\t    else\n\t    {\n\t\tret = -1;\n\t    }\n\t}\n\telse\n\t{\n\t    ret = active_method_table[i]->BMI_meth_initialize(\n                NULL, i, flags);\n\t}\n\tif (ret < 0)\n\t{\n\t    gossip_lerr(\"Error: initializing module: %s\\n\", modules[i]);\n\t    goto bmi_initialize_failure;\n\t}\n    }\n\n    return (0);\n\n  bmi_initialize_failure:\n\n    /* kill reference list */\n    if (cur_ref_list)\n    {\n\tref_list_cleanup(cur_ref_list);\n    }\n\n    /* look for loaded methods and shut down */\n    if (active_method_table)\n    {\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    if (active_method_table[i])\n\t    {\n\t\tactive_method_table[i]->BMI_meth_finalize();\n\t    }\n\t}\n\tfree(active_method_table);\n    }\n\n    /* get rid of method string list */\n    if (modules)\n    {\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    if (modules[i])\n\t    {\n\t\tfree(modules[i]);\n\t    }\n\t}\n\tfree(modules);\n    }\n\n    return (ret);\n}\n#endif /* 0 */\n\n/** Shuts down the BMI layer.\n *\n * \\return 0.\n */\nint BMI_finalize(void)\n{\n    int i = -1;\n\n    gen_mutex_lock(&bmi_initialize_mutex);\n    --bmi_initialized_count;\n    if(bmi_initialized_count > 0)\n    {\n        gen_mutex_unlock(&bmi_initialize_mutex);\n        return 0;\n    }\n    gen_mutex_unlock(&bmi_initialize_mutex);\n\n    /* destroy the reference list */\n    /* (side effect: destroys all method addresses as well) */\n//    ref_list_cleanup(cur_ref_list);\n\n    gen_mutex_lock(&active_method_count_mutex);\n    /* attempt to shut down active methods */\n    for (i = 0; i < active_method_count; i++)\n    {\n\tactive_method_table[i]->finalize();\n    }\n    active_method_count = 0;\n    free(active_method_table);\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    free(known_method_table);\n    known_method_count = 0;\n\n    if (expected_method_usage)\n        free(expected_method_usage);\n\n    if (unexpected_method_usage)\n       free(unexpected_method_usage);\n\n    /* shut down id generator */\n    id_gen_safe_finalize();\n\n    return (0);\n}\n\n/** Creates a new context to be used for communication.  This can be\n *  used, for example, to distinguish between operations posted by\n *  different threads.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_open_context(bmi_context_id* context_id)\n{\n    int context_index;\n    int i;\n    int ret = 0;\n\n    gen_mutex_lock(&context_mutex);\n\n    /* find an unused context id */\n    for(context_index=0; context_index<BMI_MAX_CONTEXTS; context_index++)\n    {\n\tif(context_array[context_index] == 0)\n\t{\n\t    break;\n\t}\n    }\n\n    if(context_index >= BMI_MAX_CONTEXTS)\n    {\n\t/* we don't have any more available! */\n\tgen_mutex_unlock(&context_mutex);\n\treturn(bmi_errno_to_pvfs(-EBUSY));\n    }\n\n    gen_mutex_lock(&active_method_count_mutex);\n    /* tell all of the modules about the new context */\n    for (i = 0; i < active_method_count; i++)\n    {\n\tret = active_method_table[i]->open_context(\n            context_index);\n\tif(ret < 0)\n\t{\n\t    /*\n              one of them failed; kill this context in the previous\n              modules\n            */\n            --i;\n            while (i >= 0)\n\t    {\n\t\tactive_method_table[i]->close_context(\n                    context_index);\n                --i;\n\t    }\n\t    goto out;\n\t}\n    }\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    context_array[context_index] = 1;\n    *context_id = context_index;\n\nout:\n\n    gen_mutex_unlock(&context_mutex);\n    return(ret);\n}\n\n\n/** Destroys a context previous generated with BMI_open_context().\n */\nvoid BMI_close_context(bmi_context_id context_id)\n{\n    int i;\n\n    gen_mutex_lock(&context_mutex);\n\n    if(!context_array[context_id])\n    {\n\tgen_mutex_unlock(&context_mutex);\n\treturn;\n    }\n\n    /* tell all of the modules to get rid of this context */\n    gen_mutex_lock(&active_method_count_mutex);\n    for (i = 0; i < active_method_count; i++)\n    {\n\tactive_method_table[i]->close_context(context_id);\n    }\n    context_array[context_id] = 0;\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    gen_mutex_unlock(&context_mutex);\n    return;\n}\n\n\n/** Submits receive operations for subsequent service.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_post_recv(bmi_op_id_t * id,\n\t\t  BMI_addr_t src,\n\t\t  void *buffer,\n\t\t  bmi_size_t expected_size,\n\t\t  bmi_size_t * actual_size,\n\t\t  enum bmi_buffer_type buffer_type,\n\t\t  bmi_msg_tag_t tag,\n\t\t  void *user_ptr,\n\t\t  bmi_context_id context_id,\n                  bmi_hint hints)\n{\n    return(BMI_post_recv_list(id, src, &buffer, &expected_size, 1,\n        expected_size, actual_size, buffer_type, tag, user_ptr, context_id,\n        hints));\n}\n\n\n/** Submits send operations for subsequent service.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_post_send(bmi_op_id_t * id,\n\t\t  BMI_addr_t dest,\n\t\t  const void *buffer,\n\t\t  bmi_size_t size,\n\t\t  enum bmi_buffer_type buffer_type,\n\t\t  bmi_msg_tag_t tag,\n\t\t  void *user_ptr,\n\t\t  bmi_context_id context_id,\n                  bmi_hint hints)\n{\n    return(BMI_post_send_list(id, dest, &buffer, &size, 1, size,\n        buffer_type, tag, user_ptr, context_id, hints));\n}\n\n\n/** Submits unexpected send operations for subsequent service.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_post_sendunexpected_class(bmi_op_id_t * id,\n\t\t\t    BMI_addr_t dest,\n\t\t\t    const void *buffer,\n\t\t\t    bmi_size_t size,\n\t\t\t    enum bmi_buffer_type buffer_type,\n\t\t\t    bmi_msg_tag_t tag,\n                            uint8_t class,\n\t\t\t    void *user_ptr,\n\t\t\t    bmi_context_id context_id,\n                            bmi_hint hints)\n{\n    return(BMI_post_sendunexpected_list_class(id, dest, &buffer, &size, 1, size,\n        buffer_type, tag, class, user_ptr, context_id, hints));\n}\n\n\n/** Checks to see if a particular message has completed.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_test(bmi_op_id_t id,\n\t     int *outcount,\n\t     bmi_error_code_t * error_code,\n\t     bmi_size_t * actual_size,\n\t     void **user_ptr,\n\t     int max_idle_time_ms,\n\t     bmi_context_id context_id)\n{\n    struct method_op *target_op = NULL;\n    int ret = -1;\n\n    if (max_idle_time_ms < 0)\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n\n    *outcount = 0;\n\n    target_op = id_gen_fast_lookup(id);\n    assert(target_op->op_id == id);\n\n    ret = active_method_table[\n        target_op->addr->method_type]->test(\n            id, outcount, error_code, actual_size, user_ptr,\n            max_idle_time_ms, context_id);\n\n    /* return 1 if anything completed */\n    if (ret == 0 && *outcount == 1)\n    {\n\tgossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                     \"BMI_test completing: %llu\\n\", llu(id));\n\treturn (1);\n    }\n    return (ret);\n}\n\n\n/** Checks to see if any messages from the specified list have completed.\n *\n * \\return 0 on success, -errno on failure.\n *\n * XXX: never used.  May want to add adaptive polling strategy of testcontext\n * if it becomes used again.\n */\nint BMI_testsome(int incount,\n\t\t bmi_op_id_t * id_array,\n\t\t int *outcount,\n\t\t int *index_array,\n\t\t bmi_error_code_t * error_code_array,\n\t\t bmi_size_t * actual_size_array,\n\t\t void **user_ptr_array,\n\t\t int max_idle_time_ms,\n\t\t bmi_context_id context_id)\n{\n    int ret = 0;\n    int idle_per_method = 0;\n    bmi_op_id_t* tmp_id_array;\n    int i,j;\n    struct method_op *query_op;\n    int need_to_test;\n    int tmp_outcount;\n    int tmp_active_method_count;\n\n    gen_mutex_lock(&active_method_count_mutex);\n    tmp_active_method_count = active_method_count;\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    if (max_idle_time_ms < 0)\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n\n    *outcount = 0;\n\n    if (tmp_active_method_count == 1) {\n\t/* shortcircuit for perhaps common case of only one method */\n\tret = active_method_table[0]->testsome(\n\t    incount, id_array, outcount, index_array,\n\t    error_code_array, actual_size_array, user_ptr_array,\n\t    max_idle_time_ms, context_id);\n\n\t/* return 1 if anything completed */\n\tif (ret == 0 && *outcount > 0)\n\t    return (1);\n\telse\n\t    return ret;\n    }\n\n    /* TODO: do something more clever here */\n    if (max_idle_time_ms)\n    {\n\tidle_per_method = max_idle_time_ms / tmp_active_method_count;\n\tif (!idle_per_method)\n\t    idle_per_method = 1;\n    }\n\n    tmp_id_array = (bmi_op_id_t*)malloc(incount*sizeof(bmi_op_id_t));\n    if(!tmp_id_array)\n\treturn(bmi_errno_to_pvfs(-ENOMEM));\n\n    /* iterate over each active method */\n    for(i=0; i<tmp_active_method_count; i++)\n    {\n\t/* setup the tmp id array with only operations that match\n\t * that method\n\t */\n\tneed_to_test = 0;\n\tfor(j=0; j<incount; j++)\n\t{\n\t    if(id_array[j])\n\t    {\n\t\tquery_op = (struct method_op*)\n                    id_gen_fast_lookup(id_array[j]);\n\t\tassert(query_op->op_id == id_array[j]);\n\t\tif(query_op->addr->method_type == i)\n\t\t{\n\t\t    tmp_id_array[j] = id_array[j];\n\t\t    need_to_test++;\n\t\t}\n\t    }\n\t}\n\n\t/* call testsome if we found any ops for this method */\n\tif(need_to_test)\n\t{\n\t    tmp_outcount = 0;\n\t    ret = active_method_table[i]->testsome(\n\t\tneed_to_test, tmp_id_array, &tmp_outcount, \n\t\t&(index_array[*outcount]),\n\t\t&(error_code_array[*outcount]),\n\t\t&(actual_size_array[*outcount]),\n\t\tuser_ptr_array ? &(user_ptr_array[*outcount]) : 0,\n\t\tidle_per_method,\n\t\tcontext_id);\n\t    if(ret < 0)\n\t    {\n\t\t/* can't recover from this... */\n\t\tgossip_lerr(\"Error: critical BMI_testsome failure.\\n\");\n\t\tgoto out;\n\t    }\n\t    *outcount += tmp_outcount;\n\t}\n    }\n\n  out:\n    free(tmp_id_array);\n\n    if(ret == 0 && *outcount > 0)\n\treturn(1);\n    else\n\treturn(0);\n}\n\n\n/*\n * If some method was recently active, poll it again for speed,\n * but be sure not to starve any method.  If multiple active,\n * poll them all.  Return idle_time per method too.\n */\nstatic void\nconstruct_poll_plan(struct method_usage_t * method_usage,\n      int nmeth, int *idle_time_ms)\n{\n    int i, numplan;\n\n    numplan = 0;\n    for (i=0; i<nmeth; i++) {\n        ++method_usage[i].iters_polled;\n        ++method_usage[i].iters_active;\n        method_usage[i].plan = 0;\n        if ((method_usage[i].iters_active <= usage_iters_active) &&\n            (!(method_usage[i].flags & BMI_METHOD_FLAG_NO_POLLING))){\n            /* recently busy, poll */\n\t    if (0) gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                         \"%s: polling active meth %d: %d / %d\\n\", __func__, i,\n                         method_usage[i].iters_active, usage_iters_active);\n            method_usage[i].plan = 1;\n            ++numplan;\n            *idle_time_ms = 0;  /* busy polling */\n        } else if (method_usage[i].iters_polled >= usage_iters_starvation) {\n            /* starving, time to poke this one */\n\t    if (0) gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                         \"%s: polling starving meth %d: %d / %d\\n\", __func__, i,\n                         method_usage[i].iters_polled, usage_iters_starvation);\n            method_usage[i].plan = 1;\n            ++numplan;\n        } \n    }\n\n    /* if nothing is starving or busy, poll everybody */\n    if (numplan == 0) {\n        for (i=0; i<nmeth; i++)\n            method_usage[i].plan = 1;\n        numplan = nmeth;\n\n        /* spread idle time evenly */\n        if (*idle_time_ms) {\n            *idle_time_ms /= numplan;\n            if (*idle_time_ms == 0)\n                *idle_time_ms = 1;\n        }\n        /* note that BMI_testunexpected is always called with idle_time 0 */\n        if (0) gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                     \"%s: polling all %d methods, idle %d ms\\n\", __func__,\n                     numplan, *idle_time_ms);\n    }\n}\n\n\n/** Checks to see if any unexpected messages have completed.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_testunexpected_class(int incount,\n\t\t       int *outcount,\n\t\t       struct BMI_unexpected_info *info_array,\n                       uint8_t class, \n\t\t       int max_idle_time_ms)\n{\n    int i = 0;\n    int ret = -1;\n    int position = 0;\n    int tmp_outcount = 0;\n    struct bmi_method_unexpected_info sub_info[incount];\n    ref_st_p tmp_ref = NULL;\n    int tmp_active_method_count = 0;\n\n    /* figure out if we need to drop any stale addresses */\n    bmi_check_forget_list();\n    bmi_check_addr_force_drop();\n\n    gen_mutex_lock(&active_method_count_mutex);\n    tmp_active_method_count = active_method_count;\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    if (max_idle_time_ms < 0)\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n\n    *outcount = 0;\n\n    construct_poll_plan(unexpected_method_usage,\n          tmp_active_method_count, &max_idle_time_ms);\n\n    while (position < incount && i < tmp_active_method_count)\n    {\n        if (unexpected_method_usage[i].plan) {\n            ret = active_method_table[i]->testunexpected(\n                (incount - position), &tmp_outcount,\n                (&(sub_info[position])), class, max_idle_time_ms);\n            if (ret < 0)\n            {\n                /* can't recover from this */\n                gossip_lerr(\"Error: critical BMI_testunexpected failure.\\n\");\n                return (ret);\n            }\n            position += tmp_outcount;\n            (*outcount) += tmp_outcount;\n            unexpected_method_usage[i].iters_polled = 0;\n            if (ret)\n                unexpected_method_usage[i].iters_active = 0;\n        }\n\ti++;\n    }\n\n    for (i = 0; i < (*outcount); i++)\n    {\n\tinfo_array[i].error_code = sub_info[i].error_code;\n\tinfo_array[i].buffer = sub_info[i].buffer;\n\tinfo_array[i].size = sub_info[i].size;\n\tinfo_array[i].tag = sub_info[i].tag;\n\tgen_mutex_lock(&ref_mutex);\n\ttmp_ref = ref_list_search_method_addr(\n            cur_ref_list, sub_info[i].addr);\n\tif (!tmp_ref)\n\t{\n\t    /* yeah, right */\n\t    gossip_lerr(\"Error: critical BMI_testunexpected failure.\\n\");\n\t    gen_mutex_unlock(&ref_mutex);\n\t    return (bmi_errno_to_pvfs(-EPROTO));\n\t}\n        if(global_flags & BMI_AUTO_REF_COUNT)\n        {\n            tmp_ref->ref_count++;\n        }\n\tgen_mutex_unlock(&ref_mutex);\n\tinfo_array[i].addr = tmp_ref->bmi_addr;\n    }\n    /* return 1 if anything completed */\n    if (ret == 0 && *outcount > 0)\n    {\n\treturn (1);\n    }\n    return (0);\n}\n\n\n/** Checks to see if any messages from the specified context have\n *  completed.\n *\n *  \\returns 0 on success, -errno on failure.\n */\nint BMI_testcontext(int incount,\n\t\t    bmi_op_id_t* out_id_array,\n\t\t    int *outcount,\n\t\t    bmi_error_code_t * error_code_array,\n\t\t    bmi_size_t * actual_size_array,\n\t\t    void **user_ptr_array,\n\t\t    int max_idle_time_ms,\n\t\t    bmi_context_id context_id)\n{\n    int i = 0;\n    int ret = -1;\n    int position = 0;\n    int tmp_outcount = 0;\n    int tmp_active_method_count = 0;\n    struct timespec ts;\n\n    gen_mutex_lock(&active_method_count_mutex);\n    tmp_active_method_count = active_method_count;\n    gen_mutex_unlock(&active_method_count_mutex);\n\n    if (max_idle_time_ms < 0)\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n\n    *outcount = 0;\n\n    if(tmp_active_method_count < 1)\n    {\n\t/* nothing active yet, just snooze and return */\n\tif(max_idle_time_ms > 0)\n\t{\n\t    ts.tv_sec = 0;\n\t    ts.tv_nsec = 2000;\n\t    nanosleep(&ts, NULL);\n\t}\n\treturn(0);\n    }\n\n    construct_poll_plan(expected_method_usage,\n          tmp_active_method_count, &max_idle_time_ms);\n\n    while (position < incount && i < tmp_active_method_count)\n    {\n        if (expected_method_usage[i].plan) {\n            ret = active_method_table[i]->testcontext(\n                incount - position, \n                &out_id_array[position],\n                &tmp_outcount,\n                &error_code_array[position], \n                &actual_size_array[position],\n                user_ptr_array ?  &user_ptr_array[position] : NULL,\n                max_idle_time_ms,\n                context_id);\n            if (ret < 0)\n            {\n                /* can't recover from this */\n                gossip_lerr(\"Error: critical BMI_testcontext failure.\\n\");\n                return (ret);\n            }\n            position += tmp_outcount;\n            (*outcount) += tmp_outcount;\n            expected_method_usage[i].iters_polled = 0;\n            if (ret)\n                expected_method_usage[i].iters_active = 0;\n        }\n\ti++;\n    }\n\n    /* return 1 if anything completed */\n    if (ret == 0 && *outcount > 0)\n    {\n\tfor(i=0; i<*outcount; i++)\n\t{\n\t    gossip_debug(GOSSIP_BMI_DEBUG_CONTROL, \n\t\t\"BMI_testcontext completing: %llu\\n\", llu(out_id_array[i]));\n\t}\n\treturn (1);\n    }\n    return (0);\n\n}\n\n\n/** Performs a reverse lookup, returning the string (URL style)\n *  address for a given opaque address.\n *\n *  NOTE: caller must not free or modify returned string\n *\n *  \\return Pointer to string on success, NULL on failure.\n */\nconst char* BMI_addr_rev_lookup(BMI_addr_t addr)\n{\n    ref_st_p tmp_ref = NULL;\n    char* tmp_str = NULL;\n\n    /* find a reference that matches this address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (NULL);\n    }\n    gen_mutex_unlock(&ref_mutex);\n    \n    tmp_str = tmp_ref->id_string;\n\n    return(tmp_str);\n}\n\n/** Performs a reverse lookup, returning a string\n *  address for a given opaque address.  Works on any address, even those\n *  generated unexpectedly, but only gives hostname instead of full\n *  BMI URL style address\n *\n *  NOTE: caller must not free or modify returned string\n *\n *  \\return Pointer to string on success, NULL on failure.\n */\nconst char* BMI_addr_rev_lookup_unexpected(BMI_addr_t addr)\n{\n    ref_st_p tmp_ref = NULL;\n\n    /* find a reference that matches this address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (\"UNKNOWN\");\n    }\n    gen_mutex_unlock(&ref_mutex);\n    \n    if(!tmp_ref->interface->rev_lookup_unexpected)\n    {\n        return(\"UNKNOWN\");\n    }\n\n    return(tmp_ref->interface->rev_lookup_unexpected(\n        tmp_ref->method_addr));\n}\n\n\n/** Allocates memory that can be used in native mode by the BMI layer.\n *\n *  \\return Pointer to buffer on success, NULL on failure.\n */\nvoid *BMI_memalloc(BMI_addr_t addr,\n\t\t   bmi_size_t size,\n\t\t   enum bmi_op_type send_recv)\n{\n    void *new_buffer = NULL;\n    ref_st_p tmp_ref = NULL;\n\n    /* find a reference that matches this address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (NULL);\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    /* allocate the buffer using the method's mechanism */\n    new_buffer = tmp_ref->interface->memalloc(size, send_recv);\n\n    return (new_buffer);\n}\n\n/** Frees memory that was allocated with BMI_memalloc().\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_memfree(BMI_addr_t addr,\n\t\tvoid *buffer,\n\t\tbmi_size_t size,\n\t\tenum bmi_op_type send_recv)\n{\n    ref_st_p tmp_ref = NULL;\n    int ret = -1;\n\n    /* find a reference that matches this address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    /* free the memory */\n    ret = tmp_ref->interface->memfree(buffer, size, send_recv);\n\n    return (ret);\n}\n\n/** Acknowledge that an unexpected message has been\n * serviced that was returned from BMI_test_unexpected().\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_unexpected_free(BMI_addr_t addr,\n\t\tvoid *buffer)\n{\n    ref_st_p tmp_ref = NULL;\n    int ret = -1;\n\n    /* find a reference that matches this address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    if (!tmp_ref->interface->unexpected_free)\n    {\n        gossip_err(\"unimplemented unexpected_free callback\\n\");\n        return bmi_errno_to_pvfs(-EOPNOTSUPP);\n    }\n    /* free the memory */\n    ret = tmp_ref->interface->unexpected_free(buffer);\n\n    return (ret);\n}\n\n/** Pass in optional parameters.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_set_info(BMI_addr_t addr,\n\t\t int option,\n\t\t void *inout_parameter)\n{\n    int ret = -1;\n    int i = 0;\n    ref_st_p tmp_ref = NULL;\n\n    gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                 \"[BMI CONTROL]: %s: set_info: %llu option: %d\\n\",\n                 __func__, llu(addr), option);\n    /* if the addr is NULL, then the set_info should apply to all\n     * available methods.\n     */\n    if (!addr)\n    {\n\tif (!active_method_table)\n\t{\n\t    return (bmi_errno_to_pvfs(-EINVAL));\n\t}\n\tgen_mutex_lock(&active_method_count_mutex);\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    ret = active_method_table[i]->set_info(\n                option, inout_parameter);\n\t    /* we bail out if even a single set_info fails */\n\t    if (ret < 0)\n\t    {\n\t\tgossip_lerr(\n                    \"Error: failure on set_info to method: %d\\n\", i);\n\t\tgen_mutex_unlock(&active_method_count_mutex);\n\t\treturn (ret);\n\t    }\n\t}\n\tgen_mutex_unlock(&active_method_count_mutex);\n\treturn (0);\n    }\n\n    gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                 \"[BMI CONTROL]: %s: searching for ref %llu\\n\",\n                 __func__, llu(addr));\n    /* find a reference that matches this address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EINVAL));\n    }\n\n    /* shortcut address reference counting */\n    if(option == BMI_INC_ADDR_REF)\n    {\n\ttmp_ref->ref_count++;\n        gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                     \"[BMI CONTROL]: %s: incremented ref %llu to: %d\\n\",\n                     __func__, llu(addr), tmp_ref->ref_count);\n\tgen_mutex_unlock(&ref_mutex);\n\treturn(0);\n    }\n    if(option == BMI_DEC_ADDR_REF)\n    {\n\ttmp_ref->ref_count--;\n        gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                     \"[BMI CONTROL]: %s: decremented ref %llu to: %d\\n\",\n                     __func__, llu(addr), tmp_ref->ref_count);\n\tassert(tmp_ref->ref_count >= 0);\n\n\tif(tmp_ref->ref_count == 0)\n\t{\n            bmi_addr_drop(tmp_ref);\n\t}\n\tgen_mutex_unlock(&ref_mutex);\n\treturn(0);\n    }\n\n    /* if the caller requests a TCP specific close socket action */\n    if (option == BMI_TCP_CLOSE_SOCKET) \n    {\n        /* check to see if the address is in fact a tcp address */\n        if(strcmp(tmp_ref->interface->method_name, \"bmi_tcp\") == 0)\n        {\n            /* take the same action as in the BMI_DEC_ADDR_REF case to clean\n             * out the entire address structure and anything linked to it so \n             * that the next addr_lookup starts from scratch\n             */\n\t    gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                         \"[BMI CONTROL]: %s: Closing bmi_tcp \"\n                         \"connection at caller's request.\\n\",\n                         __func__); \n            ref_list_rem(cur_ref_list, addr);\n            dealloc_ref_st(tmp_ref);\n        }\n        gen_mutex_unlock(&ref_mutex);\n        return 0;\n    }\n\n    gen_mutex_unlock(&ref_mutex);\n\n    ret = tmp_ref->interface->set_info(option, inout_parameter);\n\n    return (ret);\n}\n\n/** Query for optional parameters.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_get_info(BMI_addr_t addr,\n\t\t int option,\n\t\t void *inout_parameter)\n{\n    int i = 0;\n    int maxsize = 0;\n    int tmp_maxsize;\n    int ret = 0;\n    ref_st_p tmp_ref = NULL;\n\n    switch (option)\n    {\n\t/* check to see if the interface is initialized */\n    case BMI_CHECK_INIT:\n\tgen_mutex_lock(&active_method_count_mutex);\n\tif (active_method_count > 0)\n\t{\n\t    gen_mutex_unlock(&active_method_count_mutex);\n\t    return (0);\n\t}\n\telse\n\t{\n\t    gen_mutex_unlock(&active_method_count_mutex);\n\t    return (bmi_errno_to_pvfs(-ENETDOWN));\n\t}\n    case BMI_CHECK_MAXSIZE:\n\tgen_mutex_lock(&active_method_count_mutex);\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    ret = active_method_table[i]->get_info(\n                option, &tmp_maxsize);\n\t    if (ret < 0)\n\t    {\n\t\treturn (ret);\n\t    }\n\t    if (i == 0)\n\t    {\n\t\tmaxsize = tmp_maxsize;\n\t    }\n\t    else\n\t    {\n\t\tif (tmp_maxsize < maxsize)\n\t\t    maxsize = tmp_maxsize;\n\t    }\n\t    *((int *) inout_parameter) = maxsize;\n\t}\n\tgen_mutex_unlock(&active_method_count_mutex);\n\tbreak;\n    case BMI_GET_METH_ADDR:\n\tgen_mutex_lock(&ref_mutex);\n\ttmp_ref = ref_list_search_addr(cur_ref_list, addr);\n\tif(!tmp_ref)\n\t{\n\t    gen_mutex_unlock(&ref_mutex);\n\t    return (bmi_errno_to_pvfs(-EINVAL));\n\t}\n\tgen_mutex_unlock(&ref_mutex);\n\t*((void**) inout_parameter) = tmp_ref->method_addr;\n\tbreak;\n    case BMI_GET_UNEXP_SIZE:\n        gen_mutex_lock(&ref_mutex);\n        tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n        if(!tmp_ref)\n        {\n            gen_mutex_unlock(&ref_mutex);\n            return (bmi_errno_to_pvfs(-EINVAL));\n        }\n        gen_mutex_unlock(&ref_mutex);\n        ret = tmp_ref->interface->get_info(\n            option, inout_parameter);\n        if(ret < 0)\n        {\n            return ret;\n        }\n        break;\n    case BMI_TCP_GET_PORT:\n\tgen_mutex_lock(&active_method_count_mutex);\n        /* look through active methods for one that will answer this */\n\tfor (i = 0; i < active_method_count; i++)\n\t{\n\t    ret = active_method_table[i]->get_info(\n                option, inout_parameter);\n\t    if (ret == 0)\n\t    {\n\t        gen_mutex_unlock(&active_method_count_mutex);\n\t\treturn (ret);\n\t    }\n\t}\n\tgen_mutex_unlock(&active_method_count_mutex);\n        return ret;\n        break;\n    case BMI_TRANSPORT_METHODS_STRING:\n        {\n            /*\n             * [OUT] inout_parameter : contains comma-separated list of transport\n             *                         protocols, memory allocated here and must\n             *                         be free'd by the caller.\n             * @return               : total number of transport protocols\n             *                         supported by bmi.\n             */\n\n            int kmstring_length = 0;\n            int kmc = sizeof(static_methods) / sizeof(static_methods[0]) - 1;\n            int i = 0;\n            char **stringptr = (char **) &(*(char*) inout_parameter);\n\n            /* Check if there are any transport protocol supported, else return */\n            if (kmc <= 0)\n            {\n                return 0;\n            }\n\n            /* Find out the length the output string will be. */\n            for (i = 0; i < kmc; ++i)\n            {\n                kmstring_length += strlen(static_methods[i]->method_name)\n                    - strlen(\"bmi_\") + sizeof(\",\");\n            }\n\n            /* +1 for null character */\n            (*stringptr) = malloc(kmstring_length + 1);\n\n            if ((*stringptr) == NULL)\n            {\n                return bmi_errno_to_pvfs(-ENOMEM);\n            }\n\n            memset((*stringptr), 0, kmstring_length);\n\n            /* The transport protocol's names begins with bmi_, offset the\n             * method name when concatenating.\n             */\n            for (i = 0; i < kmc; ++i)\n            {\n                strcat((*stringptr), static_methods[i]->method_name + strlen(\"bmi_\"));\n                strcat((*stringptr), \",\");\n            }\n\n            return kmc;\n        }\n        break;\n        \n    default:\n\treturn (bmi_errno_to_pvfs(-ENOSYS));\n    }\n    return (0);\n}\n\n/** Given a string representation of a host/network address and a BMI\n * address handle, return whether the BMI address handle is part of the wildcard\n * address range specified by the string.\n * \\return 1 on success, -errno on failure and 0 if it is not part of\n * the specified range\n */\nint BMI_query_addr_range (BMI_addr_t addr, const char *id_string, int netmask)\n{\n    int ret = -1;\n    int i = 0, failed = 1;\n    int provided_method_length = 0;\n    char *ptr, *provided_method_name = NULL;\n    ref_st_p tmp_ref = NULL;\n\n    if((strlen(id_string)+1) > BMI_MAX_ADDR_LEN)\n    {\n\treturn(bmi_errno_to_pvfs(-ENAMETOOLONG));\n    }\n    /* lookup the provided address */\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, addr);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EPROTO));\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    ptr = strchr(id_string, ':');\n    if (ptr == NULL)\n    {\n        return (bmi_errno_to_pvfs(-EINVAL));\n    }\n    ret = -EPROTO;\n    provided_method_length = (unsigned long) ptr - (unsigned long) id_string;\n    provided_method_name = (char *) calloc(provided_method_length + 1, sizeof(char));\n    if (provided_method_name == NULL)\n    {\n        return bmi_errno_to_pvfs(-ENOMEM);\n    }\n    strncpy(provided_method_name, id_string, provided_method_length);\n\n    /* Now we will run through each method looking for one that\n     * matches the specified wildcard address. \n     */\n    i = 0;\n    gen_mutex_lock(&active_method_count_mutex);\n    while (i < active_method_count)\n    {\n        const char *active_method_name = active_method_table[i]->method_name + 4;\n        /* provided name matches this interface */\n        if (!strncmp(active_method_name, provided_method_name, provided_method_length))\n        {\n            int (*meth_fnptr)(bmi_method_addr_p, const char *, int);\n            failed = 0;\n            if ((meth_fnptr = active_method_table[i]->query_addr_range) == NULL)\n            {\n                ret = -ENOSYS;\n                gossip_lerr(\"Error: method doesn't implement querying address range/wildcards! Cannot implement FS export options!\\n\");\n                failed = 1;\n                break;\n            }\n            /* pass it into the specific bmi layer */\n            ret = meth_fnptr(tmp_ref->method_addr, id_string, netmask);\n            if (ret < 0)\n                failed = 1;\n            break;\n        }\n\ti++;\n    }\n    gen_mutex_unlock(&active_method_count_mutex);\n    free(provided_method_name);\n    if (failed)\n        return bmi_errno_to_pvfs(ret);\n    return ret;\n}\n\n/** Resolves the string representation of a host address into a BMI\n *  address handle.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_addr_lookup(BMI_addr_t * new_addr,\n                    const char *id_string)\n{\n\n    ref_st_p new_ref = NULL;\n    bmi_method_addr_p meth_addr = NULL;\n    int ret = -1;\n    int i = 0;\n    int failed;\n\n    if((strlen(id_string)+1) > BMI_MAX_ADDR_LEN)\n    {\n\treturn(bmi_errno_to_pvfs(-ENAMETOOLONG));\n    }\n\n    /* set the addr to zero in case we fail */\n    *new_addr = 0;\n\n    /* First we want to check to see if this host has already been\n     * discovered! */\n    gen_mutex_lock(&ref_mutex);\n    new_ref = ref_list_search_str(cur_ref_list, id_string);\n    gen_mutex_unlock(&ref_mutex);\n\n    if (new_ref)\n    {\n\t/* we found it. */\n\t*new_addr = new_ref->bmi_addr;\n\treturn (0);\n    }\n\n    /* Now we will run through each method looking for one that\n     * responds successfully.  It is assumed that they are already\n     * listed in order of preference\n     */\n    i = 0;\n    gen_mutex_lock(&active_method_count_mutex);\n    while ((i < active_method_count) &&\n           !(meth_addr = active_method_table[i]->method_addr_lookup(id_string)))\n    {\n\ti++;\n    }\n\n    /* if not found, try to bring it up now */\n    failed = 0;\n    if (!meth_addr) {\n\tfor (i=0; i<known_method_count; i++) {\n\t    const char *name;\n\t    /* only bother with those not active */\n\t    int j;\n\t    for (j=0; j<active_method_count; j++)\n\t\tif (known_method_table[i] == active_method_table[j])\n\t\t    break;\n\t    if (j < active_method_count)\n\t\tcontinue;\n\n\t    /* well-known that mapping is \"x\" -> \"bmi_x\" */\n\t    name = known_method_table[i]->method_name + 4;\n\t    if (!strncmp(id_string, name, strlen(name))) {\n\t        ret = activate_method(known_method_table[i]->method_name, 0, 0);\n\t        if (ret < 0) {\n                    failed = 1;\n                    break;\n                }\n\t\tmeth_addr = known_method_table[i]->\n\t\t    method_addr_lookup(id_string);\n\t\ti = active_method_count - 1;  /* point at the new one */\n\t\tbreak;\n\t    }\n\t}\n    }\n    gen_mutex_unlock(&active_method_count_mutex);\n    if (failed)\n        return bmi_errno_to_pvfs(ret);\n\n    /* make sure one was successful */\n    if (!meth_addr)\n    {\n        return bmi_errno_to_pvfs(-ENOPROTOOPT);\n    }\n\n    /* create a new reference for the addr */\n    new_ref = alloc_ref_st();\n    if (!new_ref)\n    {\n\tret = bmi_errno_to_pvfs(-ENOMEM);\n\tgoto bmi_addr_lookup_failure;\n    }\n\n    /* fill in the details */\n    new_ref->method_addr = meth_addr;\n    meth_addr->parent = new_ref;\n    new_ref->id_string = (char *) malloc(strlen(id_string) + 1);\n    if (!new_ref->id_string)\n    {\n\tret = bmi_errno_to_pvfs(errno);\n\tgoto bmi_addr_lookup_failure;\n    }\n    strcpy(new_ref->id_string, id_string);\n    new_ref->interface = active_method_table[i];\n\n    /* keep up with the reference and we are done */\n    gen_mutex_lock(&ref_mutex);\n    ref_list_add(cur_ref_list, new_ref);\n    gen_mutex_unlock(&ref_mutex);\n\n    *new_addr = new_ref->bmi_addr;\n    return (0);\n\n  bmi_addr_lookup_failure:\n\n    if (meth_addr)\n    {\n\tactive_method_table[i]->set_info(\n            BMI_DROP_ADDR, meth_addr);\n    }\n\n    if (new_ref)\n    {\n\tdealloc_ref_st(new_ref);\n    }\n\n    return (ret);\n}\n\n\n/** Similar to BMI_post_send(), except that the source buffer is \n *  replaced by a list of (possibly non contiguous) buffers.\n *\n *  \\return 0 on success, 1 on immediate successful completion,\n *  -errno on failure.\n */\nint BMI_post_send_list(bmi_op_id_t * id,\n\t\t       BMI_addr_t dest,\n\t\t       const void *const *buffer_list,\n\t\t       const bmi_size_t *size_list,\n\t\t       int list_count,\n\t\t       /* \"total_size\" is the sum of the size list */\n\t\t       bmi_size_t total_size,\n\t\t       enum bmi_buffer_type buffer_type,\n\t\t       bmi_msg_tag_t tag,\n\t\t       void *user_ptr,\n\t\t       bmi_context_id context_id,\n                       bmi_hint hints)\n{\n    ref_st_p tmp_ref = NULL;\n    int ret = -1;\n\n#ifndef GOSSIP_DISABLE_DEBUG\n    int i;\n\n    gossip_debug(GOSSIP_BMI_DEBUG_OFFSETS,\n\t\"BMI_post_send_list: addr: %ld, count: %d, total_size: %ld, tag: %d\\n\", \n\t(long)dest, list_count, (long)total_size, (int)tag);\n\n    for(i=0; i<list_count; i++)\n    {\n\tgossip_debug(GOSSIP_BMI_DEBUG_OFFSETS,\n\t    \"   element %d: offset: 0x%lx, size: %ld\\n\",\n\t    i, (long)buffer_list[i], (long)size_list[i]);\n    }\n#endif\n\n    *id = 0;\n\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, dest);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EPROTO));\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    if (tmp_ref->interface->post_send_list)\n    {\n\tret = tmp_ref->interface->post_send_list(\n            id, tmp_ref->method_addr, buffer_list, size_list,\n            list_count, total_size, buffer_type, tag, user_ptr,\n            context_id, (PVFS_hint)hints);\n\n\treturn (ret);\n    }\n\n    gossip_lerr(\"Error: method doesn't implement send_list.\\n\");\n    gossip_lerr(\"Error: send_list emulation not yet available.\\n\");\n\n    return (bmi_errno_to_pvfs(-ENOSYS));\n}\n\n\n/** Similar to BMI_post_recv(), except that the dest buffer is \n *  replaced by a list of (possibly non contiguous) buffers\n *\n *  \\param total_expected_size the sum of the size list.\n *  \\param total_actual_size the aggregate amt that was received.\n *\n *  \\return 0 on success, 1 on immediate successful completion,\n *  -errno on failure.\n */\nint BMI_post_recv_list(bmi_op_id_t * id,\n\t\t       BMI_addr_t src,\n\t\t       void *const *buffer_list,\n\t\t       const bmi_size_t *size_list,\n\t\t       int list_count,\n\t\t       bmi_size_t total_expected_size,\n\t\t       bmi_size_t * total_actual_size,\n\t\t       enum bmi_buffer_type buffer_type,\n\t\t       bmi_msg_tag_t tag,\n\t\t       void *user_ptr,\n\t\t       bmi_context_id context_id,\n                       bmi_hint hints)\n{\n    ref_st_p tmp_ref = NULL;\n    int ret = -1;\n\n#ifndef GOSSIP_DISABLE_DEBUG\n    int i;\n\n    gossip_debug(GOSSIP_BMI_DEBUG_OFFSETS,\n\t\"BMI_post_recv_list: addr: %ld, count: %d, total_size: %ld, tag: %d\\n\", \n\t(long)src, list_count, (long)total_expected_size, (int)tag);\n\n    for(i=0; i<list_count; i++)\n    {\n\tgossip_debug(GOSSIP_BMI_DEBUG_OFFSETS,\n\t    \"   element %d: offset: 0x%lx, size: %ld\\n\",\n\t    i, (long)buffer_list[i], (long)size_list[i]);\n    }\n#endif\n\n    *id = 0;\n\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, src);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EPROTO));\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    if (tmp_ref->interface->post_recv_list)\n    {\n\tret = tmp_ref->interface->post_recv_list(\n            id, tmp_ref->method_addr, buffer_list, size_list,\n            list_count, total_expected_size, total_actual_size,\n            buffer_type, tag, user_ptr, context_id, (PVFS_hint)hints);\n\n\treturn (ret);\n    }\n\n    gossip_lerr(\"Error: method doesn't implement recv_list.\\n\");\n    gossip_lerr(\"Error: recv_list emulation not yet available.\\n\");\n\n    return (bmi_errno_to_pvfs(-ENOSYS));\n}\n\n\n/** Similar to BMI_post_sendunexpected(), except that the source buffer is \n *  replaced by a list of (possibly non contiguous) buffers.\n *\n *  \\param total_size the sum of the size list.\n *\n *  \\return 0 on success, 1 on immediate successful completion,\n *  -errno on failure.\n */\nint BMI_post_sendunexpected_list_class(bmi_op_id_t * id,\n\t\t\t\t BMI_addr_t dest,\n\t\t\t\t const void *const *buffer_list,\n\t\t\t\t const bmi_size_t *size_list,\n\t\t\t\t int list_count,\n\t\t\t\t bmi_size_t total_size,\n\t\t\t\t enum bmi_buffer_type buffer_type,\n\t\t\t\t bmi_msg_tag_t tag,\n                                 uint8_t class,\n\t\t\t\t void *user_ptr,\n\t\t\t\t bmi_context_id context_id,\n                                 bmi_hint hints)\n{\n    ref_st_p tmp_ref = NULL;\n    int ret = -1;\n\n#ifndef GOSSIP_DISABLE_DEBUG\n    int i;\n\n    gossip_debug(GOSSIP_BMI_DEBUG_OFFSETS,\n\t\"BMI_post_sendunexpected_list: addr: %ld, count: %d, \"\n                 \"total_size: %ld, tag: %d\\n\",  (long)dest, list_count,\n                 (long)total_size, (int)tag);\n\n    for(i=0; i<list_count; i++)\n    {\n\tgossip_debug(GOSSIP_BMI_DEBUG_OFFSETS,\n\t    \"   element %d: offset: 0x%lx, size: %ld\\n\",\n\t    i, (long)buffer_list[i], (long)size_list[i]);\n    }\n#endif\n\n    *id = 0;\n\n    gen_mutex_lock(&ref_mutex);\n    tmp_ref = ref_list_search_addr(cur_ref_list, dest);\n    if (!tmp_ref)\n    {\n\tgen_mutex_unlock(&ref_mutex);\n\treturn (bmi_errno_to_pvfs(-EPROTO));\n    }\n    gen_mutex_unlock(&ref_mutex);\n\n    if (tmp_ref->interface->post_send_list)\n    {\n\tret = tmp_ref->interface->post_sendunexpected_list(\n            id, tmp_ref->method_addr, buffer_list, size_list,\n            list_count, total_size, buffer_type, tag, class, user_ptr,\n            context_id, (PVFS_hint)hints);\n\n\treturn (ret);\n    }\n\n    gossip_lerr(\"Error: method doesn't implement sendunexpected_list.\\n\");\n    gossip_lerr(\"Error: send_list emulation not yet available.\\n\");\n\n    return (bmi_errno_to_pvfs(-ENOSYS));\n}\n\n\n/** Attempts to cancel a pending operation that has not yet completed.\n *  Caller must still test to gather error code after calling this\n *  function even if it returns 0.\n *\n *  \\return 0 on success, -errno on failure.\n */\nint BMI_cancel(bmi_op_id_t id, \n\t       bmi_context_id context_id)\n{\n    struct method_op *target_op = NULL;\n    int ret = -1;\n\n    gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                 \"%s: cancel id %llu\\n\", __func__, llu(id));\n\n    target_op = id_gen_fast_lookup(id);\n    if(target_op == NULL)\n    {\n        /* if we can't find the operation, then assume it has already\n         * completed naturally.\n         */\n        return(0);\n    }\n\n    assert(target_op->op_id == id);\n\n    if(active_method_table[target_op->addr->method_type]->cancel)\n    {\n\tret = active_method_table[\n            target_op->addr->method_type]->cancel(\n                id, context_id);\n    }\n    else\n    {\n\tgossip_err(\"Error: BMI_cancel() unimplemented \"\n                   \"for this module.\\n\");\n\tret = bmi_errno_to_pvfs(-ENOSYS);\n    }\n\n    return (ret);\n}\n\n/**************************************************************\n * method callback functions\n */\n\n/* bmi_method_addr_reg_callback()\n * \n * Used by the methods to register new addresses when they are\n * discovered.  Only call this method when the device gets an\n * unexpected receive from a new peer, i.e., if you do the equivalent\n * of a socket accept() and get a new connection.\n *\n * Do not call this function for active lookups, that is from your\n * method_addr_lookup.  BMI already knows about the address in\n * this case, since the user provided it.\n *\n * returns 0 on success, -errno on failure\n */\nBMI_addr_t bmi_method_addr_reg_callback(bmi_method_addr_p map)\n{\n    ref_st_p new_ref = NULL;\n\n    /* NOTE: we are trusting the method to make sure that we really\n     * don't know about the address yet.  No verification done here.\n     */\n\n    /* create a new reference structure */\n    new_ref = alloc_ref_st();\n    if (!new_ref)\n    {\n\treturn 0;\n    }\n\n    /*\n      fill in the details; we don't have an id string for this one.\n    */\n    new_ref->method_addr = map;\n    new_ref->id_string = NULL;\n    map->parent = new_ref;\n\n    /* check the method_type from the method_addr pointer to know\n     * which interface to use */\n    new_ref->interface = active_method_table[map->method_type];\n\n    /* add the reference structure to the list */\n    ref_list_add(cur_ref_list, new_ref);\n\n    return new_ref->bmi_addr;\n}\n\nint bmi_method_addr_forget_callback(BMI_addr_t addr)\n{\n    struct forget_item* tmp_item = NULL;\n\n    tmp_item = (struct forget_item*)malloc(sizeof(struct forget_item));\n    if(!tmp_item)\n    {\n        return(bmi_errno_to_pvfs(-ENOMEM));\n    }\n\n    tmp_item->addr = addr;\n\n    /* add to queue of items that we want the BMI control layer to consider\n     * deallocating\n     */\n    gen_mutex_lock(&forget_list_mutex);\n    qlist_add(&tmp_item->link, &forget_list);\n    gen_mutex_unlock(&forget_list_mutex);\n\n    return (0);\n}\n\n/*\n * Signal BMI to drop inactive connections for this method.\n */\nvoid bmi_method_addr_drop_callback (char* method_name)\n{\n    struct drop_item *item =\n        (struct drop_item *) malloc(sizeof(struct drop_item));\n\n    /*\n     * If we can't allocate, just return.\n     * Maybe this will succeed next time.\n     */\n    if (!item) return;\n\n    item->method_name = method_name;\n    \n    gen_mutex_lock(&bmi_addr_force_drop_list_mutex);\n    qlist_add(&item->link, &bmi_addr_force_drop_list);\n    gen_mutex_unlock(&bmi_addr_force_drop_list_mutex);\n\n    return;\n}\n\n\n/**\n * Try to increase method_usage_t struct to include room for a new method.\n */\nstatic int grow_method_usage (struct method_usage_t ** p, int newflags)\n{\n    struct method_usage_t * x = *p;\n    *p = malloc((active_method_count + 1) * sizeof(**p));\n    if (!*p) {\n        *p = x;\n        return 0;\n    }\n    if (active_method_count) {\n        memcpy(*p, x, active_method_count * sizeof(**p));\n        free(x);\n    }\n    memset(&((*p)[active_method_count]), 0, sizeof(**p));\n    (*p)[active_method_count].flags = newflags;\n\n    return 1;\n }\n\n/*\n * Attempt to insert this name into the list of active methods,\n * and bring it up.\n * NOTE: assumes caller has protected active_method_count with a mutex lock\n */\nstatic int\nactivate_method(const char *name, const char *listen_addr, int flags)\n{\n    int i, ret;\n    void *x;\n    struct bmi_method_ops *meth;\n    bmi_method_addr_p new_addr;\n\n    /* already active? */\n    for (i=0; i<active_method_count; i++)\n\tif (!strcmp(active_method_table[i]->method_name, name)) break;\n    if (i < active_method_count)\n    {\n\treturn 0;\n    }\n\n    /* is the method known? */\n    for (i=0; i<known_method_count; i++)\n\tif (!strcmp(known_method_table[i]->method_name, name)) break;\n    if (i == known_method_count) {\n\tgossip_lerr(\"Error: no method available for %s.\\n\", name);\n\treturn -ENOPROTOOPT;\n    }\n    meth = known_method_table[i];\n\n    /*\n     * Later: try to load a dynamic module, growing the known method\n     * table and search it again.\n     */\n\n    /* toss it into the active table */\n    x = active_method_table;\n    active_method_table = malloc(\n\t(active_method_count + 1) * sizeof(*active_method_table));\n    if (!active_method_table) {\n\tactive_method_table = x;\n\treturn -ENOMEM;\n    }\n    if (active_method_count) {\n\tmemcpy(active_method_table, x,\n\t    active_method_count * sizeof(*active_method_table));\n\tfree(x);\n    }\n    active_method_table[active_method_count] = meth;\n\n    if (!grow_method_usage (&unexpected_method_usage, meth->flags))\n       return -ENOMEM;\n\n    /**\n     * If we run out of memory here, the unexpected_method_usage will be\n     * larger than strictly required but there is no memory leak.\n     */\n\n    if (!grow_method_usage (&expected_method_usage, meth->flags))\n       return -ENOMEM;\n\n    ++active_method_count;\n\n    /* initialize it */\n    new_addr = 0;\n    if (listen_addr) {\n\tnew_addr = meth->method_addr_lookup(listen_addr);\n\tif (!new_addr) {\n\t    gossip_err(\n\t\t\"Error: failed to lookup listen address %s for method %s.\\n\",\n\t\tlisten_addr, name);\n\t    --active_method_count;\n\t    return -EINVAL;\n\t}\n\t/* this is a bit of a hack */\n\tnew_addr->method_type = active_method_count - 1;\n    }\n    ret = meth->initialize(new_addr, active_method_count - 1, flags);\n    if (ret < 0) {\n\tgossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n          \"failed to initialize method %s.\\n\", name);\n\t--active_method_count;\n\treturn ret;\n    }\n\n    /* tell it about any open contexts */\n    for (i=0; i<BMI_MAX_CONTEXTS; i++)\n\tif (context_array[i]) {\n\t    ret = meth->open_context(i);\n\t    if (ret < 0)\n\t\tbreak;\n\t}\n\n    return ret;\n}\n\n \nint bmi_errno_to_pvfs(int error)\n{\n    int bmi_errno = error;\n\n#define __CASE(err)                      \\\ncase -err: bmi_errno = -BMI_##err; break;\\\ncase err: bmi_errno = BMI_##err; break\n\n    switch(error)\n    {\n        __CASE(EPERM);\n        __CASE(ENOENT);\n        __CASE(EINTR);\n        __CASE(EIO);\n        __CASE(ENXIO);\n        __CASE(EBADF);\n        __CASE(EAGAIN);\n        __CASE(ENOMEM);\n        __CASE(EFAULT);\n        __CASE(EBUSY);\n        __CASE(EEXIST);\n        __CASE(ENODEV);\n        __CASE(ENOTDIR);\n        __CASE(EISDIR);\n        __CASE(EINVAL);\n        __CASE(EMFILE);\n        __CASE(EFBIG);\n        __CASE(ENOSPC);\n        __CASE(EROFS);\n        __CASE(EMLINK);\n        __CASE(EPIPE);\n        __CASE(EDEADLK);\n        __CASE(ENAMETOOLONG);\n        __CASE(ENOLCK);\n        __CASE(ENOSYS);\n        __CASE(ENOTEMPTY);\n        __CASE(ELOOP);\n        __CASE(ENOMSG);\n        __CASE(ENODATA);\n        __CASE(ETIME);\n        __CASE(EREMOTE);\n        __CASE(EPROTO);\n        __CASE(EBADMSG);\n        __CASE(EOVERFLOW);\n        __CASE(EMSGSIZE);\n        __CASE(EPROTOTYPE);\n        __CASE(ENOPROTOOPT);\n        __CASE(EPROTONOSUPPORT);\n        __CASE(EOPNOTSUPP);\n        __CASE(EADDRINUSE);\n        __CASE(EADDRNOTAVAIL);\n        __CASE(ENETDOWN);\n        __CASE(ENETUNREACH);\n        __CASE(ENETRESET);\n        __CASE(ENOBUFS);\n        __CASE(ETIMEDOUT);\n        __CASE(ECONNREFUSED);\n        __CASE(EHOSTDOWN);\n        __CASE(EHOSTUNREACH);\n        __CASE(EALREADY);\n        __CASE(EACCES);\n        __CASE(ECONNRESET);\n#undef __CASE\n    }\n    return bmi_errno;\n}\n\n/* bmi_check_forget_list()\n * \n * Scans queue of items that methods have suggested that we forget about \n *\n * no return value\n */\nstatic void bmi_check_forget_list(void)\n{\n    BMI_addr_t tmp_addr;\n    struct forget_item* tmp_item;\n    ref_st_p tmp_ref = NULL;\n    \n    gen_mutex_lock(&forget_list_mutex);\n    while(!qlist_empty(&forget_list))\n    {\n        tmp_item = qlist_entry(forget_list.next, struct forget_item,\n            link);     \n        qlist_del(&tmp_item->link);\n        /* item is off of the list; unlock for a moment while we work on\n         * this addr \n         */\n        gen_mutex_unlock(&forget_list_mutex);\n        tmp_addr = tmp_item->addr;\n        free(tmp_item);\n\n        gen_mutex_lock(&ref_mutex);\n        tmp_ref = ref_list_search_addr(cur_ref_list, tmp_addr);\n        if(tmp_ref && tmp_ref->ref_count == 0)\n        {\n            bmi_addr_drop(tmp_ref);\n        }   \n        gen_mutex_unlock(&ref_mutex);\n\n        gen_mutex_lock(&forget_list_mutex);\n    }\n    gen_mutex_unlock(&forget_list_mutex);\n\n    return;\n}\n\n/* bmi_addr_drop\n *\n * Destroys a complete BMI address, including asking the method to clean up \n * its portion.  Will query the method for permission before proceeding\n *\n * NOTE: must be called with ref list mutex held \n */\nstatic void bmi_addr_drop(ref_st_p tmp_ref)\n{\n    struct method_drop_addr_query query;\n    query.response = 0;\n    query.addr = tmp_ref->method_addr;\n    int ret = 0;\n\n    /* reference count is zero; ask module if it wants us to discard\n     * the address; TCP will tell us to drop addresses for which the\n     * socket has died with no possibility of reconnect \n     */\n    ret = tmp_ref->interface->get_info(BMI_DROP_ADDR_QUERY,\n        &query);\n    if(ret == 0 && query.response == 1)\n    {\n        /* kill the address */\n        gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n            \"[BMI CONTROL]: %s: bmi discarding address: %llu\\n\",\n            __func__, llu(tmp_ref->bmi_addr));\n        ref_list_rem(cur_ref_list, tmp_ref->bmi_addr);\n        /* NOTE: this triggers request to module to free underlying\n         * resources if it wants to\n         */\n        dealloc_ref_st(tmp_ref);\n    }\n    return;\n}\n\n\n/* bmi_addr_force_drop\n *\n * Destroys a complete BMI address, including forcing the method to clean up \n * its portion.\n *\n * NOTE: must be called with ref list mutex held \n */\nstatic void bmi_addr_force_drop(ref_st_p ref, ref_list_p ref_list)\n{\n    gossip_debug(GOSSIP_BMI_DEBUG_CONTROL,\n                 \"[BMI CONTROL]: %s: bmi discarding address: %llu\\n\",\n                 __func__, llu(ref->bmi_addr));\n\n    ref_list_rem(ref_list, ref->bmi_addr);\n    dealloc_ref_st(ref);\n\n    return;\n}\n\n/*\n * bmi_check_addr_force_drop\n *\n * Checks to see if any method has requested freeing resources.\n */\nstatic void bmi_check_addr_force_drop (void)\n{\n    struct drop_item *drop_item = NULL;\n    ref_st_p          ref_item = NULL;\n\n    gen_mutex_lock(&bmi_addr_force_drop_list_mutex);\n    while (!qlist_empty(&bmi_addr_force_drop_list))\n    {\n        drop_item = qlist_entry(qlist_pop(&bmi_addr_force_drop_list),\n                                struct drop_item,\n                                link);\n        gen_mutex_unlock(&bmi_addr_force_drop_list_mutex);\n        gen_mutex_lock(&ref_mutex);\n        qlist_for_each_entry(ref_item, cur_ref_list, list_link)\n        {\n             if ((ref_item->ref_count == 0) &&\n                 (ref_item->interface->method_name == drop_item->method_name))\n             {\n                 bmi_addr_force_drop(ref_item, cur_ref_list);\n             }\n        }\n        gen_mutex_unlock(&ref_mutex);\n        gen_mutex_lock(&bmi_addr_force_drop_list_mutex);\n    }\n    gen_mutex_unlock(&bmi_addr_force_drop_list_mutex);\n\n    return;\n}\n\n/*\n * Local variables:\n *  c-indent-level: 4\n *  c-basic-offset: 4\n * End:\n *\n * vim: ts=8 sts=4 sw=4 expandtab\n */\n",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/io/bmi/bmi_ib/vapi.c": "/*\n * VAPI-specific functions.\n *\n * Copyright (C) 2003-6 Pete Wyckoff <pw@osc.edu>\n *\n * See COPYING in top-level directory.\n */\n#include <stdio.h>\n#include <string.h>\n#include <stdarg.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <netinet/in.h>  /* htonl */\n#define __PINT_REQPROTO_ENCODE_FUNCS_C  /* include definitions */\n#include <src/io/bmi/bmi-method-support.h>   /* struct bmi_method_addr */\n#include <src/common/misc/pvfs2-internal.h>\n#include <src/io/bmi/bmi-byteswap.h>  /* bmitoh64 */\n\n#include \"pvfs2-config.h\" /* HAVE_IB_WRAP_COMMON_H configure symbol */\n\n#include <dlfcn.h>        /* look in mosal for syms */\n\n/* otherwise undefined things in mtl_log.h */\n#define MAX_TRACE 0\n#define MAX_DEBUG 0\n#define MAX_ERROR 0\n\n#include <vapi.h>\n#include <vapi_common.h>\n#ifdef HAVE_IB_WRAP_COMMON_H\n#include <wrap_common.h>  /* reinit_mosal externs */\n#endif\n\n#include \"ib.h\"\n\n/*\n * VAPI-private device-wide state.\n */\nstruct vapi_device_priv {\n    VAPI_hca_hndl_t nic_handle;  /* NIC reference */\n    VAPI_cq_hndl_t nic_cq;  /* single completion queue for all QPs */\n    VAPI_pd_hndl_t nic_pd;  /* single protection domain for all memory/QP */\n    IB_lid_t nic_lid;  /* my lid */\n\n    /*\n     * Temp array for filling scatter/gather lists to pass to IB functions,\n     * allocated once at start to max size defined as reported by the qp.\n     */\n    VAPI_sg_lst_entry_t *sg_tmp_array;\n    unsigned int sg_max_len;\n\n    /*\n     * Maximum number of outstanding work requests in the NIC, same for both\n     * SQ and RQ.  Used to decide when to use a SIGNALED completion on a send\n     * to avoid WQE buildup.\n     */\n    unsigned int max_outstanding_wr;\n\n    /* async events */\n    EVAPI_async_handler_hndl_t nic_async_event_handler;\n    int async_event_pipe[2];\n\n    /* completion channel events */\n    EVAPI_compl_handler_hndl_t nic_cq_event_handler;\n    int cq_event_pipe[2];\n};\n\n/*\n * Per-connection state.\n */\nstruct vapi_connection_priv {\n    /* ib local params */\n    VAPI_qp_hndl_t qp;\n    VAPI_qp_num_t qp_num;\n    VAPI_mr_hndl_t eager_send_mr;\n    VAPI_mr_hndl_t eager_recv_mr;\n    VAPI_lkey_t eager_send_lkey;  /* for post_sr */\n    VAPI_lkey_t eager_recv_lkey;  /* for post_rr */\n    unsigned int num_unsignaled_wr;  /* keep track of outstanding WRs */\n    /* ib remote params */\n    IB_lid_t remote_lid;\n    VAPI_qp_num_t remote_qp_num;\n};\n\n/* constants used to initialize infiniband device */\nstatic const int VAPI_PORT = 1;\nstatic const unsigned int VAPI_NUM_CQ_ENTRIES = 1024;\nstatic const int VAPI_MTU = MTU1024;  /* default mtu, 1k best on mellanox */\n\nstatic int exchange_data(int sock, int is_server, void *xin, void *xout,\n                         size_t len);\nstatic void verify_prop_caps(VAPI_qp_cap_t *cap);\nstatic void init_connection_modify_qp(VAPI_qp_hndl_t qp,\n  VAPI_qp_num_t remote_qp_num, int remote_lid);\nstatic void vapi_post_rr(const ib_connection_t *c, struct buf_head *bh);\nstatic void __attribute__((noreturn,format(printf,2,3)))\n  error_verrno(int ecode, const char *fmt, ...);\nint vapi_ib_initialize(void);\nstatic void vapi_ib_finalize(void);\n\n/*\n * Build new conneciton and do the QP bringup dance.\n */\nstatic int vapi_new_connection(ib_connection_t *c, int sock, int is_server)\n{\n    struct vapi_connection_priv *vc;\n    struct vapi_device_priv *vd = ib_device->priv;\n    int i, ret;\n    VAPI_mr_t mr, mr_out;\n    VAPI_qp_init_attr_t qp_init_attr;\n    VAPI_qp_prop_t prop;\n    /* for connection handshake with peer */\n    struct {\n\tIB_lid_t lid;\n\tVAPI_qp_num_t qp_num;\n    } ch_in, ch_out;\n\n    vc = bmi_ib_malloc(sizeof(*vc));\n    c->priv = vc;\n\n    /* register memory region, recv */\n    mr.type = VAPI_MR;\n    mr.start = int64_from_ptr(c->eager_recv_buf_contig);\n    mr.size = ib_device->eager_buf_num * ib_device->eager_buf_size;\n    mr.pd_hndl = vd->nic_pd;\n    mr.acl = VAPI_EN_LOCAL_WRITE | VAPI_EN_REMOTE_WRITE;\n    ret = VAPI_register_mr(vd->nic_handle, &mr, &vc->eager_recv_mr, &mr_out);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: register_mr eager recv\", __func__);\n    vc->eager_recv_lkey = mr_out.l_key;\n\n    /* register memory region, send */\n    mr.type = VAPI_MR;\n    mr.start = int64_from_ptr(c->eager_send_buf_contig);\n    mr.size = ib_device->eager_buf_num * ib_device->eager_buf_size;\n    mr.pd_hndl = vd->nic_pd;\n    mr.acl = VAPI_EN_LOCAL_WRITE;\n    ret = VAPI_register_mr(vd->nic_handle, &mr, &vc->eager_send_mr, &mr_out);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: register_mr bounce\", __func__);\n    vc->eager_send_lkey = mr_out.l_key;\n\n    /* common qp properites */\n    qp_init_attr.cap.max_oust_wr_sq = 5000;  /* outstanding WQEs */\n    qp_init_attr.cap.max_oust_wr_rq = 5000;\n    qp_init_attr.cap.max_sg_size_sq = 20;  /* scatter/gather entries */\n    qp_init_attr.cap.max_sg_size_rq = 20;\n    qp_init_attr.pd_hndl            = vd->nic_pd;\n    qp_init_attr.rdd_hndl           = 0;\n    /* wire both send and recv to the same CQ */\n    qp_init_attr.sq_cq_hndl         = vd->nic_cq;\n    qp_init_attr.rq_cq_hndl         = vd->nic_cq;\n    /* only generate completion queue entries if requested */\n    qp_init_attr.sq_sig_type        = VAPI_SIGNAL_REQ_WR;\n    qp_init_attr.rq_sig_type        = VAPI_SIGNAL_REQ_WR;\n    qp_init_attr.ts_type            = VAPI_TS_RC;\n\n    /* build main qp */\n    ret = VAPI_create_qp(vd->nic_handle, &qp_init_attr, &vc->qp, &prop);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: create QP\", __func__);\n    vc->qp_num = prop.qp_num;\n    verify_prop_caps(&prop.cap);\n\n    /* initialize for post_sr */\n    vc->num_unsignaled_wr = 0;\n\n    /* share connection information across TCP */\n    /* sanity check sizes of things (actually only 24 bits in qp_num) */\n    bmi_ib_assert(sizeof(ch_in.lid) == sizeof(uint16_t),\n\t\t  \"%s: connection_handshake.lid size %d expecting %d\",\n\t\t  __func__, (int) sizeof(ch_in.lid), (int) sizeof(u_int16_t));\n    bmi_ib_assert(sizeof(ch_in.qp_num) == sizeof(uint32_t),\n\t\t  \"%s: connection_handshake.qp_num size %d expecting %d\",\n\t\t  __func__, (int) sizeof(ch_in.qp_num), (int) sizeof(uint32_t));\n\n    /* convert all to network order and back */\n    ch_out.lid = htobmi16(vd->nic_lid);\n    ch_out.qp_num = htobmi32(vc->qp_num);\n\n    ret = exchange_data(sock, is_server, &ch_in, &ch_out, sizeof(ch_in));\n    if (ret)\n\tgoto out;\n\n    vc->remote_lid = bmitoh16(ch_in.lid);\n    vc->remote_qp_num = bmitoh32(ch_in.qp_num);\n\n    /* bring the two QPs up to RTR */\n    init_connection_modify_qp(vc->qp, vc->remote_qp_num, vc->remote_lid);\n\n    /* post initial RRs */\n    for (i=0; i<ib_device->eager_buf_num; i++)\n\tvapi_post_rr(c, &c->eager_recv_buf_head_contig[i]);\n\n    /* final sychronize to ensure both sides have posted RRs */\n    ret = exchange_data(sock, is_server, &ret, &ret, sizeof(ret));\n\n  out:\n    return ret;\n}\n\n/*\n * Exchange information: server reads first, then writes; client opposite.\n */\nstatic int exchange_data(int sock, int is_server, void *xin, void *xout,\n                         size_t len)\n{\n    int i;\n    int ret;\n\n    for (i=0; i<2; i++) {\n\tif (i ^ is_server) {\n\t    ret = read_full(sock, xin, len);\n\t    if (ret < 0) {\n\t\twarning_errno(\"%s: read\", __func__);\n\t\tgoto out;\n\t    }\n\t    if (ret != (int) len) {\n\t\tret = 1;\n\t\twarning(\"%s: partial read, %d/%d bytes\", __func__, ret,\n\t\t                                        (int) len);\n\t\tgoto out;\n\t    }\n\t} else {\n\t    ret = write_full(sock, xout, len);\n\t    if (ret < 0) {\n\t\twarning_errno(\"%s: write\", __func__);\n\t\tgoto out;\n\t    }\n\t}\n    }\n\n    ret = 0;\n\n  out:\n    return ret;\n}\n\n/*\n * If not set, set them.  Otherwise verify that none of our assumed global\n * limits are different for this new connection.\n */\nstatic void verify_prop_caps(VAPI_qp_cap_t *cap)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n\n    if (vd->sg_max_len == 0) {\n\tvd->sg_max_len = cap->max_sg_size_sq;\n\tif (cap->max_sg_size_rq < vd->sg_max_len)\n\t    vd->sg_max_len = cap->max_sg_size_rq;\n\tvd->sg_tmp_array = bmi_ib_malloc(vd->sg_max_len *\n\t\t\t\t\t sizeof(*vd->sg_tmp_array));\n    } else {\n\tif (cap->max_sg_size_sq < vd->sg_max_len)\n\t    error(\n\t      \"%s: new connection has smaller send scatter/gather array size,\"\n\t      \" %d vs %d\", __func__, cap->max_sg_size_sq, vd->sg_max_len);\n\tif (cap->max_sg_size_rq < vd->sg_max_len)\n\t    error(\n\t      \"%s: new connection has smaller recv scatter/gather array size,\"\n\t      \" %d vs %d\", __func__, cap->max_sg_size_rq, vd->sg_max_len);\n    }\n\n    if (vd->max_outstanding_wr == 0) {\n\tvd->max_outstanding_wr = cap->max_oust_wr_sq;\n    } else {\n\tif (cap->max_oust_wr_sq < vd->max_outstanding_wr)\n\t    error(\n\t      \"%s: new connection has smaller max_oust_wr_sq size, %d vs %d\",\n\t      __func__, cap->max_oust_wr_sq, vd->max_outstanding_wr);\n    }\n}\n\n/*\n * Perform the many steps required to bring up both sides of an IB connection.\n */\nstatic void init_connection_modify_qp(VAPI_qp_hndl_t qp,\n                                      VAPI_qp_num_t remote_qp_num,\n\t\t\t\t      int remote_lid)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    int ret;\n    VAPI_qp_attr_t attr;\n    VAPI_qp_attr_mask_t mask;\n    VAPI_qp_cap_t cap;\n\n    /* see HCA/vip/qpm/qp_xition.h for important settings */\n    /* transition qp to init */\n    QP_ATTR_MASK_CLR_ALL(mask);\n    QP_ATTR_MASK_SET(mask,\n       QP_ATTR_QP_STATE\n     | QP_ATTR_REMOTE_ATOMIC_FLAGS\n     | QP_ATTR_PKEY_IX\n     | QP_ATTR_PORT);\n    attr.qp_state = VAPI_INIT;\n    attr.remote_atomic_flags = VAPI_EN_REM_WRITE;\n    attr.pkey_ix = 0;\n    attr.port = VAPI_PORT;\n    ret = VAPI_modify_qp(vd->nic_handle, qp, &attr, &mask, &cap);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_modify_qp RST -> INIT\", __func__);\n\n    /* transition qp to ready-to-receive */\n    QP_ATTR_MASK_CLR_ALL(mask);\n    QP_ATTR_MASK_SET(mask,\n       QP_ATTR_QP_STATE\n     | QP_ATTR_QP_OUS_RD_ATOM\n     | QP_ATTR_AV\n     | QP_ATTR_PATH_MTU\n     | QP_ATTR_RQ_PSN\n     | QP_ATTR_DEST_QP_NUM\n     | QP_ATTR_MIN_RNR_TIMER);\n    attr.qp_state = VAPI_RTR;\n    attr.qp_ous_rd_atom = 0;\n    memset(&attr.av, 0, sizeof(attr.av));\n    attr.av.dlid = remote_lid;\n    attr.path_mtu = VAPI_MTU;\n    attr.rq_psn = 0;\n    attr.dest_qp_num = remote_qp_num;\n    attr.min_rnr_timer = IB_RNR_NAK_TIMER_491_52;\n    ret = VAPI_modify_qp(vd->nic_handle, qp, &attr, &mask, &cap);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_modify_qp INIT -> RTR\", __func__);\n\n    /* transition qp to ready-to-send */\n    QP_ATTR_MASK_CLR_ALL(mask);\n    QP_ATTR_MASK_SET(mask,\n       QP_ATTR_QP_STATE\n     | QP_ATTR_SQ_PSN\n     | QP_ATTR_OUS_DST_RD_ATOM\n     | QP_ATTR_TIMEOUT\n     | QP_ATTR_RETRY_COUNT\n     | QP_ATTR_RNR_RETRY\n     );\n    attr.qp_state = VAPI_RTS;\n    attr.sq_psn = 0;\n    attr.ous_dst_rd_atom = 0;\n    attr.timeout = 26;  /* 4.096us * 2^26 = 5 min */\n    attr.retry_count = 20;\n    attr.rnr_retry = 20;\n    ret = VAPI_modify_qp(vd->nic_handle, qp, &attr, &mask, &cap);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_modify_qp RTR -> RTS\", __func__);\n}\n\n/*\n * Close the QP associated with this connection.  Used to wait for drain to\n * finish, but many seconds pass before the adapter tells us about it via an\n * asynch event.  Perhaps there is a way to do it via polling.\n */\nstatic void vapi_drain_qp(ib_connection_t *c)\n{\n    struct vapi_connection_priv *vc = c->priv;\n    struct vapi_device_priv *vd = ib_device->priv;\n    VAPI_qp_hndl_t qp = vc->qp;\n    int ret;\n    /* int trips; */\n    VAPI_qp_attr_t attr;\n    VAPI_qp_attr_mask_t mask;\n    VAPI_qp_cap_t cap;\n\n    /* transition to drain */\n    QP_ATTR_MASK_CLR_ALL(mask);\n    QP_ATTR_MASK_SET(mask,\n       QP_ATTR_QP_STATE);\n     /* | QP_ATTR_EN_SQD_ASYN_NOTIF); */\n    attr.qp_state = VAPI_SQD;\n    /* attr.en_sqd_asyn_notif = 1; */\n    ret = VAPI_modify_qp(vd->nic_handle, qp, &attr, &mask, &cap);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_modify_qp RTS -> SQD\", __func__);\n}\n\n/*\n * At an explicit BYE message, or at finalize time, shut down a connection.\n * If descriptors are posted, defer and clean up the connection structures\n * later.\n */\nstatic void vapi_close_connection(ib_connection_t *c)\n{\n    int ret;\n    struct vapi_connection_priv *vc = c->priv;\n    struct vapi_device_priv *vd = ib_device->priv;\n\n    ret = VAPI_destroy_qp(vd->nic_handle, vc->qp);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_destroy_qp\", __func__);\n    ret = VAPI_deregister_mr(vd->nic_handle, vc->eager_send_mr);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_deregister_mr eager send\", __func__);\n    ret = VAPI_deregister_mr(vd->nic_handle, vc->eager_recv_mr);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_deregister_mr eager recv\", __func__);\n\n    free(vc);\n}\n\n/*\n * VAPI interface to post sends.  Not RDMA, just SEND.\n * Called for an eager send, rts send, or cts send.\n */\nstatic void vapi_post_sr(const struct buf_head *bh, u_int32_t len)\n{\n    VAPI_sg_lst_entry_t sg;\n    VAPI_sr_desc_t sr;\n    int ret;\n    ib_connection_t *c = bh->c;\n    struct vapi_connection_priv *vc = c->priv;\n    struct vapi_device_priv *vd = ib_device->priv;\n\n    debug(2, \"%s: %s bh %d len %u wr %d/%d\", __func__, c->peername, bh->num,\n      len, vc->num_unsignaled_wr, vd->max_outstanding_wr);\n    sg.addr = int64_from_ptr(bh->buf);\n    sg.len = len;\n    sg.lkey = vc->eager_send_lkey;\n\n    memset(&sr, 0, sizeof(sr));\n    sr.opcode = VAPI_SEND;\n    sr.id = int64_from_ptr(bh);\n    sr.comp_type = VAPI_SIGNALED;\n/* no unsignaled anymore, see openib.c\n    if (++vc->num_unsignaled_wr + 100 == vd->max_outstanding_wr) {\n\tvc->num_unsignaled_wr = 0;\n    } else\n\tsr.comp_type = VAPI_UNSIGNALED;\n */\n    sr.sg_lst_p = &sg;\n    sr.sg_lst_len = 1;\n    ret = VAPI_post_sr(vd->nic_handle, vc->qp, &sr);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_post_sr\", __func__);\n}\n\n/*\n * Post one of the eager recv bufs for this connection.\n */\nstatic void vapi_post_rr(const ib_connection_t *c, struct buf_head *bh)\n{\n    VAPI_sg_lst_entry_t sg;\n    VAPI_rr_desc_t rr;\n    int ret;\n    struct vapi_connection_priv *vc = c->priv;\n    struct vapi_device_priv *vd = ib_device->priv;\n\n    debug(2, \"%s: %s bh %d\", __func__, c->peername, bh->num);\n    sg.addr = int64_from_ptr(bh->buf);\n    sg.len = ib_device->eager_buf_size;\n    sg.lkey = vc->eager_recv_lkey;\n\n    memset(&rr, 0, sizeof(rr));\n    rr.opcode = VAPI_RECEIVE;\n    rr.id = int64_from_ptr(bh);\n    rr.sg_lst_p = &sg;\n    rr.sg_lst_len = 1;\n    ret = VAPI_post_rr(vd->nic_handle, vc->qp, &rr);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_post_rr\", __func__);\n}\n\n/*\n * Called only in response to receipt of a CTS on the sender.  RDMA write\n * the big data to the other side.  A bit messy since an RDMA write may\n * not scatter to the receiver, but can gather from the sender, and we may\n * have a non-trivial buflist on both sides.  The mh_cts variable length\n * fields must be decoded as we go.\n */\nstatic void vapi_post_sr_rdmaw(struct ib_work *sq, msg_header_cts_t *mh_cts,\n                               void *mh_cts_buf)\n{\n    VAPI_sr_desc_t sr;\n    int done;\n    ib_connection_t *c = sq->c;\n    struct vapi_connection_priv *vc = c->priv;\n    struct vapi_device_priv *vd = ib_device->priv;\n\n    int send_index = 0, recv_index = 0;    /* working entry in buflist */\n    int send_offset = 0;  /* byte offset in working send entry */\n    u_int64_t *recv_bufp = (u_int64_t *) mh_cts_buf;\n    u_int32_t *recv_lenp = (u_int32_t *)(recv_bufp + mh_cts->buflist_num);\n    u_int32_t *recv_rkey = (u_int32_t *)(recv_lenp + mh_cts->buflist_num);\n    u_int32_t recv_bytes_needed = 0;\n\n    debug(2, \"%s: sq %p totlen %d\", __func__, sq, (int) sq->buflist.tot_len);\n\n#if MEMCACHE_BOUNCEBUF\n    if (reg_send_buflist.num == 0) {\n\treg_send_buflist.num = 1;\n\treg_send_buflist.buf.recv = &reg_send_buflist_buf;\n\treg_send_buflist.len = &reg_send_buflist_len;\n\treg_send_buflist.tot_len = reg_send_buflist_len;\n\treg_send_buflist_buf = bmi_ib_malloc(reg_send_buflist_len);\n\tmemcache_register(ib_device->memcache, &reg_send_buflist);\n    }\n    if (sq->buflist.tot_len > reg_send_buflist_len)\n\terror(\"%s: send prereg buflist too small, need %lld\", __func__,\n\t  lld(sq->buflist.tot_len));\n    memcpy_from_buflist(&sq->buflist, reg_send_buflist_buf);\n\n    ib_buflist_t save_buflist = sq->buflist;\n    sq->buflist = reg_send_buflist;\n\n#else\n#if !MEMCACHE_EARLY_REG\n    memcache_register(ib_device->memcache, &sq->buflist);\n#endif\n#endif\n\n    /* constant things for every send */\n    memset(&sr, 0, sizeof(sr));\n    sr.opcode = VAPI_RDMA_WRITE;\n    sr.comp_type = VAPI_UNSIGNALED;\n    sr.sg_lst_p = vd->sg_tmp_array;\n\n    done = 0;\n    while (!done) {\n\tint ret;\n\n\tif (recv_bytes_needed == 0) {\n\t    /* new one, fresh numbers */\n\t    sr.remote_addr = bmitoh64(recv_bufp[recv_index]);\n\t    recv_bytes_needed = bmitoh32(recv_lenp[recv_index]);\n\t} else {\n\t    /* continuing into unfinished remote receive index */\n\t    sr.remote_addr +=\n\t\tbmitoh32(recv_lenp[recv_index]) - recv_bytes_needed;\n\t}\n\n\tsr.r_key = bmitoh32(recv_rkey[recv_index]);\n\tsr.sg_lst_len = 0;\n\n\tdebug(4, \"%s: chunk to %s remote addr %llx rkey %x\",\n\t  __func__, c->peername, llu(sr.remote_addr), sr.r_key);\n\n\t/*\n\t * Driven by recv elements.  Sizes have already been checked.\n\t */\n\twhile (recv_bytes_needed > 0 && sr.sg_lst_len < vd->sg_max_len) {\n\t    /* consume from send buflist to fill this one receive */\n\t    u_int32_t send_bytes_offered\n\t      = sq->buflist.len[send_index] - send_offset;\n\t    u_int32_t this_bytes = send_bytes_offered;\n\t    if (this_bytes > recv_bytes_needed)\n\t\tthis_bytes = recv_bytes_needed;\n\n\t    vd->sg_tmp_array[sr.sg_lst_len].addr =\n\t      int64_from_ptr(sq->buflist.buf.send[send_index])\n\t      + send_offset;\n\t    vd->sg_tmp_array[sr.sg_lst_len].len = this_bytes;\n\t    vd->sg_tmp_array[sr.sg_lst_len].lkey =\n\t      sq->buflist.memcache[send_index]->memkeys.lkey;\n\n\t    debug(4, \"%s: chunk %d local addr %llx len %d lkey %x\",\n\t      __func__, sr.sg_lst_len,\n\t      llu(vd->sg_tmp_array[sr.sg_lst_len].addr),\n\t      vd->sg_tmp_array[sr.sg_lst_len].len,\n\t      vd->sg_tmp_array[sr.sg_lst_len].lkey);\n\n\t    ++sr.sg_lst_len;\n\t    send_offset += this_bytes;\n\t    if (send_offset == sq->buflist.len[send_index]) {\n\t\t++send_index;\n\t\tsend_offset = 0;\n\t\tif (send_index == sq->buflist.num) {\n\t\t    done = 1;\n\t\t    break;  /* short send */\n\t\t}\n\t    }\n\t    recv_bytes_needed -= this_bytes;\n\t}\n\n\t/* done with the one we were just working on, is this the last recv? */\n\tif (recv_bytes_needed == 0) {\n\t    ++recv_index;\n\t    if (recv_index == (int)mh_cts->buflist_num)\n\t\tdone = 1;\n\t}\n\n\t/* either filled the recv or exhausted the send */\n\tif (done) {\n\t    sr.id = int64_from_ptr(sq);    /* used to match in completion */\n\t    sr.comp_type = VAPI_SIGNALED;  /* completion drives the unpin */\n\t} else {\n\t    sr.id = 0;\n\t    sr.comp_type = VAPI_UNSIGNALED;\n\t}\n\tret = VAPI_post_sr(vd->nic_handle, vc->qp, &sr);\n\tif (ret < 0)\n\t    error_verrno(ret, \"%s: VAPI_post_sr\", __func__);\n    }\n#if MEMCACHE_BOUNCEBUF\n    sq->buflist = save_buflist;\n#endif\n}\n\n/*\n * Get one entry from completion queue, return 1 if found something, 0\n * if CQ empty.  Die if some error.\n */\nstatic int vapi_check_cq(struct bmi_ib_wc *wc)\n{\n    int ret;\n    VAPI_wc_desc_t desc;\n    struct vapi_device_priv *vd = ib_device->priv;\n    \n    ret = VAPI_poll_cq(vd->nic_handle, vd->nic_cq, &desc);\n    if (ret < 0) {\n\tif (ret == VAPI_CQ_EMPTY)\n\t    return 0;\n\terror_verrno(ret, \"%s: VAPI_poll_cq\", __func__);\n    }\n\n    /* convert to generic form */\n    wc->id = desc.id;\n    wc->status = desc.status;\n    wc->byte_len = desc.byte_len;\n    if (desc.opcode == VAPI_CQE_SQ_SEND_DATA)\n\twc->opcode = BMI_IB_OP_SEND;\n    else if (desc.opcode == VAPI_CQE_RQ_SEND_DATA)\n\twc->opcode = BMI_IB_OP_RECV;\n    else if (desc.opcode == VAPI_CQE_SQ_RDMA_WRITE)\n\twc->opcode = BMI_IB_OP_RDMA_WRITE;\n    else\n\terror(\"%s: unknown opcode %d\", __func__, desc.opcode);\n    return 1;\n}\n\nstatic void vapi_prepare_cq_block(int *cq_fd, int *async_fd)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    int ret;\n\n    /* ask for the next notfication */\n    ret = VAPI_req_comp_notif(vd->nic_handle, vd->nic_cq, VAPI_NEXT_COMP);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_req_comp_notif\", __func__);\n\n    /* return the fd that can be fed to poll() */\n    *cq_fd = vd->cq_event_pipe[0];\n    *async_fd = vd->async_event_pipe[0];\n}\n\n/*\n * Read an event that tells us there is some action on the CQ.  In\n * reality, just read the int from the pipe that connects us to the\n * event handler thread.\n */\nstatic void vapi_ack_cq_completion_event(void)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    int i, ret;\n\n    ret = read(vd->cq_event_pipe[0], &i, sizeof(i));\n    if (ret != sizeof(i))\n\terror_errno(\"%s: read cq event pipe\", __func__);\n}\n\n/*\n * Return string form of work completion status field.\n */\nstatic const char *vapi_wc_status_string(int status)\n{\n    return VAPI_wc_status_sym(status);\n}\n\n#define CASE(e)  case e: s = #e; break\nstatic const char *vapi_port_state_string(IB_port_state_t state)\n{\n    const char *s = \"(UNKNOWN)\";\n\n    switch (state) {\n\tCASE(PORT_NOP);\n\tCASE(PORT_DOWN);\n\tCASE(PORT_INITIALIZE);\n\tCASE(PORT_ARMED);\n\tCASE(PORT_ACTIVE);\n    }\n    return s;\n}\n#undef CASE\n\n\n/*\n * Memory registration and deregistration.  Used both by sender and\n * receiver, vary if lkey or rkey = 0.\n */\nstatic int vapi_mem_register(memcache_entry_t *c)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    VAPI_mrw_t mrw, mrw_out;\n    VAPI_mr_hndl_t mrh;\n    int ret;\n\n    /* always turn on local write and write even if just BMI_SEND */\n    mrw.acl = VAPI_EN_LOCAL_WRITE | VAPI_EN_REMOTE_WRITE;\n    mrw.type = VAPI_MR;\n    mrw.pd_hndl = vd->nic_pd;\n    mrw.start = int64_from_ptr(c->buf);\n    mrw.size = c->len;\n    ret = VAPI_register_mr(vd->nic_handle, &mrw, &mrh, &mrw_out);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_register_mr\", __func__);\n    c->memkeys.mrh = mrh;  /* store in 64-bit int */\n    c->memkeys.lkey = mrw_out.l_key;\n    c->memkeys.rkey = mrw_out.r_key;\n    debug(4, \"%s: buf %p len %lld\", __func__, c->buf, lld(c->len));\n    return 0;\n}\n\nstatic void vapi_mem_deregister(memcache_entry_t *c)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    VAPI_mr_hndl_t mrh;\n    int ret;\n    \n    mrh = c->memkeys.mrh;  /* retrieve 32-bit from 64-bit int */\n    ret = VAPI_deregister_mr(vd->nic_handle, mrh);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_deregister_mr\", __func__);\n    debug(4, \"%s: buf %p len %lld lkey %x rkey %x\", __func__,\n      c->buf, lld(c->len), c->memkeys.lkey, c->memkeys.rkey);\n}\n\n/*\n * Format vapi-specific error code.\n */\nstatic void __attribute__((noreturn,format(printf,2,3)))\nerror_verrno(int ecode, const char *fmt, ...)\n{\n    char s[2048];\n    va_list ap;\n\n    va_start(ap, fmt);\n    vsprintf(s, fmt, ap);\n    va_end(ap);\n    gossip_err(\"Error: %s: %s\\n\", s, VAPI_strerror(ecode));  /* adds a dot */\n    exit(1);\n}\n\n/*\n * Catch errors from IB.  This is invoked in its own thread created\n * by libvapi.  Just ship the event down an fd and read it later when\n * we want to get it, like the OpenIB model.\n */\nstatic void\nasync_event_handler(VAPI_hca_hndl_t nic_handle_in __attribute__((unused)),\n  VAPI_event_record_t *e, void *private_data __attribute__((unused)) )\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    int ret;\n\n    ret = write(vd->async_event_pipe[1], e, sizeof(*e));\n    if (ret != sizeof(*e))\n\terror_errno(\"%s: write async event pipe\", __func__);\n}\n\n/*\n * To deal with blocking completion events.  Just write down a pipe\n * that something is ready to go; signaling to poll that an event is\n * ready.\n */\nstatic void cq_event_handler(VAPI_hca_hndl_t hca __attribute__((unused)),\n                             VAPI_cq_hndl_t cq __attribute__((unused)),\n\t\t             void *private_data __attribute__((unused)))\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    int i = 0, ret;\n\n    ret = write(vd->cq_event_pipe[1], &i, sizeof(i));\n    if (ret != sizeof(i))\n\terror_errno(\"%s: write cq event pipe\", __func__);\n}\n\n#ifdef HAVE_IB_WRAP_COMMON_H\nextern int mosal_fd;\n#endif\n\n/*\n * Hack to work around fork in daemon mode which confuses kernel\n * state.  I wish they did not have an _init constructor function in\n * libmosal.so.  It calls into MOSAL_user_lib_init().\n * This just breaks its saved state and reinitializes.  (task->mm\n * changes due to fork after init, hash lookup on that fails.)\n *\n * Seems to work even in the case of a non-backgrounded server too,\n * fortunately.\n *\n * Note that even with shared libraries you do not have protection\n * against wandering headers.  The thca distributions have in the\n * past been eager to change critical #defines like VAPI_CQ_EMPTY\n * so libpvfs2.so is more or less tied to the vapi.h against which\n * it was compiled.\n */\nstatic void reinit_mosal(void)\n{\n    void *dlh;\n    int (*mosal_ioctl_open)(void);\n    int (*mosal_ioctl_close)(void);\n\n    dlh = dlopen(\"libmosal.so\", RTLD_LAZY);\n    if (!dlh)\n\terror(\"%s: cannot open libmosal shared library\", __func__);\n\n#ifdef HAVE_IB_WRAP_COMMON_H\n    {\n    /*\n     * What's happening here is we probe the internals of the mosal library\n     * to get it to return a structure that has the current fd and state\n     * of the connection to /dev/mosal.  We close it, reset the state, and\n     * force it to reinitialize itself.  Icky, but effective.  Only necessary\n     * for older thca distributions that install the needed header and\n     * have this symbol in the library.\n     *\n     * Else fall through to the easier method.\n     */\n    call_result_t (*_dev_mosal_init_lib)(t_lib_descriptor **pp_t_lib);\n    const char *errmsg;\n\n    _dev_mosal_init_lib = dlsym(dlh, \"_dev_mosal_init_lib\");\n    errmsg = dlerror();\n    if (errmsg == NULL) {\n\tt_lib_descriptor *desc;\n\tint ret;\n\n\tret = (*_dev_mosal_init_lib)(&desc);\n\tdebug(2, \"%s: mosal init ret %d, desc %p\", __func__, ret, desc);\n\tdebug(2, \"%s: desc->fd %d\", __func__, desc->os_lib_desc_st.fd);\n\tclose(desc->os_lib_desc_st.fd);\n\t/* both these state items protect against a reinit */\n\tdesc->state = 0;\n\tmosal_fd = -1;\n\tMOSAL_user_lib_init();\n\treturn;\n    }\n    }\n#endif\n\n    /*\n     * Recent thca distros and the 2.6 openib tree do not seem to permit\n     * any way to \"trick\" the library as above, but there's no need for\n     * the hack now that they export a \"finalize\" function to undo the init.\n     */\n    mosal_ioctl_open = dlsym(dlh, \"mosal_ioctl_open\");\n    if (dlerror())\n\terror(\"%s: mosal_ioctl_open not found in libmosal\", __func__);\n    mosal_ioctl_close = dlsym(dlh, \"mosal_ioctl_close\");\n    if (dlerror())\n\terror(\"%s: mosal_ioctl_close not found in libmosal\", __func__);\n\n    (*mosal_ioctl_close)();\n    (*mosal_ioctl_open)();\n}\n\n/*\n * Just catch and report the events, do not try to do anything with\n * them.\n */\nstatic int vapi_check_async_events(void)\n{\n    struct vapi_device_priv *vd = ib_device->priv;\n    int ret;\n    VAPI_event_record_t ev;\n\n    ret = read(vd->async_event_pipe[0], &ev, sizeof(ev));\n    if (ret < 0) {\n\tif (errno == EAGAIN)\n\t    return 0;\n\terror_errno(\"%s: read async event pipe\", __func__);\n    }\n    warning(\"%s: %s\", __func__, VAPI_event_record_sym(ev.type));\n    return 1;\n}\n\n/*\n * VAPI-specific startup.\n */\nint vapi_ib_initialize(void)\n{\n    int ret, flags;\n    u_int32_t num_hcas;\n    VAPI_hca_id_t hca_ids[10];\n    VAPI_hca_port_t nic_port_props;\n    VAPI_hca_vendor_t vendor_cap;\n    VAPI_hca_cap_t hca_cap;\n    VAPI_cqe_num_t cqe_num, cqe_num_out;\n    struct vapi_device_priv *vd;\n\n    reinit_mosal();\n\n    /* look for exactly one and take it */\n    ret = EVAPI_list_hcas(sizeof(hca_ids)/sizeof(hca_ids[0]), &num_hcas,\n                          hca_ids);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: EVAPI_list_hcas\", __func__);\n    if (num_hcas == 0) {\n\twarning(\"%s: no hcas detected\", __func__);\n\treturn -ENODEV;\n    }\n    if (num_hcas > 1)\n\twarning(\"%s: found %d HCAs, choosing the first\", __func__, num_hcas);\n\n    vd = bmi_ib_malloc(sizeof(*vd));\n    ib_device->priv = vd;\n\n    /* set the function pointers for vapi */\n    ib_device->func.new_connection = vapi_new_connection;\n    ib_device->func.close_connection = vapi_close_connection;\n    ib_device->func.drain_qp = vapi_drain_qp;\n    ib_device->func.ib_initialize = vapi_ib_initialize;\n    ib_device->func.ib_finalize = vapi_ib_finalize;\n    ib_device->func.post_sr = vapi_post_sr;\n    ib_device->func.post_rr = vapi_post_rr;\n    ib_device->func.post_sr_rdmaw = vapi_post_sr_rdmaw;\n    ib_device->func.check_cq = vapi_check_cq;\n    ib_device->func.prepare_cq_block = vapi_prepare_cq_block;\n    ib_device->func.ack_cq_completion_event = vapi_ack_cq_completion_event;\n    ib_device->func.wc_status_string = vapi_wc_status_string;\n    ib_device->func.mem_register = vapi_mem_register;\n    ib_device->func.mem_deregister = vapi_mem_deregister;\n    ib_device->func.check_async_events = vapi_check_async_events;\n\n    /*\n     * Apparently VAPI_open_hca() is a once-per-machine sort of thing, and\n     * users are not expected to call it.  It returns EBUSY every time.\n     * This call initializes the per-process user resources and starts up\n     * all the threads.  Discard const char* for silly mellanox prototype;\n     * it really is treated as constant.\n     */\n    ret = EVAPI_get_hca_hndl(hca_ids[0], &vd->nic_handle);\n    if (ret < 0)\n\terror(\"%s: could not get HCA handle\", __func__);\n\n    /* connect an asynchronous event handler to look for weirdness */\n    ret = EVAPI_set_async_event_handler(vd->nic_handle, async_event_handler, 0,\n                                        &vd->nic_async_event_handler);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: EVAPI_set_async_event_handler\", __func__);\n\n    /* get the lid and verify port state */\n    /* ignore different-width-prototype warning here, cannot pass u8 */\n    ret = VAPI_query_hca_port_prop(vd->nic_handle, VAPI_PORT, &nic_port_props);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_query_hca_port_prop\", __func__);\n    vd->nic_lid = nic_port_props.lid;\n\n    if (nic_port_props.state != PORT_ACTIVE)\n\terror(\"%s: port state is %s but should be ACTIVE; check subnet manager\",\n\t      __func__, vapi_port_state_string(nic_port_props.state));\n\n    /* build a protection domain */\n    ret = VAPI_alloc_pd(vd->nic_handle, &vd->nic_pd);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_create_pd\", __func__);\n    /* ulong in 2.6, uint32 in 2.4 */\n    debug(2, \"%s: built pd %lx\", __func__, (unsigned long) vd->nic_pd);\n\n    /* see how many cq entries we are allowed to have */\n    memset(&hca_cap, 0, sizeof(hca_cap));\n    ret = VAPI_query_hca_cap(vd->nic_handle, &vendor_cap, &hca_cap);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_query_hca_cap\", __func__);\n\n    debug(0, \"%s: max %d completion queue entries\", __func__,\n      hca_cap.max_num_cq);\n    cqe_num = VAPI_NUM_CQ_ENTRIES;\n    if (hca_cap.max_num_cq < cqe_num) {\n\tcqe_num = hca_cap.max_num_cq;\n\twarning(\"%s: hardly enough completion queue entries %d, hoping for %d\",\n\t  __func__, hca_cap.max_num_cq, cqe_num);\n    }\n\n    /* build a CQ (ignore actual number returned) */\n    debug(0, \"%s: asking for %d completion queue entries\", __func__, cqe_num);\n    ret = VAPI_create_cq(vd->nic_handle, cqe_num, &vd->nic_cq, &cqe_num_out);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_create_cq ret %d\", __func__, ret);\n\n    /* create completion \"channel\" */\n    ret = pipe(vd->cq_event_pipe);\n    if (ret < 0)\n\terror_errno(\"%s: pipe\", __func__);\n    flags = fcntl(vd->cq_event_pipe[0], F_GETFL);\n    if (flags < 0)\n\terror_errno(\"%s: get cq pipe flags\", __func__);\n    if (fcntl(vd->cq_event_pipe[0], F_SETFL, flags | O_NONBLOCK) < 0)\n\terror_errno(\"%s: set cq pipe nonblocking\", __func__);\n\n    /* register handler for the cq */\n    ret = EVAPI_set_comp_eventh(vd->nic_handle, vd->nic_cq, cq_event_handler,\n                                NULL, &vd->nic_cq_event_handler);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: EVAPI_set_comp_eventh\", __func__);\n\n    /* build a pipe to queue up async events, and set it to non-blocking\n     * on the receive side */\n    ret = pipe(vd->async_event_pipe);\n    if (ret < 0)\n\terror_errno(\"%s: pipe\", __func__);\n    flags = fcntl(vd->async_event_pipe[0], F_GETFL);\n    if (flags < 0)\n\terror_errno(\"%s: get async pipe flags\", __func__);\n    if (fcntl(vd->async_event_pipe[0], F_SETFL, flags | O_NONBLOCK) < 0)\n\terror_errno(\"%s: set async pipe nonblocking\", __func__);\n\n    /* will be set on first connection */\n    vd->sg_tmp_array = NULL;\n    vd->sg_max_len = 0;\n    vd->max_outstanding_wr = 0;\n\n    return 0;\n}\n\n/*\n * VAPI shutdown.\n */\nstatic void vapi_ib_finalize(void)\n{\n    int ret;\n    struct vapi_device_priv *vd = ib_device->priv;\n\n    if (vd->sg_tmp_array)\n\tfree(vd->sg_tmp_array);\n    ret = EVAPI_clear_comp_eventh(vd->nic_handle, vd->nic_cq_event_handler);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: EVAPI_clear_comp_eventh\", __func__);\n\n    close(vd->cq_event_pipe[0]);\n    close(vd->cq_event_pipe[1]);\n\n    ret = VAPI_destroy_cq(vd->nic_handle, vd->nic_cq);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_destroy_cq\", __func__);\n\n    ret = VAPI_dealloc_pd(vd->nic_handle, vd->nic_pd);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_dealloc_pd\", __func__);\n    ret = EVAPI_clear_async_event_handler(vd->nic_handle,\n                                          vd->nic_async_event_handler);\n    if (ret < 0)\n\terror_verrno(ret, \"%s: EVAPI_clear_async_event_handler\", __func__);\n    ret = EVAPI_release_hca_hndl(vd->nic_handle);\n\n    close(vd->async_event_pipe[0]);\n    close(vd->async_event_pipe[1]);\n\n    if (ret < 0)\n\terror_verrno(ret, \"%s: EVAPI_release_hca_hndl\", __func__);\n    ret = VAPI_close_hca(vd->nic_handle);\n    /*\n     * Buggy vapi always returns EBUSY, just like for the open\n    if (ret < 0)\n\terror_verrno(ret, \"%s: VAPI_close_hca\", __func__);\n     */\n\n    free(vd);\n    ib_device->priv = NULL;\n}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/pvfs2-util.c",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/tcache.h",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/pint-perf-counter.h",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/str-utils.c",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/pint-util.c",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/tcache.c",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/src/common/misc/pint-perf-counter.c",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/.git/objects/pack/pack-da1584e9371fb9fbf2aa505d58bcc95fdc31d337.pack",
        "/tmp/vanessa/spack-stage/spack-stage-bmi-develop-4eq2cbbbydyhtvygu7u6snwtux724ohr/spack-src/.git/objects/pack/pack-da1584e9371fb9fbf2aa505d58bcc95fdc31d337.idx"
    ],
    "total_files": 232
}