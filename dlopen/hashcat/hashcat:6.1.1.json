{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/src/backend.c": "/**\n * Author......: See docs/credits.txt\n * License.....: MIT\n */\n\n#include \"common.h\"\n#include \"types.h\"\n#include \"memory.h\"\n#include \"locking.h\"\n#include \"thread.h\"\n#include \"timer.h\"\n#include \"tuningdb.h\"\n#include \"rp.h\"\n#include \"rp_cpu.h\"\n#include \"mpsp.h\"\n#include \"convert.h\"\n#include \"stdout.h\"\n#include \"filehandling.h\"\n#include \"wordlist.h\"\n#include \"shared.h\"\n#include \"hashes.h\"\n#include \"emu_inc_hash_md5.h\"\n#include \"event.h\"\n#include \"dynloader.h\"\n#include \"backend.h\"\n\n#if defined (__linux__)\nstatic const char *dri_card0_path = \"/dev/dri/card0\";\n\nstatic const char *drm_card0_vendor_path = \"/sys/class/drm/card0/device/vendor\";\nstatic const char *drm_card0_driver_path = \"/sys/class/drm/card0/device/driver\";\n#endif\n\nstatic const u32 full01 = 0x01010101;\nstatic const u32 full06 = 0x06060606;\nstatic const u32 full80 = 0x80808080;\n\nstatic double TARGET_MSEC_PROFILE[4] = { 2, 12, 96, 480 };\n\nstatic bool is_same_device (const hc_device_param_t *src, const hc_device_param_t *dst)\n{\n  // First check by PCI address\n\n  if (src->pcie_domain   != dst->pcie_domain)   return false; // PCI domain not available on OpenCL\n  if (src->pcie_bus      != dst->pcie_bus)      return false;\n  if (src->pcie_device   != dst->pcie_device)   return false;\n  if (src->pcie_function != dst->pcie_function) return false;\n\n  // macOS still can't distinguish the devices by PCIe bus:\n\n  if (src->device_processors != dst->device_processors) return false;\n\n  // CUDA can't have aliases\n\n  if ((src->is_cuda == true) && (dst->is_cuda == true)) return false;\n\n  // But OpenCL can have aliases\n\n  if ((src->is_opencl == true) && (dst->is_opencl == true))\n  {\n    // Intel CPU and embedded GPU would survive up to here!\n\n    if (src->opencl_device_type != dst->opencl_device_type) return false;\n\n    // There should be no aliases on the same opencl platform\n\n    if (src->opencl_platform_id == dst->opencl_platform_id) return false;\n  }\n\n  return true;\n}\n\nstatic int backend_ctx_find_alias_devices (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  // first identify all aliases\n\n  for (int backend_devices_cnt_src = 0; backend_devices_cnt_src < backend_ctx->backend_devices_cnt; backend_devices_cnt_src++)\n  {\n    hc_device_param_t *device_param_src = &backend_ctx->devices_param[backend_devices_cnt_src];\n\n    for (int backend_devices_cnt_dst = backend_devices_cnt_src + 1; backend_devices_cnt_dst < backend_ctx->backend_devices_cnt; backend_devices_cnt_dst++)\n    {\n      hc_device_param_t *device_param_dst = &backend_ctx->devices_param[backend_devices_cnt_dst];\n\n      if (is_same_device (device_param_src, device_param_dst) == false) continue;\n\n      device_param_src->device_id_alias_buf[device_param_src->device_id_alias_cnt] = device_param_dst->device_id;\n      device_param_src->device_id_alias_cnt++;\n\n      device_param_dst->device_id_alias_buf[device_param_dst->device_id_alias_cnt] = device_param_src->device_id;\n      device_param_dst->device_id_alias_cnt++;\n    }\n  }\n\n  // find the alias to skip\n\n  for (int backend_devices_pos = 0; backend_devices_pos < backend_ctx->backend_devices_cnt; backend_devices_pos++)\n  {\n    hc_device_param_t *backend_device = &backend_ctx->devices_param[backend_devices_pos];\n\n    if (backend_device->skipped == true) continue;\n\n    if (backend_device->skipped_warning == true) continue;\n\n    for (int device_id_alias_pos = 0; device_id_alias_pos < backend_device->device_id_alias_cnt; device_id_alias_pos++)\n    {\n      const int alias_pos = backend_device->device_id_alias_buf[device_id_alias_pos];\n\n      hc_device_param_t *alias_device = &backend_ctx->devices_param[alias_pos];\n\n      if (alias_device->skipped == true) continue;\n\n      if (alias_device->skipped_warning == true) continue;\n\n      // this lets CUDA devices survive over OpenCL\n\n      if (alias_device->is_cuda == true) continue;\n\n        // this lets native OpenCL runtime survive over generic OpenCL runtime\n\n      if (alias_device->opencl_device_type & CL_DEVICE_TYPE_CPU)\n      {\n        if (alias_device->opencl_platform_vendor_id == alias_device->opencl_device_vendor_id) continue;\n      }\n\n      alias_device->skipped = true;\n\n      backend_ctx->opencl_devices_active--;\n\n      backend_ctx->backend_devices_active--;\n    }\n  }\n\n  return -1;\n}\n\nstatic bool is_same_device_type (const hc_device_param_t *src, const hc_device_param_t *dst)\n{\n  if (strcmp (src->device_name, dst->device_name) != 0) return false;\n\n  if (src->is_cuda   != dst->is_cuda)   return false;\n  if (src->is_opencl != dst->is_opencl) return false;\n\n  if (strcmp (src->device_name, dst->device_name) != 0) return false;\n\n  if (src->is_opencl == true)\n  {\n    if (strcmp (src->opencl_device_vendor,  dst->opencl_device_vendor)  != 0) return false;\n    if (strcmp (src->opencl_device_version, dst->opencl_device_version) != 0) return false;\n    if (strcmp (src->opencl_driver_version, dst->opencl_driver_version) != 0) return false;\n  }\n\n  if (src->device_processors         != dst->device_processors)         return false;\n  if (src->device_maxclock_frequency != dst->device_maxclock_frequency) return false;\n  if (src->device_maxworkgroup_size  != dst->device_maxworkgroup_size)  return false;\n\n  // memory size can be different, depending on which gpu has a monitor connected\n  // if (src->device_maxmem_alloc       != dst->device_maxmem_alloc)       return false;\n  // if (src->device_global_mem         != dst->device_global_mem)         return false;\n\n  if (src->sm_major != dst->sm_major) return false;\n  if (src->sm_minor != dst->sm_minor) return false;\n\n  if (src->kernel_exec_timeout != dst->kernel_exec_timeout) return false;\n\n  return true;\n}\n\nstatic int ocl_check_dri (MAYBE_UNUSED hashcat_ctx_t *hashcat_ctx)\n{\n  #if defined (__linux__)\n\n  // This check makes sense only if we're not root\n\n  const uid_t uid = getuid ();\n\n  if (uid == 0) return 0;\n\n  // No GPU available! That's fine, so we don't need to check if we have access to it.\n\n  if (hc_path_exist (dri_card0_path) == false) return 0;\n\n  // Now we need to check if this an AMD vendor, because this is when the problems start\n\n  FILE *fd_drm = fopen (drm_card0_vendor_path, \"rb\");\n\n  if (fd_drm == NULL) return 0;\n\n  u32 vendor = 0;\n\n  if (fscanf (fd_drm, \"0x%x\", &vendor) != 1)\n  {\n    fclose (fd_drm);\n\n    return 0;\n  }\n\n  fclose (fd_drm);\n\n  if (vendor != 4098) return 0;\n\n  // Now the problem is only with AMDGPU-PRO, not with oldschool AMD driver\n\n  char buf[HCBUFSIZ_TINY] = { 0 };\n\n  const ssize_t len = readlink (drm_card0_driver_path, buf, HCBUFSIZ_TINY - 1);\n\n  if (len == -1) return 0;\n\n  buf[len] = 0;\n\n  if (strstr (buf, \"amdgpu\") == NULL) return 0;\n\n  // Now do the real check\n\n  FILE *fd_dri = fopen (dri_card0_path, \"rb\");\n\n  if (fd_dri == NULL)\n  {\n    event_log_error (hashcat_ctx, \"Cannot access %s: %m.\", dri_card0_path);\n\n    event_log_warning (hashcat_ctx, \"This causes some drivers to crash when OpenCL is used!\");\n    event_log_warning (hashcat_ctx, \"Adding your user to the \\\"video\\\" group usually fixes this problem:\");\n    event_log_warning (hashcat_ctx, \"$ sudo usermod -a -G video $LOGNAME\");\n    event_log_warning (hashcat_ctx, NULL);\n\n    return -1;\n  }\n\n  fclose (fd_dri);\n\n  #endif // __linux__\n\n  return 0;\n}\n\nstatic bool setup_backend_devices_filter (hashcat_ctx_t *hashcat_ctx, const char *backend_devices, u64 *out)\n{\n  u64 backend_devices_filter = 0;\n\n  if (backend_devices)\n  {\n    char *devices = hcstrdup (backend_devices);\n\n    if (devices == NULL) return false;\n\n    char *saveptr = NULL;\n\n    char *next = strtok_r (devices, \",\", &saveptr);\n\n    do\n    {\n      const int backend_device_id = (const int) strtol (next, NULL, 10);\n\n      if ((backend_device_id <= 0) || (backend_device_id >= 64))\n      {\n        event_log_error (hashcat_ctx, \"Invalid device_id %d specified.\", backend_device_id);\n\n        hcfree (devices);\n\n        return false;\n      }\n\n      backend_devices_filter |= 1ULL << (backend_device_id - 1);\n\n    } while ((next = strtok_r ((char *) NULL, \",\", &saveptr)) != NULL);\n\n    hcfree (devices);\n  }\n  else\n  {\n    backend_devices_filter = -1ULL;\n  }\n\n  *out = backend_devices_filter;\n\n  return true;\n}\n\nstatic bool setup_opencl_device_types_filter (hashcat_ctx_t *hashcat_ctx, const char *opencl_device_types, cl_device_type *out)\n{\n  cl_device_type opencl_device_types_filter = 0;\n\n  if (opencl_device_types)\n  {\n    char *device_types = hcstrdup (opencl_device_types);\n\n    if (device_types == NULL) return false;\n\n    char *saveptr = NULL;\n\n    char *next = strtok_r (device_types, \",\", &saveptr);\n\n    do\n    {\n      const int device_type = (const int) strtol (next, NULL, 10);\n\n      if (device_type < 1 || device_type > 3)\n      {\n        event_log_error (hashcat_ctx, \"Invalid OpenCL device-type %d specified.\", device_type);\n\n        hcfree (device_types);\n\n        return false;\n      }\n\n      opencl_device_types_filter |= 1U << device_type;\n\n    } while ((next = strtok_r (NULL, \",\", &saveptr)) != NULL);\n\n    hcfree (device_types);\n  }\n  else\n  {\n    // Do not use CPU by default, this often reduces GPU performance because\n    // the CPU is too busy to handle GPU synchronization\n\n    opencl_device_types_filter = CL_DEVICE_TYPE_ALL & ~CL_DEVICE_TYPE_CPU;\n  }\n\n  *out = opencl_device_types_filter;\n\n  return true;\n}\n\n/*\nstatic bool cuda_test_instruction (hashcat_ctx_t *hashcat_ctx, const int sm_major, const int sm_minor, const char *kernel_buf)\n{\n  nvrtcProgram program;\n\n  if (hc_nvrtcCreateProgram (hashcat_ctx, &program, kernel_buf, \"test_instruction\", 0, NULL, NULL) == -1) return false;\n\n  char *nvrtc_options[4];\n\n  nvrtc_options[0] = \"--restrict\";\n  nvrtc_options[1] = \"--gpu-architecture\";\n\n  hc_asprintf (&nvrtc_options[2], \"compute_%d%d\", sm_major, sm_minor);\n\n  nvrtc_options[3] = NULL;\n\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcCompileProgram (program, 3, (const char * const *) nvrtc_options);\n\n  hcfree (nvrtc_options[2]);\n\n  size_t build_log_size = 0;\n\n  hc_nvrtcGetProgramLogSize (hashcat_ctx, program, &build_log_size);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    char *build_log = (char *) hcmalloc (build_log_size + 1);\n\n    if (hc_nvrtcGetProgramLog (hashcat_ctx, program, build_log) == -1) return false;\n\n    puts (build_log);\n\n    hcfree (build_log);\n\n    hc_nvrtcDestroyProgram (hashcat_ctx, &program);\n\n    return false;\n  }\n\n  size_t binary_size;\n\n  if (hc_nvrtcGetPTXSize (hashcat_ctx, program, &binary_size) == -1) return false;\n\n  char *binary = (char *) hcmalloc (binary_size);\n\n  if (hc_nvrtcGetPTX (hashcat_ctx, program, binary) == -1)\n  {\n    hcfree (binary);\n\n    return false;\n  }\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  CUmodule cuda_module;\n\n  const CUresult CU_err = cuda->cuModuleLoadDataEx (&cuda_module, binary, 0, NULL, NULL);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    hcfree (binary);\n\n    return false;\n  }\n\n  hcfree (binary);\n\n  if (hc_cuModuleUnload (hashcat_ctx, cuda_module) == -1) return false;\n\n  if (hc_nvrtcDestroyProgram (hashcat_ctx, &program) == -1) return false;\n\n  return true;\n}\n*/\n\nstatic bool opencl_test_instruction (hashcat_ctx_t *hashcat_ctx, cl_context context, cl_device_id device, const char *kernel_buf)\n{\n  cl_program program;\n\n  if (hc_clCreateProgramWithSource (hashcat_ctx, context, 1, &kernel_buf, NULL, &program) == -1) return false;\n\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n\n  #ifndef DEBUG\n  const int fd_stderr = fileno (stderr);\n  const int stderr_bak = dup (fd_stderr);\n  #ifdef _WIN\n  const int tmp = open (\"NUL\", O_WRONLY);\n  #else\n  const int tmp = open (\"/dev/null\", O_WRONLY);\n  #endif\n  dup2 (tmp, fd_stderr);\n  close (tmp);\n  #endif\n\n  const int CL_rc = ocl->clBuildProgram (program, 1, &device, NULL, NULL, NULL);\n\n  #ifndef DEBUG\n  dup2 (stderr_bak, fd_stderr);\n  close (stderr_bak);\n  #endif\n\n  if (CL_rc != CL_SUCCESS)\n  {\n    #if defined (DEBUG)\n\n    event_log_error (hashcat_ctx, \"clBuildProgram(): %s\", val2cstr_cl (CL_rc));\n\n    size_t build_log_size = 0;\n\n    hc_clGetProgramBuildInfo (hashcat_ctx, program, device, CL_PROGRAM_BUILD_LOG, 0, NULL, &build_log_size);\n\n    char *build_log = (char *) hcmalloc (build_log_size + 1);\n\n    hc_clGetProgramBuildInfo (hashcat_ctx, program, device, CL_PROGRAM_BUILD_LOG, build_log_size, build_log, NULL);\n\n    build_log[build_log_size] = 0;\n\n    puts (build_log);\n\n    hcfree (build_log);\n\n    #endif\n\n    hc_clReleaseProgram (hashcat_ctx, program);\n\n    return false;\n  }\n\n  if (hc_clReleaseProgram (hashcat_ctx, program) == -1) return false;\n\n  return true;\n}\n\nstatic bool read_kernel_binary (hashcat_ctx_t *hashcat_ctx, const char *kernel_file, size_t *kernel_lengths, char **kernel_sources)\n{\n  HCFILE fp;\n\n  if (hc_fopen (&fp, kernel_file, \"rb\") == true)\n  {\n    struct stat st;\n\n    if (stat (kernel_file, &st))\n    {\n      hc_fclose (&fp);\n\n      return false;\n    }\n\n    const size_t klen = st.st_size;\n\n    char *buf = (char *) hcmalloc (klen + 1);\n\n    size_t num_read = hc_fread (buf, sizeof (char), klen, &fp);\n\n    hc_fclose (&fp);\n\n    if (num_read != klen)\n    {\n      event_log_error (hashcat_ctx, \"%s: %s\", kernel_file, strerror (errno));\n\n      hcfree (buf);\n\n      return false;\n    }\n\n    buf[klen] = 0;\n\n    kernel_lengths[0] = klen;\n\n    kernel_sources[0] = buf;\n  }\n  else\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", kernel_file, strerror (errno));\n\n    return false;\n  }\n\n  return true;\n}\n\nstatic bool write_kernel_binary (hashcat_ctx_t *hashcat_ctx, const char *kernel_file, char *binary, size_t binary_size)\n{\n  if (binary_size > 0)\n  {\n    HCFILE fp;\n\n    if (hc_fopen (&fp, kernel_file, \"wb\") == false)\n    {\n      event_log_error (hashcat_ctx, \"%s: %s\", kernel_file, strerror (errno));\n\n      return false;\n    }\n\n    if (hc_lockfile (&fp) == -1)\n    {\n      hc_fclose (&fp);\n\n      event_log_error (hashcat_ctx, \"%s: %s\", kernel_file, strerror (errno));\n\n      return false;\n    }\n\n    hc_fwrite (binary, sizeof (char), binary_size, &fp);\n\n    hc_fflush (&fp);\n\n    if (hc_unlockfile (&fp) == -1)\n    {\n      hc_fclose (&fp);\n\n      event_log_error (hashcat_ctx, \"%s: %s\", kernel_file, strerror (errno));\n\n      return false;\n    }\n\n    hc_fclose (&fp);\n  }\n\n  return true;\n}\n\nvoid generate_source_kernel_filename (const bool slow_candidates, const u32 attack_exec, const u32 attack_kern, const u32 kern_type, const u32 opti_type, char *shared_dir, char *source_file)\n{\n  if (opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n  {\n    if (attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n    {\n      if (slow_candidates == true)\n      {\n        snprintf (source_file, 255, \"%s/OpenCL/m%05d_a0-optimized.cl\", shared_dir, (int) kern_type);\n      }\n      else\n      {\n        if (attack_kern == ATTACK_KERN_STRAIGHT)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a0-optimized.cl\", shared_dir, (int) kern_type);\n        else if (attack_kern == ATTACK_KERN_COMBI)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a1-optimized.cl\", shared_dir, (int) kern_type);\n        else if (attack_kern == ATTACK_KERN_BF)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a3-optimized.cl\", shared_dir, (int) kern_type);\n        else if (attack_kern == ATTACK_KERN_NONE)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a0-optimized.cl\", shared_dir, (int) kern_type);\n      }\n    }\n    else\n    {\n      snprintf (source_file, 255, \"%s/OpenCL/m%05d-optimized.cl\", shared_dir, (int) kern_type);\n    }\n  }\n  else\n  {\n    if (attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n    {\n      if (slow_candidates == true)\n      {\n        snprintf (source_file, 255, \"%s/OpenCL/m%05d_a0-pure.cl\", shared_dir, (int) kern_type);\n      }\n      else\n      {\n        if (attack_kern == ATTACK_KERN_STRAIGHT)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a0-pure.cl\", shared_dir, (int) kern_type);\n        else if (attack_kern == ATTACK_KERN_COMBI)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a1-pure.cl\", shared_dir, (int) kern_type);\n        else if (attack_kern == ATTACK_KERN_BF)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a3-pure.cl\", shared_dir, (int) kern_type);\n        else if (attack_kern == ATTACK_KERN_NONE)\n          snprintf (source_file, 255, \"%s/OpenCL/m%05d_a0-pure.cl\", shared_dir, (int) kern_type);\n      }\n    }\n    else\n    {\n      snprintf (source_file, 255, \"%s/OpenCL/m%05d-pure.cl\", shared_dir, (int) kern_type);\n    }\n  }\n}\n\nvoid generate_cached_kernel_filename (const bool slow_candidates, const u32 attack_exec, const u32 attack_kern, const u32 kern_type, const u32 opti_type, char *profile_dir, const char *device_name_chksum, char *cached_file)\n{\n  if (opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n  {\n    if (attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n    {\n      if (slow_candidates == true)\n      {\n        snprintf (cached_file, 255, \"%s/kernels/m%05d_a0-optimized.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n      }\n      else\n      {\n        if (attack_kern == ATTACK_KERN_STRAIGHT)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a0-optimized.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n        else if (attack_kern == ATTACK_KERN_COMBI)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a1-optimized.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n        else if (attack_kern == ATTACK_KERN_BF)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a3-optimized.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n        else if (attack_kern == ATTACK_KERN_NONE)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a0-optimized.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n      }\n    }\n    else\n    {\n      snprintf (cached_file, 255, \"%s/kernels/m%05d-optimized.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n    }\n  }\n  else\n  {\n    if (attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n    {\n      if (slow_candidates == true)\n      {\n        snprintf (cached_file, 255, \"%s/kernels/m%05d_a0-pure.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n      }\n      else\n      {\n        if (attack_kern == ATTACK_KERN_STRAIGHT)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a0-pure.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n        else if (attack_kern == ATTACK_KERN_COMBI)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a1-pure.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n        else if (attack_kern == ATTACK_KERN_BF)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a3-pure.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n        else if (attack_kern == ATTACK_KERN_NONE)\n          snprintf (cached_file, 255, \"%s/kernels/m%05d_a0-pure.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n      }\n    }\n    else\n    {\n      snprintf (cached_file, 255, \"%s/kernels/m%05d-pure.%s.kernel\", profile_dir, (int) kern_type, device_name_chksum);\n    }\n  }\n}\n\nvoid generate_source_kernel_shared_filename (char *shared_dir, char *source_file)\n{\n  snprintf (source_file, 255, \"%s/OpenCL/shared.cl\", shared_dir);\n}\n\nvoid generate_cached_kernel_shared_filename (char *profile_dir, const char *device_name_chksum_amp_mp, char *cached_file)\n{\n  snprintf (cached_file, 255, \"%s/kernels/shared.%s.kernel\", profile_dir, device_name_chksum_amp_mp);\n}\n\nvoid generate_source_kernel_mp_filename (const u32 opti_type, const u64 opts_type, char *shared_dir, char *source_file)\n{\n  if ((opti_type & OPTI_TYPE_BRUTE_FORCE) && (opts_type & OPTS_TYPE_PT_GENERATE_BE))\n  {\n    snprintf (source_file, 255, \"%s/OpenCL/markov_be.cl\", shared_dir);\n  }\n  else\n  {\n    snprintf (source_file, 255, \"%s/OpenCL/markov_le.cl\", shared_dir);\n  }\n}\n\nvoid generate_cached_kernel_mp_filename (const u32 opti_type, const u64 opts_type, char *profile_dir, const char *device_name_chksum_amp_mp, char *cached_file)\n{\n  if ((opti_type & OPTI_TYPE_BRUTE_FORCE) && (opts_type & OPTS_TYPE_PT_GENERATE_BE))\n  {\n    snprintf (cached_file, 255, \"%s/kernels/markov_be.%s.kernel\", profile_dir, device_name_chksum_amp_mp);\n  }\n  else\n  {\n    snprintf (cached_file, 255, \"%s/kernels/markov_le.%s.kernel\", profile_dir, device_name_chksum_amp_mp);\n  }\n}\n\nvoid generate_source_kernel_amp_filename (const u32 attack_kern, char *shared_dir, char *source_file)\n{\n  snprintf (source_file, 255, \"%s/OpenCL/amp_a%u.cl\", shared_dir, attack_kern);\n}\n\nvoid generate_cached_kernel_amp_filename (const u32 attack_kern, char *profile_dir, const char *device_name_chksum_amp_mp, char *cached_file)\n{\n  snprintf (cached_file, 255, \"%s/kernels/amp_a%u.%s.kernel\", profile_dir, attack_kern, device_name_chksum_amp_mp);\n}\n\n// NVRTC\n\nint nvrtc_init (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  memset (nvrtc, 0, sizeof (NVRTC_PTR));\n\n  #if   defined (_WIN)\n  nvrtc->lib = hc_dlopen (\"nvrtc.dll\");\n\n  if (nvrtc->lib == NULL)\n  {\n    // super annoying: nvidia is using the CUDA version in nvrtc???.dll filename!\n    // however, the cuda version string comes from nvcuda.dll which is from nvidia driver, but\n    // the driver version and the installed CUDA toolkit version can be different, so it cannot be used as a reference.\n    // brute force to the rescue\n\n    char dllname[100];\n\n    for (int major = 20; major >= 9; major--) // older than 3.x do not ship _v2 functions anyway\n                                              // older than 7.x does not support sm 5.x\n                                              // older than 8.x does not have documentation archive online, no way to check if nvrtc support whatever we need\n                                              // older than 9.x is just a theoretical limit since we define 9.0 as the minimum required version\n    {\n      for (int minor = 20; minor >= 0; minor--)\n      {\n        snprintf (dllname, sizeof (dllname), \"nvrtc64_%d%d.dll\", major, minor);\n\n        nvrtc->lib = hc_dlopen (dllname);\n\n        if (nvrtc->lib) break;\n\n        snprintf (dllname, sizeof (dllname), \"nvrtc64_%d%d_0.dll\", major, minor);\n\n        nvrtc->lib = hc_dlopen (dllname);\n\n        if (nvrtc->lib) break;\n      }\n\n      if (nvrtc->lib) break;\n    }\n  }\n  #elif defined (__APPLE__)\n  nvrtc->lib = hc_dlopen (\"nvrtc.dylib\");\n  #elif defined (__CYGWIN__)\n  nvrtc->lib = hc_dlopen (\"nvrtc.dll\");\n  #else\n  nvrtc->lib = hc_dlopen (\"libnvrtc.so\");\n\n  if (nvrtc->lib == NULL) nvrtc->lib = hc_dlopen (\"libnvrtc.so.1\");\n  #endif\n\n  if (nvrtc->lib == NULL) return -1;\n\n  HC_LOAD_FUNC (nvrtc, nvrtcAddNameExpression,  NVRTC_NVRTCADDNAMEEXPRESSION, NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcCompileProgram,     NVRTC_NVRTCCOMPILEPROGRAM,    NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcCreateProgram,      NVRTC_NVRTCCREATEPROGRAM,     NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcDestroyProgram,     NVRTC_NVRTCDESTROYPROGRAM,    NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcGetLoweredName,     NVRTC_NVRTCGETLOWEREDNAME,    NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcGetPTX,             NVRTC_NVRTCGETPTX,            NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcGetPTXSize,         NVRTC_NVRTCGETPTXSIZE,        NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcGetProgramLog,      NVRTC_NVRTCGETPROGRAMLOG,     NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcGetProgramLogSize,  NVRTC_NVRTCGETPROGRAMLOGSIZE, NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcGetErrorString,     NVRTC_NVRTCGETERRORSTRING,    NVRTC, 1);\n  HC_LOAD_FUNC (nvrtc, nvrtcVersion,            NVRTC_NVRTCVERSION,           NVRTC, 1);\n\n  return 0;\n}\n\nvoid nvrtc_close (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  if (nvrtc)\n  {\n    if (nvrtc->lib)\n    {\n      hc_dlclose (nvrtc->lib);\n    }\n\n    hcfree (backend_ctx->nvrtc);\n\n    backend_ctx->nvrtc = NULL;\n  }\n}\n\nint hc_nvrtcCreateProgram (hashcat_ctx_t *hashcat_ctx, nvrtcProgram *prog, const char *src, const char *name, int numHeaders, const char * const *headers, const char * const *includeNames)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcCreateProgram (prog, src, name, numHeaders, headers, includeNames);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcCreateProgram(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcDestroyProgram (hashcat_ctx_t *hashcat_ctx, nvrtcProgram *prog)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcDestroyProgram (prog);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcDestroyProgram(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcCompileProgram (hashcat_ctx_t *hashcat_ctx, nvrtcProgram prog, int numOptions, const char * const *options)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcCompileProgram (prog, numOptions, options);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcCompileProgram(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcGetProgramLogSize (hashcat_ctx_t *hashcat_ctx, nvrtcProgram prog, size_t *logSizeRet)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcGetProgramLogSize (prog, logSizeRet);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcGetProgramLogSize(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcGetProgramLog (hashcat_ctx_t *hashcat_ctx, nvrtcProgram prog, char *log)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcGetProgramLog (prog, log);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcGetProgramLog(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcGetPTXSize (hashcat_ctx_t *hashcat_ctx, nvrtcProgram prog, size_t *ptxSizeRet)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcGetPTXSize (prog, ptxSizeRet);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcGetPTXSize(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcGetPTX (hashcat_ctx_t *hashcat_ctx, nvrtcProgram prog, char *ptx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcGetPTX (prog, ptx);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcGetPTX(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_nvrtcVersion (hashcat_ctx_t *hashcat_ctx, int *major, int *minor)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  NVRTC_PTR *nvrtc = (NVRTC_PTR *) backend_ctx->nvrtc;\n\n  const nvrtcResult NVRTC_err = nvrtc->nvrtcVersion (major, minor);\n\n  if (NVRTC_err != NVRTC_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"nvrtcVersion(): %s\", nvrtc->nvrtcGetErrorString (NVRTC_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\n// CUDA\n\nint cuda_init (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  memset (cuda, 0, sizeof (CUDA_PTR));\n\n  #if   defined (_WIN)\n  cuda->lib = hc_dlopen (\"nvcuda.dll\");\n  #elif defined (__APPLE__)\n  cuda->lib = hc_dlopen (\"nvcuda.dylib\");\n  #elif defined (__CYGWIN__)\n  cuda->lib = hc_dlopen (\"nvcuda.dll\");\n  #else\n  cuda->lib = hc_dlopen (\"libcuda.so\");\n\n  if (cuda->lib == NULL) cuda->lib = hc_dlopen (\"libcuda.so.1\");\n  #endif\n\n  if (cuda->lib == NULL) return -1;\n\n  #define HC_LOAD_FUNC_CUDA(ptr,name,cudaname,type,libname,noerr) \\\n    do { \\\n      ptr->name = (type) hc_dlsym ((ptr)->lib, #cudaname); \\\n      if ((noerr) != -1) { \\\n        if (!(ptr)->name) { \\\n          if ((noerr) == 1) { \\\n            event_log_error (hashcat_ctx, \"%s is missing from %s shared library.\", #name, #libname); \\\n            return -1; \\\n          } \\\n          if ((noerr) != 1) { \\\n            event_log_warning (hashcat_ctx, \"%s is missing from %s shared library.\", #name, #libname); \\\n            return 0; \\\n          } \\\n        } \\\n      } \\\n    } while (0)\n\n  // finding the right symbol is a PITA, because of the _v2 suffix\n  // a good reference is cuda.h itself\n  // this needs to be verified for each new cuda release\n\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxCreate,              cuCtxCreate_v2,            CUDA_CUCTXCREATE,               CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxDestroy,             cuCtxDestroy_v2,           CUDA_CUCTXDESTROY,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxGetCacheConfig,      cuCtxGetCacheConfig,       CUDA_CUCTXGETCACHECONFIG,       CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxGetCurrent,          cuCtxGetCurrent,           CUDA_CUCTXGETCURRENT,           CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxGetSharedMemConfig,  cuCtxGetSharedMemConfig,   CUDA_CUCTXGETSHAREDMEMCONFIG,   CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxPopCurrent,          cuCtxPopCurrent_v2,        CUDA_CUCTXPOPCURRENT,           CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxPushCurrent,         cuCtxPushCurrent_v2,       CUDA_CUCTXPUSHCURRENT,          CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxSetCacheConfig,      cuCtxSetCacheConfig,       CUDA_CUCTXSETCACHECONFIG,       CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxSetCurrent,          cuCtxSetCurrent,           CUDA_CUCTXSETCURRENT,           CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxSetSharedMemConfig,  cuCtxSetSharedMemConfig,   CUDA_CUCTXSETSHAREDMEMCONFIG,   CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuCtxSynchronize,         cuCtxSynchronize,          CUDA_CUCTXSYNCHRONIZE,          CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuDeviceGetAttribute,     cuDeviceGetAttribute,      CUDA_CUDEVICEGETATTRIBUTE,      CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuDeviceGetCount,         cuDeviceGetCount,          CUDA_CUDEVICEGETCOUNT,          CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuDeviceGet,              cuDeviceGet,               CUDA_CUDEVICEGET,               CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuDeviceGetName,          cuDeviceGetName,           CUDA_CUDEVICEGETNAME,           CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuDeviceTotalMem,         cuDeviceTotalMem_v2,       CUDA_CUDEVICETOTALMEM,          CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuDriverGetVersion,       cuDriverGetVersion,        CUDA_CUDRIVERGETVERSION,        CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuEventCreate,            cuEventCreate,             CUDA_CUEVENTCREATE,             CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuEventDestroy,           cuEventDestroy_v2,         CUDA_CUEVENTDESTROY,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuEventElapsedTime,       cuEventElapsedTime,        CUDA_CUEVENTELAPSEDTIME,        CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuEventQuery,             cuEventQuery,              CUDA_CUEVENTQUERY,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuEventRecord,            cuEventRecord,             CUDA_CUEVENTRECORD,             CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuEventSynchronize,       cuEventSynchronize,        CUDA_CUEVENTSYNCHRONIZE,        CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuFuncGetAttribute,       cuFuncGetAttribute,        CUDA_CUFUNCGETATTRIBUTE,        CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuFuncSetAttribute,       cuFuncSetAttribute,        CUDA_CUFUNCSETATTRIBUTE,        CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuFuncSetCacheConfig,     cuFuncSetCacheConfig,      CUDA_CUFUNCSETCACHECONFIG,      CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuFuncSetSharedMemConfig, cuFuncSetSharedMemConfig,  CUDA_CUFUNCSETSHAREDMEMCONFIG,  CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuGetErrorName,           cuGetErrorName,            CUDA_CUGETERRORNAME,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuGetErrorString,         cuGetErrorString,          CUDA_CUGETERRORSTRING,          CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuInit,                   cuInit,                    CUDA_CUINIT,                    CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuLaunchKernel,           cuLaunchKernel,            CUDA_CULAUNCHKERNEL,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemAlloc,               cuMemAlloc_v2,             CUDA_CUMEMALLOC,                CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemAllocHost,           cuMemAllocHost_v2,         CUDA_CUMEMALLOCHOST,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemcpyDtoD,             cuMemcpyDtoD_v2,           CUDA_CUMEMCPYDTOD,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemcpyDtoH,             cuMemcpyDtoH_v2,           CUDA_CUMEMCPYDTOH,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemcpyHtoD,             cuMemcpyHtoD_v2,           CUDA_CUMEMCPYHTOD,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemFree,                cuMemFree_v2,              CUDA_CUMEMFREE,                 CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemFreeHost,            cuMemFreeHost,             CUDA_CUMEMFREEHOST,             CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemGetInfo,             cuMemGetInfo_v2,           CUDA_CUMEMGETINFO,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemsetD32,              cuMemsetD32_v2,            CUDA_CUMEMSETD32,               CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuMemsetD8,               cuMemsetD8_v2,             CUDA_CUMEMSETD8,                CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuModuleGetFunction,      cuModuleGetFunction,       CUDA_CUMODULEGETFUNCTION,       CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuModuleGetGlobal,        cuModuleGetGlobal_v2,      CUDA_CUMODULEGETGLOBAL,         CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuModuleLoad,             cuModuleLoad,              CUDA_CUMODULELOAD,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuModuleLoadData,         cuModuleLoadData,          CUDA_CUMODULELOADDATA,          CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuModuleLoadDataEx,       cuModuleLoadDataEx,        CUDA_CUMODULELOADDATAEX,        CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuModuleUnload,           cuModuleUnload,            CUDA_CUMODULEUNLOAD,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuProfilerStart,          cuProfilerStart,           CUDA_CUPROFILERSTART,           CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuProfilerStop,           cuProfilerStop,            CUDA_CUPROFILERSTOP,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuStreamCreate,           cuStreamCreate,            CUDA_CUSTREAMCREATE,            CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuStreamDestroy,          cuStreamDestroy_v2,        CUDA_CUSTREAMDESTROY,           CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuStreamSynchronize,      cuStreamSynchronize,       CUDA_CUSTREAMSYNCHRONIZE,       CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuStreamWaitEvent,        cuStreamWaitEvent,         CUDA_CUSTREAMWAITEVENT,         CUDA, 1);\n  #if defined (WITH_CUBIN)\n  HC_LOAD_FUNC_CUDA (cuda, cuLinkCreate,             cuLinkCreate_v2,           CUDA_CULINKCREATE,              CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuLinkAddData,            cuLinkAddData_v2,          CUDA_CULINKADDDATA,             CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuLinkDestroy,            cuLinkDestroy,             CUDA_CULINKDESTROY,             CUDA, 1);\n  HC_LOAD_FUNC_CUDA (cuda, cuLinkComplete,           cuLinkComplete,            CUDA_CULINKCOMPLETE,            CUDA, 1);\n  #endif\n\n  return 0;\n}\n\nvoid cuda_close (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  if (cuda)\n  {\n    if (cuda->lib)\n    {\n      hc_dlclose (cuda->lib);\n    }\n\n    hcfree (backend_ctx->cuda);\n\n    backend_ctx->cuda = NULL;\n  }\n}\n\nint hc_cuInit (hashcat_ctx_t *hashcat_ctx, unsigned int Flags)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuInit (Flags);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuInit(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuInit(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuDeviceGetAttribute (hashcat_ctx_t *hashcat_ctx, int *pi, CUdevice_attribute attrib, CUdevice dev)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuDeviceGetAttribute (pi, attrib, dev);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGetAttribute(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGetAttribute(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuDeviceGetCount (hashcat_ctx_t *hashcat_ctx, int *count)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuDeviceGetCount (count);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGetCount(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGetCount(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuDeviceGet (hashcat_ctx_t *hashcat_ctx, CUdevice* device, int ordinal)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuDeviceGet (device, ordinal);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGet(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGet(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuDeviceGetName (hashcat_ctx_t *hashcat_ctx, char *name, int len, CUdevice dev)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuDeviceGetName (name, len, dev);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGetName(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceGetName(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuDeviceTotalMem (hashcat_ctx_t *hashcat_ctx, size_t *bytes, CUdevice dev)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuDeviceTotalMem (bytes, dev);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceTotalMem(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuDeviceTotalMem(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuDriverGetVersion (hashcat_ctx_t *hashcat_ctx, int *driverVersion)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuDriverGetVersion (driverVersion);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuDriverGetVersion(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuDriverGetVersion(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxCreate (hashcat_ctx_t *hashcat_ctx, CUcontext *pctx, unsigned int flags, CUdevice dev)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxCreate (pctx, flags, dev);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxCreate(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxCreate(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxDestroy (hashcat_ctx_t *hashcat_ctx, CUcontext ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxDestroy (ctx);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxDestroy(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxDestroy(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuModuleLoadDataEx (hashcat_ctx_t *hashcat_ctx, CUmodule *module, const void *image, unsigned int numOptions, CUjit_option *options, void **optionValues)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuModuleLoadDataEx (module, image, numOptions, options, optionValues);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuModuleLoadDataEx(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuModuleLoadDataEx(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuModuleUnload (hashcat_ctx_t *hashcat_ctx, CUmodule hmod)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuModuleUnload (hmod);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuModuleUnload(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuModuleUnload(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxSetCurrent (hashcat_ctx_t *hashcat_ctx, CUcontext ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxSetCurrent (ctx);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxSetCurrent(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxSetCurrent(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuMemAlloc (hashcat_ctx_t *hashcat_ctx, CUdeviceptr *dptr, size_t bytesize)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuMemAlloc (dptr, bytesize);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuMemAlloc(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuMemAlloc(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuMemFree (hashcat_ctx_t *hashcat_ctx, CUdeviceptr dptr)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuMemFree (dptr);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuMemFree(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuMemFree(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuMemcpyDtoH (hashcat_ctx_t *hashcat_ctx, void *dstHost, CUdeviceptr srcDevice, size_t ByteCount)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuMemcpyDtoH (dstHost, srcDevice, ByteCount);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuMemcpyDtoH(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuMemcpyDtoH(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuMemcpyDtoD (hashcat_ctx_t *hashcat_ctx, CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuMemcpyDtoD (dstDevice, srcDevice, ByteCount);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuMemcpyDtoD(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuMemcpyDtoD(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuMemcpyHtoD (hashcat_ctx_t *hashcat_ctx, CUdeviceptr dstDevice, const void *srcHost, size_t ByteCount)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuMemcpyHtoD (dstDevice, srcHost, ByteCount);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuMemcpyHtoD(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuMemcpyHtoD(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuModuleGetFunction (hashcat_ctx_t *hashcat_ctx, CUfunction *hfunc, CUmodule hmod, const char *name)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuModuleGetFunction (hfunc, hmod, name);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuModuleGetFunction(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuModuleGetFunction(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuModuleGetGlobal (hashcat_ctx_t *hashcat_ctx, CUdeviceptr *dptr, size_t *bytes, CUmodule hmod, const char *name)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuModuleGetGlobal (dptr, bytes, hmod, name);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuModuleGetGlobal(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuModuleGetGlobal(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuMemGetInfo (hashcat_ctx_t *hashcat_ctx, size_t *free, size_t *total)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuMemGetInfo (free, total);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuMemGetInfo(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuMemGetInfo(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuFuncGetAttribute (hashcat_ctx_t *hashcat_ctx, int *pi, CUfunction_attribute attrib, CUfunction hfunc)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuFuncGetAttribute (pi, attrib, hfunc);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuFuncGetAttribute(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuFuncGetAttribute(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuFuncSetAttribute (hashcat_ctx_t *hashcat_ctx, CUfunction hfunc, CUfunction_attribute attrib, int value)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuFuncSetAttribute (hfunc, attrib, value);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuFuncSetAttribute(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuFuncSetAttribute(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuStreamCreate (hashcat_ctx_t *hashcat_ctx, CUstream *phStream, unsigned int Flags)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuStreamCreate (phStream, Flags);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuStreamCreate(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuStreamCreate(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuStreamDestroy (hashcat_ctx_t *hashcat_ctx, CUstream hStream)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuStreamDestroy (hStream);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuStreamDestroy(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuStreamDestroy(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuStreamSynchronize (hashcat_ctx_t *hashcat_ctx, CUstream hStream)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuStreamSynchronize (hStream);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuStreamSynchronize(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuStreamSynchronize(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuLaunchKernel (hashcat_ctx_t *hashcat_ctx, CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void **kernelParams, void **extra)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuLaunchKernel (f, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, kernelParams, extra);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuLaunchKernel(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuLaunchKernel(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxSynchronize (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxSynchronize ();\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxSynchronize(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxSynchronize(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuEventCreate (hashcat_ctx_t *hashcat_ctx, CUevent *phEvent, unsigned int Flags)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuEventCreate (phEvent, Flags);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuEventCreate(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuEventCreate(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuEventDestroy (hashcat_ctx_t *hashcat_ctx, CUevent hEvent)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuEventDestroy (hEvent);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuEventDestroy(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuEventDestroy(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuEventElapsedTime (hashcat_ctx_t *hashcat_ctx, float *pMilliseconds, CUevent hStart, CUevent hEnd)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuEventElapsedTime (pMilliseconds, hStart, hEnd);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuEventElapsedTime(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuEventElapsedTime(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuEventQuery (hashcat_ctx_t *hashcat_ctx, CUevent hEvent)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuEventQuery (hEvent);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuEventQuery(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuEventQuery(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuEventRecord (hashcat_ctx_t *hashcat_ctx, CUevent hEvent, CUstream hStream)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuEventRecord (hEvent, hStream);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuEventRecord(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuEventRecord(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuEventSynchronize (hashcat_ctx_t *hashcat_ctx, CUevent hEvent)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuEventSynchronize (hEvent);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuEventSynchronize(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuEventSynchronize(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxSetCacheConfig (hashcat_ctx_t *hashcat_ctx, CUfunc_cache config)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxSetCacheConfig (config);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxSetCacheConfig(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxSetCacheConfig(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxPushCurrent (hashcat_ctx_t *hashcat_ctx, CUcontext ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxPushCurrent (ctx);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxPushCurrent(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxPushCurrent(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuCtxPopCurrent (hashcat_ctx_t *hashcat_ctx, CUcontext *pctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuCtxPopCurrent (pctx);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuCtxPopCurrent(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuCtxPopCurrent(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuLinkCreate (hashcat_ctx_t *hashcat_ctx, unsigned int numOptions, CUjit_option *options, void **optionValues, CUlinkState *stateOut)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuLinkCreate (numOptions, options, optionValues, stateOut);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuLinkCreate(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuLinkCreate(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuLinkAddData (hashcat_ctx_t *hashcat_ctx, CUlinkState state, CUjitInputType type, void *data, size_t size, const char *name, unsigned int numOptions, CUjit_option *options, void **optionValues)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuLinkAddData (state, type, data, size, name, numOptions, options, optionValues);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuLinkAddData(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuLinkAddData(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuLinkDestroy (hashcat_ctx_t *hashcat_ctx, CUlinkState state)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuLinkDestroy (state);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuLinkDestroy(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuLinkDestroy(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_cuLinkComplete (hashcat_ctx_t *hashcat_ctx, CUlinkState state, void **cubinOut, size_t *sizeOut)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n  const CUresult CU_err = cuda->cuLinkComplete (state, cubinOut, sizeOut);\n\n  if (CU_err != CUDA_SUCCESS)\n  {\n    const char *pStr = NULL;\n\n    if (cuda->cuGetErrorString (CU_err, &pStr) == CUDA_SUCCESS)\n    {\n      event_log_error (hashcat_ctx, \"cuLinkComplete(): %s\", pStr);\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"cuLinkComplete(): %d\", CU_err);\n    }\n\n    return -1;\n  }\n\n  return 0;\n}\n\n// OpenCL\n\nint ocl_init (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  memset (ocl, 0, sizeof (OCL_PTR));\n\n  #if   defined (_WIN)\n  ocl->lib = hc_dlopen (\"OpenCL\");\n  #elif defined (__APPLE__)\n  ocl->lib = hc_dlopen (\"/System/Library/Frameworks/OpenCL.framework/OpenCL\");\n  #elif defined (__CYGWIN__)\n  ocl->lib = hc_dlopen (\"opencl.dll\");\n\n  if (ocl->lib == NULL) ocl->lib = hc_dlopen (\"cygOpenCL-1.dll\");\n  #else\n  ocl->lib = hc_dlopen (\"libOpenCL.so\");\n\n  if (ocl->lib == NULL) ocl->lib = hc_dlopen (\"libOpenCL.so.1\");\n  #endif\n\n  if (ocl->lib == NULL) return -1;\n\n  HC_LOAD_FUNC (ocl, clBuildProgram,            OCL_CLBUILDPROGRAM,             OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clCreateBuffer,            OCL_CLCREATEBUFFER,             OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clCreateCommandQueue,      OCL_CLCREATECOMMANDQUEUE,       OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clCreateContext,           OCL_CLCREATECONTEXT,            OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clCreateKernel,            OCL_CLCREATEKERNEL,             OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clCreateProgramWithBinary, OCL_CLCREATEPROGRAMWITHBINARY,  OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clCreateProgramWithSource, OCL_CLCREATEPROGRAMWITHSOURCE,  OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clEnqueueCopyBuffer,       OCL_CLENQUEUECOPYBUFFER,        OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clEnqueueMapBuffer,        OCL_CLENQUEUEMAPBUFFER,         OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clEnqueueNDRangeKernel,    OCL_CLENQUEUENDRANGEKERNEL,     OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clEnqueueReadBuffer,       OCL_CLENQUEUEREADBUFFER,        OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clEnqueueUnmapMemObject,   OCL_CLENQUEUEUNMAPMEMOBJECT,    OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clEnqueueWriteBuffer,      OCL_CLENQUEUEWRITEBUFFER,       OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clFinish,                  OCL_CLFINISH,                   OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clFlush,                   OCL_CLFLUSH,                    OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetDeviceIDs,            OCL_CLGETDEVICEIDS,             OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetDeviceInfo,           OCL_CLGETDEVICEINFO,            OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetEventInfo,            OCL_CLGETEVENTINFO,             OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetKernelWorkGroupInfo,  OCL_CLGETKERNELWORKGROUPINFO,   OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetPlatformIDs,          OCL_CLGETPLATFORMIDS,           OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetPlatformInfo,         OCL_CLGETPLATFORMINFO,          OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetProgramBuildInfo,     OCL_CLGETPROGRAMBUILDINFO,      OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetProgramInfo,          OCL_CLGETPROGRAMINFO,           OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clReleaseCommandQueue,     OCL_CLRELEASECOMMANDQUEUE,      OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clReleaseContext,          OCL_CLRELEASECONTEXT,           OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clReleaseKernel,           OCL_CLRELEASEKERNEL,            OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clReleaseMemObject,        OCL_CLRELEASEMEMOBJECT,         OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clReleaseProgram,          OCL_CLRELEASEPROGRAM,           OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clSetKernelArg,            OCL_CLSETKERNELARG,             OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clWaitForEvents,           OCL_CLWAITFOREVENTS,            OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clGetEventProfilingInfo,   OCL_CLGETEVENTPROFILINGINFO,    OpenCL, 1);\n  HC_LOAD_FUNC (ocl, clReleaseEvent,            OCL_CLRELEASEEVENT,             OpenCL, 1);\n\n  return 0;\n}\n\nvoid ocl_close (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  if (ocl)\n  {\n    if (ocl->lib)\n    {\n      hc_dlclose (ocl->lib);\n    }\n\n    hcfree (backend_ctx->ocl);\n\n    backend_ctx->ocl = NULL;\n  }\n}\n\nint hc_clEnqueueNDRangeKernel (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue, cl_kernel kernel, cl_uint work_dim, const size_t *global_work_offset, const size_t *global_work_size, const size_t *local_work_size, cl_uint num_events_in_wait_list, const cl_event *event_wait_list, cl_event *event)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clEnqueueNDRangeKernel (command_queue, kernel, work_dim, global_work_offset, global_work_size, local_work_size, num_events_in_wait_list, event_wait_list, event);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clEnqueueNDRangeKernel(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetEventInfo (hashcat_ctx_t *hashcat_ctx, cl_event event, cl_event_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetEventInfo (event, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetEventInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clFlush (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clFlush (command_queue);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clFlush(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clFinish (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clFinish (command_queue);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clFinish(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clSetKernelArg (hashcat_ctx_t *hashcat_ctx, cl_kernel kernel, cl_uint arg_index, size_t arg_size, const void *arg_value)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clSetKernelArg (kernel, arg_index, arg_size, arg_value);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clSetKernelArg(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clEnqueueWriteBuffer (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_write, size_t offset, size_t size, const void *ptr, cl_uint num_events_in_wait_list, const cl_event *event_wait_list, cl_event *event)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clEnqueueWriteBuffer (command_queue, buffer, blocking_write, offset, size, ptr, num_events_in_wait_list, event_wait_list, event);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clEnqueueWriteBuffer(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clEnqueueCopyBuffer (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_buffer, size_t src_offset, size_t dst_offset, size_t size, cl_uint num_events_in_wait_list, const cl_event *event_wait_list, cl_event *event)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clEnqueueCopyBuffer (command_queue, src_buffer, dst_buffer, src_offset, dst_offset, size, num_events_in_wait_list, event_wait_list, event);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clEnqueueCopyBuffer(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clEnqueueReadBuffer (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_read, size_t offset, size_t size, void *ptr, cl_uint num_events_in_wait_list, const cl_event *event_wait_list, cl_event *event)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clEnqueueReadBuffer (command_queue, buffer, blocking_read, offset, size, ptr, num_events_in_wait_list, event_wait_list, event);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clEnqueueReadBuffer(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetPlatformIDs (hashcat_ctx_t *hashcat_ctx, cl_uint num_entries, cl_platform_id *platforms, cl_uint *num_platforms)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetPlatformIDs (num_entries, platforms, num_platforms);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetPlatformIDs(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetPlatformInfo (hashcat_ctx_t *hashcat_ctx, cl_platform_id platform, cl_platform_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetPlatformInfo (platform, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetPlatformInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetDeviceIDs (hashcat_ctx_t *hashcat_ctx, cl_platform_id platform, cl_device_type device_type, cl_uint num_entries, cl_device_id *devices, cl_uint *num_devices)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetDeviceIDs (platform, device_type, num_entries, devices, num_devices);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetDeviceIDs(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetDeviceInfo (hashcat_ctx_t *hashcat_ctx, cl_device_id device, cl_device_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetDeviceInfo (device, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetDeviceInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clCreateContext (hashcat_ctx_t *hashcat_ctx, const cl_context_properties *properties, cl_uint num_devices, const cl_device_id *devices, void (CL_CALLBACK *pfn_notify) (const char *errinfo, const void *private_info, size_t cb, void *user_data), void *user_data, cl_context *context)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *context = ocl->clCreateContext (properties, num_devices, devices, pfn_notify, user_data, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clCreateContext(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clCreateCommandQueue (hashcat_ctx_t *hashcat_ctx, cl_context context, cl_device_id device, cl_command_queue_properties properties, cl_command_queue *command_queue)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *command_queue = ocl->clCreateCommandQueue (context, device, properties, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clCreateCommandQueue(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clCreateBuffer (hashcat_ctx_t *hashcat_ctx, cl_context context, cl_mem_flags flags, size_t size, void *host_ptr, cl_mem *mem)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *mem = ocl->clCreateBuffer (context, flags, size, host_ptr, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clCreateBuffer(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clCreateProgramWithSource (hashcat_ctx_t *hashcat_ctx, cl_context context, cl_uint count, const char **strings, const size_t *lengths, cl_program *program)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *program = ocl->clCreateProgramWithSource (context, count, strings, lengths, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clCreateProgramWithSource(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clCreateProgramWithBinary (hashcat_ctx_t *hashcat_ctx, cl_context context, cl_uint num_devices, const cl_device_id *device_list, const size_t *lengths, const unsigned char **binaries, cl_int *binary_status, cl_program *program)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *program = ocl->clCreateProgramWithBinary (context, num_devices, device_list, lengths, binaries, binary_status, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clCreateProgramWithBinary(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clBuildProgram (hashcat_ctx_t *hashcat_ctx, cl_program program, cl_uint num_devices, const cl_device_id *device_list, const char *options, void (CL_CALLBACK *pfn_notify) (cl_program program, void *user_data), void *user_data)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clBuildProgram (program, num_devices, device_list, options, pfn_notify, user_data);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clBuildProgram(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clCreateKernel (hashcat_ctx_t *hashcat_ctx, cl_program program, const char *kernel_name, cl_kernel *kernel)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *kernel = ocl->clCreateKernel (program, kernel_name, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clCreateKernel(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clReleaseMemObject (hashcat_ctx_t *hashcat_ctx, cl_mem mem)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clReleaseMemObject (mem);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clReleaseMemObject(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clReleaseKernel (hashcat_ctx_t *hashcat_ctx, cl_kernel kernel)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clReleaseKernel (kernel);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clReleaseKernel(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clReleaseProgram (hashcat_ctx_t *hashcat_ctx, cl_program program)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clReleaseProgram (program);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clReleaseProgram(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clReleaseCommandQueue (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clReleaseCommandQueue (command_queue);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clReleaseCommandQueue(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clReleaseContext (hashcat_ctx_t *hashcat_ctx, cl_context context)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clReleaseContext (context);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clReleaseContext(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clEnqueueMapBuffer (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_map, cl_map_flags map_flags, size_t offset, size_t size, cl_uint num_events_in_wait_list, const cl_event *event_wait_list, cl_event *event, void **buf)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  cl_int CL_err;\n\n  *buf = ocl->clEnqueueMapBuffer (command_queue, buffer, blocking_map, map_flags, offset, size, num_events_in_wait_list, event_wait_list, event, &CL_err);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clEnqueueMapBuffer(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clEnqueueUnmapMemObject (hashcat_ctx_t *hashcat_ctx, cl_command_queue command_queue, cl_mem memobj, void *mapped_ptr, cl_uint num_events_in_wait_list, const cl_event *event_wait_list, cl_event *event)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clEnqueueUnmapMemObject (command_queue, memobj, mapped_ptr, num_events_in_wait_list, event_wait_list, event);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clEnqueueUnmapMemObject(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetKernelWorkGroupInfo (hashcat_ctx_t *hashcat_ctx, cl_kernel kernel, cl_device_id device, cl_kernel_work_group_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetKernelWorkGroupInfo (kernel, device, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetKernelWorkGroupInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetProgramBuildInfo (hashcat_ctx_t *hashcat_ctx, cl_program program, cl_device_id device, cl_program_build_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetProgramBuildInfo (program, device, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetProgramBuildInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetProgramInfo (hashcat_ctx_t *hashcat_ctx, cl_program program, cl_program_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetProgramInfo (program, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetProgramInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clWaitForEvents (hashcat_ctx_t *hashcat_ctx, cl_uint num_events, const cl_event *event_list)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clWaitForEvents (num_events, event_list);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clWaitForEvents(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clGetEventProfilingInfo (hashcat_ctx_t *hashcat_ctx, cl_event event, cl_profiling_info param_name, size_t param_value_size, void *param_value, size_t *param_value_size_ret)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clGetEventProfilingInfo (event, param_name, param_value_size, param_value, param_value_size_ret);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clGetEventProfilingInfo(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\nint hc_clReleaseEvent (hashcat_ctx_t *hashcat_ctx, cl_event event)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n  const cl_int CL_err = ocl->clReleaseEvent (event);\n\n  if (CL_err != CL_SUCCESS)\n  {\n    event_log_error (hashcat_ctx, \"clReleaseEvent(): %s\", val2cstr_cl (CL_err));\n\n    return -1;\n  }\n\n  return 0;\n}\n\n// Backend\n\nint gidd_to_pw_t (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u64 gidd, pw_t *pw)\n{\n  pw_idx_t pw_idx;\n\n  pw_idx.off = 0;\n  pw_idx.cnt = 0;\n  pw_idx.len = 0;\n\n  if (device_param->is_cuda == true)\n  {\n    if (hc_cuCtxPushCurrent (hashcat_ctx, device_param->cuda_context) == -1) return -1;\n\n    if (hc_cuMemcpyDtoH (hashcat_ctx, &pw_idx, device_param->cuda_d_pws_idx + (gidd * sizeof (pw_idx_t)), sizeof (pw_idx_t)) == -1) return -1;\n\n    if (hc_cuCtxPopCurrent (hashcat_ctx, &device_param->cuda_context) == -1) return -1;\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    if (hc_clEnqueueReadBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_idx, CL_TRUE, gidd * sizeof (pw_idx_t), sizeof (pw_idx_t), &pw_idx, 0, NULL, NULL) == -1) return -1;\n  }\n\n  const u32 off = pw_idx.off;\n  const u32 cnt = pw_idx.cnt;\n  const u32 len = pw_idx.len;\n\n  if (device_param->is_cuda == true)\n  {\n    if (cnt > 0)\n    {\n      if (hc_cuCtxPushCurrent (hashcat_ctx, device_param->cuda_context) == -1) return -1;\n\n      if (hc_cuMemcpyDtoH (hashcat_ctx,pw->i, device_param->cuda_d_pws_comp_buf + (off * sizeof (u32)), cnt * sizeof (u32)) == -1) return -1;\n\n      if (hc_cuCtxPopCurrent (hashcat_ctx, &device_param->cuda_context) == -1) return -1;\n    }\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    if (cnt > 0)\n    {\n      if (hc_clEnqueueReadBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_comp_buf, CL_TRUE, off * sizeof (u32), cnt * sizeof (u32), pw->i, 0, NULL, NULL) == -1) return -1;\n    }\n  }\n\n  for (u32 i = cnt; i < 64; i++)\n  {\n    pw->i[i] = 0;\n  }\n\n  pw->pw_len = len;\n\n  return 0;\n}\n\nint choose_kernel (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u32 highest_pw_len, const u64 pws_cnt, const u32 fast_iteration, const u32 salt_pos)\n{\n  hashconfig_t   *hashconfig   = hashcat_ctx->hashconfig;\n  hashes_t       *hashes       = hashcat_ctx->hashes;\n  module_ctx_t   *module_ctx   = hashcat_ctx->module_ctx;\n  status_ctx_t   *status_ctx   = hashcat_ctx->status_ctx;\n  user_options_t *user_options = hashcat_ctx->user_options;\n\n  if (user_options->stdout_flag == true)\n  {\n    return process_stdout (hashcat_ctx, device_param, pws_cnt);\n  }\n\n  if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n  {\n    if (user_options->attack_mode == ATTACK_MODE_BF)\n    {\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (hashconfig->opts_type & OPTS_TYPE_TM_KERNEL)\n        {\n          const u32 size_tm = device_param->size_tm;\n\n          if (device_param->is_cuda == true)\n          {\n            if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_tm_c, size_tm) == -1) return -1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_tm_c, size_tm) == -1) return -1;\n          }\n\n          if (run_kernel_tm (hashcat_ctx, device_param) == -1) return -1;\n\n          if (device_param->is_cuda == true)\n          {\n            if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_bfs_c, device_param->cuda_d_tm_c, size_tm) == -1) return -1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_tm_c, device_param->opencl_d_bfs_c, 0, 0, size_tm, 0, NULL, NULL) == -1) return -1;\n          }\n        }\n      }\n    }\n\n    if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n    {\n      if (highest_pw_len < 16)\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_1, pws_cnt, true, fast_iteration) == -1) return -1;\n      }\n      else if (highest_pw_len < 32)\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_2, pws_cnt, true, fast_iteration) == -1) return -1;\n      }\n      else\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_3, pws_cnt, true, fast_iteration) == -1) return -1;\n      }\n    }\n    else\n    {\n      if (run_kernel (hashcat_ctx, device_param, KERN_RUN_4, pws_cnt, true, fast_iteration) == -1) return -1;\n    }\n  }\n  else\n  {\n    bool run_init = true;\n    bool run_loop = true;\n    bool run_comp = true;\n\n    if (run_init == true)\n    {\n      if (device_param->is_cuda == true)\n      {\n        if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_pws_buf, device_param->cuda_d_pws_amp_buf, pws_cnt * sizeof (pw_t)) == -1) return -1;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_amp_buf, device_param->opencl_d_pws_buf, 0, 0, pws_cnt * sizeof (pw_t), 0, NULL, NULL) == -1) return -1;\n      }\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (run_kernel_amp (hashcat_ctx, device_param, pws_cnt) == -1) return -1;\n      }\n\n      if (run_kernel (hashcat_ctx, device_param, KERN_RUN_1, pws_cnt, false, 0) == -1) return -1;\n\n      if (hashconfig->opts_type & OPTS_TYPE_HOOK12)\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_12, pws_cnt, false, 0) == -1) return -1;\n\n        if (device_param->is_cuda == true)\n        {\n          if (hc_cuMemcpyDtoH (hashcat_ctx, device_param->hooks_buf, device_param->cuda_d_hooks, pws_cnt * hashconfig->hook_size) == -1) return -1;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if (hc_clEnqueueReadBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_hooks, CL_TRUE, 0, pws_cnt * hashconfig->hook_size, device_param->hooks_buf, 0, NULL, NULL) == -1) return -1;\n        }\n\n        const int hook_threads = (int) user_options->hook_threads;\n\n        hook_thread_param_t *hook_threads_param = (hook_thread_param_t *) hccalloc (hook_threads, sizeof (hook_thread_param_t));\n\n        for (int i = 0; i < hook_threads; i++)\n        {\n          hook_thread_param_t *hook_thread_param = hook_threads_param + i;\n\n          hook_thread_param->tid = i;\n          hook_thread_param->tsz = hook_threads;\n\n          hook_thread_param->module_ctx = module_ctx;\n          hook_thread_param->status_ctx = status_ctx;\n\n          hook_thread_param->device_param = device_param;\n\n          hook_thread_param->hook_salts_buf = hashes->hook_salts_buf;\n\n          hook_thread_param->salt_pos = salt_pos;\n\n          hook_thread_param->pws_cnt = pws_cnt;\n        }\n\n        hc_thread_t *c_threads = (hc_thread_t *) hccalloc (hook_threads, sizeof (hc_thread_t));\n\n        for (int i = 0; i < hook_threads; i++)\n        {\n          hook_thread_param_t *hook_thread_param = hook_threads_param + i;\n\n          hc_thread_create (c_threads[i], hook12_thread, hook_thread_param);\n        }\n\n        hc_thread_wait (hook_threads, c_threads);\n\n        hcfree (c_threads);\n\n        hcfree (hook_threads_param);\n\n        if (device_param->is_cuda == true)\n        {\n          if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_hooks, device_param->hooks_buf, pws_cnt * hashconfig->hook_size) == -1) return -1;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_hooks, CL_TRUE, 0, pws_cnt * hashconfig->hook_size, device_param->hooks_buf, 0, NULL, NULL) == -1) return -1;\n        }\n      }\n    }\n\n    if (run_loop == true)\n    {\n      u32 iter = hashes->salts_buf[salt_pos].salt_iter;\n\n      u32 loop_step = device_param->kernel_loops;\n\n      for (u32 loop_pos = 0, slow_iteration = 0; loop_pos < iter; loop_pos += loop_step, slow_iteration++)\n      {\n        u32 loop_left = iter - loop_pos;\n\n        loop_left = MIN (loop_left, loop_step);\n\n        device_param->kernel_params_buf32[28] = loop_pos;\n        device_param->kernel_params_buf32[29] = loop_left;\n\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_2, pws_cnt, true, slow_iteration) == -1) return -1;\n\n        if (hashconfig->opts_type & OPTS_TYPE_LOOP_EXTENDED)\n        {\n          if (run_kernel (hashcat_ctx, device_param, KERN_RUN_2E, pws_cnt, true, slow_iteration) == -1) return -1;\n        }\n\n        //bug?\n        //while (status_ctx->run_thread_level2 == false) break;\n        if (status_ctx->run_thread_level2 == false) break;\n\n        /**\n         * speed\n         */\n\n        const float iter_part = (float) (loop_pos + loop_left) / iter;\n\n        const u64 perf_sum_all = (u64) (pws_cnt * iter_part);\n\n        double speed_msec = hc_timer_get (device_param->timer_speed);\n\n        const u32 speed_pos = device_param->speed_pos;\n\n        device_param->speed_cnt[speed_pos] = perf_sum_all;\n\n        device_param->speed_msec[speed_pos] = speed_msec;\n\n        if (user_options->speed_only == true)\n        {\n          if (speed_msec > 4000)\n          {\n            device_param->outerloop_multi *= (double) iter / (double) (loop_pos + loop_left);\n\n            device_param->speed_pos = 1;\n\n            device_param->speed_only_finish = true;\n\n            return 0;\n          }\n        }\n      }\n\n      if (hashconfig->opts_type & OPTS_TYPE_HOOK23)\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_23, pws_cnt, false, 0) == -1) return -1;\n\n        if (device_param->is_cuda == true)\n        {\n          if (hc_cuMemcpyDtoH (hashcat_ctx, device_param->hooks_buf, device_param->cuda_d_hooks, pws_cnt * hashconfig->hook_size) == -1) return -1;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if (hc_clEnqueueReadBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_hooks, CL_TRUE, 0, pws_cnt * hashconfig->hook_size, device_param->hooks_buf, 0, NULL, NULL) == -1) return -1;\n        }\n\n        const int hook_threads = (int) user_options->hook_threads;\n\n        hook_thread_param_t *hook_threads_param = (hook_thread_param_t *) hccalloc (hook_threads, sizeof (hook_thread_param_t));\n\n        for (int i = 0; i < hook_threads; i++)\n        {\n          hook_thread_param_t *hook_thread_param = hook_threads_param + i;\n\n          hook_thread_param->tid = i;\n          hook_thread_param->tsz = hook_threads;\n\n          hook_thread_param->module_ctx = module_ctx;\n          hook_thread_param->status_ctx = status_ctx;\n\n          hook_thread_param->device_param = device_param;\n\n          hook_thread_param->hook_salts_buf = hashes->hook_salts_buf;\n\n          hook_thread_param->salt_pos = salt_pos;\n\n          hook_thread_param->pws_cnt = pws_cnt;\n        }\n\n        hc_thread_t *c_threads = (hc_thread_t *) hccalloc (hook_threads, sizeof (hc_thread_t));\n\n        for (int i = 0; i < hook_threads; i++)\n        {\n          hook_thread_param_t *hook_thread_param = hook_threads_param + i;\n\n          hc_thread_create (c_threads[i], hook23_thread, hook_thread_param);\n        }\n\n        hc_thread_wait (hook_threads, c_threads);\n\n        hcfree (c_threads);\n\n        hcfree (hook_threads_param);\n\n        if (device_param->is_cuda == true)\n        {\n          if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_hooks, device_param->hooks_buf, pws_cnt * hashconfig->hook_size) == -1) return -1;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_hooks, CL_TRUE, 0, pws_cnt * hashconfig->hook_size, device_param->hooks_buf, 0, NULL, NULL) == -1) return -1;\n        }\n      }\n    }\n\n    // init2 and loop2 are kind of special, we use run_loop for them, too\n\n    if (run_loop == true)\n    {\n      // note: they also do not influence the performance screen\n      // in case you want to use this, this cane make sense only if your input data comes out of tmps[]\n\n      if (hashconfig->opts_type & OPTS_TYPE_INIT2)\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_INIT2, pws_cnt, false, 0) == -1) return -1;\n      }\n\n      if (hashconfig->opts_type & OPTS_TYPE_LOOP2)\n      {\n        u32 iter = hashes->salts_buf[salt_pos].salt_iter2;\n\n        u32 loop_step = device_param->kernel_loops;\n\n        for (u32 loop_pos = 0, slow_iteration = 0; loop_pos < iter; loop_pos += loop_step, slow_iteration++)\n        {\n          u32 loop_left = iter - loop_pos;\n\n          loop_left = MIN (loop_left, loop_step);\n\n          device_param->kernel_params_buf32[28] = loop_pos;\n          device_param->kernel_params_buf32[29] = loop_left;\n\n          if (run_kernel (hashcat_ctx, device_param, KERN_RUN_LOOP2, pws_cnt, true, slow_iteration) == -1) return -1;\n\n          //bug?\n          //while (status_ctx->run_thread_level2 == false) break;\n          if (status_ctx->run_thread_level2 == false) break;\n        }\n      }\n    }\n\n    if (run_comp == true)\n    {\n      if (hashconfig->opts_type & OPTS_TYPE_DEEP_COMP_KERNEL)\n      {\n        const u32 loops_cnt = hashes->salts_buf[salt_pos].digests_cnt;\n\n        for (u32 loops_pos = 0; loops_pos < loops_cnt; loops_pos++)\n        {\n          device_param->kernel_params_buf32[28] = loops_pos;\n          device_param->kernel_params_buf32[29] = loops_cnt;\n\n          const u32 deep_comp_kernel = module_ctx->module_deep_comp_kernel (hashes, salt_pos, loops_pos);\n\n          if (run_kernel (hashcat_ctx, device_param, deep_comp_kernel, pws_cnt, false, 0) == -1) return -1;\n\n          if (status_ctx->run_thread_level2 == false) break;\n        }\n      }\n      else\n      {\n        if (run_kernel (hashcat_ctx, device_param, KERN_RUN_3, pws_cnt, false, 0) == -1) return -1;\n      }\n    }\n\n    /*\n     * maybe we should add this zero of temporary buffers\n     * however it drops the performance from 7055338 to 7010621\n\n    if (device_param->is_cuda == true)\n    {\n      if (run_cuda_kernel_bzero   (hashcat_ctx, device_param, device_param->cuda_d_tmps,   device_param->size_tmps) == -1) return -1;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_tmps, device_param->size_tmps) == -1) return -1;\n    }\n    */\n\n    if ((hashconfig->opts_type & OPTS_TYPE_HOOK12) || (hashconfig->opts_type & OPTS_TYPE_HOOK23))\n    {\n      if (device_param->is_cuda == true)\n      {\n        if (run_cuda_kernel_bzero   (hashcat_ctx, device_param, device_param->cuda_d_hooks,   pws_cnt * hashconfig->hook_size) == -1) return -1;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_hooks, pws_cnt * hashconfig->hook_size) == -1) return -1;\n      }\n    }\n  }\n\n  return 0;\n}\n\nvoid rebuild_pws_compressed_append (hc_device_param_t *device_param, const u64 pws_cnt, const u8 chr)\n{\n  // this function is used if we have to modify the compressed pws buffer in order to\n  // append some data to each password candidate\n\n  u32      *tmp_pws_comp = (u32 *)      hcmalloc (device_param->size_pws_comp);\n  pw_idx_t *tmp_pws_idx  = (pw_idx_t *) hcmalloc (device_param->size_pws_idx);\n\n  for (u32 i = 0; i < pws_cnt; i++)\n  {\n    pw_idx_t *pw_idx_src = device_param->pws_idx + i;\n    pw_idx_t *pw_idx_dst = tmp_pws_idx + i;\n\n    const u32 src_off = pw_idx_src->off;\n    const u32 src_len = pw_idx_src->len;\n\n    u8 buf[256];\n\n    memcpy (buf, device_param->pws_comp + src_off, src_len);\n\n    buf[src_len] = chr;\n\n    const u32 dst_len = src_len + 1;\n\n    const u32 dst_pw_len4 = (dst_len + 3) & ~3; // round up to multiple of 4\n\n    const u32 dst_pw_len4_cnt = dst_pw_len4 / 4;\n\n    pw_idx_dst->cnt = dst_pw_len4_cnt;\n    pw_idx_dst->len = src_len; // this is intenionally! src_len can not be dst_len, we dont want the kernel to think 0x80 is part of the password\n\n    u8 *dst = (u8 *) (tmp_pws_comp + pw_idx_dst->off);\n\n    memcpy (dst, buf, dst_len);\n\n    memset (dst + dst_len, 0, dst_pw_len4 - dst_len);\n\n    // prepare next element\n\n    pw_idx_t *pw_idx_dst_next = pw_idx_dst + 1;\n\n    pw_idx_dst_next->off = pw_idx_dst->off + pw_idx_dst->cnt;\n  }\n\n  memcpy (device_param->pws_comp, tmp_pws_comp, device_param->size_pws_comp);\n  memcpy (device_param->pws_idx,  tmp_pws_idx,  device_param->size_pws_idx);\n\n  hcfree (tmp_pws_comp);\n  hcfree (tmp_pws_idx);\n}\n\nint run_cuda_kernel_atinit (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, CUdeviceptr buf, const u64 num)\n{\n  u64 num_elements = num;\n\n  device_param->kernel_params_atinit[0]       = (void *) &buf;\n  device_param->kernel_params_atinit_buf64[1] = num_elements;\n\n  const u64 kernel_threads = device_param->kernel_wgs_atinit;\n\n  num_elements = CEILDIV (num_elements, kernel_threads);\n\n  CUfunction function = device_param->cuda_function_atinit;\n\n  if (hc_cuLaunchKernel (hashcat_ctx, function, num_elements, 1, 1, kernel_threads, 1, 1, 0, device_param->cuda_stream, device_param->kernel_params_atinit, NULL) == -1) return -1;\n\n  if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n\n  return 0;\n}\n\nint run_cuda_kernel_memset (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, CUdeviceptr buf, const u32 value, const u64 size)\n{\n  const u64 num16d = size / 16;\n  const u64 num16m = size % 16;\n\n  if (num16d)\n  {\n    device_param->kernel_params_memset[0]       = (void *) &buf;\n    device_param->kernel_params_memset_buf32[1] = value;\n    device_param->kernel_params_memset_buf64[2] = num16d;\n\n    const u64 kernel_threads = device_param->kernel_wgs_memset;\n\n    u64 num_elements = num16d;\n\n    num_elements = CEILDIV (num_elements, kernel_threads);\n\n    CUfunction function = device_param->cuda_function_memset;\n\n    //CU_rc = hc_clSetKernelArg (hashcat_ctx, kernel, 0, sizeof (cl_mem),   (void *) &buf);                         if (CU_rc == -1) return -1;\n    //CU_rc = hc_clSetKernelArg (hashcat_ctx, kernel, 1, sizeof (cl_uint),  device_param->kernel_params_memset[1]); if (CU_rc == -1) return -1;\n    //CU_rc = hc_clSetKernelArg (hashcat_ctx, kernel, 2, sizeof (cl_ulong), device_param->kernel_params_memset[2]); if (CU_rc == -1) return -1;\n\n    //const size_t global_work_size[3] = { num_elements,   1, 1 };\n    //const size_t local_work_size[3]  = { kernel_threads, 1, 1 };\n\n    if (hc_cuLaunchKernel (hashcat_ctx, function, num_elements, 1, 1, kernel_threads, 1, 1, 0, device_param->cuda_stream, device_param->kernel_params_memset, NULL) == -1) return -1;\n\n    if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n  }\n\n  if (num16m)\n  {\n    u32 tmp[4];\n\n    tmp[0] = value;\n    tmp[1] = value;\n    tmp[2] = value;\n    tmp[3] = value;\n\n    // Apparently are allowed to do this: https://devtalk.nvidia.com/default/topic/761515/how-to-copy-to-device-memory-with-offset-/\n\n    if (hc_cuMemcpyHtoD (hashcat_ctx, buf + (num16d * 16), tmp, num16m) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_cuda_kernel_bzero (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, CUdeviceptr buf, const u64 size)\n{\n  return run_cuda_kernel_memset (hashcat_ctx, device_param, buf, 0, size);\n}\n\nint run_opencl_kernel_atinit (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_mem buf, const u64 num)\n{\n  u64 num_elements = num;\n\n  device_param->kernel_params_atinit_buf64[1] = num_elements;\n\n  const u64 kernel_threads = device_param->kernel_wgs_atinit;\n\n  num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n  cl_kernel kernel = device_param->opencl_kernel_atinit;\n\n  const size_t global_work_size[3] = { num_elements,    1, 1 };\n  const size_t local_work_size[3]  = { kernel_threads,  1, 1 };\n\n  if (hc_clSetKernelArg (hashcat_ctx, kernel, 0, sizeof (cl_mem), (void *) &buf) == -1) return -1;\n\n  if (hc_clSetKernelArg (hashcat_ctx, kernel, 1, sizeof (cl_ulong), device_param->kernel_params_atinit[1]) == -1) return -1;\n\n  if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL) == -1) return -1;\n\n  if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n\n  if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n\n  return 0;\n}\n\nint run_opencl_kernel_memset (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_mem buf, const u32 value, const u64 size)\n{\n  const u64 num16d = size / 16;\n  const u64 num16m = size % 16;\n\n  if (num16d)\n  {\n    device_param->kernel_params_memset_buf32[1] = value;\n    device_param->kernel_params_memset_buf64[2] = num16d;\n\n    const u64 kernel_threads = device_param->kernel_wgs_memset;\n\n    u64 num_elements = num16d;\n\n    num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n    cl_kernel kernel = device_param->opencl_kernel_memset;\n\n    if (hc_clSetKernelArg (hashcat_ctx, kernel, 0, sizeof (cl_mem),   (void *) &buf) == -1)                         return -1;\n    if (hc_clSetKernelArg (hashcat_ctx, kernel, 1, sizeof (cl_uint),  device_param->kernel_params_memset[1]) == -1) return -1;\n    if (hc_clSetKernelArg (hashcat_ctx, kernel, 2, sizeof (cl_ulong), device_param->kernel_params_memset[2]) == -1) return -1;\n\n    const size_t global_work_size[3] = { num_elements,   1, 1 };\n    const size_t local_work_size[3]  = { kernel_threads, 1, 1 };\n\n    if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL) == -1) return -1;\n\n    if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n\n    if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n  }\n\n  if (num16m)\n  {\n    u32 tmp[4];\n\n    tmp[0] = value;\n    tmp[1] = value;\n    tmp[2] = value;\n    tmp[3] = value;\n\n    if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, buf, CL_TRUE, num16d * 16, num16m, tmp, 0, NULL, NULL) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_opencl_kernel_bzero (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_mem buf, const u64 size)\n{\n  return run_opencl_kernel_memset (hashcat_ctx, device_param, buf, 0, size);\n}\n\nint run_kernel (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u32 kern_run, const u64 num, const u32 event_update, const u32 iteration)\n{\n  const hashconfig_t   *hashconfig   = hashcat_ctx->hashconfig;\n  const status_ctx_t   *status_ctx   = hashcat_ctx->status_ctx;\n\n  u64 kernel_threads = 0;\n  u64 dynamic_shared_mem = 0;\n\n  switch (kern_run)\n  {\n    case KERN_RUN_1:\n      kernel_threads     = device_param->kernel_wgs1;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size1;\n      break;\n    case KERN_RUN_12:\n      kernel_threads     = device_param->kernel_wgs12;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size12;\n      break;\n    case KERN_RUN_2:\n      kernel_threads     = device_param->kernel_wgs2;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size2;\n      break;\n    case KERN_RUN_2E:\n      kernel_threads     = device_param->kernel_wgs2e;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size2e;\n      break;\n    case KERN_RUN_23:\n      kernel_threads     = device_param->kernel_wgs23;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size23;\n      break;\n    case KERN_RUN_3:\n      kernel_threads     = device_param->kernel_wgs3;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size3;\n      break;\n    case KERN_RUN_4:\n      kernel_threads     = device_param->kernel_wgs4;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size4;\n      break;\n    case KERN_RUN_INIT2:\n      kernel_threads     = device_param->kernel_wgs_init2;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size_init2;\n      break;\n    case KERN_RUN_LOOP2:\n      kernel_threads     = device_param->kernel_wgs_loop2;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size_loop2;\n      break;\n    case KERN_RUN_AUX1:\n      kernel_threads     = device_param->kernel_wgs_aux1;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size_aux1;\n      break;\n    case KERN_RUN_AUX2:\n      kernel_threads     = device_param->kernel_wgs_aux2;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size_aux2;\n      break;\n    case KERN_RUN_AUX3:\n      kernel_threads     = device_param->kernel_wgs_aux3;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size_aux3;\n      break;\n    case KERN_RUN_AUX4:\n      kernel_threads     = device_param->kernel_wgs_aux4;\n      dynamic_shared_mem = device_param->kernel_dynamic_local_mem_size_aux4;\n      break;\n  }\n\n  if ((hashconfig->opts_type & OPTS_TYPE_DYNAMIC_SHARED) == 0)\n  {\n    dynamic_shared_mem = 0;\n  }\n\n  if (device_param->is_cuda == true)\n  {\n    if ((device_param->kernel_dynamic_local_mem_size_memset % device_param->device_local_mem_size) == 0)\n    {\n      // this is the case Compute Capability 7.5\n      // there is also Compute Capability 7.0 which offers a larger dynamic local size access\n      // however, if it's an exact multiple the driver can optimize this for us more efficient\n\n      dynamic_shared_mem = 0;\n    }\n  }\n\n  kernel_threads = MIN (kernel_threads, device_param->kernel_threads);\n\n  device_param->kernel_params_buf64[34] = num;\n\n  u64 num_elements = num;\n\n  if (device_param->is_cuda == true)\n  {\n    CUfunction cuda_function = NULL;\n\n    if (device_param->is_cuda == true)\n    {\n      switch (kern_run)\n      {\n        case KERN_RUN_1:      cuda_function = device_param->cuda_function1;      break;\n        case KERN_RUN_12:     cuda_function = device_param->cuda_function12;     break;\n        case KERN_RUN_2:      cuda_function = device_param->cuda_function2;      break;\n        case KERN_RUN_2E:     cuda_function = device_param->cuda_function2e;     break;\n        case KERN_RUN_23:     cuda_function = device_param->cuda_function23;     break;\n        case KERN_RUN_3:      cuda_function = device_param->cuda_function3;      break;\n        case KERN_RUN_4:      cuda_function = device_param->cuda_function4;      break;\n        case KERN_RUN_INIT2:  cuda_function = device_param->cuda_function_init2; break;\n        case KERN_RUN_LOOP2:  cuda_function = device_param->cuda_function_loop2; break;\n        case KERN_RUN_AUX1:   cuda_function = device_param->cuda_function_aux1;  break;\n        case KERN_RUN_AUX2:   cuda_function = device_param->cuda_function_aux2;  break;\n        case KERN_RUN_AUX3:   cuda_function = device_param->cuda_function_aux3;  break;\n        case KERN_RUN_AUX4:   cuda_function = device_param->cuda_function_aux4;  break;\n      }\n\n      if (hc_cuFuncSetAttribute (hashcat_ctx, cuda_function, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, dynamic_shared_mem) == -1) return -1;\n    }\n\n    if (kernel_threads == 0) kernel_threads = 1;\n\n    num_elements = CEILDIV (num_elements, kernel_threads);\n\n    if (kern_run == KERN_RUN_1)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_SLOW_HASH_SIMD_INIT)\n      {\n        num_elements = CEILDIV (num_elements, device_param->vector_width);\n      }\n    }\n    else if (kern_run == KERN_RUN_2)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_SLOW_HASH_SIMD_LOOP)\n      {\n        num_elements = CEILDIV (num_elements, device_param->vector_width);\n      }\n    }\n    else if (kern_run == KERN_RUN_3)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_SLOW_HASH_SIMD_COMP)\n      {\n        num_elements = CEILDIV (num_elements, device_param->vector_width);\n      }\n    }\n\n    if (hc_cuEventRecord (hashcat_ctx, device_param->cuda_event1, device_param->cuda_stream) == -1) return -1;\n\n    if (hc_cuLaunchKernel (hashcat_ctx, cuda_function, num_elements, 1, 1, kernel_threads, 1, 1, dynamic_shared_mem, device_param->cuda_stream, device_param->kernel_params, NULL) == -1) return -1;\n\n    if (hc_cuEventRecord (hashcat_ctx, device_param->cuda_event2, device_param->cuda_stream) == -1) return -1;\n\n    if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n\n    if (hc_cuEventSynchronize (hashcat_ctx, device_param->cuda_event2) == -1) return -1;\n\n    float exec_ms;\n\n    if (hc_cuEventElapsedTime (hashcat_ctx, &exec_ms, device_param->cuda_event1, device_param->cuda_event2) == -1) return -1;\n\n    if (event_update)\n    {\n      u32 exec_pos = device_param->exec_pos;\n\n      device_param->exec_msec[exec_pos] = exec_ms;\n\n      exec_pos++;\n\n      if (exec_pos == EXEC_CACHE)\n      {\n        exec_pos = 0;\n      }\n\n      device_param->exec_pos = exec_pos;\n    }\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    cl_kernel opencl_kernel = NULL;\n\n    if (device_param->is_opencl == true)\n    {\n      switch (kern_run)\n      {\n        case KERN_RUN_1:      opencl_kernel = device_param->opencl_kernel1;      break;\n        case KERN_RUN_12:     opencl_kernel = device_param->opencl_kernel12;     break;\n        case KERN_RUN_2:      opencl_kernel = device_param->opencl_kernel2;      break;\n        case KERN_RUN_2E:     opencl_kernel = device_param->opencl_kernel2e;     break;\n        case KERN_RUN_23:     opencl_kernel = device_param->opencl_kernel23;     break;\n        case KERN_RUN_3:      opencl_kernel = device_param->opencl_kernel3;      break;\n        case KERN_RUN_4:      opencl_kernel = device_param->opencl_kernel4;      break;\n        case KERN_RUN_INIT2:  opencl_kernel = device_param->opencl_kernel_init2; break;\n        case KERN_RUN_LOOP2:  opencl_kernel = device_param->opencl_kernel_loop2; break;\n        case KERN_RUN_AUX1:   opencl_kernel = device_param->opencl_kernel_aux1;  break;\n        case KERN_RUN_AUX2:   opencl_kernel = device_param->opencl_kernel_aux2;  break;\n        case KERN_RUN_AUX3:   opencl_kernel = device_param->opencl_kernel_aux3;  break;\n        case KERN_RUN_AUX4:   opencl_kernel = device_param->opencl_kernel_aux4;  break;\n      }\n    }\n\n    for (u32 i = 0; i <= 23; i++)\n    {\n      if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, i, sizeof (cl_mem), device_param->kernel_params[i]) == -1) return -1;\n    }\n\n    for (u32 i = 24; i <= 33; i++)\n    {\n      if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, i, sizeof (cl_uint), device_param->kernel_params[i]) == -1) return -1;\n    }\n\n    for (u32 i = 34; i <= 34; i++)\n    {\n      if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, i, sizeof (cl_ulong), device_param->kernel_params[i]) == -1) return -1;\n    }\n\n    num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n    cl_event opencl_event;\n\n    if (kern_run == KERN_RUN_1)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_SLOW_HASH_SIMD_INIT)\n      {\n        num_elements = CEILDIV (num_elements, device_param->vector_width);\n      }\n    }\n    else if (kern_run == KERN_RUN_2)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_SLOW_HASH_SIMD_LOOP)\n      {\n        num_elements = CEILDIV (num_elements, device_param->vector_width);\n      }\n    }\n    else if (kern_run == KERN_RUN_3)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_SLOW_HASH_SIMD_COMP)\n      {\n        num_elements = CEILDIV (num_elements, device_param->vector_width);\n      }\n    }\n\n    num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n    const size_t global_work_size[3] = { num_elements,   1, 1 };\n    const size_t local_work_size[3]  = { kernel_threads, 1, 1 };\n\n    if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, opencl_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, &opencl_event) == -1) return -1;\n\n    if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n\n    // spin damper section\n\n    const u32 iterationm = iteration % EXPECTED_ITERATIONS;\n\n    if (device_param->spin_damp > 0)\n    {\n      cl_int opencl_event_status;\n\n      size_t param_value_size_ret;\n\n      if (hc_clGetEventInfo (hashcat_ctx, opencl_event, CL_EVENT_COMMAND_EXECUTION_STATUS, sizeof (opencl_event_status), &opencl_event_status, &param_value_size_ret) == -1) return -1;\n\n      double spin_total = device_param->spin_damp;\n\n      while (opencl_event_status != CL_COMPLETE)\n      {\n        if (status_ctx->devices_status == STATUS_RUNNING)\n        {\n          switch (kern_run)\n          {\n            case KERN_RUN_1:      if (device_param->exec_us_prev1[iterationm]      > 0) usleep ((useconds_t) (device_param->exec_us_prev1[iterationm]      * device_param->spin_damp)); break;\n            case KERN_RUN_2:      if (device_param->exec_us_prev2[iterationm]      > 0) usleep ((useconds_t) (device_param->exec_us_prev2[iterationm]      * device_param->spin_damp)); break;\n            case KERN_RUN_2E:     if (device_param->exec_us_prev2e[iterationm]     > 0) usleep ((useconds_t) (device_param->exec_us_prev2e[iterationm]     * device_param->spin_damp)); break;\n            case KERN_RUN_3:      if (device_param->exec_us_prev3[iterationm]      > 0) usleep ((useconds_t) (device_param->exec_us_prev3[iterationm]      * device_param->spin_damp)); break;\n            case KERN_RUN_4:      if (device_param->exec_us_prev4[iterationm]      > 0) usleep ((useconds_t) (device_param->exec_us_prev4[iterationm]      * device_param->spin_damp)); break;\n            case KERN_RUN_INIT2:  if (device_param->exec_us_prev_init2[iterationm] > 0) usleep ((useconds_t) (device_param->exec_us_prev_init2[iterationm] * device_param->spin_damp)); break;\n            case KERN_RUN_LOOP2:  if (device_param->exec_us_prev_loop2[iterationm] > 0) usleep ((useconds_t) (device_param->exec_us_prev_loop2[iterationm] * device_param->spin_damp)); break;\n            case KERN_RUN_AUX1:   if (device_param->exec_us_prev_aux1[iterationm]  > 0) usleep ((useconds_t) (device_param->exec_us_prev_aux1[iterationm]  * device_param->spin_damp)); break;\n            case KERN_RUN_AUX2:   if (device_param->exec_us_prev_aux2[iterationm]  > 0) usleep ((useconds_t) (device_param->exec_us_prev_aux2[iterationm]  * device_param->spin_damp)); break;\n            case KERN_RUN_AUX3:   if (device_param->exec_us_prev_aux3[iterationm]  > 0) usleep ((useconds_t) (device_param->exec_us_prev_aux3[iterationm]  * device_param->spin_damp)); break;\n            case KERN_RUN_AUX4:   if (device_param->exec_us_prev_aux4[iterationm]  > 0) usleep ((useconds_t) (device_param->exec_us_prev_aux4[iterationm]  * device_param->spin_damp)); break;\n          }\n        }\n        else\n        {\n          // we were told to be nice\n\n          sleep (0);\n        }\n\n        if (hc_clGetEventInfo (hashcat_ctx, opencl_event, CL_EVENT_COMMAND_EXECUTION_STATUS, sizeof (opencl_event_status), &opencl_event_status, &param_value_size_ret) == -1) return -1;\n\n        spin_total += device_param->spin_damp;\n\n        if (spin_total > 1) break;\n      }\n    }\n\n    if (hc_clWaitForEvents (hashcat_ctx, 1, &opencl_event) == -1) return -1;\n\n    cl_ulong time_start;\n    cl_ulong time_end;\n\n    if (hc_clGetEventProfilingInfo (hashcat_ctx, opencl_event, CL_PROFILING_COMMAND_START, sizeof (time_start), &time_start, NULL) == -1) return -1;\n    if (hc_clGetEventProfilingInfo (hashcat_ctx, opencl_event, CL_PROFILING_COMMAND_END,   sizeof (time_end),   &time_end,   NULL) == -1) return -1;\n\n    const double exec_us = (double) (time_end - time_start) / 1000;\n\n    if (device_param->spin_damp > 0)\n    {\n      if (status_ctx->devices_status == STATUS_RUNNING)\n      {\n        switch (kern_run)\n        {\n          case KERN_RUN_1:      device_param->exec_us_prev1[iterationm]      = exec_us; break;\n          case KERN_RUN_2:      device_param->exec_us_prev2[iterationm]      = exec_us; break;\n          case KERN_RUN_2E:     device_param->exec_us_prev2e[iterationm]     = exec_us; break;\n          case KERN_RUN_3:      device_param->exec_us_prev3[iterationm]      = exec_us; break;\n          case KERN_RUN_4:      device_param->exec_us_prev4[iterationm]      = exec_us; break;\n          case KERN_RUN_INIT2:  device_param->exec_us_prev_init2[iterationm] = exec_us; break;\n          case KERN_RUN_LOOP2:  device_param->exec_us_prev_loop2[iterationm] = exec_us; break;\n          case KERN_RUN_AUX1:   device_param->exec_us_prev_aux1[iterationm]  = exec_us; break;\n          case KERN_RUN_AUX2:   device_param->exec_us_prev_aux2[iterationm]  = exec_us; break;\n          case KERN_RUN_AUX3:   device_param->exec_us_prev_aux3[iterationm]  = exec_us; break;\n          case KERN_RUN_AUX4:   device_param->exec_us_prev_aux4[iterationm]  = exec_us; break;\n        }\n      }\n    }\n\n    if (event_update)\n    {\n      u32 exec_pos = device_param->exec_pos;\n\n      device_param->exec_msec[exec_pos] = exec_us / 1000;\n\n      exec_pos++;\n\n      if (exec_pos == EXEC_CACHE)\n      {\n        exec_pos = 0;\n      }\n\n      device_param->exec_pos = exec_pos;\n    }\n\n    if (hc_clReleaseEvent (hashcat_ctx, opencl_event) == -1) return -1;\n\n    if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_kernel_mp (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u32 kern_run, const u64 num)\n{\n  u64 kernel_threads = 0;\n\n  switch (kern_run)\n  {\n    case KERN_RUN_MP:   kernel_threads  = device_param->kernel_wgs_mp;    break;\n    case KERN_RUN_MP_R: kernel_threads  = device_param->kernel_wgs_mp_r;  break;\n    case KERN_RUN_MP_L: kernel_threads  = device_param->kernel_wgs_mp_l;  break;\n  }\n\n  u64 num_elements = num;\n\n  switch (kern_run)\n  {\n    case KERN_RUN_MP:   device_param->kernel_params_mp_buf64[8]   = num; break;\n    case KERN_RUN_MP_R: device_param->kernel_params_mp_r_buf64[8] = num; break;\n    case KERN_RUN_MP_L: device_param->kernel_params_mp_l_buf64[9] = num; break;\n  }\n\n  if (device_param->is_cuda == true)\n  {\n    CUfunction cuda_function = NULL;\n\n    void **cuda_args = NULL;\n\n    switch (kern_run)\n    {\n      case KERN_RUN_MP:   cuda_function = device_param->cuda_function_mp;\n                          cuda_args     = device_param->kernel_params_mp;\n                          break;\n      case KERN_RUN_MP_R: cuda_function = device_param->cuda_function_mp_r;\n                          cuda_args     = device_param->kernel_params_mp_r;\n                          break;\n      case KERN_RUN_MP_L: cuda_function = device_param->cuda_function_mp_l;\n                          cuda_args     = device_param->kernel_params_mp_l;\n                          break;\n    }\n\n    num_elements = CEILDIV (num_elements, kernel_threads);\n\n    if (hc_cuLaunchKernel (hashcat_ctx, cuda_function, num_elements, 1, 1, kernel_threads, 1, 1, 0, device_param->cuda_stream, cuda_args, NULL) == -1) return -1;\n\n    if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    cl_kernel opencl_kernel = NULL;\n\n    switch (kern_run)\n    {\n      case KERN_RUN_MP:   opencl_kernel = device_param->opencl_kernel_mp;   break;\n      case KERN_RUN_MP_R: opencl_kernel = device_param->opencl_kernel_mp_r; break;\n      case KERN_RUN_MP_L: opencl_kernel = device_param->opencl_kernel_mp_l; break;\n    }\n\n    switch (kern_run)\n    {\n      case KERN_RUN_MP:   if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 3, sizeof (cl_ulong), device_param->kernel_params_mp[3]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 4, sizeof (cl_uint),  device_param->kernel_params_mp[4]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 5, sizeof (cl_uint),  device_param->kernel_params_mp[5]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 6, sizeof (cl_uint),  device_param->kernel_params_mp[6]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 7, sizeof (cl_uint),  device_param->kernel_params_mp[7]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 8, sizeof (cl_ulong), device_param->kernel_params_mp[8]) == -1) return -1;\n                          break;\n      case KERN_RUN_MP_R: if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 3, sizeof (cl_ulong), device_param->kernel_params_mp_r[3]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 4, sizeof (cl_uint),  device_param->kernel_params_mp_r[4]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 5, sizeof (cl_uint),  device_param->kernel_params_mp_r[5]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 6, sizeof (cl_uint),  device_param->kernel_params_mp_r[6]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 7, sizeof (cl_uint),  device_param->kernel_params_mp_r[7]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 8, sizeof (cl_ulong), device_param->kernel_params_mp_r[8]) == -1) return -1;\n                          break;\n      case KERN_RUN_MP_L: if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 3, sizeof (cl_ulong), device_param->kernel_params_mp_l[3]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 4, sizeof (cl_uint),  device_param->kernel_params_mp_l[4]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 5, sizeof (cl_uint),  device_param->kernel_params_mp_l[5]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 6, sizeof (cl_uint),  device_param->kernel_params_mp_l[6]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 7, sizeof (cl_uint),  device_param->kernel_params_mp_l[7]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 8, sizeof (cl_uint),  device_param->kernel_params_mp_l[8]) == -1) return -1;\n                          if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 9, sizeof (cl_ulong), device_param->kernel_params_mp_l[9]) == -1) return -1;\n                          break;\n    }\n\n    num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n    const size_t global_work_size[3] = { num_elements,   1, 1 };\n    const size_t local_work_size[3]  = { kernel_threads, 1, 1 };\n\n    if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, opencl_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL) == -1) return -1;\n\n    if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue)  == -1) return -1;\n\n    if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_kernel_tm (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param)\n{\n  const u64 num_elements = 1024; // fixed\n\n  const u64 kernel_threads = MIN (num_elements, device_param->kernel_wgs_tm);\n\n  if (device_param->is_cuda == true)\n  {\n    CUfunction cuda_function = device_param->cuda_function_tm;\n\n    if (hc_cuLaunchKernel (hashcat_ctx, cuda_function, num_elements / kernel_threads, 1, 1, kernel_threads, 1, 1, 0, device_param->cuda_stream, device_param->kernel_params_tm, NULL) == -1) return -1;\n\n    if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    cl_kernel cuda_kernel = device_param->opencl_kernel_tm;\n\n    const size_t global_work_size[3] = { num_elements,    1, 1 };\n    const size_t local_work_size[3]  = { kernel_threads,  1, 1 };\n\n    if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, cuda_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL) == -1) return -1;\n\n    if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n\n    if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_kernel_amp (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u64 num)\n{\n  device_param->kernel_params_amp_buf64[6] = num;\n\n  u64 num_elements = num;\n\n  const u64 kernel_threads = device_param->kernel_wgs_amp;\n\n  if (device_param->is_cuda == true)\n  {\n    num_elements = CEILDIV (num_elements, kernel_threads);\n\n    CUfunction cuda_function = device_param->cuda_function_amp;\n\n    if (hc_cuLaunchKernel (hashcat_ctx, cuda_function, num_elements, 1, 1, kernel_threads, 1, 1, 0, device_param->cuda_stream, device_param->kernel_params_amp, NULL) == -1) return -1;\n\n    if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n    cl_kernel opencl_kernel = device_param->opencl_kernel_amp;\n\n    if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 6, sizeof (cl_ulong), device_param->kernel_params_amp[6]) == -1) return -1;\n\n    const size_t global_work_size[3] = { num_elements,    1, 1 };\n    const size_t local_work_size[3]  = { kernel_threads,  1, 1 };\n\n    if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, opencl_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL) == -1) return -1;\n\n    if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue)  == -1) return -1;\n\n    if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_kernel_decompress (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u64 num)\n{\n  device_param->kernel_params_decompress_buf64[3] = num;\n\n  u64 num_elements = num;\n\n  const u64 kernel_threads = device_param->kernel_wgs_decompress;\n\n  if (device_param->is_cuda == true)\n  {\n    num_elements = CEILDIV (num_elements, kernel_threads);\n\n    CUfunction cuda_function = device_param->cuda_function_decompress;\n\n    if (hc_cuLaunchKernel (hashcat_ctx, cuda_function, num_elements, 1, 1, kernel_threads, 1, 1, 0, device_param->cuda_stream, device_param->kernel_params_decompress, NULL) == -1) return -1;\n\n    if (hc_cuStreamSynchronize (hashcat_ctx, device_param->cuda_stream) == -1) return -1;\n  }\n\n  if (device_param->is_opencl == true)\n  {\n    num_elements = round_up_multiple_64 (num_elements, kernel_threads);\n\n    cl_kernel opencl_kernel = device_param->opencl_kernel_decompress;\n\n    const size_t global_work_size[3] = { num_elements,    1, 1 };\n    const size_t local_work_size[3]  = { kernel_threads,  1, 1 };\n\n    if (hc_clSetKernelArg (hashcat_ctx, opencl_kernel, 3, sizeof (cl_ulong), device_param->kernel_params_decompress[3]) == -1) return -1;\n\n    if (hc_clEnqueueNDRangeKernel (hashcat_ctx, device_param->opencl_command_queue, opencl_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL) == -1) return -1;\n\n    if (hc_clFlush (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n\n    if (hc_clFinish (hashcat_ctx, device_param->opencl_command_queue) == -1) return -1;\n  }\n\n  return 0;\n}\n\nint run_copy (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u64 pws_cnt)\n{\n  combinator_ctx_t     *combinator_ctx      = hashcat_ctx->combinator_ctx;\n  hashconfig_t         *hashconfig          = hashcat_ctx->hashconfig;\n  user_options_t       *user_options        = hashcat_ctx->user_options;\n  user_options_extra_t *user_options_extra  = hashcat_ctx->user_options_extra;\n\n  // init speed timer\n\n  #if defined (_WIN)\n  if (device_param->timer_speed.QuadPart == 0)\n  {\n    hc_timer_set (&device_param->timer_speed);\n  }\n  #else\n  if (device_param->timer_speed.tv_sec == 0)\n  {\n    hc_timer_set (&device_param->timer_speed);\n  }\n  #endif\n\n  if (user_options->slow_candidates == true)\n  {\n    if (device_param->is_cuda == true)\n    {\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_idx, device_param->pws_idx, pws_cnt * sizeof (pw_idx_t)) == -1) return -1;\n\n      const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n      const u32 off = pw_idx->off;\n\n      if (off)\n      {\n        if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_comp_buf, device_param->pws_comp, off * sizeof (u32)) == -1) return -1;\n      }\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_idx, CL_TRUE, 0, pws_cnt * sizeof (pw_idx_t), device_param->pws_idx, 0, NULL, NULL) == -1) return -1;\n\n      const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n      const u32 off = pw_idx->off;\n\n      if (off)\n      {\n        if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_comp_buf, CL_TRUE, 0, off * sizeof (u32), device_param->pws_comp, 0, NULL, NULL) == -1) return -1;\n      }\n    }\n\n    if (run_kernel_decompress (hashcat_ctx, device_param, pws_cnt) == -1) return -1;\n  }\n  else\n  {\n    if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n    {\n      if (device_param->is_cuda == true)\n      {\n        if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_idx, device_param->pws_idx, pws_cnt * sizeof (pw_idx_t)) == -1) return -1;\n\n        const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n        const u32 off = pw_idx->off;\n\n        if (off)\n        {\n          if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_comp_buf, device_param->pws_comp, off * sizeof (u32)) == -1) return -1;\n        }\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_idx, CL_TRUE, 0, pws_cnt * sizeof (pw_idx_t), device_param->pws_idx, 0, NULL, NULL) == -1) return -1;\n\n        const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n        const u32 off = pw_idx->off;\n\n        if (off)\n        {\n          if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_comp_buf, CL_TRUE, 0, off * sizeof (u32), device_param->pws_comp, 0, NULL, NULL) == -1) return -1;\n        }\n      }\n\n      if (run_kernel_decompress (hashcat_ctx, device_param, pws_cnt) == -1) return -1;\n    }\n    else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n      {\n        if (user_options->attack_mode == ATTACK_MODE_COMBI)\n        {\n          if (combinator_ctx->combs_mode == COMBINATOR_MODE_BASE_RIGHT)\n          {\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)\n            {\n              rebuild_pws_compressed_append (device_param, pws_cnt, 0x01);\n            }\n            else if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)\n            {\n              rebuild_pws_compressed_append (device_param, pws_cnt, 0x06);\n            }\n            else if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)\n            {\n              rebuild_pws_compressed_append (device_param, pws_cnt, 0x80);\n            }\n          }\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n        {\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)\n          {\n            rebuild_pws_compressed_append (device_param, pws_cnt, 0x01);\n          }\n          else if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)\n          {\n            rebuild_pws_compressed_append (device_param, pws_cnt, 0x06);\n          }\n          else if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)\n          {\n            rebuild_pws_compressed_append (device_param, pws_cnt, 0x80);\n          }\n        }\n\n        if (device_param->is_cuda == true)\n        {\n          if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_idx, device_param->pws_idx, pws_cnt * sizeof (pw_idx_t)) == -1) return -1;\n\n          const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n          const u32 off = pw_idx->off;\n\n          if (off)\n          {\n            if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_comp_buf, device_param->pws_comp, off * sizeof (u32)) == -1) return -1;\n          }\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_idx, CL_TRUE, 0, pws_cnt * sizeof (pw_idx_t), device_param->pws_idx, 0, NULL, NULL) == -1) return -1;\n\n          const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n          const u32 off = pw_idx->off;\n\n          if (off)\n          {\n            if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_comp_buf, CL_TRUE, 0, off * sizeof (u32), device_param->pws_comp, 0, NULL, NULL) == -1) return -1;\n          }\n        }\n\n        if (run_kernel_decompress (hashcat_ctx, device_param, pws_cnt) == -1) return -1;\n      }\n      else\n      {\n        if (user_options->attack_mode == ATTACK_MODE_COMBI)\n        {\n          if (device_param->is_cuda == true)\n          {\n            if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_idx, device_param->pws_idx, pws_cnt * sizeof (pw_idx_t)) == -1) return -1;\n\n            const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n            const u32 off = pw_idx->off;\n\n            if (off)\n            {\n              if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_comp_buf, device_param->pws_comp, off * sizeof (u32)) == -1) return -1;\n            }\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_idx, CL_TRUE, 0, pws_cnt * sizeof (pw_idx_t), device_param->pws_idx, 0, NULL, NULL) == -1) return -1;\n\n            const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n            const u32 off = pw_idx->off;\n\n            if (off)\n            {\n              if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_comp_buf, CL_TRUE, 0, off * sizeof (u32), device_param->pws_comp, 0, NULL, NULL) == -1) return -1;\n            }\n          }\n\n          if (run_kernel_decompress (hashcat_ctx, device_param, pws_cnt) == -1) return -1;\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n        {\n          if (device_param->is_cuda == true)\n          {\n            if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_idx, device_param->pws_idx, pws_cnt * sizeof (pw_idx_t)) == -1) return -1;\n\n            const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n            const u32 off = pw_idx->off;\n\n            if (off)\n            {\n              if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_pws_comp_buf, device_param->pws_comp, off * sizeof (u32)) == -1) return -1;\n            }\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_idx, CL_TRUE, 0, pws_cnt * sizeof (pw_idx_t), device_param->pws_idx, 0, NULL, NULL) == -1) return -1;\n\n            const pw_idx_t *pw_idx = device_param->pws_idx + pws_cnt;\n\n            const u32 off = pw_idx->off;\n\n            if (off)\n            {\n              if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_pws_comp_buf, CL_TRUE, 0, off * sizeof (u32), device_param->pws_comp, 0, NULL, NULL) == -1) return -1;\n            }\n          }\n\n          if (run_kernel_decompress (hashcat_ctx, device_param, pws_cnt) == -1) return -1;\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n        {\n          const u64 off = device_param->words_off;\n\n          device_param->kernel_params_mp_buf64[3] = off;\n\n          if (run_kernel_mp (hashcat_ctx, device_param, KERN_RUN_MP, pws_cnt) == -1) return -1;\n        }\n      }\n    }\n    else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n    {\n      const u64 off = device_param->words_off;\n\n      device_param->kernel_params_mp_l_buf64[3] = off;\n\n      if (run_kernel_mp (hashcat_ctx, device_param, KERN_RUN_MP_L, pws_cnt) == -1) return -1;\n    }\n  }\n\n  return 0;\n}\n\nint run_cracker (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const u64 pws_cnt)\n{\n  combinator_ctx_t      *combinator_ctx     = hashcat_ctx->combinator_ctx;\n  hashconfig_t          *hashconfig         = hashcat_ctx->hashconfig;\n  hashes_t              *hashes             = hashcat_ctx->hashes;\n  mask_ctx_t            *mask_ctx           = hashcat_ctx->mask_ctx;\n  status_ctx_t          *status_ctx         = hashcat_ctx->status_ctx;\n  straight_ctx_t        *straight_ctx       = hashcat_ctx->straight_ctx;\n  user_options_t        *user_options       = hashcat_ctx->user_options;\n  user_options_extra_t  *user_options_extra = hashcat_ctx->user_options_extra;\n\n  // do the on-the-fly combinator mode encoding\n\n  bool iconv_enabled = false;\n\n  iconv_t iconv_ctx = NULL;\n\n  char *iconv_tmp = NULL;\n\n  if (strcmp (user_options->encoding_from, user_options->encoding_to) != 0)\n  {\n    iconv_enabled = true;\n\n    iconv_ctx = iconv_open (user_options->encoding_to, user_options->encoding_from);\n\n    if (iconv_ctx == (iconv_t) -1) return -1;\n\n    iconv_tmp = (char *) hcmalloc (HCBUFSIZ_TINY);\n  }\n\n  // find higest password length, this is for optimization stuff\n\n  u32 highest_pw_len = 0;\n\n  if (user_options->slow_candidates == true)\n  {\n    /*\n    for (u64 pws_idx = 0; pws_idx < pws_cnt; pws_idx++)\n    {\n      pw_idx_t *pw_idx = device_param->pws_idx + pws_idx;\n\n      highest_pw_len = MAX (highest_pw_len, pw_idx->len);\n    }\n    */\n  }\n  else\n  {\n    if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n    {\n    }\n    else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n    {\n    }\n    else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n    {\n      highest_pw_len = device_param->kernel_params_mp_l_buf32[4]\n                     + device_param->kernel_params_mp_l_buf32[5];\n    }\n  }\n\n  // we make use of this in status view\n\n  device_param->outerloop_multi = 1;\n  device_param->outerloop_msec  = 0;\n  device_param->outerloop_pos   = 0;\n  device_param->outerloop_left  = pws_cnt;\n\n  // we ignore the time to copy data over pci bus in this case\n\n  if (user_options->speed_only == true)\n  {\n    hc_timer_set (&device_param->timer_speed);\n  }\n\n  // loop start: most outer loop = salt iteration, then innerloops (if multi)\n\n  for (u32 salt_pos = 0; salt_pos < hashes->salts_cnt; salt_pos++)\n  {\n    while (status_ctx->devices_status == STATUS_PAUSED) sleep (1);\n\n    salt_t *salt_buf = &hashes->salts_buf[salt_pos];\n\n    device_param->kernel_params_buf32[27] = salt_pos;\n    device_param->kernel_params_buf32[31] = salt_buf->digests_cnt;\n    device_param->kernel_params_buf32[32] = salt_buf->digests_offset;\n\n    HCFILE *combs_fp = &device_param->combs_fp;\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      if ((user_options->attack_mode == ATTACK_MODE_COMBI) || (((hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL) == 0) && (user_options->attack_mode == ATTACK_MODE_HYBRID2)))\n      {\n        hc_rewind (combs_fp);\n      }\n    }\n\n    // iteration type\n\n    u32 innerloop_step = 0;\n    u32 innerloop_cnt  = 0;\n\n    if (user_options->slow_candidates == true)\n    {\n      innerloop_step = 1;\n      innerloop_cnt  = 1;\n    }\n    else\n    {\n      if   (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL) innerloop_step = device_param->kernel_loops;\n      else                                                        innerloop_step = 1;\n\n      if      (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)  innerloop_cnt = straight_ctx->kernel_rules_cnt;\n      else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)     innerloop_cnt = (u32) combinator_ctx->combs_cnt;\n      else if (user_options_extra->attack_kern == ATTACK_KERN_BF)        innerloop_cnt = (u32) mask_ctx->bfs_cnt;\n    }\n\n    // innerloops\n\n    for (u32 innerloop_pos = 0; innerloop_pos < innerloop_cnt; innerloop_pos += innerloop_step)\n    {\n      while (status_ctx->devices_status == STATUS_PAUSED) sleep (1);\n\n      u32 fast_iteration = 0;\n\n      u32 innerloop_left = innerloop_cnt - innerloop_pos;\n\n      if (innerloop_left > innerloop_step)\n      {\n        innerloop_left = innerloop_step;\n\n        fast_iteration = 1;\n      }\n\n      hc_thread_mutex_lock (status_ctx->mux_display);\n\n      device_param->innerloop_pos  = innerloop_pos;\n      device_param->innerloop_left = innerloop_left;\n\n      device_param->kernel_params_buf32[30] = innerloop_left;\n\n      device_param->outerloop_multi = (double) innerloop_cnt / (double) (innerloop_pos + innerloop_left);\n\n      hc_thread_mutex_unlock (status_ctx->mux_display);\n\n      if (hashes->salts_shown[salt_pos] == 1)\n      {\n        status_ctx->words_progress_done[salt_pos] += pws_cnt * innerloop_left;\n\n        continue;\n      }\n\n      // initialize and copy amplifiers\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n        {\n          if (device_param->is_cuda == true)\n          {\n            if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_rules_c, device_param->cuda_d_rules + (innerloop_pos * sizeof (kernel_rule_t)), innerloop_left * sizeof (kernel_rule_t)) == -1) return -1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_rules, device_param->opencl_d_rules_c, innerloop_pos * sizeof (kernel_rule_t), 0, innerloop_left * sizeof (kernel_rule_t), 0, NULL, NULL) == -1) return -1;\n          }\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n        {\n          if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n          {\n            if (user_options->attack_mode == ATTACK_MODE_COMBI)\n            {\n              char *line_buf = device_param->scratch_buf;\n\n              u32 i = 0;\n\n              while (i < innerloop_left)\n              {\n                if (hc_feof (combs_fp)) break;\n\n                size_t line_len = fgetl (combs_fp, line_buf, HCBUFSIZ_LARGE);\n\n                line_len = convert_from_hex (hashcat_ctx, line_buf, line_len);\n\n                if (line_len > PW_MAX) continue;\n\n                char *line_buf_new = line_buf;\n\n                char rule_buf_out[RP_PASSWORD_SIZE];\n\n                if (run_rule_engine (user_options_extra->rule_len_r, user_options->rule_buf_r))\n                {\n                  if (line_len >= RP_PASSWORD_SIZE) continue;\n\n                  memset (rule_buf_out, 0, sizeof (rule_buf_out));\n\n                  const int rule_len_out = _old_apply_rule (user_options->rule_buf_r, user_options_extra->rule_len_r, line_buf, (u32) line_len, rule_buf_out);\n\n                  if (rule_len_out < 0)\n                  {\n                    status_ctx->words_progress_rejected[salt_pos] += pws_cnt;\n\n                    continue;\n                  }\n\n                  line_len = rule_len_out;\n\n                  line_buf_new = rule_buf_out;\n                }\n\n                // do the on-the-fly encoding\n\n                if (iconv_enabled == true)\n                {\n                  char  *iconv_ptr = iconv_tmp;\n                  size_t iconv_sz  = HCBUFSIZ_TINY;\n\n                  if (iconv (iconv_ctx, &line_buf_new, &line_len, &iconv_ptr, &iconv_sz) == (size_t) -1) continue;\n\n                  line_buf_new = iconv_tmp;\n                  line_len     = HCBUFSIZ_TINY - iconv_sz;\n                }\n\n                line_len = MIN (line_len, PW_MAX);\n\n                u8 *ptr = (u8 *) device_param->combs_buf[i].i;\n\n                memcpy (ptr, line_buf_new, line_len);\n\n                memset (ptr + line_len, 0, PW_MAX - line_len);\n\n                if (hashconfig->opts_type & OPTS_TYPE_PT_UPPER)\n                {\n                  uppercase (ptr, line_len);\n                }\n\n                if (combinator_ctx->combs_mode == COMBINATOR_MODE_BASE_LEFT)\n                {\n                  if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)\n                  {\n                    ptr[line_len] = 0x80;\n                  }\n\n                  if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)\n                  {\n                    ptr[line_len] = 0x06;\n                  }\n\n                  if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)\n                  {\n                    ptr[line_len] = 0x01;\n                  }\n                }\n\n                device_param->combs_buf[i].pw_len = (u32) line_len;\n\n                i++;\n              }\n\n              for (u32 j = i; j < innerloop_left; j++)\n              {\n                memset (&device_param->combs_buf[j], 0, sizeof (pw_t));\n              }\n\n              innerloop_left = i;\n\n              if (device_param->is_cuda == true)\n              {\n                if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_combs_c, device_param->combs_buf, innerloop_left * sizeof (pw_t)) == -1) return -1;\n              }\n\n              if (device_param->is_opencl == true)\n              {\n                if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_combs_c, CL_TRUE, 0, innerloop_left * sizeof (pw_t), device_param->combs_buf, 0, NULL, NULL) == -1) return -1;\n              }\n            }\n            else if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n            {\n              u64 off = innerloop_pos;\n\n              device_param->kernel_params_mp_buf64[3] = off;\n\n              if (run_kernel_mp (hashcat_ctx, device_param, KERN_RUN_MP, innerloop_left) == -1) return -1;\n\n              if (device_param->is_cuda == true)\n              {\n                if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_combs_c, device_param->cuda_d_combs, innerloop_left * sizeof (pw_t)) == -1) return -1;\n              }\n\n              if (device_param->is_opencl == true)\n              {\n                if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_combs, device_param->opencl_d_combs_c, 0, 0, innerloop_left * sizeof (pw_t), 0, NULL, NULL) == -1) return -1;\n              }\n            }\n            else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n            {\n              u64 off = innerloop_pos;\n\n              device_param->kernel_params_mp_buf64[3] = off;\n\n              if (run_kernel_mp (hashcat_ctx, device_param, KERN_RUN_MP, innerloop_left) == -1) return -1;\n\n              if (device_param->is_cuda == true)\n              {\n                if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_combs_c, device_param->cuda_d_combs, innerloop_left * sizeof (pw_t)) == -1) return -1;\n              }\n\n              if (device_param->is_opencl == true)\n              {\n                if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_combs, device_param->opencl_d_combs_c, 0, 0, innerloop_left * sizeof (pw_t), 0, NULL, NULL) == -1) return -1;\n              }\n            }\n          }\n          else\n          {\n            if ((user_options->attack_mode == ATTACK_MODE_COMBI) || (user_options->attack_mode == ATTACK_MODE_HYBRID2))\n            {\n              char *line_buf = device_param->scratch_buf;\n\n              u32 i = 0;\n\n              while (i < innerloop_left)\n              {\n                if (hc_feof (combs_fp)) break;\n\n                size_t line_len = fgetl (combs_fp, line_buf, HCBUFSIZ_LARGE);\n\n                line_len = convert_from_hex (hashcat_ctx, line_buf, line_len);\n\n                if (line_len > PW_MAX) continue;\n\n                char *line_buf_new = line_buf;\n\n                char rule_buf_out[RP_PASSWORD_SIZE];\n\n                if (run_rule_engine (user_options_extra->rule_len_r, user_options->rule_buf_r))\n                {\n                  if (line_len >= RP_PASSWORD_SIZE) continue;\n\n                  memset (rule_buf_out, 0, sizeof (rule_buf_out));\n\n                  const int rule_len_out = _old_apply_rule (user_options->rule_buf_r, user_options_extra->rule_len_r, line_buf, (u32) line_len, rule_buf_out);\n\n                  if (rule_len_out < 0)\n                  {\n                    status_ctx->words_progress_rejected[salt_pos] += pws_cnt;\n\n                    continue;\n                  }\n\n                  line_len = rule_len_out;\n\n                  line_buf_new = rule_buf_out;\n                }\n\n                // do the on-the-fly encoding\n\n                if (iconv_enabled == true)\n                {\n                  char  *iconv_ptr = iconv_tmp;\n                  size_t iconv_sz  = HCBUFSIZ_TINY;\n\n                  if (iconv (iconv_ctx, &line_buf_new, &line_len, &iconv_ptr, &iconv_sz) == (size_t) -1) continue;\n\n                  line_buf_new = iconv_tmp;\n                  line_len     = HCBUFSIZ_TINY - iconv_sz;\n                }\n\n                line_len = MIN (line_len, PW_MAX);\n\n                u8 *ptr = (u8 *) device_param->combs_buf[i].i;\n\n                memcpy (ptr, line_buf_new, line_len);\n\n                memset (ptr + line_len, 0, PW_MAX - line_len);\n\n                if (hashconfig->opts_type & OPTS_TYPE_PT_UPPER)\n                {\n                  uppercase (ptr, line_len);\n                }\n\n                /*\n                if (combinator_ctx->combs_mode == COMBINATOR_MODE_BASE_LEFT)\n                {\n                  if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)\n                  {\n                    ptr[line_len] = 0x80;\n                  }\n\n                  if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)\n                  {\n                    ptr[line_len] = 0x06;\n                  }\n\n                  if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)\n                  {\n                    ptr[line_len] = 0x01;\n                  }\n                }\n                */\n\n                device_param->combs_buf[i].pw_len = (u32) line_len;\n\n                i++;\n              }\n\n              for (u32 j = i; j < innerloop_left; j++)\n              {\n                memset (&device_param->combs_buf[j], 0, sizeof (pw_t));\n              }\n\n              innerloop_left = i;\n\n              if (device_param->is_cuda == true)\n              {\n                if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_combs_c, device_param->combs_buf, innerloop_left * sizeof (pw_t)) == -1) return -1;\n              }\n\n              if (device_param->is_opencl == true)\n              {\n                if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_combs_c, CL_TRUE, 0, innerloop_left * sizeof (pw_t), device_param->combs_buf, 0, NULL, NULL) == -1) return -1;\n              }\n            }\n            else if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n            {\n              u64 off = innerloop_pos;\n\n              device_param->kernel_params_mp_buf64[3] = off;\n\n              if (run_kernel_mp (hashcat_ctx, device_param, KERN_RUN_MP, innerloop_left) == -1) return -1;\n\n              if (device_param->is_cuda == true)\n              {\n                if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_combs_c, device_param->cuda_d_combs, innerloop_left * sizeof (pw_t)) == -1) return -1;\n              }\n\n              if (device_param->is_opencl == true)\n              {\n                if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_combs, device_param->opencl_d_combs_c, 0, 0, innerloop_left * sizeof (pw_t), 0, NULL, NULL) == -1) return -1;\n              }\n            }\n          }\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n        {\n          u64 off = innerloop_pos;\n\n          device_param->kernel_params_mp_r_buf64[3] = off;\n\n          if (run_kernel_mp (hashcat_ctx, device_param, KERN_RUN_MP_R, innerloop_left) == -1) return -1;\n\n          if (device_param->is_cuda == true)\n          {\n            if (hc_cuMemcpyDtoD (hashcat_ctx, device_param->cuda_d_bfs_c, device_param->cuda_d_bfs, innerloop_left * sizeof (bf_t)) == -1) return -1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clEnqueueCopyBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bfs, device_param->opencl_d_bfs_c, 0, 0, innerloop_left * sizeof (bf_t), 0, NULL, NULL) == -1) return -1;\n          }\n        }\n      }\n\n      if (choose_kernel (hashcat_ctx, device_param, highest_pw_len, pws_cnt, fast_iteration, salt_pos) == -1) return -1;\n\n      /**\n       * benchmark was aborted because too long kernel runtime (slow hashes only)\n       */\n\n      if ((user_options->speed_only == true) && (device_param->speed_only_finish == true))\n      {\n        // nothing to do in that case\n      }\n      else\n      {\n        /**\n         * speed\n         */\n\n        if (status_ctx->run_thread_level2 == true)\n        {\n          const u64 perf_sum_all = pws_cnt * innerloop_left;\n\n          const double speed_msec = hc_timer_get (device_param->timer_speed);\n\n          hc_timer_set (&device_param->timer_speed);\n\n          u32 speed_pos = device_param->speed_pos;\n\n          device_param->speed_cnt[speed_pos] = perf_sum_all;\n\n          device_param->speed_msec[speed_pos] = speed_msec;\n\n          speed_pos++;\n\n          if (speed_pos == SPEED_CACHE)\n          {\n            speed_pos = 0;\n          }\n\n          device_param->speed_pos = speed_pos;\n\n          /**\n           * progress\n           */\n\n          hc_thread_mutex_lock (status_ctx->mux_counter);\n\n          status_ctx->words_progress_done[salt_pos] += perf_sum_all;\n\n          hc_thread_mutex_unlock (status_ctx->mux_counter);\n        }\n      }\n\n      /**\n       * benchmark, part2\n       */\n\n      if (user_options->speed_only == true)\n      {\n        // let's abort this so that the user doesn't have to wait too long on the result\n        // for slow hashes it's fine anyway as boost mode should be turned on\n\n        if (hashconfig->attack_exec == ATTACK_EXEC_OUTSIDE_KERNEL)\n        {\n          device_param->speed_only_finish = true;\n\n          break;\n        }\n\n        double total_msec = device_param->speed_msec[0];\n\n        for (u32 speed_pos = 1; speed_pos < device_param->speed_pos; speed_pos++)\n        {\n          total_msec += device_param->speed_msec[speed_pos];\n        }\n\n        if (user_options->slow_candidates == true)\n        {\n          if ((total_msec > 4000) || (device_param->speed_pos == SPEED_CACHE - 1))\n          {\n            const u32 speed_pos = device_param->speed_pos;\n\n            if (speed_pos)\n            {\n              device_param->speed_cnt[0]  = device_param->speed_cnt[speed_pos - 1];\n              device_param->speed_msec[0] = device_param->speed_msec[speed_pos - 1];\n            }\n\n            device_param->speed_pos = 0;\n\n            device_param->speed_only_finish = true;\n\n            break;\n          }\n        }\n        else\n        {\n          // it's unclear if 4s is enough to turn on boost mode for all backend device\n\n          if ((total_msec > 4000) || (device_param->speed_pos == SPEED_CACHE - 1))\n          {\n            device_param->speed_only_finish = true;\n\n            break;\n          }\n        }\n      }\n\n      if (device_param->speed_only_finish == true) break;\n\n      /**\n       * result\n       */\n\n      check_cracked (hashcat_ctx, device_param, salt_pos);\n\n      if (status_ctx->run_thread_level2 == false) break;\n    }\n\n    if (user_options->speed_only == true) break;\n\n    //status screen makes use of this, can't reset here\n    //device_param->innerloop_msec = 0;\n    //device_param->innerloop_pos  = 0;\n    //device_param->innerloop_left = 0;\n\n    if (status_ctx->run_thread_level2 == false) break;\n  }\n\n  //status screen makes use of this, can't reset here\n  //device_param->outerloop_msec = 0;\n  //device_param->outerloop_pos  = 0;\n  //device_param->outerloop_left = 0;\n\n  if (user_options->speed_only == true)\n  {\n    double total_msec = device_param->speed_msec[0];\n\n    for (u32 speed_pos = 1; speed_pos < device_param->speed_pos; speed_pos++)\n    {\n      total_msec += device_param->speed_msec[speed_pos];\n    }\n\n    device_param->outerloop_msec = total_msec * hashes->salts_cnt * device_param->outerloop_multi;\n\n    device_param->speed_only_finish = true;\n  }\n\n  return 0;\n}\n\nint backend_ctx_init (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t  *backend_ctx  = hashcat_ctx->backend_ctx;\n  user_options_t *user_options = hashcat_ctx->user_options;\n\n  backend_ctx->enabled = false;\n\n  if (user_options->example_hashes == true) return 0;\n  if (user_options->keyspace       == true) return 0;\n  if (user_options->left           == true) return 0;\n  if (user_options->show           == true) return 0;\n  if (user_options->usage          == true) return 0;\n  if (user_options->version        == true) return 0;\n\n  hc_device_param_t *devices_param = (hc_device_param_t *) hccalloc (DEVICES_MAX, sizeof (hc_device_param_t));\n\n  backend_ctx->devices_param = devices_param;\n\n  /**\n   * Load and map CUDA library calls, then init CUDA\n   */\n\n  int rc_cuda_init = -1;\n\n  if (user_options->backend_ignore_cuda == false)\n  {\n    CUDA_PTR *cuda = (CUDA_PTR *) hcmalloc (sizeof (CUDA_PTR));\n\n    backend_ctx->cuda = cuda;\n\n    rc_cuda_init = cuda_init (hashcat_ctx);\n\n    if (rc_cuda_init == -1)\n    {\n      cuda_close (hashcat_ctx);\n    }\n\n    /**\n     * Load and map NVRTC library calls\n     */\n\n    NVRTC_PTR *nvrtc = (NVRTC_PTR *) hcmalloc (sizeof (NVRTC_PTR));\n\n    backend_ctx->nvrtc = nvrtc;\n\n    int rc_nvrtc_init = nvrtc_init (hashcat_ctx);\n\n    if (rc_nvrtc_init == -1)\n    {\n      nvrtc_close (hashcat_ctx);\n    }\n\n    /**\n     * Check if both CUDA and NVRTC were load successful\n     */\n\n    if ((rc_cuda_init == 0) && (rc_nvrtc_init == 0))\n    {\n      // nvrtc version\n\n      int nvrtc_major = 0;\n      int nvrtc_minor = 0;\n\n      if (hc_nvrtcVersion (hashcat_ctx, &nvrtc_major, &nvrtc_minor) == -1) return -1;\n\n      int nvrtc_driver_version = (nvrtc_major * 1000) + (nvrtc_minor * 10);\n\n      backend_ctx->nvrtc_driver_version = nvrtc_driver_version;\n\n      if (nvrtc_driver_version < 9000)\n      {\n        event_log_error (hashcat_ctx, \"Outdated NVIDIA NVRTC driver version '%d' detected!\", nvrtc_driver_version);\n\n        event_log_warning (hashcat_ctx, \"See hashcat.net for officially supported NVIDIA CUDA Toolkit versions.\");\n        event_log_warning (hashcat_ctx, NULL);\n\n        return -1;\n      }\n\n      // cuda version\n\n      int cuda_driver_version = 0;\n\n      if (hc_cuDriverGetVersion (hashcat_ctx, &cuda_driver_version) == -1) return -1;\n\n      backend_ctx->cuda_driver_version = cuda_driver_version;\n\n      if (cuda_driver_version < 9000)\n      {\n        event_log_error (hashcat_ctx, \"Outdated NVIDIA CUDA driver version '%d' detected!\", cuda_driver_version);\n\n        event_log_warning (hashcat_ctx, \"See hashcat.net for officially supported NVIDIA CUDA Toolkit versions.\");\n        event_log_warning (hashcat_ctx, NULL);\n\n        return -1;\n      }\n    }\n    else\n    {\n      rc_cuda_init  = -1;\n      rc_nvrtc_init = -1;\n\n      cuda_close  (hashcat_ctx);\n      nvrtc_close (hashcat_ctx);\n    }\n  }\n\n  /**\n   * Load and map OpenCL library calls\n   */\n\n  int rc_ocl_init = -1;\n\n  if (user_options->backend_ignore_opencl == false)\n  {\n    OCL_PTR *ocl = (OCL_PTR *) hcmalloc (sizeof (OCL_PTR));\n\n    backend_ctx->ocl = ocl;\n\n    rc_ocl_init = ocl_init (hashcat_ctx);\n\n    if (rc_ocl_init == -1)\n    {\n      ocl_close (hashcat_ctx);\n    }\n\n    /**\n     * return if both CUDA and OpenCL initialization failed\n     */\n\n    if ((rc_cuda_init == -1) && (rc_ocl_init == -1))\n    {\n      event_log_error (hashcat_ctx, \"ATTENTION! No OpenCL or CUDA installation found.\");\n\n      event_log_warning (hashcat_ctx, \"You are probably missing the CUDA or OpenCL runtime installation.\");\n      event_log_warning (hashcat_ctx, NULL);\n\n      #if defined (__linux__)\n      event_log_warning (hashcat_ctx, \"* AMD GPUs on Linux require this driver:\");\n      event_log_warning (hashcat_ctx, \"  \\\"RadeonOpenCompute (ROCm)\\\" Software Platform (3.1 or later)\");\n      #elif defined (_WIN)\n      event_log_warning (hashcat_ctx, \"* AMD GPUs on Windows require this driver:\");\n      event_log_warning (hashcat_ctx, \"  \\\"AMD Radeon Adrenalin 2020 Edition\\\" (20.2.2 or later)\");\n      #endif\n\n      event_log_warning (hashcat_ctx, \"* Intel CPUs require this runtime:\");\n      event_log_warning (hashcat_ctx, \"  \\\"OpenCL Runtime for Intel Core and Intel Xeon Processors\\\" (16.1.1 or later)\");\n\n      event_log_warning (hashcat_ctx, \"* NVIDIA GPUs require this runtime and/or driver (both):\");\n      event_log_warning (hashcat_ctx, \"  \\\"NVIDIA Driver\\\" (440.64 or later)\");\n      event_log_warning (hashcat_ctx, \"  \\\"CUDA Toolkit\\\" (9.0 or later)\");\n      event_log_warning (hashcat_ctx, NULL);\n\n      return -1;\n    }\n\n    /**\n     * Some permission pre-check, because AMDGPU-PRO Driver crashes if the user has no permission to do this\n     */\n\n    if (ocl_check_dri (hashcat_ctx) == -1) return -1;\n  }\n\n  /**\n   * Backend device selection\n   */\n\n  u64 backend_devices_filter;\n\n  if (setup_backend_devices_filter (hashcat_ctx, user_options->backend_devices, &backend_devices_filter) == false) return -1;\n\n  backend_ctx->backend_devices_filter = backend_devices_filter;\n\n  /**\n   * OpenCL device type selection\n   */\n\n  cl_device_type opencl_device_types_filter;\n\n  if (setup_opencl_device_types_filter (hashcat_ctx, user_options->opencl_device_types, &opencl_device_types_filter) == false) return -1;\n\n  backend_ctx->opencl_device_types_filter = opencl_device_types_filter;\n\n  /**\n   * CUDA API: init\n   */\n\n  if (backend_ctx->cuda)\n  {\n    if (hc_cuInit (hashcat_ctx, 0) == -1)\n    {\n      cuda_close (hashcat_ctx);\n    }\n  }\n\n  /**\n   * OpenCL API: init\n   */\n\n  if (backend_ctx->ocl)\n  {\n    #define FREE_OPENCL_CTX_ON_ERROR          \\\n    do {                                      \\\n      hcfree (opencl_platforms);              \\\n      hcfree (opencl_platforms_devices);      \\\n      hcfree (opencl_platforms_devices_cnt);  \\\n      hcfree (opencl_platforms_name);         \\\n      hcfree (opencl_platforms_vendor);       \\\n      hcfree (opencl_platforms_vendor_id);    \\\n      hcfree (opencl_platforms_version);      \\\n    } while (0)\n\n    cl_platform_id *opencl_platforms             = (cl_platform_id *) hccalloc (CL_PLATFORMS_MAX, sizeof (cl_platform_id));\n    cl_uint         opencl_platforms_cnt         = 0;\n    cl_device_id  **opencl_platforms_devices     = (cl_device_id **)  hccalloc (CL_PLATFORMS_MAX, sizeof (cl_device_id *));\n    cl_uint        *opencl_platforms_devices_cnt = (cl_uint *)        hccalloc (CL_PLATFORMS_MAX, sizeof (cl_uint));\n    char          **opencl_platforms_name        = (char **)          hccalloc (CL_PLATFORMS_MAX, sizeof (char *));\n    char          **opencl_platforms_vendor      = (char **)          hccalloc (CL_PLATFORMS_MAX, sizeof (char *));\n    cl_uint        *opencl_platforms_vendor_id   = (cl_uint *)        hccalloc (CL_PLATFORMS_MAX, sizeof (cl_uint));\n    char          **opencl_platforms_version     = (char **)          hccalloc (CL_PLATFORMS_MAX, sizeof (char *));\n\n    if (hc_clGetPlatformIDs (hashcat_ctx, CL_PLATFORMS_MAX, opencl_platforms, &opencl_platforms_cnt) == -1)\n    {\n      opencl_platforms_cnt = 0;\n\n      FREE_OPENCL_CTX_ON_ERROR;\n\n      ocl_close (hashcat_ctx);\n    }\n\n    if (opencl_platforms_cnt)\n    {\n      for (u32 opencl_platforms_idx = 0; opencl_platforms_idx < opencl_platforms_cnt; opencl_platforms_idx++)\n      {\n        cl_platform_id opencl_platform = opencl_platforms[opencl_platforms_idx];\n\n        size_t param_value_size = 0;\n\n        // platform vendor\n\n        if (hc_clGetPlatformInfo (hashcat_ctx, opencl_platform, CL_PLATFORM_VENDOR, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_platform_vendor = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetPlatformInfo (hashcat_ctx, opencl_platform, CL_PLATFORM_VENDOR, param_value_size, opencl_platform_vendor, NULL) == -1) return -1;\n\n        opencl_platforms_vendor[opencl_platforms_idx] = opencl_platform_vendor;\n\n        // platform name\n\n        if (hc_clGetPlatformInfo (hashcat_ctx, opencl_platform, CL_PLATFORM_NAME, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_platform_name = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetPlatformInfo (hashcat_ctx, opencl_platform, CL_PLATFORM_NAME, param_value_size, opencl_platform_name, NULL) == -1) return -1;\n\n        opencl_platforms_name[opencl_platforms_idx] = opencl_platform_name;\n\n        // platform version\n\n        if (hc_clGetPlatformInfo (hashcat_ctx, opencl_platform, CL_PLATFORM_VERSION, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_platform_version = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetPlatformInfo (hashcat_ctx, opencl_platform, CL_PLATFORM_VERSION, param_value_size, opencl_platform_version, NULL) == -1) return -1;\n\n        opencl_platforms_version[opencl_platforms_idx] = opencl_platform_version;\n\n        // find our own platform vendor because pocl and mesa are pushing original vendor_id through opencl\n        // this causes trouble with vendor id based macros\n        // we'll assign generic to those without special optimization available\n\n        cl_uint opencl_platform_vendor_id = 0;\n\n        if (strcmp (opencl_platform_vendor, CL_VENDOR_AMD1) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_AMD;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_AMD2) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_AMD;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_AMD_USE_INTEL) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_AMD_USE_INTEL;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_APPLE) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_APPLE;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_INTEL_BEIGNET) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_INTEL_BEIGNET;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_INTEL_SDK) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_INTEL_SDK;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_MESA) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_MESA;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_NV) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_NV;\n        }\n        else if (strcmp (opencl_platform_vendor, CL_VENDOR_POCL) == 0)\n        {\n          opencl_platform_vendor_id = VENDOR_ID_POCL;\n        }\n        else\n        {\n          opencl_platform_vendor_id = VENDOR_ID_GENERIC;\n        }\n\n        opencl_platforms_vendor_id[opencl_platforms_idx] = opencl_platform_vendor_id;\n\n        cl_device_id *opencl_platform_devices = (cl_device_id *) hccalloc (DEVICES_MAX, sizeof (cl_device_id));\n\n        cl_uint opencl_platform_devices_cnt = 0;\n\n        const int CL_rc = hc_clGetDeviceIDs (hashcat_ctx, opencl_platform, CL_DEVICE_TYPE_ALL, DEVICES_MAX, opencl_platform_devices, &opencl_platform_devices_cnt);\n\n        if (CL_rc == -1)\n        {\n          event_log_error (hashcat_ctx, \"clGetDeviceIDs(): %s\", val2cstr_cl (CL_rc));\n\n          // Special handling for CL_DEVICE_NOT_FOUND, see: https://github.com/hashcat/hashcat/issues/2455\n\n          #define IGNORE_DEVICE_NOT_FOUND 1\n\n          if (IGNORE_DEVICE_NOT_FOUND)\n          {\n            backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n            OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n            const cl_int CL_err = ocl->clGetDeviceIDs (opencl_platform, CL_DEVICE_TYPE_ALL, DEVICES_MAX, opencl_platform_devices, &opencl_platform_devices_cnt);\n\n            if (CL_err == CL_DEVICE_NOT_FOUND)\n            {\n              // we ignore this error\n            }\n            else\n            {\n              return -1;\n            }\n          }\n          else\n          {\n            return -1;\n          }\n        }\n\n        opencl_platforms_devices[opencl_platforms_idx] = opencl_platform_devices;\n\n        opencl_platforms_devices_cnt[opencl_platforms_idx] = opencl_platform_devices_cnt;\n      }\n\n      if (user_options->opencl_device_types == NULL)\n      {\n        /**\n         * OpenCL device types:\n         *   In case the user did not specify --opencl-device-types and the user runs hashcat in a system with only a CPU only he probably want to use that CPU.\n         */\n\n        cl_device_type opencl_device_types_all = 0;\n\n        for (u32 opencl_platforms_idx = 0; opencl_platforms_idx < opencl_platforms_cnt; opencl_platforms_idx++)\n        {\n          cl_device_id *opencl_platform_devices     = opencl_platforms_devices[opencl_platforms_idx];\n          cl_uint       opencl_platform_devices_cnt = opencl_platforms_devices_cnt[opencl_platforms_idx];\n\n          for (u32 opencl_platform_devices_idx = 0; opencl_platform_devices_idx < opencl_platform_devices_cnt; opencl_platform_devices_idx++)\n          {\n            cl_device_id opencl_device = opencl_platform_devices[opencl_platform_devices_idx];\n\n            cl_device_type opencl_device_type;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, opencl_device, CL_DEVICE_TYPE, sizeof (opencl_device_type), &opencl_device_type, NULL) == -1)\n            {\n              FREE_OPENCL_CTX_ON_ERROR;\n\n              return -1;\n            }\n\n            opencl_device_types_all |= opencl_device_type;\n          }\n        }\n\n        // In such a case, automatically enable CPU device type support, since it's disabled by default.\n\n        if ((opencl_device_types_all & (CL_DEVICE_TYPE_GPU | CL_DEVICE_TYPE_ACCELERATOR)) == 0)\n        {\n          opencl_device_types_filter |= CL_DEVICE_TYPE_CPU;\n        }\n\n        // In another case, when the user uses --stdout, using CPU devices is much faster to setup\n        // If we have a CPU device, force it to be used\n\n        if (user_options->stdout_flag == true)\n        {\n          if (opencl_device_types_all & CL_DEVICE_TYPE_CPU)\n          {\n            opencl_device_types_filter = CL_DEVICE_TYPE_CPU;\n          }\n        }\n\n        backend_ctx->opencl_device_types_filter = opencl_device_types_filter;\n      }\n    }\n\n    backend_ctx->opencl_platforms             = opencl_platforms;\n    backend_ctx->opencl_platforms_cnt         = opencl_platforms_cnt;\n    backend_ctx->opencl_platforms_devices     = opencl_platforms_devices;\n    backend_ctx->opencl_platforms_devices_cnt = opencl_platforms_devices_cnt;\n    backend_ctx->opencl_platforms_name        = opencl_platforms_name;\n    backend_ctx->opencl_platforms_vendor      = opencl_platforms_vendor;\n    backend_ctx->opencl_platforms_vendor_id   = opencl_platforms_vendor_id;\n    backend_ctx->opencl_platforms_version     = opencl_platforms_version;\n\n    #undef FREE_OPENCL_CTX_ON_ERROR\n  }\n\n  /**\n   * Final checks\n   */\n\n  if ((backend_ctx->cuda == NULL) && (backend_ctx->ocl == NULL))\n  {\n    event_log_error (hashcat_ctx, \"ATTENTION! No OpenCL-compatible or CUDA-compatible platform found.\");\n\n    event_log_warning (hashcat_ctx, \"You are probably missing the OpenCL or CUDA runtime installation.\");\n    event_log_warning (hashcat_ctx, NULL);\n\n    #if defined (__linux__)\n    event_log_warning (hashcat_ctx, \"* AMD GPUs on Linux require this driver:\");\n    event_log_warning (hashcat_ctx, \"  \\\"RadeonOpenCompute (ROCm)\\\" Software Platform (3.1 or later)\");\n    #elif defined (_WIN)\n    event_log_warning (hashcat_ctx, \"* AMD GPUs on Windows require this driver:\");\n    event_log_warning (hashcat_ctx, \"  \\\"AMD Radeon Adrenalin 2020 Edition\\\" (20.2.2 or later)\");\n    #endif\n\n    event_log_warning (hashcat_ctx, \"* Intel CPUs require this runtime:\");\n    event_log_warning (hashcat_ctx, \"  \\\"OpenCL Runtime for Intel Core and Intel Xeon Processors\\\" (16.1.1 or later)\");\n\n    event_log_warning (hashcat_ctx, \"* NVIDIA GPUs require this runtime and/or driver (both):\");\n    event_log_warning (hashcat_ctx, \"  \\\"NVIDIA Driver\\\" (440.64 or later)\");\n    event_log_warning (hashcat_ctx, \"  \\\"CUDA Toolkit\\\" (9.0 or later)\");\n    event_log_warning (hashcat_ctx, NULL);\n\n    return -1;\n  }\n\n  backend_ctx->enabled = true;\n\n  return 0;\n}\n\nvoid backend_ctx_destroy (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (backend_ctx->enabled == false) return;\n\n  hcfree (backend_ctx->devices_param);\n\n  if (backend_ctx->ocl)\n  {\n    hcfree (backend_ctx->opencl_platforms);\n    hcfree (backend_ctx->opencl_platforms_devices);\n    hcfree (backend_ctx->opencl_platforms_devices_cnt);\n    hcfree (backend_ctx->opencl_platforms_name);\n    hcfree (backend_ctx->opencl_platforms_vendor);\n    hcfree (backend_ctx->opencl_platforms_vendor_id);\n    hcfree (backend_ctx->opencl_platforms_version);\n  }\n\n  nvrtc_close (hashcat_ctx);\n  cuda_close  (hashcat_ctx);\n  ocl_close   (hashcat_ctx);\n\n  memset (backend_ctx, 0, sizeof (backend_ctx_t));\n}\n\nint backend_ctx_devices_init (hashcat_ctx_t *hashcat_ctx, const int comptime)\n{\n  backend_ctx_t  *backend_ctx  = hashcat_ctx->backend_ctx;\n  user_options_t *user_options = hashcat_ctx->user_options;\n\n  if (backend_ctx->enabled == false) return 0;\n\n  hc_device_param_t *devices_param = backend_ctx->devices_param;\n\n  bool need_adl     = false;\n  bool need_nvml    = false;\n  bool need_nvapi   = false;\n  bool need_sysfs   = false;\n\n  int backend_devices_idx = 0;\n\n  int cuda_devices_cnt    = 0;\n  int cuda_devices_active = 0;\n\n  if (backend_ctx->cuda)\n  {\n    // device count\n\n    if (hc_cuDeviceGetCount (hashcat_ctx, &cuda_devices_cnt) == -1)\n    {\n      cuda_close (hashcat_ctx);\n    }\n\n    backend_ctx->cuda_devices_cnt = cuda_devices_cnt;\n\n    // device specific\n\n    for (int cuda_devices_idx = 0; cuda_devices_idx < cuda_devices_cnt; cuda_devices_idx++, backend_devices_idx++)\n    {\n      const u32 device_id = backend_devices_idx;\n\n      hc_device_param_t *device_param = &devices_param[backend_devices_idx];\n\n      device_param->device_id = device_id;\n\n      backend_ctx->backend_device_from_cuda[cuda_devices_idx] = backend_devices_idx;\n\n      CUdevice cuda_device;\n\n      if (hc_cuDeviceGet (hashcat_ctx, &cuda_device, cuda_devices_idx) == -1) return -1;\n\n      device_param->cuda_device = cuda_device;\n\n      device_param->is_cuda = true;\n\n      device_param->is_opencl = false;\n\n      device_param->use_opencl12 = false;\n      device_param->use_opencl20 = false;\n      device_param->use_opencl21 = false;\n\n      // device_name\n\n      char *device_name = (char *) hcmalloc (HCBUFSIZ_TINY);\n\n      if (hc_cuDeviceGetName (hashcat_ctx, device_name, HCBUFSIZ_TINY, cuda_device) == -1) return -1;\n\n      device_param->device_name = device_name;\n\n      hc_string_trim_leading (device_name);\n\n      hc_string_trim_trailing (device_name);\n\n      // device_processors\n\n      int device_processors = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &device_processors, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, cuda_device) == -1) return -1;\n\n      device_param->device_processors = device_processors;\n\n      // device_global_mem, device_maxmem_alloc, device_available_mem\n\n      size_t bytes = 0;\n\n      if (hc_cuDeviceTotalMem (hashcat_ctx, &bytes, cuda_device) == -1) return -1;\n\n      device_param->device_global_mem = (u64) bytes;\n\n      device_param->device_maxmem_alloc = (u64) bytes;\n\n      device_param->device_available_mem = 0;\n\n      // warp size\n\n      int cuda_warp_size = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &cuda_warp_size, CU_DEVICE_ATTRIBUTE_WARP_SIZE, cuda_device) == -1) return -1;\n\n      device_param->cuda_warp_size = cuda_warp_size;\n\n      // sm_minor, sm_major\n\n      int sm_major = 0;\n      int sm_minor = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &sm_major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cuda_device) == -1) return -1;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &sm_minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cuda_device) == -1) return -1;\n\n      device_param->sm_major = sm_major;\n      device_param->sm_minor = sm_minor;\n\n      // device_maxworkgroup_size\n\n      int device_maxworkgroup_size = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &device_maxworkgroup_size, CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK, cuda_device) == -1) return -1;\n\n      device_param->device_maxworkgroup_size = device_maxworkgroup_size;\n\n      // max_clock_frequency\n\n      int device_maxclock_frequency = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &device_maxclock_frequency, CU_DEVICE_ATTRIBUTE_CLOCK_RATE, cuda_device) == -1) return -1;\n\n      device_param->device_maxclock_frequency = device_maxclock_frequency / 1000;\n\n      // pcie_bus, pcie_device, pcie_function\n\n      int pci_domain_id_nv  = 0;\n      int pci_bus_id_nv     = 0;\n      int pci_slot_id_nv    = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &pci_domain_id_nv, CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID, cuda_device) == -1) return -1;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &pci_bus_id_nv, CU_DEVICE_ATTRIBUTE_PCI_BUS_ID, cuda_device) == -1) return -1;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &pci_slot_id_nv, CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID, cuda_device) == -1) return -1;\n\n      device_param->pcie_domain   = (u8) (pci_domain_id_nv);\n      device_param->pcie_bus      = (u8) (pci_bus_id_nv);\n      device_param->pcie_device   = (u8) (pci_slot_id_nv >> 3);\n      device_param->pcie_function = (u8) (pci_slot_id_nv & 7);\n\n      // kernel_exec_timeout\n\n      int kernel_exec_timeout = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &kernel_exec_timeout, CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT, cuda_device) == -1) return -1;\n\n      device_param->kernel_exec_timeout = kernel_exec_timeout;\n\n      // max_shared_memory_per_block\n\n      int max_shared_memory_per_block = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &max_shared_memory_per_block, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK, cuda_device) == -1) return -1;\n\n      if (max_shared_memory_per_block < 32768)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: This device's shared buffer size is too small.\", device_id + 1);\n\n        device_param->skipped = true;\n      }\n\n      device_param->device_local_mem_size = max_shared_memory_per_block;\n\n      // device_max_constant_buffer_size\n\n      int device_max_constant_buffer_size = 0;\n\n      if (hc_cuDeviceGetAttribute (hashcat_ctx, &device_max_constant_buffer_size, CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY, cuda_device) == -1) return -1;\n\n      if (device_max_constant_buffer_size < 65536)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: This device's local mem size is too small.\", device_id + 1);\n\n        device_param->skipped = true;\n      }\n\n      // some attributes have to be hardcoded because they are used for instance in the build options\n\n      device_param->device_local_mem_type     = CL_LOCAL;\n      device_param->opencl_device_type        = CL_DEVICE_TYPE_GPU;\n      device_param->opencl_device_vendor_id   = VENDOR_ID_NV;\n      device_param->opencl_platform_vendor_id = VENDOR_ID_NV;\n\n      // or in the cached kernel checksum\n\n      device_param->opencl_device_version     = \"\";\n      device_param->opencl_driver_version     = \"\";\n\n      // or just to make sure they are not NULL\n\n      device_param->opencl_device_vendor     = \"\";\n      device_param->opencl_device_c_version  = \"\";\n\n      // skipped\n\n      if ((backend_ctx->backend_devices_filter & (1ULL << device_id)) == 0)\n      {\n        device_param->skipped = true;\n      }\n\n      if ((backend_ctx->opencl_device_types_filter & CL_DEVICE_TYPE_GPU) == 0)\n      {\n        device_param->skipped = true;\n      }\n\n      if ((device_param->opencl_platform_vendor_id == VENDOR_ID_NV) && (device_param->opencl_device_vendor_id == VENDOR_ID_NV))\n      {\n        need_nvml = true;\n\n        #if defined (_WIN) || defined (__CYGWIN__)\n        need_nvapi = true;\n        #endif\n      }\n\n      // CPU burning loop damper\n      // Value is given as number between 0-100\n      // By default 8%\n      // in theory not needed with CUDA\n\n      device_param->spin_damp = (double) user_options->spin_damp / 100;\n\n      // common driver check\n\n      if (device_param->skipped == false)\n      {\n        if ((user_options->force == false) && (user_options->backend_info == false))\n        {\n          // CUDA does not support query nvidia driver version, therefore no driver checks here\n          // IF needed, could be retrieved using nvmlSystemGetDriverVersion()\n\n          if (device_param->sm_major < 5)\n          {\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: This hardware has outdated CUDA compute capability (%u.%u).\", device_id + 1, device_param->sm_major, device_param->sm_minor);\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             For modern OpenCL performance, upgrade to hardware that supports\");\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             CUDA compute capability version 5.0 (Maxwell) or higher.\");\n          }\n\n          if (device_param->kernel_exec_timeout != 0)\n          {\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: WARNING! Kernel exec timeout is not disabled.\", device_id + 1);\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             This may cause \\\"CL_OUT_OF_RESOURCES\\\" or related errors.\");\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             To disable the timeout, see: https://hashcat.net/q/timeoutpatch\");\n          }\n        }\n\n        /**\n         * activate device\n         */\n\n        cuda_devices_active++;\n      }\n\n      CUcontext cuda_context;\n\n      if (hc_cuCtxCreate (hashcat_ctx, &cuda_context, CU_CTX_SCHED_BLOCKING_SYNC, device_param->cuda_device) == -1) return -1;\n\n      if (hc_cuCtxSetCurrent (hashcat_ctx, cuda_context) == -1) return -1;\n\n      // bcrypt optimization?\n      //const int rc_cuCtxSetCacheConfig = hc_cuCtxSetCacheConfig (hashcat_ctx, CU_FUNC_CACHE_PREFER_SHARED);\n      //\n      //if (rc_cuCtxSetCacheConfig == -1) return -1;\n\n      const int sm = (device_param->sm_major * 10) + device_param->sm_minor;\n\n      device_param->has_add   = (sm >= 12) ? true : false;\n      device_param->has_addc  = (sm >= 12) ? true : false;\n      device_param->has_sub   = (sm >= 12) ? true : false;\n      device_param->has_subc  = (sm >= 12) ? true : false;\n      device_param->has_bfe   = (sm >= 20) ? true : false;\n      device_param->has_lop3  = (sm >= 50) ? true : false;\n      device_param->has_mov64 = (sm >= 10) ? true : false;\n      device_param->has_prmt  = (sm >= 20) ? true : false;\n\n      /*\n      #define RUN_INSTRUCTION_CHECKS()                                                                                                                                                                                                                      \\\n        device_param->has_add   = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"add.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                              \\\n        device_param->has_addc  = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"addc.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                             \\\n        device_param->has_sub   = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"sub.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                              \\\n        device_param->has_subc  = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"subc.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                             \\\n        device_param->has_bfe   = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"bfe.u32 %0, 0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                              \\\n        device_param->has_lop3  = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"lop3.b32 %0, 0, 0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                          \\\n        device_param->has_mov64 = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned long long r; unsigned int a; unsigned int b; asm volatile (\\\"mov.b64 %0, {%1, %2};\\\" : \\\"=l\\\"(r) : \\\"r\\\"(a), \\\"r\\\"(b)); }\");  \\\n        device_param->has_prmt  = cuda_test_instruction (hashcat_ctx, sm_major, sm_minor, \"__global__ void test () { unsigned int r; asm volatile (\\\"prmt.b32 %0, 0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                                             \\\n\n      if (backend_devices_idx > 0)\n      {\n        hc_device_param_t *device_param_prev = &devices_param[backend_devices_idx - 1];\n\n        if (is_same_device_type (device_param, device_param_prev) == true)\n        {\n          device_param->has_add   = device_param_prev->has_add;\n          device_param->has_addc  = device_param_prev->has_addc;\n          device_param->has_sub   = device_param_prev->has_sub;\n          device_param->has_subc  = device_param_prev->has_subc;\n          device_param->has_bfe   = device_param_prev->has_bfe;\n          device_param->has_lop3  = device_param_prev->has_lop3;\n          device_param->has_mov64 = device_param_prev->has_mov64;\n          device_param->has_prmt  = device_param_prev->has_prmt;\n        }\n        else\n        {\n          RUN_INSTRUCTION_CHECKS();\n        }\n      }\n      else\n      {\n        RUN_INSTRUCTION_CHECKS();\n      }\n\n      #undef RUN_INSTRUCTION_CHECKS\n      */\n\n      // device_available_mem\n\n      size_t free  = 0;\n      size_t total = 0;\n\n      if (hc_cuMemGetInfo (hashcat_ctx, &free, &total) == -1) return -1;\n\n      device_param->device_available_mem = (u64) free;\n\n      if (hc_cuCtxDestroy (hashcat_ctx, cuda_context) == -1) return -1;\n    }\n  }\n\n  backend_ctx->cuda_devices_cnt     = cuda_devices_cnt;\n  backend_ctx->cuda_devices_active  = cuda_devices_active;\n\n  int opencl_devices_cnt    = 0;\n  int opencl_devices_active = 0;\n\n  if (backend_ctx->ocl)\n  {\n    /**\n     * OpenCL devices: simply push all devices from all platforms into the same device array\n     */\n\n    cl_uint         opencl_platforms_cnt         = backend_ctx->opencl_platforms_cnt;\n    cl_device_id  **opencl_platforms_devices     = backend_ctx->opencl_platforms_devices;\n    cl_uint        *opencl_platforms_devices_cnt = backend_ctx->opencl_platforms_devices_cnt;\n    cl_uint        *opencl_platforms_vendor_id   = backend_ctx->opencl_platforms_vendor_id;\n    char          **opencl_platforms_version     = backend_ctx->opencl_platforms_version;\n\n    for (u32 opencl_platforms_idx = 0; opencl_platforms_idx < opencl_platforms_cnt; opencl_platforms_idx++)\n    {\n      cl_device_id   *opencl_platform_devices     = opencl_platforms_devices[opencl_platforms_idx];\n      cl_uint         opencl_platform_devices_cnt = opencl_platforms_devices_cnt[opencl_platforms_idx];\n      cl_uint         opencl_platform_vendor_id   = opencl_platforms_vendor_id[opencl_platforms_idx];\n      char           *opencl_platform_version     = opencl_platforms_version[opencl_platforms_idx];\n\n      for (u32 opencl_platform_devices_idx = 0; opencl_platform_devices_idx < opencl_platform_devices_cnt; opencl_platform_devices_idx++, backend_devices_idx++, opencl_devices_cnt++)\n      {\n        const u32 device_id = backend_devices_idx;\n\n        hc_device_param_t *device_param = &devices_param[device_id];\n\n        device_param->device_id = device_id;\n\n        backend_ctx->backend_device_from_opencl[opencl_devices_cnt] = backend_devices_idx;\n\n        backend_ctx->backend_device_from_opencl_platform[opencl_platforms_idx][opencl_platform_devices_idx] = backend_devices_idx;\n\n        device_param->opencl_platform_vendor_id = opencl_platform_vendor_id;\n\n        device_param->opencl_device = opencl_platform_devices[opencl_platform_devices_idx];\n\n        //device_param->opencl_platform = opencl_platform;\n\n        device_param->is_cuda = false;\n\n        device_param->is_opencl = true;\n\n        // store opencl platform i\n\n        device_param->opencl_platform_id = opencl_platforms_idx;\n\n        // check OpenCL version\n\n        device_param->use_opencl12 = false;\n        device_param->use_opencl20 = false;\n        device_param->use_opencl21 = false;\n\n        int opencl_version_min = 0;\n        int opencl_version_maj = 0;\n\n        if (sscanf (opencl_platform_version, \"OpenCL %d.%d\", &opencl_version_min, &opencl_version_maj) == 2)\n        {\n          if ((opencl_version_min == 1) && (opencl_version_maj == 2))\n          {\n            device_param->use_opencl12 = true;\n          }\n          else if ((opencl_version_min == 2) && (opencl_version_maj == 0))\n          {\n            device_param->use_opencl20 = true;\n          }\n          else if ((opencl_version_min == 2) && (opencl_version_maj == 1))\n          {\n            device_param->use_opencl21 = true;\n          }\n        }\n\n        size_t param_value_size = 0;\n\n        // opencl_device_type\n\n        cl_device_type opencl_device_type;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_TYPE, sizeof (opencl_device_type), &opencl_device_type, NULL) == -1) return -1;\n\n        opencl_device_type &= ~CL_DEVICE_TYPE_DEFAULT;\n\n        device_param->opencl_device_type = opencl_device_type;\n\n        // device_name\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_NAME, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *device_name = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_NAME, param_value_size, device_name, NULL) == -1) return -1;\n\n        device_param->device_name = device_name;\n\n        hc_string_trim_leading (device_param->device_name);\n\n        hc_string_trim_trailing (device_param->device_name);\n\n        // device_vendor\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_VENDOR, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_device_vendor = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_VENDOR, param_value_size, opencl_device_vendor, NULL) == -1) return -1;\n\n        device_param->opencl_device_vendor = opencl_device_vendor;\n\n        cl_uint opencl_device_vendor_id = 0;\n\n        if (strcmp (opencl_device_vendor, CL_VENDOR_AMD1) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_AMD;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_AMD2) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_AMD;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_AMD_USE_INTEL) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_AMD_USE_INTEL;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_APPLE) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_APPLE;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_APPLE_USE_AMD) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_AMD;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_APPLE_USE_NV) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_NV;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_APPLE_USE_INTEL) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_INTEL_SDK;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_INTEL_BEIGNET) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_INTEL_BEIGNET;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_INTEL_SDK) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_INTEL_SDK;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_MESA) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_MESA;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_NV) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_NV;\n        }\n        else if (strcmp (opencl_device_vendor, CL_VENDOR_POCL) == 0)\n        {\n          opencl_device_vendor_id = VENDOR_ID_POCL;\n        }\n        else\n        {\n          opencl_device_vendor_id = VENDOR_ID_GENERIC;\n        }\n\n        device_param->opencl_device_vendor_id = opencl_device_vendor_id;\n\n        // device_version\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_VERSION, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_device_version = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_VERSION, param_value_size, opencl_device_version, NULL) == -1) return -1;\n\n        device_param->opencl_device_version = opencl_device_version;\n\n        // opencl_device_c_version\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_OPENCL_C_VERSION, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_device_c_version = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_OPENCL_C_VERSION, param_value_size, opencl_device_c_version, NULL) == -1) return -1;\n\n        device_param->opencl_device_c_version = opencl_device_c_version;\n\n        // max_compute_units\n\n        cl_uint device_processors = 0;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_MAX_COMPUTE_UNITS, sizeof (device_processors), &device_processors, NULL) == -1) return -1;\n\n        device_param->device_processors = device_processors;\n\n        // device_global_mem\n\n        cl_ulong device_global_mem = 0;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof (device_global_mem), &device_global_mem, NULL) == -1) return -1;\n\n        device_param->device_global_mem = device_global_mem;\n\n        device_param->device_available_mem = 0;\n\n        // device_maxmem_alloc\n\n        cl_ulong device_maxmem_alloc = 0;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof (device_maxmem_alloc), &device_maxmem_alloc, NULL) == -1) return -1;\n\n        device_param->device_maxmem_alloc = device_maxmem_alloc;\n\n        // note we'll limit to 2gb, otherwise this causes all kinds of weird errors because of possible integer overflows in opencl runtimes\n        // testwise disabling that\n        //device_param->device_maxmem_alloc = MIN (device_maxmem_alloc, 0x7fffffff);\n\n        // max_work_group_size\n\n        size_t device_maxworkgroup_size = 0;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof (device_maxworkgroup_size), &device_maxworkgroup_size, NULL) == -1) return -1;\n\n        device_param->device_maxworkgroup_size = device_maxworkgroup_size;\n\n        // max_clock_frequency\n\n        cl_uint device_maxclock_frequency = 0;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_MAX_CLOCK_FREQUENCY, sizeof (device_maxclock_frequency), &device_maxclock_frequency, NULL) == -1) return -1;\n\n        device_param->device_maxclock_frequency = device_maxclock_frequency;\n\n        // device_endian_little\n\n        cl_bool device_endian_little = CL_FALSE;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_ENDIAN_LITTLE, sizeof (device_endian_little), &device_endian_little, NULL) == -1) return -1;\n\n        if (device_endian_little == CL_FALSE)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: This device is not little-endian.\", device_id + 1);\n\n          device_param->skipped = true;\n        }\n\n        // device_available\n\n        cl_bool device_available = CL_FALSE;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_AVAILABLE, sizeof (device_available), &device_available, NULL) == -1) return -1;\n\n        if (device_available == CL_FALSE)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: This device is not available.\", device_id + 1);\n\n          device_param->skipped = true;\n        }\n\n        // device_compiler_available\n\n        cl_bool device_compiler_available = CL_FALSE;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_COMPILER_AVAILABLE, sizeof (device_compiler_available), &device_compiler_available, NULL) == -1) return -1;\n\n        if (device_compiler_available == CL_FALSE)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: No compiler is available for this device.\", device_id + 1);\n\n          device_param->skipped = true;\n        }\n\n        // device_execution_capabilities\n\n        cl_device_exec_capabilities device_execution_capabilities;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_EXECUTION_CAPABILITIES, sizeof (device_execution_capabilities), &device_execution_capabilities, NULL) == -1) return -1;\n\n        if ((device_execution_capabilities & CL_EXEC_KERNEL) == 0)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: This device does not support executing kernels.\", device_id + 1);\n\n          device_param->skipped = true;\n        }\n\n        // device_extensions\n\n        size_t device_extensions_size;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_EXTENSIONS, 0, NULL, &device_extensions_size) == -1) return -1;\n\n        char *device_extensions = (char *) hcmalloc (device_extensions_size + 1);\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_EXTENSIONS, device_extensions_size, device_extensions, NULL) == -1) return -1;\n\n        if (strstr (device_extensions, \"base_atomics\") == 0)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: This device does not support base atomics.\", device_id + 1);\n\n          device_param->skipped = true;\n        }\n\n        if (strstr (device_extensions, \"byte_addressable_store\") == 0)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: This device does not support byte-addressable store.\", device_id + 1);\n\n          device_param->skipped = true;\n        }\n\n        hcfree (device_extensions);\n\n        // device_local_mem_type\n\n        cl_device_local_mem_type device_local_mem_type;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_LOCAL_MEM_TYPE, sizeof (device_local_mem_type), &device_local_mem_type, NULL) == -1) return -1;\n\n        device_param->device_local_mem_type = device_local_mem_type;\n\n        // device_max_constant_buffer_size\n\n        cl_ulong device_max_constant_buffer_size;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE, sizeof (device_max_constant_buffer_size), &device_max_constant_buffer_size, NULL) == -1) return -1;\n\n        if (device_local_mem_type == CL_LOCAL)\n        {\n          if (device_max_constant_buffer_size < 65536)\n          {\n            event_log_error (hashcat_ctx, \"* Device #%u: This device's constant buffer size is too small.\", device_id + 1);\n\n            device_param->skipped = true;\n          }\n        }\n\n        // device_local_mem_size\n\n        cl_ulong device_local_mem_size = 0;\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_LOCAL_MEM_SIZE, sizeof (device_local_mem_size), &device_local_mem_size, NULL) == -1) return -1;\n\n        if (device_local_mem_type == CL_LOCAL)\n        {\n          if (device_local_mem_size < 32768)\n          {\n            event_log_error (hashcat_ctx, \"* Device #%u: This device's local mem size is too small.\", device_id + 1);\n\n            device_param->skipped = true;\n          }\n        }\n\n        device_param->device_local_mem_size = device_local_mem_size;\n\n        // older POCL version and older LLVM versions are known to fail compiling kernels\n        // we need to inform the user to update\n        // https://github.com/hashcat/hashcat/issues/2344\n\n        if (opencl_platform_vendor_id == VENDOR_ID_POCL)\n        {\n          char *pocl_version_ptr = strstr (opencl_platform_version, \"pocl \");\n          char *llvm_version_ptr = strstr (opencl_platform_version, \"LLVM \");\n\n          if ((pocl_version_ptr != NULL) && (llvm_version_ptr != NULL))\n          {\n            bool pocl_skip = false;\n\n            int pocl_maj = 0;\n            int pocl_min = 0;\n\n            const int res1 = sscanf (pocl_version_ptr, \"pocl %d.%d\", &pocl_maj, &pocl_min);\n\n            if (res1 == 2)\n            {\n              const int pocl_version = (pocl_maj * 100) + pocl_min;\n\n              if (pocl_version < 105)\n              {\n                pocl_skip = true;\n              }\n            }\n\n            int llvm_maj = 0;\n            int llvm_min = 0;\n\n            const int res2 = sscanf (llvm_version_ptr, \"LLVM %d.%d\", &llvm_maj, &llvm_min);\n\n            if (res2 == 2)\n            {\n              const int llvm_version = (llvm_maj * 100) + llvm_min;\n\n              if (llvm_version < 900)\n              {\n                pocl_skip = true;\n              }\n            }\n\n            if (pocl_skip == true)\n            {\n              if (user_options->force == false)\n              {\n                event_log_error (hashcat_ctx, \"* Device #%u: Outdated POCL OpenCL driver detected!\", device_id + 1);\n\n                if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"This OpenCL driver has been marked as likely to fail kernel compilation or to produce false negatives.\");\n                if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"You can use --force to override this, but do not report related errors.\");\n                if (user_options->quiet == false) event_log_warning (hashcat_ctx, NULL);\n\n                device_param->skipped = true;\n              }\n            }\n          }\n        }\n\n        char *opencl_device_version_lower = hcstrdup (opencl_device_version);\n\n        lowercase ((u8 *) opencl_device_version_lower, strlen (opencl_device_version_lower));\n\n        if ((strstr (opencl_device_version_lower, \"neo \"))\n         || (strstr (opencl_device_version_lower, \" neo\"))\n         || (strstr (opencl_device_version_lower, \"beignet \"))\n         || (strstr (opencl_device_version_lower, \" beignet\"))\n         || (strstr (opencl_device_version_lower, \"mesa \"))\n         || (strstr (opencl_device_version_lower, \" mesa\")))\n        {\n          // NEO:     https://github.com/hashcat/hashcat/issues/2342\n          // BEIGNET: https://github.com/hashcat/hashcat/issues/2243\n          // MESA:    https://github.com/hashcat/hashcat/issues/2269\n\n          if (user_options->force == false)\n          {\n            event_log_error (hashcat_ctx, \"* Device #%u: Unstable OpenCL driver detected!\", device_id + 1);\n\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"This OpenCL driver has been marked as likely to fail kernel compilation or to produce false negatives.\");\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"You can use --force to override this, but do not report related errors.\");\n            if (user_options->quiet == false) event_log_warning (hashcat_ctx, NULL);\n\n            device_param->skipped = true;\n          }\n        }\n\n        hcfree (opencl_device_version_lower);\n\n        // Since some times we get reports from users about not working hashcat, dropping error messages like:\n        // CL_INVALID_COMMAND_QUEUE and CL_OUT_OF_RESOURCES\n        // Turns out that this is caused by Intel OpenCL runtime handling their GPU devices\n        // Disable such devices unless the user forces to use it\n        // This is successfully workaround with new threading model and new memory management\n        // Tested on Windows 10\n        // OpenCL.Version.: OpenCL C 2.1\n        // Driver.Version.: 23.20.16.4973\n\n        /*\n        #if !defined (__APPLE__)\n        if (opencl_device_type & CL_DEVICE_TYPE_GPU)\n        {\n          if ((device_param->opencl_device_vendor_id == VENDOR_ID_INTEL_SDK) || (device_param->opencl_device_vendor_id == VENDOR_ID_INTEL_BEIGNET))\n          {\n            if (user_options->force == false)\n            {\n              if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: Intel's OpenCL runtime (GPU only) is currently broken.\", device_id + 1);\n              if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             We are waiting for updated OpenCL drivers from Intel.\");\n              if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             You can use --force to override, but do not report related errors.\");\n\n              device_param->skipped = true;\n            }\n          }\n        }\n        #endif // __APPLE__\n        */\n\n        // skipped\n\n        if ((backend_ctx->backend_devices_filter & (1ULL << device_id)) == 0)\n        {\n          device_param->skipped = true;\n        }\n\n        if ((backend_ctx->opencl_device_types_filter & (opencl_device_type)) == 0)\n        {\n          device_param->skipped = true;\n        }\n\n        // driver_version\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DRIVER_VERSION, 0, NULL, &param_value_size) == -1) return -1;\n\n        char *opencl_driver_version = (char *) hcmalloc (param_value_size);\n\n        if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DRIVER_VERSION, param_value_size, opencl_driver_version, NULL) == -1) return -1;\n\n        device_param->opencl_driver_version = opencl_driver_version;\n\n        // vendor specific\n\n        if (device_param->opencl_device_type & CL_DEVICE_TYPE_GPU)\n        {\n          if ((device_param->opencl_platform_vendor_id == VENDOR_ID_AMD) && (device_param->opencl_device_vendor_id == VENDOR_ID_AMD))\n          {\n            need_adl = true;\n\n            #if defined (__linux__)\n            need_sysfs = true;\n            #endif\n          }\n\n          if ((device_param->opencl_platform_vendor_id == VENDOR_ID_NV) && (device_param->opencl_device_vendor_id == VENDOR_ID_NV))\n          {\n            need_nvml = true;\n\n            #if defined (_WIN) || defined (__CYGWIN__)\n            need_nvapi = true;\n            #endif\n          }\n        }\n\n        if (device_param->opencl_device_type & CL_DEVICE_TYPE_GPU)\n        {\n          if ((device_param->opencl_platform_vendor_id == VENDOR_ID_AMD) && (device_param->opencl_device_vendor_id == VENDOR_ID_AMD))\n          {\n            cl_device_topology_amd amdtopo;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_TOPOLOGY_AMD, sizeof (amdtopo), &amdtopo, NULL) == -1) return -1;\n\n            device_param->pcie_domain   = 0; // no attribute to query\n            device_param->pcie_bus      = amdtopo.pcie.bus;\n            device_param->pcie_device   = amdtopo.pcie.device;\n            device_param->pcie_function = amdtopo.pcie.function;\n          }\n\n          if ((device_param->opencl_platform_vendor_id == VENDOR_ID_NV) && (device_param->opencl_device_vendor_id == VENDOR_ID_NV))\n          {\n            cl_uint pci_bus_id_nv;  // is cl_uint the right type for them??\n            cl_uint pci_slot_id_nv;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_PCI_BUS_ID_NV, sizeof (pci_bus_id_nv), &pci_bus_id_nv, NULL) == -1) return -1;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_PCI_SLOT_ID_NV, sizeof (pci_slot_id_nv), &pci_slot_id_nv, NULL) == -1) return -1;\n\n            device_param->pcie_domain   = 0; // no attribute to query\n            device_param->pcie_bus      = (u8) (pci_bus_id_nv);\n            device_param->pcie_device   = (u8) (pci_slot_id_nv >> 3);\n            device_param->pcie_function = (u8) (pci_slot_id_nv & 7);\n\n            int sm_minor = 0;\n            int sm_major = 0;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_COMPUTE_CAPABILITY_MINOR_NV, sizeof (sm_minor), &sm_minor, NULL) == -1) return -1;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_COMPUTE_CAPABILITY_MAJOR_NV, sizeof (sm_major), &sm_major, NULL) == -1) return -1;\n\n            device_param->sm_minor = sm_minor;\n            device_param->sm_major = sm_major;\n\n            cl_uint kernel_exec_timeout = 0;\n\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_KERNEL_EXEC_TIMEOUT_NV, sizeof (kernel_exec_timeout), &kernel_exec_timeout, NULL) == -1) return -1;\n\n            device_param->kernel_exec_timeout = kernel_exec_timeout;\n\n            // CPU burning loop damper\n            // Value is given as number between 0-100\n            // By default 8%\n\n            device_param->spin_damp = (double) user_options->spin_damp / 100;\n\n            // recommend CUDA\n\n            if ((backend_ctx->cuda == NULL) || (backend_ctx->nvrtc == NULL))\n            {\n              event_log_warning (hashcat_ctx, \"* Device #%u: CUDA SDK Toolkit installation NOT detected.\", device_id + 1);\n              event_log_warning (hashcat_ctx, \"             CUDA SDK Toolkit installation required for proper device support and utilization\");\n              event_log_warning (hashcat_ctx, \"             Falling back to OpenCL Runtime\");\n\n              event_log_warning (hashcat_ctx, NULL);\n            }\n          }\n        }\n\n        // common driver check\n\n        if (device_param->skipped == false)\n        {\n          if ((user_options->force == false) && (user_options->backend_info == false))\n          {\n            if (opencl_device_type & CL_DEVICE_TYPE_CPU)\n            {\n              if (device_param->opencl_platform_vendor_id == VENDOR_ID_INTEL_SDK)\n              {\n                bool intel_warn = false;\n\n                // Intel OpenCL runtime 18\n\n                int opencl_driver1 = 0;\n                int opencl_driver2 = 0;\n                int opencl_driver3 = 0;\n                int opencl_driver4 = 0;\n\n                const int res18 = sscanf (device_param->opencl_driver_version, \"%d.%d.%d.%d\", &opencl_driver1, &opencl_driver2, &opencl_driver3, &opencl_driver4);\n\n                if (res18 == 4)\n                {\n                  // so far all versions 18 are ok\n                }\n                else\n                {\n                  // Intel OpenCL runtime 16\n\n                  float opencl_version = 0;\n                  int   opencl_build   = 0;\n\n                  const int res16 = sscanf (device_param->opencl_device_version, \"OpenCL %f (Build %d)\", &opencl_version, &opencl_build);\n\n                  if (res16 == 2)\n                  {\n                    if (opencl_build < 25) intel_warn = true;\n                  }\n                }\n\n                if (intel_warn == true)\n                {\n                  event_log_error (hashcat_ctx, \"* Device #%u: Outdated or broken Intel OpenCL runtime '%s' detected!\", device_id + 1, device_param->opencl_driver_version);\n\n                  event_log_warning (hashcat_ctx, \"You are STRONGLY encouraged to use the officially supported Intel OpenCL runtime.\");\n                  event_log_warning (hashcat_ctx, \"See hashcat.net for officially supported Intel OpenCL runtime.\");\n                  event_log_warning (hashcat_ctx, \"See also: https://hashcat.net/faq/wrongdriver\");\n                  event_log_warning (hashcat_ctx, \"You can use --force to override this, but do not report related errors.\");\n                  event_log_warning (hashcat_ctx, NULL);\n\n                  return -1;\n                }\n              }\n            }\n            else if (opencl_device_type & CL_DEVICE_TYPE_GPU)\n            {\n              if (device_param->opencl_platform_vendor_id == VENDOR_ID_AMD)\n              {\n                bool amd_warn = true;\n\n                #if defined (__linux__)\n                // AMDGPU-PRO Driver 16.40 and higher\n                if (strtoul (device_param->opencl_driver_version, NULL, 10) >= 2117) amd_warn = false;\n                // AMDGPU-PRO Driver 16.50 is known to be broken\n                if (strtoul (device_param->opencl_driver_version, NULL, 10) == 2236) amd_warn = true;\n                // AMDGPU-PRO Driver 16.60 is known to be broken\n                if (strtoul (device_param->opencl_driver_version, NULL, 10) == 2264) amd_warn = true;\n                // AMDGPU-PRO Driver 17.10 is known to be broken\n                if (strtoul (device_param->opencl_driver_version, NULL, 10) == 2348) amd_warn = true;\n                // AMDGPU-PRO Driver 17.20 (2416) is fine, doesn't need check will match >= 2117\n                #elif defined (_WIN)\n                // AMD Radeon Software 14.9 and higher, should be updated to 15.12\n                if (strtoul (device_param->opencl_driver_version, NULL, 10) >= 1573) amd_warn = false;\n                #else\n                // we have no information about other os\n                if (amd_warn == true) amd_warn = false;\n                #endif\n\n                if (amd_warn == true)\n                {\n                  event_log_error (hashcat_ctx, \"* Device #%u: Outdated or broken AMD driver '%s' detected!\", device_id + 1, device_param->opencl_driver_version);\n\n                  event_log_warning (hashcat_ctx, \"You are STRONGLY encouraged to use the officially supported AMD driver.\");\n                  event_log_warning (hashcat_ctx, \"See hashcat.net for officially supported AMD drivers.\");\n                  event_log_warning (hashcat_ctx, \"See also: https://hashcat.net/faq/wrongdriver\");\n                  event_log_warning (hashcat_ctx, \"You can use --force to override this, but do not report related errors.\");\n                  event_log_warning (hashcat_ctx, NULL);\n\n                  return -1;\n                }\n              }\n\n              if (device_param->opencl_platform_vendor_id == VENDOR_ID_NV)\n              {\n                int nv_warn = true;\n\n                int version_maj = 0;\n                int version_min = 0;\n\n                const int r = sscanf (device_param->opencl_driver_version, \"%d.%d\", &version_maj, &version_min);\n\n                if (r == 2)\n                {\n                  // nvidia 441.x looks ok\n\n                  if (version_maj == 440)\n                  {\n                    if (version_min >= 64)\n                    {\n                      nv_warn = false;\n                    }\n                  }\n                  else\n                  {\n                    // unknown version scheme, probably new driver version\n\n                    nv_warn = false;\n                  }\n                }\n                else\n                {\n                  // unknown version scheme, probably new driver version\n\n                  nv_warn = false;\n                }\n\n                if (nv_warn == true)\n                {\n                  event_log_warning (hashcat_ctx, \"* Device #%u: Outdated or broken NVIDIA driver '%s' detected!\", device_id + 1, device_param->opencl_driver_version);\n                  event_log_warning (hashcat_ctx, NULL);\n\n                  event_log_warning (hashcat_ctx, \"You are STRONGLY encouraged to use the officially supported NVIDIA driver.\");\n                  event_log_warning (hashcat_ctx, \"See hashcat's homepage for officially supported NVIDIA drivers.\");\n                  event_log_warning (hashcat_ctx, \"See also: https://hashcat.net/faq/wrongdriver\");\n                  event_log_warning (hashcat_ctx, \"You can use --force to override this, but do not report related errors.\");\n                  event_log_warning (hashcat_ctx, NULL);\n\n                  return -1;\n                }\n\n                if (device_param->sm_major < 5)\n                {\n                  if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: This hardware has outdated CUDA compute capability (%u.%u).\", device_id + 1, device_param->sm_major, device_param->sm_minor);\n                  if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             For modern OpenCL performance, upgrade to hardware that supports\");\n                  if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             CUDA compute capability version 5.0 (Maxwell) or higher.\");\n                }\n\n                if (device_param->kernel_exec_timeout != 0)\n                {\n                  if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: WARNING! Kernel exec timeout is not disabled.\", device_id + 1);\n                  if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             This may cause \\\"CL_OUT_OF_RESOURCES\\\" or related errors.\");\n                  if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"             To disable the timeout, see: https://hashcat.net/q/timeoutpatch\");\n                }\n              }\n            }\n          }\n\n          /**\n           * activate device\n           */\n\n          opencl_devices_active++;\n        }\n\n        /**\n         * create context for each device\n         */\n\n        cl_context context;\n\n        /*\n        cl_context_properties properties[3];\n\n        properties[0] = CL_CONTEXT_PLATFORM;\n        properties[1] = (cl_context_properties) device_param->opencl_platform;\n        properties[2] = 0;\n\n        CL_rc = hc_clCreateContext (hashcat_ctx, properties, 1, &device_param->opencl_device, NULL, NULL, &context);\n        */\n\n        if (hc_clCreateContext (hashcat_ctx, NULL, 1, &device_param->opencl_device, NULL, NULL, &context) == -1) return -1;\n\n        /**\n         * create command-queue\n         */\n\n        cl_command_queue command_queue;\n\n        if (hc_clCreateCommandQueue (hashcat_ctx, context, device_param->opencl_device, 0, &command_queue) == -1) return -1;\n\n        if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) && (device_param->opencl_platform_vendor_id == VENDOR_ID_AMD))\n        {\n          #define RUN_INSTRUCTION_CHECKS()\n            device_param->has_vadd     = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_ADD_U32     %0, vcc, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vaddc    = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_ADDC_U32    %0, vcc, 0, 0, vcc;\\\" : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vadd_co  = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_ADD_CO_U32  %0, vcc, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vaddc_co = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_ADDC_CO_U32 %0, vcc, 0, 0, vcc;\\\" : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vsub     = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_SUB_U32     %0, vcc, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vsubb    = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_SUBB_U32    %0, vcc, 0, 0, vcc;\\\" : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vsub_co  = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_SUB_CO_U32  %0, vcc, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vsubb_co = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_SUBB_CO_U32 %0, vcc, 0, 0, vcc;\\\" : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vadd3    = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_ADD3_U32    %0,   0, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vbfe     = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_BFE_U32     %0,   0, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n            device_param->has_vperm    = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r1; __asm__ __volatile__ (\\\"V_PERM_B32    %0,   0, 0, 0;\\\"      : \\\"=v\\\"(r1)); }\"); \\\n\n          if (backend_devices_idx > 0)\n          {\n            hc_device_param_t *device_param_prev = &devices_param[backend_devices_idx - 1];\n\n            if (is_same_device_type (device_param, device_param_prev) == true)\n            {\n              device_param->has_vadd     = device_param_prev->has_vadd;\n              device_param->has_vaddc    = device_param_prev->has_vaddc;\n              device_param->has_vadd_co  = device_param_prev->has_vadd_co;\n              device_param->has_vaddc_co = device_param_prev->has_vaddc_co;\n              device_param->has_vsub     = device_param_prev->has_vsub;\n              device_param->has_vsubb    = device_param_prev->has_vsubb;\n              device_param->has_vsub_co  = device_param_prev->has_vsub_co;\n              device_param->has_vsubb_co = device_param_prev->has_vsubb_co;\n              device_param->has_vadd3    = device_param_prev->has_vadd3;\n              device_param->has_vbfe     = device_param_prev->has_vbfe;\n              device_param->has_vperm    = device_param_prev->has_vperm;\n            }\n            else\n            {\n              RUN_INSTRUCTION_CHECKS();\n            }\n          }\n          else\n          {\n            RUN_INSTRUCTION_CHECKS();\n          }\n\n          #undef RUN_INSTRUCTION_CHECKS\n        }\n\n        if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) && (device_param->opencl_platform_vendor_id == VENDOR_ID_NV))\n        {\n          const int sm = (device_param->sm_major * 10) + device_param->sm_minor;\n\n          device_param->has_add   = (sm >= 12) ? true : false;\n          device_param->has_addc  = (sm >= 12) ? true : false;\n          device_param->has_sub   = (sm >= 12) ? true : false;\n          device_param->has_subc  = (sm >= 12) ? true : false;\n          device_param->has_bfe   = (sm >= 20) ? true : false;\n          device_param->has_lop3  = (sm >= 50) ? true : false;\n          device_param->has_mov64 = (sm >= 10) ? true : false;\n          device_param->has_prmt  = (sm >= 20) ? true : false;\n\n          /*\n          #define RUN_INSTRUCTION_CHECKS()                                                                                                                                                                                                          \\\n            device_param->has_add   = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"add.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                        \\\n            device_param->has_addc  = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"addc.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                       \\\n            device_param->has_sub   = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"sub.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                        \\\n            device_param->has_subc  = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"subc.cc.u32 %0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                       \\\n            device_param->has_bfe   = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"bfe.u32 %0, 0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                        \\\n            device_param->has_lop3  = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"lop3.b32 %0, 0, 0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                    \\\n            device_param->has_mov64 = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { ulong r; uint a; uint b; asm volatile (\\\"mov.b64 %0, {%1, %2};\\\" : \\\"=l\\\"(r) : \\\"r\\\"(a), \\\"r\\\"(b)); }\"); \\\n            device_param->has_prmt  = opencl_test_instruction (hashcat_ctx, context, device_param->opencl_device, \"__kernel void test () { uint r; asm volatile (\\\"prmt.b32 %0, 0, 0, 0;\\\" : \\\"=r\\\"(r)); }\");                                       \\\n\n          if (backend_devices_idx > 0)\n          {\n            hc_device_param_t *device_param_prev = &devices_param[backend_devices_idx - 1];\n\n            if (is_same_device_type (device_param, device_param_prev) == true)\n            {\n              device_param->has_add   = device_param_prev->has_add;\n              device_param->has_addc  = device_param_prev->has_addc;\n              device_param->has_sub   = device_param_prev->has_sub;\n              device_param->has_subc  = device_param_prev->has_subc;\n              device_param->has_bfe   = device_param_prev->has_bfe;\n              device_param->has_lop3  = device_param_prev->has_lop3;\n              device_param->has_mov64 = device_param_prev->has_mov64;\n              device_param->has_prmt  = device_param_prev->has_prmt;\n            }\n            else\n            {\n              RUN_INSTRUCTION_CHECKS();\n            }\n          }\n          else\n          {\n            RUN_INSTRUCTION_CHECKS();\n          }\n\n          #undef RUN_INSTRUCTION_CHECKS\n          */\n        }\n\n        // device_available_mem\n\n        #define MAX_ALLOC_CHECKS_CNT  8192\n        #define MAX_ALLOC_CHECKS_SIZE (64 * 1024 * 1024)\n\n        device_param->device_available_mem = device_param->device_global_mem - MAX_ALLOC_CHECKS_SIZE;\n\n        #if defined (_WIN)\n        if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) && (device_param->opencl_platform_vendor_id == VENDOR_ID_NV))\n        #else\n        if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) && ((device_param->opencl_platform_vendor_id == VENDOR_ID_NV) || (device_param->opencl_platform_vendor_id == VENDOR_ID_AMD)))\n        #endif\n        {\n          // OK, so the problem here is the following:\n          // There's just CL_DEVICE_GLOBAL_MEM_SIZE to ask OpenCL about the total memory on the device,\n          // but there's no way to ask for available memory on the device.\n          // In combination, most OpenCL runtimes implementation of clCreateBuffer()\n          // are doing so called lazy memory allocation on the device.\n          // Now, if the user has X11 (or a game or anything that takes a lot of GPU memory)\n          // running on the host we end up with an error type of this:\n          // clEnqueueNDRangeKernel(): CL_MEM_OBJECT_ALLOCATION_FAILURE\n          // The clEnqueueNDRangeKernel() is because of the lazy allocation\n          // The best way to workaround this problem is if we would be able to ask for available memory,\n          // The idea here is to try to evaluate available memory by allocating it till it errors\n\n          cl_mem *tmp_device = (cl_mem *) hccalloc (MAX_ALLOC_CHECKS_CNT, sizeof (cl_mem));\n\n          u64 c;\n\n          for (c = 0; c < MAX_ALLOC_CHECKS_CNT; c++)\n          {\n            if (((c + 1 + 1) * MAX_ALLOC_CHECKS_SIZE) >= device_param->device_global_mem) break;\n\n            cl_int CL_err;\n\n            OCL_PTR *ocl = (OCL_PTR *) backend_ctx->ocl;\n\n            tmp_device[c] = ocl->clCreateBuffer (context, CL_MEM_READ_WRITE, MAX_ALLOC_CHECKS_SIZE, NULL, &CL_err);\n\n            if (CL_err != CL_SUCCESS)\n            {\n              c--;\n\n              break;\n            }\n\n            // transfer only a few byte should be enough to force the runtime to actually allocate the memory\n\n            u8 tmp_host[8];\n\n            if (ocl->clEnqueueReadBuffer  (command_queue, tmp_device[c], CL_TRUE, 0, sizeof (tmp_host), tmp_host, 0, NULL, NULL) != CL_SUCCESS) break;\n\n            if (ocl->clEnqueueWriteBuffer (command_queue, tmp_device[c], CL_TRUE, 0, sizeof (tmp_host), tmp_host, 0, NULL, NULL) != CL_SUCCESS) break;\n\n            if (ocl->clEnqueueReadBuffer  (command_queue, tmp_device[c], CL_TRUE, MAX_ALLOC_CHECKS_SIZE - sizeof (tmp_host), sizeof (tmp_host), tmp_host, 0, NULL, NULL) != CL_SUCCESS) break;\n\n            if (ocl->clEnqueueWriteBuffer (command_queue, tmp_device[c], CL_TRUE, MAX_ALLOC_CHECKS_SIZE - sizeof (tmp_host), sizeof (tmp_host), tmp_host, 0, NULL, NULL) != CL_SUCCESS) break;\n          }\n\n          device_param->device_available_mem = MAX_ALLOC_CHECKS_SIZE;\n          if (c > 0)\n          {\n            device_param->device_available_mem *= c;\n          }\n\n          // clean up\n\n          for (c = 0; c < MAX_ALLOC_CHECKS_CNT; c++)\n          {\n            if (((c + 1 + 1) * MAX_ALLOC_CHECKS_SIZE) >= device_param->device_global_mem) break;\n\n            if (tmp_device[c] != NULL)\n            {\n              if (hc_clReleaseMemObject (hashcat_ctx, tmp_device[c]) == -1) return -1;\n            }\n          }\n\n          hcfree (tmp_device);\n        }\n\n        hc_clReleaseCommandQueue (hashcat_ctx, command_queue);\n\n        hc_clReleaseContext (hashcat_ctx, context);\n      }\n    }\n  }\n\n  backend_ctx->opencl_devices_cnt     = opencl_devices_cnt;\n  backend_ctx->opencl_devices_active  = opencl_devices_active;\n\n  // all devices combined go into backend_* variables\n\n  backend_ctx->backend_devices_cnt    = cuda_devices_cnt    + opencl_devices_cnt;\n  backend_ctx->backend_devices_active = cuda_devices_active + opencl_devices_active;\n\n  // find duplicate devices\n\n  //if ((cuda_devices_cnt > 0) && (opencl_devices_cnt > 0))\n  //{\n    // using force here enables both devices, which is the worst possible outcome\n    // many users force by default, so this is not a good idea\n\n    //if (user_options->force == false)\n    //{\n    backend_ctx_find_alias_devices (hashcat_ctx);\n    //{\n  //}\n\n  if (backend_ctx->backend_devices_active == 0)\n  {\n    event_log_error (hashcat_ctx, \"No devices found/left.\");\n\n    return -1;\n  }\n\n  // now we can calculate the number of parallel running hook threads based on\n  // the number cpu cores and the number of active compute devices\n  // unless overwritten by the user\n\n  if (user_options->hook_threads == HOOK_THREADS)\n  {\n    const u32 processor_count = hc_get_processor_count ();\n\n    const u32 processor_count_cu = CEILDIV (processor_count, backend_ctx->backend_devices_active); // should never reach 0\n\n    user_options->hook_threads = processor_count_cu;\n  }\n\n  // additional check to see if the user has chosen a device that is not within the range of available devices (i.e. larger than devices_cnt)\n\n  if (backend_ctx->backend_devices_filter != (u64) -1)\n  {\n    const u64 backend_devices_cnt_mask = ~(((u64) -1 >> backend_ctx->backend_devices_cnt) << backend_ctx->backend_devices_cnt);\n\n    if (backend_ctx->backend_devices_filter > backend_devices_cnt_mask)\n    {\n      event_log_error (hashcat_ctx, \"An invalid device was specified using the --backend-devices parameter.\");\n      event_log_error (hashcat_ctx, \"The specified device was higher than the number of available devices (%u).\", backend_ctx->backend_devices_cnt);\n\n      return -1;\n    }\n  }\n\n  backend_ctx->target_msec  = TARGET_MSEC_PROFILE[user_options->workload_profile - 1];\n\n  backend_ctx->need_adl     = need_adl;\n  backend_ctx->need_nvml    = need_nvml;\n  backend_ctx->need_nvapi   = need_nvapi;\n  backend_ctx->need_sysfs   = need_sysfs;\n\n  backend_ctx->comptime     = comptime;\n\n  return 0;\n}\n\nvoid backend_ctx_devices_destroy (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (backend_ctx->enabled == false) return;\n\n  for (u32 opencl_platforms_idx = 0; opencl_platforms_idx < backend_ctx->opencl_platforms_cnt; opencl_platforms_idx++)\n  {\n    hcfree (backend_ctx->opencl_platforms_devices[opencl_platforms_idx]);\n    hcfree (backend_ctx->opencl_platforms_name[opencl_platforms_idx]);\n    hcfree (backend_ctx->opencl_platforms_vendor[opencl_platforms_idx]);\n    hcfree (backend_ctx->opencl_platforms_version[opencl_platforms_idx]);\n  }\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    hcfree (device_param->device_name);\n\n    if (device_param->is_opencl == true)\n    {\n      hcfree (device_param->opencl_driver_version);\n      hcfree (device_param->opencl_device_version);\n      hcfree (device_param->opencl_device_c_version);\n      hcfree (device_param->opencl_device_vendor);\n    }\n  }\n\n  backend_ctx->backend_devices_cnt    = 0;\n  backend_ctx->backend_devices_active = 0;\n  backend_ctx->cuda_devices_cnt       = 0;\n  backend_ctx->cuda_devices_active    = 0;\n  backend_ctx->opencl_devices_cnt     = 0;\n  backend_ctx->opencl_devices_active  = 0;\n\n  backend_ctx->need_adl    = false;\n  backend_ctx->need_nvml   = false;\n  backend_ctx->need_nvapi  = false;\n  backend_ctx->need_sysfs  = false;\n}\n\nvoid backend_ctx_devices_sync_tuning (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (backend_ctx->enabled == false) return;\n\n  for (int backend_devices_cnt_src = 0; backend_devices_cnt_src < backend_ctx->backend_devices_cnt; backend_devices_cnt_src++)\n  {\n    hc_device_param_t *device_param_src = &backend_ctx->devices_param[backend_devices_cnt_src];\n\n    if (device_param_src->skipped == true) continue;\n\n    if (device_param_src->skipped_warning == true) continue;\n\n    for (int backend_devices_cnt_dst = backend_devices_cnt_src + 1; backend_devices_cnt_dst < backend_ctx->backend_devices_cnt; backend_devices_cnt_dst++)\n    {\n      hc_device_param_t *device_param_dst = &backend_ctx->devices_param[backend_devices_cnt_dst];\n\n      if (device_param_dst->skipped == true) continue;\n\n      if (device_param_dst->skipped_warning == true) continue;\n\n      if (is_same_device_type (device_param_src, device_param_dst) == false) continue;\n\n      device_param_dst->kernel_accel   = device_param_src->kernel_accel;\n      device_param_dst->kernel_loops   = device_param_src->kernel_loops;\n      device_param_dst->kernel_threads = device_param_src->kernel_threads;\n\n      const u32 hardware_power = device_param_dst->device_processors * device_param_dst->kernel_threads;\n\n      device_param_dst->hardware_power = hardware_power;\n\n      const u32 kernel_power = device_param_dst->hardware_power * device_param_dst->kernel_accel;\n\n      device_param_dst->kernel_power = kernel_power;\n    }\n  }\n}\n\nvoid backend_ctx_devices_update_power (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t        *backend_ctx         = hashcat_ctx->backend_ctx;\n  status_ctx_t         *status_ctx          = hashcat_ctx->status_ctx;\n  user_options_extra_t *user_options_extra  = hashcat_ctx->user_options_extra;\n  user_options_t       *user_options        = hashcat_ctx->user_options;\n\n  if (backend_ctx->enabled == false) return;\n\n  u32 kernel_power_all = 0;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    if (device_param->skipped_warning == true) continue;\n\n    kernel_power_all += device_param->kernel_power;\n  }\n\n  backend_ctx->kernel_power_all = kernel_power_all;\n\n  /*\n   * Inform user about possible slow speeds\n   */\n\n  if ((user_options_extra->wordlist_mode == WL_MODE_FILE) || (user_options_extra->wordlist_mode == WL_MODE_MASK))\n  {\n    if (status_ctx->words_base < kernel_power_all)\n    {\n      if (user_options->quiet == false)\n      {\n        event_log_advice (hashcat_ctx, \"The wordlist or mask that you are using is too small.\");\n        event_log_advice (hashcat_ctx, \"This means that hashcat cannot use the full parallel power of your device(s).\");\n        event_log_advice (hashcat_ctx, \"Unless you supply more work, your cracking speed will drop.\");\n        event_log_advice (hashcat_ctx, \"For tips on supplying more work, see: https://hashcat.net/faq/morework\");\n        event_log_advice (hashcat_ctx, NULL);\n      }\n    }\n  }\n}\n\nvoid backend_ctx_devices_kernel_loops (hashcat_ctx_t *hashcat_ctx)\n{\n  combinator_ctx_t     *combinator_ctx      = hashcat_ctx->combinator_ctx;\n  hashconfig_t         *hashconfig          = hashcat_ctx->hashconfig;\n  hashes_t             *hashes              = hashcat_ctx->hashes;\n  mask_ctx_t           *mask_ctx            = hashcat_ctx->mask_ctx;\n  backend_ctx_t        *backend_ctx         = hashcat_ctx->backend_ctx;\n  straight_ctx_t       *straight_ctx        = hashcat_ctx->straight_ctx;\n  user_options_t       *user_options        = hashcat_ctx->user_options;\n  user_options_extra_t *user_options_extra  = hashcat_ctx->user_options_extra;\n\n  if (backend_ctx->enabled == false) return;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    if (device_param->skipped_warning == true) continue;\n\n    device_param->kernel_loops_min = device_param->kernel_loops_min_sav;\n    device_param->kernel_loops_max = device_param->kernel_loops_max_sav;\n\n    if (device_param->kernel_loops_min < device_param->kernel_loops_max)\n    {\n      u32 innerloop_cnt = 0;\n\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n        if (user_options->slow_candidates == true)\n        {\n          innerloop_cnt = 1;\n        }\n        else\n        {\n          if      (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)  innerloop_cnt = MIN (KERNEL_RULES, (u32) straight_ctx->kernel_rules_cnt);\n          else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)     innerloop_cnt = MIN (KERNEL_COMBS, (u32) combinator_ctx->combs_cnt);\n          else if (user_options_extra->attack_kern == ATTACK_KERN_BF)        innerloop_cnt = MIN (KERNEL_BFS,   (u32) mask_ctx->bfs_cnt);\n        }\n      }\n      else\n      {\n        innerloop_cnt = hashes->salts_buf[0].salt_iter;\n      }\n\n      if ((innerloop_cnt >= device_param->kernel_loops_min) &&\n          (innerloop_cnt <= device_param->kernel_loops_max))\n      {\n        device_param->kernel_loops_max = innerloop_cnt;\n      }\n    }\n  }\n}\n\nstatic int get_cuda_kernel_wgs (hashcat_ctx_t *hashcat_ctx, CUfunction function, u32 *result)\n{\n  int max_threads_per_block;\n\n  if (hc_cuFuncGetAttribute (hashcat_ctx, &max_threads_per_block, CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK, function) == -1) return -1;\n\n  *result = (u32) max_threads_per_block;\n\n  return 0;\n}\n\nstatic int get_cuda_kernel_local_mem_size (hashcat_ctx_t *hashcat_ctx, CUfunction function, u64 *result)\n{\n  int shared_size_bytes;\n\n  if (hc_cuFuncGetAttribute (hashcat_ctx, &shared_size_bytes, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, function) == -1) return -1;\n\n  *result = (u64) shared_size_bytes;\n\n  return 0;\n}\n\nstatic int get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx_t *hashcat_ctx, CUfunction function, u64 *result)\n{\n  // AFAIK there's no way to query the maximum value for dynamic shared memory available (because it depends on kernel code).\n  // let's brute force it, therefore workaround the hashcat wrapper of cuFuncSetAttribute()\n\n  #define MAX_ASSUMED_SHARED (1024 * 1024)\n\n  u64 dynamic_shared_size_bytes = 0;\n\n  for (int i = 1; i <= MAX_ASSUMED_SHARED; i++)\n  {\n    backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n    CUDA_PTR *cuda = (CUDA_PTR *) backend_ctx->cuda;\n\n    const CUresult CU_err = cuda->cuFuncSetAttribute (function, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, i);\n\n    if (CU_err == CUDA_SUCCESS)\n    {\n      dynamic_shared_size_bytes = i;\n\n      continue;\n    }\n\n    break;\n  }\n\n  *result = dynamic_shared_size_bytes;\n\n  if (hc_cuFuncSetAttribute (hashcat_ctx, function, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, 0) == -1) return -1;\n\n  return 0;\n}\n\nstatic int get_opencl_kernel_wgs (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_kernel kernel, u32 *result)\n{\n  size_t work_group_size = 0;\n\n  if (hc_clGetKernelWorkGroupInfo (hashcat_ctx, kernel, device_param->opencl_device, CL_KERNEL_WORK_GROUP_SIZE, sizeof (work_group_size), &work_group_size, NULL) == -1) return -1;\n\n  u32 kernel_threads = (u32) work_group_size;\n\n  size_t compile_work_group_size[3] = { 0, 0, 0 };\n\n  if (hc_clGetKernelWorkGroupInfo (hashcat_ctx, kernel, device_param->opencl_device, CL_KERNEL_COMPILE_WORK_GROUP_SIZE, sizeof (compile_work_group_size), &compile_work_group_size, NULL) == -1) return -1;\n\n  const size_t cwgs_total = compile_work_group_size[0] * compile_work_group_size[1] * compile_work_group_size[2];\n\n  if (cwgs_total > 0)\n  {\n    kernel_threads = MIN (kernel_threads, (u32) cwgs_total);\n  }\n\n  *result = kernel_threads;\n\n  return 0;\n}\n\nstatic int get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_kernel kernel, u32 *result)\n{\n  size_t preferred_work_group_size_multiple = 0;\n\n  if (hc_clGetKernelWorkGroupInfo (hashcat_ctx, kernel, device_param->opencl_device, CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE, sizeof (preferred_work_group_size_multiple), &preferred_work_group_size_multiple, NULL) == -1) return -1;\n\n  *result = (u32) preferred_work_group_size_multiple;\n\n  return 0;\n}\n\nstatic int get_opencl_kernel_local_mem_size (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_kernel kernel, u64 *result)\n{\n  cl_ulong local_mem_size = 0;\n\n  if (hc_clGetKernelWorkGroupInfo (hashcat_ctx, kernel, device_param->opencl_device, CL_KERNEL_LOCAL_MEM_SIZE, sizeof (local_mem_size), &local_mem_size, NULL) == -1) return -1;\n\n  *result = local_mem_size;\n\n  return 0;\n}\n\nstatic int get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, cl_kernel kernel, u64 *result)\n{\n  cl_ulong dynamic_local_mem_size = 0;\n\n  if (hc_clGetKernelWorkGroupInfo (hashcat_ctx, kernel, device_param->opencl_device, CL_KERNEL_LOCAL_MEM_SIZE, sizeof (dynamic_local_mem_size), &dynamic_local_mem_size, NULL) == -1) return -1;\n\n  // unknown how to query this information in OpenCL\n  // we therefore reset to zero\n  // the above call to hc_clGetKernelWorkGroupInfo() is just to avoid compiler warnings\n\n  dynamic_local_mem_size = 0;\n\n  *result = dynamic_local_mem_size;\n\n  return 0;\n}\n\nstatic u32 get_kernel_threads (const hc_device_param_t *device_param)\n{\n  // this is an upper limit, a good start, since our strategy is to reduce thread counts only.\n\n  u32 kernel_threads_min = device_param->kernel_threads_min;\n  u32 kernel_threads_max = device_param->kernel_threads_max;\n\n  // the changes we do here are just optimizations, since the module always has priority.\n\n  const u32 device_maxworkgroup_size = (const u32) device_param->device_maxworkgroup_size;\n\n  kernel_threads_max = MIN (kernel_threads_max, device_maxworkgroup_size);\n\n  if (device_param->opencl_device_type & CL_DEVICE_TYPE_CPU)\n  {\n    // for all CPU we just do 1 ...\n\n    const u32 cpu_prefered_thread_count = 1;\n\n    kernel_threads_max = MIN (kernel_threads_max, cpu_prefered_thread_count);\n  }\n  else if (device_param->opencl_device_type & CL_DEVICE_TYPE_GPU)\n  {\n    // for GPU we need to distinguish by vendor\n\n    if (device_param->opencl_device_vendor_id == VENDOR_ID_INTEL_SDK)\n    {\n      const u32 gpu_prefered_thread_count = 8;\n\n      kernel_threads_max = MIN (kernel_threads_max, gpu_prefered_thread_count);\n    }\n    else if (device_param->opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      const u32 gpu_prefered_thread_count = 64;\n\n      kernel_threads_max = MIN (kernel_threads_max, gpu_prefered_thread_count);\n    }\n  }\n\n  // this is intenionally! at this point, kernel_threads_min can be higher than kernel_threads_max.\n  // in this case we actually want kernel_threads_min selected.\n\n  const u32 kernel_threads = MAX (kernel_threads_min, kernel_threads_max);\n\n  return kernel_threads;\n}\n\nstatic bool load_kernel (hashcat_ctx_t *hashcat_ctx, hc_device_param_t *device_param, const char *kernel_name, char *source_file, char *cached_file, const char *build_options_buf, const bool cache_disable, cl_program *opencl_program, CUmodule *cuda_module)\n{\n  const hashconfig_t *hashconfig = hashcat_ctx->hashconfig;\n\n  bool cached = true;\n\n  if (cache_disable == true)\n  {\n    cached = false;\n  }\n\n  if (hc_path_read (cached_file) == false)\n  {\n    cached = false;\n  }\n\n  if (hc_path_is_empty (cached_file) == true)\n  {\n    cached = false;\n  }\n\n  /**\n   * kernel compile or load\n   */\n\n  size_t kernel_lengths_buf = 0;\n\n  size_t *kernel_lengths = &kernel_lengths_buf;\n\n  char *kernel_sources_buf = NULL;\n\n  char **kernel_sources = &kernel_sources_buf;\n\n  if (cached == false)\n  {\n    #if defined (DEBUG)\n    const user_options_t *user_options = hashcat_ctx->user_options;\n\n    if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: Kernel %s not found in cache! Building may take a while...\", device_param->device_id + 1, filename_from_filepath (cached_file));\n    #endif\n\n    if (read_kernel_binary (hashcat_ctx, source_file, kernel_lengths, kernel_sources) == false) return false;\n\n    if (device_param->is_cuda == true)\n    {\n      nvrtcProgram program;\n\n      if (hc_nvrtcCreateProgram (hashcat_ctx, &program, kernel_sources[0], kernel_name, 0, NULL, NULL) == -1) return false;\n\n      char **nvrtc_options = (char **) hccalloc (4 + strlen (build_options_buf) + 1, sizeof (char *)); // ...\n\n      nvrtc_options[0] = \"--restrict\";\n      nvrtc_options[1] = \"--device-as-default-execution-space\";\n      nvrtc_options[2] = \"--gpu-architecture\";\n\n      hc_asprintf (&nvrtc_options[3], \"compute_%d%d\", device_param->sm_major, device_param->sm_minor);\n\n      char *nvrtc_options_string = hcstrdup (build_options_buf);\n\n      const int num_options = 4 + nvrtc_make_options_array_from_string (nvrtc_options_string, nvrtc_options + 4);\n\n      const int rc_nvrtcCompileProgram = hc_nvrtcCompileProgram (hashcat_ctx, program, num_options, (const char * const *) nvrtc_options);\n\n      size_t build_log_size = 0;\n\n      hc_nvrtcGetProgramLogSize (hashcat_ctx, program, &build_log_size);\n\n      #if defined (DEBUG)\n      if ((build_log_size > 1) || (rc_nvrtcCompileProgram == -1))\n      #else\n      if (rc_nvrtcCompileProgram == -1)\n      #endif\n      {\n        char *build_log = (char *) hcmalloc (build_log_size + 1);\n\n        if (hc_nvrtcGetProgramLog (hashcat_ctx, program, build_log) == -1) return false;\n\n        puts (build_log);\n\n        hcfree (build_log);\n      }\n\n      if (rc_nvrtcCompileProgram == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s build failed.\", device_param->device_id + 1, source_file);\n\n        return false;\n      }\n\n      hcfree (nvrtc_options);\n      hcfree (nvrtc_options_string);\n\n      size_t binary_size = 0;\n\n      if (hc_nvrtcGetPTXSize (hashcat_ctx, program, &binary_size) == -1) return false;\n\n      char *binary = (char *) hcmalloc (binary_size);\n\n      if (hc_nvrtcGetPTX (hashcat_ctx, program, binary) == -1) return false;\n\n      if (hc_nvrtcDestroyProgram (hashcat_ctx, &program) == -1) return false;\n\n      #define LOG_SIZE 8192\n\n      char *mod_info_log  = (char *) hcmalloc (LOG_SIZE + 1);\n      char *mod_error_log = (char *) hcmalloc (LOG_SIZE + 1);\n\n      int mod_cnt = 6;\n\n      CUjit_option mod_opts[7];\n      void *mod_vals[7];\n\n      mod_opts[0] = CU_JIT_TARGET_FROM_CUCONTEXT;\n      mod_vals[0] = (void *) 0;\n\n      mod_opts[1] = CU_JIT_LOG_VERBOSE;\n      mod_vals[1] = (void *) 1;\n\n      mod_opts[2] = CU_JIT_INFO_LOG_BUFFER;\n      mod_vals[2] = (void *) mod_info_log;\n\n      mod_opts[3] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;\n      mod_vals[3] = (void *) LOG_SIZE;\n\n      mod_opts[4] = CU_JIT_ERROR_LOG_BUFFER;\n      mod_vals[4] = (void *) mod_error_log;\n\n      mod_opts[5] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;\n      mod_vals[5] = (void *) LOG_SIZE;\n\n      if (hashconfig->opti_type & OPTI_TYPE_REGISTER_LIMIT)\n      {\n        mod_opts[6] = CU_JIT_MAX_REGISTERS;\n        mod_vals[6] = (void *) 128;\n\n        mod_cnt++;\n      }\n\n      #if defined (WITH_CUBIN)\n\n      char *jit_info_log  = (char *) hcmalloc (LOG_SIZE + 1);\n      char *jit_error_log = (char *) hcmalloc (LOG_SIZE + 1);\n\n      int jit_cnt = 6;\n\n      CUjit_option jit_opts[7];\n      void *jit_vals[7];\n\n      jit_opts[0] = CU_JIT_TARGET_FROM_CUCONTEXT;\n      jit_vals[0] = (void *) 0;\n\n      jit_opts[1] = CU_JIT_LOG_VERBOSE;\n      jit_vals[1] = (void *) 1;\n\n      jit_opts[2] = CU_JIT_INFO_LOG_BUFFER;\n      jit_vals[2] = (void *) jit_info_log;\n\n      jit_opts[3] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;\n      jit_vals[3] = (void *) LOG_SIZE;\n\n      jit_opts[4] = CU_JIT_ERROR_LOG_BUFFER;\n      jit_vals[4] = (void *) jit_error_log;\n\n      jit_opts[5] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;\n      jit_vals[5] = (void *) LOG_SIZE;\n\n      if (hashconfig->opti_type & OPTI_TYPE_REGISTER_LIMIT)\n      {\n        jit_opts[6] = CU_JIT_MAX_REGISTERS;\n        jit_vals[6] = (void *) 128;\n\n        jit_cnt++;\n      }\n\n      CUlinkState state;\n\n      if (hc_cuLinkCreate (hashcat_ctx, jit_cnt, jit_opts, jit_vals, &state) == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s link failed. Error Log:\", device_param->device_id + 1, source_file);\n        event_log_error (hashcat_ctx, \"%s\", jit_error_log);\n        event_log_error (hashcat_ctx, NULL);\n\n        return false;\n      }\n\n      if (hc_cuLinkAddData (hashcat_ctx, state, CU_JIT_INPUT_PTX, binary, binary_size, kernel_name, 0, NULL, NULL) == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s link failed. Error Log:\", device_param->device_id + 1, source_file);\n        event_log_error (hashcat_ctx, \"%s\", jit_error_log);\n        event_log_error (hashcat_ctx, NULL);\n\n        return false;\n      }\n\n      void *cubin = NULL;\n\n      size_t cubin_size = 0;\n\n      if (hc_cuLinkComplete (hashcat_ctx, state, &cubin, &cubin_size) == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s link failed. Error Log:\", device_param->device_id + 1, source_file);\n        event_log_error (hashcat_ctx, \"%s\", jit_error_log);\n        event_log_error (hashcat_ctx, NULL);\n\n        return false;\n      }\n\n      #if defined (DEBUG)\n      event_log_info (hashcat_ctx, \"* Device #%u: Kernel %s link successful. Info Log:\", device_param->device_id + 1, source_file);\n      event_log_info (hashcat_ctx, \"%s\", jit_info_log);\n      event_log_info (hashcat_ctx, NULL);\n      #endif\n\n      if (hc_cuModuleLoadDataEx (hashcat_ctx, cuda_module, cubin, mod_cnt, mod_opts, mod_vals) == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s load failed. Error Log:\", device_param->device_id + 1, source_file);\n        event_log_error (hashcat_ctx, \"%s\", mod_error_log);\n        event_log_error (hashcat_ctx, NULL);\n\n        return false;\n      }\n\n      #if defined (DEBUG)\n      event_log_info (hashcat_ctx, \"* Device #%u: Kernel %s load successful. Info Log:\", device_param->device_id + 1, source_file);\n      event_log_info (hashcat_ctx, \"%s\", mod_info_log);\n      event_log_info (hashcat_ctx, NULL);\n      #endif\n\n      if (cache_disable == false)\n      {\n        if (write_kernel_binary (hashcat_ctx, cached_file, cubin, cubin_size) == false) return false;\n      }\n\n      if (hc_cuLinkDestroy (hashcat_ctx, state) == -1) return false;\n\n      hcfree (jit_info_log);\n      hcfree (jit_error_log);\n\n      #else\n\n      if (hc_cuModuleLoadDataEx (hashcat_ctx, cuda_module, binary, mod_cnt, mod_opts, mod_vals) == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s load failed. Error Log:\", device_param->device_id + 1, source_file);\n        event_log_error (hashcat_ctx, \"%s\", mod_error_log);\n        event_log_error (hashcat_ctx, NULL);\n\n        return false;\n      }\n\n      #if defined (DEBUG)\n      event_log_info (hashcat_ctx, \"* Device #%u: Kernel %s load successful. Info Log:\", device_param->device_id + 1, source_file);\n      event_log_info (hashcat_ctx, \"%s\", mod_info_log);\n      event_log_info (hashcat_ctx, NULL);\n      #endif\n\n      if (cache_disable == false)\n      {\n        if (write_kernel_binary (hashcat_ctx, cached_file, binary, binary_size) == false) return false;\n      }\n\n      #endif\n\n      hcfree (mod_info_log);\n      hcfree (mod_error_log);\n\n      hcfree (binary);\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (hc_clCreateProgramWithSource (hashcat_ctx, device_param->opencl_context, 1, (const char **) kernel_sources, NULL, opencl_program) == -1) return false;\n\n      const int CL_rc = hc_clBuildProgram (hashcat_ctx, *opencl_program, 1, &device_param->opencl_device, build_options_buf, NULL, NULL);\n\n      //if (CL_rc == -1) return -1;\n\n      size_t build_log_size = 0;\n\n      hc_clGetProgramBuildInfo (hashcat_ctx, *opencl_program, device_param->opencl_device, CL_PROGRAM_BUILD_LOG, 0, NULL, &build_log_size);\n\n      //if (CL_rc == -1) return -1;\n\n      #if defined (DEBUG)\n      if ((build_log_size > 1) || (CL_rc == -1))\n      #else\n      if (CL_rc == -1)\n      #endif\n      {\n        char *build_log = (char *) hcmalloc (build_log_size + 1);\n\n        const int rc_clGetProgramBuildInfo = hc_clGetProgramBuildInfo (hashcat_ctx, *opencl_program, device_param->opencl_device, CL_PROGRAM_BUILD_LOG, build_log_size, build_log, NULL);\n\n        if (rc_clGetProgramBuildInfo == -1) return false;\n\n        puts (build_log);\n\n        hcfree (build_log);\n      }\n\n      if (CL_rc == -1) return false;\n\n      if (cache_disable == false)\n      {\n        size_t binary_size;\n\n        if (hc_clGetProgramInfo (hashcat_ctx, *opencl_program, CL_PROGRAM_BINARY_SIZES, sizeof (size_t), &binary_size, NULL) == -1) return false;\n\n        char *binary = (char *) hcmalloc (binary_size);\n\n        if (hc_clGetProgramInfo (hashcat_ctx, *opencl_program, CL_PROGRAM_BINARIES, sizeof (char *), &binary, NULL) == -1) return false;\n\n        if (write_kernel_binary (hashcat_ctx, cached_file, binary, binary_size) == false) return false;\n\n        hcfree (binary);\n      }\n    }\n  }\n  else\n  {\n    if (read_kernel_binary (hashcat_ctx, cached_file, kernel_lengths, kernel_sources) == false) return false;\n\n    if (device_param->is_cuda == true)\n    {\n      #define LOG_SIZE 8192\n\n      char *mod_info_log  = (char *) hcmalloc (LOG_SIZE + 1);\n      char *mod_error_log = (char *) hcmalloc (LOG_SIZE + 1);\n\n      int mod_cnt = 6;\n\n      CUjit_option mod_opts[7];\n      void *mod_vals[7];\n\n      mod_opts[0] = CU_JIT_TARGET_FROM_CUCONTEXT;\n      mod_vals[0] = (void *) 0;\n\n      mod_opts[1] = CU_JIT_LOG_VERBOSE;\n      mod_vals[1] = (void *) 1;\n\n      mod_opts[2] = CU_JIT_INFO_LOG_BUFFER;\n      mod_vals[2] = (void *) mod_info_log;\n\n      mod_opts[3] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;\n      mod_vals[3] = (void *) LOG_SIZE;\n\n      mod_opts[4] = CU_JIT_ERROR_LOG_BUFFER;\n      mod_vals[4] = (void *) mod_error_log;\n\n      mod_opts[5] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;\n      mod_vals[5] = (void *) LOG_SIZE;\n\n      if (hashconfig->opti_type & OPTI_TYPE_REGISTER_LIMIT)\n      {\n        mod_opts[6] = CU_JIT_MAX_REGISTERS;\n        mod_vals[6] = (void *) 128;\n\n        mod_cnt++;\n      }\n\n      if (hc_cuModuleLoadDataEx (hashcat_ctx, cuda_module, kernel_sources[0], mod_cnt, mod_opts, mod_vals) == -1)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s load failed. Error Log:\", device_param->device_id + 1, source_file);\n        event_log_error (hashcat_ctx, \"%s\", mod_error_log);\n        event_log_error (hashcat_ctx, NULL);\n\n        return false;\n      }\n\n      #if defined (DEBUG)\n      event_log_info (hashcat_ctx, \"* Device #%u: Kernel %s load successful. Info Log:\", device_param->device_id + 1, source_file);\n      event_log_info (hashcat_ctx, \"%s\", mod_info_log);\n      event_log_info (hashcat_ctx, NULL);\n      #endif\n\n      hcfree (mod_info_log);\n      hcfree (mod_error_log);\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (hc_clCreateProgramWithBinary (hashcat_ctx, device_param->opencl_context, 1, &device_param->opencl_device, kernel_lengths, (const unsigned char **) kernel_sources, NULL, opencl_program) == -1) return false;\n\n      if (hc_clBuildProgram (hashcat_ctx, *opencl_program, 1, &device_param->opencl_device, build_options_buf, NULL, NULL) == -1) return false;\n    }\n  }\n\n  hcfree (kernel_sources[0]);\n\n  return true;\n}\n\nint backend_session_begin (hashcat_ctx_t *hashcat_ctx)\n{\n  const bitmap_ctx_t         *bitmap_ctx          = hashcat_ctx->bitmap_ctx;\n  const folder_config_t      *folder_config       = hashcat_ctx->folder_config;\n  const hashconfig_t         *hashconfig          = hashcat_ctx->hashconfig;\n  const hashes_t             *hashes              = hashcat_ctx->hashes;\n  const module_ctx_t         *module_ctx          = hashcat_ctx->module_ctx;\n        backend_ctx_t        *backend_ctx         = hashcat_ctx->backend_ctx;\n  const straight_ctx_t       *straight_ctx        = hashcat_ctx->straight_ctx;\n  const user_options_extra_t *user_options_extra  = hashcat_ctx->user_options_extra;\n  const user_options_t       *user_options        = hashcat_ctx->user_options;\n\n  if (backend_ctx->enabled == false) return 0;\n\n  u64 size_total_host_all = 0;\n\n  u32 hardware_power_all = 0;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    /**\n     * host buffer\n     */\n\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    EVENT_DATA (EVENT_BACKEND_DEVICE_INIT_PRE, &backend_devices_idx, sizeof (int));\n\n    const int device_id = device_param->device_id;\n\n    /**\n     * module depending checks\n     */\n\n    device_param->skipped_warning = false;\n\n    if (module_ctx->module_unstable_warning != MODULE_DEFAULT)\n    {\n      const bool unstable_warning = module_ctx->module_unstable_warning (hashconfig, user_options, user_options_extra, device_param);\n\n      if ((unstable_warning == true) && (user_options->force == false))\n      {\n        event_log_warning (hashcat_ctx, \"* Device #%u: Skipping hash-mode %u - known CUDA/OpenCL Runtime/Driver issue (not a hashcat issue)\", device_id + 1, hashconfig->hash_mode);\n        event_log_warning (hashcat_ctx, \"             You can use --force to override, but do not report related errors.\");\n\n        device_param->skipped_warning = true;\n\n        continue;\n      }\n    }\n\n    // vector_width\n\n    int vector_width = 0;\n\n    if (user_options->backend_vector_width_chgd == false)\n    {\n      // tuning db\n\n      tuning_db_entry_t *tuningdb_entry;\n\n      if (user_options->slow_candidates == true)\n      {\n        tuningdb_entry = tuning_db_search (hashcat_ctx, device_param->device_name, device_param->opencl_device_type, 0, hashconfig->hash_mode);\n      }\n      else\n      {\n        tuningdb_entry = tuning_db_search (hashcat_ctx, device_param->device_name, device_param->opencl_device_type, user_options->attack_mode, hashconfig->hash_mode);\n      }\n\n      if (tuningdb_entry == NULL || tuningdb_entry->vector_width == -1)\n      {\n        if (hashconfig->opti_type & OPTI_TYPE_USES_BITS_64)\n        {\n          if (device_param->is_cuda == true)\n          {\n            // cuda does not support this query\n\n            vector_width = 1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_NATIVE_VECTOR_WIDTH_LONG, sizeof (vector_width), &vector_width, NULL) == -1) return -1;\n          }\n        }\n        else\n        {\n          if (device_param->is_cuda == true)\n          {\n            // cuda does not support this query\n\n            vector_width = 1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            if (hc_clGetDeviceInfo (hashcat_ctx, device_param->opencl_device, CL_DEVICE_NATIVE_VECTOR_WIDTH_INT,  sizeof (vector_width), &vector_width, NULL) == -1) return -1;\n          }\n        }\n      }\n      else\n      {\n        vector_width = (cl_uint) tuningdb_entry->vector_width;\n      }\n    }\n    else\n    {\n      vector_width = user_options->backend_vector_width;\n    }\n\n    // We can't have SIMD in kernels where we have an unknown final password length\n    // It also turns out that pure kernels (that have a higher register pressure)\n    // actually run faster on scalar GPU (like 1080) without SIMD\n\n    if ((hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL) == 0)\n    {\n      if (device_param->opencl_device_type & CL_DEVICE_TYPE_GPU)\n      {\n        vector_width = 1;\n      }\n    }\n\n    if (vector_width > 16) vector_width = 16;\n\n    device_param->vector_width = vector_width;\n\n    /**\n     * kernel accel and loops tuning db adjustment\n     */\n\n    device_param->kernel_accel_min   = hashconfig->kernel_accel_min;\n    device_param->kernel_accel_max   = hashconfig->kernel_accel_max;\n    device_param->kernel_loops_min   = hashconfig->kernel_loops_min;\n    device_param->kernel_loops_max   = hashconfig->kernel_loops_max;\n    device_param->kernel_threads_min = hashconfig->kernel_threads_min;\n    device_param->kernel_threads_max = hashconfig->kernel_threads_max;\n\n    tuning_db_entry_t *tuningdb_entry = NULL;\n\n    if (user_options->slow_candidates == true)\n    {\n      tuningdb_entry = tuning_db_search (hashcat_ctx, device_param->device_name, device_param->opencl_device_type, 0, hashconfig->hash_mode);\n    }\n    else\n    {\n      tuningdb_entry = tuning_db_search (hashcat_ctx, device_param->device_name, device_param->opencl_device_type, user_options->attack_mode, hashconfig->hash_mode);\n    }\n\n    // user commandline option override tuning db\n    // but both have to stay inside the boundaries of the module\n\n    if (user_options->kernel_accel_chgd == true)\n    {\n      const u32 _kernel_accel = user_options->kernel_accel;\n\n      if ((_kernel_accel >= device_param->kernel_accel_min) && (_kernel_accel <= device_param->kernel_accel_max))\n      {\n        device_param->kernel_accel_min = _kernel_accel;\n        device_param->kernel_accel_max = _kernel_accel;\n      }\n    }\n    else\n    {\n      if (tuningdb_entry != NULL)\n      {\n        const u32 _kernel_accel = tuningdb_entry->kernel_accel;\n\n        if (_kernel_accel)\n        {\n          if ((_kernel_accel >= device_param->kernel_accel_min) && (_kernel_accel <= device_param->kernel_accel_max))\n          {\n            device_param->kernel_accel_min = _kernel_accel;\n            device_param->kernel_accel_max = _kernel_accel;\n          }\n        }\n      }\n    }\n\n    if (user_options->kernel_loops_chgd == true)\n    {\n      const u32 _kernel_loops = user_options->kernel_loops;\n\n      if ((_kernel_loops >= device_param->kernel_loops_min) && (_kernel_loops <= device_param->kernel_loops_max))\n      {\n        device_param->kernel_loops_min = _kernel_loops;\n        device_param->kernel_loops_max = _kernel_loops;\n      }\n    }\n    else\n    {\n      if (tuningdb_entry != NULL)\n      {\n        u32 _kernel_loops = tuningdb_entry->kernel_loops;\n\n        if (_kernel_loops)\n        {\n          if (user_options->workload_profile == 1)\n          {\n            _kernel_loops = (_kernel_loops > 8) ? _kernel_loops / 8 : 1;\n          }\n          else if (user_options->workload_profile == 2)\n          {\n            _kernel_loops = (_kernel_loops > 4) ? _kernel_loops / 4 : 1;\n          }\n\n          if ((_kernel_loops >= device_param->kernel_loops_min) && (_kernel_loops <= device_param->kernel_loops_max))\n          {\n            device_param->kernel_loops_min = _kernel_loops;\n            device_param->kernel_loops_max = _kernel_loops;\n          }\n        }\n      }\n    }\n\n    // there's no thread column in tuning db, stick to commandline if defined\n\n    if (user_options->kernel_threads_chgd == true)\n    {\n      const u32 _kernel_threads = user_options->kernel_threads;\n\n      if ((_kernel_threads >= device_param->kernel_threads_min) && (_kernel_threads <= device_param->kernel_threads_max))\n      {\n        device_param->kernel_threads_min = _kernel_threads;\n        device_param->kernel_threads_max = _kernel_threads;\n      }\n    }\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      // we have some absolute limits for fast hashes (because of limit constant memory), make sure not to overstep\n\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n        if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n        {\n          device_param->kernel_loops_min = MIN (device_param->kernel_loops_min, KERNEL_RULES);\n          device_param->kernel_loops_max = MIN (device_param->kernel_loops_max, KERNEL_RULES);\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n        {\n          device_param->kernel_loops_min = MIN (device_param->kernel_loops_min, KERNEL_COMBS);\n          device_param->kernel_loops_max = MIN (device_param->kernel_loops_max, KERNEL_COMBS);\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n        {\n          device_param->kernel_loops_min = MIN (device_param->kernel_loops_min, KERNEL_BFS);\n          device_param->kernel_loops_max = MIN (device_param->kernel_loops_max, KERNEL_BFS);\n        }\n      }\n    }\n\n    device_param->kernel_loops_min_sav = device_param->kernel_loops_min;\n    device_param->kernel_loops_max_sav = device_param->kernel_loops_max;\n\n    /**\n     * device properties\n     */\n\n    const u32 device_processors = device_param->device_processors;\n\n    /**\n     * create context for each device\n     */\n\n    if (device_param->is_cuda == true)\n    {\n      if (hc_cuCtxCreate (hashcat_ctx, &device_param->cuda_context, CU_CTX_SCHED_BLOCKING_SYNC, device_param->cuda_device) == -1) return -1;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      /*\n      cl_context_properties properties[3];\n\n      properties[0] = CL_CONTEXT_PLATFORM;\n      properties[1] = (cl_context_properties) device_param->opencl_platform;\n      properties[2] = 0;\n\n      CL_rc = hc_clCreateContext (hashcat_ctx, properties, 1, &device_param->opencl_device, NULL, NULL, &device_param->opencl_context);\n      */\n\n      if (hc_clCreateContext (hashcat_ctx, NULL, 1, &device_param->opencl_device, NULL, NULL, &device_param->opencl_context) == -1) return -1;\n\n      /**\n       * create command-queue\n       */\n\n      // not supported with NV\n      // device_param->opencl_command_queue = hc_clCreateCommandQueueWithProperties (hashcat_ctx, device_param->opencl_device, NULL);\n\n      if (hc_clCreateCommandQueue (hashcat_ctx, device_param->opencl_context, device_param->opencl_device, CL_QUEUE_PROFILING_ENABLE, &device_param->opencl_command_queue) == -1) return -1;\n    }\n\n    /**\n     * create stream for CUDA devices\n     */\n\n    if (device_param->is_cuda == true)\n    {\n      if (hc_cuStreamCreate (hashcat_ctx, &device_param->cuda_stream, CU_STREAM_DEFAULT) == -1) return -1;\n    }\n\n    /**\n     * create events for CUDA devices\n     */\n\n    if (device_param->is_cuda == true)\n    {\n      if (hc_cuEventCreate (hashcat_ctx, &device_param->cuda_event1, CU_EVENT_DEFAULT) == -1) return -1;\n\n      if (hc_cuEventCreate (hashcat_ctx, &device_param->cuda_event2, CU_EVENT_DEFAULT) == -1) return -1;\n    }\n\n    /**\n     * create input buffers on device : calculate size of fixed memory buffers\n     */\n\n    u64 size_root_css   = SP_PW_MAX *           sizeof (cs_t);\n    u64 size_markov_css = SP_PW_MAX * CHARSIZ * sizeof (cs_t);\n\n    device_param->size_root_css   = size_root_css;\n    device_param->size_markov_css = size_markov_css;\n\n    u64 size_results = sizeof (u32);\n\n    device_param->size_results = size_results;\n\n    u64 size_rules   = (u64) straight_ctx->kernel_rules_cnt * sizeof (kernel_rule_t);\n    u64 size_rules_c = (u64) KERNEL_RULES                   * sizeof (kernel_rule_t);\n\n    device_param->size_rules    = size_rules;\n    device_param->size_rules_c  = size_rules_c;\n\n    u64 size_plains  = (u64) hashes->digests_cnt * sizeof (plain_t);\n    u64 size_salts   = (u64) hashes->salts_cnt   * sizeof (salt_t);\n    u64 size_esalts  = (u64) hashes->digests_cnt * hashconfig->esalt_size;\n    u64 size_shown   = (u64) hashes->digests_cnt * sizeof (u32);\n    u64 size_digests = (u64) hashes->digests_cnt * (u64) hashconfig->dgst_size;\n\n    device_param->size_plains   = size_plains;\n    device_param->size_digests  = size_digests;\n    device_param->size_shown    = size_shown;\n    device_param->size_salts    = size_salts;\n    device_param->size_esalts   = size_esalts;\n\n    u64 size_combs = KERNEL_COMBS * sizeof (pw_t);\n    u64 size_bfs   = KERNEL_BFS   * sizeof (bf_t);\n    u64 size_tm    = 32           * sizeof (bs_word_t);\n\n    device_param->size_bfs      = size_bfs;\n    device_param->size_combs    = size_combs;\n    device_param->size_tm       = size_tm;\n\n    u64 size_st_digests = 1 * hashconfig->dgst_size;\n    u64 size_st_salts   = 1 * sizeof (salt_t);\n    u64 size_st_esalts  = 1 * hashconfig->esalt_size;\n\n    device_param->size_st_digests = size_st_digests;\n    device_param->size_st_salts   = size_st_salts;\n    device_param->size_st_esalts  = size_st_esalts;\n\n    u64 size_extra_buffer = 4;\n\n    if (module_ctx->module_extra_buffer_size != MODULE_DEFAULT)\n    {\n      const u64 extra_buffer_size = module_ctx->module_extra_buffer_size (hashconfig, user_options, user_options_extra, hashes, device_param);\n\n      if (extra_buffer_size == (u64) -1)\n      {\n        event_log_error (hashcat_ctx, \"Invalid extra buffer size.\");\n\n        return -1;\n      }\n\n      device_param->extra_buffer_size = extra_buffer_size;\n\n      size_extra_buffer = extra_buffer_size;\n    }\n\n    // kern type\n\n    u32 kern_type = hashconfig->kern_type;\n\n    if (module_ctx->module_kern_type_dynamic != MODULE_DEFAULT)\n    {\n      if (user_options->benchmark == true)\n      {\n      }\n      else\n      {\n        void        *digests_buf    = hashes->digests_buf;\n        salt_t      *salts_buf      = hashes->salts_buf;\n        void        *esalts_buf     = hashes->esalts_buf;\n        void        *hook_salts_buf = hashes->hook_salts_buf;\n        hashinfo_t **hash_info      = hashes->hash_info;\n\n        hashinfo_t *hash_info_ptr = NULL;\n\n        if (hash_info) hash_info_ptr = hash_info[0];\n\n        kern_type = (u32) module_ctx->module_kern_type_dynamic (hashconfig, digests_buf, salts_buf, esalts_buf, hook_salts_buf, hash_info_ptr);\n      }\n    }\n\n    // built options\n\n    const size_t build_options_sz = 4096;\n\n    char *build_options_buf = (char *) hcmalloc (build_options_sz);\n\n    int build_options_len = 0;\n\n    #if defined (_WIN)\n    build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-D KERNEL_STATIC -I OpenCL -I \\\"%s\\\" \", folder_config->cpath_real);\n    #else\n    build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-D KERNEL_STATIC -I OpenCL -I %s \", folder_config->cpath_real);\n    #endif\n\n    /* currently disabled, hangs NEO drivers since 20.09.\n       was required for NEO driver 20.08 to workaround the same issue!\n       we go with the latest version\n\n    if (device_param->is_opencl == true)\n    {\n      if (device_param->use_opencl12 == true)\n      {\n        build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-cl-std=CL1.2 \");\n      }\n      else if (device_param->use_opencl20 == true)\n      {\n        build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-cl-std=CL2.0 \");\n      }\n      else if (device_param->use_opencl21 == true)\n      {\n        build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-cl-std=CL2.1 \");\n      }\n    }\n    */\n\n    // we don't have sm_* on vendors not NV but it doesn't matter\n\n    #if defined (DEBUG)\n    build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-D LOCAL_MEM_TYPE=%d -D VENDOR_ID=%u -D CUDA_ARCH=%u -D HAS_ADD=%u -D HAS_ADDC=%u -D HAS_SUB=%u -D HAS_SUBC=%u -D HAS_VADD=%u -D HAS_VADDC=%u -D HAS_VADD_CO=%u -D HAS_VADDC_CO=%u -D HAS_VSUB=%u -D HAS_VSUBB=%u -D HAS_VSUB_CO=%u -D HAS_VSUBB_CO=%u -D HAS_VPERM=%u -D HAS_VADD3=%u -D HAS_VBFE=%u -D HAS_BFE=%u -D HAS_LOP3=%u -D HAS_MOV64=%u -D HAS_PRMT=%u -D VECT_SIZE=%d -D DEVICE_TYPE=%u -D DGST_R0=%u -D DGST_R1=%u -D DGST_R2=%u -D DGST_R3=%u -D DGST_ELEM=%u -D KERN_TYPE=%u -D ATTACK_EXEC=%u -D ATTACK_KERN=%u \", device_param->device_local_mem_type, device_param->opencl_platform_vendor_id, (device_param->sm_major * 100) + (device_param->sm_minor * 10), device_param->has_add, device_param->has_addc, device_param->has_sub, device_param->has_subc, device_param->has_vadd, device_param->has_vaddc, device_param->has_vadd_co, device_param->has_vaddc_co, device_param->has_vsub, device_param->has_vsubb, device_param->has_vsub_co, device_param->has_vsubb_co, device_param->has_vperm, device_param->has_vadd3, device_param->has_vbfe, device_param->has_bfe, device_param->has_lop3, device_param->has_mov64, device_param->has_prmt, device_param->vector_width, (u32) device_param->opencl_device_type, hashconfig->dgst_pos0, hashconfig->dgst_pos1, hashconfig->dgst_pos2, hashconfig->dgst_pos3, hashconfig->dgst_size / 4, kern_type, hashconfig->attack_exec, user_options_extra->attack_kern);\n    #else\n    build_options_len += snprintf (build_options_buf + build_options_len, build_options_sz - build_options_len, \"-D LOCAL_MEM_TYPE=%d -D VENDOR_ID=%u -D CUDA_ARCH=%u -D HAS_ADD=%u -D HAS_ADDC=%u -D HAS_SUB=%u -D HAS_SUBC=%u -D HAS_VADD=%u -D HAS_VADDC=%u -D HAS_VADD_CO=%u -D HAS_VADDC_CO=%u -D HAS_VSUB=%u -D HAS_VSUBB=%u -D HAS_VSUB_CO=%u -D HAS_VSUBB_CO=%u -D HAS_VPERM=%u -D HAS_VADD3=%u -D HAS_VBFE=%u -D HAS_BFE=%u -D HAS_LOP3=%u -D HAS_MOV64=%u -D HAS_PRMT=%u -D VECT_SIZE=%d -D DEVICE_TYPE=%u -D DGST_R0=%u -D DGST_R1=%u -D DGST_R2=%u -D DGST_R3=%u -D DGST_ELEM=%u -D KERN_TYPE=%u -D ATTACK_EXEC=%u -D ATTACK_KERN=%u -w \", device_param->device_local_mem_type, device_param->opencl_platform_vendor_id, (device_param->sm_major * 100) + (device_param->sm_minor * 10), device_param->has_add, device_param->has_addc, device_param->has_sub, device_param->has_subc, device_param->has_vadd, device_param->has_vaddc, device_param->has_vadd_co, device_param->has_vaddc_co, device_param->has_vsub, device_param->has_vsubb, device_param->has_vsub_co, device_param->has_vsubb_co, device_param->has_vperm, device_param->has_vadd3, device_param->has_vbfe, device_param->has_bfe, device_param->has_lop3, device_param->has_mov64, device_param->has_prmt, device_param->vector_width, (u32) device_param->opencl_device_type, hashconfig->dgst_pos0, hashconfig->dgst_pos1, hashconfig->dgst_pos2, hashconfig->dgst_pos3, hashconfig->dgst_size / 4, kern_type, hashconfig->attack_exec, user_options_extra->attack_kern);\n    #endif\n\n    build_options_buf[build_options_len] = 0;\n\n    /*\n    if (device_param->opencl_device_type & CL_DEVICE_TYPE_CPU)\n    {\n      if (device_param->opencl_platform_vendor_id == VENDOR_ID_INTEL_SDK)\n      {\n        strncat (build_options_buf, \" -cl-opt-disable\", 16);\n      }\n    }\n    */\n\n    #if defined (DEBUG)\n    if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: build_options '%s'\", device_id + 1, build_options_buf);\n    #endif\n\n    /**\n     * device_name_chksum\n     */\n\n    char *device_name_chksum        = (char *) hcmalloc (HCBUFSIZ_TINY);\n    char *device_name_chksum_amp_mp = (char *) hcmalloc (HCBUFSIZ_TINY);\n\n    const size_t dnclen = snprintf (device_name_chksum, HCBUFSIZ_TINY, \"%d-%d-%d-%u-%s-%s-%s-%d-%u\",\n      backend_ctx->comptime,\n      backend_ctx->cuda_driver_version,\n      device_param->is_opencl,\n      device_param->opencl_platform_vendor_id,\n      device_param->device_name,\n      device_param->opencl_device_version,\n      device_param->opencl_driver_version,\n      device_param->vector_width,\n      hashconfig->kern_type);\n\n    const size_t dnclen_amp_mp = snprintf (device_name_chksum_amp_mp, HCBUFSIZ_TINY, \"%d-%d-%d-%u-%s-%s-%s\",\n      backend_ctx->comptime,\n      backend_ctx->cuda_driver_version,\n      device_param->is_opencl,\n      device_param->opencl_platform_vendor_id,\n      device_param->device_name,\n      device_param->opencl_device_version,\n      device_param->opencl_driver_version);\n\n    md5_ctx_t md5_ctx;\n\n    md5_init   (&md5_ctx);\n    md5_update (&md5_ctx, (u32 *) device_name_chksum, dnclen);\n    md5_final  (&md5_ctx);\n\n    snprintf (device_name_chksum, HCBUFSIZ_TINY, \"%08x\", md5_ctx.h[0]);\n\n    md5_init   (&md5_ctx);\n    md5_update (&md5_ctx, (u32 *) device_name_chksum_amp_mp, dnclen_amp_mp);\n    md5_final  (&md5_ctx);\n\n    snprintf (device_name_chksum_amp_mp, HCBUFSIZ_TINY, \"%08x\", md5_ctx.h[0]);\n\n    /**\n     * kernel cache\n     */\n\n    bool cache_disable = false;\n\n    // Seems to be completely broken on Apple + (Intel?) CPU\n    // To reproduce set cache_disable to false and run benchmark -b\n\n    if (device_param->opencl_platform_vendor_id == VENDOR_ID_APPLE)\n    {\n      if (device_param->opencl_device_type & CL_DEVICE_TYPE_CPU)\n      {\n        cache_disable = true;\n      }\n    }\n\n    if (module_ctx->module_jit_cache_disable != MODULE_DEFAULT)\n    {\n      cache_disable = module_ctx->module_jit_cache_disable (hashconfig, user_options, user_options_extra, hashes, device_param);\n    }\n\n    /**\n     * shared kernel with no hashconfig dependencies\n     */\n\n    {\n      /**\n       * kernel shared source filename\n       */\n\n      char source_file[256] = { 0 };\n\n      generate_source_kernel_shared_filename (folder_config->shared_dir, source_file);\n\n      if (hc_path_read (source_file) == false)\n      {\n        event_log_error (hashcat_ctx, \"%s: %s\", source_file, strerror (errno));\n\n        return -1;\n      }\n\n      /**\n       * kernel shared cached filename\n       */\n\n      char cached_file[256] = { 0 };\n\n      generate_cached_kernel_shared_filename (folder_config->profile_dir, device_name_chksum_amp_mp, cached_file);\n\n      const bool rc_load_kernel = load_kernel (hashcat_ctx, device_param, \"shared_kernel\", source_file, cached_file, build_options_buf, cache_disable, &device_param->opencl_program_shared, &device_param->cuda_module_shared);\n\n      if (rc_load_kernel == false)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s build failed.\", device_param->device_id + 1, source_file);\n\n        return -1;\n      }\n\n      if (device_param->is_cuda == true)\n      {\n        // GPU memset\n\n        if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_memset, device_param->cuda_module_shared, \"gpu_memset\") == -1) return -1;\n\n        if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_memset, &device_param->kernel_wgs_memset) == -1) return -1;\n\n        if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_memset, &device_param->kernel_local_mem_size_memset) == -1) return -1;\n\n        if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_memset, &device_param->kernel_dynamic_local_mem_size_memset) == -1) return -1;\n\n        device_param->kernel_preferred_wgs_multiple_memset = device_param->cuda_warp_size;\n\n        //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_memset, 0, sizeof (cl_mem),   device_param->kernel_params_memset[0]); if (CL_rc == -1) return -1;\n        //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_memset, 1, sizeof (cl_uint),  device_param->kernel_params_memset[1]); if (CL_rc == -1) return -1;\n        //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_memset, 2, sizeof (cl_ulong), device_param->kernel_params_memset[2]); if (CL_rc == -1) return -1;\n\n        // GPU autotune init\n\n        if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_atinit, device_param->cuda_module_shared, \"gpu_atinit\") == -1) return -1;\n\n        if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_atinit, &device_param->kernel_wgs_atinit) == -1) return -1;\n\n        if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_atinit, &device_param->kernel_local_mem_size_atinit) == -1) return -1;\n\n        if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_atinit, &device_param->kernel_dynamic_local_mem_size_atinit) == -1) return -1;\n\n        device_param->kernel_preferred_wgs_multiple_atinit = device_param->cuda_warp_size;\n\n        // CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_atinit, 0, sizeof (cl_mem),   device_param->kernel_params_atinit[0]); if (CL_rc == -1) return -1;\n        // CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_atinit, 1, sizeof (cl_ulong), device_param->kernel_params_atinit[1]); if (CL_rc == -1) return -1;\n\n        // GPU decompress\n\n        if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_decompress, device_param->cuda_module_shared, \"gpu_decompress\") == -1) return -1;\n\n        if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_decompress, &device_param->kernel_wgs_decompress) == -1) return -1;\n\n        if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_decompress, &device_param->kernel_local_mem_size_decompress) == -1) return -1;\n\n        if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_decompress, &device_param->kernel_dynamic_local_mem_size_decompress) == -1) return -1;\n\n        device_param->kernel_preferred_wgs_multiple_decompress = device_param->cuda_warp_size;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        // GPU memset\n\n        if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_shared, \"gpu_memset\", &device_param->opencl_kernel_memset) == -1) return -1;\n\n        if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_memset, &device_param->kernel_wgs_memset) == -1) return -1;\n\n        if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_memset, &device_param->kernel_local_mem_size_memset) == -1) return -1;\n\n        if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_memset, &device_param->kernel_dynamic_local_mem_size_memset) == -1) return -1;\n\n        if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_memset, &device_param->kernel_preferred_wgs_multiple_memset) == -1) return -1;\n\n        // GPU autotune init\n\n        if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_shared, \"gpu_atinit\", &device_param->opencl_kernel_atinit) == -1) return -1;\n\n        if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_atinit, &device_param->kernel_wgs_atinit) == -1) return -1;\n\n        if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_atinit, &device_param->kernel_local_mem_size_atinit) == -1) return -1;\n\n        if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_atinit, &device_param->kernel_dynamic_local_mem_size_atinit) == -1) return -1;\n\n        if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_atinit, &device_param->kernel_preferred_wgs_multiple_atinit) == -1) return -1;\n\n        // GPU decompress\n\n        if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_shared, \"gpu_decompress\", &device_param->opencl_kernel_decompress) == -1) return -1;\n\n        if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_decompress, &device_param->kernel_wgs_decompress) == -1) return -1;\n\n        if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_decompress, &device_param->kernel_local_mem_size_decompress) == -1) return -1;\n\n        if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_decompress, &device_param->kernel_dynamic_local_mem_size_decompress) == -1) return -1;\n\n        if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_decompress, &device_param->kernel_preferred_wgs_multiple_decompress) == -1) return -1;\n      }\n    }\n\n    /**\n     * main kernel\n     */\n\n    {\n      char *build_options_module_buf = (char *) hcmalloc (build_options_sz);\n\n      int build_options_module_len = 0;\n\n      build_options_module_len += snprintf (build_options_module_buf + build_options_module_len, build_options_sz - build_options_module_len, \"%s \", build_options_buf);\n\n      if (module_ctx->module_jit_build_options != MODULE_DEFAULT)\n      {\n        char *jit_build_options = module_ctx->module_jit_build_options (hashconfig, user_options, user_options_extra, hashes, device_param);\n\n        if (jit_build_options != NULL)\n        {\n          build_options_module_len += snprintf (build_options_module_buf + build_options_module_len, build_options_sz - build_options_module_len, \"%s\", jit_build_options);\n\n          // this is a bit ugly\n          // would be better to have the module return the value as value\n\n          u32 fixed_local_size = 0;\n\n          if (sscanf (jit_build_options, \"-D FIXED_LOCAL_SIZE=%u\", &fixed_local_size) == 1)\n          {\n            device_param->kernel_threads_min = fixed_local_size;\n            device_param->kernel_threads_max = fixed_local_size;\n          }\n        }\n      }\n\n      build_options_module_buf[build_options_module_len] = 0;\n\n      #if defined (DEBUG)\n      if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"* Device #%u: build_options_module '%s'\", device_id + 1, build_options_module_buf);\n      #endif\n\n      /**\n       * kernel source filename\n       */\n\n      char source_file[256] = { 0 };\n\n      generate_source_kernel_filename (user_options->slow_candidates, hashconfig->attack_exec, user_options_extra->attack_kern, kern_type, hashconfig->opti_type, folder_config->shared_dir, source_file);\n\n      if (hc_path_read (source_file) == false)\n      {\n        event_log_error (hashcat_ctx, \"%s: %s\", source_file, strerror (errno));\n\n        return -1;\n      }\n\n      /**\n       * kernel cached filename\n       */\n\n      char cached_file[256] = { 0 };\n\n      generate_cached_kernel_filename (user_options->slow_candidates, hashconfig->attack_exec, user_options_extra->attack_kern, kern_type, hashconfig->opti_type, folder_config->profile_dir, device_name_chksum, cached_file);\n\n      /**\n       * load kernel\n       */\n\n      const bool rc_load_kernel = load_kernel (hashcat_ctx, device_param, \"main_kernel\", source_file, cached_file, build_options_module_buf, cache_disable, &device_param->opencl_program, &device_param->cuda_module);\n\n      if (rc_load_kernel == false)\n      {\n        event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s build failed.\", device_param->device_id + 1, source_file);\n\n        return -1;\n      }\n\n      hcfree (build_options_module_buf);\n    }\n\n    /**\n     * word generator kernel\n     */\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      if (user_options->attack_mode != ATTACK_MODE_STRAIGHT)\n      {\n        /**\n         * kernel mp source filename\n         */\n\n        char source_file[256] = { 0 };\n\n        generate_source_kernel_mp_filename (hashconfig->opti_type, hashconfig->opts_type, folder_config->shared_dir, source_file);\n\n        if (hc_path_read (source_file) == false)\n        {\n          event_log_error (hashcat_ctx, \"%s: %s\", source_file, strerror (errno));\n\n          return -1;\n        }\n\n        /**\n         * kernel mp cached filename\n         */\n\n        char cached_file[256] = { 0 };\n\n        generate_cached_kernel_mp_filename (hashconfig->opti_type, hashconfig->opts_type, folder_config->profile_dir, device_name_chksum_amp_mp, cached_file);\n\n        const bool rc_load_kernel = load_kernel (hashcat_ctx, device_param, \"mp_kernel\", source_file, cached_file, build_options_buf, cache_disable, &device_param->opencl_program_mp, &device_param->cuda_module_mp);\n\n        if (rc_load_kernel == false)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s build failed.\", device_param->device_id + 1, source_file);\n\n          return -1;\n        }\n      }\n    }\n\n    /**\n     * amplifier kernel\n     */\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n\n      }\n      else\n      {\n        /**\n         * kernel amp source filename\n         */\n\n        char source_file[256] = { 0 };\n\n        generate_source_kernel_amp_filename (user_options_extra->attack_kern, folder_config->shared_dir, source_file);\n\n        if (hc_path_read (source_file) == false)\n        {\n          event_log_error (hashcat_ctx, \"%s: %s\", source_file, strerror (errno));\n\n          return -1;\n        }\n\n        /**\n         * kernel amp cached filename\n         */\n\n        char cached_file[256] = { 0 };\n\n        generate_cached_kernel_amp_filename (user_options_extra->attack_kern, folder_config->profile_dir, device_name_chksum_amp_mp, cached_file);\n\n        const bool rc_load_kernel = load_kernel (hashcat_ctx, device_param, \"amp_kernel\", source_file, cached_file, build_options_buf, cache_disable, &device_param->opencl_program_amp, &device_param->cuda_module_amp);\n\n        if (rc_load_kernel == false)\n        {\n          event_log_error (hashcat_ctx, \"* Device #%u: Kernel %s build failed.\", device_param->device_id + 1, source_file);\n\n          return -1;\n        }\n\n        hcfree (build_options_buf);\n      }\n    }\n\n    hcfree (device_name_chksum);\n    hcfree (device_name_chksum_amp_mp);\n\n    // some algorithm collide too fast, make that impossible\n\n    if (user_options->benchmark == true)\n    {\n      ((u32 *) hashes->digests_buf)[0] = -1U;\n      ((u32 *) hashes->digests_buf)[1] = -1U;\n      ((u32 *) hashes->digests_buf)[2] = -1U;\n      ((u32 *) hashes->digests_buf)[3] = -1U;\n    }\n\n    /**\n     * global buffers\n     */\n\n    const u64 size_total_fixed\n      = bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + bitmap_ctx->bitmap_size\n      + size_plains\n      + size_digests\n      + size_shown\n      + size_salts\n      + size_results\n      + size_extra_buffer\n      + size_st_digests\n      + size_st_salts\n      + size_st_esalts\n      + size_esalts\n      + size_markov_css\n      + size_root_css\n      + size_rules\n      + size_rules_c\n      + size_tm;\n\n    if (size_total_fixed > device_param->device_available_mem)\n    {\n      event_log_error (hashcat_ctx, \"* Device #%u: Not enough allocatable device memory for this hashlist and/or ruleset.\", device_id + 1);\n\n      return -1;\n    }\n\n    if (device_param->is_cuda == true)\n    {\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s1_a,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s1_b,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s1_c,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s1_d,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s2_a,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s2_b,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s2_c,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bitmap_s2_d,    bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_plain_bufs,     size_plains)             == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_digests_buf,    size_digests)            == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_digests_shown,  size_shown)              == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_salt_bufs,      size_salts)              == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_result,         size_results)            == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_extra0_buf,     size_extra_buffer / 4)   == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_extra1_buf,     size_extra_buffer / 4)   == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_extra2_buf,     size_extra_buffer / 4)   == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_extra3_buf,     size_extra_buffer / 4)   == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_st_digests_buf, size_st_digests)         == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_st_salts_buf,   size_st_salts)           == -1) return -1;\n\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s1_a, bitmap_ctx->bitmap_s1_a, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s1_b, bitmap_ctx->bitmap_s1_b, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s1_c, bitmap_ctx->bitmap_s1_c, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s1_d, bitmap_ctx->bitmap_s1_d, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s2_a, bitmap_ctx->bitmap_s2_a, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s2_b, bitmap_ctx->bitmap_s2_b, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s2_c, bitmap_ctx->bitmap_s2_c, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_bitmap_s2_d, bitmap_ctx->bitmap_s2_d, bitmap_ctx->bitmap_size) == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_digests_buf, hashes->digests_buf,     size_digests)            == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_salt_bufs,   hashes->salts_buf,       size_salts)              == -1) return -1;\n\n      /**\n       * special buffers\n       */\n\n      if (user_options->slow_candidates == true)\n      {\n        if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_rules_c, size_rules_c) == -1) return -1;\n      }\n      else\n      {\n        if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n        {\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_rules,   size_rules) == -1) return -1;\n\n          if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n          {\n            size_t dummy = 0;\n\n            if (hc_cuModuleGetGlobal (hashcat_ctx, &device_param->cuda_d_rules_c, &dummy, device_param->cuda_module, \"generic_constant\") == -1) return -1;\n          }\n          else\n          {\n            if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_rules_c, size_rules_c) == -1) return -1;\n          }\n\n          if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_rules, straight_ctx->kernel_rules_buf, size_rules) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n        {\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_combs,          size_combs)      == -1) return -1;\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_combs_c,        size_combs)      == -1) return -1;\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_root_css_buf,   size_root_css)   == -1) return -1;\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_markov_css_buf, size_markov_css) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n        {\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bfs,            size_bfs)        == -1) return -1;\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_root_css_buf,   size_root_css)   == -1) return -1;\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_markov_css_buf, size_markov_css) == -1) return -1;\n\n          if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n          {\n            size_t dummy = 0;\n\n            if (hc_cuModuleGetGlobal (hashcat_ctx, &device_param->cuda_d_bfs_c, &dummy, device_param->cuda_module, \"generic_constant\") == -1) return -1;\n\n            if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_tm_c,           size_tm)       == -1) return -1;\n          }\n          else\n          {\n            if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_bfs_c,          size_bfs)      == -1) return -1;\n            if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_tm_c,           size_tm)       == -1) return -1;\n          }\n        }\n      }\n\n      if (size_esalts)\n      {\n        if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_esalt_bufs, size_esalts) == -1) return -1;\n\n        if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_esalt_bufs, hashes->esalts_buf, size_esalts) == -1) return -1;\n      }\n\n      if (hashconfig->st_hash != NULL)\n      {\n        if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_st_digests_buf, hashes->st_digests_buf, size_st_digests) == -1) return -1;\n        if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_st_salts_buf,   hashes->st_salts_buf,   size_st_salts)   == -1) return -1;\n\n        if (size_esalts)\n        {\n          if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_st_esalts_buf, size_st_esalts) == -1) return -1;\n\n          if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_st_esalts_buf, hashes->st_esalts_buf, size_st_esalts) == -1) return -1;\n        }\n      }\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s1_a)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s1_b)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s1_c)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s1_d)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s2_a)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s2_b)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s2_c)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   bitmap_ctx->bitmap_size, NULL, &device_param->opencl_d_bitmap_s2_d)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_plains,             NULL, &device_param->opencl_d_plain_bufs)     == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   size_digests,            NULL, &device_param->opencl_d_digests_buf)    == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_shown,              NULL, &device_param->opencl_d_digests_shown)  == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   size_salts,              NULL, &device_param->opencl_d_salt_bufs)      == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_results,            NULL, &device_param->opencl_d_result)         == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_extra_buffer / 4,   NULL, &device_param->opencl_d_extra0_buf)     == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_extra_buffer / 4,   NULL, &device_param->opencl_d_extra1_buf)     == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_extra_buffer / 4,   NULL, &device_param->opencl_d_extra2_buf)     == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_extra_buffer / 4,   NULL, &device_param->opencl_d_extra3_buf)     == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   size_st_digests,         NULL, &device_param->opencl_d_st_digests_buf) == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   size_st_salts,           NULL, &device_param->opencl_d_st_salts_buf)   == -1) return -1;\n\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s1_a, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s1_a, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s1_b, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s1_b, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s1_c, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s1_c, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s1_d, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s1_d, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s2_a, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s2_a, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s2_b, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s2_b, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s2_c, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s2_c, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_bitmap_s2_d, CL_TRUE, 0, bitmap_ctx->bitmap_size, bitmap_ctx->bitmap_s2_d, 0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_digests_buf, CL_TRUE, 0, size_digests,            hashes->digests_buf,     0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_salt_bufs,   CL_TRUE, 0, size_salts,              hashes->salts_buf,       0, NULL, NULL) == -1) return -1;\n\n      /**\n       * special buffers\n       */\n\n      if (user_options->slow_candidates == true)\n      {\n        if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_rules_c, NULL, &device_param->opencl_d_rules_c)   == -1) return -1;\n      }\n      else\n      {\n        if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n        {\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_rules,   NULL, &device_param->opencl_d_rules)   == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_rules_c, NULL, &device_param->opencl_d_rules_c) == -1) return -1;\n\n          if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_rules, CL_TRUE, 0, size_rules, straight_ctx->kernel_rules_buf, 0, NULL, NULL) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n        {\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_combs,      NULL, &device_param->opencl_d_combs)          == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_combs,      NULL, &device_param->opencl_d_combs_c)        == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_root_css,   NULL, &device_param->opencl_d_root_css_buf)   == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_markov_css, NULL, &device_param->opencl_d_markov_css_buf) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n        {\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_bfs,        NULL, &device_param->opencl_d_bfs)            == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_bfs,        NULL, &device_param->opencl_d_bfs_c)          == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_tm,         NULL, &device_param->opencl_d_tm_c)           == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_root_css,   NULL, &device_param->opencl_d_root_css_buf)   == -1) return -1;\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_markov_css, NULL, &device_param->opencl_d_markov_css_buf) == -1) return -1;\n        }\n      }\n\n      if (size_esalts)\n      {\n        if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_esalts, NULL, &device_param->opencl_d_esalt_bufs) == -1) return -1;\n\n        if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_esalt_bufs, CL_TRUE, 0, size_esalts, hashes->esalts_buf, 0, NULL, NULL) == -1) return -1;\n      }\n\n      if (hashconfig->st_hash != NULL)\n      {\n        if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_st_digests_buf,  CL_TRUE, 0, size_st_digests,         hashes->st_digests_buf,  0, NULL, NULL) == -1) return -1;\n        if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_st_salts_buf,    CL_TRUE, 0, size_st_salts,           hashes->st_salts_buf,    0, NULL, NULL) == -1) return -1;\n\n        if (size_esalts)\n        {\n          if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY, size_st_esalts, NULL, &device_param->opencl_d_st_esalts_buf) == -1) return -1;\n\n          if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_st_esalts_buf, CL_TRUE, 0, size_st_esalts, hashes->st_esalts_buf, 0, NULL, NULL) == -1) return -1;\n        }\n      }\n    }\n\n    /**\n     * kernel args\n     */\n\n    device_param->kernel_params_buf32[24] = bitmap_ctx->bitmap_mask;\n    device_param->kernel_params_buf32[25] = bitmap_ctx->bitmap_shift1;\n    device_param->kernel_params_buf32[26] = bitmap_ctx->bitmap_shift2;\n    device_param->kernel_params_buf32[27] = 0; // salt_pos\n    device_param->kernel_params_buf32[28] = 0; // loop_pos\n    device_param->kernel_params_buf32[29] = 0; // loop_cnt\n    device_param->kernel_params_buf32[30] = 0; // kernel_rules_cnt\n    device_param->kernel_params_buf32[31] = 0; // digests_cnt\n    device_param->kernel_params_buf32[32] = 0; // digests_offset\n    device_param->kernel_params_buf32[33] = 0; // combs_mode\n    device_param->kernel_params_buf64[34] = 0; // gid_max\n\n    if (device_param->is_cuda == true)\n    {\n      device_param->kernel_params[ 0] = NULL; // &device_param->cuda_d_pws_buf;\n      device_param->kernel_params[ 1] = &device_param->cuda_d_rules_c;\n      device_param->kernel_params[ 2] = &device_param->cuda_d_combs_c;\n      device_param->kernel_params[ 3] = &device_param->cuda_d_bfs_c;\n      device_param->kernel_params[ 4] = NULL; // &device_param->cuda_d_tmps;\n      device_param->kernel_params[ 5] = NULL; // &device_param->cuda_d_hooks;\n      device_param->kernel_params[ 6] = &device_param->cuda_d_bitmap_s1_a;\n      device_param->kernel_params[ 7] = &device_param->cuda_d_bitmap_s1_b;\n      device_param->kernel_params[ 8] = &device_param->cuda_d_bitmap_s1_c;\n      device_param->kernel_params[ 9] = &device_param->cuda_d_bitmap_s1_d;\n      device_param->kernel_params[10] = &device_param->cuda_d_bitmap_s2_a;\n      device_param->kernel_params[11] = &device_param->cuda_d_bitmap_s2_b;\n      device_param->kernel_params[12] = &device_param->cuda_d_bitmap_s2_c;\n      device_param->kernel_params[13] = &device_param->cuda_d_bitmap_s2_d;\n      device_param->kernel_params[14] = &device_param->cuda_d_plain_bufs;\n      device_param->kernel_params[15] = &device_param->cuda_d_digests_buf;\n      device_param->kernel_params[16] = &device_param->cuda_d_digests_shown;\n      device_param->kernel_params[17] = &device_param->cuda_d_salt_bufs;\n      device_param->kernel_params[18] = &device_param->cuda_d_esalt_bufs;\n      device_param->kernel_params[19] = &device_param->cuda_d_result;\n      device_param->kernel_params[20] = &device_param->cuda_d_extra0_buf;\n      device_param->kernel_params[21] = &device_param->cuda_d_extra1_buf;\n      device_param->kernel_params[22] = &device_param->cuda_d_extra2_buf;\n      device_param->kernel_params[23] = &device_param->cuda_d_extra3_buf;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      device_param->kernel_params[ 0] = NULL; // &device_param->opencl_d_pws_buf;\n      device_param->kernel_params[ 1] = &device_param->opencl_d_rules_c;\n      device_param->kernel_params[ 2] = &device_param->opencl_d_combs_c;\n      device_param->kernel_params[ 3] = &device_param->opencl_d_bfs_c;\n      device_param->kernel_params[ 4] = NULL; // &device_param->opencl_d_tmps;\n      device_param->kernel_params[ 5] = NULL; // &device_param->opencl_d_hooks;\n      device_param->kernel_params[ 6] = &device_param->opencl_d_bitmap_s1_a;\n      device_param->kernel_params[ 7] = &device_param->opencl_d_bitmap_s1_b;\n      device_param->kernel_params[ 8] = &device_param->opencl_d_bitmap_s1_c;\n      device_param->kernel_params[ 9] = &device_param->opencl_d_bitmap_s1_d;\n      device_param->kernel_params[10] = &device_param->opencl_d_bitmap_s2_a;\n      device_param->kernel_params[11] = &device_param->opencl_d_bitmap_s2_b;\n      device_param->kernel_params[12] = &device_param->opencl_d_bitmap_s2_c;\n      device_param->kernel_params[13] = &device_param->opencl_d_bitmap_s2_d;\n      device_param->kernel_params[14] = &device_param->opencl_d_plain_bufs;\n      device_param->kernel_params[15] = &device_param->opencl_d_digests_buf;\n      device_param->kernel_params[16] = &device_param->opencl_d_digests_shown;\n      device_param->kernel_params[17] = &device_param->opencl_d_salt_bufs;\n      device_param->kernel_params[18] = &device_param->opencl_d_esalt_bufs;\n      device_param->kernel_params[19] = &device_param->opencl_d_result;\n      device_param->kernel_params[20] = &device_param->opencl_d_extra0_buf;\n      device_param->kernel_params[21] = &device_param->opencl_d_extra1_buf;\n      device_param->kernel_params[22] = &device_param->opencl_d_extra2_buf;\n      device_param->kernel_params[23] = &device_param->opencl_d_extra3_buf;\n    }\n\n    device_param->kernel_params[24] = &device_param->kernel_params_buf32[24];\n    device_param->kernel_params[25] = &device_param->kernel_params_buf32[25];\n    device_param->kernel_params[26] = &device_param->kernel_params_buf32[26];\n    device_param->kernel_params[27] = &device_param->kernel_params_buf32[27];\n    device_param->kernel_params[28] = &device_param->kernel_params_buf32[28];\n    device_param->kernel_params[29] = &device_param->kernel_params_buf32[29];\n    device_param->kernel_params[30] = &device_param->kernel_params_buf32[30];\n    device_param->kernel_params[31] = &device_param->kernel_params_buf32[31];\n    device_param->kernel_params[32] = &device_param->kernel_params_buf32[32];\n    device_param->kernel_params[33] = &device_param->kernel_params_buf32[33];\n    device_param->kernel_params[34] = &device_param->kernel_params_buf64[34];\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      device_param->kernel_params_mp_buf64[3] = 0;\n      device_param->kernel_params_mp_buf32[4] = 0;\n      device_param->kernel_params_mp_buf32[5] = 0;\n      device_param->kernel_params_mp_buf32[6] = 0;\n      device_param->kernel_params_mp_buf32[7] = 0;\n      device_param->kernel_params_mp_buf64[8] = 0;\n\n      if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n      {\n        if (device_param->is_cuda == true)\n        {\n          device_param->kernel_params_mp[0] = &device_param->cuda_d_combs;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          device_param->kernel_params_mp[0] = &device_param->opencl_d_combs;\n        }\n      }\n      else\n      {\n        if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n        {\n          if (device_param->is_cuda == true)\n          {\n            device_param->kernel_params_mp[0] = &device_param->cuda_d_combs;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            device_param->kernel_params_mp[0] = &device_param->opencl_d_combs;\n          }\n        }\n        else\n        {\n          device_param->kernel_params_mp[0] = NULL; // (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                                    // ? &device_param->opencl_d_pws_buf\n                                                    // : &device_param->opencl_d_pws_amp_buf;\n        }\n      }\n\n      if (device_param->is_cuda == true)\n      {\n        device_param->kernel_params_mp[1] = &device_param->cuda_d_root_css_buf;\n        device_param->kernel_params_mp[2] = &device_param->cuda_d_markov_css_buf;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        device_param->kernel_params_mp[1] = &device_param->opencl_d_root_css_buf;\n        device_param->kernel_params_mp[2] = &device_param->opencl_d_markov_css_buf;\n      }\n\n      device_param->kernel_params_mp[3] = &device_param->kernel_params_mp_buf64[3];\n      device_param->kernel_params_mp[4] = &device_param->kernel_params_mp_buf32[4];\n      device_param->kernel_params_mp[5] = &device_param->kernel_params_mp_buf32[5];\n      device_param->kernel_params_mp[6] = &device_param->kernel_params_mp_buf32[6];\n      device_param->kernel_params_mp[7] = &device_param->kernel_params_mp_buf32[7];\n      device_param->kernel_params_mp[8] = &device_param->kernel_params_mp_buf64[8];\n\n      device_param->kernel_params_mp_l_buf64[3] = 0;\n      device_param->kernel_params_mp_l_buf32[4] = 0;\n      device_param->kernel_params_mp_l_buf32[5] = 0;\n      device_param->kernel_params_mp_l_buf32[6] = 0;\n      device_param->kernel_params_mp_l_buf32[7] = 0;\n      device_param->kernel_params_mp_l_buf32[8] = 0;\n      device_param->kernel_params_mp_l_buf64[9] = 0;\n\n      device_param->kernel_params_mp_l[0] = NULL; // (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                                  // ? &device_param->opencl_d_pws_buf\n                                                  // : &device_param->opencl_d_pws_amp_buf;\n      if (device_param->is_cuda == true)\n      {\n        device_param->kernel_params_mp_l[1] = &device_param->cuda_d_root_css_buf;\n        device_param->kernel_params_mp_l[2] = &device_param->cuda_d_markov_css_buf;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        device_param->kernel_params_mp_l[1] = &device_param->opencl_d_root_css_buf;\n        device_param->kernel_params_mp_l[2] = &device_param->opencl_d_markov_css_buf;\n      }\n\n      device_param->kernel_params_mp_l[3] = &device_param->kernel_params_mp_l_buf64[3];\n      device_param->kernel_params_mp_l[4] = &device_param->kernel_params_mp_l_buf32[4];\n      device_param->kernel_params_mp_l[5] = &device_param->kernel_params_mp_l_buf32[5];\n      device_param->kernel_params_mp_l[6] = &device_param->kernel_params_mp_l_buf32[6];\n      device_param->kernel_params_mp_l[7] = &device_param->kernel_params_mp_l_buf32[7];\n      device_param->kernel_params_mp_l[8] = &device_param->kernel_params_mp_l_buf32[8];\n      device_param->kernel_params_mp_l[9] = &device_param->kernel_params_mp_l_buf64[9];\n\n      device_param->kernel_params_mp_r_buf64[3] = 0;\n      device_param->kernel_params_mp_r_buf32[4] = 0;\n      device_param->kernel_params_mp_r_buf32[5] = 0;\n      device_param->kernel_params_mp_r_buf32[6] = 0;\n      device_param->kernel_params_mp_r_buf32[7] = 0;\n      device_param->kernel_params_mp_r_buf64[8] = 0;\n\n      if (device_param->is_cuda == true)\n      {\n        device_param->kernel_params_mp_r[0] = &device_param->cuda_d_bfs;\n        device_param->kernel_params_mp_r[1] = &device_param->cuda_d_root_css_buf;\n        device_param->kernel_params_mp_r[2] = &device_param->cuda_d_markov_css_buf;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        device_param->kernel_params_mp_r[0] = &device_param->opencl_d_bfs;\n        device_param->kernel_params_mp_r[1] = &device_param->opencl_d_root_css_buf;\n        device_param->kernel_params_mp_r[2] = &device_param->opencl_d_markov_css_buf;\n      }\n\n      device_param->kernel_params_mp_r[3] = &device_param->kernel_params_mp_r_buf64[3];\n      device_param->kernel_params_mp_r[4] = &device_param->kernel_params_mp_r_buf32[4];\n      device_param->kernel_params_mp_r[5] = &device_param->kernel_params_mp_r_buf32[5];\n      device_param->kernel_params_mp_r[6] = &device_param->kernel_params_mp_r_buf32[6];\n      device_param->kernel_params_mp_r[7] = &device_param->kernel_params_mp_r_buf32[7];\n      device_param->kernel_params_mp_r[8] = &device_param->kernel_params_mp_r_buf64[8];\n\n      device_param->kernel_params_amp_buf32[5] = 0; // combs_mode\n      device_param->kernel_params_amp_buf64[6] = 0; // gid_max\n\n      if (device_param->is_cuda == true)\n      {\n        device_param->kernel_params_amp[0] = NULL; // &device_param->cuda_d_pws_buf;\n        device_param->kernel_params_amp[1] = NULL; // &device_param->cuda_d_pws_amp_buf;\n        device_param->kernel_params_amp[2] = &device_param->cuda_d_rules_c;\n        device_param->kernel_params_amp[3] = &device_param->cuda_d_combs_c;\n        device_param->kernel_params_amp[4] = &device_param->cuda_d_bfs_c;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        device_param->kernel_params_amp[0] = NULL; // &device_param->opencl_d_pws_buf;\n        device_param->kernel_params_amp[1] = NULL; // &device_param->opencl_d_pws_amp_buf;\n        device_param->kernel_params_amp[2] = &device_param->opencl_d_rules_c;\n        device_param->kernel_params_amp[3] = &device_param->opencl_d_combs_c;\n        device_param->kernel_params_amp[4] = &device_param->opencl_d_bfs_c;\n      }\n\n      device_param->kernel_params_amp[5] = &device_param->kernel_params_amp_buf32[5];\n      device_param->kernel_params_amp[6] = &device_param->kernel_params_amp_buf64[6];\n\n      if (device_param->is_cuda == true)\n      {\n        device_param->kernel_params_tm[0] = &device_param->cuda_d_bfs_c;\n        device_param->kernel_params_tm[1] = &device_param->cuda_d_tm_c;\n      }\n\n      if (device_param->is_opencl == true)\n      {\n        device_param->kernel_params_tm[0] = &device_param->opencl_d_bfs_c;\n        device_param->kernel_params_tm[1] = &device_param->opencl_d_tm_c;\n      }\n    }\n\n    device_param->kernel_params_memset_buf32[1] = 0; // value\n    device_param->kernel_params_memset_buf64[2] = 0; // gid_max\n\n    device_param->kernel_params_memset[0] = NULL;\n    device_param->kernel_params_memset[1] = &device_param->kernel_params_memset_buf32[1];\n    device_param->kernel_params_memset[2] = &device_param->kernel_params_memset_buf64[2];\n\n    device_param->kernel_params_atinit_buf64[1] = 0; // gid_max\n\n    device_param->kernel_params_atinit[0] = NULL;\n    device_param->kernel_params_atinit[1] = &device_param->kernel_params_atinit_buf64[1];\n\n    device_param->kernel_params_decompress_buf64[3] = 0; // gid_max\n\n    if (device_param->is_cuda == true)\n    {\n      device_param->kernel_params_decompress[0] = NULL; // &device_param->cuda_d_pws_idx;\n      device_param->kernel_params_decompress[1] = NULL; // &device_param->cuda_d_pws_comp_buf;\n      device_param->kernel_params_decompress[2] = NULL; // (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                                        // ? &device_param->cuda_d_pws_buf\n                                                        // : &device_param->cuda_d_pws_amp_buf;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      device_param->kernel_params_decompress[0] = NULL; // &device_param->opencl_d_pws_idx;\n      device_param->kernel_params_decompress[1] = NULL; // &device_param->opencl_d_pws_comp_buf;\n      device_param->kernel_params_decompress[2] = NULL; // (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                                        // ? &device_param->opencl_d_pws_buf\n                                                        // : &device_param->opencl_d_pws_amp_buf;\n    }\n\n    device_param->kernel_params_decompress[3] = &device_param->kernel_params_decompress_buf64[3];\n\n    /**\n     * kernel name\n     */\n\n    if (device_param->is_cuda == true)\n    {\n      char kernel_name[64] = { 0 };\n\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n        if (hashconfig->opti_type & OPTI_TYPE_SINGLE_HASH)\n        {\n          if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n          {\n            // kernel1\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_s%02d\", kern_type, 4);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function1, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_wgs1) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_local_mem_size1) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_dynamic_local_mem_size1) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple1 = device_param->cuda_warp_size;\n\n            // kernel2\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_s%02d\", kern_type, 8);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function2, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_wgs2) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_local_mem_size2) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_dynamic_local_mem_size2) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple2 = device_param->cuda_warp_size;\n\n            // kernel3\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_s%02d\", kern_type, 16);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function3, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_wgs3) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_local_mem_size3) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_dynamic_local_mem_size3) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple3 = device_param->cuda_warp_size;\n          }\n          else\n          {\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_sxx\", kern_type);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function4, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function4, &device_param->kernel_wgs4) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function4, &device_param->kernel_local_mem_size4) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function4, &device_param->kernel_dynamic_local_mem_size4) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple4 = device_param->cuda_warp_size;\n          }\n        }\n        else\n        {\n          if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n          {\n            // kernel1\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_m%02d\", kern_type, 4);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function1, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_wgs1) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_local_mem_size1) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_dynamic_local_mem_size1) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple1 = device_param->cuda_warp_size;\n\n            // kernel2\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_m%02d\", kern_type, 8);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function2, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_wgs2) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_local_mem_size2) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_dynamic_local_mem_size2) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple2 = device_param->cuda_warp_size;\n\n            // kernel3\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_m%02d\", kern_type, 16);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function3, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_wgs3) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_local_mem_size3) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_dynamic_local_mem_size3) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple3 = device_param->cuda_warp_size;\n          }\n          else\n          {\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_mxx\", kern_type);\n\n            if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function4, device_param->cuda_module, kernel_name) == -1) return -1;\n\n            if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function4, &device_param->kernel_wgs4) == -1) return -1;\n\n            if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function4, &device_param->kernel_local_mem_size4) == -1) return -1;\n\n            if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function4, &device_param->kernel_dynamic_local_mem_size4) == -1) return -1;\n\n            device_param->kernel_preferred_wgs_multiple4 = device_param->cuda_warp_size;\n          }\n        }\n\n        if (user_options->slow_candidates == true)\n        {\n        }\n        else\n        {\n          if (user_options->attack_mode == ATTACK_MODE_BF)\n          {\n            if (hashconfig->opts_type & OPTS_TYPE_TM_KERNEL)\n            {\n              snprintf (kernel_name, sizeof (kernel_name), \"m%05u_tm\", kern_type);\n\n              if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_tm, device_param->cuda_module, kernel_name) == -1) return -1;\n\n              if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_tm, &device_param->kernel_wgs_tm) == -1) return -1;\n\n              if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_tm, &device_param->kernel_local_mem_size_tm) == -1) return -1;\n\n              if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_tm, &device_param->kernel_dynamic_local_mem_size_tm) == -1) return -1;\n\n              device_param->kernel_preferred_wgs_multiple_tm = device_param->cuda_warp_size;\n            }\n          }\n        }\n      }\n      else\n      {\n        // kernel1\n\n        snprintf (kernel_name, sizeof (kernel_name), \"m%05u_init\", kern_type);\n\n        if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function1, device_param->cuda_module, kernel_name) == -1) return -1;\n\n        if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_wgs1) == -1) return -1;\n\n        if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_local_mem_size1) == -1) return -1;\n\n        if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function1, &device_param->kernel_dynamic_local_mem_size1) == -1) return -1;\n\n        device_param->kernel_preferred_wgs_multiple1 = device_param->cuda_warp_size;\n\n        // kernel2\n\n        snprintf (kernel_name, sizeof (kernel_name), \"m%05u_loop\", kern_type);\n\n        if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function2, device_param->cuda_module, kernel_name) == -1) return -1;\n\n        if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_wgs2) == -1) return -1;\n\n        if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_local_mem_size2) == -1) return -1;\n\n        if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function2, &device_param->kernel_dynamic_local_mem_size2) == -1) return -1;\n\n        device_param->kernel_preferred_wgs_multiple2 = device_param->cuda_warp_size;\n\n        // kernel3\n\n        snprintf (kernel_name, sizeof (kernel_name), \"m%05u_comp\", kern_type);\n\n        if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function3, device_param->cuda_module, kernel_name) == -1) return -1;\n\n        if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_wgs3) == -1) return -1;\n\n        if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_local_mem_size3) == -1) return -1;\n\n        if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function3, &device_param->kernel_dynamic_local_mem_size3) == -1) return -1;\n\n        device_param->kernel_preferred_wgs_multiple3 = device_param->cuda_warp_size;\n\n        if (hashconfig->opts_type & OPTS_TYPE_LOOP_EXTENDED)\n        {\n          // kernel2e\n\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_loop_extended\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function2e, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function2e, &device_param->kernel_wgs2e) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function2e, &device_param->kernel_local_mem_size2e) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function2e, &device_param->kernel_dynamic_local_mem_size2e) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple2e = device_param->cuda_warp_size;\n        }\n\n        // kernel12\n\n        if (hashconfig->opts_type & OPTS_TYPE_HOOK12)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_hook12\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function12, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function12, &device_param->kernel_wgs12) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function12, &device_param->kernel_local_mem_size12) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function12, &device_param->kernel_dynamic_local_mem_size12) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple12 = device_param->cuda_warp_size;\n        }\n\n        // kernel23\n\n        if (hashconfig->opts_type & OPTS_TYPE_HOOK23)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_hook23\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function23, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function23, &device_param->kernel_wgs23) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function23, &device_param->kernel_local_mem_size23) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function23, &device_param->kernel_dynamic_local_mem_size23) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple23 = device_param->cuda_warp_size;\n        }\n\n        // init2\n\n        if (hashconfig->opts_type & OPTS_TYPE_INIT2)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_init2\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_init2, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_init2, &device_param->kernel_wgs_init2) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_init2, &device_param->kernel_local_mem_size_init2) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_init2, &device_param->kernel_dynamic_local_mem_size_init2) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_init2 = device_param->cuda_warp_size;\n        }\n\n        // loop2\n\n        if (hashconfig->opts_type & OPTS_TYPE_LOOP2)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_loop2\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_loop2, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_loop2, &device_param->kernel_wgs_loop2) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_loop2, &device_param->kernel_local_mem_size_loop2) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_loop2, &device_param->kernel_dynamic_local_mem_size_loop2) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_loop2 = device_param->cuda_warp_size;\n        }\n\n        // aux1\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX1)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux1\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_aux1, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_aux1, &device_param->kernel_wgs_aux1) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_aux1, &device_param->kernel_local_mem_size_aux1) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_aux1, &device_param->kernel_dynamic_local_mem_size_aux1) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_aux1 = device_param->cuda_warp_size;\n        }\n\n        // aux2\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX2)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux2\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_aux2, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_aux2, &device_param->kernel_wgs_aux2) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_aux2, &device_param->kernel_local_mem_size_aux2) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_aux2, &device_param->kernel_dynamic_local_mem_size_aux2) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_aux2 = device_param->cuda_warp_size;\n        }\n\n        // aux3\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX3)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux3\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_aux3, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_aux3, &device_param->kernel_wgs_aux3) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_aux3, &device_param->kernel_local_mem_size_aux3) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_aux3, &device_param->kernel_dynamic_local_mem_size_aux3) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_aux3 = device_param->cuda_warp_size;\n        }\n\n        // aux4\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX4)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux4\", kern_type);\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_aux4, device_param->cuda_module, kernel_name) == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_aux4, &device_param->kernel_wgs_aux4) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_aux4, &device_param->kernel_local_mem_size_aux4) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_aux4, &device_param->kernel_dynamic_local_mem_size_aux4) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_aux4 = device_param->cuda_warp_size;\n        }\n      }\n\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 0, sizeof (cl_mem),   device_param->kernel_params_decompress[0]); if (CL_rc == -1) return -1;\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 1, sizeof (cl_mem),   device_param->kernel_params_decompress[1]); if (CL_rc == -1) return -1;\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 2, sizeof (cl_mem),   device_param->kernel_params_decompress[2]); if (CL_rc == -1) return -1;\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 3, sizeof (cl_ulong), device_param->kernel_params_decompress[3]); if (CL_rc == -1) return -1;\n\n      // MP start\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (user_options->attack_mode == ATTACK_MODE_BF)\n        {\n          // mp_l\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_mp_l, device_param->cuda_module_mp, \"l_markov\") == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_mp_l, &device_param->kernel_wgs_mp_l) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_mp_l, &device_param->kernel_local_mem_size_mp_l) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_mp_l, &device_param->kernel_dynamic_local_mem_size_mp_l) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_mp_l = device_param->cuda_warp_size;\n\n          // mp_r\n\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_mp_r, device_param->cuda_module_mp, \"r_markov\") == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_mp_r, &device_param->kernel_wgs_mp_r) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_mp_r, &device_param->kernel_local_mem_size_mp_r) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_mp_r, &device_param->kernel_dynamic_local_mem_size_mp_r) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_mp_r = device_param->cuda_warp_size;\n\n          if (user_options->attack_mode == ATTACK_MODE_BF)\n          {\n            if (hashconfig->opts_type & OPTS_TYPE_TM_KERNEL)\n            {\n              //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_tm, 0, sizeof (cl_mem), device_param->kernel_params_tm[0]); if (CL_rc == -1) return -1;\n              //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_tm, 1, sizeof (cl_mem), device_param->kernel_params_tm[1]); if (CL_rc == -1) return -1;\n            }\n          }\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n        {\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_mp, device_param->cuda_module_mp, \"C_markov\") == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_mp, &device_param->kernel_wgs_mp) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_mp, &device_param->kernel_local_mem_size_mp) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_mp, &device_param->kernel_dynamic_local_mem_size_mp) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_mp = device_param->cuda_warp_size;\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n        {\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_mp, device_param->cuda_module_mp, \"C_markov\") == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_mp, &device_param->kernel_wgs_mp) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_mp, &device_param->kernel_local_mem_size_mp) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_mp, &device_param->kernel_dynamic_local_mem_size_mp) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_mp = device_param->cuda_warp_size;\n        }\n      }\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n        {\n          // nothing to do\n        }\n        else\n        {\n          if (hc_cuModuleGetFunction (hashcat_ctx, &device_param->cuda_function_amp, device_param->cuda_module_amp, \"amp\") == -1) return -1;\n\n          if (get_cuda_kernel_wgs (hashcat_ctx, device_param->cuda_function_amp, &device_param->kernel_wgs_amp) == -1) return -1;\n\n          if (get_cuda_kernel_local_mem_size (hashcat_ctx, device_param->cuda_function_amp, &device_param->kernel_local_mem_size_amp) == -1) return -1;\n\n          if (get_cuda_kernel_dynamic_local_mem_size (hashcat_ctx, device_param->cuda_function_amp, &device_param->kernel_dynamic_local_mem_size_amp) == -1) return -1;\n\n          device_param->kernel_preferred_wgs_multiple_amp = device_param->cuda_warp_size;\n        }\n\n        /*\n        if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n        {\n          // nothing to do\n        }\n        else\n        {\n          for (u32 i = 0; i < 5; i++)\n          {\n            //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, i, sizeof (cl_mem), device_param->kernel_params_amp[i]);\n\n            //if (CL_rc == -1) return -1;\n          }\n\n          for (u32 i = 5; i < 6; i++)\n          {\n            //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, i, sizeof (cl_uint), device_param->kernel_params_amp[i]);\n\n            //if (CL_rc == -1) return -1;\n          }\n\n          for (u32 i = 6; i < 7; i++)\n          {\n            //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, i, sizeof (cl_ulong), device_param->kernel_params_amp[i]);\n\n            //if (CL_rc == -1) return -1;\n          }\n        }\n        */\n      }\n\n      // zero some data buffers\n\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_plain_bufs,    device_param->size_plains)  == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_digests_shown, device_param->size_shown)   == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_result,        device_param->size_results) == -1) return -1;\n\n      /**\n       * special buffers\n       */\n\n      if (user_options->slow_candidates == true)\n      {\n        if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_rules_c, size_rules_c) == -1) return -1;\n      }\n      else\n      {\n        if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n        {\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_rules_c, size_rules_c) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n        {\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_combs,          size_combs)       == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_combs_c,        size_combs)       == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_root_css_buf,   size_root_css)    == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_markov_css_buf, size_markov_css)  == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n        {\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_bfs,            size_bfs)         == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_bfs_c,          size_bfs)         == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_tm_c,           size_tm)          == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_root_css_buf,   size_root_css)    == -1) return -1;\n          if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_markov_css_buf, size_markov_css)  == -1) return -1;\n        }\n      }\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if ((user_options->attack_mode == ATTACK_MODE_HYBRID1) || (user_options->attack_mode == ATTACK_MODE_HYBRID2))\n        {\n          /**\n           * prepare mp\n           */\n\n          if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n          {\n            device_param->kernel_params_mp_buf32[5] = 0;\n            device_param->kernel_params_mp_buf32[6] = 0;\n            device_param->kernel_params_mp_buf32[7] = 0;\n\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)     device_param->kernel_params_mp_buf32[5] = full01;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)     device_param->kernel_params_mp_buf32[5] = full06;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)     device_param->kernel_params_mp_buf32[5] = full80;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS14) device_param->kernel_params_mp_buf32[6] = 1;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS15) device_param->kernel_params_mp_buf32[7] = 1;\n          }\n          else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n          {\n            device_param->kernel_params_mp_buf32[5] = 0;\n            device_param->kernel_params_mp_buf32[6] = 0;\n            device_param->kernel_params_mp_buf32[7] = 0;\n          }\n\n          //for (u32 i = 0; i < 3; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, i, sizeof (cl_mem), device_param->kernel_params_mp[i]); if (CL_rc == -1) return -1; }\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_BF)\n        {\n          /**\n           * prepare mp_r and mp_l\n           */\n\n          device_param->kernel_params_mp_l_buf32[6] = 0;\n          device_param->kernel_params_mp_l_buf32[7] = 0;\n          device_param->kernel_params_mp_l_buf32[8] = 0;\n\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)     device_param->kernel_params_mp_l_buf32[6] = full01;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)     device_param->kernel_params_mp_l_buf32[6] = full06;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)     device_param->kernel_params_mp_l_buf32[6] = full80;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS14) device_param->kernel_params_mp_l_buf32[7] = 1;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS15) device_param->kernel_params_mp_l_buf32[8] = 1;\n\n          //for (u32 i = 0; i < 3; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_mem), device_param->kernel_params_mp_l[i]); if (CL_rc == -1) return -1; }\n          //for (u32 i = 0; i < 3; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_mem), device_param->kernel_params_mp_r[i]); if (CL_rc == -1) return -1; }\n        }\n      }\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      // GPU memset\n\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_memset, 0, sizeof (cl_mem),   device_param->kernel_params_memset[0]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_memset, 1, sizeof (cl_uint),  device_param->kernel_params_memset[1]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_memset, 2, sizeof (cl_ulong), device_param->kernel_params_memset[2]) == -1) return -1;\n\n      // GPU autotune init\n\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_atinit, 0, sizeof (cl_mem),   device_param->kernel_params_atinit[0]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_atinit, 1, sizeof (cl_ulong), device_param->kernel_params_atinit[1]) == -1) return -1;\n\n      // GPU decompress\n\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 0, sizeof (cl_mem),   device_param->kernel_params_decompress[0]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 1, sizeof (cl_mem),   device_param->kernel_params_decompress[1]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 2, sizeof (cl_mem),   device_param->kernel_params_decompress[2]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 3, sizeof (cl_ulong), device_param->kernel_params_decompress[3]) == -1) return -1;\n\n      char kernel_name[64] = { 0 };\n\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n        if (hashconfig->opti_type & OPTI_TYPE_SINGLE_HASH)\n        {\n          if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n          {\n            // kernel1\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_s%02d\", kern_type, 4);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel1) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_wgs1) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_local_mem_size1) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_dynamic_local_mem_size1) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_preferred_wgs_multiple1) == -1) return -1;\n\n            // kernel2\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_s%02d\", kern_type, 8);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel2) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_wgs2) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_local_mem_size2) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_dynamic_local_mem_size2) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_preferred_wgs_multiple2) == -1) return -1;\n\n            // kernel3\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_s%02d\", kern_type, 16);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel3) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_wgs3) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_local_mem_size3) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_dynamic_local_mem_size3) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_preferred_wgs_multiple3) == -1) return -1;\n          }\n          else\n          {\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_sxx\", kern_type);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel4) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_wgs4) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_local_mem_size4) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_dynamic_local_mem_size4) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_preferred_wgs_multiple4) == -1) return -1;\n          }\n        }\n        else\n        {\n          if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n          {\n            // kernel1\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_m%02d\", kern_type, 4);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel1) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_wgs1) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_local_mem_size1) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_dynamic_local_mem_size1) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_preferred_wgs_multiple1) == -1) return -1;\n\n            // kernel2\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_m%02d\", kern_type, 8);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel2) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_wgs2) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_local_mem_size2) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_dynamic_local_mem_size2) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_preferred_wgs_multiple2) == -1) return -1;\n\n            // kernel3\n\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_m%02d\", kern_type, 16);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel3) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_wgs3) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_local_mem_size3) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_dynamic_local_mem_size3) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_preferred_wgs_multiple3) == -1) return -1;\n          }\n          else\n          {\n            snprintf (kernel_name, sizeof (kernel_name), \"m%05u_mxx\", kern_type);\n\n            if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel4) == -1) return -1;\n\n            if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_wgs4) == -1) return -1;\n\n            if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_local_mem_size4) == -1) return -1;\n\n            if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_dynamic_local_mem_size4) == -1) return -1;\n\n            if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel4, &device_param->kernel_preferred_wgs_multiple4) == -1) return -1;\n          }\n        }\n\n        if (user_options->slow_candidates == true)\n        {\n        }\n        else\n        {\n          if (user_options->attack_mode == ATTACK_MODE_BF)\n          {\n            if (hashconfig->opts_type & OPTS_TYPE_TM_KERNEL)\n            {\n              snprintf (kernel_name, sizeof (kernel_name), \"m%05u_tm\", kern_type);\n\n              if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_tm) == -1) return -1;\n\n              if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_tm, &device_param->kernel_wgs_tm) == -1) return -1;\n\n              if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_tm, &device_param->kernel_local_mem_size_tm) == -1) return -1;\n\n              if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_tm, &device_param->kernel_dynamic_local_mem_size_tm) == -1) return -1;\n\n              if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_tm, &device_param->kernel_preferred_wgs_multiple_tm) == -1) return -1;\n            }\n          }\n        }\n      }\n      else\n      {\n        // kernel1\n\n        snprintf (kernel_name, sizeof (kernel_name), \"m%05u_init\", kern_type);\n\n        if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel1) == -1) return -1;\n\n        if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_wgs1) == -1) return -1;\n\n        if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_local_mem_size1) == -1) return -1;\n\n        if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_dynamic_local_mem_size1) == -1) return -1;\n\n        if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel1, &device_param->kernel_preferred_wgs_multiple1) == -1) return -1;\n\n        // kernel2\n\n        snprintf (kernel_name, sizeof (kernel_name), \"m%05u_loop\", kern_type);\n\n        if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel2) == -1) return -1;\n\n        if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_wgs2) == -1) return -1;\n\n        if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_local_mem_size2) == -1) return -1;\n\n        if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_dynamic_local_mem_size2) == -1) return -1;\n\n        if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel2, &device_param->kernel_preferred_wgs_multiple2) == -1) return -1;\n\n        // kernel3\n\n        snprintf (kernel_name, sizeof (kernel_name), \"m%05u_comp\", kern_type);\n\n        if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel3) == -1) return -1;\n\n        if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_wgs3) == -1) return -1;\n\n        if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_local_mem_size3) == -1) return -1;\n\n        if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_dynamic_local_mem_size3) == -1) return -1;\n\n        if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel3, &device_param->kernel_preferred_wgs_multiple3) == -1) return -1;\n\n        // aux1\n\n        if (hashconfig->opts_type & OPTS_TYPE_LOOP_EXTENDED)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_loop_extended\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel2e) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel2e, &device_param->kernel_wgs2e) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2e, &device_param->kernel_local_mem_size2e) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel2e, &device_param->kernel_dynamic_local_mem_size2e) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel2e, &device_param->kernel_preferred_wgs_multiple2e) == -1) return -1;\n        }\n\n        // kernel12\n\n        if (hashconfig->opts_type & OPTS_TYPE_HOOK12)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_hook12\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel12) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel12, &device_param->kernel_wgs12) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel12, &device_param->kernel_local_mem_size12) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel12, &device_param->kernel_dynamic_local_mem_size12) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel12, &device_param->kernel_preferred_wgs_multiple12) == -1) return -1;\n        }\n\n        // kernel23\n\n        if (hashconfig->opts_type & OPTS_TYPE_HOOK23)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_hook23\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel23) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel23, &device_param->kernel_wgs23) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel23, &device_param->kernel_local_mem_size23) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel23, &device_param->kernel_dynamic_local_mem_size23) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel23, &device_param->kernel_preferred_wgs_multiple23) == -1) return -1;\n        }\n\n        // init2\n\n        if (hashconfig->opts_type & OPTS_TYPE_INIT2)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_init2\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_init2) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_init2, &device_param->kernel_wgs_init2) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_init2, &device_param->kernel_local_mem_size_init2) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_init2, &device_param->kernel_dynamic_local_mem_size_init2) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_init2, &device_param->kernel_preferred_wgs_multiple_init2) == -1) return -1;\n        }\n\n        // loop2\n\n        if (hashconfig->opts_type & OPTS_TYPE_LOOP2)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_loop2\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_loop2) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_loop2, &device_param->kernel_wgs_loop2) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_loop2, &device_param->kernel_local_mem_size_loop2) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_loop2, &device_param->kernel_dynamic_local_mem_size_loop2) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_loop2, &device_param->kernel_preferred_wgs_multiple_loop2) == -1) return -1;\n        }\n\n        // aux1\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX1)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux1\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_aux1) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_aux1, &device_param->kernel_wgs_aux1) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux1, &device_param->kernel_local_mem_size_aux1) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux1, &device_param->kernel_dynamic_local_mem_size_aux1) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_aux1, &device_param->kernel_preferred_wgs_multiple_aux1) == -1) return -1;\n        }\n\n        // aux2\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX2)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux2\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_aux2) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_aux2, &device_param->kernel_wgs_aux2) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux2, &device_param->kernel_local_mem_size_aux2) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux2, &device_param->kernel_dynamic_local_mem_size_aux2) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_aux2, &device_param->kernel_preferred_wgs_multiple_aux2) == -1) return -1;\n        }\n\n        // aux3\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX3)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux3\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_aux3) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_aux3, &device_param->kernel_wgs_aux3) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux3, &device_param->kernel_local_mem_size_aux3) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux3, &device_param->kernel_dynamic_local_mem_size_aux3) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_aux3, &device_param->kernel_preferred_wgs_multiple_aux3) == -1) return -1;\n        }\n\n        // aux4\n\n        if (hashconfig->opts_type & OPTS_TYPE_AUX4)\n        {\n          snprintf (kernel_name, sizeof (kernel_name), \"m%05u_aux4\", kern_type);\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program, kernel_name, &device_param->opencl_kernel_aux4) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_aux4, &device_param->kernel_wgs_aux4) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux4, &device_param->kernel_local_mem_size_aux4) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_aux4, &device_param->kernel_dynamic_local_mem_size_aux4) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_aux4, &device_param->kernel_preferred_wgs_multiple_aux4) == -1) return -1;\n        }\n      }\n\n      // MP start\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (user_options->attack_mode == ATTACK_MODE_BF)\n        {\n          // mp_l\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_mp, \"l_markov\", &device_param->opencl_kernel_mp_l) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_mp_l, &device_param->kernel_wgs_mp_l) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp_l, &device_param->kernel_local_mem_size_mp_l) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp_l, &device_param->kernel_dynamic_local_mem_size_mp_l) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_mp_l, &device_param->kernel_preferred_wgs_multiple_mp_l) == -1) return -1;\n\n          // mp_r\n\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_mp, \"r_markov\", &device_param->opencl_kernel_mp_r) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_mp_r, &device_param->kernel_wgs_mp_r) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp_r, &device_param->kernel_local_mem_size_mp_r) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp_r, &device_param->kernel_dynamic_local_mem_size_mp_r) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_mp_r, &device_param->kernel_preferred_wgs_multiple_mp_r) == -1) return -1;\n\n          if (user_options->attack_mode == ATTACK_MODE_BF)\n          {\n            if (hashconfig->opts_type & OPTS_TYPE_TM_KERNEL)\n            {\n              if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_tm, 0, sizeof (cl_mem), device_param->kernel_params_tm[0]) == -1) return -1;\n              if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_tm, 1, sizeof (cl_mem), device_param->kernel_params_tm[1]) == -1) return -1;\n            }\n          }\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n        {\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_mp, \"C_markov\", &device_param->opencl_kernel_mp) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_wgs_mp) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_local_mem_size_mp) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_dynamic_local_mem_size_mp) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_preferred_wgs_multiple_mp) == -1) return -1;\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n        {\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_mp, \"C_markov\", &device_param->opencl_kernel_mp) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_wgs_mp) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_local_mem_size_mp) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_dynamic_local_mem_size_mp) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_mp, &device_param->kernel_preferred_wgs_multiple_mp) == -1) return -1;\n        }\n      }\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n        {\n          // nothing to do\n        }\n        else\n        {\n          if (hc_clCreateKernel (hashcat_ctx, device_param->opencl_program_amp, \"amp\", &device_param->opencl_kernel_amp) == -1) return -1;\n\n          if (get_opencl_kernel_wgs (hashcat_ctx, device_param, device_param->opencl_kernel_amp, &device_param->kernel_wgs_amp) == -1) return -1;\n\n          if (get_opencl_kernel_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_amp, &device_param->kernel_local_mem_size_amp) == -1) return -1;\n\n          if (get_opencl_kernel_dynamic_local_mem_size (hashcat_ctx, device_param, device_param->opencl_kernel_amp, &device_param->kernel_dynamic_local_mem_size_amp) == -1) return -1;\n\n          if (get_opencl_kernel_preferred_wgs_multiple (hashcat_ctx, device_param, device_param->opencl_kernel_amp, &device_param->kernel_preferred_wgs_multiple_amp) == -1) return -1;\n        }\n\n        if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n        {\n          // nothing to do\n        }\n        else\n        {\n          for (u32 i = 0; i < 5; i++)\n          {\n            if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, i, sizeof (cl_mem), device_param->kernel_params_amp[i]) == -1) return -1;\n          }\n\n          for (u32 i = 5; i < 6; i++)\n          {\n            if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, i, sizeof (cl_uint), device_param->kernel_params_amp[i]) == -1) return -1;\n          }\n\n          for (u32 i = 6; i < 7; i++)\n          {\n            if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, i, sizeof (cl_ulong), device_param->kernel_params_amp[i]) == -1) return -1;\n          }\n        }\n      }\n\n      // zero some data buffers\n\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_plain_bufs,    device_param->size_plains)   == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_digests_shown, device_param->size_shown)    == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_result,        device_param->size_results)  == -1) return -1;\n\n      /**\n       * special buffers\n       */\n\n      if (user_options->slow_candidates == true)\n      {\n        if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_rules_c, size_rules_c) == -1) return -1;\n      }\n      else\n      {\n        if (user_options_extra->attack_kern == ATTACK_KERN_STRAIGHT)\n        {\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_rules_c, size_rules_c) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_COMBI)\n        {\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_combs,          size_combs)      == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_combs_c,        size_combs)      == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_root_css_buf,   size_root_css)   == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_markov_css_buf, size_markov_css) == -1) return -1;\n        }\n        else if (user_options_extra->attack_kern == ATTACK_KERN_BF)\n        {\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_bfs,            size_bfs)        == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_bfs_c,          size_bfs)        == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_tm_c,           size_tm)         == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_root_css_buf,   size_root_css)   == -1) return -1;\n          if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_markov_css_buf, size_markov_css) == -1) return -1;\n        }\n      }\n\n      if (user_options->slow_candidates == true)\n      {\n      }\n      else\n      {\n        if ((user_options->attack_mode == ATTACK_MODE_HYBRID1) || (user_options->attack_mode == ATTACK_MODE_HYBRID2))\n        {\n          /**\n           * prepare mp\n           */\n\n          if (user_options->attack_mode == ATTACK_MODE_HYBRID1)\n          {\n            device_param->kernel_params_mp_buf32[5] = 0;\n            device_param->kernel_params_mp_buf32[6] = 0;\n            device_param->kernel_params_mp_buf32[7] = 0;\n\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)     device_param->kernel_params_mp_buf32[5] = full01;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)     device_param->kernel_params_mp_buf32[5] = full06;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)     device_param->kernel_params_mp_buf32[5] = full80;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS14) device_param->kernel_params_mp_buf32[6] = 1;\n            if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS15) device_param->kernel_params_mp_buf32[7] = 1;\n          }\n          else if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n          {\n            device_param->kernel_params_mp_buf32[5] = 0;\n            device_param->kernel_params_mp_buf32[6] = 0;\n            device_param->kernel_params_mp_buf32[7] = 0;\n          }\n\n          for (u32 i = 0; i < 3; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, i, sizeof (cl_mem), device_param->kernel_params_mp[i]) == -1) return -1; }\n        }\n        else if (user_options->attack_mode == ATTACK_MODE_BF)\n        {\n          /**\n           * prepare mp_r and mp_l\n           */\n\n          device_param->kernel_params_mp_l_buf32[6] = 0;\n          device_param->kernel_params_mp_l_buf32[7] = 0;\n          device_param->kernel_params_mp_l_buf32[8] = 0;\n\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD01)     device_param->kernel_params_mp_l_buf32[6] = full01;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD06)     device_param->kernel_params_mp_l_buf32[6] = full06;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADD80)     device_param->kernel_params_mp_l_buf32[6] = full80;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS14) device_param->kernel_params_mp_l_buf32[7] = 1;\n          if (hashconfig->opts_type & OPTS_TYPE_PT_ADDBITS15) device_param->kernel_params_mp_l_buf32[8] = 1;\n\n          for (u32 i = 0; i < 3; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_mem), device_param->kernel_params_mp_l[i]) == -1) return -1; }\n          for (u32 i = 0; i < 3; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_mem), device_param->kernel_params_mp_r[i]) == -1) return -1; }\n        }\n      }\n    }\n\n    // this is required because inside the kernels there is this:\n    // __local pw_t s_pws[64];\n\n    if (user_options->attack_mode == ATTACK_MODE_STRAIGHT)\n    {\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n        if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n        {\n          // not required\n        }\n        else\n        {\n          device_param->kernel_threads_max = MIN (device_param->kernel_threads_max, 64);\n        }\n      }\n    }\n\n    /**\n     * now everything that depends on threads and accel, basically dynamic workload\n     */\n\n    const u32 kernel_threads = get_kernel_threads (device_param);\n\n    device_param->kernel_threads = kernel_threads;\n\n    device_param->hardware_power = device_processors * kernel_threads;\n\n    u32 kernel_accel_min = device_param->kernel_accel_min;\n    u32 kernel_accel_max = device_param->kernel_accel_max;\n\n    /**\n     * We need a kernel accel limiter otherwise we will allocate too much memory (Example 4* GTX1080):\n     * 4 (gpus) * 260 (sizeof pw_t) * 3 (pws, pws_comp, pw_pre) * 20 (MCU) * 1024 (threads) * 1024 (accel) = 65,431,142,400 bytes RAM!!\n     */\n\n    const u32 accel_limit = CEILDIV ((64 * 1024), kernel_threads); // this should result in less than 4GB per GPU, but allow higher accel in case user reduces the threads manually using -T\n\n    kernel_accel_max = MIN (kernel_accel_max, accel_limit);\n\n    if (kernel_accel_min > kernel_accel_max)\n    {\n      event_log_error (hashcat_ctx, \"* Device #%u: Too many compute units to keep minimum kernel accel limit. Retry with lower --backend-kernel-threads value.\", device_id + 1);\n\n      return -1;\n    }\n\n    // find out if we would request too much memory on memory blocks which are based on kernel_accel\n\n    u64 size_pws      = 4;\n    u64 size_pws_amp  = 4;\n    u64 size_pws_comp = 4;\n    u64 size_pws_idx  = 4;\n    u64 size_pws_pre  = 4;\n    u64 size_pws_base = 4;\n    u64 size_tmps     = 4;\n    u64 size_hooks    = 4;\n    #ifdef WITH_BRAIN\n    u64 size_brain_link_in  = 4;\n    u64 size_brain_link_out = 4;\n    #endif\n\n    // instead of a thread limit we can also use a memory limit.\n    // this value should represent a reasonable amount of memory a host system has per GPU.\n    // note we're allocating 3 blocks of that size.\n\n    const u64 PWS_SPACE = 1024ULL * 1024ULL * 1024ULL;\n\n    // sometimes device_available_mem and device_maxmem_alloc reported back from the opencl runtime are a bit inaccurate.\n    // let's add some extra space just to be sure.\n\n    const u64 EXTRA_SPACE = 64ULL * 1024ULL * 1024ULL;\n\n    while (kernel_accel_max >= kernel_accel_min)\n    {\n      const u64 kernel_power_max = device_param->hardware_power * kernel_accel_max;\n\n      // size_pws\n\n      size_pws = kernel_power_max * sizeof (pw_t);\n\n      size_pws_amp = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL) ? 1 : size_pws;\n\n      // size_pws_comp\n\n      size_pws_comp = kernel_power_max * (sizeof (u32) * 64);\n\n      // size_pws_idx\n\n      size_pws_idx = (u64) (kernel_power_max + 1) * sizeof (pw_idx_t);\n\n      // size_tmps\n\n      size_tmps = kernel_power_max * (hashconfig->tmp_size + hashconfig->extra_tmp_size);\n\n      // size_hooks\n\n      size_hooks = kernel_power_max * hashconfig->hook_size;\n\n      #ifdef WITH_BRAIN\n      // size_brains\n\n      size_brain_link_in  = kernel_power_max * 1;\n      size_brain_link_out = kernel_power_max * 8;\n      #endif\n\n      if (user_options->slow_candidates == true)\n      {\n        // size_pws_pre\n\n        size_pws_pre = kernel_power_max * sizeof (pw_pre_t);\n\n        // size_pws_base\n\n        size_pws_base = kernel_power_max * sizeof (pw_pre_t);\n      }\n\n      // now check if all device-memory sizes which depend on the kernel_accel_max amplifier are within its boundaries\n      // if not, decrease amplifier and try again\n\n      int memory_limit_hit = 0;\n\n      if (size_pws > PWS_SPACE) memory_limit_hit = 1;\n\n      if ((size_pws   + EXTRA_SPACE) > device_param->device_maxmem_alloc) memory_limit_hit = 1;\n      if ((size_tmps  + EXTRA_SPACE) > device_param->device_maxmem_alloc) memory_limit_hit = 1;\n      if ((size_hooks + EXTRA_SPACE) > device_param->device_maxmem_alloc) memory_limit_hit = 1;\n\n      const u64 size_total\n        = bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + bitmap_ctx->bitmap_size\n        + size_bfs\n        + size_combs\n        + size_digests\n        + size_esalts\n        + size_hooks\n        + size_markov_css\n        + size_plains\n        + size_pws\n        + size_pws_amp\n        + size_pws_comp\n        + size_pws_idx\n        + size_results\n        + size_root_css\n        + size_rules\n        + size_rules_c\n        + size_salts\n        + size_extra_buffer\n        + size_shown\n        + size_tm\n        + size_tmps\n        + size_st_digests\n        + size_st_salts\n        + size_st_esalts;\n\n      if ((size_total + EXTRA_SPACE) > device_param->device_available_mem) memory_limit_hit = 1;\n\n      if (memory_limit_hit == 1)\n      {\n        kernel_accel_max--;\n\n        continue;\n      }\n\n      const u64 size_total_host\n        = size_pws_comp\n        + size_pws_idx\n        + size_hooks\n        #ifdef WITH_BRAIN\n        + size_brain_link_in\n        + size_brain_link_out\n        #endif\n        + size_pws_pre\n        + size_pws_base;\n\n      size_total_host_all += size_total_host + EXTRA_SPACE;\n\n      break;\n    }\n\n    if (kernel_accel_max < kernel_accel_min)\n    {\n      event_log_error (hashcat_ctx, \"* Device #%u: Not enough allocatable device memory for this attack.\", device_id + 1);\n\n      return -1;\n    }\n\n    device_param->kernel_accel_min = kernel_accel_min;\n    device_param->kernel_accel_max = kernel_accel_max;\n\n    device_param->size_pws      = size_pws;\n    device_param->size_pws_amp  = size_pws_amp;\n    device_param->size_pws_comp = size_pws_comp;\n    device_param->size_pws_idx  = size_pws_idx;\n    device_param->size_pws_pre  = size_pws_pre;\n    device_param->size_pws_base = size_pws_base;\n    device_param->size_tmps     = size_tmps;\n    device_param->size_hooks    = size_hooks;\n    #ifdef WITH_BRAIN\n    device_param->size_brain_link_in  = size_brain_link_in;\n    device_param->size_brain_link_out = size_brain_link_out;\n    #endif\n\n    if (device_param->is_cuda == true)\n    {\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_pws_buf,      size_pws)      == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_pws_amp_buf,  size_pws_amp)  == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_pws_comp_buf, size_pws_comp) == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_pws_idx,      size_pws_idx)  == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_tmps,         size_tmps)     == -1) return -1;\n      if (hc_cuMemAlloc (hashcat_ctx, &device_param->cuda_d_hooks,        size_hooks)    == -1) return -1;\n\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_pws_buf,       device_param->size_pws)      == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_pws_amp_buf,   device_param->size_pws_amp)  == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_pws_comp_buf,  device_param->size_pws_comp) == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_pws_idx,       device_param->size_pws_idx)  == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_tmps,          device_param->size_tmps)     == -1) return -1;\n      if (run_cuda_kernel_bzero (hashcat_ctx, device_param, device_param->cuda_d_hooks,         device_param->size_hooks)    == -1) return -1;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_pws,      NULL, &device_param->opencl_d_pws_buf)      == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_pws_amp,  NULL, &device_param->opencl_d_pws_amp_buf)  == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   size_pws_comp, NULL, &device_param->opencl_d_pws_comp_buf) == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_ONLY,   size_pws_idx,  NULL, &device_param->opencl_d_pws_idx)      == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_tmps,     NULL, &device_param->opencl_d_tmps)         == -1) return -1;\n      if (hc_clCreateBuffer (hashcat_ctx, device_param->opencl_context, CL_MEM_READ_WRITE,  size_hooks,    NULL, &device_param->opencl_d_hooks)        == -1) return -1;\n\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_pws_buf,       device_param->size_pws)      == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_pws_amp_buf,   device_param->size_pws_amp)  == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_pws_comp_buf,  device_param->size_pws_comp) == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_pws_idx,       device_param->size_pws_idx)  == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_tmps,          device_param->size_tmps)     == -1) return -1;\n      if (run_opencl_kernel_bzero (hashcat_ctx, device_param, device_param->opencl_d_hooks,         device_param->size_hooks)    == -1) return -1;\n    }\n\n    /**\n     * main host data\n     */\n\n    u32 *pws_comp = (u32 *) hcmalloc (size_pws_comp);\n\n    device_param->pws_comp = pws_comp;\n\n    pw_idx_t *pws_idx = (pw_idx_t *) hcmalloc (size_pws_idx);\n\n    device_param->pws_idx = pws_idx;\n\n    pw_t *combs_buf = (pw_t *) hccalloc (KERNEL_COMBS, sizeof (pw_t));\n\n    device_param->combs_buf = combs_buf;\n\n    void *hooks_buf = hcmalloc (size_hooks);\n\n    device_param->hooks_buf = hooks_buf;\n\n    char *scratch_buf = (char *) hcmalloc (HCBUFSIZ_LARGE);\n\n    device_param->scratch_buf = scratch_buf;\n\n    #ifdef WITH_BRAIN\n\n    u8 *brain_link_in_buf = (u8 *) hcmalloc (size_brain_link_in);\n\n    device_param->brain_link_in_buf = brain_link_in_buf;\n\n    u32 *brain_link_out_buf = (u32 *) hcmalloc (size_brain_link_out);\n\n    device_param->brain_link_out_buf = brain_link_out_buf;\n    #endif\n\n    pw_pre_t *pws_pre_buf = (pw_pre_t *) hcmalloc (size_pws_pre);\n\n    device_param->pws_pre_buf = pws_pre_buf;\n\n    pw_pre_t *pws_base_buf = (pw_pre_t *) hcmalloc (size_pws_base);\n\n    device_param->pws_base_buf = pws_base_buf;\n\n    /**\n     * kernel args\n     */\n\n    if (device_param->is_cuda == true)\n    {\n      device_param->kernel_params[ 0] = &device_param->cuda_d_pws_buf;\n      device_param->kernel_params[ 4] = &device_param->cuda_d_tmps;\n      device_param->kernel_params[ 5] = &device_param->cuda_d_hooks;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      device_param->kernel_params[ 0] = &device_param->opencl_d_pws_buf;\n      device_param->kernel_params[ 4] = &device_param->opencl_d_tmps;\n      device_param->kernel_params[ 5] = &device_param->opencl_d_hooks;\n    }\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      if (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL)\n      {\n        // nothing to do\n      }\n      else\n      {\n        if (user_options->attack_mode == ATTACK_MODE_HYBRID2)\n        {\n          if (device_param->is_cuda == true)\n          {\n            device_param->kernel_params_mp[0] = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                              ? &device_param->cuda_d_pws_buf\n                                              : &device_param->cuda_d_pws_amp_buf;\n\n            //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, 0, sizeof (cl_mem), device_param->kernel_params_mp[0]); if (CL_rc == -1) return -1;\n          }\n\n          if (device_param->is_opencl == true)\n          {\n            device_param->kernel_params_mp[0] = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                              ? &device_param->opencl_d_pws_buf\n                                              : &device_param->opencl_d_pws_amp_buf;\n\n            if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, 0, sizeof (cl_mem), device_param->kernel_params_mp[0]) == -1) return -1;\n          }\n        }\n      }\n\n      if (user_options->attack_mode == ATTACK_MODE_BF)\n      {\n        if (device_param->is_cuda == true)\n        {\n          device_param->kernel_params_mp_l[0] = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                              ? &device_param->cuda_d_pws_buf\n                                              : &device_param->cuda_d_pws_amp_buf;\n\n          //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, 0, sizeof (cl_mem), device_param->kernel_params_mp_l[0]); if (CL_rc == -1) return -1;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          device_param->kernel_params_mp_l[0] = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                              ? &device_param->opencl_d_pws_buf\n                                              : &device_param->opencl_d_pws_amp_buf;\n\n          if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, 0, sizeof (cl_mem), device_param->kernel_params_mp_l[0]) == -1) return -1;\n        }\n      }\n\n      if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n      {\n        // nothing to do\n      }\n      else\n      {\n        if (device_param->is_cuda == true)\n        {\n          device_param->kernel_params_amp[0] = &device_param->cuda_d_pws_buf;\n          device_param->kernel_params_amp[1] = &device_param->cuda_d_pws_amp_buf;\n\n          //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, 0, sizeof (cl_mem), device_param->kernel_params_amp[0]); if (CL_rc == -1) return -1;\n          //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, 1, sizeof (cl_mem), device_param->kernel_params_amp[1]); if (CL_rc == -1) return -1;\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          device_param->kernel_params_amp[0] = &device_param->opencl_d_pws_buf;\n          device_param->kernel_params_amp[1] = &device_param->opencl_d_pws_amp_buf;\n\n          if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, 0, sizeof (cl_mem), device_param->kernel_params_amp[0]) == -1) return -1;\n          if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, 1, sizeof (cl_mem), device_param->kernel_params_amp[1]) == -1) return -1;\n        }\n      }\n    }\n\n    if (device_param->is_cuda == true)\n    {\n      device_param->kernel_params_decompress[0] = &device_param->cuda_d_pws_idx;\n      device_param->kernel_params_decompress[1] = &device_param->cuda_d_pws_comp_buf;\n      device_param->kernel_params_decompress[2] = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                                ? &device_param->cuda_d_pws_buf\n                                                : &device_param->cuda_d_pws_amp_buf;\n\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 0, sizeof (cl_mem), device_param->kernel_params_decompress[0]); if (CL_rc == -1) return -1;\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 1, sizeof (cl_mem), device_param->kernel_params_decompress[1]); if (CL_rc == -1) return -1;\n      //CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 2, sizeof (cl_mem), device_param->kernel_params_decompress[2]); if (CL_rc == -1) return -1;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      device_param->kernel_params_decompress[0] = &device_param->opencl_d_pws_idx;\n      device_param->kernel_params_decompress[1] = &device_param->opencl_d_pws_comp_buf;\n      device_param->kernel_params_decompress[2] = (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n                                                ? &device_param->opencl_d_pws_buf\n                                                : &device_param->opencl_d_pws_amp_buf;\n\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 0, sizeof (cl_mem), device_param->kernel_params_decompress[0]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 1, sizeof (cl_mem), device_param->kernel_params_decompress[1]) == -1) return -1;\n      if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_decompress, 2, sizeof (cl_mem), device_param->kernel_params_decompress[2]) == -1) return -1;\n    }\n\n    hardware_power_all += device_param->hardware_power;\n\n    EVENT_DATA (EVENT_BACKEND_DEVICE_INIT_POST, &backend_devices_idx, sizeof (int));\n  }\n\n  if (user_options->benchmark == false)\n  {\n    if (hardware_power_all == 0) return -1;\n  }\n\n  backend_ctx->hardware_power_all = hardware_power_all;\n\n  EVENT_DATA (EVENT_BACKEND_SESSION_HOSTMEM, &size_total_host_all, sizeof (u64));\n\n  return 0;\n}\n\nvoid backend_session_destroy (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (backend_ctx->enabled == false) return;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    hcfree (device_param->pws_comp);\n    hcfree (device_param->pws_idx);\n    hcfree (device_param->pws_pre_buf);\n    hcfree (device_param->pws_base_buf);\n    hcfree (device_param->combs_buf);\n    hcfree (device_param->hooks_buf);\n    hcfree (device_param->scratch_buf);\n    #ifdef WITH_BRAIN\n    hcfree (device_param->brain_link_in_buf);\n    hcfree (device_param->brain_link_out_buf);\n    #endif\n\n    if (device_param->is_cuda == true)\n    {\n      if (device_param->cuda_d_pws_buf)        hc_cuMemFree (hashcat_ctx, device_param->cuda_d_pws_buf);\n      if (device_param->cuda_d_pws_amp_buf)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_pws_amp_buf);\n      if (device_param->cuda_d_pws_comp_buf)   hc_cuMemFree (hashcat_ctx, device_param->cuda_d_pws_comp_buf);\n      if (device_param->cuda_d_pws_idx)        hc_cuMemFree (hashcat_ctx, device_param->cuda_d_pws_idx);\n      if (device_param->cuda_d_rules)          hc_cuMemFree (hashcat_ctx, device_param->cuda_d_rules);\n      //if (device_param->cuda_d_rules_c)        hc_cuMemFree (hashcat_ctx, device_param->cuda_d_rules_c);\n      if (device_param->cuda_d_combs)          hc_cuMemFree (hashcat_ctx, device_param->cuda_d_combs);\n      if (device_param->cuda_d_combs_c)        hc_cuMemFree (hashcat_ctx, device_param->cuda_d_combs_c);\n      if (device_param->cuda_d_bfs)            hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bfs);\n      //if (device_param->cuda_d_bfs_c)          hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bfs_c);\n      if (device_param->cuda_d_bitmap_s1_a)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s1_a);\n      if (device_param->cuda_d_bitmap_s1_b)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s1_b);\n      if (device_param->cuda_d_bitmap_s1_c)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s1_c);\n      if (device_param->cuda_d_bitmap_s1_d)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s1_d);\n      if (device_param->cuda_d_bitmap_s2_a)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s2_a);\n      if (device_param->cuda_d_bitmap_s2_b)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s2_b);\n      if (device_param->cuda_d_bitmap_s2_c)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s2_c);\n      if (device_param->cuda_d_bitmap_s2_d)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_bitmap_s2_d);\n      if (device_param->cuda_d_plain_bufs)     hc_cuMemFree (hashcat_ctx, device_param->cuda_d_plain_bufs);\n      if (device_param->cuda_d_digests_buf)    hc_cuMemFree (hashcat_ctx, device_param->cuda_d_digests_buf);\n      if (device_param->cuda_d_digests_shown)  hc_cuMemFree (hashcat_ctx, device_param->cuda_d_digests_shown);\n      if (device_param->cuda_d_salt_bufs)      hc_cuMemFree (hashcat_ctx, device_param->cuda_d_salt_bufs);\n      if (device_param->cuda_d_esalt_bufs)     hc_cuMemFree (hashcat_ctx, device_param->cuda_d_esalt_bufs);\n      if (device_param->cuda_d_tmps)           hc_cuMemFree (hashcat_ctx, device_param->cuda_d_tmps);\n      if (device_param->cuda_d_hooks)          hc_cuMemFree (hashcat_ctx, device_param->cuda_d_hooks);\n      if (device_param->cuda_d_result)         hc_cuMemFree (hashcat_ctx, device_param->cuda_d_result);\n      if (device_param->cuda_d_extra0_buf)     hc_cuMemFree (hashcat_ctx, device_param->cuda_d_extra0_buf);\n      if (device_param->cuda_d_extra1_buf)     hc_cuMemFree (hashcat_ctx, device_param->cuda_d_extra1_buf);\n      if (device_param->cuda_d_extra2_buf)     hc_cuMemFree (hashcat_ctx, device_param->cuda_d_extra2_buf);\n      if (device_param->cuda_d_extra3_buf)     hc_cuMemFree (hashcat_ctx, device_param->cuda_d_extra3_buf);\n      if (device_param->cuda_d_root_css_buf)   hc_cuMemFree (hashcat_ctx, device_param->cuda_d_root_css_buf);\n      if (device_param->cuda_d_markov_css_buf) hc_cuMemFree (hashcat_ctx, device_param->cuda_d_markov_css_buf);\n      if (device_param->cuda_d_tm_c)           hc_cuMemFree (hashcat_ctx, device_param->cuda_d_tm_c);\n      if (device_param->cuda_d_st_digests_buf) hc_cuMemFree (hashcat_ctx, device_param->cuda_d_st_digests_buf);\n      if (device_param->cuda_d_st_salts_buf)   hc_cuMemFree (hashcat_ctx, device_param->cuda_d_st_salts_buf);\n      if (device_param->cuda_d_st_esalts_buf)  hc_cuMemFree (hashcat_ctx, device_param->cuda_d_st_esalts_buf);\n\n      if (device_param->cuda_event1)           hc_cuEventDestroy (hashcat_ctx, device_param->cuda_event1);\n      if (device_param->cuda_event2)           hc_cuEventDestroy (hashcat_ctx, device_param->cuda_event2);\n\n      if (device_param->cuda_stream)           hc_cuStreamDestroy (hashcat_ctx, device_param->cuda_stream);\n\n      if (device_param->cuda_module)           hc_cuModuleUnload (hashcat_ctx, device_param->cuda_module);\n      if (device_param->cuda_module_mp)        hc_cuModuleUnload (hashcat_ctx, device_param->cuda_module_mp);\n      if (device_param->cuda_module_amp)       hc_cuModuleUnload (hashcat_ctx, device_param->cuda_module_amp);\n\n      if (device_param->cuda_context)          hc_cuCtxDestroy (hashcat_ctx, device_param->cuda_context);\n\n      device_param->cuda_d_pws_buf            = 0;\n      device_param->cuda_d_pws_amp_buf        = 0;\n      device_param->cuda_d_pws_comp_buf       = 0;\n      device_param->cuda_d_pws_idx            = 0;\n      device_param->cuda_d_rules              = 0;\n      device_param->cuda_d_rules_c            = 0;\n      device_param->cuda_d_combs              = 0;\n      device_param->cuda_d_combs_c            = 0;\n      device_param->cuda_d_bfs                = 0;\n      device_param->cuda_d_bfs_c              = 0;\n      device_param->cuda_d_bitmap_s1_a        = 0;\n      device_param->cuda_d_bitmap_s1_b        = 0;\n      device_param->cuda_d_bitmap_s1_c        = 0;\n      device_param->cuda_d_bitmap_s1_d        = 0;\n      device_param->cuda_d_bitmap_s2_a        = 0;\n      device_param->cuda_d_bitmap_s2_b        = 0;\n      device_param->cuda_d_bitmap_s2_c        = 0;\n      device_param->cuda_d_bitmap_s2_d        = 0;\n      device_param->cuda_d_plain_bufs         = 0;\n      device_param->cuda_d_digests_buf        = 0;\n      device_param->cuda_d_digests_shown      = 0;\n      device_param->cuda_d_salt_bufs          = 0;\n      device_param->cuda_d_esalt_bufs         = 0;\n      device_param->cuda_d_tmps               = 0;\n      device_param->cuda_d_hooks              = 0;\n      device_param->cuda_d_result             = 0;\n      device_param->cuda_d_extra0_buf         = 0;\n      device_param->cuda_d_extra1_buf         = 0;\n      device_param->cuda_d_extra2_buf         = 0;\n      device_param->cuda_d_extra3_buf         = 0;\n      device_param->cuda_d_root_css_buf       = 0;\n      device_param->cuda_d_markov_css_buf     = 0;\n      device_param->cuda_d_tm_c               = 0;\n      device_param->cuda_d_st_digests_buf     = 0;\n      device_param->cuda_d_st_salts_buf       = 0;\n      device_param->cuda_d_st_esalts_buf      = 0;\n\n      device_param->cuda_function1            = NULL;\n      device_param->cuda_function12           = NULL;\n      device_param->cuda_function2            = NULL;\n      device_param->cuda_function2e           = NULL;\n      device_param->cuda_function23           = NULL;\n      device_param->cuda_function3            = NULL;\n      device_param->cuda_function4            = NULL;\n      device_param->cuda_function_init2       = NULL;\n      device_param->cuda_function_loop2       = NULL;\n      device_param->cuda_function_mp          = NULL;\n      device_param->cuda_function_mp_l        = NULL;\n      device_param->cuda_function_mp_r        = NULL;\n      device_param->cuda_function_tm          = NULL;\n      device_param->cuda_function_amp         = NULL;\n      device_param->cuda_function_memset      = NULL;\n      device_param->cuda_function_atinit      = NULL;\n      device_param->cuda_function_decompress  = NULL;\n      device_param->cuda_function_aux1        = NULL;\n      device_param->cuda_function_aux2        = NULL;\n      device_param->cuda_function_aux3        = NULL;\n      device_param->cuda_function_aux4        = NULL;\n\n      device_param->cuda_module               = NULL;\n      device_param->cuda_module_mp            = NULL;\n      device_param->cuda_module_amp           = NULL;\n\n      device_param->cuda_context              = NULL;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if (device_param->opencl_d_pws_buf)        hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_pws_buf);\n      if (device_param->opencl_d_pws_amp_buf)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_pws_amp_buf);\n      if (device_param->opencl_d_pws_comp_buf)   hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_pws_comp_buf);\n      if (device_param->opencl_d_pws_idx)        hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_pws_idx);\n      if (device_param->opencl_d_rules)          hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_rules);\n      if (device_param->opencl_d_rules_c)        hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_rules_c);\n      if (device_param->opencl_d_combs)          hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_combs);\n      if (device_param->opencl_d_combs_c)        hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_combs_c);\n      if (device_param->opencl_d_bfs)            hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bfs);\n      if (device_param->opencl_d_bfs_c)          hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bfs_c);\n      if (device_param->opencl_d_bitmap_s1_a)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s1_a);\n      if (device_param->opencl_d_bitmap_s1_b)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s1_b);\n      if (device_param->opencl_d_bitmap_s1_c)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s1_c);\n      if (device_param->opencl_d_bitmap_s1_d)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s1_d);\n      if (device_param->opencl_d_bitmap_s2_a)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s2_a);\n      if (device_param->opencl_d_bitmap_s2_b)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s2_b);\n      if (device_param->opencl_d_bitmap_s2_c)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s2_c);\n      if (device_param->opencl_d_bitmap_s2_d)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_bitmap_s2_d);\n      if (device_param->opencl_d_plain_bufs)     hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_plain_bufs);\n      if (device_param->opencl_d_digests_buf)    hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_digests_buf);\n      if (device_param->opencl_d_digests_shown)  hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_digests_shown);\n      if (device_param->opencl_d_salt_bufs)      hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_salt_bufs);\n      if (device_param->opencl_d_esalt_bufs)     hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_esalt_bufs);\n      if (device_param->opencl_d_tmps)           hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_tmps);\n      if (device_param->opencl_d_hooks)          hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_hooks);\n      if (device_param->opencl_d_result)         hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_result);\n      if (device_param->opencl_d_extra0_buf)     hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_extra0_buf);\n      if (device_param->opencl_d_extra1_buf)     hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_extra1_buf);\n      if (device_param->opencl_d_extra2_buf)     hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_extra2_buf);\n      if (device_param->opencl_d_extra3_buf)     hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_extra3_buf);\n      if (device_param->opencl_d_root_css_buf)   hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_root_css_buf);\n      if (device_param->opencl_d_markov_css_buf) hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_markov_css_buf);\n      if (device_param->opencl_d_tm_c)           hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_tm_c);\n      if (device_param->opencl_d_st_digests_buf) hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_st_digests_buf);\n      if (device_param->opencl_d_st_salts_buf)   hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_st_salts_buf);\n      if (device_param->opencl_d_st_esalts_buf)  hc_clReleaseMemObject (hashcat_ctx, device_param->opencl_d_st_esalts_buf);\n\n      if (device_param->opencl_kernel1)          hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel1);\n      if (device_param->opencl_kernel12)         hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel12);\n      if (device_param->opencl_kernel2)          hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel2);\n      if (device_param->opencl_kernel2e)         hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel2e);\n      if (device_param->opencl_kernel23)         hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel23);\n      if (device_param->opencl_kernel3)          hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel3);\n      if (device_param->opencl_kernel4)          hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel4);\n      if (device_param->opencl_kernel_init2)     hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_init2);\n      if (device_param->opencl_kernel_loop2)     hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_loop2);\n      if (device_param->opencl_kernel_mp)        hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_mp);\n      if (device_param->opencl_kernel_mp_l)      hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_mp_l);\n      if (device_param->opencl_kernel_mp_r)      hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_mp_r);\n      if (device_param->opencl_kernel_tm)        hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_tm);\n      if (device_param->opencl_kernel_amp)       hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_amp);\n      if (device_param->opencl_kernel_memset)    hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_memset);\n      if (device_param->opencl_kernel_atinit)    hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_atinit);\n      if (device_param->opencl_kernel_decompress)hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_decompress);\n      if (device_param->opencl_kernel_aux1)      hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_aux1);\n      if (device_param->opencl_kernel_aux2)      hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_aux2);\n      if (device_param->opencl_kernel_aux3)      hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_aux3);\n      if (device_param->opencl_kernel_aux4)      hc_clReleaseKernel (hashcat_ctx, device_param->opencl_kernel_aux4);\n\n      if (device_param->opencl_program)          hc_clReleaseProgram (hashcat_ctx, device_param->opencl_program);\n      if (device_param->opencl_program_mp)       hc_clReleaseProgram (hashcat_ctx, device_param->opencl_program_mp);\n      if (device_param->opencl_program_amp)      hc_clReleaseProgram (hashcat_ctx, device_param->opencl_program_amp);\n\n      if (device_param->opencl_command_queue)    hc_clReleaseCommandQueue (hashcat_ctx, device_param->opencl_command_queue);\n\n      if (device_param->opencl_context)          hc_clReleaseContext (hashcat_ctx, device_param->opencl_context);\n\n      device_param->opencl_d_pws_buf           = NULL;\n      device_param->opencl_d_pws_amp_buf       = NULL;\n      device_param->opencl_d_pws_comp_buf      = NULL;\n      device_param->opencl_d_pws_idx           = NULL;\n      device_param->opencl_d_rules             = NULL;\n      device_param->opencl_d_rules_c           = NULL;\n      device_param->opencl_d_combs             = NULL;\n      device_param->opencl_d_combs_c           = NULL;\n      device_param->opencl_d_bfs               = NULL;\n      device_param->opencl_d_bfs_c             = NULL;\n      device_param->opencl_d_bitmap_s1_a       = NULL;\n      device_param->opencl_d_bitmap_s1_b       = NULL;\n      device_param->opencl_d_bitmap_s1_c       = NULL;\n      device_param->opencl_d_bitmap_s1_d       = NULL;\n      device_param->opencl_d_bitmap_s2_a       = NULL;\n      device_param->opencl_d_bitmap_s2_b       = NULL;\n      device_param->opencl_d_bitmap_s2_c       = NULL;\n      device_param->opencl_d_bitmap_s2_d       = NULL;\n      device_param->opencl_d_plain_bufs        = NULL;\n      device_param->opencl_d_digests_buf       = NULL;\n      device_param->opencl_d_digests_shown     = NULL;\n      device_param->opencl_d_salt_bufs         = NULL;\n      device_param->opencl_d_esalt_bufs        = NULL;\n      device_param->opencl_d_tmps              = NULL;\n      device_param->opencl_d_hooks             = NULL;\n      device_param->opencl_d_result            = NULL;\n      device_param->opencl_d_extra0_buf        = NULL;\n      device_param->opencl_d_extra1_buf        = NULL;\n      device_param->opencl_d_extra2_buf        = NULL;\n      device_param->opencl_d_extra3_buf        = NULL;\n      device_param->opencl_d_root_css_buf      = NULL;\n      device_param->opencl_d_markov_css_buf    = NULL;\n      device_param->opencl_d_tm_c              = NULL;\n      device_param->opencl_d_st_digests_buf    = NULL;\n      device_param->opencl_d_st_salts_buf      = NULL;\n      device_param->opencl_d_st_esalts_buf     = NULL;\n      device_param->opencl_kernel1             = NULL;\n      device_param->opencl_kernel12            = NULL;\n      device_param->opencl_kernel2             = NULL;\n      device_param->opencl_kernel2e            = NULL;\n      device_param->opencl_kernel23            = NULL;\n      device_param->opencl_kernel3             = NULL;\n      device_param->opencl_kernel4             = NULL;\n      device_param->opencl_kernel_init2        = NULL;\n      device_param->opencl_kernel_loop2        = NULL;\n      device_param->opencl_kernel_mp           = NULL;\n      device_param->opencl_kernel_mp_l         = NULL;\n      device_param->opencl_kernel_mp_r         = NULL;\n      device_param->opencl_kernel_tm           = NULL;\n      device_param->opencl_kernel_amp          = NULL;\n      device_param->opencl_kernel_memset       = NULL;\n      device_param->opencl_kernel_atinit       = NULL;\n      device_param->opencl_kernel_decompress   = NULL;\n      device_param->opencl_kernel_aux1         = NULL;\n      device_param->opencl_kernel_aux2         = NULL;\n      device_param->opencl_kernel_aux3         = NULL;\n      device_param->opencl_kernel_aux4         = NULL;\n      device_param->opencl_program             = NULL;\n      device_param->opencl_program_mp          = NULL;\n      device_param->opencl_program_amp         = NULL;\n      device_param->opencl_command_queue       = NULL;\n      device_param->opencl_context             = NULL;\n    }\n\n    device_param->pws_comp            = NULL;\n    device_param->pws_idx             = NULL;\n    device_param->pws_pre_buf         = NULL;\n    device_param->pws_base_buf        = NULL;\n    device_param->combs_buf           = NULL;\n    device_param->hooks_buf           = NULL;\n    device_param->scratch_buf         = NULL;\n    #ifdef WITH_BRAIN\n    device_param->brain_link_in_buf   = NULL;\n    device_param->brain_link_out_buf  = NULL;\n    #endif\n  }\n}\n\nvoid backend_session_reset (hashcat_ctx_t *hashcat_ctx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (backend_ctx->enabled == false) return;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    device_param->speed_pos = 0;\n\n    memset (device_param->speed_cnt,  0, SPEED_CACHE * sizeof (u64));\n    memset (device_param->speed_msec, 0, SPEED_CACHE * sizeof (double));\n\n    device_param->speed_only_finish = false;\n\n    device_param->exec_pos = 0;\n\n    memset (device_param->exec_msec, 0, EXEC_CACHE * sizeof (double));\n\n    device_param->outerloop_msec = 0;\n    device_param->outerloop_pos  = 0;\n    device_param->outerloop_left = 0;\n    device_param->innerloop_pos  = 0;\n    device_param->innerloop_left = 0;\n\n    // some more resets:\n\n    if (device_param->pws_comp) memset (device_param->pws_comp, 0, device_param->size_pws_comp);\n    if (device_param->pws_idx)  memset (device_param->pws_idx,  0, device_param->size_pws_idx);\n\n    device_param->pws_cnt = 0;\n\n    device_param->words_off  = 0;\n    device_param->words_done = 0;\n\n    #if defined (_WIN)\n    device_param->timer_speed.QuadPart = 0;\n    #else\n    device_param->timer_speed.tv_sec = 0;\n    #endif\n  }\n\n  backend_ctx->kernel_power_all   = 0;\n  backend_ctx->kernel_power_final = 0;\n}\n\nint backend_session_update_combinator (hashcat_ctx_t *hashcat_ctx)\n{\n  combinator_ctx_t *combinator_ctx = hashcat_ctx->combinator_ctx;\n  hashconfig_t     *hashconfig     = hashcat_ctx->hashconfig;\n  backend_ctx_t    *backend_ctx    = hashcat_ctx->backend_ctx;\n  user_options_t   *user_options   = hashcat_ctx->user_options;\n\n  if (backend_ctx->enabled == false) return 0;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    if (device_param->skipped_warning == true) continue;\n\n    // kernel_params\n\n    device_param->kernel_params_buf32[33] = combinator_ctx->combs_mode;\n\n    /*\n    if (device_param->is_opencl == true)\n    {\n      CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel1, 33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1;\n      CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel2, 33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1;\n      CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel3, 33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1;\n      CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel4, 33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1;\n\n      if (hashconfig->opts_type & OPTS_TYPE_HOOK12) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel12,     33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1; }\n      if (hashconfig->opts_type & OPTS_TYPE_HOOK23) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel23,     33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1; }\n      if (hashconfig->opts_type & OPTS_TYPE_INIT2)  { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_init2, 33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1; }\n      if (hashconfig->opts_type & OPTS_TYPE_LOOP2)  { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_loop2, 33, sizeof (cl_uint), device_param->kernel_params[33]); if (CL_rc == -1) return -1; }\n    }\n    */\n\n    // kernel_params_amp\n\n    if (user_options->slow_candidates == true)\n    {\n    }\n    else\n    {\n      device_param->kernel_params_amp_buf32[5] = combinator_ctx->combs_mode;\n\n      if (hashconfig->attack_exec == ATTACK_EXEC_OUTSIDE_KERNEL)\n      {\n        if (device_param->is_opencl == true)\n        {\n          const int rc_clSetKernelArg = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_amp, 5, sizeof (cl_uint), device_param->kernel_params_amp[5]);\n\n          if (rc_clSetKernelArg == -1) return -1;\n        }\n      }\n    }\n  }\n\n  return 0;\n}\n\nint backend_session_update_mp (hashcat_ctx_t *hashcat_ctx)\n{\n  mask_ctx_t     *mask_ctx     = hashcat_ctx->mask_ctx;\n  backend_ctx_t  *backend_ctx  = hashcat_ctx->backend_ctx;\n  user_options_t *user_options = hashcat_ctx->user_options;\n\n  if (backend_ctx->enabled == false) return 0;\n\n  if (user_options->slow_candidates == true) return 0;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    if (device_param->skipped_warning == true) continue;\n\n    device_param->kernel_params_mp_buf64[3] = 0;\n    device_param->kernel_params_mp_buf32[4] = mask_ctx->css_cnt;\n\n    if (device_param->is_cuda == true)\n    {\n      //for (u32 i = 3; i < 4; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, i, sizeof (cl_ulong), device_param->kernel_params_mp[i]); if (CL_rc == -1) return -1; }\n      //for (u32 i = 4; i < 8; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, i, sizeof (cl_uint),  device_param->kernel_params_mp[i]); if (CL_rc == -1) return -1; }\n\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_root_css_buf,   mask_ctx->root_css_buf,   device_param->size_root_css)   == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_markov_css_buf, mask_ctx->markov_css_buf, device_param->size_markov_css) == -1) return -1;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      for (u32 i = 3; i < 4; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, i, sizeof (cl_ulong), device_param->kernel_params_mp[i]) == -1) return -1; }\n      for (u32 i = 4; i < 8; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp, i, sizeof (cl_uint),  device_param->kernel_params_mp[i]) == -1) return -1; }\n\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_root_css_buf,   CL_TRUE, 0, device_param->size_root_css,   mask_ctx->root_css_buf,   0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_markov_css_buf, CL_TRUE, 0, device_param->size_markov_css, mask_ctx->markov_css_buf, 0, NULL, NULL) == -1) return -1;\n    }\n  }\n\n  return 0;\n}\n\nint backend_session_update_mp_rl (hashcat_ctx_t *hashcat_ctx, const u32 css_cnt_l, const u32 css_cnt_r)\n{\n  mask_ctx_t     *mask_ctx     = hashcat_ctx->mask_ctx;\n  backend_ctx_t   *backend_ctx   = hashcat_ctx->backend_ctx;\n  user_options_t *user_options = hashcat_ctx->user_options;\n\n  if (backend_ctx->enabled == false) return 0;\n\n  if (user_options->slow_candidates == true) return 0;\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    if (device_param->skipped_warning == true) continue;\n\n    device_param->kernel_params_mp_l_buf64[3] = 0;\n    device_param->kernel_params_mp_l_buf32[4] = css_cnt_l;\n    device_param->kernel_params_mp_l_buf32[5] = css_cnt_r;\n\n    device_param->kernel_params_mp_r_buf64[3] = 0;\n    device_param->kernel_params_mp_r_buf32[4] = css_cnt_r;\n\n    if (device_param->is_cuda == true)\n    {\n      //for (u32 i = 3; i < 4; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_ulong), device_param->kernel_params_mp_l[i]); if (CL_rc == -1) return -1; }\n      //for (u32 i = 4; i < 8; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_uint),  device_param->kernel_params_mp_l[i]); if (CL_rc == -1) return -1; }\n      //for (u32 i = 9; i < 9; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_ulong), device_param->kernel_params_mp_l[i]); if (CL_rc == -1) return -1; }\n\n      //for (u32 i = 3; i < 4; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_ulong), device_param->kernel_params_mp_r[i]); if (CL_rc == -1) return -1; }\n      //for (u32 i = 4; i < 7; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_uint),  device_param->kernel_params_mp_r[i]); if (CL_rc == -1) return -1; }\n      //for (u32 i = 8; i < 8; i++) { CL_rc = hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_ulong), device_param->kernel_params_mp_r[i]); if (CL_rc == -1) return -1; }\n\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_root_css_buf,   mask_ctx->root_css_buf,   device_param->size_root_css)   == -1) return -1;\n      if (hc_cuMemcpyHtoD (hashcat_ctx, device_param->cuda_d_markov_css_buf, mask_ctx->markov_css_buf, device_param->size_markov_css) == -1) return -1;\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      for (u32 i = 3; i < 4; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_ulong), device_param->kernel_params_mp_l[i]) == -1) return -1; }\n      for (u32 i = 4; i < 8; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_uint),  device_param->kernel_params_mp_l[i]) == -1) return -1; }\n      for (u32 i = 9; i < 9; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_l, i, sizeof (cl_ulong), device_param->kernel_params_mp_l[i]) == -1) return -1; }\n\n      for (u32 i = 3; i < 4; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_ulong), device_param->kernel_params_mp_r[i]) == -1) return -1; }\n      for (u32 i = 4; i < 7; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_uint),  device_param->kernel_params_mp_r[i]) == -1) return -1; }\n      for (u32 i = 8; i < 8; i++) { if (hc_clSetKernelArg (hashcat_ctx, device_param->opencl_kernel_mp_r, i, sizeof (cl_ulong), device_param->kernel_params_mp_r[i]) == -1) return -1; }\n\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_root_css_buf,   CL_TRUE, 0, device_param->size_root_css,   mask_ctx->root_css_buf,   0, NULL, NULL) == -1) return -1;\n      if (hc_clEnqueueWriteBuffer (hashcat_ctx, device_param->opencl_command_queue, device_param->opencl_d_markov_css_buf, CL_TRUE, 0, device_param->size_markov_css, mask_ctx->markov_css_buf, 0, NULL, NULL) == -1) return -1;\n    }\n  }\n\n  return 0;\n}\n\nvoid *hook12_thread (void *p)\n{\n  hook_thread_param_t *hook_thread_param = (hook_thread_param_t *) p;\n\n  module_ctx_t *module_ctx = hook_thread_param->module_ctx;\n  status_ctx_t *status_ctx = hook_thread_param->status_ctx;\n\n  const u64 tid     = hook_thread_param->tid;\n  const u64 tsz     = hook_thread_param->tsz;\n  const u64 pws_cnt = hook_thread_param->pws_cnt;\n\n  for (u64 pw_pos = tid; pw_pos < pws_cnt; pw_pos += tsz)\n  {\n    while (status_ctx->devices_status == STATUS_PAUSED) sleep (1);\n\n    if (status_ctx->devices_status == STATUS_RUNNING)\n    {\n      module_ctx->module_hook12 (hook_thread_param->device_param, hook_thread_param->hook_salts_buf, hook_thread_param->salt_pos, pw_pos);\n    }\n  }\n\n  return NULL;\n}\n\nvoid *hook23_thread (void *p)\n{\n  hook_thread_param_t *hook_thread_param = (hook_thread_param_t *) p;\n\n  module_ctx_t *module_ctx = hook_thread_param->module_ctx;\n  status_ctx_t *status_ctx = hook_thread_param->status_ctx;\n\n  const u64 tid     = hook_thread_param->tid;\n  const u64 tsz     = hook_thread_param->tsz;\n  const u64 pws_cnt = hook_thread_param->pws_cnt;\n\n  for (u64 pw_pos = tid; pw_pos < pws_cnt; pw_pos += tsz)\n  {\n    while (status_ctx->devices_status == STATUS_PAUSED) sleep (1);\n\n    if (status_ctx->devices_status == STATUS_RUNNING)\n    {\n      module_ctx->module_hook23 (hook_thread_param->device_param, hook_thread_param->hook_salts_buf, hook_thread_param->salt_pos, pw_pos);\n    }\n  }\n\n  return NULL;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/src/dynloader.c": "/**\n * Author......: See docs/credits.txt\n * License.....: MIT\n */\n\n#include \"common.h\"\n#include \"types.h\"\n#include \"dynloader.h\"\n\n#ifdef _WIN\n\nhc_dynlib_t hc_dlopen (LPCSTR lpLibFileName)\n{\n  return LoadLibraryA (lpLibFileName);\n}\n\nBOOL hc_dlclose (hc_dynlib_t hLibModule)\n{\n  return FreeLibrary (hLibModule);\n}\n\nhc_dynfunc_t hc_dlsym (hc_dynlib_t hModule, LPCSTR lpProcName)\n{\n  return GetProcAddress (hModule, lpProcName);\n}\n\n#else\n\nhc_dynlib_t hc_dlopen (const char *filename)\n{\n  return dlopen (filename, RTLD_NOW);\n}\n\nint hc_dlclose (hc_dynlib_t handle)\n{\n  return dlclose (handle);\n}\n\nhc_dynfunc_t hc_dlsym (hc_dynlib_t handle, const char *symbol)\n{\n  return dlsym (handle, symbol);\n}\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/src/hwmon.c": "/**\n * Author......: See docs/credits.txt\n * License.....: MIT\n */\n\n#include \"common.h\"\n#include \"types.h\"\n#include \"memory.h\"\n#include \"event.h\"\n#include \"dynloader.h\"\n#include \"shared.h\"\n#include \"folder.h\"\n#include \"hwmon.h\"\n\n// sysfs functions\n\nstatic bool sysfs_init (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  SYSFS_PTR *sysfs = (SYSFS_PTR *) hwmon_ctx->hm_sysfs;\n\n  memset (sysfs, 0, sizeof (SYSFS_PTR));\n\n  char *path;\n\n  hc_asprintf (&path, \"%s\", SYS_BUS_PCI_DEVICES);\n\n  const bool r = hc_path_read (path);\n\n  hcfree (path);\n\n  return r;\n}\n\nstatic void sysfs_close (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  SYSFS_PTR *sysfs = (SYSFS_PTR *) hwmon_ctx->hm_sysfs;\n\n  if (sysfs)\n  {\n    hcfree (sysfs);\n  }\n}\n\nstatic char *hm_SYSFS_get_syspath_device (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  hc_device_param_t *device_param = &backend_ctx->devices_param[backend_device_idx];\n\n  char *syspath;\n\n  hc_asprintf (&syspath, \"%s/0000:%02x:%02x.%01x\", SYS_BUS_PCI_DEVICES, device_param->pcie_bus, device_param->pcie_device, device_param->pcie_function);\n\n  return syspath;\n}\n\nstatic char *hm_SYSFS_get_syspath_hwmon (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  char *syspath = hm_SYSFS_get_syspath_device (hashcat_ctx, backend_device_idx);\n\n  if (syspath == NULL)\n  {\n    event_log_error (hashcat_ctx, \"hm_SYSFS_get_syspath_device() failed.\");\n\n    return NULL;\n  }\n\n  char *hwmon = (char *) hcmalloc (HCBUFSIZ_TINY);\n\n  snprintf (hwmon, HCBUFSIZ_TINY, \"%s/hwmon\", syspath);\n\n  char *hwmonN = first_file_in_directory (hwmon);\n\n  if (hwmonN == NULL)\n  {\n    event_log_error (hashcat_ctx, \"First_file_in_directory() failed.\");\n\n    hcfree (syspath);\n\n    hcfree (hwmon);\n    hcfree (hwmonN);\n\n    return NULL;\n  }\n\n  snprintf (hwmon, HCBUFSIZ_TINY, \"%s/hwmon/%s\", syspath, hwmonN);\n\n  hcfree (syspath);\n\n  hcfree (hwmonN);\n\n  return hwmon;\n}\n\nstatic int hm_SYSFS_get_fan_speed_current (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx, int *val)\n{\n  char *syspath = hm_SYSFS_get_syspath_hwmon (hashcat_ctx, backend_device_idx);\n\n  if (syspath == NULL) return -1;\n\n  char *path_cur;\n  char *path_max;\n\n  hc_asprintf (&path_cur, \"%s/pwm1\",     syspath);\n  hc_asprintf (&path_max, \"%s/pwm1_max\", syspath);\n\n  hcfree (syspath);\n\n  HCFILE fp_cur;\n\n  if (hc_fopen (&fp_cur, path_cur, \"r\") == false)\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", path_cur, strerror (errno));\n\n    hcfree (path_cur);\n    hcfree (path_max);\n\n    return -1;\n  }\n\n  int pwm1_cur = 0;\n\n  if (hc_fscanf (&fp_cur, \"%d\", &pwm1_cur) != 1)\n  {\n    hc_fclose (&fp_cur);\n\n    event_log_error (hashcat_ctx, \"%s: unexpected data.\", path_cur);\n\n    hcfree (path_cur);\n    hcfree (path_max);\n\n    return -1;\n  }\n\n  hc_fclose (&fp_cur);\n\n  HCFILE fp_max;\n\n  if (hc_fopen (&fp_max, path_max, \"r\") == false)\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", path_max, strerror (errno));\n\n    hcfree (path_cur);\n    hcfree (path_max);\n\n    return -1;\n  }\n\n  int pwm1_max = 0;\n\n  if (hc_fscanf (&fp_max, \"%d\", &pwm1_max) != 1)\n  {\n    hc_fclose (&fp_max);\n\n    event_log_error (hashcat_ctx, \"%s: unexpected data.\", path_max);\n\n    hcfree (path_cur);\n    hcfree (path_max);\n\n    return -1;\n  }\n\n  hc_fclose (&fp_max);\n\n  if (pwm1_max == 0)\n  {\n    event_log_error (hashcat_ctx, \"%s: pwm1_max cannot be 0.\", path_max);\n\n    hcfree (path_cur);\n    hcfree (path_max);\n\n    return -1;\n  }\n\n  const float p1 = (float) pwm1_max / 100.0F;\n\n  const float pwm1_percent = (float) pwm1_cur / p1;\n\n  *val = (int) pwm1_percent;\n\n  hcfree (path_cur);\n  hcfree (path_max);\n\n  return 0;\n}\n\nstatic int hm_SYSFS_get_temperature_current (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx, int *val)\n{\n  char *syspath = hm_SYSFS_get_syspath_hwmon (hashcat_ctx, backend_device_idx);\n\n  if (syspath == NULL) return -1;\n\n  char *path;\n\n  hc_asprintf (&path, \"%s/temp1_input\", syspath);\n\n  hcfree (syspath);\n\n  HCFILE fp;\n\n  if (hc_fopen (&fp, path, \"r\") == false)\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", path, strerror (errno));\n\n    hcfree (path);\n\n    return -1;\n  }\n\n  int temperature = 0;\n\n  if (hc_fscanf (&fp, \"%d\", &temperature) != 1)\n  {\n    hc_fclose (&fp);\n\n    event_log_error (hashcat_ctx, \"%s: unexpected data.\", path);\n\n    hcfree (path);\n\n    return -1;\n  }\n\n  hc_fclose (&fp);\n\n  *val = temperature / 1000;\n\n  hcfree (path);\n\n  return 0;\n}\n\nstatic int hm_SYSFS_get_pp_dpm_sclk (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx, int *val)\n{\n  char *syspath = hm_SYSFS_get_syspath_device (hashcat_ctx, backend_device_idx);\n\n  if (syspath == NULL) return -1;\n\n  char *path;\n\n  hc_asprintf (&path, \"%s/pp_dpm_sclk\", syspath);\n\n  hcfree (syspath);\n\n  HCFILE fp;\n\n  if (hc_fopen (&fp, path, \"r\") == false)\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", path, strerror (errno));\n\n    hcfree (path);\n\n    return -1;\n  }\n\n  int clockfreq = 0;\n\n  while (!hc_feof (&fp))\n  {\n    char buf[HCBUFSIZ_TINY] = { 0 };\n\n    char *ptr = hc_fgets (buf, sizeof (buf), &fp);\n\n    if (ptr == NULL) continue;\n\n    size_t len = strlen (ptr);\n\n    if (len < 2) continue;\n\n    if (ptr[len - 2] != '*') continue;\n\n    int profile = 0;\n\n    int rc = sscanf (ptr, \"%d: %dMHz\", &profile, &clockfreq);\n\n    if (rc == 2) break;\n  }\n\n  hc_fclose (&fp);\n\n  *val = clockfreq;\n\n  hcfree (path);\n\n  return 0;\n}\n\nstatic int hm_SYSFS_get_pp_dpm_mclk (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx, int *val)\n{\n  char *syspath = hm_SYSFS_get_syspath_device (hashcat_ctx, backend_device_idx);\n\n  if (syspath == NULL) return -1;\n\n  char *path;\n\n  hc_asprintf (&path, \"%s/pp_dpm_mclk\", syspath);\n\n  hcfree (syspath);\n\n  HCFILE fp;\n\n  if (hc_fopen (&fp, path, \"r\") == false)\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", path, strerror (errno));\n\n    hcfree (path);\n\n    return -1;\n  }\n\n  int clockfreq = 0;\n\n  while (!hc_feof (&fp))\n  {\n    char buf[HCBUFSIZ_TINY];\n\n    char *ptr = hc_fgets (buf, sizeof (buf), &fp);\n\n    if (ptr == NULL) continue;\n\n    size_t len = strlen (ptr);\n\n    if (len < 2) continue;\n\n    if (ptr[len - 2] != '*') continue;\n\n    int profile = 0;\n\n    int rc = sscanf (ptr, \"%d: %dMHz\", &profile, &clockfreq);\n\n    if (rc == 2) break;\n  }\n\n  hc_fclose (&fp);\n\n  *val = clockfreq;\n\n  hcfree (path);\n\n  return 0;\n}\n\nstatic int hm_SYSFS_get_pp_dpm_pcie (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx, int *val)\n{\n  char *syspath = hm_SYSFS_get_syspath_device (hashcat_ctx, backend_device_idx);\n\n  if (syspath == NULL) return -1;\n\n  char *path;\n\n  hc_asprintf (&path, \"%s/pp_dpm_pcie\", syspath);\n\n  hcfree (syspath);\n\n  HCFILE fp;\n\n  if (hc_fopen (&fp, path, \"r\") == false)\n  {\n    event_log_error (hashcat_ctx, \"%s: %s\", path, strerror (errno));\n\n    hcfree (path);\n\n    return -1;\n  }\n\n  int lanes = 0;\n\n  while (!hc_feof (&fp))\n  {\n    char buf[HCBUFSIZ_TINY];\n\n    char *ptr = hc_fgets (buf, sizeof (buf), &fp);\n\n    if (ptr == NULL) continue;\n\n    size_t len = strlen (ptr);\n\n    if (len < 2) continue;\n\n    if (ptr[len - 2] != '*') continue;\n\n    int   profile = 0;\n    float speed = 0;\n\n    int rc = sscanf (ptr, \"%d: %fGB, x%d *\", &profile, &speed, &lanes);\n\n    if (rc == 3) break;\n  }\n\n  hc_fclose (&fp);\n\n  *val = lanes;\n\n  hcfree (path);\n\n  return 0;\n}\n\n// nvml functions\n\nstatic int nvml_init (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  memset (nvml, 0, sizeof (NVML_PTR));\n\n  #if defined (_WIN)\n\n  nvml->lib = hc_dlopen (\"nvml.dll\");\n\n  if (!nvml->lib)\n  {\n    DWORD BufferSize = 1024;\n\n    DWORD Type = REG_SZ;\n\n    char *Buffer = (char *) hcmalloc (BufferSize + 1);\n\n    HKEY hKey = 0;\n\n    if (RegOpenKeyExA (HKEY_LOCAL_MACHINE, \"SOFTWARE\\\\NVIDIA Corporation\\\\Global\\\\NVSMI\", 0, KEY_QUERY_VALUE, &hKey) == ERROR_SUCCESS)\n    {\n      if (RegQueryValueExA (hKey, \"NVSMIPATH\", NULL, &Type, (LPBYTE)Buffer, &BufferSize) == ERROR_SUCCESS)\n      {\n        Buffer[BufferSize] = 0;\n      }\n      else\n      {\n        //if (user_options->quiet == false)\n        //  event_log_error (hashcat_ctx, \"NVML library load failed. Proceeding without NVML HWMon enabled.\");\n\n        return -1;\n      }\n\n      RegCloseKey (hKey);\n    }\n    else\n    {\n      //if (user_options->quiet == false)\n      //  event_log_error (hashcat_ctx, \"NVML library load failed. Proceeding without NVML HWMon enabled.\");\n\n      return -1;\n    }\n\n    strcat (Buffer, \"\\\\nvml.dll\");\n\n    nvml->lib = hc_dlopen (Buffer);\n\n    hcfree (Buffer);\n  }\n\n  #elif defined (__CYGWIN__)\n\n  nvml->lib = hc_dlopen (\"nvml.dll\");\n\n  if (!nvml->lib)\n  {\n    HCFILE nvml_lib;\n\n    if (hc_fopen (&nvml_lib, \"/proc/registry/HKEY_LOCAL_MACHINE/SOFTWARE/NVIDIA Corporation/Global/NVSMI/NVSMIPATH\", \"rb\") == false)\n    {\n      //if (user_options->quiet == false)\n      //  event_log_error (hashcat_ctx, \"NVML library load failed: %m. Proceeding without NVML HWMon enabled.\");\n\n      return -1;\n    }\n\n    char *nvml_winpath, *nvml_cygpath;\n\n    nvml_winpath = (char *) hcmalloc (100);\n\n    hc_fread (nvml_winpath, 100, 1, &nvml_lib);\n\n    hc_fclose (&nvml_lib);\n\n    ssize_t size = cygwin_conv_path (CCP_WIN_A_TO_POSIX | CCP_PROC_CYGDRIVE, nvml_winpath, NULL, 0);\n\n    if (size > 0)\n    {\n      nvml_cygpath = (char *) hcmalloc (size + 9);\n\n      cygwin_conv_path (CCP_WIN_A_TO_POSIX | CCP_PROC_CYGDRIVE, nvml_winpath, nvml_cygpath, size);\n    }\n    else\n    {\n      //if (user_options->quiet == false)\n      //  event_log_error (hashcat_ctx, \"Could not find NVML on this system. Proceeding without NVML HWMon enabled.\");\n\n      return -1;\n    }\n\n    strcat (nvml_cygpath, \"/nvml.dll\");\n\n    nvml->lib = hc_dlopen (nvml_cygpath);\n  }\n\n  #elif defined (_POSIX)\n\n  nvml->lib = hc_dlopen (\"libnvidia-ml.so\");\n\n  if (!nvml->lib)\n  {\n    nvml->lib = hc_dlopen (\"libnvidia-ml.so.1\");\n  }\n\n  #endif\n\n  if (!nvml->lib)\n  {\n    //if (user_options->quiet == false)\n    //  event_log_error (hashcat_ctx, \"NVML library load failed. Proceeding without NVML HWMon enabled.\");\n\n    return -1;\n  }\n\n  HC_LOAD_FUNC(nvml, nvmlErrorString, NVML_ERROR_STRING, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlInit, NVML_INIT, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlShutdown, NVML_SHUTDOWN, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetCount, NVML_DEVICE_GET_COUNT, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetName, NVML_DEVICE_GET_NAME, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetHandleByIndex, NVML_DEVICE_GET_HANDLE_BY_INDEX, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetTemperature, NVML_DEVICE_GET_TEMPERATURE, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetFanSpeed, NVML_DEVICE_GET_FAN_SPEED, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetUtilizationRates, NVML_DEVICE_GET_UTILIZATION_RATES, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetClockInfo, NVML_DEVICE_GET_CLOCKINFO, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetTemperatureThreshold, NVML_DEVICE_GET_THRESHOLD, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetCurrPcieLinkGeneration, NVML_DEVICE_GET_CURRPCIELINKGENERATION, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetCurrPcieLinkWidth, NVML_DEVICE_GET_CURRPCIELINKWIDTH, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetCurrentClocksThrottleReasons, NVML_DEVICE_GET_CURRENTCLOCKSTHROTTLEREASONS, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetSupportedClocksThrottleReasons, NVML_DEVICE_GET_SUPPORTEDCLOCKSTHROTTLEREASONS, NVML, 0);\n  HC_LOAD_FUNC(nvml, nvmlDeviceGetPciInfo, NVML_DEVICE_GET_PCIINFO, NVML, 0);\n\n  return 0;\n}\n\nstatic void nvml_close (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  if (nvml)\n  {\n    if (nvml->lib)\n      hc_dlclose (nvml->lib);\n\n    hcfree (nvml);\n  }\n}\n\nstatic const char *hm_NVML_nvmlErrorString (NVML_PTR *nvml, const nvmlReturn_t nvml_rc)\n{\n  return nvml->nvmlErrorString (nvml_rc);\n}\n\nstatic int hm_NVML_nvmlInit (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = (nvmlReturn_t) nvml->nvmlInit ();\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlInit(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlShutdown (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = (nvmlReturn_t) nvml->nvmlShutdown ();\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlShutdown(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetCount (hashcat_ctx_t *hashcat_ctx, unsigned int *deviceCount)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetCount (deviceCount);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetCount(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetHandleByIndex (hashcat_ctx_t *hashcat_ctx, unsigned int device_index, nvmlDevice_t *device)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetHandleByIndex (device_index, device);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetHandleByIndex(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetTemperature (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, nvmlTemperatureSensors_t sensorType, unsigned int *temp)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetTemperature (device, sensorType, temp);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetTemperature(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetFanSpeed (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, unsigned int *speed)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetFanSpeed (device, speed);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetFanSpeed(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetUtilizationRates (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, nvmlUtilization_t *utilization)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetUtilizationRates (device, utilization);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetUtilizationRates(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetClockInfo (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, nvmlClockType_t type, unsigned int *clockfreq)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetClockInfo (device, type, clockfreq);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetClockInfo(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetTemperatureThreshold (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, nvmlTemperatureThresholds_t thresholdType, unsigned int *temp)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetTemperatureThreshold (device, thresholdType, temp);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetTemperatureThreshold(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetCurrPcieLinkWidth (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, unsigned int *currLinkWidth)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetCurrPcieLinkWidth (device, currLinkWidth);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetCurrPcieLinkWidth(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NVML_nvmlDeviceGetPciInfo (hashcat_ctx_t *hashcat_ctx, nvmlDevice_t device, nvmlPciInfo_t *pci)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVML_PTR *nvml = (NVML_PTR *) hwmon_ctx->hm_nvml;\n\n  const nvmlReturn_t nvml_rc = nvml->nvmlDeviceGetPciInfo (device, pci);\n\n  if (nvml_rc != NVML_SUCCESS)\n  {\n    const char *string = hm_NVML_nvmlErrorString (nvml, nvml_rc);\n\n    event_log_error (hashcat_ctx, \"nvmlDeviceGetPciInfo(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\n// nvapi functions\n\nstatic int nvapi_init (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  memset (nvapi, 0, sizeof (NVAPI_PTR));\n\n  #if defined (_WIN)\n\n  #if defined (_WIN64)\n  nvapi->lib = hc_dlopen (\"nvapi64.dll\");\n  #else\n  nvapi->lib = hc_dlopen (\"nvapi.dll\");\n  #endif\n\n  #else\n\n  #if defined (__CYGWIN__)\n\n  #if defined (__x86_x64__)\n  nvapi->lib = hc_dlopen (\"nvapi64.dll\");\n  #else\n  nvapi->lib = hc_dlopen (\"nvapi.dll\");\n  #endif\n\n  #else\n  nvapi->lib = hc_dlopen (\"nvapi.so\"); // uhm yes, but .. yeah\n  #endif\n\n  #endif\n\n  if (!nvapi->lib)\n  {\n    //if (user_options->quiet == false)\n    //  event_log_error (hashcat_ctx, \"Load of NVAPI library failed. Proceeding without NVAPI HWMon enabled.\");\n\n    return -1;\n  }\n\n  HC_LOAD_FUNC(nvapi, nvapi_QueryInterface,             NVAPI_QUERYINTERFACE,             NVAPI,                0);\n  HC_LOAD_ADDR(nvapi, NvAPI_Initialize,                 NVAPI_INITIALIZE,                 nvapi_QueryInterface, 0x0150E828U, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_Unload,                     NVAPI_UNLOAD,                     nvapi_QueryInterface, 0xD22BDD7EU, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_GetErrorMessage,            NVAPI_GETERRORMESSAGE,            nvapi_QueryInterface, 0x6C2D048CU, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_EnumPhysicalGPUs,           NVAPI_ENUMPHYSICALGPUS,           nvapi_QueryInterface, 0xE5AC921FU, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_GPU_GetPerfPoliciesInfo,    NVAPI_GPU_GETPERFPOLICIESINFO,    nvapi_QueryInterface, 0x409D9841U, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_GPU_GetPerfPoliciesStatus,  NVAPI_GPU_GETPERFPOLICIESSTATUS,  nvapi_QueryInterface, 0x3D358A0CU, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_GPU_GetBusId,               NVAPI_GPU_GETBUSID,               nvapi_QueryInterface, 0x1BE0B8E5U, NVAPI, 0);\n  HC_LOAD_ADDR(nvapi, NvAPI_GPU_GetBusSlotId,           NVAPI_GPU_GETBUSSLOTID,           nvapi_QueryInterface, 0x2A0A350FU, NVAPI, 0);\n\n  return 0;\n}\n\nstatic void nvapi_close (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  if (nvapi)\n  {\n    if (nvapi->lib)\n      hc_dlclose (nvapi->lib);\n\n    hcfree (nvapi);\n  }\n}\n\nstatic void hm_NvAPI_GetErrorMessage (NVAPI_PTR *nvapi, const NvAPI_Status NvAPI_rc, NvAPI_ShortString string)\n{\n  nvapi->NvAPI_GetErrorMessage (NvAPI_rc, string);\n}\n\nstatic int hm_NvAPI_Initialize (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_Initialize ();\n\n  if (NvAPI_rc == NVAPI_LIBRARY_NOT_FOUND) return -1;\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_Initialize(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NvAPI_Unload (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_Unload ();\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_Unload(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NvAPI_EnumPhysicalGPUs (hashcat_ctx_t *hashcat_ctx, NvPhysicalGpuHandle nvGPUHandle[NVAPI_MAX_PHYSICAL_GPUS], NvU32 *pGpuCount)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_EnumPhysicalGPUs (nvGPUHandle, pGpuCount);\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_EnumPhysicalGPUs(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NvAPI_GPU_GetPerfPoliciesInfo (hashcat_ctx_t *hashcat_ctx, NvPhysicalGpuHandle hPhysicalGpu, NV_GPU_PERF_POLICIES_INFO_PARAMS_V1 *perfPolicies_info)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_GPU_GetPerfPoliciesInfo (hPhysicalGpu, perfPolicies_info);\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_GPU_GetPerfPoliciesInfo(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NvAPI_GPU_GetPerfPoliciesStatus (hashcat_ctx_t *hashcat_ctx, NvPhysicalGpuHandle hPhysicalGpu, NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1 *perfPolicies_status)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_GPU_GetPerfPoliciesStatus (hPhysicalGpu, perfPolicies_status);\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_GPU_GetPerfPoliciesStatus(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NvAPI_GPU_GetBusId (hashcat_ctx_t *hashcat_ctx, NvPhysicalGpuHandle hPhysicalGpu, NvU32 *pBusId)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_GPU_GetBusId (hPhysicalGpu, pBusId);\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_GPU_GetBusId(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_NvAPI_GPU_GetBusSlotId (hashcat_ctx_t *hashcat_ctx, NvPhysicalGpuHandle hPhysicalGpu, NvU32 *pBusSlotId)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  NVAPI_PTR *nvapi = (NVAPI_PTR *) hwmon_ctx->hm_nvapi;\n\n  const NvAPI_Status NvAPI_rc = (NvAPI_Status) nvapi->NvAPI_GPU_GetBusSlotId (hPhysicalGpu, pBusSlotId);\n\n  if (NvAPI_rc != NVAPI_OK)\n  {\n    NvAPI_ShortString string = { 0 };\n\n    hm_NvAPI_GetErrorMessage (nvapi, NvAPI_rc, string);\n\n    event_log_error (hashcat_ctx, \"NvAPI_GPU_GetBusSlotId(): %s\", string);\n\n    return -1;\n  }\n\n  return 0;\n}\n\n// ADL functions\n\nstatic int adl_init (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  memset (adl, 0, sizeof (ADL_PTR));\n\n  #if defined (_WIN)\n  adl->lib = hc_dlopen (\"atiadlxx.dll\");\n\n  if (!adl->lib)\n  {\n    adl->lib = hc_dlopen (\"atiadlxy.dll\");\n  }\n  #elif defined (__CYGWIN__)\n  adl->lib = hc_dlopen (\"atiadlxx.dll\");\n\n  if (!adl->lib)\n  {\n    adl->lib = hc_dlopen (\"atiadlxy.dll\");\n  }\n  #elif defined (_POSIX)\n  adl->lib = hc_dlopen (\"libatiadlxx.so\");\n  #endif\n\n  if (!adl->lib)\n  {\n    //if (user_options->quiet == false)\n    //  event_log_error (hashcat_ctx, \"Load of ADL library failed. Proceeding without ADL HWMon enabled.\");\n\n    return -1;\n  }\n\n  HC_LOAD_FUNC(adl, ADL_Main_Control_Destroy, ADL_MAIN_CONTROL_DESTROY, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Main_Control_Create, ADL_MAIN_CONTROL_CREATE, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Adapter_NumberOfAdapters_Get, ADL_ADAPTER_NUMBEROFADAPTERS_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Adapter_AdapterInfo_Get, ADL_ADAPTER_ADAPTERINFO_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Display_DisplayInfo_Get, ADL_DISPLAY_DISPLAYINFO_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Adapter_ID_Get, ADL_ADAPTER_ID_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Adapter_VideoBiosInfo_Get, ADL_ADAPTER_VIDEOBIOSINFO_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive5_ThermalDevices_Enum, ADL_OVERDRIVE5_THERMALDEVICES_ENUM, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive5_Temperature_Get, ADL_OVERDRIVE5_TEMPERATURE_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_Temperature_Get, ADL_OVERDRIVE6_TEMPERATURE_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive5_CurrentActivity_Get, ADL_OVERDRIVE5_CURRENTACTIVITY_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive5_FanSpeedInfo_Get, ADL_OVERDRIVE5_FANSPEEDINFO_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive5_FanSpeed_Get, ADL_OVERDRIVE5_FANSPEED_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_FanSpeed_Get, ADL_OVERDRIVE6_FANSPEED_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Adapter_Active_Get, ADL_ADAPTER_ACTIVE_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive_Caps, ADL_OVERDRIVE_CAPS, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_Capabilities_Get, ADL_OVERDRIVE6_CAPABILITIES_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_StateInfo_Get, ADL_OVERDRIVE6_STATEINFO_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_CurrentStatus_Get, ADL_OVERDRIVE6_CURRENTSTATUS_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_TargetTemperatureData_Get, ADL_OVERDRIVE6_TARGETTEMPERATUREDATA_GET, ADL, 0);\n  HC_LOAD_FUNC(adl, ADL_Overdrive6_TargetTemperatureRangeInfo_Get, ADL_OVERDRIVE6_TARGETTEMPERATURERANGEINFO_GET, ADL, 0);\n\n  return 0;\n}\n\nstatic void adl_close (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  if (adl)\n  {\n    if (adl->lib)\n      hc_dlclose (adl->lib);\n\n    hcfree (adl);\n  }\n}\n\nstatic int hm_ADL_Main_Control_Destroy (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Main_Control_Destroy ();\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Main_Control_Destroy(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Main_Control_Create (hashcat_ctx_t *hashcat_ctx, ADL_MAIN_MALLOC_CALLBACK callback, int iEnumConnectedAdapters)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Main_Control_Create (callback, iEnumConnectedAdapters);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Main_Control_Create(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Adapter_NumberOfAdapters_Get (hashcat_ctx_t *hashcat_ctx, int *lpNumAdapters)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Adapter_NumberOfAdapters_Get (lpNumAdapters);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Adapter_NumberOfAdapters_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Adapter_AdapterInfo_Get (hashcat_ctx_t *hashcat_ctx, LPAdapterInfo lpInfo, int iInputSize)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Adapter_AdapterInfo_Get (lpInfo, iInputSize);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Adapter_AdapterInfo_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive5_Temperature_Get (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, int iThermalControllerIndex, ADLTemperature *lpTemperature)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive5_Temperature_Get (iAdapterIndex, iThermalControllerIndex, lpTemperature);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive5_Temperature_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive6_Temperature_Get (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, int *iTemperature)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive6_Temperature_Get (iAdapterIndex, iTemperature);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive6_Temperature_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive_CurrentActivity_Get (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, ADLPMActivity *lpActivity)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive5_CurrentActivity_Get (iAdapterIndex, lpActivity);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive5_CurrentActivity_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive5_FanSpeed_Get (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, int iThermalControllerIndex, ADLFanSpeedValue *lpFanSpeedValue)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive5_FanSpeed_Get (iAdapterIndex, iThermalControllerIndex, lpFanSpeedValue);\n\n  if ((ADL_rc != ADL_OK) && (ADL_rc != ADL_ERR_NOT_SUPPORTED)) // exception allowed only here\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive5_FanSpeed_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive6_FanSpeed_Get (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, ADLOD6FanSpeedInfo *lpFanSpeedInfo)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive6_FanSpeed_Get (iAdapterIndex, lpFanSpeedInfo);\n\n  if ((ADL_rc != ADL_OK) && (ADL_rc != ADL_ERR_NOT_SUPPORTED)) // exception allowed only here\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive6_FanSpeed_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive_Caps (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, int *od_supported, int *od_enabled, int *od_version)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive_Caps (iAdapterIndex, od_supported, od_enabled, od_version);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive_Caps(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_ADL_Overdrive6_TargetTemperatureData_Get (hashcat_ctx_t *hashcat_ctx, int iAdapterIndex, int *cur_temp, int *default_temp)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  ADL_PTR *adl = (ADL_PTR *) hwmon_ctx->hm_adl;\n\n  const int ADL_rc = adl->ADL_Overdrive6_TargetTemperatureData_Get (iAdapterIndex, cur_temp, default_temp);\n\n  if (ADL_rc != ADL_OK)\n  {\n    event_log_error (hashcat_ctx, \"ADL_Overdrive6_TargetTemperatureData_Get(): %d\", ADL_rc);\n\n    return -1;\n  }\n\n  return 0;\n}\n\n// general functions\n\nstatic int get_adapters_num_adl (hashcat_ctx_t *hashcat_ctx, int *iNumberAdapters)\n{\n  const int hm_rc = hm_ADL_Adapter_NumberOfAdapters_Get (hashcat_ctx, iNumberAdapters);\n\n  if (hm_rc == -1) return -1;\n\n  if (iNumberAdapters == NULL)\n  {\n    event_log_error (hashcat_ctx, \"No ADL adapters found.\");\n\n    return -1;\n  }\n\n  return 0;\n}\n\nstatic int hm_get_adapter_index_nvapi (hashcat_ctx_t *hashcat_ctx, HM_ADAPTER_NVAPI *nvapiGPUHandle)\n{\n  NvU32 pGpuCount;\n\n  if (hm_NvAPI_EnumPhysicalGPUs (hashcat_ctx, nvapiGPUHandle, &pGpuCount) == -1) return 0;\n\n  if (pGpuCount == 0)\n  {\n    event_log_error (hashcat_ctx, \"No NvAPI adapters found.\");\n\n    return 0;\n  }\n\n  return (pGpuCount);\n}\n\nstatic int hm_get_adapter_index_nvml (hashcat_ctx_t *hashcat_ctx, HM_ADAPTER_NVML *nvmlGPUHandle)\n{\n  unsigned int deviceCount = 0;\n\n  hm_NVML_nvmlDeviceGetCount (hashcat_ctx, &deviceCount);\n\n  if (deviceCount == 0)\n  {\n    event_log_error (hashcat_ctx, \"No NVML adapters found.\");\n\n    return 0;\n  }\n\n  for (u32 i = 0; i < deviceCount; i++)\n  {\n    if (hm_NVML_nvmlDeviceGetHandleByIndex (hashcat_ctx, i, &nvmlGPUHandle[i]) == -1) break;\n\n    // can be used to determine if the device by index matches the cuda device by index\n    // char name[100]; memset (name, 0, sizeof (name));\n    // hm_NVML_nvmlDeviceGetName (hashcat_ctx, nvGPUHandle[i], name, sizeof (name) - 1);\n  }\n\n  return (deviceCount);\n}\n\nint hm_get_threshold_slowdown_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].threshold_slowdown_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      int target = 0;\n\n      if (hm_NVML_nvmlDeviceGetTemperatureThreshold (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_TEMPERATURE_THRESHOLD_SLOWDOWN, (unsigned int *) &target) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].threshold_slowdown_get_supported = false;\n\n        return -1;\n      }\n\n      return target;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 5)\n        {\n\n        }\n        else if (hwmon_ctx->hm_device[backend_device_idx].od_version == 6)\n        {\n          int CurrentValue = 0;\n          int DefaultValue = 0;\n\n          if (hm_ADL_Overdrive6_TargetTemperatureData_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &CurrentValue, &DefaultValue) == -1)\n          {\n            hwmon_ctx->hm_device[backend_device_idx].threshold_slowdown_get_supported = false;\n\n            return -1;\n          }\n\n          // the return value has never been tested since hm_ADL_Overdrive6_TargetTemperatureData_Get() never worked on any system. expect problems.\n\n          return DefaultValue;\n        }\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        int target = 0;\n\n        if (hm_NVML_nvmlDeviceGetTemperatureThreshold (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_TEMPERATURE_THRESHOLD_SLOWDOWN, (unsigned int *) &target) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].threshold_slowdown_get_supported = false;\n\n          return -1;\n        }\n\n        return target;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].threshold_slowdown_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_threshold_shutdown_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].threshold_shutdown_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      int target = 0;\n\n      if (hm_NVML_nvmlDeviceGetTemperatureThreshold (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_TEMPERATURE_THRESHOLD_SHUTDOWN, (unsigned int *) &target) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].threshold_shutdown_get_supported = false;\n\n        return -1;\n      }\n\n      return target;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 5)\n        {\n\n        }\n        else if (hwmon_ctx->hm_device[backend_device_idx].od_version == 6)\n        {\n\n        }\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        int target = 0;\n\n        if (hm_NVML_nvmlDeviceGetTemperatureThreshold (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_TEMPERATURE_THRESHOLD_SHUTDOWN, (unsigned int *) &target) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].threshold_shutdown_get_supported = false;\n\n          return -1;\n        }\n\n        return target;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].threshold_shutdown_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_temperature_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      int temperature = 0;\n\n      if (hm_NVML_nvmlDeviceGetTemperature (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_TEMPERATURE_GPU, (u32 *) &temperature) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported = false;\n\n        return -1;\n      }\n\n      return temperature;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 5)\n        {\n          ADLTemperature Temperature;\n\n          Temperature.iSize = sizeof (ADLTemperature);\n\n          if (hm_ADL_Overdrive5_Temperature_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, 0, &Temperature) == -1)\n          {\n            hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported = false;\n\n            return -1;\n          }\n\n          return Temperature.iTemperature / 1000;\n        }\n\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 6)\n        {\n          int Temperature = 0;\n\n          if (hm_ADL_Overdrive6_Temperature_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &Temperature) == -1)\n          {\n            hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported = false;\n\n            return -1;\n          }\n\n          return Temperature / 1000;\n        }\n      }\n\n      if (hwmon_ctx->hm_sysfs)\n      {\n        int temperature = 0;\n\n        if (hm_SYSFS_get_temperature_current (hashcat_ctx, backend_device_idx, &temperature) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported = false;\n\n          return -1;\n        }\n\n        return temperature;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        int temperature = 0;\n\n        if (hm_NVML_nvmlDeviceGetTemperature (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_TEMPERATURE_GPU, (u32 *) &temperature) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported = false;\n\n          return -1;\n        }\n\n        return temperature;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].temperature_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_fanpolicy_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].fanpolicy_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    return 1;\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 5)\n        {\n          ADLFanSpeedValue lpFanSpeedValue;\n\n          memset (&lpFanSpeedValue, 0, sizeof (lpFanSpeedValue));\n\n          lpFanSpeedValue.iSize      = sizeof (lpFanSpeedValue);\n          lpFanSpeedValue.iSpeedType = ADL_DL_FANCTRL_SPEED_TYPE_PERCENT;\n\n          if (hm_ADL_Overdrive5_FanSpeed_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, 0, &lpFanSpeedValue) == -1)\n          {\n            hwmon_ctx->hm_device[backend_device_idx].fanpolicy_get_supported = false;\n            hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported  = false;\n\n            return -1;\n          }\n\n          return (lpFanSpeedValue.iFanSpeed & ADL_DL_FANCTRL_FLAG_USER_DEFINED_SPEED) ? 0 : 1;\n        }\n\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 6)\n        {\n          return 1;\n        }\n      }\n\n      if (hwmon_ctx->hm_sysfs)\n      {\n        return 1;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      return 1;\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].fanpolicy_get_supported = false;\n  hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported  = false;\n\n  return -1;\n}\n\nint hm_get_fanspeed_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      int speed = 0;\n\n      if (hm_NVML_nvmlDeviceGetFanSpeed (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, (u32 *) &speed) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported = false;\n\n        return -1;\n      }\n\n      return speed;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 5)\n        {\n          ADLFanSpeedValue lpFanSpeedValue;\n\n          memset (&lpFanSpeedValue, 0, sizeof (lpFanSpeedValue));\n\n          lpFanSpeedValue.iSize      = sizeof (lpFanSpeedValue);\n          lpFanSpeedValue.iSpeedType = ADL_DL_FANCTRL_SPEED_TYPE_PERCENT;\n          lpFanSpeedValue.iFlags     = ADL_DL_FANCTRL_FLAG_USER_DEFINED_SPEED;\n\n          if (hm_ADL_Overdrive5_FanSpeed_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, 0, &lpFanSpeedValue) == -1)\n          {\n            hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported = false;\n\n            return -1;\n          }\n\n          return lpFanSpeedValue.iFanSpeed;\n        }\n\n        if (hwmon_ctx->hm_device[backend_device_idx].od_version == 6)\n        {\n          ADLOD6FanSpeedInfo faninfo;\n\n          memset (&faninfo, 0, sizeof (faninfo));\n\n          if (hm_ADL_Overdrive6_FanSpeed_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &faninfo) == -1)\n          {\n            hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported = false;\n\n            return -1;\n          }\n\n          return faninfo.iFanSpeedPercent;\n        }\n      }\n\n      if (hwmon_ctx->hm_sysfs)\n      {\n        int speed = 0;\n\n        if (hm_SYSFS_get_fan_speed_current (hashcat_ctx, backend_device_idx, &speed) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported = false;\n\n          return -1;\n        }\n\n        return speed;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        int speed = 0;\n\n        if (hm_NVML_nvmlDeviceGetFanSpeed (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, (u32 *) &speed) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported = false;\n\n          return -1;\n        }\n\n        return speed;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].fanspeed_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_buslanes_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].buslanes_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      unsigned int currLinkWidth;\n\n      if (hm_NVML_nvmlDeviceGetCurrPcieLinkWidth (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &currLinkWidth) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].buslanes_get_supported = false;\n\n        return -1;\n      }\n\n      return currLinkWidth;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        ADLPMActivity PMActivity;\n\n        PMActivity.iSize = sizeof (ADLPMActivity);\n\n        if (hm_ADL_Overdrive_CurrentActivity_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &PMActivity) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].buslanes_get_supported = false;\n\n          return -1;\n        }\n\n        return PMActivity.iCurrentBusLanes;\n      }\n\n      if (hwmon_ctx->hm_sysfs)\n      {\n        int lanes;\n\n        if (hm_SYSFS_get_pp_dpm_pcie (hashcat_ctx, backend_device_idx, &lanes) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].buslanes_get_supported = false;\n\n          return -1;\n        }\n\n        return lanes;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        unsigned int currLinkWidth;\n\n        if (hm_NVML_nvmlDeviceGetCurrPcieLinkWidth (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &currLinkWidth) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].buslanes_get_supported = false;\n\n          return -1;\n        }\n\n        return currLinkWidth;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].buslanes_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_utilization_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].utilization_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      nvmlUtilization_t utilization;\n\n      if (hm_NVML_nvmlDeviceGetUtilizationRates (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &utilization) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].utilization_get_supported = false;\n\n        return -1;\n      }\n\n      return utilization.gpu;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        ADLPMActivity PMActivity;\n\n        PMActivity.iSize = sizeof (ADLPMActivity);\n\n        if (hm_ADL_Overdrive_CurrentActivity_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &PMActivity) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].utilization_get_supported = false;\n\n          return -1;\n        }\n\n        return PMActivity.iActivityPercent;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        nvmlUtilization_t utilization;\n\n        if (hm_NVML_nvmlDeviceGetUtilizationRates (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &utilization) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].utilization_get_supported = false;\n\n          return -1;\n        }\n\n        return utilization.gpu;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].utilization_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_memoryspeed_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].memoryspeed_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      unsigned int clockfreq;\n\n      if (hm_NVML_nvmlDeviceGetClockInfo (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_CLOCK_MEM, &clockfreq) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].memoryspeed_get_supported = false;\n\n        return -1;\n      }\n\n      return clockfreq;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        ADLPMActivity PMActivity;\n\n        PMActivity.iSize = sizeof (ADLPMActivity);\n\n        if (hm_ADL_Overdrive_CurrentActivity_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &PMActivity) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].memoryspeed_get_supported = false;\n\n          return -1;\n        }\n\n        return PMActivity.iMemoryClock / 100;\n      }\n\n      if (hwmon_ctx->hm_sysfs)\n      {\n        int clockfreq;\n\n        if (hm_SYSFS_get_pp_dpm_mclk (hashcat_ctx, backend_device_idx, &clockfreq) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].memoryspeed_get_supported = false;\n\n          return -1;\n        }\n\n        return clockfreq;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        unsigned int clockfreq;\n\n        if (hm_NVML_nvmlDeviceGetClockInfo (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_CLOCK_MEM, &clockfreq) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].memoryspeed_get_supported = false;\n\n          return -1;\n        }\n\n        return clockfreq;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].memoryspeed_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_corespeed_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].corespeed_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      unsigned int clockfreq;\n\n      if (hm_NVML_nvmlDeviceGetClockInfo (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_CLOCK_SM, &clockfreq) == -1)\n      {\n        hwmon_ctx->hm_device[backend_device_idx].corespeed_get_supported = false;\n\n        return -1;\n      }\n\n      return clockfreq;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n      if (hwmon_ctx->hm_adl)\n      {\n        ADLPMActivity PMActivity;\n\n        PMActivity.iSize = sizeof (ADLPMActivity);\n\n        if (hm_ADL_Overdrive_CurrentActivity_Get (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].adl, &PMActivity) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].corespeed_get_supported = false;\n\n          return -1;\n        }\n\n        return PMActivity.iEngineClock / 100;\n      }\n\n      if (hwmon_ctx->hm_sysfs)\n      {\n        int clockfreq;\n\n        if (hm_SYSFS_get_pp_dpm_sclk (hashcat_ctx, backend_device_idx, &clockfreq) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].corespeed_get_supported = false;\n\n          return -1;\n        }\n\n        return clockfreq;\n      }\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        unsigned int clockfreq;\n\n        if (hm_NVML_nvmlDeviceGetClockInfo (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, NVML_CLOCK_SM, &clockfreq) == -1)\n        {\n          hwmon_ctx->hm_device[backend_device_idx].corespeed_get_supported = false;\n\n          return -1;\n        }\n\n        return clockfreq;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].corespeed_get_supported = false;\n\n  return -1;\n}\n\nint hm_get_throttle_with_devices_idx (hashcat_ctx_t *hashcat_ctx, const int backend_device_idx)\n{\n  hwmon_ctx_t   *hwmon_ctx   = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t *backend_ctx = hashcat_ctx->backend_ctx;\n\n  if (hwmon_ctx->enabled == false) return -1;\n\n  if (hwmon_ctx->hm_device[backend_device_idx].throttle_get_supported == false) return -1;\n\n  if (backend_ctx->devices_param[backend_device_idx].is_cuda == true)\n  {\n    if (hwmon_ctx->hm_nvml)\n    {\n      /* this is triggered by mask generator, too. therefore useless\n      unsigned long long clocksThrottleReasons = 0;\n      unsigned long long supportedThrottleReasons = 0;\n\n      if (hm_NVML_nvmlDeviceGetCurrentClocksThrottleReasons   (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &clocksThrottleReasons)    == -1) return -1;\n      if (hm_NVML_nvmlDeviceGetSupportedClocksThrottleReasons (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &supportedThrottleReasons) == -1) return -1;\n\n      clocksThrottleReasons &=  supportedThrottleReasons;\n      clocksThrottleReasons &= ~nvmlClocksThrottleReasonGpuIdle;\n      clocksThrottleReasons &= ~nvmlClocksThrottleReasonApplicationsClocksSetting;\n      clocksThrottleReasons &= ~nvmlClocksThrottleReasonUnknown;\n\n      if (backend_ctx->kernel_power_final)\n      {\n        clocksThrottleReasons &= ~nvmlClocksThrottleReasonHwSlowdown;\n      }\n\n      return (clocksThrottleReasons != nvmlClocksThrottleReasonNone);\n      */\n    }\n\n    if (hwmon_ctx->hm_nvapi)\n    {\n      NV_GPU_PERF_POLICIES_INFO_PARAMS_V1   perfPolicies_info;\n      NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1 perfPolicies_status;\n\n      memset (&perfPolicies_info,   0, sizeof (NV_GPU_PERF_POLICIES_INFO_PARAMS_V1));\n      memset (&perfPolicies_status, 0, sizeof (NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1));\n\n      perfPolicies_info.version   = MAKE_NVAPI_VERSION (NV_GPU_PERF_POLICIES_INFO_PARAMS_V1, 1);\n      perfPolicies_status.version = MAKE_NVAPI_VERSION (NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1, 1);\n\n      hm_NvAPI_GPU_GetPerfPoliciesInfo (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvapi, &perfPolicies_info);\n\n      perfPolicies_status.info_value = perfPolicies_info.info_value;\n\n      hm_NvAPI_GPU_GetPerfPoliciesStatus (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvapi, &perfPolicies_status);\n\n      return perfPolicies_status.throttle & 2;\n    }\n  }\n\n  if (backend_ctx->devices_param[backend_device_idx].is_opencl == true)\n  {\n    if ((backend_ctx->devices_param[backend_device_idx].opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) return -1;\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_AMD)\n    {\n    }\n\n    if (backend_ctx->devices_param[backend_device_idx].opencl_device_vendor_id == VENDOR_ID_NV)\n    {\n      if (hwmon_ctx->hm_nvml)\n      {\n        /* this is triggered by mask generator, too. therefore useless\n        unsigned long long clocksThrottleReasons = 0;\n        unsigned long long supportedThrottleReasons = 0;\n\n        if (hm_NVML_nvmlDeviceGetCurrentClocksThrottleReasons   (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &clocksThrottleReasons)    == -1) return -1;\n        if (hm_NVML_nvmlDeviceGetSupportedClocksThrottleReasons (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvml, &supportedThrottleReasons) == -1) return -1;\n\n        clocksThrottleReasons &=  supportedThrottleReasons;\n        clocksThrottleReasons &= ~nvmlClocksThrottleReasonGpuIdle;\n        clocksThrottleReasons &= ~nvmlClocksThrottleReasonApplicationsClocksSetting;\n        clocksThrottleReasons &= ~nvmlClocksThrottleReasonUnknown;\n\n        if (backend_ctx->kernel_power_final)\n        {\n          clocksThrottleReasons &= ~nvmlClocksThrottleReasonHwSlowdown;\n        }\n\n        return (clocksThrottleReasons != nvmlClocksThrottleReasonNone);\n        */\n      }\n\n      if (hwmon_ctx->hm_nvapi)\n      {\n        NV_GPU_PERF_POLICIES_INFO_PARAMS_V1   perfPolicies_info;\n        NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1 perfPolicies_status;\n\n        memset (&perfPolicies_info,   0, sizeof (NV_GPU_PERF_POLICIES_INFO_PARAMS_V1));\n        memset (&perfPolicies_status, 0, sizeof (NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1));\n\n        perfPolicies_info.version   = MAKE_NVAPI_VERSION (NV_GPU_PERF_POLICIES_INFO_PARAMS_V1, 1);\n        perfPolicies_status.version = MAKE_NVAPI_VERSION (NV_GPU_PERF_POLICIES_STATUS_PARAMS_V1, 1);\n\n        hm_NvAPI_GPU_GetPerfPoliciesInfo (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvapi, &perfPolicies_info);\n\n        perfPolicies_status.info_value = perfPolicies_info.info_value;\n\n        hm_NvAPI_GPU_GetPerfPoliciesStatus (hashcat_ctx, hwmon_ctx->hm_device[backend_device_idx].nvapi, &perfPolicies_status);\n\n        return perfPolicies_status.throttle & 2;\n      }\n    }\n  }\n\n  hwmon_ctx->hm_device[backend_device_idx].throttle_get_supported = false;\n\n  return -1;\n}\n\nint hwmon_ctx_init (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t    *hwmon_ctx    = hashcat_ctx->hwmon_ctx;\n  backend_ctx_t  *backend_ctx  = hashcat_ctx->backend_ctx;\n  user_options_t *user_options = hashcat_ctx->user_options;\n\n  hwmon_ctx->enabled = false;\n\n  #if !defined (WITH_HWMON)\n  return 0;\n  #endif // WITH_HWMON\n\n  if (user_options->example_hashes  == true) return 0;\n  if (user_options->keyspace        == true) return 0;\n  if (user_options->left            == true) return 0;\n  if (user_options->backend_info    == true) return 0;\n  if (user_options->show            == true) return 0;\n  if (user_options->stdout_flag     == true) return 0;\n  if (user_options->usage           == true) return 0;\n  if (user_options->version         == true) return 0;\n  if (user_options->hwmon_disable   == true) return 0;\n\n  hwmon_ctx->hm_device = (hm_attrs_t *) hccalloc (DEVICES_MAX, sizeof (hm_attrs_t));\n\n  /**\n   * Initialize shared libraries\n   */\n\n  hm_attrs_t *hm_adapters_adl   = (hm_attrs_t *) hccalloc (DEVICES_MAX, sizeof (hm_attrs_t));\n  hm_attrs_t *hm_adapters_nvapi = (hm_attrs_t *) hccalloc (DEVICES_MAX, sizeof (hm_attrs_t));\n  hm_attrs_t *hm_adapters_nvml  = (hm_attrs_t *) hccalloc (DEVICES_MAX, sizeof (hm_attrs_t));\n  hm_attrs_t *hm_adapters_sysfs = (hm_attrs_t *) hccalloc (DEVICES_MAX, sizeof (hm_attrs_t));\n\n  #define FREE_ADAPTERS         \\\n  do {                          \\\n    hcfree (hm_adapters_adl);   \\\n    hcfree (hm_adapters_nvapi); \\\n    hcfree (hm_adapters_nvml);  \\\n    hcfree (hm_adapters_sysfs); \\\n  } while (0)\n\n  if (backend_ctx->need_nvml == true)\n  {\n    hwmon_ctx->hm_nvml = (NVML_PTR *) hcmalloc (sizeof (NVML_PTR));\n\n    if (nvml_init (hashcat_ctx) == -1)\n    {\n      hcfree (hwmon_ctx->hm_nvml);\n\n      hwmon_ctx->hm_nvml = NULL;\n    }\n  }\n\n  if ((backend_ctx->need_nvapi == true) && (hwmon_ctx->hm_nvml)) // nvapi can't work alone, we need nvml, too\n  {\n    hwmon_ctx->hm_nvapi = (NVAPI_PTR *) hcmalloc (sizeof (NVAPI_PTR));\n\n    if (nvapi_init (hashcat_ctx) == -1)\n    {\n      hcfree (hwmon_ctx->hm_nvapi);\n\n      hwmon_ctx->hm_nvapi = NULL;\n    }\n  }\n\n  if (backend_ctx->need_adl == true)\n  {\n    hwmon_ctx->hm_adl = (ADL_PTR *) hcmalloc (sizeof (ADL_PTR));\n\n    if (adl_init (hashcat_ctx) == -1)\n    {\n      hcfree (hwmon_ctx->hm_adl);\n\n      hwmon_ctx->hm_adl = NULL;\n    }\n  }\n\n  if (backend_ctx->need_sysfs == true)\n  {\n    hwmon_ctx->hm_sysfs = (SYSFS_PTR *) hcmalloc (sizeof (SYSFS_PTR));\n\n    if (sysfs_init (hashcat_ctx) == false)\n    {\n      hcfree (hwmon_ctx->hm_sysfs);\n\n      hwmon_ctx->hm_sysfs = NULL;\n    }\n\n    // also if there's ADL, we don't need sysfs\n\n    if (hwmon_ctx->hm_adl)\n    {\n      hcfree (hwmon_ctx->hm_sysfs);\n\n      hwmon_ctx->hm_sysfs = NULL;\n    }\n  }\n\n  if (hwmon_ctx->hm_nvml)\n  {\n    if (hm_NVML_nvmlInit (hashcat_ctx) == 0)\n    {\n      HM_ADAPTER_NVML *nvmlGPUHandle = (HM_ADAPTER_NVML *) hccalloc (DEVICES_MAX, sizeof (HM_ADAPTER_NVML));\n\n      int tmp_in = hm_get_adapter_index_nvml (hashcat_ctx, nvmlGPUHandle);\n\n      for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n      {\n        hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n        if (device_param->skipped == true) continue;\n\n        if (device_param->is_cuda == true)\n        {\n          for (int i = 0; i < tmp_in; i++)\n          {\n            nvmlPciInfo_t pci;\n\n            int rc = hm_NVML_nvmlDeviceGetPciInfo (hashcat_ctx, nvmlGPUHandle[i], &pci);\n\n            if (rc == -1) continue;\n\n            if ((device_param->pcie_bus      == pci.bus)\n             && (device_param->pcie_device   == (pci.device >> 3))\n             && (device_param->pcie_function == (pci.device & 7)))\n            {\n              const u32 device_id = device_param->device_id;\n\n              hm_adapters_nvml[device_id].nvml = nvmlGPUHandle[i];\n\n              hm_adapters_nvml[device_id].buslanes_get_supported            = true;\n              hm_adapters_nvml[device_id].corespeed_get_supported           = true;\n              hm_adapters_nvml[device_id].fanspeed_get_supported            = true;\n              hm_adapters_nvml[device_id].memoryspeed_get_supported         = true;\n              hm_adapters_nvml[device_id].temperature_get_supported         = true;\n              hm_adapters_nvml[device_id].threshold_shutdown_get_supported  = true;\n              hm_adapters_nvml[device_id].threshold_slowdown_get_supported  = true;\n              hm_adapters_nvml[device_id].utilization_get_supported         = true;\n            }\n          }\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) continue;\n\n          if (device_param->opencl_device_vendor_id != VENDOR_ID_NV) continue;\n\n          for (int i = 0; i < tmp_in; i++)\n          {\n            nvmlPciInfo_t pci;\n\n            int rc = hm_NVML_nvmlDeviceGetPciInfo (hashcat_ctx, nvmlGPUHandle[i], &pci);\n\n            if (rc == -1) continue;\n\n            if ((device_param->pcie_bus      == pci.bus)\n             && (device_param->pcie_device   == (pci.device >> 3))\n             && (device_param->pcie_function == (pci.device & 7)))\n            {\n              const u32 device_id = device_param->device_id;\n\n              hm_adapters_nvml[device_id].nvml = nvmlGPUHandle[i];\n\n              hm_adapters_nvml[device_id].buslanes_get_supported            = true;\n              hm_adapters_nvml[device_id].corespeed_get_supported           = true;\n              hm_adapters_nvml[device_id].fanspeed_get_supported            = true;\n              hm_adapters_nvml[device_id].memoryspeed_get_supported         = true;\n              hm_adapters_nvml[device_id].temperature_get_supported         = true;\n              hm_adapters_nvml[device_id].threshold_shutdown_get_supported  = true;\n              hm_adapters_nvml[device_id].threshold_slowdown_get_supported  = true;\n              hm_adapters_nvml[device_id].utilization_get_supported         = true;\n            }\n          }\n        }\n      }\n\n      hcfree (nvmlGPUHandle);\n    }\n  }\n\n  if (hwmon_ctx->hm_nvapi)\n  {\n    if (hm_NvAPI_Initialize (hashcat_ctx) == 0)\n    {\n      HM_ADAPTER_NVAPI *nvGPUHandle = (HM_ADAPTER_NVAPI *) hccalloc (NVAPI_MAX_PHYSICAL_GPUS, sizeof (HM_ADAPTER_NVAPI));\n\n      int tmp_in = hm_get_adapter_index_nvapi (hashcat_ctx, nvGPUHandle);\n\n      for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n      {\n        hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n        if (device_param->skipped == true) continue;\n\n        if (device_param->is_cuda == true)\n        {\n          for (int i = 0; i < tmp_in; i++)\n          {\n            NvU32 BusId     = 0;\n            NvU32 BusSlotId = 0;\n\n            int rc1 = hm_NvAPI_GPU_GetBusId (hashcat_ctx, nvGPUHandle[i], &BusId);\n\n            if (rc1 == -1) continue;\n\n            int rc2 = hm_NvAPI_GPU_GetBusSlotId (hashcat_ctx, nvGPUHandle[i], &BusSlotId);\n\n            if (rc2 == -1) continue;\n\n            if ((device_param->pcie_bus      == BusId)\n             && (device_param->pcie_device   == (BusSlotId >> 3))\n             && (device_param->pcie_function == (BusSlotId & 7)))\n            {\n              const u32 device_id = device_param->device_id;\n\n              hm_adapters_nvapi[device_id].nvapi = nvGPUHandle[i];\n\n              hm_adapters_nvapi[device_id].fanpolicy_get_supported  = true;\n              hm_adapters_nvapi[device_id].throttle_get_supported   = true;\n            }\n          }\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) continue;\n\n          if (device_param->opencl_device_vendor_id != VENDOR_ID_NV) continue;\n\n          for (int i = 0; i < tmp_in; i++)\n          {\n            NvU32 BusId     = 0;\n            NvU32 BusSlotId = 0;\n\n            int rc1 = hm_NvAPI_GPU_GetBusId (hashcat_ctx, nvGPUHandle[i], &BusId);\n\n            if (rc1 == -1) continue;\n\n            int rc2 = hm_NvAPI_GPU_GetBusSlotId (hashcat_ctx, nvGPUHandle[i], &BusSlotId);\n\n            if (rc2 == -1) continue;\n\n            if ((device_param->pcie_bus      == BusId)\n             && (device_param->pcie_device   == (BusSlotId >> 3))\n             && (device_param->pcie_function == (BusSlotId & 7)))\n            {\n              const u32 device_id = device_param->device_id;\n\n              hm_adapters_nvapi[device_id].nvapi = nvGPUHandle[i];\n\n              hm_adapters_nvapi[device_id].fanpolicy_get_supported  = true;\n              hm_adapters_nvapi[device_id].throttle_get_supported   = true;\n            }\n          }\n        }\n      }\n\n      hcfree (nvGPUHandle);\n    }\n  }\n\n  if (hwmon_ctx->hm_adl)\n  {\n    if (hm_ADL_Main_Control_Create (hashcat_ctx, ADL_Main_Memory_Alloc, 0) == 0)\n    {\n      // total number of adapters\n\n      int tmp_in;\n\n      if (get_adapters_num_adl (hashcat_ctx, &tmp_in) == -1)\n      {\n        FREE_ADAPTERS;\n\n        return -1;\n      }\n\n      // adapter info\n\n      LPAdapterInfo lpAdapterInfo = (LPAdapterInfo) hccalloc (tmp_in, sizeof (AdapterInfo));\n\n      const int rc_adapter_info_adl = hm_ADL_Adapter_AdapterInfo_Get (hashcat_ctx, lpAdapterInfo, tmp_in * sizeof (AdapterInfo));\n\n      if (rc_adapter_info_adl == -1)\n      {\n        FREE_ADAPTERS;\n\n        return -1;\n      }\n\n      for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n      {\n        hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n        if (device_param->skipped == true) continue;\n\n        if (device_param->is_cuda == true)\n        {\n          // nothing to do\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) continue;\n\n          if (device_param->opencl_device_vendor_id != VENDOR_ID_AMD) continue;\n\n          for (int i = 0; i < tmp_in; i++)\n          {\n            if ((device_param->pcie_bus      == lpAdapterInfo[i].iBusNumber)\n             && (device_param->pcie_device   == (lpAdapterInfo[i].iDeviceNumber >> 3))\n             && (device_param->pcie_function == (lpAdapterInfo[i].iDeviceNumber & 7)))\n            {\n              const u32 device_id = device_param->device_id;\n\n              int od_supported = 0;\n              int od_enabled   = 0;\n              int od_version   = 0;\n\n              hm_ADL_Overdrive_Caps (hashcat_ctx, lpAdapterInfo[i].iAdapterIndex, &od_supported, &od_enabled, &od_version);\n\n              hm_adapters_adl[device_id].od_version = od_version;\n\n              hm_adapters_adl[device_id].adl = lpAdapterInfo[i].iAdapterIndex;\n\n              hm_adapters_adl[device_id].buslanes_get_supported            = true;\n              hm_adapters_adl[device_id].corespeed_get_supported           = true;\n              hm_adapters_adl[device_id].fanspeed_get_supported            = true;\n              hm_adapters_adl[device_id].fanpolicy_get_supported           = true;\n              hm_adapters_adl[device_id].memoryspeed_get_supported         = true;\n              hm_adapters_adl[device_id].temperature_get_supported         = true;\n              hm_adapters_adl[device_id].threshold_slowdown_get_supported  = true;\n              hm_adapters_adl[device_id].utilization_get_supported         = true;\n            }\n          }\n        }\n      }\n\n      hcfree (lpAdapterInfo);\n    }\n  }\n\n  if (hwmon_ctx->hm_sysfs)\n  {\n    if (true)\n    {\n      int hm_adapters_id = 0;\n\n      for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n      {\n        hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n        if (device_param->is_cuda == true)\n        {\n          // nothing to do\n        }\n\n        if (device_param->is_opencl == true)\n        {\n          if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) continue;\n\n          hm_adapters_sysfs[hm_adapters_id].sysfs = backend_devices_idx; // ????\n\n          hm_adapters_sysfs[hm_adapters_id].buslanes_get_supported    = true;\n          hm_adapters_sysfs[hm_adapters_id].corespeed_get_supported   = true;\n          hm_adapters_sysfs[hm_adapters_id].fanspeed_get_supported    = true;\n          hm_adapters_sysfs[hm_adapters_id].fanpolicy_get_supported   = true;\n          hm_adapters_sysfs[hm_adapters_id].memoryspeed_get_supported = true;\n          hm_adapters_sysfs[hm_adapters_id].temperature_get_supported = true;\n\n          hm_adapters_id++;\n        }\n      }\n    }\n  }\n\n  if (hwmon_ctx->hm_adl == NULL && hwmon_ctx->hm_nvml == NULL && hwmon_ctx->hm_sysfs == NULL)\n  {\n    FREE_ADAPTERS;\n\n    return 0;\n  }\n\n  /**\n   * looks like we have some manageable device\n   */\n\n  hwmon_ctx->enabled = true;\n\n  /**\n   * save buffer required for later restores\n   */\n\n  hwmon_ctx->od_clock_mem_status = (ADLOD6MemClockState *) hccalloc (backend_ctx->backend_devices_cnt, sizeof (ADLOD6MemClockState));\n\n  /**\n   * HM devices: copy\n   */\n\n  for (int backend_devices_idx = 0; backend_devices_idx < backend_ctx->backend_devices_cnt; backend_devices_idx++)\n  {\n    hc_device_param_t *device_param = &backend_ctx->devices_param[backend_devices_idx];\n\n    if (device_param->skipped == true) continue;\n\n    const u32 device_id = device_param->device_id;\n\n    if (device_param->is_cuda == true)\n    {\n      hwmon_ctx->hm_device[backend_devices_idx].adl         = 0;\n      hwmon_ctx->hm_device[backend_devices_idx].sysfs       = 0;\n      hwmon_ctx->hm_device[backend_devices_idx].nvapi       = hm_adapters_nvapi[device_id].nvapi;\n      hwmon_ctx->hm_device[backend_devices_idx].nvml        = hm_adapters_nvml[device_id].nvml;\n      hwmon_ctx->hm_device[backend_devices_idx].od_version  = 0;\n\n      if (hwmon_ctx->hm_nvml)\n      {\n        hwmon_ctx->hm_device[backend_devices_idx].buslanes_get_supported            |= hm_adapters_nvml[device_id].buslanes_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].corespeed_get_supported           |= hm_adapters_nvml[device_id].corespeed_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].fanspeed_get_supported            |= hm_adapters_nvml[device_id].fanspeed_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].fanpolicy_get_supported           |= hm_adapters_nvml[device_id].fanpolicy_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].memoryspeed_get_supported         |= hm_adapters_nvml[device_id].memoryspeed_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].temperature_get_supported         |= hm_adapters_nvml[device_id].temperature_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].threshold_shutdown_get_supported  |= hm_adapters_nvml[device_id].threshold_shutdown_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].threshold_slowdown_get_supported  |= hm_adapters_nvml[device_id].threshold_slowdown_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].throttle_get_supported            |= hm_adapters_nvml[device_id].throttle_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].utilization_get_supported         |= hm_adapters_nvml[device_id].utilization_get_supported;\n      }\n\n      if (hwmon_ctx->hm_nvapi)\n      {\n        hwmon_ctx->hm_device[backend_devices_idx].buslanes_get_supported            |= hm_adapters_nvapi[device_id].buslanes_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].corespeed_get_supported           |= hm_adapters_nvapi[device_id].corespeed_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].fanspeed_get_supported            |= hm_adapters_nvapi[device_id].fanspeed_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].fanpolicy_get_supported           |= hm_adapters_nvapi[device_id].fanpolicy_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].memoryspeed_get_supported         |= hm_adapters_nvapi[device_id].memoryspeed_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].temperature_get_supported         |= hm_adapters_nvapi[device_id].temperature_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].threshold_shutdown_get_supported  |= hm_adapters_nvapi[device_id].threshold_shutdown_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].threshold_slowdown_get_supported  |= hm_adapters_nvapi[device_id].threshold_slowdown_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].throttle_get_supported            |= hm_adapters_nvapi[device_id].throttle_get_supported;\n        hwmon_ctx->hm_device[backend_devices_idx].utilization_get_supported         |= hm_adapters_nvapi[device_id].utilization_get_supported;\n      }\n    }\n\n    if (device_param->is_opencl == true)\n    {\n      if ((device_param->opencl_device_type & CL_DEVICE_TYPE_GPU) == 0) continue;\n\n      if (device_param->opencl_device_vendor_id == VENDOR_ID_AMD)\n      {\n        hwmon_ctx->hm_device[backend_devices_idx].adl         = hm_adapters_adl[device_id].adl;\n        hwmon_ctx->hm_device[backend_devices_idx].sysfs       = hm_adapters_sysfs[device_id].sysfs;\n        hwmon_ctx->hm_device[backend_devices_idx].nvapi       = 0;\n        hwmon_ctx->hm_device[backend_devices_idx].nvml        = 0;\n        hwmon_ctx->hm_device[backend_devices_idx].od_version  = 0;\n\n        if (hwmon_ctx->hm_adl)\n        {\n          hwmon_ctx->hm_device[backend_devices_idx].od_version = hm_adapters_adl[device_id].od_version;\n\n          hwmon_ctx->hm_device[backend_devices_idx].buslanes_get_supported            |= hm_adapters_adl[device_id].buslanes_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].corespeed_get_supported           |= hm_adapters_adl[device_id].corespeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanspeed_get_supported            |= hm_adapters_adl[device_id].fanspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanpolicy_get_supported           |= hm_adapters_adl[device_id].fanpolicy_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].memoryspeed_get_supported         |= hm_adapters_adl[device_id].memoryspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].temperature_get_supported         |= hm_adapters_adl[device_id].temperature_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_shutdown_get_supported  |= hm_adapters_adl[device_id].threshold_shutdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_slowdown_get_supported  |= hm_adapters_adl[device_id].threshold_slowdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].throttle_get_supported            |= hm_adapters_adl[device_id].throttle_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].utilization_get_supported         |= hm_adapters_adl[device_id].utilization_get_supported;\n        }\n\n        if (hwmon_ctx->hm_sysfs)\n        {\n          hwmon_ctx->hm_device[backend_devices_idx].buslanes_get_supported            |= hm_adapters_sysfs[device_id].buslanes_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].corespeed_get_supported           |= hm_adapters_sysfs[device_id].corespeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanspeed_get_supported            |= hm_adapters_sysfs[device_id].fanspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanpolicy_get_supported           |= hm_adapters_sysfs[device_id].fanpolicy_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].memoryspeed_get_supported         |= hm_adapters_sysfs[device_id].memoryspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].temperature_get_supported         |= hm_adapters_sysfs[device_id].temperature_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_shutdown_get_supported  |= hm_adapters_sysfs[device_id].threshold_shutdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_slowdown_get_supported  |= hm_adapters_sysfs[device_id].threshold_slowdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].throttle_get_supported            |= hm_adapters_sysfs[device_id].throttle_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].utilization_get_supported         |= hm_adapters_sysfs[device_id].utilization_get_supported;\n        }\n      }\n\n      if (device_param->opencl_device_vendor_id == VENDOR_ID_NV)\n      {\n        hwmon_ctx->hm_device[backend_devices_idx].adl         = 0;\n        hwmon_ctx->hm_device[backend_devices_idx].sysfs       = 0;\n        hwmon_ctx->hm_device[backend_devices_idx].nvapi       = hm_adapters_nvapi[device_id].nvapi;\n        hwmon_ctx->hm_device[backend_devices_idx].nvml        = hm_adapters_nvml[device_id].nvml;\n        hwmon_ctx->hm_device[backend_devices_idx].od_version  = 0;\n\n        if (hwmon_ctx->hm_nvml)\n        {\n          hwmon_ctx->hm_device[backend_devices_idx].buslanes_get_supported            |= hm_adapters_nvml[device_id].buslanes_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].corespeed_get_supported           |= hm_adapters_nvml[device_id].corespeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanspeed_get_supported            |= hm_adapters_nvml[device_id].fanspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanpolicy_get_supported           |= hm_adapters_nvml[device_id].fanpolicy_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].memoryspeed_get_supported         |= hm_adapters_nvml[device_id].memoryspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].temperature_get_supported         |= hm_adapters_nvml[device_id].temperature_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_shutdown_get_supported  |= hm_adapters_nvml[device_id].threshold_shutdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_slowdown_get_supported  |= hm_adapters_nvml[device_id].threshold_slowdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].throttle_get_supported            |= hm_adapters_nvml[device_id].throttle_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].utilization_get_supported         |= hm_adapters_nvml[device_id].utilization_get_supported;\n        }\n\n        if (hwmon_ctx->hm_nvapi)\n        {\n          hwmon_ctx->hm_device[backend_devices_idx].buslanes_get_supported            |= hm_adapters_nvapi[device_id].buslanes_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].corespeed_get_supported           |= hm_adapters_nvapi[device_id].corespeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanspeed_get_supported            |= hm_adapters_nvapi[device_id].fanspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].fanpolicy_get_supported           |= hm_adapters_nvapi[device_id].fanpolicy_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].memoryspeed_get_supported         |= hm_adapters_nvapi[device_id].memoryspeed_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].temperature_get_supported         |= hm_adapters_nvapi[device_id].temperature_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_shutdown_get_supported  |= hm_adapters_nvapi[device_id].threshold_shutdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].threshold_slowdown_get_supported  |= hm_adapters_nvapi[device_id].threshold_slowdown_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].throttle_get_supported            |= hm_adapters_nvapi[device_id].throttle_get_supported;\n          hwmon_ctx->hm_device[backend_devices_idx].utilization_get_supported         |= hm_adapters_nvapi[device_id].utilization_get_supported;\n        }\n      }\n    }\n\n    // by calling the different functions here this will disable them in case they will error out\n    // this will also reduce the error itself printed to the user to a single print on startup\n\n    hm_get_buslanes_with_devices_idx           (hashcat_ctx, backend_devices_idx);\n    hm_get_corespeed_with_devices_idx          (hashcat_ctx, backend_devices_idx);\n    hm_get_fanpolicy_with_devices_idx          (hashcat_ctx, backend_devices_idx);\n    hm_get_fanspeed_with_devices_idx           (hashcat_ctx, backend_devices_idx);\n    hm_get_memoryspeed_with_devices_idx        (hashcat_ctx, backend_devices_idx);\n    hm_get_temperature_with_devices_idx        (hashcat_ctx, backend_devices_idx);\n    hm_get_threshold_shutdown_with_devices_idx (hashcat_ctx, backend_devices_idx);\n    hm_get_threshold_slowdown_with_devices_idx (hashcat_ctx, backend_devices_idx);\n    hm_get_throttle_with_devices_idx           (hashcat_ctx, backend_devices_idx);\n    hm_get_utilization_with_devices_idx        (hashcat_ctx, backend_devices_idx);\n  }\n\n  FREE_ADAPTERS;\n\n  return 0;\n}\n\nvoid hwmon_ctx_destroy (hashcat_ctx_t *hashcat_ctx)\n{\n  hwmon_ctx_t *hwmon_ctx = hashcat_ctx->hwmon_ctx;\n\n  if (hwmon_ctx->enabled == false) return;\n\n  // unload shared libraries\n\n  if (hwmon_ctx->hm_nvml)\n  {\n    hm_NVML_nvmlShutdown (hashcat_ctx);\n\n    nvml_close (hashcat_ctx);\n  }\n\n  if (hwmon_ctx->hm_nvapi)\n  {\n    hm_NvAPI_Unload (hashcat_ctx);\n\n    nvapi_close (hashcat_ctx);\n  }\n\n  if (hwmon_ctx->hm_adl)\n  {\n    hm_ADL_Main_Control_Destroy (hashcat_ctx);\n\n    adl_close (hashcat_ctx);\n  }\n\n  if (hwmon_ctx->hm_sysfs)\n  {\n\n    sysfs_close (hashcat_ctx);\n  }\n\n  // free memory\n\n  hcfree (hwmon_ctx->od_clock_mem_status);\n\n  hcfree (hwmon_ctx->hm_device);\n\n  memset (hwmon_ctx, 0, sizeof (hwmon_ctx_t));\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/src/interface.c": "/**\n * Author......: See docs/credits.txt\n * License.....: MIT\n */\n\n#include \"common.h\"\n#include \"types.h\"\n#include \"memory.h\"\n#include \"event.h\"\n#include \"shared.h\"\n#include \"backend.h\"\n#include \"modules.h\"\n#include \"dynloader.h\"\n#include \"interface.h\"\n\n/**\n * parsing\n */\n\nint module_filename (const folder_config_t *folder_config, const int hash_mode, char *out_buf, const size_t out_size)\n{\n  // native compiled\n  #if defined (_WIN) || defined (__CYGWIN__)\n  return snprintf (out_buf, out_size, \"%s/modules/module_%05d.dll\", folder_config->shared_dir, hash_mode);\n  #else\n  return snprintf (out_buf, out_size, \"%s/modules/module_%05d.so\", folder_config->shared_dir, hash_mode);\n  #endif\n}\n\nbool module_load (hashcat_ctx_t *hashcat_ctx, module_ctx_t *module_ctx, const u32 hash_mode)\n{\n  const folder_config_t *folder_config = hashcat_ctx->folder_config;\n\n  memset (module_ctx, 0, sizeof (module_ctx_t));\n\n  char *module_file = (char *) hcmalloc (HCBUFSIZ_TINY);\n\n  module_filename (folder_config, hash_mode, module_file, HCBUFSIZ_TINY);\n\n  module_ctx->module_handle = hc_dlopen (module_file);\n\n  if (module_ctx->module_handle == NULL)\n  {\n    #if defined (_WIN)\n    event_log_error (hashcat_ctx, \"Cannot load module %s\", module_file); // todo: maybe there's a dlerror () equivalent\n    #else\n    event_log_error (hashcat_ctx, \"%s\", dlerror ());\n    #endif\n\n    return false;\n  }\n\n  module_ctx->module_init = (MODULE_INIT) hc_dlsym (module_ctx->module_handle, \"module_init\");\n\n  if (module_ctx->module_init == NULL)\n  {\n    event_log_error (hashcat_ctx, \"Cannot load symbol 'module_init' in module %s\", module_file);\n\n    return false;\n  }\n\n  hcfree (module_file);\n\n  return true;\n}\n\nvoid module_unload (module_ctx_t *module_ctx)\n{\n  if (module_ctx->module_handle)\n  {\n    hc_dlclose (module_ctx->module_handle);\n  }\n}\n\nint hashconfig_init (hashcat_ctx_t *hashcat_ctx)\n{\n  const folder_config_t      *folder_config      = hashcat_ctx->folder_config;\n        hashconfig_t         *hashconfig         = hashcat_ctx->hashconfig;\n        module_ctx_t         *module_ctx         = hashcat_ctx->module_ctx;\n  const user_options_t       *user_options       = hashcat_ctx->user_options;\n  const user_options_extra_t *user_options_extra = hashcat_ctx->user_options_extra;\n\n  // set some boring defaults\n\n  hashconfig->benchmark_mask          = default_benchmark_mask          (hashconfig, user_options, user_options_extra);\n  hashconfig->dictstat_disable        = default_dictstat_disable        (hashconfig, user_options, user_options_extra);\n  hashconfig->esalt_size              = default_esalt_size              (hashconfig, user_options, user_options_extra);\n  hashconfig->forced_outfile_format   = default_forced_outfile_format   (hashconfig, user_options, user_options_extra);\n  hashconfig->hash_mode               = default_hash_mode               (hashconfig, user_options, user_options_extra);\n  hashconfig->hashes_count_min        = default_hashes_count_min        (hashconfig, user_options, user_options_extra);\n  hashconfig->hashes_count_max        = default_hashes_count_max        (hashconfig, user_options, user_options_extra);\n  hashconfig->hlfmt_disable           = default_hlfmt_disable           (hashconfig, user_options, user_options_extra);\n  hashconfig->hook_salt_size          = default_hook_salt_size          (hashconfig, user_options, user_options_extra);\n  hashconfig->hook_size               = default_hook_size               (hashconfig, user_options, user_options_extra);\n  hashconfig->outfile_check_disable   = default_outfile_check_disable   (hashconfig, user_options, user_options_extra);\n  hashconfig->outfile_check_nocomp    = default_outfile_check_nocomp    (hashconfig, user_options, user_options_extra);\n  hashconfig->potfile_disable         = default_potfile_disable         (hashconfig, user_options, user_options_extra);\n  hashconfig->potfile_keep_all_hashes = default_potfile_keep_all_hashes (hashconfig, user_options, user_options_extra);\n  hashconfig->pwdump_column           = default_pwdump_column           (hashconfig, user_options, user_options_extra);\n  hashconfig->separator               = default_separator               (hashconfig, user_options, user_options_extra);\n  hashconfig->tmp_size                = default_tmp_size                (hashconfig, user_options, user_options_extra);\n  hashconfig->warmup_disable          = default_warmup_disable          (hashconfig, user_options, user_options_extra);\n\n  // finally, the real stuff\n\n  const bool rc_load = module_load (hashcat_ctx, module_ctx, user_options->hash_mode);\n\n  if (rc_load == false) return -1;\n\n  module_ctx->module_init (module_ctx);\n\n  if (module_ctx->module_context_size != MODULE_CONTEXT_SIZE_CURRENT)\n  {\n    event_log_error (hashcat_ctx, \"module context size is invalid. Old template?\");\n\n    return -1;\n  }\n\n  if (module_ctx->module_interface_version < MODULE_INTERFACE_VERSION_MINIMUM)\n  {\n    event_log_error (hashcat_ctx, \"module interface version is outdated, please compile\");\n\n    return -1;\n  }\n\n  // check for missing pointer assignements\n\n  #define CHECK_DEFINED(func)                                                     \\\n    if ((func) == NULL)                                                           \\\n    {                                                                             \\\n      event_log_error (hashcat_ctx, \"Missing symbol definitions. Old template?\"); \\\n                                                                                  \\\n      return -1;                                                                  \\\n    }\n\n  CHECK_DEFINED (module_ctx->module_attack_exec);\n  CHECK_DEFINED (module_ctx->module_benchmark_esalt);\n  CHECK_DEFINED (module_ctx->module_benchmark_hook_salt);\n  CHECK_DEFINED (module_ctx->module_benchmark_mask);\n  CHECK_DEFINED (module_ctx->module_benchmark_salt);\n  CHECK_DEFINED (module_ctx->module_build_plain_postprocess);\n  CHECK_DEFINED (module_ctx->module_deep_comp_kernel);\n  CHECK_DEFINED (module_ctx->module_dgst_pos0);\n  CHECK_DEFINED (module_ctx->module_dgst_pos1);\n  CHECK_DEFINED (module_ctx->module_dgst_pos2);\n  CHECK_DEFINED (module_ctx->module_dgst_pos3);\n  CHECK_DEFINED (module_ctx->module_dgst_size);\n  CHECK_DEFINED (module_ctx->module_dictstat_disable);\n  CHECK_DEFINED (module_ctx->module_esalt_size);\n  CHECK_DEFINED (module_ctx->module_extra_buffer_size);\n  CHECK_DEFINED (module_ctx->module_extra_tmp_size);\n  CHECK_DEFINED (module_ctx->module_forced_outfile_format);\n  CHECK_DEFINED (module_ctx->module_hash_binary_count);\n  CHECK_DEFINED (module_ctx->module_hash_binary_parse);\n  CHECK_DEFINED (module_ctx->module_hash_binary_save);\n  CHECK_DEFINED (module_ctx->module_hash_category);\n  CHECK_DEFINED (module_ctx->module_hash_decode);\n  CHECK_DEFINED (module_ctx->module_hash_decode_potfile);\n  CHECK_DEFINED (module_ctx->module_hash_decode_zero_hash);\n  CHECK_DEFINED (module_ctx->module_hash_encode);\n  CHECK_DEFINED (module_ctx->module_hash_encode_potfile);\n  CHECK_DEFINED (module_ctx->module_hash_encode_status);\n  CHECK_DEFINED (module_ctx->module_hash_init_selftest);\n  CHECK_DEFINED (module_ctx->module_hash_mode);\n  CHECK_DEFINED (module_ctx->module_hash_name);\n  CHECK_DEFINED (module_ctx->module_hashes_count_max);\n  CHECK_DEFINED (module_ctx->module_hashes_count_min);\n  CHECK_DEFINED (module_ctx->module_hlfmt_disable);\n  CHECK_DEFINED (module_ctx->module_hook12);\n  CHECK_DEFINED (module_ctx->module_hook23);\n  CHECK_DEFINED (module_ctx->module_hook_salt_size);\n  CHECK_DEFINED (module_ctx->module_hook_size);\n  CHECK_DEFINED (module_ctx->module_jit_build_options);\n  CHECK_DEFINED (module_ctx->module_jit_cache_disable);\n  CHECK_DEFINED (module_ctx->module_kern_type);\n  CHECK_DEFINED (module_ctx->module_kern_type_dynamic);\n  CHECK_DEFINED (module_ctx->module_kernel_accel_max);\n  CHECK_DEFINED (module_ctx->module_kernel_accel_min);\n  CHECK_DEFINED (module_ctx->module_kernel_loops_max);\n  CHECK_DEFINED (module_ctx->module_kernel_loops_min);\n  CHECK_DEFINED (module_ctx->module_kernel_threads_max);\n  CHECK_DEFINED (module_ctx->module_kernel_threads_min);\n  CHECK_DEFINED (module_ctx->module_opti_type);\n  CHECK_DEFINED (module_ctx->module_opts_type);\n  CHECK_DEFINED (module_ctx->module_outfile_check_disable);\n  CHECK_DEFINED (module_ctx->module_outfile_check_nocomp);\n  CHECK_DEFINED (module_ctx->module_potfile_custom_check);\n  CHECK_DEFINED (module_ctx->module_potfile_disable);\n  CHECK_DEFINED (module_ctx->module_potfile_keep_all_hashes);\n  CHECK_DEFINED (module_ctx->module_pw_max);\n  CHECK_DEFINED (module_ctx->module_pw_min);\n  CHECK_DEFINED (module_ctx->module_pwdump_column);\n  CHECK_DEFINED (module_ctx->module_salt_max);\n  CHECK_DEFINED (module_ctx->module_salt_min);\n  CHECK_DEFINED (module_ctx->module_salt_type);\n  CHECK_DEFINED (module_ctx->module_separator);\n  CHECK_DEFINED (module_ctx->module_st_hash);\n  CHECK_DEFINED (module_ctx->module_st_pass);\n  CHECK_DEFINED (module_ctx->module_tmp_size);\n  CHECK_DEFINED (module_ctx->module_unstable_warning);\n  CHECK_DEFINED (module_ctx->module_warmup_disable);\n\n  #undef CHECK_DEFINED\n\n  // mandatory functions check\n\n  #define CHECK_MANDATORY(func)                                               \\\n    if ((func) == MODULE_DEFAULT)                                             \\\n    {                                                                         \\\n      event_log_error (hashcat_ctx, \"Missing mandatory symbol definitions\");  \\\n                                                                              \\\n      return -1;                                                              \\\n    }\n\n  CHECK_MANDATORY (module_ctx->module_attack_exec);\n  CHECK_MANDATORY (module_ctx->module_dgst_pos0);\n  CHECK_MANDATORY (module_ctx->module_dgst_pos1);\n  CHECK_MANDATORY (module_ctx->module_dgst_pos2);\n  CHECK_MANDATORY (module_ctx->module_dgst_pos3);\n  CHECK_MANDATORY (module_ctx->module_dgst_size);\n  CHECK_MANDATORY (module_ctx->module_hash_decode);\n  // CHECK_MANDATORY (module_ctx->module_hash_encode); we do that one later\n  CHECK_MANDATORY (module_ctx->module_hash_category);\n  CHECK_MANDATORY (module_ctx->module_hash_name);\n  CHECK_MANDATORY (module_ctx->module_kern_type);\n  CHECK_MANDATORY (module_ctx->module_opti_type);\n  CHECK_MANDATORY (module_ctx->module_opts_type);\n  CHECK_MANDATORY (module_ctx->module_salt_type);\n  CHECK_MANDATORY (module_ctx->module_st_hash);\n  CHECK_MANDATORY (module_ctx->module_st_pass);\n\n  hashconfig->attack_exec   = module_ctx->module_attack_exec    (hashconfig, user_options, user_options_extra);\n  hashconfig->dgst_pos0     = module_ctx->module_dgst_pos0      (hashconfig, user_options, user_options_extra);\n  hashconfig->dgst_pos1     = module_ctx->module_dgst_pos1      (hashconfig, user_options, user_options_extra);\n  hashconfig->dgst_pos2     = module_ctx->module_dgst_pos2      (hashconfig, user_options, user_options_extra);\n  hashconfig->dgst_pos3     = module_ctx->module_dgst_pos3      (hashconfig, user_options, user_options_extra);\n  hashconfig->dgst_size     = module_ctx->module_dgst_size      (hashconfig, user_options, user_options_extra);\n  hashconfig->hash_category = module_ctx->module_hash_category  (hashconfig, user_options, user_options_extra);\n  hashconfig->hash_name     = module_ctx->module_hash_name      (hashconfig, user_options, user_options_extra);\n  hashconfig->kern_type     = module_ctx->module_kern_type      (hashconfig, user_options, user_options_extra);\n  hashconfig->opti_type     = module_ctx->module_opti_type      (hashconfig, user_options, user_options_extra);\n  hashconfig->opts_type     = module_ctx->module_opts_type      (hashconfig, user_options, user_options_extra);\n  hashconfig->salt_type     = module_ctx->module_salt_type      (hashconfig, user_options, user_options_extra);\n  hashconfig->st_hash       = module_ctx->module_st_hash        (hashconfig, user_options, user_options_extra);\n  hashconfig->st_pass       = module_ctx->module_st_pass        (hashconfig, user_options, user_options_extra);\n\n  if ((hashconfig->opts_type & OPTS_TYPE_BINARY_HASHFILE) == 0)\n  {\n    CHECK_MANDATORY (module_ctx->module_hash_encode);\n  }\n\n  #undef CHECK_MANDATORY\n\n  if (module_ctx->module_benchmark_mask           != MODULE_DEFAULT) hashconfig->benchmark_mask          = module_ctx->module_benchmark_mask           (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_dictstat_disable         != MODULE_DEFAULT) hashconfig->dictstat_disable        = module_ctx->module_dictstat_disable         (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_esalt_size               != MODULE_DEFAULT) hashconfig->esalt_size              = module_ctx->module_esalt_size               (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_forced_outfile_format    != MODULE_DEFAULT) hashconfig->forced_outfile_format   = module_ctx->module_forced_outfile_format    (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_hash_mode                != MODULE_DEFAULT) hashconfig->hash_mode               = module_ctx->module_hash_mode                (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_hashes_count_min         != MODULE_DEFAULT) hashconfig->hashes_count_min        = module_ctx->module_hashes_count_min         (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_hashes_count_max         != MODULE_DEFAULT) hashconfig->hashes_count_max        = module_ctx->module_hashes_count_max         (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_hlfmt_disable            != MODULE_DEFAULT) hashconfig->hlfmt_disable           = module_ctx->module_hlfmt_disable            (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_hook_salt_size           != MODULE_DEFAULT) hashconfig->hook_salt_size          = module_ctx->module_hook_salt_size           (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_hook_size                != MODULE_DEFAULT) hashconfig->hook_size               = module_ctx->module_hook_size                (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_outfile_check_disable    != MODULE_DEFAULT) hashconfig->outfile_check_disable   = module_ctx->module_outfile_check_disable    (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_outfile_check_nocomp     != MODULE_DEFAULT) hashconfig->outfile_check_nocomp    = module_ctx->module_outfile_check_nocomp     (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_potfile_disable          != MODULE_DEFAULT) hashconfig->potfile_disable         = module_ctx->module_potfile_disable          (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_potfile_keep_all_hashes  != MODULE_DEFAULT) hashconfig->potfile_keep_all_hashes = module_ctx->module_potfile_keep_all_hashes  (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_pwdump_column            != MODULE_DEFAULT) hashconfig->pwdump_column           = module_ctx->module_pwdump_column            (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_separator                != MODULE_DEFAULT) hashconfig->separator               = module_ctx->module_separator                (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_tmp_size                 != MODULE_DEFAULT) hashconfig->tmp_size                = module_ctx->module_tmp_size                 (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_warmup_disable           != MODULE_DEFAULT) hashconfig->warmup_disable          = module_ctx->module_warmup_disable           (hashconfig, user_options, user_options_extra);\n\n  if (user_options->keyboard_layout_mapping)\n  {\n    if ((hashconfig->opts_type & OPTS_TYPE_KEYBOARD_MAPPING) == 0)\n    {\n      event_log_error (hashcat_ctx, \"Parameter --keyboard-layout-mapping not valid for hash-type %u\", hashconfig->hash_mode);\n\n      return -1;\n    }\n  }\n\n  if (user_options->self_test_disable == true)\n  {\n    hashconfig->opts_type |= OPTS_TYPE_SELF_TEST_DISABLE;\n  }\n\n  if (user_options->hex_charset)\n  {\n    hashconfig->opts_type |= OPTS_TYPE_PT_HEX;\n  }\n\n  if (user_options->hex_wordlist)\n  {\n    hashconfig->opts_type |= OPTS_TYPE_PT_HEX;\n  }\n\n  if (user_options->hex_salt)\n  {\n    if (hashconfig->salt_type == SALT_TYPE_GENERIC)\n    {\n      hashconfig->opts_type |= OPTS_TYPE_ST_HEX;\n    }\n    else\n    {\n      event_log_error (hashcat_ctx, \"Parameter --hex-salt not valid for hash-type %u\", hashconfig->hash_mode);\n\n      return -1;\n    }\n  }\n\n  if (user_options->keep_guessing)\n  {\n    hashconfig->opts_type |= OPTS_TYPE_PT_NEVERCRACK;\n  }\n  else\n  {\n    if (hashconfig->opti_type & OPTS_TYPE_SUGGEST_KG)\n    {\n      if (user_options->quiet == false)\n      {\n        event_log_warning (hashcat_ctx, \"This hash-mode is known to emit multiple valid password candidates for the same hash.\");\n        event_log_warning (hashcat_ctx, \"Use --keep-guessing to prevent hashcat from shutdown after the hash has been cracked.\");\n        event_log_warning (hashcat_ctx, NULL);\n      }\n    }\n  }\n\n  hashconfig->has_optimized_kernel  = false;\n  hashconfig->has_pure_kernel       = false;\n\n  if (module_ctx->module_kern_type_dynamic != MODULE_DEFAULT)\n  {\n    // some hash modes tell hashcat about their exact hash-mode inside the parser (eg. luks and jwt)\n  }\n  else\n  {\n    // some kernels do not have an optimized kernel, simply because they do not need them\n    // or because they are not yet converted, for them we should switch off optimized mode\n\n    char source_file[256] = { 0 };\n\n    generate_source_kernel_filename (user_options->slow_candidates, hashconfig->attack_exec, user_options_extra->attack_kern, hashconfig->kern_type, false, folder_config->shared_dir, source_file);\n\n    hashconfig->has_pure_kernel = hc_path_read (source_file);\n\n    generate_source_kernel_filename (user_options->slow_candidates, hashconfig->attack_exec, user_options_extra->attack_kern, hashconfig->kern_type, true, folder_config->shared_dir, source_file);\n\n    hashconfig->has_optimized_kernel = hc_path_read (source_file);\n\n    if (user_options->example_hashes == false)\n    {\n      if (user_options->optimized_kernel_enable == true)\n      {\n        if (hashconfig->has_optimized_kernel == false)\n        {\n          if (user_options->quiet == false)\n          {\n            event_log_warning (hashcat_ctx, \"Kernel %s:\", source_file);\n            event_log_warning (hashcat_ctx, \"Optimized kernel requested but not needed - falling back to pure kernel\");\n            event_log_warning (hashcat_ctx, NULL);\n          }\n        }\n        else\n        {\n          hashconfig->opti_type |= OPTI_TYPE_OPTIMIZED_KERNEL;\n        }\n      }\n      else\n      {\n        if (hashconfig->has_pure_kernel == false)\n        {\n          if (user_options->quiet == false) event_log_warning (hashcat_ctx, \"%s: Pure kernel not found, falling back to optimized kernel\", source_file);\n\n          hashconfig->opti_type |= OPTI_TYPE_OPTIMIZED_KERNEL;\n        }\n        else\n        {\n          // nothing to do\n        }\n      }\n    }\n  }\n\n  if ((hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL) == 0)\n  {\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_UTF16LE;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_UTF16BE;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_ADD01;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_ADD02;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_ADD06;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_ADD80;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_ADDBITS14;\n    hashconfig->opts_type &= ~OPTS_TYPE_PT_ADDBITS15;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_UTF16LE;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_UTF16BE;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_ADD01;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_ADD02;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_ADD80;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_ADDBITS14;\n    hashconfig->opts_type &= ~OPTS_TYPE_ST_ADDBITS15;\n\n    hashconfig->opti_type &= ~OPTI_TYPE_PRECOMPUTE_INIT;\n    hashconfig->opti_type &= ~OPTI_TYPE_MEET_IN_MIDDLE;\n    hashconfig->opti_type &= ~OPTI_TYPE_PREPENDED_SALT;\n    hashconfig->opti_type &= ~OPTI_TYPE_APPENDED_SALT;\n  }\n\n  const bool is_salted = ((hashconfig->salt_type == SALT_TYPE_GENERIC)\n                       |  (hashconfig->salt_type == SALT_TYPE_EMBEDDED)\n                       |  (hashconfig->salt_type == SALT_TYPE_VIRTUAL));\n\n  hashconfig->is_salted = is_salted;\n\n  // those depend on some previously defined values\n\n  hashconfig->pw_max              = default_pw_max              (hashconfig, user_options, user_options_extra);\n  hashconfig->pw_min              = default_pw_min              (hashconfig, user_options, user_options_extra);\n  hashconfig->salt_max            = default_salt_max            (hashconfig, user_options, user_options_extra);\n  hashconfig->salt_min            = default_salt_min            (hashconfig, user_options, user_options_extra);\n  hashconfig->kernel_accel_min    = default_kernel_accel_min    (hashconfig, user_options, user_options_extra);\n  hashconfig->kernel_accel_max    = default_kernel_accel_max    (hashconfig, user_options, user_options_extra);\n  hashconfig->kernel_loops_min    = default_kernel_loops_min    (hashconfig, user_options, user_options_extra);\n  hashconfig->kernel_loops_max    = default_kernel_loops_max    (hashconfig, user_options, user_options_extra);\n  hashconfig->kernel_threads_min  = default_kernel_threads_min  (hashconfig, user_options, user_options_extra);\n  hashconfig->kernel_threads_max  = default_kernel_threads_max  (hashconfig, user_options, user_options_extra);\n\n  if (module_ctx->module_pw_max             != MODULE_DEFAULT) hashconfig->pw_max             = module_ctx->module_pw_max             (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_pw_min             != MODULE_DEFAULT) hashconfig->pw_min             = module_ctx->module_pw_min             (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_salt_max           != MODULE_DEFAULT) hashconfig->salt_max           = module_ctx->module_salt_max           (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_salt_min           != MODULE_DEFAULT) hashconfig->salt_min           = module_ctx->module_salt_min           (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_kernel_accel_min   != MODULE_DEFAULT) hashconfig->kernel_accel_min   = module_ctx->module_kernel_accel_min   (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_kernel_accel_max   != MODULE_DEFAULT) hashconfig->kernel_accel_max   = module_ctx->module_kernel_accel_max   (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_kernel_loops_min   != MODULE_DEFAULT) hashconfig->kernel_loops_min   = module_ctx->module_kernel_loops_min   (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_kernel_loops_max   != MODULE_DEFAULT) hashconfig->kernel_loops_max   = module_ctx->module_kernel_loops_max   (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_kernel_threads_min != MODULE_DEFAULT) hashconfig->kernel_threads_min = module_ctx->module_kernel_threads_min (hashconfig, user_options, user_options_extra);\n  if (module_ctx->module_kernel_threads_max != MODULE_DEFAULT) hashconfig->kernel_threads_max = module_ctx->module_kernel_threads_max (hashconfig, user_options, user_options_extra);\n\n  return 0;\n}\n\nvoid hashconfig_destroy (hashcat_ctx_t *hashcat_ctx)\n{\n  hashconfig_t *hashconfig = hashcat_ctx->hashconfig;\n  module_ctx_t *module_ctx = hashcat_ctx->module_ctx;\n\n  module_unload (module_ctx);\n\n  memset (hashconfig, 0, sizeof (hashconfig_t));\n}\n\n/**\n * default functions\n */\n\nconst char *default_benchmark_mask (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const char *mask = \"?b?b?b?b?b?b?b\";\n\n  return mask;\n}\n\nu32 default_hash_mode (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 hash_mode = user_options->hash_mode;\n\n  return hash_mode;\n}\n\nu64 default_tmp_size (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u64 tmp_size = 4;\n\n  return tmp_size;\n}\n\nu64 default_esalt_size (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u64 esalt_size = 0;\n\n  return esalt_size;\n}\n\nu32 default_kernel_accel_min (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 kernel_accel_min = KERNEL_ACCEL_MIN;\n\n  return kernel_accel_min;\n}\n\nu32 default_kernel_accel_max (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 kernel_accel_max = KERNEL_ACCEL_MAX;\n\n  return kernel_accel_max;\n}\n\nu32 default_kernel_loops_min (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 kernel_loops_min = KERNEL_LOOPS_MIN;\n\n  return kernel_loops_min;\n}\n\nu32 default_kernel_loops_max (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 kernel_loops_max = KERNEL_LOOPS_MAX;\n\n  return kernel_loops_max;\n}\n\nu32 default_kernel_threads_min (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 kernel_threads_min = KERNEL_THREADS_MIN;\n\n  return kernel_threads_min;\n}\n\nu32 default_kernel_threads_max (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 kernel_threads_max = KERNEL_THREADS_MAX;\n\n  return kernel_threads_max;\n}\n\nu32 default_forced_outfile_format (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 forced_outfile_format = user_options->outfile_format;\n\n  return forced_outfile_format;\n}\n\nu64 default_hook_salt_size (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u64 hook_salt_size = 0;\n\n  return hook_salt_size;\n}\n\nu64 default_hook_size (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u64 hook_size = 4;\n\n  return hook_size;\n}\n\nchar default_separator (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  return user_options->separator;\n}\n\nbool default_dictstat_disable (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool dictstat_disable = false;\n\n  return dictstat_disable;\n}\n\nbool default_warmup_disable (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool warmup_disable = false;\n\n  return warmup_disable;\n}\n\nbool default_outfile_check_disable (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool outfile_check_disable = false;\n\n  return outfile_check_disable;\n}\n\nbool default_outfile_check_nocomp (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool outfile_check_nocomp = false;\n\n  return outfile_check_nocomp;\n}\n\nbool default_hlfmt_disable (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool hlfmt_disable = false;\n\n  return hlfmt_disable;\n}\n\nbool default_potfile_keep_all_hashes (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  bool potfile_keep_all_hashes = false;\n\n  // keep all hashes if --username was combined with --left or --show\n\n  if (user_options->username == true)\n  {\n    if ((user_options->show == true) || (user_options->left == true))\n    {\n      potfile_keep_all_hashes = true;\n    }\n  }\n\n  return potfile_keep_all_hashes;\n}\n\nu32 default_pwdump_column (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 pwdump_column = PWDUMP_COLUMN_INVALID;\n\n  return pwdump_column;\n}\n\nbool default_potfile_disable (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool potfile_disable = false;\n\n  return potfile_disable;\n}\n\nu32 default_hashes_count_min (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 hashes_count_min = 1;\n\n  return hashes_count_min;\n}\n\nu32 default_hashes_count_max (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const u32 hashes_count_max = 0xffffffff;\n\n  return hashes_count_max;\n}\n\nu32 default_pw_min (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool optimized_kernel = (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL);\n\n  // pw_min : algo specific hard min length\n\n  u32 pw_min = PW_MIN;\n\n  if (optimized_kernel == true)\n  {\n    // unused case\n  }\n\n  return pw_min;\n}\n\nu32 default_pw_max (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool optimized_kernel = (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL);\n\n  // pw_max : some algo suffer from support for long passwords,\n  //          the user need to add -L to enable support for them\n\n  u32 pw_max = PW_MAX;\n\n  if (optimized_kernel == true)\n  {\n    pw_max = PW_MAX_OLD;\n\n    if ((hashconfig->opts_type & OPTS_TYPE_PT_UTF16LE) || (hashconfig->opts_type & OPTS_TYPE_PT_UTF16BE))\n    {\n      pw_max /= 2;\n    }\n\n    #define PW_DICTMAX 31\n\n    if ((user_options->rp_files_cnt > 0) || (user_options->rp_gen > 0))\n    {\n      if (user_options->slow_candidates == true)\n      {\n        pw_max = MIN (pw_max, PW_DICTMAX);\n      }\n      else\n      {\n        switch (user_options_extra->attack_kern)\n        {\n          case ATTACK_KERN_STRAIGHT:  pw_max = MIN (pw_max, PW_DICTMAX);\n                                      break;\n          case ATTACK_KERN_COMBI:     pw_max = MIN (pw_max, PW_DICTMAX);\n                                      break;\n        }\n      }\n    }\n    else\n    {\n      if (user_options->slow_candidates == true)\n      {\n        if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n        {\n          pw_max = MIN (pw_max, PW_DICTMAX);\n        }\n        else\n        {\n          // If we have a NOOP rule then we can process words from wordlists > PW_DICTMAX for slow hashes\n        }\n      }\n      else\n      {\n        if (hashconfig->attack_exec == ATTACK_EXEC_INSIDE_KERNEL)\n        {\n          switch (user_options_extra->attack_kern)\n          {\n            case ATTACK_KERN_STRAIGHT:  pw_max = MIN (pw_max, PW_DICTMAX);\n                                        break;\n            case ATTACK_KERN_COMBI:     pw_max = MIN (pw_max, PW_DICTMAX);\n                                        break;\n          }\n        }\n        else\n        {\n          // If we have a NOOP rule then we can process words from wordlists > PW_DICTMAX for slow hashes\n        }\n      }\n    }\n  }\n\n  return pw_max;\n}\n\nu32 default_salt_min (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  // salt_min : this limit is only interessting for generic hash types that support a salt\n\n  u32 salt_min = SALT_MIN;\n\n  if (hashconfig->salt_type == SALT_TYPE_GENERIC)\n  {\n    if (hashconfig->opts_type & OPTS_TYPE_ST_HEX)\n    {\n      salt_min *= 2;\n    }\n  }\n\n  return salt_min;\n}\n\nu32 default_salt_max (MAYBE_UNUSED const hashconfig_t *hashconfig, MAYBE_UNUSED const user_options_t *user_options, MAYBE_UNUSED const user_options_extra_t *user_options_extra)\n{\n  const bool optimized_kernel = (hashconfig->opti_type & OPTI_TYPE_OPTIMIZED_KERNEL);\n\n  // salt_max : this limit is only interessting for generic hash types that support a salt\n\n  u32 salt_max = SALT_MAX;\n\n  if (optimized_kernel == true)\n  {\n    salt_max = SALT_MAX_OLD;\n\n    if ((hashconfig->opts_type & OPTS_TYPE_ST_UTF16LE) || (hashconfig->opts_type & OPTS_TYPE_ST_UTF16BE))\n    {\n      salt_max /= 2;\n    }\n  }\n\n  if (hashconfig->salt_type == SALT_TYPE_GENERIC)\n  {\n    if (hashconfig->opts_type & OPTS_TYPE_ST_HEX)\n    {\n      salt_max *= 2;\n    }\n  }\n\n  return salt_max;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/include/dynloader.h": "/**\n * Author......: See docs/credits.txt\n * License.....: MIT\n */\n\n#ifndef _DYNLOADER_H\n#define _DYNLOADER_H\n\n#include <stdlib.h>\n\n#ifdef _WIN\n#include <windows.h>\n#else\n#include <dlfcn.h>\n#if defined (__APPLE__)\n#include <mach-o/dyld.h>\n#endif // __APPLE__\n#endif // _WIN\n\n#ifdef _WIN\nhc_dynlib_t  hc_dlopen  (LPCSTR lpLibFileName);\nBOOL         hc_dlclose (hc_dynlib_t hLibModule);\nhc_dynfunc_t hc_dlsym   (hc_dynlib_t hModule, LPCSTR lpProcName);\n#else\nhc_dynlib_t  hc_dlopen  (const char *filename);\nint          hc_dlclose (hc_dynlib_t handle);\nhc_dynfunc_t hc_dlsym   (hc_dynlib_t handle, const char *symbol);\n#endif\n\n#define HC_LOAD_FUNC2(ptr,name,type,var,libname,noerr) \\\n  do { \\\n    ptr->name = (type) hc_dlsym (ptr->var, #name); \\\n    if (noerr != -1) { \\\n      if (!ptr->name) { \\\n        if (noerr == 1) { \\\n          event_log_error (hashcat_ctx, \"%s is missing from %s shared library.\", #name, #libname); \\\n          return -1; \\\n        } \\\n        if (noerr != 1) { \\\n          event_log_warning (hashcat_ctx, \"%s is missing from %s shared library.\", #name, #libname); \\\n          return 0; \\\n        } \\\n      } \\\n    } \\\n  } while (0)\n\n#define HC_LOAD_FUNC(ptr,name,type,libname,noerr) \\\n  do { \\\n    ptr->name = (type) hc_dlsym (ptr->lib, #name); \\\n    if (noerr != -1) { \\\n      if (!ptr->name) { \\\n        if (noerr == 1) { \\\n          event_log_error (hashcat_ctx, \"%s is missing from %s shared library.\", #name, #libname); \\\n          return -1; \\\n        } \\\n        if (noerr != 1) { \\\n          event_log_warning (hashcat_ctx, \"%s is missing from %s shared library.\", #name, #libname); \\\n          return 0; \\\n        } \\\n      } \\\n    } \\\n  } while (0)\n\n#define HC_LOAD_ADDR(ptr,name,type,func,addr,libname,noerr) \\\n  do { \\\n    ptr->name = (type) (*ptr->func) (addr); \\\n    if (!ptr->name) { \\\n      if (noerr == 1) { \\\n        event_log_error (hashcat_ctx, \"%s at address %08x is missing from %s shared library.\", #name, addr, #libname); \\\n        return -1; \\\n      } \\\n      if (noerr != 1) { \\\n        event_log_warning (hashcat_ctx, \"%s at address %08x is missing from %s shared library.\", #name, addr, #libname); \\\n        return 0; \\\n      } \\\n    } \\\n  } while (0)\n\n#endif // _DYNALOADER_H\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/hashcat.hcstat2",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Italian/it_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Italian/it_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Italian/it_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Spanish/es_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Spanish/es_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Spanish/es_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Slovak/sk_ISO-8859-2-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Slovak/sk_cp1250-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Russian/ru_cp1251-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Russian/ru_ISO-8859-5-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Polish/pl_cp1250-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Catalan/ca_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Catalan/ca_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Catalan/ca_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Portuguese/pt_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Portuguese/pt_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Portuguese/pt_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Greek/el_ISO-8859-7-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Greek/el_cp1253-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Castilian/es-ES_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Castilian/es-ES_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/Castilian/es-ES_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/French/fr_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/French/fr_ISO-8859-16-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/French/fr_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/French/fr_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/German/de_ISO-8859-1-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/German/de_cp1252-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/special/German/de_ISO-8859-15-special.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/GreekPolytonic.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/French.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/English.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Castilian.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Slovak.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Greek.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Polish.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Italian.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/German.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Bulgarian.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Portuguese.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Lithuanian.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Russian.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Catalan.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/combined/Spanish.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Italian/it_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Italian/it_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Italian/it_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Spanish/es_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Spanish/es_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Spanish/es_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Slovak/sk_cp1250.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Slovak/sk_ISO-8859-2.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Russian/ru_KOI8-R.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Russian/ru_ISO-8859-5.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Russian/ru_cp1251.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Lithuanian/lt_ISO-8859-4.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Lithuanian/lt_ISO-8859-13.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Lithuanian/lt_cp1257.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Polish/pl_ISO-8859-2.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Polish/pl_cp1250.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/GreekPolytonic/el_polytonic_ISO-8859-7.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/GreekPolytonic/el_polytonic_cp1253.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Hungarian/hu_cp1250.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Catalan/ca_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Catalan/ca_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Catalan/ca_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Bulgarian/bg_cp1251.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Bulgarian/bg_ISO-8859-5.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Bulgarian/bg_KOI8-R.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Portuguese/pt_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Portuguese/pt_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Portuguese/pt_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Greek/el_ISO-8859-7.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Greek/el_cp1253.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Castilian/es-ES_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Castilian/es-ES_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/Castilian/es-ES_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/French/fr_ISO-8859-16.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/French/fr_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/French/fr_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/French/fr_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/English/en_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/English/en_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/English/en_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/German/de_ISO-8859-1.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/German/de_cp1252.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/charsets/standard/German/de_ISO-8859-15.hcchr",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/LZMA-SDK/C/Util/SfxSetup/setup.ico",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/zlib.3.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/blast/test.pk",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/puff/zeros.raw",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib.chm",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/Deflater.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/ChecksumImpl.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/CircularBuffer.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/DotZLib.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/Inflater.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/GZipStream.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/deps/zlib/contrib/dotzlib/DotZLib/CodecBase.cs",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_aes-twofish-serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_serpent-twofish-aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_serpent-aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_serpent-twofish-aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_serpent-aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_twofish-serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_aes_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_serpent-aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_aes-twofish_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_aes-twofish.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_twofish.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_aes-twofish-serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_aes-twofish-serpent_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_serpent-twofish-aes_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_serpent_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_twofish_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_twofish.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_aes-twofish.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_serpent-aes_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_twofish.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_serpent-twofish-aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_ripemd160_twofish-serpent_boot.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_aes.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_aes-twofish.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_twofish-serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_whirlpool_twofish-serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/tc_tests/hashcat_sha512_aes-twofish-serpent.tc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_aes-twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_serpent-twofish-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_camellia-kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_twofish_boot.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_camellia-kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_kuznyechik-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_kuznyechik-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_aes-twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_kuznyechik-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_camellia-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_serpent-twofish-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_aes-twofish_boot.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_serpent-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_aes-twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_kuznyechik-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_aes-twofish-serpent_boot.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_serpent-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_serpent-twofish-aes_boot.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_camellia-kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_aes-twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_camellia-kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_kuznyechik-serpent-camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_kuznyechik-serpent-camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_aes-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_serpent-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_aes-twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_camellia-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_serpent-twofish-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_kuznyechik-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_kuznyechik-serpent-camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_serpent-aes_boot.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_camellia-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_kuznyechik-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_twofish-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_kuznyechik-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_aes_boot_pim500.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_serpent-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_kuznyechik-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_serpent-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_kuznyechik-serpent-camellia.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_serpent-twofish-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_aes-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha256_serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_serpent-twofish-aes.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_aes-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_kuznyechik.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_whirlpool_aes-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_aes_boot.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_ripemd160_aes-twofish.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_streebog_camellia-serpent.vc",
        "/tmp/vanessa/spack-stage/spack-stage-hashcat-6.1.1-zgkq6lqooibsxjx5xauuarm7ramj3l2b/spack-src/tools/vc_tests/hashcat_sha512_camellia.vc"
    ],
    "total_files": 2502
}