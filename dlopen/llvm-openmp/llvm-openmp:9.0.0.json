{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/runtime/src/z_Linux_util.cpp": "/*\n * z_Linux_util.cpp -- platform specific routines.\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#include \"kmp.h\"\n#include \"kmp_affinity.h\"\n#include \"kmp_i18n.h\"\n#include \"kmp_io.h\"\n#include \"kmp_itt.h\"\n#include \"kmp_lock.h\"\n#include \"kmp_stats.h\"\n#include \"kmp_str.h\"\n#include \"kmp_wait_release.h\"\n#include \"kmp_wrapper_getpid.h\"\n\n#if !KMP_OS_DRAGONFLY && !KMP_OS_FREEBSD && !KMP_OS_NETBSD && !KMP_OS_OPENBSD\n#include <alloca.h>\n#endif\n#include <math.h> // HUGE_VAL.\n#include <sys/resource.h>\n#include <sys/syscall.h>\n#include <sys/time.h>\n#include <sys/times.h>\n#include <unistd.h>\n\n#if KMP_OS_LINUX && !KMP_OS_CNK\n#include <sys/sysinfo.h>\n#if KMP_USE_FUTEX\n// We should really include <futex.h>, but that causes compatibility problems on\n// different Linux* OS distributions that either require that you include (or\n// break when you try to include) <pci/types.h>. Since all we need is the two\n// macros below (which are part of the kernel ABI, so can't change) we just\n// define the constants here and don't include <futex.h>\n#ifndef FUTEX_WAIT\n#define FUTEX_WAIT 0\n#endif\n#ifndef FUTEX_WAKE\n#define FUTEX_WAKE 1\n#endif\n#endif\n#elif KMP_OS_DARWIN\n#include <mach/mach.h>\n#include <sys/sysctl.h>\n#elif KMP_OS_DRAGONFLY || KMP_OS_FREEBSD\n#include <pthread_np.h>\n#elif KMP_OS_NETBSD\n#include <sys/types.h>\n#include <sys/sysctl.h>\n#endif\n\n#include <ctype.h>\n#include <dirent.h>\n#include <fcntl.h>\n\n#include \"tsan_annotations.h\"\n\nstruct kmp_sys_timer {\n  struct timespec start;\n};\n\n// Convert timespec to nanoseconds.\n#define TS2NS(timespec) (((timespec).tv_sec * 1e9) + (timespec).tv_nsec)\n\nstatic struct kmp_sys_timer __kmp_sys_timer_data;\n\n#if KMP_HANDLE_SIGNALS\ntypedef void (*sig_func_t)(int);\nSTATIC_EFI2_WORKAROUND struct sigaction __kmp_sighldrs[NSIG];\nstatic sigset_t __kmp_sigset;\n#endif\n\nstatic int __kmp_init_runtime = FALSE;\n\nstatic int __kmp_fork_count = 0;\n\nstatic pthread_condattr_t __kmp_suspend_cond_attr;\nstatic pthread_mutexattr_t __kmp_suspend_mutex_attr;\n\nstatic kmp_cond_align_t __kmp_wait_cv;\nstatic kmp_mutex_align_t __kmp_wait_mx;\n\nkmp_uint64 __kmp_ticks_per_msec = 1000000;\n\n#ifdef DEBUG_SUSPEND\nstatic void __kmp_print_cond(char *buffer, kmp_cond_align_t *cond) {\n  KMP_SNPRINTF(buffer, 128, \"(cond (lock (%ld, %d)), (descr (%p)))\",\n               cond->c_cond.__c_lock.__status, cond->c_cond.__c_lock.__spinlock,\n               cond->c_cond.__c_waiting);\n}\n#endif\n\n#if (KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED)\n\n/* Affinity support */\n\nvoid __kmp_affinity_bind_thread(int which) {\n  KMP_ASSERT2(KMP_AFFINITY_CAPABLE(),\n              \"Illegal set affinity operation when not capable\");\n\n  kmp_affin_mask_t *mask;\n  KMP_CPU_ALLOC_ON_STACK(mask);\n  KMP_CPU_ZERO(mask);\n  KMP_CPU_SET(which, mask);\n  __kmp_set_system_affinity(mask, TRUE);\n  KMP_CPU_FREE_FROM_STACK(mask);\n}\n\n/* Determine if we can access affinity functionality on this version of\n * Linux* OS by checking __NR_sched_{get,set}affinity system calls, and set\n * __kmp_affin_mask_size to the appropriate value (0 means not capable). */\nvoid __kmp_affinity_determine_capable(const char *env_var) {\n// Check and see if the OS supports thread affinity.\n\n#define KMP_CPU_SET_SIZE_LIMIT (1024 * 1024)\n\n  int gCode;\n  int sCode;\n  unsigned char *buf;\n  buf = (unsigned char *)KMP_INTERNAL_MALLOC(KMP_CPU_SET_SIZE_LIMIT);\n\n  // If Linux* OS:\n  // If the syscall fails or returns a suggestion for the size,\n  // then we don't have to search for an appropriate size.\n  gCode = syscall(__NR_sched_getaffinity, 0, KMP_CPU_SET_SIZE_LIMIT, buf);\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"initial getaffinity call returned %d errno = %d\\n\",\n                gCode, errno));\n\n  // if ((gCode < 0) && (errno == ENOSYS))\n  if (gCode < 0) {\n    // System call not supported\n    if (__kmp_affinity_verbose ||\n        (__kmp_affinity_warnings && (__kmp_affinity_type != affinity_none) &&\n         (__kmp_affinity_type != affinity_default) &&\n         (__kmp_affinity_type != affinity_disabled))) {\n      int error = errno;\n      kmp_msg_t err_code = KMP_ERR(error);\n      __kmp_msg(kmp_ms_warning, KMP_MSG(GetAffSysCallNotSupported, env_var),\n                err_code, __kmp_msg_null);\n      if (__kmp_generate_warnings == kmp_warnings_off) {\n        __kmp_str_free(&err_code.str);\n      }\n    }\n    KMP_AFFINITY_DISABLE();\n    KMP_INTERNAL_FREE(buf);\n    return;\n  }\n  if (gCode > 0) { // Linux* OS only\n    // The optimal situation: the OS returns the size of the buffer it expects.\n    //\n    // A verification of correct behavior is that Isetaffinity on a NULL\n    // buffer with the same size fails with errno set to EFAULT.\n    sCode = syscall(__NR_sched_setaffinity, 0, gCode, NULL);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"setaffinity for mask size %d returned %d errno = %d\\n\",\n                  gCode, sCode, errno));\n    if (sCode < 0) {\n      if (errno == ENOSYS) {\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(SetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n      }\n      if (errno == EFAULT) {\n        KMP_AFFINITY_ENABLE(gCode);\n        KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                      \"affinity supported (mask size %d)\\n\",\n                      (int)__kmp_affin_mask_size));\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n    }\n  }\n\n  // Call the getaffinity system call repeatedly with increasing set sizes\n  // until we succeed, or reach an upper bound on the search.\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"searching for proper set size\\n\"));\n  int size;\n  for (size = 1; size <= KMP_CPU_SET_SIZE_LIMIT; size *= 2) {\n    gCode = syscall(__NR_sched_getaffinity, 0, size, buf);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"getaffinity for mask size %d returned %d errno = %d\\n\",\n                  size, gCode, errno));\n\n    if (gCode < 0) {\n      if (errno == ENOSYS) {\n        // We shouldn't get here\n        KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                      \"inconsistent OS call behavior: errno == ENOSYS for mask \"\n                      \"size %d\\n\",\n                      size));\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(GetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n      continue;\n    }\n\n    sCode = syscall(__NR_sched_setaffinity, 0, gCode, NULL);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"setaffinity for mask size %d returned %d errno = %d\\n\",\n                  gCode, sCode, errno));\n    if (sCode < 0) {\n      if (errno == ENOSYS) { // Linux* OS only\n        // We shouldn't get here\n        KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                      \"inconsistent OS call behavior: errno == ENOSYS for mask \"\n                      \"size %d\\n\",\n                      size));\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(SetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n      if (errno == EFAULT) {\n        KMP_AFFINITY_ENABLE(gCode);\n        KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                      \"affinity supported (mask size %d)\\n\",\n                      (int)__kmp_affin_mask_size));\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n    }\n  }\n  // save uncaught error code\n  // int error = errno;\n  KMP_INTERNAL_FREE(buf);\n  // restore uncaught error code, will be printed at the next KMP_WARNING below\n  // errno = error;\n\n  // Affinity is not supported\n  KMP_AFFINITY_DISABLE();\n  KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                \"cannot determine mask size - affinity not supported\\n\"));\n  if (__kmp_affinity_verbose ||\n      (__kmp_affinity_warnings && (__kmp_affinity_type != affinity_none) &&\n       (__kmp_affinity_type != affinity_default) &&\n       (__kmp_affinity_type != affinity_disabled))) {\n    KMP_WARNING(AffCantGetMaskSize, env_var);\n  }\n}\n\n#endif // KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED\n\n#if KMP_USE_FUTEX\n\nint __kmp_futex_determine_capable() {\n  int loc = 0;\n  int rc = syscall(__NR_futex, &loc, FUTEX_WAKE, 1, NULL, NULL, 0);\n  int retval = (rc == 0) || (errno != ENOSYS);\n\n  KA_TRACE(10,\n           (\"__kmp_futex_determine_capable: rc = %d errno = %d\\n\", rc, errno));\n  KA_TRACE(10, (\"__kmp_futex_determine_capable: futex syscall%s supported\\n\",\n                retval ? \"\" : \" not\"));\n\n  return retval;\n}\n\n#endif // KMP_USE_FUTEX\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (!KMP_ASM_INTRINS)\n/* Only 32-bit \"add-exchange\" instruction on IA-32 architecture causes us to\n   use compare_and_store for these routines */\n\nkmp_int8 __kmp_test_then_or8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value | d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_int8 __kmp_test_then_and8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value & d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\nkmp_uint32 __kmp_test_then_or32(volatile kmp_uint32 *p, kmp_uint32 d) {\n  kmp_uint32 old_value, new_value;\n\n  old_value = TCR_4(*p);\n  new_value = old_value | d;\n\n  while (!KMP_COMPARE_AND_STORE_REL32(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_4(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_uint32 __kmp_test_then_and32(volatile kmp_uint32 *p, kmp_uint32 d) {\n  kmp_uint32 old_value, new_value;\n\n  old_value = TCR_4(*p);\n  new_value = old_value & d;\n\n  while (!KMP_COMPARE_AND_STORE_REL32(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_4(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\n#if KMP_ARCH_X86\nkmp_int8 __kmp_test_then_add8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value + d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value + d;\n  }\n  return old_value;\n}\n\nkmp_int64 __kmp_test_then_add64(volatile kmp_int64 *p, kmp_int64 d) {\n  kmp_int64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value + d;\n\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value + d;\n  }\n  return old_value;\n}\n#endif /* KMP_ARCH_X86 */\n\nkmp_uint64 __kmp_test_then_or64(volatile kmp_uint64 *p, kmp_uint64 d) {\n  kmp_uint64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value | d;\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_uint64 __kmp_test_then_and64(volatile kmp_uint64 *p, kmp_uint64 d) {\n  kmp_uint64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value & d;\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\n#endif /* (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (! KMP_ASM_INTRINS) */\n\nvoid __kmp_terminate_thread(int gtid) {\n  int status;\n  kmp_info_t *th = __kmp_threads[gtid];\n\n  if (!th)\n    return;\n\n#ifdef KMP_CANCEL_THREADS\n  KA_TRACE(10, (\"__kmp_terminate_thread: kill (%d)\\n\", gtid));\n  status = pthread_cancel(th->th.th_info.ds.ds_thread);\n  if (status != 0 && status != ESRCH) {\n    __kmp_fatal(KMP_MSG(CantTerminateWorkerThread), KMP_ERR(status),\n                __kmp_msg_null);\n  }\n#endif\n  KMP_YIELD(TRUE);\n} //\n\n/* Set thread stack info according to values returned by pthread_getattr_np().\n   If values are unreasonable, assume call failed and use incremental stack\n   refinement method instead. Returns TRUE if the stack parameters could be\n   determined exactly, FALSE if incremental refinement is necessary. */\nstatic kmp_int32 __kmp_set_stack_info(int gtid, kmp_info_t *th) {\n  int stack_data;\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_HURD\n  pthread_attr_t attr;\n  int status;\n  size_t size = 0;\n  void *addr = 0;\n\n  /* Always do incremental stack refinement for ubermaster threads since the\n     initial thread stack range can be reduced by sibling thread creation so\n     pthread_attr_getstack may cause thread gtid aliasing */\n  if (!KMP_UBER_GTID(gtid)) {\n\n    /* Fetch the real thread attributes */\n    status = pthread_attr_init(&attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_init\", status);\n#if KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD\n    status = pthread_attr_get_np(pthread_self(), &attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_get_np\", status);\n#else\n    status = pthread_getattr_np(pthread_self(), &attr);\n    KMP_CHECK_SYSFAIL(\"pthread_getattr_np\", status);\n#endif\n    status = pthread_attr_getstack(&attr, &addr, &size);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_getstack\", status);\n    KA_TRACE(60,\n             (\"__kmp_set_stack_info: T#%d pthread_attr_getstack returned size:\"\n              \" %lu, low addr: %p\\n\",\n              gtid, size, addr));\n    status = pthread_attr_destroy(&attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_destroy\", status);\n  }\n\n  if (size != 0 && addr != 0) { // was stack parameter determination successful?\n    /* Store the correct base and size */\n    TCW_PTR(th->th.th_info.ds.ds_stackbase, (((char *)addr) + size));\n    TCW_PTR(th->th.th_info.ds.ds_stacksize, size);\n    TCW_4(th->th.th_info.ds.ds_stackgrow, FALSE);\n    return TRUE;\n  }\n#endif /* KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||\n              KMP_OS_HURD */\n  /* Use incremental refinement starting from initial conservative estimate */\n  TCW_PTR(th->th.th_info.ds.ds_stacksize, 0);\n  TCW_PTR(th->th.th_info.ds.ds_stackbase, &stack_data);\n  TCW_4(th->th.th_info.ds.ds_stackgrow, TRUE);\n  return FALSE;\n}\n\nstatic void *__kmp_launch_worker(void *thr) {\n  int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n  sigset_t new_set, old_set;\n#endif /* KMP_BLOCK_SIGNALS */\n  void *exit_val;\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_OPENBSD || KMP_OS_HURD\n  void *volatile padding = 0;\n#endif\n  int gtid;\n\n  gtid = ((kmp_info_t *)thr)->th.th_info.ds.ds_gtid;\n  __kmp_gtid_set_specific(gtid);\n#ifdef KMP_TDATA_GTID\n  __kmp_gtid = gtid;\n#endif\n#if KMP_STATS_ENABLED\n  // set thread local index to point to thread-specific stats\n  __kmp_stats_thread_ptr = ((kmp_info_t *)thr)->th.th_stats;\n  __kmp_stats_thread_ptr->startLife();\n  KMP_SET_THREAD_STATE(IDLE);\n  KMP_INIT_PARTITIONED_TIMERS(OMP_idle);\n#endif\n\n#if USE_ITT_BUILD\n  __kmp_itt_thread_name(gtid);\n#endif /* USE_ITT_BUILD */\n\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_set_init_mask(gtid, FALSE);\n#endif\n\n#ifdef KMP_CANCEL_THREADS\n  status = pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, &old_type);\n  KMP_CHECK_SYSFAIL(\"pthread_setcanceltype\", status);\n  // josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads?\n  status = pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n  // Set FP control regs to be a copy of the parallel initialization thread's.\n  __kmp_clear_x87_fpu_status_word();\n  __kmp_load_x87_fpu_control_word(&__kmp_init_x87_fpu_control_word);\n  __kmp_load_mxcsr(&__kmp_init_mxcsr);\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = sigfillset(&new_set);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigfillset\", status);\n  status = pthread_sigmask(SIG_BLOCK, &new_set, &old_set);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_OPENBSD\n  if (__kmp_stkoffset > 0 && gtid > 0) {\n    padding = KMP_ALLOCA(gtid * __kmp_stkoffset);\n  }\n#endif\n\n  KMP_MB();\n  __kmp_set_stack_info(gtid, (kmp_info_t *)thr);\n\n  __kmp_check_stack_overlap((kmp_info_t *)thr);\n\n  exit_val = __kmp_launch_thread((kmp_info_t *)thr);\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = pthread_sigmask(SIG_SETMASK, &old_set, NULL);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n  return exit_val;\n}\n\n#if KMP_USE_MONITOR\n/* The monitor thread controls all of the threads in the complex */\n\nstatic void *__kmp_launch_monitor(void *thr) {\n  int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n  sigset_t new_set;\n#endif /* KMP_BLOCK_SIGNALS */\n  struct timespec interval;\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #1 launched\\n\"));\n\n  /* register us as the monitor thread */\n  __kmp_gtid_set_specific(KMP_GTID_MONITOR);\n#ifdef KMP_TDATA_GTID\n  __kmp_gtid = KMP_GTID_MONITOR;\n#endif\n\n  KMP_MB();\n\n#if USE_ITT_BUILD\n  // Instruct Intel(R) Threading Tools to ignore monitor thread.\n  __kmp_itt_thread_ignore();\n#endif /* USE_ITT_BUILD */\n\n  __kmp_set_stack_info(((kmp_info_t *)thr)->th.th_info.ds.ds_gtid,\n                       (kmp_info_t *)thr);\n\n  __kmp_check_stack_overlap((kmp_info_t *)thr);\n\n#ifdef KMP_CANCEL_THREADS\n  status = pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, &old_type);\n  KMP_CHECK_SYSFAIL(\"pthread_setcanceltype\", status);\n  // josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads?\n  status = pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n\n#if KMP_REAL_TIME_FIX\n  // This is a potential fix which allows application with real-time scheduling\n  // policy work. However, decision about the fix is not made yet, so it is\n  // disabled by default.\n  { // Are program started with real-time scheduling policy?\n    int sched = sched_getscheduler(0);\n    if (sched == SCHED_FIFO || sched == SCHED_RR) {\n      // Yes, we are a part of real-time application. Try to increase the\n      // priority of the monitor.\n      struct sched_param param;\n      int max_priority = sched_get_priority_max(sched);\n      int rc;\n      KMP_WARNING(RealTimeSchedNotSupported);\n      sched_getparam(0, &param);\n      if (param.sched_priority < max_priority) {\n        param.sched_priority += 1;\n        rc = sched_setscheduler(0, sched, &param);\n        if (rc != 0) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(CantChangeMonitorPriority),\n                    err_code, KMP_MSG(MonitorWillStarve), __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n      } else {\n        // We cannot abort here, because number of CPUs may be enough for all\n        // the threads, including the monitor thread, so application could\n        // potentially work...\n        __kmp_msg(kmp_ms_warning, KMP_MSG(RunningAtMaxPriority),\n                  KMP_MSG(MonitorWillStarve), KMP_HNT(RunningAtMaxPriority),\n                  __kmp_msg_null);\n      }\n    }\n    // AC: free thread that waits for monitor started\n    TCW_4(__kmp_global.g.g_time.dt.t_value, 0);\n  }\n#endif // KMP_REAL_TIME_FIX\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  if (__kmp_monitor_wakeups == 1) {\n    interval.tv_sec = 1;\n    interval.tv_nsec = 0;\n  } else {\n    interval.tv_sec = 0;\n    interval.tv_nsec = (KMP_NSEC_PER_SEC / __kmp_monitor_wakeups);\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #2 monitor\\n\"));\n\n  while (!TCR_4(__kmp_global.g.g_done)) {\n    struct timespec now;\n    struct timeval tval;\n\n    /*  This thread monitors the state of the system */\n\n    KA_TRACE(15, (\"__kmp_launch_monitor: update\\n\"));\n\n    status = gettimeofday(&tval, NULL);\n    KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n    TIMEVAL_TO_TIMESPEC(&tval, &now);\n\n    now.tv_sec += interval.tv_sec;\n    now.tv_nsec += interval.tv_nsec;\n\n    if (now.tv_nsec >= KMP_NSEC_PER_SEC) {\n      now.tv_sec += 1;\n      now.tv_nsec -= KMP_NSEC_PER_SEC;\n    }\n\n    status = pthread_mutex_lock(&__kmp_wait_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n    // AC: the monitor should not fall asleep if g_done has been set\n    if (!TCR_4(__kmp_global.g.g_done)) { // check once more under mutex\n      status = pthread_cond_timedwait(&__kmp_wait_cv.c_cond,\n                                      &__kmp_wait_mx.m_mutex, &now);\n      if (status != 0) {\n        if (status != ETIMEDOUT && status != EINTR) {\n          KMP_SYSFAIL(\"pthread_cond_timedwait\", status);\n        }\n      }\n    }\n    status = pthread_mutex_unlock(&__kmp_wait_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n\n    TCW_4(__kmp_global.g.g_time.dt.t_value,\n          TCR_4(__kmp_global.g.g_time.dt.t_value) + 1);\n\n    KMP_MB(); /* Flush all pending memory write invalidates.  */\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #3 cleanup\\n\"));\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = sigfillset(&new_set);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigfillset\", status);\n  status = pthread_sigmask(SIG_UNBLOCK, &new_set, NULL);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #4 finished\\n\"));\n\n  if (__kmp_global.g.g_abort != 0) {\n    /* now we need to terminate the worker threads  */\n    /* the value of t_abort is the signal we caught */\n\n    int gtid;\n\n    KA_TRACE(10, (\"__kmp_launch_monitor: #5 terminate sig=%d\\n\",\n                  __kmp_global.g.g_abort));\n\n    /* terminate the OpenMP worker threads */\n    /* TODO this is not valid for sibling threads!!\n     * the uber master might not be 0 anymore.. */\n    for (gtid = 1; gtid < __kmp_threads_capacity; ++gtid)\n      __kmp_terminate_thread(gtid);\n\n    __kmp_cleanup();\n\n    KA_TRACE(10, (\"__kmp_launch_monitor: #6 raise sig=%d\\n\",\n                  __kmp_global.g.g_abort));\n\n    if (__kmp_global.g.g_abort > 0)\n      raise(__kmp_global.g.g_abort);\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #7 exit\\n\"));\n\n  return thr;\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_create_worker(int gtid, kmp_info_t *th, size_t stack_size) {\n  pthread_t handle;\n  pthread_attr_t thread_attr;\n  int status;\n\n  th->th.th_info.ds.ds_gtid = gtid;\n\n#if KMP_STATS_ENABLED\n  // sets up worker thread stats\n  __kmp_acquire_tas_lock(&__kmp_stats_lock, gtid);\n\n  // th->th.th_stats is used to transfer thread-specific stats-pointer to\n  // __kmp_launch_worker. So when thread is created (goes into\n  // __kmp_launch_worker) it will set its thread local pointer to\n  // th->th.th_stats\n  if (!KMP_UBER_GTID(gtid)) {\n    th->th.th_stats = __kmp_stats_list->push_back(gtid);\n  } else {\n    // For root threads, __kmp_stats_thread_ptr is set in __kmp_register_root(),\n    // so set the th->th.th_stats field to it.\n    th->th.th_stats = __kmp_stats_thread_ptr;\n  }\n  __kmp_release_tas_lock(&__kmp_stats_lock, gtid);\n\n#endif // KMP_STATS_ENABLED\n\n  if (KMP_UBER_GTID(gtid)) {\n    KA_TRACE(10, (\"__kmp_create_worker: uber thread (%d)\\n\", gtid));\n    th->th.th_info.ds.ds_thread = pthread_self();\n    __kmp_set_stack_info(gtid, th);\n    __kmp_check_stack_overlap(th);\n    return;\n  }\n\n  KA_TRACE(10, (\"__kmp_create_worker: try to create thread (%d)\\n\", gtid));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_init(&thread_attr);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantInitThreadAttrs), KMP_ERR(status), __kmp_msg_null);\n  }\n  status = pthread_attr_setdetachstate(&thread_attr, PTHREAD_CREATE_JOINABLE);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetWorkerState), KMP_ERR(status), __kmp_msg_null);\n  }\n\n  /* Set stack size for this thread now.\n     The multiple of 2 is there because on some machines, requesting an unusual\n     stacksize causes the thread to have an offset before the dummy alloca()\n     takes place to create the offset.  Since we want the user to have a\n     sufficient stacksize AND support a stack offset, we alloca() twice the\n     offset so that the upcoming alloca() does not eliminate any premade offset,\n     and also gives the user the stack space they requested for all threads */\n  stack_size += gtid * __kmp_stkoffset * 2;\n\n  KA_TRACE(10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                \"__kmp_stksize = %lu bytes, final stacksize = %lu bytes\\n\",\n                gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size));\n\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  status = pthread_attr_setstacksize(&thread_attr, stack_size);\n#ifdef KMP_BACKUP_STKSIZE\n  if (status != 0) {\n    if (!__kmp_env_stksize) {\n      stack_size = KMP_BACKUP_STKSIZE + gtid * __kmp_stkoffset;\n      __kmp_stksize = KMP_BACKUP_STKSIZE;\n      KA_TRACE(10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                    \"__kmp_stksize = %lu bytes, (backup) final stacksize = %lu \"\n                    \"bytes\\n\",\n                    gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size));\n      status = pthread_attr_setstacksize(&thread_attr, stack_size);\n    }\n  }\n#endif /* KMP_BACKUP_STKSIZE */\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                KMP_HNT(ChangeWorkerStackSize), __kmp_msg_null);\n  }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n#endif /* KMP_THREAD_ATTR */\n\n  status =\n      pthread_create(&handle, &thread_attr, __kmp_launch_worker, (void *)th);\n  if (status != 0 || !handle) { // ??? Why do we check handle??\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n    if (status == EINVAL) {\n      __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                  KMP_HNT(IncreaseWorkerStackSize), __kmp_msg_null);\n    }\n    if (status == ENOMEM) {\n      __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                  KMP_HNT(DecreaseWorkerStackSize), __kmp_msg_null);\n    }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    if (status == EAGAIN) {\n      __kmp_fatal(KMP_MSG(NoResourcesForWorkerThread), KMP_ERR(status),\n                  KMP_HNT(Decrease_NUM_THREADS), __kmp_msg_null);\n    }\n    KMP_SYSFAIL(\"pthread_create\", status);\n  }\n\n  th->th.th_info.ds.ds_thread = handle;\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_destroy(&thread_attr);\n  if (status) {\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, KMP_MSG(CantDestroyThreadAttrs), err_code,\n              __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif /* KMP_THREAD_ATTR */\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_create_worker: done creating thread (%d)\\n\", gtid));\n\n} // __kmp_create_worker\n\n#if KMP_USE_MONITOR\nvoid __kmp_create_monitor(kmp_info_t *th) {\n  pthread_t handle;\n  pthread_attr_t thread_attr;\n  size_t size;\n  int status;\n  int auto_adj_size = FALSE;\n\n  if (__kmp_dflt_blocktime == KMP_MAX_BLOCKTIME) {\n    // We don't need monitor thread in case of MAX_BLOCKTIME\n    KA_TRACE(10, (\"__kmp_create_monitor: skipping monitor thread because of \"\n                  \"MAX blocktime\\n\"));\n    th->th.th_info.ds.ds_tid = 0; // this makes reap_monitor no-op\n    th->th.th_info.ds.ds_gtid = 0;\n    return;\n  }\n  KA_TRACE(10, (\"__kmp_create_monitor: try to create monitor\\n\"));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  th->th.th_info.ds.ds_tid = KMP_GTID_MONITOR;\n  th->th.th_info.ds.ds_gtid = KMP_GTID_MONITOR;\n#if KMP_REAL_TIME_FIX\n  TCW_4(__kmp_global.g.g_time.dt.t_value,\n        -1); // Will use it for synchronization a bit later.\n#else\n  TCW_4(__kmp_global.g.g_time.dt.t_value, 0);\n#endif // KMP_REAL_TIME_FIX\n\n#ifdef KMP_THREAD_ATTR\n  if (__kmp_monitor_stksize == 0) {\n    __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n    auto_adj_size = TRUE;\n  }\n  status = pthread_attr_init(&thread_attr);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantInitThreadAttrs), KMP_ERR(status), __kmp_msg_null);\n  }\n  status = pthread_attr_setdetachstate(&thread_attr, PTHREAD_CREATE_JOINABLE);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetMonitorState), KMP_ERR(status), __kmp_msg_null);\n  }\n\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  status = pthread_attr_getstacksize(&thread_attr, &size);\n  KMP_CHECK_SYSFAIL(\"pthread_attr_getstacksize\", status);\n#else\n  size = __kmp_sys_min_stksize;\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n#endif /* KMP_THREAD_ATTR */\n\n  if (__kmp_monitor_stksize == 0) {\n    __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n  }\n  if (__kmp_monitor_stksize < __kmp_sys_min_stksize) {\n    __kmp_monitor_stksize = __kmp_sys_min_stksize;\n  }\n\n  KA_TRACE(10, (\"__kmp_create_monitor: default stacksize = %lu bytes,\"\n                \"requested stacksize = %lu bytes\\n\",\n                size, __kmp_monitor_stksize));\n\nretry:\n\n/* Set stack size for this thread now. */\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  KA_TRACE(10, (\"__kmp_create_monitor: setting stacksize = %lu bytes,\",\n                __kmp_monitor_stksize));\n  status = pthread_attr_setstacksize(&thread_attr, __kmp_monitor_stksize);\n  if (status != 0) {\n    if (auto_adj_size) {\n      __kmp_monitor_stksize *= 2;\n      goto retry;\n    }\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, // should this be fatal?  BB\n              KMP_MSG(CantSetMonitorStackSize, (long int)__kmp_monitor_stksize),\n              err_code, KMP_HNT(ChangeMonitorStackSize), __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n  status =\n      pthread_create(&handle, &thread_attr, __kmp_launch_monitor, (void *)th);\n\n  if (status != 0) {\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n    if (status == EINVAL) {\n      if (auto_adj_size && (__kmp_monitor_stksize < (size_t)0x40000000)) {\n        __kmp_monitor_stksize *= 2;\n        goto retry;\n      }\n      __kmp_fatal(KMP_MSG(CantSetMonitorStackSize, __kmp_monitor_stksize),\n                  KMP_ERR(status), KMP_HNT(IncreaseMonitorStackSize),\n                  __kmp_msg_null);\n    }\n    if (status == ENOMEM) {\n      __kmp_fatal(KMP_MSG(CantSetMonitorStackSize, __kmp_monitor_stksize),\n                  KMP_ERR(status), KMP_HNT(DecreaseMonitorStackSize),\n                  __kmp_msg_null);\n    }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    if (status == EAGAIN) {\n      __kmp_fatal(KMP_MSG(NoResourcesForMonitorThread), KMP_ERR(status),\n                  KMP_HNT(DecreaseNumberOfThreadsInUse), __kmp_msg_null);\n    }\n    KMP_SYSFAIL(\"pthread_create\", status);\n  }\n\n  th->th.th_info.ds.ds_thread = handle;\n\n#if KMP_REAL_TIME_FIX\n  // Wait for the monitor thread is really started and set its *priority*.\n  KMP_DEBUG_ASSERT(sizeof(kmp_uint32) ==\n                   sizeof(__kmp_global.g.g_time.dt.t_value));\n  __kmp_wait_4((kmp_uint32 volatile *)&__kmp_global.g.g_time.dt.t_value, -1,\n               &__kmp_neq_4, NULL);\n#endif // KMP_REAL_TIME_FIX\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_destroy(&thread_attr);\n  if (status != 0) {\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, KMP_MSG(CantDestroyThreadAttrs), err_code,\n              __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_create_monitor: monitor created %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n} // __kmp_create_monitor\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_exit_thread(int exit_status) {\n  pthread_exit((void *)(intptr_t)exit_status);\n} // __kmp_exit_thread\n\n#if KMP_USE_MONITOR\nvoid __kmp_resume_monitor();\n\nvoid __kmp_reap_monitor(kmp_info_t *th) {\n  int status;\n  void *exit_val;\n\n  KA_TRACE(10, (\"__kmp_reap_monitor: try to reap monitor thread with handle\"\n                \" %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n  // If monitor has been created, its tid and gtid should be KMP_GTID_MONITOR.\n  // If both tid and gtid are 0, it means the monitor did not ever start.\n  // If both tid and gtid are KMP_GTID_DNE, the monitor has been shut down.\n  KMP_DEBUG_ASSERT(th->th.th_info.ds.ds_tid == th->th.th_info.ds.ds_gtid);\n  if (th->th.th_info.ds.ds_gtid != KMP_GTID_MONITOR) {\n    KA_TRACE(10, (\"__kmp_reap_monitor: monitor did not start, returning\\n\"));\n    return;\n  }\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  /* First, check to see whether the monitor thread exists to wake it up. This\n     is to avoid performance problem when the monitor sleeps during\n     blocktime-size interval */\n\n  status = pthread_kill(th->th.th_info.ds.ds_thread, 0);\n  if (status != ESRCH) {\n    __kmp_resume_monitor(); // Wake up the monitor thread\n  }\n  KA_TRACE(10, (\"__kmp_reap_monitor: try to join with monitor\\n\"));\n  status = pthread_join(th->th.th_info.ds.ds_thread, &exit_val);\n  if (exit_val != th) {\n    __kmp_fatal(KMP_MSG(ReapMonitorError), KMP_ERR(status), __kmp_msg_null);\n  }\n\n  th->th.th_info.ds.ds_tid = KMP_GTID_DNE;\n  th->th.th_info.ds.ds_gtid = KMP_GTID_DNE;\n\n  KA_TRACE(10, (\"__kmp_reap_monitor: done reaping monitor thread with handle\"\n                \" %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_reap_worker(kmp_info_t *th) {\n  int status;\n  void *exit_val;\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(\n      10, (\"__kmp_reap_worker: try to reap T#%d\\n\", th->th.th_info.ds.ds_gtid));\n\n  status = pthread_join(th->th.th_info.ds.ds_thread, &exit_val);\n#ifdef KMP_DEBUG\n  /* Don't expose these to the user until we understand when they trigger */\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(ReapWorkerError), KMP_ERR(status), __kmp_msg_null);\n  }\n  if (exit_val != th) {\n    KA_TRACE(10, (\"__kmp_reap_worker: worker T#%d did not reap properly, \"\n                  \"exit_val = %p\\n\",\n                  th->th.th_info.ds.ds_gtid, exit_val));\n  }\n#endif /* KMP_DEBUG */\n\n  KA_TRACE(10, (\"__kmp_reap_worker: done reaping T#%d\\n\",\n                th->th.th_info.ds.ds_gtid));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n}\n\n#if KMP_HANDLE_SIGNALS\n\nstatic void __kmp_null_handler(int signo) {\n  //  Do nothing, for doing SIG_IGN-type actions.\n} // __kmp_null_handler\n\nstatic void __kmp_team_handler(int signo) {\n  if (__kmp_global.g.g_abort == 0) {\n/* Stage 1 signal handler, let's shut down all of the threads */\n#ifdef KMP_DEBUG\n    __kmp_debug_printf(\"__kmp_team_handler: caught signal = %d\\n\", signo);\n#endif\n    switch (signo) {\n    case SIGHUP:\n    case SIGINT:\n    case SIGQUIT:\n    case SIGILL:\n    case SIGABRT:\n    case SIGFPE:\n    case SIGBUS:\n    case SIGSEGV:\n#ifdef SIGSYS\n    case SIGSYS:\n#endif\n    case SIGTERM:\n      if (__kmp_debug_buf) {\n        __kmp_dump_debug_buffer();\n      }\n      KMP_MB(); // Flush all pending memory write invalidates.\n      TCW_4(__kmp_global.g.g_abort, signo);\n      KMP_MB(); // Flush all pending memory write invalidates.\n      TCW_4(__kmp_global.g.g_done, TRUE);\n      KMP_MB(); // Flush all pending memory write invalidates.\n      break;\n    default:\n#ifdef KMP_DEBUG\n      __kmp_debug_printf(\"__kmp_team_handler: unknown signal type\");\n#endif\n      break;\n    }\n  }\n} // __kmp_team_handler\n\nstatic void __kmp_sigaction(int signum, const struct sigaction *act,\n                            struct sigaction *oldact) {\n  int rc = sigaction(signum, act, oldact);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigaction\", rc);\n}\n\nstatic void __kmp_install_one_handler(int sig, sig_func_t handler_func,\n                                      int parallel_init) {\n  KMP_MB(); // Flush all pending memory write invalidates.\n  KB_TRACE(60,\n           (\"__kmp_install_one_handler( %d, ..., %d )\\n\", sig, parallel_init));\n  if (parallel_init) {\n    struct sigaction new_action;\n    struct sigaction old_action;\n    new_action.sa_handler = handler_func;\n    new_action.sa_flags = 0;\n    sigfillset(&new_action.sa_mask);\n    __kmp_sigaction(sig, &new_action, &old_action);\n    if (old_action.sa_handler == __kmp_sighldrs[sig].sa_handler) {\n      sigaddset(&__kmp_sigset, sig);\n    } else {\n      // Restore/keep user's handler if one previously installed.\n      __kmp_sigaction(sig, &old_action, NULL);\n    }\n  } else {\n    // Save initial/system signal handlers to see if user handlers installed.\n    __kmp_sigaction(sig, NULL, &__kmp_sighldrs[sig]);\n  }\n  KMP_MB(); // Flush all pending memory write invalidates.\n} // __kmp_install_one_handler\n\nstatic void __kmp_remove_one_handler(int sig) {\n  KB_TRACE(60, (\"__kmp_remove_one_handler( %d )\\n\", sig));\n  if (sigismember(&__kmp_sigset, sig)) {\n    struct sigaction old;\n    KMP_MB(); // Flush all pending memory write invalidates.\n    __kmp_sigaction(sig, &__kmp_sighldrs[sig], &old);\n    if ((old.sa_handler != __kmp_team_handler) &&\n        (old.sa_handler != __kmp_null_handler)) {\n      // Restore the users signal handler.\n      KB_TRACE(10, (\"__kmp_remove_one_handler: oops, not our handler, \"\n                    \"restoring: sig=%d\\n\",\n                    sig));\n      __kmp_sigaction(sig, &old, NULL);\n    }\n    sigdelset(&__kmp_sigset, sig);\n    KMP_MB(); // Flush all pending memory write invalidates.\n  }\n} // __kmp_remove_one_handler\n\nvoid __kmp_install_signals(int parallel_init) {\n  KB_TRACE(10, (\"__kmp_install_signals( %d )\\n\", parallel_init));\n  if (__kmp_handle_signals || !parallel_init) {\n    // If ! parallel_init, we do not install handlers, just save original\n    // handlers. Let us do it even __handle_signals is 0.\n    sigemptyset(&__kmp_sigset);\n    __kmp_install_one_handler(SIGHUP, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGINT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGQUIT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGILL, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGABRT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGFPE, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGBUS, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGSEGV, __kmp_team_handler, parallel_init);\n#ifdef SIGSYS\n    __kmp_install_one_handler(SIGSYS, __kmp_team_handler, parallel_init);\n#endif // SIGSYS\n    __kmp_install_one_handler(SIGTERM, __kmp_team_handler, parallel_init);\n#ifdef SIGPIPE\n    __kmp_install_one_handler(SIGPIPE, __kmp_team_handler, parallel_init);\n#endif // SIGPIPE\n  }\n} // __kmp_install_signals\n\nvoid __kmp_remove_signals(void) {\n  int sig;\n  KB_TRACE(10, (\"__kmp_remove_signals()\\n\"));\n  for (sig = 1; sig < NSIG; ++sig) {\n    __kmp_remove_one_handler(sig);\n  }\n} // __kmp_remove_signals\n\n#endif // KMP_HANDLE_SIGNALS\n\nvoid __kmp_enable(int new_state) {\n#ifdef KMP_CANCEL_THREADS\n  int status, old_state;\n  status = pthread_setcancelstate(new_state, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n  KMP_DEBUG_ASSERT(old_state == PTHREAD_CANCEL_DISABLE);\n#endif\n}\n\nvoid __kmp_disable(int *old_state) {\n#ifdef KMP_CANCEL_THREADS\n  int status;\n  status = pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n}\n\nstatic void __kmp_atfork_prepare(void) {\n  __kmp_acquire_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_acquire_bootstrap_lock(&__kmp_forkjoin_lock);\n}\n\nstatic void __kmp_atfork_parent(void) {\n  __kmp_release_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_release_bootstrap_lock(&__kmp_forkjoin_lock);\n}\n\n/* Reset the library so execution in the child starts \"all over again\" with\n   clean data structures in initial states.  Don't worry about freeing memory\n   allocated by parent, just abandon it to be safe. */\nstatic void __kmp_atfork_child(void) {\n  __kmp_release_bootstrap_lock(&__kmp_forkjoin_lock);\n  /* TODO make sure this is done right for nested/sibling */\n  // ATT:  Memory leaks are here? TODO: Check it and fix.\n  /* KMP_ASSERT( 0 ); */\n\n  ++__kmp_fork_count;\n\n#if KMP_AFFINITY_SUPPORTED\n#if KMP_OS_LINUX\n  // reset the affinity in the child to the initial thread\n  // affinity in the parent\n  kmp_set_thread_affinity_mask_initial();\n#endif\n  // Set default not to bind threads tightly in the child (we\u2019re expecting\n  // over-subscription after the fork and this can improve things for\n  // scripting languages that use OpenMP inside process-parallel code).\n  __kmp_affinity_type = affinity_none;\n  if (__kmp_nested_proc_bind.bind_types != NULL) {\n    __kmp_nested_proc_bind.bind_types[0] = proc_bind_false;\n  }\n#endif // KMP_AFFINITY_SUPPORTED\n\n  __kmp_init_runtime = FALSE;\n#if KMP_USE_MONITOR\n  __kmp_init_monitor = 0;\n#endif\n  __kmp_init_parallel = FALSE;\n  __kmp_init_middle = FALSE;\n  __kmp_init_serial = FALSE;\n  TCW_4(__kmp_init_gtid, FALSE);\n  __kmp_init_common = FALSE;\n\n  TCW_4(__kmp_init_user_locks, FALSE);\n#if !KMP_USE_DYNAMIC_LOCK\n  __kmp_user_lock_table.used = 1;\n  __kmp_user_lock_table.allocated = 0;\n  __kmp_user_lock_table.table = NULL;\n  __kmp_lock_blocks = NULL;\n#endif\n\n  __kmp_all_nth = 0;\n  TCW_4(__kmp_nth, 0);\n\n  __kmp_thread_pool = NULL;\n  __kmp_thread_pool_insert_pt = NULL;\n  __kmp_team_pool = NULL;\n\n  /* Must actually zero all the *cache arguments passed to __kmpc_threadprivate\n     here so threadprivate doesn't use stale data */\n  KA_TRACE(10, (\"__kmp_atfork_child: checking cache address list %p\\n\",\n                __kmp_threadpriv_cache_list));\n\n  while (__kmp_threadpriv_cache_list != NULL) {\n\n    if (*__kmp_threadpriv_cache_list->addr != NULL) {\n      KC_TRACE(50, (\"__kmp_atfork_child: zeroing cache at address %p\\n\",\n                    &(*__kmp_threadpriv_cache_list->addr)));\n\n      *__kmp_threadpriv_cache_list->addr = NULL;\n    }\n    __kmp_threadpriv_cache_list = __kmp_threadpriv_cache_list->next;\n  }\n\n  __kmp_init_runtime = FALSE;\n\n  /* reset statically initialized locks */\n  __kmp_init_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_init_bootstrap_lock(&__kmp_stdio_lock);\n  __kmp_init_bootstrap_lock(&__kmp_console_lock);\n  __kmp_init_bootstrap_lock(&__kmp_task_team_lock);\n\n#if USE_ITT_BUILD\n  __kmp_itt_reset(); // reset ITT's global state\n#endif /* USE_ITT_BUILD */\n\n  /* This is necessary to make sure no stale data is left around */\n  /* AC: customers complain that we use unsafe routines in the atfork\n     handler. Mathworks: dlsym() is unsafe. We call dlsym and dlopen\n     in dynamic_link when check the presence of shared tbbmalloc library.\n     Suggestion is to make the library initialization lazier, similar\n     to what done for __kmpc_begin(). */\n  // TODO: synchronize all static initializations with regular library\n  //       startup; look at kmp_global.cpp and etc.\n  //__kmp_internal_begin ();\n}\n\nvoid __kmp_register_atfork(void) {\n  if (__kmp_need_register_atfork) {\n    int status = pthread_atfork(__kmp_atfork_prepare, __kmp_atfork_parent,\n                                __kmp_atfork_child);\n    KMP_CHECK_SYSFAIL(\"pthread_atfork\", status);\n    __kmp_need_register_atfork = FALSE;\n  }\n}\n\nvoid __kmp_suspend_initialize(void) {\n  int status;\n  status = pthread_mutexattr_init(&__kmp_suspend_mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutexattr_init\", status);\n  status = pthread_condattr_init(&__kmp_suspend_cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_condattr_init\", status);\n}\n\nvoid __kmp_suspend_initialize_thread(kmp_info_t *th) {\n  ANNOTATE_HAPPENS_AFTER(&th->th.th_suspend_init_count);\n  int old_value = KMP_ATOMIC_LD_RLX(&th->th.th_suspend_init_count);\n  int new_value = __kmp_fork_count + 1;\n  // Return if already initialized\n  if (old_value == new_value)\n    return;\n  // Wait, then return if being initialized\n  if (old_value == -1 ||\n      !__kmp_atomic_compare_store(&th->th.th_suspend_init_count, old_value,\n                                  -1)) {\n    while (KMP_ATOMIC_LD_ACQ(&th->th.th_suspend_init_count) != new_value) {\n      KMP_CPU_PAUSE();\n    }\n  } else {\n    // Claim to be the initializer and do initializations\n    int status;\n    status = pthread_cond_init(&th->th.th_suspend_cv.c_cond,\n                               &__kmp_suspend_cond_attr);\n    KMP_CHECK_SYSFAIL(\"pthread_cond_init\", status);\n    status = pthread_mutex_init(&th->th.th_suspend_mx.m_mutex,\n                                &__kmp_suspend_mutex_attr);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_init\", status);\n    KMP_ATOMIC_ST_REL(&th->th.th_suspend_init_count, new_value);\n    ANNOTATE_HAPPENS_BEFORE(&th->th.th_suspend_init_count);\n  }\n}\n\nvoid __kmp_suspend_uninitialize_thread(kmp_info_t *th) {\n  if (KMP_ATOMIC_LD_ACQ(&th->th.th_suspend_init_count) > __kmp_fork_count) {\n    /* this means we have initialize the suspension pthread objects for this\n       thread in this instance of the process */\n    int status;\n\n    status = pthread_cond_destroy(&th->th.th_suspend_cv.c_cond);\n    if (status != 0 && status != EBUSY) {\n      KMP_SYSFAIL(\"pthread_cond_destroy\", status);\n    }\n    status = pthread_mutex_destroy(&th->th.th_suspend_mx.m_mutex);\n    if (status != 0 && status != EBUSY) {\n      KMP_SYSFAIL(\"pthread_mutex_destroy\", status);\n    }\n    --th->th.th_suspend_init_count;\n    KMP_DEBUG_ASSERT(KMP_ATOMIC_LD_RLX(&th->th.th_suspend_init_count) ==\n                     __kmp_fork_count);\n  }\n}\n\n// return true if lock obtained, false otherwise\nint __kmp_try_suspend_mx(kmp_info_t *th) {\n  return (pthread_mutex_trylock(&th->th.th_suspend_mx.m_mutex) == 0);\n}\n\nvoid __kmp_lock_suspend_mx(kmp_info_t *th) {\n  int status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n}\n\nvoid __kmp_unlock_suspend_mx(kmp_info_t *th) {\n  int status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n}\n\n/* This routine puts the calling thread to sleep after setting the\n   sleep bit for the indicated flag variable to true. */\ntemplate <class C>\nstatic inline void __kmp_suspend_template(int th_gtid, C *flag) {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_suspend);\n  kmp_info_t *th = __kmp_threads[th_gtid];\n  int status;\n  typename C::flag_t old_spin;\n\n  KF_TRACE(30, (\"__kmp_suspend_template: T#%d enter for flag = %p\\n\", th_gtid,\n                flag->get()));\n\n  __kmp_suspend_initialize_thread(th);\n\n  status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n\n  KF_TRACE(10, (\"__kmp_suspend_template: T#%d setting sleep bit for spin(%p)\\n\",\n                th_gtid, flag->get()));\n\n  /* TODO: shouldn't this use release semantics to ensure that\n     __kmp_suspend_initialize_thread gets called first? */\n  old_spin = flag->set_sleeping();\n  if (__kmp_dflt_blocktime == KMP_MAX_BLOCKTIME &&\n      __kmp_pause_status != kmp_soft_paused) {\n    flag->unset_sleeping();\n    status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n    return;\n  }\n  KF_TRACE(5, (\"__kmp_suspend_template: T#%d set sleep bit for spin(%p)==%x,\"\n               \" was %x\\n\",\n               th_gtid, flag->get(), flag->load(), old_spin));\n\n  if (flag->done_check_val(old_spin)) {\n    old_spin = flag->unset_sleeping();\n    KF_TRACE(5, (\"__kmp_suspend_template: T#%d false alarm, reset sleep bit \"\n                 \"for spin(%p)\\n\",\n                 th_gtid, flag->get()));\n  } else {\n    /* Encapsulate in a loop as the documentation states that this may\n       \"with low probability\" return when the condition variable has\n       not been signaled or broadcast */\n    int deactivated = FALSE;\n    TCW_PTR(th->th.th_sleep_loc, (void *)flag);\n\n    while (flag->is_sleeping()) {\n#ifdef DEBUG_SUSPEND\n      char buffer[128];\n      __kmp_suspend_count++;\n      __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n      __kmp_printf(\"__kmp_suspend_template: suspending T#%d: %s\\n\", th_gtid,\n                   buffer);\n#endif\n      // Mark the thread as no longer active (only in the first iteration of the\n      // loop).\n      if (!deactivated) {\n        th->th.th_active = FALSE;\n        if (th->th.th_active_in_pool) {\n          th->th.th_active_in_pool = FALSE;\n          KMP_ATOMIC_DEC(&__kmp_thread_pool_active_nth);\n          KMP_DEBUG_ASSERT(TCR_4(__kmp_thread_pool_active_nth) >= 0);\n        }\n        deactivated = TRUE;\n      }\n\n#if USE_SUSPEND_TIMEOUT\n      struct timespec now;\n      struct timeval tval;\n      int msecs;\n\n      status = gettimeofday(&tval, NULL);\n      KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n      TIMEVAL_TO_TIMESPEC(&tval, &now);\n\n      msecs = (4 * __kmp_dflt_blocktime) + 200;\n      now.tv_sec += msecs / 1000;\n      now.tv_nsec += (msecs % 1000) * 1000;\n\n      KF_TRACE(15, (\"__kmp_suspend_template: T#%d about to perform \"\n                    \"pthread_cond_timedwait\\n\",\n                    th_gtid));\n      status = pthread_cond_timedwait(&th->th.th_suspend_cv.c_cond,\n                                      &th->th.th_suspend_mx.m_mutex, &now);\n#else\n      KF_TRACE(15, (\"__kmp_suspend_template: T#%d about to perform\"\n                    \" pthread_cond_wait\\n\",\n                    th_gtid));\n      status = pthread_cond_wait(&th->th.th_suspend_cv.c_cond,\n                                 &th->th.th_suspend_mx.m_mutex);\n#endif\n\n      if ((status != 0) && (status != EINTR) && (status != ETIMEDOUT)) {\n        KMP_SYSFAIL(\"pthread_cond_wait\", status);\n      }\n#ifdef KMP_DEBUG\n      if (status == ETIMEDOUT) {\n        if (flag->is_sleeping()) {\n          KF_TRACE(100,\n                   (\"__kmp_suspend_template: T#%d timeout wakeup\\n\", th_gtid));\n        } else {\n          KF_TRACE(2, (\"__kmp_suspend_template: T#%d timeout wakeup, sleep bit \"\n                       \"not set!\\n\",\n                       th_gtid));\n        }\n      } else if (flag->is_sleeping()) {\n        KF_TRACE(100,\n                 (\"__kmp_suspend_template: T#%d spurious wakeup\\n\", th_gtid));\n      }\n#endif\n    } // while\n\n    // Mark the thread as active again (if it was previous marked as inactive)\n    if (deactivated) {\n      th->th.th_active = TRUE;\n      if (TCR_4(th->th.th_in_pool)) {\n        KMP_ATOMIC_INC(&__kmp_thread_pool_active_nth);\n        th->th.th_active_in_pool = TRUE;\n      }\n    }\n  }\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n    __kmp_printf(\"__kmp_suspend_template: T#%d has awakened: %s\\n\", th_gtid,\n                 buffer);\n  }\n#endif\n\n  status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_suspend_template: T#%d exit\\n\", th_gtid));\n}\n\nvoid __kmp_suspend_32(int th_gtid, kmp_flag_32 *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_64(int th_gtid, kmp_flag_64 *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_oncore(int th_gtid, kmp_flag_oncore *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\n\n/* This routine signals the thread specified by target_gtid to wake up\n   after setting the sleep bit indicated by the flag argument to FALSE.\n   The target thread must already have called __kmp_suspend_template() */\ntemplate <class C>\nstatic inline void __kmp_resume_template(int target_gtid, C *flag) {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_resume);\n  kmp_info_t *th = __kmp_threads[target_gtid];\n  int status;\n\n#ifdef KMP_DEBUG\n  int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n#endif\n\n  KF_TRACE(30, (\"__kmp_resume_template: T#%d wants to wakeup T#%d enter\\n\",\n                gtid, target_gtid));\n  KMP_DEBUG_ASSERT(gtid != target_gtid);\n\n  __kmp_suspend_initialize_thread(th);\n\n  status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n\n  if (!flag) { // coming from __kmp_null_resume_wrapper\n    flag = (C *)CCAST(void *, th->th.th_sleep_loc);\n  }\n\n  // First, check if the flag is null or its type has changed. If so, someone\n  // else woke it up.\n  if (!flag || flag->get_type() != flag->get_ptr_type()) { // get_ptr_type\n    // simply shows what\n    // flag was cast to\n    KF_TRACE(5, (\"__kmp_resume_template: T#%d exiting, thread T#%d already \"\n                 \"awake: flag(%p)\\n\",\n                 gtid, target_gtid, NULL));\n    status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n    return;\n  } else { // if multiple threads are sleeping, flag should be internally\n    // referring to a specific thread here\n    typename C::flag_t old_spin = flag->unset_sleeping();\n    if (!flag->is_sleeping_val(old_spin)) {\n      KF_TRACE(5, (\"__kmp_resume_template: T#%d exiting, thread T#%d already \"\n                   \"awake: flag(%p): \"\n                   \"%u => %u\\n\",\n                   gtid, target_gtid, flag->get(), old_spin, flag->load()));\n      status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n      KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n      return;\n    }\n    KF_TRACE(5, (\"__kmp_resume_template: T#%d about to wakeup T#%d, reset \"\n                 \"sleep bit for flag's loc(%p): \"\n                 \"%u => %u\\n\",\n                 gtid, target_gtid, flag->get(), old_spin, flag->load()));\n  }\n  TCW_PTR(th->th.th_sleep_loc, NULL);\n\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n    __kmp_printf(\"__kmp_resume_template: T#%d resuming T#%d: %s\\n\", gtid,\n                 target_gtid, buffer);\n  }\n#endif\n  status = pthread_cond_signal(&th->th.th_suspend_cv.c_cond);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_signal\", status);\n  status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_resume_template: T#%d exiting after signaling wake up\"\n                \" for T#%d\\n\",\n                gtid, target_gtid));\n}\n\nvoid __kmp_resume_32(int target_gtid, kmp_flag_32 *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_64(int target_gtid, kmp_flag_64 *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_oncore(int target_gtid, kmp_flag_oncore *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\n\n#if KMP_USE_MONITOR\nvoid __kmp_resume_monitor() {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_resume);\n  int status;\n#ifdef KMP_DEBUG\n  int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n  KF_TRACE(30, (\"__kmp_resume_monitor: T#%d wants to wakeup T#%d enter\\n\", gtid,\n                KMP_GTID_MONITOR));\n  KMP_DEBUG_ASSERT(gtid != KMP_GTID_MONITOR);\n#endif\n  status = pthread_mutex_lock(&__kmp_wait_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &__kmp_wait_cv.c_cond);\n    __kmp_printf(\"__kmp_resume_monitor: T#%d resuming T#%d: %s\\n\", gtid,\n                 KMP_GTID_MONITOR, buffer);\n  }\n#endif\n  status = pthread_cond_signal(&__kmp_wait_cv.c_cond);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_signal\", status);\n  status = pthread_mutex_unlock(&__kmp_wait_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_resume_monitor: T#%d exiting after signaling wake up\"\n                \" for T#%d\\n\",\n                gtid, KMP_GTID_MONITOR));\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_yield() { sched_yield(); }\n\nvoid __kmp_gtid_set_specific(int gtid) {\n  if (__kmp_init_gtid) {\n    int status;\n    status = pthread_setspecific(__kmp_gtid_threadprivate_key,\n                                 (void *)(intptr_t)(gtid + 1));\n    KMP_CHECK_SYSFAIL(\"pthread_setspecific\", status);\n  } else {\n    KA_TRACE(50, (\"__kmp_gtid_set_specific: runtime shutdown, returning\\n\"));\n  }\n}\n\nint __kmp_gtid_get_specific() {\n  int gtid;\n  if (!__kmp_init_gtid) {\n    KA_TRACE(50, (\"__kmp_gtid_get_specific: runtime shutdown, returning \"\n                  \"KMP_GTID_SHUTDOWN\\n\"));\n    return KMP_GTID_SHUTDOWN;\n  }\n  gtid = (int)(size_t)pthread_getspecific(__kmp_gtid_threadprivate_key);\n  if (gtid == 0) {\n    gtid = KMP_GTID_DNE;\n  } else {\n    gtid--;\n  }\n  KA_TRACE(50, (\"__kmp_gtid_get_specific: key:%d gtid:%d\\n\",\n                __kmp_gtid_threadprivate_key, gtid));\n  return gtid;\n}\n\ndouble __kmp_read_cpu_time(void) {\n  /*clock_t   t;*/\n  struct tms buffer;\n\n  /*t =*/times(&buffer);\n\n  return (buffer.tms_utime + buffer.tms_cutime) / (double)CLOCKS_PER_SEC;\n}\n\nint __kmp_read_system_info(struct kmp_sys_info *info) {\n  int status;\n  struct rusage r_usage;\n\n  memset(info, 0, sizeof(*info));\n\n  status = getrusage(RUSAGE_SELF, &r_usage);\n  KMP_CHECK_SYSFAIL_ERRNO(\"getrusage\", status);\n\n  // The maximum resident set size utilized (in kilobytes)\n  info->maxrss = r_usage.ru_maxrss;\n  // The number of page faults serviced without any I/O\n  info->minflt = r_usage.ru_minflt;\n  // The number of page faults serviced that required I/O\n  info->majflt = r_usage.ru_majflt;\n  // The number of times a process was \"swapped\" out of memory\n  info->nswap = r_usage.ru_nswap;\n  // The number of times the file system had to perform input\n  info->inblock = r_usage.ru_inblock;\n  // The number of times the file system had to perform output\n  info->oublock = r_usage.ru_oublock;\n  // The number of times a context switch was voluntarily\n  info->nvcsw = r_usage.ru_nvcsw;\n  // The number of times a context switch was forced\n  info->nivcsw = r_usage.ru_nivcsw;\n\n  return (status != 0);\n}\n\nvoid __kmp_read_system_time(double *delta) {\n  double t_ns;\n  struct timeval tval;\n  struct timespec stop;\n  int status;\n\n  status = gettimeofday(&tval, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  TIMEVAL_TO_TIMESPEC(&tval, &stop);\n  t_ns = TS2NS(stop) - TS2NS(__kmp_sys_timer_data.start);\n  *delta = (t_ns * 1e-9);\n}\n\nvoid __kmp_clear_system_time(void) {\n  struct timeval tval;\n  int status;\n  status = gettimeofday(&tval, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  TIMEVAL_TO_TIMESPEC(&tval, &__kmp_sys_timer_data.start);\n}\n\nstatic int __kmp_get_xproc(void) {\n\n  int r = 0;\n\n#if KMP_OS_LINUX || KMP_OS_DRAGONFLY || KMP_OS_FREEBSD || KMP_OS_NETBSD ||     \\\n        KMP_OS_OPENBSD || KMP_OS_HURD\n\n  r = sysconf(_SC_NPROCESSORS_ONLN);\n\n#elif KMP_OS_DARWIN\n\n  // Bug C77011 High \"OpenMP Threads and number of active cores\".\n\n  // Find the number of available CPUs.\n  kern_return_t rc;\n  host_basic_info_data_t info;\n  mach_msg_type_number_t num = HOST_BASIC_INFO_COUNT;\n  rc = host_info(mach_host_self(), HOST_BASIC_INFO, (host_info_t)&info, &num);\n  if (rc == 0 && num == HOST_BASIC_INFO_COUNT) {\n    // Cannot use KA_TRACE() here because this code works before trace support\n    // is initialized.\n    r = info.avail_cpus;\n  } else {\n    KMP_WARNING(CantGetNumAvailCPU);\n    KMP_INFORM(AssumedNumCPU);\n  }\n\n#else\n\n#error \"Unknown or unsupported OS.\"\n\n#endif\n\n  return r > 0 ? r : 2; /* guess value of 2 if OS told us 0 */\n\n} // __kmp_get_xproc\n\nint __kmp_read_from_file(char const *path, char const *format, ...) {\n  int result;\n  va_list args;\n\n  va_start(args, format);\n  FILE *f = fopen(path, \"rb\");\n  if (f == NULL)\n    return 0;\n  result = vfscanf(f, format, args);\n  fclose(f);\n\n  return result;\n}\n\nvoid __kmp_runtime_initialize(void) {\n  int status;\n  pthread_mutexattr_t mutex_attr;\n  pthread_condattr_t cond_attr;\n\n  if (__kmp_init_runtime) {\n    return;\n  }\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64)\n  if (!__kmp_cpuinfo.initialized) {\n    __kmp_query_cpuid(&__kmp_cpuinfo);\n  }\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n  __kmp_xproc = __kmp_get_xproc();\n\n#if ! KMP_32_BIT_ARCH\n  struct rlimit rlim;\n  // read stack size of calling thread, save it as default for worker threads;\n  // this should be done before reading environment variables\n  status = getrlimit(RLIMIT_STACK, &rlim);\n  if (status == 0) { // success?\n    __kmp_stksize = rlim.rlim_cur;\n    __kmp_check_stksize(&__kmp_stksize); // check value and adjust if needed\n  }\n#endif /* KMP_32_BIT_ARCH */\n\n  if (sysconf(_SC_THREADS)) {\n\n    /* Query the maximum number of threads */\n    __kmp_sys_max_nth = sysconf(_SC_THREAD_THREADS_MAX);\n    if (__kmp_sys_max_nth == -1) {\n      /* Unlimited threads for NPTL */\n      __kmp_sys_max_nth = INT_MAX;\n    } else if (__kmp_sys_max_nth <= 1) {\n      /* Can't tell, just use PTHREAD_THREADS_MAX */\n      __kmp_sys_max_nth = KMP_MAX_NTH;\n    }\n\n    /* Query the minimum stack size */\n    __kmp_sys_min_stksize = sysconf(_SC_THREAD_STACK_MIN);\n    if (__kmp_sys_min_stksize <= 1) {\n      __kmp_sys_min_stksize = KMP_MIN_STKSIZE;\n    }\n  }\n\n  /* Set up minimum number of threads to switch to TLS gtid */\n  __kmp_tls_gtid_min = KMP_TLS_GTID_MIN;\n\n  status = pthread_key_create(&__kmp_gtid_threadprivate_key,\n                              __kmp_internal_end_dest);\n  KMP_CHECK_SYSFAIL(\"pthread_key_create\", status);\n  status = pthread_mutexattr_init(&mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutexattr_init\", status);\n  status = pthread_mutex_init(&__kmp_wait_mx.m_mutex, &mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_init\", status);\n  status = pthread_condattr_init(&cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_condattr_init\", status);\n  status = pthread_cond_init(&__kmp_wait_cv.c_cond, &cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_init\", status);\n#if USE_ITT_BUILD\n  __kmp_itt_initialize();\n#endif /* USE_ITT_BUILD */\n\n  __kmp_init_runtime = TRUE;\n}\n\nvoid __kmp_runtime_destroy(void) {\n  int status;\n\n  if (!__kmp_init_runtime) {\n    return; // Nothing to do.\n  }\n\n#if USE_ITT_BUILD\n  __kmp_itt_destroy();\n#endif /* USE_ITT_BUILD */\n\n  status = pthread_key_delete(__kmp_gtid_threadprivate_key);\n  KMP_CHECK_SYSFAIL(\"pthread_key_delete\", status);\n\n  status = pthread_mutex_destroy(&__kmp_wait_mx.m_mutex);\n  if (status != 0 && status != EBUSY) {\n    KMP_SYSFAIL(\"pthread_mutex_destroy\", status);\n  }\n  status = pthread_cond_destroy(&__kmp_wait_cv.c_cond);\n  if (status != 0 && status != EBUSY) {\n    KMP_SYSFAIL(\"pthread_cond_destroy\", status);\n  }\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_uninitialize();\n#endif\n\n  __kmp_init_runtime = FALSE;\n}\n\n/* Put the thread to sleep for a time period */\n/* NOTE: not currently used anywhere */\nvoid __kmp_thread_sleep(int millis) { sleep((millis + 500) / 1000); }\n\n/* Calculate the elapsed wall clock time for the user */\nvoid __kmp_elapsed(double *t) {\n  int status;\n#ifdef FIX_SGI_CLOCK\n  struct timespec ts;\n\n  status = clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ts);\n  KMP_CHECK_SYSFAIL_ERRNO(\"clock_gettime\", status);\n  *t =\n      (double)ts.tv_nsec * (1.0 / (double)KMP_NSEC_PER_SEC) + (double)ts.tv_sec;\n#else\n  struct timeval tv;\n\n  status = gettimeofday(&tv, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  *t =\n      (double)tv.tv_usec * (1.0 / (double)KMP_USEC_PER_SEC) + (double)tv.tv_sec;\n#endif\n}\n\n/* Calculate the elapsed wall clock tick for the user */\nvoid __kmp_elapsed_tick(double *t) { *t = 1 / (double)CLOCKS_PER_SEC; }\n\n/* Return the current time stamp in nsec */\nkmp_uint64 __kmp_now_nsec() {\n  struct timeval t;\n  gettimeofday(&t, NULL);\n  kmp_uint64 nsec = (kmp_uint64)KMP_NSEC_PER_SEC * (kmp_uint64)t.tv_sec +\n                    (kmp_uint64)1000 * (kmp_uint64)t.tv_usec;\n  return nsec;\n}\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n/* Measure clock ticks per millisecond */\nvoid __kmp_initialize_system_tick() {\n  kmp_uint64 now, nsec2, diff;\n  kmp_uint64 delay = 100000; // 50~100 usec on most machines.\n  kmp_uint64 nsec = __kmp_now_nsec();\n  kmp_uint64 goal = __kmp_hardware_timestamp() + delay;\n  while ((now = __kmp_hardware_timestamp()) < goal)\n    ;\n  nsec2 = __kmp_now_nsec();\n  diff = nsec2 - nsec;\n  if (diff > 0) {\n    kmp_uint64 tpms = (kmp_uint64)(1e6 * (delay + (now - goal)) / diff);\n    if (tpms > 0)\n      __kmp_ticks_per_msec = tpms;\n  }\n}\n#endif\n\n/* Determine whether the given address is mapped into the current address\n   space. */\n\nint __kmp_is_address_mapped(void *addr) {\n\n  int found = 0;\n  int rc;\n\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_HURD\n\n  /* On GNUish OSes, read the /proc/<pid>/maps pseudo-file to get all the address\n     ranges mapped into the address space. */\n\n  char *name = __kmp_str_format(\"/proc/%d/maps\", getpid());\n  FILE *file = NULL;\n\n  file = fopen(name, \"r\");\n  KMP_ASSERT(file != NULL);\n\n  for (;;) {\n\n    void *beginning = NULL;\n    void *ending = NULL;\n    char perms[5];\n\n    rc = fscanf(file, \"%p-%p %4s %*[^\\n]\\n\", &beginning, &ending, perms);\n    if (rc == EOF) {\n      break;\n    }\n    KMP_ASSERT(rc == 3 &&\n               KMP_STRLEN(perms) == 4); // Make sure all fields are read.\n\n    // Ending address is not included in the region, but beginning is.\n    if ((addr >= beginning) && (addr < ending)) {\n      perms[2] = 0; // 3th and 4th character does not matter.\n      if (strcmp(perms, \"rw\") == 0) {\n        // Memory we are looking for should be readable and writable.\n        found = 1;\n      }\n      break;\n    }\n  }\n\n  // Free resources.\n  fclose(file);\n  KMP_INTERNAL_FREE(name);\n\n#elif KMP_OS_DARWIN\n\n  /* On OS X*, /proc pseudo filesystem is not available. Try to read memory\n     using vm interface. */\n\n  int buffer;\n  vm_size_t count;\n  rc = vm_read_overwrite(\n      mach_task_self(), // Task to read memory of.\n      (vm_address_t)(addr), // Address to read from.\n      1, // Number of bytes to be read.\n      (vm_address_t)(&buffer), // Address of buffer to save read bytes in.\n      &count // Address of var to save number of read bytes in.\n      );\n  if (rc == 0) {\n    // Memory successfully read.\n    found = 1;\n  }\n\n#elif KMP_OS_NETBSD\n\n  int mib[5];\n  mib[0] = CTL_VM;\n  mib[1] = VM_PROC;\n  mib[2] = VM_PROC_MAP;\n  mib[3] = getpid();\n  mib[4] = sizeof(struct kinfo_vmentry);\n\n  size_t size;\n  rc = sysctl(mib, __arraycount(mib), NULL, &size, NULL, 0);\n  KMP_ASSERT(!rc);\n  KMP_ASSERT(size);\n\n  size = size * 4 / 3;\n  struct kinfo_vmentry *kiv = (struct kinfo_vmentry *)KMP_INTERNAL_MALLOC(size);\n  KMP_ASSERT(kiv);\n\n  rc = sysctl(mib, __arraycount(mib), kiv, &size, NULL, 0);\n  KMP_ASSERT(!rc);\n  KMP_ASSERT(size);\n\n  for (size_t i = 0; i < size; i++) {\n    if (kiv[i].kve_start >= (uint64_t)addr &&\n        kiv[i].kve_end <= (uint64_t)addr) {\n      found = 1;\n      break;\n    }\n  }\n  KMP_INTERNAL_FREE(kiv);\n#elif KMP_OS_DRAGONFLY || KMP_OS_OPENBSD\n\n  // FIXME(DragonFly, OpenBSD): Implement this\n  found = 1;\n\n#else\n\n#error \"Unknown or unsupported OS\"\n\n#endif\n\n  return found;\n\n} // __kmp_is_address_mapped\n\n#ifdef USE_LOAD_BALANCE\n\n#if KMP_OS_DARWIN || KMP_OS_NETBSD\n\n// The function returns the rounded value of the system load average\n// during given time interval which depends on the value of\n// __kmp_load_balance_interval variable (default is 60 sec, other values\n// may be 300 sec or 900 sec).\n// It returns -1 in case of error.\nint __kmp_get_load_balance(int max) {\n  double averages[3];\n  int ret_avg = 0;\n\n  int res = getloadavg(averages, 3);\n\n  // Check __kmp_load_balance_interval to determine which of averages to use.\n  // getloadavg() may return the number of samples less than requested that is\n  // less than 3.\n  if (__kmp_load_balance_interval < 180 && (res >= 1)) {\n    ret_avg = averages[0]; // 1 min\n  } else if ((__kmp_load_balance_interval >= 180 &&\n              __kmp_load_balance_interval < 600) &&\n             (res >= 2)) {\n    ret_avg = averages[1]; // 5 min\n  } else if ((__kmp_load_balance_interval >= 600) && (res == 3)) {\n    ret_avg = averages[2]; // 15 min\n  } else { // Error occurred\n    return -1;\n  }\n\n  return ret_avg;\n}\n\n#else // Linux* OS\n\n// The fuction returns number of running (not sleeping) threads, or -1 in case\n// of error. Error could be reported if Linux* OS kernel too old (without\n// \"/proc\" support). Counting running threads stops if max running threads\n// encountered.\nint __kmp_get_load_balance(int max) {\n  static int permanent_error = 0;\n  static int glb_running_threads = 0; // Saved count of the running threads for\n  // the thread balance algortihm\n  static double glb_call_time = 0; /* Thread balance algorithm call time */\n\n  int running_threads = 0; // Number of running threads in the system.\n\n  DIR *proc_dir = NULL; // Handle of \"/proc/\" directory.\n  struct dirent *proc_entry = NULL;\n\n  kmp_str_buf_t task_path; // \"/proc/<pid>/task/<tid>/\" path.\n  DIR *task_dir = NULL; // Handle of \"/proc/<pid>/task/<tid>/\" directory.\n  struct dirent *task_entry = NULL;\n  int task_path_fixed_len;\n\n  kmp_str_buf_t stat_path; // \"/proc/<pid>/task/<tid>/stat\" path.\n  int stat_file = -1;\n  int stat_path_fixed_len;\n\n  int total_processes = 0; // Total number of processes in system.\n  int total_threads = 0; // Total number of threads in system.\n\n  double call_time = 0.0;\n\n  __kmp_str_buf_init(&task_path);\n  __kmp_str_buf_init(&stat_path);\n\n  __kmp_elapsed(&call_time);\n\n  if (glb_call_time &&\n      (call_time - glb_call_time < __kmp_load_balance_interval)) {\n    running_threads = glb_running_threads;\n    goto finish;\n  }\n\n  glb_call_time = call_time;\n\n  // Do not spend time on scanning \"/proc/\" if we have a permanent error.\n  if (permanent_error) {\n    running_threads = -1;\n    goto finish;\n  }\n\n  if (max <= 0) {\n    max = INT_MAX;\n  }\n\n  // Open \"/proc/\" directory.\n  proc_dir = opendir(\"/proc\");\n  if (proc_dir == NULL) {\n    // Cannot open \"/prroc/\". Probably the kernel does not support it. Return an\n    // error now and in subsequent calls.\n    running_threads = -1;\n    permanent_error = 1;\n    goto finish;\n  }\n\n  // Initialize fixed part of task_path. This part will not change.\n  __kmp_str_buf_cat(&task_path, \"/proc/\", 6);\n  task_path_fixed_len = task_path.used; // Remember number of used characters.\n\n  proc_entry = readdir(proc_dir);\n  while (proc_entry != NULL) {\n    // Proc entry is a directory and name starts with a digit. Assume it is a\n    // process' directory.\n    if (proc_entry->d_type == DT_DIR && isdigit(proc_entry->d_name[0])) {\n\n      ++total_processes;\n      // Make sure init process is the very first in \"/proc\", so we can replace\n      // strcmp( proc_entry->d_name, \"1\" ) == 0 with simpler total_processes ==\n      // 1. We are going to check that total_processes == 1 => d_name == \"1\" is\n      // true (where \"=>\" is implication). Since C++ does not have => operator,\n      // let us replace it with its equivalent: a => b == ! a || b.\n      KMP_DEBUG_ASSERT(total_processes != 1 ||\n                       strcmp(proc_entry->d_name, \"1\") == 0);\n\n      // Construct task_path.\n      task_path.used = task_path_fixed_len; // Reset task_path to \"/proc/\".\n      __kmp_str_buf_cat(&task_path, proc_entry->d_name,\n                        KMP_STRLEN(proc_entry->d_name));\n      __kmp_str_buf_cat(&task_path, \"/task\", 5);\n\n      task_dir = opendir(task_path.str);\n      if (task_dir == NULL) {\n        // Process can finish between reading \"/proc/\" directory entry and\n        // opening process' \"task/\" directory. So, in general case we should not\n        // complain, but have to skip this process and read the next one. But on\n        // systems with no \"task/\" support we will spend lot of time to scan\n        // \"/proc/\" tree again and again without any benefit. \"init\" process\n        // (its pid is 1) should exist always, so, if we cannot open\n        // \"/proc/1/task/\" directory, it means \"task/\" is not supported by\n        // kernel. Report an error now and in the future.\n        if (strcmp(proc_entry->d_name, \"1\") == 0) {\n          running_threads = -1;\n          permanent_error = 1;\n          goto finish;\n        }\n      } else {\n        // Construct fixed part of stat file path.\n        __kmp_str_buf_clear(&stat_path);\n        __kmp_str_buf_cat(&stat_path, task_path.str, task_path.used);\n        __kmp_str_buf_cat(&stat_path, \"/\", 1);\n        stat_path_fixed_len = stat_path.used;\n\n        task_entry = readdir(task_dir);\n        while (task_entry != NULL) {\n          // It is a directory and name starts with a digit.\n          if (proc_entry->d_type == DT_DIR && isdigit(task_entry->d_name[0])) {\n            ++total_threads;\n\n            // Consruct complete stat file path. Easiest way would be:\n            //  __kmp_str_buf_print( & stat_path, \"%s/%s/stat\", task_path.str,\n            //  task_entry->d_name );\n            // but seriae of __kmp_str_buf_cat works a bit faster.\n            stat_path.used =\n                stat_path_fixed_len; // Reset stat path to its fixed part.\n            __kmp_str_buf_cat(&stat_path, task_entry->d_name,\n                              KMP_STRLEN(task_entry->d_name));\n            __kmp_str_buf_cat(&stat_path, \"/stat\", 5);\n\n            // Note: Low-level API (open/read/close) is used. High-level API\n            // (fopen/fclose)  works ~ 30 % slower.\n            stat_file = open(stat_path.str, O_RDONLY);\n            if (stat_file == -1) {\n              // We cannot report an error because task (thread) can terminate\n              // just before reading this file.\n            } else {\n              /* Content of \"stat\" file looks like:\n                 24285 (program) S ...\n\n                 It is a single line (if program name does not include funny\n                 symbols). First number is a thread id, then name of executable\n                 file name in paretheses, then state of the thread. We need just\n                 thread state.\n\n                 Good news: Length of program name is 15 characters max. Longer\n                 names are truncated.\n\n                 Thus, we need rather short buffer: 15 chars for program name +\n                 2 parenthesis, + 3 spaces + ~7 digits of pid = 37.\n\n                 Bad news: Program name may contain special symbols like space,\n                 closing parenthesis, or even new line. This makes parsing\n                 \"stat\" file not 100 % reliable. In case of fanny program names\n                 parsing may fail (report incorrect thread state).\n\n                 Parsing \"status\" file looks more promissing (due to different\n                 file structure and escaping special symbols) but reading and\n                 parsing of \"status\" file works slower.\n                  -- ln\n              */\n              char buffer[65];\n              int len;\n              len = read(stat_file, buffer, sizeof(buffer) - 1);\n              if (len >= 0) {\n                buffer[len] = 0;\n                // Using scanf:\n                //     sscanf( buffer, \"%*d (%*s) %c \", & state );\n                // looks very nice, but searching for a closing parenthesis\n                // works a bit faster.\n                char *close_parent = strstr(buffer, \") \");\n                if (close_parent != NULL) {\n                  char state = *(close_parent + 2);\n                  if (state == 'R') {\n                    ++running_threads;\n                    if (running_threads >= max) {\n                      goto finish;\n                    }\n                  }\n                }\n              }\n              close(stat_file);\n              stat_file = -1;\n            }\n          }\n          task_entry = readdir(task_dir);\n        }\n        closedir(task_dir);\n        task_dir = NULL;\n      }\n    }\n    proc_entry = readdir(proc_dir);\n  }\n\n  // There _might_ be a timing hole where the thread executing this\n  // code get skipped in the load balance, and running_threads is 0.\n  // Assert in the debug builds only!!!\n  KMP_DEBUG_ASSERT(running_threads > 0);\n  if (running_threads <= 0) {\n    running_threads = 1;\n  }\n\nfinish: // Clean up and exit.\n  if (proc_dir != NULL) {\n    closedir(proc_dir);\n  }\n  __kmp_str_buf_free(&task_path);\n  if (task_dir != NULL) {\n    closedir(task_dir);\n  }\n  __kmp_str_buf_free(&stat_path);\n  if (stat_file != -1) {\n    close(stat_file);\n  }\n\n  glb_running_threads = running_threads;\n\n  return running_threads;\n\n} // __kmp_get_load_balance\n\n#endif // KMP_OS_DARWIN\n\n#endif // USE_LOAD_BALANCE\n\n#if !(KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_MIC ||                            \\\n      ((KMP_OS_LINUX || KMP_OS_DARWIN) && KMP_ARCH_AARCH64) || KMP_ARCH_PPC64)\n\n// we really only need the case with 1 argument, because CLANG always build\n// a struct of pointers to shared variables referenced in the outlined function\nint __kmp_invoke_microtask(microtask_t pkfn, int gtid, int tid, int argc,\n                           void *p_argv[]\n#if OMPT_SUPPORT\n                           ,\n                           void **exit_frame_ptr\n#endif\n                           ) {\n#if OMPT_SUPPORT\n  *exit_frame_ptr = OMPT_GET_FRAME_ADDRESS(0);\n#endif\n\n  switch (argc) {\n  default:\n    fprintf(stderr, \"Too many args to microtask: %d!\\n\", argc);\n    fflush(stderr);\n    exit(-1);\n  case 0:\n    (*pkfn)(&gtid, &tid);\n    break;\n  case 1:\n    (*pkfn)(&gtid, &tid, p_argv[0]);\n    break;\n  case 2:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1]);\n    break;\n  case 3:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2]);\n    break;\n  case 4:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3]);\n    break;\n  case 5:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4]);\n    break;\n  case 6:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5]);\n    break;\n  case 7:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6]);\n    break;\n  case 8:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7]);\n    break;\n  case 9:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8]);\n    break;\n  case 10:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9]);\n    break;\n  case 11:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10]);\n    break;\n  case 12:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11]);\n    break;\n  case 13:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12]);\n    break;\n  case 14:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13]);\n    break;\n  case 15:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14]);\n    break;\n  }\n\n#if OMPT_SUPPORT\n  *exit_frame_ptr = 0;\n#endif\n\n  return 1;\n}\n\n#endif\n\n// end of file //\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/runtime/src/ompt-general.cpp": "/*\n * ompt-general.cpp -- OMPT implementation of interface functions\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n/*****************************************************************************\n * system include files\n ****************************************************************************/\n\n#include <assert.h>\n\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#if KMP_OS_UNIX\n#include <dlfcn.h>\n#endif\n\n/*****************************************************************************\n * ompt include files\n ****************************************************************************/\n\n#include \"ompt-specific.cpp\"\n\n/*****************************************************************************\n * macros\n ****************************************************************************/\n\n#define ompt_get_callback_success 1\n#define ompt_get_callback_failure 0\n\n#define no_tool_present 0\n\n#define OMPT_API_ROUTINE static\n\n#ifndef OMPT_STR_MATCH\n#define OMPT_STR_MATCH(haystack, needle) (!strcasecmp(haystack, needle))\n#endif\n\n/*****************************************************************************\n * types\n ****************************************************************************/\n\ntypedef struct {\n  const char *state_name;\n  ompt_state_t state_id;\n} ompt_state_info_t;\n\ntypedef struct {\n  const char *name;\n  kmp_mutex_impl_t id;\n} kmp_mutex_impl_info_t;\n\nenum tool_setting_e {\n  omp_tool_error,\n  omp_tool_unset,\n  omp_tool_disabled,\n  omp_tool_enabled\n};\n\n/*****************************************************************************\n * global variables\n ****************************************************************************/\n\nompt_callbacks_active_t ompt_enabled;\n\nompt_state_info_t ompt_state_info[] = {\n#define ompt_state_macro(state, code) {#state, state},\n    FOREACH_OMPT_STATE(ompt_state_macro)\n#undef ompt_state_macro\n};\n\nkmp_mutex_impl_info_t kmp_mutex_impl_info[] = {\n#define kmp_mutex_impl_macro(name, id) {#name, name},\n    FOREACH_KMP_MUTEX_IMPL(kmp_mutex_impl_macro)\n#undef kmp_mutex_impl_macro\n};\n\nompt_callbacks_internal_t ompt_callbacks;\n\nstatic ompt_start_tool_result_t *ompt_start_tool_result = NULL;\n\n/*****************************************************************************\n * forward declarations\n ****************************************************************************/\n\nstatic ompt_interface_fn_t ompt_fn_lookup(const char *s);\n\nOMPT_API_ROUTINE ompt_data_t *ompt_get_thread_data(void);\n\n/*****************************************************************************\n * initialization and finalization (private operations)\n ****************************************************************************/\n\ntypedef ompt_start_tool_result_t *(*ompt_start_tool_t)(unsigned int,\n                                                       const char *);\n\n#if KMP_OS_DARWIN\n\n// While Darwin supports weak symbols, the library that wishes to provide a new\n// implementation has to link against this runtime which defeats the purpose\n// of having tools that are agnostic of the underlying runtime implementation.\n//\n// Fortunately, the linker includes all symbols of an executable in the global\n// symbol table by default so dlsym() even finds static implementations of\n// ompt_start_tool. For this to work on Linux, -Wl,--export-dynamic needs to be\n// passed when building the application which we don't want to rely on.\n\nstatic ompt_start_tool_result_t *ompt_tool_darwin(unsigned int omp_version,\n                                                  const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  // Search symbol in the current address space.\n  ompt_start_tool_t start_tool =\n      (ompt_start_tool_t)dlsym(RTLD_DEFAULT, \"ompt_start_tool\");\n  if (start_tool) {\n    ret = start_tool(omp_version, runtime_version);\n  }\n  return ret;\n}\n\n#elif OMPT_HAVE_WEAK_ATTRIBUTE\n\n// On Unix-like systems that support weak symbols the following implementation\n// of ompt_start_tool() will be used in case no tool-supplied implementation of\n// this function is present in the address space of a process.\n\n_OMP_EXTERN OMPT_WEAK_ATTRIBUTE ompt_start_tool_result_t *\nompt_start_tool(unsigned int omp_version, const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  // Search next symbol in the current address space. This can happen if the\n  // runtime library is linked before the tool. Since glibc 2.2 strong symbols\n  // don't override weak symbols that have been found before unless the user\n  // sets the environment variable LD_DYNAMIC_WEAK.\n  ompt_start_tool_t next_tool =\n      (ompt_start_tool_t)dlsym(RTLD_NEXT, \"ompt_start_tool\");\n  if (next_tool) {\n    ret = next_tool(omp_version, runtime_version);\n  }\n  return ret;\n}\n\n#elif OMPT_HAVE_PSAPI\n\n// On Windows, the ompt_tool_windows function is used to find the\n// ompt_start_tool symbol across all modules loaded by a process. If\n// ompt_start_tool is found, ompt_start_tool's return value is used to\n// initialize the tool. Otherwise, NULL is returned and OMPT won't be enabled.\n\n#include <psapi.h>\n#pragma comment(lib, \"psapi.lib\")\n\n// The number of loaded modules to start enumeration with EnumProcessModules()\n#define NUM_MODULES 128\n\nstatic ompt_start_tool_result_t *\nompt_tool_windows(unsigned int omp_version, const char *runtime_version) {\n  int i;\n  DWORD needed, new_size;\n  HMODULE *modules;\n  HANDLE process = GetCurrentProcess();\n  modules = (HMODULE *)malloc(NUM_MODULES * sizeof(HMODULE));\n  ompt_start_tool_t ompt_tool_p = NULL;\n\n#if OMPT_DEBUG\n  printf(\"ompt_tool_windows(): looking for ompt_start_tool\\n\");\n#endif\n  if (!EnumProcessModules(process, modules, NUM_MODULES * sizeof(HMODULE),\n                          &needed)) {\n    // Regardless of the error reason use the stub initialization function\n    free(modules);\n    return NULL;\n  }\n  // Check if NUM_MODULES is enough to list all modules\n  new_size = needed / sizeof(HMODULE);\n  if (new_size > NUM_MODULES) {\n#if OMPT_DEBUG\n    printf(\"ompt_tool_windows(): resize buffer to %d bytes\\n\", needed);\n#endif\n    modules = (HMODULE *)realloc(modules, needed);\n    // If resizing failed use the stub function.\n    if (!EnumProcessModules(process, modules, needed, &needed)) {\n      free(modules);\n      return NULL;\n    }\n  }\n  for (i = 0; i < new_size; ++i) {\n    (FARPROC &)ompt_tool_p = GetProcAddress(modules[i], \"ompt_start_tool\");\n    if (ompt_tool_p) {\n#if OMPT_DEBUG\n      TCHAR modName[MAX_PATH];\n      if (GetModuleFileName(modules[i], modName, MAX_PATH))\n        printf(\"ompt_tool_windows(): ompt_start_tool found in module %s\\n\",\n               modName);\n#endif\n      free(modules);\n      return (*ompt_tool_p)(omp_version, runtime_version);\n    }\n#if OMPT_DEBUG\n    else {\n      TCHAR modName[MAX_PATH];\n      if (GetModuleFileName(modules[i], modName, MAX_PATH))\n        printf(\"ompt_tool_windows(): ompt_start_tool not found in module %s\\n\",\n               modName);\n    }\n#endif\n  }\n  free(modules);\n  return NULL;\n}\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n\nstatic ompt_start_tool_result_t *\nompt_try_start_tool(unsigned int omp_version, const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  ompt_start_tool_t start_tool = NULL;\n#if KMP_OS_WINDOWS\n  // Cannot use colon to describe a list of absolute paths on Windows\n  const char *sep = \";\";\n#else\n  const char *sep = \":\";\n#endif\n\n#if KMP_OS_DARWIN\n  // Try in the current address space\n  ret = ompt_tool_darwin(omp_version, runtime_version);\n#elif OMPT_HAVE_WEAK_ATTRIBUTE\n  ret = ompt_start_tool(omp_version, runtime_version);\n#elif OMPT_HAVE_PSAPI\n  ret = ompt_tool_windows(omp_version, runtime_version);\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n  if (ret)\n    return ret;\n\n  // Try tool-libraries-var ICV\n  const char *tool_libs = getenv(\"OMP_TOOL_LIBRARIES\");\n  if (tool_libs) {\n    char *libs = __kmp_str_format(\"%s\", tool_libs);\n    char *buf;\n    char *fname = __kmp_str_token(libs, sep, &buf);\n    while (fname) {\n#if KMP_OS_UNIX\n      void *h = dlopen(fname, RTLD_LAZY);\n      if (h) {\n        start_tool = (ompt_start_tool_t)dlsym(h, \"ompt_start_tool\");\n#elif KMP_OS_WINDOWS\n      HMODULE h = LoadLibrary(fname);\n      if (h) {\n        start_tool = (ompt_start_tool_t)GetProcAddress(h, \"ompt_start_tool\");\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n        if (start_tool && (ret = (*start_tool)(omp_version, runtime_version)))\n          break;\n      }\n      fname = __kmp_str_token(NULL, sep, &buf);\n    }\n    __kmp_str_free(&libs);\n  }\n  return ret;\n}\n\nvoid ompt_pre_init() {\n  //--------------------------------------------------\n  // Execute the pre-initialization logic only once.\n  //--------------------------------------------------\n  static int ompt_pre_initialized = 0;\n\n  if (ompt_pre_initialized)\n    return;\n\n  ompt_pre_initialized = 1;\n\n  //--------------------------------------------------\n  // Use a tool iff a tool is enabled and available.\n  //--------------------------------------------------\n  const char *ompt_env_var = getenv(\"OMP_TOOL\");\n  tool_setting_e tool_setting = omp_tool_error;\n\n  if (!ompt_env_var || !strcmp(ompt_env_var, \"\"))\n    tool_setting = omp_tool_unset;\n  else if (OMPT_STR_MATCH(ompt_env_var, \"disabled\"))\n    tool_setting = omp_tool_disabled;\n  else if (OMPT_STR_MATCH(ompt_env_var, \"enabled\"))\n    tool_setting = omp_tool_enabled;\n\n#if OMPT_DEBUG\n  printf(\"ompt_pre_init(): tool_setting = %d\\n\", tool_setting);\n#endif\n  switch (tool_setting) {\n  case omp_tool_disabled:\n    break;\n\n  case omp_tool_unset:\n  case omp_tool_enabled:\n\n    //--------------------------------------------------\n    // Load tool iff specified in environment variable\n    //--------------------------------------------------\n    ompt_start_tool_result =\n        ompt_try_start_tool(__kmp_openmp_version, ompt_get_runtime_version());\n\n    memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n    break;\n\n  case omp_tool_error:\n    fprintf(stderr, \"Warning: OMP_TOOL has invalid value \\\"%s\\\".\\n\"\n                    \"  legal values are (NULL,\\\"\\\",\\\"disabled\\\",\"\n                    \"\\\"enabled\\\").\\n\",\n            ompt_env_var);\n    break;\n  }\n#if OMPT_DEBUG\n  printf(\"ompt_pre_init(): ompt_enabled = %d\\n\", ompt_enabled);\n#endif\n}\n\nextern \"C\" int omp_get_initial_device(void);\n\nvoid ompt_post_init() {\n  //--------------------------------------------------\n  // Execute the post-initialization logic only once.\n  //--------------------------------------------------\n  static int ompt_post_initialized = 0;\n\n  if (ompt_post_initialized)\n    return;\n\n  ompt_post_initialized = 1;\n\n  //--------------------------------------------------\n  // Initialize the tool if so indicated.\n  //--------------------------------------------------\n  if (ompt_start_tool_result) {\n    ompt_enabled.enabled = !!ompt_start_tool_result->initialize(\n        ompt_fn_lookup, omp_get_initial_device(), &(ompt_start_tool_result->tool_data));\n\n    if (!ompt_enabled.enabled) {\n      // tool not enabled, zero out the bitmap, and done\n      memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n      return;\n    }\n\n    kmp_info_t *root_thread = ompt_get_thread();\n\n    ompt_set_thread_state(root_thread, ompt_state_overhead);\n\n    if (ompt_enabled.ompt_callback_thread_begin) {\n      ompt_callbacks.ompt_callback(ompt_callback_thread_begin)(\n          ompt_thread_initial, __ompt_get_thread_data_internal());\n    }\n    ompt_data_t *task_data;\n    ompt_data_t *parallel_data;\n    __ompt_get_task_info_internal(0, NULL, &task_data, NULL, &parallel_data, NULL);\n    if (ompt_enabled.ompt_callback_implicit_task) {\n      ompt_callbacks.ompt_callback(ompt_callback_implicit_task)(\n          ompt_scope_begin, parallel_data, task_data, 1, 1, ompt_task_initial);\n    }\n\n    ompt_set_thread_state(root_thread, ompt_state_work_serial);\n  }\n}\n\nvoid ompt_fini() {\n  if (ompt_enabled.enabled) {\n    ompt_start_tool_result->finalize(&(ompt_start_tool_result->tool_data));\n  }\n\n  memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n}\n\n/*****************************************************************************\n * interface operations\n ****************************************************************************/\n\n/*****************************************************************************\n * state\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_enumerate_states(int current_state, int *next_state,\n                                           const char **next_state_name) {\n  const static int len = sizeof(ompt_state_info) / sizeof(ompt_state_info_t);\n  int i = 0;\n\n  for (i = 0; i < len - 1; i++) {\n    if (ompt_state_info[i].state_id == current_state) {\n      *next_state = ompt_state_info[i + 1].state_id;\n      *next_state_name = ompt_state_info[i + 1].state_name;\n      return 1;\n    }\n  }\n\n  return 0;\n}\n\nOMPT_API_ROUTINE int ompt_enumerate_mutex_impls(int current_impl,\n                                                int *next_impl,\n                                                const char **next_impl_name) {\n  const static int len =\n      sizeof(kmp_mutex_impl_info) / sizeof(kmp_mutex_impl_info_t);\n  int i = 0;\n  for (i = 0; i < len - 1; i++) {\n    if (kmp_mutex_impl_info[i].id != current_impl)\n      continue;\n    *next_impl = kmp_mutex_impl_info[i + 1].id;\n    *next_impl_name = kmp_mutex_impl_info[i + 1].name;\n    return 1;\n  }\n  return 0;\n}\n\n/*****************************************************************************\n * callbacks\n ****************************************************************************/\n\nOMPT_API_ROUTINE ompt_set_result_t ompt_set_callback(ompt_callbacks_t which,\n                                       ompt_callback_t callback) {\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case event_name:                                                             \\\n    if (ompt_event_implementation_status(event_name)) {                        \\\n      ompt_callbacks.ompt_callback(event_name) = (callback_type)callback;      \\\n      ompt_enabled.event_name = (callback != 0);                               \\\n    }                                                                          \\\n    if (callback)                                                              \\\n      return ompt_event_implementation_status(event_name);                     \\\n    else                                                                       \\\n      return ompt_set_always;\n\n    FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_set_error;\n  }\n}\n\nOMPT_API_ROUTINE int ompt_get_callback(ompt_callbacks_t which,\n                                       ompt_callback_t *callback) {\n  if (!ompt_enabled.enabled)\n    return ompt_get_callback_failure;\n\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case event_name:                                                             \\\n    if (ompt_event_implementation_status(event_name)) {                        \\\n      ompt_callback_t mycb =                                                   \\\n          (ompt_callback_t)ompt_callbacks.ompt_callback(event_name);           \\\n      if (ompt_enabled.event_name && mycb) {                                   \\\n        *callback = mycb;                                                      \\\n        return ompt_get_callback_success;                                      \\\n      }                                                                        \\\n    }                                                                          \\\n    return ompt_get_callback_failure;\n\n    FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_get_callback_failure;\n  }\n}\n\n/*****************************************************************************\n * parallel regions\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_parallel_info(int ancestor_level,\n                                            ompt_data_t **parallel_data,\n                                            int *team_size) {\n  if (!ompt_enabled.enabled)\n    return 0;\n  return __ompt_get_parallel_info_internal(ancestor_level, parallel_data,\n                                           team_size);\n}\n\nOMPT_API_ROUTINE int ompt_get_state(ompt_wait_id_t *wait_id) {\n  if (!ompt_enabled.enabled)\n    return ompt_state_work_serial;\n  int thread_state = __ompt_get_state_internal(wait_id);\n\n  if (thread_state == ompt_state_undefined) {\n    thread_state = ompt_state_work_serial;\n  }\n\n  return thread_state;\n}\n\n/*****************************************************************************\n * tasks\n ****************************************************************************/\n\nOMPT_API_ROUTINE ompt_data_t *ompt_get_thread_data(void) {\n  if (!ompt_enabled.enabled)\n    return NULL;\n  return __ompt_get_thread_data_internal();\n}\n\nOMPT_API_ROUTINE int ompt_get_task_info(int ancestor_level, int *type,\n                                        ompt_data_t **task_data,\n                                        ompt_frame_t **task_frame,\n                                        ompt_data_t **parallel_data,\n                                        int *thread_num) {\n  if (!ompt_enabled.enabled)\n    return 0;\n  return __ompt_get_task_info_internal(ancestor_level, type, task_data,\n                                       task_frame, parallel_data, thread_num);\n}\n\nOMPT_API_ROUTINE int ompt_get_task_memory(void **addr, size_t *size,\n                                          int block) {\n  return __ompt_get_task_memory_internal(addr, size, block);\n}\n\n/*****************************************************************************\n * num_procs\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_num_procs(void) {\n  // copied from kmp_ftn_entry.h (but modified: OMPT can only be called when\n  // runtime is initialized)\n  return __kmp_avail_proc;\n}\n\n/*****************************************************************************\n * places\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_num_places(void) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  return __kmp_affinity_num_masks;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_place_proc_ids(int place_num, int ids_size,\n                                             int *ids) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  int i, count;\n  int tmp_ids[ids_size];\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  if (place_num < 0 || place_num >= (int)__kmp_affinity_num_masks)\n    return 0;\n  /* TODO: Is this safe for asynchronous call from signal handler during runtime\n   * shutdown? */\n  kmp_affin_mask_t *mask = KMP_CPU_INDEX(__kmp_affinity_masks, place_num);\n  count = 0;\n  KMP_CPU_SET_ITERATE(i, mask) {\n    if ((!KMP_CPU_ISSET(i, __kmp_affin_fullMask)) ||\n        (!KMP_CPU_ISSET(i, mask))) {\n      continue;\n    }\n    if (count < ids_size)\n      tmp_ids[count] = i;\n    count++;\n  }\n  if (ids_size >= count) {\n    for (i = 0; i < count; i++) {\n      ids[i] = tmp_ids[i];\n    }\n  }\n  return count;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_place_num(void) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (!ompt_enabled.enabled || __kmp_get_gtid() < 0)\n    return -1;\n\n  int gtid;\n  kmp_info_t *thread;\n  if (!KMP_AFFINITY_CAPABLE())\n    return -1;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread == NULL || thread->th.th_current_place < 0)\n    return -1;\n  return thread->th.th_current_place;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_partition_place_nums(int place_nums_size,\n                                                   int *place_nums) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (!ompt_enabled.enabled || __kmp_get_gtid() < 0)\n    return 0;\n\n  int i, gtid, place_num, first_place, last_place, start, end;\n  kmp_info_t *thread;\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread == NULL)\n    return 0;\n  first_place = thread->th.th_first_place;\n  last_place = thread->th.th_last_place;\n  if (first_place < 0 || last_place < 0)\n    return 0;\n  if (first_place <= last_place) {\n    start = first_place;\n    end = last_place;\n  } else {\n    start = last_place;\n    end = first_place;\n  }\n  if (end - start <= place_nums_size)\n    for (i = 0, place_num = start; place_num <= end; ++place_num, ++i) {\n      place_nums[i] = place_num;\n    }\n  return end - start + 1;\n#endif\n}\n\n/*****************************************************************************\n * places\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_proc_id(void) {\n  if (!ompt_enabled.enabled || __kmp_get_gtid() < 0)\n    return -1;\n#if KMP_OS_LINUX\n  return sched_getcpu();\n#elif KMP_OS_WINDOWS\n  PROCESSOR_NUMBER pn;\n  GetCurrentProcessorNumberEx(&pn);\n  return 64 * pn.Group + pn.Number;\n#else\n  return -1;\n#endif\n}\n\n/*****************************************************************************\n * compatability\n ****************************************************************************/\n\n/*\n * Currently unused function\nOMPT_API_ROUTINE int ompt_get_ompt_version() { return OMPT_VERSION; }\n*/\n\n/*****************************************************************************\n* application-facing API\n ****************************************************************************/\n\n/*----------------------------------------------------------------------------\n | control\n ---------------------------------------------------------------------------*/\n\nint __kmp_control_tool(uint64_t command, uint64_t modifier, void *arg) {\n\n  if (ompt_enabled.enabled) {\n    if (ompt_enabled.ompt_callback_control_tool) {\n      return ompt_callbacks.ompt_callback(ompt_callback_control_tool)(\n          command, modifier, arg, OMPT_LOAD_RETURN_ADDRESS(__kmp_entry_gtid()));\n    } else {\n      return -1;\n    }\n  } else {\n    return -2;\n  }\n}\n\n/*****************************************************************************\n * misc\n ****************************************************************************/\n\nOMPT_API_ROUTINE uint64_t ompt_get_unique_id(void) {\n  return __ompt_get_unique_id_internal();\n}\n\nOMPT_API_ROUTINE void ompt_finalize_tool(void) { __kmp_internal_end_atexit(); }\n\n/*****************************************************************************\n * Target\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_target_info(uint64_t *device_num,\n                                          ompt_id_t *target_id,\n                                          ompt_id_t *host_op_id) {\n  return 0; // thread is not in a target region\n}\n\nOMPT_API_ROUTINE int ompt_get_num_devices(void) {\n  return 1; // only one device (the current device) is available\n}\n\n/*****************************************************************************\n * API inquiry for tool\n ****************************************************************************/\n\nstatic ompt_interface_fn_t ompt_fn_lookup(const char *s) {\n\n#define ompt_interface_fn(fn)                                                  \\\n  fn##_t fn##_f = fn;                                                          \\\n  if (strcmp(s, #fn) == 0)                                                     \\\n    return (ompt_interface_fn_t)fn##_f;\n\n  FOREACH_OMPT_INQUIRY_FN(ompt_interface_fn)\n\n  return (ompt_interface_fn_t)0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/runtime/src/kmp_alloc.cpp": "/*\n * kmp_alloc.cpp -- private/shared dynamic memory allocation and management\n */\n\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#include \"kmp.h\"\n#include \"kmp_io.h\"\n#include \"kmp_wrapper_malloc.h\"\n\n// Disable bget when it is not used\n#if KMP_USE_BGET\n\n/* Thread private buffer management code */\n\ntypedef int (*bget_compact_t)(size_t, int);\ntypedef void *(*bget_acquire_t)(size_t);\ntypedef void (*bget_release_t)(void *);\n\n/* NOTE: bufsize must be a signed datatype */\n\n#if KMP_OS_WINDOWS\n#if KMP_ARCH_X86 || KMP_ARCH_ARM\ntypedef kmp_int32 bufsize;\n#else\ntypedef kmp_int64 bufsize;\n#endif\n#else\ntypedef ssize_t bufsize;\n#endif // KMP_OS_WINDOWS\n\n/* The three modes of operation are, fifo search, lifo search, and best-fit */\n\ntypedef enum bget_mode {\n  bget_mode_fifo = 0,\n  bget_mode_lifo = 1,\n  bget_mode_best = 2\n} bget_mode_t;\n\nstatic void bpool(kmp_info_t *th, void *buffer, bufsize len);\nstatic void *bget(kmp_info_t *th, bufsize size);\nstatic void *bgetz(kmp_info_t *th, bufsize size);\nstatic void *bgetr(kmp_info_t *th, void *buffer, bufsize newsize);\nstatic void brel(kmp_info_t *th, void *buf);\nstatic void bectl(kmp_info_t *th, bget_compact_t compact,\n                  bget_acquire_t acquire, bget_release_t release,\n                  bufsize pool_incr);\n\n/* BGET CONFIGURATION */\n/* Buffer allocation size quantum: all buffers allocated are a\n   multiple of this size.  This MUST be a power of two. */\n\n/* On IA-32 architecture with  Linux* OS, malloc() does not\n   ensure 16 byte alignmnent */\n\n#if KMP_ARCH_X86 || !KMP_HAVE_QUAD\n\n#define SizeQuant 8\n#define AlignType double\n\n#else\n\n#define SizeQuant 16\n#define AlignType _Quad\n\n#endif\n\n// Define this symbol to enable the bstats() function which calculates the\n// total free space in the buffer pool, the largest available buffer, and the\n// total space currently allocated.\n#define BufStats 1\n\n#ifdef KMP_DEBUG\n\n// Define this symbol to enable the bpoold() function which dumps the buffers\n// in a buffer pool.\n#define BufDump 1\n\n// Define this symbol to enable the bpoolv() function for validating a buffer\n// pool.\n#define BufValid 1\n\n// Define this symbol to enable the bufdump() function which allows dumping the\n// contents of an allocated or free buffer.\n#define DumpData 1\n\n#ifdef NOT_USED_NOW\n\n// Wipe free buffers to a guaranteed pattern of garbage to trip up miscreants\n// who attempt to use pointers into released buffers.\n#define FreeWipe 1\n\n// Use a best fit algorithm when searching for space for an allocation request.\n// This uses memory more efficiently, but allocation will be much slower.\n#define BestFit 1\n\n#endif /* NOT_USED_NOW */\n#endif /* KMP_DEBUG */\n\nstatic bufsize bget_bin_size[] = {\n    0,\n    //    1 << 6,    /* .5 Cache line */\n    1 << 7, /* 1 Cache line, new */\n    1 << 8, /* 2 Cache lines */\n    1 << 9, /* 4 Cache lines, new */\n    1 << 10, /* 8 Cache lines */\n    1 << 11, /* 16 Cache lines, new */\n    1 << 12, 1 << 13, /* new */\n    1 << 14, 1 << 15, /* new */\n    1 << 16, 1 << 17, 1 << 18, 1 << 19, 1 << 20, /*  1MB */\n    1 << 21, /*  2MB */\n    1 << 22, /*  4MB */\n    1 << 23, /*  8MB */\n    1 << 24, /* 16MB */\n    1 << 25, /* 32MB */\n};\n\n#define MAX_BGET_BINS (int)(sizeof(bget_bin_size) / sizeof(bufsize))\n\nstruct bfhead;\n\n//  Declare the interface, including the requested buffer size type, bufsize.\n\n/* Queue links */\ntypedef struct qlinks {\n  struct bfhead *flink; /* Forward link */\n  struct bfhead *blink; /* Backward link */\n} qlinks_t;\n\n/* Header in allocated and free buffers */\ntypedef struct bhead2 {\n  kmp_info_t *bthr; /* The thread which owns the buffer pool */\n  bufsize prevfree; /* Relative link back to previous free buffer in memory or\n                       0 if previous buffer is allocated.  */\n  bufsize bsize; /* Buffer size: positive if free, negative if allocated. */\n} bhead2_t;\n\n/* Make sure the bhead structure is a multiple of SizeQuant in size. */\ntypedef union bhead {\n  KMP_ALIGN(SizeQuant)\n  AlignType b_align;\n  char b_pad[sizeof(bhead2_t) + (SizeQuant - (sizeof(bhead2_t) % SizeQuant))];\n  bhead2_t bb;\n} bhead_t;\n#define BH(p) ((bhead_t *)(p))\n\n/*  Header in directly allocated buffers (by acqfcn) */\ntypedef struct bdhead {\n  bufsize tsize; /* Total size, including overhead */\n  bhead_t bh; /* Common header */\n} bdhead_t;\n#define BDH(p) ((bdhead_t *)(p))\n\n/* Header in free buffers */\ntypedef struct bfhead {\n  bhead_t bh; /* Common allocated/free header */\n  qlinks_t ql; /* Links on free list */\n} bfhead_t;\n#define BFH(p) ((bfhead_t *)(p))\n\ntypedef struct thr_data {\n  bfhead_t freelist[MAX_BGET_BINS];\n#if BufStats\n  size_t totalloc; /* Total space currently allocated */\n  long numget, numrel; /* Number of bget() and brel() calls */\n  long numpblk; /* Number of pool blocks */\n  long numpget, numprel; /* Number of block gets and rels */\n  long numdget, numdrel; /* Number of direct gets and rels */\n#endif /* BufStats */\n\n  /* Automatic expansion block management functions */\n  bget_compact_t compfcn;\n  bget_acquire_t acqfcn;\n  bget_release_t relfcn;\n\n  bget_mode_t mode; /* what allocation mode to use? */\n\n  bufsize exp_incr; /* Expansion block size */\n  bufsize pool_len; /* 0: no bpool calls have been made\n                       -1: not all pool blocks are the same size\n                       >0: (common) block size for all bpool calls made so far\n                    */\n  bfhead_t *last_pool; /* Last pool owned by this thread (delay dealocation) */\n} thr_data_t;\n\n/*  Minimum allocation quantum: */\n#define QLSize (sizeof(qlinks_t))\n#define SizeQ ((SizeQuant > QLSize) ? SizeQuant : QLSize)\n#define MaxSize                                                                \\\n  (bufsize)(                                                                   \\\n      ~(((bufsize)(1) << (sizeof(bufsize) * CHAR_BIT - 1)) | (SizeQuant - 1)))\n// Maximun for the requested size.\n\n/* End sentinel: value placed in bsize field of dummy block delimiting\n   end of pool block.  The most negative number which will  fit  in  a\n   bufsize, defined in a way that the compiler will accept. */\n\n#define ESent                                                                  \\\n  ((bufsize)(-(((((bufsize)1) << ((int)sizeof(bufsize) * 8 - 2)) - 1) * 2) - 2))\n\n/* Thread Data management routines */\nstatic int bget_get_bin(bufsize size) {\n  // binary chop bins\n  int lo = 0, hi = MAX_BGET_BINS - 1;\n\n  KMP_DEBUG_ASSERT(size > 0);\n\n  while ((hi - lo) > 1) {\n    int mid = (lo + hi) >> 1;\n    if (size < bget_bin_size[mid])\n      hi = mid - 1;\n    else\n      lo = mid;\n  }\n\n  KMP_DEBUG_ASSERT((lo >= 0) && (lo < MAX_BGET_BINS));\n\n  return lo;\n}\n\nstatic void set_thr_data(kmp_info_t *th) {\n  int i;\n  thr_data_t *data;\n\n  data = (thr_data_t *)((!th->th.th_local.bget_data)\n                            ? __kmp_allocate(sizeof(*data))\n                            : th->th.th_local.bget_data);\n\n  memset(data, '\\0', sizeof(*data));\n\n  for (i = 0; i < MAX_BGET_BINS; ++i) {\n    data->freelist[i].ql.flink = &data->freelist[i];\n    data->freelist[i].ql.blink = &data->freelist[i];\n  }\n\n  th->th.th_local.bget_data = data;\n  th->th.th_local.bget_list = 0;\n#if !USE_CMP_XCHG_FOR_BGET\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n  __kmp_init_lock(&th->th.th_local.bget_lock);\n#else\n  __kmp_init_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif /* USE_LOCK_FOR_BGET */\n#endif /* ! USE_CMP_XCHG_FOR_BGET */\n}\n\nstatic thr_data_t *get_thr_data(kmp_info_t *th) {\n  thr_data_t *data;\n\n  data = (thr_data_t *)th->th.th_local.bget_data;\n\n  KMP_DEBUG_ASSERT(data != 0);\n\n  return data;\n}\n\n/* Walk the free list and release the enqueued buffers */\nstatic void __kmp_bget_dequeue(kmp_info_t *th) {\n  void *p = TCR_SYNC_PTR(th->th.th_local.bget_list);\n\n  if (p != 0) {\n#if USE_CMP_XCHG_FOR_BGET\n    {\n      volatile void *old_value = TCR_SYNC_PTR(th->th.th_local.bget_list);\n      while (!KMP_COMPARE_AND_STORE_PTR(&th->th.th_local.bget_list,\n                                        CCAST(void *, old_value), nullptr)) {\n        KMP_CPU_PAUSE();\n        old_value = TCR_SYNC_PTR(th->th.th_local.bget_list);\n      }\n      p = CCAST(void *, old_value);\n    }\n#else /* ! USE_CMP_XCHG_FOR_BGET */\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n    __kmp_acquire_lock(&th->th.th_local.bget_lock, __kmp_gtid_from_thread(th));\n#else\n    __kmp_acquire_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif /* USE_QUEUING_LOCK_FOR_BGET */\n\n    p = (void *)th->th.th_local.bget_list;\n    th->th.th_local.bget_list = 0;\n\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n    __kmp_release_lock(&th->th.th_local.bget_lock, __kmp_gtid_from_thread(th));\n#else\n    __kmp_release_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif\n#endif /* USE_CMP_XCHG_FOR_BGET */\n\n    /* Check again to make sure the list is not empty */\n    while (p != 0) {\n      void *buf = p;\n      bfhead_t *b = BFH(((char *)p) - sizeof(bhead_t));\n\n      KMP_DEBUG_ASSERT(b->bh.bb.bsize != 0);\n      KMP_DEBUG_ASSERT(((kmp_uintptr_t)TCR_PTR(b->bh.bb.bthr) & ~1) ==\n                       (kmp_uintptr_t)th); // clear possible mark\n      KMP_DEBUG_ASSERT(b->ql.blink == 0);\n\n      p = (void *)b->ql.flink;\n\n      brel(th, buf);\n    }\n  }\n}\n\n/* Chain together the free buffers by using the thread owner field */\nstatic void __kmp_bget_enqueue(kmp_info_t *th, void *buf\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n                               ,\n                               kmp_int32 rel_gtid\n#endif\n                               ) {\n  bfhead_t *b = BFH(((char *)buf) - sizeof(bhead_t));\n\n  KMP_DEBUG_ASSERT(b->bh.bb.bsize != 0);\n  KMP_DEBUG_ASSERT(((kmp_uintptr_t)TCR_PTR(b->bh.bb.bthr) & ~1) ==\n                   (kmp_uintptr_t)th); // clear possible mark\n\n  b->ql.blink = 0;\n\n  KC_TRACE(10, (\"__kmp_bget_enqueue: moving buffer to T#%d list\\n\",\n                __kmp_gtid_from_thread(th)));\n\n#if USE_CMP_XCHG_FOR_BGET\n  {\n    volatile void *old_value = TCR_PTR(th->th.th_local.bget_list);\n    /* the next pointer must be set before setting bget_list to buf to avoid\n       exposing a broken list to other threads, even for an instant. */\n    b->ql.flink = BFH(CCAST(void *, old_value));\n\n    while (!KMP_COMPARE_AND_STORE_PTR(&th->th.th_local.bget_list,\n                                      CCAST(void *, old_value), buf)) {\n      KMP_CPU_PAUSE();\n      old_value = TCR_PTR(th->th.th_local.bget_list);\n      /* the next pointer must be set before setting bget_list to buf to avoid\n         exposing a broken list to other threads, even for an instant. */\n      b->ql.flink = BFH(CCAST(void *, old_value));\n    }\n  }\n#else /* ! USE_CMP_XCHG_FOR_BGET */\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n  __kmp_acquire_lock(&th->th.th_local.bget_lock, rel_gtid);\n#else\n  __kmp_acquire_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif\n\n  b->ql.flink = BFH(th->th.th_local.bget_list);\n  th->th.th_local.bget_list = (void *)buf;\n\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n  __kmp_release_lock(&th->th.th_local.bget_lock, rel_gtid);\n#else\n  __kmp_release_bootstrap_lock(&th->th.th_local.bget_lock);\n#endif\n#endif /* USE_CMP_XCHG_FOR_BGET */\n}\n\n/* insert buffer back onto a new freelist */\nstatic void __kmp_bget_insert_into_freelist(thr_data_t *thr, bfhead_t *b) {\n  int bin;\n\n  KMP_DEBUG_ASSERT(((size_t)b) % SizeQuant == 0);\n  KMP_DEBUG_ASSERT(b->bh.bb.bsize % SizeQuant == 0);\n\n  bin = bget_get_bin(b->bh.bb.bsize);\n\n  KMP_DEBUG_ASSERT(thr->freelist[bin].ql.blink->ql.flink ==\n                   &thr->freelist[bin]);\n  KMP_DEBUG_ASSERT(thr->freelist[bin].ql.flink->ql.blink ==\n                   &thr->freelist[bin]);\n\n  b->ql.flink = &thr->freelist[bin];\n  b->ql.blink = thr->freelist[bin].ql.blink;\n\n  thr->freelist[bin].ql.blink = b;\n  b->ql.blink->ql.flink = b;\n}\n\n/* unlink the buffer from the old freelist */\nstatic void __kmp_bget_remove_from_freelist(bfhead_t *b) {\n  KMP_DEBUG_ASSERT(b->ql.blink->ql.flink == b);\n  KMP_DEBUG_ASSERT(b->ql.flink->ql.blink == b);\n\n  b->ql.blink->ql.flink = b->ql.flink;\n  b->ql.flink->ql.blink = b->ql.blink;\n}\n\n/*  GET STATS -- check info on free list */\nstatic void bcheck(kmp_info_t *th, bufsize *max_free, bufsize *total_free) {\n  thr_data_t *thr = get_thr_data(th);\n  int bin;\n\n  *total_free = *max_free = 0;\n\n  for (bin = 0; bin < MAX_BGET_BINS; ++bin) {\n    bfhead_t *b, *best;\n\n    best = &thr->freelist[bin];\n    b = best->ql.flink;\n\n    while (b != &thr->freelist[bin]) {\n      *total_free += (b->bh.bb.bsize - sizeof(bhead_t));\n      if ((best == &thr->freelist[bin]) || (b->bh.bb.bsize < best->bh.bb.bsize))\n        best = b;\n\n      /* Link to next buffer */\n      b = b->ql.flink;\n    }\n\n    if (*max_free < best->bh.bb.bsize)\n      *max_free = best->bh.bb.bsize;\n  }\n\n  if (*max_free > (bufsize)sizeof(bhead_t))\n    *max_free -= sizeof(bhead_t);\n}\n\n/*  BGET  --  Allocate a buffer.  */\nstatic void *bget(kmp_info_t *th, bufsize requested_size) {\n  thr_data_t *thr = get_thr_data(th);\n  bufsize size = requested_size;\n  bfhead_t *b;\n  void *buf;\n  int compactseq = 0;\n  int use_blink = 0;\n  /* For BestFit */\n  bfhead_t *best;\n\n  if (size < 0 || size + sizeof(bhead_t) > MaxSize) {\n    return NULL;\n  }\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n  if (size < (bufsize)SizeQ) { // Need at least room for the queue links.\n    size = SizeQ;\n  }\n#if defined(SizeQuant) && (SizeQuant > 1)\n  size = (size + (SizeQuant - 1)) & (~(SizeQuant - 1));\n#endif\n\n  size += sizeof(bhead_t); // Add overhead in allocated buffer to size required.\n  KMP_DEBUG_ASSERT(size >= 0);\n  KMP_DEBUG_ASSERT(size % SizeQuant == 0);\n\n  use_blink = (thr->mode == bget_mode_lifo);\n\n  /* If a compact function was provided in the call to bectl(), wrap\n     a loop around the allocation process  to  allow  compaction  to\n     intervene in case we don't find a suitable buffer in the chain. */\n\n  for (;;) {\n    int bin;\n\n    for (bin = bget_get_bin(size); bin < MAX_BGET_BINS; ++bin) {\n      /* Link to next buffer */\n      b = (use_blink ? thr->freelist[bin].ql.blink\n                     : thr->freelist[bin].ql.flink);\n\n      if (thr->mode == bget_mode_best) {\n        best = &thr->freelist[bin];\n\n        /* Scan the free list searching for the first buffer big enough\n           to hold the requested size buffer. */\n        while (b != &thr->freelist[bin]) {\n          if (b->bh.bb.bsize >= (bufsize)size) {\n            if ((best == &thr->freelist[bin]) ||\n                (b->bh.bb.bsize < best->bh.bb.bsize)) {\n              best = b;\n            }\n          }\n\n          /* Link to next buffer */\n          b = (use_blink ? b->ql.blink : b->ql.flink);\n        }\n        b = best;\n      }\n\n      while (b != &thr->freelist[bin]) {\n        if ((bufsize)b->bh.bb.bsize >= (bufsize)size) {\n\n          // Buffer is big enough to satisfy the request. Allocate it to the\n          // caller. We must decide whether the buffer is large enough to split\n          // into the part given to the caller and a free buffer that remains\n          // on the free list, or whether the entire buffer should be removed\n          // from the free list and given to the caller in its entirety. We\n          // only split the buffer if enough room remains for a header plus the\n          // minimum quantum of allocation.\n          if ((b->bh.bb.bsize - (bufsize)size) >\n              (bufsize)(SizeQ + (sizeof(bhead_t)))) {\n            bhead_t *ba, *bn;\n\n            ba = BH(((char *)b) + (b->bh.bb.bsize - (bufsize)size));\n            bn = BH(((char *)ba) + size);\n\n            KMP_DEBUG_ASSERT(bn->bb.prevfree == b->bh.bb.bsize);\n\n            /* Subtract size from length of free block. */\n            b->bh.bb.bsize -= (bufsize)size;\n\n            /* Link allocated buffer to the previous free buffer. */\n            ba->bb.prevfree = b->bh.bb.bsize;\n\n            /* Plug negative size into user buffer. */\n            ba->bb.bsize = -size;\n\n            /* Mark this buffer as owned by this thread. */\n            TCW_PTR(ba->bb.bthr,\n                    th); // not an allocated address (do not mark it)\n            /* Mark buffer after this one not preceded by free block. */\n            bn->bb.prevfree = 0;\n\n            // unlink buffer from old freelist, and reinsert into new freelist\n            __kmp_bget_remove_from_freelist(b);\n            __kmp_bget_insert_into_freelist(thr, b);\n#if BufStats\n            thr->totalloc += (size_t)size;\n            thr->numget++; /* Increment number of bget() calls */\n#endif\n            buf = (void *)((((char *)ba) + sizeof(bhead_t)));\n            KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n            return buf;\n          } else {\n            bhead_t *ba;\n\n            ba = BH(((char *)b) + b->bh.bb.bsize);\n\n            KMP_DEBUG_ASSERT(ba->bb.prevfree == b->bh.bb.bsize);\n\n            /* The buffer isn't big enough to split.  Give  the  whole\n               shebang to the caller and remove it from the free list. */\n\n            __kmp_bget_remove_from_freelist(b);\n#if BufStats\n            thr->totalloc += (size_t)b->bh.bb.bsize;\n            thr->numget++; /* Increment number of bget() calls */\n#endif\n            /* Negate size to mark buffer allocated. */\n            b->bh.bb.bsize = -(b->bh.bb.bsize);\n\n            /* Mark this buffer as owned by this thread. */\n            TCW_PTR(ba->bb.bthr, th); // not an allocated address (do not mark)\n            /* Zero the back pointer in the next buffer in memory\n               to indicate that this buffer is allocated. */\n            ba->bb.prevfree = 0;\n\n            /* Give user buffer starting at queue links. */\n            buf = (void *)&(b->ql);\n            KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n            return buf;\n          }\n        }\n\n        /* Link to next buffer */\n        b = (use_blink ? b->ql.blink : b->ql.flink);\n      }\n    }\n\n    /* We failed to find a buffer. If there's a compact function defined,\n       notify it of the size requested. If it returns TRUE, try the allocation\n       again. */\n\n    if ((thr->compfcn == 0) || (!(*thr->compfcn)(size, ++compactseq))) {\n      break;\n    }\n  }\n\n  /* No buffer available with requested size free. */\n\n  /* Don't give up yet -- look in the reserve supply. */\n  if (thr->acqfcn != 0) {\n    if (size > (bufsize)(thr->exp_incr - sizeof(bhead_t))) {\n      /* Request is too large to fit in a single expansion block.\n         Try to satisy it by a direct buffer acquisition. */\n      bdhead_t *bdh;\n\n      size += sizeof(bdhead_t) - sizeof(bhead_t);\n\n      KE_TRACE(10, (\"%%%%%% MALLOC( %d )\\n\", (int)size));\n\n      /* richryan */\n      bdh = BDH((*thr->acqfcn)((bufsize)size));\n      if (bdh != NULL) {\n\n        // Mark the buffer special by setting size field of its header to zero.\n        bdh->bh.bb.bsize = 0;\n\n        /* Mark this buffer as owned by this thread. */\n        TCW_PTR(bdh->bh.bb.bthr, th); // don't mark buffer as allocated,\n        // because direct buffer never goes to free list\n        bdh->bh.bb.prevfree = 0;\n        bdh->tsize = size;\n#if BufStats\n        thr->totalloc += (size_t)size;\n        thr->numget++; /* Increment number of bget() calls */\n        thr->numdget++; /* Direct bget() call count */\n#endif\n        buf = (void *)(bdh + 1);\n        KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n        return buf;\n      }\n\n    } else {\n\n      /*  Try to obtain a new expansion block */\n      void *newpool;\n\n      KE_TRACE(10, (\"%%%%%% MALLOCB( %d )\\n\", (int)thr->exp_incr));\n\n      /* richryan */\n      newpool = (*thr->acqfcn)((bufsize)thr->exp_incr);\n      KMP_DEBUG_ASSERT(((size_t)newpool) % SizeQuant == 0);\n      if (newpool != NULL) {\n        bpool(th, newpool, thr->exp_incr);\n        buf = bget(\n            th, requested_size); /* This can't, I say, can't get into a loop. */\n        return buf;\n      }\n    }\n  }\n\n  /*  Still no buffer available */\n\n  return NULL;\n}\n\n/*  BGETZ  --  Allocate a buffer and clear its contents to zero.  We clear\n               the  entire  contents  of  the buffer to zero, not just the\n               region requested by the caller. */\n\nstatic void *bgetz(kmp_info_t *th, bufsize size) {\n  char *buf = (char *)bget(th, size);\n\n  if (buf != NULL) {\n    bhead_t *b;\n    bufsize rsize;\n\n    b = BH(buf - sizeof(bhead_t));\n    rsize = -(b->bb.bsize);\n    if (rsize == 0) {\n      bdhead_t *bd;\n\n      bd = BDH(buf - sizeof(bdhead_t));\n      rsize = bd->tsize - (bufsize)sizeof(bdhead_t);\n    } else {\n      rsize -= sizeof(bhead_t);\n    }\n\n    KMP_DEBUG_ASSERT(rsize >= size);\n\n    (void)memset(buf, 0, (bufsize)rsize);\n  }\n  return ((void *)buf);\n}\n\n/*  BGETR  --  Reallocate a buffer.  This is a minimal implementation,\n               simply in terms of brel()  and  bget().   It  could  be\n               enhanced to allow the buffer to grow into adjacent free\n               blocks and to avoid moving data unnecessarily.  */\n\nstatic void *bgetr(kmp_info_t *th, void *buf, bufsize size) {\n  void *nbuf;\n  bufsize osize; /* Old size of buffer */\n  bhead_t *b;\n\n  nbuf = bget(th, size);\n  if (nbuf == NULL) { /* Acquire new buffer */\n    return NULL;\n  }\n  if (buf == NULL) {\n    return nbuf;\n  }\n  b = BH(((char *)buf) - sizeof(bhead_t));\n  osize = -b->bb.bsize;\n  if (osize == 0) {\n    /*  Buffer acquired directly through acqfcn. */\n    bdhead_t *bd;\n\n    bd = BDH(((char *)buf) - sizeof(bdhead_t));\n    osize = bd->tsize - (bufsize)sizeof(bdhead_t);\n  } else {\n    osize -= sizeof(bhead_t);\n  }\n\n  KMP_DEBUG_ASSERT(osize > 0);\n\n  (void)KMP_MEMCPY((char *)nbuf, (char *)buf, /* Copy the data */\n                   (size_t)((size < osize) ? size : osize));\n  brel(th, buf);\n\n  return nbuf;\n}\n\n/*  BREL  --  Release a buffer.  */\nstatic void brel(kmp_info_t *th, void *buf) {\n  thr_data_t *thr = get_thr_data(th);\n  bfhead_t *b, *bn;\n  kmp_info_t *bth;\n\n  KMP_DEBUG_ASSERT(buf != NULL);\n  KMP_DEBUG_ASSERT(((size_t)buf) % SizeQuant == 0);\n\n  b = BFH(((char *)buf) - sizeof(bhead_t));\n\n  if (b->bh.bb.bsize == 0) { /* Directly-acquired buffer? */\n    bdhead_t *bdh;\n\n    bdh = BDH(((char *)buf) - sizeof(bdhead_t));\n    KMP_DEBUG_ASSERT(b->bh.bb.prevfree == 0);\n#if BufStats\n    thr->totalloc -= (size_t)bdh->tsize;\n    thr->numdrel++; /* Number of direct releases */\n    thr->numrel++; /* Increment number of brel() calls */\n#endif /* BufStats */\n#ifdef FreeWipe\n    (void)memset((char *)buf, 0x55, (size_t)(bdh->tsize - sizeof(bdhead_t)));\n#endif /* FreeWipe */\n\n    KE_TRACE(10, (\"%%%%%% FREE( %p )\\n\", (void *)bdh));\n\n    KMP_DEBUG_ASSERT(thr->relfcn != 0);\n    (*thr->relfcn)((void *)bdh); /* Release it directly. */\n    return;\n  }\n\n  bth = (kmp_info_t *)((kmp_uintptr_t)TCR_PTR(b->bh.bb.bthr) &\n                       ~1); // clear possible mark before comparison\n  if (bth != th) {\n    /* Add this buffer to be released by the owning thread later */\n    __kmp_bget_enqueue(bth, buf\n#ifdef USE_QUEUING_LOCK_FOR_BGET\n                       ,\n                       __kmp_gtid_from_thread(th)\n#endif\n                           );\n    return;\n  }\n\n  /* Buffer size must be negative, indicating that the buffer is allocated. */\n  if (b->bh.bb.bsize >= 0) {\n    bn = NULL;\n  }\n  KMP_DEBUG_ASSERT(b->bh.bb.bsize < 0);\n\n  /*  Back pointer in next buffer must be zero, indicating the same thing: */\n\n  KMP_DEBUG_ASSERT(BH((char *)b - b->bh.bb.bsize)->bb.prevfree == 0);\n\n#if BufStats\n  thr->numrel++; /* Increment number of brel() calls */\n  thr->totalloc += (size_t)b->bh.bb.bsize;\n#endif\n\n  /* If the back link is nonzero, the previous buffer is free.  */\n\n  if (b->bh.bb.prevfree != 0) {\n    /* The previous buffer is free. Consolidate this buffer with it by adding\n       the length of this buffer to the previous free buffer. Note that we\n       subtract the size in the buffer being released, since it's negative to\n       indicate that the buffer is allocated. */\n    bufsize size = b->bh.bb.bsize;\n\n    /* Make the previous buffer the one we're working on. */\n    KMP_DEBUG_ASSERT(BH((char *)b - b->bh.bb.prevfree)->bb.bsize ==\n                     b->bh.bb.prevfree);\n    b = BFH(((char *)b) - b->bh.bb.prevfree);\n    b->bh.bb.bsize -= size;\n\n    /* unlink the buffer from the old freelist */\n    __kmp_bget_remove_from_freelist(b);\n  } else {\n    /* The previous buffer isn't allocated. Mark this buffer size as positive\n       (i.e. free) and fall through to place the buffer on the free list as an\n       isolated free block. */\n    b->bh.bb.bsize = -b->bh.bb.bsize;\n  }\n\n  /* insert buffer back onto a new freelist */\n  __kmp_bget_insert_into_freelist(thr, b);\n\n  /* Now we look at the next buffer in memory, located by advancing from\n     the  start  of  this  buffer  by its size, to see if that buffer is\n     free.  If it is, we combine  this  buffer  with  the  next  one  in\n     memory, dechaining the second buffer from the free list. */\n  bn = BFH(((char *)b) + b->bh.bb.bsize);\n  if (bn->bh.bb.bsize > 0) {\n\n    /* The buffer is free.  Remove it from the free list and add\n       its size to that of our buffer. */\n    KMP_DEBUG_ASSERT(BH((char *)bn + bn->bh.bb.bsize)->bb.prevfree ==\n                     bn->bh.bb.bsize);\n\n    __kmp_bget_remove_from_freelist(bn);\n\n    b->bh.bb.bsize += bn->bh.bb.bsize;\n\n    /* unlink the buffer from the old freelist, and reinsert it into the new\n     * freelist */\n    __kmp_bget_remove_from_freelist(b);\n    __kmp_bget_insert_into_freelist(thr, b);\n\n    /* Finally,  advance  to   the  buffer  that   follows  the  newly\n       consolidated free block.  We must set its  backpointer  to  the\n       head  of  the  consolidated free block.  We know the next block\n       must be an allocated block because the process of recombination\n       guarantees  that  two  free  blocks will never be contiguous in\n       memory.  */\n    bn = BFH(((char *)b) + b->bh.bb.bsize);\n  }\n#ifdef FreeWipe\n  (void)memset(((char *)b) + sizeof(bfhead_t), 0x55,\n               (size_t)(b->bh.bb.bsize - sizeof(bfhead_t)));\n#endif\n  KMP_DEBUG_ASSERT(bn->bh.bb.bsize < 0);\n\n  /* The next buffer is allocated.  Set the backpointer in it  to  point\n     to this buffer; the previous free buffer in memory. */\n\n  bn->bh.bb.prevfree = b->bh.bb.bsize;\n\n  /*  If  a  block-release function is defined, and this free buffer\n      constitutes the entire block, release it.  Note that  pool_len\n      is  defined  in  such a way that the test will fail unless all\n      pool blocks are the same size.  */\n  if (thr->relfcn != 0 &&\n      b->bh.bb.bsize == (bufsize)(thr->pool_len - sizeof(bhead_t))) {\n#if BufStats\n    if (thr->numpblk !=\n        1) { /* Do not release the last buffer until finalization time */\n#endif\n\n      KMP_DEBUG_ASSERT(b->bh.bb.prevfree == 0);\n      KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.bsize == ESent);\n      KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.prevfree ==\n                       b->bh.bb.bsize);\n\n      /*  Unlink the buffer from the free list  */\n      __kmp_bget_remove_from_freelist(b);\n\n      KE_TRACE(10, (\"%%%%%% FREE( %p )\\n\", (void *)b));\n\n      (*thr->relfcn)(b);\n#if BufStats\n      thr->numprel++; /* Nr of expansion block releases */\n      thr->numpblk--; /* Total number of blocks */\n      KMP_DEBUG_ASSERT(thr->numpblk == thr->numpget - thr->numprel);\n\n      // avoid leaving stale last_pool pointer around if it is being dealloced\n      if (thr->last_pool == b)\n        thr->last_pool = 0;\n    } else {\n      thr->last_pool = b;\n    }\n#endif /* BufStats */\n  }\n}\n\n/*  BECTL  --  Establish automatic pool expansion control  */\nstatic void bectl(kmp_info_t *th, bget_compact_t compact,\n                  bget_acquire_t acquire, bget_release_t release,\n                  bufsize pool_incr) {\n  thr_data_t *thr = get_thr_data(th);\n\n  thr->compfcn = compact;\n  thr->acqfcn = acquire;\n  thr->relfcn = release;\n  thr->exp_incr = pool_incr;\n}\n\n/*  BPOOL  --  Add a region of memory to the buffer pool.  */\nstatic void bpool(kmp_info_t *th, void *buf, bufsize len) {\n  /*    int bin = 0; */\n  thr_data_t *thr = get_thr_data(th);\n  bfhead_t *b = BFH(buf);\n  bhead_t *bn;\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n#ifdef SizeQuant\n  len &= ~(SizeQuant - 1);\n#endif\n  if (thr->pool_len == 0) {\n    thr->pool_len = len;\n  } else if (len != thr->pool_len) {\n    thr->pool_len = -1;\n  }\n#if BufStats\n  thr->numpget++; /* Number of block acquisitions */\n  thr->numpblk++; /* Number of blocks total */\n  KMP_DEBUG_ASSERT(thr->numpblk == thr->numpget - thr->numprel);\n#endif /* BufStats */\n\n  /* Since the block is initially occupied by a single free  buffer,\n     it  had  better  not  be  (much) larger than the largest buffer\n     whose size we can store in bhead.bb.bsize. */\n  KMP_DEBUG_ASSERT(len - sizeof(bhead_t) <= -((bufsize)ESent + 1));\n\n  /* Clear  the  backpointer at  the start of the block to indicate that\n     there  is  no  free  block  prior  to  this   one.    That   blocks\n     recombination when the first block in memory is released. */\n  b->bh.bb.prevfree = 0;\n\n  /* Create a dummy allocated buffer at the end of the pool.  This dummy\n     buffer is seen when a buffer at the end of the pool is released and\n     blocks  recombination  of  the last buffer with the dummy buffer at\n     the end.  The length in the dummy buffer  is  set  to  the  largest\n     negative  number  to  denote  the  end  of  the pool for diagnostic\n     routines (this specific value is  not  counted  on  by  the  actual\n     allocation and release functions). */\n  len -= sizeof(bhead_t);\n  b->bh.bb.bsize = (bufsize)len;\n  /* Set the owner of this buffer */\n  TCW_PTR(b->bh.bb.bthr,\n          (kmp_info_t *)((kmp_uintptr_t)th |\n                         1)); // mark the buffer as allocated address\n\n  /* Chain the new block to the free list. */\n  __kmp_bget_insert_into_freelist(thr, b);\n\n#ifdef FreeWipe\n  (void)memset(((char *)b) + sizeof(bfhead_t), 0x55,\n               (size_t)(len - sizeof(bfhead_t)));\n#endif\n  bn = BH(((char *)b) + len);\n  bn->bb.prevfree = (bufsize)len;\n  /* Definition of ESent assumes two's complement! */\n  KMP_DEBUG_ASSERT((~0) == -1 && (bn != 0));\n\n  bn->bb.bsize = ESent;\n}\n\n/*  BFREED  --  Dump the free lists for this thread. */\nstatic void bfreed(kmp_info_t *th) {\n  int bin = 0, count = 0;\n  int gtid = __kmp_gtid_from_thread(th);\n  thr_data_t *thr = get_thr_data(th);\n\n#if BufStats\n  __kmp_printf_no_lock(\"__kmp_printpool: T#%d total=%\" KMP_UINT64_SPEC\n                       \" get=%\" KMP_INT64_SPEC \" rel=%\" KMP_INT64_SPEC\n                       \" pblk=%\" KMP_INT64_SPEC \" pget=%\" KMP_INT64_SPEC\n                       \" prel=%\" KMP_INT64_SPEC \" dget=%\" KMP_INT64_SPEC\n                       \" drel=%\" KMP_INT64_SPEC \"\\n\",\n                       gtid, (kmp_uint64)thr->totalloc, (kmp_int64)thr->numget,\n                       (kmp_int64)thr->numrel, (kmp_int64)thr->numpblk,\n                       (kmp_int64)thr->numpget, (kmp_int64)thr->numprel,\n                       (kmp_int64)thr->numdget, (kmp_int64)thr->numdrel);\n#endif\n\n  for (bin = 0; bin < MAX_BGET_BINS; ++bin) {\n    bfhead_t *b;\n\n    for (b = thr->freelist[bin].ql.flink; b != &thr->freelist[bin];\n         b = b->ql.flink) {\n      bufsize bs = b->bh.bb.bsize;\n\n      KMP_DEBUG_ASSERT(b->ql.blink->ql.flink == b);\n      KMP_DEBUG_ASSERT(b->ql.flink->ql.blink == b);\n      KMP_DEBUG_ASSERT(bs > 0);\n\n      count += 1;\n\n      __kmp_printf_no_lock(\n          \"__kmp_printpool: T#%d Free block: 0x%p size %6ld bytes.\\n\", gtid, b,\n          (long)bs);\n#ifdef FreeWipe\n      {\n        char *lerr = ((char *)b) + sizeof(bfhead_t);\n        if ((bs > sizeof(bfhead_t)) &&\n            ((*lerr != 0x55) ||\n             (memcmp(lerr, lerr + 1, (size_t)(bs - (sizeof(bfhead_t) + 1))) !=\n              0))) {\n          __kmp_printf_no_lock(\"__kmp_printpool: T#%d     (Contents of above \"\n                               \"free block have been overstored.)\\n\",\n                               gtid);\n        }\n      }\n#endif\n    }\n  }\n\n  if (count == 0)\n    __kmp_printf_no_lock(\"__kmp_printpool: T#%d No free blocks\\n\", gtid);\n}\n\nvoid __kmp_initialize_bget(kmp_info_t *th) {\n  KMP_DEBUG_ASSERT(SizeQuant >= sizeof(void *) && (th != 0));\n\n  set_thr_data(th);\n\n  bectl(th, (bget_compact_t)0, (bget_acquire_t)malloc, (bget_release_t)free,\n        (bufsize)__kmp_malloc_pool_incr);\n}\n\nvoid __kmp_finalize_bget(kmp_info_t *th) {\n  thr_data_t *thr;\n  bfhead_t *b;\n\n  KMP_DEBUG_ASSERT(th != 0);\n\n#if BufStats\n  thr = (thr_data_t *)th->th.th_local.bget_data;\n  KMP_DEBUG_ASSERT(thr != NULL);\n  b = thr->last_pool;\n\n  /*  If a block-release function is defined, and this free buffer constitutes\n      the entire block, release it. Note that pool_len is defined in such a way\n      that the test will fail unless all pool blocks are the same size.  */\n\n  // Deallocate the last pool if one exists because we no longer do it in brel()\n  if (thr->relfcn != 0 && b != 0 && thr->numpblk != 0 &&\n      b->bh.bb.bsize == (bufsize)(thr->pool_len - sizeof(bhead_t))) {\n    KMP_DEBUG_ASSERT(b->bh.bb.prevfree == 0);\n    KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.bsize == ESent);\n    KMP_DEBUG_ASSERT(BH((char *)b + b->bh.bb.bsize)->bb.prevfree ==\n                     b->bh.bb.bsize);\n\n    /*  Unlink the buffer from the free list  */\n    __kmp_bget_remove_from_freelist(b);\n\n    KE_TRACE(10, (\"%%%%%% FREE( %p )\\n\", (void *)b));\n\n    (*thr->relfcn)(b);\n    thr->numprel++; /* Nr of expansion block releases */\n    thr->numpblk--; /* Total number of blocks */\n    KMP_DEBUG_ASSERT(thr->numpblk == thr->numpget - thr->numprel);\n  }\n#endif /* BufStats */\n\n  /* Deallocate bget_data */\n  if (th->th.th_local.bget_data != NULL) {\n    __kmp_free(th->th.th_local.bget_data);\n    th->th.th_local.bget_data = NULL;\n  }\n}\n\nvoid kmpc_set_poolsize(size_t size) {\n  bectl(__kmp_get_thread(), (bget_compact_t)0, (bget_acquire_t)malloc,\n        (bget_release_t)free, (bufsize)size);\n}\n\nsize_t kmpc_get_poolsize(void) {\n  thr_data_t *p;\n\n  p = get_thr_data(__kmp_get_thread());\n\n  return p->exp_incr;\n}\n\nvoid kmpc_set_poolmode(int mode) {\n  thr_data_t *p;\n\n  if (mode == bget_mode_fifo || mode == bget_mode_lifo ||\n      mode == bget_mode_best) {\n    p = get_thr_data(__kmp_get_thread());\n    p->mode = (bget_mode_t)mode;\n  }\n}\n\nint kmpc_get_poolmode(void) {\n  thr_data_t *p;\n\n  p = get_thr_data(__kmp_get_thread());\n\n  return p->mode;\n}\n\nvoid kmpc_get_poolstat(size_t *maxmem, size_t *allmem) {\n  kmp_info_t *th = __kmp_get_thread();\n  bufsize a, b;\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n  bcheck(th, &a, &b);\n\n  *maxmem = a;\n  *allmem = b;\n}\n\nvoid kmpc_poolprint(void) {\n  kmp_info_t *th = __kmp_get_thread();\n\n  __kmp_bget_dequeue(th); /* Release any queued buffers */\n\n  bfreed(th);\n}\n\n#endif // #if KMP_USE_BGET\n\nvoid *kmpc_malloc(size_t size) {\n  void *ptr;\n  ptr = bget(__kmp_entry_thread(), (bufsize)(size + sizeof(ptr)));\n  if (ptr != NULL) {\n    // save allocated pointer just before one returned to user\n    *(void **)ptr = ptr;\n    ptr = (void **)ptr + 1;\n  }\n  return ptr;\n}\n\n#define IS_POWER_OF_TWO(n) (((n) & ((n)-1)) == 0)\n\nvoid *kmpc_aligned_malloc(size_t size, size_t alignment) {\n  void *ptr;\n  void *ptr_allocated;\n  KMP_DEBUG_ASSERT(alignment < 32 * 1024); // Alignment should not be too big\n  if (!IS_POWER_OF_TWO(alignment)) {\n    // AC: do we need to issue a warning here?\n    errno = EINVAL;\n    return NULL;\n  }\n  size = size + sizeof(void *) + alignment;\n  ptr_allocated = bget(__kmp_entry_thread(), (bufsize)size);\n  if (ptr_allocated != NULL) {\n    // save allocated pointer just before one returned to user\n    ptr = (void *)(((kmp_uintptr_t)ptr_allocated + sizeof(void *) + alignment) &\n                   ~(alignment - 1));\n    *((void **)ptr - 1) = ptr_allocated;\n  } else {\n    ptr = NULL;\n  }\n  return ptr;\n}\n\nvoid *kmpc_calloc(size_t nelem, size_t elsize) {\n  void *ptr;\n  ptr = bgetz(__kmp_entry_thread(), (bufsize)(nelem * elsize + sizeof(ptr)));\n  if (ptr != NULL) {\n    // save allocated pointer just before one returned to user\n    *(void **)ptr = ptr;\n    ptr = (void **)ptr + 1;\n  }\n  return ptr;\n}\n\nvoid *kmpc_realloc(void *ptr, size_t size) {\n  void *result = NULL;\n  if (ptr == NULL) {\n    // If pointer is NULL, realloc behaves like malloc.\n    result = bget(__kmp_entry_thread(), (bufsize)(size + sizeof(ptr)));\n    // save allocated pointer just before one returned to user\n    if (result != NULL) {\n      *(void **)result = result;\n      result = (void **)result + 1;\n    }\n  } else if (size == 0) {\n    // If size is 0, realloc behaves like free.\n    // The thread must be registered by the call to kmpc_malloc() or\n    // kmpc_calloc() before.\n    // So it should be safe to call __kmp_get_thread(), not\n    // __kmp_entry_thread().\n    KMP_ASSERT(*((void **)ptr - 1));\n    brel(__kmp_get_thread(), *((void **)ptr - 1));\n  } else {\n    result = bgetr(__kmp_entry_thread(), *((void **)ptr - 1),\n                   (bufsize)(size + sizeof(ptr)));\n    if (result != NULL) {\n      *(void **)result = result;\n      result = (void **)result + 1;\n    }\n  }\n  return result;\n}\n\n// NOTE: the library must have already been initialized by a previous allocate\nvoid kmpc_free(void *ptr) {\n  if (!__kmp_init_serial) {\n    return;\n  }\n  if (ptr != NULL) {\n    kmp_info_t *th = __kmp_get_thread();\n    __kmp_bget_dequeue(th); /* Release any queued buffers */\n    // extract allocated pointer and free it\n    KMP_ASSERT(*((void **)ptr - 1));\n    brel(th, *((void **)ptr - 1));\n  }\n}\n\nvoid *___kmp_thread_malloc(kmp_info_t *th, size_t size KMP_SRC_LOC_DECL) {\n  void *ptr;\n  KE_TRACE(30, (\"-> __kmp_thread_malloc( %p, %d ) called from %s:%d\\n\", th,\n                (int)size KMP_SRC_LOC_PARM));\n  ptr = bget(th, (bufsize)size);\n  KE_TRACE(30, (\"<- __kmp_thread_malloc() returns %p\\n\", ptr));\n  return ptr;\n}\n\nvoid *___kmp_thread_calloc(kmp_info_t *th, size_t nelem,\n                           size_t elsize KMP_SRC_LOC_DECL) {\n  void *ptr;\n  KE_TRACE(30, (\"-> __kmp_thread_calloc( %p, %d, %d ) called from %s:%d\\n\", th,\n                (int)nelem, (int)elsize KMP_SRC_LOC_PARM));\n  ptr = bgetz(th, (bufsize)(nelem * elsize));\n  KE_TRACE(30, (\"<- __kmp_thread_calloc() returns %p\\n\", ptr));\n  return ptr;\n}\n\nvoid *___kmp_thread_realloc(kmp_info_t *th, void *ptr,\n                            size_t size KMP_SRC_LOC_DECL) {\n  KE_TRACE(30, (\"-> __kmp_thread_realloc( %p, %p, %d ) called from %s:%d\\n\", th,\n                ptr, (int)size KMP_SRC_LOC_PARM));\n  ptr = bgetr(th, ptr, (bufsize)size);\n  KE_TRACE(30, (\"<- __kmp_thread_realloc() returns %p\\n\", ptr));\n  return ptr;\n}\n\nvoid ___kmp_thread_free(kmp_info_t *th, void *ptr KMP_SRC_LOC_DECL) {\n  KE_TRACE(30, (\"-> __kmp_thread_free( %p, %p ) called from %s:%d\\n\", th,\n                ptr KMP_SRC_LOC_PARM));\n  if (ptr != NULL) {\n    __kmp_bget_dequeue(th); /* Release any queued buffers */\n    brel(th, ptr);\n  }\n  KE_TRACE(30, (\"<- __kmp_thread_free()\\n\"));\n}\n\n/* OMP 5.0 Memory Management support */\nstatic const char *kmp_mk_lib_name;\nstatic void *h_memkind;\n/* memkind experimental API: */\n// memkind_alloc\nstatic void *(*kmp_mk_alloc)(void *k, size_t sz);\n// memkind_free\nstatic void (*kmp_mk_free)(void *kind, void *ptr);\n// memkind_check_available\nstatic int (*kmp_mk_check)(void *kind);\n// kinds we are going to use\nstatic void **mk_default;\nstatic void **mk_interleave;\nstatic void **mk_hbw;\nstatic void **mk_hbw_interleave;\nstatic void **mk_hbw_preferred;\nstatic void **mk_hugetlb;\nstatic void **mk_hbw_hugetlb;\nstatic void **mk_hbw_preferred_hugetlb;\n\n#if KMP_OS_UNIX && KMP_DYNAMIC_LIB\nstatic inline void chk_kind(void ***pkind) {\n  KMP_DEBUG_ASSERT(pkind);\n  if (*pkind) // symbol found\n    if (kmp_mk_check(**pkind)) // kind not available or error\n      *pkind = NULL;\n}\n#endif\n\nvoid __kmp_init_memkind() {\n// as of 2018-07-31 memkind does not support Windows*, exclude it for now\n#if KMP_OS_UNIX && KMP_DYNAMIC_LIB\n  // use of statically linked memkind is problematic, as it depends on libnuma\n  kmp_mk_lib_name = \"libmemkind.so\";\n  h_memkind = dlopen(kmp_mk_lib_name, RTLD_LAZY);\n  if (h_memkind) {\n    kmp_mk_check = (int (*)(void *))dlsym(h_memkind, \"memkind_check_available\");\n    kmp_mk_alloc =\n        (void *(*)(void *, size_t))dlsym(h_memkind, \"memkind_malloc\");\n    kmp_mk_free = (void (*)(void *, void *))dlsym(h_memkind, \"memkind_free\");\n    mk_default = (void **)dlsym(h_memkind, \"MEMKIND_DEFAULT\");\n    if (kmp_mk_check && kmp_mk_alloc && kmp_mk_free && mk_default &&\n        !kmp_mk_check(*mk_default)) {\n      __kmp_memkind_available = 1;\n      mk_interleave = (void **)dlsym(h_memkind, \"MEMKIND_INTERLEAVE\");\n      chk_kind(&mk_interleave);\n      mk_hbw = (void **)dlsym(h_memkind, \"MEMKIND_HBW\");\n      chk_kind(&mk_hbw);\n      mk_hbw_interleave = (void **)dlsym(h_memkind, \"MEMKIND_HBW_INTERLEAVE\");\n      chk_kind(&mk_hbw_interleave);\n      mk_hbw_preferred = (void **)dlsym(h_memkind, \"MEMKIND_HBW_PREFERRED\");\n      chk_kind(&mk_hbw_preferred);\n      mk_hugetlb = (void **)dlsym(h_memkind, \"MEMKIND_HUGETLB\");\n      chk_kind(&mk_hugetlb);\n      mk_hbw_hugetlb = (void **)dlsym(h_memkind, \"MEMKIND_HBW_HUGETLB\");\n      chk_kind(&mk_hbw_hugetlb);\n      mk_hbw_preferred_hugetlb =\n          (void **)dlsym(h_memkind, \"MEMKIND_HBW_PREFERRED_HUGETLB\");\n      chk_kind(&mk_hbw_preferred_hugetlb);\n      KE_TRACE(25, (\"__kmp_init_memkind: memkind library initialized\\n\"));\n      return; // success\n    }\n    dlclose(h_memkind); // failure\n    h_memkind = NULL;\n  }\n  kmp_mk_check = NULL;\n  kmp_mk_alloc = NULL;\n  kmp_mk_free = NULL;\n  mk_default = NULL;\n  mk_interleave = NULL;\n  mk_hbw = NULL;\n  mk_hbw_interleave = NULL;\n  mk_hbw_preferred = NULL;\n  mk_hugetlb = NULL;\n  mk_hbw_hugetlb = NULL;\n  mk_hbw_preferred_hugetlb = NULL;\n#else\n  kmp_mk_lib_name = \"\";\n  h_memkind = NULL;\n  kmp_mk_check = NULL;\n  kmp_mk_alloc = NULL;\n  kmp_mk_free = NULL;\n  mk_default = NULL;\n  mk_interleave = NULL;\n  mk_hbw = NULL;\n  mk_hbw_interleave = NULL;\n  mk_hbw_preferred = NULL;\n  mk_hugetlb = NULL;\n  mk_hbw_hugetlb = NULL;\n  mk_hbw_preferred_hugetlb = NULL;\n#endif\n}\n\nvoid __kmp_fini_memkind() {\n#if KMP_OS_UNIX && KMP_DYNAMIC_LIB\n  if (__kmp_memkind_available)\n    KE_TRACE(25, (\"__kmp_fini_memkind: finalize memkind library\\n\"));\n  if (h_memkind) {\n    dlclose(h_memkind);\n    h_memkind = NULL;\n  }\n  kmp_mk_check = NULL;\n  kmp_mk_alloc = NULL;\n  kmp_mk_free = NULL;\n  mk_default = NULL;\n  mk_interleave = NULL;\n  mk_hbw = NULL;\n  mk_hbw_interleave = NULL;\n  mk_hbw_preferred = NULL;\n  mk_hugetlb = NULL;\n  mk_hbw_hugetlb = NULL;\n  mk_hbw_preferred_hugetlb = NULL;\n#endif\n}\n\nomp_allocator_handle_t __kmpc_init_allocator(int gtid, omp_memspace_handle_t ms,\n                                             int ntraits,\n                                             omp_alloctrait_t traits[]) {\n  // OpenMP 5.0 only allows predefined memspaces\n  KMP_DEBUG_ASSERT(ms == omp_default_mem_space || ms == omp_low_lat_mem_space ||\n                   ms == omp_large_cap_mem_space || ms == omp_const_mem_space ||\n                   ms == omp_high_bw_mem_space);\n  kmp_allocator_t *al;\n  int i;\n  al = (kmp_allocator_t *)__kmp_allocate(sizeof(kmp_allocator_t)); // zeroed\n  al->memspace = ms; // not used currently\n  for (i = 0; i < ntraits; ++i) {\n    switch (traits[i].key) {\n    case OMP_ATK_THREADMODEL:\n    case OMP_ATK_ACCESS:\n    case OMP_ATK_PINNED:\n      break;\n    case OMP_ATK_ALIGNMENT:\n      al->alignment = traits[i].value;\n      KMP_ASSERT(IS_POWER_OF_TWO(al->alignment));\n      break;\n    case OMP_ATK_POOL_SIZE:\n      al->pool_size = traits[i].value;\n      break;\n    case OMP_ATK_FALLBACK:\n      al->fb = (omp_alloctrait_value_t)traits[i].value;\n      KMP_DEBUG_ASSERT(\n          al->fb == OMP_ATV_DEFAULT_MEM_FB || al->fb == OMP_ATV_NULL_FB ||\n          al->fb == OMP_ATV_ABORT_FB || al->fb == OMP_ATV_ALLOCATOR_FB);\n      break;\n    case OMP_ATK_FB_DATA:\n      al->fb_data = RCAST(kmp_allocator_t *, traits[i].value);\n      break;\n    case OMP_ATK_PARTITION:\n      al->memkind = RCAST(void **, traits[i].value);\n      break;\n    default:\n      KMP_ASSERT2(0, \"Unexpected allocator trait\");\n    }\n  }\n  if (al->fb == 0) {\n    // set default allocator\n    al->fb = OMP_ATV_DEFAULT_MEM_FB;\n    al->fb_data = (kmp_allocator_t *)omp_default_mem_alloc;\n  } else if (al->fb == OMP_ATV_ALLOCATOR_FB) {\n    KMP_ASSERT(al->fb_data != NULL);\n  } else if (al->fb == OMP_ATV_DEFAULT_MEM_FB) {\n    al->fb_data = (kmp_allocator_t *)omp_default_mem_alloc;\n  }\n  if (__kmp_memkind_available) {\n    // Let's use memkind library if available\n    if (ms == omp_high_bw_mem_space) {\n      if (al->memkind == (void *)OMP_ATV_INTERLEAVED && mk_hbw_interleave) {\n        al->memkind = mk_hbw_interleave;\n      } else if (mk_hbw_preferred) {\n        // AC: do not try to use MEMKIND_HBW for now, because memkind library\n        // cannot reliably detect exhaustion of HBW memory.\n        // It could be possible using hbw_verify_memory_region() but memkind\n        // manual says: \"Using this function in production code may result in\n        // serious performance penalty\".\n        al->memkind = mk_hbw_preferred;\n      } else {\n        // HBW is requested but not available --> return NULL allocator\n        __kmp_free(al);\n        return omp_null_allocator;\n      }\n    } else {\n      if (al->memkind == (void *)OMP_ATV_INTERLEAVED && mk_interleave) {\n        al->memkind = mk_interleave;\n      } else {\n        al->memkind = mk_default;\n      }\n    }\n  } else {\n    if (ms == omp_high_bw_mem_space) {\n      // cannot detect HBW memory presence without memkind library\n      __kmp_free(al);\n      return omp_null_allocator;\n    }\n  }\n  return (omp_allocator_handle_t)al;\n}\n\nvoid __kmpc_destroy_allocator(int gtid, omp_allocator_handle_t allocator) {\n  if (allocator > kmp_max_mem_alloc)\n    __kmp_free(allocator);\n}\n\nvoid __kmpc_set_default_allocator(int gtid, omp_allocator_handle_t allocator) {\n  if (allocator == omp_null_allocator)\n    allocator = omp_default_mem_alloc;\n  __kmp_threads[gtid]->th.th_def_allocator = allocator;\n}\n\nomp_allocator_handle_t __kmpc_get_default_allocator(int gtid) {\n  return __kmp_threads[gtid]->th.th_def_allocator;\n}\n\ntypedef struct kmp_mem_desc { // Memory block descriptor\n  void *ptr_alloc; // Pointer returned by allocator\n  size_t size_a; // Size of allocated memory block (initial+descriptor+align)\n  void *ptr_align; // Pointer to aligned memory, returned\n  kmp_allocator_t *allocator; // allocator\n} kmp_mem_desc_t;\nstatic int alignment = sizeof(void *); // let's align to pointer size\n\nvoid *__kmpc_alloc(int gtid, size_t size, omp_allocator_handle_t allocator) {\n  void *ptr = NULL;\n  kmp_allocator_t *al;\n  KMP_DEBUG_ASSERT(__kmp_init_serial);\n  if (allocator == omp_null_allocator)\n    allocator = __kmp_threads[gtid]->th.th_def_allocator;\n\n  KE_TRACE(25, (\"__kmpc_alloc: T#%d (%d, %p)\\n\", gtid, (int)size, allocator));\n  al = RCAST(kmp_allocator_t *, CCAST(omp_allocator_handle_t, allocator));\n\n  int sz_desc = sizeof(kmp_mem_desc_t);\n  kmp_mem_desc_t desc;\n  kmp_uintptr_t addr; // address returned by allocator\n  kmp_uintptr_t addr_align; // address to return to caller\n  kmp_uintptr_t addr_descr; // address of memory block descriptor\n  int align = alignment; // default alignment\n  if (allocator > kmp_max_mem_alloc && al->alignment > 0) {\n    align = al->alignment; // alignment requested by user\n  }\n  desc.size_a = size + sz_desc + align;\n\n  if (__kmp_memkind_available) {\n    if (allocator < kmp_max_mem_alloc) {\n      // pre-defined allocator\n      if (allocator == omp_high_bw_mem_alloc && mk_hbw_preferred) {\n        ptr = kmp_mk_alloc(*mk_hbw_preferred, desc.size_a);\n      } else {\n        ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n      }\n    } else if (al->pool_size > 0) {\n      // custom allocator with pool size requested\n      kmp_uint64 used =\n          KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, desc.size_a);\n      if (used + desc.size_a > al->pool_size) {\n        // not enough space, need to go fallback path\n        KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n        if (al->fb == OMP_ATV_DEFAULT_MEM_FB) {\n          al = (kmp_allocator_t *)omp_default_mem_alloc;\n          ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n        } else if (al->fb == OMP_ATV_ABORT_FB) {\n          KMP_ASSERT(0); // abort fallback requested\n        } else if (al->fb == OMP_ATV_ALLOCATOR_FB) {\n          KMP_ASSERT(al != al->fb_data);\n          al = al->fb_data;\n          return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n        } // else ptr == NULL;\n      } else {\n        // pool has enough space\n        ptr = kmp_mk_alloc(*al->memkind, desc.size_a);\n        if (ptr == NULL) {\n          if (al->fb == OMP_ATV_DEFAULT_MEM_FB) {\n            al = (kmp_allocator_t *)omp_default_mem_alloc;\n            ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n          } else if (al->fb == OMP_ATV_ABORT_FB) {\n            KMP_ASSERT(0); // abort fallback requested\n          } else if (al->fb == OMP_ATV_ALLOCATOR_FB) {\n            KMP_ASSERT(al != al->fb_data);\n            al = al->fb_data;\n            return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n          }\n        }\n      }\n    } else {\n      // custom allocator, pool size not requested\n      ptr = kmp_mk_alloc(*al->memkind, desc.size_a);\n      if (ptr == NULL) {\n        if (al->fb == OMP_ATV_DEFAULT_MEM_FB) {\n          al = (kmp_allocator_t *)omp_default_mem_alloc;\n          ptr = kmp_mk_alloc(*mk_default, desc.size_a);\n        } else if (al->fb == OMP_ATV_ABORT_FB) {\n          KMP_ASSERT(0); // abort fallback requested\n        } else if (al->fb == OMP_ATV_ALLOCATOR_FB) {\n          KMP_ASSERT(al != al->fb_data);\n          al = al->fb_data;\n          return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n        }\n      }\n    }\n  } else if (allocator < kmp_max_mem_alloc) {\n    // pre-defined allocator\n    if (allocator == omp_high_bw_mem_alloc) {\n      // ptr = NULL;\n    } else {\n      ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n    }\n  } else if (al->pool_size > 0) {\n    // custom allocator with pool size requested\n    kmp_uint64 used =\n        KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, desc.size_a);\n    if (used + desc.size_a > al->pool_size) {\n      // not enough space, need to go fallback path\n      KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n      if (al->fb == OMP_ATV_DEFAULT_MEM_FB) {\n        al = (kmp_allocator_t *)omp_default_mem_alloc;\n        ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n      } else if (al->fb == OMP_ATV_ABORT_FB) {\n        KMP_ASSERT(0); // abort fallback requested\n      } else if (al->fb == OMP_ATV_ALLOCATOR_FB) {\n        KMP_ASSERT(al != al->fb_data);\n        al = al->fb_data;\n        return __kmpc_alloc(gtid, size, (omp_allocator_handle_t)al);\n      } // else ptr == NULL;\n    } else {\n      // pool has enough space\n      ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n      if (ptr == NULL && al->fb == OMP_ATV_ABORT_FB) {\n        KMP_ASSERT(0); // abort fallback requested\n      } // no sense to look for another fallback because of same internal alloc\n    }\n  } else {\n    // custom allocator, pool size not requested\n    ptr = __kmp_thread_malloc(__kmp_thread_from_gtid(gtid), desc.size_a);\n    if (ptr == NULL && al->fb == OMP_ATV_ABORT_FB) {\n      KMP_ASSERT(0); // abort fallback requested\n    } // no sense to look for another fallback because of same internal alloc\n  }\n  KE_TRACE(10, (\"__kmpc_alloc: T#%d %p=alloc(%d)\\n\", gtid, ptr, desc.size_a));\n  if (ptr == NULL)\n    return NULL;\n\n  addr = (kmp_uintptr_t)ptr;\n  addr_align = (addr + sz_desc + align - 1) & ~(align - 1);\n  addr_descr = addr_align - sz_desc;\n\n  desc.ptr_alloc = ptr;\n  desc.ptr_align = (void *)addr_align;\n  desc.allocator = al;\n  *((kmp_mem_desc_t *)addr_descr) = desc; // save descriptor contents\n  KMP_MB();\n\n  KE_TRACE(25, (\"__kmpc_alloc returns %p, T#%d\\n\", desc.ptr_align, gtid));\n  return desc.ptr_align;\n}\n\nvoid __kmpc_free(int gtid, void *ptr, const omp_allocator_handle_t allocator) {\n  KE_TRACE(25, (\"__kmpc_free: T#%d free(%p,%p)\\n\", gtid, ptr, allocator));\n  if (ptr == NULL)\n    return;\n\n  kmp_allocator_t *al;\n  omp_allocator_handle_t oal;\n  al = RCAST(kmp_allocator_t *, CCAST(omp_allocator_handle_t, allocator));\n  kmp_mem_desc_t desc;\n  kmp_uintptr_t addr_align; // address to return to caller\n  kmp_uintptr_t addr_descr; // address of memory block descriptor\n\n  addr_align = (kmp_uintptr_t)ptr;\n  addr_descr = addr_align - sizeof(kmp_mem_desc_t);\n  desc = *((kmp_mem_desc_t *)addr_descr); // read descriptor\n\n  KMP_DEBUG_ASSERT(desc.ptr_align == ptr);\n  if (allocator) {\n    KMP_DEBUG_ASSERT(desc.allocator == al || desc.allocator == al->fb_data);\n  }\n  al = desc.allocator;\n  oal = (omp_allocator_handle_t)al; // cast to void* for comparisons\n  KMP_DEBUG_ASSERT(al);\n\n  if (__kmp_memkind_available) {\n    if (oal < kmp_max_mem_alloc) {\n      // pre-defined allocator\n      if (oal == omp_high_bw_mem_alloc && mk_hbw_preferred) {\n        kmp_mk_free(*mk_hbw_preferred, desc.ptr_alloc);\n      } else {\n        kmp_mk_free(*mk_default, desc.ptr_alloc);\n      }\n    } else {\n      if (al->pool_size > 0) { // custom allocator with pool size requested\n        kmp_uint64 used =\n            KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n        (void)used; // to suppress compiler warning\n        KMP_DEBUG_ASSERT(used >= desc.size_a);\n      }\n      kmp_mk_free(*al->memkind, desc.ptr_alloc);\n    }\n  } else {\n    if (oal > kmp_max_mem_alloc && al->pool_size > 0) {\n      kmp_uint64 used =\n          KMP_TEST_THEN_ADD64((kmp_int64 *)&al->pool_used, -desc.size_a);\n      (void)used; // to suppress compiler warning\n      KMP_DEBUG_ASSERT(used >= desc.size_a);\n    }\n    __kmp_thread_free(__kmp_thread_from_gtid(gtid), desc.ptr_alloc);\n  }\n  KE_TRACE(10, (\"__kmpc_free: T#%d freed %p (%p)\\n\", gtid, desc.ptr_alloc,\n                allocator));\n}\n\n/* If LEAK_MEMORY is defined, __kmp_free() will *not* free memory. It causes\n   memory leaks, but it may be useful for debugging memory corruptions, used\n   freed pointers, etc. */\n/* #define LEAK_MEMORY */\nstruct kmp_mem_descr { // Memory block descriptor.\n  void *ptr_allocated; // Pointer returned by malloc(), subject for free().\n  size_t size_allocated; // Size of allocated memory block.\n  void *ptr_aligned; // Pointer to aligned memory, to be used by client code.\n  size_t size_aligned; // Size of aligned memory block.\n};\ntypedef struct kmp_mem_descr kmp_mem_descr_t;\n\n/* Allocate memory on requested boundary, fill allocated memory with 0x00.\n   NULL is NEVER returned, __kmp_abort() is called in case of memory allocation\n   error. Must use __kmp_free when freeing memory allocated by this routine! */\nstatic void *___kmp_allocate_align(size_t size,\n                                   size_t alignment KMP_SRC_LOC_DECL) {\n  /* __kmp_allocate() allocates (by call to malloc()) bigger memory block than\n     requested to return properly aligned pointer. Original pointer returned\n     by malloc() and size of allocated block is saved in descriptor just\n     before the aligned pointer. This information used by __kmp_free() -- it\n     has to pass to free() original pointer, not aligned one.\n\n          +---------+------------+-----------------------------------+---------+\n          | padding | descriptor |           aligned block           | padding |\n          +---------+------------+-----------------------------------+---------+\n          ^                      ^\n          |                      |\n          |                      +- Aligned pointer returned to caller\n          +- Pointer returned by malloc()\n\n      Aligned block is filled with zeros, paddings are filled with 0xEF. */\n\n  kmp_mem_descr_t descr;\n  kmp_uintptr_t addr_allocated; // Address returned by malloc().\n  kmp_uintptr_t addr_aligned; // Aligned address to return to caller.\n  kmp_uintptr_t addr_descr; // Address of memory block descriptor.\n\n  KE_TRACE(25, (\"-> ___kmp_allocate_align( %d, %d ) called from %s:%d\\n\",\n                (int)size, (int)alignment KMP_SRC_LOC_PARM));\n\n  KMP_DEBUG_ASSERT(alignment < 32 * 1024); // Alignment should not be too\n  KMP_DEBUG_ASSERT(sizeof(void *) <= sizeof(kmp_uintptr_t));\n  // Make sure kmp_uintptr_t is enough to store addresses.\n\n  descr.size_aligned = size;\n  descr.size_allocated =\n      descr.size_aligned + sizeof(kmp_mem_descr_t) + alignment;\n\n#if KMP_DEBUG\n  descr.ptr_allocated = _malloc_src_loc(descr.size_allocated, _file_, _line_);\n#else\n  descr.ptr_allocated = malloc_src_loc(descr.size_allocated KMP_SRC_LOC_PARM);\n#endif\n  KE_TRACE(10, (\"   malloc( %d ) returned %p\\n\", (int)descr.size_allocated,\n                descr.ptr_allocated));\n  if (descr.ptr_allocated == NULL) {\n    KMP_FATAL(OutOfHeapMemory);\n  }\n\n  addr_allocated = (kmp_uintptr_t)descr.ptr_allocated;\n  addr_aligned =\n      (addr_allocated + sizeof(kmp_mem_descr_t) + alignment) & ~(alignment - 1);\n  addr_descr = addr_aligned - sizeof(kmp_mem_descr_t);\n\n  descr.ptr_aligned = (void *)addr_aligned;\n\n  KE_TRACE(26, (\"   ___kmp_allocate_align: \"\n                \"ptr_allocated=%p, size_allocated=%d, \"\n                \"ptr_aligned=%p, size_aligned=%d\\n\",\n                descr.ptr_allocated, (int)descr.size_allocated,\n                descr.ptr_aligned, (int)descr.size_aligned));\n\n  KMP_DEBUG_ASSERT(addr_allocated <= addr_descr);\n  KMP_DEBUG_ASSERT(addr_descr + sizeof(kmp_mem_descr_t) == addr_aligned);\n  KMP_DEBUG_ASSERT(addr_aligned + descr.size_aligned <=\n                   addr_allocated + descr.size_allocated);\n  KMP_DEBUG_ASSERT(addr_aligned % alignment == 0);\n#ifdef KMP_DEBUG\n  memset(descr.ptr_allocated, 0xEF, descr.size_allocated);\n// Fill allocated memory block with 0xEF.\n#endif\n  memset(descr.ptr_aligned, 0x00, descr.size_aligned);\n  // Fill the aligned memory block (which is intended for using by caller) with\n  // 0x00. Do not\n  // put this filling under KMP_DEBUG condition! Many callers expect zeroed\n  // memory. (Padding\n  // bytes remain filled with 0xEF in debugging library.)\n  *((kmp_mem_descr_t *)addr_descr) = descr;\n\n  KMP_MB();\n\n  KE_TRACE(25, (\"<- ___kmp_allocate_align() returns %p\\n\", descr.ptr_aligned));\n  return descr.ptr_aligned;\n} // func ___kmp_allocate_align\n\n/* Allocate memory on cache line boundary, fill allocated memory with 0x00.\n   Do not call this func directly! Use __kmp_allocate macro instead.\n   NULL is NEVER returned, __kmp_abort() is called in case of memory allocation\n   error. Must use __kmp_free when freeing memory allocated by this routine! */\nvoid *___kmp_allocate(size_t size KMP_SRC_LOC_DECL) {\n  void *ptr;\n  KE_TRACE(25, (\"-> __kmp_allocate( %d ) called from %s:%d\\n\",\n                (int)size KMP_SRC_LOC_PARM));\n  ptr = ___kmp_allocate_align(size, __kmp_align_alloc KMP_SRC_LOC_PARM);\n  KE_TRACE(25, (\"<- __kmp_allocate() returns %p\\n\", ptr));\n  return ptr;\n} // func ___kmp_allocate\n\n/* Allocate memory on page boundary, fill allocated memory with 0x00.\n   Does not call this func directly! Use __kmp_page_allocate macro instead.\n   NULL is NEVER returned, __kmp_abort() is called in case of memory allocation\n   error. Must use __kmp_free when freeing memory allocated by this routine! */\nvoid *___kmp_page_allocate(size_t size KMP_SRC_LOC_DECL) {\n  int page_size = 8 * 1024;\n  void *ptr;\n\n  KE_TRACE(25, (\"-> __kmp_page_allocate( %d ) called from %s:%d\\n\",\n                (int)size KMP_SRC_LOC_PARM));\n  ptr = ___kmp_allocate_align(size, page_size KMP_SRC_LOC_PARM);\n  KE_TRACE(25, (\"<- __kmp_page_allocate( %d ) returns %p\\n\", (int)size, ptr));\n  return ptr;\n} // ___kmp_page_allocate\n\n/* Free memory allocated by __kmp_allocate() and __kmp_page_allocate().\n   In debug mode, fill the memory block with 0xEF before call to free(). */\nvoid ___kmp_free(void *ptr KMP_SRC_LOC_DECL) {\n  kmp_mem_descr_t descr;\n  kmp_uintptr_t addr_allocated; // Address returned by malloc().\n  kmp_uintptr_t addr_aligned; // Aligned address passed by caller.\n\n  KE_TRACE(25,\n           (\"-> __kmp_free( %p ) called from %s:%d\\n\", ptr KMP_SRC_LOC_PARM));\n  KMP_ASSERT(ptr != NULL);\n\n  descr = *(kmp_mem_descr_t *)((kmp_uintptr_t)ptr - sizeof(kmp_mem_descr_t));\n\n  KE_TRACE(26, (\"   __kmp_free:     \"\n                \"ptr_allocated=%p, size_allocated=%d, \"\n                \"ptr_aligned=%p, size_aligned=%d\\n\",\n                descr.ptr_allocated, (int)descr.size_allocated,\n                descr.ptr_aligned, (int)descr.size_aligned));\n\n  addr_allocated = (kmp_uintptr_t)descr.ptr_allocated;\n  addr_aligned = (kmp_uintptr_t)descr.ptr_aligned;\n\n  KMP_DEBUG_ASSERT(addr_aligned % CACHE_LINE == 0);\n  KMP_DEBUG_ASSERT(descr.ptr_aligned == ptr);\n  KMP_DEBUG_ASSERT(addr_allocated + sizeof(kmp_mem_descr_t) <= addr_aligned);\n  KMP_DEBUG_ASSERT(descr.size_aligned < descr.size_allocated);\n  KMP_DEBUG_ASSERT(addr_aligned + descr.size_aligned <=\n                   addr_allocated + descr.size_allocated);\n\n#ifdef KMP_DEBUG\n  memset(descr.ptr_allocated, 0xEF, descr.size_allocated);\n// Fill memory block with 0xEF, it helps catch using freed memory.\n#endif\n\n#ifndef LEAK_MEMORY\n  KE_TRACE(10, (\"   free( %p )\\n\", descr.ptr_allocated));\n#ifdef KMP_DEBUG\n  _free_src_loc(descr.ptr_allocated, _file_, _line_);\n#else\n  free_src_loc(descr.ptr_allocated KMP_SRC_LOC_PARM);\n#endif\n#endif\n  KMP_MB();\n  KE_TRACE(25, (\"<- __kmp_free() returns\\n\"));\n} // func ___kmp_free\n\n#if USE_FAST_MEMORY == 3\n// Allocate fast memory by first scanning the thread's free lists\n// If a chunk the right size exists, grab it off the free list.\n// Otherwise allocate normally using kmp_thread_malloc.\n\n// AC: How to choose the limit? Just get 16 for now...\n#define KMP_FREE_LIST_LIMIT 16\n\n// Always use 128 bytes for determining buckets for caching memory blocks\n#define DCACHE_LINE 128\n\nvoid *___kmp_fast_allocate(kmp_info_t *this_thr, size_t size KMP_SRC_LOC_DECL) {\n  void *ptr;\n  int num_lines;\n  int idx;\n  int index;\n  void *alloc_ptr;\n  size_t alloc_size;\n  kmp_mem_descr_t *descr;\n\n  KE_TRACE(25, (\"-> __kmp_fast_allocate( T#%d, %d ) called from %s:%d\\n\",\n                __kmp_gtid_from_thread(this_thr), (int)size KMP_SRC_LOC_PARM));\n\n  num_lines = (size + DCACHE_LINE - 1) / DCACHE_LINE;\n  idx = num_lines - 1;\n  KMP_DEBUG_ASSERT(idx >= 0);\n  if (idx < 2) {\n    index = 0; // idx is [ 0, 1 ], use first free list\n    num_lines = 2; // 1, 2 cache lines or less than cache line\n  } else if ((idx >>= 2) == 0) {\n    index = 1; // idx is [ 2, 3 ], use second free list\n    num_lines = 4; // 3, 4 cache lines\n  } else if ((idx >>= 2) == 0) {\n    index = 2; // idx is [ 4, 15 ], use third free list\n    num_lines = 16; // 5, 6, ..., 16 cache lines\n  } else if ((idx >>= 2) == 0) {\n    index = 3; // idx is [ 16, 63 ], use fourth free list\n    num_lines = 64; // 17, 18, ..., 64 cache lines\n  } else {\n    goto alloc_call; // 65 or more cache lines ( > 8KB ), don't use free lists\n  }\n\n  ptr = this_thr->th.th_free_lists[index].th_free_list_self;\n  if (ptr != NULL) {\n    // pop the head of no-sync free list\n    this_thr->th.th_free_lists[index].th_free_list_self = *((void **)ptr);\n    KMP_DEBUG_ASSERT(\n        this_thr ==\n        ((kmp_mem_descr_t *)((kmp_uintptr_t)ptr - sizeof(kmp_mem_descr_t)))\n            ->ptr_aligned);\n    goto end;\n  }\n  ptr = TCR_SYNC_PTR(this_thr->th.th_free_lists[index].th_free_list_sync);\n  if (ptr != NULL) {\n    // no-sync free list is empty, use sync free list (filled in by other\n    // threads only)\n    // pop the head of the sync free list, push NULL instead\n    while (!KMP_COMPARE_AND_STORE_PTR(\n        &this_thr->th.th_free_lists[index].th_free_list_sync, ptr, nullptr)) {\n      KMP_CPU_PAUSE();\n      ptr = TCR_SYNC_PTR(this_thr->th.th_free_lists[index].th_free_list_sync);\n    }\n    // push the rest of chain into no-sync free list (can be NULL if there was\n    // the only block)\n    this_thr->th.th_free_lists[index].th_free_list_self = *((void **)ptr);\n    KMP_DEBUG_ASSERT(\n        this_thr ==\n        ((kmp_mem_descr_t *)((kmp_uintptr_t)ptr - sizeof(kmp_mem_descr_t)))\n            ->ptr_aligned);\n    goto end;\n  }\n\nalloc_call:\n  // haven't found block in the free lists, thus allocate it\n  size = num_lines * DCACHE_LINE;\n\n  alloc_size = size + sizeof(kmp_mem_descr_t) + DCACHE_LINE;\n  KE_TRACE(25, (\"__kmp_fast_allocate: T#%d Calling __kmp_thread_malloc with \"\n                \"alloc_size %d\\n\",\n                __kmp_gtid_from_thread(this_thr), alloc_size));\n  alloc_ptr = bget(this_thr, (bufsize)alloc_size);\n\n  // align ptr to DCACHE_LINE\n  ptr = (void *)((((kmp_uintptr_t)alloc_ptr) + sizeof(kmp_mem_descr_t) +\n                  DCACHE_LINE) &\n                 ~(DCACHE_LINE - 1));\n  descr = (kmp_mem_descr_t *)(((kmp_uintptr_t)ptr) - sizeof(kmp_mem_descr_t));\n\n  descr->ptr_allocated = alloc_ptr; // remember allocated pointer\n  // we don't need size_allocated\n  descr->ptr_aligned = (void *)this_thr; // remember allocating thread\n  // (it is already saved in bget buffer,\n  // but we may want to use another allocator in future)\n  descr->size_aligned = size;\n\nend:\n  KE_TRACE(25, (\"<- __kmp_fast_allocate( T#%d ) returns %p\\n\",\n                __kmp_gtid_from_thread(this_thr), ptr));\n  return ptr;\n} // func __kmp_fast_allocate\n\n// Free fast memory and place it on the thread's free list if it is of\n// the correct size.\nvoid ___kmp_fast_free(kmp_info_t *this_thr, void *ptr KMP_SRC_LOC_DECL) {\n  kmp_mem_descr_t *descr;\n  kmp_info_t *alloc_thr;\n  size_t size;\n  size_t idx;\n  int index;\n\n  KE_TRACE(25, (\"-> __kmp_fast_free( T#%d, %p ) called from %s:%d\\n\",\n                __kmp_gtid_from_thread(this_thr), ptr KMP_SRC_LOC_PARM));\n  KMP_ASSERT(ptr != NULL);\n\n  descr = (kmp_mem_descr_t *)(((kmp_uintptr_t)ptr) - sizeof(kmp_mem_descr_t));\n\n  KE_TRACE(26, (\"   __kmp_fast_free:     size_aligned=%d\\n\",\n                (int)descr->size_aligned));\n\n  size = descr->size_aligned; // 2, 4, 16, 64, 65, 66, ... cache lines\n\n  idx = DCACHE_LINE * 2; // 2 cache lines is minimal size of block\n  if (idx == size) {\n    index = 0; // 2 cache lines\n  } else if ((idx <<= 1) == size) {\n    index = 1; // 4 cache lines\n  } else if ((idx <<= 2) == size) {\n    index = 2; // 16 cache lines\n  } else if ((idx <<= 2) == size) {\n    index = 3; // 64 cache lines\n  } else {\n    KMP_DEBUG_ASSERT(size > DCACHE_LINE * 64);\n    goto free_call; // 65 or more cache lines ( > 8KB )\n  }\n\n  alloc_thr = (kmp_info_t *)descr->ptr_aligned; // get thread owning the block\n  if (alloc_thr == this_thr) {\n    // push block to self no-sync free list, linking previous head (LIFO)\n    *((void **)ptr) = this_thr->th.th_free_lists[index].th_free_list_self;\n    this_thr->th.th_free_lists[index].th_free_list_self = ptr;\n  } else {\n    void *head = this_thr->th.th_free_lists[index].th_free_list_other;\n    if (head == NULL) {\n      // Create new free list\n      this_thr->th.th_free_lists[index].th_free_list_other = ptr;\n      *((void **)ptr) = NULL; // mark the tail of the list\n      descr->size_allocated = (size_t)1; // head of the list keeps its length\n    } else {\n      // need to check existed \"other\" list's owner thread and size of queue\n      kmp_mem_descr_t *dsc =\n          (kmp_mem_descr_t *)((char *)head - sizeof(kmp_mem_descr_t));\n      // allocating thread, same for all queue nodes\n      kmp_info_t *q_th = (kmp_info_t *)(dsc->ptr_aligned);\n      size_t q_sz =\n          dsc->size_allocated + 1; // new size in case we add current task\n      if (q_th == alloc_thr && q_sz <= KMP_FREE_LIST_LIMIT) {\n        // we can add current task to \"other\" list, no sync needed\n        *((void **)ptr) = head;\n        descr->size_allocated = q_sz;\n        this_thr->th.th_free_lists[index].th_free_list_other = ptr;\n      } else {\n        // either queue blocks owner is changing or size limit exceeded\n        // return old queue to allocating thread (q_th) synchroneously,\n        // and start new list for alloc_thr's tasks\n        void *old_ptr;\n        void *tail = head;\n        void *next = *((void **)head);\n        while (next != NULL) {\n          KMP_DEBUG_ASSERT(\n              // queue size should decrease by 1 each step through the list\n              ((kmp_mem_descr_t *)((char *)next - sizeof(kmp_mem_descr_t)))\n                      ->size_allocated +\n                  1 ==\n              ((kmp_mem_descr_t *)((char *)tail - sizeof(kmp_mem_descr_t)))\n                  ->size_allocated);\n          tail = next; // remember tail node\n          next = *((void **)next);\n        }\n        KMP_DEBUG_ASSERT(q_th != NULL);\n        // push block to owner's sync free list\n        old_ptr = TCR_PTR(q_th->th.th_free_lists[index].th_free_list_sync);\n        /* the next pointer must be set before setting free_list to ptr to avoid\n           exposing a broken list to other threads, even for an instant. */\n        *((void **)tail) = old_ptr;\n\n        while (!KMP_COMPARE_AND_STORE_PTR(\n            &q_th->th.th_free_lists[index].th_free_list_sync, old_ptr, head)) {\n          KMP_CPU_PAUSE();\n          old_ptr = TCR_PTR(q_th->th.th_free_lists[index].th_free_list_sync);\n          *((void **)tail) = old_ptr;\n        }\n\n        // start new list of not-selt tasks\n        this_thr->th.th_free_lists[index].th_free_list_other = ptr;\n        *((void **)ptr) = NULL;\n        descr->size_allocated = (size_t)1; // head of queue keeps its length\n      }\n    }\n  }\n  goto end;\n\nfree_call:\n  KE_TRACE(25, (\"__kmp_fast_free: T#%d Calling __kmp_thread_free for size %d\\n\",\n                __kmp_gtid_from_thread(this_thr), size));\n  __kmp_bget_dequeue(this_thr); /* Release any queued buffers */\n  brel(this_thr, descr->ptr_allocated);\n\nend:\n  KE_TRACE(25, (\"<- __kmp_fast_free() returns\\n\"));\n\n} // func __kmp_fast_free\n\n// Initialize the thread free lists related to fast memory\n// Only do this when a thread is initially created.\nvoid __kmp_initialize_fast_memory(kmp_info_t *this_thr) {\n  KE_TRACE(10, (\"__kmp_initialize_fast_memory: Called from th %p\\n\", this_thr));\n\n  memset(this_thr->th.th_free_lists, 0, NUM_LISTS * sizeof(kmp_free_list_t));\n}\n\n// Free the memory in the thread free lists related to fast memory\n// Only do this when a thread is being reaped (destroyed).\nvoid __kmp_free_fast_memory(kmp_info_t *th) {\n  // Suppose we use BGET underlying allocator, walk through its structures...\n  int bin;\n  thr_data_t *thr = get_thr_data(th);\n  void **lst = NULL;\n\n  KE_TRACE(\n      5, (\"__kmp_free_fast_memory: Called T#%d\\n\", __kmp_gtid_from_thread(th)));\n\n  __kmp_bget_dequeue(th); // Release any queued buffers\n\n  // Dig through free lists and extract all allocated blocks\n  for (bin = 0; bin < MAX_BGET_BINS; ++bin) {\n    bfhead_t *b = thr->freelist[bin].ql.flink;\n    while (b != &thr->freelist[bin]) {\n      if ((kmp_uintptr_t)b->bh.bb.bthr & 1) { // the buffer is allocated address\n        *((void **)b) =\n            lst; // link the list (override bthr, but keep flink yet)\n        lst = (void **)b; // push b into lst\n      }\n      b = b->ql.flink; // get next buffer\n    }\n  }\n  while (lst != NULL) {\n    void *next = *lst;\n    KE_TRACE(10, (\"__kmp_free_fast_memory: freeing %p, next=%p th %p (%d)\\n\",\n                  lst, next, th, __kmp_gtid_from_thread(th)));\n    (*thr->relfcn)(lst);\n#if BufStats\n    // count blocks to prevent problems in __kmp_finalize_bget()\n    thr->numprel++; /* Nr of expansion block releases */\n    thr->numpblk--; /* Total number of blocks */\n#endif\n    lst = (void **)next;\n  }\n\n  KE_TRACE(\n      5, (\"__kmp_free_fast_memory: Freed T#%d\\n\", __kmp_gtid_from_thread(th)));\n}\n\n#endif // USE_FAST_MEMORY\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/runtime/src/thirdparty/ittnotify/ittnotify_config.h": "\n//===----------------------------------------------------------------------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _ITTNOTIFY_CONFIG_H_\n#define _ITTNOTIFY_CONFIG_H_\n\n/** @cond exclude_from_documentation */\n#ifndef ITT_OS_WIN\n#  define ITT_OS_WIN   1\n#endif /* ITT_OS_WIN */\n\n#ifndef ITT_OS_LINUX\n#  define ITT_OS_LINUX 2\n#endif /* ITT_OS_LINUX */\n\n#ifndef ITT_OS_MAC\n#  define ITT_OS_MAC   3\n#endif /* ITT_OS_MAC */\n\n#ifndef ITT_OS_FREEBSD\n#  define ITT_OS_FREEBSD   4\n#endif /* ITT_OS_FREEBSD */\n\n#ifndef ITT_OS\n#  if defined WIN32 || defined _WIN32\n#    define ITT_OS ITT_OS_WIN\n#  elif defined( __APPLE__ ) && defined( __MACH__ )\n#    define ITT_OS ITT_OS_MAC\n#  elif defined( __FreeBSD__ )\n#    define ITT_OS ITT_OS_FREEBSD\n#  else\n#    define ITT_OS ITT_OS_LINUX\n#  endif\n#endif /* ITT_OS */\n\n#ifndef ITT_PLATFORM_WIN\n#  define ITT_PLATFORM_WIN 1\n#endif /* ITT_PLATFORM_WIN */\n\n#ifndef ITT_PLATFORM_POSIX\n#  define ITT_PLATFORM_POSIX 2\n#endif /* ITT_PLATFORM_POSIX */\n\n#ifndef ITT_PLATFORM_MAC\n#  define ITT_PLATFORM_MAC 3\n#endif /* ITT_PLATFORM_MAC */\n\n#ifndef ITT_PLATFORM_FREEBSD\n#  define ITT_PLATFORM_FREEBSD 4\n#endif /* ITT_PLATFORM_FREEBSD */\n\n#ifndef ITT_PLATFORM\n#  if ITT_OS==ITT_OS_WIN\n#    define ITT_PLATFORM ITT_PLATFORM_WIN\n#  elif ITT_OS==ITT_OS_MAC\n#    define ITT_PLATFORM ITT_PLATFORM_MAC\n#  elif ITT_OS==ITT_OS_FREEBSD\n#    define ITT_PLATFORM ITT_PLATFORM_FREEBSD\n#  else\n#    define ITT_PLATFORM ITT_PLATFORM_POSIX\n#  endif\n#endif /* ITT_PLATFORM */\n\n#if defined(_UNICODE) && !defined(UNICODE)\n#define UNICODE\n#endif\n\n#include <stddef.h>\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <tchar.h>\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <stdint.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE || _UNICODE */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#ifndef ITTAPI_CDECL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define ITTAPI_CDECL __cdecl\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define ITTAPI_CDECL __attribute__ ((cdecl))\n#    else  /* _M_IX86 || __i386__ */\n#      define ITTAPI_CDECL /* actual only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* ITTAPI_CDECL */\n\n#ifndef STDCALL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define STDCALL __stdcall\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define STDCALL __attribute__ ((stdcall))\n#    else  /* _M_IX86 || __i386__ */\n#      define STDCALL /* supported only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* STDCALL */\n\n#define ITTAPI    ITTAPI_CDECL\n#define LIBITTAPI ITTAPI_CDECL\n\n/* TODO: Temporary for compatibility! */\n#define ITTAPI_CALL    ITTAPI_CDECL\n#define LIBITTAPI_CALL ITTAPI_CDECL\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n/* use __forceinline (VC++ specific) */\n#define ITT_INLINE           __forceinline\n#define ITT_INLINE_ATTRIBUTE /* nothing */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/*\n * Generally, functions are not inlined unless optimization is specified.\n * For functions declared inline, this attribute inlines the function even\n * if no optimization level was specified.\n */\n#ifdef __STRICT_ANSI__\n#define ITT_INLINE           static\n#define ITT_INLINE_ATTRIBUTE __attribute__((unused))\n#else  /* __STRICT_ANSI__ */\n#define ITT_INLINE           static inline\n#define ITT_INLINE_ATTRIBUTE __attribute__((always_inline, unused))\n#endif /* __STRICT_ANSI__ */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/** @endcond */\n\n#ifndef ITT_ARCH_IA32\n#  define ITT_ARCH_IA32  1\n#endif /* ITT_ARCH_IA32 */\n\n#ifndef ITT_ARCH_IA32E\n#  define ITT_ARCH_IA32E 2\n#endif /* ITT_ARCH_IA32E */\n\n/* Was there a magical reason we didn't have 3 here before? */\n#ifndef ITT_ARCH_AARCH64\n#  define ITT_ARCH_AARCH64  3\n#endif /* ITT_ARCH_AARCH64 */\n\n#ifndef ITT_ARCH_ARM\n#  define ITT_ARCH_ARM  4\n#endif /* ITT_ARCH_ARM */\n\n#ifndef ITT_ARCH_PPC64\n#  define ITT_ARCH_PPC64  5\n#endif /* ITT_ARCH_PPC64 */\n\n#ifndef ITT_ARCH_MIPS\n#  define ITT_ARCH_MIPS  6\n#endif /* ITT_ARCH_MIPS */\n\n#ifndef ITT_ARCH_MIPS64\n#  define ITT_ARCH_MIPS64  6\n#endif /* ITT_ARCH_MIPS64 */\n\n#ifndef ITT_ARCH\n#  if defined _M_IX86 || defined __i386__\n#    define ITT_ARCH ITT_ARCH_IA32\n#  elif defined _M_X64 || defined _M_AMD64 || defined __x86_64__\n#    define ITT_ARCH ITT_ARCH_IA32E\n#  elif defined _M_IA64 || defined __ia64__\n#    define ITT_ARCH ITT_ARCH_IA64\n#  elif defined _M_ARM || defined __arm__\n#    define ITT_ARCH ITT_ARCH_ARM\n#  elif defined __powerpc64__\n#    define ITT_ARCH ITT_ARCH_PPC64\n#  elif defined __aarch64__\n#    define ITT_ARCH ITT_ARCH_AARCH64\n#  elif defined __mips__ && !defined __mips64\n#    define ITT_ARCH ITT_ARCH_MIPS\n#  elif defined __mips__ && defined __mips64\n#    define ITT_ARCH ITT_ARCH_MIPS64\n#  endif\n#endif\n\n#ifdef __cplusplus\n#  define ITT_EXTERN_C extern \"C\"\n#  define ITT_EXTERN_C_BEGIN extern \"C\" {\n#  define ITT_EXTERN_C_END }\n#else\n#  define ITT_EXTERN_C /* nothing */\n#  define ITT_EXTERN_C_BEGIN /* nothing */\n#  define ITT_EXTERN_C_END /* nothing */\n#endif /* __cplusplus */\n\n#define ITT_TO_STR_AUX(x) #x\n#define ITT_TO_STR(x)     ITT_TO_STR_AUX(x)\n\n#define __ITT_BUILD_ASSERT(expr, suffix) do { \\\n    static char __itt_build_check_##suffix[(expr) ? 1 : -1]; \\\n    __itt_build_check_##suffix[0] = 0; \\\n} while(0)\n#define _ITT_BUILD_ASSERT(expr, suffix)  __ITT_BUILD_ASSERT((expr), suffix)\n#define ITT_BUILD_ASSERT(expr)           _ITT_BUILD_ASSERT((expr), __LINE__)\n\n#define ITT_MAGIC { 0xED, 0xAB, 0xAB, 0xEC, 0x0D, 0xEE, 0xDA, 0x30 }\n\n/* Replace with snapshot date YYYYMMDD for promotion build. */\n#define API_VERSION_BUILD    20151119\n\n#ifndef API_VERSION_NUM\n#define API_VERSION_NUM 0.0.0\n#endif /* API_VERSION_NUM */\n\n#define API_VERSION \"ITT-API-Version \" ITT_TO_STR(API_VERSION_NUM) \\\n                                \" (\" ITT_TO_STR(API_VERSION_BUILD) \")\"\n\n/* OS communication functions */\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <windows.h>\ntypedef HMODULE           lib_t;\ntypedef DWORD             TIDT;\ntypedef CRITICAL_SECTION  mutex_t;\n#define MUTEX_INITIALIZER { 0 }\n#define strong_alias(name, aliasname) /* empty for Windows */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <dlfcn.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE */\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1 /* need for PTHREAD_MUTEX_RECURSIVE */\n#endif /* _GNU_SOURCE */\n#ifndef __USE_UNIX98\n#define __USE_UNIX98 1 /* need for PTHREAD_MUTEX_RECURSIVE, on SLES11.1 with gcc 4.3.4 wherein pthread.h missing dependency on __USE_XOPEN2K8 */\n#endif /*__USE_UNIX98*/\n#include <pthread.h>\ntypedef void*             lib_t;\ntypedef pthread_t         TIDT;\ntypedef pthread_mutex_t   mutex_t;\n#define MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n#define _strong_alias(name, aliasname) \\\n            extern __typeof (name) aliasname __attribute__ ((alias (#name)));\n#define strong_alias(name, aliasname) _strong_alias(name, aliasname)\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#define __itt_get_proc(lib, name) GetProcAddress(lib, name)\n#define __itt_mutex_init(mutex)   InitializeCriticalSection(mutex)\n#define __itt_mutex_lock(mutex)   EnterCriticalSection(mutex)\n#define __itt_mutex_unlock(mutex) LeaveCriticalSection(mutex)\n#define __itt_load_lib(name)      LoadLibraryA(name)\n#define __itt_unload_lib(handle)  FreeLibrary(handle)\n#define __itt_system_error()      (int)GetLastError()\n#define __itt_fstrcmp(s1, s2)     lstrcmpA(s1, s2)\n#define __itt_fstrnlen(s, l)      strnlen_s(s, l)\n#define __itt_fstrcpyn(s1, b, s2, l) strncpy_s(s1, b, s2, l)\n#define __itt_fstrdup(s)          _strdup(s)\n#define __itt_thread_id()         GetCurrentThreadId()\n#define __itt_thread_yield()      SwitchToThread()\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return InterlockedIncrement(ptr);\n}\n#endif /* ITT_SIMPLE_INIT */\n\n#define DL_SYMBOLS (1)\n#define PTHREAD_SYMBOLS (1)\n\n#else /* ITT_PLATFORM!=ITT_PLATFORM_WIN */\n#define __itt_get_proc(lib, name) dlsym(lib, name)\n#define __itt_mutex_init(mutex)   {\\\n    pthread_mutexattr_t mutex_attr;                                         \\\n    int error_code = pthread_mutexattr_init(&mutex_attr);                   \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_init\",    \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_settype(&mutex_attr,                     \\\n                                           PTHREAD_MUTEX_RECURSIVE);        \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_settype\", \\\n                           error_code);                                     \\\n    error_code = pthread_mutex_init(mutex, &mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutex_init\",        \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_destroy(&mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_destroy\", \\\n                           error_code);                                     \\\n}\n#define __itt_mutex_lock(mutex)   pthread_mutex_lock(mutex)\n#define __itt_mutex_unlock(mutex) pthread_mutex_unlock(mutex)\n#define __itt_load_lib(name)      dlopen(name, RTLD_LAZY)\n#define __itt_unload_lib(handle)  dlclose(handle)\n#define __itt_system_error()      errno\n#define __itt_fstrcmp(s1, s2)     strcmp(s1, s2)\n\n/* makes customer code define safe APIs for SDL_STRNLEN_S and SDL_STRNCPY_S */\n#ifdef SDL_STRNLEN_S\n#define __itt_fstrnlen(s, l)      SDL_STRNLEN_S(s, l)\n#else\n#define __itt_fstrnlen(s, l)      strlen(s)\n#endif /* SDL_STRNLEN_S */\n#ifdef SDL_STRNCPY_S\n#define __itt_fstrcpyn(s1, b, s2, l) SDL_STRNCPY_S(s1, b, s2, l)\n#else\n#define __itt_fstrcpyn(s1, b, s2, l) strncpy(s1, s2, l)\n#endif /* SDL_STRNCPY_S */\n\n#define __itt_fstrdup(s)          strdup(s)\n#define __itt_thread_id()         pthread_self()\n#define __itt_thread_yield()      sched_yield()\n#if ITT_ARCH==ITT_ARCH_IA64\n#ifdef __INTEL_COMPILER\n#define __TBB_machine_fetchadd4(addr, val) __fetchadd4_acq((void *)addr, val)\n#else  /* __INTEL_COMPILER */\n/* TODO: Add Support for not Intel compilers for IA-64 architecture */\n#endif /* __INTEL_COMPILER */\n#elif ITT_ARCH==ITT_ARCH_IA32 || ITT_ARCH==ITT_ARCH_IA32E /* ITT_ARCH!=ITT_ARCH_IA64 */\nITT_INLINE long\n__TBB_machine_fetchadd4(volatile void* ptr, long addend) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __TBB_machine_fetchadd4(volatile void* ptr, long addend)\n{\n    long result;\n    __asm__ __volatile__(\"lock\\nxadd %0,%1\"\n                          : \"=r\"(result),\"=m\"(*(volatile int*)ptr)\n                          : \"0\"(addend), \"m\"(*(volatile int*)ptr)\n                          : \"memory\");\n    return result;\n}\n#elif ITT_ARCH==ITT_ARCH_ARM || ITT_ARCH==ITT_ARCH_PPC64 || ITT_ARCH==ITT_ARCH_AARCH64 || ITT_ARCH==ITT_ARCH_MIPS ||  ITT_ARCH==ITT_ARCH_MIPS64\n#define __TBB_machine_fetchadd4(addr, val) __sync_fetch_and_add(addr, val)\n#endif /* ITT_ARCH==ITT_ARCH_IA64 */\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return __TBB_machine_fetchadd4(ptr, 1) + 1L;\n}\n#endif /* ITT_SIMPLE_INIT */\n\nvoid* dlopen(const char*, int) __attribute__((weak));\nvoid* dlsym(void*, const char*) __attribute__((weak));\nint dlclose(void*) __attribute__((weak));\n#define DL_SYMBOLS (dlopen && dlsym && dlclose)\n\nint pthread_mutex_init(pthread_mutex_t*, const pthread_mutexattr_t*) __attribute__((weak));\nint pthread_mutex_lock(pthread_mutex_t*) __attribute__((weak));\nint pthread_mutex_unlock(pthread_mutex_t*) __attribute__((weak));\nint pthread_mutex_destroy(pthread_mutex_t*) __attribute__((weak));\nint pthread_mutexattr_init(pthread_mutexattr_t*) __attribute__((weak));\nint pthread_mutexattr_settype(pthread_mutexattr_t*, int) __attribute__((weak));\nint pthread_mutexattr_destroy(pthread_mutexattr_t*) __attribute__((weak));\npthread_t pthread_self(void) __attribute__((weak));\n#define PTHREAD_SYMBOLS (pthread_mutex_init && pthread_mutex_lock && pthread_mutex_unlock && pthread_mutex_destroy && pthread_mutexattr_init && pthread_mutexattr_settype && pthread_mutexattr_destroy && pthread_self)\n\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\ntypedef enum {\n    __itt_collection_normal = 0,\n    __itt_collection_paused = 1\n} __itt_collection_state;\n\ntypedef enum {\n    __itt_thread_normal  = 0,\n    __itt_thread_ignored = 1\n} __itt_thread_state;\n\n#pragma pack(push, 8)\n\ntypedef struct ___itt_thread_info\n{\n    const char* nameA; /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    TIDT               tid;\n    __itt_thread_state state;   /*!< Thread state (paused or normal) */\n    int                extra1;  /*!< Reserved to the runtime */\n    void*              extra2;  /*!< Reserved to the runtime */\n    struct ___itt_thread_info* next;\n} __itt_thread_info;\n\n#include \"ittnotify_types.h\" /* For __itt_group_id definition */\n\ntypedef struct ___itt_api_info_20101001\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    __itt_group_id group;\n}  __itt_api_info_20101001;\n\ntypedef struct ___itt_api_info\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    void*          null_func;\n    __itt_group_id group;\n}  __itt_api_info;\n\ntypedef struct __itt_counter_info\n{\n    const char* nameA;  /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    const char* domainA;  /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* domainW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* domainW;\n#endif /* UNICODE || _UNICODE */\n    int type;\n    long index;\n    int   extra1; /*!< Reserved to the runtime */\n    void* extra2; /*!< Reserved to the runtime */\n    struct __itt_counter_info* next;\n}  __itt_counter_info_t;\n\nstruct ___itt_domain;\nstruct ___itt_string_handle;\n\ntypedef struct ___itt_global\n{\n    unsigned char          magic[8];\n    unsigned long          version_major;\n    unsigned long          version_minor;\n    unsigned long          version_build;\n    volatile long          api_initialized;\n    volatile long          mutex_initialized;\n    volatile long          atomic_counter;\n    mutex_t                mutex;\n    lib_t                  lib;\n    void*                  error_handler;\n    const char**           dll_path_ptr;\n    __itt_api_info*        api_list_ptr;\n    struct ___itt_global*  next;\n    /* Joinable structures below */\n    __itt_thread_info*     thread_list;\n    struct ___itt_domain*  domain_list;\n    struct ___itt_string_handle* string_list;\n    __itt_collection_state state;\n    __itt_counter_info_t* counter_list;\n} __itt_global;\n\n#pragma pack(pop)\n\n#define NEW_THREAD_INFO_W(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = NULL; \\\n        h->nameW  = n ? _wcsdup(n) : NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_THREAD_INFO_A(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = n ? __itt_fstrdup(n) : NULL; \\\n        h->nameW  = NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_W(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 1;    /* domain is enabled by default */ \\\n        h->nameA  = NULL; \\\n        h->nameW  = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_A(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 1;    /* domain is enabled by default */ \\\n        h->nameA  = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW  = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_W(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = NULL; \\\n        h->strW   = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_A(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->strW   = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_COUNTER_W(gptr,h,h_tail,name,domain,type) { \\\n    h = (__itt_counter_info_t*)malloc(sizeof(__itt_counter_info_t)); \\\n    if (h != NULL) { \\\n        h->nameA   = NULL; \\\n        h->nameW   = name ? _wcsdup(name) : NULL; \\\n        h->domainA   = NULL; \\\n        h->domainW   = name ? _wcsdup(domain) : NULL; \\\n        h->type = type; \\\n        h->index = 0; \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->counter_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_COUNTER_A(gptr,h,h_tail,name,domain,type) { \\\n    h = (__itt_counter_info_t*)malloc(sizeof(__itt_counter_info_t)); \\\n    if (h != NULL) { \\\n        h->nameA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW   = NULL; \\\n        h->domainA   = domain ? __itt_fstrdup(domain) : NULL; \\\n        h->domainW   = NULL; \\\n        h->type = type; \\\n        h->index = 0; \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->counter_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#endif /* _ITTNOTIFY_CONFIG_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/libomptarget/src/rtl.cpp": "//===----------- rtl.cpp - Target independent OpenMP target RTL -----------===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// Functionality for handling RTL plugins.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"device.h\"\n#include \"private.h\"\n#include \"rtl.h\"\n\n#include <cassert>\n#include <cstdlib>\n#include <cstring>\n#include <dlfcn.h>\n#include <mutex>\n#include <string>\n\n// List of all plugins that can support offloading.\nstatic const char *RTLNames[] = {\n    /* PowerPC target */ \"libomptarget.rtl.ppc64.so\",\n    /* x86_64 target  */ \"libomptarget.rtl.x86_64.so\",\n    /* CUDA target    */ \"libomptarget.rtl.cuda.so\",\n    /* AArch64 target */ \"libomptarget.rtl.aarch64.so\"};\n\nRTLsTy RTLs;\nstd::mutex RTLsMtx;\n\nHostEntriesBeginToTransTableTy HostEntriesBeginToTransTable;\nstd::mutex TrlTblMtx;\n\nHostPtrToTableMapTy HostPtrToTableMap;\nstd::mutex TblMapMtx;\n\nvoid RTLsTy::LoadRTLs() {\n#ifdef OMPTARGET_DEBUG\n  if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n    DebugLevel = std::stoi(envStr);\n  }\n#endif // OMPTARGET_DEBUG\n\n  // Parse environment variable OMP_TARGET_OFFLOAD (if set)\n  TargetOffloadPolicy = (kmp_target_offload_kind_t) __kmpc_get_target_offload();\n  if (TargetOffloadPolicy == tgt_disabled) {\n    return;\n  }\n\n  DP(\"Loading RTLs...\\n\");\n\n  // Attempt to open all the plugins and, if they exist, check if the interface\n  // is correct and if they are supporting any devices.\n  for (auto *Name : RTLNames) {\n    DP(\"Loading library '%s'...\\n\", Name);\n    void *dynlib_handle = dlopen(Name, RTLD_NOW);\n\n    if (!dynlib_handle) {\n      // Library does not exist or cannot be found.\n      DP(\"Unable to load library '%s': %s!\\n\", Name, dlerror());\n      continue;\n    }\n\n    DP(\"Successfully loaded library '%s'!\\n\", Name);\n\n    // Retrieve the RTL information from the runtime library.\n    RTLInfoTy R;\n\n    R.LibraryHandler = dynlib_handle;\n    R.isUsed = false;\n\n#ifdef OMPTARGET_DEBUG\n    R.RTLName = Name;\n#endif\n\n    if (!(*((void**) &R.is_valid_binary) = dlsym(\n              dynlib_handle, \"__tgt_rtl_is_valid_binary\")))\n      continue;\n    if (!(*((void**) &R.number_of_devices) = dlsym(\n              dynlib_handle, \"__tgt_rtl_number_of_devices\")))\n      continue;\n    if (!(*((void**) &R.init_device) = dlsym(\n              dynlib_handle, \"__tgt_rtl_init_device\")))\n      continue;\n    if (!(*((void**) &R.load_binary) = dlsym(\n              dynlib_handle, \"__tgt_rtl_load_binary\")))\n      continue;\n    if (!(*((void**) &R.data_alloc) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_alloc\")))\n      continue;\n    if (!(*((void**) &R.data_submit) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_submit\")))\n      continue;\n    if (!(*((void**) &R.data_retrieve) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_retrieve\")))\n      continue;\n    if (!(*((void**) &R.data_delete) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_delete\")))\n      continue;\n    if (!(*((void**) &R.run_region) = dlsym(\n              dynlib_handle, \"__tgt_rtl_run_target_region\")))\n      continue;\n    if (!(*((void**) &R.run_team_region) = dlsym(\n              dynlib_handle, \"__tgt_rtl_run_target_team_region\")))\n      continue;\n\n    // Optional functions\n    *((void**) &R.init_requires) = dlsym(\n        dynlib_handle, \"__tgt_rtl_init_requires\");\n\n    // No devices are supported by this RTL?\n    if (!(R.NumberOfDevices = R.number_of_devices())) {\n      DP(\"No devices supported in this RTL\\n\");\n      continue;\n    }\n\n    DP(\"Registering RTL %s supporting %d devices!\\n\",\n        R.RTLName.c_str(), R.NumberOfDevices);\n\n    // The RTL is valid! Will save the information in the RTLs list.\n    AllRTLs.push_back(R);\n  }\n\n  DP(\"RTLs loaded!\\n\");\n\n  return;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Functionality for registering libs\n\nstatic void RegisterImageIntoTranslationTable(TranslationTable &TT,\n    RTLInfoTy &RTL, __tgt_device_image *image) {\n\n  // same size, as when we increase one, we also increase the other.\n  assert(TT.TargetsTable.size() == TT.TargetsImages.size() &&\n         \"We should have as many images as we have tables!\");\n\n  // Resize the Targets Table and Images to accommodate the new targets if\n  // required\n  unsigned TargetsTableMinimumSize = RTL.Idx + RTL.NumberOfDevices;\n\n  if (TT.TargetsTable.size() < TargetsTableMinimumSize) {\n    TT.TargetsImages.resize(TargetsTableMinimumSize, 0);\n    TT.TargetsTable.resize(TargetsTableMinimumSize, 0);\n  }\n\n  // Register the image in all devices for this target type.\n  for (int32_t i = 0; i < RTL.NumberOfDevices; ++i) {\n    // If we are changing the image we are also invalidating the target table.\n    if (TT.TargetsImages[RTL.Idx + i] != image) {\n      TT.TargetsImages[RTL.Idx + i] = image;\n      TT.TargetsTable[RTL.Idx + i] = 0; // lazy initialization of target table.\n    }\n  }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Functionality for registering Ctors/Dtors\n\nstatic void RegisterGlobalCtorsDtorsForImage(__tgt_bin_desc *desc,\n    __tgt_device_image *img, RTLInfoTy *RTL) {\n\n  for (int32_t i = 0; i < RTL->NumberOfDevices; ++i) {\n    DeviceTy &Device = Devices[RTL->Idx + i];\n    Device.PendingGlobalsMtx.lock();\n    Device.HasPendingGlobals = true;\n    for (__tgt_offload_entry *entry = img->EntriesBegin;\n        entry != img->EntriesEnd; ++entry) {\n      if (entry->flags & OMP_DECLARE_TARGET_CTOR) {\n        DP(\"Adding ctor \" DPxMOD \" to the pending list.\\n\",\n            DPxPTR(entry->addr));\n        Device.PendingCtorsDtors[desc].PendingCtors.push_back(entry->addr);\n      } else if (entry->flags & OMP_DECLARE_TARGET_DTOR) {\n        // Dtors are pushed in reverse order so they are executed from end\n        // to beginning when unregistering the library!\n        DP(\"Adding dtor \" DPxMOD \" to the pending list.\\n\",\n            DPxPTR(entry->addr));\n        Device.PendingCtorsDtors[desc].PendingDtors.push_front(entry->addr);\n      }\n\n      if (entry->flags & OMP_DECLARE_TARGET_LINK) {\n        DP(\"The \\\"link\\\" attribute is not yet supported!\\n\");\n      }\n    }\n    Device.PendingGlobalsMtx.unlock();\n  }\n}\n\nvoid RTLsTy::RegisterRequires(int64_t flags) {\n  // TODO: add more elaborate check.\n  // Minimal check: only set requires flags if previous value\n  // is undefined. This ensures that only the first call to this\n  // function will set the requires flags. All subsequent calls\n  // will be checked for compatibility.\n  assert(flags != OMP_REQ_UNDEFINED &&\n         \"illegal undefined flag for requires directive!\");\n  if (RequiresFlags == OMP_REQ_UNDEFINED) {\n    RequiresFlags = flags;\n    return;\n  }\n\n  // If multiple compilation units are present enforce\n  // consistency across all of them for require clauses:\n  //  - reverse_offload\n  //  - unified_address\n  //  - unified_shared_memory\n  if ((RequiresFlags & OMP_REQ_REVERSE_OFFLOAD) !=\n      (flags & OMP_REQ_REVERSE_OFFLOAD)) {\n    FATAL_MESSAGE0(1,\n        \"'#pragma omp requires reverse_offload' not used consistently!\");\n  }\n  if ((RequiresFlags & OMP_REQ_UNIFIED_ADDRESS) !=\n          (flags & OMP_REQ_UNIFIED_ADDRESS)) {\n    FATAL_MESSAGE0(1,\n        \"'#pragma omp requires unified_address' not used consistently!\");\n  }\n  if ((RequiresFlags & OMP_REQ_UNIFIED_SHARED_MEMORY) !=\n          (flags & OMP_REQ_UNIFIED_SHARED_MEMORY)) {\n    FATAL_MESSAGE0(1,\n        \"'#pragma omp requires unified_shared_memory' not used consistently!\");\n  }\n\n  // TODO: insert any other missing checks\n\n  DP(\"New requires flags %ld compatible with existing %ld!\\n\",\n     flags, RequiresFlags);\n}\n\nvoid RTLsTy::RegisterLib(__tgt_bin_desc *desc) {\n  // Attempt to load all plugins available in the system.\n  std::call_once(initFlag, &RTLsTy::LoadRTLs, this);\n\n  RTLsMtx.lock();\n  // Register the images with the RTLs that understand them, if any.\n  for (int32_t i = 0; i < desc->NumDeviceImages; ++i) {\n    // Obtain the image.\n    __tgt_device_image *img = &desc->DeviceImages[i];\n\n    RTLInfoTy *FoundRTL = NULL;\n\n    // Scan the RTLs that have associated images until we find one that supports\n    // the current image.\n    for (auto &R : RTLs.AllRTLs) {\n      if (!R.is_valid_binary(img)) {\n        DP(\"Image \" DPxMOD \" is NOT compatible with RTL %s!\\n\",\n            DPxPTR(img->ImageStart), R.RTLName.c_str());\n        continue;\n      }\n\n      DP(\"Image \" DPxMOD \" is compatible with RTL %s!\\n\",\n          DPxPTR(img->ImageStart), R.RTLName.c_str());\n\n      // If this RTL is not already in use, initialize it.\n      if (!R.isUsed) {\n        // Initialize the device information for the RTL we are about to use.\n        DeviceTy device(&R);\n        size_t start = Devices.size();\n        Devices.resize(start + R.NumberOfDevices, device);\n        for (int32_t device_id = 0; device_id < R.NumberOfDevices;\n            device_id++) {\n          // global device ID\n          Devices[start + device_id].DeviceID = start + device_id;\n          // RTL local device ID\n          Devices[start + device_id].RTLDeviceID = device_id;\n          // RTL requires flags\n          Devices[start + device_id].RTLRequiresFlags = RequiresFlags;\n        }\n\n        // Initialize the index of this RTL and save it in the used RTLs.\n        R.Idx = (RTLs.UsedRTLs.empty())\n                    ? 0\n                    : RTLs.UsedRTLs.back()->Idx +\n                          RTLs.UsedRTLs.back()->NumberOfDevices;\n        assert((size_t) R.Idx == start &&\n            \"RTL index should equal the number of devices used so far.\");\n        R.isUsed = true;\n        RTLs.UsedRTLs.push_back(&R);\n\n        DP(\"RTL \" DPxMOD \" has index %d!\\n\", DPxPTR(R.LibraryHandler), R.Idx);\n      }\n\n      // Initialize (if necessary) translation table for this library.\n      TrlTblMtx.lock();\n      if(!HostEntriesBeginToTransTable.count(desc->HostEntriesBegin)){\n        TranslationTable &tt =\n            HostEntriesBeginToTransTable[desc->HostEntriesBegin];\n        tt.HostTable.EntriesBegin = desc->HostEntriesBegin;\n        tt.HostTable.EntriesEnd = desc->HostEntriesEnd;\n      }\n\n      // Retrieve translation table for this library.\n      TranslationTable &TransTable =\n          HostEntriesBeginToTransTable[desc->HostEntriesBegin];\n\n      DP(\"Registering image \" DPxMOD \" with RTL %s!\\n\",\n          DPxPTR(img->ImageStart), R.RTLName.c_str());\n      RegisterImageIntoTranslationTable(TransTable, R, img);\n      TrlTblMtx.unlock();\n      FoundRTL = &R;\n\n      // Load ctors/dtors for static objects\n      RegisterGlobalCtorsDtorsForImage(desc, img, FoundRTL);\n\n      // if an RTL was found we are done - proceed to register the next image\n      break;\n    }\n\n    if (!FoundRTL) {\n      DP(\"No RTL found for image \" DPxMOD \"!\\n\", DPxPTR(img->ImageStart));\n    }\n  }\n  RTLsMtx.unlock();\n\n\n  DP(\"Done registering entries!\\n\");\n}\n\nvoid RTLsTy::UnregisterLib(__tgt_bin_desc *desc) {\n  DP(\"Unloading target library!\\n\");\n\n  RTLsMtx.lock();\n  // Find which RTL understands each image, if any.\n  for (int32_t i = 0; i < desc->NumDeviceImages; ++i) {\n    // Obtain the image.\n    __tgt_device_image *img = &desc->DeviceImages[i];\n\n    RTLInfoTy *FoundRTL = NULL;\n\n    // Scan the RTLs that have associated images until we find one that supports\n    // the current image. We only need to scan RTLs that are already being used.\n    for (auto *R : RTLs.UsedRTLs) {\n\n      assert(R->isUsed && \"Expecting used RTLs.\");\n\n      if (!R->is_valid_binary(img)) {\n        DP(\"Image \" DPxMOD \" is NOT compatible with RTL \" DPxMOD \"!\\n\",\n            DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n        continue;\n      }\n\n      DP(\"Image \" DPxMOD \" is compatible with RTL \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n\n      FoundRTL = R;\n\n      // Execute dtors for static objects if the device has been used, i.e.\n      // if its PendingCtors list has been emptied.\n      for (int32_t i = 0; i < FoundRTL->NumberOfDevices; ++i) {\n        DeviceTy &Device = Devices[FoundRTL->Idx + i];\n        Device.PendingGlobalsMtx.lock();\n        if (Device.PendingCtorsDtors[desc].PendingCtors.empty()) {\n          for (auto &dtor : Device.PendingCtorsDtors[desc].PendingDtors) {\n            int rc = target(Device.DeviceID, dtor, 0, NULL, NULL, NULL, NULL, 1,\n                1, true /*team*/);\n            if (rc != OFFLOAD_SUCCESS) {\n              DP(\"Running destructor \" DPxMOD \" failed.\\n\", DPxPTR(dtor));\n            }\n          }\n          // Remove this library's entry from PendingCtorsDtors\n          Device.PendingCtorsDtors.erase(desc);\n        }\n        Device.PendingGlobalsMtx.unlock();\n      }\n\n      DP(\"Unregistered image \" DPxMOD \" from RTL \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n\n      break;\n    }\n\n    // if no RTL was found proceed to unregister the next image\n    if (!FoundRTL){\n      DP(\"No RTLs in use support the image \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart));\n    }\n  }\n  RTLsMtx.unlock();\n  DP(\"Done unregistering images!\\n\");\n\n  // Remove entries from HostPtrToTableMap\n  TblMapMtx.lock();\n  for (__tgt_offload_entry *cur = desc->HostEntriesBegin;\n      cur < desc->HostEntriesEnd; ++cur) {\n    HostPtrToTableMap.erase(cur->addr);\n  }\n\n  // Remove translation table for this descriptor.\n  auto tt = HostEntriesBeginToTransTable.find(desc->HostEntriesBegin);\n  if (tt != HostEntriesBeginToTransTable.end()) {\n    DP(\"Removing translation table for descriptor \" DPxMOD \"\\n\",\n        DPxPTR(desc->HostEntriesBegin));\n    HostEntriesBeginToTransTable.erase(tt);\n  } else {\n    DP(\"Translation table for descriptor \" DPxMOD \" cannot be found, probably \"\n        \"it has been already removed.\\n\", DPxPTR(desc->HostEntriesBegin));\n  }\n\n  TblMapMtx.unlock();\n\n  // TODO: Remove RTL and the devices it manages if it's not used anymore?\n  // TODO: Write some RTL->unload_image(...) function?\n\n  DP(\"Done unregistering library!\\n\");\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/libomptarget/plugins/generic-elf-64bit/src/rtl.cpp": "//===-RTLs/generic-64bit/src/rtl.cpp - Target RTLs Implementation - C++ -*-===//\n//\n// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n// See https://llvm.org/LICENSE.txt for license information.\n// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n//\n//===----------------------------------------------------------------------===//\n//\n// RTL for generic 64-bit machine\n//\n//===----------------------------------------------------------------------===//\n\n#include <cassert>\n#include <cstdio>\n#include <cstring>\n#include <cstdlib>\n#include <dlfcn.h>\n#include <ffi.h>\n#include <gelf.h>\n#include <link.h>\n#include <list>\n#include <string>\n#include <vector>\n\n#include \"omptargetplugin.h\"\n\n#ifndef TARGET_NAME\n#define TARGET_NAME Generic ELF - 64bit\n#endif\n\n#ifndef TARGET_ELF_ID\n#define TARGET_ELF_ID 0\n#endif\n\n#ifdef OMPTARGET_DEBUG\nstatic int DebugLevel = 0;\n\n#define GETNAME2(name) #name\n#define GETNAME(name) GETNAME2(name)\n#define DP(...) \\\n  do { \\\n    if (DebugLevel > 0) { \\\n      DEBUGP(\"Target \" GETNAME(TARGET_NAME) \" RTL\", __VA_ARGS__); \\\n    } \\\n  } while (false)\n#else // OMPTARGET_DEBUG\n#define DP(...) {}\n#endif // OMPTARGET_DEBUG\n\n#include \"../../common/elf_common.c\"\n\n#define NUMBER_OF_DEVICES 4\n#define OFFLOADSECTIONNAME \".omp_offloading.entries\"\n\n/// Array of Dynamic libraries loaded for this target.\nstruct DynLibTy {\n  char *FileName;\n  void *Handle;\n};\n\n/// Keep entries table per device.\nstruct FuncOrGblEntryTy {\n  __tgt_target_table Table;\n};\n\n/// Class containing all the device information.\nclass RTLDeviceInfoTy {\n  std::vector<std::list<FuncOrGblEntryTy>> FuncGblEntries;\n\npublic:\n  std::list<DynLibTy> DynLibs;\n\n  // Record entry point associated with device.\n  void createOffloadTable(int32_t device_id, __tgt_offload_entry *begin,\n                          __tgt_offload_entry *end) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncGblEntries[device_id].emplace_back();\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id].back();\n\n    E.Table.EntriesBegin = begin;\n    E.Table.EntriesEnd = end;\n  }\n\n  // Return true if the entry is associated with device.\n  bool findOffloadEntry(int32_t device_id, void *addr) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id].back();\n\n    for (__tgt_offload_entry *i = E.Table.EntriesBegin, *e = E.Table.EntriesEnd;\n         i < e; ++i) {\n      if (i->addr == addr)\n        return true;\n    }\n\n    return false;\n  }\n\n  // Return the pointer to the target entries table.\n  __tgt_target_table *getOffloadEntriesTable(int32_t device_id) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id].back();\n\n    return &E.Table;\n  }\n\n  RTLDeviceInfoTy(int32_t num_devices) {\n#ifdef OMPTARGET_DEBUG\n    if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n      DebugLevel = std::stoi(envStr);\n    }\n#endif // OMPTARGET_DEBUG\n\n    FuncGblEntries.resize(num_devices);\n  }\n\n  ~RTLDeviceInfoTy() {\n    // Close dynamic libraries\n    for (auto &lib : DynLibs) {\n      if (lib.Handle) {\n        dlclose(lib.Handle);\n        remove(lib.FileName);\n      }\n    }\n  }\n};\n\nstatic RTLDeviceInfoTy DeviceInfo(NUMBER_OF_DEVICES);\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nint32_t __tgt_rtl_is_valid_binary(__tgt_device_image *image) {\n// If we don't have a valid ELF ID we can just fail.\n#if TARGET_ELF_ID < 1\n  return 0;\n#else\n  return elf_check_machine(image, TARGET_ELF_ID);\n#endif\n}\n\nint32_t __tgt_rtl_number_of_devices() { return NUMBER_OF_DEVICES; }\n\nint32_t __tgt_rtl_init_device(int32_t device_id) { return OFFLOAD_SUCCESS; }\n\n__tgt_target_table *__tgt_rtl_load_binary(int32_t device_id,\n                                          __tgt_device_image *image) {\n\n  DP(\"Dev %d: load binary from \" DPxMOD \" image\\n\", device_id,\n     DPxPTR(image->ImageStart));\n\n  assert(device_id >= 0 && device_id < NUMBER_OF_DEVICES && \"bad dev id\");\n\n  size_t ImageSize = (size_t)image->ImageEnd - (size_t)image->ImageStart;\n  size_t NumEntries = (size_t)(image->EntriesEnd - image->EntriesBegin);\n  DP(\"Expecting to have %zd entries defined.\\n\", NumEntries);\n\n  // Is the library version incompatible with the header file?\n  if (elf_version(EV_CURRENT) == EV_NONE) {\n    DP(\"Incompatible ELF library!\\n\");\n    return NULL;\n  }\n\n  // Obtain elf handler\n  Elf *e = elf_memory((char *)image->ImageStart, ImageSize);\n  if (!e) {\n    DP(\"Unable to get ELF handle: %s!\\n\", elf_errmsg(-1));\n    return NULL;\n  }\n\n  if (elf_kind(e) != ELF_K_ELF) {\n    DP(\"Invalid Elf kind!\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  // Find the entries section offset\n  Elf_Scn *section = 0;\n  Elf64_Off entries_offset = 0;\n\n  size_t shstrndx;\n\n  if (elf_getshdrstrndx(e, &shstrndx)) {\n    DP(\"Unable to get ELF strings index!\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  while ((section = elf_nextscn(e, section))) {\n    GElf_Shdr hdr;\n    gelf_getshdr(section, &hdr);\n\n    if (!strcmp(elf_strptr(e, shstrndx, hdr.sh_name), OFFLOADSECTIONNAME)) {\n      entries_offset = hdr.sh_addr;\n      break;\n    }\n  }\n\n  if (!entries_offset) {\n    DP(\"Entries Section Offset Not Found\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  DP(\"Offset of entries section is (\" DPxMOD \").\\n\", DPxPTR(entries_offset));\n\n  // load dynamic library and get the entry points. We use the dl library\n  // to do the loading of the library, but we could do it directly to avoid the\n  // dump to the temporary file.\n  //\n  // 1) Create tmp file with the library contents.\n  // 2) Use dlopen to load the file and dlsym to retrieve the symbols.\n  char tmp_name[] = \"/tmp/tmpfile_XXXXXX\";\n  int tmp_fd = mkstemp(tmp_name);\n\n  if (tmp_fd == -1) {\n    elf_end(e);\n    return NULL;\n  }\n\n  FILE *ftmp = fdopen(tmp_fd, \"wb\");\n\n  if (!ftmp) {\n    elf_end(e);\n    return NULL;\n  }\n\n  fwrite(image->ImageStart, ImageSize, 1, ftmp);\n  fclose(ftmp);\n\n  DynLibTy Lib = {tmp_name, dlopen(tmp_name, RTLD_LAZY)};\n\n  if (!Lib.Handle) {\n    DP(\"Target library loading error: %s\\n\", dlerror());\n    elf_end(e);\n    return NULL;\n  }\n\n  DeviceInfo.DynLibs.push_back(Lib);\n\n  struct link_map *libInfo = (struct link_map *)Lib.Handle;\n\n  // The place where the entries info is loaded is the library base address\n  // plus the offset determined from the ELF file.\n  Elf64_Addr entries_addr = libInfo->l_addr + entries_offset;\n\n  DP(\"Pointer to first entry to be loaded is (\" DPxMOD \").\\n\",\n      DPxPTR(entries_addr));\n\n  // Table of pointers to all the entries in the target.\n  __tgt_offload_entry *entries_table = (__tgt_offload_entry *)entries_addr;\n\n  __tgt_offload_entry *entries_begin = &entries_table[0];\n  __tgt_offload_entry *entries_end = entries_begin + NumEntries;\n\n  if (!entries_begin) {\n    DP(\"Can't obtain entries begin\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  DP(\"Entries table range is (\" DPxMOD \")->(\" DPxMOD \")\\n\",\n      DPxPTR(entries_begin), DPxPTR(entries_end));\n  DeviceInfo.createOffloadTable(device_id, entries_begin, entries_end);\n\n  elf_end(e);\n\n  return DeviceInfo.getOffloadEntriesTable(device_id);\n}\n\nvoid *__tgt_rtl_data_alloc(int32_t device_id, int64_t size, void *hst_ptr) {\n  void *ptr = malloc(size);\n  return ptr;\n}\n\nint32_t __tgt_rtl_data_submit(int32_t device_id, void *tgt_ptr, void *hst_ptr,\n                              int64_t size) {\n  memcpy(tgt_ptr, hst_ptr, size);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_data_retrieve(int32_t device_id, void *hst_ptr, void *tgt_ptr,\n                                int64_t size) {\n  memcpy(hst_ptr, tgt_ptr, size);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_data_delete(int32_t device_id, void *tgt_ptr) {\n  free(tgt_ptr);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_run_target_team_region(int32_t device_id, void *tgt_entry_ptr,\n    void **tgt_args, ptrdiff_t *tgt_offsets, int32_t arg_num, int32_t team_num,\n    int32_t thread_limit, uint64_t loop_tripcount /*not used*/) {\n  // ignore team num and thread limit.\n\n  // Use libffi to launch execution.\n  ffi_cif cif;\n\n  // All args are references.\n  std::vector<ffi_type *> args_types(arg_num, &ffi_type_pointer);\n  std::vector<void *> args(arg_num);\n  std::vector<void *> ptrs(arg_num);\n\n  for (int32_t i = 0; i < arg_num; ++i) {\n    ptrs[i] = (void *)((intptr_t)tgt_args[i] + tgt_offsets[i]);\n    args[i] = &ptrs[i];\n  }\n\n  ffi_status status = ffi_prep_cif(&cif, FFI_DEFAULT_ABI, arg_num,\n                                   &ffi_type_void, &args_types[0]);\n\n  assert(status == FFI_OK && \"Unable to prepare target launch!\");\n\n  if (status != FFI_OK)\n    return OFFLOAD_FAIL;\n\n  DP(\"Running entry point at \" DPxMOD \"...\\n\", DPxPTR(tgt_entry_ptr));\n\n  void (*entry)(void);\n  *((void**) &entry) = tgt_entry_ptr;\n  ffi_call(&cif, entry, NULL, &args[0]);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_run_target_region(int32_t device_id, void *tgt_entry_ptr,\n    void **tgt_args, ptrdiff_t *tgt_offsets, int32_t arg_num) {\n  // use one team and one thread.\n  return __tgt_rtl_run_target_team_region(device_id, tgt_entry_ptr, tgt_args,\n      tgt_offsets, arg_num, 1, 1, 0);\n}\n\n#ifdef __cplusplus\n}\n#endif\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/runtime/doc/Reference.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-9.0.0-ckddhw3tlhnuahi2mzcw3jvjaynag2mn/spack-src/www/Reference.pdf"
    ],
    "total_files": 430
}