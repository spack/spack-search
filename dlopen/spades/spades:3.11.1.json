{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/jemalloc/jemalloc.c": "#define\tJEMALLOC_C_\n#include \"jemalloc/internal/jemalloc_internal.h\"\n\n/******************************************************************************/\n/* Data. */\n\nmalloc_tsd_data(, arenas, arena_t *, NULL)\nmalloc_tsd_data(, thread_allocated, thread_allocated_t,\n    THREAD_ALLOCATED_INITIALIZER)\n\n#include \"intercept.inl\"\n\n/* Runtime configuration options. */\nconst char\t*je_malloc_conf;\n#ifdef JEMALLOC_DEBUG\nbool\topt_abort = true;\n#  ifdef JEMALLOC_FILL\nbool\topt_junk = true;\n#  else\nbool\topt_junk = false;\n#  endif\n#else\nbool\topt_abort = false;\nbool\topt_junk = false;\n#endif\nsize_t\topt_quarantine = ZU(0);\nbool\topt_redzone = false;\nbool\topt_utrace = false;\nbool\topt_valgrind = false;\n#ifdef JEMALLOC_XMALLOC\nbool\topt_xmalloc = true;\n#else\nbool\topt_xmalloc = false;\n#endif\nbool\topt_zero = false;\nsize_t\topt_narenas = 0;\n\nunsigned\tncpus;\n\nmalloc_mutex_t\t\tarenas_lock;\narena_t\t\t\t**arenas;\nunsigned\t\tnarenas_total;\nunsigned\t\tnarenas_auto;\n\n/* Set to true once the allocator has been initialized. */\nstatic bool\t\tmalloc_initialized = false;\n\n#ifdef JEMALLOC_THREADED_INIT\n/* Used to let the initializing thread recursively allocate. */\n#  define NO_INITIALIZER\t((unsigned long)0)\n#  define INITIALIZER\t\tpthread_self()\n#  define IS_INITIALIZER\t(malloc_initializer == pthread_self())\nstatic pthread_t\t\tmalloc_initializer = NO_INITIALIZER;\n#else\n#  define NO_INITIALIZER\tfalse\n#  define INITIALIZER\t\ttrue\n#  define IS_INITIALIZER\tmalloc_initializer\nstatic bool\t\t\tmalloc_initializer = NO_INITIALIZER;\n#endif\n\n/* Used to avoid initialization races. */\n#ifdef _WIN32\nstatic malloc_mutex_t\tinit_lock;\n\nJEMALLOC_ATTR(constructor)\nstatic void WINAPI\n_init_init_lock(void)\n{\n\n\tmalloc_mutex_init(&init_lock);\n}\n\n#ifdef _MSC_VER\n#  pragma section(\".CRT$XCU\", read)\nJEMALLOC_SECTION(\".CRT$XCU\") JEMALLOC_ATTR(used)\nstatic const void (WINAPI *init_init_lock)(void) = _init_init_lock;\n#endif\n\n#else\nstatic malloc_mutex_t\tinit_lock = MALLOC_MUTEX_INITIALIZER;\n#endif\n\ntypedef struct {\n\tvoid\t*p;\t/* Input pointer (as in realloc(p, s)). */\n\tsize_t\ts;\t/* Request size. */\n\tvoid\t*r;\t/* Result pointer. */\n} malloc_utrace_t;\n\n#ifdef JEMALLOC_UTRACE\n#  define UTRACE(a, b, c) do {\t\t\t\t\t\t\\\n\tif (opt_utrace) {\t\t\t\t\t\t\\\n\t\tmalloc_utrace_t ut;\t\t\t\t\t\\\n\t\tut.p = (a);\t\t\t\t\t\t\\\n\t\tut.s = (b);\t\t\t\t\t\t\\\n\t\tut.r = (c);\t\t\t\t\t\t\\\n\t\tutrace(&ut, sizeof(ut));\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n#else\n#  define UTRACE(a, b, c)\n#endif\n\n/******************************************************************************/\n/* Function prototypes for non-inline static functions. */\n\nstatic void\tstats_print_atexit(void);\nstatic unsigned\tmalloc_ncpus(void);\nstatic bool\tmalloc_conf_next(char const **opts_p, char const **k_p,\n    size_t *klen_p, char const **v_p, size_t *vlen_p);\nstatic void\tmalloc_conf_error(const char *msg, const char *k, size_t klen,\n    const char *v, size_t vlen);\nstatic void\tmalloc_conf_init(void);\nstatic bool\tmalloc_init_hard(void);\nstatic int\timemalign(void **memptr, size_t alignment, size_t size,\n    size_t min_alignment);\n\n/******************************************************************************/\n/*\n * Begin miscellaneous support functions.\n */\n\n/* Create a new arena and insert it into the arenas array at index ind. */\narena_t *\narenas_extend(unsigned ind)\n{\n\tarena_t *ret;\n\n\tret = (arena_t *)base_alloc(sizeof(arena_t));\n\tif (ret != NULL && arena_new(ret, ind) == false) {\n\t\tarenas[ind] = ret;\n\t\treturn (ret);\n\t}\n\t/* Only reached if there is an OOM error. */\n\n\t/*\n\t * OOM here is quite inconvenient to propagate, since dealing with it\n\t * would require a check for failure in the fast path.  Instead, punt\n\t * by using arenas[0].  In practice, this is an extremely unlikely\n\t * failure.\n\t */\n\tmalloc_write(\"<jemalloc>: Error initializing arena\\n\");\n\tif (opt_abort)\n\t\tabort();\n\n\treturn (arenas[0]);\n}\n\n/* Slow path, called only by choose_arena(). */\narena_t *\nchoose_arena_hard(void)\n{\n\tarena_t *ret;\n\n\tif (narenas_auto > 1) {\n\t\tunsigned i, choose, first_null;\n\n\t\tchoose = 0;\n\t\tfirst_null = narenas_auto;\n\t\tmalloc_mutex_lock(&arenas_lock);\n\t\tassert(arenas[0] != NULL);\n\t\tfor (i = 1; i < narenas_auto; i++) {\n\t\t\tif (arenas[i] != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Choose the first arena that has the lowest\n\t\t\t\t * number of threads assigned to it.\n\t\t\t\t */\n\t\t\t\tif (arenas[i]->nthreads <\n\t\t\t\t    arenas[choose]->nthreads)\n\t\t\t\t\tchoose = i;\n\t\t\t} else if (first_null == narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Record the index of the first uninitialized\n\t\t\t\t * arena, in case all extant arenas are in use.\n\t\t\t\t *\n\t\t\t\t * NB: It is possible for there to be\n\t\t\t\t * discontinuities in terms of initialized\n\t\t\t\t * versus uninitialized arenas, due to the\n\t\t\t\t * \"thread.arena\" mallctl.\n\t\t\t\t */\n\t\t\t\tfirst_null = i;\n\t\t\t}\n\t\t}\n\n\t\tif (arenas[choose]->nthreads == 0\n\t\t    || first_null == narenas_auto) {\n\t\t\t/*\n\t\t\t * Use an unloaded arena, or the least loaded arena if\n\t\t\t * all arenas are already initialized.\n\t\t\t */\n\t\t\tret = arenas[choose];\n\t\t} else {\n\t\t\t/* Initialize a new arena. */\n\t\t\tret = arenas_extend(first_null);\n\t\t}\n\t\tret->nthreads++;\n\t\tmalloc_mutex_unlock(&arenas_lock);\n\t} else {\n\t\tret = arenas[0];\n\t\tmalloc_mutex_lock(&arenas_lock);\n\t\tret->nthreads++;\n\t\tmalloc_mutex_unlock(&arenas_lock);\n\t}\n\n\tarenas_tsd_set(&ret);\n\n\treturn (ret);\n}\n\nstatic void\nstats_print_atexit(void)\n{\n\n\tif (config_tcache && config_stats) {\n\t\tunsigned narenas, i;\n\n\t\t/*\n\t\t * Merge stats from extant threads.  This is racy, since\n\t\t * individual threads do not lock when recording tcache stats\n\t\t * events.  As a consequence, the final stats may be slightly\n\t\t * out of date by the time they are reported, if other threads\n\t\t * continue to allocate.\n\t\t */\n\t\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\t\tarena_t *arena = arenas[i];\n\t\t\tif (arena != NULL) {\n\t\t\t\ttcache_t *tcache;\n\n\t\t\t\t/*\n\t\t\t\t * tcache_stats_merge() locks bins, so if any\n\t\t\t\t * code is introduced that acquires both arena\n\t\t\t\t * and bin locks in the opposite order,\n\t\t\t\t * deadlocks may result.\n\t\t\t\t */\n\t\t\t\tmalloc_mutex_lock(&arena->lock);\n\t\t\t\tql_foreach(tcache, &arena->tcache_ql, link) {\n\t\t\t\t\ttcache_stats_merge(tcache, arena);\n\t\t\t\t}\n\t\t\t\tmalloc_mutex_unlock(&arena->lock);\n\t\t\t}\n\t\t}\n\t}\n\tje_malloc_stats_print(NULL, NULL, NULL);\n}\n\n/*\n * End miscellaneous support functions.\n */\n/******************************************************************************/\n/*\n * Begin initialization functions.\n */\n\nstatic unsigned\nmalloc_ncpus(void)\n{\n\tunsigned ret;\n\tlong result;\n\n#ifdef _WIN32\n\tSYSTEM_INFO si;\n\tGetSystemInfo(&si);\n\tresult = si.dwNumberOfProcessors;\n#else\n\tresult = sysconf(_SC_NPROCESSORS_CONF);\n#endif\n\tif (result == -1) {\n\t\t/* Error. */\n\t\tret = 1;\n\t}  else {\n    ret = (unsigned)result;\n  }\n\n\treturn (ret);\n}\n\nvoid\narenas_cleanup(void *arg)\n{\n\tarena_t *arena = *(arena_t **)arg;\n\n\tmalloc_mutex_lock(&arenas_lock);\n\tarena->nthreads--;\n\tmalloc_mutex_unlock(&arenas_lock);\n}\n\nstatic inline bool\nmalloc_init(void)\n{\n\n\tif (malloc_initialized == false)\n\t\treturn (malloc_init_hard());\n\n\treturn (false);\n}\n\nstatic bool\nmalloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,\n    char const **v_p, size_t *vlen_p)\n{\n\tbool accept;\n\tconst char *opts = *opts_p;\n\n\t*k_p = opts;\n\n\tfor (accept = false; accept == false;) {\n\t\tswitch (*opts) {\n\t\tcase 'A': case 'B': case 'C': case 'D': case 'E': case 'F':\n\t\tcase 'G': case 'H': case 'I': case 'J': case 'K': case 'L':\n\t\tcase 'M': case 'N': case 'O': case 'P': case 'Q': case 'R':\n\t\tcase 'S': case 'T': case 'U': case 'V': case 'W': case 'X':\n\t\tcase 'Y': case 'Z':\n\t\tcase 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n\t\tcase 'g': case 'h': case 'i': case 'j': case 'k': case 'l':\n\t\tcase 'm': case 'n': case 'o': case 'p': case 'q': case 'r':\n\t\tcase 's': case 't': case 'u': case 'v': case 'w': case 'x':\n\t\tcase 'y': case 'z':\n\t\tcase '0': case '1': case '2': case '3': case '4': case '5':\n\t\tcase '6': case '7': case '8': case '9':\n\t\tcase '_':\n\t\t\topts++;\n\t\t\tbreak;\n\t\tcase ':':\n\t\t\topts++;\n\t\t\t*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;\n\t\t\t*v_p = opts;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\tif (opts != *opts_p) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with key\\n\");\n\t\t\t}\n\t\t\treturn (true);\n\t\tdefault:\n\t\t\tmalloc_write(\"<jemalloc>: Malformed conf string\\n\");\n\t\t\treturn (true);\n\t\t}\n\t}\n\n\tfor (accept = false; accept == false;) {\n\t\tswitch (*opts) {\n\t\tcase ',':\n\t\t\topts++;\n\t\t\t/*\n\t\t\t * Look ahead one character here, because the next time\n\t\t\t * this function is called, it will assume that end of\n\t\t\t * input has been cleanly reached if no input remains,\n\t\t\t * but we have optimistically already consumed the\n\t\t\t * comma if one exists.\n\t\t\t */\n\t\t\tif (*opts == '\\0') {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with comma\\n\");\n\t\t\t}\n\t\t\t*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\t*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\topts++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t*opts_p = opts;\n\treturn (false);\n}\n\nstatic void\nmalloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,\n    size_t vlen)\n{\n\n\tmalloc_printf(\"<jemalloc>: %s: %.*s:%.*s\\n\", msg, (int)klen, k,\n\t    (int)vlen, v);\n}\n\nstatic void\nmalloc_conf_init(void)\n{\n\tunsigned i;\n\tchar buf[PATH_MAX + 1];\n\tconst char *opts, *k, *v;\n\tsize_t klen, vlen;\n\n\t/*\n\t * Automatically configure valgrind before processing options.  The\n\t * valgrind option remains in jemalloc 3.x for compatibility reasons.\n\t */\n\tif (config_valgrind) {\n\t\topt_valgrind = (RUNNING_ON_VALGRIND != 0) ? true : false;\n\t\tif (config_fill && opt_valgrind) {\n\t\t\topt_junk = false;\n\t\t\tassert(opt_zero == false);\n\t\t\topt_quarantine = JEMALLOC_VALGRIND_QUARANTINE_DEFAULT;\n\t\t\topt_redzone = true;\n\t\t}\n\t\tif (config_tcache && opt_valgrind)\n\t\t\topt_tcache = false;\n\t}\n\n\tfor (i = 0; i < 3; i++) {\n\t\t/* Get runtime configuration. */\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tif (je_malloc_conf != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Use options that were compiled into the\n\t\t\t\t * program.\n\t\t\t\t */\n\t\t\t\topts = je_malloc_conf;\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 1: {\n#ifndef _WIN32\n\t\t\tint linklen;\n\t\t\tconst char *linkname =\n#  ifdef JEMALLOC_PREFIX\n\t\t\t    \"/etc/\"JEMALLOC_PREFIX\"malloc.conf\"\n#  else\n\t\t\t    \"/etc/malloc.conf\"\n#  endif\n\t\t\t    ;\n\n\t\t\tif ((linklen = readlink(linkname, buf,\n\t\t\t    sizeof(buf) - 1)) != -1) {\n\t\t\t\t/*\n\t\t\t\t * Use the contents of the \"/etc/malloc.conf\"\n\t\t\t\t * symbolic link's name.\n\t\t\t\t */\n\t\t\t\tbuf[linklen] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t} else\n#endif\n\t\t\t{\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\t} case 2: {\n\t\t\tconst char *envname =\n#ifdef JEMALLOC_PREFIX\n\t\t\t    JEMALLOC_CPREFIX\"MALLOC_CONF\"\n#else\n\t\t\t    \"MALLOC_CONF\"\n#endif\n\t\t\t    ;\n\n\t\t\tif ((opts = getenv(envname)) != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Do nothing; opts is already initialized to\n\t\t\t\t * the value of the MALLOC_CONF environment\n\t\t\t\t * variable.\n\t\t\t\t */\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\t} default:\n\t\t\t/* NOTREACHED */\n\t\t\tassert(false);\n\t\t\tbuf[0] = '\\0';\n\t\t\topts = buf;\n\t\t}\n\n\t\twhile (*opts != '\\0' && malloc_conf_next(&opts, &k, &klen, &v,\n\t\t    &vlen) == false) {\n#define\tCONF_HANDLE_BOOL_HIT(o, n, hit)\t\t\t\t\t\\\n\t\t\tif (sizeof(n)-1 == klen && strncmp(n, k,\t\\\n\t\t\t    klen) == 0) {\t\t\t\t\\\n\t\t\t\tif (strncmp(\"true\", v, vlen) == 0 &&\t\\\n\t\t\t\t    vlen == sizeof(\"true\")-1)\t\t\\\n\t\t\t\t\to = true;\t\t\t\\\n\t\t\t\telse if (strncmp(\"false\", v, vlen) ==\t\\\n\t\t\t\t    0 && vlen == sizeof(\"false\")-1)\t\\\n\t\t\t\t\to = false;\t\t\t\\\n\t\t\t\telse {\t\t\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\thit = true;\t\t\t\t\\\n\t\t\t} else\t\t\t\t\t\t\\\n\t\t\t\thit = false;\n#define\tCONF_HANDLE_BOOL(o, n) {\t\t\t\t\t\\\n\t\t\tbool hit;\t\t\t\t\t\\\n\t\t\tCONF_HANDLE_BOOL_HIT(o, n, hit);\t\t\\\n\t\t\tif (hit)\t\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n}\n#define\tCONF_HANDLE_SIZE_T(o, n, min, max)\t\t\t\t\\\n\t\t\tif (sizeof(n)-1 == klen && strncmp(n, k,\t\\\n\t\t\t    klen) == 0) {\t\t\t\t\\\n\t\t\t\tuintmax_t um;\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tum = malloc_strtoumax(v, &end, 0);\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (um < min || um > max) {\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Out-of-range conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else\t\t\t\t\t\\\n\t\t\t\t\to = um;\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_SSIZE_T(o, n, min, max)\t\t\t\t\\\n\t\t\tif (sizeof(n)-1 == klen && strncmp(n, k,\t\\\n\t\t\t    klen) == 0) {\t\t\t\t\\\n\t\t\t\tlong l;\t\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tl = strtol(v, &end, 0);\t\t\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (l < (ssize_t)min || l >\t\\\n\t\t\t\t    (ssize_t)max) {\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Out-of-range conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else\t\t\t\t\t\\\n\t\t\t\t\to = l;\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_CHAR_P(o, n, d)\t\t\t\t\t\\\n\t\t\tif (sizeof(n)-1 == klen && strncmp(n, k,\t\\\n\t\t\t    klen) == 0) {\t\t\t\t\\\n\t\t\t\tsize_t cpylen = (vlen <=\t\t\\\n\t\t\t\t    sizeof(o)-1) ? vlen :\t\t\\\n\t\t\t\t    sizeof(o)-1;\t\t\t\\\n\t\t\t\tstrncpy(o, v, cpylen);\t\t\t\\\n\t\t\t\to[cpylen] = '\\0';\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n\n\t\t\tCONF_HANDLE_BOOL(opt_abort, \"abort\")\n\t\t\t/*\n\t\t\t * Chunks always require at least one header page, plus\n\t\t\t * one data page in the absence of redzones, or three\n\t\t\t * pages in the presence of redzones.  In order to\n\t\t\t * simplify options processing, fix the limit based on\n\t\t\t * config_fill.\n\t\t\t */\n\t\t\tCONF_HANDLE_SIZE_T(opt_lg_chunk, \"lg_chunk\", LG_PAGE +\n\t\t\t    (config_fill ? 2 : 1), (sizeof(size_t) << 3) - 1)\n\t\t\tif (strncmp(\"dss\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < dss_prec_limit; i++) {\n\t\t\t\t\tif (strncmp(dss_prec_names[i], v, vlen)\n\t\t\t\t\t    == 0) {\n\t\t\t\t\t\tif (chunk_dss_prec_set(i)) {\n\t\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\t    \"Error setting dss\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_dss =\n\t\t\t\t\t\t\t    dss_prec_names[i];\n\t\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (match == false) {\n\t\t\t\t\tmalloc_conf_error(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tCONF_HANDLE_SIZE_T(opt_narenas, \"narenas\", 1,\n\t\t\t    SIZE_T_MAX)\n\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_dirty_mult, \"lg_dirty_mult\",\n\t\t\t    -1, (sizeof(size_t) << 3) - 1)\n\t\t\tCONF_HANDLE_BOOL(opt_stats_print, \"stats_print\")\n\t\t\tif (config_fill) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_junk, \"junk\")\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_quarantine, \"quarantine\",\n\t\t\t\t    0, SIZE_T_MAX)\n\t\t\t\tCONF_HANDLE_BOOL(opt_redzone, \"redzone\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_zero, \"zero\")\n\t\t\t}\n\t\t\tif (config_utrace) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_utrace, \"utrace\")\n\t\t\t}\n\t\t\tif (config_valgrind) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_valgrind, \"valgrind\")\n\t\t\t}\n\t\t\tif (config_xmalloc) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_xmalloc, \"xmalloc\")\n\t\t\t}\n\t\t\tif (config_tcache) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_tcache, \"tcache\")\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_tcache_max,\n\t\t\t\t    \"lg_tcache_max\", -1,\n\t\t\t\t    (sizeof(size_t) << 3) - 1)\n\t\t\t}\n\t\t\tif (config_prof) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof, \"prof\")\n\t\t\t\tCONF_HANDLE_CHAR_P(opt_prof_prefix,\n\t\t\t\t    \"prof_prefix\", \"jeprof\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_active, \"prof_active\")\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_sample,\n\t\t\t\t    \"lg_prof_sample\", 0,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_accum, \"prof_accum\")\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_interval,\n\t\t\t\t    \"lg_prof_interval\", -1,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_gdump, \"prof_gdump\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_final, \"prof_final\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_leak, \"prof_leak\")\n\t\t\t}\n\t\t\tmalloc_conf_error(\"Invalid conf pair\", k, klen, v,\n\t\t\t    vlen);\n#undef CONF_HANDLE_BOOL\n#undef CONF_HANDLE_SIZE_T\n#undef CONF_HANDLE_SSIZE_T\n#undef CONF_HANDLE_CHAR_P\n\t\t}\n\t}\n}\n\nstatic bool\nmalloc_init_hard(void)\n{\n\tarena_t *init_arenas[1];\n\n\tmalloc_mutex_lock(&init_lock);\n\tif (malloc_initialized || IS_INITIALIZER) {\n\t\t/*\n\t\t * Another thread initialized the allocator before this one\n\t\t * acquired init_lock, or this thread is the initializing\n\t\t * thread, and it is recursively allocating.\n\t\t */\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (false);\n\t}\n#ifdef JEMALLOC_THREADED_INIT\n\tif (malloc_initializer != NO_INITIALIZER && IS_INITIALIZER == false) {\n\t\t/* Busy-wait until the initializing thread completes. */\n\t\tdo {\n\t\t\tmalloc_mutex_unlock(&init_lock);\n\t\t\tCPU_SPINWAIT;\n\t\t\tmalloc_mutex_lock(&init_lock);\n\t\t} while (malloc_initialized == false);\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (false);\n\t}\n#endif\n\tmalloc_initializer = INITIALIZER;\n\n\tmalloc_tsd_boot();\n\tif (config_prof)\n\t\tprof_boot0();\n\n\tmalloc_conf_init();\n\n#if (!defined(JEMALLOC_MUTEX_INIT_CB) && !defined(JEMALLOC_ZONE) \\\n    && !defined(_WIN32))\n\t/* Register fork handlers. */\n\tif (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,\n\t    jemalloc_postfork_child) != 0) {\n\t\tmalloc_write(\"<jemalloc>: Error in pthread_atfork()\\n\");\n\t\tif (opt_abort)\n\t\t\tabort();\n\t}\n#endif\n\n\tif (opt_stats_print) {\n\t\t/* Print statistics at exit. */\n\t\tif (atexit(stats_print_atexit) != 0) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in atexit()\\n\");\n\t\t\tif (opt_abort)\n\t\t\t\tabort();\n\t\t}\n\t}\n\n\tif (base_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (chunk_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (ctl_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_prof)\n\t\tprof_boot1();\n\n\tarena_boot();\n\n\tif (config_tcache && tcache_boot0()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (huge_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (malloc_mutex_init(&arenas_lock))\n\t\treturn (true);\n\n\t/*\n\t * Create enough scaffolding to allow recursive allocation in\n\t * malloc_ncpus().\n\t */\n\tnarenas_total = narenas_auto = 1;\n\tarenas = init_arenas;\n\tmemset(arenas, 0, sizeof(arena_t *) * narenas_auto);\n\n\t/*\n\t * Initialize one arena here.  The rest are lazily created in\n\t * choose_arena_hard().\n\t */\n\tarenas_extend(0);\n\tif (arenas[0] == NULL) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\t/* Initialize allocation counters before any allocations can occur. */\n\tif (config_stats && thread_allocated_tsd_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (arenas_tsd_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_tcache && tcache_boot1()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_fill && quarantine_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_prof && prof_boot2()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\t/* Get number of CPUs. */\n\tmalloc_mutex_unlock(&init_lock);\n\tncpus = malloc_ncpus();\n\tmalloc_mutex_lock(&init_lock);\n\n\tif (mutex_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (opt_narenas == 0) {\n\t\t/*\n\t\t * For SMP systems, create more than one arena per CPU by\n\t\t * default.\n\t\t */\n\t\tif (ncpus > 1)\n\t\t\topt_narenas = ncpus << 2;\n\t\telse\n\t\t\topt_narenas = 1;\n\t}\n\tnarenas_auto = opt_narenas;\n\t/*\n\t * Make sure that the arenas array can be allocated.  In practice, this\n\t * limit is enough to allow the allocator to function, but the ctl\n\t * machinery will fail to allocate memory at far lower limits.\n\t */\n\tif (narenas_auto > chunksize / sizeof(arena_t *)) {\n\t\tnarenas_auto = chunksize / sizeof(arena_t *);\n\t\tmalloc_printf(\"<jemalloc>: Reducing narenas to limit (%d)\\n\",\n\t\t    narenas_auto);\n\t}\n\tnarenas_total = narenas_auto;\n\n\t/* Allocate and initialize arenas. */\n\tarenas = (arena_t **)base_alloc(sizeof(arena_t *) * narenas_total);\n\tif (arenas == NULL) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\t/*\n\t * Zero the array.  In practice, this should always be pre-zeroed,\n\t * since it was just mmap()ed, but let's be sure.\n\t */\n\tmemset(arenas, 0, sizeof(arena_t *) * narenas_total);\n\t/* Copy the pointer to the one arena that was already initialized. */\n\tarenas[0] = init_arenas[0];\n\n\tmalloc_initialized = true;\n\tmalloc_mutex_unlock(&init_lock);\n\treturn (false);\n}\n\n/*\n * End initialization functions.\n */\n/******************************************************************************/\n/*\n * Begin malloc(3)-compatible functions.\n */\n\nvoid *\nje_malloc(size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tprof_thr_cnt_t *cnt JEMALLOC_CC_SILENCE_INIT(NULL);\n\n\tif (malloc_init()) {\n\t\tret = NULL;\n\t\tgoto label_oom;\n\t}\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tif (config_prof && opt_prof) {\n\t\tusize = s2u(size);\n\t\tPROF_ALLOC_PREP(1, usize, cnt);\n\t\tif (cnt == NULL) {\n\t\t\tret = NULL;\n\t\t\tgoto label_oom;\n\t\t}\n\t\tif (prof_promote && (uintptr_t)cnt != (uintptr_t)1U && usize <=\n\t\t    SMALL_MAXCLASS) {\n\t\t\tret = imalloc(SMALL_MAXCLASS+1);\n\t\t\tif (ret != NULL)\n\t\t\t\tarena_prof_promoted(ret, usize);\n\t\t} else\n\t\t\tret = imalloc(size);\n\t} else {\n\t\tif (config_stats || (config_valgrind && opt_valgrind))\n\t\t\tusize = s2u(size);\n\t\tret = imalloc(size);\n\t}\n\nlabel_oom:\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n            size_t *cactive, sz = sizeof(cactive);\n            xmallctl(\"stats.cactive\", &cactive, &sz, NULL, 0);\n\t\t\tmalloc_printf(\"<jemalloc>: Error in malloc(): out of memory. Requested: %zu, active: %zu\\n\", size, atomic_read_z(cactive));\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_prof && opt_prof && ret != NULL)\n\t\tprof_malloc(ret, usize, cnt);\n\tif (config_stats && ret != NULL) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, false);\n\treturn (ret);\n}\n\nJEMALLOC_ATTR(nonnull(1))\n#ifdef JEMALLOC_PROF\n/*\n * Avoid any uncertainty as to how many backtrace frames to ignore in\n * PROF_ALLOC_PREP().\n */\nJEMALLOC_ATTR(noinline)\n#endif\nstatic int\nimemalign(void **memptr, size_t alignment, size_t size,\n    size_t min_alignment)\n{\n\tint ret;\n\tsize_t usize;\n\tvoid *result;\n\tprof_thr_cnt_t *cnt JEMALLOC_CC_SILENCE_INIT(NULL);\n\n\tassert(min_alignment != 0);\n\n\tif (malloc_init())\n\t\tresult = NULL;\n\telse {\n\t\tif (size == 0)\n\t\t\tsize = 1;\n\n\t\t/* Make sure that alignment is a large enough power of 2. */\n\t\tif (((alignment - 1) & alignment) != 0\n\t\t    || (alignment < min_alignment)) {\n\t\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Error allocating \"\n\t\t\t\t    \"aligned memory: invalid alignment\\n\");\n\t\t\t\tabort();\n\t\t\t}\n\t\t\tresult = NULL;\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tusize = sa2u(size, alignment);\n\t\tif (usize == 0) {\n\t\t\tresult = NULL;\n\t\t\tret = ENOMEM;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tPROF_ALLOC_PREP(2, usize, cnt);\n\t\t\tif (cnt == NULL) {\n\t\t\t\tresult = NULL;\n\t\t\t\tret = EINVAL;\n\t\t\t} else {\n\t\t\t\tif (prof_promote && (uintptr_t)cnt !=\n\t\t\t\t    (uintptr_t)1U && usize <= SMALL_MAXCLASS) {\n\t\t\t\t\tassert(sa2u(SMALL_MAXCLASS+1,\n\t\t\t\t\t    alignment) != 0);\n\t\t\t\t\tresult = ipalloc(sa2u(SMALL_MAXCLASS+1,\n\t\t\t\t\t    alignment), alignment, false);\n\t\t\t\t\tif (result != NULL) {\n\t\t\t\t\t\tarena_prof_promoted(result,\n\t\t\t\t\t\t    usize);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tresult = ipalloc(usize, alignment,\n\t\t\t\t\t    false);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tresult = ipalloc(usize, alignment, false);\n\t}\n\n\tif (result == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n            size_t *cactive, sz = sizeof(cactive);\n            xmallctl(\"stats.cactive\", &cactive, &sz, NULL, 0);\n\t\t\tmalloc_printf(\"<jemalloc>: Error allocating aligned memory: out of memory. Requested: %zu, active: %zu\\n\", size, atomic_read_z(cactive));\n\t\t\tabort();\n\t\t}\n\t\tret = ENOMEM;\n\t\tgoto label_return;\n\t}\n\n\t*memptr = result;\n\tret = 0;\n\nlabel_return:\n\tif (config_stats && result != NULL) {\n\t\tassert(usize == isalloc(result, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tif (config_prof && opt_prof && result != NULL)\n\t\tprof_malloc(result, usize, cnt);\n\tUTRACE(0, size, result);\n\treturn (ret);\n}\n\nint\nje_posix_memalign(void **memptr, size_t alignment, size_t size)\n{\n\tint ret = imemalign(memptr, alignment, size, sizeof(void *));\n\tJEMALLOC_VALGRIND_MALLOC(ret == 0, *memptr, isalloc(*memptr,\n\t    config_prof), false);\n\treturn (ret);\n}\n\nvoid *\nje_aligned_alloc(size_t alignment, size_t size)\n{\n\tvoid *ret;\n\tint err;\n\n\tif ((err = imemalign(&ret, alignment, size, 1)) != 0) {\n\t\tret = NULL;\n\t\tset_errno(err);\n\t}\n\tJEMALLOC_VALGRIND_MALLOC(err == 0, ret, isalloc(ret, config_prof),\n\t    false);\n\treturn (ret);\n}\n\nvoid *\nje_calloc(size_t num, size_t size)\n{\n\tvoid *ret;\n\tsize_t num_size;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tprof_thr_cnt_t *cnt JEMALLOC_CC_SILENCE_INIT(NULL);\n\n\tif (malloc_init()) {\n\t\tnum_size = 0;\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tnum_size = num * size;\n\tif (num_size == 0) {\n\t\tif (num == 0 || size == 0)\n\t\t\tnum_size = 1;\n\t\telse {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t/*\n\t * Try to avoid division here.  We know that it isn't possible to\n\t * overflow during multiplication if neither operand uses any of the\n\t * most significant half of the bits in a size_t.\n\t */\n\t} else if (((num | size) & (SIZE_T_MAX << (sizeof(size_t) << 2)))\n\t    && (num_size / size != num)) {\n\t\t/* size_t overflow. */\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tif (config_prof && opt_prof) {\n\t\tusize = s2u(num_size);\n\t\tPROF_ALLOC_PREP(1, usize, cnt);\n\t\tif (cnt == NULL) {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tif (prof_promote && (uintptr_t)cnt != (uintptr_t)1U && usize\n\t\t    <= SMALL_MAXCLASS) {\n\t\t\tret = icalloc(SMALL_MAXCLASS+1);\n\t\t\tif (ret != NULL)\n\t\t\t\tarena_prof_promoted(ret, usize);\n\t\t} else\n\t\t\tret = icalloc(num_size);\n\t} else {\n\t\tif (config_stats || (config_valgrind && opt_valgrind))\n\t\t\tusize = s2u(num_size);\n\t\tret = icalloc(num_size);\n\t}\n\nlabel_return:\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in calloc(): out of \"\n\t\t\t    \"memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\n\tif (config_prof && opt_prof && ret != NULL)\n\t\tprof_malloc(ret, usize, cnt);\n\tif (config_stats && ret != NULL) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, num_size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, true);\n\treturn (ret);\n}\n\nvoid *\nje_realloc(void *ptr, size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t old_size = 0;\n\tsize_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tprof_thr_cnt_t *cnt JEMALLOC_CC_SILENCE_INIT(NULL);\n\tprof_ctx_t *old_ctx JEMALLOC_CC_SILENCE_INIT(NULL);\n\n\tif (size == 0) {\n\t\tif (ptr != NULL) {\n\t\t\t/* realloc(ptr, 0) is equivalent to free(p). */\n\t\t\tif (config_prof) {\n\t\t\t\told_size = isalloc(ptr, true);\n\t\t\t\tif (config_valgrind && opt_valgrind)\n\t\t\t\t\told_rzsize = p2rz(ptr);\n\t\t\t} else if (config_stats) {\n\t\t\t\told_size = isalloc(ptr, false);\n\t\t\t\tif (config_valgrind && opt_valgrind)\n\t\t\t\t\told_rzsize = u2rz(old_size);\n\t\t\t} else if (config_valgrind && opt_valgrind) {\n\t\t\t\told_size = isalloc(ptr, false);\n\t\t\t\told_rzsize = u2rz(old_size);\n\t\t\t}\n\t\t\tif (config_prof && opt_prof) {\n\t\t\t\told_ctx = prof_ctx_get(ptr);\n\t\t\t\tcnt = NULL;\n\t\t\t}\n\t\t\tiqalloc(ptr);\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t} else\n\t\t\tsize = 1;\n\t}\n\n\tif (ptr != NULL) {\n\t\tassert(malloc_initialized || IS_INITIALIZER);\n\n\t\tif (config_prof) {\n\t\t\told_size = isalloc(ptr, true);\n\t\t\tif (config_valgrind && opt_valgrind)\n\t\t\t\told_rzsize = p2rz(ptr);\n\t\t} else if (config_stats) {\n\t\t\told_size = isalloc(ptr, false);\n\t\t\tif (config_valgrind && opt_valgrind)\n\t\t\t\told_rzsize = u2rz(old_size);\n\t\t} else if (config_valgrind && opt_valgrind) {\n\t\t\told_size = isalloc(ptr, false);\n\t\t\told_rzsize = u2rz(old_size);\n\t\t}\n\t\tif (config_prof && opt_prof) {\n\t\t\tusize = s2u(size);\n\t\t\told_ctx = prof_ctx_get(ptr);\n\t\t\tPROF_ALLOC_PREP(1, usize, cnt);\n\t\t\tif (cnt == NULL) {\n\t\t\t\told_ctx = NULL;\n\t\t\t\tret = NULL;\n\t\t\t\tgoto label_oom;\n\t\t\t}\n\t\t\tif (prof_promote && (uintptr_t)cnt != (uintptr_t)1U &&\n\t\t\t    usize <= SMALL_MAXCLASS) {\n\t\t\t\tret = iralloc(ptr, SMALL_MAXCLASS+1, 0, 0,\n\t\t\t\t    false, false);\n\t\t\t\tif (ret != NULL)\n\t\t\t\t\tarena_prof_promoted(ret, usize);\n\t\t\t\telse\n\t\t\t\t\told_ctx = NULL;\n\t\t\t} else {\n\t\t\t\tret = iralloc(ptr, size, 0, 0, false, false);\n\t\t\t\tif (ret == NULL)\n\t\t\t\t\told_ctx = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (config_stats || (config_valgrind && opt_valgrind))\n\t\t\t\tusize = s2u(size);\n\t\t\tret = iralloc(ptr, size, 0, 0, false, false);\n\t\t}\n\nlabel_oom:\n\t\tif (ret == NULL) {\n\t\t\tif (config_xmalloc && opt_xmalloc) {\n                size_t *cactive, sz = sizeof(cactive);\n                xmallctl(\"stats.cactive\", &cactive, &sz, NULL, 0);\n                malloc_printf(\"<jemalloc>: Error in realloc(): out of memory. Requested: %zu, active: %zu\\n\", size, atomic_read_z(cactive));\n                abort();\n\t\t\t}\n\t\t\tset_errno(ENOMEM);\n\t\t}\n\t} else {\n\t\t/* realloc(NULL, size) is equivalent to malloc(size). */\n\t\tif (config_prof && opt_prof)\n\t\t\told_ctx = NULL;\n\t\tif (malloc_init()) {\n\t\t\tif (config_prof && opt_prof)\n\t\t\t\tcnt = NULL;\n\t\t\tret = NULL;\n\t\t} else {\n\t\t\tif (config_prof && opt_prof) {\n\t\t\t\tusize = s2u(size);\n\t\t\t\tPROF_ALLOC_PREP(1, usize, cnt);\n\t\t\t\tif (cnt == NULL)\n\t\t\t\t\tret = NULL;\n\t\t\t\telse {\n\t\t\t\t\tif (prof_promote && (uintptr_t)cnt !=\n\t\t\t\t\t    (uintptr_t)1U && usize <=\n\t\t\t\t\t    SMALL_MAXCLASS) {\n\t\t\t\t\t\tret = imalloc(SMALL_MAXCLASS+1);\n\t\t\t\t\t\tif (ret != NULL) {\n\t\t\t\t\t\t\tarena_prof_promoted(ret,\n\t\t\t\t\t\t\t    usize);\n\t\t\t\t\t\t}\n\t\t\t\t\t} else\n\t\t\t\t\t\tret = imalloc(size);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (config_stats || (config_valgrind &&\n\t\t\t\t    opt_valgrind))\n\t\t\t\t\tusize = s2u(size);\n\t\t\t\tret = imalloc(size);\n\t\t\t}\n\t\t}\n\n\t\tif (ret == NULL) {\n\t\t\tif (config_xmalloc && opt_xmalloc) {\n                size_t *cactive, sz = sizeof(cactive);\n                xmallctl(\"stats.cactive\", &cactive, &sz, NULL, 0);\n                malloc_printf(\"<jemalloc>: Error in realloc(): out of memory. Requested: %zu, active: %zu\\n\", size, atomic_read_z(cactive));\n\t\t\t\tabort();\n\t\t\t}\n\t\t\tset_errno(ENOMEM);\n\t\t}\n\t}\n\nlabel_return:\n\tif (config_prof && opt_prof)\n\t\tprof_realloc(ret, usize, cnt, old_size, old_ctx);\n\tif (config_stats && ret != NULL) {\n\t\tthread_allocated_t *ta;\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tta = thread_allocated_tsd_get();\n\t\tta->allocated += usize;\n\t\tta->deallocated += old_size;\n\t}\n\tUTRACE(ptr, size, ret);\n\tJEMALLOC_VALGRIND_REALLOC(ret, usize, ptr, old_size, old_rzsize, false);\n\treturn (ret);\n}\n\nvoid\nje_free(void *ptr)\n{\n\n\tUTRACE(ptr, 0, 0);\n\tif (ptr != NULL) {\n\t\tsize_t usize;\n\t\tsize_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\t\tassert(malloc_initialized || IS_INITIALIZER);\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tusize = isalloc(ptr, config_prof);\n\t\t\tprof_free(ptr, usize);\n\t\t} else if (config_stats || config_valgrind)\n\t\t\tusize = isalloc(ptr, config_prof);\n\t\tif (config_stats)\n\t\t\tthread_allocated_tsd_get()->deallocated += usize;\n\t\tif (config_valgrind && opt_valgrind)\n\t\t\trzsize = p2rz(ptr);\n\t\tiqalloc(ptr);\n\t\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n\t}\n}\n\n/*\n * End malloc(3)-compatible functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard override functions.\n */\n\n#ifdef JEMALLOC_OVERRIDE_MEMALIGN\nvoid *\nje_memalign(size_t alignment, size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\timemalign(&ret, alignment, size, 1);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, size, false);\n\treturn (ret);\n}\n#endif\n\n#ifdef JEMALLOC_OVERRIDE_VALLOC\nvoid *\nje_valloc(size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\timemalign(&ret, PAGE, size, 1);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, size, false);\n\treturn (ret);\n}\n#endif\n\n/*\n * is_malloc(je_malloc) is some macro magic to detect if jemalloc_defs.h has\n * #define je_malloc malloc\n */\n#define\tmalloc_is_malloc 1\n#define\tis_malloc_(a) malloc_is_ ## a\n#define\tis_malloc(a) is_malloc_(a)\n\n#if ((is_malloc(je_malloc) == 1) && defined(__GLIBC__) && !defined(__UCLIBC__))\n/*\n * glibc provides the RTLD_DEEPBIND flag for dlopen which can make it possible\n * to inconsistently reference libc's malloc(3)-compatible functions\n * (https://bugzilla.mozilla.org/show_bug.cgi?id=493541).\n *\n * These definitions interpose hooks in glibc.  The functions are actually\n * passed an extra argument for the caller return address, which will be\n * ignored.\n */\nJEMALLOC_EXPORT void (* __free_hook)(void *ptr) = je_free;\nJEMALLOC_EXPORT void *(* __malloc_hook)(size_t size) = je_malloc;\nJEMALLOC_EXPORT void *(* __realloc_hook)(void *ptr, size_t size) = je_realloc;\nJEMALLOC_EXPORT void *(* __memalign_hook)(size_t alignment, size_t size) =\n    je_memalign;\n#endif\n\n/*\n * End non-standard override functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard functions.\n */\n\nsize_t\nje_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr)\n{\n\tsize_t ret;\n\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (config_ivsalloc)\n\t\tret = ivsalloc(ptr, config_prof);\n\telse\n\t\tret = (ptr != NULL) ? isalloc(ptr, config_prof) : 0;\n\n\treturn (ret);\n}\n\nvoid\nje_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,\n    const char *opts)\n{\n\n\tstats_print(write_cb, cbopaque, opts);\n}\n\nint\nje_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,\n    size_t newlen)\n{\n\n\tif (malloc_init())\n\t\treturn (EAGAIN);\n\n\treturn (ctl_byname(name, oldp, oldlenp, newp, newlen));\n}\n\nint\nje_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp)\n{\n\n\tif (malloc_init())\n\t\treturn (EAGAIN);\n\n\treturn (ctl_nametomib(name, mibp, miblenp));\n}\n\nint\nje_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,\n  void *newp, size_t newlen)\n{\n\n\tif (malloc_init())\n\t\treturn (EAGAIN);\n\n\treturn (ctl_bymib(mib, miblen, oldp, oldlenp, newp, newlen));\n}\n\n/*\n * End non-standard functions.\n */\n/******************************************************************************/\n/*\n * Begin experimental functions.\n */\n#ifdef JEMALLOC_EXPERIMENTAL\n\nJEMALLOC_INLINE void *\niallocm(size_t usize, size_t alignment, bool zero, bool try_tcache,\n    arena_t *arena)\n{\n\n\tassert(usize == ((alignment == 0) ? s2u(usize) : sa2u(usize,\n\t    alignment)));\n\n\tif (alignment != 0)\n\t\treturn (ipallocx(usize, alignment, zero, try_tcache, arena));\n\telse if (zero)\n\t\treturn (icallocx(usize, try_tcache, arena));\n\telse\n\t\treturn (imallocx(usize, try_tcache, arena));\n}\n\nint\nje_allocm(void **ptr, size_t *rsize, size_t size, int flags)\n{\n\tvoid *p;\n\tsize_t usize;\n\tsize_t alignment = (ZU(1) << (flags & ALLOCM_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\tbool zero = flags & ALLOCM_ZERO;\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tarena_t *arena;\n\tbool try_tcache;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\n\tif (malloc_init())\n\t\tgoto label_oom;\n\n\tif (arena_ind != UINT_MAX) {\n\t\tarena = arenas[arena_ind];\n\t\ttry_tcache = false;\n\t} else {\n\t\tarena = NULL;\n\t\ttry_tcache = true;\n\t}\n\n\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\tif (usize == 0)\n\t\tgoto label_oom;\n\n\tif (config_prof && opt_prof) {\n\t\tprof_thr_cnt_t *cnt;\n\n\t\tPROF_ALLOC_PREP(1, usize, cnt);\n\t\tif (cnt == NULL)\n\t\t\tgoto label_oom;\n\t\tif (prof_promote && (uintptr_t)cnt != (uintptr_t)1U && usize <=\n\t\t    SMALL_MAXCLASS) {\n\t\t\tsize_t usize_promoted = (alignment == 0) ?\n\t\t\t    s2u(SMALL_MAXCLASS+1) : sa2u(SMALL_MAXCLASS+1,\n\t\t\t    alignment);\n\t\t\tassert(usize_promoted != 0);\n\t\t\tp = iallocm(usize_promoted, alignment, zero,\n\t\t\t    try_tcache, arena);\n\t\t\tif (p == NULL)\n\t\t\t\tgoto label_oom;\n\t\t\tarena_prof_promoted(p, usize);\n\t\t} else {\n\t\t\tp = iallocm(usize, alignment, zero, try_tcache, arena);\n\t\t\tif (p == NULL)\n\t\t\t\tgoto label_oom;\n\t\t}\n\t\tprof_malloc(p, usize, cnt);\n\t} else {\n\t\tp = iallocm(usize, alignment, zero, try_tcache, arena);\n\t\tif (p == NULL)\n\t\t\tgoto label_oom;\n\t}\n\tif (rsize != NULL)\n\t\t*rsize = usize;\n\n\t*ptr = p;\n\tif (config_stats) {\n\t\tassert(usize == isalloc(p, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, p);\n\tJEMALLOC_VALGRIND_MALLOC(true, p, usize, zero);\n\treturn (ALLOCM_SUCCESS);\nlabel_oom:\n\tif (config_xmalloc && opt_xmalloc) {\n        size_t *cactive, sz = sizeof(cactive);\n        xmallctl(\"stats.cactive\", &cactive, &sz, NULL, 0);\n        malloc_printf(\"<jemalloc>: Error in allocm(): out of memory. Requested: %zu, active: %zu\\n\", size, atomic_read_z(cactive));\n\t\tabort();\n\t}\n\t*ptr = NULL;\n\tUTRACE(0, size, 0);\n\treturn (ALLOCM_ERR_OOM);\n}\n\nint\nje_rallocm(void **ptr, size_t *rsize, size_t size, size_t extra, int flags)\n{\n\tvoid *p, *q;\n\tsize_t usize;\n\tsize_t old_size;\n\tsize_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = (ZU(1) << (flags & ALLOCM_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\tbool zero = flags & ALLOCM_ZERO;\n\tbool no_move = flags & ALLOCM_NO_MOVE;\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tbool try_tcache_alloc, try_tcache_dalloc;\n\tarena_t *arena;\n\n\tassert(ptr != NULL);\n\tassert(*ptr != NULL);\n\tassert(size != 0);\n\tassert(SIZE_T_MAX - size >= extra);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (arena_ind != UINT_MAX) {\n\t\tarena_chunk_t *chunk;\n\t\ttry_tcache_alloc = true;\n\t\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(*ptr);\n\t\ttry_tcache_dalloc = (chunk == *ptr || chunk->arena !=\n\t\t    arenas[arena_ind]);\n\t\tarena = arenas[arena_ind];\n\t} else {\n\t\ttry_tcache_alloc = true;\n\t\ttry_tcache_dalloc = true;\n\t\tarena = NULL;\n\t}\n\n\tp = *ptr;\n\tif (config_prof && opt_prof) {\n\t\tprof_thr_cnt_t *cnt;\n\n\t\t/*\n\t\t * usize isn't knowable before iralloc() returns when extra is\n\t\t * non-zero.  Therefore, compute its maximum possible value and\n\t\t * use that in PROF_ALLOC_PREP() to decide whether to capture a\n\t\t * backtrace.  prof_realloc() will use the actual usize to\n\t\t * decide whether to sample.\n\t\t */\n\t\tsize_t max_usize = (alignment == 0) ? s2u(size+extra) :\n\t\t    sa2u(size+extra, alignment);\n\t\tprof_ctx_t *old_ctx = prof_ctx_get(p);\n\t\told_size = isalloc(p, true);\n\t\tif (config_valgrind && opt_valgrind)\n\t\t\told_rzsize = p2rz(p);\n\t\tPROF_ALLOC_PREP(1, max_usize, cnt);\n\t\tif (cnt == NULL)\n\t\t\tgoto label_oom;\n\t\t/*\n\t\t * Use minimum usize to determine whether promotion may happen.\n\t\t */\n\t\tif (prof_promote && (uintptr_t)cnt != (uintptr_t)1U\n\t\t    && ((alignment == 0) ? s2u(size) : sa2u(size, alignment))\n\t\t    <= SMALL_MAXCLASS) {\n\t\t\tq = irallocx(p, SMALL_MAXCLASS+1, (SMALL_MAXCLASS+1 >=\n\t\t\t    size+extra) ? 0 : size+extra - (SMALL_MAXCLASS+1),\n\t\t\t    alignment, zero, no_move, try_tcache_alloc,\n\t\t\t    try_tcache_dalloc, arena);\n\t\t\tif (q == NULL)\n\t\t\t\tgoto label_err;\n\t\t\tif (max_usize < PAGE) {\n\t\t\t\tusize = max_usize;\n\t\t\t\tarena_prof_promoted(q, usize);\n\t\t\t} else\n\t\t\t\tusize = isalloc(q, config_prof);\n\t\t} else {\n\t\t\tq = irallocx(p, size, extra, alignment, zero, no_move,\n\t\t\t    try_tcache_alloc, try_tcache_dalloc, arena);\n\t\t\tif (q == NULL)\n\t\t\t\tgoto label_err;\n\t\t\tusize = isalloc(q, config_prof);\n\t\t}\n\t\tprof_realloc(q, usize, cnt, old_size, old_ctx);\n\t\tif (rsize != NULL)\n\t\t\t*rsize = usize;\n\t} else {\n\t\tif (config_stats) {\n\t\t\told_size = isalloc(p, false);\n\t\t\tif (config_valgrind && opt_valgrind)\n\t\t\t\told_rzsize = u2rz(old_size);\n\t\t} else if (config_valgrind && opt_valgrind) {\n\t\t\told_size = isalloc(p, false);\n\t\t\told_rzsize = u2rz(old_size);\n\t\t}\n\t\tq = irallocx(p, size, extra, alignment, zero, no_move,\n\t\t    try_tcache_alloc, try_tcache_dalloc, arena);\n\t\tif (q == NULL)\n\t\t\tgoto label_err;\n\t\tif (config_stats)\n\t\t\tusize = isalloc(q, config_prof);\n\t\tif (rsize != NULL) {\n\t\t\tif (config_stats == false)\n\t\t\t\tusize = isalloc(q, config_prof);\n\t\t\t*rsize = usize;\n\t\t}\n\t}\n\n\t*ptr = q;\n\tif (config_stats) {\n\t\tthread_allocated_t *ta;\n\t\tta = thread_allocated_tsd_get();\n\t\tta->allocated += usize;\n\t\tta->deallocated += old_size;\n\t}\n\tUTRACE(p, size, q);\n\tJEMALLOC_VALGRIND_REALLOC(q, usize, p, old_size, old_rzsize, zero);\n\treturn (ALLOCM_SUCCESS);\nlabel_err:\n\tif (no_move) {\n\t\tUTRACE(p, size, q);\n\t\treturn (ALLOCM_ERR_NOT_MOVED);\n\t}\nlabel_oom:\n\tif (config_xmalloc && opt_xmalloc) {\n        size_t *cactive, sz = sizeof(cactive);\n        xmallctl(\"stats.cactive\", &cactive, &sz, NULL, 0);\n        malloc_printf(\"<jemalloc>: Error in rallocm(): out of memory. Requested: %zu, active: %zu\\n\", size, atomic_read_z(cactive));\n\t\tabort();\n\t}\n\tUTRACE(p, size, 0);\n\treturn (ALLOCM_ERR_OOM);\n}\n\nint\nje_sallocm(const void *ptr, size_t *rsize, int flags)\n{\n\tsize_t sz;\n\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (config_ivsalloc)\n\t\tsz = ivsalloc(ptr, config_prof);\n\telse {\n\t\tassert(ptr != NULL);\n\t\tsz = isalloc(ptr, config_prof);\n\t}\n\tassert(rsize != NULL);\n\t*rsize = sz;\n\n\treturn (ALLOCM_SUCCESS);\n}\n\nint\nje_dallocm(void *ptr, int flags)\n{\n\tsize_t usize;\n\tsize_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tbool try_tcache;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (arena_ind != UINT_MAX) {\n\t\tarena_chunk_t *chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n\t\ttry_tcache = (chunk == ptr || chunk->arena !=\n\t\t    arenas[arena_ind]);\n\t} else\n\t\ttry_tcache = true;\n\n\tUTRACE(ptr, 0, 0);\n\tif (config_stats || config_valgrind)\n\t\tusize = isalloc(ptr, config_prof);\n\tif (config_prof && opt_prof) {\n\t\tif (config_stats == false && config_valgrind == false)\n\t\t\tusize = isalloc(ptr, config_prof);\n\t\tprof_free(ptr, usize);\n\t}\n\tif (config_stats)\n\t\tthread_allocated_tsd_get()->deallocated += usize;\n\tif (config_valgrind && opt_valgrind)\n\t\trzsize = p2rz(ptr);\n\tiqallocx(ptr, try_tcache);\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n\n\treturn (ALLOCM_SUCCESS);\n}\n\nint\nje_nallocm(size_t *rsize, size_t size, int flags)\n{\n\tsize_t usize;\n\tsize_t alignment = (ZU(1) << (flags & ALLOCM_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\n\tassert(size != 0);\n\n\tif (malloc_init())\n\t\treturn (ALLOCM_ERR_OOM);\n\n\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\tif (usize == 0)\n\t\treturn (ALLOCM_ERR_OOM);\n\n\tif (rsize != NULL)\n\t\t*rsize = usize;\n\treturn (ALLOCM_SUCCESS);\n}\n\n#endif\n/*\n * End experimental functions.\n */\n/******************************************************************************/\n/*\n * The following functions are used by threading libraries for protection of\n * malloc during fork().\n */\n\n/*\n * If an application creates a thread before doing any allocation in the main\n * thread, then calls fork(2) in the main thread followed by memory allocation\n * in the child process, a race can occur that results in deadlock within the\n * child: the main thread may have forked while the created thread had\n * partially initialized the allocator.  Ordinarily jemalloc prevents\n * fork/malloc races via the following functions it registers during\n * initialization using pthread_atfork(), but of course that does no good if\n * the allocator isn't fully initialized at fork time.  The following library\n * constructor is a partial solution to this problem.  It may still possible to\n * trigger the deadlock described above, but doing so would involve forking via\n * a library constructor that runs before jemalloc's runs.\n */\nJEMALLOC_ATTR(constructor)\nstatic void\njemalloc_constructor(void)\n{\n\n\tmalloc_init();\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_prefork(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_prefork(void)\n#endif\n{\n\tunsigned i;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (malloc_initialized == false)\n\t\treturn;\n#endif\n\tassert(malloc_initialized);\n\n\t/* Acquire all mutexes in a safe order. */\n\tctl_prefork();\n\tmalloc_mutex_prefork(&arenas_lock);\n\tfor (i = 0; i < narenas_total; i++) {\n\t\tif (arenas[i] != NULL)\n\t\t\tarena_prefork(arenas[i]);\n\t}\n\tprof_prefork();\n\tchunk_prefork();\n\tbase_prefork();\n\thuge_prefork();\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_postfork_parent(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_postfork(void)\n#endif\n{\n\tunsigned i;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (malloc_initialized == false)\n\t\treturn;\n#endif\n\tassert(malloc_initialized);\n\n\t/* Release all mutexes, now that fork() has completed. */\n\thuge_postfork_parent();\n\tbase_postfork_parent();\n\tchunk_postfork_parent();\n\tprof_postfork_parent();\n\tfor (i = 0; i < narenas_total; i++) {\n\t\tif (arenas[i] != NULL)\n\t\t\tarena_postfork_parent(arenas[i]);\n\t}\n\tmalloc_mutex_postfork_parent(&arenas_lock);\n\tctl_postfork_parent();\n}\n\nvoid\njemalloc_postfork_child(void)\n{\n\tunsigned i;\n\n\tassert(malloc_initialized);\n\n\t/* Release all mutexes, now that fork() has completed. */\n\thuge_postfork_child();\n\tbase_postfork_child();\n\tchunk_postfork_child();\n\tprof_postfork_child();\n\tfor (i = 0; i < narenas_total; i++) {\n\t\tif (arenas[i] != NULL)\n\t\t\tarena_postfork_child(arenas[i]);\n\t}\n\tmalloc_mutex_postfork_child(&arenas_lock);\n\tctl_postfork_child();\n}\n\n/******************************************************************************/\n/*\n * The following functions are used for TLS allocation/deallocation in static\n * binaries on FreeBSD.  The primary difference between these and i[mcd]alloc()\n * is that these avoid accessing TLS variables.\n */\n\nstatic void *\na0alloc(size_t size, bool zero)\n{\n\n\tif (malloc_init())\n\t\treturn (NULL);\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tif (size <= arena_maxclass)\n\t\treturn (arena_malloc(arenas[0], size, zero, false));\n\telse\n\t\treturn (huge_malloc(size, zero));\n}\n\nvoid *\na0malloc(size_t size)\n{\n\n\treturn (a0alloc(size, false));\n}\n\nvoid *\na0calloc(size_t num, size_t size)\n{\n\n\treturn (a0alloc(num * size, true));\n}\n\nvoid\na0free(void *ptr)\n{\n\tarena_chunk_t *chunk;\n\n\tif (ptr == NULL)\n\t\treturn;\n\n\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n\tif (chunk != ptr)\n\t\tarena_dalloc(chunk->arena, chunk, ptr, false);\n\telse\n\t\thuge_dalloc(ptr, true);\n}\n\n/******************************************************************************/\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset_plasmid/pl1.fq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset_plasmid/pl2.fq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/common/sam_parser.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/common/__init__.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/common/SeqIO.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/common/alignment.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/truspades/generate_quality.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/truspades/moleculo_filter_contigs.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/truspades/moleculo_postprocessing.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/src/spades_pipeline/truspades/break_by_coverage.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/cyaml.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/composer.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/dumper.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/serializer.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/reader.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/representer.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/emitter.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/constructor.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/nodes.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/events.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/error.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/parser.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/__init__.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/resolver.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/tokens.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/loader.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/src/python_libs/pyyaml2/scanner.pyc",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/include/boost/smart_ptr/detail/atomic_count_gcc.hpp",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/include/boost/spirit/home/x3/support/subcontext.hpp",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/ext/include/boost/spirit/home/support/char_encoding/iso8859_1.hpp",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset_truspades/B_R2.fastq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset_truspades/B_R1.fastq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset_truspades/A_R1.fastq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset_truspades/A_R2.fastq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset/ecoli_1K_1.fq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset/ecoli_1K_2.fq.gz",
        "/tmp/vanessa/spack-stage/spack-stage-spades-3.11.1-uf72ewrn353jxwpunxrcu4n5imvrspua/spack-src/test_dataset/reference_1K.fa.gz"
    ],
    "total_files": 7317
}