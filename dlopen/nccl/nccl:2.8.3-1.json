{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.8.3-1-3bcyweyu5oveuk5a7rladbqwcq2srpds/spack-src/src/init.cc": "/*************************************************************************\n * Copyright (c) 2015-2020, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nccl.h\"\n#include \"channel.h\"\n#include \"nvmlwrap.h\"\n#include \"bootstrap.h\"\n#include \"transport.h\"\n#include \"group.h\"\n#include \"net.h\"\n#include \"coll_net.h\"\n#include \"enqueue.h\"\n#include \"graph.h\"\n#include \"argcheck.h\"\n#include <fcntl.h>\n#include <string.h>\n#include <errno.h>\n#include <assert.h>\n#include <dlfcn.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n\n#define STR2(v) #v\n#define STR(v) STR2(v)\n\n#ifdef ENABLE_TRACE\nstd::chrono::high_resolution_clock::time_point ncclEpoch;\n#endif\n\n#if CUDART_VERSION >= 9020\n#define NCCL_GROUP_CUDA_STREAM 0 // CGMD: CUDA 9.2,10.X Don't need to use an internal CUDA stream\n#else\n#define NCCL_GROUP_CUDA_STREAM 1 // CGMD: CUDA 9.0,9.1 Need to use an internal CUDA stream\n#endif\n\nconst char* ncclFuncStr[NCCL_NUM_FUNCTIONS] = { \"Broadcast\", \"Reduce\", \"AllGather\", \"ReduceScatter\", \"AllReduce\" };\nconst char* ncclAlgoStr[NCCL_NUM_ALGORITHMS] = { \"Tree\", \"Ring\", \"CollNet\" };\nconst char* ncclProtoStr[NCCL_NUM_PROTOCOLS] = { \"LL\", \"LL128\", \"Simple\" };\n\nNCCL_PARAM(GroupCudaStream, \"GROUP_CUDA_STREAM\", NCCL_GROUP_CUDA_STREAM);\n\nNCCL_PARAM(CheckPointers, \"CHECK_POINTERS\", 0);\n\nncclNet_t* ncclNet = NULL;\nncclCollNet_t* ncclCollNet = NULL;\n\n// Returns ncclInternalError if anything fails, causing that network to be ignored.\nncclResult_t initNet(ncclNet_t* net) {\n  int ndev;\n  if (net->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (net->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initCollNet(ncclCollNet_t* collnet) {\n  int ndev;\n  if (collnet->init(ncclDebugLog) != ncclSuccess) return ncclInternalError;\n  if (collnet->devices(&ndev) != ncclSuccess) return ncclInternalError;\n  if (ndev <= 0) return ncclSystemError;\n  return ncclSuccess;\n}\n\nncclResult_t initNetPlugin(ncclNet_t** net, ncclCollNet_t** collnet) {\n  void* netPluginLib = dlopen(\"libnccl-net.so\", RTLD_NOW | RTLD_LOCAL);\n  if (netPluginLib == NULL) {\n    // dlopen does not guarantee to set errno, but dlerror only gives us a\n    // string, so checking errno doesn't hurt to try to provide a better\n    // error message\n    if (errno == ENOENT) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\");\n    } else {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin : Plugin load returned %d : %s.\", errno, dlerror());\n    }\n    return ncclSuccess;\n  }\n  ncclNet_t* extNet = (ncclNet_t*) dlsym(netPluginLib, STR(NCCL_PLUGIN_SYMBOL));\n  if (extNet == NULL) {\n    INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_PLUGIN_SYMBOL) \" symbol.\");\n  } else if (initNet(extNet) == ncclSuccess) {\n    *net = extNet;\n    // Check for CollNet\n    ncclCollNet_t* extCollNet = (ncclCollNet_t*) dlsym(netPluginLib, STR(NCCL_COLLNET_PLUGIN_SYMBOL));\n    if (extCollNet == NULL) {\n      INFO(NCCL_INIT|NCCL_NET, \"NET/Plugin: Failed to find \" STR(NCCL_COLLNET_PLUGIN_SYMBOL) \" symbol.\");\n    } else if (initCollNet(extCollNet) == ncclSuccess) {\n      *collnet = extCollNet;\n    }\n    return ncclSuccess;\n  }\n  if (netPluginLib != NULL) dlclose(netPluginLib);\n  return ncclSuccess;\n}\n\nncclResult_t initNet() {\n  // Always initialize bootstrap network\n  NCCLCHECK(bootstrapNetInit());\n\n  NCCLCHECK(initNetPlugin(&ncclNet, &ncclCollNet));\n  if (ncclNet != NULL) return ncclSuccess;\n  if (initNet(&ncclNetIb) == ncclSuccess) {\n    ncclNet = &ncclNetIb;\n  } else {\n    NCCLCHECK(initNet(&ncclNetSocket));\n    ncclNet = &ncclNetSocket;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CollNetEnable, \"COLLNET_ENABLE\", 0);\n\npthread_mutex_t initLock = PTHREAD_MUTEX_INITIALIZER;\nstatic bool initialized = false;\nstatic ncclResult_t ncclInit() {\n  if (initialized) return ncclSuccess;\n  pthread_mutex_lock(&initLock);\n  if (!initialized) {\n    initEnv();\n    NCCLCHECK(initNet());\n    INFO(NCCL_INIT, \"Using network %s\", ncclNetName());\n    initialized = true;\n  }\n  pthread_mutex_unlock(&initLock);\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetVersion, int* version);\nncclResult_t ncclGetVersion(int* version) {\n  if (version == NULL) return ncclInvalidArgument;\n  *version = NCCL_VERSION_CODE;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclGetUniqueId, ncclUniqueId* out);\nncclResult_t ncclGetUniqueId(ncclUniqueId* out) {\n  NCCLCHECK(ncclInit());\n  NCCLCHECK(PtrCheck(out, \"GetUniqueId\", \"out\"));\n  return bootstrapGetUniqueId(out);\n}\n\n// Prevent compiler from optimizing out these operations\n#ifdef __clang__\n#define NCCL_NO_OPTIMIZE __attribute__((optnone))\n#else\n#define NCCL_NO_OPTIMIZE __attribute__((optimize(\"O0\")))\n#endif\n\nvoid NCCL_NO_OPTIMIZE commPoison(ncclComm_t comm) {\n  comm->rank = comm->cudaDev = comm->busId = comm->nRanks = -1;\n}\n\n#undef NCCL_NO_OPTIMIZE\n\nstatic ncclResult_t commFree(ncclComm_t comm) {\n  if (comm == NULL)\n    return ncclSuccess;\n  free(comm->connectSend);\n  free(comm->connectRecv);\n  free(comm->p2pSends);\n  free(comm->p2pRecvs);\n  free(comm->asyncOps);\n\n  free(comm->peerInfo);\n  ncclTopoFree(comm->topo);\n\n  if (comm->bootstrap)\n    NCCLCHECK(bootstrapClose(comm->bootstrap));\n\n  CUDACHECK(cudaFree(comm->hostDevComm.channels));\n  CUDACHECK(cudaFree(comm->devComm));\n\n  for (int channel=0; channel<MAXCHANNELS; channel++)\n    NCCLCHECK(freeChannel(comm->channels+channel, comm->nRanks));\n\n  if (comm->doneEvent != NULL)\n    CUDACHECK(cudaEventDestroy(comm->doneEvent));\n\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamDestroy(comm->groupStream));\n  }\n\n  // Last rank frees shared resources between threads\n  int isLast;\n  NCCLCHECK(ncclCpuBarrierIn(comm, &isLast));\n  if (isLast) {\n    free(comm->intraBarrier);\n    free(comm->intraParams);\n    free(comm->intraCudaDevs);\n    free(comm->intraCGMode);\n    free(comm->intraCC);\n  }\n  NCCLCHECK(ncclCudaHostFree((void *)comm->abortFlag));\n\n  // Poison comm to try and catch a double free\n  commPoison(comm);\n\n  free(comm);\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commAlloc(ncclComm_t* comret, int ndev, int rank) {\n  if (ndev < 1) {\n    WARN(\"invalid device count (%d) requested\", ndev);\n    return ncclInvalidArgument;\n  }\n  if (rank >= ndev || rank < 0) {\n    WARN(\"rank %d exceeds ndev=%d\", rank, ndev);\n    return ncclInvalidArgument;\n  }\n\n  // Try to create a CUDA object right away. If there is something wrong with\n  // the device we're on (failure cause #1) , better know it early.\n  cudaEvent_t doneEvent;\n  CUDACHECK(cudaEventCreateWithFlags(&doneEvent, cudaEventDisableTiming));\n\n  struct ncclComm* comm;\n  NCCLCHECK(ncclCalloc(&comm, 1));\n\n  comm->rank = comm->hostDevComm.rank = rank;\n  comm->nRanks = comm->hostDevComm.nRanks = ndev;\n  cudaGetDevice(&comm->cudaDev);\n  NCCLCHECK(getBusId(comm->cudaDev, &comm->busId));\n  TRACE(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x\", comm, rank, ndev, comm->cudaDev, comm->busId);\n\n  comm->doneEvent = doneEvent;\n  comm->checkPointers = ncclParamCheckPointers() == 1 ? true : false;\n#if CUDART_VERSION >= 9020\n  comm->groupCudaStream = ncclParamGroupCudaStream();\n#else\n  // Don't allow the user to overload the default setting in older CUDA builds\n  comm->groupCudaStream = NCCL_GROUP_CUDA_STREAM;\n#endif\n  comm->fatalError = ncclSuccess;\n\n  NCCLCHECK(ncclCudaHostCalloc((uint32_t**)&comm->abortFlag, 1));\n  comm->hostDevComm.abortFlag = comm->abortFlag;\n  *comm->abortFlag = 0;\n\n  comm->argsptr = &comm->args;\n  comm->collNetSupport = 0;\n\n  NCCLCHECK(ncclCalloc(&comm->asyncOps, NCCL_MAX_OPS));\n  comm->asyncOpCount = 0;\n  comm->asyncTotalSize = 0;\n\n  static_assert(MAXCHANNELS <= sizeof(*comm->connectSend)*8, \"comm->connectSend must have enough bits for all channels\");\n  static_assert(MAXCHANNELS <= sizeof(*comm->connectRecv)*8, \"comm->connectRecv must have enough bits for all channels\");\n  NCCLCHECK(ncclCalloc(&comm->connectSend, comm->nRanks));\n  NCCLCHECK(ncclCalloc(&comm->connectRecv, comm->nRanks));\n\n  comm->p2pSendCount = comm->p2pRecvCount = 0;\n  NCCLCHECK(ncclCalloc(&comm->p2pSends, comm->nRanks));\n  NCCLCHECK(ncclCalloc(&comm->p2pRecvs, comm->nRanks));\n\n  // Mark channels as non initialized.\n  for (int c=0; c<MAXCHANNELS; c++) comm->channels[c].id = -1;\n\n  *comret = comm;\n  return ncclSuccess;\n}\n\nstatic ncclResult_t devCommSetup(ncclComm_t comm) {\n  // Duplicate the channels on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->hostDevComm.channels, comm->p2pnChannels));\n  NCCLCHECK(ncclCudaMemcpy(comm->hostDevComm.channels, comm->channels, comm->p2pnChannels));\n\n  // Copy userRanks and peers\n  for (int r=0; r<comm->p2pnChannels; r++) {\n    NCCLCHECK(ncclCudaMemcpy(comm->channels[r].ring.devUserRanks, comm->channels[r].ring.userRanks, comm->nRanks));\n  }\n\n  // Duplicate the dev comm on the device\n  NCCLCHECK(ncclCudaCalloc(&comm->devComm, 1));\n  NCCLCHECK(ncclCudaMemcpy(comm->devComm, &comm->hostDevComm, 1));\n  return ncclSuccess;\n}\n\n// Pre-process the string so that running \"strings\" on the lib can quickly reveal the version.\n#define VERSION_STRING \"NCCL version \" STR(NCCL_MAJOR) \".\" STR(NCCL_MINOR) \".\" STR(NCCL_PATCH) NCCL_SUFFIX \"+cuda\" STR(CUDA_MAJOR) \".\" STR(CUDA_MINOR)\nstatic void showVersion() {\n  static int shown = 0;\n  if (shown == 0 && ncclDebugLevel >= NCCL_LOG_VERSION) {\n    printf(\"%s\\n\", VERSION_STRING);\n    fflush(stdout);\n    if (ncclDebugFile != stdout)\n      INFO(NCCL_ALL,\"%s\", VERSION_STRING); // Also log NCCL version in one of the files\n    shown = 1;\n  }\n}\n\nstatic ncclResult_t fillInfo(struct ncclComm* comm, struct ncclPeerInfo* info, uint64_t commHash) {\n  info->rank = comm->rank;\n  CUDACHECK(cudaGetDevice(&info->cudaDev));\n  info->hostHash=getHostHash()+commHash;\n  info->pidHash=getPidHash()+commHash;\n\n  // Get the device MAJOR:MINOR of /dev/shm so we can use that\n  // information to decide whether we can use SHM for inter-process\n  // communication in a container environment\n  struct stat statbuf;\n  SYSCHECK(stat(\"/dev/shm\", &statbuf), \"stat\");\n  info->shmDev = statbuf.st_dev;\n\n  info->busId = comm->busId;\n\n  NCCLCHECK(ncclGpuGdrSupport(&info->gdrSupport));\n  return ncclSuccess;\n}\n\nstatic ncclResult_t setupChannel(struct ncclComm* comm, int channelId, int rank, int nranks, int* ringRanks) {\n  TRACE(NCCL_INIT, \"rank %d nranks %d\", rank, nranks);\n  NCCLCHECK(initChannel(comm, channelId));\n\n  struct ncclRing* ring = &comm->channels[channelId].ring;\n  // Reorganize ranks to start with rank.\n  int shift;\n  for (shift = 0; shift<nranks; shift++) {\n    if (ringRanks[shift] == rank) {\n      break;\n    }\n  }\n  for (int i=0; i<nranks; i++) {\n    ring->userRanks[i] = ringRanks[(i+shift)%nranks];\n  }\n  return ncclSuccess;\n}\n\nvoid* waitForNonNullPtr(void* p) {\n  volatile void** ptr = (volatile void**) p;\n  while (*ptr == NULL) sched_yield();\n  return (void*)*ptr;\n}\n\nncclResult_t initParams(struct ncclComm* comm) {\n  struct cudaLaunchParams* params = comm->myParams = comm->intraParams+comm->intraRank;\n  params->args = &comm->argsptr;\n  params->stream = NULL;\n  params->sharedMem = 0;\n  params->blockDim.x = 0; params->blockDim.y = params->blockDim.z = 1;\n  params->gridDim.x = 0; params->gridDim.y = params->gridDim.z = 1;\n  return ncclSuccess;\n}\n\n// Allocate/Set Intra Process Structures and set CG options\nncclResult_t ncclCommSetIntra(struct ncclComm* comm, int rank, int ranks, struct ncclComm* comm0) {\n  comm->intraRank = rank;\n  comm->intraRanks = ranks;\n  comm->intraPhase = 0;\n\n  // Alloc shared structures\n  if (rank == 0) {\n    assert(comm == comm0);\n    int* bar;\n    NCCLCHECK(ncclCalloc(&bar, 2));\n    bar[0] = bar[1] = 0;\n    comm->intraBarrier = bar;\n    NCCLCHECK(ncclCalloc(&comm->intraParams, comm->intraRanks));\n    NCCLCHECK(ncclCalloc(&comm->intraCudaDevs, comm->intraRanks));\n    int* CGMode;\n    NCCLCHECK(ncclCalloc(&CGMode, 1));\n    *CGMode = 0x11;\n    comm->intraCGMode = CGMode;\n    int* CC;\n    NCCLCHECK(ncclCalloc(&CC, 1));\n    *CC = ncclCudaCompCap();\n    comm->intraCC = CC;\n  } else {\n    comm->intraBarrier = (int*)waitForNonNullPtr(&comm0->intraBarrier);\n    comm->intraParams = (struct cudaLaunchParams*)waitForNonNullPtr(&comm0->intraParams);\n    comm->intraCudaDevs = (int*)waitForNonNullPtr(&comm0->intraCudaDevs);\n    comm->intraCGMode = (int*)waitForNonNullPtr(&comm0->intraCGMode);\n    comm->intraCC = (int*)waitForNonNullPtr(&comm0->intraCC);\n  }\n  comm->intraCudaDevs[comm->intraRank] = comm->cudaDev;\n  NCCLCHECK(initParams(comm));\n\n  int cgMdLaunch = 0;\n\n  // Set CG Mode\n  comm->launchMode = ncclComm::GROUP;\n  char* str = getenv(\"NCCL_LAUNCH_MODE\");\n  if (str) INFO(NCCL_ENV, \"NCCL_LAUNCH_MODE set by environment to %s\", str);\n  if (comm->intraRanks == 1 || (str && strcmp(str, \"PARALLEL\") == 0)) {\n    comm->launchMode = ncclComm::PARALLEL;\n  }\n  if (comm->launchMode == ncclComm::GROUP) {\n    CUDACHECK(cudaStreamCreateWithFlags(&comm->groupStream, cudaStreamNonBlocking));\n#if CUDART_VERSION >= 9000\n    if (*comm->intraCC && (ncclCudaCompCap() == *comm->intraCC)) {\n      // Check whether the GPU supports Cooperative Group Multi Device Launch\n      (void) cudaDeviceGetAttribute(&cgMdLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, comm->cudaDev);\n    }\n#endif\n  }\n\n  // Disable cgMdLaunch if any rank does not support it\n  if (cgMdLaunch == 0) {\n    *comm->intraCGMode = 0x10;\n  }\n  return ncclSuccess;\n}\n\n#define DEFAULT_LL_BUFFSIZE (NCCL_LL_LINES_PER_THREAD*NCCL_LL_MAX_NTHREADS*NCCL_STEPS*sizeof(union ncclLLFifoLine))\n#define DEFAULT_LL128_BUFFSIZE (NCCL_LL128_ELEMS_PER_THREAD*NCCL_LL128_MAX_NTHREADS*NCCL_STEPS*sizeof(uint64_t))\n#define DEFAULT_BUFFSIZE (1 << 22) /* 4MiB */\n#define DEFAULT_BUFFSIZE_ARM (1 << 20) /* 1MiB */\nNCCL_PARAM(BuffSize, \"BUFFSIZE\", -2);\nNCCL_PARAM(LlBuffSize, \"LL_BUFFSIZE\", -2);\nNCCL_PARAM(Ll128BuffSize, \"LL128_BUFFSIZE\", -2);\n\nstatic ncclResult_t computeBuffSizes(struct ncclComm* comm) {\n  int cpuArch, cpuVendor, cpuModel;\n  NCCLCHECK(ncclTopoCpuType(comm->topo, &cpuArch, &cpuVendor, &cpuModel));\n\n  int64_t envs[NCCL_NUM_PROTOCOLS] = { ncclParamLlBuffSize(), ncclParamLl128BuffSize(), ncclParamBuffSize() };\n  int defaults[NCCL_NUM_PROTOCOLS] = { DEFAULT_LL_BUFFSIZE, DEFAULT_LL128_BUFFSIZE, DEFAULT_BUFFSIZE };\n\n  if (cpuArch == NCCL_TOPO_CPU_ARCH_ARM) defaults[NCCL_PROTO_SIMPLE] = DEFAULT_BUFFSIZE_ARM;\n\n  for (int p=0; p<NCCL_NUM_PROTOCOLS; p++) {\n    comm->buffSizes[p] = comm->hostDevComm.buffSizes[p] = envs[p] != -2 ? envs[p] : defaults[p];\n  }\n  return ncclSuccess;\n}\n\nextern struct ncclTransport collNetTransport;\n\n// All ranks must participate in collNetSetup call\n// type: 0 for send, 1 for recv\n// return: 0 - unsupported, 1 - supported\n// We do not NCCLCHECK this call because we would fall back to P2P network in case CollNet setup fails\nstatic int collNetSetup(struct ncclComm* comm, struct ncclTopoGraph* collNetGraph, struct ncclChannel* channel, int rank, int nranks,  int masterRank, int masterPeer, int nMasters, int type) {\n  int rankInCollNet = -1;\n  int supported = 0;\n  int isMaster = (rank == masterRank) ? 1 : 0;\n  struct {\n    int collNetRank;\n    ncclConnect connect;\n  } sendrecvExchange;\n\n  // check if we can connect to collnet, whose root is the nranks-th rank\n  struct ncclPeerInfo *myInfo = comm->peerInfo+rank, *peerInfo = comm->peerInfo+nranks;\n  peerInfo->rank = nranks;\n  int ret = 1;\n  if (isMaster) {\n    NCCLCHECK(collNetTransport.canConnect(&ret, comm->topo, collNetGraph, myInfo, peerInfo));\n  }\n\n  // send master receives connect info from peer recv master\n  if (isMaster && type == 0) {\n    NCCLCHECK(bootstrapRecv(comm->bootstrap, masterPeer, &sendrecvExchange, sizeof(sendrecvExchange)));\n    rankInCollNet = sendrecvExchange.collNetRank;\n    INFO(NCCL_INIT, \"CollNet [send] : rank %d collNetRank %d collNetNranks %d received connect from rank %d\", rank, rankInCollNet, nMasters, masterPeer);\n  }\n\n  // select\n  struct ncclPeer* root = channel->peers+nranks;\n  struct ncclConnector* conn = (type == 1) ? &root->recv : &root->send;\n  struct ncclTransportComm* transportComm = (type == 1) ? &(collNetTransport.recv) : &(collNetTransport.send);\n  conn->transportComm = transportComm;\n  // setup\n  struct ncclConnect myConnect;\n  if (isMaster && ret > 0) {\n    NCCLCHECK(transportComm->setup(comm, collNetGraph, myInfo, peerInfo, &myConnect, conn, channel->id));\n  }\n  // prepare connect handles\n  ncclResult_t res;\n  struct {\n    int isMaster;\n    ncclConnect connect;\n  } *allConnects = NULL;\n  ncclConnect *masterConnects = NULL;\n  NCCLCHECK(ncclCalloc(&masterConnects, nMasters));\n  if (type == 1) {  // recv side: AllGather\n    // all ranks must participate\n    NCCLCHECK(ncclCalloc(&allConnects, nranks));\n    allConnects[rank].isMaster = isMaster;\n    memcpy(&(allConnects[rank].connect), &myConnect, sizeof(struct ncclConnect));\n    NCCLCHECKGOTO(bootstrapAllGather(comm->bootstrap, allConnects, sizeof(*allConnects)), res, cleanup);\n    // consolidate\n    int c = 0;\n    for (int r = 0; r < nranks; r++) {\n      if (allConnects[r].isMaster) {\n        memcpy(masterConnects+c, &(allConnects[r].connect), sizeof(struct ncclConnect));\n        if (r == rank) rankInCollNet = c;\n        c++;\n      }\n    }\n  } else { // send side : copy in connect info received from peer recv master\n    if (isMaster) memcpy(masterConnects+rankInCollNet, &(sendrecvExchange.connect), sizeof(struct ncclConnect));\n  }\n  // connect\n  if (isMaster && ret > 0) {\n    NCCLCHECKGOTO(transportComm->connect(comm, masterConnects, nMasters, rankInCollNet, conn), res, cleanup);\n    struct ncclPeer* devRoot = channel->devPeers+nranks;\n    struct ncclConnector* devConn = (type == 1) ? &devRoot->recv : &devRoot->send;\n    CUDACHECKGOTO(cudaMemcpy(devConn, conn, sizeof(struct ncclConnector), cudaMemcpyHostToDevice), res, cleanup);\n  }\n  // recv side sends connect info to send side\n  if (isMaster && type == 1) {\n    sendrecvExchange.collNetRank = rankInCollNet;\n    memcpy(&sendrecvExchange.connect, masterConnects+rankInCollNet, sizeof(struct ncclConnect));\n    NCCLCHECKGOTO(bootstrapSend(comm->bootstrap, masterPeer, &sendrecvExchange, sizeof(sendrecvExchange)), res, cleanup);\n    INFO(NCCL_INIT, \"CollNet [recv] : rank %d collNetRank %d collNetNranks %d sent connect to rank %d\", rank, rankInCollNet, nMasters, masterPeer);\n  }\n  if (ret > 0) {\n    supported = 1;\n  }\ncleanup:\n  if (allConnects != NULL) free(allConnects);\n  if (masterConnects != NULL) free(masterConnects);\n  return supported;\n}\n\nstatic ncclResult_t checkCollNetSetup(struct ncclComm* comm, int rank, int collNetSetupFail) {\n  int nranks = comm->nRanks;\n  // AllGather collNet setup results\n  int* allGatherFailures;\n  NCCLCHECK(ncclCalloc(&allGatherFailures, nranks));\n  allGatherFailures[rank] = collNetSetupFail;\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGatherFailures, sizeof(int)));\n  for (int i=0; i<nranks; i++) {\n    if (allGatherFailures[i] != 0) {\n      collNetSetupFail = 1;\n      break;\n    }\n  }\n  free(allGatherFailures);\n  if (collNetSetupFail) {\n    if (rank == 0) WARN(\"Cannot initialize CollNet, using %s instead\", ncclNetName());\n    // Free collNet resources\n    for (int r=0; r<comm->nChannels; r++) {\n      struct ncclChannel* channel = comm->channels+r;\n      struct ncclPeer* peer = channel->peers+nranks;\n      if (peer->send.transportResources && peer->send.transportComm) NCCLCHECK(peer->send.transportComm->free(peer->send.transportResources));\n      if (peer->recv.transportResources && peer->recv.transportComm) NCCLCHECK(peer->recv.transportComm->free(peer->recv.transportResources));\n      peer->send.transportResources = NULL; // avoid double free\n      peer->recv.transportResources = NULL; // avoid double free\n    }\n    // Set support to 0\n    comm->collNetSupport = 0;\n  } else {\n    comm->collNetSupport = 1;\n  }\n  return ncclSuccess;\n}\n\nNCCL_PARAM(CrossNic, \"CROSS_NIC\", 2);\nNCCL_PARAM(GraphDumpFileRank, \"GRAPH_DUMP_FILE_RANK\", 0);\n\nstatic ncclResult_t initTransportsRank(struct ncclComm* comm, ncclUniqueId* commId) {\n  // We use 2 AllGathers\n  // 1. { peerInfo, comm, compCap}\n  // 2. { nChannels, graphInfo, topoRanks }\n\n  int rank = comm->rank;\n  int nranks = comm->nRanks;\n  uint64_t commHash = getHash(commId->internal, NCCL_UNIQUE_ID_BYTES);\n  TRACE(NCCL_INIT, \"comm %p, commHash %lx, rank %d nranks %d - BEGIN\", comm, commHash, rank, nranks);\n  NCCLCHECK(bootstrapInit(commId, rank, nranks, &comm->bootstrap));\n\n  // AllGather1 - begin\n  struct {\n    struct ncclPeerInfo peerInfo;\n    struct ncclComm* comm;\n    int cudaCompCap;\n  } *allGather1Data;\n\n  NCCLCHECK(ncclCalloc(&allGather1Data, nranks));\n  allGather1Data[rank].comm = comm;\n  allGather1Data[rank].cudaCompCap = ncclCudaCompCap();\n  struct ncclPeerInfo* myInfo = &allGather1Data[rank].peerInfo;\n  NCCLCHECK(fillInfo(comm, myInfo, commHash));\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather1Data, sizeof(*allGather1Data)));\n\n  NCCLCHECK(ncclCalloc(&comm->peerInfo, nranks+1)); // Extra rank to represent CollNet root\n  for (int i = 0; i < nranks; i++) {\n    memcpy(comm->peerInfo+i, &allGather1Data[i].peerInfo, sizeof(struct ncclPeerInfo));\n    if ((i != rank) && (comm->peerInfo[i].hostHash == myInfo->hostHash) && (comm->peerInfo[i].busId == myInfo->busId)) {\n      WARN(\"Duplicate GPU detected : rank %d and rank %d both on CUDA device %x\", rank, i, myInfo->busId);\n      return ncclInvalidUsage;\n    }\n  }\n\n  // Compute intra ranks and minimum CUDA Compute capabilities of intra-node GPUs and all GPUs\n  int intraRank0 = -1, intraRank = -1, intraRanks = 0;\n  int myCompCap = allGather1Data[rank].cudaCompCap;\n  int minCompCap = myCompCap, maxCompCap = myCompCap;\n  uint64_t otherHostHash;\n  int tmpNnodes = 1;\n  for (int i = 0; i < nranks; i++) {\n    if (allGather1Data[i].peerInfo.hostHash == allGather1Data[rank].peerInfo.hostHash) {\n      if (allGather1Data[i].peerInfo.pidHash == allGather1Data[rank].peerInfo.pidHash) {\n        if (intraRanks == 0) intraRank0 = i;\n        if (i == rank) intraRank = intraRanks;\n        intraRanks++;\n      }\n    } else {  // Determine whether number of nodes is 2 (for use in tree pattern determination)\n      if (tmpNnodes == 1) {\n        otherHostHash = allGather1Data[i].peerInfo.hostHash;\n        tmpNnodes = 2;\n      } else if (tmpNnodes == 2 && otherHostHash != allGather1Data[i].peerInfo.hostHash) {\n        tmpNnodes = 3;\n      }\n    }\n    minCompCap = std::min(allGather1Data[i].cudaCompCap, minCompCap);\n    maxCompCap = std::max(allGather1Data[i].cudaCompCap, maxCompCap);\n  }\n  TRACE(NCCL_INIT,\"hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n        rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n  if (intraRank == -1 || intraRank0 == -1 || allGather1Data[intraRank0].comm == NULL) {\n    WARN(\"Failed to determine intra ranks hostHash[%d] %lx intraRank %d intraRanks %d intraRank0 %d\",\n         rank, allGather1Data[rank].peerInfo.hostHash, intraRank, intraRanks, intraRank0);\n    return ncclInternalError;\n  }\n  struct ncclComm* intraRank0Comm = allGather1Data[intraRank0].comm;\n\n  free(allGather1Data);\n\n  // AllGather1 - end\n\n  // Topo detection / System graph creation\n  NCCLCHECK(ncclTopoGetSystem(comm, &comm->topo));\n  // Compute paths between GPUs and NICs\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Remove inaccessible GPUs and unused NICs\n  NCCLCHECK(ncclTopoTrimSystem(comm->topo, comm));\n  // Recompute paths after trimming\n  NCCLCHECK(ncclTopoComputePaths(comm->topo, comm->peerInfo));\n  // Init search\n  NCCLCHECK(ncclTopoSearchInit(comm->topo));\n  // Print final topology\n  NCCLCHECK(ncclTopoPrint(comm->topo));\n\n  // Get rings and trees\n  struct ncclTopoGraph ringGraph;\n  ringGraph.id = 0;\n  ringGraph.pattern = NCCL_TOPO_PATTERN_RING;\n  ringGraph.crossNic = ncclParamCrossNic();\n  ringGraph.collNet = 0;\n  ringGraph.minChannels = 1;\n  ringGraph.maxChannels = MAXCHANNELS/2;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &ringGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &ringGraph));\n\n  struct ncclTopoGraph treeGraph;\n  treeGraph.id = 1;\n  treeGraph.pattern = tmpNnodes <= 2 ? NCCL_TOPO_PATTERN_TREE : NCCL_TOPO_PATTERN_BALANCED_TREE;\n  treeGraph.crossNic = ncclParamCrossNic();\n  treeGraph.collNet = 0;\n  treeGraph.minChannels = 1;\n  treeGraph.maxChannels = ringGraph.nChannels;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &treeGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &treeGraph));\n\n  struct ncclTopoGraph collNetGraph;\n  collNetGraph.id = 2;\n  collNetGraph.pattern = NCCL_TOPO_PATTERN_TREE;\n  collNetGraph.collNet = 1;\n  collNetGraph.crossNic = ncclParamCrossNic();\n  collNetGraph.minChannels = collNetGraph.maxChannels = ringGraph.nChannels;\n  NCCLCHECK(ncclTopoCompute(comm->topo, &collNetGraph));\n  NCCLCHECK(ncclTopoPrintGraph(comm->topo, &collNetGraph));\n\n  if (comm->rank == ncclParamGraphDumpFileRank()) {\n    struct ncclTopoGraph* graphs[3] = { &ringGraph, &treeGraph, &collNetGraph };\n    NCCLCHECK(ncclTopoDumpGraphs(comm->topo, 3, graphs));\n  }\n\n  // AllGather3 - begin\n  struct ncclGraphInfo {\n    int pattern;\n    int sameChannels;\n    float speedIntra;\n    float speedInter;\n    int typeIntra;\n    int typeInter;\n  };\n\n  struct {\n    int cudaCompCap;\n    int nChannels;\n    struct ncclGraphInfo tree;\n    struct ncclGraphInfo ring;\n    struct ncclGraphInfo collNet;\n    struct ncclTopoRanks topoRanks;\n  } *allGather3Data;\n\n  NCCLCHECK(ncclCalloc(&allGather3Data, nranks));\n  allGather3Data[rank].nChannels = comm->nChannels = treeGraph.nChannels = ringGraph.nChannels =\n    std::min(treeGraph.nChannels, ringGraph.nChannels);\n  allGather3Data[rank].tree.pattern = treeGraph.pattern;\n  allGather3Data[rank].tree.sameChannels = treeGraph.sameChannels;\n  allGather3Data[rank].tree.speedIntra = treeGraph.speedIntra;\n  allGather3Data[rank].tree.speedInter = treeGraph.speedInter;\n  allGather3Data[rank].tree.typeIntra = treeGraph.typeIntra;\n  allGather3Data[rank].tree.typeInter = treeGraph.typeInter;\n  allGather3Data[rank].ring.pattern = ringGraph.pattern;\n  allGather3Data[rank].ring.sameChannels = ringGraph.sameChannels;\n  allGather3Data[rank].ring.speedIntra = ringGraph.speedIntra;\n  allGather3Data[rank].ring.speedInter = ringGraph.speedInter;\n  allGather3Data[rank].ring.typeIntra = ringGraph.typeIntra;\n  allGather3Data[rank].ring.typeInter = ringGraph.typeInter;\n  allGather3Data[rank].collNet.pattern = collNetGraph.pattern;\n  allGather3Data[rank].collNet.sameChannels = collNetGraph.sameChannels;\n  allGather3Data[rank].collNet.speedIntra = collNetGraph.speedIntra;\n  allGather3Data[rank].collNet.speedInter = collNetGraph.speedInter;\n  allGather3Data[rank].collNet.typeIntra = collNetGraph.typeIntra;\n  allGather3Data[rank].collNet.typeInter = collNetGraph.typeInter;\n\n  NCCLCHECK(ncclTopoPreset(comm, &treeGraph, &ringGraph, &collNetGraph, &allGather3Data[rank].topoRanks));\n\n  NCCLCHECK(bootstrapAllGather(comm->bootstrap, allGather3Data, sizeof(*allGather3Data)));\n\n  // Determine nNodes, firstRanks, ...\n  int *nodesFirstRank, *nodesTreePatterns;\n  NCCLCHECK(ncclCalloc(&nodesFirstRank, nranks));\n  NCCLCHECK(ncclCalloc(&nodesTreePatterns, nranks));\n  for (int i=0; i<nranks; i++) {\n    int node = -1;\n    int firstRank = allGather3Data[i].topoRanks.ringRecv[0];\n    for (int n=0; n<comm->nNodes; n++) {\n      if (nodesFirstRank[n] == firstRank) node = n;\n    }\n    if (node == -1) {\n      node = comm->nNodes++;\n      nodesFirstRank[node] = firstRank;\n      // Record tree pattern of each node as they can be different depending on sm arch\n      nodesTreePatterns[node] = allGather3Data[i].tree.pattern;\n    }\n    if (i == comm->rank) comm->node = node;\n  }\n\n  int nChannelsOrig = comm->nChannels;\n  struct ncclTopoRanks** allTopoRanks;\n  NCCLCHECK(ncclCalloc(&allTopoRanks, comm->nRanks));\n  for (int i=0; i<nranks; i++) {\n    allTopoRanks[i] = &allGather3Data[i].topoRanks;\n    // Make sure we align all ranks so that the tuning is consistent across ranks\n    treeGraph.nChannels = ringGraph.nChannels = comm->nChannels = std::min(allGather3Data[i].nChannels, comm->nChannels);\n    treeGraph.sameChannels = std::min(allGather3Data[i].tree.sameChannels, treeGraph.sameChannels);\n    treeGraph.speedIntra = std::min(allGather3Data[i].tree.speedIntra, treeGraph.speedIntra);\n    treeGraph.speedInter = std::min(allGather3Data[i].tree.speedInter, treeGraph.speedInter);\n    treeGraph.typeIntra = std::min(allGather3Data[i].tree.typeIntra, treeGraph.typeIntra);\n    treeGraph.typeInter = std::min(allGather3Data[i].tree.typeInter, treeGraph.typeInter);\n    ringGraph.sameChannels = std::min(allGather3Data[i].ring.sameChannels, ringGraph.sameChannels);\n    ringGraph.speedIntra = std::min(allGather3Data[i].ring.speedIntra, ringGraph.speedIntra);\n    ringGraph.speedInter = std::min(allGather3Data[i].ring.speedInter, ringGraph.speedInter);\n    ringGraph.typeIntra = std::min(allGather3Data[i].ring.typeIntra, ringGraph.typeIntra);\n    ringGraph.typeInter = std::min(allGather3Data[i].ring.typeInter, ringGraph.typeInter);\n    collNetGraph.sameChannels = std::min(allGather3Data[i].collNet.sameChannels, collNetGraph.sameChannels);\n    collNetGraph.speedIntra = std::min(allGather3Data[i].collNet.speedIntra, collNetGraph.speedIntra);\n    collNetGraph.speedInter = std::min(allGather3Data[i].collNet.speedInter, collNetGraph.speedInter);\n    collNetGraph.typeIntra = std::min(allGather3Data[i].collNet.typeIntra, collNetGraph.typeIntra);\n    collNetGraph.typeInter = std::min(allGather3Data[i].collNet.typeInter, collNetGraph.typeInter);\n  }\n\n  if (comm->nChannels < nChannelsOrig) {\n    // We started duplicating channels during Preset(), so we need to move the\n    // duplicated channels since we have removed some.\n    for (int i=0; i<comm->nChannels; i++) memcpy(comm->channels+comm->nChannels+i, comm->channels+nChannelsOrig+i, sizeof(struct ncclChannel));\n  }\n\n  int *rings;\n  NCCLCHECK(ncclCalloc(&rings, nranks*MAXCHANNELS));\n\n  NCCLCHECK(ncclTopoPostset(comm, nodesFirstRank, nodesTreePatterns, allTopoRanks, rings));\n  if (comm->nNodes > 1 &&\n      ncclParamCollNetEnable() == 1 &&\n      collNetSupport() && collNetGraph.nChannels) {\n    NCCLCHECK(ncclTopoConnectCollNet(comm, &collNetGraph, rank));\n  }\n\n  free(allTopoRanks);\n  free(nodesTreePatterns);\n  free(nodesFirstRank);\n  free(allGather3Data);\n\n  // AllGather3 - end\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - BUILT %d TREES/RINGS\", rank, nranks, comm->nChannels);\n\n  char line[1024];\n  line[0]='\\0';\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclTree* tree = &comm->channels[c].tree;\n    snprintf(line+strlen(line), 1023-strlen(line), \" [%d] %d/%d/%d->%d->%d\",\n        c, tree->down[0], tree->down[1], tree->down[2], rank, tree->up);\n  }\n  line[1023] = '\\0';\n  INFO(NCCL_INIT, \"Trees%s\", line);\n\n  // Set Affinity to a CPU local the our GPU, so that all memory we allocate\n  // on the host is local.\n  cpu_set_t affinitySave;\n  sched_getaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  NCCLCHECK(ncclTopoSetAffinity(comm->topo, comm->rank));\n  ncclResult_t ret;\n\n  NCCLCHECK(computeBuffSizes(comm));\n\n  // Connect with prev/next for each ring\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclChannel* channel = comm->channels+c;\n    NCCLCHECKGOTO(setupChannel(comm, c, rank, nranks, rings+c*nranks), ret, affinity_restore);\n    if (comm->nRanks == 1) continue;\n    NCCLCHECKGOTO(ncclTransportP2pConnect(comm, channel, 1, &channel->ring.prev, 1, &channel->ring.next), ret, affinity_restore);\n  }\n  NCCLCHECKGOTO(ncclTransportP2pSetup(comm, &ringGraph), ret, affinity_restore);\n  INFO(NCCL_INIT, \"Connected all rings\");\n\n  // Connect Trees\n  for (int c=0; c<comm->nChannels; c++) {\n    struct ncclChannel* channel = comm->channels+c;\n    if (comm->nRanks == 1) continue;\n    NCCLCHECKGOTO(ncclTransportP2pConnect(comm, channel, NCCL_MAX_TREE_ARITY, channel->tree.down, 1, &channel->tree.up), ret, affinity_restore);\n    NCCLCHECKGOTO(ncclTransportP2pConnect(comm, channel, 1, &channel->tree.up, NCCL_MAX_TREE_ARITY, channel->tree.down), ret, affinity_restore);\n  }\n  NCCLCHECKGOTO(ncclTransportP2pSetup(comm, &treeGraph), ret, affinity_restore);\n  INFO(NCCL_INIT, \"Connected all trees\");\n\n  // Check if we can setup CollNet\n  if (comm->nNodes > 1 &&\n      ncclParamCollNetEnable() == 1 &&\n      collNetSupport() && collNetGraph.nChannels) {\n    int logicChannels = comm->nChannels/2;\n    int collNetSetupFail = 0;\n    const int recvIndex = 0;  // recv GPU index is always 0\n    const int sendIndex = collNetGraph.pattern == NCCL_TOPO_PATTERN_TREE ? 0 : 1;  // send GPU index depends on topo pattern\n    for (int c=0; c<logicChannels; c++) {\n      struct ncclChannel* channelRecv = comm->channels+logicChannels+c;\n      struct ncclChannel* channelSend = comm->channels+c;\n      NCCLCHECK(ncclTransportP2pConnect(comm, channelRecv, 1, &channelRecv->collTree.up, 1, channelRecv->collTree.down));\n      NCCLCHECK(ncclTransportP2pConnect(comm, channelSend, 1, channelSend->collTree.down, 1, &channelSend->collTree.up));\n      const int recvMaster = collNetGraph.intra[c*comm->localRanks+recvIndex];\n      const int sendMaster = collNetGraph.intra[c*comm->localRanks+sendIndex];\n      if (collNetSetup(comm, &collNetGraph, channelRecv, rank, nranks, recvMaster, sendMaster, comm->nNodes, 1) != 1)\n        collNetSetupFail = 1;\n      else if (collNetSetup(comm, &collNetGraph, channelSend, rank, nranks, sendMaster, recvMaster, comm->nNodes, 0) != 1)\n        collNetSetupFail = 1;\n    }\n    NCCLCHECK(ncclTransportP2pSetup(comm, &collNetGraph));\n    // Verify CollNet setup across ranks\n    NCCLCHECK(checkCollNetSetup(comm, rank, collNetSetupFail));\n  }\n  TRACE(NCCL_INIT, \"rank %d nranks %d - CONNECTED %d RINGS AND TREES\", rank, nranks, comm->nChannels);\n  free(rings);\n\n  // Compute time models for algorithm and protocol combinations\n  NCCLCHECK(ncclTopoTuneModel(comm, minCompCap, maxCompCap, &treeGraph, &ringGraph, &collNetGraph));\n\n  // Compute nChannels per peer for p2p\n  NCCLCHECK(ncclTopoComputeP2pChannels(comm));\n\n  NCCLCHECK(ncclCommSetIntra(comm, intraRank, intraRanks, intraRank0Comm));\n\n  if (comm->nNodes) NCCLCHECK(ncclProxyCreate(comm));\n\n  // We should have allocated all buffers, collective fifos, ... we can\n  // restore the affinity.\naffinity_restore:\n  sched_setaffinity(0, sizeof(cpu_set_t), &affinitySave);\n  if (ret != ncclSuccess) return ret;\n\n  TRACE(NCCL_INIT, \"rank %d nranks %d - DONE\", rank, nranks);\n  return ncclSuccess;\n}\n\nncclResult_t ncclCommInitRankSync(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n\n  CUDACHECK(cudaSetDevice(cudaDev));\n  NCCLCHECKGOTO(commAlloc(newcomm, nranks, myrank), res, cleanup);\n  NCCLCHECKGOTO(initTransportsRank(*newcomm, &commId), res, cleanup);\n  NCCLCHECKGOTO(devCommSetup(*newcomm), res, cleanup);\n\n  INFO(NCCL_INIT,\"comm %p rank %d nranks %d cudaDev %d busId %x - Init COMPLETE\", *newcomm, myrank, nranks, (*newcomm)->cudaDev, (*newcomm)->busId);\n\n  return ncclSuccess;\ncleanup:\n  if ((*newcomm) && (*newcomm)->bootstrap) bootstrapAbort((*newcomm)->bootstrap);\n  *newcomm = NULL;\n  return res;\n}\n\nstatic ncclResult_t ncclCommInitRankDev(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank, int cudaDev) {\n  ncclResult_t res;\n  char* env = getenv(\"NCCL_COMM_ID\");\n  if (env && myrank == 0) {\n    INFO(NCCL_ENV, \"NCCL_COMM_ID set by environment to %s\", env);\n    NCCLCHECKGOTO(bootstrapCreateRoot(&commId, true), res, end);\n  }\n\n  NCCLCHECKGOTO(ncclInit(), res, end);\n  if (myrank == 0) showVersion();\n\n  // Make sure the CUDA runtime is initialized.\n  CUDACHECKGOTO(cudaFree(NULL), res, end);\n\n  NCCLCHECKGOTO(PtrCheck(newcomm, \"CommInitRank\", \"newcomm\"), res, end);\n  if (nranks < 1 || myrank < 0 || myrank >= nranks) {\n    WARN(\"Invalid rank requested : %d/%d\", myrank, nranks);\n    res = ncclInvalidArgument;\n    goto end;\n  }\n\n  if (ncclAsyncMode()) {\n    NCCLCHECKGOTO(ncclAsyncInit(ncclCommInitRankSync, newcomm, nranks, commId, myrank, cudaDev), res, end);\n  } else {\n    NCCLCHECKGOTO(ncclCommInitRankSync(newcomm, nranks, commId, myrank, cudaDev), res, end);\n  }\nend:\n  if (ncclAsyncMode()) return ncclAsyncErrCheck(res);\n  else return res;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitRank, ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank);\nncclResult_t ncclCommInitRank(ncclComm_t* newcomm, int nranks, ncclUniqueId commId, int myrank) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  int cudaDev;\n  CUDACHECK(cudaGetDevice(&cudaDev));\n  NCCLCHECK(ncclCommInitRankDev(newcomm, nranks, commId, myrank, cudaDev));\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommInitAll, ncclComm_t* comms, int ndev, const int* devlist);\nncclResult_t ncclCommInitAll(ncclComm_t* comms, int ndev, const int* devlist) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  NCCLCHECK(PtrCheck(comms, \"CommInitAll\", \"comms\"));\n  if (ndev < 0) {\n    WARN(\"Invalid device count requested : %d\", ndev);\n    return ncclInvalidArgument;\n  }\n\n  ncclUniqueId uniqueId;\n  NCCLCHECK(ncclGetUniqueId(&uniqueId));\n  NCCLCHECK(ncclGroupStart());\n  for (int i=0; i<ndev; i++) {\n    // Ignore return codes .. we need to call ncclGroupEnd to clean up anyway\n    ncclCommInitRankDev(comms+i, ndev, uniqueId, i, devlist ? devlist[i] : i);\n  }\n  NCCLCHECK(ncclGroupEnd());\n  return ncclSuccess;\n}\n\nstatic ncclResult_t commDestroy(ncclComm_t comm) {\n  int savedDevice;\n  CUDACHECK(cudaGetDevice(&savedDevice));\n  int commDevice = comm->cudaDev;\n\n  if (savedDevice != commDevice) {\n    CUDACHECK(cudaSetDevice(commDevice));\n  }\n\n  TRACE(NCCL_INIT, \"Destroying comm %p rank %d abortFlag %d fatalError %d\", comm, comm->rank, *comm->abortFlag, comm->fatalError);\n\n  CUDACHECK(cudaStreamSynchronize(comm->groupStream));\n  NCCLCHECK(ncclProxyDestroy(comm));\n  NCCLCHECK(commFree(comm));\n\n  if (savedDevice != commDevice)\n    CUDACHECK(cudaSetDevice(savedDevice));\n\n  TRACE(NCCL_INIT, \"Destroyed comm %p rank %d\", comm, comm->rank);\n\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommDestroy, ncclComm_t comm);\nncclResult_t ncclCommDestroy(ncclComm_t comm) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  if (comm == NULL)\n    return ncclSuccess;\n\n  TRACE(NCCL_INIT, \"comm %p rank %d nRanks %d cudaDev %d busId %x\", comm, comm->rank, comm->nRanks, comm->cudaDev, comm->busId);\n\n  // Try and prevent a double free of the comm struct (user error)\n  if (comm->rank == -1 || comm->nRanks <= 0 || comm->cudaDev == -1 || comm->busId == -1) {\n    WARN(\"comm %p has already been destroyed\", comm);\n    return ncclInvalidArgument;\n  }\n\n  return commDestroy(comm);\n}\n\nNCCL_API(ncclResult_t, ncclCommAbort, ncclComm_t comm);\nncclResult_t ncclCommAbort(ncclComm_t comm) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  if (comm == NULL)\n    return ncclSuccess;\n\n  // Ask anything that might still be running on the device to quit\n  *comm->abortFlag = 1;\n\n  return commDestroy(comm);\n}\n\nNCCL_API(const char*, ncclGetErrorString, ncclResult_t code);\nconst char* ncclGetErrorString(ncclResult_t code) {\n  switch (code) {\n    case ncclSuccess                : return \"no error\";\n    case ncclUnhandledCudaError     : return \"unhandled cuda error\";\n    case ncclSystemError            : return \"unhandled system error\";\n    case ncclInternalError          : return \"internal error\";\n    case ncclInvalidArgument        : return \"invalid argument\";\n    case ncclInvalidUsage           : return \"invalid usage\";\n    default                         : return \"unknown result code\";\n  }\n}\n\nNCCL_API(ncclResult_t, ncclCommGetAsyncError, ncclComm_t comm, ncclResult_t *asyncError);\nncclResult_t ncclCommGetAsyncError(ncclComm_t comm, ncclResult_t *asyncError) {\n  NCCLCHECK(PtrCheck(comm, \"ncclGetAsyncError\", \"comm\"));\n  NCCLCHECK(PtrCheck(asyncError, \"ncclGetAsyncError\", \"asyncError\"));\n  *asyncError = comm->fatalError;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCount, const ncclComm_t comm, int* count);\nncclResult_t ncclCommCount(const ncclComm_t comm, int* count) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  NCCLCHECK(PtrCheck(comm, \"CommCount\", \"comm\"));\n  NCCLCHECK(PtrCheck(count, \"CommCount\", \"count\"));\n  *count = comm->nRanks;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommCuDevice, const ncclComm_t comm, int* devid);\nncclResult_t ncclCommCuDevice(const ncclComm_t comm, int* devid) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  NCCLCHECK(PtrCheck(comm, \"CommCuDevice\", \"comm\"));\n  NCCLCHECK(PtrCheck(devid, \"CommCuDevice\", \"devid\"));\n  *devid = comm->cudaDev;\n  return ncclSuccess;\n}\n\nNCCL_API(ncclResult_t, ncclCommUserRank, const ncclComm_t comm, int* rank);\nncclResult_t ncclCommUserRank(const ncclComm_t comm, int* rank) {\n  NVTX3_FUNC_RANGE_IN(nccl_domain);\n  NCCLCHECK(PtrCheck(comm, \"CommUserRank\", \"comm\"));\n  NCCLCHECK(PtrCheck(rank, \"CommUserRank\", \"rank\"));\n  *rank = comm->rank;\n  return ncclSuccess;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.8.3-1-3bcyweyu5oveuk5a7rladbqwcq2srpds/spack-src/src/include/nvtx3/nvtxDetail/nvtxInit.h": "/*\n* Copyright 2009-2020  NVIDIA Corporation.  All rights reserved.\n*\n* Licensed under the Apache License v2.0 with LLVM Exceptions.\n* See https://llvm.org/LICENSE.txt for license information.\n* SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n*/\n\n#ifndef NVTX_IMPL_GUARD\n#error Never include this file directly -- it is automatically included by nvToolsExt.h (except when NVTX_NO_IMPL is defined).\n#endif\n\n/* ---- Platform-independent helper definitions and functions ---- */\n\n/* Prefer macros over inline functions to reduce symbol resolution at link time */\n\n#if defined(_WIN32) \n#define NVTX_PATHCHAR   wchar_t\n#define NVTX_STR(x)     L##x\n#define NVTX_GETENV     _wgetenv\n#define NVTX_BUFSIZE    MAX_PATH\n#define NVTX_DLLHANDLE  HMODULE\n#define NVTX_DLLOPEN(x) LoadLibraryW(x)\n#define NVTX_DLLFUNC    GetProcAddress\n#define NVTX_DLLCLOSE   FreeLibrary\n#define NVTX_YIELD()    SwitchToThread()\n#define NVTX_MEMBAR()   MemoryBarrier()\n#define NVTX_ATOMIC_WRITE_32(address, value)                        InterlockedExchange((volatile LONG*)address, value)\n#define NVTX_ATOMIC_CAS_32(old, address, exchange, comparand) old = InterlockedCompareExchange((volatile LONG*)address, exchange, comparand)\n#elif defined(__GNUC__)\n#define NVTX_PATHCHAR   char\n#define NVTX_STR(x)     x\n#define NVTX_GETENV     getenv\n#define NVTX_BUFSIZE    PATH_MAX\n#define NVTX_DLLHANDLE  void*\n#define NVTX_DLLOPEN(x) dlopen(x, RTLD_LAZY)\n#define NVTX_DLLFUNC    dlsym\n#define NVTX_DLLCLOSE   dlclose\n#define NVTX_YIELD()    sched_yield()\n#define NVTX_MEMBAR()   __sync_synchronize()\n/* Ensure full memory barrier for atomics, to match Windows functions */\n#define NVTX_ATOMIC_WRITE_32(address, value)                  __sync_synchronize();       __sync_lock_test_and_set(address, value)\n#define NVTX_ATOMIC_CAS_32(old, address, exchange, comparand) __sync_synchronize(); old = __sync_val_compare_and_swap(address, exchange, comparand)\n#else\n#error The library does not support your configuration!\n#endif\n\n/* Define this to 1 for platforms that where pre-injected libraries can be discovered. */\n#if defined(_WIN32)\n/* TODO */\n#define NVTX_SUPPORT_ALREADY_INJECTED_LIBRARY 0\n#else\n#define NVTX_SUPPORT_ALREADY_INJECTED_LIBRARY 0\n#endif\n\n/* Define this to 1 for platforms that support environment variables */\n/* TODO: Detect UWP, a.k.a. Windows Store app, and set this to 0. */\n/* Try:  #if defined(WINAPI_FAMILY_PARTITION) && WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_APP) */\n#define NVTX_SUPPORT_ENV_VARS 1\n\n/* Define this to 1 for platforms that support dynamic/shared libraries */\n#define NVTX_SUPPORT_DYNAMIC_INJECTION_LIBRARY 1\n\n/* Injection libraries implementing InitializeInjectionNvtx2 may be statically linked,\n*  and this will override any dynamic injection.  Useful for platforms where dynamic\n*  injection is not available.  Since weak symbols not explicitly marked extern are\n*  guaranteed to be initialized to zero if no definitions are found by the linker, the\n*  dynamic injection process proceeds normally if pfnInitializeInjectionNvtx2 is 0. */\n#if defined(__GNUC__) && !defined(_WIN32) && !defined(__CYGWIN__)\n#define NVTX_SUPPORT_STATIC_INJECTION_LIBRARY 1\n/* To statically inject an NVTX library, define InitializeInjectionNvtx2_fnptr as a normal\n*  symbol (not weak) pointing to the implementation of InitializeInjectionNvtx2 (which\n*  does not need to be named \"InitializeInjectionNvtx2\" as is necessary in a dynamic\n*  injection library. */\n__attribute__((weak)) NvtxInitializeInjectionNvtxFunc_t InitializeInjectionNvtx2_fnptr;\n#else\n#define NVTX_SUPPORT_STATIC_INJECTION_LIBRARY 0\n#endif\n\n/* This function tries to find or load an NVTX injection library and get the\n*  address of its InitializeInjection2 function.  If such a function pointer\n*  is found, it is called, and passed the address of this NVTX instance's\n*  nvtxGetExportTable function, so the injection can attach to this instance.\n*  If the initialization fails for any reason, any dynamic library loaded will\n*  be freed, and all NVTX implementation functions will be set to no-ops.  If\n*  initialization succeeds, NVTX functions not attached to the tool will be set\n*  to no-ops.  This is implemented as one function instead of several small\n*  functions to minimize the number of weak symbols the linker must resolve.\n*  Order of search is:\n*  - Pre-injected library exporting InitializeInjectionNvtx2\n*  - Loadable library exporting InitializeInjectionNvtx2\n*      - Path specified by env var NVTX_INJECTION??_PATH (?? is 32 or 64)\n*      - On Android, libNvtxInjection??.so within the package (?? is 32 or 64)\n*  - Statically-linked injection library defining InitializeInjectionNvtx2_fnptr\n*/\nNVTX_LINKONCE_FWDDECL_FUNCTION int NVTX_VERSIONED_IDENTIFIER(nvtxInitializeInjectionLibrary)(void);\nNVTX_LINKONCE_DEFINE_FUNCTION int NVTX_VERSIONED_IDENTIFIER(nvtxInitializeInjectionLibrary)(void)\n{\n    const char* const initFuncName = \"InitializeInjectionNvtx2\";\n    NvtxInitializeInjectionNvtxFunc_t init_fnptr = (NvtxInitializeInjectionNvtxFunc_t)0;\n    NVTX_DLLHANDLE injectionLibraryHandle = (NVTX_DLLHANDLE)0;\n    int entryPointStatus = 0;\n\n#if NVTX_SUPPORT_ALREADY_INJECTED_LIBRARY\n    /* Use POSIX global symbol chain to query for init function from any module */\n    init_fnptr = (NvtxInitializeInjectionNvtxFunc_t)NVTX_DLLFUNC(0, initFuncName);\n#endif\n\n#if NVTX_SUPPORT_DYNAMIC_INJECTION_LIBRARY\n    /* Try discovering dynamic injection library to load */\n    if (!init_fnptr)\n    {\n#if NVTX_SUPPORT_ENV_VARS\n        /* If env var NVTX_INJECTION64_PATH is set, it should contain the path\n        *  to a 64-bit dynamic NVTX injection library (and similar for 32-bit). */\n        const NVTX_PATHCHAR* const nvtxEnvVarName = (sizeof(void*) == 4)\n            ? NVTX_STR(\"NVTX_INJECTION32_PATH\")\n            : NVTX_STR(\"NVTX_INJECTION64_PATH\");\n#endif /* NVTX_SUPPORT_ENV_VARS */\n        NVTX_PATHCHAR injectionLibraryPathBuf[NVTX_BUFSIZE];\n        const NVTX_PATHCHAR* injectionLibraryPath = (const NVTX_PATHCHAR*)0;\n\n        /* Refer to this variable explicitly in case all references to it are #if'ed out */\n        (void)injectionLibraryPathBuf;\n\n#if NVTX_SUPPORT_ENV_VARS\n        /* Disable the warning for getenv & _wgetenv -- this usage is safe because\n        *  these functions are not called again before using the returned value. */\n#if defined(_MSC_VER)\n#pragma warning( push )\n#pragma warning( disable : 4996 )\n#endif\n        injectionLibraryPath = NVTX_GETENV(nvtxEnvVarName);\n#if defined(_MSC_VER)\n#pragma warning( pop )\n#endif\n#endif\n\n#if defined(__ANDROID__)\n        if (!injectionLibraryPath)\n        {\n            const char *bits = (sizeof(void*) == 4) ? \"32\" : \"64\";\n            char cmdlineBuf[32];\n            char pkgName[PATH_MAX];\n            int count;\n            int pid;\n            FILE *fp;\n            size_t bytesRead;\n            size_t pos;\n\n            pid = (int)getpid();\n            count = snprintf(cmdlineBuf, sizeof(cmdlineBuf), \"/proc/%d/cmdline\", pid);\n            if (count <= 0 || count >= (int)sizeof(cmdlineBuf))\n            {\n                NVTX_ERR(\"Path buffer too small for: /proc/%d/cmdline\\n\", pid);\n                return NVTX_ERR_INIT_ACCESS_LIBRARY;\n            }\n\n            fp = fopen(cmdlineBuf, \"r\");\n            if (!fp)\n            {\n                NVTX_ERR(\"File couldn't be opened: %s\\n\", cmdlineBuf);\n                return NVTX_ERR_INIT_ACCESS_LIBRARY;\n            }\n\n            bytesRead = fread(pkgName, 1, sizeof(pkgName) - 1, fp);\n            fclose(fp);\n            if (bytesRead == 0)\n            {\n                NVTX_ERR(\"Package name couldn't be read from file: %s\\n\", cmdlineBuf);\n                return NVTX_ERR_INIT_ACCESS_LIBRARY;\n            }\n\n            pkgName[bytesRead] = 0;\n\n            /* String can contain colon as a process separator. In this case the package name is before the colon. */\n            pos = 0;\n            while (pos < bytesRead && pkgName[pos] != ':' && pkgName[pos] != '\\0')\n            {\n                ++pos;\n            }\n            pkgName[pos] = 0;\n\n            count = snprintf(injectionLibraryPathBuf, NVTX_BUFSIZE, \"/data/data/%s/files/libNvtxInjection%s.so\", pkgName, bits);\n            if (count <= 0 || count >= NVTX_BUFSIZE)\n            {\n                NVTX_ERR(\"Path buffer too small for: /data/data/%s/files/libNvtxInjection%s.so\\n\", pkgName, bits);\n                return NVTX_ERR_INIT_ACCESS_LIBRARY;\n            }\n\n            /* On Android, verify path is accessible due to aggressive file access restrictions. */\n            /* For dlopen, if the filename contains a leading slash, then it is interpreted as a */\n            /* relative or absolute pathname; otherwise it will follow the rules in ld.so. */\n            if (injectionLibraryPathBuf[0] == '/')\n            {\n#if (__ANDROID_API__ < 21)\n                int access_err = access(injectionLibraryPathBuf, F_OK | R_OK);\n#else\n                int access_err = faccessat(AT_FDCWD, injectionLibraryPathBuf, F_OK | R_OK, 0);\n#endif\n                if (access_err != 0)\n                {\n                    NVTX_ERR(\"Injection library path wasn't accessible [code=%s] [path=%s]\\n\", strerror(errno), injectionLibraryPathBuf);\n                    return NVTX_ERR_INIT_ACCESS_LIBRARY;\n                }\n            }\n            injectionLibraryPath = injectionLibraryPathBuf;\n        }\n#endif\n\n        /* At this point, injectionLibraryPath is specified if a dynamic\n        *  injection library was specified by a tool. */\n        if (injectionLibraryPath)\n        {\n            /* Load the injection library */\n            injectionLibraryHandle = NVTX_DLLOPEN(injectionLibraryPath);\n            if (!injectionLibraryHandle)\n            {\n                NVTX_ERR(\"Failed to load injection library\\n\");\n                return NVTX_ERR_INIT_LOAD_LIBRARY;\n            }\n            else\n            {\n                /* Attempt to get the injection library's entry-point */\n                init_fnptr = (NvtxInitializeInjectionNvtxFunc_t)NVTX_DLLFUNC(injectionLibraryHandle, initFuncName);\n                if (!init_fnptr)\n                {\n                    NVTX_DLLCLOSE(injectionLibraryHandle);\n                    NVTX_ERR(\"Failed to get address of function InitializeInjectionNvtx2 from injection library\\n\");\n                    return NVTX_ERR_INIT_MISSING_LIBRARY_ENTRY_POINT;\n                }\n            }\n        }\n    }\n#endif\n\n#if NVTX_SUPPORT_STATIC_INJECTION_LIBRARY\n    if (!init_fnptr)\n    {\n        /* Check weakly-defined function pointer.  A statically-linked injection can define this as\n        *  a normal symbol and it will take precedence over a dynamic injection. */\n        if (InitializeInjectionNvtx2_fnptr)\n        {\n            init_fnptr = InitializeInjectionNvtx2_fnptr;\n        }\n    }\n#endif\n\n    /* At this point, if init_fnptr is not set, then no tool has specified\n    *  an NVTX injection library -- return non-success result so all NVTX\n    *  API functions will be set to no-ops. */\n    if (!init_fnptr)\n    {\n        return NVTX_ERR_NO_INJECTION_LIBRARY_AVAILABLE;\n    }\n\n    /* Invoke injection library's initialization function.  If it returns\n    *  0 (failure) and a dynamic injection was loaded, unload it. */\n    entryPointStatus = init_fnptr(NVTX_VERSIONED_IDENTIFIER(nvtxGetExportTable));\n    if (entryPointStatus == 0)\n    {\n        NVTX_ERR(\"Failed to initialize injection library -- initialization function returned 0\\n\");\n        if (injectionLibraryHandle)\n        {\n            NVTX_DLLCLOSE(injectionLibraryHandle);\n        }\n        return NVTX_ERR_INIT_FAILED_LIBRARY_ENTRY_POINT;\n    }\n\n    return NVTX_SUCCESS;\n}\n\nNVTX_LINKONCE_DEFINE_FUNCTION void NVTX_VERSIONED_IDENTIFIER(nvtxInitOnce)(void)\n{\n    unsigned int old;\n    if (NVTX_VERSIONED_IDENTIFIER(nvtxGlobals).initState == NVTX_INIT_STATE_COMPLETE)\n    {\n        return;\n    }\n\n    NVTX_ATOMIC_CAS_32(\n        old,\n        &NVTX_VERSIONED_IDENTIFIER(nvtxGlobals).initState,\n        NVTX_INIT_STATE_STARTED,\n        NVTX_INIT_STATE_FRESH);\n    if (old == NVTX_INIT_STATE_FRESH)\n    {\n        int result;\n        int forceAllToNoops;\n\n        /* Load & initialize injection library -- it will assign the function pointers */\n        result = NVTX_VERSIONED_IDENTIFIER(nvtxInitializeInjectionLibrary)();\n\n        /* Set all pointers not assigned by the injection to null */\n        forceAllToNoops = result != NVTX_SUCCESS; /* Set all to null if injection init failed */\n        NVTX_VERSIONED_IDENTIFIER(nvtxSetInitFunctionsToNoops)(forceAllToNoops);\n\n        /* Signal that initialization has finished, so now the assigned function pointers will be used */\n        NVTX_ATOMIC_WRITE_32(\n            &NVTX_VERSIONED_IDENTIFIER(nvtxGlobals).initState,\n            NVTX_INIT_STATE_COMPLETE);\n    }\n    else /* Spin-wait until initialization has finished */\n    {\n        NVTX_MEMBAR();\n        while (NVTX_VERSIONED_IDENTIFIER(nvtxGlobals).initState != NVTX_INIT_STATE_COMPLETE)\n        {\n            NVTX_YIELD();\n            NVTX_MEMBAR();\n        }\n    }\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.8.3-1-3bcyweyu5oveuk5a7rladbqwcq2srpds/spack-src/src/misc/ibvwrap.cc": "/*************************************************************************\n * Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"ibvwrap.h\"\n#include <sys/types.h>\n#include <unistd.h>\n\n#include <dlfcn.h>\n#include \"core.h\"\n\nstatic enum { ibvUninitialized, ibvInitializing, ibvInitialized, ibvError } ibvState = ibvUninitialized;\n\n/*Function Pointers*/\nint (*ibv_internal_fork_init)(void);\nstruct ibv_device** (*ibv_internal_get_device_list)(int *num_devices);\nvoid (*ibv_internal_free_device_list)(struct ibv_device **list);\nconst char * (*ibv_internal_get_device_name)(struct ibv_device *device);\nstruct ibv_context* (*ibv_internal_open_device)(struct ibv_device* device);\nint (*ibv_internal_close_device)(struct ibv_context *context);\nint (*ibv_internal_get_async_event)(struct ibv_context *context, struct ibv_async_event *event);\nvoid (*ibv_internal_ack_async_event)(struct ibv_async_event *event);\nint (*ibv_internal_query_device)(struct ibv_context *context, struct ibv_device_attr *device_attr);\nint (*ibv_internal_query_port)(struct ibv_context *context, uint8_t port_num, struct ibv_port_attr *port_attr);\nint (*ibv_internal_query_gid)(struct ibv_context *context, uint8_t port_num, int index, union ibv_gid *gid);\nint (*ibv_internal_query_qp)(struct ibv_qp *qp, struct ibv_qp_attr *attr, int attr_mask, struct ibv_qp_init_attr *init_attr);\nstruct ibv_pd * (*ibv_internal_alloc_pd)(struct ibv_context *context);\nint (*ibv_internal_dealloc_pd)(struct ibv_pd *pd);\nstruct ibv_mr * (*ibv_internal_reg_mr)(struct ibv_pd *pd, void *addr, size_t length, int access);\nint (*ibv_internal_dereg_mr)(struct ibv_mr *mr);\nstruct ibv_cq * (*ibv_internal_create_cq)(struct ibv_context *context, int cqe, void *cq_context, struct ibv_comp_channel *channel, int comp_vector);\nint (*ibv_internal_destroy_cq)(struct ibv_cq *cq);\nstruct ibv_qp * (*ibv_internal_create_qp)(struct ibv_pd *pd, struct ibv_qp_init_attr *qp_init_attr);\nint (*ibv_internal_modify_qp)(struct ibv_qp *qp, struct ibv_qp_attr *attr, int attr_mask);\nint (*ibv_internal_destroy_qp)(struct ibv_qp *qp);\nconst char * (*ibv_internal_event_type_str)(enum ibv_event_type event);\n\n// IBVERBS Library versioning\n#define IBVERBS_VERSION \"IBVERBS_1.1\"\n\nncclResult_t wrap_ibv_symbols(void) {\n  if (ibvState == ibvInitialized)\n    return ncclSuccess;\n  if (ibvState == ibvError)\n    return ncclSystemError;\n\n  if (__sync_bool_compare_and_swap(&ibvState, ibvUninitialized, ibvInitializing) == false) {\n    // Another thread raced in front of us. Wait for it to be done.\n    while (ibvState == ibvInitializing) pthread_yield();\n    return (ibvState == ibvInitialized) ? ncclSuccess : ncclSystemError;\n  }\n\n  static void* ibvhandle = NULL;\n  void* tmp;\n  void** cast;\n\n  ibvhandle=dlopen(\"libibverbs.so\", RTLD_NOW);\n  if (!ibvhandle) {\n    ibvhandle=dlopen(\"libibverbs.so.1\", RTLD_NOW);\n    if (!ibvhandle) {\n      WARN(\"Failed to open libibverbs.so[.1]\");\n      goto teardown;\n    }\n  }\n\n#define LOAD_SYM(handle, symbol, funcptr) do {         \\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlvsym(handle, symbol, IBVERBS_VERSION);       \\\n    if (tmp == NULL) {                                   \\\n      WARN(\"dlvsym failed on %s - %s version %s\", symbol, dlerror(), IBVERBS_VERSION);  \\\n      goto teardown;                                     \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n  LOAD_SYM(ibvhandle, \"ibv_get_device_list\", ibv_internal_get_device_list);\n  LOAD_SYM(ibvhandle, \"ibv_free_device_list\", ibv_internal_free_device_list);\n  LOAD_SYM(ibvhandle, \"ibv_get_device_name\", ibv_internal_get_device_name);\n  LOAD_SYM(ibvhandle, \"ibv_open_device\", ibv_internal_open_device);\n  LOAD_SYM(ibvhandle, \"ibv_close_device\", ibv_internal_close_device);\n  LOAD_SYM(ibvhandle, \"ibv_get_async_event\", ibv_internal_get_async_event);\n  LOAD_SYM(ibvhandle, \"ibv_ack_async_event\", ibv_internal_ack_async_event);\n  LOAD_SYM(ibvhandle, \"ibv_query_device\", ibv_internal_query_device);\n  LOAD_SYM(ibvhandle, \"ibv_query_port\", ibv_internal_query_port);\n  LOAD_SYM(ibvhandle, \"ibv_query_gid\", ibv_internal_query_gid);\n  LOAD_SYM(ibvhandle, \"ibv_query_qp\", ibv_internal_query_qp);\n  LOAD_SYM(ibvhandle, \"ibv_alloc_pd\", ibv_internal_alloc_pd);\n  LOAD_SYM(ibvhandle, \"ibv_dealloc_pd\", ibv_internal_dealloc_pd);\n  LOAD_SYM(ibvhandle, \"ibv_reg_mr\", ibv_internal_reg_mr);\n  LOAD_SYM(ibvhandle, \"ibv_dereg_mr\", ibv_internal_dereg_mr);\n  LOAD_SYM(ibvhandle, \"ibv_create_cq\", ibv_internal_create_cq);\n  LOAD_SYM(ibvhandle, \"ibv_destroy_cq\", ibv_internal_destroy_cq);\n  LOAD_SYM(ibvhandle, \"ibv_create_qp\", ibv_internal_create_qp);\n  LOAD_SYM(ibvhandle, \"ibv_modify_qp\", ibv_internal_modify_qp);\n  LOAD_SYM(ibvhandle, \"ibv_destroy_qp\", ibv_internal_destroy_qp);\n  LOAD_SYM(ibvhandle, \"ibv_fork_init\", ibv_internal_fork_init);\n  LOAD_SYM(ibvhandle, \"ibv_event_type_str\", ibv_internal_event_type_str);\n\n  ibvState = ibvInitialized;\n  return ncclSuccess;\n\nteardown:\n  ibv_internal_get_device_list = NULL;\n  ibv_internal_free_device_list = NULL;\n  ibv_internal_get_device_name = NULL;\n  ibv_internal_open_device = NULL;\n  ibv_internal_close_device = NULL;\n  ibv_internal_get_async_event = NULL;\n  ibv_internal_ack_async_event = NULL;\n  ibv_internal_query_device = NULL;\n  ibv_internal_query_port = NULL;\n  ibv_internal_query_gid = NULL;\n  ibv_internal_query_qp = NULL;\n  ibv_internal_alloc_pd = NULL;\n  ibv_internal_dealloc_pd = NULL;\n  ibv_internal_reg_mr = NULL;\n  ibv_internal_dereg_mr = NULL;\n  ibv_internal_create_cq = NULL;\n  ibv_internal_destroy_cq = NULL;\n  ibv_internal_create_qp = NULL;\n  ibv_internal_modify_qp = NULL;\n  ibv_internal_destroy_qp = NULL;\n  ibv_internal_fork_init = NULL;\n  ibv_internal_event_type_str = NULL;\n\n  if (ibvhandle != NULL) dlclose(ibvhandle);\n  ibvState = ibvError;\n  return ncclSystemError;\n}\n\n#define IBV_PTR_CHECK_ERRNO(name_internal, call, retval, error_retval, name) \\\n  if (name_internal == NULL) { \\\n     WARN(\"lib wrapper not initialized.\"); \\\n     return ncclInternalError; \\\n  } \\\n  retval = call; \\\n  if (retval == error_retval) { \\\n    WARN(\"Call to \" name \" failed with error %s\", strerror(errno)); \\\n    return ncclSystemError; \\\n  } \\\n  return ncclSuccess;\n\n#define IBV_PTR_CHECK(name_internal, call, retval, error_retval, name) \\\n  if (name_internal == NULL) { \\\n     WARN(\"lib wrapper not initialized.\"); \\\n     return ncclInternalError; \\\n  } \\\n  retval = call; \\\n  if (retval == error_retval) { \\\n    WARN(\"Call to \" name \" failed\"); \\\n    return ncclSystemError; \\\n  } \\\n  return ncclSuccess;\n\n#define IBV_INT_CHECK_RET_ERRNO(name_internal, call, success_retval, name) \\\n  if (name_internal == NULL) { \\\n     WARN(\"lib wrapper not initialized.\"); \\\n     return ncclInternalError; \\\n  } \\\n  int ret = call; \\\n  if (ret != success_retval) { \\\n    WARN(\"Call to \" name \" failed with error %s\", strerror(ret)); \\\n    return ncclSystemError; \\\n  } \\\n  return ncclSuccess;\n\n#define IBV_INT_CHECK(name_internal, call, error_retval, name) \\\n  if (name_internal == NULL) { \\\n     WARN(\"lib wrapper not initialized.\"); \\\n     return ncclInternalError; \\\n  } \\\n  int ret = call; \\\n  if (ret == error_retval) { \\\n    WARN(\"Call to \" name \" failed\"); \\\n    return ncclSystemError; \\\n  } \\\n  return ncclSuccess;\n\n#define IBV_PASSTHRU(name_internal, call) \\\n  if (name_internal == NULL) { \\\n     WARN(\"lib wrapper not initialized.\"); \\\n     return ncclInternalError; \\\n  } \\\n  call; \\\n  return ncclSuccess;\n\nncclResult_t wrap_ibv_fork_init() {\n  IBV_INT_CHECK(ibv_internal_fork_init, ibv_internal_fork_init(), -1, \"ibv_fork_init\");\n}\n\nncclResult_t wrap_ibv_get_device_list(struct ibv_device ***ret, int *num_devices) {\n  *ret = ibv_internal_get_device_list(num_devices);\n  if (*ret == NULL) *num_devices = 0;\n  return ncclSuccess;\n}\n\nncclResult_t wrap_ibv_free_device_list(struct ibv_device **list) {\n  IBV_PASSTHRU(ibv_internal_free_device_list, ibv_internal_free_device_list(list));\n}\n\nconst char *wrap_ibv_get_device_name(struct ibv_device *device) {\n  if (ibv_internal_get_device_name == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    exit(-1);\n  }\n  return ibv_internal_get_device_name(device);\n}\n\nncclResult_t wrap_ibv_open_device(struct ibv_context **ret, struct ibv_device *device) { /*returns 0 on success, -1 on failure*/\n  IBV_PTR_CHECK(ibv_internal_open_device, ibv_internal_open_device(device), *ret, NULL, \"ibv_open_device\");\n}\n\nncclResult_t wrap_ibv_close_device(struct ibv_context *context) { /*returns 0 on success, -1 on failure*/\n  IBV_INT_CHECK(ibv_internal_close_device, ibv_internal_close_device(context), -1, \"ibv_close_device\");\n}\n\nncclResult_t wrap_ibv_get_async_event(struct ibv_context *context, struct ibv_async_event *event) { /*returns 0 on success, and -1 on error*/\n  IBV_INT_CHECK(ibv_internal_get_async_event, ibv_internal_get_async_event(context, event), -1, \"ibv_get_async_event\");\n}\n\nncclResult_t wrap_ibv_ack_async_event(struct ibv_async_event *event) {\n  IBV_PASSTHRU(ibv_internal_ack_async_event, ibv_internal_ack_async_event(event));\n}\n\nncclResult_t wrap_ibv_query_device(struct ibv_context *context, struct ibv_device_attr *device_attr) { /*returns 0 on success, or the value of errno on failure (which indicates the failure reason)*/\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_query_device, ibv_internal_query_device(context, device_attr), 0, \"ibv_query_device\");\n}\n\nncclResult_t wrap_ibv_query_port(struct ibv_context *context, uint8_t port_num, struct ibv_port_attr *port_attr) { /*returns 0 on success, or the value of errno on failure (which indicates the failure reason)*/\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_query_port, ibv_internal_query_port(context, port_num, port_attr), 0, \"ibv_query_port\");\n}\n\nncclResult_t wrap_ibv_query_gid(struct ibv_context *context, uint8_t port_num, int index, union ibv_gid *gid) {\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_query_gid, ibv_internal_query_gid(context, port_num, index, gid), 0, \"ibv_query_gid\");\n}\n\nncclResult_t wrap_ibv_query_qp(struct ibv_qp *qp, struct ibv_qp_attr *attr, int attr_mask, struct ibv_qp_init_attr *init_attr) {\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_query_qp, ibv_internal_query_qp(qp, attr, attr_mask, init_attr), 0, \"ibv_query_qp\");\n}\n\nncclResult_t wrap_ibv_alloc_pd(struct ibv_pd **ret, struct ibv_context *context) {\n  IBV_PTR_CHECK(ibv_internal_alloc_pd, ibv_internal_alloc_pd(context), *ret, NULL, \"ibv_alloc_pd\");\n}\n\nncclResult_t wrap_ibv_dealloc_pd(struct ibv_pd *pd) { /*returns 0 on success, or the value of errno on failure (which indicates the failure reason)*/\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_dealloc_pd, ibv_internal_dealloc_pd(pd), 0, \"ibv_dealloc_pd\");\n}\n\nncclResult_t wrap_ibv_reg_mr(struct ibv_mr **ret, struct ibv_pd *pd, void *addr, size_t length, int access) {\n  IBV_PTR_CHECK(ibv_internal_reg_mr, ibv_internal_reg_mr(pd, addr, length, access), *ret, NULL, \"ibv_reg_mr\");\n}\n\nstruct ibv_mr * wrap_direct_ibv_reg_mr(struct ibv_pd *pd, void *addr, size_t length, int access) {\n  if (ibv_internal_reg_mr == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return NULL;\n  }\n  return ibv_internal_reg_mr(pd, addr, length, access);\n}\n\nncclResult_t wrap_ibv_dereg_mr(struct ibv_mr *mr) { /*returns 0 on success, or the value of errno on failure (which indicates the failure reason)*/\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_dereg_mr, ibv_internal_dereg_mr(mr), 0, \"ibv_dereg_mr\");\n}\n\nncclResult_t wrap_ibv_create_cq(struct ibv_cq **ret, struct ibv_context *context, int cqe, void *cq_context, struct ibv_comp_channel *channel, int comp_vector) {\n  IBV_PTR_CHECK(ibv_internal_create_cq, ibv_internal_create_cq(context, cqe, cq_context, channel, comp_vector), *ret, NULL, \"ibv_create_cq\");\n}\n\nncclResult_t wrap_ibv_destroy_cq(struct ibv_cq *cq) {\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_destroy_cq, ibv_internal_destroy_cq(cq), 0, \"ibv_destroy_cq\");\n}\n\nncclResult_t wrap_ibv_destroy_qp(struct ibv_qp *qp) {\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_destroy_qp, ibv_internal_destroy_qp(qp), 0, \"ibv_destroy_qp\");\n}\n\nncclResult_t wrap_ibv_create_qp(struct ibv_qp **ret, struct ibv_pd *pd, struct ibv_qp_init_attr *qp_init_attr) {\n  IBV_PTR_CHECK(ibv_internal_create_qp, ibv_internal_create_qp(pd, qp_init_attr), *ret, NULL, \"ibv_create_qp\");\n}\n\nncclResult_t wrap_ibv_modify_qp(struct ibv_qp *qp, struct ibv_qp_attr *attr, int attr_mask) { /*returns 0 on success, or the value of errno on failure (which indicates the failure reason)*/\n  IBV_INT_CHECK_RET_ERRNO(ibv_internal_modify_qp, ibv_internal_modify_qp(qp, attr, attr_mask), 0, \"ibv_modify_qp\");\n}\n\nncclResult_t wrap_ibv_event_type_str(char **ret, enum ibv_event_type event) {\n  *ret = (char *) ibv_internal_event_type_str(event);\n  return ncclSuccess;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-nccl-2.8.3-1-3bcyweyu5oveuk5a7rladbqwcq2srpds/spack-src/src/misc/nvmlwrap.cc": "/*************************************************************************\n * Copyright (c) 2015-2020, NVIDIA CORPORATION. All rights reserved.\n *\n * See LICENSE.txt for license information\n ************************************************************************/\n\n#include \"nvmlwrap.h\"\n\n#ifndef NVML_DIRECT\n#include <dlfcn.h>\n#include \"core.h\"\n\nstatic enum { nvmlUninitialized, nvmlInitializing, nvmlInitialized, nvmlError } nvmlState = nvmlUninitialized;\n\nstatic nvmlReturn_t (*nvmlInternalInit)(void);\nstatic nvmlReturn_t (*nvmlInternalShutdown)(void);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetHandleByPciBusId)(const char* pciBusId, nvmlDevice_t* device);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetIndex)(nvmlDevice_t device, unsigned* index);\nstatic const char* (*nvmlInternalErrorString)(nvmlReturn_t r);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkState)(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkRemotePciInfo)(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetNvLinkCapability)(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult);\nstatic nvmlReturn_t (*nvmlInternalDeviceGetCudaComputeCapability)(nvmlDevice_t device, int* major, int* minor);\n\n// Used to make the NVML library calls thread safe\npthread_mutex_t nvmlLock = PTHREAD_MUTEX_INITIALIZER;\n\nncclResult_t wrapNvmlSymbols(void) {\n  if (nvmlState == nvmlInitialized)\n    return ncclSuccess;\n  if (nvmlState == nvmlError)\n    return ncclSystemError;\n\n  if (__sync_bool_compare_and_swap(&nvmlState, nvmlUninitialized, nvmlInitializing) == false) {\n    // Another thread raced in front of us. Wait for it to be done.\n    while (nvmlState == nvmlInitializing) pthread_yield();\n    return (nvmlState == nvmlInitialized) ? ncclSuccess : ncclSystemError;\n  }\n\n  static void* nvmlhandle = NULL;\n  void* tmp;\n  void** cast;\n\n  nvmlhandle=dlopen(\"libnvidia-ml.so.1\", RTLD_NOW);\n  if (!nvmlhandle) {\n    WARN(\"Failed to open libnvidia-ml.so.1\");\n    goto teardown;\n  }\n\n#define LOAD_SYM(handle, symbol, funcptr) do {         \\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      WARN(\"dlsym failed on %s - %s\", symbol, dlerror());\\\n      goto teardown;                                     \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n#define LOAD_SYM_OPTIONAL(handle, symbol, funcptr) do {\\\n    cast = (void**)&funcptr;                             \\\n    tmp = dlsym(handle, symbol);                         \\\n    if (tmp == NULL) {                                   \\\n      INFO(NCCL_INIT,\"dlsym failed on %s, ignoring\", symbol); \\\n    }                                                    \\\n    *cast = tmp;                                         \\\n  } while (0)\n\n  LOAD_SYM(nvmlhandle, \"nvmlInit\", nvmlInternalInit);\n  LOAD_SYM(nvmlhandle, \"nvmlShutdown\", nvmlInternalShutdown);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetHandleByPciBusId\", nvmlInternalDeviceGetHandleByPciBusId);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetIndex\", nvmlInternalDeviceGetIndex);\n  LOAD_SYM(nvmlhandle, \"nvmlErrorString\", nvmlInternalErrorString);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkState\", nvmlInternalDeviceGetNvLinkState);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkRemotePciInfo\", nvmlInternalDeviceGetNvLinkRemotePciInfo);\n  LOAD_SYM_OPTIONAL(nvmlhandle, \"nvmlDeviceGetNvLinkCapability\", nvmlInternalDeviceGetNvLinkCapability);\n  LOAD_SYM(nvmlhandle, \"nvmlDeviceGetCudaComputeCapability\", nvmlInternalDeviceGetCudaComputeCapability);\n\n  nvmlState = nvmlInitialized;\n  return ncclSuccess;\n\nteardown:\n  nvmlInternalInit = NULL;\n  nvmlInternalShutdown = NULL;\n  nvmlInternalDeviceGetHandleByPciBusId = NULL;\n  nvmlInternalDeviceGetIndex = NULL;\n  nvmlInternalDeviceGetNvLinkState = NULL;\n  nvmlInternalDeviceGetNvLinkRemotePciInfo = NULL;\n  nvmlInternalDeviceGetNvLinkCapability = NULL;\n\n  if (nvmlhandle != NULL) dlclose(nvmlhandle);\n  nvmlState = nvmlError;\n  return ncclSystemError;\n}\n\n\nncclResult_t wrapNvmlInit(void) {\n  if (nvmlInternalInit == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalInit();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlInit() failed: %s\",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlShutdown(void) {\n  if (nvmlInternalShutdown == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret = nvmlInternalShutdown();\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlShutdown() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetHandleByPciBusId(const char* pciBusId, nvmlDevice_t* device) {\n  if (nvmlInternalDeviceGetHandleByPciBusId == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetHandleByPciBusId(pciBusId, device), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetHandleByPciBusId() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetIndex(nvmlDevice_t device, unsigned* index) {\n  if (nvmlInternalDeviceGetIndex == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetIndex(device, index), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetIndex() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkState(nvmlDevice_t device, unsigned int link, nvmlEnableState_t *isActive) {\n  if (nvmlInternalDeviceGetNvLinkState == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkState(device, link, isActive), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkState() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkRemotePciInfo(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t *pci) {\n  if (nvmlInternalDeviceGetNvLinkRemotePciInfo == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkRemotePciInfo(device, link, pci), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkRemotePciInfo() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetNvLinkCapability(nvmlDevice_t device, unsigned int link,\n    nvmlNvLinkCapability_t capability, unsigned int *capResult) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    /* Do not warn, this symbol is optional. */\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetNvLinkCapability(device, link, capability, capResult), ret);\n  if (ret != NVML_SUCCESS) {\n    if (ret != NVML_ERROR_NOT_SUPPORTED)\n      INFO(NCCL_INIT,\"nvmlDeviceGetNvLinkCapability() failed: %s \",\n          nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n\nncclResult_t wrapNvmlDeviceGetCudaComputeCapability(nvmlDevice_t device, int* major, int* minor) {\n  if (nvmlInternalDeviceGetNvLinkCapability == NULL) {\n    WARN(\"lib wrapper not initialized.\");\n    return ncclInternalError;\n  }\n  nvmlReturn_t ret;\n  NVMLLOCKCALL(nvmlInternalDeviceGetCudaComputeCapability(device, major, minor), ret);\n  if (ret != NVML_SUCCESS) {\n    WARN(\"nvmlDeviceGetCudaComputeCapability() failed: %s \",\n        nvmlInternalErrorString(ret));\n    return ncclSystemError;\n  }\n  return ncclSuccess;\n}\n#endif\n"
    },
    "skipped": [],
    "total_files": 121
}