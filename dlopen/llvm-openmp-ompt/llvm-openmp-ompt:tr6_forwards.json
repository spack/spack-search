{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/runtime/src/z_Linux_util.cpp": "/*\n * z_Linux_util.cpp -- platform specific routines.\n */\n\n//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n\n#include \"kmp.h\"\n#include \"kmp_affinity.h\"\n#include \"kmp_i18n.h\"\n#include \"kmp_io.h\"\n#include \"kmp_itt.h\"\n#include \"kmp_lock.h\"\n#include \"kmp_stats.h\"\n#include \"kmp_str.h\"\n#include \"kmp_wait_release.h\"\n#include \"kmp_wrapper_getpid.h\"\n\n#if !KMP_OS_FREEBSD && !KMP_OS_NETBSD\n#include <alloca.h>\n#endif\n#include <math.h> // HUGE_VAL.\n#include <sys/resource.h>\n#include <sys/syscall.h>\n#include <sys/time.h>\n#include <sys/times.h>\n#include <unistd.h>\n\n#if KMP_OS_LINUX && !KMP_OS_CNK\n#include <sys/sysinfo.h>\n#if KMP_USE_FUTEX\n// We should really include <futex.h>, but that causes compatibility problems on\n// different Linux* OS distributions that either require that you include (or\n// break when you try to include) <pci/types.h>. Since all we need is the two\n// macros below (which are part of the kernel ABI, so can't change) we just\n// define the constants here and don't include <futex.h>\n#ifndef FUTEX_WAIT\n#define FUTEX_WAIT 0\n#endif\n#ifndef FUTEX_WAKE\n#define FUTEX_WAKE 1\n#endif\n#endif\n#elif KMP_OS_DARWIN\n#include <mach/mach.h>\n#include <sys/sysctl.h>\n#elif KMP_OS_FREEBSD\n#include <pthread_np.h>\n#endif\n\n#include <ctype.h>\n#include <dirent.h>\n#include <fcntl.h>\n\n#include \"tsan_annotations.h\"\n\nstruct kmp_sys_timer {\n  struct timespec start;\n};\n\n// Convert timespec to nanoseconds.\n#define TS2NS(timespec) (((timespec).tv_sec * 1e9) + (timespec).tv_nsec)\n\nstatic struct kmp_sys_timer __kmp_sys_timer_data;\n\n#if KMP_HANDLE_SIGNALS\ntypedef void (*sig_func_t)(int);\nSTATIC_EFI2_WORKAROUND struct sigaction __kmp_sighldrs[NSIG];\nstatic sigset_t __kmp_sigset;\n#endif\n\nstatic int __kmp_init_runtime = FALSE;\n\nstatic int __kmp_fork_count = 0;\n\nstatic pthread_condattr_t __kmp_suspend_cond_attr;\nstatic pthread_mutexattr_t __kmp_suspend_mutex_attr;\n\nstatic kmp_cond_align_t __kmp_wait_cv;\nstatic kmp_mutex_align_t __kmp_wait_mx;\n\nkmp_uint64 __kmp_ticks_per_msec = 1000000;\n\n#ifdef DEBUG_SUSPEND\nstatic void __kmp_print_cond(char *buffer, kmp_cond_align_t *cond) {\n  KMP_SNPRINTF(buffer, 128, \"(cond (lock (%ld, %d)), (descr (%p)))\",\n               cond->c_cond.__c_lock.__status, cond->c_cond.__c_lock.__spinlock,\n               cond->c_cond.__c_waiting);\n}\n#endif\n\n#if (KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED)\n\n/* Affinity support */\n\nvoid __kmp_affinity_bind_thread(int which) {\n  KMP_ASSERT2(KMP_AFFINITY_CAPABLE(),\n              \"Illegal set affinity operation when not capable\");\n\n  kmp_affin_mask_t *mask;\n  KMP_CPU_ALLOC_ON_STACK(mask);\n  KMP_CPU_ZERO(mask);\n  KMP_CPU_SET(which, mask);\n  __kmp_set_system_affinity(mask, TRUE);\n  KMP_CPU_FREE_FROM_STACK(mask);\n}\n\n/* Determine if we can access affinity functionality on this version of\n * Linux* OS by checking __NR_sched_{get,set}affinity system calls, and set\n * __kmp_affin_mask_size to the appropriate value (0 means not capable). */\nvoid __kmp_affinity_determine_capable(const char *env_var) {\n// Check and see if the OS supports thread affinity.\n\n#define KMP_CPU_SET_SIZE_LIMIT (1024 * 1024)\n\n  int gCode;\n  int sCode;\n  unsigned char *buf;\n  buf = (unsigned char *)KMP_INTERNAL_MALLOC(KMP_CPU_SET_SIZE_LIMIT);\n\n  // If Linux* OS:\n  // If the syscall fails or returns a suggestion for the size,\n  // then we don't have to search for an appropriate size.\n  gCode = syscall(__NR_sched_getaffinity, 0, KMP_CPU_SET_SIZE_LIMIT, buf);\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"initial getaffinity call returned %d errno = %d\\n\",\n                gCode, errno));\n\n  // if ((gCode < 0) && (errno == ENOSYS))\n  if (gCode < 0) {\n    // System call not supported\n    if (__kmp_affinity_verbose ||\n        (__kmp_affinity_warnings && (__kmp_affinity_type != affinity_none) &&\n         (__kmp_affinity_type != affinity_default) &&\n         (__kmp_affinity_type != affinity_disabled))) {\n      int error = errno;\n      kmp_msg_t err_code = KMP_ERR(error);\n      __kmp_msg(kmp_ms_warning, KMP_MSG(GetAffSysCallNotSupported, env_var),\n                err_code, __kmp_msg_null);\n      if (__kmp_generate_warnings == kmp_warnings_off) {\n        __kmp_str_free(&err_code.str);\n      }\n    }\n    KMP_AFFINITY_DISABLE();\n    KMP_INTERNAL_FREE(buf);\n    return;\n  }\n  if (gCode > 0) { // Linux* OS only\n    // The optimal situation: the OS returns the size of the buffer it expects.\n    //\n    // A verification of correct behavior is that Isetaffinity on a NULL\n    // buffer with the same size fails with errno set to EFAULT.\n    sCode = syscall(__NR_sched_setaffinity, 0, gCode, NULL);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"setaffinity for mask size %d returned %d errno = %d\\n\",\n                  gCode, sCode, errno));\n    if (sCode < 0) {\n      if (errno == ENOSYS) {\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(SetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n      }\n      if (errno == EFAULT) {\n        KMP_AFFINITY_ENABLE(gCode);\n        KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                      \"affinity supported (mask size %d)\\n\",\n                      (int)__kmp_affin_mask_size));\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n    }\n  }\n\n  // Call the getaffinity system call repeatedly with increasing set sizes\n  // until we succeed, or reach an upper bound on the search.\n  KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                \"searching for proper set size\\n\"));\n  int size;\n  for (size = 1; size <= KMP_CPU_SET_SIZE_LIMIT; size *= 2) {\n    gCode = syscall(__NR_sched_getaffinity, 0, size, buf);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"getaffinity for mask size %d returned %d errno = %d\\n\",\n                  size, gCode, errno));\n\n    if (gCode < 0) {\n      if (errno == ENOSYS) {\n        // We shouldn't get here\n        KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                      \"inconsistent OS call behavior: errno == ENOSYS for mask \"\n                      \"size %d\\n\",\n                      size));\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(GetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n      continue;\n    }\n\n    sCode = syscall(__NR_sched_setaffinity, 0, gCode, NULL);\n    KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                  \"setaffinity for mask size %d returned %d errno = %d\\n\",\n                  gCode, sCode, errno));\n    if (sCode < 0) {\n      if (errno == ENOSYS) { // Linux* OS only\n        // We shouldn't get here\n        KA_TRACE(30, (\"__kmp_affinity_determine_capable: \"\n                      \"inconsistent OS call behavior: errno == ENOSYS for mask \"\n                      \"size %d\\n\",\n                      size));\n        if (__kmp_affinity_verbose ||\n            (__kmp_affinity_warnings &&\n             (__kmp_affinity_type != affinity_none) &&\n             (__kmp_affinity_type != affinity_default) &&\n             (__kmp_affinity_type != affinity_disabled))) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(SetAffSysCallNotSupported, env_var),\n                    err_code, __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n        KMP_AFFINITY_DISABLE();\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n      if (errno == EFAULT) {\n        KMP_AFFINITY_ENABLE(gCode);\n        KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                      \"affinity supported (mask size %d)\\n\",\n                      (int)__kmp_affin_mask_size));\n        KMP_INTERNAL_FREE(buf);\n        return;\n      }\n    }\n  }\n  // save uncaught error code\n  // int error = errno;\n  KMP_INTERNAL_FREE(buf);\n  // restore uncaught error code, will be printed at the next KMP_WARNING below\n  // errno = error;\n\n  // Affinity is not supported\n  KMP_AFFINITY_DISABLE();\n  KA_TRACE(10, (\"__kmp_affinity_determine_capable: \"\n                \"cannot determine mask size - affinity not supported\\n\"));\n  if (__kmp_affinity_verbose ||\n      (__kmp_affinity_warnings && (__kmp_affinity_type != affinity_none) &&\n       (__kmp_affinity_type != affinity_default) &&\n       (__kmp_affinity_type != affinity_disabled))) {\n    KMP_WARNING(AffCantGetMaskSize, env_var);\n  }\n}\n\n#endif // KMP_OS_LINUX && KMP_AFFINITY_SUPPORTED\n\n#if KMP_USE_FUTEX\n\nint __kmp_futex_determine_capable() {\n  int loc = 0;\n  int rc = syscall(__NR_futex, &loc, FUTEX_WAKE, 1, NULL, NULL, 0);\n  int retval = (rc == 0) || (errno != ENOSYS);\n\n  KA_TRACE(10,\n           (\"__kmp_futex_determine_capable: rc = %d errno = %d\\n\", rc, errno));\n  KA_TRACE(10, (\"__kmp_futex_determine_capable: futex syscall%s supported\\n\",\n                retval ? \"\" : \" not\"));\n\n  return retval;\n}\n\n#endif // KMP_USE_FUTEX\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (!KMP_ASM_INTRINS)\n/* Only 32-bit \"add-exchange\" instruction on IA-32 architecture causes us to\n   use compare_and_store for these routines */\n\nkmp_int8 __kmp_test_then_or8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value | d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_int8 __kmp_test_then_and8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value & d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\nkmp_uint32 __kmp_test_then_or32(volatile kmp_uint32 *p, kmp_uint32 d) {\n  kmp_uint32 old_value, new_value;\n\n  old_value = TCR_4(*p);\n  new_value = old_value | d;\n\n  while (!KMP_COMPARE_AND_STORE_REL32(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_4(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_uint32 __kmp_test_then_and32(volatile kmp_uint32 *p, kmp_uint32 d) {\n  kmp_uint32 old_value, new_value;\n\n  old_value = TCR_4(*p);\n  new_value = old_value & d;\n\n  while (!KMP_COMPARE_AND_STORE_REL32(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_4(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\n#if KMP_ARCH_X86\nkmp_int8 __kmp_test_then_add8(volatile kmp_int8 *p, kmp_int8 d) {\n  kmp_int8 old_value, new_value;\n\n  old_value = TCR_1(*p);\n  new_value = old_value + d;\n\n  while (!KMP_COMPARE_AND_STORE_REL8(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_1(*p);\n    new_value = old_value + d;\n  }\n  return old_value;\n}\n\nkmp_int64 __kmp_test_then_add64(volatile kmp_int64 *p, kmp_int64 d) {\n  kmp_int64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value + d;\n\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value + d;\n  }\n  return old_value;\n}\n#endif /* KMP_ARCH_X86 */\n\nkmp_uint64 __kmp_test_then_or64(volatile kmp_uint64 *p, kmp_uint64 d) {\n  kmp_uint64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value | d;\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value | d;\n  }\n  return old_value;\n}\n\nkmp_uint64 __kmp_test_then_and64(volatile kmp_uint64 *p, kmp_uint64 d) {\n  kmp_uint64 old_value, new_value;\n\n  old_value = TCR_8(*p);\n  new_value = old_value & d;\n  while (!KMP_COMPARE_AND_STORE_REL64(p, old_value, new_value)) {\n    KMP_CPU_PAUSE();\n    old_value = TCR_8(*p);\n    new_value = old_value & d;\n  }\n  return old_value;\n}\n\n#endif /* (KMP_ARCH_X86 || KMP_ARCH_X86_64) && (! KMP_ASM_INTRINS) */\n\nvoid __kmp_terminate_thread(int gtid) {\n  int status;\n  kmp_info_t *th = __kmp_threads[gtid];\n\n  if (!th)\n    return;\n\n#ifdef KMP_CANCEL_THREADS\n  KA_TRACE(10, (\"__kmp_terminate_thread: kill (%d)\\n\", gtid));\n  status = pthread_cancel(th->th.th_info.ds.ds_thread);\n  if (status != 0 && status != ESRCH) {\n    __kmp_fatal(KMP_MSG(CantTerminateWorkerThread), KMP_ERR(status),\n                __kmp_msg_null);\n  }\n#endif\n  __kmp_yield(TRUE);\n} //\n\n/* Set thread stack info according to values returned by pthread_getattr_np().\n   If values are unreasonable, assume call failed and use incremental stack\n   refinement method instead. Returns TRUE if the stack parameters could be\n   determined exactly, FALSE if incremental refinement is necessary. */\nstatic kmp_int32 __kmp_set_stack_info(int gtid, kmp_info_t *th) {\n  int stack_data;\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n  /* Linux* OS only -- no pthread_getattr_np support on OS X* */\n  pthread_attr_t attr;\n  int status;\n  size_t size = 0;\n  void *addr = 0;\n\n  /* Always do incremental stack refinement for ubermaster threads since the\n     initial thread stack range can be reduced by sibling thread creation so\n     pthread_attr_getstack may cause thread gtid aliasing */\n  if (!KMP_UBER_GTID(gtid)) {\n\n    /* Fetch the real thread attributes */\n    status = pthread_attr_init(&attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_init\", status);\n#if KMP_OS_FREEBSD || KMP_OS_NETBSD\n    status = pthread_attr_get_np(pthread_self(), &attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_get_np\", status);\n#else\n    status = pthread_getattr_np(pthread_self(), &attr);\n    KMP_CHECK_SYSFAIL(\"pthread_getattr_np\", status);\n#endif\n    status = pthread_attr_getstack(&attr, &addr, &size);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_getstack\", status);\n    KA_TRACE(60,\n             (\"__kmp_set_stack_info: T#%d pthread_attr_getstack returned size:\"\n              \" %lu, low addr: %p\\n\",\n              gtid, size, addr));\n    status = pthread_attr_destroy(&attr);\n    KMP_CHECK_SYSFAIL(\"pthread_attr_destroy\", status);\n  }\n\n  if (size != 0 && addr != 0) { // was stack parameter determination successful?\n    /* Store the correct base and size */\n    TCW_PTR(th->th.th_info.ds.ds_stackbase, (((char *)addr) + size));\n    TCW_PTR(th->th.th_info.ds.ds_stacksize, size);\n    TCW_4(th->th.th_info.ds.ds_stackgrow, FALSE);\n    return TRUE;\n  }\n#endif /* KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD */\n  /* Use incremental refinement starting from initial conservative estimate */\n  TCW_PTR(th->th.th_info.ds.ds_stacksize, 0);\n  TCW_PTR(th->th.th_info.ds.ds_stackbase, &stack_data);\n  TCW_4(th->th.th_info.ds.ds_stackgrow, TRUE);\n  return FALSE;\n}\n\nstatic void *__kmp_launch_worker(void *thr) {\n  int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n  sigset_t new_set, old_set;\n#endif /* KMP_BLOCK_SIGNALS */\n  void *exit_val;\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n  void *volatile padding = 0;\n#endif\n  int gtid;\n\n  gtid = ((kmp_info_t *)thr)->th.th_info.ds.ds_gtid;\n  __kmp_gtid_set_specific(gtid);\n#ifdef KMP_TDATA_GTID\n  __kmp_gtid = gtid;\n#endif\n#if KMP_STATS_ENABLED\n  // set thread local index to point to thread-specific stats\n  __kmp_stats_thread_ptr = ((kmp_info_t *)thr)->th.th_stats;\n  KMP_START_EXPLICIT_TIMER(OMP_worker_thread_life);\n  KMP_SET_THREAD_STATE(IDLE);\n  KMP_INIT_PARTITIONED_TIMERS(OMP_idle);\n#endif\n\n#if USE_ITT_BUILD\n  __kmp_itt_thread_name(gtid);\n#endif /* USE_ITT_BUILD */\n\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_set_init_mask(gtid, FALSE);\n#endif\n\n#ifdef KMP_CANCEL_THREADS\n  status = pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, &old_type);\n  KMP_CHECK_SYSFAIL(\"pthread_setcanceltype\", status);\n  // josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads?\n  status = pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n  // Set FP control regs to be a copy of the parallel initialization thread's.\n  __kmp_clear_x87_fpu_status_word();\n  __kmp_load_x87_fpu_control_word(&__kmp_init_x87_fpu_control_word);\n  __kmp_load_mxcsr(&__kmp_init_mxcsr);\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = sigfillset(&new_set);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigfillset\", status);\n  status = pthread_sigmask(SIG_BLOCK, &new_set, &old_set);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n  if (__kmp_stkoffset > 0 && gtid > 0) {\n    padding = KMP_ALLOCA(gtid * __kmp_stkoffset);\n  }\n#endif\n\n  KMP_MB();\n  __kmp_set_stack_info(gtid, (kmp_info_t *)thr);\n\n  __kmp_check_stack_overlap((kmp_info_t *)thr);\n\n  exit_val = __kmp_launch_thread((kmp_info_t *)thr);\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = pthread_sigmask(SIG_SETMASK, &old_set, NULL);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n  return exit_val;\n}\n\n#if KMP_USE_MONITOR\n/* The monitor thread controls all of the threads in the complex */\n\nstatic void *__kmp_launch_monitor(void *thr) {\n  int status, old_type, old_state;\n#ifdef KMP_BLOCK_SIGNALS\n  sigset_t new_set;\n#endif /* KMP_BLOCK_SIGNALS */\n  struct timespec interval;\n  int yield_count;\n  int yield_cycles = 0;\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #1 launched\\n\"));\n\n  /* register us as the monitor thread */\n  __kmp_gtid_set_specific(KMP_GTID_MONITOR);\n#ifdef KMP_TDATA_GTID\n  __kmp_gtid = KMP_GTID_MONITOR;\n#endif\n\n  KMP_MB();\n\n#if USE_ITT_BUILD\n  // Instruct Intel(R) Threading Tools to ignore monitor thread.\n  __kmp_itt_thread_ignore();\n#endif /* USE_ITT_BUILD */\n\n  __kmp_set_stack_info(((kmp_info_t *)thr)->th.th_info.ds.ds_gtid,\n                       (kmp_info_t *)thr);\n\n  __kmp_check_stack_overlap((kmp_info_t *)thr);\n\n#ifdef KMP_CANCEL_THREADS\n  status = pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, &old_type);\n  KMP_CHECK_SYSFAIL(\"pthread_setcanceltype\", status);\n  // josh todo: isn't PTHREAD_CANCEL_ENABLE default for newly-created threads?\n  status = pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n\n#if KMP_REAL_TIME_FIX\n  // This is a potential fix which allows application with real-time scheduling\n  // policy work. However, decision about the fix is not made yet, so it is\n  // disabled by default.\n  { // Are program started with real-time scheduling policy?\n    int sched = sched_getscheduler(0);\n    if (sched == SCHED_FIFO || sched == SCHED_RR) {\n      // Yes, we are a part of real-time application. Try to increase the\n      // priority of the monitor.\n      struct sched_param param;\n      int max_priority = sched_get_priority_max(sched);\n      int rc;\n      KMP_WARNING(RealTimeSchedNotSupported);\n      sched_getparam(0, &param);\n      if (param.sched_priority < max_priority) {\n        param.sched_priority += 1;\n        rc = sched_setscheduler(0, sched, &param);\n        if (rc != 0) {\n          int error = errno;\n          kmp_msg_t err_code = KMP_ERR(error);\n          __kmp_msg(kmp_ms_warning, KMP_MSG(CantChangeMonitorPriority),\n                    err_code, KMP_MSG(MonitorWillStarve), __kmp_msg_null);\n          if (__kmp_generate_warnings == kmp_warnings_off) {\n            __kmp_str_free(&err_code.str);\n          }\n        }\n      } else {\n        // We cannot abort here, because number of CPUs may be enough for all\n        // the threads, including the monitor thread, so application could\n        // potentially work...\n        __kmp_msg(kmp_ms_warning, KMP_MSG(RunningAtMaxPriority),\n                  KMP_MSG(MonitorWillStarve), KMP_HNT(RunningAtMaxPriority),\n                  __kmp_msg_null);\n      }\n    }\n    // AC: free thread that waits for monitor started\n    TCW_4(__kmp_global.g.g_time.dt.t_value, 0);\n  }\n#endif // KMP_REAL_TIME_FIX\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  if (__kmp_monitor_wakeups == 1) {\n    interval.tv_sec = 1;\n    interval.tv_nsec = 0;\n  } else {\n    interval.tv_sec = 0;\n    interval.tv_nsec = (KMP_NSEC_PER_SEC / __kmp_monitor_wakeups);\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #2 monitor\\n\"));\n\n  if (__kmp_yield_cycle) {\n    __kmp_yielding_on = 0; /* Start out with yielding shut off */\n    yield_count = __kmp_yield_off_count;\n  } else {\n    __kmp_yielding_on = 1; /* Yielding is on permanently */\n  }\n\n  while (!TCR_4(__kmp_global.g.g_done)) {\n    struct timespec now;\n    struct timeval tval;\n\n    /*  This thread monitors the state of the system */\n\n    KA_TRACE(15, (\"__kmp_launch_monitor: update\\n\"));\n\n    status = gettimeofday(&tval, NULL);\n    KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n    TIMEVAL_TO_TIMESPEC(&tval, &now);\n\n    now.tv_sec += interval.tv_sec;\n    now.tv_nsec += interval.tv_nsec;\n\n    if (now.tv_nsec >= KMP_NSEC_PER_SEC) {\n      now.tv_sec += 1;\n      now.tv_nsec -= KMP_NSEC_PER_SEC;\n    }\n\n    status = pthread_mutex_lock(&__kmp_wait_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n    // AC: the monitor should not fall asleep if g_done has been set\n    if (!TCR_4(__kmp_global.g.g_done)) { // check once more under mutex\n      status = pthread_cond_timedwait(&__kmp_wait_cv.c_cond,\n                                      &__kmp_wait_mx.m_mutex, &now);\n      if (status != 0) {\n        if (status != ETIMEDOUT && status != EINTR) {\n          KMP_SYSFAIL(\"pthread_cond_timedwait\", status);\n        }\n      }\n    }\n    status = pthread_mutex_unlock(&__kmp_wait_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n\n    if (__kmp_yield_cycle) {\n      yield_cycles++;\n      if ((yield_cycles % yield_count) == 0) {\n        if (__kmp_yielding_on) {\n          __kmp_yielding_on = 0; /* Turn it off now */\n          yield_count = __kmp_yield_off_count;\n        } else {\n          __kmp_yielding_on = 1; /* Turn it on now */\n          yield_count = __kmp_yield_on_count;\n        }\n        yield_cycles = 0;\n      }\n    } else {\n      __kmp_yielding_on = 1;\n    }\n\n    TCW_4(__kmp_global.g.g_time.dt.t_value,\n          TCR_4(__kmp_global.g.g_time.dt.t_value) + 1);\n\n    KMP_MB(); /* Flush all pending memory write invalidates.  */\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #3 cleanup\\n\"));\n\n#ifdef KMP_BLOCK_SIGNALS\n  status = sigfillset(&new_set);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigfillset\", status);\n  status = pthread_sigmask(SIG_UNBLOCK, &new_set, NULL);\n  KMP_CHECK_SYSFAIL(\"pthread_sigmask\", status);\n#endif /* KMP_BLOCK_SIGNALS */\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #4 finished\\n\"));\n\n  if (__kmp_global.g.g_abort != 0) {\n    /* now we need to terminate the worker threads  */\n    /* the value of t_abort is the signal we caught */\n\n    int gtid;\n\n    KA_TRACE(10, (\"__kmp_launch_monitor: #5 terminate sig=%d\\n\",\n                  __kmp_global.g.g_abort));\n\n    /* terminate the OpenMP worker threads */\n    /* TODO this is not valid for sibling threads!!\n     * the uber master might not be 0 anymore.. */\n    for (gtid = 1; gtid < __kmp_threads_capacity; ++gtid)\n      __kmp_terminate_thread(gtid);\n\n    __kmp_cleanup();\n\n    KA_TRACE(10, (\"__kmp_launch_monitor: #6 raise sig=%d\\n\",\n                  __kmp_global.g.g_abort));\n\n    if (__kmp_global.g.g_abort > 0)\n      raise(__kmp_global.g.g_abort);\n  }\n\n  KA_TRACE(10, (\"__kmp_launch_monitor: #7 exit\\n\"));\n\n  return thr;\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_create_worker(int gtid, kmp_info_t *th, size_t stack_size) {\n  pthread_t handle;\n  pthread_attr_t thread_attr;\n  int status;\n\n  th->th.th_info.ds.ds_gtid = gtid;\n\n#if KMP_STATS_ENABLED\n  // sets up worker thread stats\n  __kmp_acquire_tas_lock(&__kmp_stats_lock, gtid);\n\n  // th->th.th_stats is used to transfer thread-specific stats-pointer to\n  // __kmp_launch_worker. So when thread is created (goes into\n  // __kmp_launch_worker) it will set its thread local pointer to\n  // th->th.th_stats\n  if (!KMP_UBER_GTID(gtid)) {\n    th->th.th_stats = __kmp_stats_list->push_back(gtid);\n  } else {\n    // For root threads, __kmp_stats_thread_ptr is set in __kmp_register_root(),\n    // so set the th->th.th_stats field to it.\n    th->th.th_stats = __kmp_stats_thread_ptr;\n  }\n  __kmp_release_tas_lock(&__kmp_stats_lock, gtid);\n\n#endif // KMP_STATS_ENABLED\n\n  if (KMP_UBER_GTID(gtid)) {\n    KA_TRACE(10, (\"__kmp_create_worker: uber thread (%d)\\n\", gtid));\n    th->th.th_info.ds.ds_thread = pthread_self();\n    __kmp_set_stack_info(gtid, th);\n    __kmp_check_stack_overlap(th);\n    return;\n  }\n\n  KA_TRACE(10, (\"__kmp_create_worker: try to create thread (%d)\\n\", gtid));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_init(&thread_attr);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantInitThreadAttrs), KMP_ERR(status), __kmp_msg_null);\n  }\n  status = pthread_attr_setdetachstate(&thread_attr, PTHREAD_CREATE_JOINABLE);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetWorkerState), KMP_ERR(status), __kmp_msg_null);\n  }\n\n  /* Set stack size for this thread now.\n     The multiple of 2 is there because on some machines, requesting an unusual\n     stacksize causes the thread to have an offset before the dummy alloca()\n     takes place to create the offset.  Since we want the user to have a\n     sufficient stacksize AND support a stack offset, we alloca() twice the\n     offset so that the upcoming alloca() does not eliminate any premade offset,\n     and also gives the user the stack space they requested for all threads */\n  stack_size += gtid * __kmp_stkoffset * 2;\n\n  KA_TRACE(10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                \"__kmp_stksize = %lu bytes, final stacksize = %lu bytes\\n\",\n                gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size));\n\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  status = pthread_attr_setstacksize(&thread_attr, stack_size);\n#ifdef KMP_BACKUP_STKSIZE\n  if (status != 0) {\n    if (!__kmp_env_stksize) {\n      stack_size = KMP_BACKUP_STKSIZE + gtid * __kmp_stkoffset;\n      __kmp_stksize = KMP_BACKUP_STKSIZE;\n      KA_TRACE(10, (\"__kmp_create_worker: T#%d, default stacksize = %lu bytes, \"\n                    \"__kmp_stksize = %lu bytes, (backup) final stacksize = %lu \"\n                    \"bytes\\n\",\n                    gtid, KMP_DEFAULT_STKSIZE, __kmp_stksize, stack_size));\n      status = pthread_attr_setstacksize(&thread_attr, stack_size);\n    }\n  }\n#endif /* KMP_BACKUP_STKSIZE */\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                KMP_HNT(ChangeWorkerStackSize), __kmp_msg_null);\n  }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n#endif /* KMP_THREAD_ATTR */\n\n  status =\n      pthread_create(&handle, &thread_attr, __kmp_launch_worker, (void *)th);\n  if (status != 0 || !handle) { // ??? Why do we check handle??\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n    if (status == EINVAL) {\n      __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                  KMP_HNT(IncreaseWorkerStackSize), __kmp_msg_null);\n    }\n    if (status == ENOMEM) {\n      __kmp_fatal(KMP_MSG(CantSetWorkerStackSize, stack_size), KMP_ERR(status),\n                  KMP_HNT(DecreaseWorkerStackSize), __kmp_msg_null);\n    }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    if (status == EAGAIN) {\n      __kmp_fatal(KMP_MSG(NoResourcesForWorkerThread), KMP_ERR(status),\n                  KMP_HNT(Decrease_NUM_THREADS), __kmp_msg_null);\n    }\n    KMP_SYSFAIL(\"pthread_create\", status);\n  }\n\n  th->th.th_info.ds.ds_thread = handle;\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_destroy(&thread_attr);\n  if (status) {\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, KMP_MSG(CantDestroyThreadAttrs), err_code,\n              __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif /* KMP_THREAD_ATTR */\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_create_worker: done creating thread (%d)\\n\", gtid));\n\n} // __kmp_create_worker\n\n#if KMP_USE_MONITOR\nvoid __kmp_create_monitor(kmp_info_t *th) {\n  pthread_t handle;\n  pthread_attr_t thread_attr;\n  size_t size;\n  int status;\n  int auto_adj_size = FALSE;\n\n  if (__kmp_dflt_blocktime == KMP_MAX_BLOCKTIME) {\n    // We don't need monitor thread in case of MAX_BLOCKTIME\n    KA_TRACE(10, (\"__kmp_create_monitor: skipping monitor thread because of \"\n                  \"MAX blocktime\\n\"));\n    th->th.th_info.ds.ds_tid = 0; // this makes reap_monitor no-op\n    th->th.th_info.ds.ds_gtid = 0;\n    return;\n  }\n  KA_TRACE(10, (\"__kmp_create_monitor: try to create monitor\\n\"));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  th->th.th_info.ds.ds_tid = KMP_GTID_MONITOR;\n  th->th.th_info.ds.ds_gtid = KMP_GTID_MONITOR;\n#if KMP_REAL_TIME_FIX\n  TCW_4(__kmp_global.g.g_time.dt.t_value,\n        -1); // Will use it for synchronization a bit later.\n#else\n  TCW_4(__kmp_global.g.g_time.dt.t_value, 0);\n#endif // KMP_REAL_TIME_FIX\n\n#ifdef KMP_THREAD_ATTR\n  if (__kmp_monitor_stksize == 0) {\n    __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n    auto_adj_size = TRUE;\n  }\n  status = pthread_attr_init(&thread_attr);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantInitThreadAttrs), KMP_ERR(status), __kmp_msg_null);\n  }\n  status = pthread_attr_setdetachstate(&thread_attr, PTHREAD_CREATE_JOINABLE);\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(CantSetMonitorState), KMP_ERR(status), __kmp_msg_null);\n  }\n\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  status = pthread_attr_getstacksize(&thread_attr, &size);\n  KMP_CHECK_SYSFAIL(\"pthread_attr_getstacksize\", status);\n#else\n  size = __kmp_sys_min_stksize;\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n#endif /* KMP_THREAD_ATTR */\n\n  if (__kmp_monitor_stksize == 0) {\n    __kmp_monitor_stksize = KMP_DEFAULT_MONITOR_STKSIZE;\n  }\n  if (__kmp_monitor_stksize < __kmp_sys_min_stksize) {\n    __kmp_monitor_stksize = __kmp_sys_min_stksize;\n  }\n\n  KA_TRACE(10, (\"__kmp_create_monitor: default stacksize = %lu bytes,\"\n                \"requested stacksize = %lu bytes\\n\",\n                size, __kmp_monitor_stksize));\n\nretry:\n\n/* Set stack size for this thread now. */\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n  KA_TRACE(10, (\"__kmp_create_monitor: setting stacksize = %lu bytes,\",\n                __kmp_monitor_stksize));\n  status = pthread_attr_setstacksize(&thread_attr, __kmp_monitor_stksize);\n  if (status != 0) {\n    if (auto_adj_size) {\n      __kmp_monitor_stksize *= 2;\n      goto retry;\n    }\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, // should this be fatal?  BB\n              KMP_MSG(CantSetMonitorStackSize, (long int)__kmp_monitor_stksize),\n              err_code, KMP_HNT(ChangeMonitorStackSize), __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n\n  status =\n      pthread_create(&handle, &thread_attr, __kmp_launch_monitor, (void *)th);\n\n  if (status != 0) {\n#ifdef _POSIX_THREAD_ATTR_STACKSIZE\n    if (status == EINVAL) {\n      if (auto_adj_size && (__kmp_monitor_stksize < (size_t)0x40000000)) {\n        __kmp_monitor_stksize *= 2;\n        goto retry;\n      }\n      __kmp_fatal(KMP_MSG(CantSetMonitorStackSize, __kmp_monitor_stksize),\n                  KMP_ERR(status), KMP_HNT(IncreaseMonitorStackSize),\n                  __kmp_msg_null);\n    }\n    if (status == ENOMEM) {\n      __kmp_fatal(KMP_MSG(CantSetMonitorStackSize, __kmp_monitor_stksize),\n                  KMP_ERR(status), KMP_HNT(DecreaseMonitorStackSize),\n                  __kmp_msg_null);\n    }\n#endif /* _POSIX_THREAD_ATTR_STACKSIZE */\n    if (status == EAGAIN) {\n      __kmp_fatal(KMP_MSG(NoResourcesForMonitorThread), KMP_ERR(status),\n                  KMP_HNT(DecreaseNumberOfThreadsInUse), __kmp_msg_null);\n    }\n    KMP_SYSFAIL(\"pthread_create\", status);\n  }\n\n  th->th.th_info.ds.ds_thread = handle;\n\n#if KMP_REAL_TIME_FIX\n  // Wait for the monitor thread is really started and set its *priority*.\n  KMP_DEBUG_ASSERT(sizeof(kmp_uint32) ==\n                   sizeof(__kmp_global.g.g_time.dt.t_value));\n  __kmp_wait_yield_4((kmp_uint32 volatile *)&__kmp_global.g.g_time.dt.t_value,\n                     -1, &__kmp_neq_4, NULL);\n#endif // KMP_REAL_TIME_FIX\n\n#ifdef KMP_THREAD_ATTR\n  status = pthread_attr_destroy(&thread_attr);\n  if (status != 0) {\n    kmp_msg_t err_code = KMP_ERR(status);\n    __kmp_msg(kmp_ms_warning, KMP_MSG(CantDestroyThreadAttrs), err_code,\n              __kmp_msg_null);\n    if (__kmp_generate_warnings == kmp_warnings_off) {\n      __kmp_str_free(&err_code.str);\n    }\n  }\n#endif\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(10, (\"__kmp_create_monitor: monitor created %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n} // __kmp_create_monitor\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_exit_thread(int exit_status) {\n  pthread_exit((void *)(intptr_t)exit_status);\n} // __kmp_exit_thread\n\n#if KMP_USE_MONITOR\nvoid __kmp_resume_monitor();\n\nvoid __kmp_reap_monitor(kmp_info_t *th) {\n  int status;\n  void *exit_val;\n\n  KA_TRACE(10, (\"__kmp_reap_monitor: try to reap monitor thread with handle\"\n                \" %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n  // If monitor has been created, its tid and gtid should be KMP_GTID_MONITOR.\n  // If both tid and gtid are 0, it means the monitor did not ever start.\n  // If both tid and gtid are KMP_GTID_DNE, the monitor has been shut down.\n  KMP_DEBUG_ASSERT(th->th.th_info.ds.ds_tid == th->th.th_info.ds.ds_gtid);\n  if (th->th.th_info.ds.ds_gtid != KMP_GTID_MONITOR) {\n    KA_TRACE(10, (\"__kmp_reap_monitor: monitor did not start, returning\\n\"));\n    return;\n  }\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  /* First, check to see whether the monitor thread exists to wake it up. This\n     is to avoid performance problem when the monitor sleeps during\n     blocktime-size interval */\n\n  status = pthread_kill(th->th.th_info.ds.ds_thread, 0);\n  if (status != ESRCH) {\n    __kmp_resume_monitor(); // Wake up the monitor thread\n  }\n  KA_TRACE(10, (\"__kmp_reap_monitor: try to join with monitor\\n\"));\n  status = pthread_join(th->th.th_info.ds.ds_thread, &exit_val);\n  if (exit_val != th) {\n    __kmp_fatal(KMP_MSG(ReapMonitorError), KMP_ERR(status), __kmp_msg_null);\n  }\n\n  th->th.th_info.ds.ds_tid = KMP_GTID_DNE;\n  th->th.th_info.ds.ds_gtid = KMP_GTID_DNE;\n\n  KA_TRACE(10, (\"__kmp_reap_monitor: done reaping monitor thread with handle\"\n                \" %#.8lx\\n\",\n                th->th.th_info.ds.ds_thread));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_reap_worker(kmp_info_t *th) {\n  int status;\n  void *exit_val;\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n\n  KA_TRACE(\n      10, (\"__kmp_reap_worker: try to reap T#%d\\n\", th->th.th_info.ds.ds_gtid));\n\n  status = pthread_join(th->th.th_info.ds.ds_thread, &exit_val);\n#ifdef KMP_DEBUG\n  /* Don't expose these to the user until we understand when they trigger */\n  if (status != 0) {\n    __kmp_fatal(KMP_MSG(ReapWorkerError), KMP_ERR(status), __kmp_msg_null);\n  }\n  if (exit_val != th) {\n    KA_TRACE(10, (\"__kmp_reap_worker: worker T#%d did not reap properly, \"\n                  \"exit_val = %p\\n\",\n                  th->th.th_info.ds.ds_gtid, exit_val));\n  }\n#endif /* KMP_DEBUG */\n\n  KA_TRACE(10, (\"__kmp_reap_worker: done reaping T#%d\\n\",\n                th->th.th_info.ds.ds_gtid));\n\n  KMP_MB(); /* Flush all pending memory write invalidates.  */\n}\n\n#if KMP_HANDLE_SIGNALS\n\nstatic void __kmp_null_handler(int signo) {\n  //  Do nothing, for doing SIG_IGN-type actions.\n} // __kmp_null_handler\n\nstatic void __kmp_team_handler(int signo) {\n  if (__kmp_global.g.g_abort == 0) {\n/* Stage 1 signal handler, let's shut down all of the threads */\n#ifdef KMP_DEBUG\n    __kmp_debug_printf(\"__kmp_team_handler: caught signal = %d\\n\", signo);\n#endif\n    switch (signo) {\n    case SIGHUP:\n    case SIGINT:\n    case SIGQUIT:\n    case SIGILL:\n    case SIGABRT:\n    case SIGFPE:\n    case SIGBUS:\n    case SIGSEGV:\n#ifdef SIGSYS\n    case SIGSYS:\n#endif\n    case SIGTERM:\n      if (__kmp_debug_buf) {\n        __kmp_dump_debug_buffer();\n      }\n      KMP_MB(); // Flush all pending memory write invalidates.\n      TCW_4(__kmp_global.g.g_abort, signo);\n      KMP_MB(); // Flush all pending memory write invalidates.\n      TCW_4(__kmp_global.g.g_done, TRUE);\n      KMP_MB(); // Flush all pending memory write invalidates.\n      break;\n    default:\n#ifdef KMP_DEBUG\n      __kmp_debug_printf(\"__kmp_team_handler: unknown signal type\");\n#endif\n      break;\n    }\n  }\n} // __kmp_team_handler\n\nstatic void __kmp_sigaction(int signum, const struct sigaction *act,\n                            struct sigaction *oldact) {\n  int rc = sigaction(signum, act, oldact);\n  KMP_CHECK_SYSFAIL_ERRNO(\"sigaction\", rc);\n}\n\nstatic void __kmp_install_one_handler(int sig, sig_func_t handler_func,\n                                      int parallel_init) {\n  KMP_MB(); // Flush all pending memory write invalidates.\n  KB_TRACE(60,\n           (\"__kmp_install_one_handler( %d, ..., %d )\\n\", sig, parallel_init));\n  if (parallel_init) {\n    struct sigaction new_action;\n    struct sigaction old_action;\n    new_action.sa_handler = handler_func;\n    new_action.sa_flags = 0;\n    sigfillset(&new_action.sa_mask);\n    __kmp_sigaction(sig, &new_action, &old_action);\n    if (old_action.sa_handler == __kmp_sighldrs[sig].sa_handler) {\n      sigaddset(&__kmp_sigset, sig);\n    } else {\n      // Restore/keep user's handler if one previously installed.\n      __kmp_sigaction(sig, &old_action, NULL);\n    }\n  } else {\n    // Save initial/system signal handlers to see if user handlers installed.\n    __kmp_sigaction(sig, NULL, &__kmp_sighldrs[sig]);\n  }\n  KMP_MB(); // Flush all pending memory write invalidates.\n} // __kmp_install_one_handler\n\nstatic void __kmp_remove_one_handler(int sig) {\n  KB_TRACE(60, (\"__kmp_remove_one_handler( %d )\\n\", sig));\n  if (sigismember(&__kmp_sigset, sig)) {\n    struct sigaction old;\n    KMP_MB(); // Flush all pending memory write invalidates.\n    __kmp_sigaction(sig, &__kmp_sighldrs[sig], &old);\n    if ((old.sa_handler != __kmp_team_handler) &&\n        (old.sa_handler != __kmp_null_handler)) {\n      // Restore the users signal handler.\n      KB_TRACE(10, (\"__kmp_remove_one_handler: oops, not our handler, \"\n                    \"restoring: sig=%d\\n\",\n                    sig));\n      __kmp_sigaction(sig, &old, NULL);\n    }\n    sigdelset(&__kmp_sigset, sig);\n    KMP_MB(); // Flush all pending memory write invalidates.\n  }\n} // __kmp_remove_one_handler\n\nvoid __kmp_install_signals(int parallel_init) {\n  KB_TRACE(10, (\"__kmp_install_signals( %d )\\n\", parallel_init));\n  if (__kmp_handle_signals || !parallel_init) {\n    // If ! parallel_init, we do not install handlers, just save original\n    // handlers. Let us do it even __handle_signals is 0.\n    sigemptyset(&__kmp_sigset);\n    __kmp_install_one_handler(SIGHUP, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGINT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGQUIT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGILL, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGABRT, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGFPE, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGBUS, __kmp_team_handler, parallel_init);\n    __kmp_install_one_handler(SIGSEGV, __kmp_team_handler, parallel_init);\n#ifdef SIGSYS\n    __kmp_install_one_handler(SIGSYS, __kmp_team_handler, parallel_init);\n#endif // SIGSYS\n    __kmp_install_one_handler(SIGTERM, __kmp_team_handler, parallel_init);\n#ifdef SIGPIPE\n    __kmp_install_one_handler(SIGPIPE, __kmp_team_handler, parallel_init);\n#endif // SIGPIPE\n  }\n} // __kmp_install_signals\n\nvoid __kmp_remove_signals(void) {\n  int sig;\n  KB_TRACE(10, (\"__kmp_remove_signals()\\n\"));\n  for (sig = 1; sig < NSIG; ++sig) {\n    __kmp_remove_one_handler(sig);\n  }\n} // __kmp_remove_signals\n\n#endif // KMP_HANDLE_SIGNALS\n\nvoid __kmp_enable(int new_state) {\n#ifdef KMP_CANCEL_THREADS\n  int status, old_state;\n  status = pthread_setcancelstate(new_state, &old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n  KMP_DEBUG_ASSERT(old_state == PTHREAD_CANCEL_DISABLE);\n#endif\n}\n\nvoid __kmp_disable(int *old_state) {\n#ifdef KMP_CANCEL_THREADS\n  int status;\n  status = pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, old_state);\n  KMP_CHECK_SYSFAIL(\"pthread_setcancelstate\", status);\n#endif\n}\n\nstatic void __kmp_atfork_prepare(void) { /*  nothing to do  */\n}\n\nstatic void __kmp_atfork_parent(void) { /*  nothing to do  */\n}\n\n/* Reset the library so execution in the child starts \"all over again\" with\n   clean data structures in initial states.  Don't worry about freeing memory\n   allocated by parent, just abandon it to be safe. */\nstatic void __kmp_atfork_child(void) {\n  /* TODO make sure this is done right for nested/sibling */\n  // ATT:  Memory leaks are here? TODO: Check it and fix.\n  /* KMP_ASSERT( 0 ); */\n\n  ++__kmp_fork_count;\n\n#if KMP_AFFINITY_SUPPORTED\n#if KMP_OS_LINUX\n  // reset the affinity in the child to the initial thread\n  // affinity in the parent\n  kmp_set_thread_affinity_mask_initial();\n#endif\n  // Set default not to bind threads tightly in the child (we\u2019re expecting\n  // over-subscription after the fork and this can improve things for\n  // scripting languages that use OpenMP inside process-parallel code).\n  __kmp_affinity_type = affinity_none;\n#if OMP_40_ENABLED\n  if (__kmp_nested_proc_bind.bind_types != NULL) {\n    __kmp_nested_proc_bind.bind_types[0] = proc_bind_false;\n  }\n#endif // OMP_40_ENABLED\n#endif // KMP_AFFINITY_SUPPORTED\n\n  __kmp_init_runtime = FALSE;\n#if KMP_USE_MONITOR\n  __kmp_init_monitor = 0;\n#endif\n  __kmp_init_parallel = FALSE;\n  __kmp_init_middle = FALSE;\n  __kmp_init_serial = FALSE;\n  TCW_4(__kmp_init_gtid, FALSE);\n  __kmp_init_common = FALSE;\n\n  TCW_4(__kmp_init_user_locks, FALSE);\n#if !KMP_USE_DYNAMIC_LOCK\n  __kmp_user_lock_table.used = 1;\n  __kmp_user_lock_table.allocated = 0;\n  __kmp_user_lock_table.table = NULL;\n  __kmp_lock_blocks = NULL;\n#endif\n\n  __kmp_all_nth = 0;\n  TCW_4(__kmp_nth, 0);\n\n  /* Must actually zero all the *cache arguments passed to __kmpc_threadprivate\n     here so threadprivate doesn't use stale data */\n  KA_TRACE(10, (\"__kmp_atfork_child: checking cache address list %p\\n\",\n                __kmp_threadpriv_cache_list));\n\n  while (__kmp_threadpriv_cache_list != NULL) {\n\n    if (*__kmp_threadpriv_cache_list->addr != NULL) {\n      KC_TRACE(50, (\"__kmp_atfork_child: zeroing cache at address %p\\n\",\n                    &(*__kmp_threadpriv_cache_list->addr)));\n\n      *__kmp_threadpriv_cache_list->addr = NULL;\n    }\n    __kmp_threadpriv_cache_list = __kmp_threadpriv_cache_list->next;\n  }\n\n  __kmp_init_runtime = FALSE;\n\n  /* reset statically initialized locks */\n  __kmp_init_bootstrap_lock(&__kmp_initz_lock);\n  __kmp_init_bootstrap_lock(&__kmp_stdio_lock);\n  __kmp_init_bootstrap_lock(&__kmp_console_lock);\n\n  /* This is necessary to make sure no stale data is left around */\n  /* AC: customers complain that we use unsafe routines in the atfork\n     handler. Mathworks: dlsym() is unsafe. We call dlsym and dlopen\n     in dynamic_link when check the presence of shared tbbmalloc library.\n     Suggestion is to make the library initialization lazier, similar\n     to what done for __kmpc_begin(). */\n  // TODO: synchronize all static initializations with regular library\n  //       startup; look at kmp_global.cpp and etc.\n  //__kmp_internal_begin ();\n}\n\nvoid __kmp_register_atfork(void) {\n  if (__kmp_need_register_atfork) {\n    int status = pthread_atfork(__kmp_atfork_prepare, __kmp_atfork_parent,\n                                __kmp_atfork_child);\n    KMP_CHECK_SYSFAIL(\"pthread_atfork\", status);\n    __kmp_need_register_atfork = FALSE;\n  }\n}\n\nvoid __kmp_suspend_initialize(void) {\n  int status;\n  status = pthread_mutexattr_init(&__kmp_suspend_mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutexattr_init\", status);\n  status = pthread_condattr_init(&__kmp_suspend_cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_condattr_init\", status);\n}\n\nstatic void __kmp_suspend_initialize_thread(kmp_info_t *th) {\n  ANNOTATE_HAPPENS_AFTER(&th->th.th_suspend_init_count);\n  if (th->th.th_suspend_init_count <= __kmp_fork_count) {\n    /* this means we haven't initialized the suspension pthread objects for this\n       thread in this instance of the process */\n    int status;\n    status = pthread_cond_init(&th->th.th_suspend_cv.c_cond,\n                               &__kmp_suspend_cond_attr);\n    KMP_CHECK_SYSFAIL(\"pthread_cond_init\", status);\n    status = pthread_mutex_init(&th->th.th_suspend_mx.m_mutex,\n                                &__kmp_suspend_mutex_attr);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_init\", status);\n    *(volatile int *)&th->th.th_suspend_init_count = __kmp_fork_count + 1;\n    ANNOTATE_HAPPENS_BEFORE(&th->th.th_suspend_init_count);\n  }\n}\n\nvoid __kmp_suspend_uninitialize_thread(kmp_info_t *th) {\n  if (th->th.th_suspend_init_count > __kmp_fork_count) {\n    /* this means we have initialize the suspension pthread objects for this\n       thread in this instance of the process */\n    int status;\n\n    status = pthread_cond_destroy(&th->th.th_suspend_cv.c_cond);\n    if (status != 0 && status != EBUSY) {\n      KMP_SYSFAIL(\"pthread_cond_destroy\", status);\n    }\n    status = pthread_mutex_destroy(&th->th.th_suspend_mx.m_mutex);\n    if (status != 0 && status != EBUSY) {\n      KMP_SYSFAIL(\"pthread_mutex_destroy\", status);\n    }\n    --th->th.th_suspend_init_count;\n    KMP_DEBUG_ASSERT(th->th.th_suspend_init_count == __kmp_fork_count);\n  }\n}\n\n/* This routine puts the calling thread to sleep after setting the\n   sleep bit for the indicated flag variable to true. */\ntemplate <class C>\nstatic inline void __kmp_suspend_template(int th_gtid, C *flag) {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_suspend);\n  kmp_info_t *th = __kmp_threads[th_gtid];\n  int status;\n  typename C::flag_t old_spin;\n\n  KF_TRACE(30, (\"__kmp_suspend_template: T#%d enter for flag = %p\\n\", th_gtid,\n                flag->get()));\n\n  __kmp_suspend_initialize_thread(th);\n\n  status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n\n  KF_TRACE(10, (\"__kmp_suspend_template: T#%d setting sleep bit for spin(%p)\\n\",\n                th_gtid, flag->get()));\n\n  /* TODO: shouldn't this use release semantics to ensure that\n     __kmp_suspend_initialize_thread gets called first? */\n  old_spin = flag->set_sleeping();\n\n  KF_TRACE(5, (\"__kmp_suspend_template: T#%d set sleep bit for spin(%p)==%x,\"\n               \" was %x\\n\",\n               th_gtid, flag->get(), *(flag->get()), old_spin));\n\n  if (flag->done_check_val(old_spin)) {\n    old_spin = flag->unset_sleeping();\n    KF_TRACE(5, (\"__kmp_suspend_template: T#%d false alarm, reset sleep bit \"\n                 \"for spin(%p)\\n\",\n                 th_gtid, flag->get()));\n  } else {\n    /* Encapsulate in a loop as the documentation states that this may\n       \"with low probability\" return when the condition variable has\n       not been signaled or broadcast */\n    int deactivated = FALSE;\n    TCW_PTR(th->th.th_sleep_loc, (void *)flag);\n\n    while (flag->is_sleeping()) {\n#ifdef DEBUG_SUSPEND\n      char buffer[128];\n      __kmp_suspend_count++;\n      __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n      __kmp_printf(\"__kmp_suspend_template: suspending T#%d: %s\\n\", th_gtid,\n                   buffer);\n#endif\n      // Mark the thread as no longer active (only in the first iteration of the\n      // loop).\n      if (!deactivated) {\n        th->th.th_active = FALSE;\n        if (th->th.th_active_in_pool) {\n          th->th.th_active_in_pool = FALSE;\n          KMP_TEST_THEN_DEC32(&__kmp_thread_pool_active_nth);\n          KMP_DEBUG_ASSERT(TCR_4(__kmp_thread_pool_active_nth) >= 0);\n        }\n        deactivated = TRUE;\n      }\n\n#if USE_SUSPEND_TIMEOUT\n      struct timespec now;\n      struct timeval tval;\n      int msecs;\n\n      status = gettimeofday(&tval, NULL);\n      KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n      TIMEVAL_TO_TIMESPEC(&tval, &now);\n\n      msecs = (4 * __kmp_dflt_blocktime) + 200;\n      now.tv_sec += msecs / 1000;\n      now.tv_nsec += (msecs % 1000) * 1000;\n\n      KF_TRACE(15, (\"__kmp_suspend_template: T#%d about to perform \"\n                    \"pthread_cond_timedwait\\n\",\n                    th_gtid));\n      status = pthread_cond_timedwait(&th->th.th_suspend_cv.c_cond,\n                                      &th->th.th_suspend_mx.m_mutex, &now);\n#else\n      KF_TRACE(15, (\"__kmp_suspend_template: T#%d about to perform\"\n                    \" pthread_cond_wait\\n\",\n                    th_gtid));\n      status = pthread_cond_wait(&th->th.th_suspend_cv.c_cond,\n                                 &th->th.th_suspend_mx.m_mutex);\n#endif\n\n      if ((status != 0) && (status != EINTR) && (status != ETIMEDOUT)) {\n        KMP_SYSFAIL(\"pthread_cond_wait\", status);\n      }\n#ifdef KMP_DEBUG\n      if (status == ETIMEDOUT) {\n        if (flag->is_sleeping()) {\n          KF_TRACE(100,\n                   (\"__kmp_suspend_template: T#%d timeout wakeup\\n\", th_gtid));\n        } else {\n          KF_TRACE(2, (\"__kmp_suspend_template: T#%d timeout wakeup, sleep bit \"\n                       \"not set!\\n\",\n                       th_gtid));\n        }\n      } else if (flag->is_sleeping()) {\n        KF_TRACE(100,\n                 (\"__kmp_suspend_template: T#%d spurious wakeup\\n\", th_gtid));\n      }\n#endif\n    } // while\n\n    // Mark the thread as active again (if it was previous marked as inactive)\n    if (deactivated) {\n      th->th.th_active = TRUE;\n      if (TCR_4(th->th.th_in_pool)) {\n        KMP_TEST_THEN_INC32(&__kmp_thread_pool_active_nth);\n        th->th.th_active_in_pool = TRUE;\n      }\n    }\n  }\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n    __kmp_printf(\"__kmp_suspend_template: T#%d has awakened: %s\\n\", th_gtid,\n                 buffer);\n  }\n#endif\n\n  status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_suspend_template: T#%d exit\\n\", th_gtid));\n}\n\nvoid __kmp_suspend_32(int th_gtid, kmp_flag_32 *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_64(int th_gtid, kmp_flag_64 *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\nvoid __kmp_suspend_oncore(int th_gtid, kmp_flag_oncore *flag) {\n  __kmp_suspend_template(th_gtid, flag);\n}\n\n/* This routine signals the thread specified by target_gtid to wake up\n   after setting the sleep bit indicated by the flag argument to FALSE.\n   The target thread must already have called __kmp_suspend_template() */\ntemplate <class C>\nstatic inline void __kmp_resume_template(int target_gtid, C *flag) {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_resume);\n  kmp_info_t *th = __kmp_threads[target_gtid];\n  int status;\n\n#ifdef KMP_DEBUG\n  int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n#endif\n\n  KF_TRACE(30, (\"__kmp_resume_template: T#%d wants to wakeup T#%d enter\\n\",\n                gtid, target_gtid));\n  KMP_DEBUG_ASSERT(gtid != target_gtid);\n\n  __kmp_suspend_initialize_thread(th);\n\n  status = pthread_mutex_lock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n\n  if (!flag) { // coming from __kmp_null_resume_wrapper\n    flag = (C *)CCAST(void *, th->th.th_sleep_loc);\n  }\n\n  // First, check if the flag is null or its type has changed. If so, someone\n  // else woke it up.\n  if (!flag || flag->get_type() != flag->get_ptr_type()) { // get_ptr_type\n    // simply shows what\n    // flag was cast to\n    KF_TRACE(5, (\"__kmp_resume_template: T#%d exiting, thread T#%d already \"\n                 \"awake: flag(%p)\\n\",\n                 gtid, target_gtid, NULL));\n    status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n    KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n    return;\n  } else { // if multiple threads are sleeping, flag should be internally\n    // referring to a specific thread here\n    typename C::flag_t old_spin = flag->unset_sleeping();\n    if (!flag->is_sleeping_val(old_spin)) {\n      KF_TRACE(5, (\"__kmp_resume_template: T#%d exiting, thread T#%d already \"\n                   \"awake: flag(%p): \"\n                   \"%u => %u\\n\",\n                   gtid, target_gtid, flag->get(), old_spin, *flag->get()));\n      status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n      KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n      return;\n    }\n    KF_TRACE(5, (\"__kmp_resume_template: T#%d about to wakeup T#%d, reset \"\n                 \"sleep bit for flag's loc(%p): \"\n                 \"%u => %u\\n\",\n                 gtid, target_gtid, flag->get(), old_spin, *flag->get()));\n  }\n  TCW_PTR(th->th.th_sleep_loc, NULL);\n\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &th->th.th_suspend_cv);\n    __kmp_printf(\"__kmp_resume_template: T#%d resuming T#%d: %s\\n\", gtid,\n                 target_gtid, buffer);\n  }\n#endif\n  status = pthread_cond_signal(&th->th.th_suspend_cv.c_cond);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_signal\", status);\n  status = pthread_mutex_unlock(&th->th.th_suspend_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_resume_template: T#%d exiting after signaling wake up\"\n                \" for T#%d\\n\",\n                gtid, target_gtid));\n}\n\nvoid __kmp_resume_32(int target_gtid, kmp_flag_32 *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_64(int target_gtid, kmp_flag_64 *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\nvoid __kmp_resume_oncore(int target_gtid, kmp_flag_oncore *flag) {\n  __kmp_resume_template(target_gtid, flag);\n}\n\n#if KMP_USE_MONITOR\nvoid __kmp_resume_monitor() {\n  KMP_TIME_DEVELOPER_PARTITIONED_BLOCK(USER_resume);\n  int status;\n#ifdef KMP_DEBUG\n  int gtid = TCR_4(__kmp_init_gtid) ? __kmp_get_gtid() : -1;\n  KF_TRACE(30, (\"__kmp_resume_monitor: T#%d wants to wakeup T#%d enter\\n\", gtid,\n                KMP_GTID_MONITOR));\n  KMP_DEBUG_ASSERT(gtid != KMP_GTID_MONITOR);\n#endif\n  status = pthread_mutex_lock(&__kmp_wait_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_lock\", status);\n#ifdef DEBUG_SUSPEND\n  {\n    char buffer[128];\n    __kmp_print_cond(buffer, &__kmp_wait_cv.c_cond);\n    __kmp_printf(\"__kmp_resume_monitor: T#%d resuming T#%d: %s\\n\", gtid,\n                 KMP_GTID_MONITOR, buffer);\n  }\n#endif\n  status = pthread_cond_signal(&__kmp_wait_cv.c_cond);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_signal\", status);\n  status = pthread_mutex_unlock(&__kmp_wait_mx.m_mutex);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_unlock\", status);\n  KF_TRACE(30, (\"__kmp_resume_monitor: T#%d exiting after signaling wake up\"\n                \" for T#%d\\n\",\n                gtid, KMP_GTID_MONITOR));\n}\n#endif // KMP_USE_MONITOR\n\nvoid __kmp_yield(int cond) {\n  if (!cond)\n    return;\n#if KMP_USE_MONITOR\n  if (!__kmp_yielding_on)\n    return;\n#else\n  if (__kmp_yield_cycle && !KMP_YIELD_NOW())\n    return;\n#endif\n  sched_yield();\n}\n\nvoid __kmp_gtid_set_specific(int gtid) {\n  if (__kmp_init_gtid) {\n    int status;\n    status = pthread_setspecific(__kmp_gtid_threadprivate_key,\n                                 (void *)(intptr_t)(gtid + 1));\n    KMP_CHECK_SYSFAIL(\"pthread_setspecific\", status);\n  } else {\n    KA_TRACE(50, (\"__kmp_gtid_set_specific: runtime shutdown, returning\\n\"));\n  }\n}\n\nint __kmp_gtid_get_specific() {\n  int gtid;\n  if (!__kmp_init_gtid) {\n    KA_TRACE(50, (\"__kmp_gtid_get_specific: runtime shutdown, returning \"\n                  \"KMP_GTID_SHUTDOWN\\n\"));\n    return KMP_GTID_SHUTDOWN;\n  }\n  gtid = (int)(size_t)pthread_getspecific(__kmp_gtid_threadprivate_key);\n  if (gtid == 0) {\n    gtid = KMP_GTID_DNE;\n  } else {\n    gtid--;\n  }\n  KA_TRACE(50, (\"__kmp_gtid_get_specific: key:%d gtid:%d\\n\",\n                __kmp_gtid_threadprivate_key, gtid));\n  return gtid;\n}\n\ndouble __kmp_read_cpu_time(void) {\n  /*clock_t   t;*/\n  struct tms buffer;\n\n  /*t =*/times(&buffer);\n\n  return (buffer.tms_utime + buffer.tms_cutime) / (double)CLOCKS_PER_SEC;\n}\n\nint __kmp_read_system_info(struct kmp_sys_info *info) {\n  int status;\n  struct rusage r_usage;\n\n  memset(info, 0, sizeof(*info));\n\n  status = getrusage(RUSAGE_SELF, &r_usage);\n  KMP_CHECK_SYSFAIL_ERRNO(\"getrusage\", status);\n\n  // The maximum resident set size utilized (in kilobytes)\n  info->maxrss = r_usage.ru_maxrss;\n  // The number of page faults serviced without any I/O\n  info->minflt = r_usage.ru_minflt;\n  // The number of page faults serviced that required I/O\n  info->majflt = r_usage.ru_majflt;\n  // The number of times a process was \"swapped\" out of memory\n  info->nswap = r_usage.ru_nswap;\n  // The number of times the file system had to perform input\n  info->inblock = r_usage.ru_inblock;\n  // The number of times the file system had to perform output\n  info->oublock = r_usage.ru_oublock;\n  // The number of times a context switch was voluntarily\n  info->nvcsw = r_usage.ru_nvcsw;\n  // The number of times a context switch was forced\n  info->nivcsw = r_usage.ru_nivcsw;\n\n  return (status != 0);\n}\n\nvoid __kmp_read_system_time(double *delta) {\n  double t_ns;\n  struct timeval tval;\n  struct timespec stop;\n  int status;\n\n  status = gettimeofday(&tval, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  TIMEVAL_TO_TIMESPEC(&tval, &stop);\n  t_ns = TS2NS(stop) - TS2NS(__kmp_sys_timer_data.start);\n  *delta = (t_ns * 1e-9);\n}\n\nvoid __kmp_clear_system_time(void) {\n  struct timeval tval;\n  int status;\n  status = gettimeofday(&tval, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  TIMEVAL_TO_TIMESPEC(&tval, &__kmp_sys_timer_data.start);\n}\n\nstatic int __kmp_get_xproc(void) {\n\n  int r = 0;\n\n#if KMP_OS_LINUX || KMP_OS_FREEBSD || KMP_OS_NETBSD\n\n  r = sysconf(_SC_NPROCESSORS_ONLN);\n\n#elif KMP_OS_DARWIN\n\n  // Bug C77011 High \"OpenMP Threads and number of active cores\".\n\n  // Find the number of available CPUs.\n  kern_return_t rc;\n  host_basic_info_data_t info;\n  mach_msg_type_number_t num = HOST_BASIC_INFO_COUNT;\n  rc = host_info(mach_host_self(), HOST_BASIC_INFO, (host_info_t)&info, &num);\n  if (rc == 0 && num == HOST_BASIC_INFO_COUNT) {\n    // Cannot use KA_TRACE() here because this code works before trace support\n    // is initialized.\n    r = info.avail_cpus;\n  } else {\n    KMP_WARNING(CantGetNumAvailCPU);\n    KMP_INFORM(AssumedNumCPU);\n  }\n\n#else\n\n#error \"Unknown or unsupported OS.\"\n\n#endif\n\n  return r > 0 ? r : 2; /* guess value of 2 if OS told us 0 */\n\n} // __kmp_get_xproc\n\nint __kmp_read_from_file(char const *path, char const *format, ...) {\n  int result;\n  va_list args;\n\n  va_start(args, format);\n  FILE *f = fopen(path, \"rb\");\n  if (f == NULL)\n    return 0;\n  result = vfscanf(f, format, args);\n  fclose(f);\n\n  return result;\n}\n\nvoid __kmp_runtime_initialize(void) {\n  int status;\n  pthread_mutexattr_t mutex_attr;\n  pthread_condattr_t cond_attr;\n\n  if (__kmp_init_runtime) {\n    return;\n  }\n\n#if (KMP_ARCH_X86 || KMP_ARCH_X86_64)\n  if (!__kmp_cpuinfo.initialized) {\n    __kmp_query_cpuid(&__kmp_cpuinfo);\n  }\n#endif /* KMP_ARCH_X86 || KMP_ARCH_X86_64 */\n\n  __kmp_xproc = __kmp_get_xproc();\n\n  if (sysconf(_SC_THREADS)) {\n\n    /* Query the maximum number of threads */\n    __kmp_sys_max_nth = sysconf(_SC_THREAD_THREADS_MAX);\n    if (__kmp_sys_max_nth == -1) {\n      /* Unlimited threads for NPTL */\n      __kmp_sys_max_nth = INT_MAX;\n    } else if (__kmp_sys_max_nth <= 1) {\n      /* Can't tell, just use PTHREAD_THREADS_MAX */\n      __kmp_sys_max_nth = KMP_MAX_NTH;\n    }\n\n    /* Query the minimum stack size */\n    __kmp_sys_min_stksize = sysconf(_SC_THREAD_STACK_MIN);\n    if (__kmp_sys_min_stksize <= 1) {\n      __kmp_sys_min_stksize = KMP_MIN_STKSIZE;\n    }\n  }\n\n  /* Set up minimum number of threads to switch to TLS gtid */\n  __kmp_tls_gtid_min = KMP_TLS_GTID_MIN;\n\n  status = pthread_key_create(&__kmp_gtid_threadprivate_key,\n                              __kmp_internal_end_dest);\n  KMP_CHECK_SYSFAIL(\"pthread_key_create\", status);\n  status = pthread_mutexattr_init(&mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutexattr_init\", status);\n  status = pthread_mutex_init(&__kmp_wait_mx.m_mutex, &mutex_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_mutex_init\", status);\n  status = pthread_condattr_init(&cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_condattr_init\", status);\n  status = pthread_cond_init(&__kmp_wait_cv.c_cond, &cond_attr);\n  KMP_CHECK_SYSFAIL(\"pthread_cond_init\", status);\n#if USE_ITT_BUILD\n  __kmp_itt_initialize();\n#endif /* USE_ITT_BUILD */\n\n  __kmp_init_runtime = TRUE;\n}\n\nvoid __kmp_runtime_destroy(void) {\n  int status;\n\n  if (!__kmp_init_runtime) {\n    return; // Nothing to do.\n  }\n\n#if USE_ITT_BUILD\n  __kmp_itt_destroy();\n#endif /* USE_ITT_BUILD */\n\n  status = pthread_key_delete(__kmp_gtid_threadprivate_key);\n  KMP_CHECK_SYSFAIL(\"pthread_key_delete\", status);\n\n  status = pthread_mutex_destroy(&__kmp_wait_mx.m_mutex);\n  if (status != 0 && status != EBUSY) {\n    KMP_SYSFAIL(\"pthread_mutex_destroy\", status);\n  }\n  status = pthread_cond_destroy(&__kmp_wait_cv.c_cond);\n  if (status != 0 && status != EBUSY) {\n    KMP_SYSFAIL(\"pthread_cond_destroy\", status);\n  }\n#if KMP_AFFINITY_SUPPORTED\n  __kmp_affinity_uninitialize();\n#endif\n\n  __kmp_init_runtime = FALSE;\n}\n\n/* Put the thread to sleep for a time period */\n/* NOTE: not currently used anywhere */\nvoid __kmp_thread_sleep(int millis) { sleep((millis + 500) / 1000); }\n\n/* Calculate the elapsed wall clock time for the user */\nvoid __kmp_elapsed(double *t) {\n  int status;\n#ifdef FIX_SGI_CLOCK\n  struct timespec ts;\n\n  status = clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &ts);\n  KMP_CHECK_SYSFAIL_ERRNO(\"clock_gettime\", status);\n  *t =\n      (double)ts.tv_nsec * (1.0 / (double)KMP_NSEC_PER_SEC) + (double)ts.tv_sec;\n#else\n  struct timeval tv;\n\n  status = gettimeofday(&tv, NULL);\n  KMP_CHECK_SYSFAIL_ERRNO(\"gettimeofday\", status);\n  *t =\n      (double)tv.tv_usec * (1.0 / (double)KMP_USEC_PER_SEC) + (double)tv.tv_sec;\n#endif\n}\n\n/* Calculate the elapsed wall clock tick for the user */\nvoid __kmp_elapsed_tick(double *t) { *t = 1 / (double)CLOCKS_PER_SEC; }\n\n/* Return the current time stamp in nsec */\nkmp_uint64 __kmp_now_nsec() {\n  struct timeval t;\n  gettimeofday(&t, NULL);\n  return KMP_NSEC_PER_SEC * t.tv_sec + 1000 * t.tv_usec;\n}\n\n#if KMP_ARCH_X86 || KMP_ARCH_X86_64\n/* Measure clock ticks per millisecond */\nvoid __kmp_initialize_system_tick() {\n  kmp_uint64 delay = 100000; // 50~100 usec on most machines.\n  kmp_uint64 nsec = __kmp_now_nsec();\n  kmp_uint64 goal = __kmp_hardware_timestamp() + delay;\n  kmp_uint64 now;\n  while ((now = __kmp_hardware_timestamp()) < goal)\n    ;\n  __kmp_ticks_per_msec =\n      (kmp_uint64)(1e6 * (delay + (now - goal)) / (__kmp_now_nsec() - nsec));\n}\n#endif\n\n/* Determine whether the given address is mapped into the current address\n   space. */\n\nint __kmp_is_address_mapped(void *addr) {\n\n  int found = 0;\n  int rc;\n\n#if KMP_OS_LINUX || KMP_OS_FREEBSD\n\n  /* On Linux* OS, read the /proc/<pid>/maps pseudo-file to get all the address\n     ranges mapped into the address space. */\n\n  char *name = __kmp_str_format(\"/proc/%d/maps\", getpid());\n  FILE *file = NULL;\n\n  file = fopen(name, \"r\");\n  KMP_ASSERT(file != NULL);\n\n  for (;;) {\n\n    void *beginning = NULL;\n    void *ending = NULL;\n    char perms[5];\n\n    rc = fscanf(file, \"%p-%p %4s %*[^\\n]\\n\", &beginning, &ending, perms);\n    if (rc == EOF) {\n      break;\n    }\n    KMP_ASSERT(rc == 3 &&\n               KMP_STRLEN(perms) == 4); // Make sure all fields are read.\n\n    // Ending address is not included in the region, but beginning is.\n    if ((addr >= beginning) && (addr < ending)) {\n      perms[2] = 0; // 3th and 4th character does not matter.\n      if (strcmp(perms, \"rw\") == 0) {\n        // Memory we are looking for should be readable and writable.\n        found = 1;\n      }\n      break;\n    }\n  }\n\n  // Free resources.\n  fclose(file);\n  KMP_INTERNAL_FREE(name);\n\n#elif KMP_OS_DARWIN\n\n  /* On OS X*, /proc pseudo filesystem is not available. Try to read memory\n     using vm interface. */\n\n  int buffer;\n  vm_size_t count;\n  rc = vm_read_overwrite(\n      mach_task_self(), // Task to read memory of.\n      (vm_address_t)(addr), // Address to read from.\n      1, // Number of bytes to be read.\n      (vm_address_t)(&buffer), // Address of buffer to save read bytes in.\n      &count // Address of var to save number of read bytes in.\n      );\n  if (rc == 0) {\n    // Memory successfully read.\n    found = 1;\n  }\n\n#elif KMP_OS_FREEBSD || KMP_OS_NETBSD\n\n  // FIXME(FreeBSD, NetBSD): Implement this\n  found = 1;\n\n#else\n\n#error \"Unknown or unsupported OS\"\n\n#endif\n\n  return found;\n\n} // __kmp_is_address_mapped\n\n#ifdef USE_LOAD_BALANCE\n\n#if KMP_OS_DARWIN\n\n// The function returns the rounded value of the system load average\n// during given time interval which depends on the value of\n// __kmp_load_balance_interval variable (default is 60 sec, other values\n// may be 300 sec or 900 sec).\n// It returns -1 in case of error.\nint __kmp_get_load_balance(int max) {\n  double averages[3];\n  int ret_avg = 0;\n\n  int res = getloadavg(averages, 3);\n\n  // Check __kmp_load_balance_interval to determine which of averages to use.\n  // getloadavg() may return the number of samples less than requested that is\n  // less than 3.\n  if (__kmp_load_balance_interval < 180 && (res >= 1)) {\n    ret_avg = averages[0]; // 1 min\n  } else if ((__kmp_load_balance_interval >= 180 &&\n              __kmp_load_balance_interval < 600) &&\n             (res >= 2)) {\n    ret_avg = averages[1]; // 5 min\n  } else if ((__kmp_load_balance_interval >= 600) && (res == 3)) {\n    ret_avg = averages[2]; // 15 min\n  } else { // Error occurred\n    return -1;\n  }\n\n  return ret_avg;\n}\n\n#else // Linux* OS\n\n// The fuction returns number of running (not sleeping) threads, or -1 in case\n// of error. Error could be reported if Linux* OS kernel too old (without\n// \"/proc\" support). Counting running threads stops if max running threads\n// encountered.\nint __kmp_get_load_balance(int max) {\n  static int permanent_error = 0;\n  static int glb_running_threads = 0; // Saved count of the running threads for\n  // the thread balance algortihm\n  static double glb_call_time = 0; /* Thread balance algorithm call time */\n\n  int running_threads = 0; // Number of running threads in the system.\n\n  DIR *proc_dir = NULL; // Handle of \"/proc/\" directory.\n  struct dirent *proc_entry = NULL;\n\n  kmp_str_buf_t task_path; // \"/proc/<pid>/task/<tid>/\" path.\n  DIR *task_dir = NULL; // Handle of \"/proc/<pid>/task/<tid>/\" directory.\n  struct dirent *task_entry = NULL;\n  int task_path_fixed_len;\n\n  kmp_str_buf_t stat_path; // \"/proc/<pid>/task/<tid>/stat\" path.\n  int stat_file = -1;\n  int stat_path_fixed_len;\n\n  int total_processes = 0; // Total number of processes in system.\n  int total_threads = 0; // Total number of threads in system.\n\n  double call_time = 0.0;\n\n  __kmp_str_buf_init(&task_path);\n  __kmp_str_buf_init(&stat_path);\n\n  __kmp_elapsed(&call_time);\n\n  if (glb_call_time &&\n      (call_time - glb_call_time < __kmp_load_balance_interval)) {\n    running_threads = glb_running_threads;\n    goto finish;\n  }\n\n  glb_call_time = call_time;\n\n  // Do not spend time on scanning \"/proc/\" if we have a permanent error.\n  if (permanent_error) {\n    running_threads = -1;\n    goto finish;\n  }\n\n  if (max <= 0) {\n    max = INT_MAX;\n  }\n\n  // Open \"/proc/\" directory.\n  proc_dir = opendir(\"/proc\");\n  if (proc_dir == NULL) {\n    // Cannot open \"/prroc/\". Probably the kernel does not support it. Return an\n    // error now and in subsequent calls.\n    running_threads = -1;\n    permanent_error = 1;\n    goto finish;\n  }\n\n  // Initialize fixed part of task_path. This part will not change.\n  __kmp_str_buf_cat(&task_path, \"/proc/\", 6);\n  task_path_fixed_len = task_path.used; // Remember number of used characters.\n\n  proc_entry = readdir(proc_dir);\n  while (proc_entry != NULL) {\n    // Proc entry is a directory and name starts with a digit. Assume it is a\n    // process' directory.\n    if (proc_entry->d_type == DT_DIR && isdigit(proc_entry->d_name[0])) {\n\n      ++total_processes;\n      // Make sure init process is the very first in \"/proc\", so we can replace\n      // strcmp( proc_entry->d_name, \"1\" ) == 0 with simpler total_processes ==\n      // 1. We are going to check that total_processes == 1 => d_name == \"1\" is\n      // true (where \"=>\" is implication). Since C++ does not have => operator,\n      // let us replace it with its equivalent: a => b == ! a || b.\n      KMP_DEBUG_ASSERT(total_processes != 1 ||\n                       strcmp(proc_entry->d_name, \"1\") == 0);\n\n      // Construct task_path.\n      task_path.used = task_path_fixed_len; // Reset task_path to \"/proc/\".\n      __kmp_str_buf_cat(&task_path, proc_entry->d_name,\n                        KMP_STRLEN(proc_entry->d_name));\n      __kmp_str_buf_cat(&task_path, \"/task\", 5);\n\n      task_dir = opendir(task_path.str);\n      if (task_dir == NULL) {\n        // Process can finish between reading \"/proc/\" directory entry and\n        // opening process' \"task/\" directory. So, in general case we should not\n        // complain, but have to skip this process and read the next one. But on\n        // systems with no \"task/\" support we will spend lot of time to scan\n        // \"/proc/\" tree again and again without any benefit. \"init\" process\n        // (its pid is 1) should exist always, so, if we cannot open\n        // \"/proc/1/task/\" directory, it means \"task/\" is not supported by\n        // kernel. Report an error now and in the future.\n        if (strcmp(proc_entry->d_name, \"1\") == 0) {\n          running_threads = -1;\n          permanent_error = 1;\n          goto finish;\n        }\n      } else {\n        // Construct fixed part of stat file path.\n        __kmp_str_buf_clear(&stat_path);\n        __kmp_str_buf_cat(&stat_path, task_path.str, task_path.used);\n        __kmp_str_buf_cat(&stat_path, \"/\", 1);\n        stat_path_fixed_len = stat_path.used;\n\n        task_entry = readdir(task_dir);\n        while (task_entry != NULL) {\n          // It is a directory and name starts with a digit.\n          if (proc_entry->d_type == DT_DIR && isdigit(task_entry->d_name[0])) {\n            ++total_threads;\n\n            // Consruct complete stat file path. Easiest way would be:\n            //  __kmp_str_buf_print( & stat_path, \"%s/%s/stat\", task_path.str,\n            //  task_entry->d_name );\n            // but seriae of __kmp_str_buf_cat works a bit faster.\n            stat_path.used =\n                stat_path_fixed_len; // Reset stat path to its fixed part.\n            __kmp_str_buf_cat(&stat_path, task_entry->d_name,\n                              KMP_STRLEN(task_entry->d_name));\n            __kmp_str_buf_cat(&stat_path, \"/stat\", 5);\n\n            // Note: Low-level API (open/read/close) is used. High-level API\n            // (fopen/fclose)  works ~ 30 % slower.\n            stat_file = open(stat_path.str, O_RDONLY);\n            if (stat_file == -1) {\n              // We cannot report an error because task (thread) can terminate\n              // just before reading this file.\n            } else {\n              /* Content of \"stat\" file looks like:\n                 24285 (program) S ...\n\n                 It is a single line (if program name does not include funny\n                 symbols). First number is a thread id, then name of executable\n                 file name in paretheses, then state of the thread. We need just\n                 thread state.\n\n                 Good news: Length of program name is 15 characters max. Longer\n                 names are truncated.\n\n                 Thus, we need rather short buffer: 15 chars for program name +\n                 2 parenthesis, + 3 spaces + ~7 digits of pid = 37.\n\n                 Bad news: Program name may contain special symbols like space,\n                 closing parenthesis, or even new line. This makes parsing\n                 \"stat\" file not 100 % reliable. In case of fanny program names\n                 parsing may fail (report incorrect thread state).\n\n                 Parsing \"status\" file looks more promissing (due to different\n                 file structure and escaping special symbols) but reading and\n                 parsing of \"status\" file works slower.\n                  -- ln\n              */\n              char buffer[65];\n              int len;\n              len = read(stat_file, buffer, sizeof(buffer) - 1);\n              if (len >= 0) {\n                buffer[len] = 0;\n                // Using scanf:\n                //     sscanf( buffer, \"%*d (%*s) %c \", & state );\n                // looks very nice, but searching for a closing parenthesis\n                // works a bit faster.\n                char *close_parent = strstr(buffer, \") \");\n                if (close_parent != NULL) {\n                  char state = *(close_parent + 2);\n                  if (state == 'R') {\n                    ++running_threads;\n                    if (running_threads >= max) {\n                      goto finish;\n                    }\n                  }\n                }\n              }\n              close(stat_file);\n              stat_file = -1;\n            }\n          }\n          task_entry = readdir(task_dir);\n        }\n        closedir(task_dir);\n        task_dir = NULL;\n      }\n    }\n    proc_entry = readdir(proc_dir);\n  }\n\n  // There _might_ be a timing hole where the thread executing this\n  // code get skipped in the load balance, and running_threads is 0.\n  // Assert in the debug builds only!!!\n  KMP_DEBUG_ASSERT(running_threads > 0);\n  if (running_threads <= 0) {\n    running_threads = 1;\n  }\n\nfinish: // Clean up and exit.\n  if (proc_dir != NULL) {\n    closedir(proc_dir);\n  }\n  __kmp_str_buf_free(&task_path);\n  if (task_dir != NULL) {\n    closedir(task_dir);\n  }\n  __kmp_str_buf_free(&stat_path);\n  if (stat_file != -1) {\n    close(stat_file);\n  }\n\n  glb_running_threads = running_threads;\n\n  return running_threads;\n\n} // __kmp_get_load_balance\n\n#endif // KMP_OS_DARWIN\n\n#endif // USE_LOAD_BALANCE\n\n#if !(KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_MIC ||                            \\\n      ((KMP_OS_LINUX || KMP_OS_DARWIN) && KMP_ARCH_AARCH64) || KMP_ARCH_PPC64)\n\n// we really only need the case with 1 argument, because CLANG always build\n// a struct of pointers to shared variables referenced in the outlined function\nint __kmp_invoke_microtask(microtask_t pkfn, int gtid, int tid, int argc,\n                           void *p_argv[]\n#if OMPT_SUPPORT\n                           ,\n                           void **exit_frame_ptr\n#endif\n                           ) {\n#if OMPT_SUPPORT\n  *exit_frame_ptr = OMPT_GET_FRAME_ADDRESS(0);\n#endif\n\n  switch (argc) {\n  default:\n    fprintf(stderr, \"Too many args to microtask: %d!\\n\", argc);\n    fflush(stderr);\n    exit(-1);\n  case 0:\n    (*pkfn)(&gtid, &tid);\n    break;\n  case 1:\n    (*pkfn)(&gtid, &tid, p_argv[0]);\n    break;\n  case 2:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1]);\n    break;\n  case 3:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2]);\n    break;\n  case 4:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3]);\n    break;\n  case 5:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4]);\n    break;\n  case 6:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5]);\n    break;\n  case 7:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6]);\n    break;\n  case 8:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7]);\n    break;\n  case 9:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8]);\n    break;\n  case 10:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9]);\n    break;\n  case 11:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10]);\n    break;\n  case 12:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11]);\n    break;\n  case 13:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12]);\n    break;\n  case 14:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13]);\n    break;\n  case 15:\n    (*pkfn)(&gtid, &tid, p_argv[0], p_argv[1], p_argv[2], p_argv[3], p_argv[4],\n            p_argv[5], p_argv[6], p_argv[7], p_argv[8], p_argv[9], p_argv[10],\n            p_argv[11], p_argv[12], p_argv[13], p_argv[14]);\n    break;\n  }\n\n#if OMPT_SUPPORT\n  *exit_frame_ptr = 0;\n#endif\n\n  return 1;\n}\n\n#endif\n\n// end of file //\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/runtime/src/ompt-general.cpp": "/*****************************************************************************\n * system include files\n ****************************************************************************/\n\n#include <assert.h>\n\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#if KMP_OS_UNIX\n#include <dlfcn.h>\n#endif\n\n/*****************************************************************************\n * ompt include files\n ****************************************************************************/\n\n#include \"ompt-specific.cpp\"\n\n/*****************************************************************************\n * macros\n ****************************************************************************/\n\n#define ompt_get_callback_success 1\n#define ompt_get_callback_failure 0\n\n#define no_tool_present 0\n\n#define OMPT_API_ROUTINE static\n\n#ifndef OMPT_STR_MATCH\n#define OMPT_STR_MATCH(haystack, needle) (!strcasecmp(haystack, needle))\n#endif\n\n/*****************************************************************************\n * types\n ****************************************************************************/\n\ntypedef struct {\n  const char *state_name;\n  omp_state_t state_id;\n} omp_state_info_t;\n\ntypedef struct {\n  const char *name;\n  ompt_mutex_impl_t id;\n} ompt_mutex_impl_info_t;\n\nenum tool_setting_e {\n  omp_tool_error,\n  omp_tool_unset,\n  omp_tool_disabled,\n  omp_tool_enabled\n};\n\n/*****************************************************************************\n * global variables\n ****************************************************************************/\n\nompt_callbacks_active_t ompt_enabled;\n\nomp_state_info_t omp_state_info[] = {\n#define omp_state_macro(state, code) {#state, state},\n    FOREACH_OMP_STATE(omp_state_macro)\n#undef omp_state_macro\n};\n\nompt_mutex_impl_info_t ompt_mutex_impl_info[] = {\n#define ompt_mutex_impl_macro(name, id) {#name, name},\n    FOREACH_OMPT_MUTEX_IMPL(ompt_mutex_impl_macro)\n#undef ompt_mutex_impl_macro\n};\n\nompt_callbacks_internal_t ompt_callbacks;\n\nstatic ompt_start_tool_result_t *ompt_start_tool_result = NULL;\n\n/*****************************************************************************\n * forward declarations\n ****************************************************************************/\n\nstatic ompt_interface_fn_t ompt_fn_lookup(const char *s);\n\nOMPT_API_ROUTINE ompt_data_t *ompt_get_thread_data(void);\n\n/*****************************************************************************\n * initialization and finalization (private operations)\n ****************************************************************************/\n\ntypedef ompt_start_tool_result_t *(*ompt_start_tool_t)(unsigned int,\n                                                       const char *);\n\n#if KMP_OS_DARWIN\n\n// While Darwin supports weak symbols, the library that wishes to provide a new\n// implementation has to link against this runtime which defeats the purpose\n// of having tools that are agnostic of the underlying runtime implementation.\n//\n// Fortunately, the linker includes all symbols of an executable in the global\n// symbol table by default so dlsym() even finds static implementations of\n// ompt_start_tool. For this to work on Linux, -Wl,--export-dynamic needs to be\n// passed when building the application which we don't want to rely on.\n\nstatic ompt_start_tool_result_t *ompt_tool_darwin(unsigned int omp_version,\n                                                  const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  // Search symbol in the current address space.\n  ompt_start_tool_t start_tool =\n      (ompt_start_tool_t)dlsym(RTLD_DEFAULT, \"ompt_start_tool\");\n  if (start_tool) {\n    ret = start_tool(omp_version, runtime_version);\n  }\n  return ret;\n}\n\n#elif OMPT_HAVE_WEAK_ATTRIBUTE\n\n// On Unix-like systems that support weak symbols the following implementation\n// of ompt_start_tool() will be used in case no tool-supplied implementation of\n// this function is present in the address space of a process.\n\n_OMP_EXTERN OMPT_WEAK_ATTRIBUTE ompt_start_tool_result_t *\nompt_start_tool(unsigned int omp_version, const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  // Search next symbol in the current address space. This can happen if the\n  // runtime library is linked before the tool. Since glibc 2.2 strong symbols\n  // don't override weak symbols that have been found before unless the user\n  // sets the environment variable LD_DYNAMIC_WEAK.\n  ompt_start_tool_t next_tool =\n      (ompt_start_tool_t)dlsym(RTLD_NEXT, \"ompt_start_tool\");\n  if (next_tool) {\n    ret = next_tool(omp_version, runtime_version);\n  }\n  return ret;\n}\n\n#elif OMPT_HAVE_PSAPI\n\n// On Windows, the ompt_tool_windows function is used to find the\n// ompt_start_tool symbol across all modules loaded by a process. If\n// ompt_start_tool is found, ompt_start_tool's return value is used to\n// initialize the tool. Otherwise, NULL is returned and OMPT won't be enabled.\n\n#include <psapi.h>\n#pragma comment(lib, \"psapi.lib\")\n\n// The number of loaded modules to start enumeration with EnumProcessModules()\n#define NUM_MODULES 128\n\nstatic ompt_start_tool_result_t *\nompt_tool_windows(unsigned int omp_version, const char *runtime_version) {\n  int i;\n  DWORD needed, new_size;\n  HMODULE *modules;\n  HANDLE process = GetCurrentProcess();\n  modules = (HMODULE *)malloc(NUM_MODULES * sizeof(HMODULE));\n  ompt_start_tool_t ompt_tool_p = NULL;\n\n#if OMPT_DEBUG\n  printf(\"ompt_tool_windows(): looking for ompt_start_tool\\n\");\n#endif\n  if (!EnumProcessModules(process, modules, NUM_MODULES * sizeof(HMODULE),\n                          &needed)) {\n    // Regardless of the error reason use the stub initialization function\n    free(modules);\n    return NULL;\n  }\n  // Check if NUM_MODULES is enough to list all modules\n  new_size = needed / sizeof(HMODULE);\n  if (new_size > NUM_MODULES) {\n#if OMPT_DEBUG\n    printf(\"ompt_tool_windows(): resize buffer to %d bytes\\n\", needed);\n#endif\n    modules = (HMODULE *)realloc(modules, needed);\n    // If resizing failed use the stub function.\n    if (!EnumProcessModules(process, modules, needed, &needed)) {\n      free(modules);\n      return NULL;\n    }\n  }\n  for (i = 0; i < new_size; ++i) {\n    (FARPROC &)ompt_tool_p = GetProcAddress(modules[i], \"ompt_start_tool\");\n    if (ompt_tool_p) {\n#if OMPT_DEBUG\n      TCHAR modName[MAX_PATH];\n      if (GetModuleFileName(modules[i], modName, MAX_PATH))\n        printf(\"ompt_tool_windows(): ompt_start_tool found in module %s\\n\",\n               modName);\n#endif\n      free(modules);\n      return (*ompt_tool_p)(omp_version, runtime_version);\n    }\n#if OMPT_DEBUG\n    else {\n      TCHAR modName[MAX_PATH];\n      if (GetModuleFileName(modules[i], modName, MAX_PATH))\n        printf(\"ompt_tool_windows(): ompt_start_tool not found in module %s\\n\",\n               modName);\n    }\n#endif\n  }\n  free(modules);\n  return NULL;\n}\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n\nstatic ompt_start_tool_result_t *\nompt_try_start_tool(unsigned int omp_version, const char *runtime_version) {\n  ompt_start_tool_result_t *ret = NULL;\n  ompt_start_tool_t start_tool = NULL;\n#if KMP_OS_WINDOWS\n  // Cannot use colon to describe a list of absolute paths on Windows\n  const char *sep = \";\";\n#else\n  const char *sep = \":\";\n#endif\n\n  // Try in the current address space\n#if KMP_OS_DARWIN\n  ret = ompt_tool_darwin(omp_version, runtime_version);\n#elif OMPT_HAVE_WEAK_ATTRIBUTE\n  ret = ompt_start_tool(omp_version, runtime_version);\n#elif OMPT_HAVE_PSAPI\n  ret = ompt_tool_windows(omp_version, runtime_version);\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n  if (ret)\n    return ret;\n\n  // Try tool-libraries-var ICV\n  const char *tool_libs = getenv(\"OMP_TOOL_LIBRARIES\");\n  if (tool_libs) {\n    char *libs = __kmp_str_format(\"%s\", tool_libs);\n    char *buf;\n    char *fname = __kmp_str_token(libs, sep, &buf);\n    while (fname) {\n#if KMP_OS_UNIX\n      void *h = dlopen(fname, RTLD_LAZY);\n      if (h) {\n        start_tool = (ompt_start_tool_t)dlsym(h, \"ompt_start_tool\");\n#elif KMP_OS_WINDOWS\n      HMODULE h = LoadLibrary(fname);\n      if (h) {\n        start_tool = (ompt_start_tool_t)GetProcAddress(h, \"ompt_start_tool\");\n#else\n#error Activation of OMPT is not supported on this platform.\n#endif\n        if (start_tool && (ret = (*start_tool)(omp_version, runtime_version)))\n          break;\n      }\n      fname = __kmp_str_token(NULL, sep, &buf);\n    }\n    __kmp_str_free(&libs);\n  }\n  return ret;\n}\n\nvoid ompt_pre_init() {\n  //--------------------------------------------------\n  // Execute the pre-initialization logic only once.\n  //--------------------------------------------------\n  static int ompt_pre_initialized = 0;\n\n  if (ompt_pre_initialized)\n    return;\n\n  ompt_pre_initialized = 1;\n\n  //--------------------------------------------------\n  // Use a tool iff a tool is enabled and available.\n  //--------------------------------------------------\n  const char *ompt_env_var = getenv(\"OMP_TOOL\");\n  tool_setting_e tool_setting = omp_tool_error;\n\n  if (!ompt_env_var || !strcmp(ompt_env_var, \"\"))\n    tool_setting = omp_tool_unset;\n  else if (OMPT_STR_MATCH(ompt_env_var, \"disabled\"))\n    tool_setting = omp_tool_disabled;\n  else if (OMPT_STR_MATCH(ompt_env_var, \"enabled\"))\n    tool_setting = omp_tool_enabled;\n\n#if OMPT_DEBUG\n  printf(\"ompt_pre_init(): tool_setting = %d\\n\", tool_setting);\n#endif\n  switch (tool_setting) {\n  case omp_tool_disabled:\n    break;\n\n  case omp_tool_unset:\n  case omp_tool_enabled:\n\n    //--------------------------------------------------\n    // Load tool iff specified in environment variable\n    //--------------------------------------------------\n    ompt_start_tool_result =\n        ompt_try_start_tool(__kmp_openmp_version, ompt_get_runtime_version());\n\n    memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n    break;\n\n  case omp_tool_error:\n    fprintf(stderr, \"Warning: OMP_TOOL has invalid value \\\"%s\\\".\\n\"\n                    \"  legal values are (NULL,\\\"\\\",\\\"disabled\\\",\"\n                    \"\\\"enabled\\\").\\n\",\n            ompt_env_var);\n    break;\n  }\n#if OMPT_DEBUG\n  printf(\"ompt_pre_init(): ompt_enabled = %d\\n\", ompt_enabled);\n#endif\n}\n\nvoid ompt_post_init() {\n  //--------------------------------------------------\n  // Execute the post-initialization logic only once.\n  //--------------------------------------------------\n  static int ompt_post_initialized = 0;\n\n  if (ompt_post_initialized)\n    return;\n\n  ompt_post_initialized = 1;\n\n  //--------------------------------------------------\n  // Initialize the tool if so indicated.\n  //--------------------------------------------------\n  if (ompt_start_tool_result) {\n    ompt_enabled.enabled = !!ompt_start_tool_result->initialize(\n        ompt_fn_lookup, &(ompt_start_tool_result->tool_data));\n\n    ompt_thread_t *root_thread = ompt_get_thread();\n\n    ompt_set_thread_state(root_thread, omp_state_overhead);\n\n    if (ompt_enabled.ompt_callback_thread_begin) {\n      ompt_callbacks.ompt_callback(ompt_callback_thread_begin)(\n          ompt_thread_initial, __ompt_get_thread_data_internal());\n    }\n    ompt_data_t *task_data;\n    __ompt_get_task_info_internal(0, NULL, &task_data, NULL, NULL, NULL);\n    if (ompt_enabled.ompt_callback_task_create) {\n      ompt_callbacks.ompt_callback(ompt_callback_task_create)(\n          NULL, NULL, task_data, ompt_task_initial, 0, NULL);\n    }\n\n    ompt_set_thread_state(root_thread, omp_state_work_serial);\n  }\n}\n\nvoid ompt_fini() {\n  if (ompt_enabled.enabled) {\n    ompt_start_tool_result->finalize(&(ompt_start_tool_result->tool_data));\n  }\n\n  memset(&ompt_enabled, 0, sizeof(ompt_enabled));\n}\n\n/*****************************************************************************\n * interface operations\n ****************************************************************************/\n\n/*****************************************************************************\n * state\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_enumerate_states(int current_state, int *next_state,\n                                           const char **next_state_name) {\n  const static int len = sizeof(omp_state_info) / sizeof(omp_state_info_t);\n  int i = 0;\n\n  for (i = 0; i < len - 1; i++) {\n    if (omp_state_info[i].state_id == current_state) {\n      *next_state = omp_state_info[i + 1].state_id;\n      *next_state_name = omp_state_info[i + 1].state_name;\n      return 1;\n    }\n  }\n\n  return 0;\n}\n\nOMPT_API_ROUTINE int ompt_enumerate_mutex_impls(int current_impl,\n                                                int *next_impl,\n                                                const char **next_impl_name) {\n  const static int len =\n      sizeof(ompt_mutex_impl_info) / sizeof(ompt_mutex_impl_info_t);\n  int i = 0;\n  for (i = 0; i < len - 1; i++) {\n    if (ompt_mutex_impl_info[i].id != current_impl)\n      continue;\n    *next_impl = ompt_mutex_impl_info[i + 1].id;\n    *next_impl_name = ompt_mutex_impl_info[i + 1].name;\n    return 1;\n  }\n  return 0;\n}\n\n/*****************************************************************************\n * callbacks\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_set_callback(ompt_callbacks_t which,\n                                       ompt_callback_t callback) {\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case event_name:                                                             \\\n    if (ompt_event_implementation_status(event_name)) {                        \\\n      ompt_callbacks.ompt_callback(event_name) = (callback_type)callback;      \\\n      if(callback)                                                             \\\n        ompt_enabled.event_name = 1;                                           \\\n      else                                                                     \\\n        ompt_enabled.event_name = 0;                                           \\\n    }                                                                          \\\n    if(callback)                                                               \\\n      return ompt_event_implementation_status(event_name);                     \\\n    else                                                                       \\\n      return ompt_set_always;\n\n    FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_set_error;\n  }\n}\n\nOMPT_API_ROUTINE int ompt_get_callback(ompt_callbacks_t which,\n                                       ompt_callback_t *callback) {\n  switch (which) {\n\n#define ompt_event_macro(event_name, callback_type, event_id)                  \\\n  case event_name:                                                             \\\n    if (ompt_event_implementation_status(event_name)) {                        \\\n      ompt_callback_t mycb =                                                   \\\n          (ompt_callback_t)ompt_callbacks.ompt_callback(event_name);           \\\n      if (mycb) {                                                              \\\n        *callback = mycb;                                                      \\\n        return ompt_get_callback_success;                                      \\\n      }                                                                        \\\n    }                                                                          \\\n    return ompt_get_callback_failure;\n\n    FOREACH_OMPT_EVENT(ompt_event_macro)\n\n#undef ompt_event_macro\n\n  default:\n    return ompt_get_callback_failure;\n  }\n}\n\n/*****************************************************************************\n * parallel regions\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_parallel_info(int ancestor_level,\n                                            ompt_data_t **parallel_data,\n                                            int *team_size) {\n  return __ompt_get_parallel_info_internal(ancestor_level, parallel_data,\n                                           team_size);\n}\n\nOMPT_API_ROUTINE omp_state_t ompt_get_state(ompt_wait_id_t *wait_id) {\n  omp_state_t thread_state = __ompt_get_state_internal(wait_id);\n\n  if (thread_state == omp_state_undefined) {\n    thread_state = omp_state_work_serial;\n  }\n\n  return thread_state;\n}\n\n/*****************************************************************************\n * tasks\n ****************************************************************************/\n\nOMPT_API_ROUTINE ompt_data_t *ompt_get_thread_data(void) {\n  return __ompt_get_thread_data_internal();\n}\n\nOMPT_API_ROUTINE int ompt_get_task_info(int ancestor_level, int *type,\n                                        ompt_data_t **task_data,\n                                        ompt_frame_t **task_frame,\n                                        ompt_data_t **parallel_data,\n                                        int *thread_num) {\n  return __ompt_get_task_info_internal(ancestor_level, type, task_data,\n                                       task_frame, parallel_data, thread_num);\n}\n\n/*****************************************************************************\n * places\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_num_places(void) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  return __kmp_affinity_num_masks;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_place_proc_ids(int place_num, int ids_size,\n                                             int *ids) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  int i, count;\n  int tmp_ids[ids_size];\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  if (place_num < 0 || place_num >= (int)__kmp_affinity_num_masks)\n    return 0;\n  /* TODO: Is this safe for asynchronous call from signal handler during runtime\n   * shutdown? */\n  kmp_affin_mask_t *mask = KMP_CPU_INDEX(__kmp_affinity_masks, place_num);\n  count = 0;\n  KMP_CPU_SET_ITERATE(i, mask) {\n    if ((!KMP_CPU_ISSET(i, __kmp_affin_fullMask)) ||\n        (!KMP_CPU_ISSET(i, mask))) {\n      continue;\n    }\n    if (count < ids_size)\n      tmp_ids[count] = i;\n    count++;\n  }\n  if (ids_size >= count) {\n    for (i = 0; i < count; i++) {\n      ids[i] = tmp_ids[i];\n    }\n  }\n  return count;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_place_num(void) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return -1;\n#else\n  if (__kmp_get_gtid() < 0) \n    return -1;\n\n  int gtid;\n  kmp_info_t *thread;\n  if (!KMP_AFFINITY_CAPABLE())\n    return -1;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread == NULL || thread->th.th_current_place < 0)\n    return -1;\n  return thread->th.th_current_place;\n#endif\n}\n\nOMPT_API_ROUTINE int ompt_get_partition_place_nums(int place_nums_size,\n                                                   int *place_nums) {\n// copied from kmp_ftn_entry.h (but modified)\n#if !KMP_AFFINITY_SUPPORTED\n  return 0;\n#else\n  if (__kmp_get_gtid() < 0) \n    return 0;\n\n  int i, gtid, place_num, first_place, last_place, start, end;\n  kmp_info_t *thread;\n  if (!KMP_AFFINITY_CAPABLE())\n    return 0;\n  gtid = __kmp_entry_gtid();\n  thread = __kmp_thread_from_gtid(gtid);\n  if (thread == NULL)\n    return 0;\n  first_place = thread->th.th_first_place;\n  last_place = thread->th.th_last_place;\n  if (first_place < 0 || last_place < 0)\n    return 0;\n  if (first_place <= last_place) {\n    start = first_place;\n    end = last_place;\n  } else {\n    start = last_place;\n    end = first_place;\n  }\n  if (end - start <= place_nums_size)\n    for (i = 0, place_num = start; place_num <= end; ++place_num, ++i) {\n      place_nums[i] = place_num;\n    }\n  return end - start;\n#endif\n}\n\n/*****************************************************************************\n * places\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_proc_id(void) {\n#if KMP_OS_LINUX\n  if (__kmp_get_gtid() < 0) \n    return -1;\n\n  return sched_getcpu();\n#else\n  return -1;\n#endif\n}\n\n/*****************************************************************************\n * compatability\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_ompt_version() { return OMPT_VERSION; }\n\n/*****************************************************************************\n* application-facing API\n ****************************************************************************/\n\n/*----------------------------------------------------------------------------\n | control\n ---------------------------------------------------------------------------*/\n\nint __kmp_control_tool(uint64_t command, uint64_t modifier, void *arg) {\n\n  if (ompt_enabled.enabled) {\n    if (ompt_enabled.ompt_callback_control_tool) {\n      return ompt_callbacks.ompt_callback(ompt_callback_control_tool)(\n          command, modifier, arg, OMPT_LOAD_RETURN_ADDRESS(__kmp_entry_gtid()));\n    } else {\n      return -1;\n    }\n  } else {\n    return -2;\n  }\n}\n\n/*****************************************************************************\n * misc\n ****************************************************************************/\n\nOMPT_API_ROUTINE uint64_t ompt_get_unique_id(void) {\n  return __ompt_get_unique_id_internal();\n}\n\n/*****************************************************************************\n * Target\n ****************************************************************************/\n\nOMPT_API_ROUTINE int ompt_get_target_info(uint64_t *device_num,\n                                          ompt_id_t *target_id,\n                                          ompt_id_t *host_op_id) {\n  return 0; // thread is not in a target region\n}\n\nOMPT_API_ROUTINE int ompt_get_num_devices(void) {\n  return 1; // only one device (the current device) is available\n}\n\n/*****************************************************************************\n * API inquiry for tool\n ****************************************************************************/\n\nstatic ompt_interface_fn_t ompt_fn_lookup(const char *s) {\n\n#define ompt_interface_fn(fn)                                                  \\\n  fn##_t fn##_f = fn;                                                          \\\n  if (strcmp(s, #fn) == 0)                                                     \\\n    return (ompt_interface_fn_t)fn##_f;\n\n  FOREACH_OMPT_INQUIRY_FN(ompt_interface_fn)\n\n  return (ompt_interface_fn_t)0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/runtime/src/thirdparty/ittnotify/ittnotify_config.h": "\n//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n\n#ifndef _ITTNOTIFY_CONFIG_H_\n#define _ITTNOTIFY_CONFIG_H_\n\n/** @cond exclude_from_documentation */\n#ifndef ITT_OS_WIN\n#  define ITT_OS_WIN   1\n#endif /* ITT_OS_WIN */\n\n#ifndef ITT_OS_LINUX\n#  define ITT_OS_LINUX 2\n#endif /* ITT_OS_LINUX */\n\n#ifndef ITT_OS_MAC\n#  define ITT_OS_MAC   3\n#endif /* ITT_OS_MAC */\n\n#ifndef ITT_OS\n#  if defined WIN32 || defined _WIN32\n#    define ITT_OS ITT_OS_WIN\n#  elif defined( __APPLE__ ) && defined( __MACH__ )\n#    define ITT_OS ITT_OS_MAC\n#  else\n#    define ITT_OS ITT_OS_LINUX\n#  endif\n#endif /* ITT_OS */\n\n#ifndef ITT_PLATFORM_WIN\n#  define ITT_PLATFORM_WIN 1\n#endif /* ITT_PLATFORM_WIN */\n\n#ifndef ITT_PLATFORM_POSIX\n#  define ITT_PLATFORM_POSIX 2\n#endif /* ITT_PLATFORM_POSIX */\n\n#ifndef ITT_PLATFORM_MAC\n#  define ITT_PLATFORM_MAC 3\n#endif /* ITT_PLATFORM_MAC */\n\n#ifndef ITT_PLATFORM\n#  if ITT_OS==ITT_OS_WIN\n#    define ITT_PLATFORM ITT_PLATFORM_WIN\n#  elif ITT_OS==ITT_OS_MAC\n#    define ITT_PLATFORM ITT_PLATFORM_MAC\n#  else\n#    define ITT_PLATFORM ITT_PLATFORM_POSIX\n#  endif\n#endif /* ITT_PLATFORM */\n\n#if defined(_UNICODE) && !defined(UNICODE)\n#define UNICODE\n#endif\n\n#include <stddef.h>\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <tchar.h>\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <stdint.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE || _UNICODE */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#ifndef CDECL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define CDECL __cdecl\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define CDECL __attribute__ ((cdecl))\n#    else  /* _M_IX86 || __i386__ */\n#      define CDECL /* actual only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* CDECL */\n\n#ifndef STDCALL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define STDCALL __stdcall\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define STDCALL __attribute__ ((stdcall))\n#    else  /* _M_IX86 || __i386__ */\n#      define STDCALL /* supported only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* STDCALL */\n\n#define ITTAPI    CDECL\n#define LIBITTAPI CDECL\n\n/* TODO: Temporary for compatibility! */\n#define ITTAPI_CALL    CDECL\n#define LIBITTAPI_CALL CDECL\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n/* use __forceinline (VC++ specific) */\n#define ITT_INLINE           __forceinline\n#define ITT_INLINE_ATTRIBUTE /* nothing */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/*\n * Generally, functions are not inlined unless optimization is specified.\n * For functions declared inline, this attribute inlines the function even\n * if no optimization level was specified.\n */\n#ifdef __STRICT_ANSI__\n#define ITT_INLINE           static\n#define ITT_INLINE_ATTRIBUTE __attribute__((unused))\n#else  /* __STRICT_ANSI__ */\n#define ITT_INLINE           static inline\n#define ITT_INLINE_ATTRIBUTE __attribute__((always_inline, unused))\n#endif /* __STRICT_ANSI__ */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/** @endcond */\n\n#ifndef ITT_ARCH_IA32\n#  define ITT_ARCH_IA32  1\n#endif /* ITT_ARCH_IA32 */\n\n#ifndef ITT_ARCH_IA32E\n#  define ITT_ARCH_IA32E 2\n#endif /* ITT_ARCH_IA32E */\n\n/* Was there a magical reason we didn't have 3 here before? */\n#ifndef ITT_ARCH_AARCH64\n#  define ITT_ARCH_AARCH64  3\n#endif /* ITT_ARCH_AARCH64 */\n\n#ifndef ITT_ARCH_ARM\n#  define ITT_ARCH_ARM  4\n#endif /* ITT_ARCH_ARM */\n\n#ifndef ITT_ARCH_PPC64\n#  define ITT_ARCH_PPC64  5\n#endif /* ITT_ARCH_PPC64 */\n\n#ifndef ITT_ARCH_MIPS\n#  define ITT_ARCH_MIPS  6\n#endif /* ITT_ARCH_MIPS */\n\n#ifndef ITT_ARCH_MIPS64\n#  define ITT_ARCH_MIPS64  6\n#endif /* ITT_ARCH_MIPS64 */\n\n\n#ifndef ITT_ARCH\n#  if defined _M_IX86 || defined __i386__\n#    define ITT_ARCH ITT_ARCH_IA32\n#  elif defined _M_X64 || defined _M_AMD64 || defined __x86_64__\n#    define ITT_ARCH ITT_ARCH_IA32E\n#  elif defined _M_IA64 || defined __ia64__\n#    define ITT_ARCH ITT_ARCH_IA64\n#  elif defined _M_ARM || __arm__\n#    define ITT_ARCH ITT_ARCH_ARM\n#  elif defined __powerpc64__\n#    define ITT_ARCH ITT_ARCH_PPC64\n#  elif defined __aarch64__\n#    define ITT_ARCH ITT_ARCH_AARCH64\n#  elif defined __mips__ && !defined __mips64\n#    define ITT_ARCH ITT_ARCH_MIPS\n#  elif defined __mips__ && defined __mips64\n#    define ITT_ARCH ITT_ARCH_MIPS64\n#  endif\n#endif\n\n#ifdef __cplusplus\n#  define ITT_EXTERN_C extern \"C\"\n#  define ITT_EXTERN_C_BEGIN extern \"C\" {\n#  define ITT_EXTERN_C_END }\n#else\n#  define ITT_EXTERN_C /* nothing */\n#  define ITT_EXTERN_C_BEGIN /* nothing */\n#  define ITT_EXTERN_C_END /* nothing */\n#endif /* __cplusplus */\n\n#define ITT_TO_STR_AUX(x) #x\n#define ITT_TO_STR(x)     ITT_TO_STR_AUX(x)\n\n#define __ITT_BUILD_ASSERT(expr, suffix) do { \\\n    static char __itt_build_check_##suffix[(expr) ? 1 : -1]; \\\n    __itt_build_check_##suffix[0] = 0; \\\n} while(0)\n#define _ITT_BUILD_ASSERT(expr, suffix)  __ITT_BUILD_ASSERT((expr), suffix)\n#define ITT_BUILD_ASSERT(expr)           _ITT_BUILD_ASSERT((expr), __LINE__)\n\n#define ITT_MAGIC { 0xED, 0xAB, 0xAB, 0xEC, 0x0D, 0xEE, 0xDA, 0x30 }\n\n/* Replace with snapshot date YYYYMMDD for promotion build. */\n#define API_VERSION_BUILD    20111111\n\n#ifndef API_VERSION_NUM\n#define API_VERSION_NUM 0.0.0\n#endif /* API_VERSION_NUM */\n\n#define API_VERSION \"ITT-API-Version \" ITT_TO_STR(API_VERSION_NUM) \\\n                                \" (\" ITT_TO_STR(API_VERSION_BUILD) \")\"\n\n/* OS communication functions */\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <windows.h>\ntypedef HMODULE           lib_t;\ntypedef DWORD             TIDT;\ntypedef CRITICAL_SECTION  mutex_t;\n#define MUTEX_INITIALIZER { 0 }\n#define strong_alias(name, aliasname) /* empty for Windows */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <dlfcn.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE */\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1 /* need for PTHREAD_MUTEX_RECURSIVE */\n#endif /* _GNU_SOURCE */\n#ifndef __USE_UNIX98\n#define __USE_UNIX98 1 /* need for PTHREAD_MUTEX_RECURSIVE, on SLES11.1 with gcc 4.3.4 wherein pthread.h missing dependency on __USE_XOPEN2K8 */\n#endif /*__USE_UNIX98*/\n#include <pthread.h>\ntypedef void*             lib_t;\ntypedef pthread_t         TIDT;\ntypedef pthread_mutex_t   mutex_t;\n#define MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n#define _strong_alias(name, aliasname) \\\n            extern __typeof (name) aliasname __attribute__ ((alias (#name)));\n#define strong_alias(name, aliasname) _strong_alias(name, aliasname)\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#define __itt_get_proc(lib, name) GetProcAddress(lib, name)\n#define __itt_mutex_init(mutex)   InitializeCriticalSection(mutex)\n#define __itt_mutex_lock(mutex)   EnterCriticalSection(mutex)\n#define __itt_mutex_unlock(mutex) LeaveCriticalSection(mutex)\n#define __itt_load_lib(name)      LoadLibraryA(name)\n#define __itt_unload_lib(handle)  FreeLibrary(handle)\n#define __itt_system_error()      (int)GetLastError()\n#define __itt_fstrcmp(s1, s2)     lstrcmpA(s1, s2)\n#define __itt_fstrlen(s)          lstrlenA(s)\n#define __itt_fstrcpyn(s1, s2, l) lstrcpynA(s1, s2, l)\n#define __itt_fstrdup(s)          _strdup(s)\n#define __itt_thread_id()         GetCurrentThreadId()\n#define __itt_thread_yield()      SwitchToThread()\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return InterlockedIncrement(ptr);\n}\n#endif /* ITT_SIMPLE_INIT */\n#else /* ITT_PLATFORM!=ITT_PLATFORM_WIN */\n#define __itt_get_proc(lib, name) dlsym(lib, name)\n#define __itt_mutex_init(mutex)   {\\\n    pthread_mutexattr_t mutex_attr;                                         \\\n    int error_code = pthread_mutexattr_init(&mutex_attr);                   \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_init\",    \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_settype(&mutex_attr,                     \\\n                                           PTHREAD_MUTEX_RECURSIVE);        \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_settype\", \\\n                           error_code);                                     \\\n    error_code = pthread_mutex_init(mutex, &mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutex_init\",        \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_destroy(&mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_destroy\", \\\n                           error_code);                                     \\\n}\n#define __itt_mutex_lock(mutex)   pthread_mutex_lock(mutex)\n#define __itt_mutex_unlock(mutex) pthread_mutex_unlock(mutex)\n#define __itt_load_lib(name)      dlopen(name, RTLD_LAZY)\n#define __itt_unload_lib(handle)  dlclose(handle)\n#define __itt_system_error()      errno\n#define __itt_fstrcmp(s1, s2)     strcmp(s1, s2)\n#define __itt_fstrlen(s)          strlen(s)\n#define __itt_fstrcpyn(s1, s2, l) strncpy(s1, s2, l)\n#define __itt_fstrdup(s)          strdup(s)\n#define __itt_thread_id()         pthread_self()\n#define __itt_thread_yield()      sched_yield()\n#if ITT_ARCH==ITT_ARCH_IA64\n#ifdef __INTEL_COMPILER\n#define __TBB_machine_fetchadd4(addr, val) __fetchadd4_acq((void *)addr, val)\n#else  /* __INTEL_COMPILER */\n/* TODO: Add Support for not Intel compilers for IA-64 architecture */\n#endif /* __INTEL_COMPILER */\n#elif ITT_ARCH==ITT_ARCH_IA32 || ITT_ARCH==ITT_ARCH_IA32E /* ITT_ARCH!=ITT_ARCH_IA64 */\nITT_INLINE long\n__TBB_machine_fetchadd4(volatile void* ptr, long addend) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __TBB_machine_fetchadd4(volatile void* ptr, long addend)\n{\n    long result;\n    __asm__ __volatile__(\"lock\\nxadd %0,%1\"\n                          : \"=r\"(result),\"=m\"(*(volatile int*)ptr)\n                          : \"0\"(addend), \"m\"(*(volatile int*)ptr)\n                          : \"memory\");\n    return result;\n}\n#elif ITT_ARCH==ITT_ARCH_ARM || ITT_ARCH==ITT_ARCH_PPC64 || ITT_ARCH==ITT_ARCH_AARCH64 || ITT_ARCH==ITT_ARCH_MIPS ||  ITT_ARCH==ITT_ARCH_MIPS64\n#define __TBB_machine_fetchadd4(addr, val) __sync_fetch_and_add(addr, val)\n#endif /* ITT_ARCH==ITT_ARCH_IA64 */\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return __TBB_machine_fetchadd4(ptr, 1) + 1L;\n}\n#endif /* ITT_SIMPLE_INIT */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\ntypedef enum {\n    __itt_collection_normal = 0,\n    __itt_collection_paused = 1\n} __itt_collection_state;\n\ntypedef enum {\n    __itt_thread_normal  = 0,\n    __itt_thread_ignored = 1\n} __itt_thread_state;\n\n#pragma pack(push, 8)\n\ntypedef struct ___itt_thread_info\n{\n    const char* nameA; /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    TIDT               tid;\n    __itt_thread_state state;   /*!< Thread state (paused or normal) */\n    int                extra1;  /*!< Reserved to the runtime */\n    void*              extra2;  /*!< Reserved to the runtime */\n    struct ___itt_thread_info* next;\n} __itt_thread_info;\n\n#include \"ittnotify_types.h\" /* For __itt_group_id definition */\n\ntypedef struct ___itt_api_info_20101001\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    __itt_group_id group;\n}  __itt_api_info_20101001;\n\ntypedef struct ___itt_api_info\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    void*          null_func;\n    __itt_group_id group;\n}  __itt_api_info;\n\nstruct ___itt_domain;\nstruct ___itt_string_handle;\n\ntypedef struct ___itt_global\n{\n    unsigned char          magic[8];\n    unsigned long          version_major;\n    unsigned long          version_minor;\n    unsigned long          version_build;\n    volatile long          api_initialized;\n    volatile long          mutex_initialized;\n    volatile long          atomic_counter;\n    mutex_t                mutex;\n    lib_t                  lib;\n    void*                  error_handler;\n    const char**           dll_path_ptr;\n    __itt_api_info*        api_list_ptr;\n    struct ___itt_global*  next;\n    /* Joinable structures below */\n    __itt_thread_info*     thread_list;\n    struct ___itt_domain*  domain_list;\n    struct ___itt_string_handle* string_list;\n    __itt_collection_state state;\n} __itt_global;\n\n#pragma pack(pop)\n\n#define NEW_THREAD_INFO_W(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = NULL; \\\n        h->nameW  = n ? _wcsdup(n) : NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_THREAD_INFO_A(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = n ? __itt_fstrdup(n) : NULL; \\\n        h->nameW  = NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_W(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 0;    /* domain is disabled by default */ \\\n        h->nameA  = NULL; \\\n        h->nameW  = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_A(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 0;    /* domain is disabled by default */ \\\n        h->nameA  = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW  = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_W(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = NULL; \\\n        h->strW   = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_A(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->strW   = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#endif /* _ITTNOTIFY_CONFIG_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/libomptarget/src/omptarget.cpp": "//===------ omptarget.cpp - Target independent OpenMP target RTL -- C++ -*-===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n//\n// Implementation of the interface to be used by Clang during the codegen of a\n// target region.\n//\n//===----------------------------------------------------------------------===//\n\n#include <algorithm>\n#include <cassert>\n#include <climits>\n#include <cstdlib>\n#include <cstring>\n#include <dlfcn.h>\n#include <list>\n#include <map>\n#include <mutex>\n#include <string>\n#include <vector>\n\n// Header file global to this project\n#include \"omptarget.h\"\n\n#ifdef OMPTARGET_DEBUG\nstatic int DebugLevel = 0;\n\n#define DP(...) \\\n  do { \\\n    if (DebugLevel > 0) { \\\n      DEBUGP(\"Libomptarget\", __VA_ARGS__); \\\n    } \\\n  } while (false)\n#else // OMPTARGET_DEBUG\n#define DP(...) {}\n#endif // OMPTARGET_DEBUG\n\n#define INF_REF_CNT (LONG_MAX>>1) // leave room for additions/subtractions\n#define CONSIDERED_INF(x) (x > (INF_REF_CNT>>1))\n\n// List of all plugins that can support offloading.\nstatic const char *RTLNames[] = {\n    /* PowerPC target */ \"libomptarget.rtl.ppc64.so\",\n    /* x86_64 target  */ \"libomptarget.rtl.x86_64.so\",\n    /* CUDA target    */ \"libomptarget.rtl.cuda.so\",\n    /* AArch64 target */ \"libomptarget.rtl.aarch64.so\"};\n\n// forward declarations\nstruct RTLInfoTy;\nstatic int target(int64_t device_id, void *host_ptr, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types,\n    int32_t team_num, int32_t thread_limit, int IsTeamConstruct);\n\n/// Map between host data and target data.\nstruct HostDataToTargetTy {\n  uintptr_t HstPtrBase; // host info.\n  uintptr_t HstPtrBegin;\n  uintptr_t HstPtrEnd; // non-inclusive.\n\n  uintptr_t TgtPtrBegin; // target info.\n\n  long RefCount;\n\n  HostDataToTargetTy()\n      : HstPtrBase(0), HstPtrBegin(0), HstPtrEnd(0),\n        TgtPtrBegin(0), RefCount(0) {}\n  HostDataToTargetTy(uintptr_t BP, uintptr_t B, uintptr_t E, uintptr_t TB)\n      : HstPtrBase(BP), HstPtrBegin(B), HstPtrEnd(E),\n        TgtPtrBegin(TB), RefCount(1) {}\n  HostDataToTargetTy(uintptr_t BP, uintptr_t B, uintptr_t E, uintptr_t TB,\n      long RF)\n      : HstPtrBase(BP), HstPtrBegin(B), HstPtrEnd(E),\n        TgtPtrBegin(TB), RefCount(RF) {}\n};\n\ntypedef std::list<HostDataToTargetTy> HostDataToTargetListTy;\n\nstruct LookupResult {\n  struct {\n    unsigned IsContained   : 1;\n    unsigned ExtendsBefore : 1;\n    unsigned ExtendsAfter  : 1;\n  } Flags;\n\n  HostDataToTargetListTy::iterator Entry;\n\n  LookupResult() : Flags({0,0,0}), Entry() {}\n};\n\n/// Map for shadow pointers\nstruct ShadowPtrValTy {\n  void *HstPtrVal;\n  void *TgtPtrAddr;\n  void *TgtPtrVal;\n};\ntypedef std::map<void *, ShadowPtrValTy> ShadowPtrListTy;\n\n///\nstruct PendingCtorDtorListsTy {\n  std::list<void *> PendingCtors;\n  std::list<void *> PendingDtors;\n};\ntypedef std::map<__tgt_bin_desc *, PendingCtorDtorListsTy>\n    PendingCtorsDtorsPerLibrary;\n\nstruct DeviceTy {\n  int32_t DeviceID;\n  RTLInfoTy *RTL;\n  int32_t RTLDeviceID;\n\n  bool IsInit;\n  std::once_flag InitFlag;\n  bool HasPendingGlobals;\n\n  HostDataToTargetListTy HostDataToTargetMap;\n  PendingCtorsDtorsPerLibrary PendingCtorsDtors;\n\n  ShadowPtrListTy ShadowPtrMap;\n\n  std::mutex DataMapMtx, PendingGlobalsMtx, ShadowMtx;\n\n  uint64_t loopTripCnt;\n\n  DeviceTy(RTLInfoTy *RTL)\n      : DeviceID(-1), RTL(RTL), RTLDeviceID(-1), IsInit(false), InitFlag(),\n        HasPendingGlobals(false), HostDataToTargetMap(),\n        PendingCtorsDtors(), ShadowPtrMap(), DataMapMtx(), PendingGlobalsMtx(),\n        ShadowMtx(), loopTripCnt(0) {}\n\n  // The existence of mutexes makes DeviceTy non-copyable. We need to\n  // provide a copy constructor and an assignment operator explicitly.\n  DeviceTy(const DeviceTy &d)\n      : DeviceID(d.DeviceID), RTL(d.RTL), RTLDeviceID(d.RTLDeviceID),\n        IsInit(d.IsInit), InitFlag(), HasPendingGlobals(d.HasPendingGlobals),\n        HostDataToTargetMap(d.HostDataToTargetMap),\n        PendingCtorsDtors(d.PendingCtorsDtors), ShadowPtrMap(d.ShadowPtrMap),\n        DataMapMtx(), PendingGlobalsMtx(),\n        ShadowMtx(), loopTripCnt(d.loopTripCnt) {}\n\n  DeviceTy& operator=(const DeviceTy &d) {\n    DeviceID = d.DeviceID;\n    RTL = d.RTL;\n    RTLDeviceID = d.RTLDeviceID;\n    IsInit = d.IsInit;\n    HasPendingGlobals = d.HasPendingGlobals;\n    HostDataToTargetMap = d.HostDataToTargetMap;\n    PendingCtorsDtors = d.PendingCtorsDtors;\n    ShadowPtrMap = d.ShadowPtrMap;\n    loopTripCnt = d.loopTripCnt;\n\n    return *this;\n  }\n\n  long getMapEntryRefCnt(void *HstPtrBegin);\n  LookupResult lookupMapping(void *HstPtrBegin, int64_t Size);\n  void *getOrAllocTgtPtr(void *HstPtrBegin, void *HstPtrBase, int64_t Size,\n      bool &IsNew, bool IsImplicit, bool UpdateRefCount = true);\n  void *getTgtPtrBegin(void *HstPtrBegin, int64_t Size);\n  void *getTgtPtrBegin(void *HstPtrBegin, int64_t Size, bool &IsLast,\n      bool UpdateRefCount);\n  int deallocTgtPtr(void *TgtPtrBegin, int64_t Size, bool ForceDelete);\n  int associatePtr(void *HstPtrBegin, void *TgtPtrBegin, int64_t Size);\n  int disassociatePtr(void *HstPtrBegin);\n\n  // calls to RTL\n  int32_t initOnce();\n  __tgt_target_table *load_binary(void *Img);\n\n  int32_t data_submit(void *TgtPtrBegin, void *HstPtrBegin, int64_t Size);\n  int32_t data_retrieve(void *HstPtrBegin, void *TgtPtrBegin, int64_t Size);\n\n  int32_t run_region(void *TgtEntryPtr, void **TgtVarsPtr,\n      ptrdiff_t *TgtOffsets, int32_t TgtVarsSize);\n  int32_t run_team_region(void *TgtEntryPtr, void **TgtVarsPtr,\n      ptrdiff_t *TgtOffsets, int32_t TgtVarsSize, int32_t NumTeams,\n      int32_t ThreadLimit, uint64_t LoopTripCount);\n\nprivate:\n  // Call to RTL\n  void init(); // To be called only via DeviceTy::initOnce()\n};\n\n/// Map between Device ID (i.e. openmp device id) and its DeviceTy.\ntypedef std::vector<DeviceTy> DevicesTy;\nstatic DevicesTy Devices;\n\nstruct RTLInfoTy {\n  typedef int32_t(is_valid_binary_ty)(void *);\n  typedef int32_t(number_of_devices_ty)();\n  typedef int32_t(init_device_ty)(int32_t);\n  typedef __tgt_target_table *(load_binary_ty)(int32_t, void *);\n  typedef void *(data_alloc_ty)(int32_t, int64_t, void *);\n  typedef int32_t(data_submit_ty)(int32_t, void *, void *, int64_t);\n  typedef int32_t(data_retrieve_ty)(int32_t, void *, void *, int64_t);\n  typedef int32_t(data_delete_ty)(int32_t, void *);\n  typedef int32_t(run_region_ty)(int32_t, void *, void **, ptrdiff_t *,\n                                 int32_t);\n  typedef int32_t(run_team_region_ty)(int32_t, void *, void **, ptrdiff_t *,\n                                      int32_t, int32_t, int32_t, uint64_t);\n\n  int32_t Idx;                     // RTL index, index is the number of devices\n                                   // of other RTLs that were registered before,\n                                   // i.e. the OpenMP index of the first device\n                                   // to be registered with this RTL.\n  int32_t NumberOfDevices;         // Number of devices this RTL deals with.\n  std::vector<DeviceTy *> Devices; // one per device (NumberOfDevices in total).\n\n  void *LibraryHandler;\n\n#ifdef OMPTARGET_DEBUG\n  std::string RTLName;\n#endif\n\n  // Functions implemented in the RTL.\n  is_valid_binary_ty *is_valid_binary;\n  number_of_devices_ty *number_of_devices;\n  init_device_ty *init_device;\n  load_binary_ty *load_binary;\n  data_alloc_ty *data_alloc;\n  data_submit_ty *data_submit;\n  data_retrieve_ty *data_retrieve;\n  data_delete_ty *data_delete;\n  run_region_ty *run_region;\n  run_team_region_ty *run_team_region;\n\n  // Are there images associated with this RTL.\n  bool isUsed;\n\n  // Mutex for thread-safety when calling RTL interface functions.\n  // It is easier to enforce thread-safety at the libomptarget level,\n  // so that developers of new RTLs do not have to worry about it.\n  std::mutex Mtx;\n\n  // The existence of the mutex above makes RTLInfoTy non-copyable.\n  // We need to provide a copy constructor explicitly.\n  RTLInfoTy()\n      : Idx(-1), NumberOfDevices(-1), Devices(), LibraryHandler(0),\n#ifdef OMPTARGET_DEBUG\n        RTLName(),\n#endif\n        is_valid_binary(0), number_of_devices(0), init_device(0),\n        load_binary(0), data_alloc(0), data_submit(0), data_retrieve(0),\n        data_delete(0), run_region(0), run_team_region(0), isUsed(false),\n        Mtx() {}\n\n  RTLInfoTy(const RTLInfoTy &r) : Mtx() {\n    Idx = r.Idx;\n    NumberOfDevices = r.NumberOfDevices;\n    Devices = r.Devices;\n    LibraryHandler = r.LibraryHandler;\n#ifdef OMPTARGET_DEBUG\n    RTLName = r.RTLName;\n#endif\n    is_valid_binary = r.is_valid_binary;\n    number_of_devices = r.number_of_devices;\n    init_device = r.init_device;\n    load_binary = r.load_binary;\n    data_alloc = r.data_alloc;\n    data_submit = r.data_submit;\n    data_retrieve = r.data_retrieve;\n    data_delete = r.data_delete;\n    run_region = r.run_region;\n    run_team_region = r.run_team_region;\n    isUsed = r.isUsed;\n  }\n};\n\n/// RTLs identified in the system.\nclass RTLsTy {\nprivate:\n  // Mutex-like object to guarantee thread-safety and unique initialization\n  // (i.e. the library attempts to load the RTLs (plugins) only once).\n  std::once_flag initFlag;\n  void LoadRTLs(); // not thread-safe\n\npublic:\n  // List of the detected runtime libraries.\n  std::list<RTLInfoTy> AllRTLs;\n\n  // Array of pointers to the detected runtime libraries that have compatible\n  // binaries.\n  std::vector<RTLInfoTy *> UsedRTLs;\n\n  explicit RTLsTy() {}\n\n  // Load all the runtime libraries (plugins) if not done before.\n  void LoadRTLsOnce();\n};\n\nvoid RTLsTy::LoadRTLs() {\n#ifdef OMPTARGET_DEBUG\n  if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n    DebugLevel = std::stoi(envStr);\n  }\n#endif // OMPTARGET_DEBUG\n\n  // Parse environment variable OMP_TARGET_OFFLOAD (if set)\n  char *envStr = getenv(\"OMP_TARGET_OFFLOAD\");\n  if (envStr && !strcmp(envStr, \"DISABLED\")) {\n    DP(\"Target offloading disabled by environment\\n\");\n    return;\n  }\n\n  DP(\"Loading RTLs...\\n\");\n\n  // Attempt to open all the plugins and, if they exist, check if the interface\n  // is correct and if they are supporting any devices.\n  for (auto *Name : RTLNames) {\n    DP(\"Loading library '%s'...\\n\", Name);\n    void *dynlib_handle = dlopen(Name, RTLD_NOW);\n\n    if (!dynlib_handle) {\n      // Library does not exist or cannot be found.\n      DP(\"Unable to load library '%s': %s!\\n\", Name, dlerror());\n      continue;\n    }\n\n    DP(\"Successfully loaded library '%s'!\\n\", Name);\n\n    // Retrieve the RTL information from the runtime library.\n    RTLInfoTy R;\n\n    R.LibraryHandler = dynlib_handle;\n    R.isUsed = false;\n\n#ifdef OMPTARGET_DEBUG\n    R.RTLName = Name;\n#endif\n\n    if (!(*((void**) &R.is_valid_binary) = dlsym(\n              dynlib_handle, \"__tgt_rtl_is_valid_binary\")))\n      continue;\n    if (!(*((void**) &R.number_of_devices) = dlsym(\n              dynlib_handle, \"__tgt_rtl_number_of_devices\")))\n      continue;\n    if (!(*((void**) &R.init_device) = dlsym(\n              dynlib_handle, \"__tgt_rtl_init_device\")))\n      continue;\n    if (!(*((void**) &R.load_binary) = dlsym(\n              dynlib_handle, \"__tgt_rtl_load_binary\")))\n      continue;\n    if (!(*((void**) &R.data_alloc) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_alloc\")))\n      continue;\n    if (!(*((void**) &R.data_submit) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_submit\")))\n      continue;\n    if (!(*((void**) &R.data_retrieve) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_retrieve\")))\n      continue;\n    if (!(*((void**) &R.data_delete) = dlsym(\n              dynlib_handle, \"__tgt_rtl_data_delete\")))\n      continue;\n    if (!(*((void**) &R.run_region) = dlsym(\n              dynlib_handle, \"__tgt_rtl_run_target_region\")))\n      continue;\n    if (!(*((void**) &R.run_team_region) = dlsym(\n              dynlib_handle, \"__tgt_rtl_run_target_team_region\")))\n      continue;\n\n    // No devices are supported by this RTL?\n    if (!(R.NumberOfDevices = R.number_of_devices())) {\n      DP(\"No devices supported in this RTL\\n\");\n      continue;\n    }\n\n    DP(\"Registering RTL %s supporting %d devices!\\n\",\n        R.RTLName.c_str(), R.NumberOfDevices);\n\n    // The RTL is valid! Will save the information in the RTLs list.\n    AllRTLs.push_back(R);\n  }\n\n  DP(\"RTLs loaded!\\n\");\n\n  return;\n}\n\nvoid RTLsTy::LoadRTLsOnce() {\n  // RTL.LoadRTLs() is called only once in a thread-safe fashion.\n  std::call_once(initFlag, &RTLsTy::LoadRTLs, this);\n}\n\nstatic RTLsTy RTLs;\nstatic std::mutex RTLsMtx;\n\n/// Map between the host entry begin and the translation table. Each\n/// registered library gets one TranslationTable. Use the map from\n/// __tgt_offload_entry so that we may quickly determine whether we\n/// are trying to (re)register an existing lib or really have a new one.\nstruct TranslationTable {\n  __tgt_target_table HostTable;\n\n  // Image assigned to a given device.\n  std::vector<__tgt_device_image *> TargetsImages; // One image per device ID.\n\n  // Table of entry points or NULL if it was not already computed.\n  std::vector<__tgt_target_table *> TargetsTable; // One table per device ID.\n};\ntypedef std::map<__tgt_offload_entry *, TranslationTable>\n    HostEntriesBeginToTransTableTy;\nstatic HostEntriesBeginToTransTableTy HostEntriesBeginToTransTable;\nstatic std::mutex TrlTblMtx;\n\n/// Map between the host ptr and a table index\nstruct TableMap {\n  TranslationTable *Table; // table associated with the host ptr.\n  uint32_t Index; // index in which the host ptr translated entry is found.\n  TableMap() : Table(0), Index(0) {}\n  TableMap(TranslationTable *table, uint32_t index)\n      : Table(table), Index(index) {}\n};\ntypedef std::map<void *, TableMap> HostPtrToTableMapTy;\nstatic HostPtrToTableMapTy HostPtrToTableMap;\nstatic std::mutex TblMapMtx;\n\n/// Check whether a device has an associated RTL and initialize it if it's not\n/// already initialized.\nstatic bool device_is_ready(int device_num) {\n  DP(\"Checking whether device %d is ready.\\n\", device_num);\n  // Devices.size() can only change while registering a new\n  // library, so try to acquire the lock of RTLs' mutex.\n  RTLsMtx.lock();\n  size_t Devices_size = Devices.size();\n  RTLsMtx.unlock();\n  if (Devices_size <= (size_t)device_num) {\n    DP(\"Device ID  %d does not have a matching RTL\\n\", device_num);\n    return false;\n  }\n\n  // Get device info\n  DeviceTy &Device = Devices[device_num];\n\n  DP(\"Is the device %d (local ID %d) initialized? %d\\n\", device_num,\n       Device.RTLDeviceID, Device.IsInit);\n\n  // Init the device if not done before\n  if (!Device.IsInit && Device.initOnce() != OFFLOAD_SUCCESS) {\n    DP(\"Failed to init device %d\\n\", device_num);\n    return false;\n  }\n\n  DP(\"Device %d is ready to use.\\n\", device_num);\n\n  return true;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Target API functions\n//\nEXTERN int omp_get_num_devices(void) {\n  RTLsMtx.lock();\n  size_t Devices_size = Devices.size();\n  RTLsMtx.unlock();\n\n  DP(\"Call to omp_get_num_devices returning %zd\\n\", Devices_size);\n\n  return Devices_size;\n}\n\nEXTERN int omp_get_initial_device(void) {\n  DP(\"Call to omp_get_initial_device returning %d\\n\", HOST_DEVICE);\n  return HOST_DEVICE;\n}\n\nEXTERN void *omp_target_alloc(size_t size, int device_num) {\n  DP(\"Call to omp_target_alloc for device %d requesting %zu bytes\\n\",\n      device_num, size);\n\n  if (size <= 0) {\n    DP(\"Call to omp_target_alloc with non-positive length\\n\");\n    return NULL;\n  }\n\n  void *rc = NULL;\n\n  if (device_num == omp_get_initial_device()) {\n    rc = malloc(size);\n    DP(\"omp_target_alloc returns host ptr \" DPxMOD \"\\n\", DPxPTR(rc));\n    return rc;\n  }\n\n  if (!device_is_ready(device_num)) {\n    DP(\"omp_target_alloc returns NULL ptr\\n\");\n    return NULL;\n  }\n\n  DeviceTy &Device = Devices[device_num];\n  rc = Device.RTL->data_alloc(Device.RTLDeviceID, size, NULL);\n  DP(\"omp_target_alloc returns device ptr \" DPxMOD \"\\n\", DPxPTR(rc));\n  return rc;\n}\n\nEXTERN void omp_target_free(void *device_ptr, int device_num) {\n  DP(\"Call to omp_target_free for device %d and address \" DPxMOD \"\\n\",\n      device_num, DPxPTR(device_ptr));\n\n  if (!device_ptr) {\n    DP(\"Call to omp_target_free with NULL ptr\\n\");\n    return;\n  }\n\n  if (device_num == omp_get_initial_device()) {\n    free(device_ptr);\n    DP(\"omp_target_free deallocated host ptr\\n\");\n    return;\n  }\n\n  if (!device_is_ready(device_num)) {\n    DP(\"omp_target_free returns, nothing to do\\n\");\n    return;\n  }\n\n  DeviceTy &Device = Devices[device_num];\n  Device.RTL->data_delete(Device.RTLDeviceID, (void *)device_ptr);\n  DP(\"omp_target_free deallocated device ptr\\n\");\n}\n\nEXTERN int omp_target_is_present(void *ptr, int device_num) {\n  DP(\"Call to omp_target_is_present for device %d and address \" DPxMOD \"\\n\",\n      device_num, DPxPTR(ptr));\n\n  if (!ptr) {\n    DP(\"Call to omp_target_is_present with NULL ptr, returning false\\n\");\n    return false;\n  }\n\n  if (device_num == omp_get_initial_device()) {\n    DP(\"Call to omp_target_is_present on host, returning true\\n\");\n    return true;\n  }\n\n  RTLsMtx.lock();\n  size_t Devices_size = Devices.size();\n  RTLsMtx.unlock();\n  if (Devices_size <= (size_t)device_num) {\n    DP(\"Call to omp_target_is_present with invalid device ID, returning \"\n        \"false\\n\");\n    return false;\n  }\n\n  DeviceTy& Device = Devices[device_num];\n  bool IsLast; // not used\n  int rc = (Device.getTgtPtrBegin(ptr, 0, IsLast, false) != NULL);\n  DP(\"Call to omp_target_is_present returns %d\\n\", rc);\n  return rc;\n}\n\nEXTERN int omp_target_memcpy(void *dst, void *src, size_t length,\n    size_t dst_offset, size_t src_offset, int dst_device, int src_device) {\n  DP(\"Call to omp_target_memcpy, dst device %d, src device %d, \"\n      \"dst addr \" DPxMOD \", src addr \" DPxMOD \", dst offset %zu, \"\n      \"src offset %zu, length %zu\\n\", dst_device, src_device, DPxPTR(dst),\n      DPxPTR(src), dst_offset, src_offset, length);\n\n  if (!dst || !src || length <= 0) {\n    DP(\"Call to omp_target_memcpy with invalid arguments\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  if (src_device != omp_get_initial_device() && !device_is_ready(src_device)) {\n      DP(\"omp_target_memcpy returns OFFLOAD_FAIL\\n\");\n      return OFFLOAD_FAIL;\n  }\n\n  if (dst_device != omp_get_initial_device() && !device_is_ready(dst_device)) {\n      DP(\"omp_target_memcpy returns OFFLOAD_FAIL\\n\");\n      return OFFLOAD_FAIL;\n  }\n\n  int rc = OFFLOAD_SUCCESS;\n  void *srcAddr = (char *)src + src_offset;\n  void *dstAddr = (char *)dst + dst_offset;\n\n  if (src_device == omp_get_initial_device() &&\n      dst_device == omp_get_initial_device()) {\n    DP(\"copy from host to host\\n\");\n    const void *p = memcpy(dstAddr, srcAddr, length);\n    if (p == NULL)\n      rc = OFFLOAD_FAIL;\n  } else if (src_device == omp_get_initial_device()) {\n    DP(\"copy from host to device\\n\");\n    DeviceTy& DstDev = Devices[dst_device];\n    rc = DstDev.data_submit(dstAddr, srcAddr, length);\n  } else if (dst_device == omp_get_initial_device()) {\n    DP(\"copy from device to host\\n\");\n    DeviceTy& SrcDev = Devices[src_device];\n    rc = SrcDev.data_retrieve(dstAddr, srcAddr, length);\n  } else {\n    DP(\"copy from device to device\\n\");\n    void *buffer = malloc(length);\n    DeviceTy& SrcDev = Devices[src_device];\n    DeviceTy& DstDev = Devices[dst_device];\n    rc = SrcDev.data_retrieve(buffer, srcAddr, length);\n    if (rc == OFFLOAD_SUCCESS)\n      rc = DstDev.data_submit(dstAddr, buffer, length);\n  }\n\n  DP(\"omp_target_memcpy returns %d\\n\", rc);\n  return rc;\n}\n\nEXTERN int omp_target_memcpy_rect(void *dst, void *src, size_t element_size,\n    int num_dims, const size_t *volume, const size_t *dst_offsets,\n    const size_t *src_offsets, const size_t *dst_dimensions,\n    const size_t *src_dimensions, int dst_device, int src_device) {\n  DP(\"Call to omp_target_memcpy_rect, dst device %d, src device %d, \"\n      \"dst addr \" DPxMOD \", src addr \" DPxMOD \", dst offsets \" DPxMOD \", \"\n      \"src offsets \" DPxMOD \", dst dims \" DPxMOD \", src dims \" DPxMOD \", \"\n      \"volume \" DPxMOD \", element size %zu, num_dims %d\\n\", dst_device,\n      src_device, DPxPTR(dst), DPxPTR(src), DPxPTR(dst_offsets),\n      DPxPTR(src_offsets), DPxPTR(dst_dimensions), DPxPTR(src_dimensions),\n      DPxPTR(volume), element_size, num_dims);\n\n  if (!(dst || src)) {\n    DP(\"Call to omp_target_memcpy_rect returns max supported dimensions %d\\n\",\n        INT_MAX);\n    return INT_MAX;\n  }\n\n  if (!dst || !src || element_size < 1 || num_dims < 1 || !volume ||\n      !dst_offsets || !src_offsets || !dst_dimensions || !src_dimensions) {\n    DP(\"Call to omp_target_memcpy_rect with invalid arguments\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  int rc;\n  if (num_dims == 1) {\n    rc = omp_target_memcpy(dst, src, element_size * volume[0],\n        element_size * dst_offsets[0], element_size * src_offsets[0],\n        dst_device, src_device);\n  } else {\n    size_t dst_slice_size = element_size;\n    size_t src_slice_size = element_size;\n    for (int i=1; i<num_dims; ++i) {\n      dst_slice_size *= dst_dimensions[i];\n      src_slice_size *= src_dimensions[i];\n    }\n\n    size_t dst_off = dst_offsets[0] * dst_slice_size;\n    size_t src_off = src_offsets[0] * src_slice_size;\n    for (size_t i=0; i<volume[0]; ++i) {\n      rc = omp_target_memcpy_rect((char *) dst + dst_off + dst_slice_size * i,\n          (char *) src + src_off + src_slice_size * i, element_size,\n          num_dims - 1, volume + 1, dst_offsets + 1, src_offsets + 1,\n          dst_dimensions + 1, src_dimensions + 1, dst_device, src_device);\n\n      if (rc) {\n        DP(\"Recursive call to omp_target_memcpy_rect returns unsuccessfully\\n\");\n        return rc;\n      }\n    }\n  }\n\n  DP(\"omp_target_memcpy_rect returns %d\\n\", rc);\n  return rc;\n}\n\nEXTERN int omp_target_associate_ptr(void *host_ptr, void *device_ptr,\n    size_t size, size_t device_offset, int device_num) {\n  DP(\"Call to omp_target_associate_ptr with host_ptr \" DPxMOD \", \"\n      \"device_ptr \" DPxMOD \", size %zu, device_offset %zu, device_num %d\\n\",\n      DPxPTR(host_ptr), DPxPTR(device_ptr), size, device_offset, device_num);\n\n  if (!host_ptr || !device_ptr || size <= 0) {\n    DP(\"Call to omp_target_associate_ptr with invalid arguments\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  if (device_num == omp_get_initial_device()) {\n    DP(\"omp_target_associate_ptr: no association possible on the host\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  if (!device_is_ready(device_num)) {\n    DP(\"omp_target_associate_ptr returns OFFLOAD_FAIL\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  DeviceTy& Device = Devices[device_num];\n  void *device_addr = (void *)((uint64_t)device_ptr + (uint64_t)device_offset);\n  int rc = Device.associatePtr(host_ptr, device_addr, size);\n  DP(\"omp_target_associate_ptr returns %d\\n\", rc);\n  return rc;\n}\n\nEXTERN int omp_target_disassociate_ptr(void *host_ptr, int device_num) {\n  DP(\"Call to omp_target_disassociate_ptr with host_ptr \" DPxMOD \", \"\n      \"device_num %d\\n\", DPxPTR(host_ptr), device_num);\n\n  if (!host_ptr) {\n    DP(\"Call to omp_target_associate_ptr with invalid host_ptr\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  if (device_num == omp_get_initial_device()) {\n    DP(\"omp_target_disassociate_ptr: no association possible on the host\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  if (!device_is_ready(device_num)) {\n    DP(\"omp_target_disassociate_ptr returns OFFLOAD_FAIL\\n\");\n    return OFFLOAD_FAIL;\n  }\n\n  DeviceTy& Device = Devices[device_num];\n  int rc = Device.disassociatePtr(host_ptr);\n  DP(\"omp_target_disassociate_ptr returns %d\\n\", rc);\n  return rc;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// functionality for device\n\nint DeviceTy::associatePtr(void *HstPtrBegin, void *TgtPtrBegin, int64_t Size) {\n  DataMapMtx.lock();\n\n  // Check if entry exists\n  for (auto &HT : HostDataToTargetMap) {\n    if ((uintptr_t)HstPtrBegin == HT.HstPtrBegin) {\n      // Mapping already exists\n      bool isValid = HT.HstPtrBegin == (uintptr_t) HstPtrBegin &&\n                     HT.HstPtrEnd == (uintptr_t) HstPtrBegin + Size &&\n                     HT.TgtPtrBegin == (uintptr_t) TgtPtrBegin;\n      DataMapMtx.unlock();\n      if (isValid) {\n        DP(\"Attempt to re-associate the same device ptr+offset with the same \"\n            \"host ptr, nothing to do\\n\");\n        return OFFLOAD_SUCCESS;\n      } else {\n        DP(\"Not allowed to re-associate a different device ptr+offset with the \"\n            \"same host ptr\\n\");\n        return OFFLOAD_FAIL;\n      }\n    }\n  }\n\n  // Mapping does not exist, allocate it\n  HostDataToTargetTy newEntry;\n\n  // Set up missing fields\n  newEntry.HstPtrBase = (uintptr_t) HstPtrBegin;\n  newEntry.HstPtrBegin = (uintptr_t) HstPtrBegin;\n  newEntry.HstPtrEnd = (uintptr_t) HstPtrBegin + Size;\n  newEntry.TgtPtrBegin = (uintptr_t) TgtPtrBegin;\n  // refCount must be infinite\n  newEntry.RefCount = INF_REF_CNT;\n\n  DP(\"Creating new map entry: HstBase=\" DPxMOD \", HstBegin=\" DPxMOD \", HstEnd=\"\n      DPxMOD \", TgtBegin=\" DPxMOD \"\\n\", DPxPTR(newEntry.HstPtrBase),\n      DPxPTR(newEntry.HstPtrBegin), DPxPTR(newEntry.HstPtrEnd),\n      DPxPTR(newEntry.TgtPtrBegin));\n  HostDataToTargetMap.push_front(newEntry);\n\n  DataMapMtx.unlock();\n\n  return OFFLOAD_SUCCESS;\n}\n\nint DeviceTy::disassociatePtr(void *HstPtrBegin) {\n  DataMapMtx.lock();\n\n  // Check if entry exists\n  for (HostDataToTargetListTy::iterator ii = HostDataToTargetMap.begin();\n      ii != HostDataToTargetMap.end(); ++ii) {\n    if ((uintptr_t)HstPtrBegin == ii->HstPtrBegin) {\n      // Mapping exists\n      if (CONSIDERED_INF(ii->RefCount)) {\n        DP(\"Association found, removing it\\n\");\n        HostDataToTargetMap.erase(ii);\n        DataMapMtx.unlock();\n        return OFFLOAD_SUCCESS;\n      } else {\n        DP(\"Trying to disassociate a pointer which was not mapped via \"\n            \"omp_target_associate_ptr\\n\");\n        break;\n      }\n    }\n  }\n\n  // Mapping not found\n  DataMapMtx.unlock();\n  DP(\"Association not found\\n\");\n  return OFFLOAD_FAIL;\n}\n\n// Get ref count of map entry containing HstPtrBegin\nlong DeviceTy::getMapEntryRefCnt(void *HstPtrBegin) {\n  uintptr_t hp = (uintptr_t)HstPtrBegin;\n  long RefCnt = -1;\n\n  DataMapMtx.lock();\n  for (auto &HT : HostDataToTargetMap) {\n    if (hp >= HT.HstPtrBegin && hp < HT.HstPtrEnd) {\n      DP(\"DeviceTy::getMapEntry: requested entry found\\n\");\n      RefCnt = HT.RefCount;\n      break;\n    }\n  }\n  DataMapMtx.unlock();\n\n  if (RefCnt < 0) {\n    DP(\"DeviceTy::getMapEntry: requested entry not found\\n\");\n  }\n\n  return RefCnt;\n}\n\nLookupResult DeviceTy::lookupMapping(void *HstPtrBegin, int64_t Size) {\n  uintptr_t hp = (uintptr_t)HstPtrBegin;\n  LookupResult lr;\n\n  DP(\"Looking up mapping(HstPtrBegin=\" DPxMOD \", Size=%ld)...\\n\", DPxPTR(hp),\n      Size);\n  for (lr.Entry = HostDataToTargetMap.begin();\n      lr.Entry != HostDataToTargetMap.end(); ++lr.Entry) {\n    auto &HT = *lr.Entry;\n    // Is it contained?\n    lr.Flags.IsContained = hp >= HT.HstPtrBegin && hp < HT.HstPtrEnd &&\n        (hp+Size) <= HT.HstPtrEnd;\n    // Does it extend into an already mapped region?\n    lr.Flags.ExtendsBefore = hp < HT.HstPtrBegin && (hp+Size) > HT.HstPtrBegin;\n    // Does it extend beyond the mapped region?\n    lr.Flags.ExtendsAfter = hp < HT.HstPtrEnd && (hp+Size) > HT.HstPtrEnd;\n\n    if (lr.Flags.IsContained || lr.Flags.ExtendsBefore ||\n        lr.Flags.ExtendsAfter) {\n      break;\n    }\n  }\n\n  if (lr.Flags.ExtendsBefore) {\n    DP(\"WARNING: Pointer is not mapped but section extends into already \"\n        \"mapped data\\n\");\n  }\n  if (lr.Flags.ExtendsAfter) {\n    DP(\"WARNING: Pointer is already mapped but section extends beyond mapped \"\n        \"region\\n\");\n  }\n\n  return lr;\n}\n\n// Used by target_data_begin\n// Return the target pointer begin (where the data will be moved).\n// Allocate memory if this is the first occurrence if this mapping.\n// Increment the reference counter.\n// If NULL is returned, then either data allocation failed or the user tried\n// to do an illegal mapping.\nvoid *DeviceTy::getOrAllocTgtPtr(void *HstPtrBegin, void *HstPtrBase,\n    int64_t Size, bool &IsNew, bool IsImplicit, bool UpdateRefCount) {\n  void *rc = NULL;\n  DataMapMtx.lock();\n  LookupResult lr = lookupMapping(HstPtrBegin, Size);\n\n  // Check if the pointer is contained.\n  if (lr.Flags.IsContained ||\n      ((lr.Flags.ExtendsBefore || lr.Flags.ExtendsAfter) && IsImplicit)) {\n    auto &HT = *lr.Entry;\n    IsNew = false;\n\n    if (UpdateRefCount)\n      ++HT.RefCount;\n\n    uintptr_t tp = HT.TgtPtrBegin + ((uintptr_t)HstPtrBegin - HT.HstPtrBegin);\n    DP(\"Mapping exists%s with HstPtrBegin=\" DPxMOD \", TgtPtrBegin=\" DPxMOD \", \"\n        \"Size=%ld,%s RefCount=%s\\n\", (IsImplicit ? \" (implicit)\" : \"\"),\n        DPxPTR(HstPtrBegin), DPxPTR(tp), Size,\n        (UpdateRefCount ? \" updated\" : \"\"),\n        (CONSIDERED_INF(HT.RefCount)) ? \"INF\" :\n            std::to_string(HT.RefCount).c_str());\n    rc = (void *)tp;\n  } else if ((lr.Flags.ExtendsBefore || lr.Flags.ExtendsAfter) && !IsImplicit) {\n    // Explicit extension of mapped data - not allowed.\n    DP(\"Explicit extension of mapping is not allowed.\\n\");\n  } else if (Size) {\n    // If it is not contained and Size > 0 we should create a new entry for it.\n    IsNew = true;\n    uintptr_t tp = (uintptr_t)RTL->data_alloc(RTLDeviceID, Size, HstPtrBegin);\n    DP(\"Creating new map entry: HstBase=\" DPxMOD \", HstBegin=\" DPxMOD \", \"\n        \"HstEnd=\" DPxMOD \", TgtBegin=\" DPxMOD \"\\n\", DPxPTR(HstPtrBase),\n        DPxPTR(HstPtrBegin), DPxPTR((uintptr_t)HstPtrBegin + Size), DPxPTR(tp));\n    HostDataToTargetMap.push_front(HostDataToTargetTy((uintptr_t)HstPtrBase,\n        (uintptr_t)HstPtrBegin, (uintptr_t)HstPtrBegin + Size, tp));\n    rc = (void *)tp;\n  }\n\n  DataMapMtx.unlock();\n  return rc;\n}\n\n// Used by target_data_begin, target_data_end, target_data_update and target.\n// Return the target pointer begin (where the data will be moved).\n// Decrement the reference counter if called from target_data_end.\nvoid *DeviceTy::getTgtPtrBegin(void *HstPtrBegin, int64_t Size, bool &IsLast,\n    bool UpdateRefCount) {\n  void *rc = NULL;\n  DataMapMtx.lock();\n  LookupResult lr = lookupMapping(HstPtrBegin, Size);\n\n  if (lr.Flags.IsContained || lr.Flags.ExtendsBefore || lr.Flags.ExtendsAfter) {\n    auto &HT = *lr.Entry;\n    IsLast = !(HT.RefCount > 1);\n\n    if (HT.RefCount > 1 && UpdateRefCount)\n      --HT.RefCount;\n\n    uintptr_t tp = HT.TgtPtrBegin + ((uintptr_t)HstPtrBegin - HT.HstPtrBegin);\n    DP(\"Mapping exists with HstPtrBegin=\" DPxMOD \", TgtPtrBegin=\" DPxMOD \", \"\n        \"Size=%ld,%s RefCount=%s\\n\", DPxPTR(HstPtrBegin), DPxPTR(tp), Size,\n        (UpdateRefCount ? \" updated\" : \"\"),\n        (CONSIDERED_INF(HT.RefCount)) ? \"INF\" :\n            std::to_string(HT.RefCount).c_str());\n    rc = (void *)tp;\n  } else {\n    IsLast = false;\n  }\n\n  DataMapMtx.unlock();\n  return rc;\n}\n\n// Return the target pointer begin (where the data will be moved).\n// Lock-free version called when loading global symbols from the fat binary.\nvoid *DeviceTy::getTgtPtrBegin(void *HstPtrBegin, int64_t Size) {\n  uintptr_t hp = (uintptr_t)HstPtrBegin;\n  LookupResult lr = lookupMapping(HstPtrBegin, Size);\n  if (lr.Flags.IsContained || lr.Flags.ExtendsBefore || lr.Flags.ExtendsAfter) {\n    auto &HT = *lr.Entry;\n    uintptr_t tp = HT.TgtPtrBegin + (hp - HT.HstPtrBegin);\n    return (void *)tp;\n  }\n\n  return NULL;\n}\n\nint DeviceTy::deallocTgtPtr(void *HstPtrBegin, int64_t Size, bool ForceDelete) {\n  // Check if the pointer is contained in any sub-nodes.\n  int rc;\n  DataMapMtx.lock();\n  LookupResult lr = lookupMapping(HstPtrBegin, Size);\n  if (lr.Flags.IsContained || lr.Flags.ExtendsBefore || lr.Flags.ExtendsAfter) {\n    auto &HT = *lr.Entry;\n    if (ForceDelete)\n      HT.RefCount = 1;\n    if (--HT.RefCount <= 0) {\n      assert(HT.RefCount == 0 && \"did not expect a negative ref count\");\n      DP(\"Deleting tgt data \" DPxMOD \" of size %ld\\n\",\n          DPxPTR(HT.TgtPtrBegin), Size);\n      RTL->data_delete(RTLDeviceID, (void *)HT.TgtPtrBegin);\n      DP(\"Removing%s mapping with HstPtrBegin=\" DPxMOD \", TgtPtrBegin=\" DPxMOD\n          \", Size=%ld\\n\", (ForceDelete ? \" (forced)\" : \"\"),\n          DPxPTR(HT.HstPtrBegin), DPxPTR(HT.TgtPtrBegin), Size);\n      HostDataToTargetMap.erase(lr.Entry);\n    }\n    rc = OFFLOAD_SUCCESS;\n  } else {\n    DP(\"Section to delete (hst addr \" DPxMOD \") does not exist in the allocated\"\n       \" memory\\n\", DPxPTR(HstPtrBegin));\n    rc = OFFLOAD_FAIL;\n  }\n\n  DataMapMtx.unlock();\n  return rc;\n}\n\n/// Init device, should not be called directly.\nvoid DeviceTy::init() {\n  int32_t rc = RTL->init_device(RTLDeviceID);\n  if (rc == OFFLOAD_SUCCESS) {\n    IsInit = true;\n  }\n}\n\n/// Thread-safe method to initialize the device only once.\nint32_t DeviceTy::initOnce() {\n  std::call_once(InitFlag, &DeviceTy::init, this);\n\n  // At this point, if IsInit is true, then either this thread or some other\n  // thread in the past successfully initialized the device, so we can return\n  // OFFLOAD_SUCCESS. If this thread executed init() via call_once() and it\n  // failed, return OFFLOAD_FAIL. If call_once did not invoke init(), it means\n  // that some other thread already attempted to execute init() and if IsInit\n  // is still false, return OFFLOAD_FAIL.\n  if (IsInit)\n    return OFFLOAD_SUCCESS;\n  else\n    return OFFLOAD_FAIL;\n}\n\n// Load binary to device.\n__tgt_target_table *DeviceTy::load_binary(void *Img) {\n  RTL->Mtx.lock();\n  __tgt_target_table *rc = RTL->load_binary(RTLDeviceID, Img);\n  RTL->Mtx.unlock();\n  return rc;\n}\n\n// Submit data to device.\nint32_t DeviceTy::data_submit(void *TgtPtrBegin, void *HstPtrBegin,\n    int64_t Size) {\n  return RTL->data_submit(RTLDeviceID, TgtPtrBegin, HstPtrBegin, Size);\n}\n\n// Retrieve data from device.\nint32_t DeviceTy::data_retrieve(void *HstPtrBegin, void *TgtPtrBegin,\n    int64_t Size) {\n  return RTL->data_retrieve(RTLDeviceID, HstPtrBegin, TgtPtrBegin, Size);\n}\n\n// Run region on device\nint32_t DeviceTy::run_region(void *TgtEntryPtr, void **TgtVarsPtr,\n    ptrdiff_t *TgtOffsets, int32_t TgtVarsSize) {\n  return RTL->run_region(RTLDeviceID, TgtEntryPtr, TgtVarsPtr, TgtOffsets,\n      TgtVarsSize);\n}\n\n// Run team region on device.\nint32_t DeviceTy::run_team_region(void *TgtEntryPtr, void **TgtVarsPtr,\n    ptrdiff_t *TgtOffsets, int32_t TgtVarsSize, int32_t NumTeams,\n    int32_t ThreadLimit, uint64_t LoopTripCount) {\n  return RTL->run_team_region(RTLDeviceID, TgtEntryPtr, TgtVarsPtr, TgtOffsets,\n      TgtVarsSize, NumTeams, ThreadLimit, LoopTripCount);\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Functionality for registering libs\n\nstatic void RegisterImageIntoTranslationTable(TranslationTable &TT,\n    RTLInfoTy &RTL, __tgt_device_image *image) {\n\n  // same size, as when we increase one, we also increase the other.\n  assert(TT.TargetsTable.size() == TT.TargetsImages.size() &&\n         \"We should have as many images as we have tables!\");\n\n  // Resize the Targets Table and Images to accommodate the new targets if\n  // required\n  unsigned TargetsTableMinimumSize = RTL.Idx + RTL.NumberOfDevices;\n\n  if (TT.TargetsTable.size() < TargetsTableMinimumSize) {\n    TT.TargetsImages.resize(TargetsTableMinimumSize, 0);\n    TT.TargetsTable.resize(TargetsTableMinimumSize, 0);\n  }\n\n  // Register the image in all devices for this target type.\n  for (int32_t i = 0; i < RTL.NumberOfDevices; ++i) {\n    // If we are changing the image we are also invalidating the target table.\n    if (TT.TargetsImages[RTL.Idx + i] != image) {\n      TT.TargetsImages[RTL.Idx + i] = image;\n      TT.TargetsTable[RTL.Idx + i] = 0; // lazy initialization of target table.\n    }\n  }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Functionality for registering Ctors/Dtors\n\nstatic void RegisterGlobalCtorsDtorsForImage(__tgt_bin_desc *desc,\n    __tgt_device_image *img, RTLInfoTy *RTL) {\n\n  for (int32_t i = 0; i < RTL->NumberOfDevices; ++i) {\n    DeviceTy &Device = Devices[RTL->Idx + i];\n    Device.PendingGlobalsMtx.lock();\n    Device.HasPendingGlobals = true;\n    for (__tgt_offload_entry *entry = img->EntriesBegin;\n        entry != img->EntriesEnd; ++entry) {\n      if (entry->flags & OMP_DECLARE_TARGET_CTOR) {\n        DP(\"Adding ctor \" DPxMOD \" to the pending list.\\n\",\n            DPxPTR(entry->addr));\n        Device.PendingCtorsDtors[desc].PendingCtors.push_back(entry->addr);\n      } else if (entry->flags & OMP_DECLARE_TARGET_DTOR) {\n        // Dtors are pushed in reverse order so they are executed from end\n        // to beginning when unregistering the library!\n        DP(\"Adding dtor \" DPxMOD \" to the pending list.\\n\",\n            DPxPTR(entry->addr));\n        Device.PendingCtorsDtors[desc].PendingDtors.push_front(entry->addr);\n      }\n\n      if (entry->flags & OMP_DECLARE_TARGET_LINK) {\n        DP(\"The \\\"link\\\" attribute is not yet supported!\\n\");\n      }\n    }\n    Device.PendingGlobalsMtx.unlock();\n  }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n/// adds a target shared library to the target execution image\nEXTERN void __tgt_register_lib(__tgt_bin_desc *desc) {\n\n  // Attempt to load all plugins available in the system.\n  RTLs.LoadRTLsOnce();\n\n  RTLsMtx.lock();\n  // Register the images with the RTLs that understand them, if any.\n  for (int32_t i = 0; i < desc->NumDeviceImages; ++i) {\n    // Obtain the image.\n    __tgt_device_image *img = &desc->DeviceImages[i];\n\n    RTLInfoTy *FoundRTL = NULL;\n\n    // Scan the RTLs that have associated images until we find one that supports\n    // the current image.\n    for (auto &R : RTLs.AllRTLs) {\n      if (!R.is_valid_binary(img)) {\n        DP(\"Image \" DPxMOD \" is NOT compatible with RTL %s!\\n\",\n            DPxPTR(img->ImageStart), R.RTLName.c_str());\n        continue;\n      }\n\n      DP(\"Image \" DPxMOD \" is compatible with RTL %s!\\n\",\n          DPxPTR(img->ImageStart), R.RTLName.c_str());\n\n      // If this RTL is not already in use, initialize it.\n      if (!R.isUsed) {\n        // Initialize the device information for the RTL we are about to use.\n        DeviceTy device(&R);\n\n        size_t start = Devices.size();\n        Devices.resize(start + R.NumberOfDevices, device);\n        for (int32_t device_id = 0; device_id < R.NumberOfDevices;\n            device_id++) {\n          // global device ID\n          Devices[start + device_id].DeviceID = start + device_id;\n          // RTL local device ID\n          Devices[start + device_id].RTLDeviceID = device_id;\n\n          // Save pointer to device in RTL in case we want to unregister the RTL\n          R.Devices.push_back(&Devices[start + device_id]);\n        }\n\n        // Initialize the index of this RTL and save it in the used RTLs.\n        R.Idx = (RTLs.UsedRTLs.empty())\n                    ? 0\n                    : RTLs.UsedRTLs.back()->Idx +\n                          RTLs.UsedRTLs.back()->NumberOfDevices;\n        assert((size_t) R.Idx == start &&\n            \"RTL index should equal the number of devices used so far.\");\n        R.isUsed = true;\n        RTLs.UsedRTLs.push_back(&R);\n\n        DP(\"RTL \" DPxMOD \" has index %d!\\n\", DPxPTR(R.LibraryHandler), R.Idx);\n      }\n\n      // Initialize (if necessary) translation table for this library.\n      TrlTblMtx.lock();\n      if(!HostEntriesBeginToTransTable.count(desc->HostEntriesBegin)){\n        TranslationTable &tt =\n            HostEntriesBeginToTransTable[desc->HostEntriesBegin];\n        tt.HostTable.EntriesBegin = desc->HostEntriesBegin;\n        tt.HostTable.EntriesEnd = desc->HostEntriesEnd;\n      }\n\n      // Retrieve translation table for this library.\n      TranslationTable &TransTable =\n          HostEntriesBeginToTransTable[desc->HostEntriesBegin];\n\n      DP(\"Registering image \" DPxMOD \" with RTL %s!\\n\",\n          DPxPTR(img->ImageStart), R.RTLName.c_str());\n      RegisterImageIntoTranslationTable(TransTable, R, img);\n      TrlTblMtx.unlock();\n      FoundRTL = &R;\n\n      // Load ctors/dtors for static objects\n      RegisterGlobalCtorsDtorsForImage(desc, img, FoundRTL);\n\n      // if an RTL was found we are done - proceed to register the next image\n      break;\n    }\n\n    if (!FoundRTL) {\n      DP(\"No RTL found for image \" DPxMOD \"!\\n\", DPxPTR(img->ImageStart));\n    }\n  }\n  RTLsMtx.unlock();\n\n\n  DP(\"Done registering entries!\\n\");\n}\n\n////////////////////////////////////////////////////////////////////////////////\n/// unloads a target shared library\nEXTERN void __tgt_unregister_lib(__tgt_bin_desc *desc) {\n  DP(\"Unloading target library!\\n\");\n\n  RTLsMtx.lock();\n  // Find which RTL understands each image, if any.\n  for (int32_t i = 0; i < desc->NumDeviceImages; ++i) {\n    // Obtain the image.\n    __tgt_device_image *img = &desc->DeviceImages[i];\n\n    RTLInfoTy *FoundRTL = NULL;\n\n    // Scan the RTLs that have associated images until we find one that supports\n    // the current image. We only need to scan RTLs that are already being used.\n    for (auto *R : RTLs.UsedRTLs) {\n\n      assert(R->isUsed && \"Expecting used RTLs.\");\n\n      if (!R->is_valid_binary(img)) {\n        DP(\"Image \" DPxMOD \" is NOT compatible with RTL \" DPxMOD \"!\\n\",\n            DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n        continue;\n      }\n\n      DP(\"Image \" DPxMOD \" is compatible with RTL \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n\n      FoundRTL = R;\n\n      // Execute dtors for static objects if the device has been used, i.e.\n      // if its PendingCtors list has been emptied.\n      for (int32_t i = 0; i < FoundRTL->NumberOfDevices; ++i) {\n        DeviceTy &Device = Devices[FoundRTL->Idx + i];\n        Device.PendingGlobalsMtx.lock();\n        if (Device.PendingCtorsDtors[desc].PendingCtors.empty()) {\n          for (auto &dtor : Device.PendingCtorsDtors[desc].PendingDtors) {\n            int rc = target(Device.DeviceID, dtor, 0, NULL, NULL, NULL, NULL, 1,\n                1, true /*team*/);\n            if (rc != OFFLOAD_SUCCESS) {\n              DP(\"Running destructor \" DPxMOD \" failed.\\n\", DPxPTR(dtor));\n            }\n          }\n          // Remove this library's entry from PendingCtorsDtors\n          Device.PendingCtorsDtors.erase(desc);\n        }\n        Device.PendingGlobalsMtx.unlock();\n      }\n\n      DP(\"Unregistered image \" DPxMOD \" from RTL \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart), DPxPTR(R->LibraryHandler));\n\n      break;\n    }\n\n    // if no RTL was found proceed to unregister the next image\n    if (!FoundRTL){\n      DP(\"No RTLs in use support the image \" DPxMOD \"!\\n\",\n          DPxPTR(img->ImageStart));\n    }\n  }\n  RTLsMtx.unlock();\n  DP(\"Done unregistering images!\\n\");\n\n  // Remove entries from HostPtrToTableMap\n  TblMapMtx.lock();\n  for (__tgt_offload_entry *cur = desc->HostEntriesBegin;\n      cur < desc->HostEntriesEnd; ++cur) {\n    HostPtrToTableMap.erase(cur->addr);\n  }\n\n  // Remove translation table for this descriptor.\n  auto tt = HostEntriesBeginToTransTable.find(desc->HostEntriesBegin);\n  if (tt != HostEntriesBeginToTransTable.end()) {\n    DP(\"Removing translation table for descriptor \" DPxMOD \"\\n\",\n        DPxPTR(desc->HostEntriesBegin));\n    HostEntriesBeginToTransTable.erase(tt);\n  } else {\n    DP(\"Translation table for descriptor \" DPxMOD \" cannot be found, probably \"\n        \"it has been already removed.\\n\", DPxPTR(desc->HostEntriesBegin));\n  }\n\n  TblMapMtx.unlock();\n\n  // TODO: Remove RTL and the devices it manages if it's not used anymore?\n  // TODO: Write some RTL->unload_image(...) function?\n\n  DP(\"Done unregistering library!\\n\");\n}\n\n/// Map global data and execute pending ctors\nstatic int InitLibrary(DeviceTy& Device) {\n  /*\n   * Map global data\n   */\n  int32_t device_id = Device.DeviceID;\n  int rc = OFFLOAD_SUCCESS;\n\n  Device.PendingGlobalsMtx.lock();\n  TrlTblMtx.lock();\n  for (HostEntriesBeginToTransTableTy::iterator\n      ii = HostEntriesBeginToTransTable.begin();\n      ii != HostEntriesBeginToTransTable.end(); ++ii) {\n    TranslationTable *TransTable = &ii->second;\n    if (TransTable->TargetsTable[device_id] != 0) {\n      // Library entries have already been processed\n      continue;\n    }\n\n    // 1) get image.\n    assert(TransTable->TargetsImages.size() > (size_t)device_id &&\n           \"Not expecting a device ID outside the table's bounds!\");\n    __tgt_device_image *img = TransTable->TargetsImages[device_id];\n    if (!img) {\n      DP(\"No image loaded for device id %d.\\n\", device_id);\n      rc = OFFLOAD_FAIL;\n      break;\n    }\n    // 2) load image into the target table.\n    __tgt_target_table *TargetTable =\n        TransTable->TargetsTable[device_id] = Device.load_binary(img);\n    // Unable to get table for this image: invalidate image and fail.\n    if (!TargetTable) {\n      DP(\"Unable to generate entries table for device id %d.\\n\", device_id);\n      TransTable->TargetsImages[device_id] = 0;\n      rc = OFFLOAD_FAIL;\n      break;\n    }\n\n    // Verify whether the two table sizes match.\n    size_t hsize =\n        TransTable->HostTable.EntriesEnd - TransTable->HostTable.EntriesBegin;\n    size_t tsize = TargetTable->EntriesEnd - TargetTable->EntriesBegin;\n\n    // Invalid image for these host entries!\n    if (hsize != tsize) {\n      DP(\"Host and Target tables mismatch for device id %d [%zx != %zx].\\n\",\n         device_id, hsize, tsize);\n      TransTable->TargetsImages[device_id] = 0;\n      TransTable->TargetsTable[device_id] = 0;\n      rc = OFFLOAD_FAIL;\n      break;\n    }\n\n    // process global data that needs to be mapped.\n    Device.DataMapMtx.lock();\n    __tgt_target_table *HostTable = &TransTable->HostTable;\n    for (__tgt_offload_entry *CurrDeviceEntry = TargetTable->EntriesBegin,\n                             *CurrHostEntry = HostTable->EntriesBegin,\n                             *EntryDeviceEnd = TargetTable->EntriesEnd;\n         CurrDeviceEntry != EntryDeviceEnd;\n         CurrDeviceEntry++, CurrHostEntry++) {\n      if (CurrDeviceEntry->size != 0) {\n        // has data.\n        assert(CurrDeviceEntry->size == CurrHostEntry->size &&\n               \"data size mismatch\");\n\n        // Fortran may use multiple weak declarations for the same symbol,\n        // therefore we must allow for multiple weak symbols to be loaded from\n        // the fat binary. Treat these mappings as any other \"regular\" mapping.\n        // Add entry to map.\n        if (Device.getTgtPtrBegin(CurrHostEntry->addr, CurrHostEntry->size))\n          continue;\n        DP(\"Add mapping from host \" DPxMOD \" to device \" DPxMOD \" with size %zu\"\n            \"\\n\", DPxPTR(CurrHostEntry->addr), DPxPTR(CurrDeviceEntry->addr),\n            CurrDeviceEntry->size);\n        Device.HostDataToTargetMap.push_front(HostDataToTargetTy(\n            (uintptr_t)CurrHostEntry->addr /*HstPtrBase*/,\n            (uintptr_t)CurrHostEntry->addr /*HstPtrBegin*/,\n            (uintptr_t)CurrHostEntry->addr + CurrHostEntry->size /*HstPtrEnd*/,\n            (uintptr_t)CurrDeviceEntry->addr /*TgtPtrBegin*/,\n            INF_REF_CNT /*RefCount*/));\n      }\n    }\n    Device.DataMapMtx.unlock();\n  }\n  TrlTblMtx.unlock();\n\n  if (rc != OFFLOAD_SUCCESS) {\n    Device.PendingGlobalsMtx.unlock();\n    return rc;\n  }\n\n  /*\n   * Run ctors for static objects\n   */\n  if (!Device.PendingCtorsDtors.empty()) {\n    // Call all ctors for all libraries registered so far\n    for (auto &lib : Device.PendingCtorsDtors) {\n      if (!lib.second.PendingCtors.empty()) {\n        DP(\"Has pending ctors... call now\\n\");\n        for (auto &entry : lib.second.PendingCtors) {\n          void *ctor = entry;\n          int rc = target(device_id, ctor, 0, NULL, NULL, NULL,\n                          NULL, 1, 1, true /*team*/);\n          if (rc != OFFLOAD_SUCCESS) {\n            DP(\"Running ctor \" DPxMOD \" failed.\\n\", DPxPTR(ctor));\n            Device.PendingGlobalsMtx.unlock();\n            return OFFLOAD_FAIL;\n          }\n        }\n        // Clear the list to indicate that this device has been used\n        lib.second.PendingCtors.clear();\n        DP(\"Done with pending ctors for lib \" DPxMOD \"\\n\", DPxPTR(lib.first));\n      }\n    }\n  }\n  Device.HasPendingGlobals = false;\n  Device.PendingGlobalsMtx.unlock();\n\n  return OFFLOAD_SUCCESS;\n}\n\n// Check whether a device has been initialized, global ctors have been\n// executed and global data has been mapped; do so if not already done.\nstatic int CheckDevice(int32_t device_id) {\n  // Is device ready?\n  if (!device_is_ready(device_id)) {\n    DP(\"Device %d is not ready.\\n\", device_id);\n    return OFFLOAD_FAIL;\n  }\n\n  // Get device info.\n  DeviceTy &Device = Devices[device_id];\n\n  // Check whether global data has been mapped for this device\n  Device.PendingGlobalsMtx.lock();\n  bool hasPendingGlobals = Device.HasPendingGlobals;\n  Device.PendingGlobalsMtx.unlock();\n  if (hasPendingGlobals && InitLibrary(Device) != OFFLOAD_SUCCESS) {\n    DP(\"Failed to init globals on device %d\\n\", device_id);\n    return OFFLOAD_FAIL;\n  }\n\n  return OFFLOAD_SUCCESS;\n}\n\n// Following datatypes and functions (tgt_oldmap_type, combined_entry_t,\n// translate_map, cleanup_map) will be removed once the compiler starts using\n// the new map types.\n\n// Old map types\nenum tgt_oldmap_type {\n  OMP_TGT_OLDMAPTYPE_TO          = 0x001, // copy data from host to device\n  OMP_TGT_OLDMAPTYPE_FROM        = 0x002, // copy data from device to host\n  OMP_TGT_OLDMAPTYPE_ALWAYS      = 0x004, // copy regardless of the ref. count\n  OMP_TGT_OLDMAPTYPE_DELETE      = 0x008, // force unmapping of data\n  OMP_TGT_OLDMAPTYPE_MAP_PTR     = 0x010, // map pointer as well as pointee\n  OMP_TGT_OLDMAPTYPE_FIRST_MAP   = 0x020, // first occurrence of mapped variable\n  OMP_TGT_OLDMAPTYPE_RETURN_PTR  = 0x040, // return TgtBase addr of mapped data\n  OMP_TGT_OLDMAPTYPE_PRIVATE_PTR = 0x080, // private variable - not mapped\n  OMP_TGT_OLDMAPTYPE_PRIVATE_VAL = 0x100  // copy by value - not mapped\n};\n\n// Temporary functions for map translation and cleanup\nstruct combined_entry_t {\n  int num_members; // number of members in combined entry\n  void *base_addr; // base address of combined entry\n  void *begin_addr; // begin address of combined entry\n  void *end_addr; // size of combined entry\n};\n\nstatic void translate_map(int32_t arg_num, void **args_base, void **args,\n    int64_t *arg_sizes, int64_t *arg_types, int32_t &new_arg_num,\n    void **&new_args_base, void **&new_args, int64_t *&new_arg_sizes,\n    int64_t *&new_arg_types, bool is_target_construct) {\n  if (arg_num <= 0) {\n    DP(\"Nothing to translate\\n\");\n    new_arg_num = 0;\n    return;\n  }\n\n  // array of combined entries\n  combined_entry_t *cmb_entries =\n      (combined_entry_t *) alloca(arg_num * sizeof(combined_entry_t));\n  // number of combined entries\n  long num_combined = 0;\n  // old entry is MAP_PTR?\n  bool *is_ptr_old = (bool *) alloca(arg_num * sizeof(bool));\n  // old entry is member of member_of[old] cmb_entry\n  int *member_of = (int *) alloca(arg_num * sizeof(int));\n  // temporary storage for modifications of the original arg_types\n  int64_t *mod_arg_types = (int64_t *) alloca(arg_num  *sizeof(int64_t));\n\n  DP(\"Translating %d map entries\\n\", arg_num);\n  for (int i = 0; i < arg_num; ++i) {\n    member_of[i] = -1;\n    is_ptr_old[i] = false;\n    mod_arg_types[i] = arg_types[i];\n    // Scan previous entries to see whether this entry shares the same base\n    for (int j = 0; j < i; ++j) {\n      void *new_begin_addr = NULL;\n      void *new_end_addr = NULL;\n\n      if (mod_arg_types[i] & OMP_TGT_OLDMAPTYPE_MAP_PTR) {\n        if (args_base[i] == args[j]) {\n          if (!(mod_arg_types[j] & OMP_TGT_OLDMAPTYPE_MAP_PTR)) {\n            DP(\"Entry %d has the same base as entry %d's begin address\\n\", i,\n                j);\n            new_begin_addr = args_base[i];\n            new_end_addr = (char *)args_base[i] + sizeof(void *);\n            assert(arg_sizes[j] == sizeof(void *));\n            is_ptr_old[j] = true;\n          } else {\n            DP(\"Entry %d has the same base as entry %d's begin address, but \"\n                \"%d's base was a MAP_PTR too\\n\", i, j, j);\n            int32_t to_from_always_delete =\n                OMP_TGT_OLDMAPTYPE_TO | OMP_TGT_OLDMAPTYPE_FROM |\n                OMP_TGT_OLDMAPTYPE_ALWAYS | OMP_TGT_OLDMAPTYPE_DELETE;\n            if (mod_arg_types[j] & to_from_always_delete) {\n              DP(\"Resetting to/from/always/delete flags for entry %d because \"\n                  \"it is only a pointer to pointer\\n\", j);\n              mod_arg_types[j] &= ~to_from_always_delete;\n            }\n          }\n        }\n      } else {\n        if (!(mod_arg_types[i] & OMP_TGT_OLDMAPTYPE_FIRST_MAP) &&\n            args_base[i] == args_base[j]) {\n          DP(\"Entry %d has the same base address as entry %d\\n\", i, j);\n          new_begin_addr = args[i];\n          new_end_addr = (char *)args[i] + arg_sizes[i];\n        }\n      }\n\n      // If we have combined the entry with a previous one\n      if (new_begin_addr) {\n        int id;\n        if(member_of[j] == -1) {\n          // We have a new entry\n          id = num_combined++;\n          DP(\"Creating new combined entry %d for old entry %d\\n\", id, j);\n          // Initialize new entry\n          cmb_entries[id].num_members = 1;\n          cmb_entries[id].base_addr = args_base[j];\n          if (mod_arg_types[j] & OMP_TGT_OLDMAPTYPE_MAP_PTR) {\n            cmb_entries[id].begin_addr = args_base[j];\n            cmb_entries[id].end_addr = (char *)args_base[j] + arg_sizes[j];\n          } else {\n            cmb_entries[id].begin_addr = args[j];\n            cmb_entries[id].end_addr = (char *)args[j] + arg_sizes[j];\n          }\n          member_of[j] = id;\n        } else {\n          // Reuse existing combined entry\n          DP(\"Reusing existing combined entry %d\\n\", member_of[j]);\n          id = member_of[j];\n        }\n\n        // Update combined entry\n        DP(\"Adding entry %d to combined entry %d\\n\", i, id);\n        cmb_entries[id].num_members++;\n        // base_addr stays the same\n        cmb_entries[id].begin_addr =\n            std::min(cmb_entries[id].begin_addr, new_begin_addr);\n        cmb_entries[id].end_addr =\n            std::max(cmb_entries[id].end_addr, new_end_addr);\n        member_of[i] = id;\n        break;\n      }\n    }\n  }\n\n  DP(\"New entries: %ld combined + %d original\\n\", num_combined, arg_num);\n  new_arg_num = arg_num + num_combined;\n  new_args_base = (void **) malloc(new_arg_num * sizeof(void *));\n  new_args = (void **) malloc(new_arg_num * sizeof(void *));\n  new_arg_sizes = (int64_t *) malloc(new_arg_num * sizeof(int64_t));\n  new_arg_types = (int64_t *) malloc(new_arg_num * sizeof(int64_t));\n\n  const int64_t alignment = 8;\n\n  int next_id = 0; // next ID\n  int next_cid = 0; // next combined ID\n  int *combined_to_new_id = (int *) alloca(num_combined * sizeof(int));\n  for (int i = 0; i < arg_num; ++i) {\n    // It is member_of\n    if (member_of[i] == next_cid) {\n      int cid = next_cid++; // ID of this combined entry\n      int nid = next_id++; // ID of the new (global) entry\n      combined_to_new_id[cid] = nid;\n      DP(\"Combined entry %3d will become new entry %3d\\n\", cid, nid);\n\n      int64_t padding = (int64_t)cmb_entries[cid].begin_addr % alignment;\n      if (padding) {\n        DP(\"Using a padding of %\" PRId64 \" for begin address \" DPxMOD \"\\n\",\n            padding, DPxPTR(cmb_entries[cid].begin_addr));\n        cmb_entries[cid].begin_addr =\n            (char *)cmb_entries[cid].begin_addr - padding;\n      }\n\n      new_args_base[nid] = cmb_entries[cid].base_addr;\n      new_args[nid] = cmb_entries[cid].begin_addr;\n      new_arg_sizes[nid] = (int64_t) ((char *)cmb_entries[cid].end_addr -\n          (char *)cmb_entries[cid].begin_addr);\n      new_arg_types[nid] = OMP_TGT_MAPTYPE_TARGET_PARAM;\n      DP(\"Entry %3d: base_addr \" DPxMOD \", begin_addr \" DPxMOD \", \"\n          \"size %\" PRId64 \", type 0x%\" PRIx64 \"\\n\", nid,\n          DPxPTR(new_args_base[nid]), DPxPTR(new_args[nid]), new_arg_sizes[nid],\n          new_arg_types[nid]);\n    } else if (member_of[i] != -1) {\n      DP(\"Combined entry %3d has been encountered before, do nothing\\n\",\n          member_of[i]);\n    }\n\n    // Now that the combined entry (the one the old entry was a member of) has\n    // been inserted into the new arguments list, proceed with the old entry.\n    int nid = next_id++;\n    DP(\"Old entry %3d will become new entry %3d\\n\", i, nid);\n\n    new_args_base[nid] = args_base[i];\n    new_args[nid] = args[i];\n    new_arg_sizes[nid] = arg_sizes[i];\n    int64_t old_type = mod_arg_types[i];\n\n    if (is_ptr_old[i]) {\n      // Reset TO and FROM flags\n      old_type &= ~(OMP_TGT_OLDMAPTYPE_TO | OMP_TGT_OLDMAPTYPE_FROM);\n    }\n\n    if (member_of[i] == -1) {\n      if (!is_target_construct)\n        old_type &= ~OMP_TGT_MAPTYPE_TARGET_PARAM;\n      new_arg_types[nid] = old_type;\n      DP(\"Entry %3d: base_addr \" DPxMOD \", begin_addr \" DPxMOD \", size %\" PRId64\n          \", type 0x%\" PRIx64 \" (old entry %d not MEMBER_OF)\\n\", nid,\n          DPxPTR(new_args_base[nid]), DPxPTR(new_args[nid]), new_arg_sizes[nid],\n          new_arg_types[nid], i);\n    } else {\n      // Old entry is not FIRST_MAP\n      old_type &= ~OMP_TGT_OLDMAPTYPE_FIRST_MAP;\n      // Add MEMBER_OF\n      int new_member_of = combined_to_new_id[member_of[i]];\n      old_type |= ((int64_t)new_member_of + 1) << 48;\n      new_arg_types[nid] = old_type;\n      DP(\"Entry %3d: base_addr \" DPxMOD \", begin_addr \" DPxMOD \", size %\" PRId64\n        \", type 0x%\" PRIx64 \" (old entry %d MEMBER_OF %d)\\n\", nid,\n        DPxPTR(new_args_base[nid]), DPxPTR(new_args[nid]), new_arg_sizes[nid],\n        new_arg_types[nid], i, new_member_of);\n    }\n  }\n}\n\nstatic void cleanup_map(int32_t new_arg_num, void **new_args_base,\n    void **new_args, int64_t *new_arg_sizes, int64_t *new_arg_types,\n    int32_t arg_num, void **args_base) {\n  if (new_arg_num > 0) {\n    int offset = new_arg_num - arg_num;\n    for (int32_t i = 0; i < arg_num; ++i) {\n      // Restore old base address\n      args_base[i] = new_args_base[i+offset];\n    }\n    free(new_args_base);\n    free(new_args);\n    free(new_arg_sizes);\n    free(new_arg_types);\n  }\n}\n\nstatic short member_of(int64_t type) {\n  return ((type & OMP_TGT_MAPTYPE_MEMBER_OF) >> 48) - 1;\n}\n\n/// Internal function to do the mapping and transfer the data to the device\nstatic int target_data_begin(DeviceTy &Device, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types) {\n  // process each input.\n  int rc = OFFLOAD_SUCCESS;\n  for (int32_t i = 0; i < arg_num; ++i) {\n    // Ignore private variables and arrays - there is no mapping for them.\n    if ((arg_types[i] & OMP_TGT_MAPTYPE_LITERAL) ||\n        (arg_types[i] & OMP_TGT_MAPTYPE_PRIVATE))\n      continue;\n\n    void *HstPtrBegin = args[i];\n    void *HstPtrBase = args_base[i];\n    // Address of pointer on the host and device, respectively.\n    void *Pointer_HstPtrBegin, *Pointer_TgtPtrBegin;\n    bool IsNew, Pointer_IsNew;\n    bool IsImplicit = arg_types[i] & OMP_TGT_MAPTYPE_IMPLICIT;\n    bool UpdateRef = !(arg_types[i] & OMP_TGT_MAPTYPE_MEMBER_OF);\n    if (arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ) {\n      DP(\"Has a pointer entry: \\n\");\n      // base is address of pointer.\n      Pointer_TgtPtrBegin = Device.getOrAllocTgtPtr(HstPtrBase, HstPtrBase,\n          sizeof(void *), Pointer_IsNew, IsImplicit, UpdateRef);\n      if (!Pointer_TgtPtrBegin) {\n        DP(\"Call to getOrAllocTgtPtr returned null pointer (device failure or \"\n            \"illegal mapping).\\n\");\n      }\n      DP(\"There are %zu bytes allocated at target address \" DPxMOD \" - is%s new\"\n          \"\\n\", sizeof(void *), DPxPTR(Pointer_TgtPtrBegin),\n          (Pointer_IsNew ? \"\" : \" not\"));\n      Pointer_HstPtrBegin = HstPtrBase;\n      // modify current entry.\n      HstPtrBase = *(void **)HstPtrBase;\n      UpdateRef = true; // subsequently update ref count of pointee\n    }\n\n    void *TgtPtrBegin = Device.getOrAllocTgtPtr(HstPtrBegin, HstPtrBase,\n        arg_sizes[i], IsNew, IsImplicit, UpdateRef);\n    if (!TgtPtrBegin && arg_sizes[i]) {\n      // If arg_sizes[i]==0, then the argument is a pointer to NULL, so\n      // getOrAlloc() returning NULL is not an error.\n      DP(\"Call to getOrAllocTgtPtr returned null pointer (device failure or \"\n          \"illegal mapping).\\n\");\n    }\n    DP(\"There are %\" PRId64 \" bytes allocated at target address \" DPxMOD\n        \" - is%s new\\n\", arg_sizes[i], DPxPTR(TgtPtrBegin),\n        (IsNew ? \"\" : \" not\"));\n\n    if (arg_types[i] & OMP_TGT_MAPTYPE_RETURN_PARAM) {\n      void *ret_ptr;\n      if (arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ)\n        ret_ptr = Pointer_TgtPtrBegin;\n      else {\n        bool IsLast; // not used\n        ret_ptr = Device.getTgtPtrBegin(HstPtrBegin, 0, IsLast, false);\n      }\n\n      DP(\"Returning device pointer \" DPxMOD \"\\n\", DPxPTR(ret_ptr));\n      args_base[i] = ret_ptr;\n    }\n\n    if (arg_types[i] & OMP_TGT_MAPTYPE_TO) {\n      bool copy = false;\n      if (IsNew || (arg_types[i] & OMP_TGT_MAPTYPE_ALWAYS)) {\n        copy = true;\n      } else if (arg_types[i] & OMP_TGT_MAPTYPE_MEMBER_OF) {\n        // Copy data only if the \"parent\" struct has RefCount==1.\n        short parent_idx = member_of(arg_types[i]);\n        long parent_rc = Device.getMapEntryRefCnt(args[parent_idx]);\n        assert(parent_rc > 0 && \"parent struct not found\");\n        if (parent_rc == 1) {\n          copy = true;\n        }\n      }\n\n      if (copy) {\n        DP(\"Moving %\" PRId64 \" bytes (hst:\" DPxMOD \") -> (tgt:\" DPxMOD \")\\n\",\n            arg_sizes[i], DPxPTR(HstPtrBegin), DPxPTR(TgtPtrBegin));\n        int rt = Device.data_submit(TgtPtrBegin, HstPtrBegin, arg_sizes[i]);\n        if (rt != OFFLOAD_SUCCESS) {\n          DP(\"Copying data to device failed.\\n\");\n          rc = OFFLOAD_FAIL;\n        }\n      }\n    }\n\n    if (arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ) {\n      DP(\"Update pointer (\" DPxMOD \") -> [\" DPxMOD \"]\\n\",\n          DPxPTR(Pointer_TgtPtrBegin), DPxPTR(TgtPtrBegin));\n      uint64_t Delta = (uint64_t)HstPtrBegin - (uint64_t)HstPtrBase;\n      void *TgtPtrBase = (void *)((uint64_t)TgtPtrBegin - Delta);\n      int rt = Device.data_submit(Pointer_TgtPtrBegin, &TgtPtrBase,\n          sizeof(void *));\n      if (rt != OFFLOAD_SUCCESS) {\n        DP(\"Copying data to device failed.\\n\");\n        rc = OFFLOAD_FAIL;\n      }\n      // create shadow pointers for this entry\n      Device.ShadowMtx.lock();\n      Device.ShadowPtrMap[Pointer_HstPtrBegin] = {HstPtrBase,\n          Pointer_TgtPtrBegin, TgtPtrBase};\n      Device.ShadowMtx.unlock();\n    }\n  }\n\n  return rc;\n}\n\nEXTERN void __tgt_target_data_begin_nowait(int64_t device_id, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types,\n    int32_t depNum, void *depList, int32_t noAliasDepNum,\n    void *noAliasDepList) {\n  if (depNum + noAliasDepNum > 0)\n    __kmpc_omp_taskwait(NULL, 0);\n\n  __tgt_target_data_begin(device_id, arg_num, args_base, args, arg_sizes,\n                          arg_types);\n}\n\n/// creates host-to-target data mapping, stores it in the\n/// libomptarget.so internal structure (an entry in a stack of data maps)\n/// and passes the data to the device.\nEXTERN void __tgt_target_data_begin(int64_t device_id, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types) {\n  DP(\"Entering data begin region for device %ld with %d mappings\\n\", device_id,\n     arg_num);\n\n  // No devices available?\n  if (device_id == OFFLOAD_DEVICE_DEFAULT) {\n    device_id = omp_get_default_device();\n    DP(\"Use default device id %ld\\n\", device_id);\n  }\n\n  if (CheckDevice(device_id) != OFFLOAD_SUCCESS) {\n    DP(\"Failed to get device %ld ready\\n\", device_id);\n    return;\n  }\n\n  DeviceTy& Device = Devices[device_id];\n\n  // Translate maps\n  int32_t new_arg_num;\n  void **new_args_base;\n  void **new_args;\n  int64_t *new_arg_sizes;\n  int64_t *new_arg_types;\n  translate_map(arg_num, args_base, args, arg_sizes, arg_types, new_arg_num,\n      new_args_base, new_args, new_arg_sizes, new_arg_types, false);\n\n  //target_data_begin(Device, arg_num, args_base, args, arg_sizes, arg_types);\n  target_data_begin(Device, new_arg_num, new_args_base, new_args, new_arg_sizes,\n      new_arg_types);\n\n  // Cleanup translation memory\n  cleanup_map(new_arg_num, new_args_base, new_args, new_arg_sizes,\n      new_arg_types, arg_num, args_base);\n}\n\n/// Internal function to undo the mapping and retrieve the data from the device.\nstatic int target_data_end(DeviceTy &Device, int32_t arg_num, void **args_base,\n    void **args, int64_t *arg_sizes, int64_t *arg_types) {\n  int rc = OFFLOAD_SUCCESS;\n  // process each input.\n  for (int32_t i = arg_num - 1; i >= 0; --i) {\n    // Ignore private variables and arrays - there is no mapping for them.\n    // Also, ignore the use_device_ptr directive, it has no effect here.\n    if ((arg_types[i] & OMP_TGT_MAPTYPE_LITERAL) ||\n        (arg_types[i] & OMP_TGT_MAPTYPE_PRIVATE))\n      continue;\n\n    void *HstPtrBegin = args[i];\n    bool IsLast;\n    bool UpdateRef = !(arg_types[i] & OMP_TGT_MAPTYPE_MEMBER_OF) ||\n        (arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ);\n    bool ForceDelete = arg_types[i] & OMP_TGT_MAPTYPE_DELETE;\n\n    // If PTR_AND_OBJ, HstPtrBegin is address of pointee\n    void *TgtPtrBegin = Device.getTgtPtrBegin(HstPtrBegin, arg_sizes[i], IsLast,\n        UpdateRef);\n    DP(\"There are %\" PRId64 \" bytes allocated at target address \" DPxMOD\n        \" - is%s last\\n\", arg_sizes[i], DPxPTR(TgtPtrBegin),\n        (IsLast ? \"\" : \" not\"));\n\n    bool DelEntry = IsLast || ForceDelete;\n\n    if ((arg_types[i] & OMP_TGT_MAPTYPE_MEMBER_OF) &&\n        !(arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ)) {\n      DelEntry = false; // protect parent struct from being deallocated\n    }\n\n    if ((arg_types[i] & OMP_TGT_MAPTYPE_FROM) || DelEntry) {\n      // Move data back to the host\n      if (arg_types[i] & OMP_TGT_MAPTYPE_FROM) {\n        bool Always = arg_types[i] & OMP_TGT_MAPTYPE_ALWAYS;\n        bool CopyMember = false;\n        if ((arg_types[i] & OMP_TGT_MAPTYPE_MEMBER_OF) &&\n            !(arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ)) {\n          // Copy data only if the \"parent\" struct has RefCount==1.\n          short parent_idx = member_of(arg_types[i]);\n          long parent_rc = Device.getMapEntryRefCnt(args[parent_idx]);\n          assert(parent_rc > 0 && \"parent struct not found\");\n          if (parent_rc == 1) {\n            CopyMember = true;\n          }\n        }\n\n        if (DelEntry || Always || CopyMember) {\n          DP(\"Moving %\" PRId64 \" bytes (tgt:\" DPxMOD \") -> (hst:\" DPxMOD \")\\n\",\n              arg_sizes[i], DPxPTR(TgtPtrBegin), DPxPTR(HstPtrBegin));\n          int rt = Device.data_retrieve(HstPtrBegin, TgtPtrBegin, arg_sizes[i]);\n          if (rt != OFFLOAD_SUCCESS) {\n            DP(\"Copying data from device failed.\\n\");\n            rc = OFFLOAD_FAIL;\n          }\n        }\n      }\n\n      // If we copied back to the host a struct/array containing pointers, we\n      // need to restore the original host pointer values from their shadow\n      // copies. If the struct is going to be deallocated, remove any remaining\n      // shadow pointer entries for this struct.\n      uintptr_t lb = (uintptr_t) HstPtrBegin;\n      uintptr_t ub = (uintptr_t) HstPtrBegin + arg_sizes[i];\n      Device.ShadowMtx.lock();\n      for (ShadowPtrListTy::iterator it = Device.ShadowPtrMap.begin();\n          it != Device.ShadowPtrMap.end(); ++it) {\n        void **ShadowHstPtrAddr = (void**) it->first;\n\n        // An STL map is sorted on its keys; use this property\n        // to quickly determine when to break out of the loop.\n        if ((uintptr_t) ShadowHstPtrAddr < lb)\n          continue;\n        if ((uintptr_t) ShadowHstPtrAddr >= ub)\n          break;\n\n        // If we copied the struct to the host, we need to restore the pointer.\n        if (arg_types[i] & OMP_TGT_MAPTYPE_FROM) {\n          DP(\"Restoring original host pointer value \" DPxMOD \" for host \"\n              \"pointer \" DPxMOD \"\\n\", DPxPTR(it->second.HstPtrVal),\n              DPxPTR(ShadowHstPtrAddr));\n          *ShadowHstPtrAddr = it->second.HstPtrVal;\n        }\n        // If the struct is to be deallocated, remove the shadow entry.\n        if (DelEntry) {\n          DP(\"Removing shadow pointer \" DPxMOD \"\\n\", DPxPTR(ShadowHstPtrAddr));\n          Device.ShadowPtrMap.erase(it);\n        }\n      }\n      Device.ShadowMtx.unlock();\n\n      // Deallocate map\n      if (DelEntry) {\n        int rt = Device.deallocTgtPtr(HstPtrBegin, arg_sizes[i], ForceDelete);\n        if (rt != OFFLOAD_SUCCESS) {\n          DP(\"Deallocating data from device failed.\\n\");\n          rc = OFFLOAD_FAIL;\n        }\n      }\n    }\n  }\n\n  return rc;\n}\n\n/// passes data from the target, releases target memory and destroys\n/// the host-target mapping (top entry from the stack of data maps)\n/// created by the last __tgt_target_data_begin.\nEXTERN void __tgt_target_data_end(int64_t device_id, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types) {\n  DP(\"Entering data end region with %d mappings\\n\", arg_num);\n\n  // No devices available?\n  if (device_id == OFFLOAD_DEVICE_DEFAULT) {\n    device_id = omp_get_default_device();\n  }\n\n  RTLsMtx.lock();\n  size_t Devices_size = Devices.size();\n  RTLsMtx.unlock();\n  if (Devices_size <= (size_t)device_id) {\n    DP(\"Device ID  %ld does not have a matching RTL.\\n\", device_id);\n    return;\n  }\n\n  DeviceTy &Device = Devices[device_id];\n  if (!Device.IsInit) {\n    DP(\"uninit device: ignore\");\n    return;\n  }\n\n  // Translate maps\n  int32_t new_arg_num;\n  void **new_args_base;\n  void **new_args;\n  int64_t *new_arg_sizes;\n  int64_t *new_arg_types;\n  translate_map(arg_num, args_base, args, arg_sizes, arg_types, new_arg_num,\n      new_args_base, new_args, new_arg_sizes, new_arg_types, false);\n\n  //target_data_end(Device, arg_num, args_base, args, arg_sizes, arg_types);\n  target_data_end(Device, new_arg_num, new_args_base, new_args, new_arg_sizes,\n      new_arg_types);\n\n  // Cleanup translation memory\n  cleanup_map(new_arg_num, new_args_base, new_args, new_arg_sizes,\n      new_arg_types, arg_num, args_base);\n}\n\nEXTERN void __tgt_target_data_end_nowait(int64_t device_id, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types,\n    int32_t depNum, void *depList, int32_t noAliasDepNum,\n    void *noAliasDepList) {\n  if (depNum + noAliasDepNum > 0)\n    __kmpc_omp_taskwait(NULL, 0);\n\n  __tgt_target_data_end(device_id, arg_num, args_base, args, arg_sizes,\n                        arg_types);\n}\n\n/// passes data to/from the target.\nEXTERN void __tgt_target_data_update(int64_t device_id, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types) {\n  DP(\"Entering data update with %d mappings\\n\", arg_num);\n\n  // No devices available?\n  if (device_id == OFFLOAD_DEVICE_DEFAULT) {\n    device_id = omp_get_default_device();\n  }\n\n  if (CheckDevice(device_id) != OFFLOAD_SUCCESS) {\n    DP(\"Failed to get device %ld ready\\n\", device_id);\n    return;\n  }\n\n  DeviceTy& Device = Devices[device_id];\n\n  // process each input.\n  for (int32_t i = 0; i < arg_num; ++i) {\n    if ((arg_types[i] & OMP_TGT_MAPTYPE_LITERAL) ||\n        (arg_types[i] & OMP_TGT_MAPTYPE_PRIVATE))\n      continue;\n\n    void *HstPtrBegin = args[i];\n    int64_t MapSize = arg_sizes[i];\n    bool IsLast;\n    void *TgtPtrBegin = Device.getTgtPtrBegin(HstPtrBegin, MapSize, IsLast,\n        false);\n\n    if (arg_types[i] & OMP_TGT_MAPTYPE_FROM) {\n      DP(\"Moving %\" PRId64 \" bytes (tgt:\" DPxMOD \") -> (hst:\" DPxMOD \")\\n\",\n          arg_sizes[i], DPxPTR(TgtPtrBegin), DPxPTR(HstPtrBegin));\n      Device.data_retrieve(HstPtrBegin, TgtPtrBegin, MapSize);\n\n      uintptr_t lb = (uintptr_t) HstPtrBegin;\n      uintptr_t ub = (uintptr_t) HstPtrBegin + MapSize;\n      Device.ShadowMtx.lock();\n      for (ShadowPtrListTy::iterator it = Device.ShadowPtrMap.begin();\n          it != Device.ShadowPtrMap.end(); ++it) {\n        void **ShadowHstPtrAddr = (void**) it->first;\n        if ((uintptr_t) ShadowHstPtrAddr < lb)\n          continue;\n        if ((uintptr_t) ShadowHstPtrAddr >= ub)\n          break;\n        DP(\"Restoring original host pointer value \" DPxMOD \" for host pointer \"\n            DPxMOD \"\\n\", DPxPTR(it->second.HstPtrVal),\n            DPxPTR(ShadowHstPtrAddr));\n        *ShadowHstPtrAddr = it->second.HstPtrVal;\n      }\n      Device.ShadowMtx.unlock();\n    }\n\n    if (arg_types[i] & OMP_TGT_MAPTYPE_TO) {\n      DP(\"Moving %\" PRId64 \" bytes (hst:\" DPxMOD \") -> (tgt:\" DPxMOD \")\\n\",\n          arg_sizes[i], DPxPTR(HstPtrBegin), DPxPTR(TgtPtrBegin));\n      Device.data_submit(TgtPtrBegin, HstPtrBegin, MapSize);\n\n      uintptr_t lb = (uintptr_t) HstPtrBegin;\n      uintptr_t ub = (uintptr_t) HstPtrBegin + MapSize;\n      Device.ShadowMtx.lock();\n      for (ShadowPtrListTy::iterator it = Device.ShadowPtrMap.begin();\n          it != Device.ShadowPtrMap.end(); ++it) {\n        void **ShadowHstPtrAddr = (void**) it->first;\n        if ((uintptr_t) ShadowHstPtrAddr < lb)\n          continue;\n        if ((uintptr_t) ShadowHstPtrAddr >= ub)\n          break;\n        DP(\"Restoring original target pointer value \" DPxMOD \" for target \"\n            \"pointer \" DPxMOD \"\\n\", DPxPTR(it->second.TgtPtrVal),\n            DPxPTR(it->second.TgtPtrAddr));\n        Device.data_submit(it->second.TgtPtrAddr,\n            &it->second.TgtPtrVal, sizeof(void *));\n      }\n      Device.ShadowMtx.unlock();\n    }\n  }\n}\n\nEXTERN void __tgt_target_data_update_nowait(\n    int64_t device_id, int32_t arg_num, void **args_base, void **args,\n    int64_t *arg_sizes, int64_t *arg_types, int32_t depNum, void *depList,\n    int32_t noAliasDepNum, void *noAliasDepList) {\n  if (depNum + noAliasDepNum > 0)\n    __kmpc_omp_taskwait(NULL, 0);\n\n  __tgt_target_data_update(device_id, arg_num, args_base, args, arg_sizes,\n                           arg_types);\n}\n\n/// performs the same actions as data_begin in case arg_num is\n/// non-zero and initiates run of the offloaded region on the target platform;\n/// if arg_num is non-zero after the region execution is done it also\n/// performs the same action as data_update and data_end above. This function\n/// returns 0 if it was able to transfer the execution to a target and an\n/// integer different from zero otherwise.\nstatic int target(int64_t device_id, void *host_ptr, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types,\n    int32_t team_num, int32_t thread_limit, int IsTeamConstruct) {\n  DeviceTy &Device = Devices[device_id];\n\n  // Find the table information in the map or look it up in the translation\n  // tables.\n  TableMap *TM = 0;\n  TblMapMtx.lock();\n  HostPtrToTableMapTy::iterator TableMapIt = HostPtrToTableMap.find(host_ptr);\n  if (TableMapIt == HostPtrToTableMap.end()) {\n    // We don't have a map. So search all the registered libraries.\n    TrlTblMtx.lock();\n    for (HostEntriesBeginToTransTableTy::iterator\n             ii = HostEntriesBeginToTransTable.begin(),\n             ie = HostEntriesBeginToTransTable.end();\n         !TM && ii != ie; ++ii) {\n      // get the translation table (which contains all the good info).\n      TranslationTable *TransTable = &ii->second;\n      // iterate over all the host table entries to see if we can locate the\n      // host_ptr.\n      __tgt_offload_entry *begin = TransTable->HostTable.EntriesBegin;\n      __tgt_offload_entry *end = TransTable->HostTable.EntriesEnd;\n      __tgt_offload_entry *cur = begin;\n      for (uint32_t i = 0; cur < end; ++cur, ++i) {\n        if (cur->addr != host_ptr)\n          continue;\n        // we got a match, now fill the HostPtrToTableMap so that we\n        // may avoid this search next time.\n        TM = &HostPtrToTableMap[host_ptr];\n        TM->Table = TransTable;\n        TM->Index = i;\n        break;\n      }\n    }\n    TrlTblMtx.unlock();\n  } else {\n    TM = &TableMapIt->second;\n  }\n  TblMapMtx.unlock();\n\n  // No map for this host pointer found!\n  if (!TM) {\n    DP(\"Host ptr \" DPxMOD \" does not have a matching target pointer.\\n\",\n       DPxPTR(host_ptr));\n    return OFFLOAD_FAIL;\n  }\n\n  // get target table.\n  TrlTblMtx.lock();\n  assert(TM->Table->TargetsTable.size() > (size_t)device_id &&\n         \"Not expecting a device ID outside the table's bounds!\");\n  __tgt_target_table *TargetTable = TM->Table->TargetsTable[device_id];\n  TrlTblMtx.unlock();\n  assert(TargetTable && \"Global data has not been mapped\\n\");\n\n  // Move data to device.\n  int rc = target_data_begin(Device, arg_num, args_base, args, arg_sizes,\n      arg_types);\n\n  if (rc != OFFLOAD_SUCCESS) {\n    DP(\"Call to target_data_begin failed, skipping target execution.\\n\");\n    // Call target_data_end to dealloc whatever target_data_begin allocated\n    // and return OFFLOAD_FAIL.\n    target_data_end(Device, arg_num, args_base, args, arg_sizes, arg_types);\n    return OFFLOAD_FAIL;\n  }\n\n  std::vector<void *> tgt_args;\n  std::vector<ptrdiff_t> tgt_offsets;\n\n  // List of (first-)private arrays allocated for this target region\n  std::vector<void *> fpArrays;\n\n  for (int32_t i = 0; i < arg_num; ++i) {\n    if (!(arg_types[i] & OMP_TGT_MAPTYPE_TARGET_PARAM)) {\n      // This is not a target parameter, do not push it into tgt_args.\n      continue;\n    }\n    void *HstPtrBegin = args[i];\n    void *HstPtrBase = args_base[i];\n    void *TgtPtrBegin;\n    ptrdiff_t TgtBaseOffset;\n    bool IsLast; // unused.\n    if (arg_types[i] & OMP_TGT_MAPTYPE_LITERAL) {\n      DP(\"Forwarding first-private value \" DPxMOD \" to the target construct\\n\",\n          DPxPTR(HstPtrBase));\n      TgtPtrBegin = HstPtrBase;\n      TgtBaseOffset = 0;\n    } else if (arg_types[i] & OMP_TGT_MAPTYPE_PRIVATE) {\n      // Allocate memory for (first-)private array\n      TgtPtrBegin = Device.RTL->data_alloc(Device.RTLDeviceID,\n          arg_sizes[i], HstPtrBegin);\n      if (!TgtPtrBegin) {\n        DP (\"Data allocation for %sprivate array \" DPxMOD \" failed\\n\",\n            (arg_types[i] & OMP_TGT_MAPTYPE_TO ? \"first-\" : \"\"),\n            DPxPTR(HstPtrBegin));\n        rc = OFFLOAD_FAIL;\n        break;\n      } else {\n        fpArrays.push_back(TgtPtrBegin);\n        TgtBaseOffset = (intptr_t)HstPtrBase - (intptr_t)HstPtrBegin;\n#ifdef OMPTARGET_DEBUG\n        void *TgtPtrBase = (void *)((intptr_t)TgtPtrBegin + TgtBaseOffset);\n        DP(\"Allocated %\" PRId64 \" bytes of target memory at \" DPxMOD \" for \"\n            \"%sprivate array \" DPxMOD \" - pushing target argument \" DPxMOD \"\\n\",\n            arg_sizes[i], DPxPTR(TgtPtrBegin),\n            (arg_types[i] & OMP_TGT_MAPTYPE_TO ? \"first-\" : \"\"),\n            DPxPTR(HstPtrBegin), DPxPTR(TgtPtrBase));\n#endif\n        // If first-private, copy data from host\n        if (arg_types[i] & OMP_TGT_MAPTYPE_TO) {\n          int rt = Device.data_submit(TgtPtrBegin, HstPtrBegin, arg_sizes[i]);\n          if (rt != OFFLOAD_SUCCESS) {\n            DP (\"Copying data to device failed.\\n\");\n            rc = OFFLOAD_FAIL;\n            break;\n          }\n        }\n      }\n    } else if (arg_types[i] & OMP_TGT_MAPTYPE_PTR_AND_OBJ) {\n      TgtPtrBegin = Device.getTgtPtrBegin(HstPtrBase, sizeof(void *), IsLast,\n          false);\n      TgtBaseOffset = 0; // no offset for ptrs.\n      DP(\"Obtained target argument \" DPxMOD \" from host pointer \" DPxMOD \" to \"\n         \"object \" DPxMOD \"\\n\", DPxPTR(TgtPtrBegin), DPxPTR(HstPtrBase),\n         DPxPTR(HstPtrBase));\n    } else {\n      TgtPtrBegin = Device.getTgtPtrBegin(HstPtrBegin, arg_sizes[i], IsLast,\n          false);\n      TgtBaseOffset = (intptr_t)HstPtrBase - (intptr_t)HstPtrBegin;\n#ifdef OMPTARGET_DEBUG\n      void *TgtPtrBase = (void *)((intptr_t)TgtPtrBegin + TgtBaseOffset);\n      DP(\"Obtained target argument \" DPxMOD \" from host pointer \" DPxMOD \"\\n\",\n          DPxPTR(TgtPtrBase), DPxPTR(HstPtrBegin));\n#endif\n    }\n    tgt_args.push_back(TgtPtrBegin);\n    tgt_offsets.push_back(TgtBaseOffset);\n  }\n\n  assert(tgt_args.size() == tgt_offsets.size() &&\n      \"Size mismatch in arguments and offsets\");\n\n  // Pop loop trip count\n  uint64_t ltc = Device.loopTripCnt;\n  Device.loopTripCnt = 0;\n\n  // Launch device execution.\n  if (rc == OFFLOAD_SUCCESS) {\n    DP(\"Launching target execution %s with pointer \" DPxMOD \" (index=%d).\\n\",\n        TargetTable->EntriesBegin[TM->Index].name,\n        DPxPTR(TargetTable->EntriesBegin[TM->Index].addr), TM->Index);\n    if (IsTeamConstruct) {\n      rc = Device.run_team_region(TargetTable->EntriesBegin[TM->Index].addr,\n          &tgt_args[0], &tgt_offsets[0], tgt_args.size(), team_num,\n          thread_limit, ltc);\n    } else {\n      rc = Device.run_region(TargetTable->EntriesBegin[TM->Index].addr,\n          &tgt_args[0], &tgt_offsets[0], tgt_args.size());\n    }\n  } else {\n    DP(\"Errors occurred while obtaining target arguments, skipping kernel \"\n        \"execution\\n\");\n  }\n\n  // Deallocate (first-)private arrays\n  for (auto it : fpArrays) {\n    int rt = Device.RTL->data_delete(Device.RTLDeviceID, it);\n    if (rt != OFFLOAD_SUCCESS) {\n      DP(\"Deallocation of (first-)private arrays failed.\\n\");\n      rc = OFFLOAD_FAIL;\n    }\n  }\n\n  // Move data from device.\n  int rt = target_data_end(Device, arg_num, args_base, args, arg_sizes,\n      arg_types);\n\n  if (rt != OFFLOAD_SUCCESS) {\n    DP(\"Call to target_data_end failed.\\n\");\n    rc = OFFLOAD_FAIL;\n  }\n\n  return rc;\n}\n\nEXTERN int __tgt_target(int64_t device_id, void *host_ptr, int32_t arg_num,\n    void **args_base, void **args, int64_t *arg_sizes, int64_t *arg_types) {\n  DP(\"Entering target region with entry point \" DPxMOD \" and device Id %ld\\n\",\n     DPxPTR(host_ptr), device_id);\n\n  if (device_id == OFFLOAD_DEVICE_DEFAULT) {\n    device_id = omp_get_default_device();\n  }\n\n  if (CheckDevice(device_id) != OFFLOAD_SUCCESS) {\n    DP(\"Failed to get device %ld ready\\n\", device_id);\n    return OFFLOAD_FAIL;\n  }\n\n  // Translate maps\n  int32_t new_arg_num;\n  void **new_args_base;\n  void **new_args;\n  int64_t *new_arg_sizes;\n  int64_t *new_arg_types;\n  translate_map(arg_num, args_base, args, arg_sizes, arg_types, new_arg_num,\n      new_args_base, new_args, new_arg_sizes, new_arg_types, true);\n\n  //return target(device_id, host_ptr, arg_num, args_base, args, arg_sizes,\n  //    arg_types, 0, 0, false /*team*/, false /*recursive*/);\n  int rc = target(device_id, host_ptr, new_arg_num, new_args_base, new_args,\n      new_arg_sizes, new_arg_types, 0, 0, false /*team*/);\n\n  // Cleanup translation memory\n  cleanup_map(new_arg_num, new_args_base, new_args, new_arg_sizes,\n      new_arg_types, arg_num, args_base);\n\n  return rc;\n}\n\nEXTERN int __tgt_target_nowait(int64_t device_id, void *host_ptr,\n    int32_t arg_num, void **args_base, void **args, int64_t *arg_sizes,\n    int64_t *arg_types, int32_t depNum, void *depList, int32_t noAliasDepNum,\n    void *noAliasDepList) {\n  if (depNum + noAliasDepNum > 0)\n    __kmpc_omp_taskwait(NULL, 0);\n\n  return __tgt_target(device_id, host_ptr, arg_num, args_base, args, arg_sizes,\n                      arg_types);\n}\n\nEXTERN int __tgt_target_teams(int64_t device_id, void *host_ptr,\n    int32_t arg_num, void **args_base, void **args, int64_t *arg_sizes,\n    int64_t *arg_types, int32_t team_num, int32_t thread_limit) {\n  DP(\"Entering target region with entry point \" DPxMOD \" and device Id %ld\\n\",\n     DPxPTR(host_ptr), device_id);\n\n  if (device_id == OFFLOAD_DEVICE_DEFAULT) {\n    device_id = omp_get_default_device();\n  }\n\n  if (CheckDevice(device_id) != OFFLOAD_SUCCESS) {\n    DP(\"Failed to get device %ld ready\\n\", device_id);\n    return OFFLOAD_FAIL;\n  }\n\n  // Translate maps\n  int32_t new_arg_num;\n  void **new_args_base;\n  void **new_args;\n  int64_t *new_arg_sizes;\n  int64_t *new_arg_types;\n  translate_map(arg_num, args_base, args, arg_sizes, arg_types, new_arg_num,\n      new_args_base, new_args, new_arg_sizes, new_arg_types, true);\n\n  //return target(device_id, host_ptr, arg_num, args_base, args, arg_sizes,\n  //              arg_types, team_num, thread_limit, true /*team*/,\n  //              false /*recursive*/);\n  int rc = target(device_id, host_ptr, new_arg_num, new_args_base, new_args,\n      new_arg_sizes, new_arg_types, team_num, thread_limit, true /*team*/);\n\n  // Cleanup translation memory\n  cleanup_map(new_arg_num, new_args_base, new_args, new_arg_sizes,\n      new_arg_types, arg_num, args_base);\n\n  return rc;\n}\n\nEXTERN int __tgt_target_teams_nowait(int64_t device_id, void *host_ptr,\n    int32_t arg_num, void **args_base, void **args, int64_t *arg_sizes,\n    int64_t *arg_types, int32_t team_num, int32_t thread_limit, int32_t depNum,\n    void *depList, int32_t noAliasDepNum, void *noAliasDepList) {\n  if (depNum + noAliasDepNum > 0)\n    __kmpc_omp_taskwait(NULL, 0);\n\n  return __tgt_target_teams(device_id, host_ptr, arg_num, args_base, args,\n                            arg_sizes, arg_types, team_num, thread_limit);\n}\n\n\n// The trip count mechanism will be revised - this scheme is not thread-safe.\nEXTERN void __kmpc_push_target_tripcount(int64_t device_id,\n    uint64_t loop_tripcount) {\n  if (device_id == OFFLOAD_DEVICE_DEFAULT) {\n    device_id = omp_get_default_device();\n  }\n\n  if (CheckDevice(device_id) != OFFLOAD_SUCCESS) {\n    DP(\"Failed to get device %ld ready\\n\", device_id);\n    return;\n  }\n\n  DP(\"__kmpc_push_target_tripcount(%ld, %\" PRIu64 \")\\n\", device_id,\n      loop_tripcount);\n  Devices[device_id].loopTripCnt = loop_tripcount;\n}\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/libomptarget/plugins/generic-elf-64bit/src/rtl.cpp": "//===-RTLs/generic-64bit/src/rtl.cpp - Target RTLs Implementation - C++ -*-===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n//\n// RTL for generic 64-bit machine\n//\n//===----------------------------------------------------------------------===//\n\n#include <cassert>\n#include <cstdio>\n#include <cstring>\n#include <cstdlib>\n#include <dlfcn.h>\n#include <ffi.h>\n#include <gelf.h>\n#include <link.h>\n#include <list>\n#include <string>\n#include <vector>\n\n#include \"omptargetplugin.h\"\n\n#ifndef TARGET_NAME\n#define TARGET_NAME Generic ELF - 64bit\n#endif\n\n#ifndef TARGET_ELF_ID\n#define TARGET_ELF_ID 0\n#endif\n\n#ifdef OMPTARGET_DEBUG\nstatic int DebugLevel = 0;\n\n#define GETNAME2(name) #name\n#define GETNAME(name) GETNAME2(name)\n#define DP(...) \\\n  do { \\\n    if (DebugLevel > 0) { \\\n      DEBUGP(\"Target \" GETNAME(TARGET_NAME) \" RTL\", __VA_ARGS__); \\\n    } \\\n  } while (false)\n#else // OMPTARGET_DEBUG\n#define DP(...) {}\n#endif // OMPTARGET_DEBUG\n\n#include \"../../common/elf_common.c\"\n\n#define NUMBER_OF_DEVICES 4\n#define OFFLOADSECTIONNAME \".omp_offloading.entries\"\n\n/// Array of Dynamic libraries loaded for this target.\nstruct DynLibTy {\n  char *FileName;\n  void *Handle;\n};\n\n/// Keep entries table per device.\nstruct FuncOrGblEntryTy {\n  __tgt_target_table Table;\n};\n\n/// Class containing all the device information.\nclass RTLDeviceInfoTy {\n  std::vector<FuncOrGblEntryTy> FuncGblEntries;\n\npublic:\n  std::list<DynLibTy> DynLibs;\n\n  // Record entry point associated with device.\n  void createOffloadTable(int32_t device_id, __tgt_offload_entry *begin,\n                          __tgt_offload_entry *end) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id];\n\n    E.Table.EntriesBegin = begin;\n    E.Table.EntriesEnd = end;\n  }\n\n  // Return true if the entry is associated with device.\n  bool findOffloadEntry(int32_t device_id, void *addr) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id];\n\n    for (__tgt_offload_entry *i = E.Table.EntriesBegin, *e = E.Table.EntriesEnd;\n         i < e; ++i) {\n      if (i->addr == addr)\n        return true;\n    }\n\n    return false;\n  }\n\n  // Return the pointer to the target entries table.\n  __tgt_target_table *getOffloadEntriesTable(int32_t device_id) {\n    assert(device_id < (int32_t)FuncGblEntries.size() &&\n           \"Unexpected device id!\");\n    FuncOrGblEntryTy &E = FuncGblEntries[device_id];\n\n    return &E.Table;\n  }\n\n  RTLDeviceInfoTy(int32_t num_devices) {\n#ifdef OMPTARGET_DEBUG\n    if (char *envStr = getenv(\"LIBOMPTARGET_DEBUG\")) {\n      DebugLevel = std::stoi(envStr);\n    }\n#endif // OMPTARGET_DEBUG\n\n    FuncGblEntries.resize(num_devices);\n  }\n\n  ~RTLDeviceInfoTy() {\n    // Close dynamic libraries\n    for (auto &lib : DynLibs) {\n      if (lib.Handle) {\n        dlclose(lib.Handle);\n        remove(lib.FileName);\n      }\n    }\n  }\n};\n\nstatic RTLDeviceInfoTy DeviceInfo(NUMBER_OF_DEVICES);\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nint32_t __tgt_rtl_is_valid_binary(__tgt_device_image *image) {\n// If we don't have a valid ELF ID we can just fail.\n#if TARGET_ELF_ID < 1\n  return 0;\n#else\n  return elf_check_machine(image, TARGET_ELF_ID);\n#endif\n}\n\nint32_t __tgt_rtl_number_of_devices() { return NUMBER_OF_DEVICES; }\n\nint32_t __tgt_rtl_init_device(int32_t device_id) { return OFFLOAD_SUCCESS; }\n\n__tgt_target_table *__tgt_rtl_load_binary(int32_t device_id,\n                                          __tgt_device_image *image) {\n\n  DP(\"Dev %d: load binary from \" DPxMOD \" image\\n\", device_id,\n     DPxPTR(image->ImageStart));\n\n  assert(device_id >= 0 && device_id < NUMBER_OF_DEVICES && \"bad dev id\");\n\n  size_t ImageSize = (size_t)image->ImageEnd - (size_t)image->ImageStart;\n  size_t NumEntries = (size_t)(image->EntriesEnd - image->EntriesBegin);\n  DP(\"Expecting to have %zd entries defined.\\n\", NumEntries);\n\n  // Is the library version incompatible with the header file?\n  if (elf_version(EV_CURRENT) == EV_NONE) {\n    DP(\"Incompatible ELF library!\\n\");\n    return NULL;\n  }\n\n  // Obtain elf handler\n  Elf *e = elf_memory((char *)image->ImageStart, ImageSize);\n  if (!e) {\n    DP(\"Unable to get ELF handle: %s!\\n\", elf_errmsg(-1));\n    return NULL;\n  }\n\n  if (elf_kind(e) != ELF_K_ELF) {\n    DP(\"Invalid Elf kind!\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  // Find the entries section offset\n  Elf_Scn *section = 0;\n  Elf64_Off entries_offset = 0;\n\n  size_t shstrndx;\n\n  if (elf_getshdrstrndx(e, &shstrndx)) {\n    DP(\"Unable to get ELF strings index!\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  while ((section = elf_nextscn(e, section))) {\n    GElf_Shdr hdr;\n    gelf_getshdr(section, &hdr);\n\n    if (!strcmp(elf_strptr(e, shstrndx, hdr.sh_name), OFFLOADSECTIONNAME)) {\n      entries_offset = hdr.sh_addr;\n      break;\n    }\n  }\n\n  if (!entries_offset) {\n    DP(\"Entries Section Offset Not Found\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  DP(\"Offset of entries section is (\" DPxMOD \").\\n\", DPxPTR(entries_offset));\n\n  // load dynamic library and get the entry points. We use the dl library\n  // to do the loading of the library, but we could do it directly to avoid the\n  // dump to the temporary file.\n  //\n  // 1) Create tmp file with the library contents.\n  // 2) Use dlopen to load the file and dlsym to retrieve the symbols.\n  char tmp_name[] = \"/tmp/tmpfile_XXXXXX\";\n  int tmp_fd = mkstemp(tmp_name);\n\n  if (tmp_fd == -1) {\n    elf_end(e);\n    return NULL;\n  }\n\n  FILE *ftmp = fdopen(tmp_fd, \"wb\");\n\n  if (!ftmp) {\n    elf_end(e);\n    return NULL;\n  }\n\n  fwrite(image->ImageStart, ImageSize, 1, ftmp);\n  fclose(ftmp);\n\n  DynLibTy Lib = {tmp_name, dlopen(tmp_name, RTLD_LAZY)};\n\n  if (!Lib.Handle) {\n    DP(\"Target library loading error: %s\\n\", dlerror());\n    elf_end(e);\n    return NULL;\n  }\n\n  DeviceInfo.DynLibs.push_back(Lib);\n\n  struct link_map *libInfo = (struct link_map *)Lib.Handle;\n\n  // The place where the entries info is loaded is the library base address\n  // plus the offset determined from the ELF file.\n  Elf64_Addr entries_addr = libInfo->l_addr + entries_offset;\n\n  DP(\"Pointer to first entry to be loaded is (\" DPxMOD \").\\n\",\n      DPxPTR(entries_addr));\n\n  // Table of pointers to all the entries in the target.\n  __tgt_offload_entry *entries_table = (__tgt_offload_entry *)entries_addr;\n\n  __tgt_offload_entry *entries_begin = &entries_table[0];\n  __tgt_offload_entry *entries_end = entries_begin + NumEntries;\n\n  if (!entries_begin) {\n    DP(\"Can't obtain entries begin\\n\");\n    elf_end(e);\n    return NULL;\n  }\n\n  DP(\"Entries table range is (\" DPxMOD \")->(\" DPxMOD \")\\n\",\n      DPxPTR(entries_begin), DPxPTR(entries_end));\n  DeviceInfo.createOffloadTable(device_id, entries_begin, entries_end);\n\n  elf_end(e);\n\n  return DeviceInfo.getOffloadEntriesTable(device_id);\n}\n\nvoid *__tgt_rtl_data_alloc(int32_t device_id, int64_t size, void *hst_ptr) {\n  void *ptr = malloc(size);\n  return ptr;\n}\n\nint32_t __tgt_rtl_data_submit(int32_t device_id, void *tgt_ptr, void *hst_ptr,\n                              int64_t size) {\n  memcpy(tgt_ptr, hst_ptr, size);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_data_retrieve(int32_t device_id, void *hst_ptr, void *tgt_ptr,\n                                int64_t size) {\n  memcpy(hst_ptr, tgt_ptr, size);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_data_delete(int32_t device_id, void *tgt_ptr) {\n  free(tgt_ptr);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_run_target_team_region(int32_t device_id, void *tgt_entry_ptr,\n    void **tgt_args, ptrdiff_t *tgt_offsets, int32_t arg_num, int32_t team_num,\n    int32_t thread_limit, uint64_t loop_tripcount /*not used*/) {\n  // ignore team num and thread limit.\n\n  // Use libffi to launch execution.\n  ffi_cif cif;\n\n  // All args are references.\n  std::vector<ffi_type *> args_types(arg_num, &ffi_type_pointer);\n  std::vector<void *> args(arg_num);\n  std::vector<void *> ptrs(arg_num);\n\n  for (int32_t i = 0; i < arg_num; ++i) {\n    ptrs[i] = (void *)((intptr_t)tgt_args[i] + tgt_offsets[i]);\n    args[i] = &ptrs[i];\n  }\n\n  ffi_status status = ffi_prep_cif(&cif, FFI_DEFAULT_ABI, arg_num,\n                                   &ffi_type_void, &args_types[0]);\n\n  assert(status == FFI_OK && \"Unable to prepare target launch!\");\n\n  if (status != FFI_OK)\n    return OFFLOAD_FAIL;\n\n  DP(\"Running entry point at \" DPxMOD \"...\\n\", DPxPTR(tgt_entry_ptr));\n\n  void (*entry)(void);\n  *((void**) &entry) = tgt_entry_ptr;\n  ffi_call(&cif, entry, NULL, &args[0]);\n  return OFFLOAD_SUCCESS;\n}\n\nint32_t __tgt_rtl_run_target_region(int32_t device_id, void *tgt_entry_ptr,\n    void **tgt_args, ptrdiff_t *tgt_offsets, int32_t arg_num) {\n  // use one team and one thread.\n  return __tgt_rtl_run_target_team_region(device_id, tgt_entry_ptr, tgt_args,\n      tgt_offsets, arg_num, 1, 1, 0);\n}\n\n#ifdef __cplusplus\n}\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/offload/src/offload_util.h": "//===----------------------------------------------------------------------===//\n//\n//                     The LLVM Compiler Infrastructure\n//\n// This file is dual licensed under the MIT and the University of Illinois Open\n// Source Licenses. See LICENSE.txt for details.\n//\n//===----------------------------------------------------------------------===//\n\n\n#ifndef OFFLOAD_UTIL_H_INCLUDED\n#define OFFLOAD_UTIL_H_INCLUDED\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n\n#ifdef TARGET_WINNT\n#include <windows.h>\n#include <process.h>\n#else // TARGET_WINNT\n#include <dlfcn.h>\n#include <pthread.h>\n#endif // TARGET_WINNT\n\n#ifdef TARGET_WINNT\ntypedef unsigned pthread_key_t;\ntypedef int pid_t;\n\n#define __func__ __FUNCTION__\n#define strtok_r(s,d,p) strtok_s(s,d,p)\n#define strcasecmp(a,b) stricmp(a,b)\n\n#define thread_key_create(key, destructor) \\\n    (((*key = TlsAlloc()) > 0) ? 0 : GetLastError())\n#define thread_key_delete(key) TlsFree(key)\n\n#ifndef S_ISREG\n#define S_ISREG(mode)  (((mode) & S_IFMT) == S_IFREG)\n#endif\n\nvoid*   thread_getspecific(pthread_key_t key);\nint     thread_setspecific(pthread_key_t key, const void *value);\n#else\n#define thread_key_create(key, destructor) \\\n            pthread_key_create((key), (destructor))\n#define thread_key_delete(key)  pthread_key_delete(key)\n#define thread_getspecific(key) pthread_getspecific(key)\n#define thread_setspecific(key, value) pthread_setspecific(key, value)\n#endif // TARGET_WINNT\n\n// Mutex implementation\nstruct mutex_t {\n    mutex_t() {\n#ifdef TARGET_WINNT\n        InitializeCriticalSection(&m_lock);\n#else // TARGET_WINNT\n        pthread_mutex_init(&m_lock, 0);\n#endif // TARGET_WINNT\n    }\n\n    ~mutex_t() {\n#ifdef TARGET_WINNT\n        DeleteCriticalSection(&m_lock);\n#else // TARGET_WINNT\n        pthread_mutex_destroy(&m_lock);\n#endif // TARGET_WINNT\n    }\n\n    void lock() {\n#ifdef TARGET_WINNT\n        EnterCriticalSection(&m_lock);\n#else // TARGET_WINNT\n        pthread_mutex_lock(&m_lock);\n#endif // TARGET_WINNT\n    }\n\n    void unlock() {\n#ifdef TARGET_WINNT\n        LeaveCriticalSection(&m_lock);\n#else // TARGET_WINNT\n        pthread_mutex_unlock(&m_lock);\n#endif // TARGET_WINNT\n    }\n\nprivate:\n#ifdef TARGET_WINNT\n    CRITICAL_SECTION    m_lock;\n#else\n    pthread_mutex_t     m_lock;\n#endif\n};\n\nstruct mutex_locker_t {\n    mutex_locker_t(mutex_t &mutex) : m_mutex(mutex) {\n        m_mutex.lock();\n    }\n\n    ~mutex_locker_t() {\n        m_mutex.unlock();\n    }\n\nprivate:\n    mutex_t &m_mutex;\n};\n\n// Dynamic loader interface\n#ifdef TARGET_WINNT\nstruct Dl_info\n{\n    char        dli_fname[MAX_PATH];\n    void       *dli_fbase;\n    char        dli_sname[MAX_PATH];\n    const void *dli_saddr;\n};\n\nvoid*   DL_open(const char *path);\n#define DL_close(handle)        FreeLibrary((HMODULE) (handle))\nint     DL_addr(const void *addr, Dl_info *info);\n#else\n#define DL_open(path)           dlopen((path), RTLD_NOW)\n#define DL_close(handle)        dlclose(handle)\n#define DL_addr(addr, info)     dladdr((addr), (info))\n#endif // TARGET_WINNT\n\nextern void* DL_sym(void *handle, const char *name, const char *version);\n\n// One-time initialization API\n#ifdef TARGET_WINNT\ntypedef INIT_ONCE                   OffloadOnceControl;\n#define OFFLOAD_ONCE_CONTROL_INIT   INIT_ONCE_STATIC_INIT\n\nextern void __offload_run_once(OffloadOnceControl *ctrl, void (*func)(void));\n#else\ntypedef pthread_once_t              OffloadOnceControl;\n#define OFFLOAD_ONCE_CONTROL_INIT   PTHREAD_ONCE_INIT\n\n#define __offload_run_once(ctrl, func) pthread_once(ctrl, func)\n#endif // TARGET_WINNT\n\n// Parses size specification string.\nextern bool __offload_parse_size_string(const char *str, uint64_t &new_size);\n\n// Parses string with integer value\nextern bool __offload_parse_int_string(const char *str, int64_t &value);\n\n// get value by its base, offset and size\nint64_t get_el_value(\n    char   *base,\n    int64_t offset,\n    int64_t size\n);\n#endif // OFFLOAD_UTIL_H_INCLUDED\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/.git/objects/pack/pack-312dd821475e7eae49cf522e668ac31696428b7d.pack",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/.git/objects/pack/pack-312dd821475e7eae49cf522e668ac31696428b7d.idx",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/runtime/doc/Reference.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/www/Reference.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/offload/src/liboffload_msg.h",
        "/tmp/vanessa/spack-stage/spack-stage-llvm-openmp-ompt-tr6_forwards-57uyungs65mwggmkhuiivvap6h5vkwmh/spack-src/offload/doc/Reference.pdf"
    ],
    "total_files": 403
}