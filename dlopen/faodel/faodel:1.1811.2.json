{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/src/heap-checker.cc": "// -*- Mode: C++; c-basic-offset: 2; indent-tabs-mode: nil -*-\n// Copyright (c) 2005, Google Inc.\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n// ---\n// All Rights Reserved.\n//\n// Author: Maxim Lifantsev\n//\n\n#include \"config.h\"\n\n#include <fcntl.h>    // for O_RDONLY (we use syscall to do actual reads)\n#include <string.h>\n#include <errno.h>\n#ifdef HAVE_UNISTD_H\n#include <unistd.h>\n#endif\n#ifdef HAVE_MMAP\n#include <sys/mman.h>\n#endif\n#ifdef HAVE_PTHREAD\n#include <pthread.h>\n#endif\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <time.h>\n#include <assert.h>\n\n#if defined(HAVE_LINUX_PTRACE_H)\n#include <linux/ptrace.h>\n#endif\n#ifdef HAVE_SYS_SYSCALL_H\n#include <sys/syscall.h>\n#endif\n#if defined(_WIN32) || defined(__CYGWIN__) || defined(__CYGWIN32__) || defined(__MINGW32__)\n#include <wtypes.h>\n#include <winbase.h>\n#undef ERROR     // windows defines these as macros, which can cause trouble\n#undef max\n#undef min\n#endif\n\n#include <string>\n#include <vector>\n#include <map>\n#include <set>\n#include <algorithm>\n#include <functional>\n\n#include <gperftools/heap-checker.h>\n\n#include \"base/basictypes.h\"\n#include \"base/googleinit.h\"\n#include \"base/logging.h\"\n#include <gperftools/stacktrace.h>\n#include \"base/commandlineflags.h\"\n#include \"base/elfcore.h\"              // for i386_regs\n#include \"base/thread_lister.h\"\n#include \"heap-profile-table.h\"\n#include \"base/low_level_alloc.h\"\n#include \"malloc_hook-inl.h\"\n#include <gperftools/malloc_hook.h>\n#include <gperftools/malloc_extension.h>\n#include \"maybe_threads.h\"\n#include \"memory_region_map.h\"\n#include \"base/spinlock.h\"\n#include \"base/sysinfo.h\"\n#include \"base/stl_allocator.h\"\n\nusing std::string;\nusing std::basic_string;\nusing std::pair;\nusing std::map;\nusing std::set;\nusing std::vector;\nusing std::swap;\nusing std::make_pair;\nusing std::min;\nusing std::max;\nusing std::less;\nusing std::char_traits;\n\n// If current process is being ptrace()d, 'TracerPid' in /proc/self/status\n// will be non-zero.\nstatic bool IsDebuggerAttached(void) {    // only works under linux, probably\n  char buf[256];   // TracerPid comes relatively earlier in status output\n  int fd = open(\"/proc/self/status\", O_RDONLY);\n  if (fd == -1) {\n    return false;  // Can't tell for sure.\n  }\n  const int len = read(fd, buf, sizeof(buf));\n  bool rc = false;\n  if (len > 0) {\n    const char *const kTracerPid = \"TracerPid:\\t\";\n    buf[len - 1] = '\\0';\n    const char *p = strstr(buf, kTracerPid);\n    if (p != NULL) {\n      rc = (strncmp(p + strlen(kTracerPid), \"0\\n\", 2) != 0);\n    }\n  }\n  close(fd);\n  return rc;\n}\n\n// This is the default if you don't link in -lprofiler\nextern \"C\" {\nATTRIBUTE_WEAK PERFTOOLS_DLL_DECL bool ProfilingIsEnabledForAllThreads();\nbool ProfilingIsEnabledForAllThreads() { return false; }\n}\n\n//----------------------------------------------------------------------\n// Flags that control heap-checking\n//----------------------------------------------------------------------\n\nDEFINE_string(heap_check,\n              EnvToString(\"HEAPCHECK\", \"\"),\n              \"The heap leak checking to be done over the whole executable: \"\n              \"\\\"minimal\\\", \\\"normal\\\", \\\"strict\\\", \"\n              \"\\\"draconian\\\", \\\"as-is\\\", and \\\"local\\\" \"\n              \" or the empty string are the supported choices. \"\n              \"(See HeapLeakChecker_InternalInitStart for details.)\");\n\nDEFINE_bool(heap_check_report, true, \"Obsolete\");\n\nDEFINE_bool(heap_check_before_constructors,\n            true,\n            \"deprecated; pretty much always true now\");\n\nDEFINE_bool(heap_check_after_destructors,\n            EnvToBool(\"HEAP_CHECK_AFTER_DESTRUCTORS\", false),\n            \"If overall heap check is to end after global destructors \"\n            \"or right after all REGISTER_HEAPCHECK_CLEANUP's\");\n\nDEFINE_bool(heap_check_strict_check, true, \"Obsolete\");\n\nDEFINE_bool(heap_check_ignore_global_live,\n            EnvToBool(\"HEAP_CHECK_IGNORE_GLOBAL_LIVE\", true),\n            \"If overall heap check is to ignore heap objects reachable \"\n            \"from the global data\");\n\nDEFINE_bool(heap_check_identify_leaks,\n            EnvToBool(\"HEAP_CHECK_IDENTIFY_LEAKS\", false),\n            \"If heap check should generate the addresses of the leaked \"\n            \"objects in the memory leak profiles.  This may be useful \"\n            \"in tracking down leaks where only a small fraction of \"\n            \"objects allocated at the same stack trace are leaked.\");\n\nDEFINE_bool(heap_check_ignore_thread_live,\n            EnvToBool(\"HEAP_CHECK_IGNORE_THREAD_LIVE\", true),\n            \"If set to true, objects reachable from thread stacks \"\n            \"and registers are not reported as leaks\");\n\nDEFINE_bool(heap_check_test_pointer_alignment,\n            EnvToBool(\"HEAP_CHECK_TEST_POINTER_ALIGNMENT\", false),\n            \"Set to true to check if the found leak can be due to \"\n            \"use of unaligned pointers\");\n\n// Alignment at which all pointers in memory are supposed to be located;\n// use 1 if any alignment is ok.\n// heap_check_test_pointer_alignment flag guides if we try the value of 1.\n// The larger it can be, the lesser is the chance of missing real leaks.\nstatic const size_t kPointerSourceAlignment = sizeof(void*);\nDEFINE_int32(heap_check_pointer_source_alignment,\n\t     EnvToInt(\"HEAP_CHECK_POINTER_SOURCE_ALIGNMENT\",\n                      kPointerSourceAlignment),\n             \"Alignment at which all pointers in memory are supposed to be \"\n             \"located.  Use 1 if any alignment is ok.\");\n\n// A reasonable default to handle pointers inside of typical class objects:\n// Too low and we won't be able to traverse pointers to normally-used\n// nested objects and base parts of multiple-inherited objects.\n// Too high and it will both slow down leak checking (FindInsideAlloc\n// in HaveOnHeapLocked will get slower when there are large on-heap objects)\n// and make it probabilistically more likely to miss leaks\n// of large-sized objects.\nstatic const int64 kHeapCheckMaxPointerOffset = 1024;\nDEFINE_int64(heap_check_max_pointer_offset,\n\t     EnvToInt(\"HEAP_CHECK_MAX_POINTER_OFFSET\",\n                      kHeapCheckMaxPointerOffset),\n             \"Largest pointer offset for which we traverse \"\n             \"pointers going inside of heap allocated objects. \"\n             \"Set to -1 to use the actual largest heap object size.\");\n\nDEFINE_bool(heap_check_run_under_gdb,\n            EnvToBool(\"HEAP_CHECK_RUN_UNDER_GDB\", false),\n            \"If false, turns off heap-checking library when running under gdb \"\n            \"(normally, set to 'true' only when debugging the heap-checker)\");\n\nDEFINE_int32(heap_check_delay_seconds, 0,\n             \"Number of seconds to delay on-exit heap checking.\"\n             \" If you set this flag,\"\n             \" you may also want to set exit_timeout_seconds in order to\"\n             \" avoid exit timeouts.\\n\"\n             \"NOTE: This flag is to be used only to help diagnose issues\"\n             \" where it is suspected that the heap checker is reporting\"\n             \" false leaks that will disappear if the heap checker delays\"\n             \" its checks. Report any such issues to the heap-checker\"\n             \" maintainer(s).\");\n\n//----------------------------------------------------------------------\n\nDEFINE_string(heap_profile_pprof,\n              EnvToString(\"PPROF_PATH\", \"pprof\"),\n              \"OBSOLETE; not used\");\n\nDEFINE_string(heap_check_dump_directory,\n              EnvToString(\"HEAP_CHECK_DUMP_DIRECTORY\", \"/tmp\"),\n              \"Directory to put heap-checker leak dump information\");\n\n\n//----------------------------------------------------------------------\n// HeapLeakChecker global data\n//----------------------------------------------------------------------\n\n// Global lock for all the global data of this module.\nstatic SpinLock heap_checker_lock(SpinLock::LINKER_INITIALIZED);\n\n//----------------------------------------------------------------------\n\n// Heap profile prefix for leak checking profiles.\n// Gets assigned once when leak checking is turned on, then never modified.\nstatic const string* profile_name_prefix = NULL;\n\n// Whole-program heap leak checker.\n// Gets assigned once when leak checking is turned on,\n// then main_heap_checker is never deleted.\nstatic HeapLeakChecker* main_heap_checker = NULL;\n\n// Whether we will use main_heap_checker to do a check at program exit\n// automatically. In any case user can ask for more checks on main_heap_checker\n// via GlobalChecker().\nstatic bool do_main_heap_check = false;\n\n// The heap profile we use to collect info about the heap.\n// This is created in HeapLeakChecker::BeforeConstructorsLocked\n// together with setting heap_checker_on (below) to true\n// and registering our new/delete malloc hooks;\n// similarly all are unset in HeapLeakChecker::TurnItselfOffLocked.\nstatic HeapProfileTable* heap_profile = NULL;\n\n// If we are doing (or going to do) any kind of heap-checking.\nstatic bool heap_checker_on = false;\n\n// pid of the process that does whole-program heap leak checking\nstatic pid_t heap_checker_pid = 0;\n\n// If we did heap profiling during global constructors execution\nstatic bool constructor_heap_profiling = false;\n\n// RAW_VLOG level we dump key INFO messages at.  If you want to turn\n// off these messages, set the environment variable PERFTOOLS_VERBOSE=-1.\nstatic const int heap_checker_info_level = 0;\n\n//----------------------------------------------------------------------\n// HeapLeakChecker's own memory allocator that is\n// independent of the normal program allocator.\n//----------------------------------------------------------------------\n\n// Wrapper of LowLevelAlloc for STL_Allocator and direct use.\n// We always access this class under held heap_checker_lock,\n// this allows us to in particular protect the period when threads are stopped\n// at random spots with TCMalloc_ListAllProcessThreads by heap_checker_lock,\n// w/o worrying about the lock in LowLevelAlloc::Arena.\n// We rely on the fact that we use an own arena with an own lock here.\nclass HeapLeakChecker::Allocator {\n public:\n  static void Init() {\n    RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n    RAW_DCHECK(arena_ == NULL, \"\");\n    arena_ = LowLevelAlloc::NewArena(0, LowLevelAlloc::DefaultArena());\n  }\n  static void Shutdown() {\n    RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n    if (!LowLevelAlloc::DeleteArena(arena_)  ||  alloc_count_ != 0) {\n      RAW_LOG(FATAL, \"Internal heap checker leak of %d objects\", alloc_count_);\n    }\n  }\n  static int alloc_count() {\n    RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n    return alloc_count_;\n  }\n  static void* Allocate(size_t n) {\n    RAW_DCHECK(arena_  &&  heap_checker_lock.IsHeld(), \"\");\n    void* p = LowLevelAlloc::AllocWithArena(n, arena_);\n    if (p) alloc_count_ += 1;\n    return p;\n  }\n  static void Free(void* p) {\n    RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n    if (p) alloc_count_ -= 1;\n    LowLevelAlloc::Free(p);\n  }\n  static void Free(void* p, size_t /* n */) {\n    Free(p);\n  }\n  // destruct, free, and make *p to be NULL\n  template<typename T> static void DeleteAndNull(T** p) {\n    (*p)->~T();\n    Free(*p);\n    *p = NULL;\n  }\n  template<typename T> static void DeleteAndNullIfNot(T** p) {\n    if (*p != NULL) DeleteAndNull(p);\n  }\n private:\n  static LowLevelAlloc::Arena* arena_;\n  static int alloc_count_;\n};\n\nLowLevelAlloc::Arena* HeapLeakChecker::Allocator::arena_ = NULL;\nint HeapLeakChecker::Allocator::alloc_count_ = 0;\n\n//----------------------------------------------------------------------\n// HeapLeakChecker live object tracking components\n//----------------------------------------------------------------------\n\n// Cases of live object placement we distinguish\nenum ObjectPlacement {\n  MUST_BE_ON_HEAP,   // Must point to a live object of the matching size in the\n                     // heap_profile map of the heap when we get to it\n  IGNORED_ON_HEAP,   // Is a live (ignored) object on heap\n  MAYBE_LIVE,        // Is a piece of writable memory from /proc/self/maps\n  IN_GLOBAL_DATA,    // Is part of global data region of the executable\n  THREAD_DATA,       // Part of a thread stack and a thread descriptor with TLS\n  THREAD_REGISTERS,  // Values in registers of some thread\n};\n\n// Information about an allocated object\nstruct AllocObject {\n  const void* ptr;        // the object\n  uintptr_t size;         // its size\n  ObjectPlacement place;  // where ptr points to\n\n  AllocObject(const void* p, size_t s, ObjectPlacement l)\n    : ptr(p), size(s), place(l) { }\n};\n\n// All objects (memory ranges) ignored via HeapLeakChecker::IgnoreObject\n// Key is the object's address; value is its size.\ntypedef map<uintptr_t, size_t, less<uintptr_t>,\n            STL_Allocator<pair<const uintptr_t, size_t>,\n                          HeapLeakChecker::Allocator>\n           > IgnoredObjectsMap;\nstatic IgnoredObjectsMap* ignored_objects = NULL;\n\n// All objects (memory ranges) that we consider to be the sources of pointers\n// to live (not leaked) objects.\n// At different times this holds (what can be reached from) global data regions\n// and the objects we've been told to ignore.\n// For any AllocObject::ptr \"live_objects\" is supposed to contain at most one\n// record at any time. We maintain this by checking with the heap_profile map\n// of the heap and removing the live heap objects we've handled from it.\n// This vector is maintained as a stack and the frontier of reachable\n// live heap objects in our flood traversal of them.\ntypedef vector<AllocObject,\n               STL_Allocator<AllocObject, HeapLeakChecker::Allocator>\n              > LiveObjectsStack;\nstatic LiveObjectsStack* live_objects = NULL;\n\n// A special string type that uses my allocator\ntypedef basic_string<char, char_traits<char>,\n                     STL_Allocator<char, HeapLeakChecker::Allocator>\n                    > HCL_string;\n\n// A placeholder to fill-in the starting values for live_objects\n// for each library so we can keep the library-name association for logging.\ntypedef map<HCL_string, LiveObjectsStack, less<HCL_string>,\n            STL_Allocator<pair<const HCL_string, LiveObjectsStack>,\n                          HeapLeakChecker::Allocator>\n           > LibraryLiveObjectsStacks;\nstatic LibraryLiveObjectsStacks* library_live_objects = NULL;\n\n// Value stored in the map of disabled address ranges;\n// its key is the end of the address range.\n// We'll ignore allocations with a return address in a disabled range\n// if the address occurs at 'max_depth' or less in the stack trace.\nstruct HeapLeakChecker::RangeValue {\n  uintptr_t start_address;  // the start of the range\n  int       max_depth;      // the maximal stack depth to disable at\n};\ntypedef map<uintptr_t, HeapLeakChecker::RangeValue, less<uintptr_t>,\n            STL_Allocator<pair<const uintptr_t, HeapLeakChecker::RangeValue>,\n                          HeapLeakChecker::Allocator>\n           > DisabledRangeMap;\n// The disabled program counter address ranges for profile dumping\n// that are registered with HeapLeakChecker::DisableChecksFromToLocked.\nstatic DisabledRangeMap* disabled_ranges = NULL;\n\n// Set of stack tops.\n// These are used to consider live only appropriate chunks of the memory areas\n// that are used for stacks (and maybe thread-specific data as well)\n// so that we do not treat pointers from outdated stack frames as live.\ntypedef set<uintptr_t, less<uintptr_t>,\n            STL_Allocator<uintptr_t, HeapLeakChecker::Allocator>\n           > StackTopSet;\nstatic StackTopSet* stack_tops = NULL;\n\n// A map of ranges of code addresses for the system libraries\n// that can mmap/mremap/sbrk-allocate memory regions for stacks\n// and thread-local storage that we want to consider as live global data.\n// Maps from the end address to the start address.\ntypedef map<uintptr_t, uintptr_t, less<uintptr_t>,\n            STL_Allocator<pair<const uintptr_t, uintptr_t>,\n                          HeapLeakChecker::Allocator>\n           > GlobalRegionCallerRangeMap;\nstatic GlobalRegionCallerRangeMap* global_region_caller_ranges = NULL;\n\n// TODO(maxim): make our big data structs into own modules\n\n// Disabler is implemented by keeping track of a per-thread count\n// of active Disabler objects.  Any objects allocated while the\n// count > 0 are not reported.\n\n#ifdef HAVE_TLS\n\nstatic __thread int thread_disable_counter\n// The \"inital exec\" model is faster than the default TLS model, at\n// the cost you can't dlopen this library.  But dlopen on heap-checker\n// doesn't work anyway -- it must run before main -- so this is a good\n// trade-off.\n# ifdef HAVE___ATTRIBUTE__\n   __attribute__ ((tls_model (\"initial-exec\")))\n# endif\n    ;\ninline int get_thread_disable_counter() {\n  return thread_disable_counter;\n}\ninline void set_thread_disable_counter(int value) {\n  thread_disable_counter = value;\n}\n\n#else  // #ifdef HAVE_TLS\n\nstatic pthread_key_t thread_disable_counter_key;\nstatic int main_thread_counter;   // storage for use before main()\nstatic bool use_main_thread_counter = true;\n\n// TODO(csilvers): this is called from NewHook, in the middle of malloc().\n// If perftools_pthread_getspecific calls malloc, that will lead to an\n// infinite loop.  I don't know how to fix that, so I hope it never happens!\ninline int get_thread_disable_counter() {\n  if (use_main_thread_counter)  // means we're running really early\n    return main_thread_counter;\n  void* p = perftools_pthread_getspecific(thread_disable_counter_key);\n  return (intptr_t)p;   // kinda evil: store the counter directly in the void*\n}\n\ninline void set_thread_disable_counter(int value) {\n  if (use_main_thread_counter) {   // means we're running really early\n    main_thread_counter = value;\n    return;\n  }\n  intptr_t pointer_sized_value = value;\n  // kinda evil: store the counter directly in the void*\n  void* p = (void*)pointer_sized_value;\n  // NOTE: this may call malloc, which will call NewHook which will call\n  // get_thread_disable_counter() which will call pthread_getspecific().  I\n  // don't know if anything bad can happen if we call getspecific() in the\n  // middle of a setspecific() call.  It seems to work ok in practice...\n  perftools_pthread_setspecific(thread_disable_counter_key, p);\n}\n\n// The idea here is that this initializer will run pretty late: after\n// pthreads have been totally set up.  At this point we can call\n// pthreads routines, so we set those up.\nclass InitThreadDisableCounter {\n public:\n  InitThreadDisableCounter() {\n    perftools_pthread_key_create(&thread_disable_counter_key, NULL);\n    // Set up the main thread's value, which we have a special variable for.\n    void* p = (void*)main_thread_counter;   // store the counter directly\n    perftools_pthread_setspecific(thread_disable_counter_key, p);\n    use_main_thread_counter = false;\n  }\n};\nInitThreadDisableCounter init_thread_disable_counter;\n\n#endif  // #ifdef HAVE_TLS\n\nHeapLeakChecker::Disabler::Disabler() {\n  // It is faster to unconditionally increment the thread-local\n  // counter than to check whether or not heap-checking is on\n  // in a thread-safe manner.\n  int counter = get_thread_disable_counter();\n  set_thread_disable_counter(counter + 1);\n  RAW_VLOG(10, \"Increasing thread disable counter to %d\", counter + 1);\n}\n\nHeapLeakChecker::Disabler::~Disabler() {\n  int counter = get_thread_disable_counter();\n  RAW_DCHECK(counter > 0, \"\");\n  if (counter > 0) {\n    set_thread_disable_counter(counter - 1);\n    RAW_VLOG(10, \"Decreasing thread disable counter to %d\", counter);\n  } else {\n    RAW_VLOG(0, \"Thread disable counter underflow : %d\", counter);\n  }\n}\n\n//----------------------------------------------------------------------\n\n// The size of the largest heap object allocated so far.\nstatic size_t max_heap_object_size = 0;\n// The possible range of addresses that can point\n// into one of the elements of heap_objects.\nstatic uintptr_t min_heap_address = uintptr_t(-1LL);\nstatic uintptr_t max_heap_address = 0;\n\n//----------------------------------------------------------------------\n\n// Simple casting helpers for uintptr_t and void*:\ntemplate<typename T>\ninline static const void* AsPtr(T addr) {\n  return reinterpret_cast<void*>(addr);\n}\ninline static uintptr_t AsInt(const void* ptr) {\n  return reinterpret_cast<uintptr_t>(ptr);\n}\n\n//----------------------------------------------------------------------\n\n// We've seen reports that strstr causes heap-checker crashes in some\n// libc's (?):\n//    http://code.google.com/p/gperftools/issues/detail?id=263\n// It's simple enough to use our own.  This is not in time-critical code.\nstatic const char* hc_strstr(const char* s1, const char* s2) {\n  const size_t len = strlen(s2);\n  RAW_CHECK(len > 0, \"Unexpected empty string passed to strstr()\");\n  for (const char* p = strchr(s1, *s2); p != NULL; p = strchr(p+1, *s2)) {\n    if (strncmp(p, s2, len) == 0) {\n      return p;\n    }\n  }\n  return NULL;\n}\n\n//----------------------------------------------------------------------\n\n// Our hooks for MallocHook\nstatic void NewHook(const void* ptr, size_t size) {\n  if (ptr != NULL) {\n    const int counter = get_thread_disable_counter();\n    const bool ignore = (counter > 0);\n    RAW_VLOG(16, \"Recording Alloc: %p of %\" PRIuS \"; %d\", ptr, size,\n             int(counter));\n\n    // Fetch the caller's stack trace before acquiring heap_checker_lock.\n    void* stack[HeapProfileTable::kMaxStackDepth];\n    int depth = HeapProfileTable::GetCallerStackTrace(0, stack);\n\n    { SpinLockHolder l(&heap_checker_lock);\n      if (size > max_heap_object_size) max_heap_object_size = size;\n      uintptr_t addr = AsInt(ptr);\n      if (addr < min_heap_address) min_heap_address = addr;\n      addr += size;\n      if (addr > max_heap_address) max_heap_address = addr;\n      if (heap_checker_on) {\n        heap_profile->RecordAlloc(ptr, size, depth, stack);\n        if (ignore) {\n          heap_profile->MarkAsIgnored(ptr);\n        }\n      }\n    }\n    RAW_VLOG(17, \"Alloc Recorded: %p of %\" PRIuS \"\", ptr, size);\n  }\n}\n\nstatic void DeleteHook(const void* ptr) {\n  if (ptr != NULL) {\n    RAW_VLOG(16, \"Recording Free %p\", ptr);\n    { SpinLockHolder l(&heap_checker_lock);\n      if (heap_checker_on) heap_profile->RecordFree(ptr);\n    }\n    RAW_VLOG(17, \"Free Recorded: %p\", ptr);\n  }\n}\n\n//----------------------------------------------------------------------\n\nenum StackDirection {\n  GROWS_TOWARDS_HIGH_ADDRESSES,\n  GROWS_TOWARDS_LOW_ADDRESSES,\n  UNKNOWN_DIRECTION\n};\n\n// Determine which way the stack grows:\n\nstatic StackDirection ATTRIBUTE_NOINLINE GetStackDirection(\n    const uintptr_t *const ptr) {\n  uintptr_t x;\n  if (&x < ptr)\n    return GROWS_TOWARDS_LOW_ADDRESSES;\n  if (ptr < &x)\n    return GROWS_TOWARDS_HIGH_ADDRESSES;\n\n  RAW_CHECK(0, \"\");  // Couldn't determine the stack direction.\n\n  return UNKNOWN_DIRECTION;\n}\n\n// Direction of stack growth (will initialize via GetStackDirection())\nstatic StackDirection stack_direction = UNKNOWN_DIRECTION;\n\n// This routine is called for every thread stack we know about to register it.\nstatic void RegisterStackLocked(const void* top_ptr) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  RAW_DCHECK(MemoryRegionMap::LockIsHeld(), \"\");\n  RAW_VLOG(10, \"Thread stack at %p\", top_ptr);\n  uintptr_t top = AsInt(top_ptr);\n  stack_tops->insert(top);  // add for later use\n\n  // make sure stack_direction is initialized\n  if (stack_direction == UNKNOWN_DIRECTION) {\n    stack_direction = GetStackDirection(&top);\n  }\n\n  // Find memory region with this stack\n  MemoryRegionMap::Region region;\n  if (MemoryRegionMap::FindAndMarkStackRegion(top, &region)) {\n    // Make the proper portion of the stack live:\n    if (stack_direction == GROWS_TOWARDS_LOW_ADDRESSES) {\n      RAW_VLOG(11, \"Live stack at %p of %\" PRIuPTR \" bytes\",\n                  top_ptr, region.end_addr - top);\n      live_objects->push_back(AllocObject(top_ptr, region.end_addr - top,\n                                          THREAD_DATA));\n    } else {  // GROWS_TOWARDS_HIGH_ADDRESSES\n      RAW_VLOG(11, \"Live stack at %p of %\" PRIuPTR \" bytes\",\n                  AsPtr(region.start_addr),\n                  top - region.start_addr);\n      live_objects->push_back(AllocObject(AsPtr(region.start_addr),\n                                          top - region.start_addr,\n                                          THREAD_DATA));\n    }\n  // not in MemoryRegionMap, look in library_live_objects:\n  } else if (FLAGS_heap_check_ignore_global_live) {\n    for (LibraryLiveObjectsStacks::iterator lib = library_live_objects->begin();\n         lib != library_live_objects->end(); ++lib) {\n      for (LiveObjectsStack::iterator span = lib->second.begin();\n           span != lib->second.end(); ++span) {\n        uintptr_t start = AsInt(span->ptr);\n        uintptr_t end = start + span->size;\n        if (start <= top  &&  top < end) {\n          RAW_VLOG(11, \"Stack at %p is inside /proc/self/maps chunk %p..%p\",\n                      top_ptr, AsPtr(start), AsPtr(end));\n          // Shrink start..end region by chopping away the memory regions in\n          // MemoryRegionMap that land in it to undo merging of regions\n          // in /proc/self/maps, so that we correctly identify what portion\n          // of start..end is actually the stack region.\n          uintptr_t stack_start = start;\n          uintptr_t stack_end = end;\n          // can optimize-away this loop, but it does not run often\n          RAW_DCHECK(MemoryRegionMap::LockIsHeld(), \"\");\n          for (MemoryRegionMap::RegionIterator r =\n                 MemoryRegionMap::BeginRegionLocked();\n               r != MemoryRegionMap::EndRegionLocked(); ++r) {\n            if (top < r->start_addr  &&  r->start_addr < stack_end) {\n              stack_end = r->start_addr;\n            }\n            if (stack_start < r->end_addr  &&  r->end_addr <= top) {\n              stack_start = r->end_addr;\n            }\n          }\n          if (stack_start != start  ||  stack_end != end) {\n            RAW_VLOG(11, \"Stack at %p is actually inside memory chunk %p..%p\",\n                        top_ptr, AsPtr(stack_start), AsPtr(stack_end));\n          }\n          // Make the proper portion of the stack live:\n          if (stack_direction == GROWS_TOWARDS_LOW_ADDRESSES) {\n            RAW_VLOG(11, \"Live stack at %p of %\" PRIuPTR \" bytes\",\n                        top_ptr, stack_end - top);\n            live_objects->push_back(\n              AllocObject(top_ptr, stack_end - top, THREAD_DATA));\n          } else {  // GROWS_TOWARDS_HIGH_ADDRESSES\n            RAW_VLOG(11, \"Live stack at %p of %\" PRIuPTR \" bytes\",\n                        AsPtr(stack_start), top - stack_start);\n            live_objects->push_back(\n              AllocObject(AsPtr(stack_start), top - stack_start, THREAD_DATA));\n          }\n          lib->second.erase(span);  // kill the rest of the region\n          // Put the non-stack part(s) of the region back:\n          if (stack_start != start) {\n            lib->second.push_back(AllocObject(AsPtr(start), stack_start - start,\n                                  MAYBE_LIVE));\n          }\n          if (stack_end != end) {\n            lib->second.push_back(AllocObject(AsPtr(stack_end), end - stack_end,\n                                  MAYBE_LIVE));\n          }\n          return;\n        }\n      }\n    }\n    RAW_LOG(ERROR, \"Memory region for stack at %p not found. \"\n                   \"Will likely report false leak positives.\", top_ptr);\n  }\n}\n\n// Iterator for heap allocation map data to make ignored objects \"live\"\n// (i.e., treated as roots for the mark-and-sweep phase)\nstatic void MakeIgnoredObjectsLiveCallbackLocked(\n    const void* ptr, const HeapProfileTable::AllocInfo& info) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  if (info.ignored) {\n    live_objects->push_back(AllocObject(ptr, info.object_size,\n                                        MUST_BE_ON_HEAP));\n  }\n}\n\n// Iterator for heap allocation map data to make objects allocated from\n// disabled regions of code to be live.\nstatic void MakeDisabledLiveCallbackLocked(\n    const void* ptr, const HeapProfileTable::AllocInfo& info) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  bool stack_disable = false;\n  bool range_disable = false;\n  for (int depth = 0; depth < info.stack_depth; depth++) {\n    uintptr_t addr = AsInt(info.call_stack[depth]);\n    if (disabled_ranges) {\n      DisabledRangeMap::const_iterator iter\n        = disabled_ranges->upper_bound(addr);\n      if (iter != disabled_ranges->end()) {\n        RAW_DCHECK(iter->first > addr, \"\");\n        if (iter->second.start_address < addr  &&\n            iter->second.max_depth > depth) {\n          range_disable = true;  // in range; dropping\n          break;\n        }\n      }\n    }\n  }\n  if (stack_disable || range_disable) {\n    uintptr_t start_address = AsInt(ptr);\n    uintptr_t end_address = start_address + info.object_size;\n    StackTopSet::const_iterator iter\n      = stack_tops->lower_bound(start_address);\n    if (iter != stack_tops->end()) {\n      RAW_DCHECK(*iter >= start_address, \"\");\n      if (*iter < end_address) {\n        // We do not disable (treat as live) whole allocated regions\n        // if they are used to hold thread call stacks\n        // (i.e. when we find a stack inside).\n        // The reason is that we'll treat as live the currently used\n        // stack portions anyway (see RegisterStackLocked),\n        // and the rest of the region where the stack lives can well\n        // contain outdated stack variables which are not live anymore,\n        // hence should not be treated as such.\n        RAW_VLOG(11, \"Not %s-disabling %\" PRIuS \" bytes at %p\"\n                    \": have stack inside: %p\",\n                    (stack_disable ? \"stack\" : \"range\"),\n                    info.object_size, ptr, AsPtr(*iter));\n        return;\n      }\n    }\n    RAW_VLOG(11, \"%s-disabling %\" PRIuS \" bytes at %p\",\n                (stack_disable ? \"Stack\" : \"Range\"), info.object_size, ptr);\n    live_objects->push_back(AllocObject(ptr, info.object_size,\n                                        MUST_BE_ON_HEAP));\n  }\n}\n\nstatic const char kUnnamedProcSelfMapEntry[] = \"UNNAMED\";\n\n// This function takes some fields from a /proc/self/maps line:\n//\n//   start_address  start address of a memory region.\n//   end_address    end address of a memory region\n//   permissions    rwx + private/shared bit\n//   filename       filename of the mapped file\n//\n// If the region is not writeable, then it cannot have any heap\n// pointers in it, otherwise we record it as a candidate live region\n// to get filtered later.\nstatic void RecordGlobalDataLocked(uintptr_t start_address,\n                                   uintptr_t end_address,\n                                   const char* permissions,\n                                   const char* filename) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  // Ignore non-writeable regions.\n  if (strchr(permissions, 'w') == NULL) return;\n  if (filename == NULL  ||  *filename == '\\0') {\n    filename = kUnnamedProcSelfMapEntry;\n  }\n  RAW_VLOG(11, \"Looking into %s: 0x%\" PRIxPTR \"..0x%\" PRIxPTR,\n              filename, start_address, end_address);\n  (*library_live_objects)[filename].\n    push_back(AllocObject(AsPtr(start_address),\n                          end_address - start_address,\n                          MAYBE_LIVE));\n}\n\n// See if 'library' from /proc/self/maps has base name 'library_base'\n// i.e. contains it and has '.' or '-' after it.\nstatic bool IsLibraryNamed(const char* library, const char* library_base) {\n  const char* p = hc_strstr(library, library_base);\n  size_t sz = strlen(library_base);\n  return p != NULL  &&  (p[sz] == '.'  ||  p[sz] == '-');\n}\n\n// static\nvoid HeapLeakChecker::DisableLibraryAllocsLocked(const char* library,\n                                                 uintptr_t start_address,\n                                                 uintptr_t end_address) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  int depth = 0;\n  // TODO(maxim): maybe this should be extended to also use objdump\n  //              and pick the text portion of the library more precisely.\n  if (IsLibraryNamed(library, \"/libpthread\")  ||\n        // libpthread has a lot of small \"system\" leaks we don't care about.\n        // In particular it allocates memory to store data supplied via\n        // pthread_setspecific (which can be the only pointer to a heap object).\n      IsLibraryNamed(library, \"/libdl\")  ||\n        // library loaders leak some \"system\" heap that we don't care about\n      IsLibraryNamed(library, \"/libcrypto\")  ||\n        // Sometimes libcrypto of OpenSSH is compiled with -fomit-frame-pointer\n        // (any library can be, of course, but this one often is because speed\n        // is so important for making crypto usable).  We ignore all its\n        // allocations because we can't see the call stacks.  We'd prefer\n        // to ignore allocations done in files/symbols that match\n        // \"default_malloc_ex|default_realloc_ex\"\n        // but that doesn't work when the end-result binary is stripped.\n      IsLibraryNamed(library, \"/libjvm\")  ||\n        // JVM has a lot of leaks we don't care about.\n      IsLibraryNamed(library, \"/libzip\")\n        // The JVM leaks java.util.zip.Inflater after loading classes.\n     ) {\n    depth = 1;  // only disable allocation calls directly from the library code\n  } else if (IsLibraryNamed(library, \"/ld\")\n               // library loader leaks some \"system\" heap\n               // (e.g. thread-local storage) that we don't care about\n            ) {\n    depth = 2;  // disable allocation calls directly from the library code\n                // and at depth 2 from it.\n    // We need depth 2 here solely because of a libc bug that\n    // forces us to jump through __memalign_hook and MemalignOverride hoops\n    // in tcmalloc.cc.\n    // Those buggy __libc_memalign() calls are in ld-linux.so and happen for\n    // thread-local storage allocations that we want to ignore here.\n    // We go with the depth-2 hack as a workaround for this libc bug:\n    // otherwise we'd need to extend MallocHook interface\n    // so that correct stack depth adjustment can be propagated from\n    // the exceptional case of MemalignOverride.\n    // Using depth 2 here should not mask real leaks because ld-linux.so\n    // does not call user code.\n  }\n  if (depth) {\n    RAW_VLOG(10, \"Disabling allocations from %s at depth %d:\", library, depth);\n    DisableChecksFromToLocked(AsPtr(start_address), AsPtr(end_address), depth);\n    if (IsLibraryNamed(library, \"/libpthread\")  ||\n        IsLibraryNamed(library, \"/libdl\")  ||\n        IsLibraryNamed(library, \"/ld\")) {\n      RAW_VLOG(10, \"Global memory regions made by %s will be live data\",\n                  library);\n      if (global_region_caller_ranges == NULL) {\n        global_region_caller_ranges =\n          new(Allocator::Allocate(sizeof(GlobalRegionCallerRangeMap)))\n            GlobalRegionCallerRangeMap;\n      }\n      global_region_caller_ranges\n        ->insert(make_pair(end_address, start_address));\n    }\n  }\n}\n\n// static\nHeapLeakChecker::ProcMapsResult HeapLeakChecker::UseProcMapsLocked(\n                                  ProcMapsTask proc_maps_task) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  // Need to provide own scratch memory to ProcMapsIterator:\n  ProcMapsIterator::Buffer buffer;\n  ProcMapsIterator it(0, &buffer);\n  if (!it.Valid()) {\n    int errsv = errno;\n    RAW_LOG(ERROR, \"Could not open /proc/self/maps: errno=%d. \"\n                   \"Libraries will not be handled correctly.\", errsv);\n    return CANT_OPEN_PROC_MAPS;\n  }\n  uint64 start_address, end_address, file_offset;\n  int64 inode;\n  char *permissions, *filename;\n  bool saw_shared_lib = false;\n  bool saw_nonzero_inode = false;\n  bool saw_shared_lib_with_nonzero_inode = false;\n  while (it.Next(&start_address, &end_address, &permissions,\n                 &file_offset, &inode, &filename)) {\n    if (start_address >= end_address) {\n      // Warn if a line we can be interested in is ill-formed:\n      if (inode != 0) {\n        RAW_LOG(ERROR, \"Errors reading /proc/self/maps. \"\n                       \"Some global memory regions will not \"\n                       \"be handled correctly.\");\n      }\n      // Silently skip other ill-formed lines: some are possible\n      // probably due to the interplay of how /proc/self/maps is updated\n      // while we read it in chunks in ProcMapsIterator and\n      // do things in this loop.\n      continue;\n    }\n    // Determine if any shared libraries are present (this is the same\n    // list of extensions as is found in pprof).  We want to ignore\n    // 'fake' libraries with inode 0 when determining.  However, some\n    // systems don't share inodes via /proc, so we turn off this check\n    // if we don't see any evidence that we're getting inode info.\n    if (inode != 0) {\n      saw_nonzero_inode = true;\n    }\n    if ((hc_strstr(filename, \"lib\") && hc_strstr(filename, \".so\")) ||\n        hc_strstr(filename, \".dll\") ||\n        // not all .dylib filenames start with lib. .dylib is big enough\n        // that we are unlikely to get false matches just checking that.\n        hc_strstr(filename, \".dylib\") || hc_strstr(filename, \".bundle\")) {\n      saw_shared_lib = true;\n      if (inode != 0) {\n        saw_shared_lib_with_nonzero_inode = true;\n      }\n    }\n\n    switch (proc_maps_task) {\n      case DISABLE_LIBRARY_ALLOCS:\n        // All lines starting like\n        // \"401dc000-4030f000 r??p 00132000 03:01 13991972  lib/bin\"\n        // identify a data and code sections of a shared library or our binary\n        if (inode != 0 && strncmp(permissions, \"r-xp\", 4) == 0) {\n          DisableLibraryAllocsLocked(filename, start_address, end_address);\n        }\n        break;\n      case RECORD_GLOBAL_DATA:\n        RecordGlobalDataLocked(start_address, end_address,\n                               permissions, filename);\n        break;\n      default:\n        RAW_CHECK(0, \"\");\n    }\n  }\n  // If /proc/self/maps is reporting inodes properly (we saw a\n  // non-zero inode), then we only say we saw a shared lib if we saw a\n  // 'real' one, with a non-zero inode.\n  if (saw_nonzero_inode) {\n    saw_shared_lib = saw_shared_lib_with_nonzero_inode;\n  }\n  if (!saw_shared_lib) {\n    RAW_LOG(ERROR, \"No shared libs detected. Will likely report false leak \"\n                   \"positives for statically linked executables.\");\n    return NO_SHARED_LIBS_IN_PROC_MAPS;\n  }\n  return PROC_MAPS_USED;\n}\n\n// Total number and size of live objects dropped from the profile;\n// (re)initialized in IgnoreAllLiveObjectsLocked.\nstatic int64 live_objects_total;\nstatic int64 live_bytes_total;\n\n// pid of the thread that is doing the current leak check\n// (protected by our lock; IgnoreAllLiveObjectsLocked sets it)\nstatic pid_t self_thread_pid = 0;\n\n// Status of our thread listing callback execution\n// (protected by our lock; used from within IgnoreAllLiveObjectsLocked)\nstatic enum {\n  CALLBACK_NOT_STARTED,\n  CALLBACK_STARTED,\n  CALLBACK_COMPLETED,\n} thread_listing_status = CALLBACK_NOT_STARTED;\n\n// Ideally to avoid deadlocks this function should not result in any libc\n// or other function calls that might need to lock a mutex:\n// It is called when all threads of a process are stopped\n// at arbitrary points thus potentially holding those locks.\n//\n// In practice we are calling some simple i/o and sprintf-type library functions\n// for logging messages, but use only our own LowLevelAlloc::Arena allocator.\n//\n// This is known to be buggy: the library i/o function calls are able to cause\n// deadlocks when they request a lock that a stopped thread happens to hold.\n// This issue as far as we know have so far not resulted in any deadlocks\n// in practice, so for now we are taking our chance that the deadlocks\n// have insignificant frequency.\n//\n// If such deadlocks become a problem we should make the i/o calls\n// into appropriately direct system calls (or eliminate them),\n// in particular write() is not safe and vsnprintf() is potentially dangerous\n// due to reliance on locale functions (these are called through RAW_LOG\n// and in other ways).\n//\n\n#if defined(HAVE_LINUX_PTRACE_H) && defined(HAVE_SYS_SYSCALL_H) && defined(DUMPER)\n# if (defined(__i386__) || defined(__x86_64))\n#  define THREAD_REGS i386_regs\n# elif defined(__PPC__)\n#  define THREAD_REGS ppc_regs\n# endif\n#endif\n\n/*static*/ int HeapLeakChecker::IgnoreLiveThreadsLocked(void* parameter,\n                                                        int num_threads,\n                                                        pid_t* thread_pids,\n                                                        va_list /*ap*/) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  thread_listing_status = CALLBACK_STARTED;\n  RAW_VLOG(11, \"Found %d threads (from pid %d)\", num_threads, getpid());\n\n  if (FLAGS_heap_check_ignore_global_live) {\n    UseProcMapsLocked(RECORD_GLOBAL_DATA);\n  }\n\n  // We put the registers from other threads here\n  // to make pointers stored in them live.\n  vector<void*, STL_Allocator<void*, Allocator> > thread_registers;\n\n  int failures = 0;\n  for (int i = 0; i < num_threads; ++i) {\n    // the leak checking thread itself is handled\n    // specially via self_thread_stack, not here:\n    if (thread_pids[i] == self_thread_pid) continue;\n    RAW_VLOG(11, \"Handling thread with pid %d\", thread_pids[i]);\n#ifdef THREAD_REGS\n    THREAD_REGS thread_regs;\n#define sys_ptrace(r, p, a, d)  syscall(SYS_ptrace, (r), (p), (a), (d))\n    // We use sys_ptrace to avoid thread locking\n    // because this is called from TCMalloc_ListAllProcessThreads\n    // when all but this thread are suspended.\n    if (sys_ptrace(PTRACE_GETREGS, thread_pids[i], NULL, &thread_regs) == 0) {\n      // Need to use SP to get all the data from the very last stack frame:\n      COMPILE_ASSERT(sizeof(thread_regs.SP) == sizeof(void*),\n                     SP_register_does_not_look_like_a_pointer);\n      RegisterStackLocked(reinterpret_cast<void*>(thread_regs.SP));\n      // Make registers live (just in case PTRACE_ATTACH resulted in some\n      // register pointers still being in the registers and not on the stack):\n      for (void** p = reinterpret_cast<void**>(&thread_regs);\n           p < reinterpret_cast<void**>(&thread_regs + 1); ++p) {\n        RAW_VLOG(12, \"Thread register %p\", *p);\n        thread_registers.push_back(*p);\n      }\n    } else {\n      failures += 1;\n    }\n#else\n    failures += 1;\n#endif\n  }\n  // Use all the collected thread (stack) liveness sources:\n  IgnoreLiveObjectsLocked(\"threads stack data\", \"\");\n  if (thread_registers.size()) {\n    // Make thread registers be live heap data sources.\n    // we rely here on the fact that vector is in one memory chunk:\n    RAW_VLOG(11, \"Live registers at %p of %\" PRIuS \" bytes\",\n                &thread_registers[0], thread_registers.size() * sizeof(void*));\n    live_objects->push_back(AllocObject(&thread_registers[0],\n                                        thread_registers.size() * sizeof(void*),\n                                        THREAD_REGISTERS));\n    IgnoreLiveObjectsLocked(\"threads register data\", \"\");\n  }\n  // Do all other liveness walking while all threads are stopped:\n  IgnoreNonThreadLiveObjectsLocked();\n  // Can now resume the threads:\n  TCMalloc_ResumeAllProcessThreads(num_threads, thread_pids);\n  thread_listing_status = CALLBACK_COMPLETED;\n  return failures;\n}\n\n// Stack top of the thread that is doing the current leak check\n// (protected by our lock; IgnoreAllLiveObjectsLocked sets it)\nstatic const void* self_thread_stack_top;\n\n// static\nvoid HeapLeakChecker::IgnoreNonThreadLiveObjectsLocked() {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  RAW_DCHECK(MemoryRegionMap::LockIsHeld(), \"\");\n  RAW_VLOG(11, \"Handling self thread with pid %d\", self_thread_pid);\n  // Register our own stack:\n\n  // Important that all stack ranges (including the one here)\n  // are known before we start looking at them\n  // in MakeDisabledLiveCallbackLocked:\n  RegisterStackLocked(self_thread_stack_top);\n  IgnoreLiveObjectsLocked(\"stack data\", \"\");\n\n  // Make objects we were told to ignore live:\n  if (ignored_objects) {\n    for (IgnoredObjectsMap::const_iterator object = ignored_objects->begin();\n         object != ignored_objects->end(); ++object) {\n      const void* ptr = AsPtr(object->first);\n      RAW_VLOG(11, \"Ignored live object at %p of %\" PRIuS \" bytes\",\n                  ptr, object->second);\n      live_objects->\n        push_back(AllocObject(ptr, object->second, MUST_BE_ON_HEAP));\n      // we do this liveness check for ignored_objects before doing any\n      // live heap walking to make sure it does not fail needlessly:\n      size_t object_size;\n      if (!(heap_profile->FindAlloc(ptr, &object_size)  &&\n            object->second == object_size)) {\n        RAW_LOG(FATAL, \"Object at %p of %\" PRIuS \" bytes from an\"\n                       \" IgnoreObject() has disappeared\", ptr, object->second);\n      }\n    }\n    IgnoreLiveObjectsLocked(\"ignored objects\", \"\");\n  }\n\n  // Treat objects that were allocated when a Disabler was live as\n  // roots.  I.e., if X was allocated while a Disabler was active,\n  // and Y is reachable from X, arrange that neither X nor Y are\n  // treated as leaks.\n  heap_profile->IterateAllocs(MakeIgnoredObjectsLiveCallbackLocked);\n  IgnoreLiveObjectsLocked(\"disabled objects\", \"\");\n\n  // Make code-address-disabled objects live and ignored:\n  // This in particular makes all thread-specific data live\n  // because the basic data structure to hold pointers to thread-specific data\n  // is allocated from libpthreads and we have range-disabled that\n  // library code with UseProcMapsLocked(DISABLE_LIBRARY_ALLOCS);\n  // so now we declare all thread-specific data reachable from there as live.\n  heap_profile->IterateAllocs(MakeDisabledLiveCallbackLocked);\n  IgnoreLiveObjectsLocked(\"disabled code\", \"\");\n\n  // Actually make global data live:\n  if (FLAGS_heap_check_ignore_global_live) {\n    bool have_null_region_callers = false;\n    for (LibraryLiveObjectsStacks::iterator l = library_live_objects->begin();\n         l != library_live_objects->end(); ++l) {\n      RAW_CHECK(live_objects->empty(), \"\");\n      // Process library_live_objects in l->second\n      // filtering them by MemoryRegionMap:\n      // It's safe to iterate over MemoryRegionMap\n      // w/o locks here as we are inside MemoryRegionMap::Lock():\n      RAW_DCHECK(MemoryRegionMap::LockIsHeld(), \"\");\n      // The only change to MemoryRegionMap possible in this loop\n      // is region addition as a result of allocating more memory\n      // for live_objects. This won't invalidate the RegionIterator\n      // or the intent of the loop.\n      // --see the comment by MemoryRegionMap::BeginRegionLocked().\n      for (MemoryRegionMap::RegionIterator region =\n             MemoryRegionMap::BeginRegionLocked();\n           region != MemoryRegionMap::EndRegionLocked(); ++region) {\n        // \"region\" from MemoryRegionMap is to be subtracted from\n        // (tentatively live) regions in l->second\n        // if it has a stack inside or it was allocated by\n        // a non-special caller (not one covered by a range\n        // in global_region_caller_ranges).\n        // This will in particular exclude all memory chunks used\n        // by the heap itself as well as what's been allocated with\n        // any allocator on top of mmap.\n        bool subtract = true;\n        if (!region->is_stack  &&  global_region_caller_ranges) {\n          if (region->caller() == static_cast<uintptr_t>(NULL)) {\n            have_null_region_callers = true;\n          } else {\n            GlobalRegionCallerRangeMap::const_iterator iter\n              = global_region_caller_ranges->upper_bound(region->caller());\n            if (iter != global_region_caller_ranges->end()) {\n              RAW_DCHECK(iter->first > region->caller(), \"\");\n              if (iter->second < region->caller()) {  // in special region\n                subtract = false;\n              }\n            }\n          }\n        }\n        if (subtract) {\n          // The loop puts the result of filtering l->second into live_objects:\n          for (LiveObjectsStack::const_iterator i = l->second.begin();\n               i != l->second.end(); ++i) {\n            // subtract *region from *i\n            uintptr_t start = AsInt(i->ptr);\n            uintptr_t end = start + i->size;\n            if (region->start_addr <= start  &&  end <= region->end_addr) {\n              // full deletion due to subsumption\n            } else if (start < region->start_addr  &&\n                       region->end_addr < end) {  // cutting-out split\n              live_objects->push_back(AllocObject(i->ptr,\n                                                  region->start_addr - start,\n                                                  IN_GLOBAL_DATA));\n              live_objects->push_back(AllocObject(AsPtr(region->end_addr),\n                                                  end - region->end_addr,\n                                                  IN_GLOBAL_DATA));\n            } else if (region->end_addr > start  &&\n                       region->start_addr <= start) {  // cut from start\n              live_objects->push_back(AllocObject(AsPtr(region->end_addr),\n                                                  end - region->end_addr,\n                                                  IN_GLOBAL_DATA));\n            } else if (region->start_addr > start  &&\n                       region->start_addr < end) {  // cut from end\n              live_objects->push_back(AllocObject(i->ptr,\n                                                  region->start_addr - start,\n                                                  IN_GLOBAL_DATA));\n            } else {  // pass: no intersection\n              live_objects->push_back(AllocObject(i->ptr, i->size,\n                                                  IN_GLOBAL_DATA));\n            }\n          }\n          // Move live_objects back into l->second\n          // for filtering by the next region.\n          live_objects->swap(l->second);\n          live_objects->clear();\n        }\n      }\n      // Now get and use live_objects from the final version of l->second:\n      if (VLOG_IS_ON(11)) {\n        for (LiveObjectsStack::const_iterator i = l->second.begin();\n             i != l->second.end(); ++i) {\n          RAW_VLOG(11, \"Library live region at %p of %\" PRIuPTR \" bytes\",\n                      i->ptr, i->size);\n        }\n      }\n      live_objects->swap(l->second);\n      IgnoreLiveObjectsLocked(\"in globals of\\n  \", l->first.c_str());\n    }\n    if (have_null_region_callers) {\n      RAW_LOG(ERROR, \"Have memory regions w/o callers: \"\n                     \"might report false leaks\");\n    }\n    Allocator::DeleteAndNull(&library_live_objects);\n  }\n}\n\n// Callback for TCMalloc_ListAllProcessThreads in IgnoreAllLiveObjectsLocked below\n// to test/verify that we have just the one main thread, in which case\n// we can do everything in that main thread,\n// so that CPU profiler can collect all its samples.\n// Returns the number of threads in the process.\nstatic int IsOneThread(void* parameter, int num_threads,\n                       pid_t* thread_pids, va_list ap) {\n  if (num_threads != 1) {\n    RAW_LOG(WARNING, \"Have threads: Won't CPU-profile the bulk of leak \"\n                     \"checking work happening in IgnoreLiveThreadsLocked!\");\n  }\n  TCMalloc_ResumeAllProcessThreads(num_threads, thread_pids);\n  return num_threads;\n}\n\n// Dummy for IgnoreAllLiveObjectsLocked below.\n// Making it global helps with compiler warnings.\nstatic va_list dummy_ap;\n\n// static\nvoid HeapLeakChecker::IgnoreAllLiveObjectsLocked(const void* self_stack_top) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  RAW_CHECK(live_objects == NULL, \"\");\n  live_objects = new(Allocator::Allocate(sizeof(LiveObjectsStack)))\n                   LiveObjectsStack;\n  stack_tops = new(Allocator::Allocate(sizeof(StackTopSet))) StackTopSet;\n  // reset the counts\n  live_objects_total = 0;\n  live_bytes_total = 0;\n  // Reduce max_heap_object_size to FLAGS_heap_check_max_pointer_offset\n  // for the time of leak check.\n  // FLAGS_heap_check_max_pointer_offset caps max_heap_object_size\n  // to manage reasonably low chances of random bytes\n  // appearing to be pointing into large actually leaked heap objects.\n  const size_t old_max_heap_object_size = max_heap_object_size;\n  max_heap_object_size = (\n    FLAGS_heap_check_max_pointer_offset != -1\n    ? min(size_t(FLAGS_heap_check_max_pointer_offset), max_heap_object_size)\n    : max_heap_object_size);\n  // Record global data as live:\n  if (FLAGS_heap_check_ignore_global_live) {\n    library_live_objects =\n      new(Allocator::Allocate(sizeof(LibraryLiveObjectsStacks)))\n        LibraryLiveObjectsStacks;\n  }\n  // Ignore all thread stacks:\n  thread_listing_status = CALLBACK_NOT_STARTED;\n  bool need_to_ignore_non_thread_objects = true;\n  self_thread_pid = getpid();\n  self_thread_stack_top = self_stack_top;\n  if (FLAGS_heap_check_ignore_thread_live) {\n    // In case we are doing CPU profiling we'd like to do all the work\n    // in the main thread, not in the special thread created by\n    // TCMalloc_ListAllProcessThreads, so that CPU profiler can\n    // collect all its samples.  The machinery of\n    // TCMalloc_ListAllProcessThreads conflicts with the CPU profiler\n    // by also relying on signals and ::sigaction.  We can do this\n    // (run everything in the main thread) safely only if there's just\n    // the main thread itself in our process.  This variable reflects\n    // these two conditions:\n    bool want_and_can_run_in_main_thread =\n      ProfilingIsEnabledForAllThreads()  &&\n      TCMalloc_ListAllProcessThreads(NULL, IsOneThread) == 1;\n    // When the normal path of TCMalloc_ListAllProcessThreads below is taken,\n    // we fully suspend the threads right here before any liveness checking\n    // and keep them suspended for the whole time of liveness checking\n    // inside of the IgnoreLiveThreadsLocked callback.\n    // (The threads can't (de)allocate due to lock on the delete hook but\n    //  if not suspended they could still mess with the pointer\n    //  graph while we walk it).\n    int r = want_and_can_run_in_main_thread\n            ? IgnoreLiveThreadsLocked(NULL, 1, &self_thread_pid, dummy_ap)\n            : TCMalloc_ListAllProcessThreads(NULL, IgnoreLiveThreadsLocked);\n    need_to_ignore_non_thread_objects = r < 0;\n    if (r < 0) {\n      RAW_LOG(WARNING, \"Thread finding failed with %d errno=%d\", r, errno);\n      if (thread_listing_status == CALLBACK_COMPLETED) {\n        RAW_LOG(INFO, \"Thread finding callback \"\n                      \"finished ok; hopefully everything is fine\");\n        need_to_ignore_non_thread_objects = false;\n      } else if (thread_listing_status == CALLBACK_STARTED) {\n        RAW_LOG(FATAL, \"Thread finding callback was \"\n                       \"interrupted or crashed; can't fix this\");\n      } else {  // CALLBACK_NOT_STARTED\n        RAW_LOG(ERROR, \"Could not find thread stacks. \"\n                       \"Will likely report false leak positives.\");\n      }\n    } else if (r != 0) {\n      RAW_LOG(ERROR, \"Thread stacks not found for %d threads. \"\n                     \"Will likely report false leak positives.\", r);\n    } else {\n      RAW_VLOG(11, \"Thread stacks appear to be found for all threads\");\n    }\n  } else {\n    RAW_LOG(WARNING, \"Not looking for thread stacks; \"\n                     \"objects reachable only from there \"\n                     \"will be reported as leaks\");\n  }\n  // Do all other live data ignoring here if we did not do it\n  // within thread listing callback with all threads stopped.\n  if (need_to_ignore_non_thread_objects) {\n    if (FLAGS_heap_check_ignore_global_live) {\n      UseProcMapsLocked(RECORD_GLOBAL_DATA);\n    }\n    IgnoreNonThreadLiveObjectsLocked();\n  }\n  if (live_objects_total) {\n    RAW_VLOG(10, \"Ignoring %\" PRId64 \" reachable objects of %\" PRId64 \" bytes\",\n                live_objects_total, live_bytes_total);\n  }\n  // Free these: we made them here and heap_profile never saw them\n  Allocator::DeleteAndNull(&live_objects);\n  Allocator::DeleteAndNull(&stack_tops);\n  max_heap_object_size = old_max_heap_object_size;  // reset this var\n}\n\n// Alignment at which we should consider pointer positions\n// in IgnoreLiveObjectsLocked. Will normally use the value of\n// FLAGS_heap_check_pointer_source_alignment.\nstatic size_t pointer_source_alignment = kPointerSourceAlignment;\n// Global lock for HeapLeakChecker::DoNoLeaks\n// to protect pointer_source_alignment.\nstatic SpinLock alignment_checker_lock(SpinLock::LINKER_INITIALIZED);\n\n// This function changes the live bits in the heap_profile-table's state:\n// we only record the live objects to be skipped.\n//\n// When checking if a byte sequence points to a heap object we use\n// HeapProfileTable::FindInsideAlloc to handle both pointers to\n// the start and inside of heap-allocated objects.\n// The \"inside\" case needs to be checked to support\n// at least the following relatively common cases:\n// - C++ arrays allocated with new FooClass[size] for classes\n//   with destructors have their size recorded in a sizeof(int) field\n//   before the place normal pointers point to.\n// - basic_string<>-s for e.g. the C++ library of gcc 3.4\n//   have the meta-info in basic_string<...>::_Rep recorded\n//   before the place normal pointers point to.\n// - Multiple-inherited objects have their pointers when cast to\n//   different base classes pointing inside of the actually\n//   allocated object.\n// - Sometimes reachability pointers point to member objects of heap objects,\n//   and then those member objects point to the full heap object.\n// - Third party UnicodeString: it stores a 32-bit refcount\n//   (in both 32-bit and 64-bit binaries) as the first uint32\n//   in the allocated memory and a normal pointer points at\n//   the second uint32 behind the refcount.\n// By finding these additional objects here\n// we slightly increase the chance to mistake random memory bytes\n// for a pointer and miss a leak in a particular run of a binary.\n//\n/*static*/ void HeapLeakChecker::IgnoreLiveObjectsLocked(const char* name,\n                                                         const char* name2) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  int64 live_object_count = 0;\n  int64 live_byte_count = 0;\n  while (!live_objects->empty()) {\n    const char* object =\n      reinterpret_cast<const char*>(live_objects->back().ptr);\n    size_t size = live_objects->back().size;\n    const ObjectPlacement place = live_objects->back().place;\n    live_objects->pop_back();\n    if (place == MUST_BE_ON_HEAP  &&  heap_profile->MarkAsLive(object)) {\n      live_object_count += 1;\n      live_byte_count += size;\n    }\n    RAW_VLOG(13, \"Looking for heap pointers in %p of %\" PRIuS \" bytes\",\n                object, size);\n    const char* const whole_object = object;\n    size_t const whole_size = size;\n    // Try interpretting any byte sequence in object,size as a heap pointer:\n    const size_t remainder = AsInt(object) % pointer_source_alignment;\n    if (remainder) {\n      object += pointer_source_alignment - remainder;\n      if (size >= pointer_source_alignment - remainder) {\n        size -= pointer_source_alignment - remainder;\n      } else {\n        size = 0;\n      }\n    }\n    if (size < sizeof(void*)) continue;\n\n#ifdef NO_FRAME_POINTER\n    // Frame pointer omission requires us to use libunwind, which uses direct\n    // mmap and munmap system calls, and that needs special handling.\n    if (name2 == kUnnamedProcSelfMapEntry) {\n      static const uintptr_t page_mask = ~(getpagesize() - 1);\n      const uintptr_t addr = reinterpret_cast<uintptr_t>(object);\n      if ((addr & page_mask) == 0 && (size & page_mask) == 0) {\n        // This is an object we slurped from /proc/self/maps.\n        // It may or may not be readable at this point.\n        //\n        // In case all the above conditions made a mistake, and the object is\n        // not related to libunwind, we also verify that it's not readable\n        // before ignoring it.\n        if (msync(const_cast<char*>(object), size, MS_ASYNC) != 0) {\n          // Skip unreadable object, so we don't crash trying to sweep it.\n          RAW_VLOG(0, \"Ignoring inaccessible object [%p, %p) \"\n                   \"(msync error %d (%s))\",\n                   object, object + size, errno, strerror(errno));\n          continue;\n        }\n      }\n    }\n#endif\n\n    const char* const max_object = object + size - sizeof(void*);\n    while (object <= max_object) {\n      // potentially unaligned load:\n      const uintptr_t addr = *reinterpret_cast<const uintptr_t*>(object);\n      // Do fast check before the more expensive HaveOnHeapLocked lookup:\n      // this code runs for all memory words that are potentially pointers:\n      const bool can_be_on_heap =\n        // Order tests by the likelyhood of the test failing in 64/32 bit modes.\n        // Yes, this matters: we either lose 5..6% speed in 32 bit mode\n        // (which is already slower) or by a factor of 1.5..1.91 in 64 bit mode.\n        // After the alignment test got dropped the above performance figures\n        // must have changed; might need to revisit this.\n#if defined(__x86_64__)\n        addr <= max_heap_address  &&  // <= is for 0-sized object with max addr\n        min_heap_address <= addr;\n#else\n        min_heap_address <= addr  &&\n        addr <= max_heap_address;  // <= is for 0-sized object with max addr\n#endif\n      if (can_be_on_heap) {\n        const void* ptr = reinterpret_cast<const void*>(addr);\n        // Too expensive (inner loop): manually uncomment when debugging:\n        // RAW_VLOG(17, \"Trying pointer to %p at %p\", ptr, object);\n        size_t object_size;\n        if (HaveOnHeapLocked(&ptr, &object_size)  &&\n            heap_profile->MarkAsLive(ptr)) {\n          // We take the (hopefully low) risk here of encountering by accident\n          // a byte sequence in memory that matches an address of\n          // a heap object which is in fact leaked.\n          // I.e. in very rare and probably not repeatable/lasting cases\n          // we might miss some real heap memory leaks.\n          RAW_VLOG(14, \"Found pointer to %p of %\" PRIuS \" bytes at %p \"\n                      \"inside %p of size %\" PRIuS \"\",\n                      ptr, object_size, object, whole_object, whole_size);\n          if (VLOG_IS_ON(15)) {\n            // log call stacks to help debug how come something is not a leak\n            HeapProfileTable::AllocInfo alloc;\n            if (!heap_profile->FindAllocDetails(ptr, &alloc)) {\n              RAW_LOG(FATAL, \"FindAllocDetails failed on ptr %p\", ptr);\n            }\n            RAW_LOG(INFO, \"New live %p object's alloc stack:\", ptr);\n            for (int i = 0; i < alloc.stack_depth; ++i) {\n              RAW_LOG(INFO, \"  @ %p\", alloc.call_stack[i]);\n            }\n          }\n          live_object_count += 1;\n          live_byte_count += object_size;\n          live_objects->push_back(AllocObject(ptr, object_size,\n                                              IGNORED_ON_HEAP));\n        }\n      }\n      object += pointer_source_alignment;\n    }\n  }\n  live_objects_total += live_object_count;\n  live_bytes_total += live_byte_count;\n  if (live_object_count) {\n    RAW_VLOG(10, \"Removed %\" PRId64 \" live heap objects of %\" PRId64 \" bytes: %s%s\",\n                live_object_count, live_byte_count, name, name2);\n  }\n}\n\n//----------------------------------------------------------------------\n// HeapLeakChecker leak check disabling components\n//----------------------------------------------------------------------\n\n// static\nvoid HeapLeakChecker::DisableChecksIn(const char* pattern) {\n  RAW_LOG(WARNING, \"DisableChecksIn(%s) is ignored\", pattern);\n}\n\n// static\nvoid HeapLeakChecker::DoIgnoreObject(const void* ptr) {\n  SpinLockHolder l(&heap_checker_lock);\n  if (!heap_checker_on) return;\n  size_t object_size;\n  if (!HaveOnHeapLocked(&ptr, &object_size)) {\n    RAW_LOG(ERROR, \"No live heap object at %p to ignore\", ptr);\n  } else {\n    RAW_VLOG(10, \"Going to ignore live object at %p of %\" PRIuS \" bytes\",\n                ptr, object_size);\n    if (ignored_objects == NULL)  {\n      ignored_objects = new(Allocator::Allocate(sizeof(IgnoredObjectsMap)))\n                          IgnoredObjectsMap;\n    }\n    if (!ignored_objects->insert(make_pair(AsInt(ptr), object_size)).second) {\n      RAW_LOG(WARNING, \"Object at %p is already being ignored\", ptr);\n    }\n  }\n}\n\n// static\nvoid HeapLeakChecker::UnIgnoreObject(const void* ptr) {\n  SpinLockHolder l(&heap_checker_lock);\n  if (!heap_checker_on) return;\n  size_t object_size;\n  if (!HaveOnHeapLocked(&ptr, &object_size)) {\n    RAW_LOG(FATAL, \"No live heap object at %p to un-ignore\", ptr);\n  } else {\n    bool found = false;\n    if (ignored_objects) {\n      IgnoredObjectsMap::iterator object = ignored_objects->find(AsInt(ptr));\n      if (object != ignored_objects->end()  &&  object_size == object->second) {\n        ignored_objects->erase(object);\n        found = true;\n        RAW_VLOG(10, \"Now not going to ignore live object \"\n                    \"at %p of %\" PRIuS \" bytes\", ptr, object_size);\n      }\n    }\n    if (!found)  RAW_LOG(FATAL, \"Object at %p has not been ignored\", ptr);\n  }\n}\n\n//----------------------------------------------------------------------\n// HeapLeakChecker non-static functions\n//----------------------------------------------------------------------\n\nchar* HeapLeakChecker::MakeProfileNameLocked() {\n  RAW_DCHECK(lock_->IsHeld(), \"\");\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  const int len = profile_name_prefix->size() + strlen(name_) + 5 +\n                  strlen(HeapProfileTable::kFileExt) + 1;\n  char* file_name = reinterpret_cast<char*>(Allocator::Allocate(len));\n  snprintf(file_name, len, \"%s.%s-end%s\",\n           profile_name_prefix->c_str(), name_,\n           HeapProfileTable::kFileExt);\n  return file_name;\n}\n\nvoid HeapLeakChecker::Create(const char *name, bool make_start_snapshot) {\n  SpinLockHolder l(lock_);\n  name_ = NULL;  // checker is inactive\n  start_snapshot_ = NULL;\n  has_checked_ = false;\n  inuse_bytes_increase_ = 0;\n  inuse_allocs_increase_ = 0;\n  keep_profiles_ = false;\n  char* n = new char[strlen(name) + 1];   // do this before we lock\n  IgnoreObject(n);  // otherwise it might be treated as live due to our stack\n  { // Heap activity in other threads is paused for this whole scope.\n    SpinLockHolder al(&alignment_checker_lock);\n    SpinLockHolder hl(&heap_checker_lock);\n    MemoryRegionMap::LockHolder ml;\n    if (heap_checker_on  &&  profile_name_prefix != NULL) {\n      RAW_DCHECK(strchr(name, '/') == NULL, \"must be a simple name\");\n      memcpy(n, name, strlen(name) + 1);\n      name_ = n;  // checker is active\n      if (make_start_snapshot) {\n        start_snapshot_ = heap_profile->TakeSnapshot();\n      }\n\n      const HeapProfileTable::Stats& t = heap_profile->total();\n      const size_t start_inuse_bytes = t.alloc_size - t.free_size;\n      const size_t start_inuse_allocs = t.allocs - t.frees;\n      RAW_VLOG(10, \"Start check \\\"%s\\\" profile: %\" PRIuS \" bytes \"\n               \"in %\" PRIuS \" objects\",\n               name_, start_inuse_bytes, start_inuse_allocs);\n    } else {\n      RAW_LOG(WARNING, \"Heap checker is not active, \"\n                       \"hence checker \\\"%s\\\" will do nothing!\", name);\n    RAW_LOG(WARNING, \"To activate set the HEAPCHECK environment variable.\\n\");\n    }\n  }\n  if (name_ == NULL) {\n    UnIgnoreObject(n);\n    delete[] n;  // must be done after we unlock\n  }\n}\n\nHeapLeakChecker::HeapLeakChecker(const char *name) : lock_(new SpinLock) {\n  RAW_DCHECK(strcmp(name, \"_main_\") != 0, \"_main_ is reserved\");\n  Create(name, true/*create start_snapshot_*/);\n}\n\nHeapLeakChecker::HeapLeakChecker() : lock_(new SpinLock) {\n  if (FLAGS_heap_check_before_constructors) {\n    // We want to check for leaks of objects allocated during global\n    // constructors (i.e., objects allocated already).  So we do not\n    // create a baseline snapshot and hence check for leaks of objects\n    // that may have already been created.\n    Create(\"_main_\", false);\n  } else {\n    // We want to ignore leaks of objects allocated during global\n    // constructors (i.e., objects allocated already).  So we snapshot\n    // the current heap contents and use them as a baseline that is\n    // not reported by the leak checker.\n    Create(\"_main_\", true);\n  }\n}\n\nssize_t HeapLeakChecker::BytesLeaked() const {\n  SpinLockHolder l(lock_);\n  if (!has_checked_) {\n    RAW_LOG(FATAL, \"*NoLeaks|SameHeap must execute before this call\");\n  }\n  return inuse_bytes_increase_;\n}\n\nssize_t HeapLeakChecker::ObjectsLeaked() const {\n  SpinLockHolder l(lock_);\n  if (!has_checked_) {\n    RAW_LOG(FATAL, \"*NoLeaks|SameHeap must execute before this call\");\n  }\n  return inuse_allocs_increase_;\n}\n\n// Save pid of main thread for using in naming dump files\nstatic int32 main_thread_pid = getpid();\n#ifdef HAVE_PROGRAM_INVOCATION_NAME\n#ifdef __UCLIBC__\nextern const char* program_invocation_name;\nextern const char* program_invocation_short_name;\n#else\nextern char* program_invocation_name;\nextern char* program_invocation_short_name;\n#endif\nstatic const char* invocation_name() { return program_invocation_short_name; }\nstatic string invocation_path() { return program_invocation_name; }\n#else\nstatic const char* invocation_name() { return \"<your binary>\"; }\nstatic string invocation_path() { return \"<your binary>\"; }\n#endif\n\n// Prints commands that users can run to get more information\n// about the reported leaks.\nstatic void SuggestPprofCommand(const char* pprof_file_arg) {\n  // Extra help information to print for the user when the test is\n  // being run in a way where the straightforward pprof command will\n  // not suffice.\n  string extra_help;\n\n  // Common header info to print for remote runs\n  const string remote_header =\n      \"This program is being executed remotely and therefore the pprof\\n\"\n      \"command printed above will not work.  Either run this program\\n\"\n      \"locally, or adjust the pprof command as follows to allow it to\\n\"\n      \"work on your local machine:\\n\";\n\n  // Extra command for fetching remote data\n  string fetch_cmd;\n\n  RAW_LOG(WARNING,\n          \"\\n\\n\"\n          \"If the preceding stack traces are not enough to find \"\n          \"the leaks, try running THIS shell command:\\n\\n\"\n          \"%s%s %s \\\"%s\\\" --inuse_objects --lines --heapcheck \"\n          \" --edgefraction=1e-10 --nodefraction=1e-10 --gv\\n\"\n          \"\\n\"\n          \"%s\"\n          \"If you are still puzzled about why the leaks are \"\n          \"there, try rerunning this program with \"\n          \"HEAP_CHECK_TEST_POINTER_ALIGNMENT=1 and/or with \"\n          \"HEAP_CHECK_MAX_POINTER_OFFSET=-1\\n\"\n          \"If the leak report occurs in a small fraction of runs, \"\n          \"try running with TCMALLOC_MAX_FREE_QUEUE_SIZE of few hundred MB \"\n          \"or with TCMALLOC_RECLAIM_MEMORY=false, \"  // only works for debugalloc\n          \"it might help find leaks more repeatably\\n\",\n          fetch_cmd.c_str(),\n          \"pprof\",           // works as long as pprof is on your path\n          invocation_path().c_str(),\n          pprof_file_arg,\n          extra_help.c_str()\n          );\n}\n\nbool HeapLeakChecker::DoNoLeaks(ShouldSymbolize should_symbolize) {\n  SpinLockHolder l(lock_);\n  // The locking also helps us keep the messages\n  // for the two checks close together.\n  SpinLockHolder al(&alignment_checker_lock);\n\n  // thread-safe: protected by alignment_checker_lock\n  static bool have_disabled_hooks_for_symbolize = false;\n  // Once we've checked for leaks and symbolized the results once, it's\n  // not safe to do it again.  This is because in order to symbolize\n  // safely, we had to disable all the malloc hooks here, so we no\n  // longer can be confident we've collected all the data we need.\n  if (have_disabled_hooks_for_symbolize) {\n    RAW_LOG(FATAL, \"Must not call heap leak checker manually after \"\n            \" program-exit's automatic check.\");\n  }\n\n  HeapProfileTable::Snapshot* leaks = NULL;\n  char* pprof_file = NULL;\n\n  {\n    // Heap activity in other threads is paused during this function\n    // (i.e. until we got all profile difference info).\n    SpinLockHolder hl(&heap_checker_lock);\n    if (heap_checker_on == false) {\n      if (name_ != NULL) {  // leak checking enabled when created the checker\n        RAW_LOG(WARNING, \"Heap leak checker got turned off after checker \"\n                \"\\\"%s\\\" has been created, no leak check is being done for it!\",\n                name_);\n      }\n      return true;\n    }\n\n    // Update global_region_caller_ranges. They may need to change since\n    // e.g. initialization because shared libraries might have been loaded or\n    // unloaded.\n    Allocator::DeleteAndNullIfNot(&global_region_caller_ranges);\n    ProcMapsResult pm_result = UseProcMapsLocked(DISABLE_LIBRARY_ALLOCS);\n    RAW_CHECK(pm_result == PROC_MAPS_USED, \"\");\n\n    // Keep track of number of internally allocated objects so we\n    // can detect leaks in the heap-leak-checket itself\n    const int initial_allocs = Allocator::alloc_count();\n\n    if (name_ == NULL) {\n      RAW_LOG(FATAL, \"Heap leak checker must not be turned on \"\n              \"after construction of a HeapLeakChecker\");\n    }\n\n    MemoryRegionMap::LockHolder ml;\n    int a_local_var;  // Use our stack ptr to make stack data live:\n\n    // Make the heap profile, other threads are locked out.\n    HeapProfileTable::Snapshot* base =\n        reinterpret_cast<HeapProfileTable::Snapshot*>(start_snapshot_);\n    RAW_DCHECK(FLAGS_heap_check_pointer_source_alignment > 0, \"\");\n    pointer_source_alignment = FLAGS_heap_check_pointer_source_alignment;\n    IgnoreAllLiveObjectsLocked(&a_local_var);\n    leaks = heap_profile->NonLiveSnapshot(base);\n\n    inuse_bytes_increase_ = static_cast<ssize_t>(leaks->total().alloc_size);\n    inuse_allocs_increase_ = static_cast<ssize_t>(leaks->total().allocs);\n    if (leaks->Empty()) {\n      heap_profile->ReleaseSnapshot(leaks);\n      leaks = NULL;\n\n      // We can only check for internal leaks along the no-user-leak\n      // path since in the leak path we temporarily release\n      // heap_checker_lock and another thread can come in and disturb\n      // allocation counts.\n      if (Allocator::alloc_count() != initial_allocs) {\n        RAW_LOG(FATAL, \"Internal HeapChecker leak of %d objects ; %d -> %d\",\n                Allocator::alloc_count() - initial_allocs,\n                initial_allocs, Allocator::alloc_count());\n      }\n    } else if (FLAGS_heap_check_test_pointer_alignment) {\n      if (pointer_source_alignment == 1) {\n        RAW_LOG(WARNING, \"--heap_check_test_pointer_alignment has no effect: \"\n                \"--heap_check_pointer_source_alignment was already set to 1\");\n      } else {\n        // Try with reduced pointer aligment\n        pointer_source_alignment = 1;\n        IgnoreAllLiveObjectsLocked(&a_local_var);\n        HeapProfileTable::Snapshot* leaks_wo_align =\n            heap_profile->NonLiveSnapshot(base);\n        pointer_source_alignment = FLAGS_heap_check_pointer_source_alignment;\n        if (leaks_wo_align->Empty()) {\n          RAW_LOG(WARNING, \"Found no leaks without pointer alignment: \"\n                  \"something might be placing pointers at \"\n                  \"unaligned addresses! This needs to be fixed.\");\n        } else {\n          RAW_LOG(INFO, \"Found leaks without pointer alignment as well: \"\n                  \"unaligned pointers must not be the cause of leaks.\");\n          RAW_LOG(INFO, \"--heap_check_test_pointer_alignment did not help \"\n                  \"to diagnose the leaks.\");\n        }\n        heap_profile->ReleaseSnapshot(leaks_wo_align);\n      }\n    }\n\n    if (leaks != NULL) {\n      pprof_file = MakeProfileNameLocked();\n    }\n  }\n\n  has_checked_ = true;\n  if (leaks == NULL) {\n    if (FLAGS_heap_check_max_pointer_offset == -1) {\n      RAW_LOG(WARNING,\n              \"Found no leaks without max_pointer_offset restriction: \"\n              \"it's possible that the default value of \"\n              \"heap_check_max_pointer_offset flag is too low. \"\n              \"Do you use pointers with larger than that offsets \"\n              \"pointing in the middle of heap-allocated objects?\");\n    }\n    const HeapProfileTable::Stats& stats = heap_profile->total();\n    RAW_VLOG(heap_checker_info_level,\n             \"No leaks found for check \\\"%s\\\" \"\n             \"(but no 100%% guarantee that there aren't any): \"\n             \"found %\" PRId64 \" reachable heap objects of %\" PRId64 \" bytes\",\n             name_,\n             int64(stats.allocs - stats.frees),\n             int64(stats.alloc_size - stats.free_size));\n  } else {\n    if (should_symbolize == SYMBOLIZE) {\n      // To turn addresses into symbols, we need to fork, which is a\n      // problem if both parent and child end up trying to call the\n      // same malloc-hooks we've set up, at the same time.  To avoid\n      // trouble, we turn off the hooks before symbolizing.  Note that\n      // this makes it unsafe to ever leak-report again!  Luckily, we\n      // typically only want to report once in a program's run, at the\n      // very end.\n      if (MallocHook::GetNewHook() == NewHook)\n        MallocHook::SetNewHook(NULL);\n      if (MallocHook::GetDeleteHook() == DeleteHook)\n        MallocHook::SetDeleteHook(NULL);\n      MemoryRegionMap::Shutdown();\n      // Make sure all the hooks really got unset:\n      RAW_CHECK(MallocHook::GetNewHook() == NULL, \"\");\n      RAW_CHECK(MallocHook::GetDeleteHook() == NULL, \"\");\n      RAW_CHECK(MallocHook::GetMmapHook() == NULL, \"\");\n      RAW_CHECK(MallocHook::GetSbrkHook() == NULL, \"\");\n      have_disabled_hooks_for_symbolize = true;\n      leaks->ReportLeaks(name_, pprof_file, true);  // true = should_symbolize\n    } else {\n      leaks->ReportLeaks(name_, pprof_file, false);\n    }\n    if (FLAGS_heap_check_identify_leaks) {\n      leaks->ReportIndividualObjects();\n    }\n\n    SuggestPprofCommand(pprof_file);\n\n    {\n      SpinLockHolder hl(&heap_checker_lock);\n      heap_profile->ReleaseSnapshot(leaks);\n      Allocator::Free(pprof_file);\n    }\n  }\n\n  return (leaks == NULL);\n}\n\nHeapLeakChecker::~HeapLeakChecker() {\n  if (name_ != NULL) {  // had leak checking enabled when created the checker\n    if (!has_checked_) {\n      RAW_LOG(FATAL, \"Some *NoLeaks|SameHeap method\"\n                     \" must be called on any created HeapLeakChecker\");\n    }\n\n    // Deallocate any snapshot taken at start\n    if (start_snapshot_ != NULL) {\n      SpinLockHolder l(&heap_checker_lock);\n      heap_profile->ReleaseSnapshot(\n          reinterpret_cast<HeapProfileTable::Snapshot*>(start_snapshot_));\n    }\n\n    UnIgnoreObject(name_);\n    delete[] name_;\n    name_ = NULL;\n  }\n  delete lock_;\n}\n\n//----------------------------------------------------------------------\n// HeapLeakChecker overall heap check components\n//----------------------------------------------------------------------\n\n// static\nbool HeapLeakChecker::IsActive() {\n  SpinLockHolder l(&heap_checker_lock);\n  return heap_checker_on;\n}\n\nvector<HeapCleaner::void_function>* HeapCleaner::heap_cleanups_ = NULL;\n\n// When a HeapCleaner object is intialized, add its function to the static list\n// of cleaners to be run before leaks checking.\nHeapCleaner::HeapCleaner(void_function f) {\n  if (heap_cleanups_ == NULL)\n    heap_cleanups_ = new vector<HeapCleaner::void_function>;\n  heap_cleanups_->push_back(f);\n}\n\n// Run all of the cleanup functions and delete the vector.\nvoid HeapCleaner::RunHeapCleanups() {\n  if (!heap_cleanups_)\n    return;\n  for (int i = 0; i < heap_cleanups_->size(); i++) {\n    void (*f)(void) = (*heap_cleanups_)[i];\n    f();\n  }\n  delete heap_cleanups_;\n  heap_cleanups_ = NULL;\n}\n\n// Program exit heap cleanup registered as a module object destructor.\n// Will not get executed when we crash on a signal.\n//\nvoid HeapLeakChecker_RunHeapCleanups() {\n  if (FLAGS_heap_check == \"local\")   // don't check heap in this mode\n    return;\n  { SpinLockHolder l(&heap_checker_lock);\n    // can get here (via forks?) with other pids\n    if (heap_checker_pid != getpid()) return;\n  }\n  HeapCleaner::RunHeapCleanups();\n  if (!FLAGS_heap_check_after_destructors) HeapLeakChecker::DoMainHeapCheck();\n}\n\nstatic bool internal_init_start_has_run = false;\n\n// Called exactly once, before main() (but hopefully just before).\n// This picks a good unique name for the dumped leak checking heap profiles.\n//\n// Because we crash when InternalInitStart is called more than once,\n// it's fine that we hold heap_checker_lock only around pieces of\n// this function: this is still enough for thread-safety w.r.t. other functions\n// of this module.\n// We can't hold heap_checker_lock throughout because it would deadlock\n// on a memory allocation since our new/delete hooks can be on.\n//\nvoid HeapLeakChecker_InternalInitStart() {\n  { SpinLockHolder l(&heap_checker_lock);\n    RAW_CHECK(!internal_init_start_has_run,\n              \"Heap-check constructor called twice.  Perhaps you both linked\"\n              \" in the heap checker, and also used LD_PRELOAD to load it?\");\n    internal_init_start_has_run = true;\n\n#ifdef ADDRESS_SANITIZER\n    // AddressSanitizer's custom malloc conflicts with HeapChecker.\n    FLAGS_heap_check = \"\";\n#endif\n\n    if (FLAGS_heap_check.empty()) {\n      // turns out we do not need checking in the end; can stop profiling\n      HeapLeakChecker::TurnItselfOffLocked();\n      return;\n    } else if (RunningOnValgrind()) {\n      // There is no point in trying -- we'll just fail.\n      RAW_LOG(WARNING, \"Can't run under Valgrind; will turn itself off\");\n      HeapLeakChecker::TurnItselfOffLocked();\n      return;\n    }\n  }\n\n  // Changing this to false can be useful when debugging heap-checker itself:\n  if (!FLAGS_heap_check_run_under_gdb && IsDebuggerAttached()) {\n    RAW_LOG(WARNING, \"Someone is ptrace()ing us; will turn itself off\");\n    SpinLockHolder l(&heap_checker_lock);\n    HeapLeakChecker::TurnItselfOffLocked();\n    return;\n  }\n\n  { SpinLockHolder l(&heap_checker_lock);\n    if (!constructor_heap_profiling) {\n      RAW_LOG(FATAL, \"Can not start so late. You have to enable heap checking \"\n\t             \"with HEAPCHECK=<mode>.\");\n    }\n  }\n\n  // Set all flags\n  RAW_DCHECK(FLAGS_heap_check_pointer_source_alignment > 0, \"\");\n  if (FLAGS_heap_check == \"minimal\") {\n    // The least we can check.\n    FLAGS_heap_check_before_constructors = false;  // from after main\n                                                   // (ignore more)\n    FLAGS_heap_check_after_destructors = false;  // to after cleanup\n                                                 // (most data is live)\n    FLAGS_heap_check_ignore_thread_live = true;  // ignore all live\n    FLAGS_heap_check_ignore_global_live = true;  // ignore all live\n  } else if (FLAGS_heap_check == \"normal\") {\n    // Faster than 'minimal' and not much stricter.\n    FLAGS_heap_check_before_constructors = true;  // from no profile (fast)\n    FLAGS_heap_check_after_destructors = false;  // to after cleanup\n                                                 // (most data is live)\n    FLAGS_heap_check_ignore_thread_live = true;  // ignore all live\n    FLAGS_heap_check_ignore_global_live = true;  // ignore all live\n  } else if (FLAGS_heap_check == \"strict\") {\n    // A bit stricter than 'normal': global destructors must fully clean up\n    // after themselves if they are present.\n    FLAGS_heap_check_before_constructors = true;  // from no profile (fast)\n    FLAGS_heap_check_after_destructors = true;  // to after destructors\n                                                // (less data live)\n    FLAGS_heap_check_ignore_thread_live = true;  // ignore all live\n    FLAGS_heap_check_ignore_global_live = true;  // ignore all live\n  } else if (FLAGS_heap_check == \"draconian\") {\n    // Drop not very portable and not very exact live heap flooding.\n    FLAGS_heap_check_before_constructors = true;  // from no profile (fast)\n    FLAGS_heap_check_after_destructors = true;  // to after destructors\n                                                // (need them)\n    FLAGS_heap_check_ignore_thread_live = false;  // no live flood (stricter)\n    FLAGS_heap_check_ignore_global_live = false;  // no live flood (stricter)\n  } else if (FLAGS_heap_check == \"as-is\") {\n    // do nothing: use other flags as is\n  } else if (FLAGS_heap_check == \"local\") {\n    // do nothing\n  } else {\n    RAW_LOG(FATAL, \"Unsupported heap_check flag: %s\",\n                   FLAGS_heap_check.c_str());\n  }\n  // FreeBSD doesn't seem to honor atexit execution order:\n  //    http://code.google.com/p/gperftools/issues/detail?id=375\n  // Since heap-checking before destructors depends on atexit running\n  // at the right time, on FreeBSD we always check after, even in the\n  // less strict modes.  This just means FreeBSD is always a bit\n  // stricter in its checking than other OSes.\n  // This now appears to be the case in other OSes as well;\n  // so always check afterwards.\n  FLAGS_heap_check_after_destructors = true;\n\n  { SpinLockHolder l(&heap_checker_lock);\n    RAW_DCHECK(heap_checker_pid == getpid(), \"\");\n    heap_checker_on = true;\n    RAW_DCHECK(heap_profile, \"\");\n    HeapLeakChecker::ProcMapsResult pm_result = HeapLeakChecker::UseProcMapsLocked(HeapLeakChecker::DISABLE_LIBRARY_ALLOCS);\n      // might neeed to do this more than once\n      // if one later dynamically loads libraries that we want disabled\n    if (pm_result != HeapLeakChecker::PROC_MAPS_USED) {  // can't function\n      HeapLeakChecker::TurnItselfOffLocked();\n      return;\n    }\n  }\n\n  // make a good place and name for heap profile leak dumps\n  string* profile_prefix =\n    new string(FLAGS_heap_check_dump_directory + \"/\" + invocation_name());\n\n  // Finalize prefix for dumping leak checking profiles.\n  const int32 our_pid = getpid();   // safest to call getpid() outside lock\n  { SpinLockHolder l(&heap_checker_lock);\n    // main_thread_pid might still be 0 if this function is being called before\n    // global constructors.  In that case, our pid *is* the main pid.\n    if (main_thread_pid == 0)\n      main_thread_pid = our_pid;\n  }\n  char pid_buf[15];\n  snprintf(pid_buf, sizeof(pid_buf), \".%d\", main_thread_pid);\n  *profile_prefix += pid_buf;\n  { SpinLockHolder l(&heap_checker_lock);\n    RAW_DCHECK(profile_name_prefix == NULL, \"\");\n    profile_name_prefix = profile_prefix;\n  }\n\n  // Make sure new/delete hooks are installed properly\n  // and heap profiler is indeed able to keep track\n  // of the objects being allocated.\n  // We test this to make sure we are indeed checking for leaks.\n  char* test_str = new char[5];\n  size_t size;\n  { SpinLockHolder l(&heap_checker_lock);\n    RAW_CHECK(heap_profile->FindAlloc(test_str, &size),\n              \"our own new/delete not linked?\");\n  }\n  delete[] test_str;\n  { SpinLockHolder l(&heap_checker_lock);\n    // This check can fail when it should not if another thread allocates\n    // into this same spot right this moment,\n    // which is unlikely since this code runs in InitGoogle.\n    RAW_CHECK(!heap_profile->FindAlloc(test_str, &size),\n              \"our own new/delete not linked?\");\n  }\n  // If we crash in the above code, it probably means that\n  // \"nm <this_binary> | grep new\" will show that tcmalloc's new/delete\n  // implementation did not get linked-in into this binary\n  // (i.e. nm will list __builtin_new and __builtin_vec_new as undefined).\n  // If this happens, it is a BUILD bug to be fixed.\n\n  RAW_VLOG(heap_checker_info_level,\n           \"WARNING: Perftools heap leak checker is active \"\n           \"-- Performance may suffer\");\n\n  if (FLAGS_heap_check != \"local\") {\n    HeapLeakChecker* main_hc = new HeapLeakChecker();\n    SpinLockHolder l(&heap_checker_lock);\n    RAW_DCHECK(main_heap_checker == NULL,\n               \"Repeated creation of main_heap_checker\");\n    main_heap_checker = main_hc;\n    do_main_heap_check = true;\n  }\n\n  { SpinLockHolder l(&heap_checker_lock);\n    RAW_CHECK(heap_checker_on  &&  constructor_heap_profiling,\n              \"Leak checking is expected to be fully turned on now\");\n  }\n\n  // For binaries built in debug mode, this will set release queue of\n  // debugallocation.cc to 100M to make it less likely for real leaks to\n  // be hidden due to reuse of heap memory object addresses.\n  // Running a test with --malloc_reclaim_memory=0 would help find leaks even\n  // better, but the test might run out of memory as a result.\n  // The scenario is that a heap object at address X is allocated and freed,\n  // but some other data-structure still retains a pointer to X.\n  // Then the same heap memory is used for another object, which is leaked,\n  // but the leak is not noticed due to the pointer to the original object at X.\n  // TODO(csilvers): support this in some manner.\n#if 0\n  SetCommandLineOptionWithMode(\"max_free_queue_size\", \"104857600\",  // 100M\n                               SET_FLAG_IF_DEFAULT);\n#endif\n}\n\n// We want this to run early as well, but not so early as\n// ::BeforeConstructors (we want flag assignments to have already\n// happened, for instance).  Initializer-registration does the trick.\nREGISTER_MODULE_INITIALIZER(init_start, HeapLeakChecker_InternalInitStart());\nREGISTER_MODULE_DESTRUCTOR(init_start, HeapLeakChecker_RunHeapCleanups());\n\n// static\nbool HeapLeakChecker::NoGlobalLeaksMaybeSymbolize(\n    ShouldSymbolize should_symbolize) {\n  // we never delete or change main_heap_checker once it's set:\n  HeapLeakChecker* main_hc = GlobalChecker();\n  if (main_hc) {\n    RAW_VLOG(10, \"Checking for whole-program memory leaks\");\n    return main_hc->DoNoLeaks(should_symbolize);\n  }\n  return true;\n}\n\n// static\nbool HeapLeakChecker::DoMainHeapCheck() {\n  if (FLAGS_heap_check_delay_seconds > 0) {\n    sleep(FLAGS_heap_check_delay_seconds);\n  }\n  { SpinLockHolder l(&heap_checker_lock);\n    if (!do_main_heap_check) return false;\n    RAW_DCHECK(heap_checker_pid == getpid(), \"\");\n    do_main_heap_check = false;  // will do it now; no need to do it more\n  }\n\n  // The program is over, so it's safe to symbolize addresses (which\n  // requires a fork) because no serious work is expected to be done\n  // after this.  Symbolizing is really useful -- knowing what\n  // function has a leak is better than knowing just an address --\n  // and while we can only safely symbolize once in a program run,\n  // now is the time (after all, there's no \"later\" that would be better).\n  if (!NoGlobalLeaksMaybeSymbolize(SYMBOLIZE)) {\n    if (FLAGS_heap_check_identify_leaks) {\n      RAW_LOG(FATAL, \"Whole-program memory leaks found.\");\n    }\n    RAW_LOG(ERROR, \"Exiting with error code (instead of crashing) \"\n                   \"because of whole-program memory leaks\");\n    _exit(1);    // we don't want to call atexit() routines!\n  }\n  return true;\n}\n\n// static\nHeapLeakChecker* HeapLeakChecker::GlobalChecker() {\n  SpinLockHolder l(&heap_checker_lock);\n  return main_heap_checker;\n}\n\n// static\nbool HeapLeakChecker::NoGlobalLeaks() {\n  // symbolizing requires a fork, which isn't safe to do in general.\n  return NoGlobalLeaksMaybeSymbolize(DO_NOT_SYMBOLIZE);\n}\n\n// static\nvoid HeapLeakChecker::CancelGlobalCheck() {\n  SpinLockHolder l(&heap_checker_lock);\n  if (do_main_heap_check) {\n    RAW_VLOG(heap_checker_info_level,\n             \"Canceling the automatic at-exit whole-program memory leak check\");\n    do_main_heap_check = false;\n  }\n}\n\n// static\nvoid HeapLeakChecker::BeforeConstructorsLocked() {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  RAW_CHECK(!constructor_heap_profiling,\n            \"BeforeConstructorsLocked called multiple times\");\n#ifdef ADDRESS_SANITIZER\n  // AddressSanitizer's custom malloc conflicts with HeapChecker.\n  return;\n#endif\n  // Set hooks early to crash if 'new' gets called before we make heap_profile,\n  // and make sure no other hooks existed:\n  RAW_CHECK(MallocHook::AddNewHook(&NewHook), \"\");\n  RAW_CHECK(MallocHook::AddDeleteHook(&DeleteHook), \"\");\n  constructor_heap_profiling = true;\n  MemoryRegionMap::Init(1, /* use_buckets */ false);\n    // Set up MemoryRegionMap with (at least) one caller stack frame to record\n    // (important that it's done before HeapProfileTable creation below).\n  Allocator::Init();\n  RAW_CHECK(heap_profile == NULL, \"\");\n  heap_profile = new(Allocator::Allocate(sizeof(HeapProfileTable)))\n      HeapProfileTable(&Allocator::Allocate, &Allocator::Free,\n                       /* profile_mmap */ false);\n  RAW_VLOG(10, \"Starting tracking the heap\");\n  heap_checker_on = true;\n}\n\n// static\nvoid HeapLeakChecker::TurnItselfOffLocked() {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  // Set FLAGS_heap_check to \"\", for users who test for it\n  if (!FLAGS_heap_check.empty())  // be a noop in the common case\n    FLAGS_heap_check.clear();     // because clear() could allocate memory\n  if (constructor_heap_profiling) {\n    RAW_CHECK(heap_checker_on, \"\");\n    RAW_VLOG(heap_checker_info_level, \"Turning perftools heap leak checking off\");\n    heap_checker_on = false;\n    // Unset our hooks checking they were set:\n    RAW_CHECK(MallocHook::RemoveNewHook(&NewHook), \"\");\n    RAW_CHECK(MallocHook::RemoveDeleteHook(&DeleteHook), \"\");\n    Allocator::DeleteAndNull(&heap_profile);\n    // free our optional global data:\n    Allocator::DeleteAndNullIfNot(&ignored_objects);\n    Allocator::DeleteAndNullIfNot(&disabled_ranges);\n    Allocator::DeleteAndNullIfNot(&global_region_caller_ranges);\n    Allocator::Shutdown();\n    MemoryRegionMap::Shutdown();\n  }\n  RAW_CHECK(!heap_checker_on, \"\");\n}\n\nextern bool heap_leak_checker_bcad_variable;  // in heap-checker-bcad.cc\n\nstatic bool has_called_before_constructors = false;\n\n// TODO(maxim): inline this function with\n// MallocHook_InitAtFirstAllocation_HeapLeakChecker, and also rename\n// HeapLeakChecker::BeforeConstructorsLocked.\nvoid HeapLeakChecker_BeforeConstructors() {\n  SpinLockHolder l(&heap_checker_lock);\n  // We can be called from several places: the first mmap/sbrk/alloc call\n  // or the first global c-tor from heap-checker-bcad.cc:\n  // Do not re-execute initialization:\n  if (has_called_before_constructors) return;\n  has_called_before_constructors = true;\n\n  heap_checker_pid = getpid();  // set it always\n  heap_leak_checker_bcad_variable = true;\n  // just to reference it, so that heap-checker-bcad.o is linked in\n\n  // This function can be called *very* early, before the normal\n  // global-constructor that sets FLAGS_verbose.  Set it manually now,\n  // so the RAW_LOG messages here are controllable.\n  const char* verbose_str = GetenvBeforeMain(\"PERFTOOLS_VERBOSE\");\n  if (verbose_str && atoi(verbose_str)) {  // different than the default of 0?\n    FLAGS_verbose = atoi(verbose_str);\n  }\n\n  bool need_heap_check = true;\n  // The user indicates a desire for heap-checking via the HEAPCHECK\n  // environment variable.  If it's not set, there's no way to do\n  // heap-checking.\n  if (!GetenvBeforeMain(\"HEAPCHECK\")) {\n    need_heap_check = false;\n  }\n#ifdef HAVE_GETEUID\n  if (need_heap_check && getuid() != geteuid()) {\n    // heap-checker writes out files.  Thus, for security reasons, we don't\n    // recognize the env. var. to turn on heap-checking if we're setuid.\n    RAW_LOG(WARNING, (\"HeapChecker: ignoring HEAPCHECK because \"\n                      \"program seems to be setuid\\n\"));\n    need_heap_check = false;\n  }\n#endif\n  if (need_heap_check) {\n    HeapLeakChecker::BeforeConstructorsLocked();\n  }\n}\n\n// This function overrides the weak function defined in malloc_hook.cc and\n// called by one of the initial malloc hooks (malloc_hook.cc) when the very\n// first memory allocation or an mmap/sbrk happens.  This ensures that\n// HeapLeakChecker is initialized and installs all its hooks early enough to\n// track absolutely all memory allocations and all memory region acquisitions\n// via mmap and sbrk.\nextern \"C\" void MallocHook_InitAtFirstAllocation_HeapLeakChecker() {\n  HeapLeakChecker_BeforeConstructors();\n}\n\n// This function is executed after all global object destructors run.\nvoid HeapLeakChecker_AfterDestructors() {\n  { SpinLockHolder l(&heap_checker_lock);\n    // can get here (via forks?) with other pids\n    if (heap_checker_pid != getpid()) return;\n  }\n  if (FLAGS_heap_check_after_destructors) {\n    if (HeapLeakChecker::DoMainHeapCheck()) {\n      const struct timespec sleep_time = { 0, 500000000 };  // 500 ms\n      nanosleep(&sleep_time, NULL);\n        // Need this hack to wait for other pthreads to exit.\n        // Otherwise tcmalloc find errors\n        // on a free() call from pthreads.\n    }\n  }\n  SpinLockHolder l(&heap_checker_lock);\n  RAW_CHECK(!do_main_heap_check, \"should have done it\");\n}\n\n//----------------------------------------------------------------------\n// HeapLeakChecker disabling helpers\n//----------------------------------------------------------------------\n\n// These functions are at the end of the file to prevent their inlining:\n\n// static\nvoid HeapLeakChecker::DisableChecksFromToLocked(const void* start_address,\n                                                const void* end_address,\n                                                int max_depth) {\n  RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  RAW_DCHECK(start_address < end_address, \"\");\n  if (disabled_ranges == NULL) {\n    disabled_ranges = new(Allocator::Allocate(sizeof(DisabledRangeMap)))\n                        DisabledRangeMap;\n  }\n  RangeValue value;\n  value.start_address = AsInt(start_address);\n  value.max_depth = max_depth;\n  if (disabled_ranges->insert(make_pair(AsInt(end_address), value)).second) {\n    RAW_VLOG(10, \"Disabling leak checking in stack traces \"\n                \"under frame addresses between %p..%p\",\n                start_address, end_address);\n  } else {  // check that this is just a verbatim repetition\n    RangeValue const& val = disabled_ranges->find(AsInt(end_address))->second;\n    if (val.max_depth != value.max_depth  ||\n        val.start_address != value.start_address) {\n      RAW_LOG(FATAL, \"Two DisableChecksToHereFrom calls conflict: \"\n                     \"(%p, %p, %d) vs. (%p, %p, %d)\",\n                     AsPtr(val.start_address), end_address, val.max_depth,\n                     start_address, end_address, max_depth);\n    }\n  }\n}\n\n// static\ninline bool HeapLeakChecker::HaveOnHeapLocked(const void** ptr,\n                                              size_t* object_size) {\n  // Commented-out because HaveOnHeapLocked is very performance-critical:\n  // RAW_DCHECK(heap_checker_lock.IsHeld(), \"\");\n  const uintptr_t addr = AsInt(*ptr);\n  if (heap_profile->FindInsideAlloc(\n        *ptr, max_heap_object_size, ptr, object_size)) {\n    RAW_VLOG(16, \"Got pointer into %p at +%\" PRIuPTR \" offset\",\n             *ptr, addr - AsInt(*ptr));\n    return true;\n  }\n  return false;\n}\n\n// static\nconst void* HeapLeakChecker::GetAllocCaller(void* ptr) {\n  // this is used only in the unittest, so the heavy checks are fine\n  HeapProfileTable::AllocInfo info;\n  { SpinLockHolder l(&heap_checker_lock);\n    RAW_CHECK(heap_profile->FindAllocDetails(ptr, &info), \"\");\n  }\n  RAW_CHECK(info.stack_depth >= 1, \"\");\n  return info.call_stack[0];\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/src/thread_cache.h": "// -*- Mode: C++; c-basic-offset: 2; indent-tabs-mode: nil -*-\n// Copyright (c) 2008, Google Inc.\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n// ---\n// Author: Sanjay Ghemawat <opensource@google.com>\n\n#ifndef TCMALLOC_THREAD_CACHE_H_\n#define TCMALLOC_THREAD_CACHE_H_\n\n#include <config.h>\n#ifdef HAVE_PTHREAD\n#include <pthread.h>                    // for pthread_t, pthread_key_t\n#endif\n#include <stddef.h>                     // for size_t, NULL\n#ifdef HAVE_STDINT_H\n#include <stdint.h>                     // for uint32_t, uint64_t\n#endif\n#include <sys/types.h>                  // for ssize_t\n#include \"common.h\"\n#include \"linked_list.h\"\n#include \"maybe_threads.h\"\n#include \"page_heap_allocator.h\"\n#include \"sampler.h\"\n#include \"static_vars.h\"\n\n#include \"common.h\"            // for SizeMap, kMaxSize, etc\n#include \"internal_logging.h\"  // for ASSERT, etc\n#include \"linked_list.h\"       // for SLL_Pop, SLL_PopRange, etc\n#include \"page_heap_allocator.h\"  // for PageHeapAllocator\n#include \"sampler.h\"           // for Sampler\n#include \"static_vars.h\"       // for Static\n\nnamespace tcmalloc {\n\n//-------------------------------------------------------------------\n// Data kept per thread\n//-------------------------------------------------------------------\n\nclass ThreadCache {\n public:\n#ifdef HAVE_TLS\n  enum { have_tls = true };\n#else\n  enum { have_tls = false };\n#endif\n\n  // All ThreadCache objects are kept in a linked list (for stats collection)\n  ThreadCache* next_;\n  ThreadCache* prev_;\n\n  void Init(pthread_t tid);\n  void Cleanup();\n\n  // Accessors (mostly just for printing stats)\n  int freelist_length(size_t cl) const { return list_[cl].length(); }\n\n  // Total byte size in cache\n  size_t Size() const { return size_; }\n\n  // Allocate an object of the given size and class. The size given\n  // must be the same as the size of the class in the size map.\n  void* Allocate(size_t size, size_t cl);\n  void Deallocate(void* ptr, size_t size_class);\n\n  void Scavenge();\n\n  int GetSamplePeriod();\n\n  // Record allocation of \"k\" bytes.  Return true iff allocation\n  // should be sampled\n  bool SampleAllocation(size_t k);\n\n  static void         InitModule();\n  static void         DeInitModule ();\n  static void         InitTSD();\n  static void         DeInitTSD();\n  static ThreadCache* GetThreadHeap();\n  static ThreadCache* GetCache();\n  static ThreadCache* GetCacheIfPresent();\n  static ThreadCache* GetCacheWhichMustBePresent();\n  static ThreadCache* CreateCacheIfNecessary();\n  static void         BecomeIdle();\n  static size_t       MinSizeForSlowPath();\n  static void         SetMinSizeForSlowPath(size_t size);\n\n  static bool IsFastPathAllowed() { return MinSizeForSlowPath() != 0; }\n\n  // Return the number of thread heaps in use.\n  static inline int HeapsInUse();\n\n  // Adds to *total_bytes the total number of bytes used by all thread heaps.\n  // Also, if class_count is not NULL, it must be an array of size kNumClasses,\n  // and this function will increment each element of class_count by the number\n  // of items in all thread-local freelists of the corresponding size class.\n  // REQUIRES: Static::pageheap_lock is held.\n  static void GetThreadStats(uint64_t* total_bytes, uint64_t* class_count);\n\n  // Sets the total thread cache size to new_size, recomputing the\n  // individual thread cache sizes as necessary.\n  // REQUIRES: Static::pageheap lock is held.\n  static void set_overall_thread_cache_size(size_t new_size);\n  static size_t overall_thread_cache_size() {\n    return overall_thread_cache_size_;\n  }\n\n private:\n  class FreeList {\n   private:\n    void*    list_;       // Linked list of nodes\n\n#ifdef _LP64\n    // On 64-bit hardware, manipulating 16-bit values may be slightly slow.\n    uint32_t length_;      // Current length.\n    uint32_t lowater_;     // Low water mark for list length.\n    uint32_t max_length_;  // Dynamic max list length based on usage.\n    // Tracks the number of times a deallocation has caused\n    // length_ > max_length_.  After the kMaxOverages'th time, max_length_\n    // shrinks and length_overages_ is reset to zero.\n    uint32_t length_overages_;\n#else\n    // If we aren't using 64-bit pointers then pack these into less space.\n    uint16_t length_;\n    uint16_t lowater_;\n    uint16_t max_length_;\n    uint16_t length_overages_;\n#endif\n\n   public:\n    void Init() {\n      list_ = NULL;\n      length_ = 0;\n      lowater_ = 0;\n      max_length_ = 1;\n      length_overages_ = 0;\n    }\n\n    // Return current length of list\n    size_t length() const {\n      return length_;\n    }\n\n    // Return the maximum length of the list.\n    size_t max_length() const {\n      return max_length_;\n    }\n\n    // Set the maximum length of the list.  If 'new_max' > length(), the\n    // client is responsible for removing objects from the list.\n    void set_max_length(size_t new_max) {\n      max_length_ = new_max;\n    }\n\n    // Return the number of times that length() has gone over max_length().\n    size_t length_overages() const {\n      return length_overages_;\n    }\n\n    void set_length_overages(size_t new_count) {\n      length_overages_ = new_count;\n    }\n\n    // Is list empty?\n    bool empty() const {\n      return list_ == NULL;\n    }\n\n    // Low-water mark management\n    int lowwatermark() const { return lowater_; }\n    void clear_lowwatermark() { lowater_ = length_; }\n\n    void Push(void* ptr) {\n      SLL_Push(&list_, ptr);\n      length_++;\n    }\n\n    void* Pop() {\n      ASSERT(list_ != NULL);\n      length_--;\n      if (length_ < lowater_) lowater_ = length_;\n      return SLL_Pop(&list_);\n    }\n\n    void* Next() {\n      return SLL_Next(&list_);\n    }\n\n    void PushRange(int N, void *start, void *end) {\n      SLL_PushRange(&list_, start, end);\n      length_ += N;\n    }\n\n    void PopRange(int N, void **start, void **end) {\n      SLL_PopRange(&list_, N, start, end);\n      ASSERT(length_ >= N);\n      length_ -= N;\n      if (length_ < lowater_) lowater_ = length_;\n    }\n  };\n\n  // Gets and returns an object from the central cache, and, if possible,\n  // also adds some objects of that size class to this thread cache.\n  void* FetchFromCentralCache(size_t cl, size_t byte_size);\n\n  // Releases some number of items from src.  Adjusts the list's max_length\n  // to eventually converge on num_objects_to_move(cl).\n  void ListTooLong(FreeList* src, size_t cl);\n\n  // Releases N items from this thread cache.\n  void ReleaseToCentralCache(FreeList* src, size_t cl, int N);\n\n  // Increase max_size_ by reducing unclaimed_cache_space_ or by\n  // reducing the max_size_ of some other thread.  In both cases,\n  // the delta is kStealAmount.\n  void IncreaseCacheLimit();\n  // Same as above but requires Static::pageheap_lock() is held.\n  void IncreaseCacheLimitLocked();\n\n  // If TLS is available, we also store a copy of the per-thread object\n  // in a __thread variable since __thread variables are faster to read\n  // than pthread_getspecific().  We still need pthread_setspecific()\n  // because __thread variables provide no way to run cleanup code when\n  // a thread is destroyed.\n  // We also give a hint to the compiler to use the \"initial exec\" TLS\n  // model.  This is faster than the default TLS model, at the cost that\n  // you cannot dlopen this library.  (To see the difference, look at\n  // the CPU use of __tls_get_addr with and without this attribute.)\n  // Since we don't really use dlopen in google code -- and using dlopen\n  // on a malloc replacement is asking for trouble in any case -- that's\n  // a good tradeoff for us.\n#ifdef HAVE___ATTRIBUTE__\n#define ATTR_INITIAL_EXEC __attribute__ ((tls_model (\"initial-exec\")))\n#else\n#define ATTR_INITIAL_EXEC\n#endif\n\n#ifdef HAVE_TLS\n  struct ThreadLocalData {\n    ThreadCache* heap;\n    // min_size_for_slow_path is 0 if heap is NULL or kMaxSize + 1 otherwise.\n    // The latter is the common case and allows allocation to be faster\n    // than it would be otherwise: typically a single branch will\n    // determine that the requested allocation is no more than kMaxSize\n    // and we can then proceed, knowing that global and thread-local tcmalloc\n    // state is initialized.\n    size_t min_size_for_slow_path;\n  };\n  static __thread ThreadLocalData threadlocal_data_ ATTR_INITIAL_EXEC;\n#endif\n\n  // Thread-specific key.  Initialization here is somewhat tricky\n  // because some Linux startup code invokes malloc() before it\n  // is in a good enough state to handle pthread_keycreate().\n  // Therefore, we use TSD keys only after tsd_inited is set to true.\n  // Until then, we use a slow path to get the heap object.\n  static bool tsd_inited_;\n  static pthread_key_t heap_key_;\n\n  // Linked list of heap objects.  Protected by Static::pageheap_lock.\n  static ThreadCache* thread_heaps_;\n  static int thread_heap_count_;\n\n  // A pointer to one of the objects in thread_heaps_.  Represents\n  // the next ThreadCache from which a thread over its max_size_ should\n  // steal memory limit.  Round-robin through all of the objects in\n  // thread_heaps_.  Protected by Static::pageheap_lock.\n  static ThreadCache* next_memory_steal_;\n\n  // Overall thread cache size.  Protected by Static::pageheap_lock.\n  static size_t overall_thread_cache_size_;\n\n  // Global per-thread cache size.  Writes are protected by\n  // Static::pageheap_lock.  Reads are done without any locking, which should be\n  // fine as long as size_t can be written atomically and we don't place\n  // invariants between this variable and other pieces of state.\n  static volatile size_t per_thread_cache_size_;\n\n  // Represents overall_thread_cache_size_ minus the sum of max_size_\n  // across all ThreadCaches.  Protected by Static::pageheap_lock.\n  static ssize_t unclaimed_cache_space_;\n\n  // This class is laid out with the most frequently used fields\n  // first so that hot elements are placed on the same cache line.\n\n  size_t        size_;                  // Combined size of data\n  size_t        max_size_;              // size_ > max_size_ --> Scavenge()\n\n  // We sample allocations, biased by the size of the allocation\n  Sampler       sampler_;               // A sampler\n\n  FreeList      list_[kNumClasses];     // Array indexed by size-class\n\n  pthread_t     tid_;                   // Which thread owns it\n  bool          in_setspecific_;        // In call to pthread_setspecific?\n\n  // Allocate a new heap. REQUIRES: Static::pageheap_lock is held.\n  static ThreadCache* NewHeap(pthread_t tid);\n\n  // Use only as pthread thread-specific destructor function.\n  static void DestroyThreadCache(void* ptr);\n\n  static void DeleteCache(ThreadCache* heap);\n  static void RecomputePerThreadCacheSize();\n\n  // Ensure that this class is cacheline-aligned. This is critical for\n  // performance, as false sharing would negate many of the benefits\n  // of a per-thread cache.\n} CACHELINE_ALIGNED;\n\n// Allocator for thread heaps\n// This is logically part of the ThreadCache class, but MSVC, at\n// least, does not like using ThreadCache as a template argument\n// before the class is fully defined.  So we put it outside the class.\nextern PageHeapAllocator<ThreadCache> threadcache_allocator;\n\ninline int ThreadCache::HeapsInUse() {\n  return threadcache_allocator.inuse();\n}\n\ninline bool ThreadCache::SampleAllocation(size_t k) {\n  return sampler_.SampleAllocation(k);\n}\n\ninline void* ThreadCache::Allocate(size_t size, size_t cl) {\n  ASSERT(size <= kMaxSize);\n  ASSERT(size == Static::sizemap()->ByteSizeForClass(cl));\n\n  FreeList* list = &list_[cl];\n  if (UNLIKELY(list->empty())) {\n    return FetchFromCentralCache(cl, size);\n  }\n  size_ -= size;\n  return list->Pop();\n}\n\ninline void ThreadCache::Deallocate(void* ptr, size_t cl) {\n  FreeList* list = &list_[cl];\n  size_ += Static::sizemap()->ByteSizeForClass(cl);\n  ssize_t size_headroom = max_size_ - size_ - 1;\n\n  // This catches back-to-back frees of allocs in the same size\n  // class. A more comprehensive (and expensive) test would be to walk\n  // the entire freelist. But this might be enough to find some bugs.\n  ASSERT(ptr != list->Next());\n\n  list->Push(ptr);\n  ssize_t list_headroom =\n      static_cast<ssize_t>(list->max_length()) - list->length();\n\n  // There are two relatively uncommon things that require further work.\n  // In the common case we're done, and in that case we need a single branch\n  // because of the bitwise-or trick that follows.\n  if (UNLIKELY((list_headroom | size_headroom) < 0)) {\n    if (list_headroom < 0) {\n      ListTooLong(list, cl);\n    }\n    if (size_ >= max_size_) Scavenge();\n  }\n}\n\ninline ThreadCache* ThreadCache::GetThreadHeap() {\n#ifdef HAVE_TLS\n  return threadlocal_data_.heap;\n#else\n  return reinterpret_cast<ThreadCache *>(\n      perftools_pthread_getspecific(heap_key_));\n#endif\n}\n\ninline ThreadCache* ThreadCache::GetCacheWhichMustBePresent() {\n#ifdef HAVE_TLS\n  ASSERT(threadlocal_data_.heap);\n  return threadlocal_data_.heap;\n#else\n  ASSERT(perftools_pthread_getspecific(heap_key_));\n  return reinterpret_cast<ThreadCache *>(\n      perftools_pthread_getspecific(heap_key_));\n#endif\n}\n\ninline ThreadCache* ThreadCache::GetCache() {\n  ThreadCache* ptr = NULL;\n  if (!tsd_inited_) {\n    InitModule();\n  } else {\n    ptr = GetThreadHeap();\n  }\n  if (ptr == NULL) ptr = CreateCacheIfNecessary();\n  return ptr;\n}\n\n// In deletion paths, we do not try to create a thread-cache.  This is\n// because we may be in the thread destruction code and may have\n// already cleaned up the cache for this thread.\ninline ThreadCache* ThreadCache::GetCacheIfPresent() {\n  if (!tsd_inited_) return NULL;\n  return GetThreadHeap();\n}\n\ninline size_t ThreadCache::MinSizeForSlowPath() {\n#ifdef HAVE_TLS\n  return threadlocal_data_.min_size_for_slow_path;\n#else\n  return 0;\n#endif\n}\n\ninline void ThreadCache::SetMinSizeForSlowPath(size_t size) {\n#ifdef HAVE_TLS\n  threadlocal_data_.min_size_for_slow_path = size;\n#endif\n}\n\n}  // namespace tcmalloc\n\n#endif  // TCMALLOC_THREAD_CACHE_H_\n",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/src/base/elf_mem_image.cc": "// -*- Mode: C++; c-basic-offset: 2; indent-tabs-mode: nil -*-\n// Copyright (c) 2008, Google Inc.\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n// ---\n// Author: Paul Pluzhnikov\n//\n// Allow dynamic symbol lookup in an in-memory Elf image.\n//\n\n#include \"base/elf_mem_image.h\"\n\n#ifdef HAVE_ELF_MEM_IMAGE  // defined in elf_mem_image.h\n\n#include <stddef.h>   // for size_t, ptrdiff_t\n#include \"base/logging.h\"\n\n// From binutils/include/elf/common.h (this doesn't appear to be documented\n// anywhere else).\n//\n//   /* This flag appears in a Versym structure.  It means that the symbol\n//      is hidden, and is only visible with an explicit version number.\n//      This is a GNU extension.  */\n//   #define VERSYM_HIDDEN           0x8000\n//\n//   /* This is the mask for the rest of the Versym information.  */\n//   #define VERSYM_VERSION          0x7fff\n\n#define VERSYM_VERSION 0x7fff\n\nnamespace base {\n\nnamespace {\ntemplate <int N> class ElfClass {\n public:\n  static const int kElfClass = -1;\n  static int ElfBind(const ElfW(Sym) *) {\n    CHECK(false); // << \"Unexpected word size\";\n    return 0;\n  }\n  static int ElfType(const ElfW(Sym) *) {\n    CHECK(false); // << \"Unexpected word size\";\n    return 0;\n  }\n};\n\ntemplate <> class ElfClass<32> {\n public:\n  static const int kElfClass = ELFCLASS32;\n  static int ElfBind(const ElfW(Sym) *symbol) {\n    return ELF32_ST_BIND(symbol->st_info);\n  }\n  static int ElfType(const ElfW(Sym) *symbol) {\n    return ELF32_ST_TYPE(symbol->st_info);\n  }\n};\n\ntemplate <> class ElfClass<64> {\n public:\n  static const int kElfClass = ELFCLASS64;\n  static int ElfBind(const ElfW(Sym) *symbol) {\n    return ELF64_ST_BIND(symbol->st_info);\n  }\n  static int ElfType(const ElfW(Sym) *symbol) {\n    return ELF64_ST_TYPE(symbol->st_info);\n  }\n};\n\ntypedef ElfClass<__WORDSIZE> CurrentElfClass;\n\n// Extract an element from one of the ELF tables, cast it to desired type.\n// This is just a simple arithmetic and a glorified cast.\n// Callers are responsible for bounds checking.\ntemplate <class T>\nconst T* GetTableElement(const ElfW(Ehdr) *ehdr,\n                         ElfW(Off) table_offset,\n                         ElfW(Word) element_size,\n                         size_t index) {\n  return reinterpret_cast<const T*>(reinterpret_cast<const char *>(ehdr)\n                                    + table_offset\n                                    + index * element_size);\n}\n}  // namespace\n\nconst void *const ElfMemImage::kInvalidBase =\n    reinterpret_cast<const void *>(~0L);\n\nElfMemImage::ElfMemImage(const void *base) {\n  CHECK(base != kInvalidBase);\n  Init(base);\n}\n\nint ElfMemImage::GetNumSymbols() const {\n  if (!hash_) {\n    return 0;\n  }\n  // See http://www.caldera.com/developers/gabi/latest/ch5.dynamic.html#hash\n  return hash_[1];\n}\n\nconst ElfW(Sym) *ElfMemImage::GetDynsym(int index) const {\n  CHECK_LT(index, GetNumSymbols());\n  return dynsym_ + index;\n}\n\nconst ElfW(Versym) *ElfMemImage::GetVersym(int index) const {\n  CHECK_LT(index, GetNumSymbols());\n  return versym_ + index;\n}\n\nconst ElfW(Phdr) *ElfMemImage::GetPhdr(int index) const {\n  CHECK_LT(index, ehdr_->e_phnum);\n  return GetTableElement<ElfW(Phdr)>(ehdr_,\n                                     ehdr_->e_phoff,\n                                     ehdr_->e_phentsize,\n                                     index);\n}\n\nconst char *ElfMemImage::GetDynstr(ElfW(Word) offset) const {\n  CHECK_LT(offset, strsize_);\n  return dynstr_ + offset;\n}\n\nconst void *ElfMemImage::GetSymAddr(const ElfW(Sym) *sym) const {\n  if (sym->st_shndx == SHN_UNDEF || sym->st_shndx >= SHN_LORESERVE) {\n    // Symbol corresponds to \"special\" (e.g. SHN_ABS) section.\n    return reinterpret_cast<const void *>(sym->st_value);\n  }\n  CHECK_LT(link_base_, sym->st_value);\n  return GetTableElement<char>(ehdr_, 0, 1, sym->st_value) - link_base_;\n}\n\nconst ElfW(Verdef) *ElfMemImage::GetVerdef(int index) const {\n  CHECK_LE(index, verdefnum_);\n  const ElfW(Verdef) *version_definition = verdef_;\n  while (version_definition->vd_ndx < index && version_definition->vd_next) {\n    const char *const version_definition_as_char =\n        reinterpret_cast<const char *>(version_definition);\n    version_definition =\n        reinterpret_cast<const ElfW(Verdef) *>(version_definition_as_char +\n                                               version_definition->vd_next);\n  }\n  return version_definition->vd_ndx == index ? version_definition : NULL;\n}\n\nconst ElfW(Verdaux) *ElfMemImage::GetVerdefAux(\n    const ElfW(Verdef) *verdef) const {\n  return reinterpret_cast<const ElfW(Verdaux) *>(verdef+1);\n}\n\nconst char *ElfMemImage::GetVerstr(ElfW(Word) offset) const {\n  CHECK_LT(offset, strsize_);\n  return dynstr_ + offset;\n}\n\nvoid ElfMemImage::Init(const void *base) {\n  ehdr_      = NULL;\n  dynsym_    = NULL;\n  dynstr_    = NULL;\n  versym_    = NULL;\n  verdef_    = NULL;\n  hash_      = NULL;\n  strsize_   = 0;\n  verdefnum_ = 0;\n  link_base_ = ~0L;  // Sentinel: PT_LOAD .p_vaddr can't possibly be this.\n  if (!base) {\n    return;\n  }\n  const intptr_t base_as_uintptr_t = reinterpret_cast<uintptr_t>(base);\n  // Fake VDSO has low bit set.\n  const bool fake_vdso = ((base_as_uintptr_t & 1) != 0);\n  base = reinterpret_cast<const void *>(base_as_uintptr_t & ~1);\n  const char *const base_as_char = reinterpret_cast<const char *>(base);\n  if (base_as_char[EI_MAG0] != ELFMAG0 || base_as_char[EI_MAG1] != ELFMAG1 ||\n      base_as_char[EI_MAG2] != ELFMAG2 || base_as_char[EI_MAG3] != ELFMAG3) {\n    RAW_DCHECK(false, \"no ELF magic\"); // at %p\", base);\n    return;\n  }\n  int elf_class = base_as_char[EI_CLASS];\n  if (elf_class != CurrentElfClass::kElfClass) {\n    DCHECK_EQ(elf_class, CurrentElfClass::kElfClass);\n    return;\n  }\n  switch (base_as_char[EI_DATA]) {\n    case ELFDATA2LSB: {\n      if (__LITTLE_ENDIAN != __BYTE_ORDER) {\n        DCHECK_EQ(__LITTLE_ENDIAN, __BYTE_ORDER); // << \": wrong byte order\";\n        return;\n      }\n      break;\n    }\n    case ELFDATA2MSB: {\n      if (__BIG_ENDIAN != __BYTE_ORDER) {\n        DCHECK_EQ(__BIG_ENDIAN, __BYTE_ORDER); // << \": wrong byte order\";\n        return;\n      }\n      break;\n    }\n    default: {\n      RAW_DCHECK(false, \"unexpected data encoding\"); // << base_as_char[EI_DATA];\n      return;\n    }\n  }\n\n  ehdr_ = reinterpret_cast<const ElfW(Ehdr) *>(base);\n  const ElfW(Phdr) *dynamic_program_header = NULL;\n  for (int i = 0; i < ehdr_->e_phnum; ++i) {\n    const ElfW(Phdr) *const program_header = GetPhdr(i);\n    switch (program_header->p_type) {\n      case PT_LOAD:\n        if (link_base_ == ~0L) {\n          link_base_ = program_header->p_vaddr;\n        }\n        break;\n      case PT_DYNAMIC:\n        dynamic_program_header = program_header;\n        break;\n    }\n  }\n  if (link_base_ == ~0L || !dynamic_program_header) {\n    RAW_DCHECK(~0L != link_base_, \"no PT_LOADs in VDSO\");\n    RAW_DCHECK(dynamic_program_header, \"no PT_DYNAMIC in VDSO\");\n    // Mark this image as not present. Can not recur infinitely.\n    Init(0);\n    return;\n  }\n  ptrdiff_t relocation =\n      base_as_char - reinterpret_cast<const char *>(link_base_);\n  ElfW(Dyn) *dynamic_entry =\n      reinterpret_cast<ElfW(Dyn) *>(dynamic_program_header->p_vaddr +\n                                    relocation);\n  for (; dynamic_entry->d_tag != DT_NULL; ++dynamic_entry) {\n    ElfW(Xword) value = dynamic_entry->d_un.d_val;\n    if (fake_vdso) {\n      // A complication: in the real VDSO, dynamic entries are not relocated\n      // (it wasn't loaded by a dynamic loader). But when testing with a\n      // \"fake\" dlopen()ed vdso library, the loader relocates some (but\n      // not all!) of them before we get here.\n      if (dynamic_entry->d_tag == DT_VERDEF) {\n        // The only dynamic entry (of the ones we care about) libc-2.3.6\n        // loader doesn't relocate.\n        value += relocation;\n      }\n    } else {\n      // Real VDSO. Everything needs to be relocated.\n      value += relocation;\n    }\n    switch (dynamic_entry->d_tag) {\n      case DT_HASH:\n        hash_ = reinterpret_cast<ElfW(Word) *>(value);\n        break;\n      case DT_SYMTAB:\n        dynsym_ = reinterpret_cast<ElfW(Sym) *>(value);\n        break;\n      case DT_STRTAB:\n        dynstr_ = reinterpret_cast<const char *>(value);\n        break;\n      case DT_VERSYM:\n        versym_ = reinterpret_cast<ElfW(Versym) *>(value);\n        break;\n      case DT_VERDEF:\n        verdef_ = reinterpret_cast<ElfW(Verdef) *>(value);\n        break;\n      case DT_VERDEFNUM:\n        verdefnum_ = dynamic_entry->d_un.d_val;\n        break;\n      case DT_STRSZ:\n        strsize_ = dynamic_entry->d_un.d_val;\n        break;\n      default:\n        // Unrecognized entries explicitly ignored.\n        break;\n    }\n  }\n  if (!hash_ || !dynsym_ || !dynstr_ || !versym_ ||\n      !verdef_ || !verdefnum_ || !strsize_) {\n    RAW_DCHECK(hash_, \"invalid VDSO (no DT_HASH)\");\n    RAW_DCHECK(dynsym_, \"invalid VDSO (no DT_SYMTAB)\");\n    RAW_DCHECK(dynstr_, \"invalid VDSO (no DT_STRTAB)\");\n    RAW_DCHECK(versym_, \"invalid VDSO (no DT_VERSYM)\");\n    RAW_DCHECK(verdef_, \"invalid VDSO (no DT_VERDEF)\");\n    RAW_DCHECK(verdefnum_, \"invalid VDSO (no DT_VERDEFNUM)\");\n    RAW_DCHECK(strsize_, \"invalid VDSO (no DT_STRSZ)\");\n    // Mark this image as not present. Can not recur infinitely.\n    Init(0);\n    return;\n  }\n}\n\nbool ElfMemImage::LookupSymbol(const char *name,\n                               const char *version,\n                               int type,\n                               SymbolInfo *info) const {\n  for (SymbolIterator it = begin(); it != end(); ++it) {\n    if (strcmp(it->name, name) == 0 && strcmp(it->version, version) == 0 &&\n        CurrentElfClass::ElfType(it->symbol) == type) {\n      if (info) {\n        *info = *it;\n      }\n      return true;\n    }\n  }\n  return false;\n}\n\nbool ElfMemImage::LookupSymbolByAddress(const void *address,\n                                        SymbolInfo *info_out) const {\n  for (SymbolIterator it = begin(); it != end(); ++it) {\n    const char *const symbol_start =\n        reinterpret_cast<const char *>(it->address);\n    const char *const symbol_end = symbol_start + it->symbol->st_size;\n    if (symbol_start <= address && address < symbol_end) {\n      if (info_out) {\n        // Client wants to know details for that symbol (the usual case).\n        if (CurrentElfClass::ElfBind(it->symbol) == STB_GLOBAL) {\n          // Strong symbol; just return it.\n          *info_out = *it;\n          return true;\n        } else {\n          // Weak or local. Record it, but keep looking for a strong one.\n          *info_out = *it;\n        }\n      } else {\n        // Client only cares if there is an overlapping symbol.\n        return true;\n      }\n    }\n  }\n  return false;\n}\n\nElfMemImage::SymbolIterator::SymbolIterator(const void *const image, int index)\n    : index_(index), image_(image) {\n}\n\nconst ElfMemImage::SymbolInfo *ElfMemImage::SymbolIterator::operator->() const {\n  return &info_;\n}\n\nconst ElfMemImage::SymbolInfo& ElfMemImage::SymbolIterator::operator*() const {\n  return info_;\n}\n\nbool ElfMemImage::SymbolIterator::operator==(const SymbolIterator &rhs) const {\n  return this->image_ == rhs.image_ && this->index_ == rhs.index_;\n}\n\nbool ElfMemImage::SymbolIterator::operator!=(const SymbolIterator &rhs) const {\n  return !(*this == rhs);\n}\n\nElfMemImage::SymbolIterator &ElfMemImage::SymbolIterator::operator++() {\n  this->Update(1);\n  return *this;\n}\n\nElfMemImage::SymbolIterator ElfMemImage::begin() const {\n  SymbolIterator it(this, 0);\n  it.Update(0);\n  return it;\n}\n\nElfMemImage::SymbolIterator ElfMemImage::end() const {\n  return SymbolIterator(this, GetNumSymbols());\n}\n\nvoid ElfMemImage::SymbolIterator::Update(int increment) {\n  const ElfMemImage *image = reinterpret_cast<const ElfMemImage *>(image_);\n  CHECK(image->IsPresent() || increment == 0);\n  if (!image->IsPresent()) {\n    return;\n  }\n  index_ += increment;\n  if (index_ >= image->GetNumSymbols()) {\n    index_ = image->GetNumSymbols();\n    return;\n  }\n  const ElfW(Sym)    *symbol = image->GetDynsym(index_);\n  const ElfW(Versym) *version_symbol = image->GetVersym(index_);\n  CHECK(symbol && version_symbol);\n  const char *const symbol_name = image->GetDynstr(symbol->st_name);\n  const ElfW(Versym) version_index = version_symbol[0] & VERSYM_VERSION;\n  const ElfW(Verdef) *version_definition = NULL;\n  const char *version_name = \"\";\n  if (symbol->st_shndx == SHN_UNDEF) {\n    // Undefined symbols reference DT_VERNEED, not DT_VERDEF, and\n    // version_index could well be greater than verdefnum_, so calling\n    // GetVerdef(version_index) may trigger assertion.\n  } else {\n    version_definition = image->GetVerdef(version_index);\n  }\n  if (version_definition) {\n    // I am expecting 1 or 2 auxiliary entries: 1 for the version itself,\n    // optional 2nd if the version has a parent.\n    CHECK_LE(1, version_definition->vd_cnt);\n    CHECK_LE(version_definition->vd_cnt, 2);\n    const ElfW(Verdaux) *version_aux = image->GetVerdefAux(version_definition);\n    version_name = image->GetVerstr(version_aux->vda_name);\n  }\n  info_.name    = symbol_name;\n  info_.version = version_name;\n  info_.address = image->GetSymAddr(symbol);\n  info_.symbol  = symbol;\n}\n\n}  // namespace base\n\n#endif  // HAVE_ELF_MEM_IMAGE\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/src/windows/ia32_modrm_map.cc",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/src/windows/ia32_opcode_map.cc",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/src/windows/mini_disassembler.h",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.16384.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.16.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.4096.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/spanmap.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.5.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/pprof-test.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.512.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.128.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.8.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.2048.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.3.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/pprof-vsnprintf.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/pprof-test-big.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.65536.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.20.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/overview.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/pprof-vsnprintf-big.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.1024.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.131072.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.256.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.2.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/pageheap.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.12.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/threadheap.gif",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.64.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.1.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/heap-example1.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspersec.vs.size.4.threads.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.8192.bytes.png",
        "/tmp/vanessa/spack-stage/spack-stage-faodel-1.1811.2-tn4ge5t6zp4dv5ncr7sh7c5io3g2xf6a/spack-src/tpl/gperftools/doc/tcmalloc-opspercpusec.vs.threads.32768.bytes.png"
    ],
    "total_files": 920
}