{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/src/tbb_wrapper.c": "/*\n * Copyright (C) 2017 - 2019 Intel Corporation.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * 1. Redistributions of source code must retain the above copyright notice(s),\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice(s),\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n * EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <memkind/internal/memkind_default.h>\n#include <memkind/internal/memkind_log.h>\n#include <memkind/internal/memkind_private.h>\n#include <memkind/internal/tbb_wrapper.h>\n#include <memkind/internal/tbb_mem_pool_policy.h>\n#include <limits.h>\n\n#include <stdint.h>\n#include <errno.h>\n#include <stdio.h>\n#include <sys/mman.h>\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <string.h>\n\nvoid *(*pool_malloc)(void *, size_t);\nvoid *(*pool_realloc)(void *, void *, size_t);\nvoid *(*pool_aligned_malloc)(void *, size_t, size_t);\nbool (*pool_free)(void *, void *);\nint (*pool_create_v1)(intptr_t, const struct MemPoolPolicy *, void **);\nbool (*pool_destroy)(void *);\nvoid *(*pool_identify)(void *object);\nsize_t (*pool_msize)(void *, void *);\n\nstatic void *tbb_handle = NULL;\nstatic bool TBBInitDone = false;\n\nvoid load_tbb_symbols(void)\n{\n    const char so_name[]=\"libtbbmalloc.so.2\";\n    tbb_handle = dlopen(so_name, RTLD_LAZY);\n    if(!tbb_handle) {\n        log_fatal(\"%s not found.\", so_name);\n        abort();\n    }\n\n    pool_malloc = dlsym(tbb_handle, \"_ZN3rml11pool_mallocEPNS_10MemoryPoolEm\");\n    pool_realloc = dlsym(tbb_handle, \"_ZN3rml12pool_reallocEPNS_10MemoryPoolEPvm\");\n    pool_aligned_malloc = dlsym(tbb_handle,\n                                \"_ZN3rml19pool_aligned_mallocEPNS_10MemoryPoolEmm\");\n    pool_free = dlsym(tbb_handle, \"_ZN3rml9pool_freeEPNS_10MemoryPoolEPv\");\n    pool_create_v1 = dlsym(tbb_handle,\n                           \"_ZN3rml14pool_create_v1ElPKNS_13MemPoolPolicyEPPNS_10MemoryPoolE\");\n    pool_destroy = dlsym(tbb_handle, \"_ZN3rml12pool_destroyEPNS_10MemoryPoolE\");\n    pool_identify = dlsym(tbb_handle, \"_ZN3rml13pool_identifyEPv\");\n    pool_msize = dlsym(tbb_handle, \"_ZN3rml10pool_msizeEPNS_10MemoryPoolEPv\");\n\n    if(!pool_malloc ||\n       !pool_realloc ||\n       !pool_aligned_malloc ||\n       !pool_free ||\n       !pool_create_v1 ||\n       !pool_destroy ||\n       !pool_identify ) {\n        log_fatal(\"Could not find symbols in %s.\", so_name);\n        dlclose(tbb_handle);\n        abort();\n    }\n    TBBInitDone = true;\n}\n\n//Granularity of raw_alloc allocations\n#define GRANULARITY 2*1024*1024\nstatic void *raw_alloc(intptr_t pool_id, size_t *bytes/*=n*GRANULARITY*/)\n{\n    void *ptr = kind_mmap((struct memkind *)pool_id, NULL, *bytes);\n    return (ptr==MAP_FAILED) ? NULL : ptr;\n}\n\nstatic int raw_free(intptr_t pool_id, void *raw_ptr, size_t raw_bytes)\n{\n    return munmap(raw_ptr, raw_bytes);\n}\n\nstatic void *tbb_pool_malloc(struct memkind *kind, size_t size)\n{\n    if(size_out_of_bounds(size)) return NULL;\n    void *result = pool_malloc(kind->priv, size);\n    if (!result)\n        errno = ENOMEM;\n    return result;\n}\n\nstatic void *tbb_pool_calloc(struct memkind *kind, size_t num, size_t size)\n{\n    if (size_out_of_bounds(num) || size_out_of_bounds(size)) return NULL;\n\n    const size_t array_size = num*size;\n    if (array_size/num != size) {\n        errno = ENOMEM;\n        return NULL;\n    }\n    void *result = pool_malloc(kind->priv, array_size);\n    if (result) {\n        memset(result, 0, array_size);\n    } else {\n        errno = ENOMEM;\n    }\n    return result;\n}\n\nstatic void *tbb_pool_common_realloc(void *pool, void *ptr, size_t size)\n{\n    if (size_out_of_bounds(size)) {\n        pool_free(pool, ptr);\n        return NULL;\n    }\n    void *result = pool_realloc(pool, ptr, size);\n    if (!result)\n        errno = ENOMEM;\n    return result;\n}\n\nstatic void *tbb_pool_realloc(struct memkind *kind, void *ptr, size_t size)\n{\n    return tbb_pool_common_realloc(kind->priv, ptr, size);\n}\n\nvoid *tbb_pool_realloc_with_kind_detect(void *ptr, size_t size)\n{\n    if (!ptr) {\n        errno = EINVAL;\n        return NULL;\n    }\n    return tbb_pool_common_realloc(pool_identify(ptr), ptr, size);\n}\n\nint tbb_get_global_stat(memkind_stat_type stat, size_t *value)\n{\n    log_err(\"Get allocator statistic is not supported by TBB\");\n    return MEMKIND_ERROR_OPERATION_FAILED;\n}\n\nstatic int tbb_get_kind_stat(memkind_t kind, memkind_stat_type stat,\n                             size_t *value)\n{\n    log_err(\"Get kind statistic is not supported by TBB\");\n    return MEMKIND_ERROR_OPERATION_FAILED;\n}\n\nint tbb_update_cached_stats(void)\n{\n    log_err(\"Update cached statistics is not supported by TBB\");\n    return MEMKIND_ERROR_OPERATION_FAILED;\n}\n\nvoid *tbb_pool_defrag_reallocate_with_kind_detect(void *ptr)\n{\n    log_err(\"Defrag reallocate method is not supported by TBB\");\n    return NULL;\n}\n\nstatic void *tbb_defrag_reallocate(struct memkind *kind, void *ptr)\n{\n    log_err(\"Defrag reallocate method is not supported by TBB\");\n    return NULL;\n}\n\nstruct memkind *tbb_detect_kind(void *ptr)\n{\n    if (!ptr) {\n        return NULL;\n    }\n    struct memkind *kind = NULL;\n    unsigned i;\n    void *pool = pool_identify(ptr);\n    for (i = 0; i < MEMKIND_MAX_KIND; ++i) {\n        int err = memkind_get_kind_by_partition(i, &kind);\n        if (!err && kind->priv == pool) {\n            break;\n        }\n    }\n\n    return kind;\n}\n\nstatic int tbb_pool_posix_memalign(struct memkind *kind, void **memptr,\n                                   size_t alignment, size_t size)\n{\n    //Check if alignment is \"at least as large as sizeof(void *)\".\n    if(!alignment && (0 != (alignment & (alignment-sizeof(void *))))) return EINVAL;\n    //Check if alignment is \"a power of 2\".\n    if(alignment & (alignment-1)) return EINVAL;\n    if(size_out_of_bounds(size)) {\n        *memptr = NULL;\n        return 0;\n    }\n    void *result = pool_aligned_malloc(kind->priv, size, alignment);\n    if (!result) {\n        return ENOMEM;\n    }\n    *memptr = result;\n    return 0;\n}\n\nvoid tbb_pool_free_with_kind_detect(void *ptr)\n{\n    if (ptr) {\n        pool_free(pool_identify(ptr), ptr);\n    }\n}\n\nstatic void tbb_pool_free(struct memkind *kind, void *ptr)\n{\n    pool_free(kind->priv, ptr);\n}\n\nstatic size_t tbb_pool_common_malloc_usable_size(void *pool, void *ptr)\n{\n    if (pool_msize) {\n        return pool_msize(pool, ptr);\n    } else {\n        log_err(\"memkind_malloc_usable_size() is not supported by this TBB version.\");\n        return 0;\n    }\n}\n\nstatic size_t tbb_pool_malloc_usable_size(struct memkind *kind, void *ptr)\n{\n    return tbb_pool_common_malloc_usable_size(kind->priv, ptr);\n}\n\nsize_t tbb_pool_malloc_usable_size_with_kind_detect(void *ptr)\n{\n    size_t size = 0;\n    if (ptr) {\n        size = tbb_pool_common_malloc_usable_size(pool_identify(ptr), ptr);\n    }\n    return size;\n}\n\nstatic int tbb_update_memory_usage_policy(struct memkind *kind,\n                                          memkind_mem_usage_policy policy)\n{\n    log_info(\"Memkind memory usage policies have no effect in TBB manager.\");\n    return MEMKIND_SUCCESS;\n}\n\nstatic int tbb_destroy(struct memkind *kind)\n{\n    bool pool_destroy_ret = pool_destroy(kind->priv);\n    dlclose(tbb_handle);\n\n    if(!pool_destroy_ret) {\n        log_err(\"TBB pool destroy failure.\");\n        return MEMKIND_ERROR_OPERATION_FAILED;\n    }\n    return MEMKIND_SUCCESS;\n}\n\nvoid tbb_initialize(struct memkind *kind)\n{\n    if(!kind || !TBBInitDone) {\n        log_fatal(\"Failed to initialize TBB.\");\n        abort();\n    }\n\n    struct MemPoolPolicy policy = {\n        .pAlloc = raw_alloc,\n        .pFree = raw_free,\n        .granularity = GRANULARITY,\n        .version = 1,\n        .fixedPool = false,\n        .keepAllMemory = false,\n        .reserved = 0\n    };\n\n    pool_create_v1((intptr_t)kind, &policy, &kind->priv);\n    if (!kind->priv) {\n        log_fatal(\"Unable to create TBB memory pool.\");\n        abort();\n    }\n\n    kind->ops->malloc = tbb_pool_malloc;\n    kind->ops->calloc = tbb_pool_calloc;\n    kind->ops->posix_memalign = tbb_pool_posix_memalign;\n    kind->ops->realloc = tbb_pool_realloc;\n    kind->ops->free = tbb_pool_free;\n    kind->ops->finalize = tbb_destroy;\n    kind->ops->malloc_usable_size = tbb_pool_malloc_usable_size;\n    kind->ops->update_memory_usage_policy = tbb_update_memory_usage_policy;\n    kind->ops->get_stat = tbb_get_kind_stat;\n    kind->ops->defrag_reallocate = tbb_defrag_reallocate;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/copying_headers/MANIFEST.freeBSD": "Makefile.am\nastyle.sh\nautogen.sh\nautohbw/Makefile.mk\nautohbw/autohbw.c\nautohbw/autohbw_get_src_lines.pl\nautohbw/autohbw_test.sh\nbuild.sh\nconfigure.ac\nexamples/Makefile.mk\nexamples/autohbw_candidates.c\nexamples/filter_example.c\nexamples/hello_hbw_example.c\nexamples/hello_memkind_example.c\nexamples/memkind_allocated.hpp\nexamples/memkind_allocated_example.cpp\nexamples/memkind_decorator_debug.c\nexamples/memkind_get_stat.c\ninclude/hbw_allocator.h\ninclude/hbwmalloc.h\ninclude/memkind.h\ninclude/memkind/internal/heap_manager.h\ninclude/memkind/internal/memkind_arena.h\ninclude/memkind/internal/memkind_bandwidth.h\ninclude/memkind/internal/memkind_dax_kmem.h\ninclude/memkind/internal/memkind_default.h\ninclude/memkind/internal/memkind_gbtlb.h\ninclude/memkind/internal/memkind_hbw.h\ninclude/memkind/internal/memkind_hugetlb.h\ninclude/memkind/internal/memkind_interleave.h\ninclude/memkind/internal/memkind_log.h\ninclude/memkind/internal/memkind_pmem.h\ninclude/memkind/internal/memkind_private.h\ninclude/memkind/internal/memkind_regular.h\ninclude/memkind/internal/tbb_wrapper.h\ninclude/memkind/internal/vec.h\ninclude/memkind_allocator.h\ninclude/memkind_deprecated.h\ninclude/pmem_allocator.h\ninstall_astyle.sh\nman/autohbw.7\nman/hbwallocator.3\nman/hbwmalloc.3\nmna/memkind-auto-dax-kmem-nodes.1\nman/memkind-hbw-nodes.1\nman/memkind.3\nman/memkind_arena.3\nman/memkind_dax_kmem.3\nman/memkind_default.3\nman/memkind_hbw.3\nman/memkind_hugetlb.3\nman/memkind_pmem.3\nman/memkindallocator.3\nman/pmemallocator.3\nmemkind.spec.mk\nsrc/Makefile.mk\nsrc/hbwmalloc.c\nsrc/heap_manager.c\nsrc/memkind-auto-dax-kmem-nodes.c\nsrc/memkind-hbw-nodes.c\nsrc/memkind.c\nsrc/memkind_arena.c\nsrc/memkind_bandwidth.c\nsrc/memkind_dax_kmem.c\nsrc/memkind_default.c\nsrc/memkind_gbtlb.c\nsrc/memkind_hbw.c\nsrc/memkind_hugetlb.c\nsrc/memkind_interleave.c\nsrc/memkind_log.c\nsrc/memkind_pmem.c\nsrc/memkind_regular.c\nsrc/tbb_wrapper.c\ntest/Allocator.hpp\ntest/Makefile.mk\ntest/TestPolicy.hpp\ntest/alloc_benchmark.c\ntest/alloc_performance_tests.cpp\ntest/allocate_to_max_stress_test.cpp\ntest/allocator_perf_tool/AllocationSizes.hpp\ntest/allocator_perf_tool/Allocation_info.cpp\ntest/allocator_perf_tool/Allocation_info.hpp\ntest/allocator_perf_tool/Allocator.hpp\ntest/allocator_perf_tool/AllocatorFactory.hpp\ntest/allocator_perf_tool/CSVLogger.hpp\ntest/allocator_perf_tool/CommandLine.hpp\ntest/allocator_perf_tool/Configuration.hpp\ntest/allocator_perf_tool/ConsoleLog.hpp\ntest/allocator_perf_tool/FunctionCalls.hpp\ntest/allocator_perf_tool/FunctionCallsPerformanceTask.cpp\ntest/allocator_perf_tool/FunctionCallsPerformanceTask.h\ntest/allocator_perf_tool/GTestAdapter.hpp\ntest/allocator_perf_tool/HBWmallocAllocatorWithTimer.hpp\ntest/allocator_perf_tool/HugePageOrganizer.hpp\ntest/allocator_perf_tool/HugePageUnmap.hpp\ntest/allocator_perf_tool/Iterator.hpp\ntest/allocator_perf_tool/JemallocAllocatorWithTimer.hpp\ntest/allocator_perf_tool/Makefile\ntest/allocator_perf_tool/MemkindAllocatorWithTimer.hpp\ntest/allocator_perf_tool/Numastat.hpp\ntest/allocator_perf_tool/PmemMockup.cpp\ntest/allocator_perf_tool/PmemMockup.hpp\ntest/allocator_perf_tool/Runnable.hpp\ntest/allocator_perf_tool/ScenarioWorkload.cpp\ntest/allocator_perf_tool/ScenarioWorkload.h\ntest/allocator_perf_tool/StandardAllocatorWithTimer.hpp\ntest/allocator_perf_tool/Stats.hpp\ntest/allocator_perf_tool/StressIncreaseToMax.cpp\ntest/allocator_perf_tool/StressIncreaseToMax.h\ntest/allocator_perf_tool/Task.hpp\ntest/allocator_perf_tool/TaskFactory.hpp\ntest/allocator_perf_tool/Tests.hpp\ntest/allocator_perf_tool/Thread.hpp\ntest/allocator_perf_tool/TimerSysTime.hpp\ntest/allocator_perf_tool/VectorIterator.hpp\ntest/allocator_perf_tool/Workload.hpp\ntest/allocator_perf_tool/WrappersMacros.hpp\ntest/allocator_perf_tool/main.cpp\ntest/autohbw_test.py\ntest/autohbw_test_helper.c\ntest/bat_tests.cpp\ntest/check.cpp\ntest/check.h\ntest/common.h\ntest/dax_kmem_env_var_test.py\ntest/decorator_test.cpp\ntest/decorator_test.h\ntest/dlopen_test.cpp\ntest/draw_plots.py\ntest/environ_err_dax_kmem_malloc_positive_test.cpp\ntest/environ_err_dax_kmem_malloc_test.cpp\ntest/environ_err_hbw_malloc_test.cpp\ntest/error_message_tests.cpp\ntest/fragmentation_benchmark_pmem.cpp\ntest/freeing_memory_segfault_test.cpp\ntest/gb_page_tests_bind_policy.cpp\ntest/get_arena_test.cpp\ntest/hbw_allocator_performance_tests.cpp\ntest/hbw_allocator_tests.cpp\ntest/hbw_detection_test.py\ntest/hbw_verify_function_test.cpp\ntest/heap_manager_init_perf_test.cpp\ntest/huge_page_test.cpp\ntest/load_tbbmalloc_symbols.c\ntest/locality_test.cpp\ntest/main.cpp\ntest/memkind_allocator_tests.cpp\ntest/memkind_dax_kmem_test.cpp\ntest/memkind_detect_kind_tests.cpp\ntest/memkind_null_kind_test.cpp\ntest/memkind_pmem_config_tests.cpp\ntest/memkind_pmem_long_time_tests.cpp\ntest/memkind_pmem_tests.cpp\ntest/memkind_stat_test.cpp\ntest/memkind_defrag_reallocate.cpp\ntest/memkind_versioning_tests.cpp\ntest/memory_footprint_test.cpp\ntest/memory_manager.h\ntest/multithreaded_tests.cpp\ntest/negative_tests.cpp\ntest/performance/framework.cpp\ntest/performance/framework.hpp\ntest/performance/operations.hpp\ntest/performance/perf_tests.cpp\ntest/performance/perf_tests.hpp\ntest/pmem_alloc_performance_tests.cpp\ntest/pmem_allocator_tests.cpp\ntest/proc_stat.h\ntest/python_framework/__init__.py\ntest/python_framework/cmd_helper.py\ntest/python_framework/huge_page_organizer.py\ntest/random_sizes_allocator.h\ntest/run_alloc_benchmark.sh\ntest/static_kinds_list.h\ntest/static_kinds_tests.cpp\ntest/tbbmalloc.h\ntest/test.sh\ntest/test_dax_kmem.sh\ntest/trace_mechanism_test.py\ntest/trace_mechanism_test_helper.c\ntest/trial_generator.cpp\ntest/trial_generator.h\nutils/docker/Dockerfile.fedora-30\nutils/docker/Dockerfile.ubuntu-18.04\nutils/docker/docker_install_ndctl.sh\nutils/docker/docker_install_tbb.sh\nutils/docker/docker_run_build.sh\nutils/docker/docker_run_coverage.sh\nutils/docker/docker_run_test.sh\nutils/docker/run_local.sh\nutils/docker/set_host_configuration.sh\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/test/dlopen_test.cpp": "/*\n * Copyright (C) 2017 - 2018 Intel Corporation.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * 1. Redistributions of source code must retain the above copyright notice(s),\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice(s),\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n * EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n#include \"allocator_perf_tool/HugePageOrganizer.hpp\"\n\n#include <dlfcn.h>\n\n#include \"common.h\"\n\nclass DlopenTest: public :: testing::Test\n{\nprotected:\n    DlopenTest()\n    {\n        const char *path = \"/usr/lib64/libmemkind.so\";\n        if (!pathExists(path)) {\n            path = \"/usr/lib/libmemkind.so\";\n        }\n        dlerror();\n        handle = dlopen(path, RTLD_LAZY);\n        assert((handle != NULL && dlerror() == NULL) && \"Couldn't open libmemkind.so\");\n        memkind_malloc = (memkind_malloc_t)dlsym(handle, \"memkind_malloc\");\n        assert(dlerror() == NULL && \"Couldn't get memkind_malloc from memkind library\");\n        memkind_free = (memkind_free_t)dlsym(handle, \"memkind_free\");\n        assert(dlerror() == NULL && \"Couldn't get memkind_free from memkind library\");\n    }\n\n    ~DlopenTest()\n    {\n        dlclose(handle);\n    }\n\n    void test(const char *kind_name, size_t alloc_size)\n    {\n        void **kind_ptr = (void **)dlsym(handle, kind_name);\n        EXPECT_TRUE(dlerror() == NULL) << \"Couldn't get kind from memkind library\";\n        EXPECT_TRUE(kind_ptr != NULL) << \"Kind ptr to memkind library is NULL\";\n\n        void *allocation_ptr = memkind_malloc((*kind_ptr), alloc_size);\n        EXPECT_TRUE(allocation_ptr != NULL) << \"Allocation with memkind_malloc failed\";\n\n        memset(allocation_ptr, 0, alloc_size);\n\n        memkind_free((*kind_ptr), allocation_ptr);\n    }\n\n    bool pathExists(const char *p)\n    {\n        struct stat info;\n        if (0 != stat(p, &info)) {\n            return false;\n        }\n        return true;\n    }\n\nprivate:\n    void *handle;\n    typedef void *(*memkind_malloc_t)(void *, size_t);\n    typedef void (*memkind_free_t)(void *, void *);\n    memkind_malloc_t memkind_malloc;\n    memkind_free_t memkind_free;\n};\n\nTEST_F(DlopenTest, test_TC_MEMKIND_DEFAULT_4194305_bytes)\n{\n    test(\"MEMKIND_DEFAULT\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_4194305_bytes)\n{\n    test(\"MEMKIND_HBW\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_HUGETLB_4194305_bytes)\n{\n    HugePageOrganizer huge_page_organizer(8);\n    test(\"MEMKIND_HBW_HUGETLB\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_PREFERRED_4194305_bytes)\n{\n    test(\"MEMKIND_HBW_PREFERRED\", 4194305);\n}\n\nTEST_F(DlopenTest, test_TC_MEMKIND_HBW_INTERLEAVE_4194305_bytes)\n{\n    test(\"MEMKIND_HBW_INTERLEAVE\", 4194305);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/test/load_tbbmalloc_symbols.c": "/*\n * Copyright (C) 2016 - 2018 Intel Corporation.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n * 1. Redistributions of source code must retain the above copyright notice(s),\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice(s),\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n * OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n * EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"tbbmalloc.h\"\n\nint load_tbbmalloc_symbols()\n{\n    const char so_name[]=\"libtbbmalloc.so.2\";\n    void *tbb_handle = dlopen(so_name, RTLD_LAZY);\n    if(!tbb_handle) {\n        printf(\"Cannot load %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_malloc = dlsym(tbb_handle, \"scalable_malloc\");\n    if(!scalable_malloc) {\n        printf(\"Cannot load scalable_malloc symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_realloc = dlsym(tbb_handle, \"scalable_realloc\");\n    if(!scalable_realloc) {\n        printf(\"Cannot load scalable_realloc symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_calloc = dlsym(tbb_handle, \"scalable_calloc\");\n    if(!scalable_calloc) {\n        printf(\"Cannot load scalable_calloc symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    scalable_free = dlsym(tbb_handle, \"scalable_free\");\n    if(!scalable_free) {\n        printf(\"Cannot load scalable_free symbol from %s\\n\", so_name);\n        return -1;\n    }\n\n    return 0;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/test/Makefile.mk": "#\n#  Copyright (C) 2014 - 2019 Intel Corporation.\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#  1. Redistributions of source code must retain the above copyright notice(s),\n#     this list of conditions and the following disclaimer.\n#  2. Redistributions in binary form must reproduce the above copyright notice(s),\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER(S) ``AS IS'' AND ANY EXPRESS\n#  OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n#  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO\n#  EVENT SHALL THE COPYRIGHT HOLDER(S) BE LIABLE FOR ANY DIRECT, INDIRECT,\n#  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n#  OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n#  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n\nAM_CPPFLAGS += -Itest/gtest_fused -DMEMKIND_DEPRECATED\\(x\\)=x\n\ncheck_PROGRAMS += test/all_tests \\\n                  test/allocator_perf_tool_tests \\\n                  test/autohbw_test_helper \\\n                  test/dax_kmem_test \\\n                  test/decorator_test \\\n                  test/environ_err_dax_kmem_malloc_test \\\n                  test/environ_err_dax_kmem_malloc_positive_test \\\n                  test/environ_err_hbw_malloc_test \\\n                  test/freeing_memory_segfault_test \\\n                  test/gb_page_tests_bind_policy \\\n                  test/locality_test \\\n                  test/memkind_stat_test \\\n                  test/performance_test \\\n                  test/trace_mechanism_test_helper \\\n                  # end\nif HAVE_CXX11\ncheck_PROGRAMS += test/pmem_test \\\n                  test/defrag_reallocate\nendif\n\nTESTS += test/test.sh\n\nEXTRA_DIST += test/autohbw_test.py \\\n              test/draw_plots.py \\\n              test/gtest_fused/gtest/gtest-all.cc \\\n              test/gtest_fused/gtest/gtest.h \\\n              test/hbw_detection_test.py \\\n              test/dax_kmem_env_var_test.py \\\n              test/memkind-afts-ext.ts \\\n              test/memkind-afts.ts \\\n              test/memkind-perf-ext.ts \\\n              test/memkind-perf.ts \\\n              test/memkind-pytests.ts \\\n              test/memkind-slts.ts \\\n              test/python_framework/__init__.py \\\n              test/python_framework/cmd_helper.py \\\n              test/python_framework/huge_page_organizer.py \\\n              test/run_alloc_benchmark.sh \\\n              test/trace_mechanism_test.py \\\n              # end\n\ntest_all_tests_LDADD = libmemkind.la\ntest_allocator_perf_tool_tests_LDADD = libmemkind.la\ntest_autohbw_test_helper_LDADD = libmemkind.la\ntest_dax_kmem_test_LDADD = libmemkind.la\ntest_decorator_test_LDADD = libmemkind.la\ntest_environ_err_hbw_malloc_test_LDADD = libmemkind.la\ntest_environ_err_dax_kmem_malloc_test_LDADD = libmemkind.la\ntest_environ_err_dax_kmem_malloc_positive_test_LDADD = libmemkind.la\ntest_freeing_memory_segfault_test_LDADD = libmemkind.la\ntest_gb_page_tests_bind_policy_LDADD = libmemkind.la\ntest_memkind_stat_test_LDADD = libmemkind.la\ntest_trace_mechanism_test_helper_LDADD = libmemkind.la\n\nif HAVE_CXX11\ntest_pmem_test_SOURCES = $(fused_gtest) test/memkind_pmem_config_tests.cpp test/memkind_pmem_long_time_tests.cpp test/memkind_pmem_tests.cpp\ntest_pmem_test_LDADD = libmemkind.la\ntest_defrag_reallocate_SOURCES = $(fused_gtest) test/memkind_defrag_reallocate.cpp\ntest_defrag_reallocate_LDADD = libmemkind.la\nendif\n\nfused_gtest = test/gtest_fused/gtest/gtest-all.cc \\\n              test/main.cpp \\\n              # end\n\ntest_all_tests_SOURCES = $(fused_gtest) \\\n                         test/Allocator.hpp \\\n                         test/TestPolicy.hpp \\\n                         test/bat_tests.cpp \\\n                         test/check.cpp \\\n                         test/check.h \\\n                         test/common.h \\\n                         test/dlopen_test.cpp \\\n                         test/error_message_tests.cpp \\\n                         test/get_arena_test.cpp \\\n                         test/hbw_allocator_tests.cpp \\\n                         test/hbw_verify_function_test.cpp \\\n                         test/memkind_allocator_tests.cpp \\\n                         test/memkind_detect_kind_tests.cpp \\\n                         test/memkind_null_kind_test.cpp \\\n                         test/memkind_versioning_tests.cpp \\\n                         test/multithreaded_tests.cpp \\\n                         test/negative_tests.cpp \\\n                         test/pmem_allocator_tests.cpp \\\n                         test/static_kinds_list.h \\\n                         test/static_kinds_tests.cpp \\\n                         test/trial_generator.cpp \\\n                         test/trial_generator.h \\\n                         #end\n\ntest_performance_test_SOURCES = $(fused_gtest) test/performance/perf_tests.cpp \\\n                                test/performance/perf_tests.hpp \\\n                                test/performance/framework.cpp \\\n                                test/performance/framework.hpp \\\n                                test/performance/operations.hpp \\\n                                test/performance/perf_tests.hpp\n\ntest_performance_test_LDADD = libmemkind.la\n\ntest_locality_test_SOURCES = $(fused_gtest) test/allocator_perf_tool/Allocation_info.cpp test/locality_test.cpp\ntest_locality_test_LDADD = libmemkind.la\n\ntest_locality_test_CPPFLAGS = $(OPENMP_CFLAGS) -O0 -Wno-error $(AM_CPPFLAGS)\ntest_locality_test_CXXFLAGS = $(OPENMP_CFLAGS) -O0 -Wno-error $(AM_CPPFLAGS)\n\ntest_autohbw_test_helper_SOURCES = test/autohbw_test_helper.c\ntest_decorator_test_SOURCES = $(fused_gtest) test/decorator_test.cpp test/decorator_test.h\ntest_dax_kmem_test_SOURCES = $(fused_gtest) test/memkind_dax_kmem_test.cpp\ntest_environ_err_hbw_malloc_test_SOURCES = test/environ_err_hbw_malloc_test.cpp\ntest_environ_err_dax_kmem_malloc_test_SOURCES = test/environ_err_dax_kmem_malloc_test.cpp\ntest_environ_err_dax_kmem_malloc_positive_test_SOURCES = test/environ_err_dax_kmem_malloc_positive_test.cpp\ntest_freeing_memory_segfault_test_SOURCES = $(fused_gtest) test/freeing_memory_segfault_test.cpp\ntest_gb_page_tests_bind_policy_SOURCES = $(fused_gtest) test/gb_page_tests_bind_policy.cpp test/trial_generator.cpp test/check.cpp\ntest_memkind_stat_test_SOURCES = $(fused_gtest) test/memkind_stat_test.cpp\ntest_trace_mechanism_test_helper_SOURCES = test/trace_mechanism_test_helper.c\n\n#Tests based on Allocator Perf Tool\nallocator_perf_tool_library_sources = test/allocator_perf_tool/AllocationSizes.hpp \\\n                                      test/allocator_perf_tool/Allocation_info.hpp \\\n                                      test/allocator_perf_tool/Allocation_info.cpp \\\n                                      test/allocator_perf_tool/Allocator.hpp \\\n                                      test/allocator_perf_tool/AllocatorFactory.hpp \\\n                                      test/allocator_perf_tool/CSVLogger.hpp \\\n                                      test/allocator_perf_tool/CommandLine.hpp \\\n                                      test/allocator_perf_tool/Configuration.hpp \\\n                                      test/allocator_perf_tool/ConsoleLog.hpp \\\n                                      test/allocator_perf_tool/FunctionCalls.hpp \\\n                                      test/allocator_perf_tool/FunctionCallsPerformanceTask.cpp \\\n                                      test/allocator_perf_tool/FunctionCallsPerformanceTask.h \\\n                                      test/allocator_perf_tool/GTestAdapter.hpp \\\n                                      test/allocator_perf_tool/HBWmallocAllocatorWithTimer.hpp \\\n                                      test/allocator_perf_tool/HugePageOrganizer.hpp \\\n                                      test/allocator_perf_tool/HugePageUnmap.hpp \\\n                                      test/allocator_perf_tool/Iterator.hpp \\\n                                      test/allocator_perf_tool/JemallocAllocatorWithTimer.hpp \\\n                                      test/allocator_perf_tool/MemkindAllocatorWithTimer.hpp \\\n                                      test/allocator_perf_tool/Numastat.hpp \\\n                                      test/allocator_perf_tool/PmemMockup.cpp \\\n                                      test/allocator_perf_tool/PmemMockup.hpp \\\n                                      test/allocator_perf_tool/Runnable.hpp \\\n                                      test/allocator_perf_tool/ScenarioWorkload.cpp \\\n                                      test/allocator_perf_tool/ScenarioWorkload.h \\\n                                      test/allocator_perf_tool/StandardAllocatorWithTimer.hpp \\\n                                      test/allocator_perf_tool/Stats.hpp \\\n                                      test/allocator_perf_tool/StressIncreaseToMax.cpp \\\n                                      test/allocator_perf_tool/StressIncreaseToMax.h \\\n                                      test/allocator_perf_tool/Task.hpp \\\n                                      test/allocator_perf_tool/TaskFactory.hpp \\\n                                      test/allocator_perf_tool/Tests.hpp \\\n                                      test/allocator_perf_tool/Thread.hpp \\\n                                      test/allocator_perf_tool/TimerSysTime.hpp \\\n                                      test/allocator_perf_tool/VectorIterator.hpp \\\n                                      test/allocator_perf_tool/Workload.hpp \\\n                                      test/allocator_perf_tool/WrappersMacros.hpp \\\n                                      test/memory_manager.h \\\n                                      test/proc_stat.h \\\n                                      test/random_sizes_allocator.h \\\n                                      # end\n\n\ntest_allocator_perf_tool_tests_SOURCES = $(allocator_perf_tool_library_sources) \\\n                                         $(fused_gtest) \\\n                                         test/alloc_performance_tests.cpp \\\n                                         test/allocate_to_max_stress_test.cpp \\\n                                         test/hbw_allocator_performance_tests.cpp \\\n                                         test/heap_manager_init_perf_test.cpp \\\n                                         test/huge_page_test.cpp \\\n                                         test/memory_footprint_test.cpp \\\n                                         test/pmem_alloc_performance_tests.cpp \\\n                                         # end\n\n\ntest_allocator_perf_tool_tests_CPPFLAGS = -Itest/allocator_perf_tool/ -O0 -Wno-error $(AM_CPPFLAGS)\ntest_allocator_perf_tool_tests_CXXFLAGS = -Itest/allocator_perf_tool/ -O0 -Wno-error $(AM_CPPFLAGS)\ntest_allocator_perf_tool_tests_LDFLAGS = -lpthread -lnuma\n\nNUMAKIND_MAX = 2048\ntest_all_tests_CXXFLAGS = $(AM_CXXFLAGS) $(CXXFLAGS) $(OPENMP_CFLAGS) -DNUMAKIND_MAX=$(NUMAKIND_MAX)\ntest_all_tests_LDFLAGS = -ldl\n\n#Allocator Perf Tool stand alone app\ncheck_PROGRAMS += test/perf_tool\ntest_perf_tool_LDADD = libmemkind.la\ntest_perf_tool_SOURCES = $(allocator_perf_tool_library_sources) \\\n                         test/allocator_perf_tool/main.cpp \\\n                         # end\n\n\ntest_perf_tool_CPPFLAGS = -Itest/allocator_perf_tool/ -O0 -Wno-error $(AM_CPPFLAGS)\ntest_perf_tool_CXXFLAGS = -Itest/allocator_perf_tool/ -O0 -Wno-error $(AM_CPPFLAGS)\ntest_perf_tool_LDFLAGS = -lpthread -lnuma\nif HAVE_CXX11\ntest_perf_tool_CPPFLAGS += -std=c++11\ntest_perf_tool_CXXFLAGS += -std=c++11\nendif\n\n#Alloc benchmark\ncheck_PROGRAMS += test/alloc_benchmark_glibc \\\n                  test/alloc_benchmark_hbw \\\n                  test/alloc_benchmark_pmem \\\n                  test/alloc_benchmark_tbb \\\n                  # end\n\ntest_alloc_benchmark_glibc_CFLAGS = -O0 -g $(OPENMP_CFLAGS) -Wall\ntest_alloc_benchmark_glibc_LDADD = libmemkind.la\ntest_alloc_benchmark_glibc_SOURCES = test/alloc_benchmark.c\n\ntest_alloc_benchmark_hbw_CFLAGS = -O0 -g $(OPENMP_CFLAGS) -Wall -DHBWMALLOC\ntest_alloc_benchmark_hbw_LDADD = libmemkind.la\ntest_alloc_benchmark_hbw_LDFLAGS = -lmemkind\ntest_alloc_benchmark_hbw_SOURCES = test/alloc_benchmark.c\n\ntest_alloc_benchmark_pmem_CFLAGS = -O0 -g $(OPENMP_CFLAGS) -Wall -DPMEMMALLOC\ntest_alloc_benchmark_pmem_LDADD = libmemkind.la\ntest_alloc_benchmark_pmem_LDFLAGS = -ldl\ntest_alloc_benchmark_pmem_SOURCES = test/alloc_benchmark.c\n\ntest_alloc_benchmark_tbb_CFLAGS = -O0 -g $(OPENMP_CFLAGS) -Wall -DTBBMALLOC\ntest_alloc_benchmark_tbb_LDADD = libmemkind.la\ntest_alloc_benchmark_tbb_LDFLAGS = -ldl\ntest_alloc_benchmark_tbb_SOURCES = test/alloc_benchmark.c \\\n                                   test/load_tbbmalloc_symbols.c \\\n                                   test/tbbmalloc.h \\\n                                   # end\n\n# Pmem fragmentation benchmark\ncheck_PROGRAMS += test/fragmentation_benchmark_pmem\ntest_fragmentation_benchmark_pmem_LDADD = libmemkind.la\ntest_fragmentation_benchmark_pmem_SOURCES = test/fragmentation_benchmark_pmem.cpp\ntest_fragmentation_benchmark_pmem_CXXFLAGS = -O0 -Wall\nif HAVE_CXX11\ntest_fragmentation_benchmark_pmem_CXXFLAGS += -std=c++11\nendif\n\n# Examples as tests\ncheck_PROGRAMS += test/autohbw_candidates \\\n                  test/filter_memkind \\\n                  test/hello_hbw \\\n                  test/hello_memkind \\\n                  test/hello_memkind_debug \\\n                  test/memkind_get_stat \\\n                  test/pmem_alignment \\\n                  test/pmem_and_dax_kmem_kind \\\n                  test/pmem_and_default_kind \\\n                  test/pmem_config \\\n                  test/pmem_detect_kind \\\n                  test/pmem_free_with_unknown_kind \\\n                  test/pmem_kinds \\\n                  test/pmem_malloc \\\n                  test/pmem_malloc_unlimited \\\n                  test/pmem_multithreads \\\n                  test/pmem_multithreads_onekind \\\n                  test/pmem_usable_size \\\n                  # end\nif HAVE_CXX11\ncheck_PROGRAMS += test/memkind_allocated \\\n                  test/memkind_cpp_allocator \\\n                  test/pmem_cpp_allocator\nendif\n\ntest_autohbw_candidates_LDADD = libmemkind.la\ntest_filter_memkind_LDADD = libmemkind.la\ntest_hello_hbw_LDADD = libmemkind.la\ntest_hello_memkind_LDADD = libmemkind.la\ntest_hello_memkind_debug_LDADD = libmemkind.la\ntest_memkind_get_stat_LDADD = libmemkind.la\ntest_pmem_alignment_LDADD = libmemkind.la\ntest_pmem_and_dax_kmem_kind_LDADD = libmemkind.la\ntest_pmem_and_default_kind_LDADD = libmemkind.la\ntest_pmem_config_LDADD = libmemkind.la\ntest_pmem_detect_kind_LDADD = libmemkind.la\ntest_pmem_free_with_unknown_kind_LDADD = libmemkind.la\ntest_pmem_kinds_LDADD = libmemkind.la\ntest_pmem_malloc_LDADD = libmemkind.la\ntest_pmem_malloc_unlimited_LDADD = libmemkind.la\ntest_pmem_multithreads_LDADD = libmemkind.la\ntest_pmem_multithreads_onekind_LDADD = libmemkind.la\ntest_pmem_usable_size_LDADD = libmemkind.la\nif HAVE_CXX11\ntest_memkind_allocated_LDADD = libmemkind.la\ntest_memkind_cpp_allocator_LDADD = libmemkind.la\ntest_pmem_cpp_allocator_LDADD = libmemkind.la\nendif\n\ntest_autohbw_candidates_SOURCES = examples/autohbw_candidates.c\ntest_filter_memkind_SOURCES = examples/filter_example.c\ntest_hello_hbw_SOURCES = examples/hello_hbw_example.c\ntest_hello_memkind_SOURCES = examples/hello_memkind_example.c\ntest_hello_memkind_debug_SOURCES = examples/hello_memkind_example.c examples/memkind_decorator_debug.c\ntest_memkind_get_stat_SOURCES = examples/memkind_get_stat.c\ntest_pmem_alignment_SOURCES = examples/pmem_alignment.c\ntest_pmem_and_dax_kmem_kind_SOURCES = examples/pmem_and_dax_kmem_kind.c\ntest_pmem_and_default_kind_SOURCES = examples/pmem_and_default_kind.c\ntest_pmem_config_SOURCES = examples/pmem_config.c\ntest_pmem_detect_kind_SOURCES = examples/pmem_detect_kind.c\ntest_pmem_free_with_unknown_kind_SOURCES = examples/pmem_free_with_unknown_kind.c\ntest_pmem_kinds_SOURCES = examples/pmem_kinds.c\ntest_pmem_malloc_SOURCES = examples/pmem_malloc.c\ntest_pmem_malloc_unlimited_SOURCES = examples/pmem_malloc_unlimited.c\ntest_pmem_multithreads_SOURCES = examples/pmem_multithreads.c\ntest_pmem_multithreads_onekind_SOURCES = examples/pmem_multithreads_onekind.c\ntest_pmem_usable_size_SOURCES = examples/pmem_usable_size.c\n\ntest_libautohbw_la_SOURCES = autohbw/autohbw.c\nnoinst_LTLIBRARIES += test/libautohbw.la\nif HAVE_CXX11\ntest_memkind_allocated_SOURCES = examples/memkind_allocated_example.cpp examples/memkind_allocated.hpp\ntest_memkind_cpp_allocator_SOURCES = examples/memkind_cpp_allocator.cpp\ntest_pmem_cpp_allocator_SOURCES = examples/pmem_cpp_allocator.cpp\nendif\n\nclean-local: test-clean\n\ntest-clean:\n\tfind test \\( -name \"*.gcda\" -o -name \"*.gcno\" \\) -type f -delete\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/jemalloc/INSTALL.md": "Building and installing a packaged release of jemalloc can be as simple as\ntyping the following while in the root directory of the source tree:\n\n    ./configure\n    make\n    make install\n\nIf building from unpackaged developer sources, the simplest command sequence\nthat might work is:\n\n    ./autogen.sh\n    make dist\n    make\n    make install\n\nNote that documentation is not built by the default target because doing so\nwould create a dependency on xsltproc in packaged releases, hence the\nrequirement to either run 'make dist' or avoid installing docs via the various\ninstall_* targets documented below.\n\n\n## Advanced configuration\n\nThe 'configure' script supports numerous options that allow control of which\nfunctionality is enabled, where jemalloc is installed, etc.  Optionally, pass\nany of the following arguments (not a definitive list) to 'configure':\n\n* `--help`\n\n    Print a definitive list of options.\n\n* `--prefix=<install-root-dir>`\n\n    Set the base directory in which to install.  For example:\n\n        ./configure --prefix=/usr/local\n\n    will cause files to be installed into /usr/local/include, /usr/local/lib,\n    and /usr/local/man.\n\n* `--with-version=(<major>.<minor>.<bugfix>-<nrev>-g<gid>|VERSION)`\n\n    The VERSION file is mandatory for successful configuration, and the\n    following steps are taken to assure its presence:\n    1) If --with-version=<major>.<minor>.<bugfix>-<nrev>-g<gid> is specified,\n       generate VERSION using the specified value.\n    2) If --with-version is not specified in either form and the source\n       directory is inside a git repository, try to generate VERSION via 'git\n       describe' invocations that pattern-match release tags.\n    3) If VERSION is missing, generate it with a bogus version:\n       0.0.0-0-g0000000000000000000000000000000000000000\n\n    Note that --with-version=VERSION bypasses (1) and (2), which simplifies\n    VERSION configuration when embedding a jemalloc release into another\n    project's git repository.\n\n* `--with-rpath=<colon-separated-rpath>`\n\n    Embed one or more library paths, so that libjemalloc can find the libraries\n    it is linked to.  This works only on ELF-based systems.\n\n* `--with-mangling=<map>`\n\n    Mangle public symbols specified in <map> which is a comma-separated list of\n    name:mangled pairs.\n\n    For example, to use ld's --wrap option as an alternative method for\n    overriding libc's malloc implementation, specify something like:\n\n      --with-mangling=malloc:__wrap_malloc,free:__wrap_free[...]\n\n    Note that mangling happens prior to application of the prefix specified by\n    --with-jemalloc-prefix, and mangled symbols are then ignored when applying\n    the prefix.\n\n* `--with-jemalloc-prefix=<prefix>`\n\n    Prefix all public APIs with <prefix>.  For example, if <prefix> is\n    \"prefix_\", API changes like the following occur:\n\n      malloc()         --> prefix_malloc()\n      malloc_conf      --> prefix_malloc_conf\n      /etc/malloc.conf --> /etc/prefix_malloc.conf\n      MALLOC_CONF      --> PREFIX_MALLOC_CONF\n\n    This makes it possible to use jemalloc at the same time as the system\n    allocator, or even to use multiple copies of jemalloc simultaneously.\n\n    By default, the prefix is \"\", except on OS X, where it is \"je_\".  On OS X,\n    jemalloc overlays the default malloc zone, but makes no attempt to actually\n    replace the \"malloc\", \"calloc\", etc. symbols.\n\n* `--without-export`\n\n    Don't export public APIs.  This can be useful when building jemalloc as a\n    static library, or to avoid exporting public APIs when using the zone\n    allocator on OSX.\n\n* `--with-private-namespace=<prefix>`\n\n    Prefix all library-private APIs with <prefix>je_.  For shared libraries,\n    symbol visibility mechanisms prevent these symbols from being exported, but\n    for static libraries, naming collisions are a real possibility.  By\n    default, <prefix> is empty, which results in a symbol prefix of je_ .\n\n* `--with-install-suffix=<suffix>`\n\n    Append <suffix> to the base name of all installed files, such that multiple\n    versions of jemalloc can coexist in the same installation directory.  For\n    example, libjemalloc.so.0 becomes libjemalloc<suffix>.so.0.\n\n* `--with-malloc-conf=<malloc_conf>`\n\n    Embed `<malloc_conf>` as a run-time options string that is processed prior to\n    the malloc_conf global variable, the /etc/malloc.conf symlink, and the\n    MALLOC_CONF environment variable.  For example, to change the default decay\n    time to 30 seconds:\n\n      --with-malloc-conf=decay_ms:30000\n\n* `--enable-debug`\n\n    Enable assertions and validation code.  This incurs a substantial\n    performance hit, but is very useful during application development.\n\n* `--disable-stats`\n\n    Disable statistics gathering functionality.  See the \"opt.stats_print\"\n    option documentation for usage details.\n\n* `--enable-prof`\n\n    Enable heap profiling and leak detection functionality.  See the \"opt.prof\"\n    option documentation for usage details.  When enabled, there are several\n    approaches to backtracing, and the configure script chooses the first one\n    in the following list that appears to function correctly:\n\n    + libunwind      (requires --enable-prof-libunwind)\n    + libgcc         (unless --disable-prof-libgcc)\n    + gcc intrinsics (unless --disable-prof-gcc)\n\n* `--enable-prof-libunwind`\n\n    Use the libunwind library (http://www.nongnu.org/libunwind/) for stack\n    backtracing.\n\n* `--disable-prof-libgcc`\n\n    Disable the use of libgcc's backtracing functionality.\n\n* `--disable-prof-gcc`\n\n    Disable the use of gcc intrinsics for backtracing.\n\n* `--with-static-libunwind=<libunwind.a>`\n\n    Statically link against the specified libunwind.a rather than dynamically\n    linking with -lunwind.\n\n* `--disable-fill`\n\n    Disable support for junk/zero filling of memory.  See the \"opt.junk\" and\n    \"opt.zero\" option documentation for usage details.\n\n* `--disable-zone-allocator`\n\n    Disable zone allocator for Darwin.  This means jemalloc won't be hooked as\n    the default allocator on OSX/iOS.\n\n* `--enable-utrace`\n\n    Enable utrace(2)-based allocation tracing.  This feature is not broadly\n    portable (FreeBSD has it, but Linux and OS X do not).\n\n* `--enable-xmalloc`\n\n    Enable support for optional immediate termination due to out-of-memory\n    errors, as is commonly implemented by \"xmalloc\" wrapper function for malloc.\n    See the \"opt.xmalloc\" option documentation for usage details.\n\n* `--enable-lazy-lock`\n\n    Enable code that wraps pthread_create() to detect when an application\n    switches from single-threaded to multi-threaded mode, so that it can avoid\n    mutex locking/unlocking operations while in single-threaded mode.  In\n    practice, this feature usually has little impact on performance unless\n    thread-specific caching is disabled.\n\n* `--disable-cache-oblivious`\n\n    Disable cache-oblivious large allocation alignment for large allocation\n    requests with no alignment constraints.  If this feature is disabled, all\n    large allocations are page-aligned as an implementation artifact, which can\n    severely harm CPU cache utilization.  However, the cache-oblivious layout\n    comes at the cost of one extra page per large allocation, which in the\n    most extreme case increases physical memory usage for the 16 KiB size class\n    to 20 KiB.\n\n* `--disable-syscall`\n\n    Disable use of syscall(2) rather than {open,read,write,close}(2).  This is\n    intended as a workaround for systems that place security limitations on\n    syscall(2).\n\n* `--disable-cxx`\n\n    Disable C++ integration.  This will cause new and delete operator\n    implementations to be omitted.\n\n* `--with-xslroot=<path>`\n\n    Specify where to find DocBook XSL stylesheets when building the\n    documentation.\n\n* `--with-lg-page=<lg-page>`\n\n    Specify the base 2 log of the allocator page size, which must in turn be at\n    least as large as the system page size.  By default the configure script\n    determines the host's page size and sets the allocator page size equal to\n    the system page size, so this option need not be specified unless the\n    system page size may change between configuration and execution, e.g. when\n    cross compiling.\n\n* `--with-lg-hugepage=<lg-hugepage>`\n\n    Specify the base 2 log of the system huge page size.  This option is useful\n    when cross compiling, or when overriding the default for systems that do\n    not explicitly support huge pages.\n\n* `--with-lg-quantum=<lg-quantum>`\n\n    Specify the base 2 log of the minimum allocation alignment.  jemalloc needs\n    to know the minimum alignment that meets the following C standard\n    requirement (quoted from the April 12, 2011 draft of the C11 standard):\n\n    >  The pointer returned if the allocation succeeds is suitably aligned so\n      that it may be assigned to a pointer to any type of object with a\n      fundamental alignment requirement and then used to access such an object\n      or an array of such objects in the space allocated [...]\n\n    This setting is architecture-specific, and although jemalloc includes known\n    safe values for the most commonly used modern architectures, there is a\n    wrinkle related to GNU libc (glibc) that may impact your choice of\n    <lg-quantum>.  On most modern architectures, this mandates 16-byte\n    alignment (<lg-quantum>=4), but the glibc developers chose not to meet this\n    requirement for performance reasons.  An old discussion can be found at\n    <https://sourceware.org/bugzilla/show_bug.cgi?id=206> .  Unlike glibc,\n    jemalloc does follow the C standard by default (caveat: jemalloc\n    technically cheats for size classes smaller than the quantum), but the fact\n    that Linux systems already work around this allocator noncompliance means\n    that it is generally safe in practice to let jemalloc's minimum alignment\n    follow glibc's lead.  If you specify `--with-lg-quantum=3` during\n    configuration, jemalloc will provide additional size classes that are not\n    16-byte-aligned (24, 40, and 56).\n\n* `--with-lg-vaddr=<lg-vaddr>`\n\n    Specify the number of significant virtual address bits.  By default, the\n    configure script attempts to detect virtual address size on those platforms\n    where it knows how, and picks a default otherwise.  This option may be\n    useful when cross-compiling.\n\n* `--disable-initial-exec-tls`\n\n    Disable the initial-exec TLS model for jemalloc's internal thread-local\n    storage (on those platforms that support explicit settings).  This can allow\n    jemalloc to be dynamically loaded after program startup (e.g. using dlopen).\n    Note that in this case, there will be two malloc implementations operating\n    in the same process, which will almost certainly result in confusing runtime\n    crashes if pointers leak from one implementation to the other.\n\n* `--disable-libdl`\n\n    Disable the usage of libdl, namely dlsym(3) which is required by the lazy\n    lock option.  This can allow building static binaries.\n\nThe following environment variables (not a definitive list) impact configure's\nbehavior:\n\n* `CFLAGS=\"?\"`\n* `CXXFLAGS=\"?\"`\n\n    Pass these flags to the C/C++ compiler.  Any flags set by the configure\n    script are prepended, which means explicitly set flags generally take\n    precedence.  Take care when specifying flags such as -Werror, because\n    configure tests may be affected in undesirable ways.\n\n* `EXTRA_CFLAGS=\"?\"`\n* `EXTRA_CXXFLAGS=\"?\"`\n\n    Append these flags to CFLAGS/CXXFLAGS, without passing them to the\n    compiler(s) during configuration.  This makes it possible to add flags such\n    as -Werror, while allowing the configure script to determine what other\n    flags are appropriate for the specified configuration.\n\n* `CPPFLAGS=\"?\"`\n\n    Pass these flags to the C preprocessor.  Note that CFLAGS is not passed to\n    'cpp' when 'configure' is looking for include files, so you must use\n    CPPFLAGS instead if you need to help 'configure' find header files.\n\n* `LD_LIBRARY_PATH=\"?\"`\n\n    'ld' uses this colon-separated list to find libraries.\n\n* `LDFLAGS=\"?\"`\n\n    Pass these flags when linking.\n\n* `PATH=\"?\"`\n\n    'configure' uses this to find programs.\n\nIn some cases it may be necessary to work around configuration results that do\nnot match reality.  For example, Linux 4.5 added support for the MADV_FREE flag\nto madvise(2), which can cause problems if building on a host with MADV_FREE\nsupport and deploying to a target without.  To work around this, use a cache\nfile to override the relevant configuration variable defined in configure.ac,\ne.g.:\n\n    echo \"je_cv_madv_free=no\" > config.cache && ./configure -C\n\n\n## Advanced compilation\n\nTo build only parts of jemalloc, use the following targets:\n\n    build_lib_shared\n    build_lib_static\n    build_lib\n    build_doc_html\n    build_doc_man\n    build_doc\n\nTo install only parts of jemalloc, use the following targets:\n\n    install_bin\n    install_include\n    install_lib_shared\n    install_lib_static\n    install_lib_pc\n    install_lib\n    install_doc_html\n    install_doc_man\n    install_doc\n\nTo clean up build results to varying degrees, use the following make targets:\n\n    clean\n    distclean\n    relclean\n\n\n## Advanced installation\n\nOptionally, define make variables when invoking make, including (not\nexclusively):\n\n* `INCLUDEDIR=\"?\"`\n\n    Use this as the installation prefix for header files.\n\n* `LIBDIR=\"?\"`\n\n    Use this as the installation prefix for libraries.\n\n* `MANDIR=\"?\"`\n\n    Use this as the installation prefix for man pages.\n\n* `DESTDIR=\"?\"`\n\n    Prepend DESTDIR to INCLUDEDIR, LIBDIR, DATADIR, and MANDIR.  This is useful\n    when installing to a different path than was specified via --prefix.\n\n* `CC=\"?\"`\n\n    Use this to invoke the C compiler.\n\n* `CFLAGS=\"?\"`\n\n    Pass these flags to the compiler.\n\n* `CPPFLAGS=\"?\"`\n\n    Pass these flags to the C preprocessor.\n\n* `LDFLAGS=\"?\"`\n\n    Pass these flags when linking.\n\n* `PATH=\"?\"`\n\n    Use this to search for programs used during configuration and building.\n\n\n## Development\n\nIf you intend to make non-trivial changes to jemalloc, use the 'autogen.sh'\nscript rather than 'configure'.  This re-generates 'configure', enables\nconfiguration dependency rules, and enables re-generation of automatically\ngenerated source files.\n\nThe build system supports using an object directory separate from the source\ntree.  For example, you can create an 'obj' directory, and from within that\ndirectory, issue configuration and build commands:\n\n    autoconf\n    mkdir obj\n    cd obj\n    ../configure --enable-autogen\n    make\n\n\n## Documentation\n\nThe manual page is generated in both html and roff formats.  Any web browser\ncan be used to view the html manual.  The roff manual page can be formatted\nprior to installation via the following command:\n\n    nroff -man -t doc/jemalloc.3\n",
        "/tmp/vanessa/spack-stage/spack-stage-memkind-1.10.0-rd2ipb3h26xhmaayyrmthwcx2a7zyns7/spack-src/jemalloc/src/jemalloc.c": "#define JEMALLOC_C_\n#include \"jemalloc/internal/jemalloc_preamble.h\"\n#include \"jemalloc/internal/jemalloc_internal_includes.h\"\n\n#include \"jemalloc/internal/assert.h\"\n#include \"jemalloc/internal/atomic.h\"\n#include \"jemalloc/internal/ctl.h\"\n#include \"jemalloc/internal/extent_dss.h\"\n#include \"jemalloc/internal/extent_mmap.h\"\n#include \"jemalloc/internal/hook.h\"\n#include \"jemalloc/internal/jemalloc_internal_types.h\"\n#include \"jemalloc/internal/log.h\"\n#include \"jemalloc/internal/malloc_io.h\"\n#include \"jemalloc/internal/mutex.h\"\n#include \"jemalloc/internal/rtree.h\"\n#include \"jemalloc/internal/safety_check.h\"\n#include \"jemalloc/internal/sc.h\"\n#include \"jemalloc/internal/spin.h\"\n#include \"jemalloc/internal/sz.h\"\n#include \"jemalloc/internal/ticker.h\"\n#include \"jemalloc/internal/util.h\"\n\n/******************************************************************************/\n/* Data. */\n\n/* Runtime configuration options. */\nconst char\t*je_malloc_conf\n#ifndef _WIN32\n    JEMALLOC_ATTR(weak)\n#endif\n    ;\nbool\topt_abort =\n#ifdef JEMALLOC_DEBUG\n    true\n#else\n    false\n#endif\n    ;\nbool\topt_abort_conf =\n#ifdef JEMALLOC_DEBUG\n    true\n#else\n    false\n#endif\n    ;\n/* Intentionally default off, even with debug builds. */\nbool\topt_confirm_conf = false;\nconst char\t*opt_junk =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    \"true\"\n#else\n    \"false\"\n#endif\n    ;\nbool\topt_junk_alloc =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\nbool\topt_junk_free =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\n\nbool\topt_utrace = false;\nbool\topt_xmalloc = false;\nbool\topt_zero = false;\nunsigned\topt_narenas = 0;\n\nunsigned\tncpus;\n\n/* Protects arenas initialization. */\nmalloc_mutex_t arenas_lock;\n/*\n * Arenas that are used to service external requests.  Not all elements of the\n * arenas array are necessarily used; arenas are created lazily as needed.\n *\n * arenas[0..narenas_auto) are used for automatic multiplexing of threads and\n * arenas.  arenas[narenas_auto..narenas_total) are only used if the application\n * takes some action to create them and allocate from them.\n *\n * Points to an arena_t.\n */\nJEMALLOC_ALIGNED(CACHELINE)\natomic_p_t\t\tarenas[MALLOCX_ARENA_LIMIT];\nstatic atomic_u_t\tnarenas_total; /* Use narenas_total_*(). */\n/* Below three are read-only after initialization. */\nstatic arena_t\t\t*a0; /* arenas[0]. */\nunsigned\t\tnarenas_auto;\nunsigned\t\tmanual_arena_base;\n\ntypedef enum {\n\tmalloc_init_uninitialized\t= 3,\n\tmalloc_init_a0_initialized\t= 2,\n\tmalloc_init_recursible\t\t= 1,\n\tmalloc_init_initialized\t\t= 0 /* Common case --> jnz. */\n} malloc_init_t;\nstatic malloc_init_t\tmalloc_init_state = malloc_init_uninitialized;\n\n/* False should be the common case.  Set to true to trigger initialization. */\nbool\t\t\tmalloc_slow = true;\n\n/* When malloc_slow is true, set the corresponding bits for sanity check. */\nenum {\n\tflag_opt_junk_alloc\t= (1U),\n\tflag_opt_junk_free\t= (1U << 1),\n\tflag_opt_zero\t\t= (1U << 2),\n\tflag_opt_utrace\t\t= (1U << 3),\n\tflag_opt_xmalloc\t= (1U << 4)\n};\nstatic uint8_t\tmalloc_slow_flags;\n\n#ifdef JEMALLOC_THREADED_INIT\n/* Used to let the initializing thread recursively allocate. */\n#  define NO_INITIALIZER\t((unsigned long)0)\n#  define INITIALIZER\t\tpthread_self()\n#  define IS_INITIALIZER\t(malloc_initializer == pthread_self())\nstatic pthread_t\t\tmalloc_initializer = NO_INITIALIZER;\n#else\n#  define NO_INITIALIZER\tfalse\n#  define INITIALIZER\t\ttrue\n#  define IS_INITIALIZER\tmalloc_initializer\nstatic bool\t\t\tmalloc_initializer = NO_INITIALIZER;\n#endif\n\n/* Used to avoid initialization races. */\n#ifdef _WIN32\n#if _WIN32_WINNT >= 0x0600\nstatic malloc_mutex_t\tinit_lock = SRWLOCK_INIT;\n#else\nstatic malloc_mutex_t\tinit_lock;\nstatic bool init_lock_initialized = false;\n\nJEMALLOC_ATTR(constructor)\nstatic void WINAPI\n_init_init_lock(void) {\n\t/*\n\t * If another constructor in the same binary is using mallctl to e.g.\n\t * set up extent hooks, it may end up running before this one, and\n\t * malloc_init_hard will crash trying to lock the uninitialized lock. So\n\t * we force an initialization of the lock in malloc_init_hard as well.\n\t * We don't try to care about atomicity of the accessed to the\n\t * init_lock_initialized boolean, since it really only matters early in\n\t * the process creation, before any separate thread normally starts\n\t * doing anything.\n\t */\n\tif (!init_lock_initialized) {\n\t\tmalloc_mutex_init(&init_lock, \"init\", WITNESS_RANK_INIT,\n\t\t    malloc_mutex_rank_exclusive);\n\t}\n\tinit_lock_initialized = true;\n}\n\n#ifdef _MSC_VER\n#  pragma section(\".CRT$XCU\", read)\nJEMALLOC_SECTION(\".CRT$XCU\") JEMALLOC_ATTR(used)\nstatic const void (WINAPI *init_init_lock)(void) = _init_init_lock;\n#endif\n#endif\n#else\nstatic malloc_mutex_t\tinit_lock = MALLOC_MUTEX_INITIALIZER;\n#endif\n\ntypedef struct {\n\tvoid\t*p;\t/* Input pointer (as in realloc(p, s)). */\n\tsize_t\ts;\t/* Request size. */\n\tvoid\t*r;\t/* Result pointer. */\n} malloc_utrace_t;\n\n#ifdef JEMALLOC_UTRACE\n#  define UTRACE(a, b, c) do {\t\t\t\t\t\t\\\n\tif (unlikely(opt_utrace)) {\t\t\t\t\t\\\n\t\tint utrace_serrno = errno;\t\t\t\t\\\n\t\tmalloc_utrace_t ut;\t\t\t\t\t\\\n\t\tut.p = (a);\t\t\t\t\t\t\\\n\t\tut.s = (b);\t\t\t\t\t\t\\\n\t\tut.r = (c);\t\t\t\t\t\t\\\n\t\tutrace(&ut, sizeof(ut));\t\t\t\t\\\n\t\terrno = utrace_serrno;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n#else\n#  define UTRACE(a, b, c)\n#endif\n\n/* Whether encountered any invalid config options. */\nstatic bool had_conf_error = false;\n\n/******************************************************************************/\n/*\n * Function prototypes for static functions that are referenced prior to\n * definition.\n */\n\nstatic bool\tmalloc_init_hard_a0(void);\nstatic bool\tmalloc_init_hard(void);\n\n/******************************************************************************/\n/*\n * Begin miscellaneous support functions.\n */\n\nbool\nmalloc_initialized(void) {\n\treturn (malloc_init_state == malloc_init_initialized);\n}\n\nJEMALLOC_ALWAYS_INLINE bool\nmalloc_init_a0(void) {\n\tif (unlikely(malloc_init_state == malloc_init_uninitialized)) {\n\t\treturn malloc_init_hard_a0();\n\t}\n\treturn false;\n}\n\nJEMALLOC_ALWAYS_INLINE bool\nmalloc_init(void) {\n\tif (unlikely(!malloc_initialized()) && malloc_init_hard()) {\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * The a0*() functions are used instead of i{d,}alloc() in situations that\n * cannot tolerate TLS variable access.\n */\n\nstatic void *\na0ialloc(size_t size, bool zero, bool is_internal) {\n\tif (unlikely(malloc_init_a0())) {\n\t\treturn NULL;\n\t}\n\n\treturn iallocztm(TSDN_NULL, size, sz_size2index(size), zero, NULL,\n\t    is_internal, arena_get(TSDN_NULL, 0, true), true);\n}\n\nstatic void\na0idalloc(void *ptr, bool is_internal) {\n\tidalloctm(TSDN_NULL, ptr, NULL, NULL, is_internal, true);\n}\n\nvoid *\na0malloc(size_t size) {\n\treturn a0ialloc(size, false, true);\n}\n\nvoid\na0dalloc(void *ptr) {\n\ta0idalloc(ptr, true);\n}\n\n/*\n * FreeBSD's libc uses the bootstrap_*() functions in bootstrap-senstive\n * situations that cannot tolerate TLS variable access (TLS allocation and very\n * early internal data structure initialization).\n */\n\nvoid *\nbootstrap_malloc(size_t size) {\n\tif (unlikely(size == 0)) {\n\t\tsize = 1;\n\t}\n\n\treturn a0ialloc(size, false, false);\n}\n\nvoid *\nbootstrap_calloc(size_t num, size_t size) {\n\tsize_t num_size;\n\n\tnum_size = num * size;\n\tif (unlikely(num_size == 0)) {\n\t\tassert(num == 0 || size == 0);\n\t\tnum_size = 1;\n\t}\n\n\treturn a0ialloc(num_size, true, false);\n}\n\nvoid\nbootstrap_free(void *ptr) {\n\tif (unlikely(ptr == NULL)) {\n\t\treturn;\n\t}\n\n\ta0idalloc(ptr, false);\n}\n\nvoid\narena_set(unsigned ind, arena_t *arena) {\n\tatomic_store_p(&arenas[ind], arena, ATOMIC_RELEASE);\n}\n\nstatic void\nnarenas_total_set(unsigned narenas) {\n\tatomic_store_u(&narenas_total, narenas, ATOMIC_RELEASE);\n}\n\nstatic void\nnarenas_total_inc(void) {\n\tatomic_fetch_add_u(&narenas_total, 1, ATOMIC_RELEASE);\n}\n\nunsigned\nnarenas_total_get(void) {\n\treturn atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);\n}\n\n/* Create a new arena and insert it into the arenas array at index ind. */\nstatic arena_t *\narena_init_locked(tsdn_t *tsdn, unsigned ind, extent_hooks_t *extent_hooks) {\n\tarena_t *arena;\n\n\tassert(ind <= narenas_total_get());\n\tif (ind >= MALLOCX_ARENA_LIMIT) {\n\t\treturn NULL;\n\t}\n\tif (ind == narenas_total_get()) {\n\t\tnarenas_total_inc();\n\t}\n\n\t/*\n\t * Another thread may have already initialized arenas[ind] if it's an\n\t * auto arena.\n\t */\n\tarena = arena_get(tsdn, ind, false);\n\tif (arena != NULL) {\n\t\tassert(arena_is_auto(arena));\n\t\treturn arena;\n\t}\n\n\t/* Actually initialize the arena. */\n\tarena = arena_new(tsdn, ind, extent_hooks);\n\n\treturn arena;\n}\n\nstatic void\narena_new_create_background_thread(tsdn_t *tsdn, unsigned ind) {\n\tif (ind == 0) {\n\t\treturn;\n\t}\n\t/*\n\t * Avoid creating a new background thread just for the huge arena, which\n\t * purges eagerly by default.\n\t */\n\tif (have_background_thread && !arena_is_huge(ind)) {\n\t\tif (background_thread_create(tsdn_tsd(tsdn), ind)) {\n\t\t\tmalloc_printf(\"<jemalloc>: error in background thread \"\n\t\t\t\t      \"creation for arena %u. Abort.\\n\", ind);\n\t\t\tabort();\n\t\t}\n\t}\n}\n\narena_t *\narena_init(tsdn_t *tsdn, unsigned ind, extent_hooks_t *extent_hooks) {\n\tarena_t *arena;\n\n\tmalloc_mutex_lock(tsdn, &arenas_lock);\n\tarena = arena_init_locked(tsdn, ind, extent_hooks);\n\tmalloc_mutex_unlock(tsdn, &arenas_lock);\n\n\tarena_new_create_background_thread(tsdn, ind);\n\n\treturn arena;\n}\n\nstatic void\narena_bind(tsd_t *tsd, unsigned ind, bool internal) {\n\tarena_t *arena = arena_get(tsd_tsdn(tsd), ind, false);\n\tarena_nthreads_inc(arena, internal);\n\n\tif (internal) {\n\t\ttsd_iarena_set(tsd, arena);\n\t} else {\n\t\ttsd_arena_set(tsd, arena);\n\t\tunsigned shard = atomic_fetch_add_u(&arena->binshard_next, 1,\n\t\t    ATOMIC_RELAXED);\n\t\ttsd_binshards_t *bins = tsd_binshardsp_get(tsd);\n\t\tfor (unsigned i = 0; i < SC_NBINS; i++) {\n\t\t\tassert(bin_infos[i].n_shards > 0 &&\n\t\t\t    bin_infos[i].n_shards <= BIN_SHARDS_MAX);\n\t\t\tbins->binshard[i] = shard % bin_infos[i].n_shards;\n\t\t}\n\t}\n}\n\nvoid\narena_migrate(tsd_t *tsd, unsigned oldind, unsigned newind) {\n\tarena_t *oldarena, *newarena;\n\n\toldarena = arena_get(tsd_tsdn(tsd), oldind, false);\n\tnewarena = arena_get(tsd_tsdn(tsd), newind, false);\n\tarena_nthreads_dec(oldarena, false);\n\tarena_nthreads_inc(newarena, false);\n\ttsd_arena_set(tsd, newarena);\n}\n\nstatic void\narena_unbind(tsd_t *tsd, unsigned ind, bool internal) {\n\tarena_t *arena;\n\n\tarena = arena_get(tsd_tsdn(tsd), ind, false);\n\tarena_nthreads_dec(arena, internal);\n\n\tif (internal) {\n\t\ttsd_iarena_set(tsd, NULL);\n\t} else {\n\t\ttsd_arena_set(tsd, NULL);\n\t}\n}\n\narena_tdata_t *\narena_tdata_get_hard(tsd_t *tsd, unsigned ind) {\n\tarena_tdata_t *tdata, *arenas_tdata_old;\n\tarena_tdata_t *arenas_tdata = tsd_arenas_tdata_get(tsd);\n\tunsigned narenas_tdata_old, i;\n\tunsigned narenas_tdata = tsd_narenas_tdata_get(tsd);\n\tunsigned narenas_actual = narenas_total_get();\n\n\t/*\n\t * Dissociate old tdata array (and set up for deallocation upon return)\n\t * if it's too small.\n\t */\n\tif (arenas_tdata != NULL && narenas_tdata < narenas_actual) {\n\t\tarenas_tdata_old = arenas_tdata;\n\t\tnarenas_tdata_old = narenas_tdata;\n\t\tarenas_tdata = NULL;\n\t\tnarenas_tdata = 0;\n\t\ttsd_arenas_tdata_set(tsd, arenas_tdata);\n\t\ttsd_narenas_tdata_set(tsd, narenas_tdata);\n\t} else {\n\t\tarenas_tdata_old = NULL;\n\t\tnarenas_tdata_old = 0;\n\t}\n\n\t/* Allocate tdata array if it's missing. */\n\tif (arenas_tdata == NULL) {\n\t\tbool *arenas_tdata_bypassp = tsd_arenas_tdata_bypassp_get(tsd);\n\t\tnarenas_tdata = (ind < narenas_actual) ? narenas_actual : ind+1;\n\n\t\tif (tsd_nominal(tsd) && !*arenas_tdata_bypassp) {\n\t\t\t*arenas_tdata_bypassp = true;\n\t\t\tarenas_tdata = (arena_tdata_t *)a0malloc(\n\t\t\t    sizeof(arena_tdata_t) * narenas_tdata);\n\t\t\t*arenas_tdata_bypassp = false;\n\t\t}\n\t\tif (arenas_tdata == NULL) {\n\t\t\ttdata = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t\tassert(tsd_nominal(tsd) && !*arenas_tdata_bypassp);\n\t\ttsd_arenas_tdata_set(tsd, arenas_tdata);\n\t\ttsd_narenas_tdata_set(tsd, narenas_tdata);\n\t}\n\n\t/*\n\t * Copy to tdata array.  It's possible that the actual number of arenas\n\t * has increased since narenas_total_get() was called above, but that\n\t * causes no correctness issues unless two threads concurrently execute\n\t * the arenas.create mallctl, which we trust mallctl synchronization to\n\t * prevent.\n\t */\n\n\t/* Copy/initialize tickers. */\n\tfor (i = 0; i < narenas_actual; i++) {\n\t\tif (i < narenas_tdata_old) {\n\t\t\tticker_copy(&arenas_tdata[i].decay_ticker,\n\t\t\t    &arenas_tdata_old[i].decay_ticker);\n\t\t} else {\n\t\t\tticker_init(&arenas_tdata[i].decay_ticker,\n\t\t\t    DECAY_NTICKS_PER_UPDATE);\n\t\t}\n\t}\n\tif (narenas_tdata > narenas_actual) {\n\t\tmemset(&arenas_tdata[narenas_actual], 0, sizeof(arena_tdata_t)\n\t\t    * (narenas_tdata - narenas_actual));\n\t}\n\n\t/* Read the refreshed tdata array. */\n\ttdata = &arenas_tdata[ind];\nlabel_return:\n\tif (arenas_tdata_old != NULL) {\n\t\ta0dalloc(arenas_tdata_old);\n\t}\n\treturn tdata;\n}\n\n/* Slow path, called only by arena_choose(). */\narena_t *\narena_choose_hard(tsd_t *tsd, bool internal) {\n\tarena_t *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\n\tif (have_percpu_arena && PERCPU_ARENA_ENABLED(opt_percpu_arena)) {\n\t\tunsigned choose = percpu_arena_choose();\n\t\tret = arena_get(tsd_tsdn(tsd), choose, true);\n\t\tassert(ret != NULL);\n\t\tarena_bind(tsd, arena_ind_get(ret), false);\n\t\tarena_bind(tsd, arena_ind_get(ret), true);\n\n\t\treturn ret;\n\t}\n\n\tif (narenas_auto > 1) {\n\t\tunsigned i, j, choose[2], first_null;\n\t\tbool is_new_arena[2];\n\n\t\t/*\n\t\t * Determine binding for both non-internal and internal\n\t\t * allocation.\n\t\t *\n\t\t *   choose[0]: For application allocation.\n\t\t *   choose[1]: For internal metadata allocation.\n\t\t */\n\n\t\tfor (j = 0; j < 2; j++) {\n\t\t\tchoose[j] = 0;\n\t\t\tis_new_arena[j] = false;\n\t\t}\n\n\t\tfirst_null = narenas_auto;\n\t\tmalloc_mutex_lock(tsd_tsdn(tsd), &arenas_lock);\n\t\tassert(arena_get(tsd_tsdn(tsd), 0, false) != NULL);\n\t\tfor (i = 1; i < narenas_auto; i++) {\n\t\t\tif (arena_get(tsd_tsdn(tsd), i, false) != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Choose the first arena that has the lowest\n\t\t\t\t * number of threads assigned to it.\n\t\t\t\t */\n\t\t\t\tfor (j = 0; j < 2; j++) {\n\t\t\t\t\tif (arena_nthreads_get(arena_get(\n\t\t\t\t\t    tsd_tsdn(tsd), i, false), !!j) <\n\t\t\t\t\t    arena_nthreads_get(arena_get(\n\t\t\t\t\t    tsd_tsdn(tsd), choose[j], false),\n\t\t\t\t\t    !!j)) {\n\t\t\t\t\t\tchoose[j] = i;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (first_null == narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Record the index of the first uninitialized\n\t\t\t\t * arena, in case all extant arenas are in use.\n\t\t\t\t *\n\t\t\t\t * NB: It is possible for there to be\n\t\t\t\t * discontinuities in terms of initialized\n\t\t\t\t * versus uninitialized arenas, due to the\n\t\t\t\t * \"thread.arena\" mallctl.\n\t\t\t\t */\n\t\t\t\tfirst_null = i;\n\t\t\t}\n\t\t}\n\n\t\tfor (j = 0; j < 2; j++) {\n\t\t\tif (arena_nthreads_get(arena_get(tsd_tsdn(tsd),\n\t\t\t    choose[j], false), !!j) == 0 || first_null ==\n\t\t\t    narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Use an unloaded arena, or the least loaded\n\t\t\t\t * arena if all arenas are already initialized.\n\t\t\t\t */\n\t\t\t\tif (!!j == internal) {\n\t\t\t\t\tret = arena_get(tsd_tsdn(tsd),\n\t\t\t\t\t    choose[j], false);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tarena_t *arena;\n\n\t\t\t\t/* Initialize a new arena. */\n\t\t\t\tchoose[j] = first_null;\n\t\t\t\tarena = arena_init_locked(tsd_tsdn(tsd),\n\t\t\t\t    choose[j],\n\t\t\t\t    (extent_hooks_t *)&extent_hooks_default);\n\t\t\t\tif (arena == NULL) {\n\t\t\t\t\tmalloc_mutex_unlock(tsd_tsdn(tsd),\n\t\t\t\t\t    &arenas_lock);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tis_new_arena[j] = true;\n\t\t\t\tif (!!j == internal) {\n\t\t\t\t\tret = arena;\n\t\t\t\t}\n\t\t\t}\n\t\t\tarena_bind(tsd, choose[j], !!j);\n\t\t}\n\t\tmalloc_mutex_unlock(tsd_tsdn(tsd), &arenas_lock);\n\n\t\tfor (j = 0; j < 2; j++) {\n\t\t\tif (is_new_arena[j]) {\n\t\t\t\tassert(choose[j] > 0);\n\t\t\t\tarena_new_create_background_thread(\n\t\t\t\t    tsd_tsdn(tsd), choose[j]);\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\tret = arena_get(tsd_tsdn(tsd), 0, false);\n\t\tarena_bind(tsd, 0, false);\n\t\tarena_bind(tsd, 0, true);\n\t}\n\n\treturn ret;\n}\n\nvoid\niarena_cleanup(tsd_t *tsd) {\n\tarena_t *iarena;\n\n\tiarena = tsd_iarena_get(tsd);\n\tif (iarena != NULL) {\n\t\tarena_unbind(tsd, arena_ind_get(iarena), true);\n\t}\n}\n\nvoid\narena_cleanup(tsd_t *tsd) {\n\tarena_t *arena;\n\n\tarena = tsd_arena_get(tsd);\n\tif (arena != NULL) {\n\t\tarena_unbind(tsd, arena_ind_get(arena), false);\n\t}\n}\n\nvoid\narenas_tdata_cleanup(tsd_t *tsd) {\n\tarena_tdata_t *arenas_tdata;\n\n\t/* Prevent tsd->arenas_tdata from being (re)created. */\n\t*tsd_arenas_tdata_bypassp_get(tsd) = true;\n\n\tarenas_tdata = tsd_arenas_tdata_get(tsd);\n\tif (arenas_tdata != NULL) {\n\t\ttsd_arenas_tdata_set(tsd, NULL);\n\t\ta0dalloc(arenas_tdata);\n\t}\n}\n\nstatic void\nstats_print_atexit(void) {\n\tif (config_stats) {\n\t\ttsdn_t *tsdn;\n\t\tunsigned narenas, i;\n\n\t\ttsdn = tsdn_fetch();\n\n\t\t/*\n\t\t * Merge stats from extant threads.  This is racy, since\n\t\t * individual threads do not lock when recording tcache stats\n\t\t * events.  As a consequence, the final stats may be slightly\n\t\t * out of date by the time they are reported, if other threads\n\t\t * continue to allocate.\n\t\t */\n\t\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\t\tarena_t *arena = arena_get(tsdn, i, false);\n\t\t\tif (arena != NULL) {\n\t\t\t\ttcache_t *tcache;\n\n\t\t\t\tmalloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);\n\t\t\t\tql_foreach(tcache, &arena->tcache_ql, link) {\n\t\t\t\t\ttcache_stats_merge(tsdn, tcache, arena);\n\t\t\t\t}\n\t\t\t\tmalloc_mutex_unlock(tsdn,\n\t\t\t\t    &arena->tcache_ql_mtx);\n\t\t\t}\n\t\t}\n\t}\n\tje_malloc_stats_print(NULL, NULL, opt_stats_print_opts);\n}\n\n/*\n * Ensure that we don't hold any locks upon entry to or exit from allocator\n * code (in a \"broad\" sense that doesn't count a reentrant allocation as an\n * entrance or exit).\n */\nJEMALLOC_ALWAYS_INLINE void\ncheck_entry_exit_locking(tsdn_t *tsdn) {\n\tif (!config_debug) {\n\t\treturn;\n\t}\n\tif (tsdn_null(tsdn)) {\n\t\treturn;\n\t}\n\ttsd_t *tsd = tsdn_tsd(tsdn);\n\t/*\n\t * It's possible we hold locks at entry/exit if we're in a nested\n\t * allocation.\n\t */\n\tint8_t reentrancy_level = tsd_reentrancy_level_get(tsd);\n\tif (reentrancy_level != 0) {\n\t\treturn;\n\t}\n\twitness_assert_lockless(tsdn_witness_tsdp_get(tsdn));\n}\n\n/*\n * End miscellaneous support functions.\n */\n/******************************************************************************/\n/*\n * Begin initialization functions.\n */\n\nstatic char *\njemalloc_secure_getenv(const char *name) {\n#ifdef JEMALLOC_HAVE_SECURE_GETENV\n\treturn secure_getenv(name);\n#else\n#  ifdef JEMALLOC_HAVE_ISSETUGID\n\tif (issetugid() != 0) {\n\t\treturn NULL;\n\t}\n#  endif\n\treturn getenv(name);\n#endif\n}\n\nstatic unsigned\nmalloc_ncpus(void) {\n\tlong result;\n\n#ifdef _WIN32\n\tSYSTEM_INFO si;\n\tGetSystemInfo(&si);\n\tresult = si.dwNumberOfProcessors;\n#elif defined(JEMALLOC_GLIBC_MALLOC_HOOK) && defined(CPU_COUNT)\n\t/*\n\t * glibc >= 2.6 has the CPU_COUNT macro.\n\t *\n\t * glibc's sysconf() uses isspace().  glibc allocates for the first time\n\t * *before* setting up the isspace tables.  Therefore we need a\n\t * different method to get the number of CPUs.\n\t */\n\t{\n\t\tcpu_set_t set;\n\n\t\tpthread_getaffinity_np(pthread_self(), sizeof(set), &set);\n\t\tresult = CPU_COUNT(&set);\n\t}\n#else\n\tresult = sysconf(_SC_NPROCESSORS_ONLN);\n#endif\n\treturn ((result == -1) ? 1 : (unsigned)result);\n}\n\nstatic void\ninit_opt_stats_print_opts(const char *v, size_t vlen) {\n\tsize_t opts_len = strlen(opt_stats_print_opts);\n\tassert(opts_len <= stats_print_tot_num_options);\n\n\tfor (size_t i = 0; i < vlen; i++) {\n\t\tswitch (v[i]) {\n#define OPTION(o, v, d, s) case o: break;\n\t\t\tSTATS_PRINT_OPTIONS\n#undef OPTION\n\t\tdefault: continue;\n\t\t}\n\n\t\tif (strchr(opt_stats_print_opts, v[i]) != NULL) {\n\t\t\t/* Ignore repeated. */\n\t\t\tcontinue;\n\t\t}\n\n\t\topt_stats_print_opts[opts_len++] = v[i];\n\t\topt_stats_print_opts[opts_len] = '\\0';\n\t\tassert(opts_len <= stats_print_tot_num_options);\n\t}\n\tassert(opts_len == strlen(opt_stats_print_opts));\n}\n\n/* Reads the next size pair in a multi-sized option. */\nstatic bool\nmalloc_conf_multi_sizes_next(const char **slab_size_segment_cur,\n    size_t *vlen_left, size_t *slab_start, size_t *slab_end, size_t *new_size) {\n\tconst char *cur = *slab_size_segment_cur;\n\tchar *end;\n\tuintmax_t um;\n\n\tset_errno(0);\n\n\t/* First number, then '-' */\n\tum = malloc_strtoumax(cur, &end, 0);\n\tif (get_errno() != 0 || *end != '-') {\n\t\treturn true;\n\t}\n\t*slab_start = (size_t)um;\n\tcur = end + 1;\n\n\t/* Second number, then ':' */\n\tum = malloc_strtoumax(cur, &end, 0);\n\tif (get_errno() != 0 || *end != ':') {\n\t\treturn true;\n\t}\n\t*slab_end = (size_t)um;\n\tcur = end + 1;\n\n\t/* Last number */\n\tum = malloc_strtoumax(cur, &end, 0);\n\tif (get_errno() != 0) {\n\t\treturn true;\n\t}\n\t*new_size = (size_t)um;\n\n\t/* Consume the separator if there is one. */\n\tif (*end == '|') {\n\t\tend++;\n\t}\n\n\t*vlen_left -= end - *slab_size_segment_cur;\n\t*slab_size_segment_cur = end;\n\n\treturn false;\n}\n\nstatic bool\nmalloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,\n    char const **v_p, size_t *vlen_p) {\n\tbool accept;\n\tconst char *opts = *opts_p;\n\n\t*k_p = opts;\n\n\tfor (accept = false; !accept;) {\n\t\tswitch (*opts) {\n\t\tcase 'A': case 'B': case 'C': case 'D': case 'E': case 'F':\n\t\tcase 'G': case 'H': case 'I': case 'J': case 'K': case 'L':\n\t\tcase 'M': case 'N': case 'O': case 'P': case 'Q': case 'R':\n\t\tcase 'S': case 'T': case 'U': case 'V': case 'W': case 'X':\n\t\tcase 'Y': case 'Z':\n\t\tcase 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n\t\tcase 'g': case 'h': case 'i': case 'j': case 'k': case 'l':\n\t\tcase 'm': case 'n': case 'o': case 'p': case 'q': case 'r':\n\t\tcase 's': case 't': case 'u': case 'v': case 'w': case 'x':\n\t\tcase 'y': case 'z':\n\t\tcase '0': case '1': case '2': case '3': case '4': case '5':\n\t\tcase '6': case '7': case '8': case '9':\n\t\tcase '_':\n\t\t\topts++;\n\t\t\tbreak;\n\t\tcase ':':\n\t\t\topts++;\n\t\t\t*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;\n\t\t\t*v_p = opts;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\tif (opts != *opts_p) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with key\\n\");\n\t\t\t}\n\t\t\treturn true;\n\t\tdefault:\n\t\t\tmalloc_write(\"<jemalloc>: Malformed conf string\\n\");\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tfor (accept = false; !accept;) {\n\t\tswitch (*opts) {\n\t\tcase ',':\n\t\t\topts++;\n\t\t\t/*\n\t\t\t * Look ahead one character here, because the next time\n\t\t\t * this function is called, it will assume that end of\n\t\t\t * input has been cleanly reached if no input remains,\n\t\t\t * but we have optimistically already consumed the\n\t\t\t * comma if one exists.\n\t\t\t */\n\t\t\tif (*opts == '\\0') {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with comma\\n\");\n\t\t\t}\n\t\t\t*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\t*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\topts++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t*opts_p = opts;\n\treturn false;\n}\n\nstatic void\nmalloc_abort_invalid_conf(void) {\n\tassert(opt_abort_conf);\n\tmalloc_printf(\"<jemalloc>: Abort (abort_conf:true) on invalid conf \"\n\t    \"value (see above).\\n\");\n\tabort();\n}\n\nstatic void\nmalloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,\n    size_t vlen) {\n\tmalloc_printf(\"<jemalloc>: %s: %.*s:%.*s\\n\", msg, (int)klen, k,\n\t    (int)vlen, v);\n\t/* If abort_conf is set, error out after processing all options. */\n\tconst char *experimental = \"experimental_\";\n\tif (strncmp(k, experimental, strlen(experimental)) == 0) {\n\t\t/* However, tolerate experimental features. */\n\t\treturn;\n\t}\n\thad_conf_error = true;\n}\n\nstatic void\nmalloc_slow_flag_init(void) {\n\t/*\n\t * Combine the runtime options into malloc_slow for fast path.  Called\n\t * after processing all the options.\n\t */\n\tmalloc_slow_flags |= (opt_junk_alloc ? flag_opt_junk_alloc : 0)\n\t    | (opt_junk_free ? flag_opt_junk_free : 0)\n\t    | (opt_zero ? flag_opt_zero : 0)\n\t    | (opt_utrace ? flag_opt_utrace : 0)\n\t    | (opt_xmalloc ? flag_opt_xmalloc : 0);\n\n\tmalloc_slow = (malloc_slow_flags != 0);\n}\n\n/* Number of sources for initializing malloc_conf */\n#define MALLOC_CONF_NSOURCES 4\n\nstatic const char *\nobtain_malloc_conf(unsigned which_source, char buf[PATH_MAX + 1]) {\n\tif (config_debug) {\n\t\tstatic unsigned read_source = 0;\n\t\t/*\n\t\t * Each source should only be read once, to minimize # of\n\t\t * syscalls on init.\n\t\t */\n\t\tassert(read_source++ == which_source);\n\t}\n\tassert(which_source < MALLOC_CONF_NSOURCES);\n\n\tconst char *ret;\n\tswitch (which_source) {\n\tcase 0:\n\t\tret = config_malloc_conf;\n\t\tbreak;\n\tcase 1:\n\t\tif (je_malloc_conf != NULL) {\n\t\t\t/* Use options that were compiled into the program. */\n\t\t\tret = je_malloc_conf;\n\t\t} else {\n\t\t\t/* No configuration specified. */\n\t\t\tret = NULL;\n\t\t}\n\t\tbreak;\n\tcase 2: {\n\t\tssize_t linklen = 0;\n#ifndef _WIN32\n\t\tint saved_errno = errno;\n\t\tconst char *linkname =\n#  ifdef JEMALLOC_PREFIX\n\t\t    \"/etc/\"JEMALLOC_PREFIX\"malloc.conf\"\n#  else\n\t\t    \"/etc/malloc.conf\"\n#  endif\n\t\t    ;\n\n\t\t/*\n\t\t * Try to use the contents of the \"/etc/malloc.conf\" symbolic\n\t\t * link's name.\n\t\t */\n#ifndef JEMALLOC_READLINKAT\n\t\tlinklen = readlink(linkname, buf, PATH_MAX);\n#else\n\t\tlinklen = readlinkat(AT_FDCWD, linkname, buf, PATH_MAX);\n#endif\n\t\tif (linklen == -1) {\n\t\t\t/* No configuration specified. */\n\t\t\tlinklen = 0;\n\t\t\t/* Restore errno. */\n\t\t\tset_errno(saved_errno);\n\t\t}\n#endif\n\t\tbuf[linklen] = '\\0';\n\t\tret = buf;\n\t\tbreak;\n\t} case 3: {\n\t\tconst char *envname =\n#ifdef JEMALLOC_PREFIX\n\t\t    JEMALLOC_CPREFIX\"MALLOC_CONF\"\n#else\n\t\t    \"MALLOC_CONF\"\n#endif\n\t\t    ;\n\n\t\tif ((ret = jemalloc_secure_getenv(envname)) != NULL) {\n\t\t\t/*\n\t\t\t * Do nothing; opts is already initialized to the value\n\t\t\t * of the MALLOC_CONF environment variable.\n\t\t\t */\n\t\t} else {\n\t\t\t/* No configuration specified. */\n\t\t\tret = NULL;\n\t\t}\n\t\tbreak;\n\t} default:\n\t\tnot_reached();\n\t\tret = NULL;\n\t}\n\treturn ret;\n}\n\nstatic void\nmalloc_conf_init_helper(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],\n    bool initial_call, const char *opts_cache[MALLOC_CONF_NSOURCES],\n    char buf[PATH_MAX + 1]) {\n\tstatic const char *opts_explain[MALLOC_CONF_NSOURCES] = {\n\t\t\"string specified via --with-malloc-conf\",\n\t\t\"string pointed to by the global variable malloc_conf\",\n\t\t\"\\\"name\\\" of the file referenced by the symbolic link named \"\n\t\t    \"/etc/malloc.conf\",\n\t\t\"value of the environment variable MALLOC_CONF\"\n\t};\n\tunsigned i;\n\tconst char *opts, *k, *v;\n\tsize_t klen, vlen;\n\n\tfor (i = 0; i < MALLOC_CONF_NSOURCES; i++) {\n\t\t/* Get runtime configuration. */\n\t\tif (initial_call) {\n\t\t\topts_cache[i] = obtain_malloc_conf(i, buf);\n\t\t}\n\t\topts = opts_cache[i];\n\t\tif (!initial_call && opt_confirm_conf) {\n\t\t\tmalloc_printf(\n\t\t\t    \"<jemalloc>: malloc_conf #%u (%s): \\\"%s\\\"\\n\",\n\t\t\t    i + 1, opts_explain[i], opts != NULL ? opts : \"\");\n\t\t}\n\t\tif (opts == NULL) {\n\t\t\tcontinue;\n\t\t}\n\n\t\twhile (*opts != '\\0' && !malloc_conf_next(&opts, &k, &klen, &v,\n\t\t    &vlen)) {\n\n#define CONF_ERROR(msg, k, klen, v, vlen)\t\t\t\t\\\n\t\t\tif (!initial_call) {\t\t\t\t\\\n\t\t\t\tmalloc_conf_error(\t\t\t\\\n\t\t\t\t    msg, k, klen, v, vlen);\t\t\\\n\t\t\t\tcur_opt_valid = false;\t\t\t\\\n\t\t\t}\n#define CONF_CONTINUE\t{\t\t\t\t\t\t\\\n\t\t\t\tif (!initial_call && opt_confirm_conf\t\\\n\t\t\t\t    && cur_opt_valid) {\t\t\t\\\n\t\t\t\t\tmalloc_printf(\"<jemalloc>: -- \"\t\\\n\t\t\t\t\t    \"Set conf value: %.*s:%.*s\"\t\\\n\t\t\t\t\t    \"\\n\", (int)klen, k,\t\t\\\n\t\t\t\t\t    (int)vlen, v);\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define CONF_MATCH(n)\t\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == klen && strncmp(n, k, klen) == 0)\n#define CONF_MATCH_VALUE(n)\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == vlen && strncmp(n, v, vlen) == 0)\n#define CONF_HANDLE_BOOL(o, n)\t\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tif (CONF_MATCH_VALUE(\"true\")) {\t\t\\\n\t\t\t\t\to = true;\t\t\t\\\n\t\t\t\t} else if (CONF_MATCH_VALUE(\"false\")) {\t\\\n\t\t\t\t\to = false;\t\t\t\\\n\t\t\t\t} else {\t\t\t\t\\\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tCONF_CONTINUE;\t\t\t\t\\\n\t\t\t}\n      /*\n       * One of the CONF_MIN macros below expands, in one of the use points,\n       * to \"unsigned integer < 0\", which is always false, triggering the\n       * GCC -Wtype-limits warning, which we disable here and re-enable below.\n       */\n      JEMALLOC_DIAGNOSTIC_PUSH\n      JEMALLOC_DIAGNOSTIC_IGNORE_TYPE_LIMITS\n\n#define CONF_DONT_CHECK_MIN(um, min)\tfalse\n#define CONF_CHECK_MIN(um, min)\t((um) < (min))\n#define CONF_DONT_CHECK_MAX(um, max)\tfalse\n#define CONF_CHECK_MAX(um, max)\t((um) > (max))\n#define CONF_HANDLE_T_U(t, o, n, min, max, check_min, check_max, clip)\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tuintmax_t um;\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tum = malloc_strtoumax(v, &end, 0);\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (clip) {\t\t\t\\\n\t\t\t\t\tif (check_min(um, (t)(min))) {\t\\\n\t\t\t\t\t\to = (t)(min);\t\t\\\n\t\t\t\t\t} else if (\t\t\t\\\n\t\t\t\t\t    check_max(um, (t)(max))) {\t\\\n\t\t\t\t\t\to = (t)(max);\t\t\\\n\t\t\t\t\t} else {\t\t\t\\\n\t\t\t\t\t\to = (t)um;\t\t\\\n\t\t\t\t\t}\t\t\t\t\\\n\t\t\t\t} else {\t\t\t\t\\\n\t\t\t\t\tif (check_min(um, (t)(min)) ||\t\\\n\t\t\t\t\t    check_max(um, (t)(max))) {\t\\\n\t\t\t\t\t\tCONF_ERROR(\t\t\\\n\t\t\t\t\t\t    \"Out-of-range \"\t\\\n\t\t\t\t\t\t    \"conf value\",\t\\\n\t\t\t\t\t\t    k, klen, v, vlen);\t\\\n\t\t\t\t\t} else {\t\t\t\\\n\t\t\t\t\t\to = (t)um;\t\t\\\n\t\t\t\t\t}\t\t\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tCONF_CONTINUE;\t\t\t\t\\\n\t\t\t}\n#define CONF_HANDLE_UNSIGNED(o, n, min, max, check_min, check_max,\t\\\n    clip)\t\t\t\t\t\t\t\t\\\n\t\t\tCONF_HANDLE_T_U(unsigned, o, n, min, max,\t\\\n\t\t\t    check_min, check_max, clip)\n#define CONF_HANDLE_SIZE_T(o, n, min, max, check_min, check_max, clip)\t\\\n\t\t\tCONF_HANDLE_T_U(size_t, o, n, min, max,\t\t\\\n\t\t\t    check_min, check_max, clip)\n#define CONF_HANDLE_SSIZE_T(o, n, min, max)\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tlong l;\t\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tl = strtol(v, &end, 0);\t\t\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (l < (ssize_t)(min) || l >\t\\\n\t\t\t\t    (ssize_t)(max)) {\t\t\t\\\n\t\t\t\t\tCONF_ERROR(\t\t\t\\\n\t\t\t\t\t    \"Out-of-range conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else {\t\t\t\t\\\n\t\t\t\t\to = l;\t\t\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tCONF_CONTINUE;\t\t\t\t\\\n\t\t\t}\n#define CONF_HANDLE_CHAR_P(o, n, d)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tsize_t cpylen = (vlen <=\t\t\\\n\t\t\t\t    sizeof(o)-1) ? vlen :\t\t\\\n\t\t\t\t    sizeof(o)-1;\t\t\t\\\n\t\t\t\tstrncpy(o, v, cpylen);\t\t\t\\\n\t\t\t\to[cpylen] = '\\0';\t\t\t\\\n\t\t\t\tCONF_CONTINUE;\t\t\t\t\\\n\t\t\t}\n\n\t\t\tbool cur_opt_valid = true;\n\n\t\t\tCONF_HANDLE_BOOL(opt_confirm_conf, \"confirm_conf\")\n\t\t\tif (initial_call) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tCONF_HANDLE_BOOL(opt_abort, \"abort\")\n\t\t\tCONF_HANDLE_BOOL(opt_abort_conf, \"abort_conf\")\n\t\t\tif (strncmp(\"metadata_thp\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < metadata_thp_mode_limit; i++) {\n\t\t\t\t\tif (strncmp(metadata_thp_mode_names[i],\n\t\t\t\t\t    v, vlen) == 0) {\n\t\t\t\t\t\topt_metadata_thp = i;\n\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tCONF_HANDLE_BOOL(opt_retain, \"retain\")\n\t\t\tif (strncmp(\"dss\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < dss_prec_limit; i++) {\n\t\t\t\t\tif (strncmp(dss_prec_names[i], v, vlen)\n\t\t\t\t\t    == 0) {\n\t\t\t\t\t\tif (extent_dss_prec_set(i)) {\n\t\t\t\t\t\t\tCONF_ERROR(\n\t\t\t\t\t\t\t    \"Error setting dss\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_dss =\n\t\t\t\t\t\t\t    dss_prec_names[i];\n\t\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tCONF_HANDLE_UNSIGNED(opt_narenas, \"narenas\", 1,\n\t\t\t    UINT_MAX, CONF_CHECK_MIN, CONF_DONT_CHECK_MAX,\n\t\t\t    false)\n\t\t\tif (CONF_MATCH(\"bin_shards\")) {\n\t\t\t\tconst char *bin_shards_segment_cur = v;\n\t\t\t\tsize_t vlen_left = vlen;\n\t\t\t\tdo {\n\t\t\t\t\tsize_t size_start;\n\t\t\t\t\tsize_t size_end;\n\t\t\t\t\tsize_t nshards;\n\t\t\t\t\tbool err = malloc_conf_multi_sizes_next(\n\t\t\t\t\t    &bin_shards_segment_cur, &vlen_left,\n\t\t\t\t\t    &size_start, &size_end, &nshards);\n\t\t\t\t\tif (err || bin_update_shard_size(\n\t\t\t\t\t    bin_shard_sizes, size_start,\n\t\t\t\t\t    size_end, nshards)) {\n\t\t\t\t\t\tCONF_ERROR(\n\t\t\t\t\t\t    \"Invalid settings for \"\n\t\t\t\t\t\t    \"bin_shards\", k, klen, v,\n\t\t\t\t\t\t    vlen);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t} while (vlen_left > 0);\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tCONF_HANDLE_SSIZE_T(opt_dirty_decay_ms,\n\t\t\t    \"dirty_decay_ms\", -1, NSTIME_SEC_MAX * KQU(1000) <\n\t\t\t    QU(SSIZE_MAX) ? NSTIME_SEC_MAX * KQU(1000) :\n\t\t\t    SSIZE_MAX);\n\t\t\tCONF_HANDLE_SSIZE_T(opt_muzzy_decay_ms,\n\t\t\t    \"muzzy_decay_ms\", -1, NSTIME_SEC_MAX * KQU(1000) <\n\t\t\t    QU(SSIZE_MAX) ? NSTIME_SEC_MAX * KQU(1000) :\n\t\t\t    SSIZE_MAX);\n\t\t\tCONF_HANDLE_BOOL(opt_stats_print, \"stats_print\")\n\t\t\tif (CONF_MATCH(\"stats_print_opts\")) {\n\t\t\t\tinit_opt_stats_print_opts(v, vlen);\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tif (config_fill) {\n\t\t\t\tif (CONF_MATCH(\"junk\")) {\n\t\t\t\t\tif (CONF_MATCH_VALUE(\"true\")) {\n\t\t\t\t\t\topt_junk = \"true\";\n\t\t\t\t\t\topt_junk_alloc = opt_junk_free =\n\t\t\t\t\t\t    true;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"false\")) {\n\t\t\t\t\t\topt_junk = \"false\";\n\t\t\t\t\t\topt_junk_alloc = opt_junk_free =\n\t\t\t\t\t\t    false;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"alloc\")) {\n\t\t\t\t\t\topt_junk = \"alloc\";\n\t\t\t\t\t\topt_junk_alloc = true;\n\t\t\t\t\t\topt_junk_free = false;\n\t\t\t\t\t} else if (CONF_MATCH_VALUE(\"free\")) {\n\t\t\t\t\t\topt_junk = \"free\";\n\t\t\t\t\t\topt_junk_alloc = false;\n\t\t\t\t\t\topt_junk_free = true;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tCONF_ERROR(\n\t\t\t\t\t\t    \"Invalid conf value\",\n\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t\tCONF_CONTINUE;\n\t\t\t\t}\n\t\t\t\tCONF_HANDLE_BOOL(opt_zero, \"zero\")\n\t\t\t}\n\t\t\tif (config_utrace) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_utrace, \"utrace\")\n\t\t\t}\n\t\t\tif (config_xmalloc) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_xmalloc, \"xmalloc\")\n\t\t\t}\n\t\t\tCONF_HANDLE_BOOL(opt_tcache, \"tcache\")\n\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_tcache_max, \"lg_tcache_max\",\n\t\t\t    -1, (sizeof(size_t) << 3) - 1)\n\n\t\t\t/*\n\t\t\t * The runtime option of oversize_threshold remains\n\t\t\t * undocumented.  It may be tweaked in the next major\n\t\t\t * release (6.0).  The default value 8M is rather\n\t\t\t * conservative / safe.  Tuning it further down may\n\t\t\t * improve fragmentation a bit more, but may also cause\n\t\t\t * contention on the huge arena.\n\t\t\t */\n\t\t\tCONF_HANDLE_SIZE_T(opt_oversize_threshold,\n\t\t\t    \"oversize_threshold\", 0, SC_LARGE_MAXCLASS,\n\t\t\t    CONF_DONT_CHECK_MIN, CONF_CHECK_MAX, false)\n\t\t\tCONF_HANDLE_SIZE_T(opt_lg_extent_max_active_fit,\n\t\t\t    \"lg_extent_max_active_fit\", 0,\n\t\t\t    (sizeof(size_t) << 3), CONF_DONT_CHECK_MIN,\n\t\t\t    CONF_CHECK_MAX, false)\n\n\t\t\tif (strncmp(\"percpu_arena\", k, klen) == 0) {\n\t\t\t\tbool match = false;\n\t\t\t\tfor (int i = percpu_arena_mode_names_base; i <\n\t\t\t\t    percpu_arena_mode_names_limit; i++) {\n\t\t\t\t\tif (strncmp(percpu_arena_mode_names[i],\n\t\t\t\t\t    v, vlen) == 0) {\n\t\t\t\t\t\tif (!have_percpu_arena) {\n\t\t\t\t\t\t\tCONF_ERROR(\n\t\t\t\t\t\t\t    \"No getcpu support\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t}\n\t\t\t\t\t\topt_percpu_arena = i;\n\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tCONF_HANDLE_BOOL(opt_background_thread,\n\t\t\t    \"background_thread\");\n\t\t\tCONF_HANDLE_SIZE_T(opt_max_background_threads,\n\t\t\t\t\t   \"max_background_threads\", 1,\n\t\t\t\t\t   opt_max_background_threads,\n\t\t\t\t\t   CONF_CHECK_MIN, CONF_CHECK_MAX,\n\t\t\t\t\t   true);\n\t\t\tif (CONF_MATCH(\"slab_sizes\")) {\n\t\t\t\tbool err;\n\t\t\t\tconst char *slab_size_segment_cur = v;\n\t\t\t\tsize_t vlen_left = vlen;\n\t\t\t\tdo {\n\t\t\t\t\tsize_t slab_start;\n\t\t\t\t\tsize_t slab_end;\n\t\t\t\t\tsize_t pgs;\n\t\t\t\t\terr = malloc_conf_multi_sizes_next(\n\t\t\t\t\t    &slab_size_segment_cur,\n\t\t\t\t\t    &vlen_left, &slab_start, &slab_end,\n\t\t\t\t\t    &pgs);\n\t\t\t\t\tif (!err) {\n\t\t\t\t\t\tsc_data_update_slab_size(\n\t\t\t\t\t\t    sc_data, slab_start,\n\t\t\t\t\t\t    slab_end, (int)pgs);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tCONF_ERROR(\"Invalid settings \"\n\t\t\t\t\t\t    \"for slab_sizes\",\n\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t} while (!err && vlen_left > 0);\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tif (config_prof) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof, \"prof\")\n\t\t\t\tCONF_HANDLE_CHAR_P(opt_prof_prefix,\n\t\t\t\t    \"prof_prefix\", \"jeprof\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_active, \"prof_active\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_thread_active_init,\n\t\t\t\t    \"prof_thread_active_init\")\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_lg_prof_sample,\n\t\t\t\t    \"lg_prof_sample\", 0, (sizeof(uint64_t) << 3)\n\t\t\t\t    - 1, CONF_DONT_CHECK_MIN, CONF_CHECK_MAX,\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_accum, \"prof_accum\")\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_interval,\n\t\t\t\t    \"lg_prof_interval\", -1,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_gdump, \"prof_gdump\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_final, \"prof_final\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_leak, \"prof_leak\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_log, \"prof_log\")\n\t\t\t}\n\t\t\tif (config_log) {\n\t\t\t\tif (CONF_MATCH(\"log\")) {\n\t\t\t\t\tsize_t cpylen = (\n\t\t\t\t\t    vlen <= sizeof(log_var_names) ?\n\t\t\t\t\t    vlen : sizeof(log_var_names) - 1);\n\t\t\t\t\tstrncpy(log_var_names, v, cpylen);\n\t\t\t\t\tlog_var_names[cpylen] = '\\0';\n\t\t\t\t\tCONF_CONTINUE;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (CONF_MATCH(\"thp\")) {\n\t\t\t\tbool match = false;\n\t\t\t\tfor (int i = 0; i < thp_mode_names_limit; i++) {\n\t\t\t\t\tif (strncmp(thp_mode_names[i],v, vlen)\n\t\t\t\t\t    == 0) {\n\t\t\t\t\t\tif (!have_madvise_huge) {\n\t\t\t\t\t\t\tCONF_ERROR(\n\t\t\t\t\t\t\t    \"No THP support\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t}\n\t\t\t\t\t\topt_thp = i;\n\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!match) {\n\t\t\t\t\tCONF_ERROR(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tCONF_CONTINUE;\n\t\t\t}\n\t\t\tCONF_ERROR(\"Invalid conf pair\", k, klen, v, vlen);\n#undef CONF_ERROR\n#undef CONF_CONTINUE\n#undef CONF_MATCH\n#undef CONF_MATCH_VALUE\n#undef CONF_HANDLE_BOOL\n#undef CONF_DONT_CHECK_MIN\n#undef CONF_CHECK_MIN\n#undef CONF_DONT_CHECK_MAX\n#undef CONF_CHECK_MAX\n#undef CONF_HANDLE_T_U\n#undef CONF_HANDLE_UNSIGNED\n#undef CONF_HANDLE_SIZE_T\n#undef CONF_HANDLE_SSIZE_T\n#undef CONF_HANDLE_CHAR_P\n    /* Re-enable diagnostic \"-Wtype-limits\" */\n    JEMALLOC_DIAGNOSTIC_POP\n\t\t}\n\t\tif (opt_abort_conf && had_conf_error) {\n\t\t\tmalloc_abort_invalid_conf();\n\t\t}\n\t}\n\tatomic_store_b(&log_init_done, true, ATOMIC_RELEASE);\n}\n\nstatic void\nmalloc_conf_init(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS]) {\n\tconst char *opts_cache[MALLOC_CONF_NSOURCES] = {NULL, NULL, NULL, NULL};\n\tchar buf[PATH_MAX + 1];\n\n\t/* The first call only set the confirm_conf option and opts_cache */\n\tmalloc_conf_init_helper(NULL, NULL, true, opts_cache, buf);\n\tmalloc_conf_init_helper(sc_data, bin_shard_sizes, false, opts_cache,\n\t    NULL);\n}\n\n#undef MALLOC_CONF_NSOURCES\n\nstatic bool\nmalloc_init_hard_needed(void) {\n\tif (malloc_initialized() || (IS_INITIALIZER && malloc_init_state ==\n\t    malloc_init_recursible)) {\n\t\t/*\n\t\t * Another thread initialized the allocator before this one\n\t\t * acquired init_lock, or this thread is the initializing\n\t\t * thread, and it is recursively allocating.\n\t\t */\n\t\treturn false;\n\t}\n#ifdef JEMALLOC_THREADED_INIT\n\tif (malloc_initializer != NO_INITIALIZER && !IS_INITIALIZER) {\n\t\t/* Busy-wait until the initializing thread completes. */\n\t\tspin_t spinner = SPIN_INITIALIZER;\n\t\tdo {\n\t\t\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\t\t\tspin_adaptive(&spinner);\n\t\t\tmalloc_mutex_lock(TSDN_NULL, &init_lock);\n\t\t} while (!malloc_initialized());\n\t\treturn false;\n\t}\n#endif\n\treturn true;\n}\n\nstatic bool\nmalloc_init_hard_a0_locked() {\n\tmalloc_initializer = INITIALIZER;\n\n\tJEMALLOC_DIAGNOSTIC_PUSH\n\tJEMALLOC_DIAGNOSTIC_IGNORE_MISSING_STRUCT_FIELD_INITIALIZERS\n\tsc_data_t sc_data = {0};\n\tJEMALLOC_DIAGNOSTIC_POP\n\n\t/*\n\t * Ordering here is somewhat tricky; we need sc_boot() first, since that\n\t * determines what the size classes will be, and then\n\t * malloc_conf_init(), since any slab size tweaking will need to be done\n\t * before sz_boot and bin_boot, which assume that the values they read\n\t * out of sc_data_global are final.\n\t */\n\tsc_boot(&sc_data);\n\tunsigned bin_shard_sizes[SC_NBINS];\n\tbin_shard_sizes_boot(bin_shard_sizes);\n\t/*\n\t * prof_boot0 only initializes opt_prof_prefix.  We need to do it before\n\t * we parse malloc_conf options, in case malloc_conf parsing overwrites\n\t * it.\n\t */\n\tif (config_prof) {\n\t\tprof_boot0();\n\t}\n\tmalloc_conf_init(&sc_data, bin_shard_sizes);\n\tsz_boot(&sc_data);\n\tbin_boot(&sc_data, bin_shard_sizes);\n\n\tif (opt_stats_print) {\n\t\t/* Print statistics at exit. */\n\t\tif (atexit(stats_print_atexit) != 0) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in atexit()\\n\");\n\t\t\tif (opt_abort) {\n\t\t\t\tabort();\n\t\t\t}\n\t\t}\n\t}\n\tif (pages_boot()) {\n\t\treturn true;\n\t}\n\tif (base_boot(TSDN_NULL)) {\n\t\treturn true;\n\t}\n\tif (extent_boot()) {\n\t\treturn true;\n\t}\n\tif (ctl_boot()) {\n\t\treturn true;\n\t}\n\tif (config_prof) {\n\t\tprof_boot1();\n\t}\n\tarena_boot(&sc_data);\n\tif (tcache_boot(TSDN_NULL)) {\n\t\treturn true;\n\t}\n\tif (malloc_mutex_init(&arenas_lock, \"arenas\", WITNESS_RANK_ARENAS,\n\t    malloc_mutex_rank_exclusive)) {\n\t\treturn true;\n\t}\n\thook_boot();\n\t/*\n\t * Create enough scaffolding to allow recursive allocation in\n\t * malloc_ncpus().\n\t */\n\tnarenas_auto = 1;\n\tmanual_arena_base = narenas_auto + 1;\n\tmemset(arenas, 0, sizeof(arena_t *) * narenas_auto);\n\t/*\n\t * Initialize one arena here.  The rest are lazily created in\n\t * arena_choose_hard().\n\t */\n\tif (arena_init(TSDN_NULL, 0, (extent_hooks_t *)&extent_hooks_default)\n\t    == NULL) {\n\t\treturn true;\n\t}\n\ta0 = arena_get(TSDN_NULL, 0, false);\n\tmalloc_init_state = malloc_init_a0_initialized;\n\n\treturn false;\n}\n\nstatic bool\nmalloc_init_hard_a0(void) {\n\tbool ret;\n\n\tmalloc_mutex_lock(TSDN_NULL, &init_lock);\n\tret = malloc_init_hard_a0_locked();\n\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\treturn ret;\n}\n\n/* Initialize data structures which may trigger recursive allocation. */\nstatic bool\nmalloc_init_hard_recursible(void) {\n\tmalloc_init_state = malloc_init_recursible;\n\n\tncpus = malloc_ncpus();\n\n#if (defined(JEMALLOC_HAVE_PTHREAD_ATFORK) && !defined(JEMALLOC_MUTEX_INIT_CB) \\\n    && !defined(JEMALLOC_ZONE) && !defined(_WIN32) && \\\n    !defined(__native_client__))\n\t/* LinuxThreads' pthread_atfork() allocates. */\n\tif (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,\n\t    jemalloc_postfork_child) != 0) {\n\t\tmalloc_write(\"<jemalloc>: Error in pthread_atfork()\\n\");\n\t\tif (opt_abort) {\n\t\t\tabort();\n\t\t}\n\t\treturn true;\n\t}\n#endif\n\n\tif (background_thread_boot0()) {\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic unsigned\nmalloc_narenas_default(void) {\n\tassert(ncpus > 0);\n\t/*\n\t * For SMP systems, create more than one arena per CPU by\n\t * default.\n\t */\n\tif (ncpus > 1) {\n\t\treturn ncpus << 2;\n\t} else {\n\t\treturn 1;\n\t}\n}\n\nstatic percpu_arena_mode_t\npercpu_arena_as_initialized(percpu_arena_mode_t mode) {\n\tassert(!malloc_initialized());\n\tassert(mode <= percpu_arena_disabled);\n\n\tif (mode != percpu_arena_disabled) {\n\t\tmode += percpu_arena_mode_enabled_base;\n\t}\n\n\treturn mode;\n}\n\nstatic bool\nmalloc_init_narenas(void) {\n\tassert(ncpus > 0);\n\n\tif (opt_percpu_arena != percpu_arena_disabled) {\n\t\tif (!have_percpu_arena || malloc_getcpu() < 0) {\n\t\t\topt_percpu_arena = percpu_arena_disabled;\n\t\t\tmalloc_printf(\"<jemalloc>: perCPU arena getcpu() not \"\n\t\t\t    \"available. Setting narenas to %u.\\n\", opt_narenas ?\n\t\t\t    opt_narenas : malloc_narenas_default());\n\t\t\tif (opt_abort) {\n\t\t\t\tabort();\n\t\t\t}\n\t\t} else {\n\t\t\tif (ncpus >= MALLOCX_ARENA_LIMIT) {\n\t\t\t\tmalloc_printf(\"<jemalloc>: narenas w/ percpu\"\n\t\t\t\t    \"arena beyond limit (%d)\\n\", ncpus);\n\t\t\t\tif (opt_abort) {\n\t\t\t\t\tabort();\n\t\t\t\t}\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\t/* NB: opt_percpu_arena isn't fully initialized yet. */\n\t\t\tif (percpu_arena_as_initialized(opt_percpu_arena) ==\n\t\t\t    per_phycpu_arena && ncpus % 2 != 0) {\n\t\t\t\tmalloc_printf(\"<jemalloc>: invalid \"\n\t\t\t\t    \"configuration -- per physical CPU arena \"\n\t\t\t\t    \"with odd number (%u) of CPUs (no hyper \"\n\t\t\t\t    \"threading?).\\n\", ncpus);\n\t\t\t\tif (opt_abort)\n\t\t\t\t\tabort();\n\t\t\t}\n\t\t\tunsigned n = percpu_arena_ind_limit(\n\t\t\t    percpu_arena_as_initialized(opt_percpu_arena));\n\t\t\tif (opt_narenas < n) {\n\t\t\t\t/*\n\t\t\t\t * If narenas is specified with percpu_arena\n\t\t\t\t * enabled, actual narenas is set as the greater\n\t\t\t\t * of the two. percpu_arena_choose will be free\n\t\t\t\t * to use any of the arenas based on CPU\n\t\t\t\t * id. This is conservative (at a small cost)\n\t\t\t\t * but ensures correctness.\n\t\t\t\t *\n\t\t\t\t * If for some reason the ncpus determined at\n\t\t\t\t * boot is not the actual number (e.g. because\n\t\t\t\t * of affinity setting from numactl), reserving\n\t\t\t\t * narenas this way provides a workaround for\n\t\t\t\t * percpu_arena.\n\t\t\t\t */\n\t\t\t\topt_narenas = n;\n\t\t\t}\n\t\t}\n\t}\n\tif (opt_narenas == 0) {\n\t\topt_narenas = malloc_narenas_default();\n\t}\n\tassert(opt_narenas > 0);\n\n\tnarenas_auto = opt_narenas;\n\t/*\n\t * Limit the number of arenas to the indexing range of MALLOCX_ARENA().\n\t */\n\tif (narenas_auto >= MALLOCX_ARENA_LIMIT) {\n\t\tnarenas_auto = MALLOCX_ARENA_LIMIT - 1;\n\t\tmalloc_printf(\"<jemalloc>: Reducing narenas to limit (%d)\\n\",\n\t\t    narenas_auto);\n\t}\n\tnarenas_total_set(narenas_auto);\n\tif (arena_init_huge()) {\n\t\tnarenas_total_inc();\n\t}\n\tmanual_arena_base = narenas_total_get();\n\n\treturn false;\n}\n\nstatic void\nmalloc_init_percpu(void) {\n\topt_percpu_arena = percpu_arena_as_initialized(opt_percpu_arena);\n}\n\nstatic bool\nmalloc_init_hard_finish(void) {\n\tif (malloc_mutex_boot()) {\n\t\treturn true;\n\t}\n\n\tmalloc_init_state = malloc_init_initialized;\n\tmalloc_slow_flag_init();\n\n\treturn false;\n}\n\nstatic void\nmalloc_init_hard_cleanup(tsdn_t *tsdn, bool reentrancy_set) {\n\tmalloc_mutex_assert_owner(tsdn, &init_lock);\n\tmalloc_mutex_unlock(tsdn, &init_lock);\n\tif (reentrancy_set) {\n\t\tassert(!tsdn_null(tsdn));\n\t\ttsd_t *tsd = tsdn_tsd(tsdn);\n\t\tassert(tsd_reentrancy_level_get(tsd) > 0);\n\t\tpost_reentrancy(tsd);\n\t}\n}\n\nstatic bool\nmalloc_init_hard(void) {\n\ttsd_t *tsd;\n\n#if defined(_WIN32) && _WIN32_WINNT < 0x0600\n\t_init_init_lock();\n#endif\n\tmalloc_mutex_lock(TSDN_NULL, &init_lock);\n\n#define UNLOCK_RETURN(tsdn, ret, reentrancy)\t\t\\\n\tmalloc_init_hard_cleanup(tsdn, reentrancy);\t\\\n\treturn ret;\n\n\tif (!malloc_init_hard_needed()) {\n\t\tUNLOCK_RETURN(TSDN_NULL, false, false)\n\t}\n\n\tif (malloc_init_state != malloc_init_a0_initialized &&\n\t    malloc_init_hard_a0_locked()) {\n\t\tUNLOCK_RETURN(TSDN_NULL, true, false)\n\t}\n\n\tmalloc_mutex_unlock(TSDN_NULL, &init_lock);\n\t/* Recursive allocation relies on functional tsd. */\n\ttsd = malloc_tsd_boot0();\n\tif (tsd == NULL) {\n\t\treturn true;\n\t}\n\tif (malloc_init_hard_recursible()) {\n\t\treturn true;\n\t}\n\n\tmalloc_mutex_lock(tsd_tsdn(tsd), &init_lock);\n\t/* Set reentrancy level to 1 during init. */\n\tpre_reentrancy(tsd, NULL);\n\t/* Initialize narenas before prof_boot2 (for allocation). */\n\tif (malloc_init_narenas() || background_thread_boot1(tsd_tsdn(tsd))) {\n\t\tUNLOCK_RETURN(tsd_tsdn(tsd), true, true)\n\t}\n\tif (config_prof && prof_boot2(tsd)) {\n\t\tUNLOCK_RETURN(tsd_tsdn(tsd), true, true)\n\t}\n\n\tmalloc_init_percpu();\n\n\tif (malloc_init_hard_finish()) {\n\t\tUNLOCK_RETURN(tsd_tsdn(tsd), true, true)\n\t}\n\tpost_reentrancy(tsd);\n\tmalloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);\n\n\twitness_assert_lockless(witness_tsd_tsdn(\n\t    tsd_witness_tsdp_get_unsafe(tsd)));\n\tmalloc_tsd_boot1();\n\t/* Update TSD after tsd_boot1. */\n\ttsd = tsd_fetch();\n\tif (opt_background_thread) {\n\t\tassert(have_background_thread);\n\t\t/*\n\t\t * Need to finish init & unlock first before creating background\n\t\t * threads (pthread_create depends on malloc).  ctl_init (which\n\t\t * sets isthreaded) needs to be called without holding any lock.\n\t\t */\n\t\tbackground_thread_ctl_init(tsd_tsdn(tsd));\n\t\tif (background_thread_create(tsd, 0)) {\n\t\t\treturn true;\n\t\t}\n\t}\n#undef UNLOCK_RETURN\n\treturn false;\n}\n\n/*\n * End initialization functions.\n */\n/******************************************************************************/\n/*\n * Begin allocation-path internal functions and data structures.\n */\n\n/*\n * Settings determined by the documented behavior of the allocation functions.\n */\ntypedef struct static_opts_s static_opts_t;\nstruct static_opts_s {\n\t/* Whether or not allocation size may overflow. */\n\tbool may_overflow;\n\n\t/*\n\t * Whether or not allocations (with alignment) of size 0 should be\n\t * treated as size 1.\n\t */\n\tbool bump_empty_aligned_alloc;\n\t/*\n\t * Whether to assert that allocations are not of size 0 (after any\n\t * bumping).\n\t */\n\tbool assert_nonempty_alloc;\n\n\t/*\n\t * Whether or not to modify the 'result' argument to malloc in case of\n\t * error.\n\t */\n\tbool null_out_result_on_error;\n\t/* Whether to set errno when we encounter an error condition. */\n\tbool set_errno_on_error;\n\n\t/*\n\t * The minimum valid alignment for functions requesting aligned storage.\n\t */\n\tsize_t min_alignment;\n\n\t/* The error string to use if we oom. */\n\tconst char *oom_string;\n\t/* The error string to use if the passed-in alignment is invalid. */\n\tconst char *invalid_alignment_string;\n\n\t/*\n\t * False if we're configured to skip some time-consuming operations.\n\t *\n\t * This isn't really a malloc \"behavior\", but it acts as a useful\n\t * summary of several other static (or at least, static after program\n\t * initialization) options.\n\t */\n\tbool slow;\n\t/*\n\t * Return size.\n\t */\n\tbool usize;\n};\n\nJEMALLOC_ALWAYS_INLINE void\nstatic_opts_init(static_opts_t *static_opts) {\n\tstatic_opts->may_overflow = false;\n\tstatic_opts->bump_empty_aligned_alloc = false;\n\tstatic_opts->assert_nonempty_alloc = false;\n\tstatic_opts->null_out_result_on_error = false;\n\tstatic_opts->set_errno_on_error = false;\n\tstatic_opts->min_alignment = 0;\n\tstatic_opts->oom_string = \"\";\n\tstatic_opts->invalid_alignment_string = \"\";\n\tstatic_opts->slow = false;\n\tstatic_opts->usize = false;\n}\n\n/*\n * These correspond to the macros in jemalloc/jemalloc_macros.h.  Broadly, we\n * should have one constant here per magic value there.  Note however that the\n * representations need not be related.\n */\n#define TCACHE_IND_NONE ((unsigned)-1)\n#define TCACHE_IND_AUTOMATIC ((unsigned)-2)\n#define ARENA_IND_AUTOMATIC ((unsigned)-1)\n\ntypedef struct dynamic_opts_s dynamic_opts_t;\nstruct dynamic_opts_s {\n\tvoid **result;\n\tsize_t usize;\n\tsize_t num_items;\n\tsize_t item_size;\n\tsize_t alignment;\n\tbool zero;\n\tunsigned tcache_ind;\n\tunsigned arena_ind;\n};\n\nJEMALLOC_ALWAYS_INLINE void\ndynamic_opts_init(dynamic_opts_t *dynamic_opts) {\n\tdynamic_opts->result = NULL;\n\tdynamic_opts->usize = 0;\n\tdynamic_opts->num_items = 0;\n\tdynamic_opts->item_size = 0;\n\tdynamic_opts->alignment = 0;\n\tdynamic_opts->zero = false;\n\tdynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;\n\tdynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;\n}\n\n/* ind is ignored if dopts->alignment > 0. */\nJEMALLOC_ALWAYS_INLINE void *\nimalloc_no_sample(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd,\n    size_t size, size_t usize, szind_t ind) {\n\ttcache_t *tcache;\n\tarena_t *arena;\n\n\t/* Fill in the tcache. */\n\tif (dopts->tcache_ind == TCACHE_IND_AUTOMATIC) {\n\t\tif (likely(!sopts->slow)) {\n\t\t\t/* Getting tcache ptr unconditionally. */\n\t\t\ttcache = tsd_tcachep_get(tsd);\n\t\t\tassert(tcache == tcache_get(tsd));\n\t\t} else {\n\t\t\ttcache = tcache_get(tsd);\n\t\t}\n\t} else if (dopts->tcache_ind == TCACHE_IND_NONE) {\n\t\ttcache = NULL;\n\t} else {\n\t\ttcache = tcaches_get(tsd, dopts->tcache_ind);\n\t}\n\n\t/* Fill in the arena. */\n\tif (dopts->arena_ind == ARENA_IND_AUTOMATIC) {\n\t\t/*\n\t\t * In case of automatic arena management, we defer arena\n\t\t * computation until as late as we can, hoping to fill the\n\t\t * allocation out of the tcache.\n\t\t */\n\t\tarena = NULL;\n\t} else {\n\t\tarena = arena_get(tsd_tsdn(tsd), dopts->arena_ind, true);\n\t}\n\n\tif (unlikely(dopts->alignment != 0)) {\n\t\treturn ipalloct(tsd_tsdn(tsd), usize, dopts->alignment,\n\t\t    dopts->zero, tcache, arena);\n\t}\n\n\treturn iallocztm(tsd_tsdn(tsd), size, ind, dopts->zero, tcache, false,\n\t    arena, sopts->slow);\n}\n\nJEMALLOC_ALWAYS_INLINE void *\nimalloc_sample(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd,\n    size_t usize, szind_t ind) {\n\tvoid *ret;\n\n\t/*\n\t * For small allocations, sampling bumps the usize.  If so, we allocate\n\t * from the ind_large bucket.\n\t */\n\tszind_t ind_large;\n\tsize_t bumped_usize = usize;\n\n\tif (usize <= SC_SMALL_MAXCLASS) {\n\t\tassert(((dopts->alignment == 0) ?\n\t\t    sz_s2u(SC_LARGE_MINCLASS) :\n\t\t    sz_sa2u(SC_LARGE_MINCLASS, dopts->alignment))\n\t\t\t== SC_LARGE_MINCLASS);\n\t\tind_large = sz_size2index(SC_LARGE_MINCLASS);\n\t\tbumped_usize = sz_s2u(SC_LARGE_MINCLASS);\n\t\tret = imalloc_no_sample(sopts, dopts, tsd, bumped_usize,\n\t\t    bumped_usize, ind_large);\n\t\tif (unlikely(ret == NULL)) {\n\t\t\treturn NULL;\n\t\t}\n\t\tarena_prof_promote(tsd_tsdn(tsd), ret, usize);\n\t} else {\n\t\tret = imalloc_no_sample(sopts, dopts, tsd, usize, usize, ind);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Returns true if the allocation will overflow, and false otherwise.  Sets\n * *size to the product either way.\n */\nJEMALLOC_ALWAYS_INLINE bool\ncompute_size_with_overflow(bool may_overflow, dynamic_opts_t *dopts,\n    size_t *size) {\n\t/*\n\t * This function is just num_items * item_size, except that we may have\n\t * to check for overflow.\n\t */\n\n\tif (!may_overflow) {\n\t\tassert(dopts->num_items == 1);\n\t\t*size = dopts->item_size;\n\t\treturn false;\n\t}\n\n\t/* A size_t with its high-half bits all set to 1. */\n\tstatic const size_t high_bits = SIZE_T_MAX << (sizeof(size_t) * 8 / 2);\n\n\t*size = dopts->item_size * dopts->num_items;\n\n\tif (unlikely(*size == 0)) {\n\t\treturn (dopts->num_items != 0 && dopts->item_size != 0);\n\t}\n\n\t/*\n\t * We got a non-zero size, but we don't know if we overflowed to get\n\t * there.  To avoid having to do a divide, we'll be clever and note that\n\t * if both A and B can be represented in N/2 bits, then their product\n\t * can be represented in N bits (without the possibility of overflow).\n\t */\n\tif (likely((high_bits & (dopts->num_items | dopts->item_size)) == 0)) {\n\t\treturn false;\n\t}\n\tif (likely(*size / dopts->item_size == dopts->num_items)) {\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nJEMALLOC_ALWAYS_INLINE int\nimalloc_body(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd) {\n\t/* Where the actual allocated memory will live. */\n\tvoid *allocation = NULL;\n\t/* Filled in by compute_size_with_overflow below. */\n\tsize_t size = 0;\n\t/*\n\t * For unaligned allocations, we need only ind.  For aligned\n\t * allocations, or in case of stats or profiling we need usize.\n\t *\n\t * These are actually dead stores, in that their values are reset before\n\t * any branch on their value is taken.  Sometimes though, it's\n\t * convenient to pass them as arguments before this point.  To avoid\n\t * undefined behavior then, we initialize them with dummy stores.\n\t */\n\tszind_t ind = 0;\n\tsize_t usize = 0;\n\n\t/* Reentrancy is only checked on slow path. */\n\tint8_t reentrancy_level;\n\n\t/* Compute the amount of memory the user wants. */\n\tif (unlikely(compute_size_with_overflow(sopts->may_overflow, dopts,\n\t    &size))) {\n\t\tgoto label_oom;\n\t}\n\n\tif (unlikely(dopts->alignment < sopts->min_alignment\n\t    || (dopts->alignment & (dopts->alignment - 1)) != 0)) {\n\t\tgoto label_invalid_alignment;\n\t}\n\n\t/* This is the beginning of the \"core\" algorithm. */\n\n\tif (dopts->alignment == 0) {\n\t\tind = sz_size2index(size);\n\t\tif (unlikely(ind >= SC_NSIZES)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t\tif (config_stats || (config_prof && opt_prof) || sopts->usize) {\n\t\t\tusize = sz_index2size(ind);\n\t\t\tdopts->usize = usize;\n\t\t\tassert(usize > 0 && usize\n\t\t\t    <= SC_LARGE_MAXCLASS);\n\t\t}\n\t} else {\n\t\tif (sopts->bump_empty_aligned_alloc) {\n\t\t\tif (unlikely(size == 0)) {\n\t\t\t\tsize = 1;\n\t\t\t}\n\t\t}\n\t\tusize = sz_sa2u(size, dopts->alignment);\n\t\tdopts->usize = usize;\n\t\tif (unlikely(usize == 0\n\t\t    || usize > SC_LARGE_MAXCLASS)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t}\n\t/* Validate the user input. */\n\tif (sopts->assert_nonempty_alloc) {\n\t\tassert (size != 0);\n\t}\n\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\t/*\n\t * If we need to handle reentrancy, we can do it out of a\n\t * known-initialized arena (i.e. arena 0).\n\t */\n\treentrancy_level = tsd_reentrancy_level_get(tsd);\n\tif (sopts->slow && unlikely(reentrancy_level > 0)) {\n\t\t/*\n\t\t * We should never specify particular arenas or tcaches from\n\t\t * within our internal allocations.\n\t\t */\n\t\tassert(dopts->tcache_ind == TCACHE_IND_AUTOMATIC ||\n\t\t    dopts->tcache_ind == TCACHE_IND_NONE);\n\t\tassert(dopts->arena_ind == ARENA_IND_AUTOMATIC);\n\t\tdopts->tcache_ind = TCACHE_IND_NONE;\n\t\t/* We know that arena 0 has already been initialized. */\n\t\tdopts->arena_ind = 0;\n\t}\n\n\t/* If profiling is on, get our profiling context. */\n\tif (config_prof && opt_prof) {\n\t\t/*\n\t\t * Note that if we're going down this path, usize must have been\n\t\t * initialized in the previous if statement.\n\t\t */\n\t\tprof_tctx_t *tctx = prof_alloc_prep(\n\t\t    tsd, usize, prof_active_get_unlocked(), true);\n\n\t\talloc_ctx_t alloc_ctx;\n\t\tif (likely((uintptr_t)tctx == (uintptr_t)1U)) {\n\t\t\talloc_ctx.slab = (usize\n\t\t\t    <= SC_SMALL_MAXCLASS);\n\t\t\tallocation = imalloc_no_sample(\n\t\t\t    sopts, dopts, tsd, usize, usize, ind);\n\t\t} else if ((uintptr_t)tctx > (uintptr_t)1U) {\n\t\t\t/*\n\t\t\t * Note that ind might still be 0 here.  This is fine;\n\t\t\t * imalloc_sample ignores ind if dopts->alignment > 0.\n\t\t\t */\n\t\t\tallocation = imalloc_sample(\n\t\t\t    sopts, dopts, tsd, usize, ind);\n\t\t\talloc_ctx.slab = false;\n\t\t} else {\n\t\t\tallocation = NULL;\n\t\t}\n\n\t\tif (unlikely(allocation == NULL)) {\n\t\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\t\tgoto label_oom;\n\t\t}\n\t\tprof_malloc(tsd_tsdn(tsd), allocation, usize, &alloc_ctx, tctx);\n\t} else {\n\t\t/*\n\t\t * If dopts->alignment > 0, then ind is still 0, but usize was\n\t\t * computed in the previous if statement.  Down the positive\n\t\t * alignment path, imalloc_no_sample ignores ind and size\n\t\t * (relying only on usize).\n\t\t */\n\t\tallocation = imalloc_no_sample(sopts, dopts, tsd, size, usize,\n\t\t    ind);\n\t\tif (unlikely(allocation == NULL)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t}\n\n\t/*\n\t * Allocation has been done at this point.  We still have some\n\t * post-allocation work to do though.\n\t */\n\tassert(dopts->alignment == 0\n\t    || ((uintptr_t)allocation & (dopts->alignment - 1)) == ZU(0));\n\n\tif (config_stats) {\n\t\tassert(usize == isalloc(tsd_tsdn(tsd), allocation));\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t}\n\n\tif (sopts->slow) {\n\t\tUTRACE(0, size, allocation);\n\t}\n\n\t/* Success! */\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\t*dopts->result = allocation;\n\treturn 0;\n\nlabel_oom:\n\tif (unlikely(sopts->slow) && config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(sopts->oom_string);\n\t\tabort();\n\t}\n\n\tif (sopts->slow) {\n\t\tUTRACE(NULL, size, NULL);\n\t}\n\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tif (sopts->set_errno_on_error) {\n\t\tset_errno(ENOMEM);\n\t}\n\n\tif (sopts->null_out_result_on_error) {\n\t\t*dopts->result = NULL;\n\t}\n\n\treturn ENOMEM;\n\n\t/*\n\t * This label is only jumped to by one goto; we move it out of line\n\t * anyways to avoid obscuring the non-error paths, and for symmetry with\n\t * the oom case.\n\t */\nlabel_invalid_alignment:\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(sopts->invalid_alignment_string);\n\t\tabort();\n\t}\n\n\tif (sopts->set_errno_on_error) {\n\t\tset_errno(EINVAL);\n\t}\n\n\tif (sopts->slow) {\n\t\tUTRACE(NULL, size, NULL);\n\t}\n\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tif (sopts->null_out_result_on_error) {\n\t\t*dopts->result = NULL;\n\t}\n\n\treturn EINVAL;\n}\n\nJEMALLOC_ALWAYS_INLINE bool\nimalloc_init_check(static_opts_t *sopts, dynamic_opts_t *dopts) {\n\tif (unlikely(!malloc_initialized()) && unlikely(malloc_init())) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(sopts->oom_string);\n\t\t\tabort();\n\t\t}\n\t\tUTRACE(NULL, dopts->num_items * dopts->item_size, NULL);\n\t\tset_errno(ENOMEM);\n\t\t*dopts->result = NULL;\n\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/* Returns the errno-style error code of the allocation. */\nJEMALLOC_ALWAYS_INLINE int\nimalloc(static_opts_t *sopts, dynamic_opts_t *dopts) {\n\tif (tsd_get_allocates() && !imalloc_init_check(sopts, dopts)) {\n\t\treturn ENOMEM;\n\t}\n\n\t/* We always need the tsd.  Let's grab it right away. */\n\ttsd_t *tsd = tsd_fetch();\n\tassert(tsd);\n\tif (likely(tsd_fast(tsd))) {\n\t\t/* Fast and common path. */\n\t\ttsd_assert_fast(tsd);\n\t\tsopts->slow = false;\n\t\treturn imalloc_body(sopts, dopts, tsd);\n\t} else {\n\t\tif (!tsd_get_allocates() && !imalloc_init_check(sopts, dopts)) {\n\t\t\treturn ENOMEM;\n\t\t}\n\n\t\tsopts->slow = true;\n\t\treturn imalloc_body(sopts, dopts, tsd);\n\t}\n}\n\nJEMALLOC_NOINLINE\nvoid *\nmalloc_default(size_t size) {\n\tvoid *ret;\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.malloc.entry\", \"size: %zu\", size);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.null_out_result_on_error = true;\n\tsopts.set_errno_on_error = true;\n\tsopts.oom_string = \"<jemalloc>: Error in malloc(): out of memory\\n\";\n\n\tdopts.result = &ret;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\n\timalloc(&sopts, &dopts);\n\t/*\n\t * Note that this branch gets optimized away -- it immediately follows\n\t * the check on tsd_fast that sets sopts.slow.\n\t */\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {size};\n\t\thook_invoke_alloc(hook_alloc_malloc, ret, (uintptr_t)ret, args);\n\t}\n\n\tLOG(\"core.malloc.exit\", \"result: %p\", ret);\n\n\treturn ret;\n}\n\n/******************************************************************************/\n/*\n * Begin malloc(3)-compatible functions.\n */\n\n/*\n * malloc() fastpath.\n *\n * Fastpath assumes size <= SC_LOOKUP_MAXCLASS, and that we hit\n * tcache.  If either of these is false, we tail-call to the slowpath,\n * malloc_default().  Tail-calling is used to avoid any caller-saved\n * registers.\n *\n * fastpath supports ticker and profiling, both of which will also\n * tail-call to the slowpath if they fire.\n */\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)\nje_malloc(size_t size) {\n\tLOG(\"core.malloc.entry\", \"size: %zu\", size);\n\n\tif (tsd_get_allocates() && unlikely(!malloc_initialized())) {\n\t\treturn malloc_default(size);\n\t}\n\n\ttsd_t *tsd = tsd_get(false);\n\tif (unlikely(!tsd || !tsd_fast(tsd) || (size > SC_LOOKUP_MAXCLASS))) {\n\t\treturn malloc_default(size);\n\t}\n\n\ttcache_t *tcache = tsd_tcachep_get(tsd);\n\n\tif (unlikely(ticker_trytick(&tcache->gc_ticker))) {\n\t\treturn malloc_default(size);\n\t}\n\n\tszind_t ind = sz_size2index_lookup(size);\n\tsize_t usize;\n\tif (config_stats || config_prof) {\n\t\tusize = sz_index2size(ind);\n\t}\n\t/* Fast path relies on size being a bin. I.e. SC_LOOKUP_MAXCLASS < SC_SMALL_MAXCLASS */\n\tassert(ind < SC_NBINS);\n\tassert(size <= SC_SMALL_MAXCLASS);\n\n\tif (config_prof) {\n\t\tint64_t bytes_until_sample = tsd_bytes_until_sample_get(tsd);\n\t\tbytes_until_sample -= usize;\n\t\ttsd_bytes_until_sample_set(tsd, bytes_until_sample);\n\n\t\tif (unlikely(bytes_until_sample < 0)) {\n\t\t\t/*\n\t\t\t * Avoid a prof_active check on the fastpath.\n\t\t\t * If prof_active is false, set bytes_until_sample to\n\t\t\t * a large value.  If prof_active is set to true,\n\t\t\t * bytes_until_sample will be reset.\n\t\t\t */\n\t\t\tif (!prof_active) {\n\t\t\t\ttsd_bytes_until_sample_set(tsd, SSIZE_MAX);\n\t\t\t}\n\t\t\treturn malloc_default(size);\n\t\t}\n\t}\n\n\tcache_bin_t *bin = tcache_small_bin_get(tcache, ind);\n\tbool tcache_success;\n\tvoid* ret = cache_bin_alloc_easy(bin, &tcache_success);\n\n\tif (tcache_success) {\n\t\tif (config_stats) {\n\t\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t\tbin->tstats.nrequests++;\n\t\t}\n\t\tif (config_prof) {\n\t\t\ttcache->prof_accumbytes += usize;\n\t\t}\n\n\t\tLOG(\"core.malloc.exit\", \"result: %p\", ret);\n\n\t\t/* Fastpath success */\n\t\treturn ret;\n\t}\n\n\treturn malloc_default(size);\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nJEMALLOC_ATTR(nonnull(1))\nje_posix_memalign(void **memptr, size_t alignment, size_t size) {\n\tint ret;\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.posix_memalign.entry\", \"mem ptr: %p, alignment: %zu, \"\n\t    \"size: %zu\", memptr, alignment, size);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.bump_empty_aligned_alloc = true;\n\tsopts.min_alignment = sizeof(void *);\n\tsopts.oom_string =\n\t    \"<jemalloc>: Error allocating aligned memory: out of memory\\n\";\n\tsopts.invalid_alignment_string =\n\t    \"<jemalloc>: Error allocating aligned memory: invalid alignment\\n\";\n\n\tdopts.result = memptr;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\tdopts.alignment = alignment;\n\n\tret = imalloc(&sopts, &dopts);\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {(uintptr_t)memptr, (uintptr_t)alignment,\n\t\t\t(uintptr_t)size};\n\t\thook_invoke_alloc(hook_alloc_posix_memalign, *memptr,\n\t\t    (uintptr_t)ret, args);\n\t}\n\n\tLOG(\"core.posix_memalign.exit\", \"result: %d, alloc ptr: %p\", ret,\n\t    *memptr);\n\n\treturn ret;\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(2)\nje_aligned_alloc(size_t alignment, size_t size) {\n\tvoid *ret;\n\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.aligned_alloc.entry\", \"alignment: %zu, size: %zu\\n\",\n\t    alignment, size);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.bump_empty_aligned_alloc = true;\n\tsopts.null_out_result_on_error = true;\n\tsopts.set_errno_on_error = true;\n\tsopts.min_alignment = 1;\n\tsopts.oom_string =\n\t    \"<jemalloc>: Error allocating aligned memory: out of memory\\n\";\n\tsopts.invalid_alignment_string =\n\t    \"<jemalloc>: Error allocating aligned memory: invalid alignment\\n\";\n\n\tdopts.result = &ret;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\tdopts.alignment = alignment;\n\n\timalloc(&sopts, &dopts);\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {(uintptr_t)alignment, (uintptr_t)size};\n\t\thook_invoke_alloc(hook_alloc_aligned_alloc, ret,\n\t\t    (uintptr_t)ret, args);\n\t}\n\n\tLOG(\"core.aligned_alloc.exit\", \"result: %p\", ret);\n\n\treturn ret;\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE2(1, 2)\nje_calloc(size_t num, size_t size) {\n\tvoid *ret;\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.calloc.entry\", \"num: %zu, size: %zu\\n\", num, size);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.may_overflow = true;\n\tsopts.null_out_result_on_error = true;\n\tsopts.set_errno_on_error = true;\n\tsopts.oom_string = \"<jemalloc>: Error in calloc(): out of memory\\n\";\n\n\tdopts.result = &ret;\n\tdopts.num_items = num;\n\tdopts.item_size = size;\n\tdopts.zero = true;\n\n\timalloc(&sopts, &dopts);\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {(uintptr_t)num, (uintptr_t)size};\n\t\thook_invoke_alloc(hook_alloc_calloc, ret, (uintptr_t)ret, args);\n\t}\n\n\tLOG(\"core.calloc.exit\", \"result: %p\", ret);\n\n\treturn ret;\n}\n\nstatic void *\nirealloc_prof_sample(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize,\n    prof_tctx_t *tctx, hook_ralloc_args_t *hook_args) {\n\tvoid *p;\n\n\tif (tctx == NULL) {\n\t\treturn NULL;\n\t}\n\tif (usize <= SC_SMALL_MAXCLASS) {\n\t\tp = iralloc(tsd, old_ptr, old_usize,\n\t\t    SC_LARGE_MINCLASS, 0, false, hook_args);\n\t\tif (p == NULL) {\n\t\t\treturn NULL;\n\t\t}\n\t\tarena_prof_promote(tsd_tsdn(tsd), p, usize);\n\t} else {\n\t\tp = iralloc(tsd, old_ptr, old_usize, usize, 0, false,\n\t\t    hook_args);\n\t}\n\n\treturn p;\n}\n\nJEMALLOC_ALWAYS_INLINE void *\nirealloc_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize,\n   alloc_ctx_t *alloc_ctx, hook_ralloc_args_t *hook_args) {\n\tvoid *p;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(tsd_tsdn(tsd), old_ptr, alloc_ctx);\n\ttctx = prof_alloc_prep(tsd, usize, prof_active, true);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tp = irealloc_prof_sample(tsd, old_ptr, old_usize, usize, tctx,\n\t\t    hook_args);\n\t} else {\n\t\tp = iralloc(tsd, old_ptr, old_usize, usize, 0, false,\n\t\t    hook_args);\n\t}\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, true);\n\t\treturn NULL;\n\t}\n\tprof_realloc(tsd, p, usize, tctx, prof_active, true, old_ptr, old_usize,\n\t    old_tctx);\n\n\treturn p;\n}\n\nJEMALLOC_ALWAYS_INLINE void\nifree(tsd_t *tsd, void *ptr, tcache_t *tcache, bool slow_path) {\n\tif (!slow_path) {\n\t\ttsd_assert_fast(tsd);\n\t}\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\tif (tsd_reentrancy_level_get(tsd) != 0) {\n\t\tassert(slow_path);\n\t}\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\talloc_ctx_t alloc_ctx;\n\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\trtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,\n\t    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);\n\tassert(alloc_ctx.szind != SC_NSIZES);\n\n\tsize_t usize;\n\tif (config_prof && opt_prof) {\n\t\tusize = sz_index2size(alloc_ctx.szind);\n\t\tprof_free(tsd, ptr, usize, &alloc_ctx);\n\t} else if (config_stats) {\n\t\tusize = sz_index2size(alloc_ctx.szind);\n\t}\n\tif (config_stats) {\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\t}\n\n\tif (likely(!slow_path)) {\n\t\tidalloctm(tsd_tsdn(tsd), ptr, tcache, &alloc_ctx, false,\n\t\t    false);\n\t} else {\n\t\tidalloctm(tsd_tsdn(tsd), ptr, tcache, &alloc_ctx, false,\n\t\t    true);\n\t}\n}\n\nJEMALLOC_ALWAYS_INLINE void\nisfree(tsd_t *tsd, void *ptr, size_t usize, tcache_t *tcache, bool slow_path) {\n\tif (!slow_path) {\n\t\ttsd_assert_fast(tsd);\n\t}\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\tif (tsd_reentrancy_level_get(tsd) != 0) {\n\t\tassert(slow_path);\n\t}\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\talloc_ctx_t alloc_ctx, *ctx;\n\tif (!config_cache_oblivious && ((uintptr_t)ptr & PAGE_MASK) != 0) {\n\t\t/*\n\t\t * When cache_oblivious is disabled and ptr is not page aligned,\n\t\t * the allocation was not sampled -- usize can be used to\n\t\t * determine szind directly.\n\t\t */\n\t\talloc_ctx.szind = sz_size2index(usize);\n\t\talloc_ctx.slab = true;\n\t\tctx = &alloc_ctx;\n\t\tif (config_debug) {\n\t\t\talloc_ctx_t dbg_ctx;\n\t\t\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\t\t\trtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree,\n\t\t\t    rtree_ctx, (uintptr_t)ptr, true, &dbg_ctx.szind,\n\t\t\t    &dbg_ctx.slab);\n\t\t\tassert(dbg_ctx.szind == alloc_ctx.szind);\n\t\t\tassert(dbg_ctx.slab == alloc_ctx.slab);\n\t\t}\n\t} else if (config_prof && opt_prof) {\n\t\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\t\trtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,\n\t\t    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);\n\t\tassert(alloc_ctx.szind == sz_size2index(usize));\n\t\tctx = &alloc_ctx;\n\t} else {\n\t\tctx = NULL;\n\t}\n\n\tif (config_prof && opt_prof) {\n\t\tprof_free(tsd, ptr, usize, ctx);\n\t}\n\tif (config_stats) {\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\t}\n\n\tif (likely(!slow_path)) {\n\t\tisdalloct(tsd_tsdn(tsd), ptr, usize, tcache, ctx, false);\n\t} else {\n\t\tisdalloct(tsd_tsdn(tsd), ptr, usize, tcache, ctx, true);\n\t}\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ALLOC_SIZE(2)\nje_realloc(void *ptr, size_t arg_size) {\n\tvoid *ret;\n\ttsdn_t *tsdn JEMALLOC_CC_SILENCE_INIT(NULL);\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t old_usize = 0;\n\tsize_t size = arg_size;\n\n\tLOG(\"core.realloc.entry\", \"ptr: %p, size: %zu\\n\", ptr, size);\n\n\tif (unlikely(size == 0)) {\n\t\tif (ptr != NULL) {\n\t\t\t/* realloc(ptr, 0) is equivalent to free(ptr). */\n\t\t\tUTRACE(ptr, 0, 0);\n\t\t\ttcache_t *tcache;\n\t\t\ttsd_t *tsd = tsd_fetch();\n\t\t\tif (tsd_reentrancy_level_get(tsd) == 0) {\n\t\t\t\ttcache = tcache_get(tsd);\n\t\t\t} else {\n\t\t\t\ttcache = NULL;\n\t\t\t}\n\n\t\t\tuintptr_t args[3] = {(uintptr_t)ptr, size};\n\t\t\thook_invoke_dalloc(hook_dalloc_realloc, ptr, args);\n\n\t\t\tifree(tsd, ptr, tcache, true);\n\n\t\t\tLOG(\"core.realloc.exit\", \"result: %p\", NULL);\n\t\t\treturn NULL;\n\t\t}\n\t\tsize = 1;\n\t}\n\n\tif (likely(ptr != NULL)) {\n\t\tassert(malloc_initialized() || IS_INITIALIZER);\n\t\ttsd_t *tsd = tsd_fetch();\n\n\t\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\n\t\thook_ralloc_args_t hook_args = {true, {(uintptr_t)ptr,\n\t\t\t(uintptr_t)arg_size, 0, 0}};\n\n\t\talloc_ctx_t alloc_ctx;\n\t\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\t\trtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,\n\t\t    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);\n\t\tassert(alloc_ctx.szind != SC_NSIZES);\n\t\told_usize = sz_index2size(alloc_ctx.szind);\n\t\tassert(old_usize == isalloc(tsd_tsdn(tsd), ptr));\n\t\tif (config_prof && opt_prof) {\n\t\t\tusize = sz_s2u(size);\n\t\t\tif (unlikely(usize == 0\n\t\t\t    || usize > SC_LARGE_MAXCLASS)) {\n\t\t\t\tret = NULL;\n\t\t\t} else {\n\t\t\t\tret = irealloc_prof(tsd, ptr, old_usize, usize,\n\t\t\t\t    &alloc_ctx, &hook_args);\n\t\t\t}\n\t\t} else {\n\t\t\tif (config_stats) {\n\t\t\t\tusize = sz_s2u(size);\n\t\t\t}\n\t\t\tret = iralloc(tsd, ptr, old_usize, size, 0, false,\n\t\t\t    &hook_args);\n\t\t}\n\t\ttsdn = tsd_tsdn(tsd);\n\t} else {\n\t\t/* realloc(NULL, size) is equivalent to malloc(size). */\n\t\tstatic_opts_t sopts;\n\t\tdynamic_opts_t dopts;\n\n\t\tstatic_opts_init(&sopts);\n\t\tdynamic_opts_init(&dopts);\n\n\t\tsopts.null_out_result_on_error = true;\n\t\tsopts.set_errno_on_error = true;\n\t\tsopts.oom_string =\n\t\t    \"<jemalloc>: Error in realloc(): out of memory\\n\";\n\n\t\tdopts.result = &ret;\n\t\tdopts.num_items = 1;\n\t\tdopts.item_size = size;\n\n\t\timalloc(&sopts, &dopts);\n\t\tif (sopts.slow) {\n\t\t\tuintptr_t args[3] = {(uintptr_t)ptr, arg_size};\n\t\t\thook_invoke_alloc(hook_alloc_realloc, ret,\n\t\t\t    (uintptr_t)ret, args);\n\t\t}\n\n\t\treturn ret;\n\t}\n\n\tif (unlikely(ret == NULL)) {\n\t\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in realloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && likely(ret != NULL)) {\n\t\ttsd_t *tsd;\n\n\t\tassert(usize == isalloc(tsdn, ret));\n\t\ttsd = tsdn_tsd(tsdn);\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tUTRACE(ptr, size, ret);\n\tcheck_entry_exit_locking(tsdn);\n\n\tLOG(\"core.realloc.exit\", \"result: %p\", ret);\n\treturn ret;\n}\n\nJEMALLOC_NOINLINE\nvoid\nfree_default(void *ptr) {\n\tUTRACE(ptr, 0, 0);\n\tif (likely(ptr != NULL)) {\n\t\t/*\n\t\t * We avoid setting up tsd fully (e.g. tcache, arena binding)\n\t\t * based on only free() calls -- other activities trigger the\n\t\t * minimal to full transition.  This is because free() may\n\t\t * happen during thread shutdown after tls deallocation: if a\n\t\t * thread never had any malloc activities until then, a\n\t\t * fully-setup tsd won't be destructed properly.\n\t\t */\n\t\ttsd_t *tsd = tsd_fetch_min();\n\t\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\t\ttcache_t *tcache;\n\t\tif (likely(tsd_fast(tsd))) {\n\t\t\ttsd_assert_fast(tsd);\n\t\t\t/* Unconditionally get tcache ptr on fast path. */\n\t\t\ttcache = tsd_tcachep_get(tsd);\n\t\t\tifree(tsd, ptr, tcache, false);\n\t\t} else {\n\t\t\tif (likely(tsd_reentrancy_level_get(tsd) == 0)) {\n\t\t\t\ttcache = tcache_get(tsd);\n\t\t\t} else {\n\t\t\t\ttcache = NULL;\n\t\t\t}\n\t\t\tuintptr_t args_raw[3] = {(uintptr_t)ptr};\n\t\t\thook_invoke_dalloc(hook_dalloc_free, ptr, args_raw);\n\t\t\tifree(tsd, ptr, tcache, true);\n\t\t}\n\t\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\t}\n}\n\nJEMALLOC_ALWAYS_INLINE\nbool free_fastpath(void *ptr, size_t size, bool size_hint) {\n\ttsd_t *tsd = tsd_get(false);\n\tif (unlikely(!tsd || !tsd_fast(tsd))) {\n\t\treturn false;\n\t}\n\n\ttcache_t *tcache = tsd_tcachep_get(tsd);\n\n\talloc_ctx_t alloc_ctx;\n\t/*\n\t * If !config_cache_oblivious, we can check PAGE alignment to\n\t * detect sampled objects.  Otherwise addresses are\n\t * randomized, and we have to look it up in the rtree anyway.\n\t * See also isfree().\n\t */\n\tif (!size_hint || config_cache_oblivious) {\n\t\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\t\tbool res = rtree_szind_slab_read_fast(tsd_tsdn(tsd), &extents_rtree,\n\t\t\t\t\t\t      rtree_ctx, (uintptr_t)ptr,\n\t\t\t\t\t\t      &alloc_ctx.szind, &alloc_ctx.slab);\n\n\t\t/* Note: profiled objects will have alloc_ctx.slab set */\n\t\tif (!res || !alloc_ctx.slab) {\n\t\t\treturn false;\n\t\t}\n\t\tassert(alloc_ctx.szind != SC_NSIZES);\n\t} else {\n\t\t/*\n\t\t * Check for both sizes that are too large, and for sampled objects.\n\t\t * Sampled objects are always page-aligned.  The sampled object check\n\t\t * will also check for null ptr.\n\t\t */\n\t\tif (size > SC_LOOKUP_MAXCLASS || (((uintptr_t)ptr & PAGE_MASK) == 0)) {\n\t\t\treturn false;\n\t\t}\n\t\talloc_ctx.szind = sz_size2index_lookup(size);\n\t}\n\n\tif (unlikely(ticker_trytick(&tcache->gc_ticker))) {\n\t\treturn false;\n\t}\n\n\tcache_bin_t *bin = tcache_small_bin_get(tcache, alloc_ctx.szind);\n\tcache_bin_info_t *bin_info = &tcache_bin_info[alloc_ctx.szind];\n\tif (!cache_bin_dalloc_easy(bin, bin_info, ptr)) {\n\t\treturn false;\n\t}\n\n\tif (config_stats) {\n\t\tsize_t usize = sz_index2size(alloc_ctx.szind);\n\t\t*tsd_thread_deallocatedp_get(tsd) += usize;\n\t}\n\n\treturn true;\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_free(void *ptr) {\n\tLOG(\"core.free.entry\", \"ptr: %p\", ptr);\n\n\tif (!free_fastpath(ptr, 0, false)) {\n\t\tfree_default(ptr);\n\t}\n\n\tLOG(\"core.free.exit\", \"\");\n}\n\n/*\n * End malloc(3)-compatible functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard override functions.\n */\n\n#ifdef JEMALLOC_OVERRIDE_MEMALIGN\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc)\nje_memalign(size_t alignment, size_t size) {\n\tvoid *ret;\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.memalign.entry\", \"alignment: %zu, size: %zu\\n\", alignment,\n\t    size);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.min_alignment = 1;\n\tsopts.oom_string =\n\t    \"<jemalloc>: Error allocating aligned memory: out of memory\\n\";\n\tsopts.invalid_alignment_string =\n\t    \"<jemalloc>: Error allocating aligned memory: invalid alignment\\n\";\n\tsopts.null_out_result_on_error = true;\n\n\tdopts.result = &ret;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\tdopts.alignment = alignment;\n\n\timalloc(&sopts, &dopts);\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {alignment, size};\n\t\thook_invoke_alloc(hook_alloc_memalign, ret, (uintptr_t)ret,\n\t\t    args);\n\t}\n\n\tLOG(\"core.memalign.exit\", \"result: %p\", ret);\n\treturn ret;\n}\n#endif\n\n#ifdef JEMALLOC_OVERRIDE_VALLOC\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc)\nje_valloc(size_t size) {\n\tvoid *ret;\n\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.valloc.entry\", \"size: %zu\\n\", size);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.null_out_result_on_error = true;\n\tsopts.min_alignment = PAGE;\n\tsopts.oom_string =\n\t    \"<jemalloc>: Error allocating aligned memory: out of memory\\n\";\n\tsopts.invalid_alignment_string =\n\t    \"<jemalloc>: Error allocating aligned memory: invalid alignment\\n\";\n\n\tdopts.result = &ret;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\tdopts.alignment = PAGE;\n\n\timalloc(&sopts, &dopts);\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {size};\n\t\thook_invoke_alloc(hook_alloc_valloc, ret, (uintptr_t)ret, args);\n\t}\n\n\tLOG(\"core.valloc.exit\", \"result: %p\\n\", ret);\n\treturn ret;\n}\n#endif\n\n#if defined(JEMALLOC_IS_MALLOC) && defined(JEMALLOC_GLIBC_MALLOC_HOOK)\n/*\n * glibc provides the RTLD_DEEPBIND flag for dlopen which can make it possible\n * to inconsistently reference libc's malloc(3)-compatible functions\n * (https://bugzilla.mozilla.org/show_bug.cgi?id=493541).\n *\n * These definitions interpose hooks in glibc.  The functions are actually\n * passed an extra argument for the caller return address, which will be\n * ignored.\n */\nJEMALLOC_EXPORT void (*__free_hook)(void *ptr) = je_free;\nJEMALLOC_EXPORT void *(*__malloc_hook)(size_t size) = je_malloc;\nJEMALLOC_EXPORT void *(*__realloc_hook)(void *ptr, size_t size) = je_realloc;\n#  ifdef JEMALLOC_GLIBC_MEMALIGN_HOOK\nJEMALLOC_EXPORT void *(*__memalign_hook)(size_t alignment, size_t size) =\n    je_memalign;\n#  endif\n\n#  ifdef CPU_COUNT\n/*\n * To enable static linking with glibc, the libc specific malloc interface must\n * be implemented also, so none of glibc's malloc.o functions are added to the\n * link.\n */\n#    define ALIAS(je_fn)\t__attribute__((alias (#je_fn), used))\n/* To force macro expansion of je_ prefix before stringification. */\n#    define PREALIAS(je_fn)\tALIAS(je_fn)\n#    ifdef JEMALLOC_OVERRIDE___LIBC_CALLOC\nvoid *__libc_calloc(size_t n, size_t size) PREALIAS(je_calloc);\n#    endif\n#    ifdef JEMALLOC_OVERRIDE___LIBC_FREE\nvoid __libc_free(void* ptr) PREALIAS(je_free);\n#    endif\n#    ifdef JEMALLOC_OVERRIDE___LIBC_MALLOC\nvoid *__libc_malloc(size_t size) PREALIAS(je_malloc);\n#    endif\n#    ifdef JEMALLOC_OVERRIDE___LIBC_MEMALIGN\nvoid *__libc_memalign(size_t align, size_t s) PREALIAS(je_memalign);\n#    endif\n#    ifdef JEMALLOC_OVERRIDE___LIBC_REALLOC\nvoid *__libc_realloc(void* ptr, size_t size) PREALIAS(je_realloc);\n#    endif\n#    ifdef JEMALLOC_OVERRIDE___LIBC_VALLOC\nvoid *__libc_valloc(size_t size) PREALIAS(je_valloc);\n#    endif\n#    ifdef JEMALLOC_OVERRIDE___POSIX_MEMALIGN\nint __posix_memalign(void** r, size_t a, size_t s) PREALIAS(je_posix_memalign);\n#    endif\n#    undef PREALIAS\n#    undef ALIAS\n#  endif\n#endif\n\n/*\n * End non-standard override functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard functions.\n */\n\n#ifdef JEMALLOC_EXPERIMENTAL_SMALLOCX_API\n\n#define JEMALLOC_SMALLOCX_CONCAT_HELPER(x, y) x ## y\n#define JEMALLOC_SMALLOCX_CONCAT_HELPER2(x, y)  \\\n  JEMALLOC_SMALLOCX_CONCAT_HELPER(x, y)\n\ntypedef struct {\n\tvoid *ptr;\n\tsize_t size;\n} smallocx_return_t;\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nsmallocx_return_t JEMALLOC_NOTHROW\n/*\n * The attribute JEMALLOC_ATTR(malloc) cannot be used due to:\n *  - https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86488\n */\nJEMALLOC_SMALLOCX_CONCAT_HELPER2(je_smallocx_, JEMALLOC_VERSION_GID_IDENT)\n  (size_t size, int flags) {\n\t/*\n\t * Note: the attribute JEMALLOC_ALLOC_SIZE(1) cannot be\n\t * used here because it makes writing beyond the `size`\n\t * of the `ptr` undefined behavior, but the objective\n\t * of this function is to allow writing beyond `size`\n\t * up to `smallocx_return_t::size`.\n\t */\n\tsmallocx_return_t ret;\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.smallocx.entry\", \"size: %zu, flags: %d\", size, flags);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.assert_nonempty_alloc = true;\n\tsopts.null_out_result_on_error = true;\n\tsopts.oom_string = \"<jemalloc>: Error in mallocx(): out of memory\\n\";\n\tsopts.usize = true;\n\n\tdopts.result = &ret.ptr;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\tif (unlikely(flags != 0)) {\n\t\tif ((flags & MALLOCX_LG_ALIGN_MASK) != 0) {\n\t\t\tdopts.alignment = MALLOCX_ALIGN_GET_SPECIFIED(flags);\n\t\t}\n\n\t\tdopts.zero = MALLOCX_ZERO_GET(flags);\n\n\t\tif ((flags & MALLOCX_TCACHE_MASK) != 0) {\n\t\t\tif ((flags & MALLOCX_TCACHE_MASK)\n\t\t\t    == MALLOCX_TCACHE_NONE) {\n\t\t\t\tdopts.tcache_ind = TCACHE_IND_NONE;\n\t\t\t} else {\n\t\t\t\tdopts.tcache_ind = MALLOCX_TCACHE_GET(flags);\n\t\t\t}\n\t\t} else {\n\t\t\tdopts.tcache_ind = TCACHE_IND_AUTOMATIC;\n\t\t}\n\n\t\tif ((flags & MALLOCX_ARENA_MASK) != 0)\n\t\t\tdopts.arena_ind = MALLOCX_ARENA_GET(flags);\n\t}\n\n\timalloc(&sopts, &dopts);\n\tassert(dopts.usize == je_nallocx(size, flags));\n\tret.size = dopts.usize;\n\n\tLOG(\"core.smallocx.exit\", \"result: %p, size: %zu\", ret.ptr, ret.size);\n\treturn ret;\n}\n#undef JEMALLOC_SMALLOCX_CONCAT_HELPER\n#undef JEMALLOC_SMALLOCX_CONCAT_HELPER2\n#endif\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)\nje_mallocx(size_t size, int flags) {\n\tvoid *ret;\n\tstatic_opts_t sopts;\n\tdynamic_opts_t dopts;\n\n\tLOG(\"core.mallocx.entry\", \"size: %zu, flags: %d\", size, flags);\n\n\tstatic_opts_init(&sopts);\n\tdynamic_opts_init(&dopts);\n\n\tsopts.assert_nonempty_alloc = true;\n\tsopts.null_out_result_on_error = true;\n\tsopts.oom_string = \"<jemalloc>: Error in mallocx(): out of memory\\n\";\n\n\tdopts.result = &ret;\n\tdopts.num_items = 1;\n\tdopts.item_size = size;\n\tif (unlikely(flags != 0)) {\n\t\tif ((flags & MALLOCX_LG_ALIGN_MASK) != 0) {\n\t\t\tdopts.alignment = MALLOCX_ALIGN_GET_SPECIFIED(flags);\n\t\t}\n\n\t\tdopts.zero = MALLOCX_ZERO_GET(flags);\n\n\t\tif ((flags & MALLOCX_TCACHE_MASK) != 0) {\n\t\t\tif ((flags & MALLOCX_TCACHE_MASK)\n\t\t\t    == MALLOCX_TCACHE_NONE) {\n\t\t\t\tdopts.tcache_ind = TCACHE_IND_NONE;\n\t\t\t} else {\n\t\t\t\tdopts.tcache_ind = MALLOCX_TCACHE_GET(flags);\n\t\t\t}\n\t\t} else {\n\t\t\tdopts.tcache_ind = TCACHE_IND_AUTOMATIC;\n\t\t}\n\n\t\tif ((flags & MALLOCX_ARENA_MASK) != 0)\n\t\t\tdopts.arena_ind = MALLOCX_ARENA_GET(flags);\n\t}\n\n\timalloc(&sopts, &dopts);\n\tif (sopts.slow) {\n\t\tuintptr_t args[3] = {size, flags};\n\t\thook_invoke_alloc(hook_alloc_mallocx, ret, (uintptr_t)ret,\n\t\t    args);\n\t}\n\n\tLOG(\"core.mallocx.exit\", \"result: %p\", ret);\n\treturn ret;\n}\n\nstatic void *\nirallocx_prof_sample(tsdn_t *tsdn, void *old_ptr, size_t old_usize,\n    size_t usize, size_t alignment, bool zero, tcache_t *tcache, arena_t *arena,\n    prof_tctx_t *tctx, hook_ralloc_args_t *hook_args) {\n\tvoid *p;\n\n\tif (tctx == NULL) {\n\t\treturn NULL;\n\t}\n\tif (usize <= SC_SMALL_MAXCLASS) {\n\t\tp = iralloct(tsdn, old_ptr, old_usize,\n\t\t    SC_LARGE_MINCLASS, alignment, zero, tcache,\n\t\t    arena, hook_args);\n\t\tif (p == NULL) {\n\t\t\treturn NULL;\n\t\t}\n\t\tarena_prof_promote(tsdn, p, usize);\n\t} else {\n\t\tp = iralloct(tsdn, old_ptr, old_usize, usize, alignment, zero,\n\t\t    tcache, arena, hook_args);\n\t}\n\n\treturn p;\n}\n\nJEMALLOC_ALWAYS_INLINE void *\nirallocx_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t size,\n    size_t alignment, size_t *usize, bool zero, tcache_t *tcache,\n    arena_t *arena, alloc_ctx_t *alloc_ctx, hook_ralloc_args_t *hook_args) {\n\tvoid *p;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(tsd_tsdn(tsd), old_ptr, alloc_ctx);\n\ttctx = prof_alloc_prep(tsd, *usize, prof_active, false);\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tp = irallocx_prof_sample(tsd_tsdn(tsd), old_ptr, old_usize,\n\t\t    *usize, alignment, zero, tcache, arena, tctx, hook_args);\n\t} else {\n\t\tp = iralloct(tsd_tsdn(tsd), old_ptr, old_usize, size, alignment,\n\t\t    zero, tcache, arena, hook_args);\n\t}\n\tif (unlikely(p == NULL)) {\n\t\tprof_alloc_rollback(tsd, tctx, false);\n\t\treturn NULL;\n\t}\n\n\tif (p == old_ptr && alignment != 0) {\n\t\t/*\n\t\t * The allocation did not move, so it is possible that the size\n\t\t * class is smaller than would guarantee the requested\n\t\t * alignment, and that the alignment constraint was\n\t\t * serendipitously satisfied.  Additionally, old_usize may not\n\t\t * be the same as the current usize because of in-place large\n\t\t * reallocation.  Therefore, query the actual value of usize.\n\t\t */\n\t\t*usize = isalloc(tsd_tsdn(tsd), p);\n\t}\n\tprof_realloc(tsd, p, *usize, tctx, prof_active, false, old_ptr,\n\t    old_usize, old_tctx);\n\n\treturn p;\n}\n\nJEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN\nvoid JEMALLOC_NOTHROW *\nJEMALLOC_ALLOC_SIZE(2)\nje_rallocx(void *ptr, size_t size, int flags) {\n\tvoid *p;\n\ttsd_t *tsd;\n\tsize_t usize;\n\tsize_t old_usize;\n\tsize_t alignment = MALLOCX_ALIGN_GET(flags);\n\tbool zero = flags & MALLOCX_ZERO;\n\tarena_t *arena;\n\ttcache_t *tcache;\n\n\tLOG(\"core.rallocx.entry\", \"ptr: %p, size: %zu, flags: %d\", ptr,\n\t    size, flags);\n\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\ttsd = tsd_fetch();\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tif (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {\n\t\tunsigned arena_ind = MALLOCX_ARENA_GET(flags);\n\t\tarena = arena_get(tsd_tsdn(tsd), arena_ind, true);\n\t\tif (unlikely(arena == NULL)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t} else {\n\t\tarena = NULL;\n\t}\n\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {\n\t\t\ttcache = NULL;\n\t\t} else {\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t\t}\n\t} else {\n\t\ttcache = tcache_get(tsd);\n\t}\n\n\talloc_ctx_t alloc_ctx;\n\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\trtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,\n\t    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);\n\tassert(alloc_ctx.szind != SC_NSIZES);\n\told_usize = sz_index2size(alloc_ctx.szind);\n\tassert(old_usize == isalloc(tsd_tsdn(tsd), ptr));\n\n\thook_ralloc_args_t hook_args = {false, {(uintptr_t)ptr, size, flags,\n\t\t0}};\n\tif (config_prof && opt_prof) {\n\t\tusize = (alignment == 0) ?\n\t\t    sz_s2u(size) : sz_sa2u(size, alignment);\n\t\tif (unlikely(usize == 0\n\t\t    || usize > SC_LARGE_MAXCLASS)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t\tp = irallocx_prof(tsd, ptr, old_usize, size, alignment, &usize,\n\t\t    zero, tcache, arena, &alloc_ctx, &hook_args);\n\t\tif (unlikely(p == NULL)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t} else {\n\t\tp = iralloct(tsd_tsdn(tsd), ptr, old_usize, size, alignment,\n\t\t    zero, tcache, arena, &hook_args);\n\t\tif (unlikely(p == NULL)) {\n\t\t\tgoto label_oom;\n\t\t}\n\t\tif (config_stats) {\n\t\t\tusize = isalloc(tsd_tsdn(tsd), p);\n\t\t}\n\t}\n\tassert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));\n\n\tif (config_stats) {\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\n\tUTRACE(ptr, size, p);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tLOG(\"core.rallocx.exit\", \"result: %p\", p);\n\treturn p;\nlabel_oom:\n\tif (config_xmalloc && unlikely(opt_xmalloc)) {\n\t\tmalloc_write(\"<jemalloc>: Error in rallocx(): out of memory\\n\");\n\t\tabort();\n\t}\n\tUTRACE(ptr, size, 0);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tLOG(\"core.rallocx.exit\", \"result: %p\", NULL);\n\treturn NULL;\n}\n\nJEMALLOC_ALWAYS_INLINE size_t\nixallocx_helper(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero) {\n\tsize_t newsize;\n\n\tif (ixalloc(tsdn, ptr, old_usize, size, extra, alignment, zero,\n\t    &newsize)) {\n\t\treturn old_usize;\n\t}\n\n\treturn newsize;\n}\n\nstatic size_t\nixallocx_prof_sample(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero, prof_tctx_t *tctx) {\n\tsize_t usize;\n\n\tif (tctx == NULL) {\n\t\treturn old_usize;\n\t}\n\tusize = ixallocx_helper(tsdn, ptr, old_usize, size, extra, alignment,\n\t    zero);\n\n\treturn usize;\n}\n\nJEMALLOC_ALWAYS_INLINE size_t\nixallocx_prof(tsd_t *tsd, void *ptr, size_t old_usize, size_t size,\n    size_t extra, size_t alignment, bool zero, alloc_ctx_t *alloc_ctx) {\n\tsize_t usize_max, usize;\n\tbool prof_active;\n\tprof_tctx_t *old_tctx, *tctx;\n\n\tprof_active = prof_active_get_unlocked();\n\told_tctx = prof_tctx_get(tsd_tsdn(tsd), ptr, alloc_ctx);\n\t/*\n\t * usize isn't knowable before ixalloc() returns when extra is non-zero.\n\t * Therefore, compute its maximum possible value and use that in\n\t * prof_alloc_prep() to decide whether to capture a backtrace.\n\t * prof_realloc() will use the actual usize to decide whether to sample.\n\t */\n\tif (alignment == 0) {\n\t\tusize_max = sz_s2u(size+extra);\n\t\tassert(usize_max > 0\n\t\t    && usize_max <= SC_LARGE_MAXCLASS);\n\t} else {\n\t\tusize_max = sz_sa2u(size+extra, alignment);\n\t\tif (unlikely(usize_max == 0\n\t\t    || usize_max > SC_LARGE_MAXCLASS)) {\n\t\t\t/*\n\t\t\t * usize_max is out of range, and chances are that\n\t\t\t * allocation will fail, but use the maximum possible\n\t\t\t * value and carry on with prof_alloc_prep(), just in\n\t\t\t * case allocation succeeds.\n\t\t\t */\n\t\t\tusize_max = SC_LARGE_MAXCLASS;\n\t\t}\n\t}\n\ttctx = prof_alloc_prep(tsd, usize_max, prof_active, false);\n\n\tif (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {\n\t\tusize = ixallocx_prof_sample(tsd_tsdn(tsd), ptr, old_usize,\n\t\t    size, extra, alignment, zero, tctx);\n\t} else {\n\t\tusize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,\n\t\t    extra, alignment, zero);\n\t}\n\tif (usize == old_usize) {\n\t\tprof_alloc_rollback(tsd, tctx, false);\n\t\treturn usize;\n\t}\n\tprof_realloc(tsd, ptr, usize, tctx, prof_active, false, ptr, old_usize,\n\t    old_tctx);\n\n\treturn usize;\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nje_xallocx(void *ptr, size_t size, size_t extra, int flags) {\n\ttsd_t *tsd;\n\tsize_t usize, old_usize;\n\tsize_t alignment = MALLOCX_ALIGN_GET(flags);\n\tbool zero = flags & MALLOCX_ZERO;\n\n\tLOG(\"core.xallocx.entry\", \"ptr: %p, size: %zu, extra: %zu, \"\n\t    \"flags: %d\", ptr, size, extra, flags);\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(SIZE_T_MAX - size >= extra);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\ttsd = tsd_fetch();\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\talloc_ctx_t alloc_ctx;\n\trtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);\n\trtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,\n\t    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);\n\tassert(alloc_ctx.szind != SC_NSIZES);\n\told_usize = sz_index2size(alloc_ctx.szind);\n\tassert(old_usize == isalloc(tsd_tsdn(tsd), ptr));\n\t/*\n\t * The API explicitly absolves itself of protecting against (size +\n\t * extra) numerical overflow, but we may need to clamp extra to avoid\n\t * exceeding SC_LARGE_MAXCLASS.\n\t *\n\t * Ordinarily, size limit checking is handled deeper down, but here we\n\t * have to check as part of (size + extra) clamping, since we need the\n\t * clamped value in the above helper functions.\n\t */\n\tif (unlikely(size > SC_LARGE_MAXCLASS)) {\n\t\tusize = old_usize;\n\t\tgoto label_not_resized;\n\t}\n\tif (unlikely(SC_LARGE_MAXCLASS - size < extra)) {\n\t\textra = SC_LARGE_MAXCLASS - size;\n\t}\n\n\tif (config_prof && opt_prof) {\n\t\tusize = ixallocx_prof(tsd, ptr, old_usize, size, extra,\n\t\t    alignment, zero, &alloc_ctx);\n\t} else {\n\t\tusize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,\n\t\t    extra, alignment, zero);\n\t}\n\tif (unlikely(usize == old_usize)) {\n\t\tgoto label_not_resized;\n\t}\n\n\tif (config_stats) {\n\t\t*tsd_thread_allocatedp_get(tsd) += usize;\n\t\t*tsd_thread_deallocatedp_get(tsd) += old_usize;\n\t}\nlabel_not_resized:\n\tif (unlikely(!tsd_fast(tsd))) {\n\t\tuintptr_t args[4] = {(uintptr_t)ptr, size, extra, flags};\n\t\thook_invoke_expand(hook_expand_xallocx, ptr, old_usize,\n\t\t    usize, (uintptr_t)usize, args);\n\t}\n\n\tUTRACE(ptr, size, ptr);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tLOG(\"core.xallocx.exit\", \"result: %zu\", usize);\n\treturn usize;\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nJEMALLOC_ATTR(pure)\nje_sallocx(const void *ptr, int flags) {\n\tsize_t usize;\n\ttsdn_t *tsdn;\n\n\tLOG(\"core.sallocx.entry\", \"ptr: %p, flags: %d\", ptr, flags);\n\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\tassert(ptr != NULL);\n\n\ttsdn = tsdn_fetch();\n\tcheck_entry_exit_locking(tsdn);\n\n\tif (config_debug || force_ivsalloc) {\n\t\tusize = ivsalloc(tsdn, ptr);\n\t\tassert(force_ivsalloc || usize != 0);\n\t} else {\n\t\tusize = isalloc(tsdn, ptr);\n\t}\n\n\tcheck_entry_exit_locking(tsdn);\n\n\tLOG(\"core.sallocx.exit\", \"result: %zu\", usize);\n\treturn usize;\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_dallocx(void *ptr, int flags) {\n\tLOG(\"core.dallocx.entry\", \"ptr: %p, flags: %d\", ptr, flags);\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\ttsd_t *tsd = tsd_fetch();\n\tbool fast = tsd_fast(tsd);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\ttcache_t *tcache;\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\t/* Not allowed to be reentrant and specify a custom tcache. */\n\t\tassert(tsd_reentrancy_level_get(tsd) == 0);\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {\n\t\t\ttcache = NULL;\n\t\t} else {\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t\t}\n\t} else {\n\t\tif (likely(fast)) {\n\t\t\ttcache = tsd_tcachep_get(tsd);\n\t\t\tassert(tcache == tcache_get(tsd));\n\t\t} else {\n\t\t\tif (likely(tsd_reentrancy_level_get(tsd) == 0)) {\n\t\t\t\ttcache = tcache_get(tsd);\n\t\t\t}  else {\n\t\t\t\ttcache = NULL;\n\t\t\t}\n\t\t}\n\t}\n\n\tUTRACE(ptr, 0, 0);\n\tif (likely(fast)) {\n\t\ttsd_assert_fast(tsd);\n\t\tifree(tsd, ptr, tcache, false);\n\t} else {\n\t\tuintptr_t args_raw[3] = {(uintptr_t)ptr, flags};\n\t\thook_invoke_dalloc(hook_dalloc_dallocx, ptr, args_raw);\n\t\tifree(tsd, ptr, tcache, true);\n\t}\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tLOG(\"core.dallocx.exit\", \"\");\n}\n\nJEMALLOC_ALWAYS_INLINE size_t\ninallocx(tsdn_t *tsdn, size_t size, int flags) {\n\tcheck_entry_exit_locking(tsdn);\n\n\tsize_t usize;\n\tif (likely((flags & MALLOCX_LG_ALIGN_MASK) == 0)) {\n\t\tusize = sz_s2u(size);\n\t} else {\n\t\tusize = sz_sa2u(size, MALLOCX_ALIGN_GET_SPECIFIED(flags));\n\t}\n\tcheck_entry_exit_locking(tsdn);\n\treturn usize;\n}\n\nJEMALLOC_NOINLINE void\nsdallocx_default(void *ptr, size_t size, int flags) {\n\tassert(ptr != NULL);\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\ttsd_t *tsd = tsd_fetch();\n\tbool fast = tsd_fast(tsd);\n\tsize_t usize = inallocx(tsd_tsdn(tsd), size, flags);\n\tassert(usize == isalloc(tsd_tsdn(tsd), ptr));\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\ttcache_t *tcache;\n\tif (unlikely((flags & MALLOCX_TCACHE_MASK) != 0)) {\n\t\t/* Not allowed to be reentrant and specify a custom tcache. */\n\t\tassert(tsd_reentrancy_level_get(tsd) == 0);\n\t\tif ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {\n\t\t\ttcache = NULL;\n\t\t} else {\n\t\t\ttcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));\n\t\t}\n\t} else {\n\t\tif (likely(fast)) {\n\t\t\ttcache = tsd_tcachep_get(tsd);\n\t\t\tassert(tcache == tcache_get(tsd));\n\t\t} else {\n\t\t\tif (likely(tsd_reentrancy_level_get(tsd) == 0)) {\n\t\t\t\ttcache = tcache_get(tsd);\n\t\t\t} else {\n\t\t\t\ttcache = NULL;\n\t\t\t}\n\t\t}\n\t}\n\n\tUTRACE(ptr, 0, 0);\n\tif (likely(fast)) {\n\t\ttsd_assert_fast(tsd);\n\t\tisfree(tsd, ptr, usize, tcache, false);\n\t} else {\n\t\tuintptr_t args_raw[3] = {(uintptr_t)ptr, size, flags};\n\t\thook_invoke_dalloc(hook_dalloc_sdallocx, ptr, args_raw);\n\t\tisfree(tsd, ptr, usize, tcache, true);\n\t}\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_sdallocx(void *ptr, size_t size, int flags) {\n\tLOG(\"core.sdallocx.entry\", \"ptr: %p, size: %zu, flags: %d\", ptr,\n\t\tsize, flags);\n\n\tif (flags !=0 || !free_fastpath(ptr, size, true)) {\n\t\tsdallocx_default(ptr, size, flags);\n\t}\n\n\tLOG(\"core.sdallocx.exit\", \"\");\n}\n\nvoid JEMALLOC_NOTHROW\nje_sdallocx_noflags(void *ptr, size_t size) {\n\tLOG(\"core.sdallocx.entry\", \"ptr: %p, size: %zu, flags: 0\", ptr,\n\t\tsize);\n\n\tif (!free_fastpath(ptr, size, true)) {\n\t\tsdallocx_default(ptr, size, 0);\n\t}\n\n\tLOG(\"core.sdallocx.exit\", \"\");\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nJEMALLOC_ATTR(pure)\nje_nallocx(size_t size, int flags) {\n\tsize_t usize;\n\ttsdn_t *tsdn;\n\n\tassert(size != 0);\n\n\tif (unlikely(malloc_init())) {\n\t\tLOG(\"core.nallocx.exit\", \"result: %zu\", ZU(0));\n\t\treturn 0;\n\t}\n\n\ttsdn = tsdn_fetch();\n\tcheck_entry_exit_locking(tsdn);\n\n\tusize = inallocx(tsdn, size, flags);\n\tif (unlikely(usize > SC_LARGE_MAXCLASS)) {\n\t\tLOG(\"core.nallocx.exit\", \"result: %zu\", ZU(0));\n\t\treturn 0;\n\t}\n\n\tcheck_entry_exit_locking(tsdn);\n\tLOG(\"core.nallocx.exit\", \"result: %zu\", usize);\n\treturn usize;\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,\n    size_t newlen) {\n\tint ret;\n\ttsd_t *tsd;\n\n\tLOG(\"core.mallctl.entry\", \"name: %s\", name);\n\n\tif (unlikely(malloc_init())) {\n\t\tLOG(\"core.mallctl.exit\", \"result: %d\", EAGAIN);\n\t\treturn EAGAIN;\n\t}\n\n\ttsd = tsd_fetch();\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\tret = ctl_byname(tsd, name, oldp, oldlenp, newp, newlen);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tLOG(\"core.mallctl.exit\", \"result: %d\", ret);\n\treturn ret;\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp) {\n\tint ret;\n\n\tLOG(\"core.mallctlnametomib.entry\", \"name: %s\", name);\n\n\tif (unlikely(malloc_init())) {\n\t\tLOG(\"core.mallctlnametomib.exit\", \"result: %d\", EAGAIN);\n\t\treturn EAGAIN;\n\t}\n\n\ttsd_t *tsd = tsd_fetch();\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\tret = ctl_nametomib(tsd, name, mibp, miblenp);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\n\tLOG(\"core.mallctlnametomib.exit\", \"result: %d\", ret);\n\treturn ret;\n}\n\nJEMALLOC_EXPORT int JEMALLOC_NOTHROW\nje_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,\n  void *newp, size_t newlen) {\n\tint ret;\n\ttsd_t *tsd;\n\n\tLOG(\"core.mallctlbymib.entry\", \"\");\n\n\tif (unlikely(malloc_init())) {\n\t\tLOG(\"core.mallctlbymib.exit\", \"result: %d\", EAGAIN);\n\t\treturn EAGAIN;\n\t}\n\n\ttsd = tsd_fetch();\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\tret = ctl_bymib(tsd, mib, miblen, oldp, oldlenp, newp, newlen);\n\tcheck_entry_exit_locking(tsd_tsdn(tsd));\n\tLOG(\"core.mallctlbymib.exit\", \"result: %d\", ret);\n\treturn ret;\n}\n\nJEMALLOC_EXPORT void JEMALLOC_NOTHROW\nje_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,\n    const char *opts) {\n\ttsdn_t *tsdn;\n\n\tLOG(\"core.malloc_stats_print.entry\", \"\");\n\n\ttsdn = tsdn_fetch();\n\tcheck_entry_exit_locking(tsdn);\n\tstats_print(write_cb, cbopaque, opts);\n\tcheck_entry_exit_locking(tsdn);\n\tLOG(\"core.malloc_stats_print.exit\", \"\");\n}\n\nJEMALLOC_EXPORT size_t JEMALLOC_NOTHROW\nje_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr) {\n\tsize_t ret;\n\ttsdn_t *tsdn;\n\n\tLOG(\"core.malloc_usable_size.entry\", \"ptr: %p\", ptr);\n\n\tassert(malloc_initialized() || IS_INITIALIZER);\n\n\ttsdn = tsdn_fetch();\n\tcheck_entry_exit_locking(tsdn);\n\n\tif (unlikely(ptr == NULL)) {\n\t\tret = 0;\n\t} else {\n\t\tif (config_debug || force_ivsalloc) {\n\t\t\tret = ivsalloc(tsdn, ptr);\n\t\t\tassert(force_ivsalloc || ret != 0);\n\t\t} else {\n\t\t\tret = isalloc(tsdn, ptr);\n\t\t}\n\t}\n\n\tcheck_entry_exit_locking(tsdn);\n\tLOG(\"core.malloc_usable_size.exit\", \"result: %zu\", ret);\n\treturn ret;\n}\n\n/*\n * End non-standard functions.\n */\n/******************************************************************************/\n/*\n * The following functions are used by threading libraries for protection of\n * malloc during fork().\n */\n\n/*\n * If an application creates a thread before doing any allocation in the main\n * thread, then calls fork(2) in the main thread followed by memory allocation\n * in the child process, a race can occur that results in deadlock within the\n * child: the main thread may have forked while the created thread had\n * partially initialized the allocator.  Ordinarily jemalloc prevents\n * fork/malloc races via the following functions it registers during\n * initialization using pthread_atfork(), but of course that does no good if\n * the allocator isn't fully initialized at fork time.  The following library\n * constructor is a partial solution to this problem.  It may still be possible\n * to trigger the deadlock described above, but doing so would involve forking\n * via a library constructor that runs before jemalloc's runs.\n */\n#ifndef JEMALLOC_JET\nJEMALLOC_ATTR(constructor)\nstatic void\njemalloc_constructor(void) {\n\tmalloc_init();\n}\n#endif\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_prefork(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_prefork(void)\n#endif\n{\n\ttsd_t *tsd;\n\tunsigned i, j, narenas;\n\tarena_t *arena;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (!malloc_initialized()) {\n\t\treturn;\n\t}\n#endif\n\tassert(malloc_initialized());\n\n\ttsd = tsd_fetch();\n\n\tnarenas = narenas_total_get();\n\n\twitness_prefork(tsd_witness_tsdp_get(tsd));\n\t/* Acquire all mutexes in a safe order. */\n\tctl_prefork(tsd_tsdn(tsd));\n\ttcache_prefork(tsd_tsdn(tsd));\n\tmalloc_mutex_prefork(tsd_tsdn(tsd), &arenas_lock);\n\tif (have_background_thread) {\n\t\tbackground_thread_prefork0(tsd_tsdn(tsd));\n\t}\n\tprof_prefork0(tsd_tsdn(tsd));\n\tif (have_background_thread) {\n\t\tbackground_thread_prefork1(tsd_tsdn(tsd));\n\t}\n\t/* Break arena prefork into stages to preserve lock order. */\n\tfor (i = 0; i < 8; i++) {\n\t\tfor (j = 0; j < narenas; j++) {\n\t\t\tif ((arena = arena_get(tsd_tsdn(tsd), j, false)) !=\n\t\t\t    NULL) {\n\t\t\t\tswitch (i) {\n\t\t\t\tcase 0:\n\t\t\t\t\tarena_prefork0(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 1:\n\t\t\t\t\tarena_prefork1(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 2:\n\t\t\t\t\tarena_prefork2(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 3:\n\t\t\t\t\tarena_prefork3(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 4:\n\t\t\t\t\tarena_prefork4(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 5:\n\t\t\t\t\tarena_prefork5(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 6:\n\t\t\t\t\tarena_prefork6(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 7:\n\t\t\t\t\tarena_prefork7(tsd_tsdn(tsd), arena);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault: not_reached();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tprof_prefork1(tsd_tsdn(tsd));\n\ttsd_prefork(tsd);\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_postfork_parent(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_postfork(void)\n#endif\n{\n\ttsd_t *tsd;\n\tunsigned i, narenas;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (!malloc_initialized()) {\n\t\treturn;\n\t}\n#endif\n\tassert(malloc_initialized());\n\n\ttsd = tsd_fetch();\n\n\ttsd_postfork_parent(tsd);\n\n\twitness_postfork_parent(tsd_witness_tsdp_get(tsd));\n\t/* Release all mutexes, now that fork() has completed. */\n\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\tarena_t *arena;\n\n\t\tif ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL) {\n\t\t\tarena_postfork_parent(tsd_tsdn(tsd), arena);\n\t\t}\n\t}\n\tprof_postfork_parent(tsd_tsdn(tsd));\n\tif (have_background_thread) {\n\t\tbackground_thread_postfork_parent(tsd_tsdn(tsd));\n\t}\n\tmalloc_mutex_postfork_parent(tsd_tsdn(tsd), &arenas_lock);\n\ttcache_postfork_parent(tsd_tsdn(tsd));\n\tctl_postfork_parent(tsd_tsdn(tsd));\n}\n\nvoid\njemalloc_postfork_child(void) {\n\ttsd_t *tsd;\n\tunsigned i, narenas;\n\n\tassert(malloc_initialized());\n\n\ttsd = tsd_fetch();\n\n\ttsd_postfork_child(tsd);\n\n\twitness_postfork_child(tsd_witness_tsdp_get(tsd));\n\t/* Release all mutexes, now that fork() has completed. */\n\tfor (i = 0, narenas = narenas_total_get(); i < narenas; i++) {\n\t\tarena_t *arena;\n\n\t\tif ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL) {\n\t\t\tarena_postfork_child(tsd_tsdn(tsd), arena);\n\t\t}\n\t}\n\tprof_postfork_child(tsd_tsdn(tsd));\n\tif (have_background_thread) {\n\t\tbackground_thread_postfork_child(tsd_tsdn(tsd));\n\t}\n\tmalloc_mutex_postfork_child(tsd_tsdn(tsd), &arenas_lock);\n\ttcache_postfork_child(tsd_tsdn(tsd));\n\tctl_postfork_child(tsd_tsdn(tsd));\n}\n\n/******************************************************************************/\n"
    },
    "skipped": [],
    "total_files": 548
}