{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/src/libxsmm_malloc.c": "/******************************************************************************\n* Copyright (c) Intel Corporation - All rights reserved.                      *\n* This file is part of the LIBXSMM library.                                   *\n*                                                                             *\n* For information on the license, see the LICENSE file.                       *\n* Further information: https://github.com/hfp/libxsmm/                        *\n* SPDX-License-Identifier: BSD-3-Clause                                       *\n******************************************************************************/\n/* Hans Pabst (Intel Corp.)\n******************************************************************************/\n#include \"libxsmm_trace.h\"\n#include \"libxsmm_main.h\"\n#include \"libxsmm_hash.h\"\n\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(push,target(LIBXSMM_OFFLOAD_TARGET))\n#endif\n#if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n# include <features.h>\n# include <malloc.h>\n#endif\n#if !defined(LIBXSMM_MALLOC_GLIBC)\n# if defined(__GLIBC__)\n#   define LIBXSMM_MALLOC_GLIBC __GLIBC__\n# else\n#   define LIBXSMM_MALLOC_GLIBC 6\n# endif\n#endif\n#if defined(_WIN32)\n# include <windows.h>\n# include <malloc.h>\n# include <intrin.h>\n#else\n# include <sys/mman.h>\n# if defined(__linux__)\n#   include <linux/mman.h>\n#   include <sys/syscall.h>\n# endif\n# if defined(MAP_POPULATE)\n#   include <sys/utsname.h>\n# endif\n# include <sys/types.h>\n# include <unistd.h>\n# include <errno.h>\n# if defined(__MAP_ANONYMOUS)\n#   define LIBXSMM_MAP_ANONYMOUS __MAP_ANONYMOUS\n# elif defined(MAP_ANONYMOUS)\n#   define LIBXSMM_MAP_ANONYMOUS MAP_ANONYMOUS\n# elif defined(MAP_ANON)\n#   define LIBXSMM_MAP_ANONYMOUS MAP_ANON\n# else\n#  define LIBXSMM_MAP_ANONYMOUS 0x20\n# endif\n# if defined(MAP_SHARED) && 0\n#   define LIBXSMM_MAP_SHARED MAP_SHARED\n# else\n#   define LIBXSMM_MAP_SHARED 0\n# endif\nLIBXSMM_EXTERN int ftruncate(int, off_t) LIBXSMM_THROW;\nLIBXSMM_EXTERN int mkstemp(char*) LIBXSMM_NOTHROW;\n#endif\n#if !defined(LIBXSMM_MALLOC_FALLBACK)\n# define LIBXSMM_MALLOC_FINAL 3\n#endif\n#if defined(LIBXSMM_VTUNE)\n# if (2 <= LIBXSMM_VTUNE) /* no header file required */\n#   if !defined(LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JITVERSION LIBXSMM_VTUNE\n#   endif\n#   define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load_V2\n#   define LIBXSMM_VTUNE_JIT_LOAD 21\n#   define LIBXSMM_VTUNE_JIT_UNLOAD 14\n#   define iJIT_SAMPLING_ON 0x0001\nLIBXSMM_EXTERN unsigned int iJIT_GetNewMethodID(void);\nLIBXSMM_EXTERN /*iJIT_IsProfilingActiveFlags*/int iJIT_IsProfilingActive(void);\nLIBXSMM_EXTERN int iJIT_NotifyEvent(/*iJIT_JVM_EVENT*/int event_type, void *EventSpecificData);\nLIBXSMM_EXTERN_C typedef struct LineNumberInfo {\n  unsigned int Offset;\n  unsigned int LineNumber;\n} LineNumberInfo;\nLIBXSMM_EXTERN_C typedef struct iJIT_Method_Load_V2 {\n  unsigned int method_id;\n  char* method_name;\n  void* method_load_address;\n  unsigned int method_size;\n  unsigned int line_number_size;\n  LineNumberInfo* line_number_table;\n  char* class_file_name;\n  char* source_file_name;\n  char* module_name;\n} iJIT_Method_Load_V2;\n# else /* more safe due to header dependency */\n#   include <jitprofiling.h>\n#   if !defined(LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JITVERSION 2\n#   endif\n#   if (2 <= LIBXSMM_VTUNE_JITVERSION)\n#     define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load_V2\n#     define LIBXSMM_VTUNE_JIT_LOAD iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED_V2\n#   else\n#     define LIBXSMM_VTUNE_JIT_DESC_TYPE iJIT_Method_Load\n#     define LIBXSMM_VTUNE_JIT_LOAD iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED\n#   endif\n#   define LIBXSMM_VTUNE_JIT_UNLOAD iJVM_EVENT_TYPE_METHOD_UNLOAD_START\n# endif\n# if !defined(LIBXSMM_MALLOC_FALLBACK)\n#   define LIBXSMM_MALLOC_FALLBACK LIBXSMM_MALLOC_FINAL\n# endif\n#else\n# if !defined(LIBXSMM_MALLOC_FALLBACK)\n#   define LIBXSMM_MALLOC_FALLBACK 0\n# endif\n#endif /*defined(LIBXSMM_VTUNE)*/\n#if !defined(LIBXSMM_MALLOC_XMAP_TEMPLATE)\n# define LIBXSMM_MALLOC_XMAP_TEMPLATE \".libxsmm_jit.\" LIBXSMM_MKTEMP_PATTERN\n#endif\n#if defined(LIBXSMM_OFFLOAD_TARGET)\n# pragma offload_attribute(pop)\n#endif\n#if defined(LIBXSMM_PERF)\n# include \"libxsmm_perf.h\"\n#endif\n\n#if !defined(LIBXSMM_MALLOC_ALIGNMAX)\n# define LIBXSMM_MALLOC_ALIGNMAX (2 << 20) /* 2 MB */\n#endif\n#if !defined(LIBXSMM_MALLOC_ALIGNFCT)\n# define LIBXSMM_MALLOC_ALIGNFCT 16\n#endif\n#if !defined(LIBXSMM_MALLOC_SEED)\n# define LIBXSMM_MALLOC_SEED 1051981\n#endif\n\n#if !defined(LIBXSMM_MALLOC_HOOK_KMP) && 0\n# define LIBXSMM_MALLOC_HOOK_KMP\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_QKMALLOC) && 0\n# define LIBXSMM_MALLOC_HOOK_QKMALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_IMALLOC) && 1\n# define LIBXSMM_MALLOC_HOOK_IMALLOC\n#endif\n#if !defined(LIBXSMM_MALLOC_HOOK_CHECK) && 0\n# define LIBXSMM_MALLOC_HOOK_CHECK 1\n#endif\n\n#if !defined(LIBXSMM_MALLOC_CRC_LIGHT) && !defined(_DEBUG) && 1\n# define LIBXSMM_MALLOC_CRC_LIGHT\n#endif\n#if !defined(LIBXSMM_MALLOC_CRC_OFF)\n# if defined(NDEBUG) && !defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n#   define LIBXSMM_MALLOC_CRC_OFF\n# elif !defined(LIBXSMM_BUILD)\n#   define LIBXSMM_MALLOC_CRC_OFF\n# endif\n#endif\n\n#if !defined(LIBXSMM_MALLOC_SCRATCH_LIMIT)\n# define LIBXSMM_MALLOC_SCRATCH_LIMIT 0xFFFFFFFF /* ~4 GB */\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_PADDING)\n# define LIBXSMM_MALLOC_SCRATCH_PADDING LIBXSMM_CACHELINE\n#endif\n/* pointers are checked first if they belong to scratch */\n#if !defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST) && 1\n# define LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST\n#endif\n/* can clobber memory if allocations are not exactly scoped */\n#if !defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD) && 0\n# define LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD\n#endif\n#if !defined(LIBXSMM_MALLOC_SCRATCH_JOIN) && 1\n# define LIBXSMM_MALLOC_SCRATCH_JOIN\n#endif\n#if !defined(LIBXSMM_MALLOC_LOCK_ONFAULT) && 0\n# if defined(MLOCK_ONFAULT) && defined(SYS_mlock2)\n#   define LIBXSMM_MALLOC_LOCK_ONFAULT\n# endif\n#endif\n/* protected against double-delete (if possible) */\n#if !defined(LIBXSMM_MALLOC_DELETE_SAFE) && 0\n# define LIBXSMM_MALLOC_DELETE_SAFE\n#endif\n/* map memory for scratch buffers */\n#if !defined(LIBXSMM_MALLOC_MMAP_SCRATCH) && 1\n# define LIBXSMM_MALLOC_MMAP_SCRATCH\n#endif\n/* map memory for hooked allocation */\n#if !defined(LIBXSMM_MALLOC_MMAP_HOOK) && 1\n# define LIBXSMM_MALLOC_MMAP_HOOK\n#endif\n/* map memory also for non-executable buffers */\n#if !defined(LIBXSMM_MALLOC_MMAP) && 1\n# define LIBXSMM_MALLOC_MMAP\n#endif\n\n#if defined(LIBXSMM_MALLOC_ALIGN_ALL)\n# define INTERNAL_MALLOC_AUTOALIGN(SIZE, ALIGNMENT) libxsmm_alignment(SIZE, ALIGNMENT)\n#else\n# define INTERNAL_MALLOC_AUTOALIGN(SIZE, ALIGNMENT) (ALIGNMENT)\n#endif\n\n#define INTERNAL_MEMALIGN_HOOK(RESULT, FLAGS, ALIGNMENT, SIZE, CALLER) { \\\n  const int internal_memalign_hook_recursive_ = LIBXSMM_ATOMIC_ADD_FETCH( \\\n    &internal_malloc_recursive, 1, LIBXSMM_ATOMIC_RELAXED); \\\n  if ( 1 < internal_memalign_hook_recursive_ /* protect against recursion */ \\\n    || 0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    || (internal_malloc_limit[0] > (SIZE)) \\\n    || (internal_malloc_limit[1] < (SIZE) && 0 != internal_malloc_limit[1])) \\\n  { \\\n    const size_t internal_memalign_hook_alignment_ = INTERNAL_MALLOC_AUTOALIGN(SIZE, ALIGNMENT); \\\n    (RESULT) = (0 != internal_memalign_hook_alignment_ \\\n      ? __real_memalign(internal_memalign_hook_alignment_, SIZE) \\\n      : __real_malloc(SIZE)); \\\n  } \\\n  else { /* redirect */ \\\n    LIBXSMM_INIT \\\n    if (NULL == (CALLER)) { /* libxsmm_trace_caller_id may allocate memory */ \\\n      internal_scratch_malloc(&(RESULT), SIZE, ALIGNMENT, FLAGS, \\\n        libxsmm_trace_caller_id(0/*level*/)); \\\n    } \\\n    else { \\\n      internal_scratch_malloc(&(RESULT), SIZE, ALIGNMENT, FLAGS, CALLER); \\\n    } \\\n  } \\\n  LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_recursive, 1, LIBXSMM_ATOMIC_RELAXED); \\\n}\n\n#define INTERNAL_REALLOC_HOOK(RESULT, FLAGS, PTR, SIZE, CALLER) { \\\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    /*|| (0 != LIBXSMM_ATOMIC_LOAD(&internal_malloc_recursive, LIBXSMM_ATOMIC_RELAXED))*/ \\\n    || (internal_malloc_limit[0] > (SIZE)) \\\n    || (internal_malloc_limit[1] < (SIZE) && 0 != internal_malloc_limit[1])) \\\n  { \\\n    (RESULT) = __real_realloc(PTR, SIZE); \\\n  } \\\n  else { \\\n    const int nzeros = LIBXSMM_INTRINSICS_BITSCANFWD64((uintptr_t)(PTR)), alignment = 1 << nzeros; \\\n    LIBXSMM_ASSERT(0 == ((uintptr_t)(PTR) & ~(0xFFFFFFFFFFFFFFFF << nzeros))); \\\n    if (NULL == (CALLER)) { /* libxsmm_trace_caller_id may allocate memory */ \\\n      internal_scratch_malloc(&(PTR), SIZE, (size_t)alignment, FLAGS, \\\n        libxsmm_trace_caller_id(0/*level*/)); \\\n    } \\\n    else { \\\n      internal_scratch_malloc(&(PTR), SIZE, (size_t)alignment, FLAGS, CALLER); \\\n    } \\\n    (RESULT) = (PTR); \\\n  } \\\n}\n\n#define INTERNAL_FREE_HOOK(PTR, CALLER) { \\\n  LIBXSMM_UNUSED(CALLER); \\\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind \\\n    /*|| (0 != LIBXSMM_ATOMIC_LOAD(&internal_malloc_recursive, LIBXSMM_ATOMIC_RELAXED))*/ \\\n  ){ \\\n    __real_free(PTR); \\\n  } \\\n  else { /* recognize pointers not issued by LIBXSMM */ \\\n    libxsmm_free(PTR); \\\n  } \\\n}\n\n#if !defined(WIN32)\n# if defined(MAP_32BIT)\n#   define IF_INTERNAL_XMALLOC_MAP32(ENV, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR) \\\n    if (0 != (MAP_32BIT & (MFLAGS))) { \\\n      (BUFFER) = internal_xmalloc_xmap(ENV, SIZE, (MFLAGS) & ~MAP_32BIT, REPTR); \\\n    } \\\n    if (MAP_FAILED != (BUFFER)) (MAPSTATE) = 0; else\n# else\n#   define IF_INTERNAL_XMALLOC_MAP32(ENV, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR)\n# endif\n\n# define INTERNAL_XMALLOC(I, FALLBACK, ENVVAR, ENVDEF, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR) \\\n  if ((I) == (FALLBACK)) { \\\n    static const char* internal_xmalloc_env_ = NULL; \\\n    if (NULL == internal_xmalloc_env_) { \\\n      internal_xmalloc_env_ = getenv(ENVVAR); \\\n      if (NULL == internal_xmalloc_env_) internal_xmalloc_env_ = ENVDEF; \\\n    } \\\n    (BUFFER) = internal_xmalloc_xmap(internal_xmalloc_env_, SIZE, MFLAGS, REPTR); \\\n    if (MAP_FAILED == (BUFFER)) { \\\n      IF_INTERNAL_XMALLOC_MAP32(internal_xmalloc_env_, MAPSTATE, MFLAGS, SIZE, BUFFER, REPTR) \\\n        (FALLBACK) = (I) + 1; \\\n    } \\\n  }\n\n# define INTERNAL_XMALLOC_WATERMARK(NAME, WATERMARK, LIMIT, SIZE) { \\\n  const size_t internal_xmalloc_watermark_ = (WATERMARK) + (SIZE) / 2; /* accept data-race */ \\\n  if (internal_xmalloc_watermark_ < (LIMIT)) { \\\n    static size_t internal_xmalloc_watermark_verbose_ = 0; \\\n    (LIMIT) = internal_xmalloc_watermark_; /* accept data-race */ \\\n    if (internal_xmalloc_watermark_verbose_ < internal_xmalloc_watermark_ && \\\n      (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity)) \\\n    { /* muted */ \\\n      char internal_xmalloc_watermark_buffer_[32]; \\\n      /* coverity[check_return] */ \\\n      libxsmm_format_size(internal_xmalloc_watermark_buffer_, sizeof(internal_xmalloc_watermark_buffer_), \\\n        internal_xmalloc_watermark_, \"KM\", \"B\", 10); \\\n      fprintf(stderr, \"LIBXSMM WARNING: \" NAME \" watermark reached at %s!\\n\", internal_xmalloc_watermark_buffer_); \\\n      internal_xmalloc_watermark_verbose_ = internal_xmalloc_watermark_; \\\n    } \\\n  } \\\n}\n\n# define INTERNAL_XMALLOC_KIND(KIND, NAME, FLAG, FLAGS, MFLAGS, WATERMARK, LIMIT, INFO, SIZE, BUFFER) \\\n  if (0 != ((KIND) & (MFLAGS))) { \\\n    if (MAP_FAILED != (BUFFER)) { \\\n      LIBXSMM_ASSERT(NULL != (BUFFER)); \\\n      LIBXSMM_ATOMIC_ADD_FETCH(&(WATERMARK), SIZE, LIBXSMM_ATOMIC_RELAXED); \\\n      (FLAGS) |= (FLAG); \\\n    } \\\n    else { /* retry */ \\\n      (BUFFER) = mmap(NULL == (INFO) ? NULL : (INFO)->pointer, SIZE, PROT_READ | PROT_WRITE, \\\n        MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | ((MFLAGS) & ~(KIND)), -1, 0/*offset*/); \\\n      if (MAP_FAILED != (BUFFER)) { /* successful retry */ \\\n        LIBXSMM_ASSERT(NULL != (BUFFER)); \\\n        INTERNAL_XMALLOC_WATERMARK(NAME, WATERMARK, LIMIT, SIZE); \\\n      } \\\n    } \\\n  }\n#endif\n\n\nLIBXSMM_EXTERN_C typedef struct LIBXSMM_RETARGETABLE internal_malloc_info_type {\n  libxsmm_free_function free;\n  void *pointer, *reloc;\n  const void* context;\n  size_t size;\n  int flags;\n#if defined(LIBXSMM_VTUNE)\n  unsigned int code_id;\n#endif\n#if !defined(LIBXSMM_MALLOC_CRC_OFF) /* hash *must* be the last entry */\n  unsigned int hash;\n#endif\n} internal_malloc_info_type;\n\nLIBXSMM_EXTERN_C typedef union LIBXSMM_RETARGETABLE internal_malloc_pool_type {\n  char pad[LIBXSMM_MALLOC_SCRATCH_PADDING];\n  struct {\n    size_t minsize, counter, incsize;\n    char *buffer, *head;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    const void* site;\n# if (0 != LIBXSMM_SYNC)\n    unsigned int tid;\n# endif\n#endif\n  } instance;\n} internal_malloc_pool_type;\n\n/* Scratch pool, which supports up to MAX_NSCRATCH allocation sites. */\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n/* LIBXSMM_ALIGNED appears to contradict LIBXSMM_APIVAR, and causes multiple defined symbols (if below is seen in multiple translation units) */\nLIBXSMM_APIVAR_DEFINE(char internal_malloc_pool_buffer[(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS)*sizeof(internal_malloc_pool_type)+(LIBXSMM_MALLOC_SCRATCH_PADDING)-1]);\n#endif\n/* Interval of bytes that permit interception (internal_malloc_kind) */\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_limit[2]);\n/* Maximum total size of the scratch memory domain. */\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_scratch_limit);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_scratch_nmallocs);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_private_max);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_private_cur);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_public_max);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_public_cur);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_local_max);\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_local_cur);\nLIBXSMM_APIVAR_DEFINE(int internal_malloc_recursive);\n/** 0: regular, 1/odd: intercept/scratch, otherwise: all/scratch */\nLIBXSMM_APIVAR_DEFINE(int internal_malloc_kind);\n#if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\nLIBXSMM_APIVAR_DEFINE(int internal_malloc_join);\n#endif\n#if !defined(_WIN32)\n# if defined(MAP_HUGETLB)\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_hugetlb);\n# endif\n# if defined(MAP_LOCKED)\nLIBXSMM_APIVAR_DEFINE(size_t internal_malloc_plocked);\n# endif\n#endif\n\n\nLIBXSMM_API_INTERN size_t libxsmm_alignment(size_t size, size_t alignment)\n{\n  size_t result;\n  if ((LIBXSMM_MALLOC_ALIGNFCT * LIBXSMM_MALLOC_ALIGNMAX) <= size) {\n    result = libxsmm_lcm(0 == alignment ? (LIBXSMM_ALIGNMENT) : libxsmm_lcm(alignment, LIBXSMM_ALIGNMENT), LIBXSMM_MALLOC_ALIGNMAX);\n  }\n  else { /* small-size request */\n    if ((LIBXSMM_MALLOC_ALIGNFCT * LIBXSMM_ALIGNMENT) <= size) {\n      result = (0 == alignment ? (LIBXSMM_ALIGNMENT) : libxsmm_lcm(alignment, LIBXSMM_ALIGNMENT));\n    }\n    else if (0 != alignment) { /* custom alignment */\n      result = libxsmm_lcm(alignment, sizeof(void*));\n    }\n    else { /* tiny-size request */\n      result = sizeof(void*);\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API size_t libxsmm_offset(const size_t offset[], const size_t shape[], size_t ndims, size_t* size)\n{\n  size_t result = 0, size1 = 0;\n  if (0 != ndims && NULL != shape) {\n    size_t i;\n    result = (NULL != offset ? offset[0] : 0);\n    size1 = shape[0];\n    for (i = 1; i < ndims; ++i) {\n      result += (NULL != offset ? offset[i] : 0) * size1;\n      size1 *= shape[i];\n    }\n  }\n  if (NULL != size) *size = size1;\n  return result;\n}\n\n\nLIBXSMM_API_INLINE internal_malloc_info_type* internal_malloc_info(const void* memory, int check)\n{\n  const char *const buffer = (const char*)memory;\n  internal_malloc_info_type* result = (internal_malloc_info_type*)(NULL != memory\n    ? (buffer - sizeof(internal_malloc_info_type)) : NULL);\n#if defined(LIBXSMM_MALLOC_HOOK_CHECK)\n  if ((LIBXSMM_MALLOC_HOOK_CHECK) < check) check = (LIBXSMM_MALLOC_HOOK_CHECK);\n#endif\n  if (0 != check && NULL != result) { /* check ownership */\n#if !defined(_WIN32) /* mprotect: pass address rounded down to page/4k alignment */\n    if (1 == check || 0 == mprotect((void*)(((uintptr_t)result) & 0xFFFFFFFFFFFFF000),\n      sizeof(internal_malloc_info_type), PROT_READ | PROT_WRITE) || ENOMEM != errno)\n#endif\n    {\n      const size_t maxsize = LIBXSMM_MAX(LIBXSMM_MAX(internal_malloc_public_max, internal_malloc_local_max), internal_malloc_private_max);\n      const int flags_rs = LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_SCRATCH;\n      const int flags_mx = LIBXSMM_MALLOC_FLAG_MMAP | LIBXSMM_MALLOC_FLAG_X;\n      const char* const pointer = (const char*)result->pointer;\n      union { libxsmm_free_fun fun; const void* ptr; } convert;\n      convert.fun = result->free.function;\n      if (((flags_mx != (flags_mx & result->flags)) && NULL != result->reloc)\n        || (0 == (LIBXSMM_MALLOC_FLAG_X & result->flags) ? 0 : (0 != (flags_rs & result->flags)))\n        || (0 != (LIBXSMM_MALLOC_FLAG_X & result->flags) && NULL != result->context)\n#if defined(LIBXSMM_VTUNE)\n        || (0 == (LIBXSMM_MALLOC_FLAG_X & result->flags) && 0 != result->code_id)\n#endif\n        || (0 != (~LIBXSMM_MALLOC_FLAG_VALID & result->flags))\n        || (0 == (LIBXSMM_MALLOC_FLAG_R & result->flags))\n        || pointer == convert.ptr || pointer == result->context\n        || pointer >= buffer || NULL == pointer\n        || maxsize < result->size || 0 == result->size\n        || 2 > libxsmm_ninit /* before checksum calculation */\n#if !defined(LIBXSMM_MALLOC_CRC_OFF) /* last check: checksum over info */\n# if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n        || result->hash != LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &result)\n# else\n        || result->hash != libxsmm_crc32(LIBXSMM_MALLOC_SEED, result,\n            (const char*)&result->hash - (const char*)result)\n# endif\n#endif\n      ) { /* mismatch */\n        result = NULL;\n      }\n    }\n#if !defined(_WIN32)\n    else { /* mismatch */\n      result = NULL;\n    }\n#endif\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int internal_xfree(const void* /*memory*/, internal_malloc_info_type* /*info*/);\nLIBXSMM_API_INTERN int internal_xfree(const void* memory, internal_malloc_info_type* info)\n{\n#if !defined(LIBXSMM_BUILD) || !defined(_WIN32)\n  static int error_once = 0;\n#endif\n  int result = EXIT_SUCCESS, flags;\n  void* buffer;\n  size_t size;\n  LIBXSMM_ASSERT(NULL != memory && NULL != info);\n  buffer = info->pointer;\n  flags = info->flags;\n  size = info->size;\n#if !defined(LIBXSMM_BUILD) /* sanity check */\n  if (NULL != buffer || 0 == size)\n#endif\n  {\n    const size_t alloc_size = size + (((const char*)memory) - ((const char*)buffer));\n    LIBXSMM_ASSERT(NULL != buffer || 0 == size);\n    if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n      if (NULL != info->free.function) {\n#if defined(LIBXSMM_MALLOC_DELETE_SAFE)\n        info->pointer = NULL; info->size = 0;\n#endif\n        if (NULL == info->context) {\n#if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) && 0\n          if (free == info->free.function) {\n            __real_free(buffer);\n          }\n          else\n#endif\n          if (NULL != info->free.function) {\n            info->free.function(buffer);\n          }\n        }\n        else {\n          LIBXSMM_ASSERT(NULL != info->free.ctx_form);\n          info->free.ctx_form(buffer, info->context);\n        }\n      }\n    }\n    else {\n#if defined(LIBXSMM_VTUNE)\n      if (0 != (LIBXSMM_MALLOC_FLAG_X & flags) && 0 != info->code_id && iJIT_SAMPLING_ON == iJIT_IsProfilingActive()) {\n        iJIT_NotifyEvent(LIBXSMM_VTUNE_JIT_UNLOAD, &info->code_id);\n      }\n#endif\n#if defined(_WIN32)\n      result = (NULL == buffer || FALSE != VirtualFree(buffer, 0, MEM_RELEASE)) ? EXIT_SUCCESS : EXIT_FAILURE;\n#else /* !_WIN32 */\n      {\n        const size_t unmap_size = LIBXSMM_UP2(alloc_size, LIBXSMM_PAGE_MINSIZE);\n        void* const reloc = info->reloc;\n        if (0 != munmap(buffer, unmap_size)) {\n          if (0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          {\n            fprintf(stderr, \"LIBXSMM ERROR: %s (attempted to unmap buffer %p+%\" PRIuPTR \")!\\n\",\n              strerror(errno), buffer, (uintptr_t)unmap_size);\n          }\n          result = EXIT_FAILURE;\n        }\n        if (0 != (LIBXSMM_MALLOC_FLAG_X & flags) && EXIT_SUCCESS == result\n          && NULL != reloc && MAP_FAILED != reloc && buffer != reloc\n          && 0 != munmap(reloc, unmap_size))\n        {\n          if (0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          {\n            fprintf(stderr, \"LIBXSMM ERROR: %s (attempted to unmap code %p+%\" PRIuPTR \")!\\n\",\n              strerror(errno), reloc, (uintptr_t)unmap_size);\n          }\n          result = EXIT_FAILURE;\n        }\n      }\n#endif\n    }\n    if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* update statistics */\n#if !defined(_WIN32)\n# if defined(MAP_HUGETLB)\n      if (0 != (LIBXSMM_MALLOC_FLAG_PHUGE & flags)) { /* huge pages */\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n        LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_hugetlb, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n      }\n# endif\n# if defined(MAP_LOCKED)\n      if (0 != (LIBXSMM_MALLOC_FLAG_PLOCK & flags)) { /* page-locked */\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n        LIBXSMM_ATOMIC_SUB_FETCH(&internal_malloc_plocked, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n      }\n# endif\n#endif\n      if (0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags)) { /* public */\n        if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) { /* scratch */\n#if 1\n          const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n            &internal_malloc_public_cur, LIBXSMM_ATOMIC_RELAXED);\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_public_cur,\n            alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n#else\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_SUB_FETCH, LIBXSMM_BITS)(\n            &internal_malloc_public_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n#endif\n        }\n        else { /* local */\n#if 1\n          const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n            &internal_malloc_local_cur, LIBXSMM_ATOMIC_RELAXED);\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_local_cur,\n            alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n#else\n          LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_SUB_FETCH, LIBXSMM_BITS)(\n            &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n#endif\n        }\n      }\n      else { /* private */\n#if 1\n        const size_t current = (size_t)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_LOAD, LIBXSMM_BITS)(\n          &internal_malloc_private_cur, LIBXSMM_ATOMIC_RELAXED);\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_STORE, LIBXSMM_BITS)(&internal_malloc_private_cur,\n          alloc_size <= current ? (current - alloc_size) : 0, LIBXSMM_ATOMIC_RELAXED);\n#else\n        LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_SUB_FETCH, LIBXSMM_BITS)(\n          &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n#endif\n      }\n    }\n  }\n#if !defined(LIBXSMM_BUILD)\n  else if ((LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: attempt to release memory from non-matching implementation!\\n\");\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INLINE size_t internal_get_scratch_size(const internal_malloc_pool_type* exclude)\n{\n  size_t result = 0;\n#if !defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) || (1 >= (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  LIBXSMM_UNUSED(exclude);\n#else\n  const internal_malloc_pool_type* pool = (const internal_malloc_pool_type*)LIBXSMM_UP2(\n    (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const internal_malloc_pool_type *const end = pool + libxsmm_scratch_pools;\n  LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  for (; pool != end; ++pool)\n# endif /*(1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  {\n    if (0 != pool->instance.minsize) {\n# if 1 /* memory info is not used */\n      if (pool != exclude && (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n        result += pool->instance.minsize;\n      }\n# else\n      const internal_malloc_info_type* const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n      if (NULL != info && pool != exclude && (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n        result += info->size;\n      }\n# endif\n    }\n    else break; /* early exit */\n  }\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  return result;\n}\n\n\nLIBXSMM_API_INLINE internal_malloc_pool_type* internal_scratch_malloc_pool(const void* memory)\n{\n  internal_malloc_pool_type* result = NULL;\n  internal_malloc_pool_type* pool = (internal_malloc_pool_type*)LIBXSMM_UP2(\n    (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n  const char* const buffer = (const char*)memory;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const unsigned int npools = libxsmm_scratch_pools;\n#else\n  const unsigned int npools = 1;\n#endif\n  internal_malloc_pool_type *const end = pool + npools;\n  LIBXSMM_ASSERT(npools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  LIBXSMM_ASSERT(NULL != memory);\n  for (; pool != end; ++pool) {\n    if (0 != pool->instance.minsize) {\n      if (0 != pool->instance.counter\n#if 1 /* should be implied by non-zero counter */\n        && NULL != pool->instance.buffer\n#endif\n      ){/* check if memory belongs to scratch domain or local domain */\n#if 1\n        const size_t size = pool->instance.minsize;\n#else\n        const internal_malloc_info_type* const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n        const size_t size = info->size;\n#endif\n        if (pool->instance.buffer == buffer /* fast path */ ||\n           (pool->instance.buffer < buffer && buffer < (pool->instance.buffer + size)))\n        {\n          result = pool;\n          break;\n        }\n      }\n    }\n    else break; /* early exit */\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void internal_scratch_free(const void* /*memory*/, internal_malloc_pool_type* /*pool*/);\nLIBXSMM_API_INTERN void internal_scratch_free(const void* memory, internal_malloc_pool_type* pool)\n{\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  const size_t counter = LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n  char* const pool_buffer = pool->instance.buffer;\n# if !defined(NDEBUG) || defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD)\n  char *const buffer = (char*)memory; /* non-const */\n  LIBXSMM_ASSERT(pool_buffer <= buffer && buffer < pool_buffer + pool->instance.minsize);\n# endif\n  LIBXSMM_ASSERT(pool_buffer <= pool->instance.head);\n  if (0 == counter) { /* reuse or reallocate scratch domain */\n    internal_malloc_info_type *const info = internal_malloc_info(pool_buffer, 0/*no check*/);\n    const size_t scale_size = (size_t)(1 != libxsmm_scratch_scale ? (libxsmm_scratch_scale * info->size) : info->size); /* hysteresis */\n    const size_t size = pool->instance.minsize + pool->instance.incsize;\n    LIBXSMM_ASSERT(0 == (LIBXSMM_MALLOC_FLAG_X & info->flags)); /* scratch memory is not executable */\n    if (size <= scale_size) { /* reuse scratch domain */\n      pool->instance.head = pool_buffer; /* reuse scratch domain */\n    }\n    else { /* release buffer */\n# if !defined(NDEBUG)\n      static int error_once = 0;\n# endif\n      pool->instance.buffer = pool->instance.head = NULL;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      pool->instance.site = NULL; /* clear affinity */\n# endif\n# if !defined(NDEBUG)\n      if (EXIT_SUCCESS != internal_xfree(pool_buffer, info)\n        && 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n# else\n      internal_xfree(pool_buffer, info); /* !libxsmm_free */\n# endif\n    }\n  }\n# if defined(LIBXSMM_MALLOC_SCRATCH_TRIM_HEAD) /* TODO: document linear/scoped allocator policy */\n  else if (buffer < pool->instance.head) { /* reuse scratch domain */\n    pool->instance.head = buffer;\n  }\n# else\n  LIBXSMM_UNUSED(memory);\n# endif\n#else\n  LIBXSMM_UNUSED(memory); LIBXSMM_UNUSED(pool);\n#endif\n}\n\n\nLIBXSMM_API_INTERN void internal_scratch_malloc(void** /*memory*/, size_t /*size*/, size_t /*alignment*/, int /*flags*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void internal_scratch_malloc(void** memory, size_t size, size_t alignment, int flags, const void* caller)\n{\n  LIBXSMM_ASSERT(NULL != memory && 0 == (LIBXSMM_MALLOC_FLAG_X & flags));\n  if (0 == (LIBXSMM_MALLOC_FLAG_REALLOC & flags) || NULL == *memory) {\n    static int error_once = 0;\n    size_t local_size = 0;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    if (0 < libxsmm_scratch_pools) {\n      internal_malloc_pool_type *const pools = (internal_malloc_pool_type*)LIBXSMM_UP2(\n        (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n      internal_malloc_pool_type *const end = pools + libxsmm_scratch_pools, *pool = pools;\n      const size_t align_size = libxsmm_alignment(size, alignment), alloc_size = size + align_size - 1;\n# if (0 != LIBXSMM_SYNC)\n      const unsigned int tid = libxsmm_get_tid();\n# endif\n      unsigned int npools = 1;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      const void *const site = caller; /* no further attempt in case of NULL */\n      internal_malloc_pool_type *pool0 = end;\n      for (; pool != end; ++pool) { /* counter: memory info is not employed as pools are still manipulated */\n        if (NULL != pool->instance.buffer) {\n          if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) ++npools; /* count number of occupied pools */\n          if ( /* find matching pool and enter fast path (draw from pool-buffer) */\n#   if (0 != LIBXSMM_SYNC) && !defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            (site == pool->instance.site && tid == pool->instance.tid))\n#   elif (0 != LIBXSMM_SYNC)\n            (site == pool->instance.site && (0 != internal_malloc_join || tid == pool->instance.tid)))\n#   else\n            (site == pool->instance.site))\n#   endif\n          {\n            break;\n          }\n        }\n        else {\n          if (end == pool0) pool0 = pool; /* first available pool*/\n          if (0 == pool->instance.minsize) { /* early exit */\n            pool = pool0; break;\n          }\n        }\n      }\n# endif\n      LIBXSMM_ASSERT(NULL != pool);\n      if (end != pool && 0 <= internal_malloc_kind) {\n        const size_t counter = LIBXSMM_ATOMIC_ADD_FETCH(&pool->instance.counter, (size_t)1, LIBXSMM_ATOMIC_SEQ_CST);\n        if (NULL != pool->instance.buffer || 1 != counter) { /* attempt to (re-)use existing pool */\n          const internal_malloc_info_type *const info = internal_malloc_info(pool->instance.buffer, 1/*check*/);\n          const size_t pool_size = ((NULL != info && 0 != counter) ? info->size : 0);\n          const size_t used_size = pool->instance.head - pool->instance.buffer;\n          const size_t req_size = alloc_size + used_size;\n          if (req_size <= pool_size) { /* fast path: draw from pool-buffer */\n# if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            void *const headaddr = &pool->instance.head;\n            char *const head = (0 == internal_malloc_join\n              ? (pool->instance.head += alloc_size)\n              : ((char*)LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                (uintptr_t*)headaddr, alloc_size, LIBXSMM_ATOMIC_SEQ_CST)));\n# else\n            char *const head = (char*)(pool->instance.head += alloc_size);\n# endif\n            *memory = LIBXSMM_ALIGN(head - alloc_size, align_size);\n          }\n          else { /* fall-back to local memory allocation */\n            const size_t incsize = req_size - LIBXSMM_MIN(pool_size, req_size);\n            pool->instance.incsize = LIBXSMM_MAX(pool->instance.incsize, incsize);\n# if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n            if (0 == internal_malloc_join) {\n              --pool->instance.counter;\n            }\n            else {\n              LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n            }\n# else\n            --pool->instance.counter;\n# endif\n            if (\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n              (LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site &&\n# endif\n              0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags))\n            {\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_local_max < watermark) internal_malloc_local_max = watermark; /* accept data-race */\n            }\n            else {\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_private_max < watermark) internal_malloc_private_max = watermark; /* accept data-race */\n            }\n            local_size = size;\n          }\n        }\n        else { /* fresh pool */\n          const size_t scratch_limit = libxsmm_get_scratch_limit();\n          const size_t scratch_size = internal_get_scratch_size(pool); /* exclude current pool */\n          const size_t limit_size = (1 < npools ? (scratch_limit - LIBXSMM_MIN(scratch_size, scratch_limit)) : LIBXSMM_SCRATCH_UNLIMITED);\n          const size_t scale_size = (size_t)(1 != libxsmm_scratch_scale ? (libxsmm_scratch_scale * alloc_size) : alloc_size); /* hysteresis */\n          const size_t incsize = (size_t)(libxsmm_scratch_scale * pool->instance.incsize);\n          const size_t maxsize = LIBXSMM_MAX(scale_size, pool->instance.minsize) + incsize;\n          const size_t limsize = LIBXSMM_MIN(maxsize, limit_size);\n          const size_t minsize = limsize;\n          LIBXSMM_ASSERT(1 <= libxsmm_scratch_scale);\n          LIBXSMM_ASSERT(1 == counter);\n          pool->instance.incsize = 0; /* reset */\n          pool->instance.minsize = minsize;\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n          pool->instance.site = site;\n#   if (0 != LIBXSMM_SYNC)\n          pool->instance.tid = tid;\n#   endif\n# endif\n          if (alloc_size <= minsize && /* allocate scratch pool */\n            EXIT_SUCCESS == libxsmm_xmalloc(memory, minsize, 0/*auto-align*/,\n              (flags | LIBXSMM_MALLOC_FLAG_SCRATCH) & ~LIBXSMM_MALLOC_FLAG_REALLOC,\n              NULL/*extra*/, 0/*extra_size*/))\n          {\n            pool->instance.buffer = (char*)*memory;\n            pool->instance.head = pool->instance.buffer + alloc_size;\n            *memory = LIBXSMM_ALIGN((char*)*memory, align_size);\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n            if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site)\n# endif\n            {\n              LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_scratch_nmallocs, 1, LIBXSMM_ATOMIC_RELAXED);\n            }\n          }\n          else { /* fall-back to local allocation */\n            LIBXSMM_ATOMIC_SUB_FETCH(&pool->instance.counter, 1, LIBXSMM_ATOMIC_SEQ_CST);\n            if (0 != libxsmm_verbosity /* library code is expected to be mute */\n              && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n            {\n              if (alloc_size <= minsize) {\n                fprintf(stderr, \"LIBXSMM ERROR: failed to allocate scratch memory!\\n\");\n              }\n              else if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != caller\n                && (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity))\n              {\n                fprintf(stderr, \"LIBXSMM WARNING: scratch memory domain exhausted!\\n\");\n              }\n            }\n            local_size = size;\n          }\n        }\n      }\n      else { /* fall-back to local memory allocation */\n        local_size = size;\n      }\n    }\n    else { /* fall-back to local memory allocation */\n      local_size = size;\n    }\n    if (0 != local_size)\n#else\n    local_size = size;\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n    { /* local memory allocation */\n      if (EXIT_SUCCESS != libxsmm_xmalloc(memory, local_size, alignment,\n          flags & ~(LIBXSMM_MALLOC_FLAG_SCRATCH | LIBXSMM_MALLOC_FLAG_REALLOC), NULL/*extra*/, 0/*extra_size*/)\n        && /* library code is expected to be mute */0 != libxsmm_verbosity\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: scratch memory fall-back failed!\\n\");\n        LIBXSMM_ASSERT(NULL == *memory);\n      }\n      if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != caller) {\n        LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_scratch_nmallocs, 1, LIBXSMM_ATOMIC_RELAXED);\n      }\n    }\n  }\n  else { /* reallocate memory */\n    const void *const preserve = *memory;\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(preserve);\n    if (NULL != pool) {\n      const internal_malloc_info_type *const info = internal_malloc_info(pool->instance.buffer, 0/*no check*/);\n      void* buffer;\n      LIBXSMM_ASSERT(pool->instance.buffer <= pool->instance.head && NULL != info);\n      internal_scratch_malloc(&buffer, size, alignment,\n        ~LIBXSMM_MALLOC_FLAG_REALLOC & (LIBXSMM_MALLOC_FLAG_SCRATCH | flags), caller);\n      if (NULL != buffer) {\n        memcpy(buffer, preserve, LIBXSMM_MIN(size, info->size)); /* TODO: memmove? */\n        *memory = buffer;\n      }\n      internal_scratch_free(memory, pool);\n    }\n    else\n#endif\n    { /* non-pooled (potentially foreign pointer) */\n#if !defined(NDEBUG)\n      const int status =\n#endif\n      libxsmm_xmalloc(memory, size, alignment/* no need here to determine alignment of given buffer */,\n        ~LIBXSMM_MALLOC_FLAG_SCRATCH & flags, NULL/*extra*/, 0/*extra_size*/);\n      assert(EXIT_SUCCESS == status || NULL == *memory); /* !LIBXSMM_ASSERT */\n    }\n  }\n}\n\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_APIVAR_PRIVATE_DEF(libxsmm_malloc_fntype libxsmm_malloc_fn);\n\n#if defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)\nLIBXSMM_API_INTERN void* internal_memalign_malloc(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_malloc(size_t alignment, size_t size)\n{\n  LIBXSMM_UNUSED(alignment);\n  LIBXSMM_ASSERT(NULL != libxsmm_malloc_fn.malloc.dlsym);\n  return libxsmm_malloc_fn.malloc.ptr(size);\n}\n#elif defined(LIBXSMM_MALLOC_HOOK_KMP)\nLIBXSMM_API_INTERN void* internal_memalign_twiddle(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API_INTERN void* internal_memalign_twiddle(size_t alignment, size_t size)\n{\n  LIBXSMM_ASSERT(NULL != libxsmm_malloc_fn.alignmem.dlsym);\n  return libxsmm_malloc_fn.alignmem.ptr(size, alignment);\n}\n#endif\n#endif /*defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)*/\n\n\n#if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n\nLIBXSMM_API_INTERN void* internal_memalign_hook(size_t /*alignment*/, size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_memalign_hook(size_t alignment, size_t size, const void* caller)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, caller);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, caller);\n# endif\n  return result;\n}\n\nLIBXSMM_API void* __wrap_memalign(size_t /*alignment*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_memalign(size_t alignment, size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\nLIBXSMM_API_INTERN void* internal_malloc_hook(size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_malloc_hook(size_t size, const void* caller)\n{\n  return internal_memalign_hook(0/*auto-alignment*/, size, caller);\n}\n\nLIBXSMM_API void* __wrap_malloc(size_t /*size*/);\nLIBXSMM_API void* __wrap_malloc(size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API void* __wrap_calloc(size_t /*num*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_calloc(size_t num, size_t size)\n{\n  void* result;\n  const size_t nbytes = num * size;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# endif\n  /* TODO: signal anonymous/zeroed pages */\n  if (NULL != result) memset(result, 0, nbytes);\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API_INTERN void* internal_realloc_hook(void* /*ptr*/, size_t /*size*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void* internal_realloc_hook(void* ptr, size_t size, const void* caller)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, caller);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, caller);\n# endif\n  return result;\n}\n\nLIBXSMM_API void* __wrap_realloc(void* /*ptr*/, size_t /*size*/);\nLIBXSMM_API void* __wrap_realloc(void* ptr, size_t size)\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, NULL/*caller*/);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, NULL/*caller*/);\n# endif\n  return result;\n}\n#endif\n\nLIBXSMM_API_INTERN void internal_free_hook(void* /*ptr*/, const void* /*caller*/);\nLIBXSMM_API_INTERN void internal_free_hook(void* ptr, const void* caller)\n{\n  INTERNAL_FREE_HOOK(ptr, caller);\n}\n\nLIBXSMM_API void __wrap_free(void* /*ptr*/);\nLIBXSMM_API void __wrap_free(void* ptr)\n{\n  INTERNAL_FREE_HOOK(ptr, NULL/*caller*/);\n}\n\n#endif /*(defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))*/\n\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* memalign(size_t /*alignment*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* memalign(size_t alignment, size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, alignment, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, alignment, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* malloc(size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* malloc(size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, size, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, size, NULL/*caller*/);\n# endif\n  return result;\n}\n\n#if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* calloc(size_t /*num*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK LIBXSMM_ATTRIBUTE_MALLOC void* calloc(size_t num, size_t size) LIBXSMM_THROW\n{\n  void* result;\n  const size_t nbytes = num * size;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_MMAP, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# else\n  INTERNAL_MEMALIGN_HOOK(result, LIBXSMM_MALLOC_FLAG_DEFAULT, 0/*auto-alignment*/, nbytes, NULL/*caller*/);\n# endif\n  /* TODO: signal anonymous/zeroed pages */\n  if (NULL != result) memset(result, 0, nbytes);\n  return result;\n}\n#endif\n\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void* realloc(void* /*ptr*/, size_t /*size*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void* realloc(void* ptr, size_t size) LIBXSMM_THROW\n{\n  void* result;\n# if defined(LIBXSMM_MALLOC_MMAP_HOOK)\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_MMAP, ptr, size, NULL/*caller*/);\n# else\n  INTERNAL_REALLOC_HOOK(result, LIBXSMM_MALLOC_FLAG_REALLOC | LIBXSMM_MALLOC_FLAG_DEFAULT, ptr, size, NULL/*caller*/);\n# endif\n  return result;\n}\n#endif\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void free(void* /*ptr*/) LIBXSMM_THROW;\nLIBXSMM_API LIBXSMM_ATTRIBUTE_WEAK void free(void* ptr) LIBXSMM_THROW\n{\n  INTERNAL_FREE_HOOK(ptr, NULL/*caller*/);\n}\n#endif /*defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)*/\n\n\nLIBXSMM_API_INTERN void libxsmm_malloc_init(void)\n{\n#if (0 != LIBXSMM_SYNC) && defined(LIBXSMM_MALLOC_SCRATCH_JOIN)\n  const char *const env = getenv(\"LIBXSMM_MALLOC_JOIN\");\n  if (NULL != env && 0 != *env) internal_malloc_join = atoi(env);\n#endif\n#if defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)\n# if defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)\n  void* handle_qkmalloc = NULL;\n  dlerror(); /* clear an eventual error status */\n  handle_qkmalloc = dlopen(\"libqkmalloc.so\", RTLD_LAZY);\n  if (NULL != handle_qkmalloc) {\n    libxsmm_malloc_fn.memalign.ptr = internal_memalign_malloc;\n    libxsmm_malloc_fn.malloc.dlsym = dlsym(handle_qkmalloc, \"malloc\");\n    if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      libxsmm_malloc_fn.calloc.dlsym = dlsym(handle_qkmalloc, \"calloc\");\n      if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n#   endif\n      {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        libxsmm_malloc_fn.realloc.dlsym = dlsym(handle_qkmalloc, \"realloc\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n#   endif\n        {\n          libxsmm_malloc_fn.free.dlsym = dlsym(handle_qkmalloc, \"free\");\n        }\n      }\n    }\n    dlclose(handle_qkmalloc);\n  }\n  if (NULL == libxsmm_malloc_fn.free.ptr)\n# elif defined(LIBXSMM_MALLOC_HOOK_KMP)\n  dlerror(); /* clear an eventual error status */\n  libxsmm_malloc_fn.alignmem.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_aligned_malloc\");\n  if (NULL == dlerror() && NULL != libxsmm_malloc_fn.alignmem.dlsym) {\n    libxsmm_malloc_fn.memalign.ptr = internal_memalign_twiddle;\n    libxsmm_malloc_fn.malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_malloc\");\n    if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n# if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      libxsmm_malloc_fn.calloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_calloc\");\n      if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n# endif\n      {\n# if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        libxsmm_malloc_fn.realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_realloc\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n# endif\n        {\n          libxsmm_malloc_fn.free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"kmp_free\");\n        }\n      }\n    }\n  }\n  if (NULL == libxsmm_malloc_fn.free.ptr)\n# endif /*defined(LIBXSMM_MALLOC_HOOK_QKMALLOC)*/\n  {\n    dlerror(); /* clear an eventual error status */\n# if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n    libxsmm_malloc_fn.memalign.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_memalign\");\n    if (NULL == dlerror() && NULL != libxsmm_malloc_fn.memalign.dlsym) {\n      libxsmm_malloc_fn.malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_malloc\");\n      if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n        libxsmm_malloc_fn.calloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_calloc\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n#   endif\n        {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n          libxsmm_malloc_fn.realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_realloc\");\n          if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n#   endif\n          {\n            libxsmm_malloc_fn.free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__libc_free\");\n          }\n        }\n      }\n    }\n    if (NULL == libxsmm_malloc_fn.free.ptr) {\n      void* handle_libc = NULL;\n      dlerror(); /* clear an eventual error status */\n      handle_libc = dlopen(\"libc.so.\" LIBXSMM_STRINGIFY(LIBXSMM_MALLOC_GLIBC), RTLD_LAZY);\n      if (NULL != handle_libc) {\n        libxsmm_malloc_fn.memalign.dlsym = dlsym(handle_libc, \"__libc_memalign\");\n        if (NULL == dlerror() && NULL != libxsmm_malloc_fn.memalign.dlsym) {\n          libxsmm_malloc_fn.malloc.dlsym = dlsym(handle_libc, \"__libc_malloc\");\n          if (NULL == dlerror() && NULL != libxsmm_malloc_fn.malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n            libxsmm_malloc_fn.calloc.dlsym = dlsym(handle_libc, \"__libc_calloc\");\n            if (NULL == dlerror() && NULL != libxsmm_malloc_fn.calloc.dlsym)\n#   endif\n            {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n              libxsmm_malloc_fn.realloc.dlsym = dlsym(handle_libc, \"__libc_realloc\");\n              if (NULL == dlerror() && NULL != libxsmm_malloc_fn.realloc.dlsym)\n#   endif\n              {\n                libxsmm_malloc_fn.free.dlsym = dlsym(handle_libc, \"__libc_free\");\n              }\n            }\n          }\n        }\n        dlclose(handle_libc);\n      }\n    }\n#   if 0\n    { /* attempt to setup deprecated GLIBC hooks */\n      union { const void* dlsym; void* (**ptr)(size_t, size_t, const void*); } hook_memalign;\n      dlerror(); /* clear an eventual error status */\n      hook_memalign.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__memalign_hook\");\n      if (NULL == dlerror() && NULL != hook_memalign.dlsym) {\n        union { const void* dlsym; void* (**ptr)(size_t, const void*); } hook_malloc;\n        hook_malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__malloc_hook\");\n        if (NULL == dlerror() && NULL != hook_malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n          union { const void* dlsym; void* (**ptr)(void*, size_t, const void*); } hook_realloc;\n          hook_realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__realloc_hook\");\n          if (NULL == dlerror() && NULL != hook_realloc.dlsym)\n#   endif\n          {\n            union { const void* dlsym; void (**ptr)(void*, const void*); } hook_free;\n            hook_free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"__free_hook\");\n            if (NULL == dlerror() && NULL != hook_free.dlsym) {\n              *hook_memalign.ptr = internal_memalign_hook;\n              *hook_malloc.ptr = internal_malloc_hook;\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n              *hook_realloc.ptr = internal_realloc_hook;\n#   endif\n              *hook_free.ptr = internal_free_hook;\n            }\n          }\n        }\n      }\n    }\n#   endif\n# else /* TODO */\n# endif /*(defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))*/\n  }\n  if (NULL != libxsmm_malloc_fn.free.ptr) {\n# if defined(LIBXSMM_MALLOC_HOOK_IMALLOC)\n    union { const void* dlsym; libxsmm_malloc_fun* ptr; } i_malloc;\n    i_malloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_malloc\");\n    if (NULL == dlerror() && NULL != i_malloc.dlsym) {\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n      union { const void* dlsym; void* (**ptr)(size_t, size_t); } i_calloc;\n      i_calloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_calloc\");\n      if (NULL == dlerror() && NULL != i_calloc.dlsym)\n#   endif\n      {\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n        union { const void* dlsym; libxsmm_realloc_fun* ptr; } i_realloc;\n        i_realloc.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_realloc\");\n        if (NULL == dlerror() && NULL != i_realloc.dlsym)\n#   endif\n        {\n          union { const void* dlsym; libxsmm_free_fun* ptr; } i_free;\n          i_free.dlsym = dlsym(LIBXSMM_RTLD_NEXT, \"i_free\");\n          if (NULL == dlerror() && NULL != i_free.dlsym) {\n            *i_malloc.ptr = libxsmm_malloc_fn.malloc.ptr;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n            *i_calloc.ptr = libxsmm_malloc_fn.calloc.ptr;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n            *i_realloc.ptr = libxsmm_malloc_fn.realloc.ptr;\n#   endif\n            *i_free.ptr = libxsmm_malloc_fn.free.ptr;\n          }\n        }\n      }\n    }\n# endif /*defined(LIBXSMM_MALLOC_HOOK_IMALLOC)*/\n  }\n  else { /* fall-back: potentially recursive */\n# if (defined(LIBXSMM_BUILD) && (1 < (LIBXSMM_BUILD)))\n    libxsmm_malloc_fn.memalign.ptr = __libc_memalign;\n    libxsmm_malloc_fn.malloc.ptr = __libc_malloc;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n    libxsmm_malloc_fn.calloc.ptr = __libc_calloc;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n    libxsmm_malloc_fn.realloc.ptr = __libc_realloc;\n#   endif\n    libxsmm_malloc_fn.free.ptr = __libc_free;\n# else\n    libxsmm_malloc_fn.memalign.ptr = libxsmm_memalign_internal;\n    libxsmm_malloc_fn.malloc.ptr = malloc;\n#   if defined(LIBXSMM_MALLOC_HOOK_CALLOC)\n    libxsmm_malloc_fn.calloc.ptr = calloc;\n#   endif\n#   if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n    libxsmm_malloc_fn.realloc.ptr = realloc;\n#   endif\n    libxsmm_malloc_fn.free.ptr = free;\n# endif\n  }\n#endif\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_malloc_finalize(void)\n{\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xset_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != lock) {\n    LIBXSMM_INIT\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n  if (NULL != malloc_fn.function && NULL != free_fn.function) {\n    libxsmm_default_allocator_context = context;\n    libxsmm_default_malloc_fn = malloc_fn;\n    libxsmm_default_free_fn = free_fn;\n  }\n  else {\n    libxsmm_malloc_function internal_malloc_fn;\n    libxsmm_free_function internal_free_fn;\n    const void* internal_allocator = NULL;\n    internal_malloc_fn.function = __real_malloc;\n    internal_free_fn.function = __real_free;\n    /*internal_allocator = NULL;*/\n    if (NULL == malloc_fn.function && NULL == free_fn.function) {\n      libxsmm_default_allocator_context = internal_allocator;\n      libxsmm_default_malloc_fn = internal_malloc_fn;\n      libxsmm_default_free_fn = internal_free_fn;\n    }\n    else { /* invalid allocator */\n      static int error_once = 0;\n      if (0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: allocator setup without malloc or free function!\\n\");\n      }\n      /* keep any valid (previously instantiated) default allocator */\n      if (NULL == libxsmm_default_malloc_fn.function || NULL == libxsmm_default_free_fn.function) {\n        libxsmm_default_allocator_context = internal_allocator;\n        libxsmm_default_malloc_fn = internal_malloc_fn;\n        libxsmm_default_free_fn = internal_free_fn;\n      }\n      result = EXIT_FAILURE;\n    }\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xget_default_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != context || NULL != malloc_fn || NULL != free_fn) {\n    if (NULL != lock) {\n      LIBXSMM_INIT\n      LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n    }\n    if (context) *context = libxsmm_default_allocator_context;\n    if (NULL != malloc_fn) *malloc_fn = libxsmm_default_malloc_fn;\n    if (NULL != free_fn) *free_fn = libxsmm_default_free_fn;\n    if (NULL != lock) {\n      LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n    }\n  }\n  else if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    static int error_once = 0;\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid signature used to get the default memory allocator!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xset_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  int result = EXIT_SUCCESS;\n  static int error_once = 0;\n  if (NULL != lock) {\n    LIBXSMM_INIT\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n  /* make sure the default allocator is setup before adopting it eventually */\n  if (NULL == libxsmm_default_malloc_fn.function || NULL == libxsmm_default_free_fn.function) {\n    const libxsmm_malloc_function null_malloc_fn = { NULL };\n    const libxsmm_free_function null_free_fn = { NULL };\n    libxsmm_xset_default_allocator(NULL/*already locked*/, NULL/*context*/, null_malloc_fn, null_free_fn);\n  }\n  if (NULL == malloc_fn.function && NULL == free_fn.function) { /* adopt default allocator */\n    libxsmm_scratch_allocator_context = libxsmm_default_allocator_context;\n    libxsmm_scratch_malloc_fn = libxsmm_default_malloc_fn;\n    libxsmm_scratch_free_fn = libxsmm_default_free_fn;\n  }\n  else if (NULL != malloc_fn.function) {\n    if (NULL == free_fn.function\n      && /*warning*/(LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM WARNING: scratch allocator setup without free function!\\n\");\n    }\n    libxsmm_scratch_allocator_context = context;\n    libxsmm_scratch_malloc_fn = malloc_fn;\n    libxsmm_scratch_free_fn = free_fn; /* NULL allowed */\n  }\n  else { /* invalid scratch allocator */\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid scratch allocator (default used)!\\n\");\n    }\n    /* keep any valid (previously instantiated) scratch allocator */\n    if (NULL == libxsmm_scratch_malloc_fn.function) {\n      libxsmm_scratch_allocator_context = libxsmm_default_allocator_context;\n      libxsmm_scratch_malloc_fn = libxsmm_default_malloc_fn;\n      libxsmm_scratch_free_fn = libxsmm_default_free_fn;\n    }\n    result = EXIT_FAILURE;\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xget_scratch_allocator(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock,\n  const void** context, libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != context || NULL != malloc_fn || NULL != free_fn) {\n    if (NULL != lock) {\n      LIBXSMM_INIT\n      LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n    }\n    if (context) *context = libxsmm_scratch_allocator_context;\n    if (NULL != malloc_fn) *malloc_fn = libxsmm_scratch_malloc_fn;\n    if (NULL != free_fn) *free_fn = libxsmm_scratch_free_fn;\n    if (NULL != lock) {\n      LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n    }\n  }\n  else if (0 != libxsmm_verbosity) { /* library code is expected to be mute */\n    static int error_once = 0;\n    if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n      fprintf(stderr, \"LIBXSMM ERROR: invalid signature used to get the scratch memory allocator!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  LIBXSMM_ASSERT(EXIT_SUCCESS == result);\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_set_default_allocator(const void* context,\n  libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  return libxsmm_xset_default_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_default_allocator(const void** context,\n  libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  return libxsmm_xget_default_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_set_scratch_allocator(const void* context,\n  libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  return libxsmm_xset_scratch_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_scratch_allocator(const void** context,\n  libxsmm_malloc_function* malloc_fn, libxsmm_free_function* free_fn)\n{\n  return libxsmm_xget_scratch_allocator(&libxsmm_lock_global, context, malloc_fn, free_fn);\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc_xinfo(const void* memory, size_t* size, int* flags, void** extra)\n{\n  int result;\n#if !defined(NDEBUG)\n  if (NULL != size || NULL != extra)\n#endif\n  {\n    const int check = ((NULL == flags || 0 == (LIBXSMM_MALLOC_FLAG_X & *flags)) ? 2 : 1);\n    const internal_malloc_info_type *const info = internal_malloc_info(memory, check);\n    if (NULL != info) {\n      if (NULL != size) *size = info->size;\n      if (NULL != flags) *flags = info->flags;\n      if (NULL != extra) *extra = info->pointer;\n      result = EXIT_SUCCESS;\n    }\n    else { /* potentially foreign buffer */\n      result = (NULL != memory ? EXIT_FAILURE : EXIT_SUCCESS);\n      if (NULL != size) *size = 0;\n      if (NULL != flags) *flags = 0;\n      if (NULL != extra) *extra = 0;\n    }\n  }\n#if !defined(NDEBUG)\n  else {\n    static int error_once = 0;\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: attachment error for memory buffer %p!\\n\", memory);\n    }\n    LIBXSMM_ASSERT_MSG(0/*false*/, \"LIBXSMM ERROR: attachment error\");\n    result = EXIT_FAILURE;\n  }\n#endif\n  return result;\n}\n\n\n#if !defined(_WIN32)\n\nLIBXSMM_API_INLINE void internal_xmalloc_mhint(void* buffer, size_t size)\n{\n  LIBXSMM_ASSERT((MAP_FAILED != buffer && NULL != buffer) || 0 == size);\n#if defined(_DEFAULT_SOURCE) || defined(_BSD_SOURCE)\n  /* proceed after failed madvise (even in case of an error; take what we got) */\n  /* issue no warning as a failure seems to be related to the kernel version */\n  madvise(buffer, size, MADV_NORMAL/*MADV_RANDOM*/\n# if defined(MADV_NOHUGEPAGE) /* if not available, we then take what we got (THP) */\n    | ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) > size ? MADV_NOHUGEPAGE : 0)\n# endif\n# if defined(MADV_DONTDUMP)\n    | ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) > size ? 0 : MADV_DONTDUMP)\n# endif\n  );\n#else\n  LIBXSMM_UNUSED(buffer); LIBXSMM_UNUSED(size);\n#endif\n}\n\n\nLIBXSMM_API_INLINE void* internal_xmalloc_xmap(const char* dir, size_t size, int flags, void** rx)\n{\n  void* result = MAP_FAILED;\n  char filename[4096] = LIBXSMM_MALLOC_XMAP_TEMPLATE;\n  int i = 0;\n  LIBXSMM_ASSERT(NULL != rx && MAP_FAILED != *rx);\n  if (NULL != dir && 0 != *dir) {\n    i = LIBXSMM_SNPRINTF(filename, sizeof(filename), \"%s/\" LIBXSMM_MALLOC_XMAP_TEMPLATE, dir);\n  }\n  if (0 <= i && i < (int)sizeof(filename)) {\n    /* coverity[secure_temp] */\n    i = mkstemp(filename);\n    if (0 <= i) {\n      if (0 == unlink(filename) && 0 == ftruncate(i, size)) {\n        const int mflags = (flags | LIBXSMM_MAP_SHARED);\n        void *const xmap = mmap(*rx, size, PROT_READ | PROT_EXEC, mflags, i, 0/*offset*/);\n        if (MAP_FAILED != xmap) {\n          LIBXSMM_ASSERT(NULL != xmap);\n          result = mmap(NULL, size, PROT_READ | PROT_WRITE, mflags, i, 0/*offset*/);\n          if (MAP_FAILED != result) {\n            LIBXSMM_ASSERT(NULL != result);\n            internal_xmalloc_mhint(xmap, size);\n            *rx = xmap;\n          }\n          else {\n            munmap(xmap, size);\n            *rx = NULL;\n          }\n        }\n      }\n      close(i);\n    }\n  }\n  return result;\n}\n\n#endif /*!defined(_WIN32)*/\n\n\nLIBXSMM_API_INLINE void* internal_xrealloc(void** ptr, internal_malloc_info_type** info, size_t size,\n  libxsmm_realloc_fun realloc_fn, libxsmm_free_fun free_fn)\n{\n  char *const base = (char*)(NULL != *info ? (*info)->pointer : *ptr), *result;\n  LIBXSMM_ASSERT(NULL != *ptr);\n  /* may implicitly invalidate info */\n  result = (char*)realloc_fn(base, size);\n  if (result == base) { /* signal no-copy */\n    LIBXSMM_ASSERT(NULL != result);\n    *info = NULL; /* no delete */\n    *ptr = NULL; /* no copy */\n  }\n  else if (NULL != result) { /* copy */\n    const size_t offset_src = (const char*)*ptr - base;\n    *ptr = result + offset_src; /* copy */\n    *info = NULL; /* no delete */\n  }\n#if !defined(NDEBUG) && 0\n  else { /* failed */\n    if (NULL != *info) {\n      /* implicitly invalidates info */\n      internal_xfree(*ptr, *info);\n    }\n    else { /* foreign pointer */\n      free_fn(*ptr);\n    }\n    *info = NULL; /* no delete */\n    *ptr = NULL; /* no copy */\n  }\n#else\n  LIBXSMM_UNUSED(free_fn);\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void* internal_xmalloc(void** /*ptr*/, internal_malloc_info_type** /*info*/, size_t /*size*/,\n  const void* /*context*/, libxsmm_malloc_function /*malloc_fn*/, libxsmm_free_function /*free_fn*/);\nLIBXSMM_API_INTERN void* internal_xmalloc(void** ptr, internal_malloc_info_type** info, size_t size,\n  const void* context, libxsmm_malloc_function malloc_fn, libxsmm_free_function free_fn)\n{\n  void* result;\n  LIBXSMM_ASSERT(NULL != ptr && NULL != info && NULL != malloc_fn.function);\n  if (NULL == *ptr) {\n    result = (NULL == context\n      ? malloc_fn.function(size)\n      : malloc_fn.ctx_form(size, context));\n  }\n  else { /* reallocate */\n    if (NULL != free_fn.function /* prefer free_fn since it is part of pointer-info */\n      ? (__real_free == free_fn.function || free == free_fn.function)\n      : (__real_malloc == malloc_fn.function || malloc == malloc_fn.function))\n    {\n#if defined(LIBXSMM_MALLOC_HOOK_REALLOC)\n      result = internal_xrealloc(ptr, info, size, __real_realloc, __real_free);\n#else\n      result = internal_xrealloc(ptr, info, size, realloc, __real_free);\n#endif\n    }\n    else { /* fall-back with regular allocation */\n      result = (NULL == context\n        ? malloc_fn.function(size)\n        : malloc_fn.ctx_form(size, context));\n      if (NULL == result) { /* failed */\n        if (NULL != *info) {\n          internal_xfree(*ptr, *info);\n        }\n        else { /* foreign pointer */\n          (NULL != free_fn.function ? free_fn.function : __real_free)(*ptr);\n        }\n        *ptr = NULL; /* safe delete */\n      }\n    }\n  }\n  return result;\n}\n\n\nLIBXSMM_API_INTERN int libxsmm_xmalloc(void** memory, size_t size, size_t alignment,\n  int flags, const void* extra, size_t extra_size)\n{\n  int result = EXIT_SUCCESS;\n#if !defined(NDEBUG)\n  if (NULL != memory)\n#endif\n  {\n    static int error_once = 0;\n    if (0 != size) {\n      size_t alloc_alignment = 0, alloc_size = 0, max_preserve = 0;\n      internal_malloc_info_type* info = NULL;\n      void* buffer = NULL, * reloc = NULL;\n      /* ATOMIC BEGIN: this region should be atomic/locked */\n      const void* context = libxsmm_default_allocator_context;\n      libxsmm_malloc_function malloc_fn = libxsmm_default_malloc_fn;\n      libxsmm_free_function free_fn = libxsmm_default_free_fn;\n      if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) {\n        context = libxsmm_scratch_allocator_context;\n        malloc_fn = libxsmm_scratch_malloc_fn;\n        free_fn = libxsmm_scratch_free_fn;\n#if defined(LIBXSMM_MALLOC_MMAP_SCRATCH)\n        flags |= LIBXSMM_MALLOC_FLAG_MMAP;\n#endif\n      }\n      if ((0 != (internal_malloc_kind & 1) && 0 < internal_malloc_kind)\n        || NULL == malloc_fn.function || NULL == free_fn.function)\n      {\n        malloc_fn.function = __real_malloc;\n        free_fn.function = __real_free;\n        context = NULL;\n      }\n      /* ATOMIC END: this region should be atomic */\n      flags |= LIBXSMM_MALLOC_FLAG_RW; /* normalize given flags since flags=0 is accepted as well */\n      if (0 != (LIBXSMM_MALLOC_FLAG_REALLOC & flags) && NULL != *memory) {\n        info = internal_malloc_info(*memory, 2/*check*/);\n        if (NULL != info) {\n          max_preserve = info->size;\n        }\n        else { /* reallocation of unknown allocation */\n          flags &= ~LIBXSMM_MALLOC_FLAG_MMAP;\n        }\n      }\n      else *memory = NULL;\n#if !defined(LIBXSMM_MALLOC_MMAP)\n      if (0 == (LIBXSMM_MALLOC_FLAG_X & flags) && 0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n        alloc_alignment = (0 == (LIBXSMM_MALLOC_FLAG_REALLOC & flags) ? libxsmm_alignment(size, alignment) : alignment);\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1;\n        buffer = internal_xmalloc(memory, &info, alloc_size, context, malloc_fn, free_fn);\n      }\n      else\n#endif\n      if (NULL == info || size != info->size) {\n#if defined(_WIN32) ||defined(__CYGWIN__)\n        const int mflags = (0 != (LIBXSMM_MALLOC_FLAG_X & flags) ? PAGE_EXECUTE_READWRITE : PAGE_READWRITE);\n        static SIZE_T alloc_alignmax = 0, alloc_pagesize = 0;\n        if (0 == alloc_alignmax) { /* first/one time */\n          SYSTEM_INFO system_info;\n          GetSystemInfo(&system_info);\n          alloc_pagesize = system_info.dwPageSize;\n          alloc_alignmax = GetLargePageMinimum();\n        }\n        if ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size) { /* attempt to use large pages */\n          HANDLE process_token;\n          alloc_alignment = (NULL == info\n            ? (0 == alignment ? alloc_alignmax : libxsmm_lcm(alignment, alloc_alignmax))\n            : libxsmm_lcm(alignment, alloc_alignmax));\n          alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_alignmax);\n          if (TRUE == OpenProcessToken(GetCurrentProcess(), TOKEN_ADJUST_PRIVILEGES | TOKEN_QUERY, &process_token)) {\n            TOKEN_PRIVILEGES tp;\n            if (TRUE == LookupPrivilegeValue(NULL, TEXT(\"SeLockMemoryPrivilege\"), &tp.Privileges[0].Luid)) {\n              tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED; tp.PrivilegeCount = 1; /* enable privilege */\n              if (TRUE == AdjustTokenPrivileges(process_token, FALSE, &tp, 0, (PTOKEN_PRIVILEGES)NULL, 0)\n                && ERROR_SUCCESS == GetLastError()/*may has failed (regardless of TRUE)*/)\n              {\n                /* VirtualAlloc cannot be used to reallocate memory */\n                buffer = VirtualAlloc(NULL, alloc_size, MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES, mflags);\n              }\n              tp.Privileges[0].Attributes = 0; /* disable privilege */\n              AdjustTokenPrivileges(process_token, FALSE, &tp, 0, (PTOKEN_PRIVILEGES)NULL, 0);\n            }\n            CloseHandle(process_token);\n          }\n        }\n        else { /* small allocation using regular page-size */\n          alloc_alignment = (NULL == info ? libxsmm_alignment(size, alignment) : alignment);\n          alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_pagesize);\n        }\n        if (NULL == buffer) { /* small allocation or retry with regular page size */\n          /* VirtualAlloc cannot be used to reallocate memory */\n          buffer = VirtualAlloc(NULL, alloc_size, MEM_RESERVE | MEM_COMMIT, mflags);\n        }\n        if (NULL != buffer) {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP; /* select the corresponding deallocation */\n        }\n        else if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) { /* fall-back allocation */\n          buffer = internal_xmalloc(memory, &info, alloc_size, context, malloc_fn, free_fn);\n        }\n#else /* !defined(_WIN32) */\n# if defined(MAP_HUGETLB)\n        static size_t limit_hugetlb = LIBXSMM_SCRATCH_UNLIMITED;\n# endif\n# if defined(MAP_LOCKED)\n        static size_t limit_plocked = LIBXSMM_SCRATCH_UNLIMITED;\n# endif\n# if defined(MAP_32BIT)\n        static int map32 = 1;\n# endif\n        int mflags = 0\n# if defined(MAP_UNINITIALIZED) && 0/*fails with WSL*/\n          | MAP_UNINITIALIZED /* unlikely available */\n# endif\n# if defined(MAP_NORESERVE)\n          | (LIBXSMM_MALLOC_ALIGNMAX < size ? 0 : MAP_NORESERVE)\n# endif\n# if defined(MAP_32BIT)\n          | ((0 != (LIBXSMM_MALLOC_FLAG_X & flags) && 0 != map32\n            && LIBXSMM_X86_AVX512_CORE > libxsmm_target_archid\n            && LIBXSMM_X86_AVX512 < libxsmm_target_archid) ? MAP_32BIT : 0)\n# endif\n# if defined(MAP_HUGETLB) /* may fail depending on system settings */\n          | ((0 == (LIBXSMM_MALLOC_FLAG_X & flags)\n            && ((LIBXSMM_MALLOC_ALIGNMAX * LIBXSMM_MALLOC_ALIGNFCT) <= size ||\n              0 != (LIBXSMM_MALLOC_FLAG_PHUGE & flags))\n            && (internal_malloc_hugetlb + size) < limit_hugetlb) ? MAP_HUGETLB : 0)\n# endif\n# if defined(MAP_LOCKED) && !defined(LIBXSMM_MALLOC_LOCK_ONFAULT)\n          | ((0 == (LIBXSMM_MALLOC_FLAG_X & flags)\n            && (internal_malloc_plocked + size) < limit_plocked) ? MAP_LOCKED : 0)\n# endif\n        ; /* mflags */\n# if defined(MAP_POPULATE)\n        { static int prefault = 0;\n          if (0 == prefault) { /* prefault only on Linux 3.10.0-327 (and later) to avoid data race in page-fault handler */\n            struct utsname osinfo; unsigned int version_major = 3, version_minor = 10, version_update = 0, version_patch = 327;\n            if (0 <= uname(&osinfo) && 0 == strcmp(\"Linux\", osinfo.sysname)\n              && 4 == sscanf(osinfo.release, \"%u.%u.%u-%u\", &version_major, &version_minor, &version_update, &version_patch)\n              && LIBXSMM_VERSION4(3, 10, 0, 327) > LIBXSMM_VERSION4(version_major, version_minor, version_update, version_patch))\n            {\n              mflags |= MAP_POPULATE; prefault = 1;\n            }\n            else prefault = -1;\n          }\n          else if (1 == prefault) mflags |= MAP_POPULATE;\n        }\n# endif\n        /* make allocated size at least a multiple of the smallest page-size to avoid split-pages (unmap!) */\n        alloc_alignment = libxsmm_lcm(0 == alignment ? libxsmm_alignment(size, alignment) : alignment, LIBXSMM_PAGE_MINSIZE);\n        alloc_size = LIBXSMM_UP2(size + extra_size + sizeof(internal_malloc_info_type) + alloc_alignment - 1, alloc_alignment);\n        if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* anonymous and non-executable */\n# if defined(MAP_32BIT)\n          LIBXSMM_ASSERT(0 == (MAP_32BIT & mflags));\n# endif\n# if 0\n          LIBXSMM_ASSERT(NULL != info || NULL == *memory); /* no memory mapping of foreign pointer */\n# endif\n          buffer = mmap(NULL == info ? NULL : info->pointer, alloc_size, PROT_READ | PROT_WRITE,\n            MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | mflags, -1, 0/*offset*/);\n# if defined(MAP_HUGETLB)\n          INTERNAL_XMALLOC_KIND(MAP_HUGETLB, \"huge-page\", LIBXSMM_MALLOC_FLAG_PHUGE, flags, mflags,\n            internal_malloc_hugetlb, limit_hugetlb, info, alloc_size, buffer);\n# endif\n# if defined(MAP_LOCKED)\n#   if !defined(LIBXSMM_MALLOC_LOCK_ONFAULT)\n          INTERNAL_XMALLOC_KIND(MAP_LOCKED, \"locked-page\", LIBXSMM_MALLOC_FLAG_PLOCK, flags, mflags,\n            internal_malloc_plocked, limit_plocked, info, alloc_size, buffer);\n#   else\n          if (0 != (MAP_LOCKED & mflags) && MAP_FAILED != buffer) {\n            LIBXSMM_ASSERT(NULL != buffer);\n#     if 0 /* mlock2 is potentially not exposed */\n            if (0 == mlock2(buffer, alloc_size, MLOCK_ONFAULT))\n#     else\n            if (0 == syscall(SYS_mlock2, buffer, alloc_size, MLOCK_ONFAULT))\n#     endif\n            {\n              LIBXSMM_ATOMIC_ADD_FETCH(&internal_malloc_plocked, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              flags |= LIBXSMM_MALLOC_FLAG_PLOCK;\n            }\n            else { /* update watermark */\n              INTERNAL_XMALLOC_WATERMARK(\"locked-page\", internal_malloc_plocked, limit_plocked, alloc_size);\n            }\n          }\n#   endif\n# endif\n        }\n        else { /* executable buffer requested */\n          static /*LIBXSMM_TLS*/ int fallback = -1; /* fall-back allocation method */\n# if defined(MAP_HUGETLB)\n          LIBXSMM_ASSERT(0 == (MAP_HUGETLB & mflags));\n# endif\n# if defined(MAP_LOCKED)\n          LIBXSMM_ASSERT(0 == (MAP_LOCKED & mflags));\n# endif\n          if (0 > (int)LIBXSMM_ATOMIC_LOAD(&fallback, LIBXSMM_ATOMIC_RELAXED)) {\n            const char *const env = getenv(\"LIBXSMM_SE\");\n            LIBXSMM_ATOMIC_STORE(&fallback, NULL == env\n              /* libxsmm_se decides */\n              ? (0 == libxsmm_se ? LIBXSMM_MALLOC_FINAL : LIBXSMM_MALLOC_FALLBACK)\n              /* user's choice takes precedence */\n              : ('0' != *env ? LIBXSMM_MALLOC_FALLBACK : LIBXSMM_MALLOC_FINAL),\n              LIBXSMM_ATOMIC_SEQ_CST);\n            LIBXSMM_ASSERT(0 <= fallback);\n          }\n          INTERNAL_XMALLOC(0, fallback, \"TMPDIR\", \"/tmp\", map32, mflags, alloc_size, buffer, &reloc); /* 1st try */\n          if (1 <= fallback) { /* continue with fall-back */\n            INTERNAL_XMALLOC(1, fallback, \"JITDUMPDIR\", \"\", map32, mflags, alloc_size, buffer, &reloc); /* 2nd try */\n            if (2 <= fallback) { /* continue with fall-back */\n              INTERNAL_XMALLOC(2, fallback, \"HOME\", \"\", map32, mflags, alloc_size, buffer, &reloc); /* 3rd try */\n              if (3 <= fallback) { /* continue with fall-back */\n                if (3 == fallback) { /* 4th try */\n                  buffer = mmap(reloc, alloc_size, PROT_READ | PROT_WRITE | PROT_EXEC,\n# if defined(MAP_32BIT)\n                    MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | (mflags & ~MAP_32BIT),\n# else\n                    MAP_PRIVATE | LIBXSMM_MAP_ANONYMOUS | mflags,\n# endif\n                    -1, 0/*offset*/);\n                  if (MAP_FAILED == buffer) fallback = 4;\n                }\n                if (4 == fallback && MAP_FAILED != buffer) { /* final */\n                  LIBXSMM_ASSERT(fallback == LIBXSMM_MALLOC_FINAL + 1);\n                  buffer = MAP_FAILED; /* trigger final fall-back */\n                }\n              }\n            }\n          }\n        }\n        if (MAP_FAILED != buffer && NULL != buffer) {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP; /* select deallocation */\n        }\n        else { /* allocation failed */\n          if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) { /* ultimate fall-back */\n            buffer = (NULL != malloc_fn.function\n              ? (NULL == context ? malloc_fn.function(alloc_size) : malloc_fn.ctx_form(alloc_size, context))\n              : (NULL));\n          }\n          reloc = NULL;\n        }\n        if (MAP_FAILED != buffer && NULL != buffer) {\n          internal_xmalloc_mhint(buffer, alloc_size);\n        }\n#endif /* !defined(_WIN32) */\n      }\n      else { /* reallocation of the same pointer and size */\n        alloc_size = size + extra_size + sizeof(internal_malloc_info_type) + alignment - 1;\n        if (NULL != info) {\n          buffer = info->pointer;\n          flags |= info->flags;\n        }\n        else {\n          flags |= LIBXSMM_MALLOC_FLAG_MMAP;\n          buffer = *memory;\n        }\n        alloc_alignment = alignment;\n        *memory = NULL; /* signal no-copy */\n      }\n      if (\n#if !defined(_WIN32) && !defined(__clang_analyzer__)\n        MAP_FAILED != buffer &&\n#endif\n        NULL != buffer)\n      {\n        char *const cbuffer = (char*)buffer, *const aligned = LIBXSMM_ALIGN(\n          cbuffer + extra_size + sizeof(internal_malloc_info_type), alloc_alignment);\n        internal_malloc_info_type *const buffer_info = (internal_malloc_info_type*)(\n          aligned - sizeof(internal_malloc_info_type));\n        LIBXSMM_ASSERT((aligned + size) <= (cbuffer + alloc_size));\n        LIBXSMM_ASSERT(0 < alloc_alignment);\n        /* former content must be preserved prior to setup of buffer_info */\n        if (NULL != *memory) { /* preserve/copy previous content */\n#if 0\n          LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_REALLOC & flags));\n#endif\n          /* content behind foreign pointers is not explicitly preserved; buffers may overlap */\n          memmove(aligned, *memory, LIBXSMM_MIN(max_preserve, size));\n          if (NULL != info /* known allocation (non-foreign pointer) */\n            && EXIT_SUCCESS != internal_xfree(*memory, info) /* !libxsmm_free */\n            && 0 != libxsmm_verbosity /* library code is expected to be mute */\n            && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n          { /* display some extra context of the failure (reallocation) */\n            fprintf(stderr, \"LIBXSMM ERROR: memory reallocation failed to release memory!\\n\");\n          }\n        }\n        if (NULL != extra || 0 == extra_size) {\n          const char *const src = (const char*)extra;\n          int i; for (i = 0; i < (int)extra_size; ++i) cbuffer[i] = src[i];\n        }\n        else if (0 != libxsmm_verbosity /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: incorrect extraneous data specification!\\n\");\n          /* no EXIT_FAILURE because valid buffer is returned */\n        }\n        if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* update statistics */\n          if (0 == (LIBXSMM_MALLOC_FLAG_PRIVATE & flags)) { /* public */\n            if (0 != (LIBXSMM_MALLOC_FLAG_SCRATCH & flags)) { /* scratch */\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_public_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_public_max < watermark) internal_malloc_public_max = watermark; /* accept data-race */\n            }\n            else { /* local */\n              const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n                &internal_malloc_local_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n              if (internal_malloc_local_max < watermark) internal_malloc_local_max = watermark; /* accept data-race */\n            }\n          }\n          else { /* private */\n            const size_t watermark = LIBXSMM_ATOMIC(LIBXSMM_ATOMIC_ADD_FETCH, LIBXSMM_BITS)(\n              &internal_malloc_private_cur, alloc_size, LIBXSMM_ATOMIC_RELAXED);\n            if (internal_malloc_private_max < watermark) internal_malloc_private_max = watermark; /* accept data-race */\n          }\n        }\n        /* keep allocation function on record */\n        if (0 == (LIBXSMM_MALLOC_FLAG_MMAP & flags)) {\n          buffer_info->context = context;\n          buffer_info->free = free_fn;\n        }\n        else {\n          buffer_info->free.function = NULL;\n          buffer_info->context = NULL;\n        }\n        buffer_info->size = size; /* record user's size rather than allocated size */\n        buffer_info->pointer = buffer;\n        buffer_info->reloc = reloc;\n        buffer_info->flags = flags;\n#if defined(LIBXSMM_VTUNE)\n        buffer_info->code_id = 0;\n#endif /* info must be initialized to calculate correct checksum */\n#if !defined(LIBXSMM_MALLOC_CRC_OFF)\n# if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n        buffer_info->hash = LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &buffer_info);\n# else\n        buffer_info->hash = libxsmm_crc32(LIBXSMM_MALLOC_SEED, buffer_info,\n          (unsigned int)(((char*)&buffer_info->hash) - ((char*)buffer_info)));\n# endif\n#endif  /* finally commit/return allocated buffer */\n        *memory = aligned;\n      }\n      else {\n        if (0 != libxsmm_verbosity /* library code is expected to be mute */\n         && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          char alloc_size_buffer[32];\n          libxsmm_format_size(alloc_size_buffer, sizeof(alloc_size_buffer), alloc_size, \"KM\", \"B\", 10);\n          fprintf(stderr, \"LIBXSMM ERROR: failed to allocate %s with flag=%i!\\n\", alloc_size_buffer, flags);\n        }\n        result = EXIT_FAILURE;\n        *memory = NULL;\n      }\n    }\n    else {\n      if ((LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM WARNING: zero-sized memory allocation detected!\\n\");\n      }\n      *memory = NULL; /* no EXIT_FAILURE */\n    }\n  }\n#if !defined(NDEBUG)\n  else if (0 != size) {\n    result = EXIT_FAILURE;\n  }\n#endif\n  return result;\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_xfree(const void* memory, int check)\n{\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n  static int error_once = 0;\n#endif\n  /*const*/ internal_malloc_info_type *const info = internal_malloc_info(memory, check);\n  if (NULL != info) { /* !libxsmm_free */\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n    if (EXIT_SUCCESS != internal_xfree(memory, info)) {\n      if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n    }\n#else\n    internal_xfree(memory, info);\n#endif\n  }\n  else if (NULL != memory) {\n#if 1\n    union { const void* const_ptr; void* ptr; } cast;\n    cast.const_ptr = memory; /* C-cast still warns */\n    __real_free(cast.ptr);\n#endif\n#if (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC)) || defined(_DEBUG)\n    if ( 0 != libxsmm_verbosity /* library code is expected to be mute */\n      && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: deallocation does not match allocation!\\n\");\n    }\n#endif\n  }\n}\n\n\n#if defined(LIBXSMM_VTUNE)\nLIBXSMM_API_INLINE void internal_get_vtune_jitdesc(const void* code,\n  unsigned int code_id, size_t code_size, const char* code_name,\n  LIBXSMM_VTUNE_JIT_DESC_TYPE* desc)\n{\n  LIBXSMM_ASSERT(NULL != code && 0 != code_id && 0 != code_size && NULL != desc);\n  desc->method_id = code_id;\n  /* incorrect constness (method_name) */\n  desc->method_name = (char*)code_name;\n  /* incorrect constness (method_load_address) */\n  desc->method_load_address = (void*)code;\n  desc->method_size = code_size;\n  desc->line_number_size = 0;\n  desc->line_number_table = NULL;\n  desc->class_file_name = NULL;\n  desc->source_file_name = NULL;\n# if (2 <= LIBXSMM_VTUNE_JITVERSION)\n  desc->module_name = \"libxsmm.jit\";\n# endif\n}\n#endif\n\n\nLIBXSMM_API_INTERN int libxsmm_malloc_attrib(void** memory, int flags, const char* name)\n{\n  internal_malloc_info_type *const info = (NULL != memory ? internal_malloc_info(*memory, 0/*no check*/) : NULL);\n  int result = EXIT_SUCCESS;\n  static int error_once = 0;\n  if (NULL != info) {\n    void *const buffer = info->pointer;\n    const size_t size = info->size;\n#if defined(_WIN32)\n    LIBXSMM_ASSERT(NULL != buffer || 0 == size);\n#else\n    LIBXSMM_ASSERT((NULL != buffer && MAP_FAILED != buffer) || 0 == size);\n#endif\n    flags |= (info->flags & ~LIBXSMM_MALLOC_FLAG_RWX); /* merge with current flags */\n    /* quietly keep the read permission, but eventually revoke write permissions */\n    if (0 == (LIBXSMM_MALLOC_FLAG_W & flags) || 0 != (LIBXSMM_MALLOC_FLAG_X & flags)) {\n      const size_t alignment = (size_t)(((const char*)(*memory)) - ((const char*)buffer));\n      const size_t alloc_size = size + alignment;\n      if (0 == (LIBXSMM_MALLOC_FLAG_X & flags)) { /* data-buffer; non-executable */\n#if defined(_WIN32)\n        /* TODO: implement memory protection under Microsoft Windows */\n        LIBXSMM_UNUSED(alloc_size);\n#else\n        if (EXIT_SUCCESS != mprotect(buffer, alloc_size/*entire memory region*/, PROT_READ)\n          && (LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n          && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM WARNING: read-only request for buffer failed!\\n\");\n        }\n#endif\n      }\n      else { /* executable buffer requested */\n        void *const code_ptr = NULL != info->reloc ? ((void*)(((char*)info->reloc) + alignment)) : *memory;\n        LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_X & flags));\n        if (name && *name) { /* profiler support requested */\n          if (0 > libxsmm_verbosity) { /* avoid dump when only the profiler is enabled */\n            FILE* code_file = fopen(name, \"rb\");\n            int diff = 0;\n            if (NULL == code_file) { /* file does not exist */\n              code_file = fopen(name, \"wb\");\n              if (NULL != code_file) { /* dump byte-code into a file */\n                fwrite(code_ptr, 1, size, code_file);\n                fclose(code_file);\n              }\n            }\n            else { /* check existing file */\n              const char* check_a = (const char*)code_ptr;\n              char check_b[4096];\n              size_t rest = size;\n              do {\n                const size_t n = fread(check_b, 1, LIBXSMM_MIN(sizeof(check_b), rest), code_file);\n                diff += memcmp(check_a, check_b, LIBXSMM_MIN(sizeof(check_b), n));\n                check_a += n;\n                rest -= n;\n              } while (0 < rest && 0 == diff);\n              fclose(code_file);\n            }\n            fprintf(stderr, \"LIBXSMM-JIT-DUMP(ptr:file) %p : %s\\n\", code_ptr, name);\n            if (0 != diff) { /* override existing dump and warn about erroneous condition */\n              fprintf(stderr, \"LIBXSMM ERROR: %s is shared by different code!\\n\", name);\n              code_file = fopen(name, \"wb\");\n              if (NULL != code_file) { /* dump byte-code into a file */\n                fwrite(code_ptr, 1, size, code_file);\n                fclose(code_file);\n              }\n            }\n          }\n#if defined(LIBXSMM_VTUNE)\n          if (iJIT_SAMPLING_ON == iJIT_IsProfilingActive()) {\n            LIBXSMM_VTUNE_JIT_DESC_TYPE vtune_jit_desc;\n            const unsigned int code_id = iJIT_GetNewMethodID();\n            internal_get_vtune_jitdesc(code_ptr, code_id, size, name, &vtune_jit_desc);\n            iJIT_NotifyEvent(LIBXSMM_VTUNE_JIT_LOAD, &vtune_jit_desc);\n            info->code_id = code_id;\n          }\n          else {\n            info->code_id = 0;\n          }\n#endif\n#if defined(LIBXSMM_PERF)\n          /* If JIT is enabled and a valid name is given, emit information for profiler\n           * In jitdump case this needs to be done after mprotect as it gets overwritten\n           * otherwise. */\n          libxsmm_perf_dump_code(code_ptr, size, name);\n#endif\n        }\n        if (NULL != info->reloc && info->pointer != info->reloc) {\n#if defined(_WIN32)\n          /* TODO: implement memory protection under Microsoft Windows */\n#else\n          /* memory is already protected at this point; relocate code */\n          LIBXSMM_ASSERT(0 != (LIBXSMM_MALLOC_FLAG_MMAP & flags));\n          *memory = code_ptr; /* relocate */\n          info->pointer = info->reloc;\n          info->reloc = NULL;\n# if !defined(LIBXSMM_MALLOC_CRC_OFF) /* update checksum */\n#   if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n          { const internal_malloc_info_type *const code_info = internal_malloc_info(code_ptr, 0/*no check*/);\n            info->hash = LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &code_info);\n          }\n#   else\n          info->hash = libxsmm_crc32(LIBXSMM_MALLOC_SEED, info,\n            /* info size minus actual hash value */\n            (unsigned int)(((char*)&info->hash) - ((char*)info)));\n#   endif\n# endif   /* treat memory protection errors as soft error; ignore return value */\n          munmap(buffer, alloc_size);\n#endif\n        }\n#if !defined(_WIN32)\n        else { /* malloc-based fall-back */\n          int mprotect_result;\n# if !defined(LIBXSMM_MALLOC_CRC_OFF) && defined(LIBXSMM_VTUNE) /* check checksum */\n#   if defined(LIBXSMM_MALLOC_CRC_LIGHT)\n          assert(info->hash == LIBXSMM_CRC32U(LIBXSMM_BITS)(LIBXSMM_MALLOC_SEED, &info)); /* !LIBXSMM_ASSERT */\n#   else\n          assert(info->hash == libxsmm_crc32(LIBXSMM_MALLOC_SEED, info, /* !LIBXSMM_ASSERT */\n            /* info size minus actual hash value */\n            (unsigned int)(((char*)&info->hash) - ((char*)info))));\n#   endif\n# endif   /* treat memory protection errors as soft error; ignore return value */\n          mprotect_result = mprotect(buffer, alloc_size/*entire memory region*/, PROT_READ | PROT_EXEC);\n          if (EXIT_SUCCESS != mprotect_result) {\n            if (0 != libxsmm_se) { /* hard-error in case of SELinux */\n              if (0 != libxsmm_verbosity /* library code is expected to be mute */\n                && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n              {\n                fprintf(stderr, \"LIBXSMM ERROR: failed to allocate an executable buffer!\\n\");\n              }\n              result = mprotect_result;\n            }\n            else if ((LIBXSMM_VERBOSITY_HIGH <= libxsmm_verbosity || 0 > libxsmm_verbosity) /* library code is expected to be mute */\n              && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n            {\n              fprintf(stderr, \"LIBXSMM WARNING: read-only request for JIT-buffer failed!\\n\");\n            }\n          }\n        }\n#endif\n      }\n    }\n  }\n  else if (NULL == memory || NULL == *memory) {\n    if (0 != libxsmm_verbosity /* library code is expected to be mute */\n     && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n    {\n      fprintf(stderr, \"LIBXSMM ERROR: libxsmm_malloc_attrib failed because NULL cannot be attributed!\\n\");\n    }\n    result = EXIT_FAILURE;\n  }\n  else if ((LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity)\n    && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n  {\n    fprintf(stderr, \"LIBXSMM WARNING: %s buffer %p does not match!\\n\",\n      0 != (LIBXSMM_MALLOC_FLAG_X & flags) ? \"executable\" : \"memory\", *memory);\n  }\n  return result;\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_MALLOC void* libxsmm_aligned_malloc(size_t size, size_t alignment)\n{\n  void* result = NULL;\n  LIBXSMM_INIT\n  if (2 > internal_malloc_kind) {\n#if !defined(NDEBUG)\n    int status =\n#endif\n    libxsmm_xmalloc(&result, size, alignment, LIBXSMM_MALLOC_FLAG_DEFAULT, NULL/*extra*/, 0/*extra_size*/);\n    assert(EXIT_SUCCESS == status || NULL == result); /* !LIBXSMM_ASSERT */\n  }\n  else { /* scratch */\n    const void *const caller = libxsmm_trace_caller_id(0/*level*/);\n    internal_scratch_malloc(&result, size, alignment, LIBXSMM_MALLOC_FLAG_DEFAULT, caller);\n  }\n  return result;\n}\n\n\nLIBXSMM_API void* libxsmm_realloc(size_t size, void* ptr)\n{\n  const int nzeros = LIBXSMM_INTRINSICS_BITSCANFWD64((uintptr_t)ptr), alignment = 1 << nzeros;\n  LIBXSMM_ASSERT(0 == ((uintptr_t)ptr & ~(0xFFFFFFFFFFFFFFFF << nzeros)));\n  LIBXSMM_INIT\n  if (2 > internal_malloc_kind) {\n#if !defined(NDEBUG)\n    int status =\n#endif\n    libxsmm_xmalloc(&ptr, size, alignment, LIBXSMM_MALLOC_FLAG_REALLOC, NULL/*extra*/, 0/*extra_size*/);\n    assert(EXIT_SUCCESS == status || NULL == ptr); /* !LIBXSMM_ASSERT */\n  }\n  else { /* scratch */\n    const void *const caller = libxsmm_trace_caller_id(0/*level*/);\n    internal_scratch_malloc(&ptr, size, alignment, LIBXSMM_MALLOC_FLAG_REALLOC, caller);\n  }\n  return ptr;\n}\n\n\nLIBXSMM_API void* libxsmm_scratch_malloc(size_t size, size_t alignment, const void* caller)\n{\n  void* result;\n  LIBXSMM_INIT\n  internal_scratch_malloc(&result, size, alignment,\n    LIBXSMM_MALLOC_INTERNAL_CALLER != caller ? LIBXSMM_MALLOC_FLAG_DEFAULT : LIBXSMM_MALLOC_FLAG_PRIVATE,\n    caller);\n  return result;\n}\n\n\nLIBXSMM_API LIBXSMM_ATTRIBUTE_MALLOC void* libxsmm_malloc(size_t size)\n{\n  return libxsmm_aligned_malloc(size, 0/*auto*/);\n}\n\n\nLIBXSMM_API void libxsmm_free(const void* memory)\n{\n  if (NULL != memory) {\n#if defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST) || /* prefer safe method if possible */ \\\n  (!defined(LIBXSMM_MALLOC_HOOK_STATIC) && !defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(memory);\n    if (NULL != pool) { /* memory belongs to scratch domain */\n      internal_scratch_free(memory, pool);\n    }\n    else\n# endif\n    { /* local */\n      libxsmm_xfree(memory, 2/*check*/);\n    }\n#else /* lookup matching pool */\n    internal_malloc_info_type *const info = internal_malloc_info(memory, 2/*check*/);\n    static int error_once = 0;\n    if (NULL != info && 0 == (LIBXSMM_MALLOC_FLAG_SCRATCH & info->flags)) { /* !libxsmm_free */\n# if !defined(NDEBUG)\n      if (EXIT_SUCCESS != internal_xfree(memory, info)\n        && 0 != libxsmm_verbosity /* library code is expected to be mute */\n        && 1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n      {\n        fprintf(stderr, \"LIBXSMM ERROR: memory deallocation failed!\\n\");\n      }\n# else\n      internal_xfree(memory, info); /* !libxsmm_free */\n# endif\n    }\n    else {\n# if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      internal_malloc_pool_type *const pool = internal_scratch_malloc_pool(memory);\n      if (NULL != pool) { /* memory belongs to scratch domain */\n        internal_scratch_free(memory, pool);\n      }\n      else\n# endif\n      {\n# if defined(NDEBUG) && (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n        __real_free((void*)memory);\n# else\n#   if (defined(LIBXSMM_MALLOC_HOOK_STATIC) || defined(LIBXSMM_MALLOC_HOOK_DYNAMIC))\n        __real_free((void*)memory);\n#   endif\n        if (0 != libxsmm_verbosity && /* library code is expected to be mute */\n            1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED))\n        {\n          fprintf(stderr, \"LIBXSMM ERROR: deallocation does not match allocation!\\n\");\n        }\n# endif\n      }\n    }\n#endif\n  }\n}\n\n\nLIBXSMM_API_INTERN void libxsmm_xrelease_scratch(LIBXSMM_LOCK_TYPE(LIBXSMM_LOCK)* lock)\n{\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n  internal_malloc_pool_type* pools = NULL;\n  libxsmm_scratch_info scratch_info;\n  LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n  if (NULL != lock) {\n    LIBXSMM_LOCK_ACQUIRE(LIBXSMM_LOCK, lock);\n  }\n# if defined(LIBXSMM_MALLOC_DELETE_SAFE)\n  if (0 == (internal_malloc_kind & 1) || 0 >= internal_malloc_kind)\n# endif\n  {\n    unsigned int i;\n    pools = (internal_malloc_pool_type*)LIBXSMM_UP2(\n      (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n    for (i = 0; i < libxsmm_scratch_pools; ++i) {\n      if (0 != pools[i].instance.minsize) {\n        if (\n# if !defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST)\n          1 < pools[i].instance.counter &&\n# endif\n          NULL != pools[i].instance.buffer)\n        {\n          internal_malloc_info_type* const info = internal_malloc_info(pools[i].instance.buffer, 2/*check*/);\n          if (NULL != info) internal_xfree(info->pointer, info);\n        }\n      }\n      else break; /* early exit */\n    }\n  }\n  LIBXSMM_EXPECT(EXIT_SUCCESS, libxsmm_get_scratch_info(&scratch_info));\n  if (0 != scratch_info.npending && /* library code is expected to be mute */\n    (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity))\n  {\n    char pending_size_buffer[32];\n    libxsmm_format_size(pending_size_buffer, sizeof(pending_size_buffer),\n      internal_malloc_public_cur + internal_malloc_local_cur, \"KM\", \"B\", 10);\n    fprintf(stderr, \"LIBXSMM WARNING: %s pending scratch-memory by %\" PRIuPTR \" allocation%s!\\n\",\n      pending_size_buffer, (uintptr_t)scratch_info.npending, 1 < scratch_info.npending ? \"s\" : \"\");\n  }\n  if (NULL != pools) {\n    memset(pools, 0, (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) * sizeof(internal_malloc_pool_type));\n    /* no reset: keep private watermark (internal_malloc_private_max, internal_malloc_private_cur) */\n    internal_malloc_public_max = internal_malloc_public_cur = 0;\n    internal_malloc_local_max = internal_malloc_local_cur = 0;\n    internal_malloc_scratch_nmallocs = 0;\n  }\n  if (NULL != lock) {\n    LIBXSMM_LOCK_RELEASE(LIBXSMM_LOCK, lock);\n  }\n#endif\n}\n\n\nLIBXSMM_API void libxsmm_release_scratch(void)\n{\n  libxsmm_xrelease_scratch(&libxsmm_lock_global);\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc_info(const void* memory, libxsmm_malloc_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != info) {\n    size_t size;\n    result = libxsmm_get_malloc_xinfo(memory, &size, NULL/*flags*/, NULL/*extra*/);\n    LIBXSMM_MEMZERO127(info);\n    if (EXIT_SUCCESS == result) {\n      info->size = size;\n    }\n#if !defined(NDEBUG) /* library code is expected to be mute */\n    else if (LIBXSMM_VERBOSITY_WARN <= libxsmm_verbosity || 0 > libxsmm_verbosity) {\n      static int error_once = 0;\n      if (1 == LIBXSMM_ATOMIC_ADD_FETCH(&error_once, 1, LIBXSMM_ATOMIC_RELAXED)) {\n        fprintf(stderr, \"LIBXSMM WARNING: foreign memory buffer %p discovered!\\n\", memory);\n      }\n    }\n#endif\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API int libxsmm_get_scratch_info(libxsmm_scratch_info* info)\n{\n  int result = EXIT_SUCCESS;\n  if (NULL != info) {\n#if defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n    LIBXSMM_MEMZERO127(info);\n    info->nmallocs = internal_malloc_scratch_nmallocs;\n    info->internal = internal_malloc_private_max;\n    info->local = internal_malloc_local_max;\n    info->size = internal_malloc_public_max;\n    { const internal_malloc_pool_type* pool = (const internal_malloc_pool_type*)LIBXSMM_UP2(\n        (uintptr_t)internal_malloc_pool_buffer, LIBXSMM_MALLOC_SCRATCH_PADDING);\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n      const internal_malloc_pool_type *const end = pool + libxsmm_scratch_pools;\n      LIBXSMM_ASSERT(libxsmm_scratch_pools <= LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS);\n      for (; pool != end; ++pool) if ((LIBXSMM_MALLOC_INTERNAL_CALLER) != pool->instance.site) {\n# endif\n        if (0 != pool->instance.minsize) {\n          const size_t npending = pool->instance.counter;\n# if defined(LIBXSMM_MALLOC_SCRATCH_DELETE_FIRST)\n          info->npending += npending;\n# else\n          info->npending += 1 < npending ? (npending - 1) : 0;\n# endif\n          ++info->npools;\n        }\n# if (1 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))\n        else break; /* early exit */\n      }\n# endif\n    }\n#else\n    LIBXSMM_MEMZERO127(info);\n#endif /*defined(LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS) && (0 < (LIBXSMM_MALLOC_SCRATCH_MAX_NPOOLS))*/\n  }\n  else {\n    result = EXIT_FAILURE;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_set_scratch_limit(size_t nbytes)\n{\n  /* !LIBXSMM_INIT */\n  internal_malloc_scratch_limit = nbytes;\n}\n\n\nLIBXSMM_API size_t libxsmm_get_scratch_limit(void)\n{\n  size_t result;\n  /* !LIBXSMM_INIT */\n  if (LIBXSMM_SCRATCH_DEFAULT != internal_malloc_scratch_limit) {\n    result = internal_malloc_scratch_limit;\n  }\n  else if (0 == internal_malloc_kind) {\n    result = LIBXSMM_MALLOC_SCRATCH_LIMIT;\n  }\n  else {\n    result = LIBXSMM_SCRATCH_UNLIMITED;\n  }\n  return result;\n}\n\n\nLIBXSMM_API void libxsmm_set_malloc(int enabled, const size_t* lo, const size_t* hi)\n{\n  /* !LIBXSMM_INIT */\n#if !(defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) || defined(LIBXSMM_INTERCEPT_DYNAMIC))\n  LIBXSMM_UNUSED(enabled);\n  internal_malloc_kind = 0;\n#elif defined(LIBXSMM_MALLOC) && (0 < LIBXSMM_MALLOC)\n  LIBXSMM_UNUSED(enabled);\n  internal_malloc_kind = LIBXSMM_MALLOC;\n#else\n  internal_malloc_kind = enabled;\n#endif\n  /* setup lo/hi after internal_malloc_kind! */\n  if (NULL != lo) internal_malloc_limit[0] = *lo;\n  if (NULL != hi) {\n    const size_t scratch_limit = libxsmm_get_scratch_limit();\n    const size_t malloc_upper = LIBXSMM_MIN(*hi, scratch_limit);\n    internal_malloc_limit[1] = LIBXSMM_MAX(malloc_upper, internal_malloc_limit[0]);\n  }\n  libxsmm_malloc_init();\n}\n\n\nLIBXSMM_API int libxsmm_get_malloc(size_t* lo, size_t* hi)\n{\n  int result;\n  LIBXSMM_INIT\n  if (NULL != lo) *lo = internal_malloc_limit[0];\n  if (NULL != hi) *hi = internal_malloc_limit[1];\n#if (defined(LIBXSMM_MALLOC_HOOK_DYNAMIC) || defined(LIBXSMM_INTERCEPT_DYNAMIC))\n  result = 0 != (internal_malloc_kind & 1) && 0 < internal_malloc_kind;\n#else\n  result = 0;\n#endif\n  return result;\n}\n\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm_mm.docx",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm_prof-vtune.png",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/tensorflow.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/documentation/libxsmm_samples.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/tests/mhd_image.raw",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/samples/magazine/magazine.docx",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/samples/utilities/mhd/mhd_in.mhd",
        "/tmp/vanessa/spack-stage/spack-stage-libxsmm-1.16-cfijcte6wgq2lnzunskfukeaqwnzvuvj/spack-src/samples/deeplearning/gxm/model_zoo/cifar10/mean.binaryproto"
    ],
    "total_files": 875
}