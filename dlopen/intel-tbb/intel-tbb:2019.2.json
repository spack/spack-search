{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/build/android.inc": "# Copyright (c) 2005-2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n#\n#\n\n#\n# Extra gmake command-line parameters for use with Android:\n#\n#    dlopen_workaround:  Some OS versions need workaround for dlopen to avoid recursive calls.\n#\n\n####### Detections and Commands ###############################################\n\nifeq (android,$(findstring android,$(tbb_os)))\n    $(error TBB only supports cross-compilation for Android. Specify \"target=android\" instead.)\nendif\n\nifndef BUILDING_PHASE\n    ifneq (\"command line\",\"$(origin arch)\")\n        ifeq (icc,$(compiler))\n            export COMPILER_VERSION := ICC: $(shell icc -V </dev/null 2>&1 | grep 'Version')\n            ifneq (,$(findstring running on IA-32, $(COMPILER_VERSION)))\n                export arch:=ia32\n            else ifneq (,$(findstring running on Intel(R) 64, $(COMPILER_VERSION)))\n                export arch:=intel64\n            else\n                $(error \"No support for Android in $(COMPILER_VERSION)\")\n            endif\n\n        else\n            ifdef ANDROID_SERIAL\n                uname_m:=$(shell adb shell uname -m)\n                ifeq (i686,$(uname_m))\n                    export arch:=ia32\n                else\n                    export arch:=$(uname_m)\n                endif\n            endif\n        endif\n    endif\nendif\n\nifeq (\"$(arch)\",\"\")\n    $(error \"No target architecture specified and \\'ANDROID_SERIAL\\' environment variable specifying target device not set\")\nendif\n\n# Android platform only supported from TBB 4.1 forward\nNO_LEGACY_TESTS = 1\n\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/tbb/itt_notify.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#if DO_ITT_NOTIFY\n\n#if _WIN32||_WIN64\n    #ifndef UNICODE\n        #define UNICODE\n    #endif\n#else\n    #pragma weak dlopen\n    #pragma weak dlsym\n    #pragma weak dlerror\n#endif /* WIN */\n\n#if __TBB_BUILD\n\nextern \"C\" void ITT_DoOneTimeInitialization();\n#define __itt_init_ittlib_name(x,y) (ITT_DoOneTimeInitialization(), true)\n\n#elif __TBBMALLOC_BUILD\n\nextern \"C\" void MallocInitializeITT();\n#define __itt_init_ittlib_name(x,y) (MallocInitializeITT(), true)\n\n#else\n#error This file is expected to be used for either TBB or TBB allocator build.\n#endif // __TBB_BUILD\n\n#include \"tools_api/ittnotify_static.c\"\n\nnamespace tbb {\nnamespace internal {\nint __TBB_load_ittnotify() {\n#if !(_WIN32||_WIN64)\n    // tool_api crashes without dlopen, check that it's present. Common case\n    // for lack of dlopen is static binaries, i.e. ones build with -static.\n    if (dlopen == NULL)\n        return 0;\n#endif\n    return __itt_init_ittlib(NULL,          // groups for:\n      (__itt_group_id)(__itt_group_sync     // prepare/cancel/acquired/releasing\n                       | __itt_group_thread // name threads\n                       | __itt_group_stitch // stack stitching\n                       | __itt_group_structure\n                           ));\n}\n\n}} // namespaces\n\n#endif /* DO_ITT_NOTIFY */\n\n#define __TBB_NO_IMPLICIT_LINKAGE 1\n#include \"itt_notify.h\"\n\nnamespace tbb {\n\n#if DO_ITT_NOTIFY\n    const tchar\n            *SyncType_GlobalLock = _T(\"TbbGlobalLock\"),\n            *SyncType_Scheduler = _T(\"%Constant\")\n            ;\n    const tchar\n            *SyncObj_SchedulerInitialization = _T(\"TbbSchedulerInitialization\"),\n            *SyncObj_SchedulersList = _T(\"TbbSchedulersList\"),\n            *SyncObj_WorkerLifeCycleMgmt = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskStealingLoop = _T(\"TBB Scheduler\"),\n            *SyncObj_WorkerTaskPool = _T(\"TBB Scheduler\"),\n            *SyncObj_MasterTaskPool = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskPoolSpinning = _T(\"TBB Scheduler\"),\n            *SyncObj_Mailbox = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskReturnList = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskStream = _T(\"TBB Scheduler\"),\n            *SyncObj_ContextsList = _T(\"TBB Scheduler\")\n            ;\n#endif /* DO_ITT_NOTIFY */\n\n} // namespace tbb\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/tbb/dynamic_link.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#include \"dynamic_link.h\"\n#include \"tbb/tbb_config.h\"\n\n/*\n    This file is used by both TBB and OpenMP RTL. Do not use __TBB_ASSERT() macro\n    and runtime_warning() function because they are not available in OpenMP. Use\n    LIBRARY_ASSERT and DYNAMIC_LINK_WARNING instead.\n*/\n\n#include <cstdarg>          // va_list etc.\n#if _WIN32\n    #include <malloc.h>\n\n    // Unify system calls\n    #define dlopen( name, flags )   LoadLibrary( name )\n    #define dlsym( handle, name )   GetProcAddress( handle, name )\n    #define dlclose( handle )       ( ! FreeLibrary( handle ) )\n    #define dlerror()               GetLastError()\n#ifndef PATH_MAX\n    #define PATH_MAX                MAX_PATH\n#endif\n#else /* _WIN32 */\n    #include <dlfcn.h>\n    #include <string.h>\n    #include <unistd.h>\n    #include <limits.h>\n    #include <stdlib.h>\n#endif /* _WIN32 */\n\n#if __TBB_WEAK_SYMBOLS_PRESENT && !__TBB_DYNAMIC_LOAD_ENABLED\n    //TODO: use function attribute for weak symbols instead of the pragma.\n    #pragma weak dlopen\n    #pragma weak dlsym\n    #pragma weak dlclose\n#endif /* __TBB_WEAK_SYMBOLS_PRESENT && !__TBB_DYNAMIC_LOAD_ENABLED */\n\n#include \"tbb_misc.h\"\n\n#define __USE_TBB_ATOMICS       ( !(__linux__&&__ia64__) || __TBB_BUILD )\n#define __USE_STATIC_DL_INIT    ( !__ANDROID__ )\n\n#if !__USE_TBB_ATOMICS\n#include <pthread.h>\n#endif\n\n/*\ndynamic_link is a common interface for searching for required symbols in an\nexecutable and dynamic libraries.\n\ndynamic_link provides certain guarantees:\n  1. Either all or none of the requested symbols are resolved. Moreover, if\n  symbols are not resolved, the dynamic_link_descriptor table is not modified;\n  2. All returned symbols have secured lifetime: this means that none of them\n  can be invalidated until dynamic_unlink is called;\n  3. Any loaded library is loaded only via the full path. The full path is that\n  from which the runtime itself was loaded. (This is done to avoid security\n  issues caused by loading libraries from insecure paths).\n\ndynamic_link searches for the requested symbols in three stages, stopping as\nsoon as all of the symbols have been resolved.\n\n  1. Search the global scope:\n    a. On Windows: dynamic_link tries to obtain the handle of the requested\n    library and if it succeeds it resolves the symbols via that handle.\n    b. On Linux: dynamic_link tries to search for the symbols in the global\n    scope via the main program handle. If the symbols are present in the global\n    scope their lifetime is not guaranteed (since dynamic_link does not know\n    anything about the library from which they are exported). Therefore it\n    tries to \"pin\" the symbols by obtaining the library name and reopening it.\n    dlopen may fail to reopen the library in two cases:\n       i. The symbols are exported from the executable. Currently dynamic _link\n      cannot handle this situation, so it will not find these symbols in this\n      step.\n      ii. The necessary library has been unloaded and cannot be reloaded. It\n      seems there is nothing that can be done in this case. No symbols are\n      returned.\n\n  2. Dynamic load: an attempt is made to load the requested library via the\n  full path.\n    The full path used is that from which the runtime itself was loaded. If the\n    library can be loaded, then an attempt is made to resolve the requested\n    symbols in the newly loaded library.\n    If the symbols are not found the library is unloaded.\n\n  3. Weak symbols: if weak symbols are available they are returned.\n*/\n\nOPEN_INTERNAL_NAMESPACE\n\n#if __TBB_WEAK_SYMBOLS_PRESENT || __TBB_DYNAMIC_LOAD_ENABLED\n\n#if !defined(DYNAMIC_LINK_WARNING) && !__TBB_WIN8UI_SUPPORT && __TBB_DYNAMIC_LOAD_ENABLED\n    // Report runtime errors and continue.\n    #define DYNAMIC_LINK_WARNING dynamic_link_warning\n    static void dynamic_link_warning( dynamic_link_error_t code, ... ) {\n        (void) code;\n    } // library_warning\n#endif /* !defined(DYNAMIC_LINK_WARNING) && !__TBB_WIN8UI_SUPPORT && __TBB_DYNAMIC_LOAD_ENABLED */\n\n    static bool resolve_symbols( dynamic_link_handle module, const dynamic_link_descriptor descriptors[], size_t required )\n    {\n        if ( !module )\n            return false;\n\n        #if !__TBB_DYNAMIC_LOAD_ENABLED /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n            if ( !dlsym ) return false;\n        #endif /* !__TBB_DYNAMIC_LOAD_ENABLED */\n\n        const size_t n_desc=20; // Usually we don't have more than 20 descriptors per library\n        LIBRARY_ASSERT( required <= n_desc, \"Too many descriptors is required\" );\n        if ( required > n_desc ) return false;\n        pointer_to_handler h[n_desc];\n\n        for ( size_t k = 0; k < required; ++k ) {\n            dynamic_link_descriptor const & desc = descriptors[k];\n            pointer_to_handler addr = (pointer_to_handler)dlsym( module, desc.name );\n            if ( !addr ) {\n                return false;\n            }\n            h[k] = addr;\n        }\n\n        // Commit the entry points.\n        // Cannot use memset here, because the writes must be atomic.\n        for( size_t k = 0; k < required; ++k )\n            *descriptors[k].handler = h[k];\n        return true;\n    }\n\n#if __TBB_WIN8UI_SUPPORT\n    bool dynamic_link( const char*  library, const dynamic_link_descriptor descriptors[], size_t required, dynamic_link_handle*, int flags ) {\n        dynamic_link_handle tmp_handle = NULL;\n        TCHAR wlibrary[256];\n        if ( MultiByteToWideChar(CP_UTF8, 0, library, -1, wlibrary, 255) == 0 ) return false;\n        if ( flags & DYNAMIC_LINK_LOAD )\n            tmp_handle = LoadPackagedLibrary( wlibrary, 0 );\n        if (tmp_handle != NULL){\n            return resolve_symbols(tmp_handle, descriptors, required);\n        }else{\n            return false;\n        }\n    }\n    void dynamic_unlink( dynamic_link_handle ) {}\n    void dynamic_unlink_all() {}\n#else\n#if __TBB_DYNAMIC_LOAD_ENABLED\n/*\n    There is a security issue on Windows: LoadLibrary() may load and execute malicious code.\n    See http://www.microsoft.com/technet/security/advisory/2269637.mspx for details.\n    To avoid the issue, we have to pass full path (not just library name) to LoadLibrary. This\n    function constructs full path to the specified library (it is assumed the library located\n    side-by-side with the tbb.dll.\n\n    The function constructs absolute path for given relative path. Important: Base directory is not\n    current one, it is the directory tbb.dll loaded from.\n\n    Example:\n        Let us assume \"tbb.dll\" is located in \"c:\\program files\\common\\intel\\\" directory, e. g.\n        absolute path of tbb library is \"c:\\program files\\common\\intel\\tbb.dll\". Absolute path for\n        \"tbbmalloc.dll\" would be \"c:\\program files\\common\\intel\\tbbmalloc.dll\". Absolute path for\n        \"malloc\\tbbmalloc.dll\" would be \"c:\\program files\\common\\intel\\malloc\\tbbmalloc.dll\".\n*/\n\n    // Struct handle_storage is used by dynamic_link routine to store handles of\n    // all loaded or pinned dynamic libraries. When TBB is shut down, it calls\n    // dynamic_unlink_all() that unloads modules referenced by handle_storage.\n    // This struct should not have any constructors since it may be used before\n    // the constructor is called.\n    #define MAX_LOADED_MODULES 8 // The number of maximum possible modules which can be loaded\n\n#if __USE_TBB_ATOMICS\n    typedef ::tbb::atomic<size_t> atomic_incrementer;\n    void init_atomic_incrementer( atomic_incrementer & ) {}\n\n    static void atomic_once( void( *func ) (void), tbb::atomic< tbb::internal::do_once_state > &once_state ) {\n        tbb::internal::atomic_do_once( func, once_state );\n    }\n    #define ATOMIC_ONCE_DECL( var ) tbb::atomic< tbb::internal::do_once_state > var\n#else\n    static void pthread_assert( int error_code, const char* msg ) {\n        LIBRARY_ASSERT( error_code == 0, msg );\n    }\n\n    class atomic_incrementer {\n        size_t my_val;\n        pthread_spinlock_t my_lock;\n    public:\n        void init() {\n            my_val = 0;\n            pthread_assert( pthread_spin_init( &my_lock, PTHREAD_PROCESS_PRIVATE ), \"pthread_spin_init failed\" );\n        }\n        size_t operator++(int) {\n            pthread_assert( pthread_spin_lock( &my_lock ), \"pthread_spin_lock failed\" );\n            size_t prev_val = my_val++;\n            pthread_assert( pthread_spin_unlock( &my_lock ), \"pthread_spin_unlock failed\" );\n            return prev_val;\n        }\n        operator size_t() {\n            pthread_assert( pthread_spin_lock( &my_lock ), \"pthread_spin_lock failed\" );\n            size_t val = my_val;\n            pthread_assert( pthread_spin_unlock( &my_lock ), \"pthread_spin_unlock failed\" );\n            return val;\n        }\n        ~atomic_incrementer() {\n            pthread_assert( pthread_spin_destroy( &my_lock ), \"pthread_spin_destroy failed\" );\n        }\n    };\n\n    void init_atomic_incrementer( atomic_incrementer &r ) {\n        r.init();\n    }\n\n    static void atomic_once( void( *func ) (), pthread_once_t &once_state ) {\n        pthread_assert( pthread_once( &once_state, func ), \"pthread_once failed\" );\n    }\n    #define ATOMIC_ONCE_DECL( var ) pthread_once_t var = PTHREAD_ONCE_INIT\n#endif /* __USE_TBB_ATOMICS */\n\n    struct handles_t {\n        atomic_incrementer my_size;\n        dynamic_link_handle my_handles[MAX_LOADED_MODULES];\n\n        void init() {\n            init_atomic_incrementer( my_size );\n        }\n\n        void add(const dynamic_link_handle &handle) {\n            const size_t ind = my_size++;\n            LIBRARY_ASSERT( ind < MAX_LOADED_MODULES, \"Too many modules are loaded\" );\n            my_handles[ind] = handle;\n        }\n\n        void free() {\n            const size_t size = my_size;\n            for (size_t i=0; i<size; ++i)\n                dynamic_unlink( my_handles[i] );\n        }\n    } handles;\n\n    ATOMIC_ONCE_DECL( init_dl_data_state );\n\n    static struct ap_data_t {\n        char _path[PATH_MAX+1];\n        size_t _len;\n    } ap_data;\n\n    static void init_ap_data() {\n    #if _WIN32\n        // Get handle of our DLL first.\n        HMODULE handle;\n        BOOL brc = GetModuleHandleEx(\n            GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS | GET_MODULE_HANDLE_EX_FLAG_UNCHANGED_REFCOUNT,\n            (LPCSTR)( & dynamic_link ), // any function inside the library can be used for the address\n            & handle\n            );\n        if ( !brc ) { // Error occurred.\n            int err = GetLastError();\n            DYNAMIC_LINK_WARNING( dl_sys_fail, \"GetModuleHandleEx\", err );\n            return;\n        }\n        // Now get path to our DLL.\n        DWORD drc = GetModuleFileName( handle, ap_data._path, static_cast< DWORD >( PATH_MAX ) );\n        if ( drc == 0 ) { // Error occurred.\n            int err = GetLastError();\n            DYNAMIC_LINK_WARNING( dl_sys_fail, \"GetModuleFileName\", err );\n            return;\n        }\n        if ( drc >= PATH_MAX ) { // Buffer too short.\n            DYNAMIC_LINK_WARNING( dl_buff_too_small );\n            return;\n        }\n        // Find the position of the last backslash.\n        char *backslash = strrchr( ap_data._path, '\\\\' );\n\n        if ( !backslash ) {    // Backslash not found.\n            LIBRARY_ASSERT( backslash!=NULL, \"Unbelievable.\");\n            return;\n        }\n        LIBRARY_ASSERT( backslash >= ap_data._path, \"Unbelievable.\");\n        ap_data._len = (size_t)(backslash - ap_data._path) + 1;\n        *(backslash+1) = 0;\n    #else\n        // Get the library path\n        Dl_info dlinfo;\n        int res = dladdr( (void*)&dynamic_link, &dlinfo ); // any function inside the library can be used for the address\n        if ( !res ) {\n            char const * err = dlerror();\n            DYNAMIC_LINK_WARNING( dl_sys_fail, \"dladdr\", err );\n            return;\n        } else {\n            LIBRARY_ASSERT( dlinfo.dli_fname!=NULL, \"Unbelievable.\" );\n        }\n\n        char const *slash = strrchr( dlinfo.dli_fname, '/' );\n        size_t fname_len=0;\n        if ( slash ) {\n            LIBRARY_ASSERT( slash >= dlinfo.dli_fname, \"Unbelievable.\");\n            fname_len = (size_t)(slash - dlinfo.dli_fname) + 1;\n        }\n\n        size_t rc;\n        if ( dlinfo.dli_fname[0]=='/' ) {\n            // The library path is absolute\n            rc = 0;\n            ap_data._len = 0;\n        } else {\n            // The library path is relative so get the current working directory\n            if ( !getcwd( ap_data._path, sizeof(ap_data._path)/sizeof(ap_data._path[0]) ) ) {\n                DYNAMIC_LINK_WARNING( dl_buff_too_small );\n                return;\n            }\n            ap_data._len = strlen( ap_data._path );\n            ap_data._path[ap_data._len++]='/';\n            rc = ap_data._len;\n        }\n\n        if ( fname_len>0 ) {\n            if ( ap_data._len>PATH_MAX ) {\n                DYNAMIC_LINK_WARNING( dl_buff_too_small );\n                ap_data._len=0;\n                return;\n            }\n            strncpy( ap_data._path+rc, dlinfo.dli_fname, fname_len );\n            ap_data._len += fname_len;\n            ap_data._path[ap_data._len]=0;\n        }\n    #endif /* _WIN32 */\n    }\n\n    static void init_dl_data() {\n        handles.init();\n        init_ap_data();\n    }\n\n    /*\n        The function constructs absolute path for given relative path. Important: Base directory is not\n        current one, it is the directory libtbb.so loaded from.\n\n        Arguments:\n        in  name -- Name of a file (may be with relative path; it must not be an absolute one).\n        out path -- Buffer to save result (absolute path) to.\n        in  len  -- Size of buffer.\n        ret      -- 0         -- Error occurred.\n                    > len     -- Buffer too short, required size returned.\n                    otherwise -- Ok, number of characters (incl. terminating null) written to buffer.\n    */\n    static size_t abs_path( char const * name, char * path, size_t len ) {\n        if ( ap_data._len == 0 )\n            return 0;\n\n        size_t name_len = strlen( name );\n        size_t full_len = name_len+ap_data._len;\n        if ( full_len < len ) {\n            __TBB_ASSERT(ap_data._path[ap_data._len] == 0, NULL);\n            strcpy( path, ap_data._path );\n            strcat( path, name );\n        }\n        return full_len+1; // +1 for null character\n    }\n#endif  // __TBB_DYNAMIC_LOAD_ENABLED\n\n    void init_dynamic_link_data() {\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        atomic_once( &init_dl_data, init_dl_data_state );\n    #endif\n    }\n\n    #if __USE_STATIC_DL_INIT\n    // ap_data structure is initialized with current directory on Linux.\n    // So it should be initialized as soon as possible since the current directory may be changed.\n    // static_init_ap_data object provides this initialization during library loading.\n    static struct static_init_dl_data_t {\n        static_init_dl_data_t() {\n            init_dynamic_link_data();\n        }\n    } static_init_dl_data;\n    #endif\n\n    #if __TBB_WEAK_SYMBOLS_PRESENT\n    static bool weak_symbol_link( const dynamic_link_descriptor descriptors[], size_t required )\n    {\n        // Check if the required entries are present in what was loaded into our process.\n        for ( size_t k = 0; k < required; ++k )\n            if ( !descriptors[k].ptr )\n                return false;\n        // Commit the entry points.\n        for ( size_t k = 0; k < required; ++k )\n            *descriptors[k].handler = (pointer_to_handler) descriptors[k].ptr;\n        return true;\n    }\n    #else\n    static bool weak_symbol_link( const dynamic_link_descriptor[], size_t ) {\n        return false;\n    }\n    #endif /* __TBB_WEAK_SYMBOLS_PRESENT */\n\n    void dynamic_unlink( dynamic_link_handle handle ) {\n    #if !__TBB_DYNAMIC_LOAD_ENABLED /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n        if ( !dlclose ) return;\n    #endif\n        if ( handle ) {\n            dlclose( handle );\n        }\n    }\n\n    void dynamic_unlink_all() {\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        handles.free();\n    #endif\n    }\n\n#if !_WIN32\n#if __TBB_DYNAMIC_LOAD_ENABLED\n    static dynamic_link_handle pin_symbols( dynamic_link_descriptor desc, const dynamic_link_descriptor* descriptors, size_t required ) {\n        // It is supposed that all symbols are from the only one library\n        // The library has been loaded by another module and contains at least one requested symbol.\n        // But after we obtained the symbol the library can be unloaded by another thread\n        // invalidating our symbol. Therefore we need to pin the library in memory.\n        dynamic_link_handle library_handle = 0;\n        Dl_info info;\n        // Get library's name from earlier found symbol\n        if ( dladdr( (void*)*desc.handler, &info ) ) {\n            // Pin the library\n            library_handle = dlopen( info.dli_fname, RTLD_LAZY );\n            if ( library_handle ) {\n                // If original library was unloaded before we pinned it\n                // and then another module loaded in its place, the earlier\n                // found symbol would become invalid. So revalidate them.\n                if ( !resolve_symbols( library_handle, descriptors, required ) ) {\n                    // Wrong library.\n                    dynamic_unlink(library_handle);\n                    library_handle = 0;\n                }\n            } else {\n                char const * err = dlerror();\n                DYNAMIC_LINK_WARNING( dl_lib_not_found, info.dli_fname, err );\n            }\n        }\n        // else the library has been unloaded by another thread\n        return library_handle;\n    }\n#endif /* __TBB_DYNAMIC_LOAD_ENABLED */\n#endif /* !_WIN32 */\n\n    static dynamic_link_handle global_symbols_link( const char* library, const dynamic_link_descriptor descriptors[], size_t required ) {\n        ::tbb::internal::suppress_unused_warning( library );\n        dynamic_link_handle library_handle;\n#if _WIN32\n        if ( GetModuleHandleEx( 0, library, &library_handle ) ) {\n            if ( resolve_symbols( library_handle, descriptors, required ) )\n                return library_handle;\n            else\n                FreeLibrary( library_handle );\n        }\n#else /* _WIN32 */\n    #if !__TBB_DYNAMIC_LOAD_ENABLED /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n        if ( !dlopen ) return 0;\n    #endif /* !__TBB_DYNAMIC_LOAD_ENABLED */\n        library_handle = dlopen( NULL, RTLD_LAZY );\n    #if !__ANDROID__\n        // On Android dlopen( NULL ) returns NULL if it is called during dynamic module initialization.\n        LIBRARY_ASSERT( library_handle, \"The handle for the main program is NULL\" );\n    #endif\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        // Check existence of the first symbol only, then use it to find the library and load all necessary symbols.\n        pointer_to_handler handler;\n        dynamic_link_descriptor desc;\n        desc.name = descriptors[0].name;\n        desc.handler = &handler;\n        if ( resolve_symbols( library_handle, &desc, 1 ) ) {\n            dynamic_unlink( library_handle );\n            return pin_symbols( desc, descriptors, required );\n        }\n    #else  /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n        if ( resolve_symbols( library_handle, descriptors, required ) )\n            return library_handle;\n    #endif\n        dynamic_unlink( library_handle );\n#endif /* _WIN32 */\n        return 0;\n    }\n\n    static void save_library_handle( dynamic_link_handle src, dynamic_link_handle *dst ) {\n        LIBRARY_ASSERT( src, \"The library handle to store must be non-zero\" );\n        if ( dst )\n            *dst = src;\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        else\n            handles.add( src );\n    #endif /* __TBB_DYNAMIC_LOAD_ENABLED */\n    }\n\n    dynamic_link_handle dynamic_load( const char* library, const dynamic_link_descriptor descriptors[], size_t required ) {\n        ::tbb::internal::suppress_unused_warning( library, descriptors, required );\n#if __TBB_DYNAMIC_LOAD_ENABLED\n\n        size_t const len = PATH_MAX + 1;\n        char path[ len ];\n        size_t rc = abs_path( library, path, len );\n        if ( 0 < rc && rc <= len ) {\n#if _WIN32\n            // Prevent Windows from displaying silly message boxes if it fails to load library\n            // (e.g. because of MS runtime problems - one of those crazy manifest related ones)\n            UINT prev_mode = SetErrorMode (SEM_FAILCRITICALERRORS);\n#endif /* _WIN32 */\n            dynamic_link_handle library_handle = dlopen( path, RTLD_LAZY );\n#if _WIN32\n            SetErrorMode (prev_mode);\n#endif /* _WIN32 */\n            if( library_handle ) {\n                if( !resolve_symbols( library_handle, descriptors, required ) ) {\n                    // The loaded library does not contain all the expected entry points\n                    dynamic_unlink( library_handle );\n                    library_handle = NULL;\n                }\n            } else\n                DYNAMIC_LINK_WARNING( dl_lib_not_found, path, dlerror() );\n            return library_handle;\n        } else if ( rc>len )\n                DYNAMIC_LINK_WARNING( dl_buff_too_small );\n                // rc == 0 means failing of init_ap_data so the warning has already been issued.\n\n#endif /* __TBB_DYNAMIC_LOAD_ENABLED */\n            return 0;\n    }\n\n    bool dynamic_link( const char* library, const dynamic_link_descriptor descriptors[], size_t required, dynamic_link_handle *handle, int flags ) {\n        init_dynamic_link_data();\n\n        // TODO: May global_symbols_link find weak symbols?\n        dynamic_link_handle library_handle = ( flags & DYNAMIC_LINK_GLOBAL ) ? global_symbols_link( library, descriptors, required ) : 0;\n\n        if ( !library_handle && ( flags & DYNAMIC_LINK_LOAD ) )\n            library_handle = dynamic_load( library, descriptors, required );\n\n        if ( !library_handle && ( flags & DYNAMIC_LINK_WEAK ) )\n            return weak_symbol_link( descriptors, required );\n\n        if ( library_handle ) {\n            save_library_handle( library_handle, handle );\n            return true;\n        }\n        return false;\n    }\n\n#endif /*__TBB_WIN8UI_SUPPORT*/\n#else /* __TBB_WEAK_SYMBOLS_PRESENT || __TBB_DYNAMIC_LOAD_ENABLED */\n    bool dynamic_link( const char*, const dynamic_link_descriptor*, size_t, dynamic_link_handle *handle, int ) {\n        if ( handle )\n            *handle=0;\n        return false;\n    }\n    void dynamic_unlink( dynamic_link_handle ) {}\n    void dynamic_unlink_all() {}\n#endif /* __TBB_WEAK_SYMBOLS_PRESENT || __TBB_DYNAMIC_LOAD_ENABLED */\n\nCLOSE_INTERNAL_NAMESPACE\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/tbb/tools_api/ittnotify_config.h": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#ifndef _ITTNOTIFY_CONFIG_H_\n#define _ITTNOTIFY_CONFIG_H_\n\n/** @cond exclude_from_documentation */\n#ifndef ITT_OS_WIN\n#  define ITT_OS_WIN   1\n#endif /* ITT_OS_WIN */\n\n#ifndef ITT_OS_LINUX\n#  define ITT_OS_LINUX 2\n#endif /* ITT_OS_LINUX */\n\n#ifndef ITT_OS_MAC\n#  define ITT_OS_MAC   3\n#endif /* ITT_OS_MAC */\n\n#ifndef ITT_OS_FREEBSD\n#  define ITT_OS_FREEBSD   4\n#endif /* ITT_OS_FREEBSD */\n\n#ifndef ITT_OS\n#  if defined WIN32 || defined _WIN32\n#    define ITT_OS ITT_OS_WIN\n#  elif defined( __APPLE__ ) && defined( __MACH__ )\n#    define ITT_OS ITT_OS_MAC\n#  elif defined( __FreeBSD__ )\n#    define ITT_OS ITT_OS_FREEBSD\n#  else\n#    define ITT_OS ITT_OS_LINUX\n#  endif\n#endif /* ITT_OS */\n\n#ifndef ITT_PLATFORM_WIN\n#  define ITT_PLATFORM_WIN 1\n#endif /* ITT_PLATFORM_WIN */\n\n#ifndef ITT_PLATFORM_POSIX\n#  define ITT_PLATFORM_POSIX 2\n#endif /* ITT_PLATFORM_POSIX */\n\n#ifndef ITT_PLATFORM_MAC\n#  define ITT_PLATFORM_MAC 3\n#endif /* ITT_PLATFORM_MAC */\n\n#ifndef ITT_PLATFORM_FREEBSD\n#  define ITT_PLATFORM_FREEBSD 4\n#endif /* ITT_PLATFORM_FREEBSD */\n\n#ifndef ITT_PLATFORM\n#  if ITT_OS==ITT_OS_WIN\n#    define ITT_PLATFORM ITT_PLATFORM_WIN\n#  elif ITT_OS==ITT_OS_MAC\n#    define ITT_PLATFORM ITT_PLATFORM_MAC\n#  elif ITT_OS==ITT_OS_FREEBSD\n#    define ITT_PLATFORM ITT_PLATFORM_FREEBSD\n#  else\n#    define ITT_PLATFORM ITT_PLATFORM_POSIX\n#  endif\n#endif /* ITT_PLATFORM */\n\n#if defined(_UNICODE) && !defined(UNICODE)\n#define UNICODE\n#endif\n\n#include <stddef.h>\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <tchar.h>\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <stdint.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE || _UNICODE */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#ifndef CDECL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define CDECL __cdecl\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define CDECL __attribute__ ((cdecl))\n#    else  /* _M_IX86 || __i386__ */\n#      define CDECL /* actual only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* CDECL */\n\n#ifndef STDCALL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define STDCALL __stdcall\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define STDCALL __attribute__ ((stdcall))\n#    else  /* _M_IX86 || __i386__ */\n#      define STDCALL /* supported only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* STDCALL */\n\n#define ITTAPI    CDECL\n#define LIBITTAPI CDECL\n\n/* TODO: Temporary for compatibility! */\n#define ITTAPI_CALL    CDECL\n#define LIBITTAPI_CALL CDECL\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n/* use __forceinline (VC++ specific) */\n#define ITT_INLINE           __forceinline\n#define ITT_INLINE_ATTRIBUTE /* nothing */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/*\n * Generally, functions are not inlined unless optimization is specified.\n * For functions declared inline, this attribute inlines the function even\n * if no optimization level was specified.\n */\n#ifdef __STRICT_ANSI__\n#define ITT_INLINE           static\n#define ITT_INLINE_ATTRIBUTE __attribute__((unused))\n#else  /* __STRICT_ANSI__ */\n#define ITT_INLINE           static inline\n#define ITT_INLINE_ATTRIBUTE __attribute__((always_inline, unused))\n#endif /* __STRICT_ANSI__ */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/** @endcond */\n\n#ifndef ITT_ARCH_IA32\n#  define ITT_ARCH_IA32  1\n#endif /* ITT_ARCH_IA32 */\n\n#ifndef ITT_ARCH_IA32E\n#  define ITT_ARCH_IA32E 2\n#endif /* ITT_ARCH_IA32E */\n\n#ifndef ITT_ARCH_ARM\n#  define ITT_ARCH_ARM  4\n#endif /* ITT_ARCH_ARM */\n\n#ifndef ITT_ARCH_PPC64\n#  define ITT_ARCH_PPC64  5\n#endif /* ITT_ARCH_PPC64 */\n\n#ifndef ITT_ARCH\n#  if defined _M_IX86 || defined __i386__\n#    define ITT_ARCH ITT_ARCH_IA32\n#  elif defined _M_X64 || defined _M_AMD64 || defined __x86_64__\n#    define ITT_ARCH ITT_ARCH_IA32E\n#  elif defined _M_IA64 || defined __ia64__\n#    define ITT_ARCH ITT_ARCH_IA64\n#  elif defined _M_ARM || __arm__\n#    define ITT_ARCH ITT_ARCH_ARM\n#  elif defined __powerpc64__\n#    define ITT_ARCH ITT_ARCH_PPC64\n#  endif\n#endif\n\n#ifdef __cplusplus\n#  define ITT_EXTERN_C extern \"C\"\n#  define ITT_EXTERN_C_BEGIN extern \"C\" {\n#  define ITT_EXTERN_C_END }\n#else\n#  define ITT_EXTERN_C /* nothing */\n#  define ITT_EXTERN_C_BEGIN /* nothing */\n#  define ITT_EXTERN_C_END /* nothing */\n#endif /* __cplusplus */\n\n#define ITT_TO_STR_AUX(x) #x\n#define ITT_TO_STR(x)     ITT_TO_STR_AUX(x)\n\n#define __ITT_BUILD_ASSERT(expr, suffix) do { \\\n    static char __itt_build_check_##suffix[(expr) ? 1 : -1]; \\\n    __itt_build_check_##suffix[0] = 0; \\\n} while(0)\n#define _ITT_BUILD_ASSERT(expr, suffix)  __ITT_BUILD_ASSERT((expr), suffix)\n#define ITT_BUILD_ASSERT(expr)           _ITT_BUILD_ASSERT((expr), __LINE__)\n\n#define ITT_MAGIC { 0xED, 0xAB, 0xAB, 0xEC, 0x0D, 0xEE, 0xDA, 0x30 }\n\n/* Replace with snapshot date YYYYMMDD for promotion build. */\n#define API_VERSION_BUILD    20111111\n\n#ifndef API_VERSION_NUM\n#define API_VERSION_NUM 0.0.0\n#endif /* API_VERSION_NUM */\n\n#define API_VERSION \"ITT-API-Version \" ITT_TO_STR(API_VERSION_NUM) \\\n                                \" (\" ITT_TO_STR(API_VERSION_BUILD) \")\"\n\n/* OS communication functions */\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <windows.h>\ntypedef HMODULE           lib_t;\ntypedef DWORD             TIDT;\ntypedef CRITICAL_SECTION  mutex_t;\n#define MUTEX_INITIALIZER { 0 }\n#define strong_alias(name, aliasname) /* empty for Windows */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <dlfcn.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE */\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1 /* need for PTHREAD_MUTEX_RECURSIVE */\n#endif /* _GNU_SOURCE */\n#ifndef __USE_UNIX98\n#define __USE_UNIX98 1 /* need for PTHREAD_MUTEX_RECURSIVE, on SLES11.1 with gcc 4.3.4 wherein pthread.h missing dependency on __USE_XOPEN2K8 */\n#endif /*__USE_UNIX98*/\n#include <pthread.h>\ntypedef void*             lib_t;\ntypedef pthread_t         TIDT;\ntypedef pthread_mutex_t   mutex_t;\n#define MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n#define _strong_alias(name, aliasname) \\\n            extern __typeof (name) aliasname __attribute__ ((alias (#name)));\n#define strong_alias(name, aliasname) _strong_alias(name, aliasname)\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#define __itt_get_proc(lib, name) GetProcAddress(lib, name)\n#define __itt_mutex_init(mutex)   InitializeCriticalSection(mutex)\n#define __itt_mutex_lock(mutex)   EnterCriticalSection(mutex)\n#define __itt_mutex_unlock(mutex) LeaveCriticalSection(mutex)\n#define __itt_load_lib(name)      LoadLibraryA(name)\n#define __itt_unload_lib(handle)  FreeLibrary(handle)\n#define __itt_system_error()      (int)GetLastError()\n#define __itt_fstrcmp(s1, s2)     lstrcmpA(s1, s2)\n#define __itt_fstrnlen(s, l)      strnlen_s(s, l)\n#define __itt_fstrcpyn(s1, b, s2, l) strncpy_s(s1, b, s2, l)\n#define __itt_fstrdup(s)          _strdup(s)\n#define __itt_thread_id()         GetCurrentThreadId()\n#define __itt_thread_yield()      SwitchToThread()\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return InterlockedIncrement(ptr);\n}\n#endif /* ITT_SIMPLE_INIT */\n#else /* ITT_PLATFORM!=ITT_PLATFORM_WIN */\n#define __itt_get_proc(lib, name) dlsym(lib, name)\n#define __itt_mutex_init(mutex)   {\\\n    pthread_mutexattr_t mutex_attr;                                         \\\n    int error_code = pthread_mutexattr_init(&mutex_attr);                   \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_init\",    \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_settype(&mutex_attr,                     \\\n                                           PTHREAD_MUTEX_RECURSIVE);        \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_settype\", \\\n                           error_code);                                     \\\n    error_code = pthread_mutex_init(mutex, &mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutex_init\",        \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_destroy(&mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_destroy\", \\\n                           error_code);                                     \\\n}\n#define __itt_mutex_lock(mutex)   pthread_mutex_lock(mutex)\n#define __itt_mutex_unlock(mutex) pthread_mutex_unlock(mutex)\n#define __itt_load_lib(name)      dlopen(name, RTLD_LAZY)\n#define __itt_unload_lib(handle)  dlclose(handle)\n#define __itt_system_error()      errno\n#define __itt_fstrcmp(s1, s2)     strcmp(s1, s2)\n\n/* makes customer code define safe APIs for SDL_STRNLEN_S and SDL_STRNCPY_S */\n#ifdef SDL_STRNLEN_S\n#define __itt_fstrnlen(s, l)      SDL_STRNLEN_S(s, l)\n#else\n#define __itt_fstrnlen(s, l)      strlen(s)\n#endif /* SDL_STRNLEN_S */\n#ifdef SDL_STRNCPY_S\n#define __itt_fstrcpyn(s1, b, s2, l) SDL_STRNCPY_S(s1, b, s2, l)\n#else\n#define __itt_fstrcpyn(s1, b, s2, l) {                                      \\\n    if (b > 0) {                                                            \\\n        /* 'volatile' is used to suppress the warning that a destination */ \\\n        /*  bound depends on the length of the source.                   */ \\\n        volatile size_t num_to_copy = (size_t)(b - 1) < (size_t)(l) ?       \\\n                (size_t)(b - 1) : (size_t)(l);                              \\\n        strncpy(s1, s2, num_to_copy);                                       \\\n        s1[num_to_copy] = 0;                                                \\\n    }                                                                       \\\n}\n#endif /* SDL_STRNCPY_S */\n\n#define __itt_fstrdup(s)          strdup(s)\n#define __itt_thread_id()         pthread_self()\n#define __itt_thread_yield()      sched_yield()\n#if ITT_ARCH==ITT_ARCH_IA64\n#ifdef __INTEL_COMPILER\n#define __TBB_machine_fetchadd4(addr, val) __fetchadd4_acq((void *)addr, val)\n#else  /* __INTEL_COMPILER */\n/* TODO: Add Support for not Intel compilers for IA-64 architecture */\n#endif /* __INTEL_COMPILER */\n#elif ITT_ARCH==ITT_ARCH_IA32 || ITT_ARCH==ITT_ARCH_IA32E /* ITT_ARCH!=ITT_ARCH_IA64 */\nITT_INLINE long\n__TBB_machine_fetchadd4(volatile void* ptr, long addend) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __TBB_machine_fetchadd4(volatile void* ptr, long addend)\n{\n    long result;\n    __asm__ __volatile__(\"lock\\nxadd %0,%1\"\n                          : \"=r\"(result),\"=m\"(*(int*)ptr)\n                          : \"0\"(addend), \"m\"(*(int*)ptr)\n                          : \"memory\");\n    return result;\n}\n#elif ITT_ARCH==ITT_ARCH_ARM || ITT_ARCH==ITT_ARCH_PPC64\n#define __TBB_machine_fetchadd4(addr, val) __sync_fetch_and_add(addr, val)\n#endif /* ITT_ARCH==ITT_ARCH_IA64 */\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return __TBB_machine_fetchadd4(ptr, 1) + 1L;\n}\n#endif /* ITT_SIMPLE_INIT */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\ntypedef enum {\n    __itt_collection_normal = 0,\n    __itt_collection_paused = 1\n} __itt_collection_state;\n\ntypedef enum {\n    __itt_thread_normal  = 0,\n    __itt_thread_ignored = 1\n} __itt_thread_state;\n\n#pragma pack(push, 8)\n\ntypedef struct ___itt_thread_info\n{\n    const char* nameA; /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    TIDT               tid;\n    __itt_thread_state state;   /*!< Thread state (paused or normal) */\n    int                extra1;  /*!< Reserved to the runtime */\n    void*              extra2;  /*!< Reserved to the runtime */\n    struct ___itt_thread_info* next;\n} __itt_thread_info;\n\n#include \"ittnotify_types.h\" /* For __itt_group_id definition */\n\ntypedef struct ___itt_api_info_20101001\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    __itt_group_id group;\n}  __itt_api_info_20101001;\n\ntypedef struct ___itt_api_info\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    void*          null_func;\n    __itt_group_id group;\n}  __itt_api_info;\n\nstruct ___itt_domain;\nstruct ___itt_string_handle;\n\ntypedef struct ___itt_global\n{\n    unsigned char          magic[8];\n    unsigned long          version_major;\n    unsigned long          version_minor;\n    unsigned long          version_build;\n    volatile long          api_initialized;\n    volatile long          mutex_initialized;\n    volatile long          atomic_counter;\n    mutex_t                mutex;\n    lib_t                  lib;\n    void*                  error_handler;\n    const char**           dll_path_ptr;\n    __itt_api_info*        api_list_ptr;\n    struct ___itt_global*  next;\n    /* Joinable structures below */\n    __itt_thread_info*     thread_list;\n    struct ___itt_domain*  domain_list;\n    struct ___itt_string_handle* string_list;\n    __itt_collection_state state;\n} __itt_global;\n\n#pragma pack(pop)\n\n#define NEW_THREAD_INFO_W(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = NULL; \\\n        h->nameW  = n ? _wcsdup(n) : NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_THREAD_INFO_A(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = n ? __itt_fstrdup(n) : NULL; \\\n        h->nameW  = NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_W(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 1;    /* domain is enabled by default */ \\\n        h->nameA  = NULL; \\\n        h->nameW  = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_A(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 1;    /* domain is enabled by default */ \\\n        h->nameA  = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW  = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_W(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = NULL; \\\n        h->strW   = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_A(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->strW   = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#endif /* _ITTNOTIFY_CONFIG_H_ */\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/rml/server/rml_server.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#include \"rml_tbb.h\"\n#define private public /* Sleazy trick to avoid publishing internal names in public header. */\n#include \"rml_omp.h\"\n#undef private\n\n#include \"tbb/tbb_allocator.h\"\n#include \"tbb/cache_aligned_allocator.h\"\n#include \"tbb/aligned_space.h\"\n#include \"tbb/atomic.h\"\n#include \"tbb/spin_mutex.h\"\n#include \"tbb/tbb_misc.h\"           // Get AvailableHwConcurrency() from here.\n#if _MSC_VER==1500 && !defined(__INTEL_COMPILER)\n// VS2008/VC9 seems to have an issue;\n#pragma warning( push )\n#pragma warning( disable: 4985 )\n#endif\n#include \"tbb/concurrent_vector.h\"\n#if _MSC_VER==1500 && !defined(__INTEL_COMPILER)\n#pragma warning( pop )\n#endif\n#if _MSC_VER && defined(_Wp64)\n// Workaround for overzealous compiler warnings\n#pragma warning (push)\n#pragma warning (disable: 4244)\n#endif\n\n#include \"job_automaton.h\"\n#include \"wait_counter.h\"\n#include \"thread_monitor.h\"\n\n#if RML_USE_WCRM\n#include <concrt.h>\n#include <concrtrm.h>\nusing namespace Concurrency;\n#include <vector>\n#include <hash_map>\n#define __RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED 0\n#endif /* RML_USE_WCRM */\n\n#define STRINGIFY(x) #x\n#define TOSTRING(x) STRINGIFY(x)\n\nnamespace rml {\nnamespace internal {\n\nusing tbb::internal::rml::tbb_client;\nusing tbb::internal::rml::tbb_server;\n\nusing __kmp::rml::omp_client;\nusing __kmp::rml::omp_server;\n\ntypedef versioned_object::version_type version_type;\n\n#define SERVER_VERSION 2\n#define EARLIEST_COMPATIBLE_CLIENT_VERSION 2\n\nstatic const size_t cache_line_size = tbb::internal::NFS_MaxLineSize;\n\ntemplate<typename Server, typename Client> class generic_connection;\nclass tbb_connection_v2;\nclass omp_connection_v2;\n\n#if RML_USE_WCRM\n//! State of a server_thread\n/** Below are diagrams of legal state transitions.\n\n                          ts_busy\n                          ^      ^\n                         /        \\\n                        /          V\n    ts_done <----- ts_asleep <------> ts_idle\n*/\n\nenum thread_state_t {\n    ts_idle,\n    ts_asleep,\n    ts_busy,\n    ts_done\n};\n\n//! Extra state of an omp server thread\nenum thread_extra_state_t {\n    ts_none,\n    ts_removed,\n    ts_lent\n};\n\n//! Results from try_grab_for()\nenum thread_grab_t {\n    wk_failed,\n    wk_from_asleep,\n    wk_from_idle\n};\n\n#else /* !RML_USE_WCRM */\n\n//! State of a server_thread\n/** Below are diagrams of legal state transitions.\n\n    OMP\n              ts_omp_busy\n              ^          ^\n             /            \\\n            /              V\n    ts_asleep <-----------> ts_idle\n\n\n              ts_deactivated\n             ^            ^\n            /              \\\n           V                \\\n    ts_none  <--------------> ts_reactivated\n\n    TBB\n              ts_tbb_busy\n              ^          ^\n             /            \\\n            /              V\n    ts_asleep <-----------> ts_idle --> ts_done\n\n    For TBB only. Extra state transition.\n\n    ts_created -> ts_started -> ts_visited\n */\nenum thread_state_t {\n    //! Thread not doing anything useful, but running and looking for work.\n    ts_idle,\n    //! Thread not doing anything useful and is asleep */\n    ts_asleep,\n    //! Thread is enlisted into OpenMP team\n    ts_omp_busy,\n    //! Thread is busy doing TBB work.\n    ts_tbb_busy,\n    //! For tbb threads only\n    ts_done,\n    ts_created,\n    ts_started,\n    ts_visited,\n    //! For omp threads only\n    ts_none,\n    ts_deactivated,\n    ts_reactivated\n};\n#endif /* RML_USE_WCRM */\n\n#if TBB_USE_ASSERT\n#define PRODUCE_ARG(x) ,x\n#else\n#define PRODUCE_ARG(x)\n#endif /* TBB_USE_ASSERT */\n\n//! Synchronizes dispatch of OpenMP work.\nclass omp_dispatch_type {\n    typedef ::rml::job job_type;\n    omp_client* client;\n    void* cookie;\n    omp_client::size_type index;\n    tbb::atomic<job_type*> job;\n#if TBB_USE_ASSERT\n    omp_connection_v2* server;\n#endif /* TBB_USE_ASSERT */\npublic:\n    omp_dispatch_type() {job=NULL;}\n    void consume();\n    void produce( omp_client& c, job_type* j, void* cookie_, omp_client::size_type index_ PRODUCE_ARG( omp_connection_v2& s )) {\n        __TBB_ASSERT( j, NULL );\n        __TBB_ASSERT( !job, \"job already set\" );\n        client = &c;\n#if TBB_USE_ASSERT\n        server = &s;\n#endif /* TBB_USE_ASSERT */\n        cookie = cookie_;\n        index = index_;\n        // Must be last\n        job = j;\n    }\n};\n\n//! A reference count.\n/** No default constructor, because users of ref_count must be very careful about whether the\n    initial reference count is 0 or 1. */\nclass ref_count: no_copy {\n    friend class thread_map;\n    tbb::atomic<int> my_ref_count;\npublic:\n    ref_count(int k ) {my_ref_count=k;}\n    ~ref_count() {__TBB_ASSERT( !my_ref_count, \"premature destruction of refcounted object\" );}\n    //! Add one and return new value.\n    int add_ref() {\n        int k = ++my_ref_count;\n        __TBB_ASSERT(k>=1,\"reference count underflowed before add_ref\");\n        return k;\n    }\n    //! Subtract one and return new value.\n    int remove_ref() {\n        int k = --my_ref_count;\n        __TBB_ASSERT(k>=0,\"reference count underflow\");\n        return k;\n    }\n};\n\n#if RML_USE_WCRM\n\n#if USE_UMS_THREAD\n#define RML_THREAD_KIND UmsThreadDefault\n#define RML_THREAD_KIND_STRING \"UmsThread\"\n#else\n#define RML_THREAD_KIND ThreadScheduler\n#define RML_THREAD_KIND_STRING \"WinThread\"\n#endif\n\n// Forward declaration\nclass thread_map;\n\nstatic const IExecutionResource* c_remove_prepare = (IExecutionResource*)0;\nstatic const IExecutionResource* c_remove_returned = (IExecutionResource*)1;\n\n//! Server thread representation\nclass server_thread_rep : no_copy {\n    friend class thread_map;\n    friend class omp_connection_v2;\n    friend class server_thread;\n    friend class tbb_server_thread;\n    friend class omp_server_thread;\n    template<typename Connection> friend void make_job( Connection& c, typename Connection::server_thread_type& t );\n    typedef int thread_state_rep_t;\npublic:\n    //! Ctor\n    server_thread_rep( bool assigned, IScheduler* s, IExecutionResource* r, thread_map& map, rml::client& cl ) :\n        uid( GetExecutionContextId() ), my_scheduler(s), my_proxy(NULL),\n        my_thread_map(map), my_client(cl), my_job(NULL)\n    {\n        my_state = assigned ? ts_busy : ts_idle;\n        my_extra_state = ts_none;\n        terminate = false;\n        my_execution_resource = r;\n    }\n    //! Dtor\n    ~server_thread_rep() {}\n\n    //! Synchronization routine\n    inline rml::job* wait_for_job() {\n        if( !my_job ) my_job = my_job_automaton.wait_for_job();\n        return my_job;\n    }\n\n    // Getters and setters\n    inline thread_state_t read_state() const { thread_state_rep_t s = my_state; return static_cast<thread_state_t>(s); }\n    inline void set_state( thread_state_t to ) {my_state = to;}\n    inline void set_removed() { __TBB_ASSERT( my_extra_state==ts_none, NULL ); my_extra_state = ts_removed; }\n    inline bool is_removed() const { return my_extra_state==ts_removed; }\n    inline bool is_lent() const {return my_extra_state==ts_lent;}\n    inline void set_lent() { my_extra_state=ts_lent; }\n    inline void set_returned() { my_extra_state=ts_none; }\n    inline IExecutionResource* get_execution_resource() { return my_execution_resource; }\n    inline IVirtualProcessorRoot* get_virtual_processor() { return (IVirtualProcessorRoot*)get_execution_resource(); }\n\n    //! Enlist the thread for work\n    inline bool wakeup( thread_state_t to, thread_state_t from ) {\n        __TBB_ASSERT( from==ts_asleep && (to==ts_idle||to==ts_busy||to==ts_done), NULL );\n        return my_state.compare_and_swap( to, from )==from;\n    }\n\n    //! Enlist the thread for.\n    thread_grab_t try_grab_for();\n\n    //! Destroy the client job associated with the thread\n    template<typename Connection> bool destroy_job( Connection* c );\n\n    //! Try to re-use the thread\n    void revive( IScheduler* s, IExecutionResource* r, rml::client& c ) {\n        // the variables may not have been set before a thread was told to quit\n        __TBB_ASSERT( my_scheduler==s, \"my_scheduler has been altered?\\n\" );\n        my_scheduler = s;\n        __TBB_ASSERT( &my_client==&c, \"my_client has been altered?\\n\" );\n        if( r ) my_execution_resource = r;\n        my_client = c;\n        my_state = ts_idle;\n        __TBB_ASSERT( my_extra_state==ts_removed, NULL );\n        my_extra_state = ts_none;\n    }\n\nprotected:\n    const int uid;\n    IScheduler* my_scheduler;\n    IThreadProxy* my_proxy;\n    tbb::atomic<IExecutionResource*> my_execution_resource; /* for non-masters, it is IVirtualProcessorRoot */\n    thread_map& my_thread_map;\n    rml::client& my_client;\n    job* my_job;\n    job_automaton my_job_automaton;\n    tbb::atomic<bool> terminate;\n    tbb::atomic<thread_state_rep_t> my_state;\n    tbb::atomic<thread_extra_state_t> my_extra_state;\n};\n\n//! Class that implements IExecutionContext\nclass server_thread : public IExecutionContext, public server_thread_rep {\n    friend class tbb_connection_v2;\n    friend class omp_connection_v2;\n    friend class tbb_server_thread;\n    friend class omp_server_thread;\n    friend class thread_map;\n    template<typename Connection> friend void make_job( Connection& c, typename Connection::server_thread_type& t );\nprotected:\n    server_thread( bool is_tbb, bool assigned, IScheduler* s, IExecutionResource* r, thread_map& map, rml::client& cl ) : server_thread_rep(assigned,s,r,map,cl), tbb_thread(is_tbb) {}\n    ~server_thread() {}\n    unsigned int GetId() const __TBB_override { return uid; }\n    IScheduler* GetScheduler() __TBB_override { return my_scheduler; }\n    IThreadProxy* GetProxy()   __TBB_override { return my_proxy; }\n    void SetProxy( IThreadProxy* thr_proxy ) __TBB_override { my_proxy = thr_proxy; }\n\nprivate:\n    bool tbb_thread;\n};\n\n// Forward declaration\nclass tbb_connection_v2;\nclass omp_connection_v2;\n\n//! TBB server thread\nclass tbb_server_thread : public server_thread {\n    friend class tbb_connection_v2;\npublic:\n    tbb_server_thread( bool assigned, IScheduler* s, IExecutionResource* r, tbb_connection_v2* con, thread_map& map, rml::client& cl ) : server_thread(true,assigned,s,r,map,cl), my_conn(con) {\n        activation_count = 0;\n    }\n    ~tbb_server_thread() {}\n    void Dispatch( DispatchState* ) __TBB_override;\n    inline bool initiate_termination();\n    bool sleep_perhaps();\n    //! Switch out this thread\n    bool switch_out();\nprivate:\n    tbb_connection_v2* my_conn;\npublic:\n    tbb::atomic<int> activation_count;\n};\n\n//! OMP server thread\nclass omp_server_thread : public server_thread {\n    friend class omp_connection_v2;\npublic:\n    omp_server_thread( bool assigned, IScheduler* s, IExecutionResource* r, omp_connection_v2* con, thread_map& map, rml::client& cl ) :\n        server_thread(false,assigned,s,r,map,cl), my_conn(con), my_cookie(NULL), my_index(UINT_MAX) {}\n    ~omp_server_thread() {}\n    void Dispatch( DispatchState* ) __TBB_override;\n    inline void* get_cookie() {return my_cookie;}\n    inline ::__kmp::rml::omp_client::size_type get_index() {return my_index;}\n\n    inline IExecutionResource* get_execution_resource() { return get_execution_resource(); }\n    inline bool initiate_termination() { return destroy_job( (omp_connection_v2*) my_conn ); }\n    void sleep_perhaps();\nprivate:\n    omp_connection_v2* my_conn;\n    void* my_cookie;\n    ::__kmp::rml::omp_client::size_type my_index;\n    omp_dispatch_type omp_data;\n};\n\n//! Class that implements IScheduler\ntemplate<typename Connection>\nclass scheduler : no_copy, public IScheduler {\npublic:\n    unsigned int GetId() const __TBB_override {return uid;}\n    void Statistics( unsigned int* /*pTaskCompletionRate*/, unsigned int* /*pTaskArrivalRate*/, unsigned int* /*pNumberOfTaskEnqueued*/) __TBB_override {}\n    SchedulerPolicy GetPolicy() const __TBB_override { __TBB_ASSERT(my_policy,NULL); return *my_policy; }\n    void AddVirtualProcessors( IVirtualProcessorRoot** vproots, unsigned int count ) __TBB_override { if( !my_conn.is_closing() ) my_conn.add_virtual_processors( vproots, count); }\n    void RemoveVirtualProcessors( IVirtualProcessorRoot** vproots, unsigned int count ) __TBB_override;\n    void NotifyResourcesExternallyIdle( IVirtualProcessorRoot** vproots, unsigned int count ) __TBB_override { __TBB_ASSERT( false, \"This call is not allowed for TBB\" ); }\n    void NotifyResourcesExternallyBusy( IVirtualProcessorRoot** vproots, unsigned int count ) __TBB_override { __TBB_ASSERT( false, \"This call is not allowed for TBB\" ); }\nprotected:\n    scheduler( Connection& conn );\n    virtual ~scheduler() { __TBB_ASSERT( my_policy, NULL ); delete my_policy; }\n\npublic:\n    static scheduler* create( Connection& conn ) {return new scheduler( conn );}\n\nprivate:\n    const int uid;\n    Connection& my_conn;\n    SchedulerPolicy* my_policy;\n};\n\n\n/*\n * --> ts_busy --> ts_done\n */\nclass thread_scavenger_thread : public IExecutionContext, no_copy {\npublic:\n    thread_scavenger_thread( IScheduler* s, IVirtualProcessorRoot* r, thread_map& map ) :\n        uid( GetExecutionContextId() ), my_scheduler(s), my_virtual_processor_root(r), my_proxy(NULL), my_thread_map(map)\n    {\n        my_state = ts_busy;\n#if TBB_USE_ASSERT\n        activation_count = 0;\n#endif\n    }\n    ~thread_scavenger_thread() {}\n    unsigned int GetId() const __TBB_override { return uid; }\n    IScheduler* GetScheduler() __TBB_override { return my_scheduler; }\n    IThreadProxy* GetProxy()   __TBB_override { return my_proxy; }\n    void SetProxy( IThreadProxy* thr_proxy ) __TBB_override { my_proxy = thr_proxy; }\n    void Dispatch( DispatchState* ) __TBB_override;\n    inline thread_state_t read_state() { return my_state; }\n    inline void set_state( thread_state_t s ) { my_state = s; }\n    inline IVirtualProcessorRoot* get_virtual_processor() { return my_virtual_processor_root; }\nprivate:\n    const int uid;\n    IScheduler* my_scheduler;\n    IVirtualProcessorRoot* my_virtual_processor_root;\n    IThreadProxy* my_proxy;\n    thread_map& my_thread_map;\n    tbb::atomic<thread_state_t> my_state;\n#if TBB_USE_ASSERT\npublic:\n    tbb::atomic<int> activation_count;\n#endif\n};\n\nstatic const thread_scavenger_thread* c_claimed = reinterpret_cast<thread_scavenger_thread*>(1);\n\nstruct garbage_connection_queue {\n    tbb::atomic<uintptr_t> head;\n    tbb::atomic<uintptr_t> tail;\n    static const uintptr_t empty = 0; // connection scavenger thread empty list\n    static const uintptr_t plugged = 1;  // end of use of the list\n    static const uintptr_t plugged_acked = 2;  // connection scavenger saw the plugged flag, and it freed all connections\n};\n\n//! Connection scavenger\n/** It collects closed connection objects, wait for worker threads belonging to the connection to return to ConcRT RM\n *  then return the object to the memory manager.\n */\nclass connection_scavenger_thread {\n    friend void assist_cleanup_connections();\n    /*\n     * connection_scavenger_thread's state\n     * ts_busy <----> ts_asleep <--\n     */\n    tbb::atomic<thread_state_t> state;\n\n    /* We steal two bits from a connection pointer to encode\n     * whether the connection is for TBB or for OMP.\n     *\n     * ----------------------------------\n     * |                          |  |  |\n     * ----------------------------------\n     *                              ^  ^\n     *                             /   |\n     *            1 : tbb, 0 : omp     |\n     *                  if set, terminate\n     */\n    // FIXME: pad these?\n    thread_monitor monitor;\n    HANDLE thr_handle;\n#if TBB_USE_ASSERT\n    tbb::atomic<int> n_scavenger_threads;\n#endif\n\npublic:\n    connection_scavenger_thread() : thr_handle(NULL) {\n        state = ts_asleep;\n#if TBB_USE_ASSERT\n        n_scavenger_threads = 0;\n#endif\n    }\n\n    ~connection_scavenger_thread() {}\n\n    void wakeup() {\n        if( state.compare_and_swap( ts_busy, ts_asleep )==ts_asleep )\n            monitor.notify();\n    }\n\n    void sleep_perhaps();\n\n    void process_requests( uintptr_t conn_ex );\n\n    static __RML_DECL_THREAD_ROUTINE thread_routine( void* arg );\n\n    void launch() {\n        thread_monitor::launch( connection_scavenger_thread::thread_routine, this, NULL );\n    }\n\n    template<typename Server, typename Client>\n    void add_request( generic_connection<Server,Client>* conn_to_close );\n\n    template<typename Server, typename Client>\n    uintptr_t grab_and_prepend( generic_connection<Server,Client>* last_conn_to_close );\n};\n\nvoid free_all_connections( uintptr_t );\n\n#endif /* RML_USE_WCRM */\n\n#if !RML_USE_WCRM\nclass server_thread;\n\n//! thread_map_base; we need to make the iterator type available to server_thread\nstruct thread_map_base {\n    //! A value in the map\n    class value_type {\n    public:\n        server_thread& thread() {\n            __TBB_ASSERT( my_thread, \"thread_map::value_type::thread() called when !my_thread\" );\n            return *my_thread;\n        }\n        rml::job& job() {\n            __TBB_ASSERT( my_job, \"thread_map::value_type::job() called when !my_job\" );\n            return *my_job;\n        }\n        value_type() : my_thread(NULL), my_job(NULL) {}\n        server_thread& wait_for_thread() const {\n            for(;;) {\n                server_thread* ptr=const_cast<server_thread*volatile&>(my_thread);\n                if( ptr )\n                    return *ptr;\n                __TBB_Yield();\n            }\n        }\n        /** Shortly after when a connection is established, it is possible for the server\n            to grab a server_thread that has not yet created a job object for that server. */\n        rml::job* wait_for_job() const {\n            if( !my_job ) {\n                my_job = my_automaton.wait_for_job();\n            }\n            return my_job;\n        }\n    private:\n        server_thread* my_thread;\n        /** Marked mutable because though it is physically modified, conceptually it is a duplicate of\n            the job held by job_automaton. */\n        mutable rml::job* my_job;\n        job_automaton my_automaton;\n        // FIXME - pad out to cache line, because my_automaton is hit hard by thread()\n        friend class thread_map;\n    };\n    typedef tbb::concurrent_vector<value_type,tbb::zero_allocator<value_type,tbb::cache_aligned_allocator> > array_type;\n};\n#endif /* !RML_USE_WCRM */\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    // Suppress overzealous compiler warnings about uninstantiable class\n    #pragma warning(push)\n    #pragma warning(disable:4510 4610)\n#endif\n\ntemplate<typename T>\nclass padded: public T {\n    char pad[cache_line_size - sizeof(T)%cache_line_size];\n};\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    #pragma warning(pop)\n#endif\n\n// FIXME - should we pad out memory to avoid false sharing of our global variables?\nstatic unsigned the_default_concurrency;\nstatic tbb::atomic<int> the_balance;\nstatic tbb::atomic<tbb::internal::do_once_state> rml_module_state;\n\n#if !RML_USE_WCRM\n//! Per thread information\n/** ref_count holds number of clients that are using this,\n    plus 1 if a host thread owns this instance. */\nclass server_thread: public ref_count {\n    friend class thread_map;\n    template<typename Server, typename Client> friend class generic_connection;\n    friend class tbb_connection_v2;\n    friend class omp_connection_v2;\n    //! Integral type that can hold a thread_state_t\n    typedef int thread_state_rep_t;\n    tbb::atomic<thread_state_rep_t> state;\npublic:\n    thread_monitor monitor;\nprivate:\n    bool    is_omp_thread;\n    tbb::atomic<thread_state_rep_t> my_extra_state;\n    server_thread* link;\n    thread_map_base::array_type::iterator my_map_pos;\n    rml::server *my_conn;\n    rml::job* my_job;\n    job_automaton* my_ja;\n    size_t my_index;\n    tbb::atomic<bool> terminate;\n    omp_dispatch_type omp_dispatch;\n\n#if TBB_USE_ASSERT\n    //! Flag used to check if thread is still using *this.\n    bool has_active_thread;\n#endif /* TBB_USE_ASSERT */\n\n    //! Volunteer to sleep.\n    void sleep_perhaps( thread_state_t asleep );\n\n    //! Destroy job corresponding to given client\n    /** Return true if thread must quit. */\n    template<typename Connection>\n    bool destroy_job( Connection& c );\n\n    //! Do terminate the thread\n    /** Return true if thread must quit. */\n    bool do_termination();\n\n    void loop();\n    static __RML_DECL_THREAD_ROUTINE thread_routine( void* arg );\n\npublic:\n    server_thread();\n\n    ~server_thread();\n\n    //! Read the thread state\n    thread_state_t read_state() const {\n        thread_state_rep_t s = state;\n        __TBB_ASSERT( unsigned(s)<=unsigned(ts_done), \"corrupted server thread?\" );\n        return thread_state_t(s);\n    }\n\n    //! Read the tbb-specific extra thread state\n    thread_state_t read_extra_state() const {\n        thread_state_rep_t s = my_extra_state;\n        return thread_state_t(s);\n    }\n\n    //! Launch a thread that is bound to *this.\n    void launch( size_t stack_size );\n\n    //! Attempt to wakeup a thread\n    /** The value \"to\" is the new state for the thread, if it was woken up.\n        Returns true if thread was woken up, false otherwise. */\n    bool wakeup( thread_state_t to, thread_state_t from );\n\n    //! Attempt to enslave a thread for OpenMP/TBB.\n    /** Returns true if state is successfully changed.  's' takes either ts_omp_busy or ts_tbb_busy */\n    bool try_grab_for( thread_state_t s );\n\n#if _WIN32||_WIN64\n    //! Send the worker thread to sleep temporarily\n    void deactivate();\n\n    //! Wake the worker thread up\n    void reactivate();\n#endif /* _WIN32||_WIN64 */\n};\n\n//! Bag of threads that are private to a client.\nclass private_thread_bag {\n    struct list_thread: server_thread {\n       list_thread* next;\n    };\n    //! Root of atomic linked list of list_thread\n    /** ABA problem is avoided because items are only atomically pushed, never popped. */\n    tbb::atomic<list_thread*> my_root;\n    tbb::cache_aligned_allocator<padded<list_thread> > my_allocator;\npublic:\n    //! Construct empty bag\n    private_thread_bag() {my_root=NULL;}\n\n    //! Create a fresh server_thread object.\n    server_thread& add_one_thread() {\n        list_thread* t = my_allocator.allocate(1);\n        new( t ) list_thread;\n        // Atomically add to list\n        list_thread* old_root;\n        do {\n            old_root = my_root;\n            t->next = old_root;\n        } while( my_root.compare_and_swap( t, old_root )!=old_root );\n        return *t;\n    }\n\n    //! Destroy the bag and threads in it.\n    ~private_thread_bag() {\n        while( my_root ) {\n            // Unlink thread from list.\n            list_thread* t = my_root;\n            my_root = t->next;\n            // Destroy and deallocate the thread.\n            t->~list_thread();\n            my_allocator.deallocate(static_cast<padded<list_thread>*>(t),1);\n        }\n    }\n};\n\n//! Forward declaration\nvoid wakeup_some_tbb_threads();\n\n//! Type-independent part of class generic_connection.\n/** One to one map from server threads to jobs, and associated reference counting. */\nclass thread_map : public thread_map_base {\npublic:\n    typedef rml::client::size_type size_type;\n    //! ctor\n    thread_map( wait_counter& fc, ::rml::client& client ) :\n        all_visited_at_least_once(false), my_min_stack_size(0), my_server_ref_count(1),\n        my_client_ref_count(1), my_client(client), my_factory_counter(fc)\n    { my_unrealized_threads = 0; }\n    //! dtor\n    ~thread_map() {}\n    typedef array_type::iterator iterator;\n    iterator begin() {return my_array.begin();}\n    iterator end() {return my_array.end();}\n    void bind();\n    void unbind();\n    void assist_cleanup( bool assist_null_only );\n\n    /** Returns number of unrealized threads to create. */\n    size_type wakeup_tbb_threads( size_type n );\n    bool wakeup_next_thread( iterator i, tbb_connection_v2& conn );\n    void release_tbb_threads( server_thread* t );\n    void adjust_balance( int delta );\n\n    //! Add a server_thread object to the map, but do not bind it.\n    /** Return NULL if out of unrealized threads. */\n    value_type* add_one_thread( bool is_omp_thread_ );\n\n    void bind_one_thread( rml::server& server, value_type& x );\n\n    void remove_client_ref();\n    int add_server_ref() {return my_server_ref_count.add_ref();}\n    int remove_server_ref() {return my_server_ref_count.remove_ref();}\n\n    ::rml::client& client() const {return my_client;}\n\n    size_type get_unrealized_threads() { return my_unrealized_threads; }\n\nprivate:\n    private_thread_bag my_private_threads;\n    bool all_visited_at_least_once;\n    array_type my_array;\n    size_t my_min_stack_size;\n    tbb::atomic<size_type> my_unrealized_threads;\n\n    //! Number of threads referencing *this, plus one extra.\n    /** When it becomes zero, the containing server object can be safely deleted. */\n    ref_count my_server_ref_count;\n\n    //! Number of jobs that need cleanup, plus one extra.\n    /** When it becomes zero, acknowledge_close_connection is called. */\n    ref_count my_client_ref_count;\n\n    ::rml::client& my_client;\n    //! Counter owned by factory that produced this thread_map.\n    wait_counter& my_factory_counter;\n};\n\nvoid thread_map::bind_one_thread( rml::server& server, value_type& x ) {\n    // Add one to account for the thread referencing this map hereforth.\n    server_thread& t = x.thread();\n    my_server_ref_count.add_ref();\n    my_client_ref_count.add_ref();\n#if TBB_USE_ASSERT\n    __TBB_ASSERT( t.add_ref()==1, NULL );\n#else\n    t.add_ref();\n#endif\n    // Have responsibility to start the thread.\n    t.my_conn = &server;\n    t.my_ja = &x.my_automaton;\n    t.launch( my_min_stack_size );\n    /* Must wake thread up so it can fill in its \"my_job\" field in *this.\n       Otherwise deadlock can occur where wait_for_job spins on thread that is sleeping. */\n    __TBB_ASSERT( t.state!=ts_tbb_busy, NULL );\n    t.wakeup( ts_idle, ts_asleep );\n}\n\nthread_map::value_type* thread_map::add_one_thread( bool is_omp_thread_ ) {\n    size_type u;\n    do {\n        u = my_unrealized_threads;\n        if( !u ) return NULL;\n    } while( my_unrealized_threads.compare_and_swap(u-1,u)!=u );\n    server_thread& t = my_private_threads.add_one_thread();\n    t.is_omp_thread = is_omp_thread_;\n    __TBB_ASSERT( u>=1, NULL );\n    t.my_index = u - 1;\n    __TBB_ASSERT( t.state!=ts_tbb_busy, NULL );\n    t.my_extra_state = t.is_omp_thread ? ts_none : ts_created;\n\n    iterator i = t.my_map_pos = my_array.grow_by(1);\n    value_type& v = *i;\n    v.my_thread = &t;\n    return &v;\n}\n\nvoid thread_map::bind() {\n    ++my_factory_counter;\n    my_min_stack_size = my_client.min_stack_size();\n    __TBB_ASSERT( my_unrealized_threads==0, \"already called bind?\" );\n    my_unrealized_threads = my_client.max_job_count();\n}\n\nvoid thread_map::unbind() {\n    // Ask each server_thread to cleanup its job for this server.\n    for( iterator i=begin(); i!=end(); ++i ) {\n        server_thread& t = i->thread();\n        t.terminate = true;\n        t.wakeup( ts_idle, ts_asleep );\n    }\n    // Remove extra ref to client.\n    remove_client_ref();\n}\n\nvoid thread_map::assist_cleanup( bool assist_null_only ) {\n    // To avoid deadlock, the current thread *must* help out with cleanups that have not started,\n    // because the thread that created the job may be busy for a long time.\n    for( iterator i = begin(); i!=end(); ++i ) {\n        rml::job* j=0;\n        job_automaton& ja = i->my_automaton;\n        if( assist_null_only ? ja.try_plug_null() : ja.try_plug(j) ) {\n            if( j ) {\n                my_client.cleanup(*j);\n            } else {\n                // server thread did not get a chance to create a job.\n            }\n            remove_client_ref();\n        }\n    }\n}\n\nthread_map::size_type thread_map::wakeup_tbb_threads( size_type n ) {\n    __TBB_ASSERT(n>0,\"must specify positive number of threads to wake up\");\n    iterator e = end();\n    for( iterator k=begin(); k!=e; ++k ) {\n        // If another thread added *k, there is a tiny timing window where thread() is invalid.\n        server_thread& t = k->wait_for_thread();\n        thread_state_t thr_s = t.read_state();\n        if( t.read_extra_state()==ts_created || thr_s==ts_tbb_busy || thr_s==ts_done )\n            continue;\n        if( --the_balance>=0 ) { // try to withdraw a coin from the deposit\n            while( !t.try_grab_for( ts_tbb_busy ) ) {\n                thr_s = t.read_state();\n                if( thr_s==ts_tbb_busy || thr_s==ts_done ) {\n                    // we lost; move on to the next.\n                    ++the_balance;\n                    goto skip;\n                }\n            }\n            if( --n==0 )\n                return 0;\n        } else {\n            // overdraft.\n            ++the_balance;\n            break;\n        }\nskip:\n        ;\n    }\n    return n<my_unrealized_threads ? n : size_type(my_unrealized_threads);\n}\n#else /* RML_USE_WCRM */\n\nclass thread_map : no_copy {\n    friend class omp_connection_v2;\n    typedef ::std::hash_map<uintptr_t,server_thread*> hash_map_type;\n    size_t my_min_stack_size;\n    size_t my_unrealized_threads;\n    ::rml::client& my_client;\n    //! Counter owned by factory that produced this thread_map.\n    wait_counter& my_factory_counter;\n    //! Ref counters\n    ref_count my_server_ref_count;\n    ref_count my_client_ref_count;\n    // FIXME: pad this?\n    hash_map_type my_map;\n    bool shutdown_in_progress;\n    std::vector<IExecutionResource*> original_exec_resources;\n    tbb::cache_aligned_allocator<padded<tbb_server_thread> > my_tbb_allocator;\n    tbb::cache_aligned_allocator<padded<omp_server_thread> > my_omp_allocator;\n    tbb::cache_aligned_allocator<padded<thread_scavenger_thread> > my_scavenger_allocator;\n    IResourceManager* my_concrt_resource_manager;\n    IScheduler* my_scheduler;\n    ISchedulerProxy* my_scheduler_proxy;\n    tbb::atomic<thread_scavenger_thread*> my_thread_scavenger_thread;\n#if TBB_USE_ASSERT\n    tbb::atomic<int> n_add_vp_requests;\n    tbb::atomic<int> n_thread_scavengers_created;\n#endif\npublic:\n    thread_map( wait_counter& fc, ::rml::client& client ) :\n        my_min_stack_size(0), my_client(client), my_factory_counter(fc),\n        my_server_ref_count(1), my_client_ref_count(1), shutdown_in_progress(false),\n        my_concrt_resource_manager(NULL), my_scheduler(NULL), my_scheduler_proxy(NULL)\n    {\n        my_thread_scavenger_thread = NULL;\n#if TBB_USE_ASSERT\n        n_add_vp_requests = 0;\n        n_thread_scavengers_created;\n#endif\n    }\n\n    ~thread_map() {\n        __TBB_ASSERT( n_thread_scavengers_created<=1, \"too many scavenger thread created\" );\n        // if thread_scavenger_thread is launched, wait for it to complete\n        if( my_thread_scavenger_thread ) {\n            __TBB_ASSERT( my_thread_scavenger_thread!=c_claimed, NULL );\n            while( my_thread_scavenger_thread->read_state()==ts_busy )\n                __TBB_Yield();\n            thread_scavenger_thread* tst = my_thread_scavenger_thread;\n            my_scavenger_allocator.deallocate(static_cast<padded<thread_scavenger_thread>*>(tst),1);\n        }\n        // deallocate thread contexts\n        for( hash_map_type::const_iterator hi=my_map.begin(); hi!=my_map.end(); ++hi ) {\n            server_thread* thr = hi->second;\n            if( thr->tbb_thread ) {\n                while( ((tbb_server_thread*)thr)->activation_count>1 )\n                    __TBB_Yield();\n                ((tbb_server_thread*)thr)->~tbb_server_thread();\n                my_tbb_allocator.deallocate(static_cast<padded<tbb_server_thread>*>(thr),1);\n            } else {\n                ((omp_server_thread*)thr)->~omp_server_thread();\n                my_omp_allocator.deallocate(static_cast<padded<omp_server_thread>*>(thr),1);\n            }\n        }\n        if( my_scheduler_proxy ) {\n            my_scheduler_proxy->Shutdown();\n            my_concrt_resource_manager->Release();\n            __TBB_ASSERT( my_scheduler, NULL );\n            delete my_scheduler;\n        } else {\n            __TBB_ASSERT( !my_scheduler, NULL );\n        }\n    }\n    typedef hash_map_type::key_type key_type;\n    typedef hash_map_type::value_type value_type;\n    typedef hash_map_type::iterator iterator;\n    iterator begin() {return my_map.begin();}\n    iterator end() {return my_map.end();}\n    iterator find( key_type k ) {return my_map.find( k );}\n    iterator insert( key_type k, server_thread* v ) {\n        std::pair<iterator,bool> res = my_map.insert( value_type(k,v) );\n        return res.first;\n    }\n    void bind( IScheduler* s ) {\n        ++my_factory_counter;\n        if( s ) {\n            my_unrealized_threads = s->GetPolicy().GetPolicyValue( MaxConcurrency );\n            __TBB_ASSERT( my_unrealized_threads>0, NULL );\n            my_scheduler = s;\n            my_concrt_resource_manager = CreateResourceManager(); // reference count==3 when first created.\n            my_scheduler_proxy = my_concrt_resource_manager->RegisterScheduler( s, CONCRT_RM_VERSION_1 );\n            my_scheduler_proxy->RequestInitialVirtualProcessors( false );\n        }\n    }\n    bool is_closing() { return shutdown_in_progress; }\n    void unbind( rml::server& server, ::tbb::spin_mutex& mtx );\n    void add_client_ref() { my_server_ref_count.add_ref(); }\n    void remove_client_ref();\n    void add_server_ref() {my_server_ref_count.add_ref();}\n    int remove_server_ref() {return my_server_ref_count.remove_ref();}\n    int get_server_ref_count() { int k = my_server_ref_count.my_ref_count; return k; }\n    void assist_cleanup( bool assist_null_only );\n    void adjust_balance( int delta );\n    int current_balance() const {int k = the_balance; return k;}\n    ::rml::client& client() const {return my_client;}\n    void register_as_master( server::execution_resource_t& v ) const { (IExecutionResource*&)v = my_scheduler_proxy ? my_scheduler_proxy->SubscribeCurrentThread() : NULL; }\n    // Remove() should be called from the same thread that subscribed the current h/w thread (i.e., the one that\n    // called register_as_master() ).\n    void unregister( server::execution_resource_t v ) const {if( v ) ((IExecutionResource*)v)->Remove( my_scheduler );}\n    void add_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count, tbb_connection_v2& conn, ::tbb::spin_mutex& mtx );\n    void add_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count, omp_connection_v2& conn, ::tbb::spin_mutex& mtx );\n    void remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx );\n    void mark_virtual_processors_as_lent( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx );\n    void create_oversubscribers( unsigned n, std::vector<server_thread*>& thr_vec, omp_connection_v2& conn, ::tbb::spin_mutex& mtx );\n    void wakeup_tbb_threads( int c, ::tbb::spin_mutex& mtx );\n    void mark_virtual_processors_as_returned( IVirtualProcessorRoot** vprocs, unsigned int count, tbb::spin_mutex& mtx );\n    inline void addto_original_exec_resources( IExecutionResource* r, ::tbb::spin_mutex& mtx ) {\n        ::tbb::spin_mutex::scoped_lock lck(mtx);\n        __TBB_ASSERT( !is_closing(), \"trying to register master while connection is being shutdown?\" );\n        original_exec_resources.push_back( r );\n    }\n#if !__RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\n    void allocate_thread_scavenger( IExecutionResource* v );\n#endif\n    inline thread_scavenger_thread* get_thread_scavenger() { return my_thread_scavenger_thread; }\n};\n\ngarbage_connection_queue connections_to_reclaim;\nconnection_scavenger_thread connection_scavenger;\n\n#endif /* !RML_USE_WCRM */\n\n//------------------------------------------------------------------------\n// generic_connection\n//------------------------------------------------------------------------\n\ntemplate<typename Server, typename Client>\nstruct connection_traits {};\n\n// head of the active tbb connections\nstatic tbb::atomic<uintptr_t> active_tbb_connections;\nstatic tbb::atomic<int> current_tbb_conn_readers;\nstatic size_t current_tbb_conn_reader_epoch;\nstatic tbb::atomic<size_t> close_tbb_connection_event_count;\n\n#if RML_USE_WCRM\ntemplate<typename Connection>\nvoid make_job( Connection& c, server_thread& t );\n#endif\n\ntemplate<typename Server, typename Client>\nclass generic_connection: public Server, no_copy {\n    version_type version() const __TBB_override {return SERVER_VERSION;}\n    void yield() __TBB_override {thread_monitor::yield();}\n    void independent_thread_number_changed( int delta ) __TBB_override { my_thread_map.adjust_balance( -delta ); }\n    unsigned default_concurrency() const __TBB_override { return the_default_concurrency; }\n    friend void wakeup_some_tbb_threads();\n    friend class connection_scavenger_thread;\n\nprotected:\n    thread_map my_thread_map;\n    generic_connection* next_conn;\n    size_t my_ec;\n#if RML_USE_WCRM\n    // FIXME: pad it?\n    tbb::spin_mutex map_mtx;\n    IScheduler* my_scheduler;\n    void do_open( IScheduler* s ) {\n        my_scheduler = s;\n        my_thread_map.bind( s );\n    }\n    bool is_closing() { return my_thread_map.is_closing(); }\n    void request_close_connection( bool existing );\n#else\n    void do_open() {my_thread_map.bind();}\n    void request_close_connection( bool );\n#endif /* RML_USE_WCRM */\n    //! Make destructor virtual\n    virtual ~generic_connection() {}\n#if !RML_USE_WCRM\n    generic_connection( wait_counter& fc, Client& c ) : my_thread_map(fc,c), next_conn(NULL), my_ec(0) {}\n#else\n    generic_connection( wait_counter& fc, Client& c ) :\n            my_thread_map(fc,c), next_conn(NULL), my_ec(0), map_mtx(), my_scheduler(NULL) {}\n    void add_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count );\n    void remove_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count );\n    void notify_resources_externally_busy( IVirtualProcessorRoot** vprocs, unsigned int count ) { my_thread_map.mark_virtual_processors_as_lent( vprocs, count, map_mtx ); }\n    void notify_resources_externally_idle( IVirtualProcessorRoot** vprocs, unsigned int count ) {\n        my_thread_map.mark_virtual_processors_as_returned( vprocs, count, map_mtx );\n    }\n#endif /* !RML_USE_WCRM */\n\npublic:\n    typedef Server server_type;\n    typedef Client client_type;\n    Client& client() const {return static_cast<Client&>(my_thread_map.client());}\n    void set_scratch_ptr( job& j, void* ptr ) { ::rml::server::scratch_ptr(j) = ptr; }\n#if RML_USE_WCRM\n    template<typename Connection>\n    friend void make_job( Connection& c, server_thread& t );\n    void add_server_ref ()   {my_thread_map.add_server_ref();}\n    void remove_server_ref() {if( my_thread_map.remove_server_ref()==0 ) delete this;}\n    void add_client_ref ()   {my_thread_map.add_client_ref();}\n    void remove_client_ref() {my_thread_map.remove_client_ref();}\n#else /* !RML_USE_WCRM */\n    int  add_server_ref ()   {return my_thread_map.add_server_ref();}\n    void remove_server_ref() {if( my_thread_map.remove_server_ref()==0 ) delete this;}\n    void remove_client_ref() {my_thread_map.remove_client_ref();}\n    void make_job( server_thread& t, job_automaton& ja );\n#endif /* RML_USE_WCRM */\n    static generic_connection* get_addr( uintptr_t addr_ex ) {\n        return reinterpret_cast<generic_connection*>( addr_ex&~(uintptr_t)3 );\n    }\n};\n\n//------------------------------------------------------------------------\n// TBB server\n//------------------------------------------------------------------------\n\ntemplate<>\nstruct connection_traits<tbb_server,tbb_client> {\n    static const bool assist_null_only = true;\n    static const bool is_tbb = true;\n};\n\n//! Represents a server/client binding.\n/** The internal representation uses inheritance for the server part and a pointer for the client part. */\nclass tbb_connection_v2: public generic_connection<tbb_server,tbb_client> {\n    void adjust_job_count_estimate( int delta ) __TBB_override;\n#if !RML_USE_WCRM\n#if _WIN32||_WIN64\n    void register_master ( rml::server::execution_resource_t& /*v*/ ) __TBB_override {}\n    void unregister_master ( rml::server::execution_resource_t /*v*/ ) __TBB_override {}\n#endif\n#else\n    void register_master ( rml::server::execution_resource_t& v ) __TBB_override {\n        my_thread_map.register_as_master(v);\n        if( v ) ++nesting;\n    }\n    void unregister_master ( rml::server::execution_resource_t v ) __TBB_override {\n        if( v ) {\n            __TBB_ASSERT( nesting>0, NULL );\n            if( --nesting==0 ) {\n#if !__RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\n                my_thread_map.allocate_thread_scavenger( (IExecutionResource*)v );\n#endif\n            }\n        }\n        my_thread_map.unregister(v);\n    }\n    IScheduler* create_scheduler() {return( scheduler<tbb_connection_v2>::create( *this ) );}\n    friend void  free_all_connections( uintptr_t );\n    friend class scheduler<tbb_connection_v2>;\n    friend class execution_context;\n    friend class connection_scavenger_thread;\n#endif /* RML_USE_WCRM */\n    friend void wakeup_some_tbb_threads();\n    //! Estimate on number of jobs without threads working on them.\n    tbb::atomic<int> my_slack;\n    friend class dummy_class_to_shut_up_gratuitous_warning_from_gcc_3_2_3;\n#if TBB_USE_ASSERT\n    tbb::atomic<int> my_job_count_estimate;\n#endif /* TBB_USE_ASSERT */\n\n    tbb::atomic<int> n_adjust_job_count_requests;\n#if RML_USE_WCRM\n    tbb::atomic<int> nesting;\n#endif\n\n    // dtor\n    ~tbb_connection_v2();\n\npublic:\n#if RML_USE_WCRM\n    typedef tbb_server_thread server_thread_type;\n#endif\n    //! True if there is slack that try_process can use.\n    bool has_slack() const {return my_slack>0;}\n\n#if RML_USE_WCRM\n    bool try_process( job& job )\n#else\n    bool try_process( server_thread& t, job& job )\n#endif\n    {\n        bool visited = false;\n        // No check for my_slack>0 here because caller is expected to do that check.\n        int k = --my_slack;\n        if( k>=0 ) {\n#if !RML_USE_WCRM\n            t.my_extra_state = ts_visited; // remember the thread paid a trip to process() at least once\n#endif\n            client().process(job);\n            visited = true;\n        }\n        ++my_slack;\n        return visited;\n    }\n\n    tbb_connection_v2( wait_counter& fc, tbb_client& client ) : generic_connection<tbb_server,tbb_client>(fc,client)\n    {\n        my_slack = 0;\n#if RML_USE_WCRM\n        nesting = 0;\n#endif\n#if TBB_USE_ASSERT\n        my_job_count_estimate = 0;\n#endif /* TBB_USE_ASSERT */\n        __TBB_ASSERT( !my_slack, NULL );\n\n#if RML_USE_WCRM\n        do_open( client.max_job_count()>0 ? create_scheduler() : NULL );\n#else\n        do_open();\n#endif /* !RML_USE_WCRM */\n        n_adjust_job_count_requests = 0;\n\n        // Acquire head of active_tbb_connections & push the connection into the list\n        uintptr_t conn;\n        do {\n            for( ; (conn=active_tbb_connections)&1; )\n                __TBB_Yield();\n        } while( active_tbb_connections.compare_and_swap( conn|1, conn )!=conn );\n\n        this->next_conn = generic_connection<tbb_server,tbb_client>::get_addr(conn);\n        // Update and release head of active_tbb_connections\n        active_tbb_connections = (uintptr_t) this; // set and release\n    }\n    inline void wakeup_tbb_threads( unsigned n ) {\n        my_thread_map.wakeup_tbb_threads( n\n#if RML_USE_WCRM\n                , map_mtx\n#endif\n                );\n    }\n#if RML_USE_WCRM\n    inline int get_nesting_level() { return nesting; }\n#else\n    inline bool wakeup_next_thread( thread_map::iterator i ) {return my_thread_map.wakeup_next_thread( i, *this );}\n    inline thread_map::size_type get_unrealized_threads () {return my_thread_map.get_unrealized_threads();}\n#endif /* !RML_USE_WCRM */\n};\n\n//------------------------------------------------------------------------\n// OpenMP server\n//------------------------------------------------------------------------\n\ntemplate<>\nstruct connection_traits<omp_server,omp_client> {\n    static const bool assist_null_only = false;\n    static const bool is_tbb = false;\n};\n\nclass omp_connection_v2: public generic_connection<omp_server,omp_client> {\n#if !RML_USE_WCRM\n    int  current_balance() const __TBB_override {return the_balance;}\n#else\n    friend void  free_all_connections( uintptr_t );\n    friend class scheduler<omp_connection_v2>;\n    int current_balance() const __TBB_override {return my_thread_map.current_balance();}\n#endif /* !RML_USE_WCRM */\n    int  try_increase_load( size_type n, bool strict ) __TBB_override;\n    void decrease_load( size_type n ) __TBB_override;\n    void get_threads( size_type request_size, void* cookie, job* array[] ) __TBB_override;\n#if !RML_USE_WCRM\n#if _WIN32||_WIN64\n    void register_master ( rml::server::execution_resource_t& /*v*/ ) __TBB_override {}\n    void unregister_master ( rml::server::execution_resource_t /*v*/ ) __TBB_override {}\n#endif\n#else\n    void register_master ( rml::server::execution_resource_t& v ) __TBB_override {\n        my_thread_map.register_as_master( v );\n        my_thread_map.addto_original_exec_resources( (IExecutionResource*)v, map_mtx );\n    }\n    void unregister_master ( rml::server::execution_resource_t v ) __TBB_override { my_thread_map.unregister(v); }\n#endif /* !RML_USE_WCRM */\n#if _WIN32||_WIN64\n    void deactivate( rml::job* j ) __TBB_override;\n    void reactivate( rml::job* j ) __TBB_override;\n#endif /* _WIN32||_WIN64 */\n#if RML_USE_WCRM\npublic:\n    typedef omp_server_thread server_thread_type;\nprivate:\n    IScheduler* create_scheduler() {return( scheduler<omp_connection_v2>::create( *this ) );}\n#endif /* RML_USE_WCRM */\npublic:\n#if TBB_USE_ASSERT\n    //! Net change in delta caused by this connection.\n    /** Should be zero when connection is broken */\n    tbb::atomic<int> net_delta;\n#endif /* TBB_USE_ASSERT */\n\n    omp_connection_v2( wait_counter& fc, omp_client& client ) : generic_connection<omp_server,omp_client>(fc,client) {\n#if TBB_USE_ASSERT\n        net_delta = 0;\n#endif /* TBB_USE_ASSERT */\n#if RML_USE_WCRM\n        do_open( create_scheduler() );\n#else\n        do_open();\n#endif /* RML_USE_WCRM */\n    }\n    ~omp_connection_v2() {__TBB_ASSERT( net_delta==0, \"net increase/decrease of load is nonzero\" );}\n};\n\n#if !RML_USE_WCRM\n/* to deal with cases where the machine is oversubscribed; we want each thread to trip to try_process() at least once */\n/* this should not involve computing the_balance */\nbool thread_map::wakeup_next_thread( thread_map::iterator this_thr, tbb_connection_v2& conn ) {\n    if( all_visited_at_least_once )\n        return false;\n\n    iterator e = end();\nretry:\n    bool exist = false;\n    iterator k=this_thr;\n    for( ++k; k!=e; ++k ) {\n        // If another thread added *k, there is a tiny timing window where thread() is invalid.\n        server_thread& t = k->wait_for_thread();\n        if( t.my_extra_state!=ts_visited )\n            exist = true;\n        if( t.read_state()!=ts_tbb_busy && t.my_extra_state==ts_started )\n            if( t.try_grab_for( ts_tbb_busy ) )\n                return true;\n    }\n    for( k=begin(); k!=this_thr; ++k ) {\n        server_thread& t = k->wait_for_thread();\n        if( t.my_extra_state!=ts_visited )\n            exist = true;\n        if( t.read_state()!=ts_tbb_busy && t.my_extra_state==ts_started )\n            if( t.try_grab_for( ts_tbb_busy ) )\n                return true;\n    }\n\n    if( exist )\n        if( conn.has_slack() )\n            goto retry;\n    else\n        all_visited_at_least_once = true;\n    return false;\n}\n\nvoid thread_map::release_tbb_threads( server_thread* t ) {\n    for( ; t; t = t->link ) {\n        while( t->read_state()!=ts_asleep )\n            __TBB_Yield();\n        t->my_extra_state = ts_started;\n    }\n}\n#endif /* !RML_USE_WCRM */\n\nvoid thread_map::adjust_balance( int delta ) {\n    int new_balance = the_balance += delta;\n    if( new_balance>0 && 0>=new_balance-delta /*== old the_balance*/ )\n        wakeup_some_tbb_threads();\n}\n\nvoid thread_map::remove_client_ref() {\n    int k = my_client_ref_count.remove_ref();\n    if( k==0 ) {\n        // Notify factory that thread has crossed back into RML.\n        --my_factory_counter;\n        // Notify client that RML is done with the client object.\n        my_client.acknowledge_close_connection();\n    }\n}\n\n#if RML_USE_WCRM\n/** Not a member of generic_connection because we need Connection to be the derived class. */\ntemplate<typename Connection>\nvoid make_job( Connection& c, typename Connection::server_thread_type& t ) {\n    if( t.my_job_automaton.try_acquire() ) {\n        rml::job* j = t.my_client.create_one_job();\n        __TBB_ASSERT( j!=NULL, \"client:::create_one_job returned NULL\" );\n        __TBB_ASSERT( (intptr_t(j)&1)==0, \"client::create_one_job returned misaligned job\" );\n        t.my_job_automaton.set_and_release( j );\n        c.set_scratch_ptr( *j, (void*) &t );\n    }\n}\n#endif /* RML_USE_WCRM */\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n// Suppress \"conditional expression is constant\" warning.\n#pragma warning( push )\n#pragma warning( disable: 4127 )\n#endif\n#if RML_USE_WCRM\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::request_close_connection( bool exiting ) {\n    // for TBB connections, exiting should always be false\n    if( connection_traits<Server,Client>::is_tbb )\n        __TBB_ASSERT( !exiting, NULL);\n#if TBB_USE_ASSERT\n    else if( exiting )\n        reinterpret_cast<omp_connection_v2*>(this)->net_delta = 0;\n#endif\n    if( exiting ) {\n        uintptr_t tail = connections_to_reclaim.tail;\n        while( connections_to_reclaim.tail.compare_and_swap( garbage_connection_queue::plugged, tail )!=tail )\n            __TBB_Yield();\n        my_thread_map.unbind( *this, map_mtx );\n        my_thread_map.assist_cleanup( connection_traits<Server,Client>::assist_null_only );\n        // It is assumed that the client waits for all other threads to terminate before\n        // calling request_close_connection with true.  Thus, it is safe to return all\n        // outstanding connection objects that are reachable. It is possible that there may\n        // be some unreachable connection objects lying somewhere.\n        free_all_connections( connection_scavenger.grab_and_prepend( this ) );\n        return;\n    }\n#else /* !RML_USE_WCRM */\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::request_close_connection( bool ) {\n#endif /* RML_USE_WCRM */\n    if( connection_traits<Server,Client>::is_tbb ) {\n        // acquire the head of active tbb connections\n        uintptr_t conn;\n        do {\n            for( ; (conn=active_tbb_connections)&1; )\n                __TBB_Yield();\n        } while( active_tbb_connections.compare_and_swap( conn|1, conn )!=conn );\n\n        // Locate the current connection\n        generic_connection* pred_conn = NULL;\n        generic_connection* curr_conn = (generic_connection*) conn;\n        for( ; curr_conn && curr_conn!=this; curr_conn=curr_conn->next_conn )\n            pred_conn = curr_conn;\n        __TBB_ASSERT( curr_conn==this, \"the current connection is not in the list?\" );\n\n        // Remove this from the list\n        if( pred_conn ) {\n            pred_conn->next_conn = curr_conn->next_conn;\n            active_tbb_connections = reinterpret_cast<uintptr_t>(generic_connection<tbb_server,tbb_client>::get_addr(active_tbb_connections)); // release it\n        } else\n            active_tbb_connections = (uintptr_t) curr_conn->next_conn; // update & release it\n        curr_conn->next_conn = NULL;\n        // Increment the tbb connection close event count\n        my_ec = ++close_tbb_connection_event_count;\n        // Wait happens in tbb_connection_v2::~tbb_connection_v2()\n    }\n#if RML_USE_WCRM\n    my_thread_map.unbind( *this, map_mtx );\n    my_thread_map.assist_cleanup( connection_traits<Server,Client>::assist_null_only );\n    connection_scavenger.add_request( this );\n#else\n    my_thread_map.unbind();\n    my_thread_map.assist_cleanup( connection_traits<Server,Client>::assist_null_only );\n    // Remove extra reference\n    remove_server_ref();\n#endif\n}\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n#pragma warning( pop )\n#endif\n\n#if RML_USE_WCRM\n\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{}\n\ntemplate<>\nvoid generic_connection<tbb_server,tbb_client>::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    my_thread_map.add_virtual_processors( vproots, count, (tbb_connection_v2&)*this, map_mtx );\n}\ntemplate<>\nvoid generic_connection<omp_server,omp_client>::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    // For OMP, since it uses ScheudlerPolicy of MinThreads==MaxThreads, this is called once when\n    // RequestInitialVirtualProcessors() is  called.\n    my_thread_map.add_virtual_processors( vproots, count, (omp_connection_v2&)*this, map_mtx );\n}\n\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    __TBB_ASSERT( false, \"should not be called\" );\n}\n/* For OMP, RemoveVirtualProcessors() will never be called. */\n\ntemplate<>\nvoid generic_connection<tbb_server,tbb_client>::remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    my_thread_map.remove_virtual_processors( vproots, count, map_mtx );\n}\n\nvoid tbb_connection_v2::adjust_job_count_estimate( int delta ) {\n#if TBB_USE_ASSERT\n    my_job_count_estimate += delta;\n#endif /* TBB_USE_ASSERT */\n    // Atomically update slack.\n    int c = my_slack+=delta;\n    if( c>0 ) {\n        ++n_adjust_job_count_requests;\n        my_thread_map.wakeup_tbb_threads( c, map_mtx );\n        --n_adjust_job_count_requests;\n    }\n}\n#endif /* RML_USE_WCRM */\n\ntbb_connection_v2::~tbb_connection_v2() {\n#if TBB_USE_ASSERT\n    if( my_job_count_estimate!=0 ) {\n        fprintf(stderr, \"TBB client tried to disconnect with non-zero net job count estimate of %d\\n\", int(my_job_count_estimate ));\n        abort();\n    }\n    __TBB_ASSERT( !my_slack, \"attempt to destroy tbb_server with nonzero slack\" );\n    __TBB_ASSERT( this!=static_cast<tbb_connection_v2*>(generic_connection<tbb_server,tbb_client >::get_addr(active_tbb_connections)), \"request_close_connection() must be called\" );\n#endif /* TBB_USE_ASSERT */\n#if !RML_USE_WCRM\n    // If there are other threads ready for work, give them coins\n    if( the_balance>0 )\n        wakeup_some_tbb_threads();\n#endif\n    // Someone might be accessing my data members\n    while( current_tbb_conn_readers>0 && (ptrdiff_t)(my_ec-current_tbb_conn_reader_epoch)>0 )\n        __TBB_Yield();\n}\n\n#if !RML_USE_WCRM\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::make_job( server_thread& t, job_automaton& ja ) {\n    if( ja.try_acquire() ) {\n        rml::job* j = client().create_one_job();\n        __TBB_ASSERT( j!=NULL, \"client:::create_one_job returned NULL\" );\n        __TBB_ASSERT( (intptr_t(j)&1)==0, \"client::create_one_job returned misaligned job\" );\n        ja.set_and_release( j );\n        __TBB_ASSERT( t.my_conn && t.my_ja && t.my_job==NULL, NULL );\n        t.my_job  = j;\n        set_scratch_ptr( *j, (void*) &t );\n    }\n}\n\nvoid tbb_connection_v2::adjust_job_count_estimate( int delta ) {\n#if TBB_USE_ASSERT\n    my_job_count_estimate += delta;\n#endif /* TBB_USE_ASSERT */\n    // Atomically update slack.\n    int c = my_slack+=delta;\n    if( c>0 ) {\n        ++n_adjust_job_count_requests;\n        // The client has work to do and there are threads available\n        thread_map::size_type n = my_thread_map.wakeup_tbb_threads(c);\n\n        server_thread* new_threads_anchor = NULL;\n        thread_map::size_type i;\n        {\n        tbb::internal::affinity_helper fpa;\n        for( i=0; i<n; ++i ) {\n            // Obtain unrealized threads\n            thread_map::value_type* k = my_thread_map.add_one_thread( false );\n            if( !k )\n                // No unrealized threads left.\n                break;\n            // Eagerly start the thread off.\n            fpa.protect_affinity_mask( /*restore_process_mask=*/true );\n            my_thread_map.bind_one_thread( *this, *k );\n            server_thread& t = k->thread();\n            __TBB_ASSERT( !t.link, NULL );\n            t.link = new_threads_anchor;\n            new_threads_anchor = &t;\n        }\n        // Implicit destruction of fpa resets original affinity mask.\n        }\n\n        thread_map::size_type j=0;\n        for( ; the_balance>0 && j<i; ++j ) {\n            if( --the_balance>=0 ) {\n                // Withdraw a coin from the bank\n                __TBB_ASSERT( new_threads_anchor, NULL );\n\n                server_thread* t = new_threads_anchor;\n                new_threads_anchor = t->link;\n                while( !t->try_grab_for( ts_tbb_busy ) )\n                    __TBB_Yield();\n                t->my_extra_state = ts_started;\n            } else {\n                // Overdraft. return it to the bank\n                ++the_balance;\n                break;\n            }\n        }\n        __TBB_ASSERT( i-j!=0||new_threads_anchor==NULL, NULL );\n        // Mark the ones that did not get started as eligible for being snatched.\n        if( new_threads_anchor )\n            my_thread_map.release_tbb_threads( new_threads_anchor );\n\n        --n_adjust_job_count_requests;\n    }\n}\n#endif /* RML_USE_WCRM */\n\n#if RML_USE_WCRM\nint omp_connection_v2::try_increase_load( size_type n, bool strict ) {\n    __TBB_ASSERT(int(n)>=0,NULL);\n    if( strict ) {\n        the_balance -= int(n);\n    } else {\n        int avail, old;\n        do {\n            avail = the_balance;\n            if( avail<=0 ) {\n                // No atomic read-write-modify operation necessary.\n                return avail;\n            }\n            // Don't read the_system_balance; if it changes, compare_and_swap will fail anyway.\n            old = the_balance.compare_and_swap( int(n)<avail ? avail-n : 0, avail );\n        } while( old!=avail );\n        if( int(n)>avail )\n            n=avail;\n    }\n#if TBB_USE_ASSERT\n    net_delta += n;\n#endif /* TBB_USE_ASSERT */\n    return n;\n}\n\nvoid omp_connection_v2::decrease_load( size_type /*n*/ ) {}\n\nvoid omp_connection_v2::get_threads( size_type request_size, void* cookie, job* array[] ) {\n    unsigned index = 0;\n    std::vector<omp_server_thread*> enlisted(request_size);\n    std::vector<thread_grab_t> to_activate(request_size);\n\n    if( request_size==0 ) return;\n\n    {\n        tbb::spin_mutex::scoped_lock lock(map_mtx);\n\n        __TBB_ASSERT( !is_closing(), \"try to get threads while connection is being shutdown?\" );\n\n        for( int scan=0; scan<2; ++scan ) {\n            for( thread_map::iterator i=my_thread_map.begin(); i!=my_thread_map.end(); ++i ) {\n                omp_server_thread* thr = (omp_server_thread*) (*i).second;\n                // in the first scan, skip VPs that are lent\n                if( scan==0 && thr->is_lent() ) continue;\n                thread_grab_t res = thr->try_grab_for();\n                if( res!=wk_failed ) {// && if is not busy by some other scheduler\n                    to_activate[index] = res;\n                    enlisted[index] = thr;\n                    if( ++index==request_size )\n                        goto activate_threads;\n                }\n            }\n        }\n    }\n\nactivate_threads:\n\n    for( unsigned i=0; i<index; ++i ) {\n        omp_server_thread* thr = enlisted[i];\n        if( to_activate[i]==wk_from_asleep )\n            thr->get_virtual_processor()->Activate( thr );\n        job* j = thr->wait_for_job();\n        array[i] = j;\n        thr->omp_data.produce( client(), j, cookie, i PRODUCE_ARG(*this) );\n    }\n\n    if( index==request_size )\n        return;\n\n    // If we come to this point, it must be because dynamic==false\n    // Create Oversubscribers..\n\n    // Note that our policy is such that MinConcurrency==MaxConcurrency.\n    // RM will deliver MaxConcurrency of VirtualProcessors and no more.\n    __TBB_ASSERT( request_size>index, NULL );\n    unsigned n = request_size - index;\n    std::vector<server_thread*> thr_vec(n);\n    typedef std::vector<server_thread*>::iterator iterator_thr;\n    my_thread_map.create_oversubscribers( n, thr_vec, *this, map_mtx );\n    for( iterator_thr ti=thr_vec.begin(); ti!=thr_vec.end(); ++ti ) {\n        omp_server_thread* thr = (omp_server_thread*) *ti;\n        __TBB_ASSERT( thr, \"thread not created?\" );\n        // Thread is already grabbed; since it is newly created, we need to activate it.\n        thr->get_virtual_processor()->Activate( thr );\n        job* j = thr->wait_for_job();\n        array[index] = j;\n        thr->omp_data.produce( client(), j, cookie, index PRODUCE_ARG(*this) );\n        ++index;\n    }\n}\n\n#if _WIN32||_WIN64\nvoid omp_connection_v2::deactivate( rml::job* j )\n{\n    my_thread_map.adjust_balance(1);\n#if TBB_USE_ASSERT\n    net_delta -= 1;\n#endif\n    omp_server_thread* thr = (omp_server_thread*) scratch_ptr( *j );\n    (thr->get_virtual_processor())->Deactivate( thr );\n}\n\nvoid omp_connection_v2::reactivate( rml::job* j )\n{\n    // Should not adjust the_balance because OMP client is supposed to\n    // do try_increase_load() to reserve the threads to use.\n    omp_server_thread* thr = (omp_server_thread*) scratch_ptr( *j );\n    (thr->get_virtual_processor())->Activate( thr );\n}\n#endif /* !_WIN32||_WIN64 */\n\n#endif  /* RML_USE_WCRM */\n\n//! Wake up some available tbb threads\nvoid wakeup_some_tbb_threads()\n{\n    /* First, atomically grab the connection, then increase the server ref count to keep\n       it from being released prematurely.  Second, check if the balance is available for TBB\n       and the tbb conneciton has slack to exploit.  If the answer is true, go ahead and\n       try to wake some up. */\n    if( generic_connection<tbb_server,tbb_client >::get_addr(active_tbb_connections)==0 )\n        // the next connection will see the change; return.\n        return;\n\nstart_it_over:\n    int n_curr_readers = ++current_tbb_conn_readers;\n    if( n_curr_readers>1 ) // I lost\n        return;\n    // if n_curr_readers==1, i am the first one, so I will take responsibility for waking tbb threads up.\n\n    // update the current epoch\n    current_tbb_conn_reader_epoch = close_tbb_connection_event_count;\n\n    // read and clear\n    // Newly added connection will not invalidate the pointer, and it will\n    // compete with the current one to claim coins.\n    // One that is about to close the connection increments the event count\n    // after it removes the connection from the list.  But it will keep around\n    // the connection until all readers including this one catch up. So, reading\n    // the head and clearing the lock bit should be o.k.\n    generic_connection<tbb_server,tbb_client>* next_conn_wake_up = generic_connection<tbb_server,tbb_client>::get_addr( active_tbb_connections );\n\n    for( ; next_conn_wake_up; ) {\n        /* some threads are creating tbb server threads; they may not see my changes made to the_balance */\n        /* When a thread is in adjust_job_count_estimate() to increase the slack\n           RML tries to activate worker threads on behalf of the requesting thread\n           by repeatedly drawing a coin from the bank optimistically and grabbing a\n           thread.  If it finds the bank overdrafted, it returns the coin back to\n           the bank and returns the control to the thread (return from the method).\n           There lies a tiny timing hole.\n\n           When the overdraft occurs (note that multiple masters may be in\n           adjust_job_count_estimate() so the_balance can be any negative value) and\n           a worker returns from the TBB work at that moment, its returning the coin\n           does not bump up the_balance over 0, so it happily returns from\n           wakeup_some_tbb_threads() without attempting to give coins to worker threads\n           that are ready.\n        */\n        while( ((tbb_connection_v2*)next_conn_wake_up)->n_adjust_job_count_requests>0 )\n            __TBB_Yield();\n\n        int bal = the_balance;\n        n_curr_readers = current_tbb_conn_readers; // get the snapshot\n        if( bal<=0 ) break;\n        // if the connection is deleted, the following will immediately return because its slack would be 0 or less.\n\n        tbb_connection_v2* tbb_conn = (tbb_connection_v2*)next_conn_wake_up;\n        int my_slack = tbb_conn->my_slack;\n        if( my_slack>0 ) tbb_conn->wakeup_tbb_threads( my_slack );\n        next_conn_wake_up = next_conn_wake_up->next_conn;\n    }\n\n    int delta = current_tbb_conn_readers -= n_curr_readers;\n    //if delta>0, more threads entered the routine since this one took the snapshot\n    if( delta>0 ) {\n        current_tbb_conn_readers = 0;\n        if( the_balance>0 && generic_connection<tbb_server,tbb_client >::get_addr(active_tbb_connections)!=0 )\n            goto start_it_over;\n    }\n\n    // Signal any connection that is waiting for me to complete my access that I am done.\n    current_tbb_conn_reader_epoch = close_tbb_connection_event_count;\n}\n\n#if !RML_USE_WCRM\nint omp_connection_v2::try_increase_load( size_type n, bool strict ) {\n    __TBB_ASSERT(int(n)>=0,NULL);\n    if( strict ) {\n        the_balance -= int(n);\n    } else {\n        int avail, old;\n        do {\n            avail = the_balance;\n            if( avail<=0 ) {\n                // No atomic read-write-modify operation necessary.\n                return avail;\n            }\n            // don't read the_balance; if it changes, compare_and_swap will fail anyway.\n            old = the_balance.compare_and_swap( int(n)<avail ? avail-n : 0, avail );\n        } while( old!=avail );\n        if( int(n)>avail )\n            n=avail;\n    }\n#if TBB_USE_ASSERT\n    net_delta += n;\n#endif /* TBB_USE_ASSERT */\n    return n;\n}\n\nvoid omp_connection_v2::decrease_load( size_type n ) {\n    __TBB_ASSERT(int(n)>=0,NULL);\n    my_thread_map.adjust_balance(int(n));\n#if TBB_USE_ASSERT\n    net_delta -= n;\n#endif /* TBB_USE_ASSERT */\n}\n\nvoid omp_connection_v2::get_threads( size_type request_size, void* cookie, job* array[] ) {\n\n    if( !request_size )\n        return;\n\n    unsigned index = 0;\n    for(;;) { // don't return until all request_size threads are grabbed.\n        // Need to grab some threads\n        thread_map::iterator k_end=my_thread_map.end();\n        for( thread_map::iterator k=my_thread_map.begin(); k!=k_end; ++k ) {\n            // If another thread added *k, there is a tiny timing window where thread() is invalid.\n            server_thread& t = k->wait_for_thread();\n            if( t.try_grab_for( ts_omp_busy ) ) {\n                // The preincrement instead of post-increment of index is deliberate.\n                job* j = k->wait_for_job();\n                array[index] = j;\n                t.omp_dispatch.produce( client(), j, cookie, index PRODUCE_ARG(*this) );\n                if( ++index==request_size )\n                    return;\n            }\n        }\n        // Need to allocate more threads\n        for( unsigned i=index; i<request_size; ++i ) {\n            __TBB_ASSERT( index<request_size, NULL );\n            thread_map::value_type* k = my_thread_map.add_one_thread( true );\n#if TBB_USE_ASSERT\n            if( !k ) {\n                // Client erred\n                __TBB_ASSERT(false, \"server::get_threads: exceeded job_count\\n\");\n            }\n#endif\n            my_thread_map.bind_one_thread( *this, *k );\n            server_thread& t = k->thread();\n            if( t.try_grab_for( ts_omp_busy ) ) {\n                job* j = k->wait_for_job();\n                array[index] = j;\n                // The preincrement instead of post-increment of index is deliberate.\n                t.omp_dispatch.produce( client(), j, cookie, index PRODUCE_ARG(*this) );\n                if( ++index==request_size )\n                    return;\n            } // else someone else snatched it.\n        }\n    }\n}\n#endif /* !RML_USE_WCRM */\n\n//------------------------------------------------------------------------\n// Methods of omp_dispatch_type\n//------------------------------------------------------------------------\nvoid omp_dispatch_type::consume() {\n    // Wait for short window between when master sets state of this thread to ts_omp_busy\n    // and master thread calls produce.\n    job_type* j;\n    tbb::internal::atomic_backoff backoff;\n    while( (j = job)==NULL ) backoff.pause();\n    job = static_cast<job_type*>(NULL);\n    client->process(*j,cookie,index);\n#if TBB_USE_ASSERT\n    // Return of method process implies \"decrease_load\" from client's viewpoint, even though\n    // the actual adjustment of the_balance only happens when this thread really goes to sleep.\n    --server->net_delta;\n#endif /* TBB_USE_ASSERT */\n}\n\n#if !RML_USE_WCRM\n#if _WIN32||_WIN64\nvoid omp_connection_v2::deactivate( rml::job* j )\n{\n#if TBB_USE_ASSERT\n    net_delta -= 1;\n#endif\n    __TBB_ASSERT( j, NULL );\n    server_thread* thr = (server_thread*) scratch_ptr( *j );\n    thr->deactivate();\n}\n\nvoid omp_connection_v2::reactivate( rml::job* j )\n{\n    // Should not adjust the_balance because OMP client is supposed to\n    // do try_increase_load() to reserve the threads to use.\n    __TBB_ASSERT( j, NULL );\n    server_thread* thr = (server_thread*) scratch_ptr( *j );\n    thr->reactivate();\n}\n#endif /* _WIN32||_WIN64 */\n\n//------------------------------------------------------------------------\n// Methods of server_thread\n//------------------------------------------------------------------------\n\nserver_thread::server_thread() :\n    ref_count(0),\n    link(NULL),\n    my_map_pos(),\n    my_conn(NULL), my_job(NULL), my_ja(NULL)\n{\n    state = ts_idle;\n    terminate = false;\n#if TBB_USE_ASSERT\n    has_active_thread = false;\n#endif /* TBB_USE_ASSERT */\n}\n\nserver_thread::~server_thread() {\n    __TBB_ASSERT( !has_active_thread, NULL );\n}\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    // Suppress overzealous compiler warnings about an initialized variable 'sink_for_alloca' not referenced\n    #pragma warning(push)\n    #pragma warning(disable:4189)\n#endif\n__RML_DECL_THREAD_ROUTINE server_thread::thread_routine( void* arg ) {\n    server_thread* self = static_cast<server_thread*>(arg);\n    AVOID_64K_ALIASING( self->my_index );\n#if TBB_USE_ASSERT\n    __TBB_ASSERT( !self->has_active_thread, NULL );\n    self->has_active_thread = true;\n#endif /* TBB_USE_ASSERT */\n    self->loop();\n    return 0;\n}\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    #pragma warning(pop)\n#endif\n\nvoid server_thread::launch( size_t stack_size ) {\n#if USE_WINTHREAD\n    thread_monitor::launch( thread_routine, this, stack_size, &this->my_index );\n#else\n    thread_monitor::launch( thread_routine, this, stack_size );\n#endif /* USE_PTHREAD */\n}\n\nvoid server_thread::sleep_perhaps( thread_state_t asleep ) {\n    if( terminate ) return;\n    __TBB_ASSERT( asleep==ts_asleep, NULL );\n    thread_monitor::cookie c;\n    monitor.prepare_wait(c);\n    if( state.compare_and_swap( asleep, ts_idle )==ts_idle ) {\n        if( !terminate ) {\n            monitor.commit_wait(c);\n            // Someone else woke me up.  The compare_and_swap further below deals with spurious wakeups.\n        } else {\n            monitor.cancel_wait();\n        }\n        thread_state_t s = read_state();\n        if( s==ts_asleep ) {\n            state.compare_and_swap( ts_idle, ts_asleep );\n            // I woke myself up, either because I cancelled the wait or suffered a spurious wakeup.\n        } else {\n            // Someone else woke me up; there the_balance is decremented by 1. -- tbb only\n            if( !is_omp_thread ) {\n                __TBB_ASSERT( s==ts_tbb_busy||s==ts_idle, NULL );\n            }\n        }\n    } else {\n        // someone else made it busy ; see try_grab_for when state==ts_idle.\n        __TBB_ASSERT( state==ts_omp_busy||state==ts_tbb_busy, NULL );\n        monitor.cancel_wait();\n    }\n    __TBB_ASSERT( read_state()!=asleep, \"a thread can only put itself to sleep\" );\n}\n\nbool server_thread::wakeup( thread_state_t to, thread_state_t from ) {\n    bool success = false;\n    __TBB_ASSERT( from==ts_asleep && (to==ts_idle||to==ts_omp_busy||to==ts_tbb_busy), NULL );\n    if( state.compare_and_swap( to, from )==from ) {\n        if( !is_omp_thread ) __TBB_ASSERT( to==ts_idle||to==ts_tbb_busy, NULL );\n        // There is a small timing window that permits balance to become negative,\n        // but such occurrences are probably rare enough to not worry about, since\n        // at worst the result is slight temporary oversubscription.\n        monitor.notify();\n        success = true;\n    }\n    return success;\n}\n\n//! Attempt to change a thread's state to ts_omp_busy, and waking it up if necessary.\nbool server_thread::try_grab_for( thread_state_t target_state ) {\n    bool success = false;\n    switch( read_state() ) {\n        case ts_asleep:\n            success = wakeup( target_state, ts_asleep );\n            break;\n        case ts_idle:\n            success = state.compare_and_swap( target_state, ts_idle )==ts_idle;\n            break;\n        default:\n            // Thread is not available to be part of an OpenMP thread team.\n            break;\n    }\n    return success;\n}\n\n#if _WIN32||_WIN64\nvoid server_thread::deactivate() {\n    thread_state_t es = (thread_state_t) my_extra_state.fetch_and_store( ts_deactivated );\n    __TBB_ASSERT( my_extra_state==ts_deactivated, \"someone else tampered with my_extra_state?\" );\n    if( es==ts_none )\n        state = ts_idle;\n    else\n        __TBB_ASSERT( es==ts_reactivated, \"Cannot call deactivate() while in ts_deactivated\" );\n        // only the thread can transition itself from ts_deactivted to ts_none\n    __TBB_ASSERT( my_extra_state==ts_deactivated, \"someone else tampered with my_extra_state?\" );\n    my_extra_state = ts_none; // release the critical section\n    int bal = ++the_balance;\n    if( bal>0 )\n        wakeup_some_tbb_threads();\n    if( es==ts_none )\n        sleep_perhaps( ts_asleep );\n}\n\nvoid server_thread::reactivate() {\n    thread_state_t es;\n    do {\n        while( (es=read_extra_state())==ts_deactivated )\n            __TBB_Yield();\n        if( es==ts_reactivated ) {\n            __TBB_ASSERT( false, \"two Reactivate() calls in a row.  Should not happen\" );\n            return;\n        }\n        __TBB_ASSERT( es==ts_none, NULL );\n    } while( (thread_state_t)my_extra_state.compare_and_swap( ts_reactivated, ts_none )!=ts_none );\n    if( state!=ts_omp_busy ) {\n        my_extra_state = ts_none;\n        while( !try_grab_for( ts_omp_busy ) )\n            __TBB_Yield();\n    }\n}\n#endif /* _WIN32||_WIN64 */\n\n\ntemplate<typename Connection>\nbool server_thread::destroy_job( Connection& c ) {\n    __TBB_ASSERT( !is_omp_thread||(state==ts_idle||state==ts_omp_busy), NULL );\n    __TBB_ASSERT(  is_omp_thread||(state==ts_idle||state==ts_tbb_busy), NULL );\n    if( !is_omp_thread ) {\n        __TBB_ASSERT( state==ts_idle||state==ts_tbb_busy, NULL );\n        if( state==ts_idle )\n            state.compare_and_swap( ts_done, ts_idle );\n        // 'state' may be set to ts_tbb_busy by another thread.\n\n        if( state==ts_tbb_busy ) { // return the coin to the deposit\n            // need to deposit first to let the next connection see the change\n            ++the_balance;\n            state = ts_done; // no other thread changes the state when it is ts_*_busy\n        }\n    }\n    if( job_automaton* ja = my_ja ) {\n        rml::job* j;\n        if( ja->try_plug(j) ) {\n            __TBB_ASSERT( j, NULL );\n            c.client().cleanup(*j);\n            c.remove_client_ref();\n        } else {\n            // Some other thread took responsibility for cleaning up the job.\n        }\n    }\n    // Must do remove client reference first, because execution of\n    // c.remove_ref() can cause *this to be destroyed.\n    int k = remove_ref();\n    __TBB_ASSERT_EX( k==0, \"more than one references?\" );\n#if TBB_USE_ASSERT\n    has_active_thread = false;\n#endif /* TBB_USE_ASSERT */\n    c.remove_server_ref();\n    return true;\n}\n\nbool server_thread::do_termination() {\n    if( is_omp_thread )\n        return destroy_job( *static_cast<omp_connection_v2*>(my_conn) );\n    else\n        return destroy_job( *static_cast<tbb_connection_v2*>(my_conn) );\n}\n\n//! Loop that each thread executes\nvoid server_thread::loop() {\n    if( is_omp_thread )\n        static_cast<omp_connection_v2*>(my_conn)->make_job( *this, *my_ja );\n    else\n        static_cast<tbb_connection_v2*>(my_conn)->make_job( *this, *my_ja );\n    for(;;) {\n        __TBB_Yield();\n        if( state==ts_idle )\n            sleep_perhaps( ts_asleep );\n\n        // Check whether I should quit.\n        if( terminate )\n            if( do_termination() )\n                return;\n\n        // read the state\n        thread_state_t s = read_state();\n        __TBB_ASSERT( s==ts_idle||s==ts_omp_busy||s==ts_tbb_busy, NULL );\n\n        if( s==ts_omp_busy ) {\n            // Enslaved by OpenMP team.\n            omp_dispatch.consume();\n            /* here wake tbb threads up if feasible */\n            if( ++the_balance>0 )\n                wakeup_some_tbb_threads();\n            state = ts_idle;\n        } else if( s==ts_tbb_busy ) {\n            // do some TBB work.\n            __TBB_ASSERT( my_conn && my_job, NULL );\n            tbb_connection_v2& conn = *static_cast<tbb_connection_v2*>(my_conn);\n            // give openmp higher priority\n            bool has_coin = true;\n            if( conn.has_slack() ) {\n                // it has the coin, it should trip to the scheduler at least once as long as its slack is positive\n                do {\n                    if( conn.try_process( *this, *my_job ) )\n                        if( conn.has_slack() && the_balance>=0 )\n                            has_coin = !conn.wakeup_next_thread( my_map_pos );\n                } while( has_coin && conn.has_slack() && the_balance>=0 );\n            }\n            state = ts_idle;\n            if( has_coin ) {\n                ++the_balance; // return the coin back to the deposit\n                if( conn.has_slack() ) { // a new adjust_job_request_estimate() is in progress\n                                         // it may have missed my changes to state and/or the_balance\n                    if( --the_balance>=0 ) { // try to grab the coin back\n                        // I got the coin\n                        if( state.compare_and_swap( ts_tbb_busy, ts_idle )!=ts_idle )\n                            ++the_balance; // someone else enlisted me.\n                    } else {\n                        // overdraft. return the coin\n                        ++the_balance;\n                    }\n                } // else the new request will see my changes to state & the_balance.\n            }\n            /* here wake tbb threads up if feasible */\n            if( the_balance>0 )\n                wakeup_some_tbb_threads();\n        }\n    }\n}\n#endif /* !RML_USE_WCRM */\n\n#if RML_USE_WCRM\n\nclass tbb_connection_v2;\nclass omp_connection_v2;\n\n#define CREATE_SCHEDULER_POLICY(policy,min_thrs,max_thrs,stack_size) \\\n    try {                                                                 \\\n        policy = new SchedulerPolicy (7,                                  \\\n                          SchedulerKind, RML_THREAD_KIND, /*defined in _rml_serer_msrt.h*/ \\\n                          MinConcurrency, min_thrs,                       \\\n                          MaxConcurrency, max_thrs,                       \\\n                          TargetOversubscriptionFactor, 1,                \\\n                          ContextStackSize, stack_size/1000, /*ConcRT:kB, iRML:bytes*/ \\\n                          ContextPriority, THREAD_PRIORITY_NORMAL,        \\\n                          DynamicProgressFeedback, ProgressFeedbackDisabled ); \\\n    } catch ( invalid_scheduler_policy_key & ) {                               \\\n        __TBB_ASSERT( false, \"invalid scheduler policy key exception caught\" );\\\n    } catch ( invalid_scheduler_policy_value & ) {                        \\\n        __TBB_ASSERT( false, \"invalid scheduler policy value exception caught\" );\\\n    }\n\nstatic unsigned int core_count;\nstatic tbb::atomic<int> core_count_inited;\n\n\nstatic unsigned int get_processor_count()\n{\n    if( core_count_inited!=2 ) {\n        if( core_count_inited.compare_and_swap( 1, 0 )==0 ) {\n            core_count = GetProcessorCount();\n            core_count_inited = 2;\n        } else {\n            tbb::internal::spin_wait_until_eq( core_count_inited, 2 );\n        }\n    }\n    return core_count;\n}\n\ntemplate<typename Connection>\nscheduler<Connection>::scheduler( Connection& conn ) : uid(GetSchedulerId()), my_conn(conn) {}\n\ntemplate<>\nscheduler<tbb_connection_v2>::scheduler( tbb_connection_v2& conn ) : uid(GetSchedulerId()), my_conn(conn)\n{\n    rml::client& cl = my_conn.client();\n    unsigned max_job_count = cl.max_job_count();\n    unsigned count = get_processor_count();\n    __TBB_ASSERT( max_job_count>0, \"max job count must be positive\" );\n    __TBB_ASSERT( count>1, \"The processor count must be greater than 1\" );\n    if( max_job_count>count-1) max_job_count = count-1;\n    CREATE_SCHEDULER_POLICY( my_policy, 0, max_job_count, cl.min_stack_size() );\n}\n\n#if __RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\ntemplate<>\nvoid scheduler<tbb_connection_v2>::RemoveVirtualProcessors( IVirtualProcessorRoot**, unsigned int)\n{\n}\n#else\ntemplate<>\nvoid scheduler<tbb_connection_v2>::RemoveVirtualProcessors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    if( !my_conn.is_closing() )\n        my_conn.remove_virtual_processors( vproots, count );\n}\n#endif\n\ntemplate<>\nvoid scheduler<tbb_connection_v2>::NotifyResourcesExternallyIdle( IVirtualProcessorRoot** /*vproots*/, unsigned int /*count*/)\n{\n    __TBB_ASSERT( false, \"NotifyResourcesExternallyIdle() is not allowed for TBB\" );\n}\n\ntemplate<>\nvoid scheduler<tbb_connection_v2>::NotifyResourcesExternallyBusy( IVirtualProcessorRoot** /*vproots*/, unsigned int /*count*/ )\n{\n    __TBB_ASSERT( false, \"NotifyResourcesExternallyBusy() is not allowed for TBB\" );\n}\n\ntemplate<>\nscheduler<omp_connection_v2>::scheduler( omp_connection_v2& conn ) : uid(GetSchedulerId()), my_conn(conn)\n{\n    unsigned count = get_processor_count();\n    rml::client& cl = my_conn.client();\n    __TBB_ASSERT( count>1, \"The processor count must be greater than 1\" );\n    CREATE_SCHEDULER_POLICY( my_policy, count-1, count-1, cl.min_stack_size() );\n}\n\ntemplate<>\nvoid scheduler<omp_connection_v2>::RemoveVirtualProcessors( IVirtualProcessorRoot** /*vproots*/, unsigned int /*count*/ ) {\n    __TBB_ASSERT( false, \"RemoveVirtualProcessors() is not allowed for OMP\" );\n}\n\ntemplate<>\nvoid scheduler<omp_connection_v2>::NotifyResourcesExternallyIdle( IVirtualProcessorRoot** vproots, unsigned int count ){\n    if( !my_conn.is_closing() )\n        my_conn.notify_resources_externally_idle( vproots, count );\n}\n\ntemplate<>\nvoid scheduler<omp_connection_v2>::NotifyResourcesExternallyBusy( IVirtualProcessorRoot** vproots, unsigned int count ){\n    if( !my_conn.is_closing() )\n        my_conn.notify_resources_externally_busy( vproots, count );\n}\n\n/* ts_idle, ts_asleep, ts_busy */\nvoid tbb_server_thread::Dispatch( DispatchState* ) {\n    // Activate() will resume a thread right after Deactivate() as if it returns from the call\n    tbb_connection_v2* tbb_conn = static_cast<tbb_connection_v2*>(my_conn);\n    make_job( *tbb_conn, *this );\n\n    for( ;; ) {\n        // Try to wake some tbb threads if the balance is positive.\n        // When a thread is added by ConcRT and enter here for the first time,\n        // the thread may wake itself up (i.e., atomically change its state to ts_busy.\n        if( the_balance>0 )\n             wakeup_some_tbb_threads();\n        if( read_state()!=ts_busy )\n            if( sleep_perhaps() )\n                return;\n        if( terminate )\n            if( initiate_termination() )\n                return;\n        if( read_state()==ts_busy ) {\n            // this thread has a coin (i.e., state=ts_busy; it should trip to the scheduler at least once\n            if ( tbb_conn->has_slack() ) {\n                do {\n                    tbb_conn->try_process( *wait_for_job() );\n                } while( tbb_conn->has_slack() && the_balance>=0 && !is_removed() );\n            }\n            __TBB_ASSERT( read_state()==ts_busy, \"thread is not in busy state after returning from process()\" );\n            // see remove_virtual_processors()\n            if( my_state.compare_and_swap( ts_idle, ts_busy )==ts_busy ) {\n                int bal = ++the_balance;\n                if( tbb_conn->has_slack() ) {\n                    // slack is positive, volunteer to help\n                    bal = --the_balance;  // try to grab the coin back\n                    if( bal>=0 ) { // got the coin back\n                        if( my_state.compare_and_swap( ts_busy, ts_idle )!=ts_idle )\n                            ++the_balance; // someone else enlisted me.\n                        // else my_state is ts_busy, I will come back to tbb_conn->try_process().\n                    } else {\n                        // overdraft. return the coin\n                        ++the_balance;\n                    }\n                } // else the new request will see my changes to state & the_balance.\n            } else {\n                __TBB_ASSERT( false, \"someone tampered with my state\" );\n            }\n        } // someone else might set the state to something other than ts_idle\n    }\n}\n\nvoid omp_server_thread::Dispatch( DispatchState* ) {\n    // Activate() will resume a thread right after Deactivate() as if it returns from the call\n    make_job( *static_cast<omp_connection_v2*>(my_conn), *this );\n\n    for( ;; ) {\n        if( read_state()!=ts_busy )\n            sleep_perhaps();\n        if( terminate ) {\n            if( initiate_termination() )\n                return;\n        }\n        if( read_state()==ts_busy ) {\n            omp_data.consume();\n            __TBB_ASSERT( read_state()==ts_busy, \"thread is not in busy state after returning from process()\" );\n            my_thread_map.adjust_balance( 1 );\n            set_state( ts_idle );\n        }\n        // someone else might set the state to something other than ts_idle\n    }\n}\n\n//! Attempt to change a thread's state to ts_omp_busy, and waking it up if necessary.\nthread_grab_t server_thread_rep::try_grab_for() {\n    thread_grab_t res = wk_failed;\n    thread_state_t s = read_state();\n    switch( s ) {\n    case ts_asleep:\n        if( wakeup( ts_busy, ts_asleep ) )\n            res = wk_from_asleep;\n        __TBB_ASSERT( res==wk_failed||read_state()==ts_busy, NULL );\n        break;\n    case ts_idle:\n        if( my_state.compare_and_swap( ts_busy, ts_idle )==ts_idle )\n            res = wk_from_idle;\n        // At this point a thread is grabbed (i.e., its state has  changed to ts_busy.\n        // It is possible that the thread 1) processes the job, returns from process() and\n        // sets its state ts_idle again.  In some cases, it even sets its state to ts_asleep.\n        break;\n    default:\n        break;\n    }\n    return res;\n}\n\nbool tbb_server_thread::switch_out() {\n    thread_state_t s = read_state();\n    __TBB_ASSERT( s==ts_asleep||s==ts_busy, NULL );\n    // This thread comes back from the TBB scheduler, and changed its state to ts_asleep successfully.\n    // The master enlisted it and woke it up by Activate()'ing it; now it is emerging from Deactivated().\n    // ConcRT requested for removal of the vp associated with the thread, and RML marks it removed.\n    // Now, it has ts_busy, and removed. -- we should remove it.\n    IExecutionResource* old_vp = my_execution_resource;\n    if( s==ts_busy ) {\n        ++the_balance;\n        my_state = ts_asleep;\n    }\n    IThreadProxy* proxy = my_proxy;\n    __TBB_ASSERT( proxy, NULL );\n    my_execution_resource = (IExecutionResource*) c_remove_prepare;\n    old_vp->Remove( my_scheduler );\n    my_execution_resource = (IExecutionResource*) c_remove_returned;\n    int cnt = --activation_count;\n    __TBB_ASSERT_EX( cnt==0||cnt==1, \"too many activations?\" );\n    proxy->SwitchOut();\n    if( terminate ) {\n        bool activated = activation_count==1;\n#if TBB_USE_ASSERT\n        /* In a rare sequence of events, a thread comes out of SwitchOut with activation_count==1.\n         * 1) The thread is SwitchOut'ed.\n         * 2) AddVirtualProcessors() arrived and the thread is Activated.\n         * 3) The thread is coming out of SwitchOut().\n         * 4) request_close_connection arrives and inform the thread that it is time to terminate.\n         * 5) The thread hits the check and falls into the path with 'activated==true'.\n         * In that case, do the clean-up but do not switch to the thread scavenger; rather simply return to RM.\n         */\n        if( activated ) {\n            // thread is 'revived' in add_virtual_processors after being Activated().\n            // so, if the thread extra state is still marked 'removed', it will shortly change to 'none'\n            // i.e., !is_remove().  The thread state is changed to ts_idle before the extra state, so\n            // the thread's state should be either ts_idle or ts_done.\n            while( is_removed() )\n                __TBB_Yield();\n            thread_state_t s = read_state();\n            __TBB_ASSERT( s==ts_idle || s==ts_done, NULL );\n        }\n#endif\n        __TBB_ASSERT( my_state==ts_asleep||my_state==ts_idle, NULL );\n        // it is possible that in make_job() the thread may not have a chance to create a job.\n        // my_job may not be set if the thread did not get a chance to process client's job (i.e., call try_process())\n        rml::job* j;\n        if( my_job_automaton.try_plug(j) ) {\n            __TBB_ASSERT( j, NULL );\n            my_client.cleanup(*j);\n            my_conn->remove_client_ref();\n        }\n        // Must do remove client reference first, because execution of\n        // c.remove_ref() can cause *this to be destroyed.\n        if( !activated )\n            proxy->SwitchTo( my_thread_map.get_thread_scavenger(), Idle );\n        my_conn->remove_server_ref();\n        return true;\n    }\n    // We revive a thread in add_virtual_processors() after we Activate the thread on a new virtual processor.\n    // So briefly wait until the thread's my_execution_resource gets set.\n    while( get_virtual_processor()==c_remove_returned )\n        __TBB_Yield();\n    return false;\n}\n\nbool tbb_server_thread::sleep_perhaps () {\n    if( terminate ) return false;\n    thread_state_t s = read_state();\n    if( s==ts_idle ) {\n        if( my_state.compare_and_swap( ts_asleep, ts_idle )==ts_idle ) {\n            // If a thread is between read_state() and compare_and_swap(), and the master tries to terminate,\n            // the master's compare_and_swap() will fail because the thread's state is ts_idle.\n            // We need to check if terminate is true or not before letting the thread go to sleep,\n            // otherwise we will miss the terminate signal.\n            if( !terminate ) {\n                if( !is_removed() ) {\n                    --activation_count;\n                    get_virtual_processor()->Deactivate( this );\n                }\n                if( is_removed() ) {\n                    if( switch_out() )\n                        return true;\n                    __TBB_ASSERT( my_execution_resource>c_remove_returned, NULL );\n                }\n                // in add_virtual_processors(), when we revive a thread, we change its state after Activate the thread\n                // in that case the state may be ts_asleep for a short period\n                while( read_state()==ts_asleep )\n                    __TBB_Yield();\n            } else {\n                if( my_state.compare_and_swap( ts_done, ts_asleep )!=ts_asleep ) {\n                    --activation_count;\n                    // unbind() changed my state. It will call Activate(). So issue a matching Deactivate()\n                    get_virtual_processor()->Deactivate( this );\n                }\n            }\n        }\n    } else {\n        __TBB_ASSERT( s==ts_busy, NULL );\n    }\n    return false;\n}\n\nvoid omp_server_thread::sleep_perhaps () {\n    if( terminate ) return;\n    thread_state_t s = read_state();\n    if( s==ts_idle ) {\n        if( my_state.compare_and_swap( ts_asleep, ts_idle )==ts_idle ) {\n            // If a thread is between read_state() and compare_and_swap(), and the master tries to terminate,\n            // the master's compare_and_swap() will fail because the thread's state is ts_idle.\n            // We need to check if terminate is true or not before letting the thread go to sleep,\n            // otherwise we will miss the terminate signal.\n            if( !terminate ) {\n                get_virtual_processor()->Deactivate( this );\n                __TBB_ASSERT( !is_removed(), \"OMP threads should not be deprived of a virtual processor\" );\n                __TBB_ASSERT( read_state()!=ts_asleep, NULL );\n            } else {\n                if( my_state.compare_and_swap( ts_done, ts_asleep )!=ts_asleep )\n                    // unbind() changed my state. It will call Activate(). So issue a matching Deactivate()\n                    get_virtual_processor()->Deactivate( this );\n            }\n        }\n    } else {\n        __TBB_ASSERT( s==ts_busy, NULL );\n    }\n}\n\nbool tbb_server_thread::initiate_termination() {\n    if( read_state()==ts_busy ) {\n        int bal = ++the_balance;\n        if( bal>0 ) wakeup_some_tbb_threads();\n    }\n    return destroy_job( (tbb_connection_v2*) my_conn );\n}\n\ntemplate<typename Connection>\nbool server_thread_rep::destroy_job( Connection* c ) {\n    __TBB_ASSERT( my_state!=ts_asleep, NULL );\n    rml::job* j;\n    if( my_job_automaton.try_plug(j) ) {\n        __TBB_ASSERT( j, NULL );\n        my_client.cleanup(*j);\n        c->remove_client_ref();\n    }\n    // Must do remove client reference first, because execution of\n    // c.remove_ref() can cause *this to be destroyed.\n    c->remove_server_ref();\n    return true;\n}\n\nvoid thread_map::assist_cleanup( bool assist_null_only ) {\n    // To avoid deadlock, the current thread *must* help out with cleanups that have not started,\n    // because the thread that created the job may be busy for a long time.\n    for( iterator i = begin(); i!=end(); ++i ) {\n        rml::job* j=0;\n        server_thread* thr = (*i).second;\n        job_automaton& ja = thr->my_job_automaton;\n        if( assist_null_only ? ja.try_plug_null() : ja.try_plug(j) ) {\n            if( j ) {\n                my_client.cleanup(*j);\n            } else {\n                // server thread did not get a chance to create a job.\n            }\n            remove_client_ref();\n        }\n    }\n}\n\nvoid thread_map::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count, tbb_connection_v2& conn, ::tbb::spin_mutex& mtx )\n{\n#if TBB_USE_ASSERT\n    int req_cnt = ++n_add_vp_requests;\n    __TBB_ASSERT( req_cnt==1, NULL );\n#endif\n    std::vector<thread_map::iterator> vec(count);\n    std::vector<tbb_server_thread*> tvec(count);\n    iterator end;\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        __TBB_ASSERT( my_map.size()==0||count==1, NULL );\n        end = my_map.end(); //remember 'end' at the time of 'find'\n        // find entries in the map for those VPs that were previously added and then removed.\n        for( size_t i=0; i<count; ++i ) {\n            vec[i] = my_map.find( (key_type) vproots[i] );\n#if TBB_USE_DEBUG\n            if( vec[i]!=end ) {\n                tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n                IVirtualProcessorRoot* v = t->get_virtual_processor();\n                __TBB_ASSERT( v==c_remove_prepare||v==c_remove_returned, NULL );\n            }\n#endif\n        }\n\n        iterator nxt = my_map.begin();\n        for( size_t i=0; i<count; ++i ) {\n            if( vec[i]!=end ) {\n#if TBB_USE_ASSERT\n                tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n                __TBB_ASSERT( t->read_state()==ts_asleep, NULL );\n                IVirtualProcessorRoot* r = t->get_virtual_processor();\n                __TBB_ASSERT( r==c_remove_prepare||r==c_remove_returned, NULL );\n#endif\n                continue;\n            }\n\n            if( my_unrealized_threads>0 ) {\n                --my_unrealized_threads;\n            } else {\n                __TBB_ASSERT( nxt!=end, \"nxt should not be thread_map::iterator::end\" );\n                // find a removed thread context for i\n                for( ; nxt!=end; ++nxt ) {\n                    tbb_server_thread* t = (tbb_server_thread*) (*nxt).second;\n                    if( t->is_removed() && t->read_state()==ts_asleep && t->get_virtual_processor()==c_remove_returned ) {\n                        vec[i] = nxt++;\n                        break;\n                    }\n                }\n                // break target\n                if( vec[i]==end ) // ignore excessive VP.\n                    vproots[i] = NULL;\n            }\n        }\n    }\n\n    for( size_t i=0; i<count; ++i ) {\n        __TBB_ASSERT( !tvec[i], NULL );\n        if( vec[i]==end ) {\n            if( vproots[i] ) {\n                tvec[i] = my_tbb_allocator.allocate(1);\n                new ( tvec[i] ) tbb_server_thread( false, my_scheduler, (IExecutionResource*)vproots[i], &conn, *this, my_client );\n            }\n#if TBB_USE_ASSERT\n        } else {\n            tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n            __TBB_ASSERT( t->GetProxy(), \"Proxy is cleared?\" );\n#endif\n        }\n    }\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        bool closing = is_closing();\n\n        for( size_t i=0; i<count; ++i ) {\n            if( vec[i]==end ) {\n                if( vproots[i] ) {\n                    thread_map::key_type key = (thread_map::key_type) vproots[i];\n                    vec[i] = insert( key, (server_thread*) tvec[i] );\n                    my_client_ref_count.add_ref();\n                    my_server_ref_count.add_ref();\n                }\n            } else if( !closing ) {\n                tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n\n                if( (*vec[i]).first!=(thread_map::key_type)vproots[i] ) {\n                    my_map.erase( vec[i] );\n                    thread_map::key_type key = (thread_map::key_type) vproots[i];\n                    __TBB_ASSERT( key, NULL );\n                    vec[i] = insert( key, t );\n                }\n                __TBB_ASSERT( t->read_state()==ts_asleep, NULL );\n                // We did not decrement server/client ref count when a thread is removed.\n                // So, don't increment server/client ref count here.\n            }\n        }\n\n        // we could check is_closing() earlier.  That requires marking the newly allocated server_thread objects\n        // that are not inserted into the thread_map, and deallocate them.  Doing so seems more cumbersome\n        // than simply adding these to the thread_map and let thread_map's destructor take care of reclamation.\n        __TBB_ASSERT( closing==is_closing(), NULL );\n        if( closing ) return;\n    }\n\n    for( size_t i=0; i<count; ++i ) {\n        if( vproots[i] ) {\n            tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n            __TBB_ASSERT( tvec[i]!=NULL||t->GetProxy(), \"Proxy is cleared?\" );\n            if( t->is_removed() )\n                __TBB_ASSERT( t->get_virtual_processor()==c_remove_returned, NULL );\n            int cnt = ++t->activation_count;\n            __TBB_ASSERT_EX( cnt==0||cnt==1, NULL );\n            vproots[i]->Activate( t );\n            if( t->is_removed() )\n                t->revive( my_scheduler, vproots[i], my_client );\n        }\n    }\n#if TBB_USE_ASSERT\n    req_cnt = --n_add_vp_requests;\n    __TBB_ASSERT( req_cnt==0, NULL );\n#endif\n}\n\nvoid thread_map::remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx ) {\n    if( my_map.size()==0 )\n        return;\n    tbb::spin_mutex::scoped_lock lck( mtx );\n\n    if( is_closing() ) return;\n\n    for( unsigned int c=0; c<count; ++c ) {\n        iterator i = my_map.find( (key_type) vproots[c] );\n        if( i==my_map.end() ) {\n            thread_scavenger_thread* tst = my_thread_scavenger_thread;\n            if( !tst ) {\n                // Remove unknown vp from my scheduler;\n                vproots[c]->Remove( my_scheduler );\n            } else {\n                while( (tst=my_thread_scavenger_thread)==c_claimed )\n                    __TBB_Yield();\n                if( vproots[c]!=tst->get_virtual_processor() )\n                    vproots[c]->Remove( my_scheduler );\n            }\n            continue;\n        }\n        tbb_server_thread* thr = (tbb_server_thread*) (*i).second;\n        __TBB_ASSERT( thr->tbb_thread, \"incorrect type of server_thread\" );\n        thr->set_removed();\n        if( thr->read_state()==ts_asleep ) {\n            while( thr->activation_count>0 ) {\n                if( thr->get_virtual_processor()<=c_remove_returned )\n                    break;\n                __TBB_Yield();\n            }\n            if( thr->get_virtual_processor()>c_remove_returned ) {\n                // the thread is in Deactivated state\n                ++thr->activation_count;\n                // wake the thread up so that it Switches Out itself.\n                thr->get_virtual_processor()->Activate( thr );\n            } // else, it is Switched Out\n        } // else the thread will see that it is removed and proceed to switch itself out without Deactivation\n    }\n}\n\nvoid thread_map::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count, omp_connection_v2& conn, ::tbb::spin_mutex& mtx )\n{\n    std::vector<thread_map::iterator> vec(count);\n    std::vector<server_thread*> tvec(count);\n    iterator end;\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        // read the map\n        end = my_map.end(); //remember 'end' at the time of 'find'\n        for( size_t i=0; i<count; ++i )\n            vec[i] = my_map.find( (key_type) vproots[i] );\n    }\n\n    for( size_t i=0; i<count; ++i ) {\n        __TBB_ASSERT( !tvec[i], NULL );\n        if( vec[i]==end ) {\n            tvec[i] = my_omp_allocator.allocate(1);\n            new ( tvec[i] ) omp_server_thread( false, my_scheduler, (IExecutionResource*)vproots[i], &conn, *this, my_client );\n        }\n    }\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        for( size_t i=0; i<count; ++i ) {\n            if( vec[i]==my_map.end() ) {\n                thread_map::key_type key = (thread_map::key_type) vproots[i];\n                vec[i] = insert( key, tvec[i] );\n                my_client_ref_count.add_ref();\n                my_server_ref_count.add_ref();\n            }\n        }\n\n        // we could check is_closing() earlier.  That requires marking the newly allocated server_thread objects\n        // that are not inserted into the thread_map, and deallocate them.  Doing so seems more cumbersome\n        // than simply adding these to the thread_map and let thread_map's destructor take care of reclamation.\n        if( is_closing() ) return;\n    }\n\n    for( size_t i=0; i<count; ++i )\n        vproots[i]->Activate( (*vec[i]).second );\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        for( size_t i=0; i<count; ++i )\n            original_exec_resources.push_back( vproots[i] );\n    }\n}\n\nvoid thread_map::mark_virtual_processors_as_lent( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx ) {\n    tbb::spin_mutex::scoped_lock lck( mtx );\n\n    if( is_closing() ) return;\n\n    iterator end = my_map.end();\n    for( unsigned int c=0; c<count; ++c ) {\n        iterator i = my_map.find( (key_type) vproots[c] );\n        if( i==end ) {\n            // The vproc has not been added to the map in create_oversubscribers()\n            my_map.insert( hash_map_type::value_type( (key_type) vproots[c], (server_thread*)1 ) );\n        } else {\n            server_thread* thr = (*i).second;\n            if( ((uintptr_t)thr)&~(uintptr_t)1 ) {\n                __TBB_ASSERT( !thr->is_removed(), \"incorrectly removed\" );\n                ((omp_server_thread*)thr)->set_lent();\n            }\n        }\n    }\n}\n\nvoid thread_map::create_oversubscribers( unsigned n, std::vector<server_thread*>& thr_vec, omp_connection_v2& conn, ::tbb::spin_mutex& mtx ) {\n    std::vector<IExecutionResource*> curr_exec_rsc;\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        curr_exec_rsc = original_exec_resources; // copy construct\n    }\n    typedef std::vector<IExecutionResource*>::iterator iterator_er;\n    typedef ::std::vector<std::pair<hash_map_type::key_type, hash_map_type::mapped_type> > hash_val_vector_t;\n    hash_val_vector_t v_vec(n);\n    iterator_er begin = curr_exec_rsc.begin();\n    iterator_er end   = curr_exec_rsc.end();\n    iterator_er i = begin;\n    for( unsigned c=0; c<n; ++c ) {\n        IVirtualProcessorRoot* vpr = my_scheduler_proxy->CreateOversubscriber( *i );\n        omp_server_thread* t = new ( my_omp_allocator.allocate(1) ) omp_server_thread( true, my_scheduler, (IExecutionResource*)vpr, &conn, *this, my_client );\n        thr_vec[c] = t;\n        v_vec[c] = hash_map_type::value_type( (key_type) vpr, t );\n        if( ++i==end ) i = begin;\n    }\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        if( is_closing() ) return;\n\n        iterator end = my_map.end();\n        unsigned c = 0;\n        for( hash_val_vector_t::iterator vi=v_vec.begin(); vi!=v_vec.end(); ++vi, ++c ) {\n            iterator i = my_map.find( (key_type) (*vi).first );\n            if( i==end ) {\n                my_map.insert( *vi );\n            } else {\n                // the vproc has not been added to the map in mark_virtual_processors_as_returned();\n                uintptr_t lent = (uintptr_t) (*i).second;\n                __TBB_ASSERT( lent<=1, \"vproc map entry added incorrectly?\");\n                (*i).second = thr_vec[c];\n                if( lent )\n                    ((omp_server_thread*)thr_vec[c])->set_lent();\n                else\n                    ((omp_server_thread*)thr_vec[c])->set_returned();\n            }\n            my_client_ref_count.add_ref();\n            my_server_ref_count.add_ref();\n        }\n    }\n}\n\nvoid thread_map::wakeup_tbb_threads( int c, ::tbb::spin_mutex& mtx ) {\n    std::vector<tbb_server_thread*> vec(c);\n\n    size_t idx = 0;\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        if( is_closing() ) return;\n        // only one RML thread is in here to wake worker threads up.\n\n        int bal = the_balance;\n        int cnt = c<bal ? c : bal;\n\n        if( cnt<=0 ) { return; }\n\n        for( iterator i=begin(); i!=end(); ++i ) {\n            tbb_server_thread* thr = (tbb_server_thread*) (*i).second;\n            // ConcRT RM should take threads away from TBB scheduler instead of lending them to another scheduler\n            if( thr->is_removed() )\n                continue;\n\n            if( --the_balance>=0 ) {\n                thread_grab_t res;\n                while( (res=thr->try_grab_for())!=wk_from_idle ) {\n                    if( res==wk_from_asleep ) {\n                        vec[idx++] = thr;\n                        break;\n                    } else {\n                        thread_state_t s = thr->read_state();\n                        if( s==ts_busy ) {// failed because already assigned. move on.\n                            ++the_balance;\n                            goto skip;\n                        }\n                    }\n                }\n                thread_state_t s = thr->read_state();\n                __TBB_ASSERT_EX( s==ts_busy, \"should have set the state to ts_busy\" );\n                if( --cnt==0 )\n                    break;\n            } else {\n                // overdraft\n                ++the_balance;\n                break;\n            }\nskip:\n            ;\n        }\n    }\n\n    for( size_t i=0; i<idx; ++i ) {\n        tbb_server_thread* thr = vec[i];\n        __TBB_ASSERT( thr, NULL );\n        thread_state_t s = thr->read_state();\n        __TBB_ASSERT_EX( s==ts_busy, \"should have set the state to ts_busy\" );\n        ++thr->activation_count;\n        thr->get_virtual_processor()->Activate( thr );\n    }\n\n}\n\nvoid thread_map::mark_virtual_processors_as_returned( IVirtualProcessorRoot** vprocs, unsigned int count, tbb::spin_mutex& mtx ) {\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        if( is_closing() ) return;\n\n        iterator end = my_map.end();\n        for(unsigned c=0; c<count; ++c ) {\n            iterator i = my_map.find( (key_type) vprocs[c] );\n            if( i==end ) {\n                // the vproc has not been added to the map in create_oversubscribers()\n                my_map.insert( hash_map_type::value_type( (key_type) vprocs[c], static_cast<server_thread*>(0) ) );\n            } else {\n                omp_server_thread* thr = (omp_server_thread*) (*i).second;\n                if( ((uintptr_t)thr)&~(uintptr_t)1 ) {\n                    __TBB_ASSERT( !thr->is_removed(), \"incorrectly removed\" );\n                    // we should not make any assumption on the initial state of an added vproc.\n                    thr->set_returned();\n                }\n            }\n        }\n    }\n}\n\n\nvoid thread_map::unbind( rml::server& /*server*/, tbb::spin_mutex& mtx ) {\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        shutdown_in_progress = true;  // ignore any callbacks from ConcRT RM\n\n        // Ask each server_thread to cleanup its job for this server.\n        for( iterator i = begin(); i!=end(); ++i ) {\n            server_thread* t = (*i).second;\n            t->terminate = true;\n            if( t->is_removed() ) {\n                // This is for TBB only as ConcRT RM does not request OMP schedulers to remove virtual processors\n                if( t->read_state()==ts_asleep ) {\n                    __TBB_ASSERT( my_thread_scavenger_thread, \"this is TBB connection; thread_scavenger_thread must be allocated\" );\n                    // thread is on its way to switch_out; see remove_virtual_processors() where\n                    // the thread is Activated() to bring it back from 'Deactivated' in sleep_perhaps()\n                    // now assume that the thread will go to SwitchOut()\n#if TBB_USE_ASSERT\n                    while( t->get_virtual_processor()>c_remove_returned )\n                        __TBB_Yield();\n#endif\n                    // A removed thread is supposed to proceed to SwithcOut.\n                    // There, we remove client&server references.\n                }\n            } else {\n                if( t->wakeup( ts_done, ts_asleep ) ) {\n                    if( t->tbb_thread )\n                        ++((tbb_server_thread*)t)->activation_count;\n                    t->get_virtual_processor()->Activate( t );\n                    // We mark in the thread_map such that when termination sequence started, we ignore\n                    // all notification from ConcRT RM.\n                }\n            }\n        }\n    }\n    // Remove extra ref to client.\n    remove_client_ref();\n\n    if( my_thread_scavenger_thread ) {\n        thread_scavenger_thread* tst;\n        while( (tst=my_thread_scavenger_thread)==c_claimed )\n            __TBB_Yield();\n#if TBB_USE_ASSERT\n        ++my_thread_scavenger_thread->activation_count;\n#endif\n        tst->get_virtual_processor()->Activate( tst );\n    }\n}\n\n#if !__RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\nvoid thread_map::allocate_thread_scavenger( IExecutionResource* v )\n{\n    if( my_thread_scavenger_thread>c_claimed ) return;\n    thread_scavenger_thread* c = my_thread_scavenger_thread.fetch_and_store((thread_scavenger_thread*)c_claimed);\n    if( c==NULL ) { // successfully claimed\n        add_server_ref();\n#if TBB_USE_ASSERT\n        ++n_thread_scavengers_created;\n#endif\n        __TBB_ASSERT( v, NULL );\n        IVirtualProcessorRoot* vpr = my_scheduler_proxy->CreateOversubscriber( v );\n        my_thread_scavenger_thread = c = new ( my_scavenger_allocator.allocate(1) ) thread_scavenger_thread( my_scheduler, vpr, *this );\n#if TBB_USE_ASSERT\n        ++c->activation_count;\n#endif\n        vpr->Activate( c );\n    } else if( c>c_claimed ) {\n        my_thread_scavenger_thread = c;\n    }\n}\n#endif\n\nvoid thread_scavenger_thread::Dispatch( DispatchState* )\n{\n    __TBB_ASSERT( my_proxy, NULL );\n#if TBB_USE_ASSERT\n    --activation_count;\n#endif\n    get_virtual_processor()->Deactivate( this );\n    for( thread_map::iterator i=my_thread_map.begin(); i!=my_thread_map.end(); ++i ) {\n        tbb_server_thread* t = (tbb_server_thread*) (*i).second;\n        if( t->read_state()==ts_asleep && t->is_removed() ) {\n            while( t->get_execution_resource()!=c_remove_returned )\n                __TBB_Yield();\n            my_proxy->SwitchTo( t, Blocking );\n        }\n    }\n    get_virtual_processor()->Remove( my_scheduler );\n    my_thread_map.remove_server_ref();\n    // signal to the connection scavenger that i am done with the map.\n    __TBB_ASSERT( activation_count==1, NULL );\n    set_state( ts_done );\n}\n\n//! Windows \"DllMain\" that handles startup and shutdown of dynamic library.\nextern \"C\" bool WINAPI DllMain( HINSTANCE /*hinstDLL*/, DWORD fwdReason, LPVOID lpvReserved ) {\n    void assist_cleanup_connections();\n    if( fwdReason==DLL_PROCESS_DETACH ) {\n        // dll is being unloaded\n        if( !lpvReserved ) // if FreeLibrary has been called\n            assist_cleanup_connections();\n    }\n    return true;\n}\n\nvoid free_all_connections( uintptr_t conn_ex ) {\n    while( conn_ex ) {\n        bool is_tbb = (conn_ex&2)>0;\n        //clear extra bits\n        uintptr_t curr_conn = conn_ex & ~(uintptr_t)3;\n        __TBB_ASSERT( curr_conn, NULL );\n\n        // Wait for worker threads to return\n        if( is_tbb ) {\n            tbb_connection_v2* tbb_conn = reinterpret_cast<tbb_connection_v2*>(curr_conn);\n            conn_ex = reinterpret_cast<uintptr_t>(tbb_conn->next_conn);\n            while( tbb_conn->my_thread_map.remove_server_ref()>0 )\n                __TBB_Yield();\n            delete tbb_conn;\n        } else {\n            omp_connection_v2* omp_conn = reinterpret_cast<omp_connection_v2*>(curr_conn);\n            conn_ex = reinterpret_cast<uintptr_t>(omp_conn->next_conn);\n            while( omp_conn->my_thread_map.remove_server_ref()>0 )\n                __TBB_Yield();\n            delete omp_conn;\n        }\n    }\n}\n\nvoid assist_cleanup_connections()\n{\n    //signal to connection_scavenger_thread to terminate\n    uintptr_t tail = connections_to_reclaim.tail;\n    while( connections_to_reclaim.tail.compare_and_swap( garbage_connection_queue::plugged, tail )!=tail ) {\n        __TBB_Yield();\n        tail = connections_to_reclaim.tail;\n    }\n\n    __TBB_ASSERT( connection_scavenger.state==ts_busy || connection_scavenger.state==ts_asleep, NULL );\n    // Scavenger thread may be busy freeing connections\n    DWORD thr_exit_code = STILL_ACTIVE;\n    while( connection_scavenger.state==ts_busy ) {\n        if( GetExitCodeThread( connection_scavenger.thr_handle, &thr_exit_code )>0 )\n            if( thr_exit_code!=STILL_ACTIVE )\n                break;\n        __TBB_Yield();\n        thr_exit_code = STILL_ACTIVE;\n    }\n    if( connection_scavenger.state==ts_asleep && thr_exit_code==STILL_ACTIVE )\n        connection_scavenger.wakeup(); // wake the connection scavenger thread up\n\n    // it is possible that the connection scavenger thread already exited.  Take over its responsibility.\n    if( tail && connections_to_reclaim.tail!=garbage_connection_queue::plugged_acked ) {\n        // atomically claim the head of the list.\n        uintptr_t head = connections_to_reclaim.head.fetch_and_store( garbage_connection_queue::empty );\n        if( head==garbage_connection_queue::empty )\n            head = tail;\n        connection_scavenger.process_requests( head );\n    }\n    __TBB_ASSERT( connections_to_reclaim.tail==garbage_connection_queue::plugged||connections_to_reclaim.tail==garbage_connection_queue::plugged_acked, \"someone else added a request after termination has initiated\" );\n    __TBB_ASSERT( (unsigned)the_balance==the_default_concurrency, NULL );\n}\n\nvoid connection_scavenger_thread::sleep_perhaps() {\n    uintptr_t tail = connections_to_reclaim.tail;\n    // connections_to_reclaim.tail==garbage_connection_queue::plugged --> terminate,\n    // connections_to_reclaim.tail>garbage_connection_queue::plugged : we got work to do\n    if( tail>=garbage_connection_queue::plugged ) return;\n    __TBB_ASSERT( !tail, NULL );\n    thread_monitor::cookie c;\n    monitor.prepare_wait(c);\n    if( state.compare_and_swap( ts_asleep, ts_busy )==ts_busy ) {\n        if( connections_to_reclaim.tail!=garbage_connection_queue::plugged ) {\n            monitor.commit_wait(c);\n            // Someone else woke me up.  The compare_and_swap further below deals with spurious wakeups.\n        } else {\n            monitor.cancel_wait();\n        }\n        thread_state_t s = state;\n        if( s==ts_asleep ) // if spurious wakeup.\n            state.compare_and_swap( ts_busy, ts_asleep );\n            // I woke myself up, either because I cancelled the wait or suffered a spurious wakeup.\n    } else {\n        __TBB_ASSERT( false, \"someone else tampered with my state\" );\n    }\n    __TBB_ASSERT( state==ts_busy, \"a thread can only put itself to sleep\" );\n}\n\nvoid connection_scavenger_thread::process_requests( uintptr_t conn_ex )\n{\n    __TBB_ASSERT( conn_ex>1, NULL );\n    __TBB_ASSERT( n_scavenger_threads==1||connections_to_reclaim.tail==garbage_connection_queue::plugged, \"more than one connection_scavenger_thread being active?\" );\n\n    bool done = false;\n    while( !done ) {\n        bool is_tbb = (conn_ex&2)>0;\n        //clear extra bits\n        uintptr_t curr_conn = conn_ex & ~(uintptr_t)3;\n\n        // no contention. there is only one connection_scavenger_thread!!\n        uintptr_t next_conn;\n        tbb_connection_v2* tbb_conn = NULL;\n        omp_connection_v2* omp_conn = NULL;\n        // Wait for worker threads to return\n        if( is_tbb ) {\n            tbb_conn = reinterpret_cast<tbb_connection_v2*>(curr_conn);\n            next_conn = reinterpret_cast<uintptr_t>(tbb_conn->next_conn);\n            while( tbb_conn->my_thread_map.get_server_ref_count()>1 )\n                __TBB_Yield();\n        } else {\n            omp_conn = reinterpret_cast<omp_connection_v2*>(curr_conn);\n            next_conn = reinterpret_cast<uintptr_t>(omp_conn->next_conn);\n            while( omp_conn->my_thread_map.get_server_ref_count()>1 )\n                __TBB_Yield();\n        }\n\n        //someone else may try to write into this connection object.\n        //So access next_conn field first before remove the extra server ref count.\n\n        if( next_conn==0 ) {\n            uintptr_t tail = connections_to_reclaim.tail;\n            if( tail==garbage_connection_queue::plugged ) {\n                tail = garbage_connection_queue::plugged_acked; // connection scavenger saw the flag, and it freed all connections.\n                done = true;\n            } else if( tail==conn_ex ) {\n                if( connections_to_reclaim.tail.compare_and_swap( garbage_connection_queue::empty, tail )==tail ) {\n                    __TBB_ASSERT( !connections_to_reclaim.head, NULL );\n                    done = true;\n                }\n            }\n\n            if( !done ) {\n                // A new connection to close is added to connections_to_reclaim.tail;\n                // Wait for curr_conn->next_conn to be set.\n                if( is_tbb ) {\n                    while( !tbb_conn->next_conn )\n                        __TBB_Yield();\n                    conn_ex = reinterpret_cast<uintptr_t>(tbb_conn->next_conn);\n                } else {\n                    while( !omp_conn->next_conn )\n                        __TBB_Yield();\n                    conn_ex = reinterpret_cast<uintptr_t>(omp_conn->next_conn);\n                }\n            }\n        } else {\n            conn_ex = next_conn;\n        }\n        __TBB_ASSERT( conn_ex, NULL );\n        if( is_tbb )\n            // remove extra server ref count; this will trigger Shutdown/Release of ConcRT RM\n            tbb_conn->remove_server_ref();\n        else\n            // remove extra server ref count; this will trigger Shutdown/Release of ConcRT RM\n            omp_conn->remove_server_ref();\n    }\n}\n\n__RML_DECL_THREAD_ROUTINE connection_scavenger_thread::thread_routine( void* arg ) {\n    connection_scavenger_thread* thr = (connection_scavenger_thread*) arg;\n    thr->state = ts_busy;\n    thr->thr_handle = GetCurrentThread();\n#if TBB_USE_ASSERT\n    ++thr->n_scavenger_threads;\n#endif\n    for(;;) {\n        __TBB_Yield();\n        thr->sleep_perhaps();\n        if( connections_to_reclaim.tail==garbage_connection_queue::plugged || connections_to_reclaim.tail==garbage_connection_queue::plugged_acked ) {\n            thr->state = ts_asleep;\n            return 0;\n        }\n\n        __TBB_ASSERT( connections_to_reclaim.tail!=garbage_connection_queue::plugged_acked, NULL );\n        __TBB_ASSERT( connections_to_reclaim.tail>garbage_connection_queue::plugged && (connections_to_reclaim.tail&garbage_connection_queue::plugged)==0 , NULL );\n        while( connections_to_reclaim.head==garbage_connection_queue::empty )\n            __TBB_Yield();\n        uintptr_t head = connections_to_reclaim.head.fetch_and_store( garbage_connection_queue::empty );\n        thr->process_requests( head );\n        wakeup_some_tbb_threads();\n    }\n}\n\ntemplate<typename Server, typename Client>\nvoid connection_scavenger_thread::add_request( generic_connection<Server,Client>* conn_to_close )\n{\n    uintptr_t conn_ex = (uintptr_t)conn_to_close | (connection_traits<Server,Client>::is_tbb<<1);\n    __TBB_ASSERT( !conn_to_close->next_conn, NULL );\n    const uintptr_t old_tail_ex = connections_to_reclaim.tail.fetch_and_store(conn_ex);\n    __TBB_ASSERT( old_tail_ex==0||old_tail_ex>garbage_connection_queue::plugged_acked, \"Unloading DLL called while this connection is being closed?\" );\n\n    if( old_tail_ex==garbage_connection_queue::empty )\n        connections_to_reclaim.head = conn_ex;\n    else {\n        bool is_tbb = (old_tail_ex&2)>0;\n        uintptr_t old_tail = old_tail_ex & ~(uintptr_t)3;\n        if( is_tbb )\n            reinterpret_cast<tbb_connection_v2*>(old_tail)->next_conn = reinterpret_cast<tbb_connection_v2*>(conn_ex);\n        else\n            reinterpret_cast<omp_connection_v2*>(old_tail)->next_conn = reinterpret_cast<omp_connection_v2*>(conn_ex);\n    }\n\n    if( state==ts_asleep )\n        wakeup();\n}\n\ntemplate<>\nuintptr_t connection_scavenger_thread::grab_and_prepend( generic_connection<tbb_server,tbb_client>* /*last_conn_to_close*/ ) { return 0;}\n\ntemplate<>\nuintptr_t connection_scavenger_thread::grab_and_prepend( generic_connection<omp_server,omp_client>* last_conn_to_close )\n{\n    uintptr_t conn_ex = (uintptr_t)last_conn_to_close;\n    uintptr_t head = connections_to_reclaim.head.fetch_and_store( garbage_connection_queue::empty );\n    reinterpret_cast<omp_connection_v2*>(last_conn_to_close)->next_conn = reinterpret_cast<omp_connection_v2*>(head);\n    return conn_ex;\n}\n\nextern \"C\" ULONGLONG NTAPI VerSetConditionMask( ULONGLONG, DWORD, BYTE);\n\nbool is_windows7_or_later ()\n{\n    try {\n        return GetOSVersion()>=IResourceManager::Win7OrLater;\n    } catch( ... ) {\n        return false;\n    }\n}\n\n#endif /* RML_USE_WCRM */\n\ntemplate<typename Connection, typename Server, typename Client>\nstatic factory::status_type connect( factory& f, Server*& server, Client& client ) {\n    server = new Connection(*static_cast<wait_counter*>(f.scratch_ptr),client);\n    return factory::st_success;\n}\n\nvoid init_rml_module () {\n    the_balance = the_default_concurrency = tbb::internal::AvailableHwConcurrency() - 1;\n#if RML_USE_WCRM\n    connection_scavenger.launch();\n#endif\n}\n\nextern \"C\" factory::status_type __RML_open_factory( factory& f, version_type& server_version, version_type client_version ) {\n    // Hack to keep this library from being closed by causing the first client's dlopen to not have a corresponding dlclose.\n    // This code will be removed once we figure out how to do shutdown of the RML perfectly.\n    static tbb::atomic<bool> one_time_flag;\n    if( one_time_flag.compare_and_swap(true,false)==false) {\n        __TBB_ASSERT( (size_t)f.library_handle!=factory::c_dont_unload, NULL );\n#if _WIN32||_WIN64\n        f.library_handle = reinterpret_cast<HMODULE>(factory::c_dont_unload);\n#else\n        f.library_handle = reinterpret_cast<void*>(factory::c_dont_unload);\n#endif\n    }\n    // End of hack\n\n    // Initialize the_balance only once\n    tbb::internal::atomic_do_once ( &init_rml_module, rml_module_state );\n\n    server_version = SERVER_VERSION;\n    f.scratch_ptr = 0;\n    if( client_version==0 ) {\n        return factory::st_incompatible;\n#if RML_USE_WCRM\n    } else if ( !is_windows7_or_later() ) {\n#if TBB_USE_DEBUG\n        fprintf(stderr, \"This version of the RML library requires Windows 7 to run on.\\nConnection request denied.\\n\");\n#endif\n        return factory::st_incompatible;\n#endif\n    } else {\n#if TBB_USE_DEBUG\n        if( client_version<EARLIEST_COMPATIBLE_CLIENT_VERSION )\n            fprintf(stderr, \"This client library is too old for the current RML server.\\nThe connection request is granted but oversubscription/undersubscription may occur.\\n\");\n#endif\n        f.scratch_ptr = new wait_counter;\n        return factory::st_success;\n    }\n}\n\nextern \"C\" void __RML_close_factory( factory& f ) {\n    if( wait_counter* fc = static_cast<wait_counter*>(f.scratch_ptr) ) {\n        f.scratch_ptr = 0;\n        fc->wait();\n        size_t bal = the_balance;\n        f.scratch_ptr = (void*)bal;\n        delete fc;\n    }\n}\n\nvoid call_with_build_date_str( ::rml::server_info_callback_t cb, void* arg );\n\n}} // rml::internal\n\nnamespace tbb {\nnamespace internal {\nnamespace rml {\n\nextern \"C\" tbb_factory::status_type __TBB_make_rml_server( tbb_factory& f, tbb_server*& server, tbb_client& client ) {\n    return ::rml::internal::connect< ::rml::internal::tbb_connection_v2>(f,server,client);\n}\n\nextern \"C\" void __TBB_call_with_my_server_info( ::rml::server_info_callback_t cb, void* arg ) {\n    return ::rml::internal::call_with_build_date_str( cb, arg );\n}\n\n}}}\n\nnamespace __kmp {\nnamespace rml {\n\nextern \"C\" omp_factory::status_type __KMP_make_rml_server( omp_factory& f, omp_server*& server, omp_client& client ) {\n    return ::rml::internal::connect< ::rml::internal::omp_connection_v2>(f,server,client);\n}\n\nextern \"C\" void __KMP_call_with_my_server_info( ::rml::server_info_callback_t cb, void* arg ) {\n    return ::rml::internal::call_with_build_date_str( cb, arg );\n}\n\n}}\n\n/*\n * RML server info\n */\n#include \"version_string.ver\"\n\n#ifndef __TBB_VERSION_STRINGS\n#pragma message(\"Warning: version_string.ver isn't generated properly by version_info.sh script!\")\n#endif\n\n// We use the build time as the RML server info. TBB is required to build RML, so we make it the same as the TBB build time.\n#ifndef __TBB_DATETIME\n#define __TBB_DATETIME __DATE__ \" \" __TIME__\n#endif\n\n#if !RML_USE_WCRM\n#define RML_SERVER_BUILD_TIME \"Intel(R) RML library built: \" __TBB_DATETIME\n#define RML_SERVER_VERSION_ST \"Intel(R) RML library version: v\" TOSTRING(SERVER_VERSION)\n#else\n#define RML_SERVER_BUILD_TIME \"Intel(R) RML library built: \" __TBB_DATETIME\n#define RML_SERVER_VERSION_ST \"Intel(R) RML library version: v\" TOSTRING(SERVER_VERSION) \" on ConcRT RM with \" RML_THREAD_KIND_STRING\n#endif\n\nnamespace rml {\nnamespace internal {\n\nvoid call_with_build_date_str( ::rml::server_info_callback_t cb, void* arg )\n{\n    (*cb)( arg, RML_SERVER_BUILD_TIME );\n    (*cb)( arg, RML_SERVER_VERSION_ST );\n}\n}} // rml::internal\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/tbbproxy/tbbproxy.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#include \"tbb/tbb_config.h\"\n#if !__TBB_WIN8UI_SUPPORT\n#define TBB_PREVIEW_RUNTIME_LOADER 1\n#include \"tbb/runtime_loader.h\"\n#include \"tbb/tbb_stddef.h\"\n\n// C standard headers.\n#include <cctype>            // isspace\n#include <cstdarg>           // va_list, etc.\n#include <cstdio>            // fprintf, stderr, etc.\n#include <cstdlib>           // malloc, free, abort.\n#include <cstring>           // strlen, etc.\n\n// C++ standard headers.\n#include <typeinfo>\n\n// OS-specific includes.\n#if _WIN32 || _WIN64\n    #include <windows.h>\n    #define snprintf _snprintf\n    #undef max\n#else\n    #include <dlfcn.h>    // dlopen, dlsym, dlclose, dlerror.\n#endif\n\n#if TBB_USE_ASSERT\n    // We cannot use __TBB_ASSERT as it is because it calls a function from tbb library which may\n    // be not yet loaded. Redefine __TBB_ASSERT not to call tbb functions.\n    #undef __TBB_ASSERT\n    #define __TBB_ASSERT( cond, msg ) {                                                            \\\n        if ( ! (cond) ) {                                                                          \\\n            say( \"%s:%d: Assertion failed: %s.\", __FILE__, __LINE__, (msg) );                      \\\n        } /* if */                                                                                 \\\n        /* TODO: abort? */                                                                         \\\n    }\n#endif\n\n// Declare here, define at the bottom.\nextern \"C\" int __tbb_internal_runtime_loader_stub();\n\nnamespace tbb {\n\nnamespace interface6 {\n\nnamespace internal {\n\nnamespace runtime_loader {\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    User interaction utilities.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n// Print message to stderr. Do not call it directly, use say() or tell() instead.\nstatic void _say( char const * format, va_list args ) {\n    /*\n        On 64-bit Linux* OS, vsnprintf() modifies args argument,\n        so vsnprintf() crashes if it is called for the second time with the same args.\n        To prevent the crash, we have to pass a fresh intact copy of args to vsnprintf() each time.\n\n        On Windows* OS, unfortunately, standard va_copy() macro is not available. However, it\n        seems vsnprintf() does not modify args argument.\n    */\n    #if ! ( _WIN32 || _WIN64 )\n        va_list _args;\n        __va_copy( _args, args );  // Make copy of args.\n        #define args _args         // Substitute args with its copy, _args.\n    #endif\n    int len = vsnprintf( NULL, 0, format, args );\n    #if ! ( _WIN32 || _WIN64 )\n        #undef args                // Remove substitution.\n        va_end( _args );\n    #endif\n    char * buf = reinterpret_cast< char * >( malloc( len + 1 ) );\n    if ( buf != NULL ) {\n        vsnprintf( buf, len + 1, format, args );\n        fprintf( stderr, \"TBB: %s\\n\", buf );\n        free( buf );\n    } else {\n        fprintf( stderr, \"TBB: Not enough memory for message: %s\\n\", format );\n    }\n} // _say\n\n\n// Debug/test/troubleshooting printing controlled by TBB_VERSION environment variable.\n// To enable printing, the variable must be set and not empty.\n// Do not call it directly, use tell() instead.\nstatic void _tell( char const * format, va_list args ) {\n    char const * var = getenv( \"TBB_VERSION\" );\n    if ( var != NULL && var[ 0 ] != 0 ) {\n        _say( format, args );\n    } // if\n} // _tell\n\n\n// Print message to stderr unconditionally.\nstatic void say( char const * format, ... ) {\n    va_list args;\n    va_start( args, format );\n    _say( format, args );\n    va_end( args );\n} // say\n\n\n// Debug/test/troubleshooting printing controlled by TBB_VERSION environment variable.\n// To enable printing, the variable must be set and not empty.\nstatic void tell( char const * format, ... ) {\n    va_list args;\n    va_start( args, format );\n    _tell( format, args );\n    va_end( args );\n} // tell\n\n\n// Error reporting utility. Behavior depends on mode.\nstatic tbb::runtime_loader::error_code error( tbb::runtime_loader::error_mode mode, tbb::runtime_loader::error_code err, char const * format, ... ) {\n    va_list args;\n    va_start( args, format );\n    if ( mode == tbb::runtime_loader::em_abort ) {\n        // In em_abort mode error message printed unconditionally.\n        _say( format, args );\n    } else {\n        // In other modes printing depends on TBB_VERSION environment variable.\n        _tell( format, args );\n    } // if\n    va_end( args );\n    switch ( mode ) {\n        case tbb::runtime_loader::em_abort : {\n            say( \"Aborting...\" );\n            #if TBB_USE_DEBUG && ( _WIN32 || _WIN64 )\n                DebugBreak();\n            #endif\n            abort();\n        } break;\n        case tbb::runtime_loader::em_throw : {\n            throw err;\n        } break;\n        case tbb::runtime_loader::em_status : {\n            // Do nothing.\n        } break;\n    } // switch\n    return err;\n} // error\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    General-purpose string manipulation utilities.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n// Delete character ch from string str in-place.\nstatic void strip( char * str, char ch ) {\n    int in  = 0;  // Input character index.\n    int out = 0;  // Output character index.\n    for ( ; ; ) {\n        if ( str[ in ] != ch ) {\n            str[ out ] = str[ in ];\n            ++ out;\n        } // if\n        if ( str[ in ] == 0 ) {\n            break;\n        } // if\n        ++ in;\n    } // forever\n} // func strip\n\n\n// Strip trailing whitespaces in-place.\nstatic void trim( char * str ) {\n    size_t len = strlen( str );\n    while ( len > 0 && isspace( str[ len - 1 ] ) ) {\n        -- len;\n    } // while\n    str[ len ] = 0;\n} // func trim\n\n\n#if _WIN32 || _WIN64\n    // \"When specifying a path, be sure to use backslashes (\\), not forward slashes (/).\"\n    // (see http://msdn.microsoft.com/en-us/library/ms886736.aspx).\n    const char proper_slash = '\\\\';\n    inline char char_or_slash( char c ) { return c=='/'? '\\\\': c; }\n#else\n    const char proper_slash = '/';\n    inline char char_or_slash( char c ) { return c; }\n#endif\n\n// Concatenate name of directory and name of file.\nvoid cat_file( char const * dir, char const * file, char * buffer, size_t len ) {\n    size_t i = 0;\n    // Copy directory name\n    for( ; i<len && *dir; ++i, ++dir ) {\n        buffer[i] = char_or_slash(*dir);\n    }\n    // Append trailing slash if missed.\n    if( i>0 && i<len && buffer[i-1]!=proper_slash ) {\n        buffer[i++] = proper_slash;\n    }\n    // Copy file name\n    __TBB_ASSERT( char_or_slash(*file)!=proper_slash, \"File name starts with a slash\" );\n    for( ; i<len && *file; ++i, ++file ) {\n        buffer[i] = *file;\n    }\n    // Append null terminator\n    buffer[ i<len? i: len-1 ] = '\\0';\n} // cat_file\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    Windows implementation of dlopen, dlclose, dlsym, dlerror.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n#if _WIN32 || _WIN64\n\n    // Implement Unix-like interface (dlopen, dlclose, dlsym, dlerror) via Win32 API functions.\n\n    // Type of dlopen result.\n    typedef HMODULE handle_t;\n\n    enum rtld_flags_t {\n        RTLD_NOW,\n        RTLD_GLOBAL\n    }; // enum rtld_flags_t\n\n    // Unix-like dlopen().\n    static handle_t dlopen( char const * name, rtld_flags_t ) {\n        return LoadLibrary( name );\n    } // dlopen\n\n    // Unix-like dlsym().\n    static void * dlsym( handle_t lib, char const * sym ) {\n        return (void*)GetProcAddress( lib, sym );\n    } // dlsym\n\n    // Unix-like dlclose().\n    static int dlclose( handle_t lib ) {\n        return ! FreeLibrary( lib );\n    } // dlclose\n\n    // The function mimics Unix dlerror() function.\n    // Note: Not thread-safe due to statically allocated buffer.\n    static char * dlerror() {\n\n        static char buffer[ 2048 ];  // Note: statically allocated buffer.\n\n        DWORD err = GetLastError();\n        if ( err == ERROR_SUCCESS ) {\n            return NULL;\n        } // if\n\n        DWORD rc;\n        rc =\n            FormatMessage(\n                FORMAT_MESSAGE_FROM_SYSTEM,\n                NULL,\n                err,\n                MAKELANGID( LANG_NEUTRAL, SUBLANG_DEFAULT ), // Default language.\n                reinterpret_cast< LPTSTR >( & buffer ),\n                sizeof( buffer ),\n                NULL\n            );\n        if ( rc == 0 ) {\n            // FormatMessage() failed to format system error message. Buffer to short or another issue.\n            snprintf( buffer, sizeof( buffer ), \"System error %u.\", err );\n        } else {\n            /*\n                FormatMessage() returns Windows-style end-of-lines, \"\\r\\n\". When string is printed,\n                printf() also replaces all the occurrences of \"\\n\" with \"\\r\\n\" (again!), so sequences\n                like \"\\r\\r\\r\\n\" appear in output. It is not too good. Stripping all \"\\r\" normalizes\n                string and returns it to canonical form, so printf() will produce correct end-of-line\n                sequences.\n            */\n            strip( buffer, '\\r' );   // Delete carriage returns if any.\n            trim( buffer );          // Delete trailing newlines and spaces.\n        } // if\n\n        return buffer;\n\n    } // dlerror\n\n#else\n\n    // Type of dlopen() result.\n    typedef void * handle_t;\n\n#endif\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    Runtime loader stuff.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n// Descriptor table declaration. It is defined in assembler file.\nenum symbol_type_t {\n    st_object   = 0,\n    st_function = 1\n}; // enum symbol_type_t\nstruct symbol_t {\n    void *        addr;\n    char const *  name;\n    int           size;\n    symbol_type_t type;\n}; // symbol_t\nextern \"C\" symbol_t __tbb_internal_runtime_loader_symbols[];\n\n// Hooks for internal use (e. g. for testing).\ntbb::runtime_loader::error_mode stub_mode = tbb::runtime_loader::em_abort;\n\nstatic char const * tbb_dll_name = __TBB_STRING(__TBB_DLL_NAME);  // Name of TBB library.\nstatic handle_t     handle       = NULL;                          // Handle of loaded TBB library or NULL.\nstatic int          version      = 0;                             // Version of the loaded library.\nstatic int          counter      = 0;                             // Number of runtime_loader objects using the loaded library.\n\n#define ANOTHER_RTL \"probably multiple runtime_loader objects work in parallel\"\n\n\n// One attempt to load library (dll_name can be a full path or just a file name).\nstatic tbb::runtime_loader::error_code _load( char const * dll_name, int min_ver, int max_ver ) {\n\n    tbb::runtime_loader::error_mode mode = tbb::runtime_loader::em_status;\n    tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;\n\n    /*\n        If these variables declared at the first usage, Intel C++ Compiler may issue warning(s):\n            transfer of control [goto error] bypasses initialization of: ...\n        Declaring variables at the beginning of the function eliminates warnings.\n    */\n    typedef int (*int_func_t)( void );\n    char const * get_ver_name = \"TBB_runtime_interface_version\"; // Name of function.\n    int_func_t   get_ver_func = NULL;                            // Pointer to function.\n    handle_t     _handle      = NULL;\n    int          _version     = 0;\n    int          total        = 0;\n    int          not_found    = 0;\n\n    // This function should be called iff there is no loaded library.\n    __TBB_ASSERT( handle  == NULL, \"Handle is invalid; \"  ANOTHER_RTL );\n    __TBB_ASSERT( version == 0,    \"Version is invalid; \" ANOTHER_RTL );\n    __TBB_ASSERT( counter == 0,    \"Counter is invalid; \" ANOTHER_RTL );\n\n    tell( \"Loading \\\"%s\\\"...\", dll_name );\n\n    // First load the library.\n    _handle = dlopen( dll_name, RTLD_NOW );\n    if ( _handle == NULL ) {\n        const char * msg = dlerror();\n        code = error( mode, tbb::runtime_loader::ec_no_lib, \"Loading \\\"%s\\\" failed; system error: %s\", dll_name, msg );\n        goto error;\n    } // if\n\n    // Then try to find out its version.\n    /*\n        g++ 3.4 issues error:\n            ISO C++ forbids casting between pointer-to-function and pointer-to-object\n        on reinterpret_cast<>. Thus, we have no choice but using C-style type cast.\n    */\n    get_ver_func = (int_func_t) dlsym( _handle, get_ver_name );\n    if ( get_ver_func == NULL ) {\n        code = error( mode, tbb::runtime_loader::ec_bad_lib, \"Symbol \\\"%s\\\" not found; library rejected.\", get_ver_name );\n        goto error;\n    } // if\n    _version = get_ver_func();\n    if ( ! ( min_ver <= _version && _version <= max_ver ) ) {\n        code = error( mode, tbb::runtime_loader::ec_bad_ver, \"Version %d is out of requested range; library rejected.\", _version );\n        goto error;\n    } // if\n\n    // Library is suitable. Mark it as loaded.\n    handle   = _handle;\n    version  = _version;\n    counter += 1;\n    __TBB_ASSERT( counter == 1, \"Counter is invalid; \" ANOTHER_RTL );\n\n    // Now search for all known symbols.\n    for ( int i = 0; __tbb_internal_runtime_loader_symbols[ i ].name != NULL; ++ i ) {\n        symbol_t & symbol = __tbb_internal_runtime_loader_symbols[ i ];\n        // Verify symbol descriptor.\n        __TBB_ASSERT( symbol.type == st_object || symbol.type == st_function, \"Invalid symbol type\" );\n        #if _WIN32 || _WIN64\n            __TBB_ASSERT( symbol.type == st_function, \"Should not be symbols of object type on Windows\" );\n        #endif\n        if ( symbol.type == st_object ) {\n            __TBB_ASSERT( symbol.addr != NULL, \"Object address invalid\" );\n            __TBB_ASSERT( symbol.size > 0, \"Symbol size must be > 0\" );\n            __TBB_ASSERT( symbol.size <= 0x1000, \"Symbol size too big\" );\n        } else {                     // Function\n            // __TBB_ASSERT( symbol.addr == reinterpret_cast< void * >( & stub ), \"Invalid symbol address\" );\n            __TBB_ASSERT( symbol.size == sizeof( void * ), \"Invalid symbol size\" );\n        } // if\n        void * addr = dlsym( _handle, symbol.name );\n        if ( addr != NULL ) {\n            if ( symbol.type == st_object ) {\n                if ( strncmp( symbol.name, \"_ZTS\", 4 ) == 0 ) {\n                    // If object name begins with \"_ZTS\", it is a string, mangled type name.\n                    // Its value must equal to name of symbol without \"_ZTS\" prefix.\n                    char const * name = static_cast< char const * >( addr );\n                    __TBB_ASSERT( strlen( name ) + 1 == size_t( symbol.size ), \"Unexpected size of typeinfo name\" );\n                    __TBB_ASSERT( strcmp( symbol.name + 4, name ) == 0, \"Unexpected content of typeinfo name\" );\n                    strncpy( reinterpret_cast< char * >( symbol.addr ), name, symbol.size );\n                    reinterpret_cast< char * >( symbol.addr )[ symbol.size - 1 ] = 0;\n                } else {\n                    #if TBB_USE_ASSERT\n                        // If object name begins with \"_ZTI\", it is an object of std::type_info class.\n                        // Its protected value must equal to name of symbol without \"_ZTI\" prefix.\n                        if ( strncmp( symbol.name, \"_ZTI\", 4 ) == 0 ) {\n                            std::type_info const * info = static_cast< std::type_info const * >( addr );\n                            __TBB_ASSERT( size_t( symbol.size ) >= sizeof( std::type_info ), \"typeinfo size is too small\" );\n                            // std::type_info::name is not a virtual method, it is safe to call it.\n                            __TBB_ASSERT( strcmp( symbol.name + 4, info->name() ) == 0, \"Unexpected content of typeinfo\" );\n                        } // if\n                    #endif\n                    // Copy object content from libtbb into runtime_loader.\n                    memcpy( symbol.addr, addr, symbol.size );\n                }; // if\n            } else {                     // Function\n                symbol.addr = addr;\n            } // if\n        } else {\n            char const * msg = dlerror();\n            tell( \"Symbol \\\"%s\\\" not found; system error: %s\", symbol.name, msg );\n            ++ not_found;\n        } // if\n        ++ total;\n    } // for i\n\n    if ( not_found > 0 ) {\n        tell( \"%d of %d symbols not found.\", not_found, total );\n    } // if\n\n    tell( \"The library successfully loaded.\" );\n    return code;\n\n    error:\n        if ( _handle != NULL ) {\n            int rc = dlclose( _handle );\n            if ( rc != 0 ) {\n                // Error occurred.\n                __TBB_ASSERT( rc != 0, \"Unexpected error: dlclose() failed\" );\n            } // if\n        } // if\n        _handle = NULL;\n        return code;\n\n} // _load\n\n\nstatic tbb::runtime_loader::error_code load( tbb::runtime_loader::error_mode mode, char const * path[], int min_ver, int max_ver ) {\n    // Check arguments first.\n    if ( min_ver <= 0 ) {\n        return error( mode, tbb::runtime_loader::ec_bad_arg, \"tbb::runtime_loader::load(): Invalid value of min_ver argument: %d.\", min_ver );\n    } // if\n    if ( max_ver <= 0 ) {\n        return error( mode, tbb::runtime_loader::ec_bad_arg, \"tbb::runtime_loader::load(): Invalid value of max_ver argument: %d.\", max_ver );\n    } // if\n    if ( min_ver > max_ver ) {\n        return error( mode, tbb::runtime_loader::ec_bad_arg, \"tbb::runtime_loader::load(): min_ver and max_ver specify empty range: [%d, %d].\", min_ver, max_ver );\n    } // if\n    if ( min_ver == max_ver ) {\n        tell( \"Searching for \\\"%s\\\" version %d...\", tbb_dll_name, min_ver );\n    } else if ( max_ver == INT_MAX ) {\n        tell( \"Searching for \\\"%s\\\" version %d+...\", tbb_dll_name, min_ver );\n    } else {\n        tell( \"Searching for \\\"%s\\\" version in range [%d, %d]...\", tbb_dll_name, min_ver, max_ver );\n    } // if\n    // Then check whether a library already loaded.\n    if ( handle != NULL ) {\n        // Library already loaded. Check whether the version is compatible.\n        __TBB_ASSERT( version > 0, \"Version is invalid; \" ANOTHER_RTL );\n        __TBB_ASSERT( counter > 0, \"Counter is invalid; \" ANOTHER_RTL );\n        if ( min_ver <= version && version <= max_ver ) {\n            // Version is ok, let us use this library.\n            tell( \"Library version %d is already loaded.\", version );\n            counter += 1;\n            return tbb::runtime_loader::ec_ok;\n        } else {\n            // Version is not suitable.\n            return error( mode, tbb::runtime_loader::ec_bad_ver, \"Library version %d is already loaded.\", version );\n        } // if\n    } // if\n    // There is no loaded library, try to load it using provided directories.\n    __TBB_ASSERT( version == 0, \"Version is invalid; \" ANOTHER_RTL );\n    __TBB_ASSERT( counter == 0, \"Counter is invalid; \" ANOTHER_RTL );\n    size_t namelen = strlen(tbb_dll_name);\n    size_t buflen = 0;\n    char * buffer = NULL;\n    for ( int i = 0; path[i] != NULL; ++ i ) {\n        size_t len = strlen(path[i]) + namelen + 2; // 1 for slash and 1 for null terminator\n        if( buflen<len ) {\n            free( buffer );\n            buflen = len;\n            buffer = (char*)malloc( buflen );\n            if( !buffer )\n                return error( mode, tbb::runtime_loader::ec_no_lib, \"Not enough memory.\" );\n        }\n        cat_file( path[i], tbb_dll_name, buffer, buflen );\n        __TBB_ASSERT(strstr(buffer,tbb_dll_name), \"Name concatenation error\");\n        tbb::runtime_loader::error_code ec = _load( buffer, min_ver, max_ver );\n        if ( ec == tbb::runtime_loader::ec_ok ) {\n            return ec;       // Success. Exiting...\n        } // if\n    } // for i\n    free( buffer );\n    return error( mode, tbb::runtime_loader::ec_no_lib, \"No suitable library found.\" );\n} // load\n\n\n\n\n// Suppress \"defined but not used\" compiler warnings.\nstatic void const * dummy[] = {\n    (void *) & strip,\n    (void *) & trim,\n    & dummy,\n    NULL\n};\n\n\n} // namespace runtime_loader\n\n} // namespace internal\n\n\nruntime_loader::runtime_loader( error_mode mode ) :\n    my_mode( mode ),\n    my_status( ec_ok ),\n    my_loaded( false )\n{\n} // ctor\n\n\nruntime_loader::runtime_loader( char const * path[], int min_ver, int max_ver, error_mode mode ) :\n    my_mode( mode ),\n    my_status( ec_ok ),\n    my_loaded( false )\n{\n    load( path, min_ver, max_ver );\n} // ctor\n\n\nruntime_loader::~runtime_loader() {\n} // dtor\n\n\ntbb::runtime_loader::error_code runtime_loader::load( char const * path[], int min_ver, int max_ver ) {\n    if ( my_loaded ) {\n        my_status = tbb::interface6::internal::runtime_loader::error( my_mode, ec_bad_call, \"tbb::runtime_loader::load(): Library already loaded by this runtime_loader object.\" );\n    } else {\n        my_status = internal::runtime_loader::load( my_mode, path, min_ver, max_ver );\n        if ( my_status == ec_ok ) {\n            my_loaded = true;\n        } // if\n    } // if\n    return my_status;\n} // load\n\n\n\n\ntbb::runtime_loader::error_code runtime_loader::status() {\n    return my_status;\n} // status\n\n\n} // namespace interface6\n\n} // namespace tbb\n\n\n// Stub function replaces all TBB entry points when no library is loaded.\nint __tbb_internal_runtime_loader_stub() {\n    char const * msg = NULL;\n    if ( tbb::interface6::internal::runtime_loader::handle == NULL ) {\n        msg = \"A function is called while TBB library is not loaded\";\n    } else {\n        msg = \"A function is called which is not present in loaded TBB library\";\n    } // if\n    return tbb::interface6::internal::runtime_loader::error( tbb::interface6::internal::runtime_loader::stub_mode, tbb::runtime_loader::ec_no_lib, msg );\n} // stub\n\n#endif // !__TBB_WIN8UI_SUPPORT //\n// end of file //\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/tbbmalloc/tbbmalloc.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#include \"TypeDefinitions.h\" // Customize.h and proxy.h get included\n#include \"tbbmalloc_internal_api.h\"\n\n#include \"../tbb/tbb_assert_impl.h\" // Out-of-line TBB assertion handling routines are instantiated here.\n\n#undef UNICODE\n\n#if USE_PTHREAD\n#include <dlfcn.h> // dlopen\n#elif USE_WINTHREAD\n#include \"tbb/machine/windows_api.h\"\n#endif\n\nnamespace rml {\nnamespace internal {\n\n#if TBB_USE_DEBUG\n#define DEBUG_SUFFIX \"_debug\"\n#else\n#define DEBUG_SUFFIX\n#endif /* TBB_USE_DEBUG */\n\n// MALLOCLIB_NAME is the name of the TBB memory allocator library.\n#if _WIN32||_WIN64\n#define MALLOCLIB_NAME \"tbbmalloc\" DEBUG_SUFFIX \".dll\"\n#elif __APPLE__\n#define MALLOCLIB_NAME \"libtbbmalloc\" DEBUG_SUFFIX \".dylib\"\n#elif __FreeBSD__ || __NetBSD__ || __OpenBSD__ || __sun || _AIX || __ANDROID__\n#define MALLOCLIB_NAME \"libtbbmalloc\" DEBUG_SUFFIX \".so\"\n#elif __linux__\n#define MALLOCLIB_NAME \"libtbbmalloc\" DEBUG_SUFFIX  __TBB_STRING(.so.TBB_COMPATIBLE_INTERFACE_VERSION)\n#else\n#error Unknown OS\n#endif\n\nvoid init_tbbmalloc() {\n#if DO_ITT_NOTIFY\n    MallocInitializeITT();\n#endif\n\n/* Preventing TBB allocator library from unloading to prevent\n   resource leak, as memory is not released on the library unload.\n*/\n#if USE_WINTHREAD && !__TBB_SOURCE_DIRECTLY_INCLUDED && !__TBB_WIN8UI_SUPPORT\n    // Prevent Windows from displaying message boxes if it fails to load library\n    UINT prev_mode = SetErrorMode (SEM_FAILCRITICALERRORS);\n    HMODULE lib;\n    BOOL ret = GetModuleHandleEx(GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS\n                                 |GET_MODULE_HANDLE_EX_FLAG_PIN,\n                                 (LPCTSTR)&scalable_malloc, &lib);\n    MALLOC_ASSERT(lib && ret, \"Allocator can't find itself.\");\n    SetErrorMode (prev_mode);\n#endif /* USE_PTHREAD && !__TBB_SOURCE_DIRECTLY_INCLUDED */\n}\n\n#if !__TBB_SOURCE_DIRECTLY_INCLUDED\n#if USE_WINTHREAD\nextern \"C\" BOOL WINAPI DllMain( HINSTANCE /*hInst*/, DWORD callReason, LPVOID lpvReserved)\n{\n    if (callReason==DLL_THREAD_DETACH)\n    {\n        __TBB_mallocThreadShutdownNotification();\n    }\n    else if (callReason==DLL_PROCESS_DETACH)\n    {\n        __TBB_mallocProcessShutdownNotification(lpvReserved != NULL);\n    }\n    return TRUE;\n}\n#else /* !USE_WINTHREAD */\nstruct RegisterProcessShutdownNotification {\n// Work around non-reentrancy in dlopen() on Android\n#if !__TBB_USE_DLOPEN_REENTRANCY_WORKAROUND\n    RegisterProcessShutdownNotification() {\n        // prevents unloading, POSIX case\n        dlopen(MALLOCLIB_NAME, RTLD_NOW);\n    }\n#endif /* !__TBB_USE_DLOPEN_REENTRANCY_WORKAROUND */\n    ~RegisterProcessShutdownNotification() {\n        __TBB_mallocProcessShutdownNotification(false);\n    }\n};\n\nstatic RegisterProcessShutdownNotification reg;\n#endif /* !USE_WINTHREAD */\n#endif /* !__TBB_SOURCE_DIRECTLY_INCLUDED */\n\n} } // namespaces\n\n#if __TBB_ipf\n/* It was found that on IA-64 architecture inlining of __TBB_machine_lockbyte leads\n   to serious performance regression with ICC. So keep it out-of-line.\n\n   This code is copy-pasted from tbb_misc.cpp.\n */\nextern \"C\" intptr_t __TBB_machine_lockbyte( volatile unsigned char& flag ) {\n    tbb::internal::atomic_backoff backoff;\n    while( !__TBB_TryLockByte(flag) ) backoff.pause();\n    return 0;\n}\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/harness_dynamic_libs.h": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#include \"tbb/tbb_config.h\"\n\n// Include this header file before harness.h for HARNESS_SKIP_TEST to take effect\n#if !__TBB_DYNAMIC_LOAD_ENABLED\n#define HARNESS_SKIP_TEST 1\n#else\n\n#if _WIN32 || _WIN64\n#include \"tbb/machine/windows_api.h\"\n#else\n#include <dlfcn.h>\n#endif\n#include \"harness_assert.h\"\n\nnamespace Harness {\n\n#if TBB_USE_DEBUG\n#define SUFFIX1 \"_debug\"\n#define SUFFIX2\n#else\n#define SUFFIX1\n#define SUFFIX2 \"_debug\"\n#endif /* TBB_USE_DEBUG */\n\n#if _WIN32||_WIN64\n#define PREFIX\n#define EXT \".dll\"\n#else\n#define PREFIX \"lib\"\n#if __APPLE__\n#define EXT \".dylib\"\n// Android SDK build system does not support .so file name versioning\n#elif __FreeBSD__ || __NetBSD__ || __sun || _AIX || __ANDROID__\n#define EXT \".so\"\n#elif __linux__  // Order of these elif's matters!\n#define EXT __TBB_STRING(.so.TBB_COMPATIBLE_INTERFACE_VERSION)\n#else\n#error Unknown OS\n#endif\n#endif\n\n// Form the names of the TBB memory allocator binaries.\n#define MALLOCLIB_NAME1 PREFIX \"tbbmalloc\" SUFFIX1 EXT\n#define MALLOCLIB_NAME2 PREFIX \"tbbmalloc\" SUFFIX2 EXT\n\n#if _WIN32 || _WIN64\ntypedef  HMODULE LIBRARY_HANDLE;\n#else\ntypedef void *LIBRARY_HANDLE;\n#endif\n\n#if _WIN32 || _WIN64\n#define TEST_LIBRARY_NAME(base) base\".dll\"\n#elif __APPLE__\n#define TEST_LIBRARY_NAME(base) base\".dylib\"\n#else\n#define TEST_LIBRARY_NAME(base) base\".so\"\n#endif\n\nLIBRARY_HANDLE OpenLibrary(const char *name)\n{\n#if _WIN32 || _WIN64\n#if __TBB_WIN8UI_SUPPORT\n    TCHAR wlibrary[MAX_PATH];\n    if ( MultiByteToWideChar(CP_UTF8, 0, name, -1, wlibrary, MAX_PATH) == 0 ) return false;\n    return :: LoadPackagedLibrary( wlibrary, 0 );\n#else\n    return ::LoadLibrary(name);\n#endif\n#else\n    return dlopen(name, RTLD_NOW|RTLD_GLOBAL);\n#endif\n}\n\nvoid CloseLibrary(LIBRARY_HANDLE lib)\n{\n#if _WIN32 || _WIN64\n    BOOL ret = FreeLibrary(lib);\n    ASSERT(ret, \"FreeLibrary must be successful\");\n#else\n    int ret = dlclose(lib);\n    ASSERT(ret == 0, \"dlclose must be successful\");\n#endif\n}\n\ntypedef void (*FunctionAddress)();\n\ntemplate <typename FunctionPointer>\nvoid GetAddress(Harness::LIBRARY_HANDLE lib, const char *name, FunctionPointer& func)\n{\n#if _WIN32 || _WIN64\n    func = (FunctionPointer)(void*)GetProcAddress(lib, name);\n#else\n    func = (FunctionPointer)dlsym(lib, name);\n#endif\n    ASSERT(func, \"Can't find required symbol in dynamic library\");\n}\n\nFunctionAddress GetAddress(Harness::LIBRARY_HANDLE lib, const char *name)\n{\n    FunctionAddress func;\n    GetAddress(lib, name, func);\n    return func;\n}\n\n}  // namespace Harness\n\n#endif // __TBB_DYNAMIC_LOAD_ENABLED\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/test_model_plugin.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#define HARNESS_DEFAULT_MIN_THREADS 4\n#define HARNESS_DEFAULT_MAX_THREADS 4\n\n// Need to include \"tbb/tbb_config.h\" to obtain the definition of __TBB_DEFINE_MIC.\n#include \"tbb/tbb_config.h\"\n\n#if !__TBB_TODO || __TBB_WIN8UI_SUPPORT\n#define HARNESS_NO_PARSE_COMMAND_LINE 1\n#include \"harness.h\"\nint TestMain() {\n    return Harness::Skipped;\n}\n#else /* __TBB_TODO */\n// TODO: There are a lot of problems with unloading DLL which uses TBB with automatic initialization\n\n#if __TBB_DEFINE_MIC\n\n#ifndef _USRDLL\n#define HARNESS_NO_PARSE_COMMAND_LINE 1\n#include \"harness.h\"\nint TestMain() {\n    return Harness::Skipped;\n}\n#endif\n\n#else /* !__MIC__ */\n\n#if _WIN32 || _WIN64\n#include \"tbb/machine/windows_api.h\"\n#else\n#include <dlfcn.h>\n#endif\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <stdexcept>\n\n#if TBB_USE_EXCEPTIONS\n    #include \"harness_report.h\"\n#endif\n\n#ifdef _USRDLL\n#include \"tbb/task_scheduler_init.h\"\n\nclass CModel {\npublic:\n    CModel(void) {};\n    static tbb::task_scheduler_init tbb_init;\n\n    void init_and_terminate( int );\n};\n\ntbb::task_scheduler_init CModel::tbb_init(1);\n\n//! Test that task::initialize and task::terminate work when doing nothing else.\n/** maxthread is treated as the \"maximum\" number of worker threads. */\nvoid CModel::init_and_terminate( int maxthread ) {\n    for( int i=0; i<200; ++i ) {\n        switch( i&3 ) {\n            default: {\n                tbb::task_scheduler_init init( rand() % maxthread + 1 );\n                break;\n            }\n            case 0: {\n                tbb::task_scheduler_init init;\n                break;\n            }\n            case 1: {\n                tbb::task_scheduler_init init( tbb::task_scheduler_init::automatic );\n                break;\n            }\n            case 2: {\n                tbb::task_scheduler_init init( tbb::task_scheduler_init::deferred );\n                init.initialize( rand() % maxthread + 1 );\n                init.terminate();\n                break;\n            }\n        }\n    }\n}\n\nextern \"C\"\n#if _WIN32 || _WIN64\n__declspec(dllexport)\n#endif\nvoid plugin_call(int maxthread)\n{\n    srand(2);\n    __TBB_TRY {\n        CModel model;\n        model.init_and_terminate(maxthread);\n    } __TBB_CATCH( std::runtime_error& error ) {\n#if TBB_USE_EXCEPTIONS\n        REPORT(\"ERROR: %s\\n\", error.what());\n#endif /* TBB_USE_EXCEPTIONS */\n    }\n}\n\n#else /* _USRDLL undefined */\n\n#include \"harness.h\"\n#include \"harness_dynamic_libs.h\"\n#include \"harness_tls.h\"\n\nextern \"C\" void plugin_call(int);\n\nvoid report_error_in(const char* function_name)\n{\n#if _WIN32 || _WIN64\n    char* message;\n    int code = GetLastError();\n\n    FormatMessage(\n        FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM,\n        NULL, code,MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT),\n        (char*)&message, 0, NULL );\n#else\n    char* message = (char*)dlerror();\n    int code = 0;\n#endif\n    REPORT( \"%s failed with error %d: %s\\n\", function_name, code, message);\n\n#if _WIN32 || _WIN64\n    LocalFree(message);\n#endif\n}\n\ntypedef void (*PLUGIN_CALL)(int);\n\n#if __linux__\n    #define RML_LIBRARY_NAME(base) TEST_LIBRARY_NAME(base) \".1\"\n#else\n    #define RML_LIBRARY_NAME(base) TEST_LIBRARY_NAME(base)\n#endif\n\nint TestMain () {\n#if !RML_USE_WCRM\n    PLUGIN_CALL my_plugin_call;\n\n    LimitTLSKeysTo limitTLS(10);\n\n    Harness::LIBRARY_HANDLE hLib;\n#if _WIN32 || _WIN64\n    hLib = LoadLibrary(\"irml.dll\");\n    if ( !hLib )\n        hLib = LoadLibrary(\"irml_debug.dll\");\n    if ( !hLib )\n        return Harness::Skipped; // No shared RML, skip the test\n    FreeLibrary(hLib);\n#else /* !WIN */\n#if __TBB_ARENA_PER_MASTER\n    hLib = dlopen(RML_LIBRARY_NAME(\"libirml\"), RTLD_LAZY);\n    if ( !hLib )\n        hLib = dlopen(RML_LIBRARY_NAME(\"libirml_debug\"), RTLD_LAZY);\n    if ( !hLib )\n        return Harness::Skipped;\n    dlclose(hLib);\n#endif /* __TBB_ARENA_PER_MASTER */\n#endif /* OS */\n    for( int i=1; i<100; ++i ) {  \n        REMARK(\"Iteration %d, loading plugin library...\\n\", i);\n        hLib = Harness::OpenLibrary(TEST_LIBRARY_NAME(\"test_model_plugin_dll\"));\n        if ( !hLib ) {\n#if !__TBB_NO_IMPLICIT_LINKAGE\n#if _WIN32 || _WIN64\n            report_error_in(\"LoadLibrary\");\n#else\n            report_error_in(\"dlopen\");\n#endif\n            return -1;\n#else\n            return Harness::Skipped;\n#endif\n        }\n        my_plugin_call = (PLUGIN_CALL)Harness::GetAddress(hLib, \"plugin_call\");\n        if (my_plugin_call==NULL) {\n#if _WIN32 || _WIN64\n            report_error_in(\"GetProcAddress\");\n#else\n            report_error_in(\"dlsym\");\n#endif\n            return -1;\n        }\n        REMARK(\"Calling plugin method...\\n\");\n        my_plugin_call(MaxThread);\n\n        REMARK(\"Unloading plugin library...\\n\");\n        Harness::CloseLibrary(hLib);\n    } // end for(1,100)\n\n    return Harness::Done;\n#else\n    return Harness::Skipped;\n#endif /* !RML_USE_WCRM */\n}\n\n#endif//_USRDLL\n#endif//__MIC__\n\n#endif /*__TBB_WIN8UI_SUPPORT*/\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/test_cilk_dynamic_load.cpp": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#include \"tbb/tbb_config.h\"\n\n// Skip the test if no interoperability with cilkrts\n#define __TBB_CILK_INTEROP   (__TBB_SURVIVE_THREAD_SWITCH && __INTEL_COMPILER>=1200)\n// Skip the test when cilkrts did not have dlopen()/dlclose() start up feature\n#define CILK_SYMBOLS_VISIBLE (_WIN32||_WIN64)\n// The compiler does not add \"-lcilkrts\" linker option on some linux systems\n#define CILK_LINKAGE_BROKEN  (__linux__ && __GNUC__<4 && __INTEL_COMPILER_BUILD_DATE <= 20110427)\n// Currently, the interop doesn't support the situation:\n//1) Intel(R) Threading Building Blocks (Intel(R) TBB) is outermost;\n//2)   Intel(R) Cilk(TM) Plus, and it should be dynamically loaded with dlopen/LoadLibrary (possibly via a 3rd party module);\n//3)     Intel(R) TBB again;\n//4)       Intel(R) Cilk(TM) Plus again.\n#define HEAVY_NESTED_INTEROP_SUPPORT ( __INTEL_COMPILER_BUILD_DATE < 20110427 )\n\n#if __TBB_CILK_INTEROP && CILK_SYMBOLS_VISIBLE && !CILK_LINKAGE_BROKEN && HEAVY_NESTED_INTEROP_SUPPORT\n\n#include \"tbb/task_scheduler_init.h\"\n#include \"tbb/task.h\"\n\nstatic const int N = 25;\nstatic const int P_outer = 4;\nstatic const int P_nested = 2;\n\n#ifdef _USRDLL\n\n#include <cilk/cilk.h>\n#define HARNESS_CUSTOM_MAIN 1\n#include \"harness.h\"\n#undef HARNESS_CUSTOM_MAIN\n\n#if _WIN32 || _WIN64\n#define CILK_TEST_EXPORT extern \"C\" __declspec(dllexport)\n#else\n#define CILK_TEST_EXPORT extern \"C\"\n#endif /* _WIN32 || _WIN64 */\n\nbool g_sandwich = true; // have to be declare before #include \"test_cilk_common.h\"\n#include \"test_cilk_common.h\"\n\nCILK_TEST_EXPORT int CilkFib( int n )\n{\n    return TBB_Fib(n);\n}\n\nCILK_TEST_EXPORT void CilkShutdown()\n{\n    __cilkrts_end_cilk();\n}\n\n#else /* _USRDLL undefined */\n\n#include \"harness.h\"\n#include \"harness_dynamic_libs.h\"\n\nint SerialFib( int n ) {\n    int a=0, b=1;\n    for( int i=0; i<n; ++i ) {\n        b += a;\n        a = b-a;\n    }\n    return a;\n}\n\nint F = SerialFib(N);\n\ntypedef int (*CILK_CALL)(int);\nCILK_CALL CilkFib = 0;\n\ntypedef void (*CILK_SHUTDOWN)();\nCILK_SHUTDOWN CilkShutdown = 0;\n\nclass FibTask: public tbb::task {\n    int n;\n    int& result;\n    task* execute() __TBB_override {\n        if( n<2 ) {\n            result = n;\n        } else {\n\n            // TODO: why RTLD_LAZY was used here?\n            Harness::LIBRARY_HANDLE hLib =\n                Harness::OpenLibrary(TEST_LIBRARY_NAME(\"test_cilk_dynamic_load_dll\"));\n            CilkFib = (CILK_CALL)Harness::GetAddress(hLib, \"CilkFib\");\n            CilkShutdown = (CILK_SHUTDOWN)Harness::GetAddress(hLib, \"CilkShutdown\");\n\n            int x, y;\n            x = CilkFib(n-2);\n            y = CilkFib(n-1);\n            result = x+y;\n\n            CilkShutdown();\n\n            Harness::CloseLibrary(hLib);\n        }\n        return NULL;\n    }\npublic:\n    FibTask( int& result_, int n_ ) : result(result_), n(n_) {}\n};\n\n\nint TBB_Fib( int n ) {\n    if( n<2 ) {\n        return n;\n    } else {\n        int result;\n        tbb::task_scheduler_init init(P_nested);\n        tbb::task::spawn_root_and_wait(*new( tbb::task::allocate_root()) FibTask(result,n) );\n        return result;\n    }\n}\n\nvoid RunSandwich() {\n    tbb::task_scheduler_init init(P_outer);\n    int m = TBB_Fib(N);\n    ASSERT( m == F, NULL );\n}\n\nint TestMain () {\n    for ( int i = 0; i < 20; ++i )\n        RunSandwich();\n    return Harness::Done;\n}\n\n#endif /* _USRDLL */\n\n#else /* !__TBB_CILK_INTEROP */\n\n#include \"harness.h\"\n\nint TestMain () {\n    return Harness::Skipped;\n}\n\n#endif /* !__TBB_CILK_INTEROP */\n",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/include/tbb/tbb_config.h": "/*\n    Copyright (c) 2005-2018 Intel Corporation\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n\n\n*/\n\n#ifndef __TBB_tbb_config_H\n#define __TBB_tbb_config_H\n\n/** This header is supposed to contain macro definitions and C style comments only.\n    The macros defined here are intended to control such aspects of TBB build as\n    - presence of compiler features\n    - compilation modes\n    - feature sets\n    - known compiler/platform issues\n**/\n\n/* This macro marks incomplete code or comments describing ideas which are considered for the future.\n * See also for plain comment with TODO and FIXME marks for small improvement opportunities.\n */\n#define __TBB_TODO 0\n\n/* Check which standard library we use. */\n/* __TBB_SYMBOL is defined only while processing exported symbols list where C++ is not allowed. */\n#if !defined(__TBB_SYMBOL) && !__TBB_CONFIG_PREPROC_ONLY\n    #include <cstddef>\n#endif\n\n// note that when ICC or Clang is in use, __TBB_GCC_VERSION might not fully match\n// the actual GCC version on the system.\n#define __TBB_GCC_VERSION (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__)\n\n// Since GNU libstdc++ does not have a convenient macro for its version,\n// we rely on the version of GCC or the user-specified macro below.\n// The format of TBB_USE_GLIBCXX_VERSION should match the __TBB_GCC_VERSION above,\n// e.g. it should be set to 40902 for libstdc++ coming with GCC 4.9.2.\n#ifdef TBB_USE_GLIBCXX_VERSION\n#define __TBB_GLIBCXX_VERSION TBB_USE_GLIBCXX_VERSION\n#elif __GLIBCPP__ || __GLIBCXX__\n#define __TBB_GLIBCXX_VERSION __TBB_GCC_VERSION\n//TODO: analyze __GLIBCXX__ instead of __TBB_GCC_VERSION ?\n#endif\n\n#if __clang__\n    // according to clang documentation, version can be vendor specific\n    #define __TBB_CLANG_VERSION (__clang_major__ * 10000 + __clang_minor__ * 100 + __clang_patchlevel__)\n#endif\n\n/** Target OS is either iOS* or iOS* simulator **/\n#if __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__\n    #define __TBB_IOS 1\n#endif\n\n#if __APPLE__\n    #if __INTEL_COMPILER && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ > 1099 \\\n                         && __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 101000\n        // ICC does not correctly set the macro if -mmacosx-min-version is not specified\n        #define __TBB_MACOS_TARGET_VERSION  (100000 + 10*(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ - 1000))\n    #else\n        #define __TBB_MACOS_TARGET_VERSION  __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__\n    #endif\n#endif\n\n/** Preprocessor symbols to determine HW architecture **/\n\n#if _WIN32||_WIN64\n#   if defined(_M_X64)||defined(__x86_64__)  // the latter for MinGW support\n#       define __TBB_x86_64 1\n#   elif defined(_M_IA64)\n#       define __TBB_ipf 1\n#   elif defined(_M_IX86)||defined(__i386__) // the latter for MinGW support\n#       define __TBB_x86_32 1\n#   else\n#       define __TBB_generic_arch 1\n#   endif\n#else /* Assume generic Unix */\n#   if !__linux__ && !__APPLE__\n#       define __TBB_generic_os 1\n#   endif\n#   if __TBB_IOS\n#       define __TBB_generic_arch 1\n#   elif __x86_64__\n#       define __TBB_x86_64 1\n#   elif __ia64__\n#       define __TBB_ipf 1\n#   elif __i386__||__i386  // __i386 is for Sun OS\n#       define __TBB_x86_32 1\n#   else\n#       define __TBB_generic_arch 1\n#   endif\n#endif\n\n#if __MIC__ || __MIC2__\n#define __TBB_DEFINE_MIC 1\n#endif\n\n#define __TBB_TSX_AVAILABLE  ((__TBB_x86_32 || __TBB_x86_64) && !__TBB_DEFINE_MIC)\n\n/** Presence of compiler features **/\n\n#if __INTEL_COMPILER == 9999 && __INTEL_COMPILER_BUILD_DATE == 20110811\n/* Intel(R) Composer XE 2011 Update 6 incorrectly sets __INTEL_COMPILER. Fix it. */\n    #undef __INTEL_COMPILER\n    #define __INTEL_COMPILER 1210\n#endif\n\n#if __clang__ && !__INTEL_COMPILER\n#define __TBB_USE_OPTIONAL_RTTI __has_feature(cxx_rtti)\n#elif defined(_CPPRTTI)\n#define __TBB_USE_OPTIONAL_RTTI 1\n#else\n#define __TBB_USE_OPTIONAL_RTTI (__GXX_RTTI || __RTTI || __INTEL_RTTI__)\n#endif\n\n#if __TBB_GCC_VERSION >= 40400 && !defined(__INTEL_COMPILER)\n    /** warning suppression pragmas available in GCC since 4.4 **/\n    #define __TBB_GCC_WARNING_SUPPRESSION_PRESENT 1\n#endif\n\n/* Select particular features of C++11 based on compiler version.\n   ICC 12.1 (Linux*), GCC 4.3 and higher, clang 2.9 and higher\n   set __GXX_EXPERIMENTAL_CXX0X__ in c++11 mode.\n\n   Compilers that mimics other compilers (ICC, clang) must be processed before\n   compilers they mimic (GCC, MSVC).\n\n   TODO: The following conditions should be extended when new compilers/runtimes\n   support added.\n */\n\n/**\n    __TBB_CPP11_PRESENT macro indicates that the compiler supports vast majority of C++11 features.\n    Depending on the compiler, some features might still be unsupported or work incorrectly.\n    Use it when enabling C++11 features individually is not practical, and be aware that\n    some \"good enough\" compilers might be excluded. **/\n#define __TBB_CPP11_PRESENT (__cplusplus >= 201103L || _MSC_VER >= 1900)\n\n#define __TBB_CPP17_FALLTHROUGH_PRESENT (__cplusplus >= 201703L)\n#define __TBB_FALLTHROUGH_PRESENT       (__TBB_GCC_VERSION >= 70000 && !__INTEL_COMPILER)\n\n/** C++11 mode detection macros for Intel(R) C++ Compiler (enabled by -std=c++XY option):\n    __INTEL_CXX11_MODE__ for version >=13.0 (not available for ICC 15.0 if -std=c++14 is used),\n    __STDC_HOSTED__ for version >=12.0 (useful only on Windows),\n    __GXX_EXPERIMENTAL_CXX0X__ for version >=12.0 on Linux and macOS. **/\n#if __INTEL_COMPILER &&  !__INTEL_CXX11_MODE__\n    // __INTEL_CXX11_MODE__ is not set, try to deduce it\n    #define __INTEL_CXX11_MODE__ (__GXX_EXPERIMENTAL_CXX0X__ || (_MSC_VER && __STDC_HOSTED__))\n#endif\n\n// Intel(R) C++ Compiler offloading API to the Intel(R) Graphics Technology presence macro\n// TODO: add support for ICC 15.00 _GFX_enqueue API and then decrease Intel C++ Compiler supported version\n// TODO: add linux support and restict it with (__linux__ && __TBB_x86_64 && !__ANDROID__) macro\n#if __INTEL_COMPILER >= 1600 && _WIN32\n#define __TBB_GFX_PRESENT 1\n#endif\n\n#if __INTEL_COMPILER && (!_MSC_VER || __INTEL_CXX11_MODE__)\n    //  On Windows, C++11 features supported by Visual Studio 2010 and higher are enabled by default,\n    //  so in absence of /Qstd= use MSVC branch for feature detection.\n    //  On other platforms, no -std= means C++03.\n\n    #define __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT          (__INTEL_CXX11_MODE__ && __VARIADIC_TEMPLATES)\n    // Both r-value reference support in compiler and std::move/std::forward\n    // presence in C++ standard library is checked.\n    #define __TBB_CPP11_RVALUE_REF_PRESENT                  ((_MSC_VER >= 1700 || __GXX_EXPERIMENTAL_CXX0X__ && (__TBB_GLIBCXX_VERSION >= 40500 || _LIBCPP_VERSION)) && __INTEL_COMPILER >= 1400)\n    #define __TBB_IMPLICIT_MOVE_PRESENT                     (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1400 && (_MSC_VER >= 1900 || __TBB_GCC_VERSION >= 40600 || __clang__))\n    #if  _MSC_VER >= 1600\n        #define __TBB_EXCEPTION_PTR_PRESENT                 ( __INTEL_COMPILER > 1300                                                \\\n                                                            /*ICC 12.1 Upd 10 and 13 beta Upd 2 fixed exception_ptr linking  issue*/ \\\n                                                            || (__INTEL_COMPILER == 1300 && __INTEL_COMPILER_BUILD_DATE >= 20120530) \\\n                                                            || (__INTEL_COMPILER == 1210 && __INTEL_COMPILER_BUILD_DATE >= 20120410) )\n    /** libstdc++ that comes with GCC 4.6 use C++11 features not supported by ICC 12.1.\n     *  Because of that ICC 12.1 does not support C++11 mode with gcc 4.6 (or higher),\n     *  and therefore does not define __GXX_EXPERIMENTAL_CXX0X__ macro **/\n    #elif __TBB_GLIBCXX_VERSION >= 40404 && __TBB_GLIBCXX_VERSION < 40600\n        #define __TBB_EXCEPTION_PTR_PRESENT                 (__GXX_EXPERIMENTAL_CXX0X__ && __INTEL_COMPILER >= 1200)\n    #elif __TBB_GLIBCXX_VERSION >= 40600\n        #define __TBB_EXCEPTION_PTR_PRESENT                 (__GXX_EXPERIMENTAL_CXX0X__ && __INTEL_COMPILER >= 1300)\n    #elif _LIBCPP_VERSION\n        #define __TBB_EXCEPTION_PTR_PRESENT                 __GXX_EXPERIMENTAL_CXX0X__\n    #else\n        #define __TBB_EXCEPTION_PTR_PRESENT                 0\n    #endif\n    #define __TBB_STATIC_ASSERT_PRESENT                     (__INTEL_CXX11_MODE__ || _MSC_VER >= 1600)\n    #define __TBB_CPP11_TUPLE_PRESENT                       (_MSC_VER >= 1600 || __GXX_EXPERIMENTAL_CXX0X__ && (__TBB_GLIBCXX_VERSION >= 40300 || _LIBCPP_VERSION))\n    #if (__clang__ && __INTEL_COMPILER > 1400)\n        /* Older versions of Intel C++ Compiler do not have __has_include */\n        #if (__has_feature(__cxx_generalized_initializers__) && __has_include(<initializer_list>))\n            #define __TBB_INITIALIZER_LISTS_PRESENT         1\n        #endif\n    #else\n        #define __TBB_INITIALIZER_LISTS_PRESENT             (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1400 && (_MSC_VER >= 1800 || __TBB_GLIBCXX_VERSION >= 40400 || _LIBCPP_VERSION))\n    #endif\n    #define __TBB_CONSTEXPR_PRESENT                         (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1400)\n    #define __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT        (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1200)\n    /** ICC seems to disable support of noexcept event in c++11 when compiling in compatibility mode for gcc <4.6 **/\n    #define __TBB_NOEXCEPT_PRESENT                          (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1300 && (__TBB_GLIBCXX_VERSION >= 40600 || _LIBCPP_VERSION || _MSC_VER))\n    #define __TBB_CPP11_STD_BEGIN_END_PRESENT               (_MSC_VER >= 1700 || __GXX_EXPERIMENTAL_CXX0X__ && __INTEL_COMPILER >= 1310 && (__TBB_GLIBCXX_VERSION >= 40600 || _LIBCPP_VERSION))\n    #define __TBB_CPP11_AUTO_PRESENT                        (_MSC_VER >= 1600 || __GXX_EXPERIMENTAL_CXX0X__ && __INTEL_COMPILER >= 1210)\n    #define __TBB_CPP11_DECLTYPE_PRESENT                    (_MSC_VER >= 1600 || __GXX_EXPERIMENTAL_CXX0X__ && __INTEL_COMPILER >= 1210)\n    #define __TBB_CPP11_LAMBDAS_PRESENT                     (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1200)\n    #define __TBB_CPP11_DEFAULT_FUNC_TEMPLATE_ARGS_PRESENT  (_MSC_VER >= 1800 || __GXX_EXPERIMENTAL_CXX0X__ && __INTEL_COMPILER >= 1210)\n    #define __TBB_OVERRIDE_PRESENT                          (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1400)\n    #define __TBB_ALIGNAS_PRESENT                           (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1500)\n    #define __TBB_CPP11_TEMPLATE_ALIASES_PRESENT            (__INTEL_CXX11_MODE__ && __INTEL_COMPILER >= 1210)\n    #define __TBB_CPP14_INTEGER_SEQUENCE_PRESENT            (__cplusplus >= 201402L)\n    #define __TBB_CPP17_DEDUCTION_GUIDES_PRESENT            __INTEL_COMPILER > 1900\n#elif __clang__\n/** TODO: these options need to be rechecked **/\n    #define __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT          __has_feature(__cxx_variadic_templates__)\n    #define __TBB_CPP11_RVALUE_REF_PRESENT                  (__has_feature(__cxx_rvalue_references__) && (_LIBCPP_VERSION || __TBB_GLIBCXX_VERSION >= 40500))\n    #define __TBB_IMPLICIT_MOVE_PRESENT                     __has_feature(cxx_implicit_moves)\n/** TODO: extend exception_ptr related conditions to cover libstdc++ **/\n    #define __TBB_EXCEPTION_PTR_PRESENT                     (__cplusplus >= 201103L && (_LIBCPP_VERSION || __TBB_GLIBCXX_VERSION >= 40600))\n    #define __TBB_STATIC_ASSERT_PRESENT                     __has_feature(__cxx_static_assert__)\n    /**Clang (preprocessor) has problems with dealing with expression having __has_include in #ifs\n     * used inside C++ code. (At least version that comes with OS X 10.8 : Apple LLVM version 4.2 (clang-425.0.28) (based on LLVM 3.2svn)) **/\n    #if (__GXX_EXPERIMENTAL_CXX0X__ && __has_include(<tuple>))\n        #define __TBB_CPP11_TUPLE_PRESENT                   1\n    #endif\n    #if (__has_feature(__cxx_generalized_initializers__) && __has_include(<initializer_list>))\n        #define __TBB_INITIALIZER_LISTS_PRESENT             1\n    #endif\n    #define __TBB_CONSTEXPR_PRESENT                         __has_feature(__cxx_constexpr__)\n    #define __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT        (__has_feature(__cxx_defaulted_functions__) && __has_feature(__cxx_deleted_functions__))\n    /**For some unknown reason  __has_feature(__cxx_noexcept) does not yield true for all cases. Compiler bug ? **/\n    #define __TBB_NOEXCEPT_PRESENT                          (__cplusplus >= 201103L)\n    #define __TBB_CPP11_STD_BEGIN_END_PRESENT               (__has_feature(__cxx_range_for__) && (_LIBCPP_VERSION || __TBB_GLIBCXX_VERSION >= 40600))\n    #define __TBB_CPP11_AUTO_PRESENT                        __has_feature(__cxx_auto_type__)\n    #define __TBB_CPP11_DECLTYPE_PRESENT                    __has_feature(__cxx_decltype__)\n    #define __TBB_CPP11_LAMBDAS_PRESENT                     __has_feature(cxx_lambdas)\n    #define __TBB_CPP11_DEFAULT_FUNC_TEMPLATE_ARGS_PRESENT  __has_feature(cxx_default_function_template_args)\n    #define __TBB_OVERRIDE_PRESENT                          __has_feature(cxx_override_control)\n    #define __TBB_ALIGNAS_PRESENT                           __has_feature(cxx_alignas)\n    #define __TBB_CPP11_TEMPLATE_ALIASES_PRESENT            __has_feature(cxx_alias_templates)\n    #define __TBB_CPP14_INTEGER_SEQUENCE_PRESENT            (__cplusplus >= 201402L)\n    #define __TBB_CPP17_DEDUCTION_GUIDES_PRESENT            (__has_feature(__cpp_deduction_guides))\n#elif __GNUC__\n    #define __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT          __GXX_EXPERIMENTAL_CXX0X__\n    #define __TBB_CPP11_VARIADIC_FIXED_LENGTH_EXP_PRESENT   (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40700)\n    #define __TBB_CPP11_RVALUE_REF_PRESENT                  (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40500)\n    #define __TBB_IMPLICIT_MOVE_PRESENT                     (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40600)\n    /** __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 here is a substitution for _GLIBCXX_ATOMIC_BUILTINS_4, which is a prerequisite\n        for exception_ptr but cannot be used in this file because it is defined in a header, not by the compiler.\n        If the compiler has no atomic intrinsics, the C++ library should not expect those as well. **/\n    #define __TBB_EXCEPTION_PTR_PRESENT                     (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40404 && __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4)\n    #define __TBB_STATIC_ASSERT_PRESENT                     (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40300)\n    #define __TBB_CPP11_TUPLE_PRESENT                       (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40300)\n    #define __TBB_INITIALIZER_LISTS_PRESENT                 (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40400)\n    /** gcc seems have to support constexpr from 4.4 but tests in (test_atomic) seeming reasonable fail to compile prior 4.6**/\n    #define __TBB_CONSTEXPR_PRESENT                         (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40400)\n    #define __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT        (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40400)\n    #define __TBB_NOEXCEPT_PRESENT                          (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40600)\n    #define __TBB_CPP11_STD_BEGIN_END_PRESENT               (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40600)\n    #define __TBB_CPP11_AUTO_PRESENT                        (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40400)\n    #define __TBB_CPP11_DECLTYPE_PRESENT                    (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40400)\n    #define __TBB_CPP11_LAMBDAS_PRESENT                     (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40500)\n    #define __TBB_CPP11_DEFAULT_FUNC_TEMPLATE_ARGS_PRESENT  (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40300)\n    #define __TBB_OVERRIDE_PRESENT                          (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40700)\n    #define __TBB_ALIGNAS_PRESENT                           (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40800)\n    #define __TBB_CPP11_TEMPLATE_ALIASES_PRESENT            (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GCC_VERSION >= 40700)\n    #define __TBB_CPP14_INTEGER_SEQUENCE_PRESENT            (__cplusplus >= 201402L     && __TBB_GCC_VERSION >= 50000)\n    #define __TBB_CPP17_DEDUCTION_GUIDES_PRESENT            (__cpp_deduction_guides >= 201606)\n#elif _MSC_VER\n    // These definitions are also used with Intel C++ Compiler in \"default\" mode (__INTEL_CXX11_MODE__ == 0);\n    // see a comment in \"__INTEL_COMPILER\" section above.\n\n    #define __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT          (_MSC_VER >= 1800)\n    // Contains a workaround for ICC 13\n    #define __TBB_CPP11_RVALUE_REF_PRESENT                  (_MSC_VER >= 1700 && (!__INTEL_COMPILER || __INTEL_COMPILER >= 1400))\n    #define __TBB_IMPLICIT_MOVE_PRESENT                     (_MSC_VER >= 1900)\n    #define __TBB_EXCEPTION_PTR_PRESENT                     (_MSC_VER >= 1600)\n    #define __TBB_STATIC_ASSERT_PRESENT                     (_MSC_VER >= 1600)\n    #define __TBB_CPP11_TUPLE_PRESENT                       (_MSC_VER >= 1600)\n    #define __TBB_INITIALIZER_LISTS_PRESENT                 (_MSC_VER >= 1800)\n    #define __TBB_CONSTEXPR_PRESENT                         (_MSC_VER >= 1900)\n    #define __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT        (_MSC_VER >= 1800)\n    #define __TBB_NOEXCEPT_PRESENT                          (_MSC_VER >= 1900)\n    #define __TBB_CPP11_STD_BEGIN_END_PRESENT               (_MSC_VER >= 1700)\n    #define __TBB_CPP11_AUTO_PRESENT                        (_MSC_VER >= 1600)\n    #define __TBB_CPP11_DECLTYPE_PRESENT                    (_MSC_VER >= 1600)\n    #define __TBB_CPP11_LAMBDAS_PRESENT                     (_MSC_VER >= 1600)\n    #define __TBB_CPP11_DEFAULT_FUNC_TEMPLATE_ARGS_PRESENT  (_MSC_VER >= 1800)\n    #define __TBB_OVERRIDE_PRESENT                          (_MSC_VER >= 1700)\n    #define __TBB_ALIGNAS_PRESENT                           (_MSC_VER >= 1900)\n    #define __TBB_CPP11_TEMPLATE_ALIASES_PRESENT            (_MSC_VER >= 1800)\n    #define __TBB_CPP14_INTEGER_SEQUENCE_PRESENT            (_MSC_VER >= 1900)\n    #define __TBB_CPP17_DEDUCTION_GUIDES_PRESENT            (_MSVC_LANG >= 201703L)\n#else\n    #define __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT          0\n    #define __TBB_CPP11_RVALUE_REF_PRESENT                  0\n    #define __TBB_IMPLICIT_MOVE_PRESENT                     0\n    #define __TBB_EXCEPTION_PTR_PRESENT                     0\n    #define __TBB_STATIC_ASSERT_PRESENT                     0\n    #define __TBB_CPP11_TUPLE_PRESENT                       0\n    #define __TBB_INITIALIZER_LISTS_PRESENT                 0\n    #define __TBB_CONSTEXPR_PRESENT                         0\n    #define __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT        0\n    #define __TBB_NOEXCEPT_PRESENT                          0\n    #define __TBB_CPP11_STD_BEGIN_END_PRESENT               0\n    #define __TBB_CPP11_AUTO_PRESENT                        0\n    #define __TBB_CPP11_DECLTYPE_PRESENT                    0\n    #define __TBB_CPP11_LAMBDAS_PRESENT                     0\n    #define __TBB_CPP11_DEFAULT_FUNC_TEMPLATE_ARGS_PRESENT  0\n    #define __TBB_OVERRIDE_PRESENT                          0\n    #define __TBB_ALIGNAS_PRESENT                           0\n    #define __TBB_CPP11_TEMPLATE_ALIASES_PRESENT            0\n    #define __TBB_CPP14_INTEGER_SEQUENCE_PRESENT            (__cplusplus >= 201402L)\n    #define __TBB_CPP17_DEDUCTION_GUIDES_PRESENT            0\n#endif\n\n// C++11 standard library features\n\n#define __TBB_CPP11_ARRAY_PRESENT                           (_MSC_VER >= 1700 || _LIBCPP_VERSION || __GXX_EXPERIMENTAL_CXX0X__ && __TBB_GLIBCXX_VERSION >= 40300)\n\n#ifndef __TBB_CPP11_VARIADIC_FIXED_LENGTH_EXP_PRESENT\n#define __TBB_CPP11_VARIADIC_FIXED_LENGTH_EXP_PRESENT       __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT\n#endif\n#define __TBB_CPP11_VARIADIC_TUPLE_PRESENT                  (!_MSC_VER || _MSC_VER >= 1800)\n\n#define __TBB_CPP11_TYPE_PROPERTIES_PRESENT                 (_LIBCPP_VERSION || _MSC_VER >= 1700 || (__TBB_GLIBCXX_VERSION >= 50000 && __GXX_EXPERIMENTAL_CXX0X__))\n#define __TBB_TR1_TYPE_PROPERTIES_IN_STD_PRESENT            (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GLIBCXX_VERSION >= 40300 || _MSC_VER >= 1600)\n// GCC supported some of type properties since 4.7\n#define __TBB_CPP11_IS_COPY_CONSTRUCTIBLE_PRESENT           (__GXX_EXPERIMENTAL_CXX0X__ && __TBB_GLIBCXX_VERSION >= 40700 || __TBB_CPP11_TYPE_PROPERTIES_PRESENT)\n\n// In GCC, std::move_if_noexcept appeared later than noexcept\n#define __TBB_MOVE_IF_NOEXCEPT_PRESENT                      (__TBB_NOEXCEPT_PRESENT && (__TBB_GLIBCXX_VERSION >= 40700 || _MSC_VER >= 1900 || _LIBCPP_VERSION))\n#define __TBB_ALLOCATOR_TRAITS_PRESENT                      (__cplusplus >= 201103L && _LIBCPP_VERSION  || _MSC_VER >= 1700 ||  \\\n                                                            __GXX_EXPERIMENTAL_CXX0X__ && __TBB_GLIBCXX_VERSION >= 40700 && !(__TBB_GLIBCXX_VERSION == 40700 && __TBB_DEFINE_MIC))\n#define __TBB_MAKE_EXCEPTION_PTR_PRESENT                    (__TBB_EXCEPTION_PTR_PRESENT && (_MSC_VER >= 1700 || __TBB_GLIBCXX_VERSION >= 40600 || _LIBCPP_VERSION))\n\n// Due to libc++ limitations in C++03 mode, do not pass rvalues to std::make_shared()\n#define __TBB_CPP11_SMART_POINTERS_PRESENT                  ( _MSC_VER >= 1600 || _LIBCPP_VERSION   \\\n                                                            || ((__cplusplus >= 201103L || __GXX_EXPERIMENTAL_CXX0X__)  \\\n                                                            && (__TBB_GLIBCXX_VERSION >= 40500 || __TBB_GLIBCXX_VERSION >= 40400 && __TBB_USE_OPTIONAL_RTTI)) )\n\n#define __TBB_CPP11_FUTURE_PRESENT                          (_MSC_VER >= 1700 || __TBB_GLIBCXX_VERSION >= 40600 && __GXX_EXPERIMENTAL_CXX0X__ || _LIBCPP_VERSION)\n\n#define __TBB_CPP11_GET_NEW_HANDLER_PRESENT                 (_MSC_VER >= 1900 || __TBB_GLIBCXX_VERSION >= 40900 && __GXX_EXPERIMENTAL_CXX0X__ || _LIBCPP_VERSION)\n\n#define __TBB_CPP17_UNCAUGHT_EXCEPTIONS_PRESENT             (_MSC_VER >= 1900 || __GLIBCXX__ && __cpp_lib_uncaught_exceptions \\\n                                                            || _LIBCPP_VERSION >= 3700 && (!__TBB_MACOS_TARGET_VERSION || __TBB_MACOS_TARGET_VERSION >= 101200))\n\n// std::swap is in <utility> only since C++11, though MSVC had it at least since VS2005\n#if _MSC_VER>=1400 || _LIBCPP_VERSION || __GXX_EXPERIMENTAL_CXX0X__\n#define __TBB_STD_SWAP_HEADER <utility>\n#else\n#define __TBB_STD_SWAP_HEADER <algorithm>\n#endif\n\n//TODO: not clear how exactly this macro affects exception_ptr - investigate\n// On linux ICC fails to find existing std::exception_ptr in libstdc++ without this define\n#if __INTEL_COMPILER && __GNUC__ && __TBB_EXCEPTION_PTR_PRESENT && !defined(__GCC_HAVE_SYNC_COMPARE_AND_SWAP_4)\n    #define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 1\n#endif\n\n// Work around a bug in MinGW32\n#if __MINGW32__ && __TBB_EXCEPTION_PTR_PRESENT && !defined(_GLIBCXX_ATOMIC_BUILTINS_4)\n    #define _GLIBCXX_ATOMIC_BUILTINS_4\n#endif\n\n#if __GNUC__ || __SUNPRO_CC || __IBMCPP__\n    /* ICC defines __GNUC__ and so is covered */\n    #define __TBB_ATTRIBUTE_ALIGNED_PRESENT 1\n#elif _MSC_VER && (_MSC_VER >= 1300 || __INTEL_COMPILER)\n    #define __TBB_DECLSPEC_ALIGN_PRESENT 1\n#endif\n\n/* Actually ICC supports gcc __sync_* intrinsics starting 11.1,\n * but 64 bit support for 32 bit target comes in later ones*/\n/* TODO: change the version back to 4.1.2 once macro __TBB_WORD_SIZE become optional */\n/* Assumed that all clang versions have these gcc compatible intrinsics. */\n#if __TBB_GCC_VERSION >= 40306 || __INTEL_COMPILER >= 1200 || __clang__\n    /** built-in atomics available in GCC since 4.1.2 **/\n    #define __TBB_GCC_BUILTIN_ATOMICS_PRESENT 1\n#endif\n\n#if __TBB_GCC_VERSION >= 70000 && !__INTEL_COMPILER && !__clang__\n    // After GCC7 there was possible reordering problem in generic atomic load/store operations.\n    // So always using builtins.\n    #define TBB_USE_GCC_BUILTINS 1\n#endif\n\n#if __INTEL_COMPILER >= 1200\n    /** built-in C++11 style atomics available in ICC since 12.0 **/\n    #define __TBB_ICC_BUILTIN_ATOMICS_PRESENT 1\n#endif\n\n#if _MSC_VER>=1600 && (!__INTEL_COMPILER || __INTEL_COMPILER>=1310)\n    #define __TBB_MSVC_PART_WORD_INTERLOCKED_INTRINSICS_PRESENT 1\n#endif\n\n#define __TBB_TSX_INTRINSICS_PRESENT ((__RTM__ || _MSC_VER>=1700 || __INTEL_COMPILER>=1300) && !__TBB_DEFINE_MIC && !__ANDROID__)\n\n/** Macro helpers **/\n#define __TBB_CONCAT_AUX(A,B) A##B\n// The additional level of indirection is needed to expand macros A and B (not to get the AB macro).\n// See [cpp.subst] and [cpp.concat] for more details.\n#define __TBB_CONCAT(A,B) __TBB_CONCAT_AUX(A,B)\n// The IGNORED argument and comma are needed to always have 2 arguments (even when A is empty).\n#define __TBB_IS_MACRO_EMPTY(A,IGNORED) __TBB_CONCAT_AUX(__TBB_MACRO_EMPTY,A)\n#define __TBB_MACRO_EMPTY 1\n\n/** User controlled TBB features & modes **/\n#ifndef TBB_USE_DEBUG\n/*\nThere are four cases that are supported:\n  1. \"_DEBUG is undefined\" means \"no debug\";\n  2. \"_DEBUG defined to something that is evaluated to 0\" (including \"garbage\", as per [cpp.cond]) means \"no debug\";\n  3. \"_DEBUG defined to something that is evaluated to a non-zero value\" means \"debug\";\n  4. \"_DEBUG defined to nothing (empty)\" means \"debug\".\n*/\n#ifdef _DEBUG\n// Check if _DEBUG is empty.\n#define __TBB_IS__DEBUG_EMPTY (__TBB_IS_MACRO_EMPTY(_DEBUG,IGNORED)==__TBB_MACRO_EMPTY)\n#if __TBB_IS__DEBUG_EMPTY\n#define TBB_USE_DEBUG 1\n#else\n#define TBB_USE_DEBUG _DEBUG\n#endif /* __TBB_IS__DEBUG_EMPTY */\n#else\n#define TBB_USE_DEBUG 0\n#endif\n#endif /* TBB_USE_DEBUG */\n\n#ifndef TBB_USE_ASSERT\n#define TBB_USE_ASSERT TBB_USE_DEBUG\n#endif /* TBB_USE_ASSERT */\n\n#ifndef TBB_USE_THREADING_TOOLS\n#define TBB_USE_THREADING_TOOLS TBB_USE_DEBUG\n#endif /* TBB_USE_THREADING_TOOLS */\n\n#ifndef TBB_USE_PERFORMANCE_WARNINGS\n#ifdef TBB_PERFORMANCE_WARNINGS\n#define TBB_USE_PERFORMANCE_WARNINGS TBB_PERFORMANCE_WARNINGS\n#else\n#define TBB_USE_PERFORMANCE_WARNINGS TBB_USE_DEBUG\n#endif /* TBB_PERFORMANCE_WARNINGS */\n#endif /* TBB_USE_PERFORMANCE_WARNINGS */\n\n#if __TBB_DEFINE_MIC\n    #if TBB_USE_EXCEPTIONS\n        #error The platform does not properly support exception handling. Please do not set TBB_USE_EXCEPTIONS macro or set it to 0.\n    #elif !defined(TBB_USE_EXCEPTIONS)\n        #define TBB_USE_EXCEPTIONS 0\n    #endif\n#elif !(__EXCEPTIONS || defined(_CPPUNWIND) || __SUNPRO_CC)\n    #if TBB_USE_EXCEPTIONS\n        #error Compilation settings do not support exception handling. Please do not set TBB_USE_EXCEPTIONS macro or set it to 0.\n    #elif !defined(TBB_USE_EXCEPTIONS)\n        #define TBB_USE_EXCEPTIONS 0\n    #endif\n#elif !defined(TBB_USE_EXCEPTIONS)\n    #define TBB_USE_EXCEPTIONS 1\n#endif\n\n#ifndef TBB_IMPLEMENT_CPP0X\n/** By default, use C++11 classes if available **/\n    #if __clang__\n        /* Old versions of Intel C++ Compiler do not have __has_include or cannot use it in #define */\n        #if (__INTEL_COMPILER && (__INTEL_COMPILER < 1500 || __INTEL_COMPILER == 1500 && __INTEL_COMPILER_UPDATE <= 1))\n            #define TBB_IMPLEMENT_CPP0X (__cplusplus < 201103L || !_LIBCPP_VERSION)\n        #else\n            #define TBB_IMPLEMENT_CPP0X (__cplusplus < 201103L || (!__has_include(<thread>) && !__has_include(<condition_variable>)))\n        #endif\n    #elif __GNUC__\n        #define TBB_IMPLEMENT_CPP0X (__TBB_GCC_VERSION < 40400 || !__GXX_EXPERIMENTAL_CXX0X__)\n    #elif _MSC_VER\n        #define TBB_IMPLEMENT_CPP0X (_MSC_VER < 1700)\n    #else\n        // TODO: Reconsider general approach to be more reliable, e.g. (!(__cplusplus >= 201103L && __ STDC_HOSTED__))\n        #define TBB_IMPLEMENT_CPP0X (!__STDCPP_THREADS__)\n    #endif\n#endif /* TBB_IMPLEMENT_CPP0X */\n\n/* TBB_USE_CAPTURED_EXCEPTION should be explicitly set to either 0 or 1, as it is used as C++ const */\n#ifndef TBB_USE_CAPTURED_EXCEPTION\n    /** IA-64 architecture pre-built TBB binaries do not support exception_ptr. **/\n    #if __TBB_EXCEPTION_PTR_PRESENT && !defined(__ia64__)\n        #define TBB_USE_CAPTURED_EXCEPTION 0\n    #else\n        #define TBB_USE_CAPTURED_EXCEPTION 1\n    #endif\n#else /* defined TBB_USE_CAPTURED_EXCEPTION */\n    #if !TBB_USE_CAPTURED_EXCEPTION && !__TBB_EXCEPTION_PTR_PRESENT\n        #error Current runtime does not support std::exception_ptr. Set TBB_USE_CAPTURED_EXCEPTION and make sure that your code is ready to catch tbb::captured_exception.\n    #endif\n#endif /* defined TBB_USE_CAPTURED_EXCEPTION */\n\n/** Check whether the request to use GCC atomics can be satisfied **/\n#if TBB_USE_GCC_BUILTINS && !__TBB_GCC_BUILTIN_ATOMICS_PRESENT\n    #error \"GCC atomic built-ins are not supported.\"\n#endif\n\n/** Internal TBB features & modes **/\n\n/** __TBB_WEAK_SYMBOLS_PRESENT denotes that the system supports the weak symbol mechanism **/\n#ifndef __TBB_WEAK_SYMBOLS_PRESENT\n#define __TBB_WEAK_SYMBOLS_PRESENT ( !_WIN32 && !__APPLE__ && !__sun && (__TBB_GCC_VERSION >= 40000 || __INTEL_COMPILER ) )\n#endif\n\n/** __TBB_DYNAMIC_LOAD_ENABLED describes the system possibility to load shared libraries at run time **/\n#ifndef __TBB_DYNAMIC_LOAD_ENABLED\n    #define __TBB_DYNAMIC_LOAD_ENABLED 1\n#endif\n\n/** __TBB_SOURCE_DIRECTLY_INCLUDED is a mode used in whitebox testing when\n    it's necessary to test internal functions not exported from TBB DLLs\n**/\n#if (_WIN32||_WIN64) && (__TBB_SOURCE_DIRECTLY_INCLUDED || TBB_USE_PREVIEW_BINARY)\n    #define __TBB_NO_IMPLICIT_LINKAGE 1\n    #define __TBBMALLOC_NO_IMPLICIT_LINKAGE 1\n#endif\n\n#ifndef __TBB_COUNT_TASK_NODES\n    #define __TBB_COUNT_TASK_NODES TBB_USE_ASSERT\n#endif\n\n#ifndef __TBB_TASK_GROUP_CONTEXT\n    #define __TBB_TASK_GROUP_CONTEXT 1\n#endif /* __TBB_TASK_GROUP_CONTEXT */\n\n#ifndef __TBB_SCHEDULER_OBSERVER\n    #define __TBB_SCHEDULER_OBSERVER 1\n#endif /* __TBB_SCHEDULER_OBSERVER */\n\n#ifndef __TBB_FP_CONTEXT\n    #define __TBB_FP_CONTEXT __TBB_TASK_GROUP_CONTEXT\n#endif /* __TBB_FP_CONTEXT */\n\n#if __TBB_FP_CONTEXT && !__TBB_TASK_GROUP_CONTEXT\n    #error __TBB_FP_CONTEXT requires __TBB_TASK_GROUP_CONTEXT to be enabled\n#endif\n\n#define __TBB_RECYCLE_TO_ENQUEUE __TBB_BUILD // keep non-official\n\n#ifndef __TBB_ARENA_OBSERVER\n    #define __TBB_ARENA_OBSERVER ((__TBB_BUILD||TBB_PREVIEW_LOCAL_OBSERVER)&& __TBB_SCHEDULER_OBSERVER)\n#endif /* __TBB_ARENA_OBSERVER */\n\n#ifndef __TBB_SLEEP_PERMISSION\n    #define __TBB_SLEEP_PERMISSION ((__TBB_CPF_BUILD||TBB_PREVIEW_LOCAL_OBSERVER)&& __TBB_SCHEDULER_OBSERVER)\n#endif /* __TBB_SLEEP_PERMISSION */\n\n#ifndef __TBB_TASK_ISOLATION\n    #define __TBB_TASK_ISOLATION 1\n#endif /* __TBB_TASK_ISOLATION */\n\n#if TBB_USE_EXCEPTIONS && !__TBB_TASK_GROUP_CONTEXT\n    #error TBB_USE_EXCEPTIONS requires __TBB_TASK_GROUP_CONTEXT to be enabled\n#endif\n\n#ifndef __TBB_TASK_PRIORITY\n    #define __TBB_TASK_PRIORITY (__TBB_TASK_GROUP_CONTEXT)\n#endif /* __TBB_TASK_PRIORITY */\n\n#if __TBB_TASK_PRIORITY && !__TBB_TASK_GROUP_CONTEXT\n    #error __TBB_TASK_PRIORITY requires __TBB_TASK_GROUP_CONTEXT to be enabled\n#endif\n\n#if TBB_PREVIEW_WAITING_FOR_WORKERS || __TBB_BUILD\n    #define __TBB_SUPPORTS_WORKERS_WAITING_IN_TERMINATE 1\n#endif\n\n#ifndef __TBB_ENQUEUE_ENFORCED_CONCURRENCY\n    #define __TBB_ENQUEUE_ENFORCED_CONCURRENCY 1\n#endif\n\n#if !defined(__TBB_SURVIVE_THREAD_SWITCH) && \\\n          (_WIN32 || _WIN64 || __APPLE__ || (__linux__ && !__ANDROID__))\n    #define __TBB_SURVIVE_THREAD_SWITCH 1\n#endif /* __TBB_SURVIVE_THREAD_SWITCH */\n\n#ifndef __TBB_DEFAULT_PARTITIONER\n#define __TBB_DEFAULT_PARTITIONER tbb::auto_partitioner\n#endif\n\n#ifndef __TBB_USE_PROPORTIONAL_SPLIT_IN_BLOCKED_RANGES\n#define __TBB_USE_PROPORTIONAL_SPLIT_IN_BLOCKED_RANGES 1\n#endif\n\n#ifndef __TBB_ENABLE_RANGE_FEEDBACK\n#define __TBB_ENABLE_RANGE_FEEDBACK 0\n#endif\n\n#ifdef _VARIADIC_MAX\n    #define __TBB_VARIADIC_MAX _VARIADIC_MAX\n#else\n    #if _MSC_VER == 1700\n        #define __TBB_VARIADIC_MAX 5 // VS11 setting, issue resolved in VS12\n    #elif _MSC_VER == 1600\n        #define __TBB_VARIADIC_MAX 10 // VS10 setting\n    #else\n        #define __TBB_VARIADIC_MAX 15\n    #endif\n#endif\n\n/** __TBB_WIN8UI_SUPPORT enables support of Windows* Store Apps and limit a possibility to load\n    shared libraries at run time only from application container **/\n#if defined(WINAPI_FAMILY) && WINAPI_FAMILY == WINAPI_FAMILY_APP\n    #define __TBB_WIN8UI_SUPPORT 1\n#else\n    #define __TBB_WIN8UI_SUPPORT 0\n#endif\n\n/** Macros of the form __TBB_XXX_BROKEN denote known issues that are caused by\n    the bugs in compilers, standard or OS specific libraries. They should be\n    removed as soon as the corresponding bugs are fixed or the buggy OS/compiler\n    versions go out of the support list.\n**/\n\n#if __SIZEOF_POINTER__ < 8 && __ANDROID__ && __TBB_GCC_VERSION <= 40403 && !__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8\n    /** Necessary because on Android 8-byte CAS and F&A are not available for some processor architectures,\n        but no mandatory warning message appears from GCC 4.4.3. Instead, only a linkage error occurs when\n        these atomic operations are used (such as in unit test test_atomic.exe). **/\n    #define __TBB_GCC_64BIT_ATOMIC_BUILTINS_BROKEN 1\n#elif __TBB_x86_32 && __TBB_GCC_VERSION == 40102 && ! __GNUC_RH_RELEASE__\n    /** GCC 4.1.2 erroneously emit call to external function for 64 bit sync_ intrinsics.\n        However these functions are not defined anywhere. It seems that this problem was fixed later on\n        and RHEL got an updated version of gcc 4.1.2. **/\n    #define __TBB_GCC_64BIT_ATOMIC_BUILTINS_BROKEN 1\n#endif\n\n#if __GNUC__ && __TBB_x86_64 && __INTEL_COMPILER == 1200\n    #define __TBB_ICC_12_0_INL_ASM_FSTCW_BROKEN 1\n#endif\n\n#if _MSC_VER && __INTEL_COMPILER && (__INTEL_COMPILER<1110 || __INTEL_COMPILER==1110 && __INTEL_COMPILER_BUILD_DATE < 20091012)\n    /** Necessary to avoid ICL error (or warning in non-strict mode):\n        \"exception specification for implicitly declared virtual destructor is\n        incompatible with that of overridden one\". **/\n    #define __TBB_DEFAULT_DTOR_THROW_SPEC_BROKEN 1\n#endif\n\n#if !__INTEL_COMPILER && (_MSC_VER && _MSC_VER < 1500 || __GNUC__ && __TBB_GCC_VERSION < 40102)\n    /** gcc 3.4.6 (and earlier) and VS2005 (and earlier) do not allow declaring template class as a friend\n        of classes defined in other namespaces. **/\n    #define __TBB_TEMPLATE_FRIENDS_BROKEN 1\n#endif\n\n#if __GLIBC__==2 && __GLIBC_MINOR__==3 ||  (__APPLE__ && ( __INTEL_COMPILER==1200 && !TBB_USE_DEBUG))\n    /** Macro controlling EH usages in TBB tests.\n        Some older versions of glibc crash when exception handling happens concurrently. **/\n    #define __TBB_THROW_ACROSS_MODULE_BOUNDARY_BROKEN 1\n#endif\n\n#if (_WIN32||_WIN64) && __INTEL_COMPILER == 1110\n    /** That's a bug in Intel C++ Compiler 11.1.044/IA-32 architecture/Windows* OS, that leads to a worker thread crash on the thread's startup. **/\n    #define __TBB_ICL_11_1_CODE_GEN_BROKEN 1\n#endif\n\n#if __clang__ || (__GNUC__==3 && __GNUC_MINOR__==3 && !defined(__INTEL_COMPILER))\n    /** Bugs with access to nested classes declared in protected area */\n    #define __TBB_PROTECTED_NESTED_CLASS_BROKEN 1\n#endif\n\n#if __MINGW32__ && __TBB_GCC_VERSION < 40200\n    /** MinGW has a bug with stack alignment for routines invoked from MS RTLs.\n        Since GCC 4.2, the bug can be worked around via a special attribute. **/\n    #define __TBB_SSE_STACK_ALIGNMENT_BROKEN 1\n#endif\n\n#if __TBB_GCC_VERSION==40300 && !__INTEL_COMPILER && !__clang__\n    /* GCC of this version may rashly ignore control dependencies */\n    #define __TBB_GCC_OPTIMIZER_ORDERING_BROKEN 1\n#endif\n\n#if __FreeBSD__\n    /** A bug in FreeBSD 8.0 results in kernel panic when there is contention\n        on a mutex created with this attribute. **/\n    #define __TBB_PRIO_INHERIT_BROKEN 1\n\n    /** A bug in FreeBSD 8.0 results in test hanging when an exception occurs\n        during (concurrent?) object construction by means of placement new operator. **/\n    #define __TBB_PLACEMENT_NEW_EXCEPTION_SAFETY_BROKEN 1\n#endif /* __FreeBSD__ */\n\n#if (__linux__ || __APPLE__) && __i386__ && defined(__INTEL_COMPILER)\n    /** The Intel C++ Compiler for IA-32 architecture (Linux* OS|macOS) crashes or generates\n        incorrect code when __asm__ arguments have a cast to volatile. **/\n    #define __TBB_ICC_ASM_VOLATILE_BROKEN 1\n#endif\n\n#if !__INTEL_COMPILER && (_MSC_VER && _MSC_VER < 1700 || __GNUC__==3 && __GNUC_MINOR__<=2)\n    /** Bug in GCC 3.2 and MSVC compilers that sometimes return 0 for __alignof(T)\n        when T has not yet been instantiated. **/\n    #define __TBB_ALIGNOF_NOT_INSTANTIATED_TYPES_BROKEN 1\n#endif\n\n#if __TBB_DEFINE_MIC\n    /** Main thread and user's thread have different default thread affinity masks. **/\n    #define __TBB_MAIN_THREAD_AFFINITY_BROKEN 1\n#endif\n\n#if __GXX_EXPERIMENTAL_CXX0X__ && !defined(__EXCEPTIONS) && \\\n    ((!__INTEL_COMPILER && !__clang__ && (__TBB_GCC_VERSION>=40400 && __TBB_GCC_VERSION<40600)) || \\\n     (__INTEL_COMPILER<=1400 && (__TBB_GLIBCXX_VERSION>=40400 && __TBB_GLIBCXX_VERSION<=40801)))\n/* There is an issue for specific GCC toolchain when C++11 is enabled\n   and exceptions are disabled:\n   exceprion_ptr.h/nested_exception.h use throw unconditionally.\n   GCC can ignore 'throw' since 4.6; but with ICC the issue still exists.\n */\n    #define __TBB_LIBSTDCPP_EXCEPTION_HEADERS_BROKEN 1\n#endif\n\n#if __INTEL_COMPILER==1300 && __TBB_GLIBCXX_VERSION>=40700 && defined(__GXX_EXPERIMENTAL_CXX0X__)\n/* Some C++11 features used inside libstdc++ are not supported by Intel C++ Compiler. */\n    #define __TBB_ICC_13_0_CPP11_STDLIB_SUPPORT_BROKEN 1\n#endif\n\n#if (__GNUC__==4 && __GNUC_MINOR__==4 ) && !defined(__INTEL_COMPILER) && !defined(__clang__)\n    /** excessive warnings related to strict aliasing rules in GCC 4.4 **/\n    #define __TBB_GCC_STRICT_ALIASING_BROKEN 1\n    /* topical remedy: #pragma GCC diagnostic ignored \"-Wstrict-aliasing\" */\n    #if !__TBB_GCC_WARNING_SUPPRESSION_PRESENT\n        #error Warning suppression is not supported, while should.\n    #endif\n#endif\n\n/* In a PIC mode some versions of GCC 4.1.2 generate incorrect inlined code for 8 byte __sync_val_compare_and_swap intrinsic */\n#if __TBB_GCC_VERSION == 40102 && __PIC__ && !defined(__INTEL_COMPILER) && !defined(__clang__)\n    #define __TBB_GCC_CAS8_BUILTIN_INLINING_BROKEN 1\n#endif\n\n#if __TBB_x86_32 && ( __INTEL_COMPILER || (__GNUC__==5 && __GNUC_MINOR__>=2 && __GXX_EXPERIMENTAL_CXX0X__) \\\n    || (__GNUC__==3 && __GNUC_MINOR__==3) || (__MINGW32__ && __GNUC__==4 && __GNUC_MINOR__==5) || __SUNPRO_CC )\n    // Some compilers for IA-32 architecture fail to provide 8-byte alignment of objects on the stack,\n    // even if the object specifies 8-byte alignment. On such platforms, the implementation\n    // of 64 bit atomics for IA-32 architecture (e.g. atomic<long long>) use different tactics\n    // depending upon whether the object is properly aligned or not.\n    #define __TBB_FORCE_64BIT_ALIGNMENT_BROKEN 1\n#else\n    // Define to 0 explicitly because the macro is used in a compiled code of test_atomic\n    #define __TBB_FORCE_64BIT_ALIGNMENT_BROKEN 0\n#endif\n\n#if __GNUC__ && !__INTEL_COMPILER && !__clang__ && __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT && __TBB_GCC_VERSION < 40700\n    #define __TBB_ZERO_INIT_WITH_DEFAULTED_CTOR_BROKEN 1\n#endif\n\n#if _MSC_VER && _MSC_VER <= 1800 && !__INTEL_COMPILER\n    // With MSVC, when an array is passed by const reference to a template function,\n    // constness from the function parameter may get propagated to the template parameter.\n    #define __TBB_CONST_REF_TO_ARRAY_TEMPLATE_PARAM_BROKEN 1\n#endif\n\n// A compiler bug: a disabled copy constructor prevents use of the moving constructor\n#define __TBB_IF_NO_COPY_CTOR_MOVE_SEMANTICS_BROKEN (_MSC_VER && (__INTEL_COMPILER >= 1300 && __INTEL_COMPILER <= 1310) && !__INTEL_CXX11_MODE__)\n\n#define __TBB_CPP11_DECLVAL_BROKEN (_MSC_VER == 1600 || (__GNUC__ && __TBB_GCC_VERSION < 40500) )\n// Intel C++ Compiler has difficulties with copying std::pair with VC11 std::reference_wrapper being a const member\n#define __TBB_COPY_FROM_NON_CONST_REF_BROKEN (_MSC_VER == 1700 && __INTEL_COMPILER && __INTEL_COMPILER < 1600)\n\n// The implicit upcasting of the tuple of a reference of a derived class to a base class fails on icc 13.X if the system's gcc environment is 4.8\n// Also in gcc 4.4 standard library the implementation of the tuple<&> conversion (tuple<A&> a = tuple<B&>, B is inherited from A) is broken.\n#if __GXX_EXPERIMENTAL_CXX0X__ && __GLIBCXX__ && ((__INTEL_COMPILER >=1300 && __INTEL_COMPILER <=1310 && __TBB_GLIBCXX_VERSION>=40700) || (__TBB_GLIBCXX_VERSION < 40500))\n#define __TBB_UPCAST_OF_TUPLE_OF_REF_BROKEN 1\n#endif\n\n// In some cases decltype of a function adds a reference to a return type.\n#define __TBB_CPP11_DECLTYPE_OF_FUNCTION_RETURN_TYPE_BROKEN (_MSC_VER == 1600 && !__INTEL_COMPILER)\n\n/** End of __TBB_XXX_BROKEN macro section **/\n\n#if defined(_MSC_VER) && _MSC_VER>=1500 && !defined(__INTEL_COMPILER)\n    // A macro to suppress erroneous or benign \"unreachable code\" MSVC warning (4702)\n    #define __TBB_MSVC_UNREACHABLE_CODE_IGNORED 1\n#endif\n\n#define __TBB_ATOMIC_CTORS     (__TBB_CONSTEXPR_PRESENT && __TBB_DEFAULTED_AND_DELETED_FUNC_PRESENT && (!__TBB_ZERO_INIT_WITH_DEFAULTED_CTOR_BROKEN))\n\n// Many OS versions (Android 4.0.[0-3] for example) need workaround for dlopen to avoid non-recursive loader lock hang\n// Setting the workaround for all compile targets ($APP_PLATFORM) below Android 4.4 (android-19)\n#if __ANDROID__\n#include <android/api-level.h>\n#define __TBB_USE_DLOPEN_REENTRANCY_WORKAROUND  (__ANDROID_API__ < 19)\n#endif\n\n#define __TBB_ALLOCATOR_CONSTRUCT_VARIADIC      (__TBB_CPP11_VARIADIC_TEMPLATES_PRESENT && __TBB_CPP11_RVALUE_REF_PRESENT)\n\n#define __TBB_VARIADIC_PARALLEL_INVOKE          (TBB_PREVIEW_VARIADIC_PARALLEL_INVOKE && __TBB_CPP11_VARIADIC_TEMPLATES_PRESENT && __TBB_CPP11_RVALUE_REF_PRESENT)\n#define __TBB_FLOW_GRAPH_CPP11_FEATURES         (__TBB_CPP11_VARIADIC_TEMPLATES_PRESENT \\\n                                                && __TBB_CPP11_SMART_POINTERS_PRESENT && __TBB_CPP11_RVALUE_REF_PRESENT && __TBB_CPP11_AUTO_PRESENT) \\\n                                                && __TBB_CPP11_VARIADIC_TUPLE_PRESENT && __TBB_CPP11_DEFAULT_FUNC_TEMPLATE_ARGS_PRESENT \\\n                                                && !__TBB_UPCAST_OF_TUPLE_OF_REF_BROKEN\n#define __TBB_PREVIEW_STREAMING_NODE            (__TBB_CPP11_VARIADIC_FIXED_LENGTH_EXP_PRESENT && __TBB_FLOW_GRAPH_CPP11_FEATURES \\\n                                                && TBB_PREVIEW_FLOW_GRAPH_NODES && !TBB_IMPLEMENT_CPP0X && !__TBB_UPCAST_OF_TUPLE_OF_REF_BROKEN)\n#define __TBB_PREVIEW_OPENCL_NODE               (__TBB_PREVIEW_STREAMING_NODE && __TBB_CPP11_TEMPLATE_ALIASES_PRESENT)\n#define __TBB_PREVIEW_MESSAGE_BASED_KEY_MATCHING (TBB_PREVIEW_FLOW_GRAPH_FEATURES || __TBB_PREVIEW_OPENCL_NODE)\n#define __TBB_PREVIEW_ASYNC_MSG                 (TBB_PREVIEW_FLOW_GRAPH_FEATURES && __TBB_FLOW_GRAPH_CPP11_FEATURES)\n\n#define __TBB_PREVIEW_GFX_FACTORY               (__TBB_GFX_PRESENT && TBB_PREVIEW_FLOW_GRAPH_FEATURES && !__TBB_MIC_OFFLOAD \\\n                                                && __TBB_FLOW_GRAPH_CPP11_FEATURES && __TBB_CPP11_TEMPLATE_ALIASES_PRESENT \\\n                                                && __TBB_CPP11_FUTURE_PRESENT)\n\n#endif /* __TBB_tbb_config_H */\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/test_opencl_kernel_64.spir",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/test_opencl_kernel_32.spir",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/test_opencl_precompiled_kernel_gpu_64.ir",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/src/test/test_opencl_precompiled_kernel_gpu_32.ir",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/common/gui/xcode/tbbExample/en.lproj/MainMenu.nib",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/game_of_life/msvs/app.ico",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/polygon_overlay/speedup.gif",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/tachyon/msvs/small.ico",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/tachyon/msvs/gui.ico",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/Logo.png",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/SplashScreen.png",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/SmallLogo.png",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/StoreLogo.png",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/seismic/msvs/small.ico",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/parallel_for/seismic/msvs/SeismicSimulation.ico",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/task_arena/fractal/msvs/small.ico",
        "/tmp/vanessa/spack-stage/spack-stage-intel-tbb-2019.2-bstfc4b5z5yqc5wadb2v54i45uaeocie/spack-src/examples/task_arena/fractal/msvs/gui.ico"
    ],
    "total_files": 940
}