{
    "matches": {
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/test/test_model_plugin.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#define HARNESS_DEFAULT_MIN_THREADS 4\n#define HARNESS_DEFAULT_MAX_THREADS 4\n\n// Need to include \"tbb/tbb_config.h\" to obtain the definition of __TBB_DEFINE_MIC.\n#include \"tbb/tbb_config.h\"\n\n#if !__TBB_TODO || __TBB_WIN8UI_SUPPORT\n#define HARNESS_NO_PARSE_COMMAND_LINE 1\n#include \"harness.h\"\nint TestMain() {\n    return Harness::Skipped;\n}\n#else /* __TBB_TODO */\n// TODO: There are a lot of problems with unloading DLL which uses TBB with automatic initialization\n\n#if __TBB_DEFINE_MIC\n\n#ifndef _USRDLL\n#define HARNESS_NO_PARSE_COMMAND_LINE 1\n#include \"harness.h\"\nint TestMain() {\n    return Harness::Skipped;\n}\n#endif\n\n#else /* !__MIC__ */\n\n#if _WIN32 || _WIN64\n#include \"tbb/machine/windows_api.h\"\n#else\n#include <dlfcn.h>\n#endif\n\n#include <stdlib.h>\n#include <stdio.h>\n\n#if !TBB_USE_EXCEPTIONS && _MSC_VER\n    // Suppress \"C++ exception handler used, but unwind semantics are not enabled\" warning in STL headers\n    #pragma warning (push)\n    #pragma warning (disable: 4530)\n#endif\n\n#include <stdexcept>\n\n#if !TBB_USE_EXCEPTIONS && _MSC_VER\n    #pragma warning (pop)\n#endif\n\n#if TBB_USE_EXCEPTIONS\n    #include \"harness_report.h\"\n#endif\n\n#ifdef _USRDLL\n#include \"tbb/task_scheduler_init.h\"\n\nclass CModel {\npublic:\n    CModel(void) {};\n    static tbb::task_scheduler_init tbb_init;\n\n    void init_and_terminate( int );\n};\n\ntbb::task_scheduler_init CModel::tbb_init(1);\n\n//! Test that task::initialize and task::terminate work when doing nothing else.\n/** maxthread is treated as the \"maximum\" number of worker threads. */\nvoid CModel::init_and_terminate( int maxthread ) {\n    for( int i=0; i<200; ++i ) {\n        switch( i&3 ) {\n            default: {\n                tbb::task_scheduler_init init( rand() % maxthread + 1 );\n                break;\n            }\n            case 0: {\n                tbb::task_scheduler_init init;\n                break;\n            }\n            case 1: {\n                tbb::task_scheduler_init init( tbb::task_scheduler_init::automatic );\n                break;\n            }\n            case 2: {\n                tbb::task_scheduler_init init( tbb::task_scheduler_init::deferred );\n                init.initialize( rand() % maxthread + 1 );\n                init.terminate();\n                break;\n            }\n        }\n    }\n}\n\nextern \"C\"\n#if _WIN32 || _WIN64\n__declspec(dllexport)\n#endif\nvoid plugin_call(int maxthread)\n{\n    srand(2);\n    __TBB_TRY {\n        CModel model;\n        model.init_and_terminate(maxthread);\n    } __TBB_CATCH( std::runtime_error& error ) {\n#if TBB_USE_EXCEPTIONS\n        REPORT(\"ERROR: %s\\n\", error.what());\n#endif /* TBB_USE_EXCEPTIONS */\n    }\n}\n\n#else /* _USRDLL undefined */\n\n#define HARNESS_NO_ASSERT 1\n#include \"harness.h\"\n#include \"harness_dynamic_libs.h\"\n\nextern \"C\" void plugin_call(int);\n\nvoid report_error_in(const char* function_name)\n{\n#if _WIN32 || _WIN64\n    char* message;\n    int code = GetLastError();\n\n    FormatMessage(\n        FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM,\n        NULL, code,MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT),\n        (char*)&message, 0, NULL );\n#else\n    char* message = (char*)dlerror();\n    int code = 0;\n#endif\n    REPORT( \"%s failed with error %d: %s\\n\", function_name, code, message);\n\n#if _WIN32 || _WIN64\n    LocalFree(message);\n#endif\n}\n\nint use_lot_of_tls() {\n    int count = 0;\n#if _WIN32 || _WIN64\n    DWORD last_handles[10];\n    DWORD result;\n    result = TlsAlloc();\n    while( result!=TLS_OUT_OF_INDEXES ) {\n        last_handles[++count%10] = result;\n        result = TlsAlloc();\n    }\n    for( int i=0; i<10; ++i )\n        TlsFree(last_handles[i]);\n#else\n    pthread_key_t last_handles[10];\n    pthread_key_t result;\n    int setspecific_dummy=10;\n    while( pthread_key_create(&result, NULL)==0 \n           && count < 4096 ) // Sun Solaris doesn't have any built-in limit, so we set something big enough\n    {\n        last_handles[++count%10] = result;\n        pthread_setspecific(result,&setspecific_dummy);\n    }\n    REMARK(\"Created %d keys\\n\", count);\n    for( int i=0; i<10; ++i )\n        pthread_key_delete(last_handles[i]);\n#endif\n    return count-10;\n}\n\ntypedef void (*PLUGIN_CALL)(int);\n\n#if __linux__\n    #define RML_LIBRARY_NAME(base) TEST_LIBRARY_NAME(base) \".1\"\n#else\n    #define RML_LIBRARY_NAME(base) TEST_LIBRARY_NAME(base)\n#endif\n\nint TestMain () {\n#if !RML_USE_WCRM\n    PLUGIN_CALL my_plugin_call;\n\n    int tls_key_count = use_lot_of_tls();\n    REMARK(\"%d thread local objects allocated in advance\\n\", tls_key_count);\n\n    Harness::LIBRARY_HANDLE hLib;\n#if _WIN32 || _WIN64\n    hLib = LoadLibrary(\"irml.dll\");\n    if ( !hLib )\n        hLib = LoadLibrary(\"irml_debug.dll\");\n    if ( !hLib )\n        return Harness::Skipped; // No shared RML, skip the test\n    FreeLibrary(hLib);\n#else /* !WIN */\n#if __TBB_ARENA_PER_MASTER\n    hLib = dlopen(RML_LIBRARY_NAME(\"libirml\"), RTLD_LAZY);\n    if ( !hLib )\n        hLib = dlopen(RML_LIBRARY_NAME(\"libirml_debug\"), RTLD_LAZY);\n    if ( !hLib )\n        return Harness::Skipped;\n    dlclose(hLib);\n#endif /* __TBB_ARENA_PER_MASTER */\n#endif /* OS */\n    for( int i=1; i<100; ++i ) {  \n        REMARK(\"Iteration %d, loading plugin library...\\n\", i);\n        hLib = Harness::OpenLibrary(TEST_LIBRARY_NAME(\"test_model_plugin_dll\"));\n        if ( !hLib ) {\n#if !__TBB_NO_IMPLICIT_LINKAGE\n#if _WIN32 || _WIN64\n            report_error_in(\"LoadLibrary\");\n#else\n            report_error_in(\"dlopen\");\n#endif\n            return -1;\n#else\n            return Harness::Skipped;\n#endif\n        }\n        my_plugin_call = (PLUGIN_CALL)Harness::GetAddress(hLib, \"plugin_call\");\n        if (my_plugin_call==NULL) {\n#if _WIN32 || _WIN64\n            report_error_in(\"GetProcAddress\");\n#else\n            report_error_in(\"dlsym\");\n#endif\n            return -1;\n        }\n        REMARK(\"Calling plugin method...\\n\");\n        my_plugin_call(MaxThread);\n\n        REMARK(\"Unloading plugin library...\\n\");\n        Harness::CloseLibrary(hLib);\n    } // end for(1,100)\n\n    return Harness::Done;\n#else\n    return Harness::Skipped;\n#endif /* !RML_USE_WCRM */\n}\n\n#endif//_USRDLL\n#endif//__MIC__\n\n#endif /*__TBB_WIN8UI_SUPPORT*/\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/test/test_cilk_dynamic_load.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#include \"tbb/tbb_config.h\"\n\n// Skip the test if no interoperability with cilkrts\n#define __TBB_CILK_INTEROP   (__TBB_SURVIVE_THREAD_SWITCH && __INTEL_COMPILER>=1200)\n// Skip the test when cilkrts did not have dlopen()/dlclose() start up feature\n#define CILK_SYMBOLS_VISIBLE (_WIN32||_WIN64)\n// The compiler does not add \"-lcilkrts\" linker option on some linux systems\n#define CILK_LINKAGE_BROKEN  (__linux__ && __GNUC__<4 && __INTEL_COMPILER_BUILD_DATE <= 20110427)\n// Currently, the interop doesn't support the situation:\n//1) Intel(R) Threading Building Blocks (Intel(R) TBB) is outermost;\n//2)   Intel(R) Cilk(TM) Plus, and it should be dynamically loaded with dlopen/LoadLibrary (possibly via a 3rd party module);\n//3)     Intel(R) TBB again;\n//4)       Intel(R) Cilk(TM) Plus again.\n#define HEAVY_NESTED_INTEROP_SUPPORT ( __INTEL_COMPILER_BUILD_DATE < 20110427 )\n\n#if __TBB_CILK_INTEROP && CILK_SYMBOLS_VISIBLE && !CILK_LINKAGE_BROKEN && HEAVY_NESTED_INTEROP_SUPPORT\n\n#include \"tbb/task_scheduler_init.h\"\n#include \"tbb/task.h\"\n\nstatic const int N = 25;\nstatic const int P_outer = 4;\nstatic const int P_nested = 2;\n\n#ifdef _USRDLL\n\n#include <cilk/cilk.h>\n#define HARNESS_CUSTOM_MAIN 1\n#include \"harness.h\"\n#undef HARNESS_CUSTOM_MAIN\n\n#if _WIN32 || _WIN64\n#define CILK_TEST_EXPORT extern \"C\" __declspec(dllexport)\n#else\n#define CILK_TEST_EXPORT extern \"C\"\n#endif /* _WIN32 || _WIN64 */\n\nbool g_sandwich = true; // have to be declare before #include \"test_cilk_common.h\"\n#include \"test_cilk_common.h\"\n\nCILK_TEST_EXPORT int CilkFib( int n )\n{\n    return TBB_Fib(n);\n}\n\nCILK_TEST_EXPORT void CilkShutdown()\n{\n    __cilkrts_end_cilk();\n}\n\n#else /* _USRDLL undefined */\n\n#include \"harness.h\"\n#include \"harness_dynamic_libs.h\"\n\nint SerialFib( int n ) {\n    int a=0, b=1;\n    for( int i=0; i<n; ++i ) {\n        b += a;\n        a = b-a;\n    }\n    return a;\n}\n\nint F = SerialFib(N);\n\ntypedef int (*CILK_CALL)(int);\nCILK_CALL CilkFib = 0;\n\ntypedef void (*CILK_SHUTDOWN)();\nCILK_SHUTDOWN CilkShutdown = 0;\n\nclass FibTask: public tbb::task {\n    int n;\n    int& result;\n    /*override*/ task* execute() {\n        if( n<2 ) {\n            result = n;\n        } else {\n\n            // TODO: why RTLD_LAZY was used here?\n            Harness::LIBRARY_HANDLE hLib =\n                Harness::OpenLibrary(TEST_LIBRARY_NAME(\"test_cilk_dynamic_load_dll\"));\n            CilkFib = (CILK_CALL)Harness::GetAddress(hLib, \"CilkFib\");\n            CilkShutdown = (CILK_SHUTDOWN)Harness::GetAddress(hLib, \"CilkShutdown\");\n\n            int x, y;\n            x = CilkFib(n-2);\n            y = CilkFib(n-1);\n            result = x+y;\n\n            CilkShutdown();\n\n            Harness::CloseLibrary(hLib);\n        }\n        return NULL;\n    }\npublic:\n    FibTask( int& result_, int n_ ) : result(result_), n(n_) {}\n};\n\n\nint TBB_Fib( int n ) {\n    if( n<2 ) {\n        return n;\n    } else {\n        int result;\n        tbb::task_scheduler_init init(P_nested);\n        tbb::task::spawn_root_and_wait(*new( tbb::task::allocate_root()) FibTask(result,n) );\n        return result;\n    }\n}\n\nvoid RunSandwich() { \n    tbb::task_scheduler_init init(P_outer);\n    int m = TBB_Fib(N);\n    ASSERT( m == F, NULL );\n}\n\nint TestMain () {\n    for ( int i = 0; i < 20; ++i )\n        RunSandwich();\n    return Harness::Done;\n}\n\n#endif /* _USRDLL */\n\n#else /* !__TBB_CILK_INTEROP */\n\n#include \"harness.h\"\n\nint TestMain () {\n    return Harness::Skipped;\n}\n\n#endif /* !__TBB_CILK_INTEROP */\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/test/harness_dynamic_libs.h": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#include \"tbb/tbb_config.h\"\n\n// Include this header file before harness.h for HARNESS_SKIP_TEST to take effect\n#if !__TBB_DYNAMIC_LOAD_ENABLED\n#define HARNESS_SKIP_TEST 1\n#else\n\n#if _WIN32 || _WIN64\n#include \"tbb/machine/windows_api.h\"\n#else\n#include <dlfcn.h>\n#endif\n#include \"harness_assert.h\"\n\nnamespace Harness {\n\n#if TBB_USE_DEBUG\n#define SUFFIX1 \"_debug\"\n#define SUFFIX2\n#else\n#define SUFFIX1\n#define SUFFIX2 \"_debug\"\n#endif /* TBB_USE_DEBUG */\n\n#if _WIN32||_WIN64\n#define PREFIX\n#define EXT \".dll\"\n#else\n#define PREFIX \"lib\"\n#if __APPLE__\n#define EXT \".dylib\"\n// Android SDK build system does not support .so file name versioning\n#elif __FreeBSD__ || __NetBSD__ || __sun || _AIX || __ANDROID__\n#define EXT \".so\"\n#elif __linux__  // Order of these elif's matters!\n#define EXT __TBB_STRING(.so.TBB_COMPATIBLE_INTERFACE_VERSION)\n#else\n#error Unknown OS\n#endif\n#endif\n\n// Form the names of the TBB memory allocator binaries.\n#define MALLOCLIB_NAME1 PREFIX \"tbbmalloc\" SUFFIX1 EXT\n#define MALLOCLIB_NAME2 PREFIX \"tbbmalloc\" SUFFIX2 EXT\n\n#if _WIN32 || _WIN64\ntypedef  HMODULE LIBRARY_HANDLE;\n#else\ntypedef void *LIBRARY_HANDLE;\n#endif\n\n#if _WIN32 || _WIN64\n#define TEST_LIBRARY_NAME(base) base\".dll\"\n#elif __APPLE__\n#define TEST_LIBRARY_NAME(base) base\".dylib\"\n#else\n#define TEST_LIBRARY_NAME(base) base\".so\"\n#endif\n\nLIBRARY_HANDLE OpenLibrary(const char *name)\n{\n#if _WIN32 || _WIN64\n#if __TBB_WIN8UI_SUPPORT\t\n    TCHAR wlibrary[MAX_PATH];\n    if ( MultiByteToWideChar(CP_UTF8, 0, name, -1, wlibrary, MAX_PATH) == 0 ) return false;\n    return :: LoadPackagedLibrary( wlibrary, 0 );\n#else\n    return ::LoadLibrary(name);\n#endif\n#else\n    return dlopen(name, RTLD_NOW|RTLD_GLOBAL);\n#endif\n}\n\nvoid CloseLibrary(LIBRARY_HANDLE lib)\n{\n#if _WIN32 || _WIN64\n    BOOL ret = FreeLibrary(lib);\n    ASSERT(ret, \"FreeLibrary must be successful\");\n#else\n    int ret = dlclose(lib);\n    ASSERT(ret == 0, \"dlclose must be successful\");\n#endif\n}\n\ntypedef void (*FunctionAddress)();\n\nFunctionAddress GetAddress(Harness::LIBRARY_HANDLE lib, const char *name)\n{\n    union { FunctionAddress func; void *symb; } converter;\n#if _WIN32 || _WIN64\n    converter.symb = (void*)GetProcAddress(lib, name);\n#else\n    converter.symb = (void*)dlsym(lib, name);\n#endif\n    ASSERT(converter.func, \"Can't find required symbol in dynamic library\");\n    return converter.func;\n}\n\n}  // namespace Harness\n\n#endif // __TBB_DYNAMIC_LOAD_ENABLED\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/tbbproxy/tbbproxy.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#include \"tbb/tbb_config.h\"\n#if !__TBB_WIN8UI_SUPPORT\n#define TBB_PREVIEW_RUNTIME_LOADER 1\n#include \"tbb/runtime_loader.h\"\n#include \"tbb/tbb_stddef.h\"\n\n// C standard headers.\n#include <cctype>            // isspace\n#include <cstdarg>           // va_list, etc.\n#include <cstdio>            // fprintf, stderr, etc.\n#include <cstdlib>           // malloc, free, abort.\n#include <cstring>           // strlen, etc.\n\n// C++ standard headers.\n#include <typeinfo>\n\n// OS-specific includes.\n#if _WIN32 || _WIN64\n    #include <windows.h>\n    #define snprintf _snprintf\n    #undef max\n#else\n    #include <dlfcn.h>    // dlopen, dlsym, dlclose, dlerror.\n#endif\n\n#if TBB_USE_ASSERT\n    // We cannot use __TBB_ASSERT as it is because it calls a function from tbb library which may\n    // be not yet loaded. Redefine __TBB_ASSERT not to call tbb functions.\n    #undef __TBB_ASSERT\n    #define __TBB_ASSERT( cond, msg ) {                                                            \\\n        if ( ! (cond) ) {                                                                          \\\n            say( \"%s:%d: Assertion failed: %s.\", __FILE__, __LINE__, (msg) );                      \\\n        } /* if */                                                                                 \\\n        /* TODO: abort? */                                                                         \\\n    }\n#endif\n\n// Declare here, define at the bottom.\nextern \"C\" int __tbb_internal_runtime_loader_stub();\n\nnamespace tbb {\n\nnamespace interface6 {\n\nnamespace internal {\n\nnamespace runtime_loader {\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    User interaction utilities.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n// Print message to stderr. Do not call it directly, use say() or tell() instead.\nstatic void _say( char const * format, va_list args ) {\n    /*\n        On Linux Intel 64, vsnprintf() modifies args argument, so vsnprintf() crashes if it\n        is called for the second time with the same args. To prevent the crash, we have to\n        pass a fresh intact copy of args to vsnprintf() each time.\n\n        On Windows, unfortunately, standard va_copy() macro is not available. However, it\n        seems vsnprintf() does not modify args argument.\n    */\n    #if ! ( _WIN32 || _WIN64 )\n        va_list _args;\n        __va_copy( _args, args );  // Make copy of args.\n        #define args _args         // Substitute args with its copy, _args.\n    #endif\n    int len = vsnprintf( NULL, 0, format, args );\n    #if ! ( _WIN32 || _WIN64 )\n        #undef args                // Remove substitution.\n        va_end( _args );\n    #endif\n    char * buf = reinterpret_cast< char * >( malloc( len + 1 ) );\n    if ( buf == NULL ) {\n        abort();\n    } // if\n    vsnprintf( buf, len + 1, format, args );\n    fprintf( stderr, \"TBB: %s\\n\", buf );\n    free( buf );\n} // _say\n\n\n// Debug/test/troubleshooting printing controlled by TBB_VERSION environment variable.\n// To enable printing, the variable must be set and not empty.\n// Do not call it directly, use tell() instead.\nstatic void _tell( char const * format, va_list args ) {\n    char const * var = getenv( \"TBB_VERSION\" );\n    if ( var != NULL && var[ 0 ] != 0 ) {\n        _say( format, args );\n    } // if\n} // _tell\n\n\n// Print message to stderr unconditionally.\nstatic void say( char const * format, ... ) {\n    va_list args;\n    va_start( args, format );\n    _say( format, args );\n    va_end( args );\n} // say\n\n\n// Debug/test/troubleshooting printing controlled by TBB_VERSION environment variable.\n// To enable printing, the variable must be set and not empty.\nstatic void tell( char const * format, ... ) {\n    va_list args;\n    va_start( args, format );\n    _tell( format, args );\n    va_end( args );\n} // tell\n\n\n// Error reporting utility. Behavior depends on mode.\nstatic tbb::runtime_loader::error_code error( tbb::runtime_loader::error_mode mode, tbb::runtime_loader::error_code err, char const * format, ... ) {\n    va_list args;\n    va_start( args, format );\n    if ( mode == tbb::runtime_loader::em_abort ) {\n        // In em_abort mode error message printed unconditionally.\n        _say( format, args );\n    } else {\n        // In other modes printing depends on TBB_VERSION environment variable.\n        _tell( format, args );\n    } // if\n    va_end( args );\n    switch ( mode ) {\n        case tbb::runtime_loader::em_abort : {\n            say( \"Aborting...\" );\n            #if TBB_USE_DEBUG && ( _WIN32 || _WIN64 )\n                DebugBreak();\n            #endif\n            abort();\n        } break;\n        case tbb::runtime_loader::em_throw : {\n            throw err;\n        } break;\n        case tbb::runtime_loader::em_status : {\n            // Do nothing.\n        } break;\n    } // switch\n    return err;\n} // error\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    General-purpose string manipulation utilities.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n// Delete character ch from string str in-place.\nstatic void strip( char * str, char ch ) {\n    int in  = 0;  // Input character index.\n    int out = 0;  // Output character index.\n    for ( ; ; ) {\n        if ( str[ in ] != ch ) {\n            str[ out ] = str[ in ];\n            ++ out;\n        } // if\n        if ( str[ in ] == 0 ) {\n            break;\n        } // if\n        ++ in;\n    } // forever\n} // func strip\n\n\n// Strip trailing whitespaces in-place.\nstatic void trim( char * str ) {\n    size_t len = strlen( str );\n    while ( len > 0 && isspace( str[ len - 1 ] ) ) {\n        -- len;\n    } // while\n    str[ len ] = 0;\n} // func trim\n\n\n#if _WIN32 || _WIN64\n    // \"When specifying a path, be sure to use backslashes (\\), not forward slashes (/).\"\n    // (see http://msdn.microsoft.com/en-us/library/ms886736.aspx).\n    const char proper_slash = '\\\\';\n    inline char char_or_slash( char c ) { return c=='/'? '\\\\': c; }\n#else\n    const char proper_slash = '/';\n    inline char char_or_slash( char c ) { return c; }\n#endif\n\n// Concatenate name of directory and name of file.\nvoid cat_file( char const * dir, char const * file, char * buffer, size_t len ) {\n    size_t i = 0;\n    // Copy directory name\n    for( ; i<len && *dir; ++i, ++dir ) {\n        buffer[i] = char_or_slash(*dir);\n    }\n    // Append trailing slash if missed.\n    if( i>0 && i<len && buffer[i-1]!=proper_slash ) {\n        buffer[i++] = proper_slash;\n    }\n    // Copy file name\n    __TBB_ASSERT( char_or_slash(*file)!=proper_slash, \"File name starts with a slash\" );\n    for( ; i<len && *file; ++i, ++file ) {\n        buffer[i] = *file;\n    }\n    // Append null terminator\n    buffer[ i<len? i: len-1 ] = '\\0';\n} // cat_file\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    Windows implementation of dlopen, dlclose, dlsym, dlerror.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n#if _WIN32 || _WIN64\n\n    // Implement Unix-like interface (dlopen, dlclose, dlsym, dlerror) via Win32 API functions.\n\n    // Type of dlopen result.\n    typedef HMODULE handle_t;\n\n    enum rtld_flags_t {\n        RTLD_NOW,\n        RTLD_GLOBAL\n    }; // enum rtld_flags_t\n\n    // Unix-like dlopen().\n    static handle_t dlopen( char const * name, rtld_flags_t ) {\n        return LoadLibrary( name );\n    } // dlopen\n\n    // Unix-like dlsym().\n    static void * dlsym( handle_t lib, char const * sym ) {\n        return (void*)GetProcAddress( lib, sym );\n    } // dlsym\n\n    // Unix-like dlclose().\n    static int dlclose( handle_t lib ) {\n        return ! FreeLibrary( lib );\n    } // dlclose\n\n    // The function mimics Unix dlerror() function.\n    // Note: Not thread-safe due to statically allocated buffer.\n    static char * dlerror() {\n\n        static char buffer[ 2048 ];  // Note: statically allocated buffer.\n\n        DWORD err = GetLastError();\n        if ( err == ERROR_SUCCESS ) {\n            return NULL;\n        } // if\n\n        DWORD rc;\n        rc =\n            FormatMessage(\n                FORMAT_MESSAGE_FROM_SYSTEM,\n                NULL,\n                err,\n                MAKELANGID( LANG_NEUTRAL, SUBLANG_DEFAULT ), // Default language.\n                reinterpret_cast< LPTSTR >( & buffer ),\n                sizeof( buffer ),\n                NULL\n            );\n        if ( rc == 0 ) {\n            // FormatMessage() failed to format system error message. Buffer to short or another issue.\n            snprintf( buffer, sizeof( buffer ), \"System error %u.\", err );\n        } else {\n            /*\n                FormatMessage() returns Windows-style end-of-lines, \"\\r\\n\". When string is printed,\n                printf() also replaces all the occurrences of \"\\n\" with \"\\r\\n\" (again!), so sequences\n                like \"\\r\\r\\r\\n\" appear in output. It is not too good. Stripping all \"\\r\" normalizes\n                string and returns it to canonical form, so printf() will produce correct end-of-line\n                sequences.\n            */\n            strip( buffer, '\\r' );   // Delete carriage returns if any.\n            trim( buffer );          // Delete trailing newlines and spaces.\n        } // if\n\n        return buffer;\n\n    } // dlerror\n\n#else\n\n    // Type of dlopen() result.\n    typedef void * handle_t;\n\n#endif\n\n\n/*\n    ------------------------------------------------------------------------------------------------\n    Runtime loader stuff.\n    ------------------------------------------------------------------------------------------------\n*/\n\n\n// Descriptor table declaration. It is defined in assembler file.\nenum symbol_type_t {\n    st_object   = 0,\n    st_function = 1\n}; // enum symbol_type_t\nstruct symbol_t {\n    void *        addr;\n    char const *  name;\n    int           size;\n    symbol_type_t type;\n}; // symbol_t\nextern \"C\" symbol_t __tbb_internal_runtime_loader_symbols[];\n\n// Hooks for internal use (e. g. for testing).\ntbb::runtime_loader::error_mode stub_mode = tbb::runtime_loader::em_abort;\n\nstatic char const * tbb_dll_name = __TBB_STRING(__TBB_DLL_NAME);  // Name of TBB library.\nstatic handle_t     handle       = NULL;                          // Handle of loaded TBB library or NULL.\nstatic int          version      = 0;                             // Version of the loaded library.\nstatic int          counter      = 0;                             // Number of runtime_loader objects using the loaded library.\n\n#define ANOTHER_RTL \"probably multiple runtime_loader objects work in parallel\"\n\n\n// One attempt to load library (dll_name can be a full path or just a file name).\nstatic tbb::runtime_loader::error_code _load( char const * dll_name, int min_ver, int max_ver ) {\n\n    tbb::runtime_loader::error_mode mode = tbb::runtime_loader::em_status;\n    tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;\n\n    /*\n        If these variables declared at the first usage, Intel compiler (on Windows IA-32) isues\n        warning(s):\n            transfer of control [goto error] bypasses initialization of: ...\n        Declaring variables at the beginning of the function eliminates warnings.\n    */\n    typedef int (*int_func_t)( void );\n    char const * get_ver_name = \"TBB_runtime_interface_version\"; // Name of function.\n    int_func_t   get_ver_func = NULL;                            // Pointer to function.\n    handle_t     _handle      = NULL;\n    int          _version     = 0;\n    int          total        = 0;\n    int          not_found    = 0;\n\n    // This function should be called iff there is no loaded library.\n    __TBB_ASSERT( handle  == NULL, \"Handle is invalid; \"  ANOTHER_RTL );\n    __TBB_ASSERT( version == 0,    \"Version is invalid; \" ANOTHER_RTL );\n    __TBB_ASSERT( counter == 0,    \"Counter is invalid; \" ANOTHER_RTL );\n\n    tell( \"Loading \\\"%s\\\"...\", dll_name );\n\n    // First load the library.\n    _handle = dlopen( dll_name, RTLD_NOW );\n    if ( _handle == NULL ) {\n        const char * msg = dlerror();\n        code = error( mode, tbb::runtime_loader::ec_no_lib, \"Loading \\\"%s\\\" failed; system error: %s\", dll_name, msg );\n        goto error;\n    } // if\n\n    // Then try to find out its version.\n    /*\n        g++ 3.4 issues error:\n            ISO C++ forbids casting between pointer-to-function and pointer-to-object\n        on reinterpret_cast<>. Thus, we have no choice but using C-style type cast.\n    */\n    get_ver_func = (int_func_t) dlsym( _handle, get_ver_name );\n    if ( get_ver_func == NULL ) {\n        code = error( mode, tbb::runtime_loader::ec_bad_lib, \"Symbol \\\"%s\\\" not found; library rejected.\", get_ver_name );\n        goto error;\n    } // if\n    _version = get_ver_func();\n    if ( ! ( min_ver <= _version && _version <= max_ver ) ) {\n        code = error( mode, tbb::runtime_loader::ec_bad_ver, \"Version %d is out of requested range; library rejected.\", _version );\n        goto error;\n    } // if\n\n    // Library is suitable. Mark it as loaded.\n    handle   = _handle;\n    version  = _version;\n    counter += 1;\n    __TBB_ASSERT( counter == 1, \"Counter is invalid; \" ANOTHER_RTL );\n\n    // Now search for all known symbols.\n    for ( int i = 0; __tbb_internal_runtime_loader_symbols[ i ].name != NULL; ++ i ) {\n        symbol_t & symbol = __tbb_internal_runtime_loader_symbols[ i ];\n        // Verify symbol descriptor.\n        __TBB_ASSERT( symbol.type == st_object || symbol.type == st_function, \"Invalid symbol type\" );\n        #if _WIN32 || _WIN64\n            __TBB_ASSERT( symbol.type == st_function, \"Should not be symbols of object type on Windows\" );\n        #endif\n        if ( symbol.type == st_object ) {\n            __TBB_ASSERT( symbol.addr != NULL, \"Object address invalid\" );\n            __TBB_ASSERT( symbol.size > 0, \"Symbol size must be > 0\" );\n            __TBB_ASSERT( symbol.size <= 0x1000, \"Symbol size too big\" );\n        } else {                     // Function\n            // __TBB_ASSERT( symbol.addr == reinterpret_cast< void * >( & stub ), \"Invalid symbol address\" );\n            __TBB_ASSERT( symbol.size == sizeof( void * ), \"Invalid symbol size\" );\n        } // if\n        void * addr = dlsym( _handle, symbol.name );\n        if ( addr != NULL ) {\n            if ( symbol.type == st_object ) {\n                if ( strncmp( symbol.name, \"_ZTS\", 4 ) == 0 ) {\n                    // If object name begins with \"_ZTS\", it is a string, mangled type name.\n                    // Its value must equal to name of symbol without \"_ZTS\" prefix.\n                    char const * name = static_cast< char const * >( addr );\n                    __TBB_ASSERT( strlen( name ) + 1 == size_t( symbol.size ), \"Unexpected size of typeinfo name\" );\n                    __TBB_ASSERT( strcmp( symbol.name + 4, name ) == 0, \"Unexpected content of typeinfo name\" );\n                    strncpy( reinterpret_cast< char * >( symbol.addr ), name, symbol.size );\n                    reinterpret_cast< char * >( symbol.addr )[ symbol.size - 1 ] = 0;\n                } else {\n                    #if TBB_USE_ASSERT\n                        // If object name begins with \"_ZTI\", it is an object of std::type_info class.\n                        // Its protected value must equal to name of symbol without \"_ZTI\" prefix.\n                        if ( strncmp( symbol.name, \"_ZTI\", 4 ) == 0 ) {\n                            std::type_info const * info = static_cast< std::type_info const * >( addr );\n                            __TBB_ASSERT( size_t( symbol.size ) >= sizeof( std::type_info ), \"typeinfo size is too small\" );\n                            // std::type_info::name is not a virtual method, it is safe to call it.\n                            __TBB_ASSERT( strcmp( symbol.name + 4, info->name() ) == 0, \"Unexpected content of typeinfo\" );\n                        } // if\n                    #endif\n                    // Copy object content from libtbb into runtime_loader.\n                    memcpy( symbol.addr, addr, symbol.size );\n                }; // if\n            } else {                     // Function\n                symbol.addr = addr;\n            } // if\n        } else {\n            char const * msg = dlerror();\n            tell( \"Symbol \\\"%s\\\" not found; system error: %s\", symbol.name, msg );\n            ++ not_found;\n        } // if\n        ++ total;\n    } // for i\n\n    if ( not_found > 0 ) {\n        tell( \"%d of %d symbols not found.\", not_found, total );\n    } // if\n\n    tell( \"The library successfully loaded.\" );\n    return code;\n\n    error:\n        if ( _handle != NULL ) {\n            int rc = dlclose( _handle );\n            if ( rc != 0 ) {\n                // Error occurred.\n                __TBB_ASSERT( rc != 0, \"Unexpected error: dlclose() failed\" );\n            } // if\n        } // if\n        _handle = NULL;\n        return code;\n\n} // _load\n\n\nstatic tbb::runtime_loader::error_code load( tbb::runtime_loader::error_mode mode, char const * path[], int min_ver, int max_ver ) {\n    // Check arguments first.\n    if ( min_ver <= 0 ) {\n        return error( mode, tbb::runtime_loader::ec_bad_arg, \"tbb::runtime_loader::load(): Invalid value of min_ver argument: %d.\", min_ver );\n    } // if\n    if ( max_ver <= 0 ) {\n        return error( mode, tbb::runtime_loader::ec_bad_arg, \"tbb::runtime_loader::load(): Invalid value of max_ver argument: %d.\", max_ver );\n    } // if\n    if ( min_ver > max_ver ) {\n        return error( mode, tbb::runtime_loader::ec_bad_arg, \"tbb::runtime_loader::load(): min_ver and max_ver specify empty range: [%d, %d].\", min_ver, max_ver );\n    } // if\n    if ( min_ver == max_ver ) {\n        tell( \"Searching for \\\"%s\\\" version %d...\", tbb_dll_name, min_ver );\n    } else if ( max_ver == INT_MAX ) {\n        tell( \"Searching for \\\"%s\\\" version %d+...\", tbb_dll_name, min_ver );\n    } else {\n        tell( \"Searching for \\\"%s\\\" version in range [%d, %d]...\", tbb_dll_name, min_ver, max_ver );\n    } // if\n    // Then check whether a library already loaded.\n    if ( handle != NULL ) {\n        // Library already loaded. Check whether the version is compatible.\n        __TBB_ASSERT( version > 0, \"Version is invalid; \" ANOTHER_RTL );\n        __TBB_ASSERT( counter > 0, \"Counter is invalid; \" ANOTHER_RTL );\n        if ( min_ver <= version && version <= max_ver ) {\n            // Version is ok, let us use this library.\n            tell( \"Library version %d is already loaded.\", version );\n            counter += 1;\n            return tbb::runtime_loader::ec_ok;\n        } else {\n            // Version is not suitable.\n            return error( mode, tbb::runtime_loader::ec_bad_ver, \"Library version %d is already loaded.\", version );\n        } // if\n    } // if\n    // There is no loaded library, try to load it using provided directories.\n    __TBB_ASSERT( version == 0, \"Version is invalid; \" ANOTHER_RTL );\n    __TBB_ASSERT( counter == 0, \"Counter is invalid; \" ANOTHER_RTL );\n    size_t namelen = strlen(tbb_dll_name);\n    size_t buflen = 0;\n    char * buffer = NULL;\n    for ( int i = 0; path[i] != NULL; ++ i ) {\n        size_t len = strlen(path[i]) + namelen + 2; // 1 for slash and 1 for null terminator\n        if( buflen<len ) {\n            free( buffer );\n            buflen = len;\n            buffer = (char*)malloc( buflen );\n        }\n        cat_file( path[i], tbb_dll_name, buffer, buflen );\n        __TBB_ASSERT(strstr(buffer,tbb_dll_name), \"Name concatenation error\");\n        tbb::runtime_loader::error_code ec = _load( buffer, min_ver, max_ver );\n        if ( ec == tbb::runtime_loader::ec_ok ) {\n            return ec;       // Success. Exiting...\n        } // if\n    } // for i\n    free( buffer );\n    return error( mode, tbb::runtime_loader::ec_no_lib, \"No suitable library found.\" );\n} // load\n\n\n\n\n// Suppress \"defined but not used\" compiler warnings.\nstatic void const * dummy[] = {\n    (void *) & strip,\n    (void *) & trim,\n    & dummy,\n    NULL\n};\n\n\n} // namespace runtime_loader\n\n} // namespace internal\n\n\nruntime_loader::runtime_loader( error_mode mode ) :\n    my_mode( mode ),\n    my_status( ec_ok ),\n    my_loaded( false )\n{\n} // ctor\n\n\nruntime_loader::runtime_loader( char const * path[], int min_ver, int max_ver, error_mode mode ) :\n    my_mode( mode ),\n    my_status( ec_ok ),\n    my_loaded( false )\n{\n    load( path, min_ver, max_ver );\n} // ctor\n\n\nruntime_loader::~runtime_loader() {\n} // dtor\n\n\ntbb::runtime_loader::error_code runtime_loader::load( char const * path[], int min_ver, int max_ver ) {\n    if ( my_loaded ) {\n        my_status = tbb::interface6::internal::runtime_loader::error( my_mode, ec_bad_call, \"tbb::runtime_loader::load(): Library already loaded by this runtime_loader object.\" );\n    } else {\n        my_status = internal::runtime_loader::load( my_mode, path, min_ver, max_ver );\n        if ( my_status == ec_ok ) {\n            my_loaded = true;\n        } // if\n    } // if\n    return my_status;\n} // load\n\n\n\n\ntbb::runtime_loader::error_code runtime_loader::status() {\n    return my_status;\n} // status\n\n\n} // namespace interface6\n\n} // namespace tbb\n\n\n// Stub function replaces all TBB entry points when no library is loaded.\nint __tbb_internal_runtime_loader_stub() {\n    char const * msg = NULL;\n    if ( tbb::interface6::internal::runtime_loader::handle == NULL ) {\n        msg = \"A function is called while TBB library is not loaded\";\n    } else {\n        msg = \"A function is called which is not present in loaded TBB library\";\n    } // if\n    return tbb::interface6::internal::runtime_loader::error( tbb::interface6::internal::runtime_loader::stub_mode, tbb::runtime_loader::ec_no_lib, msg );\n} // stub\n\n#endif // !__TBB_WIN8UI_SUPPORT //\n// end of file //\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/tbbmalloc/tbbmalloc.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#include \"TypeDefinitions.h\" // Customize.h and proxy.h get included\n#include \"tbbmalloc_internal_api.h\"\n\n#include \"../tbb/itt_notify.h\" // for __TBB_load_ittnotify()\n\n#include \"../tbb/tbb_assert_impl.h\" // Out-of-line TBB assertion handling routines are instantiated here.\n\n#undef UNICODE\n\n#if USE_PTHREAD\n#include <dlfcn.h> // dlopen\n#elif USE_WINTHREAD\n#include \"tbb/machine/windows_api.h\"\n#endif\n\nnamespace rml {\nnamespace internal {\n\n/** Caller is responsible for ensuring this routine is called exactly once. */\nextern \"C\" void MallocInitializeITT() {\n#if DO_ITT_NOTIFY\n    tbb::internal::__TBB_load_ittnotify();\n#endif\n}\n\n#if TBB_USE_DEBUG\n#define DEBUG_SUFFIX \"_debug\"\n#else\n#define DEBUG_SUFFIX\n#endif /* TBB_USE_DEBUG */\n\n// MALLOCLIB_NAME is the name of the TBB memory allocator library.\n#if _WIN32||_WIN64\n#define MALLOCLIB_NAME \"tbbmalloc\" DEBUG_SUFFIX \".dll\"\n#elif __APPLE__\n#define MALLOCLIB_NAME \"libtbbmalloc\" DEBUG_SUFFIX \".dylib\"\n#elif __FreeBSD__ || __NetBSD__ || __sun || _AIX || __ANDROID__\n#define MALLOCLIB_NAME \"libtbbmalloc\" DEBUG_SUFFIX \".so\"\n#elif __linux__\n#define MALLOCLIB_NAME \"libtbbmalloc\" DEBUG_SUFFIX  __TBB_STRING(.so.TBB_COMPATIBLE_INTERFACE_VERSION)\n#else\n#error Unknown OS\n#endif\n\nvoid init_tbbmalloc() {\n#if DO_ITT_NOTIFY\n    MallocInitializeITT();\n#endif\n\n/* Preventing TBB allocator library from unloading to prevent\n   resource leak, as memory is not released on the library unload.\n*/\n#if USE_WINTHREAD && !__TBB_SOURCE_DIRECTLY_INCLUDED && !__TBB_WIN8UI_SUPPORT\n    // Prevent Windows from displaying message boxes if it fails to load library\n    UINT prev_mode = SetErrorMode (SEM_FAILCRITICALERRORS);\n    HMODULE lib;\n    BOOL ret = GetModuleHandleEx(GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS\n                                 |GET_MODULE_HANDLE_EX_FLAG_PIN,\n                                 (LPCTSTR)&scalable_malloc, &lib);\n    MALLOC_ASSERT(lib && ret, \"Allocator can't find itself.\");\n    SetErrorMode (prev_mode);\n#endif /* USE_PTHREAD && !__TBB_SOURCE_DIRECTLY_INCLUDED */\n}\n\n#if !__TBB_SOURCE_DIRECTLY_INCLUDED\n#if USE_WINTHREAD\nextern \"C\" BOOL WINAPI DllMain( HINSTANCE /*hInst*/, DWORD callReason, LPVOID )\n{\n\n    if (callReason==DLL_THREAD_DETACH)\n    {\n        __TBB_mallocThreadShutdownNotification();\n    }\n    else if (callReason==DLL_PROCESS_DETACH)\n    {\n        __TBB_mallocProcessShutdownNotification();\n    }\n    return TRUE;\n}\n#else /* !USE_WINTHREAD */\nstruct RegisterProcessShutdownNotification {\n// Work around non-reentrancy in dlopen() on Android\n#if !__TBB_USE_DLOPEN_REENTRANCY_WORKAROUND\n    RegisterProcessShutdownNotification() {\n        // prevents unloading, POSIX case\n        dlopen(MALLOCLIB_NAME, RTLD_NOW);\n    }\n#endif /* !__ANDROID__ */\n    ~RegisterProcessShutdownNotification() {\n        __TBB_mallocProcessShutdownNotification();\n    }\n};\n\nstatic RegisterProcessShutdownNotification reg;\n#endif /* !USE_WINTHREAD */\n#endif /* !__TBB_SOURCE_DIRECTLY_INCLUDED */\n\n} } // namespaces\n\n#if __TBB_ipf\n/* It was found that on IA-64 architecture inlining of __TBB_machine_lockbyte leads\n   to serious performance regression with ICC. So keep it out-of-line.\n\n   This code is copy-pasted from tbb_misc.cpp.\n */\nextern \"C\" intptr_t __TBB_machine_lockbyte( volatile unsigned char& flag ) {\n    tbb::internal::atomic_backoff backoff;\n    while( !__TBB_TryLockByte(flag) ) backoff.pause();\n    return 0;\n}\n#endif\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/tbb/itt_notify.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#if DO_ITT_NOTIFY\n\n#if _WIN32||_WIN64\n    #ifndef UNICODE\n        #define UNICODE\n    #endif\n#else\n    #pragma weak dlopen\n    #pragma weak dlsym\n    #pragma weak dlerror\n#endif /* WIN */\n\n#if __TBB_BUILD\n\nextern \"C\" void ITT_DoOneTimeInitialization();\n#define __itt_init_ittlib_name(x,y) (ITT_DoOneTimeInitialization(), true)\n\n#elif __TBBMALLOC_BUILD\n\nextern \"C\" void MallocInitializeITT();\n#define __itt_init_ittlib_name(x,y) (MallocInitializeITT(), true)\n\n#else\n#error This file is expected to be used for either TBB or TBB allocator build.\n#endif // __TBB_BUILD\n\n#include \"tools_api/ittnotify_static.c\"\n\nnamespace tbb {\nnamespace internal {\nint __TBB_load_ittnotify() {\n    return __itt_init_ittlib(NULL,          // groups for:\n      (__itt_group_id)(__itt_group_sync     // prepare/cancel/acquired/releasing\n                       | __itt_group_thread // name threads\n                       | __itt_group_stitch // stack stitching\n#if __TBB_CPF_BUILD\n                       | __itt_group_structure\n#endif\n                           ));\n}\n\n}} // namespaces\n\n#endif /* DO_ITT_NOTIFY */\n\n#define __TBB_NO_IMPLICIT_LINKAGE 1\n#include \"itt_notify.h\"\n\nnamespace tbb {\n\n#if DO_ITT_NOTIFY\n    const tchar \n            *SyncType_GlobalLock = _T(\"TbbGlobalLock\"),\n            *SyncType_Scheduler = _T(\"%Constant\")\n            ;\n    const tchar \n            *SyncObj_SchedulerInitialization = _T(\"TbbSchedulerInitialization\"),\n            *SyncObj_SchedulersList = _T(\"TbbSchedulersList\"),\n            *SyncObj_WorkerLifeCycleMgmt = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskStealingLoop = _T(\"TBB Scheduler\"),\n            *SyncObj_WorkerTaskPool = _T(\"TBB Scheduler\"),\n            *SyncObj_MasterTaskPool = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskPoolSpinning = _T(\"TBB Scheduler\"),\n            *SyncObj_Mailbox = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskReturnList = _T(\"TBB Scheduler\"),\n            *SyncObj_TaskStream = _T(\"TBB Scheduler\"),\n            *SyncObj_ContextsList = _T(\"TBB Scheduler\")\n            ;\n#endif /* DO_ITT_NOTIFY */\n\n} // namespace tbb\n\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/tbb/dynamic_link.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#include \"dynamic_link.h\"\n#include \"tbb/tbb_config.h\"\n\n/*\n    This file is used by both TBB and OpenMP RTL. Do not use __TBB_ASSERT() macro\n    and runtime_warning() function because they are not available in OpenMP. Use\n    LIBRARY_ASSERT and DYNAMIC_LINK_WARNING instead.\n*/\n\n#include <cstdarg>          // va_list etc.\n#if _WIN32\n    #include <malloc.h>\n\n    // Unify system calls\n    #define dlopen( name, flags )   LoadLibrary( name )\n    #define dlsym( handle, name )   GetProcAddress( handle, name )\n    #define dlclose( handle )       ( ! FreeLibrary( handle ) )\n    #define dlerror()               GetLastError()\n#ifndef PATH_MAX\n    #define PATH_MAX                MAX_PATH\n#endif\n#else /* _WIN32 */\n    #include <dlfcn.h>\n    #include <string.h>\n    #include <unistd.h>\n    #include <limits.h>\n    #include <stdlib.h>\n#endif /* _WIN32 */\n\n#if __TBB_WEAK_SYMBOLS_PRESENT && !__TBB_DYNAMIC_LOAD_ENABLED\n    //TODO: use function attribute for weak symbols instead of the pragma.\n    #pragma weak dlopen\n    #pragma weak dlsym\n    #pragma weak dlclose\n#endif /* __TBB_WEAK_SYMBOLS_PRESENT && !__TBB_DYNAMIC_LOAD_ENABLED */\n\n#include \"tbb/tbb_misc.h\"\n\n#define __USE_TBB_ATOMICS       ( !(__linux__&&__ia64__) || __TBB_BUILD )\n#define __USE_STATIC_DL_INIT    ( !__ANDROID__ )\n\n#if !__USE_TBB_ATOMICS\n#include <pthread.h>\n#endif\n\n/*\ndynamic_link is a common interface for searching for required symbols in an\nexecutable and dynamic libraries.\n\ndynamic_link provides certain guarantees:\n  1. Either all or none of the requested symbols are resolved. Moreover, if\n  symbols are not resolved, the dynamic_link_descriptor table is not modified;\n  2. All returned symbols have secured life time: this means that none of them\n  can be invalidated until dynamic_unlink is called;\n  3. Any loaded library is loaded only via the full path. The full path is that\n  from which the runtime itself was loaded. (This is done to avoid security\n  issues caused by loading libraries from insecure paths).\n\ndynamic_link searches for the requested symbols in three stages, stopping as\nsoon as all of the symbols have been resolved.\n\n  1. Search the global scope:\n    a. On Windows: dynamic_link tries to obtain the handle of the requested\n    library and if it succeeds it resolves the symbols via that handle.\n    b. On Linux: dynamic_link tries to search for the symbols in the global\n    scope via the main program handle. If the symbols are present in the global\n    scope their life time is not guaranteed (since dynamic_link does not know\n    anything about the library from which they are exported). Therefore it\n    tries to \"pin\" the symbols by obtaining the library name and reopening it.\n    dlopen may fail to reopen the library in two cases:\n       i. The symbols are exported from the executable. Currently dynamic _link\n      cannot handle this situation, so it will not find these symbols in this\n      step.\n      ii. The necessary library has been unloaded and cannot be reloaded. It\n      seems there is nothing that can be done in this case. No symbols are\n      returned.\n\n  2. Dynamic load: an attempt is made to load the requested library via the\n  full path.\n    The full path used is that from which the runtime itself was loaded. If the\n    library can be loaded, then an attempt is made to resolve the requested\n    symbols in the newly loaded library.\n    If the symbols are not found the library is unloaded.\n\n  3. Weak symbols: if weak symbols are available they are returned.\n*/\n\nOPEN_INTERNAL_NAMESPACE\n\n#if __TBB_WEAK_SYMBOLS_PRESENT || __TBB_DYNAMIC_LOAD_ENABLED\n\n#if !defined(DYNAMIC_LINK_WARNING) && !__TBB_WIN8UI_SUPPORT && __TBB_DYNAMIC_LOAD_ENABLED\n    // Report runtime errors and continue.\n    #define DYNAMIC_LINK_WARNING dynamic_link_warning\n    static void dynamic_link_warning( dynamic_link_error_t code, ... ) {\n        (void) code;\n    } // library_warning\n#endif /* !defined(DYNAMIC_LINK_WARNING) && !__TBB_WIN8UI_SUPPORT && __TBB_DYNAMIC_LOAD_ENABLED */\n\n    static bool resolve_symbols( dynamic_link_handle module, const dynamic_link_descriptor descriptors[], size_t required )\n    {\n        if ( !module )\n            return false;\n\n        #if !__TBB_DYNAMIC_LOAD_ENABLED /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n            if ( !dlsym ) return false;\n        #endif /* !__TBB_DYNAMIC_LOAD_ENABLED */\n\n        const size_t n_desc=20; // Usually we don't have more than 20 descriptors per library\n        LIBRARY_ASSERT( required <= n_desc, \"Too many descriptors is required\" );\n        if ( required > n_desc ) return false;\n        pointer_to_handler h[n_desc];\n\n        for ( size_t k = 0; k < required; ++k ) {\n            dynamic_link_descriptor const & desc = descriptors[k];\n            pointer_to_handler addr = (pointer_to_handler)dlsym( module, desc.name );\n            if ( !addr ) {\n                return false;\n            }\n            h[k] = addr;\n        }\n\n        // Commit the entry points.\n        // Cannot use memset here, because the writes must be atomic.\n        for( size_t k = 0; k < required; ++k )\n            *descriptors[k].handler = h[k];\n        return true;\n    }\n\n#if __TBB_WIN8UI_SUPPORT\n    bool dynamic_link( const char*  library, const dynamic_link_descriptor descriptors[], size_t required, dynamic_link_handle*, int flags ) {\n        dynamic_link_handle tmp_handle = NULL;\n        TCHAR wlibrary[256];\n        if ( MultiByteToWideChar(CP_UTF8, 0, library, -1, wlibrary, 255) == 0 ) return false;\n        if ( flags & DYNAMIC_LINK_LOAD )\n            tmp_handle = LoadPackagedLibrary( wlibrary, 0 );\n        if (tmp_handle != NULL){\n            return resolve_symbols(tmp_handle, descriptors, required);\n        }else{\n            return false;\n        }\n    }\n    void dynamic_unlink( dynamic_link_handle ) {}\n    void dynamic_unlink_all() {}\n#else\n#if __TBB_DYNAMIC_LOAD_ENABLED\n/*\n    There is a security issue on Windows: LoadLibrary() may load and execute malicious code.\n    See http://www.microsoft.com/technet/security/advisory/2269637.mspx for details.\n    To avoid the issue, we have to pass full path (not just library name) to LoadLibrary. This\n    function constructs full path to the specified library (it is assumed the library located\n    side-by-side with the tbb.dll.\n\n    The function constructs absolute path for given relative path. Important: Base directory is not\n    current one, it is the directory tbb.dll loaded from.\n\n    Example:\n        Let us assume \"tbb.dll\" is located in \"c:\\program files\\common\\intel\\\" directory, e. g.\n        absolute path of tbb library is \"c:\\program files\\common\\intel\\tbb.dll\". Absolute path for\n        \"tbbmalloc.dll\" would be \"c:\\program files\\common\\intel\\tbbmalloc.dll\". Absolute path for\n        \"malloc\\tbbmalloc.dll\" would be \"c:\\program files\\common\\intel\\malloc\\tbbmalloc.dll\".\n*/\n\n    // Struct handle_storage is used by dynamic_link routine to store handles of\n    // all loaded or pinned dynamic libraries. When TBB is shut down, it calls\n    // dynamic_unlink_all() that unloads modules referenced by handle_storage.\n    // This struct should not have any constructors since it may be used before\n    // the constructor is called.\n    #define MAX_LOADED_MODULES 8 // The number of maximum possible modules which can be loaded\n\n#if __USE_TBB_ATOMICS\n    typedef ::tbb::atomic<size_t> atomic_incrementer;\n    void init_atomic_incrementer( atomic_incrementer & ) {}\n\n    static void atomic_once( void( *func ) (void), tbb::atomic< tbb::internal::do_once_state > &once_state ) {\n        tbb::internal::atomic_do_once( func, once_state );\n    }\n    #define ATOMIC_ONCE_DECL( var ) tbb::atomic< tbb::internal::do_once_state > var\n#else\n    static void pthread_assert( int error_code, const char* msg ) {\n        LIBRARY_ASSERT( error_code == 0, msg );\n    }\n\n    class atomic_incrementer {\n        size_t my_val;\n        pthread_spinlock_t my_lock;\n    public:\n        void init() {\n            my_val = 0;\n            pthread_assert( pthread_spin_init( &my_lock, PTHREAD_PROCESS_PRIVATE ), \"pthread_spin_init failed\" );\n        }\n        size_t operator++(int) {\n            pthread_assert( pthread_spin_lock( &my_lock ), \"pthread_spin_lock failed\" );\n            size_t prev_val = my_val++;\n            pthread_assert( pthread_spin_unlock( &my_lock ), \"pthread_spin_unlock failed\" );\n            return prev_val;\n        }\n        operator size_t() {\n            pthread_assert( pthread_spin_lock( &my_lock ), \"pthread_spin_lock failed\" );\n            size_t val = my_val;\n            pthread_assert( pthread_spin_unlock( &my_lock ), \"pthread_spin_unlock failed\" );\n            return val;\n        }\n        ~atomic_incrementer() {\n            pthread_assert( pthread_spin_destroy( &my_lock ), \"pthread_spin_destroy failed\" );\n        }\n    };\n\n    void init_atomic_incrementer( atomic_incrementer &r ) {\n        r.init();\n    }\n\n    static void atomic_once( void( *func ) (), pthread_once_t &once_state ) {\n        pthread_assert( pthread_once( &once_state, func ), \"pthread_once failed\" );\n    }\n    #define ATOMIC_ONCE_DECL( var ) pthread_once_t var = PTHREAD_ONCE_INIT\n#endif /* __USE_TBB_ATOMICS */\n\n    struct handles_t {\n        atomic_incrementer my_size;\n        dynamic_link_handle my_handles[MAX_LOADED_MODULES];\n\n        void init() {\n            init_atomic_incrementer( my_size );\n        }\n\n        void add(const dynamic_link_handle &handle) {\n            const size_t ind = my_size++;\n            LIBRARY_ASSERT( ind < MAX_LOADED_MODULES, \"Too many modules are loaded\" );\n            my_handles[ind] = handle;\n        }\n\n        void free() {\n            const size_t size = my_size;\n            for (size_t i=0; i<size; ++i)\n                dynamic_unlink( my_handles[i] );\n        }\n    } handles;\n\n    ATOMIC_ONCE_DECL( init_dl_data_state );\n\n    static struct ap_data_t {\n        char _path[PATH_MAX+1];\n        size_t _len;\n    } ap_data;\n\n    static void init_ap_data() {\n    #if _WIN32\n        // Get handle of our DLL first.\n        HMODULE handle;\n        BOOL brc = GetModuleHandleEx(\n            GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS | GET_MODULE_HANDLE_EX_FLAG_UNCHANGED_REFCOUNT,\n            (LPCSTR)( & dynamic_link ), // any function inside the library can be used for the address\n            & handle\n            );\n        if ( !brc ) { // Error occurred.\n            int err = GetLastError();\n            DYNAMIC_LINK_WARNING( dl_sys_fail, \"GetModuleHandleEx\", err );\n            return;\n        }\n        // Now get path to our DLL.\n        DWORD drc = GetModuleFileName( handle, ap_data._path, static_cast< DWORD >( PATH_MAX ) );\n        if ( drc == 0 ) { // Error occurred.\n            int err = GetLastError();\n            DYNAMIC_LINK_WARNING( dl_sys_fail, \"GetModuleFileName\", err );\n            return;\n        }\n        if ( drc >= PATH_MAX ) { // Buffer too short.\n            DYNAMIC_LINK_WARNING( dl_buff_too_small );\n            return;\n        }\n        // Find the position of the last backslash.\n        char *backslash = strrchr( ap_data._path, '\\\\' );\n\n        if ( !backslash ) {    // Backslash not found.\n            LIBRARY_ASSERT( backslash!=NULL, \"Unbelievable.\");\n            return;\n        }\n        LIBRARY_ASSERT( backslash >= ap_data._path, \"Unbelievable.\");\n        ap_data._len = (size_t)(backslash - ap_data._path) + 1;\n        *(backslash+1) = 0;\n    #else\n        // Get the library path\n        Dl_info dlinfo;\n        int res = dladdr( (void*)&dynamic_link, &dlinfo ); // any function inside the library can be used for the address\n        if ( !res ) {\n            char const * err = dlerror();\n            DYNAMIC_LINK_WARNING( dl_sys_fail, \"dladdr\", err );\n            return;\n        } else {\n            LIBRARY_ASSERT( dlinfo.dli_fname!=NULL, \"Unbelievable.\" );\n        }\n\n        char const *slash = strrchr( dlinfo.dli_fname, '/' );\n        size_t fname_len=0;\n        if ( slash ) {\n            LIBRARY_ASSERT( slash >= dlinfo.dli_fname, \"Unbelievable.\");\n            fname_len = (size_t)(slash - dlinfo.dli_fname) + 1;\n        }\n\n        size_t rc;\n        if ( dlinfo.dli_fname[0]=='/' ) {\n            // The library path is absolute\n            rc = 0;\n            ap_data._len = 0;\n        } else {\n            // The library path is relative so get the current working directory\n            if ( !getcwd( ap_data._path, sizeof(ap_data._path)/sizeof(ap_data._path[0]) ) ) {\n                DYNAMIC_LINK_WARNING( dl_buff_too_small );\n                return;\n            }\n            ap_data._len = strlen( ap_data._path );\n            ap_data._path[ap_data._len++]='/';\n            rc = ap_data._len;\n        }\n\n        if ( fname_len>0 ) {\n            if ( ap_data._len>PATH_MAX ) {\n                DYNAMIC_LINK_WARNING( dl_buff_too_small );\n                ap_data._len=0;\n                return;\n            }\n            strncpy( ap_data._path+rc, dlinfo.dli_fname, fname_len );\n            ap_data._len += fname_len;\n            ap_data._path[ap_data._len]=0;\n        }\n    #endif /* _WIN32 */\n    }\n\n    static void init_dl_data() {\n        handles.init();\n        init_ap_data();\n    }\n\n    /*\n        The function constructs absolute path for given relative path. Important: Base directory is not\n        current one, it is the directory libtbb.so loaded from.\n\n        Arguments:\n        in  name -- Name of a file (may be with relative path; it must not be an absolute one).\n        out path -- Buffer to save result (absolute path) to.\n        in  len  -- Size of buffer.\n        ret      -- 0         -- Error occurred.\n                    > len     -- Buffer too short, required size returned.\n                    otherwise -- Ok, number of characters (not counting terminating null) written to\n                    buffer.\n    */\n    static size_t abs_path( char const * name, char * path, size_t len ) {\n        if ( !ap_data._len )\n            return 0;\n\n        size_t name_len = strlen( name );\n        size_t full_len = name_len+ap_data._len;\n        if ( full_len < len ) {\n            strncpy( path, ap_data._path, ap_data._len );\n            strncpy( path+ap_data._len, name, name_len );\n            path[full_len] = 0;\n        }\n        return full_len;\n    }\n#endif  // __TBB_DYNAMIC_LOAD_ENABLED\n\n    void init_dynamic_link_data() {\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        atomic_once( &init_dl_data, init_dl_data_state );\n    #endif\n    }\n\n    #if __USE_STATIC_DL_INIT\n    // ap_data structure is initialized with current directory on Linux.\n    // So it should be initialized as soon as possible since the current directory may be changed.\n    // static_init_ap_data object provides this initialization during library loading.\n    static struct static_init_dl_data_t {\n        static_init_dl_data_t() {\n            init_dynamic_link_data();\n        }\n    } static_init_dl_data;\n    #endif\n\n    #if __TBB_WEAK_SYMBOLS_PRESENT\n    static bool weak_symbol_link( const dynamic_link_descriptor descriptors[], size_t required )\n    {\n        // Check if the required entries are present in what was loaded into our process.\n        for ( size_t k = 0; k < required; ++k )\n            if ( !descriptors[k].ptr )\n                return false;\n        // Commit the entry points.\n        for ( size_t k = 0; k < required; ++k )\n            *descriptors[k].handler = (pointer_to_handler) descriptors[k].ptr;\n        return true;\n    }\n    #else\n    static bool weak_symbol_link( const dynamic_link_descriptor[], size_t ) {\n        return false;\n    }\n    #endif /* __TBB_WEAK_SYMBOLS_PRESENT */\n\n    void dynamic_unlink( dynamic_link_handle handle ) {\n    #if !__TBB_DYNAMIC_LOAD_ENABLED /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n        if ( !dlclose ) return;\n    #endif\n        if ( handle ) {\n            dlclose( handle );\n        }\n    }\n\n    void dynamic_unlink_all() {\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        handles.free();\n    #endif\n    }\n\n#if !_WIN32\n#if __TBB_DYNAMIC_LOAD_ENABLED\n    static dynamic_link_handle pin_symbols( dynamic_link_descriptor desc, const dynamic_link_descriptor* descriptors, size_t required ) {\n        // It is supposed that all symbols are from the only one library\n        // The library has been loaded by another module and contains at least one requested symbol.\n        // But after we obtained the symbol the library can be unloaded by another thread\n        // invalidating our symbol. Therefore we need to pin the library in memory.\n        dynamic_link_handle library_handle = 0;\n        Dl_info info;\n        // Get library's name from earlier found symbol\n        if ( dladdr( (void*)*desc.handler, &info ) ) {\n            // Pin the library\n            library_handle = dlopen( info.dli_fname, RTLD_LAZY );\n            if ( library_handle ) {\n                // If original library was unloaded before we pinned it\n                // and then another module loaded in its place, the earlier\n                // found symbol would become invalid. So revalidate them.\n                if ( !resolve_symbols( library_handle, descriptors, required ) ) {\n                    // Wrong library.\n                    dynamic_unlink(library_handle);\n                    library_handle = 0;\n                }\n            } else {\n                char const * err = dlerror();\n                DYNAMIC_LINK_WARNING( dl_lib_not_found, info.dli_fname, err );\n            }\n        }\n        // else the library has been unloaded by another thread\n        return library_handle;\n    }\n#endif /* __TBB_DYNAMIC_LOAD_ENABLED */\n#endif /* !_WIN32 */\n\n    static dynamic_link_handle global_symbols_link( const char* library, const dynamic_link_descriptor descriptors[], size_t required ) {\n        ::tbb::internal::suppress_unused_warning( library );\n        dynamic_link_handle library_handle;\n#if _WIN32\n        if ( GetModuleHandleEx( 0, library, &library_handle ) ) {\n            if ( resolve_symbols( library_handle, descriptors, required ) )\n                return library_handle;\n            else\n                FreeLibrary( library_handle );\n        }\n#else /* _WIN32 */\n    #if !__TBB_DYNAMIC_LOAD_ENABLED /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n        if ( !dlopen ) return 0;\n    #endif /* !__TBB_DYNAMIC_LOAD_ENABLED */\n        library_handle = dlopen( NULL, RTLD_LAZY );\n    #if !__ANDROID__\n        // On Android dlopen( NULL ) returns NULL if it is called during dynamic module initialization.\n        LIBRARY_ASSERT( library_handle, \"The handle for the main program is NULL\" );\n    #endif\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        // Check existence of the first symbol only, then use it to find the library and load all necessary symbols.\n        pointer_to_handler handler;\n        dynamic_link_descriptor desc;\n        desc.name = descriptors[0].name;\n        desc.handler = &handler;\n        if ( resolve_symbols( library_handle, &desc, 1 ) ) {\n            dynamic_unlink( library_handle );\n            return pin_symbols( desc, descriptors, required );\n        }\n    #else  /* only __TBB_WEAK_SYMBOLS_PRESENT is defined */\n        if ( resolve_symbols( library_handle, descriptors, required ) )\n            return library_handle;\n    #endif\n        dynamic_unlink( library_handle );\n#endif /* _WIN32 */\n        return 0;\n    }\n\n    static void save_library_handle( dynamic_link_handle src, dynamic_link_handle *dst ) {\n        LIBRARY_ASSERT( src, \"The library handle to store must be non-zero\" );\n        if ( dst )\n            *dst = src;\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n        else\n            handles.add( src );\n    #endif /* __TBB_DYNAMIC_LOAD_ENABLED */\n    }\n\n    dynamic_link_handle dynamic_load( const char* library, const dynamic_link_descriptor descriptors[], size_t required ) {\n    ::tbb::internal::suppress_unused_warning( library, descriptors, required );\n    #if __TBB_DYNAMIC_LOAD_ENABLED\n    #if _XBOX\n        return LoadLibrary (library);\n    #else /* _XBOX */\n        size_t const len = PATH_MAX + 1;\n        char path[ len ];\n        size_t rc = abs_path( library, path, len );\n        if ( 0 < rc && rc < len ) {\n    #if _WIN32\n            // Prevent Windows from displaying silly message boxes if it fails to load library\n            // (e.g. because of MS runtime problems - one of those crazy manifest related ones)\n            UINT prev_mode = SetErrorMode (SEM_FAILCRITICALERRORS);\n    #endif /* _WIN32 */\n            dynamic_link_handle library_handle = dlopen( path, RTLD_LAZY );\n    #if _WIN32\n            SetErrorMode (prev_mode);\n    #endif /* _WIN32 */\n            if( library_handle ) {\n                if( !resolve_symbols( library_handle, descriptors, required ) ) {\n                    // The loaded library does not contain all the expected entry points\n                    dynamic_unlink( library_handle );\n                    library_handle = NULL;\n                }\n            } else\n                DYNAMIC_LINK_WARNING( dl_lib_not_found, path, dlerror() );\n            return library_handle;\n        } else if ( rc>=len )\n                DYNAMIC_LINK_WARNING( dl_buff_too_small );\n                // rc == 0 means failing of init_ap_data so the warning has already been issued.\n    #endif /* _XBOX */\n    #endif /* __TBB_DYNAMIC_LOAD_ENABLED */\n        return 0;\n    }\n\n    bool dynamic_link( const char* library, const dynamic_link_descriptor descriptors[], size_t required, dynamic_link_handle *handle, int flags ) {\n        init_dynamic_link_data();\n\n        // TODO: May global_symbols_link find weak symbols?\n        dynamic_link_handle library_handle = ( flags & DYNAMIC_LINK_GLOBAL ) ? global_symbols_link( library, descriptors, required ) : 0;\n\n        if ( !library_handle && ( flags & DYNAMIC_LINK_LOAD ) )\n            library_handle = dynamic_load( library, descriptors, required );\n\n        if ( !library_handle && ( flags & DYNAMIC_LINK_WEAK ) )\n            return weak_symbol_link( descriptors, required );\n\n        if ( library_handle ) {\n            save_library_handle( library_handle, handle );\n            return true;\n        }\n        return false;\n    }\n\n#endif /*__TBB_WIN8UI_SUPPORT*/\n#else /* __TBB_WEAK_SYMBOLS_PRESENT || __TBB_DYNAMIC_LOAD_ENABLED */\n    bool dynamic_link( const char*, const dynamic_link_descriptor*, size_t, dynamic_link_handle *handle, int ) {\n        if ( handle )\n            *handle=0;\n        return false;\n    }\n    void dynamic_unlink( dynamic_link_handle ) {}\n    void dynamic_unlink_all() {}\n#endif /* __TBB_WEAK_SYMBOLS_PRESENT || __TBB_DYNAMIC_LOAD_ENABLED */\n\nCLOSE_INTERNAL_NAMESPACE\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/tbb/tools_api/ittnotify_config.h": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#ifndef _ITTNOTIFY_CONFIG_H_\n#define _ITTNOTIFY_CONFIG_H_\n\n/** @cond exclude_from_documentation */\n#ifndef ITT_OS_WIN\n#  define ITT_OS_WIN   1\n#endif /* ITT_OS_WIN */\n\n#ifndef ITT_OS_LINUX\n#  define ITT_OS_LINUX 2\n#endif /* ITT_OS_LINUX */\n\n#ifndef ITT_OS_MAC\n#  define ITT_OS_MAC   3\n#endif /* ITT_OS_MAC */\n\n#ifndef ITT_OS\n#  if defined WIN32 || defined _WIN32\n#    define ITT_OS ITT_OS_WIN\n#  elif defined( __APPLE__ ) && defined( __MACH__ )\n#    define ITT_OS ITT_OS_MAC\n#  else\n#    define ITT_OS ITT_OS_LINUX\n#  endif\n#endif /* ITT_OS */\n\n#ifndef ITT_PLATFORM_WIN\n#  define ITT_PLATFORM_WIN 1\n#endif /* ITT_PLATFORM_WIN */\n\n#ifndef ITT_PLATFORM_POSIX\n#  define ITT_PLATFORM_POSIX 2\n#endif /* ITT_PLATFORM_POSIX */\n\n#ifndef ITT_PLATFORM_MAC\n#  define ITT_PLATFORM_MAC 3\n#endif /* ITT_PLATFORM_MAC */\n\n#ifndef ITT_PLATFORM\n#  if ITT_OS==ITT_OS_WIN\n#    define ITT_PLATFORM ITT_PLATFORM_WIN\n#  elif ITT_OS==ITT_OS_MAC\n#    define ITT_PLATFORM ITT_PLATFORM_MAC\n#  else\n#    define ITT_PLATFORM ITT_PLATFORM_POSIX\n#  endif\n#endif /* ITT_PLATFORM */\n\n#if defined(_UNICODE) && !defined(UNICODE)\n#define UNICODE\n#endif\n\n#include <stddef.h>\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <tchar.h>\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <stdint.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE || _UNICODE */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#ifndef CDECL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define CDECL __cdecl\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__ \n#      define CDECL __attribute__ ((cdecl))\n#    else  /* _M_IX86 || __i386__ */\n#      define CDECL /* actual only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* CDECL */\n\n#ifndef STDCALL\n#  if ITT_PLATFORM==ITT_PLATFORM_WIN\n#    define STDCALL __stdcall\n#  else /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#    if defined _M_IX86 || defined __i386__\n#      define STDCALL __attribute__ ((stdcall)) \n#    else  /* _M_IX86 || __i386__ */\n#      define STDCALL /* supported only on x86 platform */\n#    endif /* _M_IX86 || __i386__ */\n#  endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#endif /* STDCALL */\n\n#define ITTAPI    CDECL\n#define LIBITTAPI CDECL\n\n/* TODO: Temporary for compatibility! */\n#define ITTAPI_CALL    CDECL\n#define LIBITTAPI_CALL CDECL\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n/* use __forceinline (VC++ specific) */\n#define ITT_INLINE           __forceinline\n#define ITT_INLINE_ATTRIBUTE /* nothing */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/*\n * Generally, functions are not inlined unless optimization is specified.\n * For functions declared inline, this attribute inlines the function even\n * if no optimization level was specified.\n */\n#ifdef __STRICT_ANSI__\n#define ITT_INLINE           static inline\n#else  /* __STRICT_ANSI__ */\n#define ITT_INLINE           static inline\n#endif /* __STRICT_ANSI__ */\n#define ITT_INLINE_ATTRIBUTE __attribute__ ((always_inline, unused))\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n/** @endcond */\n\n#ifndef ITT_ARCH_IA32\n#  define ITT_ARCH_IA32  1\n#endif /* ITT_ARCH_IA32 */\n\n#ifndef ITT_ARCH_IA32E\n#  define ITT_ARCH_IA32E 2\n#endif /* ITT_ARCH_IA32E */\n\n#ifndef ITT_ARCH_ARM\n#  define ITT_ARCH_ARM  4\n#endif /* ITT_ARCH_ARM */\n\n#ifndef ITT_ARCH\n#  if defined _M_IX86 || defined __i386__\n#    define ITT_ARCH ITT_ARCH_IA32\n#  elif defined _M_X64 || defined _M_AMD64 || defined __x86_64__\n#    define ITT_ARCH ITT_ARCH_IA32E\n#  elif defined _M_IA64 || defined __ia64__\n#    define ITT_ARCH ITT_ARCH_IA64\n#  elif defined _M_ARM || __arm__\n#    define ITT_ARCH ITT_ARCH_ARM\n#  endif\n#endif\n\n#ifdef __cplusplus\n#  define ITT_EXTERN_C extern \"C\"\n#else\n#  define ITT_EXTERN_C /* nothing */\n#endif /* __cplusplus */\n\n#define ITT_TO_STR_AUX(x) #x\n#define ITT_TO_STR(x)     ITT_TO_STR_AUX(x)\n\n#define __ITT_BUILD_ASSERT(expr, suffix) do { \\\n    static char __itt_build_check_##suffix[(expr) ? 1 : -1]; \\\n    __itt_build_check_##suffix[0] = 0; \\\n} while(0)\n#define _ITT_BUILD_ASSERT(expr, suffix)  __ITT_BUILD_ASSERT((expr), suffix)\n#define ITT_BUILD_ASSERT(expr)           _ITT_BUILD_ASSERT((expr), __LINE__)\n\n#define ITT_MAGIC { 0xED, 0xAB, 0xAB, 0xEC, 0x0D, 0xEE, 0xDA, 0x30 }\n\n/* Replace with snapshot date YYYYMMDD for promotion build. */\n#define API_VERSION_BUILD    20111111\n\n#ifndef API_VERSION_NUM\n#define API_VERSION_NUM 0.0.0\n#endif /* API_VERSION_NUM */\n\n#define API_VERSION \"ITT-API-Version \" ITT_TO_STR(API_VERSION_NUM) \\\n                                \" (\" ITT_TO_STR(API_VERSION_BUILD) \")\"\n\n/* OS communication functions */\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#include <windows.h>\ntypedef HMODULE           lib_t;\ntypedef DWORD             TIDT;\ntypedef CRITICAL_SECTION  mutex_t;\n#define MUTEX_INITIALIZER { 0 }\n#define strong_alias(name, aliasname) /* empty for Windows */\n#else  /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n#include <dlfcn.h>\n#if defined(UNICODE) || defined(_UNICODE)\n#include <wchar.h>\n#endif /* UNICODE */\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE 1 /* need for PTHREAD_MUTEX_RECURSIVE */\n#endif /* _GNU_SOURCE */\n#ifndef __USE_UNIX98\n#define __USE_UNIX98 1 /* need for PTHREAD_MUTEX_RECURSIVE, on SLES11.1 with gcc 4.3.4 wherein pthread.h missing dependency on __USE_XOPEN2K8 */\n#endif /*__USE_UNIX98*/\n#include <pthread.h>\ntypedef void*             lib_t;\ntypedef pthread_t         TIDT;\ntypedef pthread_mutex_t   mutex_t;\n#define MUTEX_INITIALIZER PTHREAD_MUTEX_INITIALIZER\n#define _strong_alias(name, aliasname) \\\n            extern __typeof (name) aliasname __attribute__ ((alias (#name)));\n#define strong_alias(name, aliasname) _strong_alias(name, aliasname)\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\n#if ITT_PLATFORM==ITT_PLATFORM_WIN\n#define __itt_get_proc(lib, name) GetProcAddress(lib, name)\n#define __itt_mutex_init(mutex)   InitializeCriticalSection(mutex)\n#define __itt_mutex_lock(mutex)   EnterCriticalSection(mutex)\n#define __itt_mutex_unlock(mutex) LeaveCriticalSection(mutex)\n#define __itt_load_lib(name)      LoadLibraryA(name)\n#define __itt_unload_lib(handle)  FreeLibrary(handle)\n#define __itt_system_error()      (int)GetLastError()\n#define __itt_fstrcmp(s1, s2)     lstrcmpA(s1, s2)\n#define __itt_fstrlen(s)          lstrlenA(s)\n#define __itt_fstrcpyn(s1, s2, l) lstrcpynA(s1, s2, l)\n#define __itt_fstrdup(s)          _strdup(s)\n#define __itt_thread_id()         GetCurrentThreadId()\n#define __itt_thread_yield()      SwitchToThread()\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return InterlockedIncrement(ptr);\n}\n#endif /* ITT_SIMPLE_INIT */\n#else /* ITT_PLATFORM!=ITT_PLATFORM_WIN */\n#define __itt_get_proc(lib, name) dlsym(lib, name)\n#define __itt_mutex_init(mutex)   {\\\n    pthread_mutexattr_t mutex_attr;                                         \\\n    int error_code = pthread_mutexattr_init(&mutex_attr);                   \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_init\",    \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_settype(&mutex_attr,                     \\\n                                           PTHREAD_MUTEX_RECURSIVE);        \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_settype\", \\\n                           error_code);                                     \\\n    error_code = pthread_mutex_init(mutex, &mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutex_init\",        \\\n                           error_code);                                     \\\n    error_code = pthread_mutexattr_destroy(&mutex_attr);                    \\\n    if (error_code)                                                         \\\n        __itt_report_error(__itt_error_system, \"pthread_mutexattr_destroy\", \\\n                           error_code);                                     \\\n}\n#define __itt_mutex_lock(mutex)   pthread_mutex_lock(mutex)\n#define __itt_mutex_unlock(mutex) pthread_mutex_unlock(mutex)\n#define __itt_load_lib(name)      dlopen(name, RTLD_LAZY)\n#define __itt_unload_lib(handle)  dlclose(handle)\n#define __itt_system_error()      errno\n#define __itt_fstrcmp(s1, s2)     strcmp(s1, s2)\n#define __itt_fstrlen(s)          strlen(s)\n#define __itt_fstrcpyn(s1, s2, l) strncpy(s1, s2, l)\n#define __itt_fstrdup(s)          strdup(s)\n#define __itt_thread_id()         pthread_self()\n#define __itt_thread_yield()      sched_yield()\n#if ITT_ARCH==ITT_ARCH_IA64\n#ifdef __INTEL_COMPILER\n#define __TBB_machine_fetchadd4(addr, val) __fetchadd4_acq((void *)addr, val)\n#else  /* __INTEL_COMPILER */\n/* TODO: Add Support for not Intel compilers for IA-64 architecture */\n#endif /* __INTEL_COMPILER */\n#elif ITT_ARCH==ITT_ARCH_IA32 || ITT_ARCH==ITT_ARCH_IA32E /* ITT_ARCH!=ITT_ARCH_IA64 */\nITT_INLINE long\n__TBB_machine_fetchadd4(volatile void* ptr, long addend) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __TBB_machine_fetchadd4(volatile void* ptr, long addend)\n{\n    long result;\n    __asm__ __volatile__(\"lock\\nxadd %0,%1\"\n                          : \"=r\"(result),\"=m\"(*(int*)ptr)\n                          : \"0\"(addend), \"m\"(*(int*)ptr)\n                          : \"memory\");\n    return result;\n}\n#elif ITT_ARCH==ITT_ARCH_ARM\n#define __TBB_machine_fetchadd4(addr, val) __sync_fetch_and_add(addr, val)\n#endif /* ITT_ARCH==ITT_ARCH_IA64 */\n#ifndef ITT_SIMPLE_INIT\nITT_INLINE long\n__itt_interlocked_increment(volatile long* ptr) ITT_INLINE_ATTRIBUTE;\nITT_INLINE long __itt_interlocked_increment(volatile long* ptr)\n{\n    return __TBB_machine_fetchadd4(ptr, 1) + 1L;\n}\n#endif /* ITT_SIMPLE_INIT */\n#endif /* ITT_PLATFORM==ITT_PLATFORM_WIN */\n\ntypedef enum {\n    __itt_collection_normal = 0,\n    __itt_collection_paused = 1\n} __itt_collection_state;\n\ntypedef enum {\n    __itt_thread_normal  = 0,\n    __itt_thread_ignored = 1\n} __itt_thread_state;\n\n#pragma pack(push, 8)\n\ntypedef struct ___itt_thread_info\n{\n    const char* nameA; /*!< Copy of original name in ASCII. */\n#if defined(UNICODE) || defined(_UNICODE)\n    const wchar_t* nameW; /*!< Copy of original name in UNICODE. */\n#else  /* UNICODE || _UNICODE */\n    void* nameW;\n#endif /* UNICODE || _UNICODE */\n    TIDT               tid;\n    __itt_thread_state state;   /*!< Thread state (paused or normal) */\n    int                extra1;  /*!< Reserved to the runtime */\n    void*              extra2;  /*!< Reserved to the runtime */\n    struct ___itt_thread_info* next;\n} __itt_thread_info;\n\n#include \"ittnotify_types.h\" /* For __itt_group_id definition */\n\ntypedef struct ___itt_api_info_20101001\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    __itt_group_id group;\n}  __itt_api_info_20101001;\n\ntypedef struct ___itt_api_info\n{\n    const char*    name;\n    void**         func_ptr;\n    void*          init_func;\n    void*          null_func;\n    __itt_group_id group;\n}  __itt_api_info;\n\nstruct ___itt_domain;\nstruct ___itt_string_handle;\n\ntypedef struct ___itt_global\n{\n    unsigned char          magic[8];\n    unsigned long          version_major;\n    unsigned long          version_minor;\n    unsigned long          version_build;\n    volatile long          api_initialized;\n    volatile long          mutex_initialized;\n    volatile long          atomic_counter;\n    mutex_t                mutex;\n    lib_t                  lib;\n    void*                  error_handler;\n    const char**           dll_path_ptr;\n    __itt_api_info*        api_list_ptr;\n    struct ___itt_global*  next;\n    /* Joinable structures below */\n    __itt_thread_info*     thread_list;\n    struct ___itt_domain*  domain_list;\n    struct ___itt_string_handle* string_list;\n    __itt_collection_state state;\n} __itt_global;\n\n#pragma pack(pop)\n\n#define NEW_THREAD_INFO_W(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = NULL; \\\n        h->nameW  = n ? _wcsdup(n) : NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_THREAD_INFO_A(gptr,h,h_tail,t,s,n) { \\\n    h = (__itt_thread_info*)malloc(sizeof(__itt_thread_info)); \\\n    if (h != NULL) { \\\n        h->tid    = t; \\\n        h->nameA  = n ? __itt_fstrdup(n) : NULL; \\\n        h->nameW  = NULL; \\\n        h->state  = s; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->thread_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_W(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 0;    /* domain is disabled by default */ \\\n        h->nameA  = NULL; \\\n        h->nameW  = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_DOMAIN_A(gptr,h,h_tail,name) { \\\n    h = (__itt_domain*)malloc(sizeof(__itt_domain)); \\\n    if (h != NULL) { \\\n        h->flags  = 0;    /* domain is disabled by default */ \\\n        h->nameA  = name ? __itt_fstrdup(name) : NULL; \\\n        h->nameW  = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->domain_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_W(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = NULL; \\\n        h->strW   = name ? _wcsdup(name) : NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#define NEW_STRING_HANDLE_A(gptr,h,h_tail,name) { \\\n    h = (__itt_string_handle*)malloc(sizeof(__itt_string_handle)); \\\n    if (h != NULL) { \\\n        h->strA   = name ? __itt_fstrdup(name) : NULL; \\\n        h->strW   = NULL; \\\n        h->extra1 = 0;    /* reserved */ \\\n        h->extra2 = NULL; /* reserved */ \\\n        h->next   = NULL; \\\n        if (h_tail == NULL) \\\n            (gptr)->string_list = h; \\\n        else \\\n            h_tail->next = h; \\\n    } \\\n}\n\n#endif /* _ITTNOTIFY_CONFIG_H_ */\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/rml/server/rml_server.cpp": "/*\n    Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n\n    This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n    you can redistribute it and/or modify it under the terms of the GNU General Public License\n    version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n    distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n    implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See  the GNU General Public License for more details.   You should have received a copy of\n    the  GNU General Public License along with Threading Building Blocks; if not, write to the\n    Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n\n    As a special exception,  you may use this file  as part of a free software library without\n    restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n    functions from this file, or you compile this file and link it with other files to produce\n    an executable,  this file does not by itself cause the resulting executable to be covered\n    by the GNU General Public License. This exception does not however invalidate any other\n    reasons why the executable file might be covered by the GNU General Public License.\n*/\n\n#include \"rml_tbb.h\"\n#define private public /* Sleazy trick to avoid publishing internal names in public header. */\n#include \"rml_omp.h\"\n#undef private\n\n#include \"tbb/tbb_allocator.h\"\n#include \"tbb/cache_aligned_allocator.h\"\n#include \"tbb/aligned_space.h\"\n#include \"tbb/atomic.h\"\n#include \"tbb/spin_mutex.h\"\n#include \"tbb/tbb_misc.h\"           // Get AvailableHwConcurrency() from here.\n#if _MSC_VER==1500 && !defined(__INTEL_COMPILER)\n// VS2008/VC9 seems to have an issue;\n#pragma warning( push )\n#pragma warning( disable: 4985 )\n#endif\n#include \"tbb/concurrent_vector.h\"\n#if _MSC_VER==1500 && !defined(__INTEL_COMPILER)\n#pragma warning( pop )\n#endif\n#if _MSC_VER && defined(_Wp64)\n// Workaround for overzealous compiler warnings\n#pragma warning (push)\n#pragma warning (disable: 4244)\n#endif\n\n#include \"job_automaton.h\"\n#include \"wait_counter.h\"\n#include \"thread_monitor.h\"\n\n#if RML_USE_WCRM\n#include <concrt.h>\n#include <concrtrm.h>\nusing namespace Concurrency;\n#include <vector>\n#include <hash_map>\n#define __RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED 0\n#endif /* RML_USE_WCRM */\n\n#define STRINGIFY(x) #x\n#define TOSTRING(x) STRINGIFY(x)\n\nnamespace rml {\nnamespace internal {\n\nusing tbb::internal::rml::tbb_client;\nusing tbb::internal::rml::tbb_server;\n\nusing __kmp::rml::omp_client;\nusing __kmp::rml::omp_server;\n\ntypedef versioned_object::version_type version_type;\n\n#define SERVER_VERSION 2\n#define EARLIEST_COMPATIBLE_CLIENT_VERSION 2\n\nstatic const size_t cache_line_size = tbb::internal::NFS_MaxLineSize;\n\ntemplate<typename Server, typename Client> class generic_connection;\nclass tbb_connection_v2;\nclass omp_connection_v2;\n\n#if RML_USE_WCRM\n//! State of a server_thread\n/** Below are diagrams of legal state transitions.\n\n                          ts_busy\n                          ^      ^\n                         /        \\\n                        /          V\n    ts_done <----- ts_asleep <------> ts_idle\n*/\n\nenum thread_state_t {\n    ts_idle,\n    ts_asleep,\n    ts_busy,\n    ts_done\n};\n\n//! Extra state of an omp server thread\nenum thread_extra_state_t {\n    ts_none,\n    ts_removed,\n    ts_lent\n};\n\n//! Results from try_grab_for()\nenum thread_grab_t {\n    wk_failed,\n    wk_from_asleep,\n    wk_from_idle\n};\n\n#else /* !RML_USE_WCRM */\n\n//! State of a server_thread\n/** Below are diagrams of legal state transitions.\n\n    OMP\n              ts_omp_busy\n              ^          ^\n             /            \\\n            /              V\n    ts_asleep <-----------> ts_idle\n\n\n              ts_deactivated\n             ^            ^\n            /              \\\n           V                \\\n    ts_none  <--------------> ts_reactivated\n\n    TBB\n              ts_tbb_busy\n              ^          ^\n             /            \\\n            /              V\n    ts_asleep <-----------> ts_idle --> ts_done\n\n    For TBB only. Extra state transition.\n\n    ts_created -> ts_started -> ts_visited\n */\nenum thread_state_t {\n    //! Thread not doing anything useful, but running and looking for work.\n    ts_idle,\n    //! Thread not doing anything useful and is asleep */\n    ts_asleep,\n    //! Thread is enlisted into OpenMP team\n    ts_omp_busy,\n    //! Thread is busy doing TBB work.\n    ts_tbb_busy,\n    //! For tbb threads only\n    ts_done,\n    ts_created,\n    ts_started,\n    ts_visited,\n    //! For omp threads only\n    ts_none,\n    ts_deactivated,\n    ts_reactivated\n};\n#endif /* RML_USE_WCRM */\n\n#if TBB_USE_ASSERT\n#define PRODUCE_ARG(x) ,x\n#else\n#define PRODUCE_ARG(x)\n#endif /* TBB_USE_ASSERT */\n\n//! Synchronizes dispatch of OpenMP work.\nclass omp_dispatch_type {\n    typedef ::rml::job job_type;\n    omp_client* client;\n    void* cookie;\n    omp_client::size_type index;\n    tbb::atomic<job_type*> job;\n#if TBB_USE_ASSERT\n    omp_connection_v2* server;\n#endif /* TBB_USE_ASSERT */\npublic:\n    omp_dispatch_type() {job=NULL;}\n    void consume();\n    void produce( omp_client& c, job_type& j, void* cookie_, omp_client::size_type index_ PRODUCE_ARG( omp_connection_v2& s )) {\n        __TBB_ASSERT( &j, NULL );\n        __TBB_ASSERT( !job, \"job already set\" );\n        client = &c;\n#if TBB_USE_ASSERT\n        server = &s;\n#endif /* TBB_USE_ASSERT */\n        cookie = cookie_;\n        index = index_;\n        // Must be last\n        job = &j;\n    }\n};\n\n//! A reference count.\n/** No default constructor, because users of ref_count must be very careful about whether the\n    initial reference count is 0 or 1. */\nclass ref_count: no_copy {\n    friend class thread_map;\n    tbb::atomic<int> my_ref_count;\npublic:\n    ref_count(int k ) {my_ref_count=k;}\n    ~ref_count() {__TBB_ASSERT( !my_ref_count, \"premature destruction of refcounted object\" );}\n    //! Add one and return new value.\n    int add_ref() {\n        int k = ++my_ref_count;\n        __TBB_ASSERT(k>=1,\"reference count underflowed before add_ref\");\n        return k;\n    }\n    //! Subtract one and return new value.\n    int remove_ref() {\n        int k = --my_ref_count;\n        __TBB_ASSERT(k>=0,\"reference count underflow\");\n        return k;\n    }\n};\n\n#if RML_USE_WCRM\n\n#if USE_UMS_THREAD\n#define RML_THREAD_KIND UmsThreadDefault\n#define RML_THREAD_KIND_STRING \"UmsThread\"\n#else\n#define RML_THREAD_KIND ThreadScheduler\n#define RML_THREAD_KIND_STRING \"WinThread\"\n#endif\n\n// Forward declaration\nclass thread_map;\n\nstatic const IExecutionResource* c_remove_prepare = (IExecutionResource*)0;\nstatic const IExecutionResource* c_remove_returned = (IExecutionResource*)1;\n\n//! Server thread representation\nclass server_thread_rep : no_copy {\n    friend class thread_map;\n    friend class omp_connection_v2;\n    friend class server_thread;\n    friend class tbb_server_thread;\n    friend class omp_server_thread;\n    template<typename Connection> friend void make_job( Connection& c, typename Connection::server_thread_type& t );\n    typedef int thread_state_rep_t;\npublic:\n    //! Ctor\n    server_thread_rep( bool assigned, IScheduler* s, IExecutionResource* r, thread_map& map, rml::client& cl ) :\n        uid( GetExecutionContextId() ), my_scheduler(s), my_proxy(NULL),\n        my_thread_map(map), my_client(cl), my_job(NULL)\n    {\n        my_state = assigned ? ts_busy : ts_idle;\n        my_extra_state = ts_none;\n        terminate = false;\n        my_execution_resource = r;\n    }\n    //! Dtor\n    ~server_thread_rep() {}\n\n    //! Synchronization routine\n    inline rml::job* wait_for_job() {\n        if( !my_job ) my_job = &my_job_automaton.wait_for_job();\n        return my_job;\n    }\n\n    // Getters and setters\n    inline thread_state_t read_state() const { thread_state_rep_t s = my_state; return static_cast<thread_state_t>(s); }\n    inline void set_state( thread_state_t to ) {my_state = to;}\n    inline void set_removed() { __TBB_ASSERT( my_extra_state==ts_none, NULL ); my_extra_state = ts_removed; }\n    inline bool is_removed() const { return my_extra_state==ts_removed; }\n    inline bool is_lent() const {return my_extra_state==ts_lent;}\n    inline void set_lent() { my_extra_state=ts_lent; }\n    inline void set_returned() { my_extra_state=ts_none; }\n    inline IExecutionResource* get_execution_resource() { return my_execution_resource; }\n    inline IVirtualProcessorRoot* get_virtual_processor() { return (IVirtualProcessorRoot*)get_execution_resource(); }\n\n    //! Enlist the thread for work\n    inline bool wakeup( thread_state_t to, thread_state_t from ) {\n        __TBB_ASSERT( from==ts_asleep && (to==ts_idle||to==ts_busy||to==ts_done), NULL );\n        return my_state.compare_and_swap( to, from )==from;\n    }\n\n    //! Enlist the thread for.\n    thread_grab_t try_grab_for();\n\n    //! Destroy the client job associated with the thread\n    template<typename Connection> bool destroy_job( Connection* c );\n\n    //! Try to re-use the thread\n    void revive( IScheduler* s, IExecutionResource* r, rml::client& c ) {\n        // the variables may not have been set before a thread was told to quit\n        __TBB_ASSERT( my_scheduler==s, \"my_scheduler has been altered?\\n\" );\n        my_scheduler = s;\n        __TBB_ASSERT( &my_client==&c, \"my_client has been altered?\\n\" );\n        if( r ) my_execution_resource = r;\n        my_client = c;\n        my_state = ts_idle;\n        __TBB_ASSERT( my_extra_state==ts_removed, NULL );\n        my_extra_state = ts_none;\n    }\n\nprotected:\n    const int uid;\n    IScheduler* my_scheduler;\n    IThreadProxy* my_proxy;\n    tbb::atomic<IExecutionResource*> my_execution_resource; /* for non-masters, it is IVirtualProcessorRoot */\n    thread_map& my_thread_map;\n    rml::client& my_client;\n    job* my_job;\n    job_automaton my_job_automaton;\n    tbb::atomic<bool> terminate;\n    tbb::atomic<thread_state_rep_t> my_state;\n    tbb::atomic<thread_extra_state_t> my_extra_state;\n};\n\n//! Class that implements IExecutionContext\nclass server_thread : public IExecutionContext, public server_thread_rep {\n    friend class tbb_connection_v2;\n    friend class omp_connection_v2;\n    friend class tbb_server_thread;\n    friend class omp_server_thread;\n    friend class thread_map;\n    template<typename Connection> friend void make_job( Connection& c, typename Connection::server_thread_type& t );\nprotected:\n    server_thread( bool is_tbb, bool assigned, IScheduler* s, IExecutionResource* r, thread_map& map, rml::client& cl ) : server_thread_rep(assigned,s,r,map,cl), tbb_thread(is_tbb) {}\n    ~server_thread() {}\n    /*override*/ unsigned int GetId() const { return uid; }\n    /*override*/ IScheduler* GetScheduler() { return my_scheduler; }\n    /*override*/ IThreadProxy* GetProxy()   { return my_proxy; }\n    /*override*/ void SetProxy( IThreadProxy* thr_proxy ) { my_proxy = thr_proxy; }\n\nprivate:\n    bool tbb_thread;\n};\n\n// Forward declaration\nclass tbb_connection_v2;\nclass omp_connection_v2;\n\n//! TBB server thread\nclass tbb_server_thread : public server_thread {\n    friend class tbb_connection_v2;\npublic:\n    tbb_server_thread( bool assigned, IScheduler* s, IExecutionResource* r, tbb_connection_v2* con, thread_map& map, rml::client& cl ) : server_thread(true,assigned,s,r,map,cl), my_conn(con) {\n        activation_count = 0;\n    }\n    ~tbb_server_thread() {}\n    /*override*/ void Dispatch( DispatchState* );\n    inline bool initiate_termination();\n    bool sleep_perhaps();\n    //! Switch out this thread\n    bool switch_out();\nprivate:\n    tbb_connection_v2* my_conn;\npublic:\n    tbb::atomic<int> activation_count;\n};\n\n//! OMP server thread\nclass omp_server_thread : public server_thread {\n    friend class omp_connection_v2;\npublic:\n    omp_server_thread( bool assigned, IScheduler* s, IExecutionResource* r, omp_connection_v2* con, thread_map& map, rml::client& cl ) :\n        server_thread(false,assigned,s,r,map,cl), my_conn(con), my_cookie(NULL), my_index(UINT_MAX) {}\n    ~omp_server_thread() {}\n    /*override*/ void Dispatch( DispatchState* );\n    inline void* get_cookie() {return my_cookie;}\n    inline ::__kmp::rml::omp_client::size_type get_index() {return my_index;}\n\n    inline IExecutionResource* get_execution_resource() { return get_execution_resource(); }\n    inline bool initiate_termination() { return destroy_job( (omp_connection_v2*) my_conn ); }\n    void sleep_perhaps();\nprivate:\n    omp_connection_v2* my_conn;\n    void* my_cookie;\n    ::__kmp::rml::omp_client::size_type my_index;\n    omp_dispatch_type omp_data;\n};\n\n//! Class that implements IScheduler\ntemplate<typename Connection>\nclass scheduler : no_copy, public IScheduler {\npublic:\n    /*override*/ unsigned int GetId() const {return uid;}\n    /*override*/ void Statistics( unsigned int* /*pTaskCompletionRate*/, unsigned int* /*pTaskArrivalRate*/, unsigned int* /*pNumberOfTaskEnqueued*/) {}\n    /*override*/ SchedulerPolicy GetPolicy() const { __TBB_ASSERT(my_policy,NULL); return *my_policy; }\n    /*override*/ void AddVirtualProcessors( IVirtualProcessorRoot** vproots, unsigned int count ) { if( !my_conn.is_closing() ) my_conn.add_virtual_processors( vproots, count); }\n    /*override*/ void RemoveVirtualProcessors( IVirtualProcessorRoot** vproots, unsigned int count );\n    /*override*/ void NotifyResourcesExternallyIdle( IVirtualProcessorRoot** vproots, unsigned int count ) { __TBB_ASSERT( false, \"This call is not allowed for TBB\" ); }\n    /*override*/ void NotifyResourcesExternallyBusy( IVirtualProcessorRoot** vproots, unsigned int count ) { __TBB_ASSERT( false, \"This call is not allowed for TBB\" ); }\nprotected:\n    scheduler( Connection& conn );\n    virtual ~scheduler() { __TBB_ASSERT( my_policy, NULL ); delete my_policy; }\n\npublic:\n    static scheduler* create( Connection& conn ) {return new scheduler( conn );}\n\nprivate:\n    const int uid;\n    Connection& my_conn;\n    SchedulerPolicy* my_policy;\n};\n\n\n/*\n * --> ts_busy --> ts_done\n */\nclass thread_scavenger_thread : public IExecutionContext, no_copy {\npublic:\n    thread_scavenger_thread( IScheduler* s, IVirtualProcessorRoot* r, thread_map& map ) :\n        uid( GetExecutionContextId() ), my_scheduler(s), my_virtual_processor_root(r), my_proxy(NULL), my_thread_map(map)\n    {\n        my_state = ts_busy;\n#if TBB_USE_ASSERT\n        activation_count = 0;\n#endif\n    }\n    ~thread_scavenger_thread() {}\n    /*override*/ unsigned int GetId() const { return uid; }\n    /*override*/ IScheduler* GetScheduler() { return my_scheduler; }\n    /*override*/ IThreadProxy* GetProxy()   { return my_proxy; }\n    /*override*/ void SetProxy( IThreadProxy* thr_proxy ) { my_proxy = thr_proxy; }\n    /*override*/ void Dispatch( DispatchState* );\n    inline thread_state_t read_state() { return my_state; }\n    inline void set_state( thread_state_t s ) { my_state = s; }\n    inline IVirtualProcessorRoot* get_virtual_processor() { return my_virtual_processor_root; }\nprivate:\n    const int uid;\n    IScheduler* my_scheduler;\n    IVirtualProcessorRoot* my_virtual_processor_root;\n    IThreadProxy* my_proxy;\n    thread_map& my_thread_map;\n    tbb::atomic<thread_state_t> my_state;\n#if TBB_USE_ASSERT\npublic:\n    tbb::atomic<int> activation_count;\n#endif\n};\n\nstatic const thread_scavenger_thread* c_claimed = reinterpret_cast<thread_scavenger_thread*>(1);\n\nstruct garbage_connection_queue {\n    tbb::atomic<uintptr_t> head;\n    tbb::atomic<uintptr_t> tail;\n    static const uintptr_t empty = 0; // connection scavenger thread empty list\n    static const uintptr_t plugged = 1;  // end of use of the list\n    static const uintptr_t plugged_acked = 2;  // connection scavenger saw the plugged flag, and it freed all connections\n};\n\n//! Connection scavenger\n/** It collects closed connection objects, wait for worker threads belonging to the connection to return to ConcRT RM\n *  then return the object to the memory manager.\n */\nclass connection_scavenger_thread {\n    friend void assist_cleanup_connections();\n    /*\n     * connection_scavenger_thread's state\n     * ts_busy <----> ts_asleep <--\n     */\n    tbb::atomic<thread_state_t> state;\n\n    /* We steal two bits from a connection pointer to encode\n     * whether the connection is for TBB or for OMP.\n     *\n     * ----------------------------------\n     * |                          |  |  |\n     * ----------------------------------\n     *                              ^  ^\n     *                             /   |\n     *            1 : tbb, 0 : omp     |\n     *                  if set, terminate\n     */\n    // FIXME: pad these?\n    thread_monitor monitor;\n    HANDLE thr_handle;\n#if TBB_USE_ASSERT\n    tbb::atomic<int> n_scavenger_threads;\n#endif\n\npublic:\n    connection_scavenger_thread() : thr_handle(NULL) {\n        state = ts_asleep;\n#if TBB_USE_ASSERT\n        n_scavenger_threads = 0;\n#endif\n    }\n\n    ~connection_scavenger_thread() {}\n\n    void wakeup() {\n        if( state.compare_and_swap( ts_busy, ts_asleep )==ts_asleep )\n            monitor.notify();\n    }\n\n    void sleep_perhaps();\n\n    void process_requests( uintptr_t conn_ex );\n\n    static __RML_DECL_THREAD_ROUTINE thread_routine( void* arg );\n\n    void launch() {\n        thread_monitor::launch( connection_scavenger_thread::thread_routine, this, NULL );\n    }\n\n    template<typename Server, typename Client>\n    void add_request( generic_connection<Server,Client>* conn_to_close );\n\n    template<typename Server, typename Client>\n    uintptr_t grab_and_prepend( generic_connection<Server,Client>* last_conn_to_close );\n};\n\nvoid free_all_connections( uintptr_t );\n\n#endif /* RML_USE_WCRM */\n\n#if !RML_USE_WCRM\nclass server_thread;\n\n//! thread_map_base; we need to make the iterator type available to server_thread\nstruct thread_map_base {\n    //! A value in the map\n    class value_type {\n    public:\n        server_thread& thread() {\n            __TBB_ASSERT( my_thread, \"thread_map::value_type::thread() called when !my_thread\" );\n            return *my_thread;\n        }\n        rml::job& job() {\n            __TBB_ASSERT( my_job, \"thread_map::value_type::job() called when !my_job\" );\n            return *my_job;\n        }\n        value_type() : my_thread(NULL), my_job(NULL) {}\n        server_thread& wait_for_thread() const {\n            for(;;) {\n                server_thread* ptr=const_cast<server_thread*volatile&>(my_thread);\n                if( ptr )\n                    return *ptr;\n                __TBB_Yield();\n            }\n        }\n        /** Shortly after when a connection is established, it is possible for the server\n            to grab a server_thread that has not yet created a job object for that server. */\n        rml::job& wait_for_job() const {\n            if( !my_job ) {\n                my_job = &my_automaton.wait_for_job();\n            }\n            return *my_job;\n        }\n    private:\n        server_thread* my_thread;\n        /** Marked mutable because though it is physically modified, conceptually it is a duplicate of\n            the job held by job_automaton. */\n        mutable rml::job* my_job;\n        job_automaton my_automaton;\n        // FIXME - pad out to cache line, because my_automaton is hit hard by thread()\n        friend class thread_map;\n    };\n    typedef tbb::concurrent_vector<value_type,tbb::zero_allocator<value_type,tbb::cache_aligned_allocator> > array_type;\n};\n#endif /* !RML_USE_WCRM */\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    // Suppress overzealous compiler warnings about uninstantiable class\n    #pragma warning(push)\n    #pragma warning(disable:4510 4610)\n#endif\n\ntemplate<typename T>\nclass padded: public T {\n    char pad[cache_line_size - sizeof(T)%cache_line_size];\n};\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    #pragma warning(pop)\n#endif\n\n// FIXME - should we pad out memory to avoid false sharing of our global variables?\nstatic unsigned the_default_concurrency;\nstatic tbb::atomic<int> the_balance;\nstatic tbb::atomic<tbb::internal::do_once_state> rml_module_state;\n\n#if !RML_USE_WCRM\n//! Per thread information\n/** ref_count holds number of clients that are using this,\n    plus 1 if a host thread owns this instance. */\nclass server_thread: public ref_count {\n    friend class thread_map;\n    template<typename Server, typename Client> friend class generic_connection;\n    friend class tbb_connection_v2;\n    friend class omp_connection_v2;\n    //! Integral type that can hold a thread_state_t\n    typedef int thread_state_rep_t;\n    tbb::atomic<thread_state_rep_t> state;\npublic:\n    thread_monitor monitor;\nprivate:\n    bool    is_omp_thread;\n    tbb::atomic<thread_state_rep_t> my_extra_state;\n    server_thread* link;\n    thread_map_base::array_type::iterator my_map_pos;\n    rml::server *my_conn;\n    rml::job* my_job;\n    job_automaton* my_ja;\n    size_t my_index;\n    tbb::atomic<bool> terminate;\n    omp_dispatch_type omp_dispatch;\n\n#if TBB_USE_ASSERT\n    //! Flag used to check if thread is still using *this.\n    bool has_active_thread;\n#endif /* TBB_USE_ASSERT */\n\n    //! Volunteer to sleep.\n    void sleep_perhaps( thread_state_t asleep );\n\n    //! Destroy job corresponding to given client\n    /** Return true if thread must quit. */\n    template<typename Connection>\n    bool destroy_job( Connection& c );\n\n    //! Do terminate the thread\n    /** Return true if thread must quit. */\n    bool do_termination();\n\n    void loop();\n    static __RML_DECL_THREAD_ROUTINE thread_routine( void* arg );\n\npublic:\n    server_thread();\n\n    ~server_thread();\n\n    //! Read the thread state\n    thread_state_t read_state() const {\n        thread_state_rep_t s = state;\n        __TBB_ASSERT( unsigned(s)<=unsigned(ts_done), \"corrupted server thread?\" );\n        return thread_state_t(s);\n    }\n\n    //! Read the tbb-specific extra thread state\n    thread_state_t read_extra_state() const {\n        thread_state_rep_t s = my_extra_state;\n        return thread_state_t(s);\n    }\n\n    //! Launch a thread that is bound to *this.\n    void launch( size_t stack_size );\n\n    //! Attempt to wakeup a thread\n    /** The value \"to\" is the new state for the thread, if it was woken up.\n        Returns true if thread was woken up, false otherwise. */\n    bool wakeup( thread_state_t to, thread_state_t from );\n\n    //! Attempt to enslave a thread for OpenMP/TBB.\n    /** Returns true if state is successfully changed.  's' takes either ts_omp_busy or ts_tbb_busy */\n    bool try_grab_for( thread_state_t s );\n\n#if _WIN32||_WIN64\n    //! Send the worker thread to sleep temporarily\n    void deactivate();\n\n    //! Wake the worker thread up\n    void reactivate();\n#endif /* _WIN32||_WIN64 */\n};\n\n//! Bag of threads that are private to a client.\nclass private_thread_bag {\n    struct list_thread: server_thread {\n       list_thread* next;\n    };\n    //! Root of atomic linked list of list_thread\n    /** ABA problem is avoided because items are only atomically pushed, never popped. */\n    tbb::atomic<list_thread*> my_root;\n    tbb::cache_aligned_allocator<padded<list_thread> > my_allocator;\npublic:\n    //! Construct empty bag\n    private_thread_bag() {my_root=NULL;}\n\n    //! Create a fresh server_thread object.\n    server_thread& add_one_thread() {\n        list_thread* t = my_allocator.allocate(1);\n        new( t ) list_thread;\n        // Atomically add to list\n        list_thread* old_root;\n        do {\n            old_root = my_root;\n            t->next = old_root;\n        } while( my_root.compare_and_swap( t, old_root )!=old_root );\n        return *t;\n    }\n\n    //! Destroy the bag and threads in it.\n    ~private_thread_bag() {\n        while( my_root ) {\n            // Unlink thread from list.\n            list_thread* t = my_root;\n            my_root = t->next;\n            // Destroy and deallocate the thread.\n            t->~list_thread();\n            my_allocator.deallocate(static_cast<padded<list_thread>*>(t),1);\n        }\n    }\n};\n\n//! Forward declaration\nvoid wakeup_some_tbb_threads();\n\n//! Type-independent part of class generic_connection.\n/** One to one map from server threads to jobs, and associated reference counting. */\nclass thread_map : public thread_map_base {\npublic:\n    typedef rml::client::size_type size_type;\n    //! ctor\n    thread_map( wait_counter& fc, ::rml::client& client ) :\n        all_visited_at_least_once(false), my_min_stack_size(0), my_server_ref_count(1),\n        my_client_ref_count(1), my_client(client), my_factory_counter(fc)\n    { my_unrealized_threads = 0; }\n    //! dtor\n    ~thread_map() {}\n    typedef array_type::iterator iterator;\n    iterator begin() {return my_array.begin();}\n    iterator end() {return my_array.end();}\n    void bind();\n    void unbind();\n    void assist_cleanup( bool assist_null_only );\n\n    /** Returns number of unrealized threads to create. */\n    size_type wakeup_tbb_threads( size_type n );\n    bool wakeup_next_thread( iterator i, tbb_connection_v2& conn );\n    void release_tbb_threads( server_thread* t );\n    void adjust_balance( int delta );\n\n    //! Add a server_thread object to the map, but do not bind it.\n    /** Return NULL if out of unrealized threads. */\n    value_type* add_one_thread( bool is_omp_thread_ );\n\n    void bind_one_thread( rml::server& server, value_type& x );\n\n    void remove_client_ref();\n    int add_server_ref() {return my_server_ref_count.add_ref();}\n    int remove_server_ref() {return my_server_ref_count.remove_ref();}\n\n    ::rml::client& client() const {return my_client;}\n\n    size_type get_unrealized_threads() { return my_unrealized_threads; }\n\nprivate:\n    private_thread_bag my_private_threads;\n    bool all_visited_at_least_once;\n    array_type my_array;\n    size_t my_min_stack_size;\n    tbb::atomic<size_type> my_unrealized_threads;\n\n    //! Number of threads referencing *this, plus one extra.\n    /** When it becomes zero, the containing server object can be safely deleted. */\n    ref_count my_server_ref_count;\n\n    //! Number of jobs that need cleanup, plus one extra.\n    /** When it becomes zero, acknowledge_close_connection is called. */\n    ref_count my_client_ref_count;\n\n    ::rml::client& my_client;\n    //! Counter owned by factory that produced this thread_map.\n    wait_counter& my_factory_counter;\n};\n\nvoid thread_map::bind_one_thread( rml::server& server, value_type& x ) {\n    // Add one to account for the thread referencing this map hereforth.\n    server_thread& t = x.thread();\n    my_server_ref_count.add_ref();\n    my_client_ref_count.add_ref();\n#if TBB_USE_ASSERT\n    __TBB_ASSERT( t.add_ref()==1, NULL );\n#else\n    t.add_ref();\n#endif\n    // Have responsibility to start the thread.\n    t.my_conn = &server;\n    t.my_ja = &x.my_automaton;\n    t.launch( my_min_stack_size );\n    /* Must wake thread up so it can fill in its \"my_job\" field in *this.\n       Otherwise deadlock can occur where wait_for_job spins on thread that is sleeping. */\n    __TBB_ASSERT( t.state!=ts_tbb_busy, NULL );\n    t.wakeup( ts_idle, ts_asleep );\n}\n\nthread_map::value_type* thread_map::add_one_thread( bool is_omp_thread_ ) {\n    size_type u;\n    do {\n        u = my_unrealized_threads;\n        if( !u ) return NULL;\n    } while( my_unrealized_threads.compare_and_swap(u-1,u)!=u );\n    server_thread& t = my_private_threads.add_one_thread();\n    t.is_omp_thread = is_omp_thread_;\n    __TBB_ASSERT( u>=1, NULL );\n    t.my_index = u - 1;\n    __TBB_ASSERT( t.state!=ts_tbb_busy, NULL );\n    t.my_extra_state = t.is_omp_thread ? ts_none : ts_created;\n\n    iterator i = t.my_map_pos = my_array.grow_by(1);\n    value_type& v = *i;\n    v.my_thread = &t;\n    return &v;\n}\n\nvoid thread_map::bind() {\n    ++my_factory_counter;\n    my_min_stack_size = my_client.min_stack_size();\n    __TBB_ASSERT( my_unrealized_threads==0, \"already called bind?\" );\n    my_unrealized_threads = my_client.max_job_count();\n}\n\nvoid thread_map::unbind() {\n    // Ask each server_thread to cleanup its job for this server.\n    for( iterator i=begin(); i!=end(); ++i ) {\n        server_thread& t = i->thread();\n        t.terminate = true;\n        t.wakeup( ts_idle, ts_asleep );\n    }\n    // Remove extra ref to client.\n    remove_client_ref();\n}\n\nvoid thread_map::assist_cleanup( bool assist_null_only ) {\n    // To avoid deadlock, the current thread *must* help out with cleanups that have not started,\n    // because the thread that created the job may be busy for a long time.\n    for( iterator i = begin(); i!=end(); ++i ) {\n        rml::job* j=0;\n        job_automaton& ja = i->my_automaton;\n        if( assist_null_only ? ja.try_plug_null() : ja.try_plug(j) ) {\n            if( j ) {\n                my_client.cleanup(*j);\n            } else {\n                // server thread did not get a chance to create a job.\n            }\n            remove_client_ref();\n        }\n    }\n}\n\nthread_map::size_type thread_map::wakeup_tbb_threads( size_type n ) {\n    __TBB_ASSERT(n>0,\"must specify positive number of threads to wake up\");\n    iterator e = end();\n    for( iterator k=begin(); k!=e; ++k ) {\n        // If another thread added *k, there is a tiny timing window where thread() is invalid.\n        server_thread& t = k->wait_for_thread();\n        thread_state_t thr_s = t.read_state();\n        if( t.read_extra_state()==ts_created || thr_s==ts_tbb_busy || thr_s==ts_done )\n            continue;\n        if( --the_balance>=0 ) { // try to withdraw a coin from the deposit\n            while( !t.try_grab_for( ts_tbb_busy ) ) {\n                thr_s = t.read_state();\n                if( thr_s==ts_tbb_busy || thr_s==ts_done ) {\n                    // we lost; move on to the next.\n                    ++the_balance;\n                    goto skip;\n                }\n            }\n            if( --n==0 )\n                return 0;\n        } else {\n            // overdraft.\n            ++the_balance;\n            break;\n        }\nskip:\n        ;\n    }\n    return n<my_unrealized_threads ? n : my_unrealized_threads;\n}\n#else /* RML_USE_WCRM */\n\nclass thread_map : no_copy {\n    friend class omp_connection_v2;\n    typedef ::std::hash_map<uintptr_t,server_thread*> hash_map_type;\n    size_t my_min_stack_size;\n    size_t my_unrealized_threads;\n    ::rml::client& my_client;\n    //! Counter owned by factory that produced this thread_map.\n    wait_counter& my_factory_counter;\n    //! Ref counters\n    ref_count my_server_ref_count;\n    ref_count my_client_ref_count;\n    // FIXME: pad this?\n    hash_map_type my_map;\n    bool shutdown_in_progress;\n    std::vector<IExecutionResource*> original_exec_resources;\n    tbb::cache_aligned_allocator<padded<tbb_server_thread> > my_tbb_allocator;\n    tbb::cache_aligned_allocator<padded<omp_server_thread> > my_omp_allocator;\n    tbb::cache_aligned_allocator<padded<thread_scavenger_thread> > my_scavenger_allocator;\n    IResourceManager* my_concrt_resource_manager;\n    IScheduler* my_scheduler;\n    ISchedulerProxy* my_scheduler_proxy;\n    tbb::atomic<thread_scavenger_thread*> my_thread_scavenger_thread;\n#if TBB_USE_ASSERT\n    tbb::atomic<int> n_add_vp_requests;\n    tbb::atomic<int> n_thread_scavengers_created;\n#endif\npublic:\n    thread_map( wait_counter& fc, ::rml::client& client ) :\n        my_min_stack_size(0), my_client(client), my_factory_counter(fc),\n        my_server_ref_count(1), my_client_ref_count(1), shutdown_in_progress(false),\n        my_concrt_resource_manager(NULL), my_scheduler(NULL), my_scheduler_proxy(NULL)\n    {\n        my_thread_scavenger_thread = NULL;\n#if TBB_USE_ASSERT\n        n_add_vp_requests = 0;\n        n_thread_scavengers_created;\n#endif\n    }\n\n    ~thread_map() {\n        __TBB_ASSERT( n_thread_scavengers_created<=1, \"too many scavenger thread created\" );\n        // if thread_scavenger_thread is launched, wait for it to complete\n        if( my_thread_scavenger_thread ) {\n            __TBB_ASSERT( my_thread_scavenger_thread!=c_claimed, NULL );\n            while( my_thread_scavenger_thread->read_state()==ts_busy )\n                __TBB_Yield();\n            thread_scavenger_thread* tst = my_thread_scavenger_thread;\n            my_scavenger_allocator.deallocate(static_cast<padded<thread_scavenger_thread>*>(tst),1);\n        }\n        // deallocate thread contexts\n        for( hash_map_type::const_iterator hi=my_map.begin(); hi!=my_map.end(); ++hi ) {\n            server_thread* thr = hi->second;\n            if( thr->tbb_thread ) {\n                while( ((tbb_server_thread*)thr)->activation_count>1 )\n                    __TBB_Yield();\n                ((tbb_server_thread*)thr)->~tbb_server_thread();\n                my_tbb_allocator.deallocate(static_cast<padded<tbb_server_thread>*>(thr),1);\n            } else {\n                ((omp_server_thread*)thr)->~omp_server_thread();\n                my_omp_allocator.deallocate(static_cast<padded<omp_server_thread>*>(thr),1);\n            }\n        }\n        if( my_scheduler_proxy ) {\n            my_scheduler_proxy->Shutdown();\n            my_concrt_resource_manager->Release();\n            __TBB_ASSERT( my_scheduler, NULL );\n            delete my_scheduler;\n        } else {\n            __TBB_ASSERT( !my_scheduler, NULL );\n        }\n    }\n    typedef hash_map_type::key_type key_type;\n    typedef hash_map_type::value_type value_type;\n    typedef hash_map_type::iterator iterator;\n    iterator begin() {return my_map.begin();}\n    iterator end() {return my_map.end();}\n    iterator find( key_type k ) {return my_map.find( k );}\n    iterator insert( key_type k, server_thread* v ) {\n        std::pair<iterator,bool> res = my_map.insert( value_type(k,v) );\n        return res.first;\n    }\n    void bind( IScheduler* s ) {\n        ++my_factory_counter;\n        if( s ) {\n            my_unrealized_threads = s->GetPolicy().GetPolicyValue( MaxConcurrency );\n            __TBB_ASSERT( my_unrealized_threads>0, NULL );\n            my_scheduler = s;\n            my_concrt_resource_manager = CreateResourceManager(); // reference count==3 when first created.\n            my_scheduler_proxy = my_concrt_resource_manager->RegisterScheduler( s, CONCRT_RM_VERSION_1 );\n            my_scheduler_proxy->RequestInitialVirtualProcessors( false );\n        }\n    }\n    bool is_closing() { return shutdown_in_progress; }\n    void unbind( rml::server& server, ::tbb::spin_mutex& mtx );\n    void add_client_ref() { my_server_ref_count.add_ref(); }\n    void remove_client_ref();\n    void add_server_ref() {my_server_ref_count.add_ref();}\n    int remove_server_ref() {return my_server_ref_count.remove_ref();}\n    int get_server_ref_count() { int k = my_server_ref_count.my_ref_count; return k; }\n    void assist_cleanup( bool assist_null_only );\n    void adjust_balance( int delta );\n    int current_balance() const {int k = the_balance; return k;}\n    ::rml::client& client() const {return my_client;}\n    void register_as_master( server::execution_resource_t& v ) const { (IExecutionResource*&)v = my_scheduler_proxy ? my_scheduler_proxy->SubscribeCurrentThread() : NULL; }\n    // Rremove() should be called from the same thread that subscribed the current h/w thread (i.e., the one that\n    // called register_as_master() ).\n    void unregister( server::execution_resource_t v ) const {if( v ) ((IExecutionResource*)v)->Remove( my_scheduler );}\n    void add_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count, tbb_connection_v2& conn, ::tbb::spin_mutex& mtx );\n    void add_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count, omp_connection_v2& conn, ::tbb::spin_mutex& mtx );\n    void remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx );\n    void mark_virtual_processors_as_lent( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx );\n    void create_oversubscribers( unsigned n, std::vector<server_thread*>& thr_vec, omp_connection_v2& conn, ::tbb::spin_mutex& mtx );\n    void wakeup_tbb_threads( int c, ::tbb::spin_mutex& mtx );\n    void mark_virtual_processors_as_returned( IVirtualProcessorRoot** vprocs, unsigned int count, tbb::spin_mutex& mtx );\n    inline void addto_original_exec_resources( IExecutionResource* r, ::tbb::spin_mutex& mtx ) {\n        ::tbb::spin_mutex::scoped_lock lck(mtx);\n        __TBB_ASSERT( !is_closing(), \"trying to register master while connection is being shutdown?\" );\n        original_exec_resources.push_back( r );\n    }\n#if !__RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\n    void allocate_thread_scavenger( IExecutionResource* v );\n#endif\n    inline thread_scavenger_thread* get_thread_scavenger() { return my_thread_scavenger_thread; }\n};\n\ngarbage_connection_queue connections_to_reclaim;\nconnection_scavenger_thread connection_scavenger;\n\n#endif /* !RML_USE_WCRM */\n\n//------------------------------------------------------------------------\n// generic_connection\n//------------------------------------------------------------------------\n\ntemplate<typename Server, typename Client>\nstruct connection_traits {};\n\n// head of the active tbb connections\nstatic tbb::atomic<uintptr_t> active_tbb_connections;\nstatic tbb::atomic<int> current_tbb_conn_readers;\nstatic size_t current_tbb_conn_reader_epoch;\nstatic tbb::atomic<size_t> close_tbb_connection_event_count;\n\n#if RML_USE_WCRM\ntemplate<typename Connection>\nvoid make_job( Connection& c, server_thread& t );\n#endif\n\ntemplate<typename Server, typename Client>\nclass generic_connection: public Server, no_copy {\n    /*override*/ version_type version() const {return SERVER_VERSION;}\n    /*override*/ void yield() {thread_monitor::yield();}\n    /*override*/ void independent_thread_number_changed( int delta ) { my_thread_map.adjust_balance( -delta ); }\n    /*override*/ unsigned default_concurrency() const { return the_default_concurrency; }\n    friend void wakeup_some_tbb_threads();\n    friend class connection_scavenger_thread;\n\nprotected:\n    thread_map my_thread_map;\n    generic_connection* next_conn;\n    size_t my_ec;\n#if RML_USE_WCRM\n    // FIXME: pad it?\n    tbb::spin_mutex map_mtx;\n    IScheduler* my_scheduler;\n    void do_open( IScheduler* s ) {\n        my_scheduler = s;\n        my_thread_map.bind( s );\n    }\n    bool is_closing() { return my_thread_map.is_closing(); }\n    void request_close_connection( bool existing );\n#else\n    void do_open() {my_thread_map.bind();}\n    void request_close_connection( bool );\n#endif /* RML_USE_WCRM */\n    //! Make destructor virtual\n    virtual ~generic_connection() {}\n#if !RML_USE_WCRM\n    generic_connection( wait_counter& fc, Client& c ) : my_thread_map(fc,c), next_conn(NULL), my_ec(0) {}\n#else\n    generic_connection( wait_counter& fc, Client& c ) :\n            my_thread_map(fc,c), next_conn(NULL), my_ec(0), map_mtx(), my_scheduler(NULL) {}\n    void add_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count );\n    void remove_virtual_processors( IVirtualProcessorRoot** vprocs, unsigned int count );\n    void notify_resources_externally_busy( IVirtualProcessorRoot** vprocs, unsigned int count ) { my_thread_map.mark_virtual_processors_as_lent( vprocs, count, map_mtx ); }\n    void notify_resources_externally_idle( IVirtualProcessorRoot** vprocs, unsigned int count ) {\n        my_thread_map.mark_virtual_processors_as_returned( vprocs, count, map_mtx );\n    }\n#endif /* !RML_USE_WCRM */\n\npublic:\n    typedef Server server_type;\n    typedef Client client_type;\n    Client& client() const {return static_cast<Client&>(my_thread_map.client());}\n    void set_scratch_ptr( job& j, void* ptr ) { ::rml::server::scratch_ptr(j) = ptr; }\n#if RML_USE_WCRM\n    template<typename Connection>\n    friend void make_job( Connection& c, server_thread& t );\n    void add_server_ref ()   {my_thread_map.add_server_ref();}\n    void remove_server_ref() {if( my_thread_map.remove_server_ref()==0 ) delete this;}\n    void add_client_ref ()   {my_thread_map.add_client_ref();}\n    void remove_client_ref() {my_thread_map.remove_client_ref();}\n#else /* !RML_USE_WCRM */\n    int  add_server_ref ()   {return my_thread_map.add_server_ref();}\n    void remove_server_ref() {if( my_thread_map.remove_server_ref()==0 ) delete this;}\n    void remove_client_ref() {my_thread_map.remove_client_ref();}\n    void make_job( server_thread& t, job_automaton& ja );\n#endif /* RML_USE_WCRM */\n    static generic_connection* get_addr( uintptr_t addr_ex ) {\n        return reinterpret_cast<generic_connection*>( addr_ex&~(uintptr_t)3 );\n    }\n};\n\n//------------------------------------------------------------------------\n// TBB server\n//------------------------------------------------------------------------\n\ntemplate<>\nstruct connection_traits<tbb_server,tbb_client> {\n    static const bool assist_null_only = true;\n    static const bool is_tbb = true;\n};\n\n//! Represents a server/client binding.\n/** The internal representation uses inheritance for the server part and a pointer for the client part. */\nclass tbb_connection_v2: public generic_connection<tbb_server,tbb_client> {\n    /*override*/ void adjust_job_count_estimate( int delta );\n#if !RML_USE_WCRM\n#if _WIN32||_WIN64\n    /*override*/ void register_master ( rml::server::execution_resource_t& /*v*/ ) {}\n    /*override*/ void unregister_master ( rml::server::execution_resource_t /*v*/ ) {}\n#endif\n#else\n    /*override*/ void register_master ( rml::server::execution_resource_t& v ) {\n        my_thread_map.register_as_master(v);\n        if( v ) ++nesting;\n    }\n    /*override*/ void unregister_master ( rml::server::execution_resource_t v ) {\n        if( v ) {\n            __TBB_ASSERT( nesting>0, NULL );\n            if( --nesting==0 ) {\n#if !__RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\n                my_thread_map.allocate_thread_scavenger( (IExecutionResource*)v );\n#endif\n            }\n        }\n        my_thread_map.unregister(v);\n    }\n    IScheduler* create_scheduler() {return( scheduler<tbb_connection_v2>::create( *this ) );}\n    friend void  free_all_connections( uintptr_t );\n    friend class scheduler<tbb_connection_v2>;\n    friend class execution_context;\n    friend class connection_scavenger_thread;\n#endif /* RML_USE_WCRM */\n    friend void wakeup_some_tbb_threads();\n    //! Estimate on number of jobs without threads working on them.\n    tbb::atomic<int> my_slack;\n    friend class dummy_class_to_shut_up_gratuitous_warning_from_gcc_3_2_3;\n#if TBB_USE_ASSERT\n    tbb::atomic<int> my_job_count_estimate;\n#endif /* TBB_USE_ASSERT */\n\n    tbb::atomic<int> n_adjust_job_count_requests;\n#if RML_USE_WCRM\n    tbb::atomic<int> nesting;\n#endif\n\n    // dtor\n    ~tbb_connection_v2();\n\npublic:\n#if RML_USE_WCRM\n    typedef tbb_server_thread server_thread_type;\n#endif\n    //! True if there is slack that try_process can use.\n    bool has_slack() const {return my_slack>0;}\n\n#if RML_USE_WCRM\n    bool try_process( job& job )\n#else\n    bool try_process( server_thread& t, job& job )\n#endif\n    {\n        bool visited = false;\n        // No check for my_slack>0 here because caller is expected to do that check.\n        int k = --my_slack;\n        if( k>=0 ) {\n#if !RML_USE_WCRM\n            t.my_extra_state = ts_visited; // remember the thread paid a trip to process() at least once\n#endif\n            client().process(job);\n            visited = true;\n        }\n        ++my_slack;\n        return visited;\n    }\n\n    tbb_connection_v2( wait_counter& fc, tbb_client& client ) : generic_connection<tbb_server,tbb_client>(fc,client)\n    {\n        my_slack = 0;\n#if RML_USE_WCRM\n        nesting = 0;\n#endif\n#if TBB_USE_ASSERT\n        my_job_count_estimate = 0;\n#endif /* TBB_USE_ASSERT */\n        __TBB_ASSERT( !my_slack, NULL );\n\n#if RML_USE_WCRM\n        do_open( client.max_job_count()>0 ? create_scheduler() : NULL );\n#else\n        do_open();\n#endif /* !RML_USE_WCRM */\n        n_adjust_job_count_requests = 0;\n\n        // Acquire head of active_tbb_connections & push the connection into the list\n        uintptr_t conn;\n        do {\n            for( ; (conn=active_tbb_connections)&1; )\n                __TBB_Yield();\n        } while( active_tbb_connections.compare_and_swap( conn|1, conn )!=conn );\n\n        this->next_conn = generic_connection<tbb_server,tbb_client>::get_addr(conn);\n        // Update and release head of active_tbb_connections\n        active_tbb_connections = (uintptr_t) this; // set and release\n    }\n    inline void wakeup_tbb_threads( unsigned n ) {\n        my_thread_map.wakeup_tbb_threads( n\n#if RML_USE_WCRM\n                , map_mtx\n#endif\n                );\n    }\n#if RML_USE_WCRM\n    inline int get_nesting_level() { return nesting; }\n#else\n    inline bool wakeup_next_thread( thread_map::iterator i ) {return my_thread_map.wakeup_next_thread( i, *this );}\n    inline thread_map::size_type get_unrealized_threads () {return my_thread_map.get_unrealized_threads();}\n#endif /* !RML_USE_WCRM */\n};\n\n//------------------------------------------------------------------------\n// OpenMP server\n//------------------------------------------------------------------------\n\ntemplate<>\nstruct connection_traits<omp_server,omp_client> {\n    static const bool assist_null_only = false;\n    static const bool is_tbb = false;\n};\n\nclass omp_connection_v2: public generic_connection<omp_server,omp_client> {\n#if !RML_USE_WCRM\n    /*override*/ int  current_balance() const {return the_balance;}\n#else\n    friend void  free_all_connections( uintptr_t );\n    friend class scheduler<omp_connection_v2>;\n    /*override*/ int current_balance() const {return my_thread_map.current_balance();}\n#endif /* !RML_USE_WCRM */\n    /*override*/ int  try_increase_load( size_type n, bool strict );\n    /*override*/ void decrease_load( size_type n );\n    /*override*/ void get_threads( size_type request_size, void* cookie, job* array[] );\n#if !RML_USE_WCRM\n#if _WIN32||_WIN64\n    /*override*/ void register_master ( rml::server::execution_resource_t& /*v*/ ) {}\n    /*override*/ void unregister_master ( rml::server::execution_resource_t /*v*/ ) {}\n#endif\n#else\n    /*override*/ void register_master ( rml::server::execution_resource_t& v ) {\n        my_thread_map.register_as_master( v );\n        my_thread_map.addto_original_exec_resources( (IExecutionResource*)v, map_mtx );\n    }\n    /*override*/ void unregister_master ( rml::server::execution_resource_t v ) { my_thread_map.unregister(v); }\n#endif /* !RML_USE_WCRM */\n#if _WIN32||_WIN64\n    /*override*/ void deactivate( rml::job* j );\n    /*override*/ void reactivate( rml::job* j );\n#endif /* _WIN32||_WIN64 */\n#if RML_USE_WCRM\npublic:\n    typedef omp_server_thread server_thread_type;\nprivate:\n    IScheduler* create_scheduler() {return( scheduler<omp_connection_v2>::create( *this ) );}\n#endif /* RML_USE_WCRM */\npublic:\n#if TBB_USE_ASSERT\n    //! Net change in delta caused by this connection.\n    /** Should be zero when connection is broken */\n    tbb::atomic<int> net_delta;\n#endif /* TBB_USE_ASSERT */\n\n    omp_connection_v2( wait_counter& fc, omp_client& client ) : generic_connection<omp_server,omp_client>(fc,client) {\n#if TBB_USE_ASSERT\n        net_delta = 0;\n#endif /* TBB_USE_ASSERT */\n#if RML_USE_WCRM\n        do_open( create_scheduler() );\n#else\n        do_open();\n#endif /* RML_USE_WCRM */\n    }\n    ~omp_connection_v2() {__TBB_ASSERT( net_delta==0, \"net increase/decrease of load is nonzero\" );}\n};\n\n#if !RML_USE_WCRM\n/* to deal with cases where the machine is oversubscribed; we want each thread to trip to try_process() at least once */\n/* this should not involve computing the_balance */\nbool thread_map::wakeup_next_thread( thread_map::iterator this_thr, tbb_connection_v2& conn ) {\n    if( all_visited_at_least_once )\n        return false;\n\n    iterator e = end();\nretry:\n    bool exist = false;\n    iterator k=this_thr;\n    for( ++k; k!=e; ++k ) {\n        // If another thread added *k, there is a tiny timing window where thread() is invalid.\n        server_thread& t = k->wait_for_thread();\n        if( t.my_extra_state!=ts_visited )\n            exist = true;\n        if( t.read_state()!=ts_tbb_busy && t.my_extra_state==ts_started )\n            if( t.try_grab_for( ts_tbb_busy ) )\n                return true;\n    }\n    for( k=begin(); k!=this_thr; ++k ) {\n        server_thread& t = k->wait_for_thread();\n        if( t.my_extra_state!=ts_visited )\n            exist = true;\n        if( t.read_state()!=ts_tbb_busy && t.my_extra_state==ts_started )\n            if( t.try_grab_for( ts_tbb_busy ) )\n                return true;\n    }\n\n    if( exist )\n        if( conn.has_slack() )\n            goto retry;\n    else\n        all_visited_at_least_once = true;\n    return false;\n}\n\nvoid thread_map::release_tbb_threads( server_thread* t ) {\n    for( ; t; t = t->link ) {\n        while( t->read_state()!=ts_asleep )\n            __TBB_Yield();\n        t->my_extra_state = ts_started;\n    }\n}\n#endif /* !RML_USE_WCRM */\n\nvoid thread_map::adjust_balance( int delta ) {\n    int new_balance = the_balance += delta;\n    if( new_balance>0 && 0>=new_balance-delta /*== old the_balance*/ )\n        wakeup_some_tbb_threads();\n}\n\nvoid thread_map::remove_client_ref() {\n    int k = my_client_ref_count.remove_ref();\n    if( k==0 ) {\n        // Notify factory that thread has crossed back into RML.\n        --my_factory_counter;\n        // Notify client that RML is done with the client object.\n        my_client.acknowledge_close_connection();\n    }\n}\n\n#if RML_USE_WCRM\n/** Not a member of generic_connection because we need Connection to be the derived class. */\ntemplate<typename Connection>\nvoid make_job( Connection& c, typename Connection::server_thread_type& t ) {\n    if( t.my_job_automaton.try_acquire() ) {\n        rml::job& j = *t.my_client.create_one_job();\n        __TBB_ASSERT( &j!=NULL, \"client:::create_one_job returned NULL\" );\n        __TBB_ASSERT( (intptr_t(&j)&1)==0, \"client::create_one_job returned misaligned job\" );\n        t.my_job_automaton.set_and_release( j );\n        c.set_scratch_ptr( j, (void*) &t );\n    }\n}\n#endif /* RML_USE_WCRM */\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n// Suppress \"conditional expression is constant\" warning.\n#pragma warning( push )\n#pragma warning( disable: 4127 )\n#endif\n#if RML_USE_WCRM\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::request_close_connection( bool exiting ) {\n    // for TBB connections, exiting should always be false\n    if( connection_traits<Server,Client>::is_tbb )\n        __TBB_ASSERT( !exiting, NULL);\n#if TBB_USE_ASSERT\n    else if( exiting )\n        reinterpret_cast<omp_connection_v2*>(this)->net_delta = 0;\n#endif\n    if( exiting ) {\n        uintptr_t tail = connections_to_reclaim.tail;\n        while( connections_to_reclaim.tail.compare_and_swap( garbage_connection_queue::plugged, tail )!=tail )\n            __TBB_Yield();\n        my_thread_map.unbind( *this, map_mtx );\n        my_thread_map.assist_cleanup( connection_traits<Server,Client>::assist_null_only );\n        // It is assumed that the client waits for all other threads to terminate before\n        // calling request_close_connection with true.  Thus, it is safe to return all\n        // outstanding connection objects that are reachable. It is possible that there may\n        // be some unreachable connection objects lying somewhere.\n        free_all_connections( connection_scavenger.grab_and_prepend( this ) );\n        return;\n    }\n#else /* !RML_USE_WCRM */\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::request_close_connection( bool ) {\n#endif /* RML_USE_WCRM */\n    if( connection_traits<Server,Client>::is_tbb ) {\n        // acquire the head of active tbb connections\n        uintptr_t conn;\n        do {\n            for( ; (conn=active_tbb_connections)&1; )\n                __TBB_Yield();\n        } while( active_tbb_connections.compare_and_swap( conn|1, conn )!=conn );\n\n        // Locate the current connection\n        generic_connection* pred_conn = NULL;\n        generic_connection* curr_conn = (generic_connection*) conn;\n        for( ; curr_conn && curr_conn!=this; curr_conn=curr_conn->next_conn )\n            pred_conn = curr_conn;\n        __TBB_ASSERT( curr_conn==this, \"the current connection is not in the list?\" );\n\n        // Remove this from the list\n        if( pred_conn ) {\n            pred_conn->next_conn = curr_conn->next_conn;\n            active_tbb_connections = reinterpret_cast<uintptr_t>(generic_connection<tbb_server,tbb_client>::get_addr(active_tbb_connections)); // release it\n        } else\n            active_tbb_connections = (uintptr_t) curr_conn->next_conn; // update & release it\n        curr_conn->next_conn = NULL;\n        // Increment the tbb connection close event count\n        my_ec = ++close_tbb_connection_event_count;\n        // Wait happens in tbb_connection_v2::~tbb_connection_v2()\n    }\n#if RML_USE_WCRM\n    my_thread_map.unbind( *this, map_mtx );\n    my_thread_map.assist_cleanup( connection_traits<Server,Client>::assist_null_only );\n    connection_scavenger.add_request( this );\n#else\n    my_thread_map.unbind();\n    my_thread_map.assist_cleanup( connection_traits<Server,Client>::assist_null_only );\n    // Remove extra reference\n    remove_server_ref();\n#endif\n}\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n#pragma warning( pop )\n#endif\n\n#if RML_USE_WCRM\n\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{}\n\ntemplate<>\nvoid generic_connection<tbb_server,tbb_client>::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    my_thread_map.add_virtual_processors( vproots, count, (tbb_connection_v2&)*this, map_mtx );\n}\ntemplate<>\nvoid generic_connection<omp_server,omp_client>::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    // For OMP, since it uses ScheudlerPolicy of MinThreads==MaxThreads, this is called once when\n    // RequestInitialVirtualProcessors() is  called.\n    my_thread_map.add_virtual_processors( vproots, count, (omp_connection_v2&)*this, map_mtx );\n}\n\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    __TBB_ASSERT( false, \"should not be called\" );\n}\n/* For OMP, RemoveVirtualProcessors() will never be called. */\n\ntemplate<>\nvoid generic_connection<tbb_server,tbb_client>::remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    my_thread_map.remove_virtual_processors( vproots, count, map_mtx );\n}\n\nvoid tbb_connection_v2::adjust_job_count_estimate( int delta ) {\n#if TBB_USE_ASSERT\n    my_job_count_estimate += delta;\n#endif /* TBB_USE_ASSERT */\n    // Atomically update slack.\n    int c = my_slack+=delta;\n    if( c>0 ) {\n        ++n_adjust_job_count_requests;\n        my_thread_map.wakeup_tbb_threads( c, map_mtx );\n        --n_adjust_job_count_requests;\n    }\n}\n#endif /* RML_USE_WCRM */\n\ntbb_connection_v2::~tbb_connection_v2() {\n#if TBB_USE_ASSERT\n    if( my_job_count_estimate!=0 ) {\n        fprintf(stderr, \"TBB client tried to disconnect with non-zero net job count estimate of %d\\n\", int(my_job_count_estimate ));\n        abort();\n    }\n    __TBB_ASSERT( !my_slack, \"attempt to destroy tbb_server with nonzero slack\" );\n    __TBB_ASSERT( this!=static_cast<tbb_connection_v2*>(generic_connection<tbb_server,tbb_client >::get_addr(active_tbb_connections)), \"request_close_connection() must be called\" );\n#endif /* TBB_USE_ASSERT */\n#if !RML_USE_WCRM\n    // If there are other threads ready for work, give them coins\n    if( the_balance>0 )\n        wakeup_some_tbb_threads();\n#endif\n    // Someone might be accessing my data members\n    while( current_tbb_conn_readers>0 && (ptrdiff_t)(my_ec-current_tbb_conn_reader_epoch)>0 )\n        __TBB_Yield();\n}\n\n#if !RML_USE_WCRM\ntemplate<typename Server, typename Client>\nvoid generic_connection<Server,Client>::make_job( server_thread& t, job_automaton& ja ) {\n    if( ja.try_acquire() ) {\n        rml::job& j = *client().create_one_job();\n        __TBB_ASSERT( &j!=NULL, \"client:::create_one_job returned NULL\" );\n        __TBB_ASSERT( (intptr_t(&j)&1)==0, \"client::create_one_job returned misaligned job\" );\n        ja.set_and_release( j );\n        __TBB_ASSERT( t.my_conn && t.my_ja && t.my_job==NULL, NULL );\n        t.my_job  = &j;\n        set_scratch_ptr( j, (void*) &t );\n    }\n}\n\nvoid tbb_connection_v2::adjust_job_count_estimate( int delta ) {\n#if TBB_USE_ASSERT\n    my_job_count_estimate += delta;\n#endif /* TBB_USE_ASSERT */\n    // Atomically update slack.\n    int c = my_slack+=delta;\n    if( c>0 ) {\n        ++n_adjust_job_count_requests;\n        // The client has work to do and there are threads available\n        thread_map::size_type n = my_thread_map.wakeup_tbb_threads(c);\n\n        server_thread* new_threads_anchor = NULL;\n        thread_map::size_type i;\n        {\n        tbb::internal::affinity_helper fpa;\n        for( i=0; i<n; ++i ) {\n            // Obtain unrealized threads\n            thread_map::value_type* k = my_thread_map.add_one_thread( false );\n            if( !k )\n                // No unrealized threads left.\n                break;\n            // Eagerly start the thread off.\n            fpa.protect_affinity_mask( /*restore_process_mask=*/true );\n            my_thread_map.bind_one_thread( *this, *k );\n            server_thread& t = k->thread();\n            __TBB_ASSERT( !t.link, NULL );\n            t.link = new_threads_anchor;\n            new_threads_anchor = &t;\n        }\n        // Implicit destruction of fpa resets original affinity mask.\n        }\n\n        thread_map::size_type j=0;\n        for( ; the_balance>0 && j<i; ++j ) {\n            if( --the_balance>=0 ) {\n                // Withdraw a coin from the bank\n                __TBB_ASSERT( new_threads_anchor, NULL );\n\n                server_thread* t = new_threads_anchor;\n                new_threads_anchor = t->link;\n                while( !t->try_grab_for( ts_tbb_busy ) )\n                    __TBB_Yield();\n                t->my_extra_state = ts_started;\n            } else {\n                // Overdraft. return it to the bank\n                ++the_balance;\n                break;\n            }\n        }\n        __TBB_ASSERT( i-j!=0||new_threads_anchor==NULL, NULL );\n        // Mark the ones that did not get started as eligible for being snatched.\n        if( new_threads_anchor )\n            my_thread_map.release_tbb_threads( new_threads_anchor );\n\n        --n_adjust_job_count_requests;\n    }\n}\n#endif /* RML_USE_WCRM */\n\n#if RML_USE_WCRM\nint omp_connection_v2::try_increase_load( size_type n, bool strict ) {\n    __TBB_ASSERT(int(n)>=0,NULL);\n    if( strict ) {\n        the_balance -= int(n);\n    } else {\n        int avail, old;\n        do {\n            avail = the_balance;\n            if( avail<=0 ) {\n                // No atomic read-write-modify operation necessary.\n                return avail;\n            }\n            // Don't read the_system_balance; if it changes, compare_and_swap will fail anyway.\n            old = the_balance.compare_and_swap( int(n)<avail ? avail-n : 0, avail );\n        } while( old!=avail );\n        if( int(n)>avail )\n            n=avail;\n    }\n#if TBB_USE_ASSERT\n    net_delta += n;\n#endif /* TBB_USE_ASSERT */\n    return n;\n}\n\nvoid omp_connection_v2::decrease_load( size_type /*n*/ ) {}\n\nvoid omp_connection_v2::get_threads( size_type request_size, void* cookie, job* array[] ) {\n    unsigned index = 0;\n    std::vector<omp_server_thread*> enlisted(request_size);\n    std::vector<thread_grab_t> to_activate(request_size);\n\n    if( request_size==0 ) return;\n\n    {\n        tbb::spin_mutex::scoped_lock lock(map_mtx);\n\n        __TBB_ASSERT( !is_closing(), \"try to get threads while connection is being shutdown?\" );\n\n        for( int scan=0; scan<2; ++scan ) {\n            for( thread_map::iterator i=my_thread_map.begin(); i!=my_thread_map.end(); ++i ) {\n                omp_server_thread* thr = (omp_server_thread*) (*i).second;\n                // in the first scan, skip VPs that are lent\n                if( scan==0 && thr->is_lent() ) continue;\n                thread_grab_t res = thr->try_grab_for();\n                if( res!=wk_failed ) {// && if is not busy by some other scheduler\n                    to_activate[index] = res;\n                    enlisted[index] = thr;\n                    if( ++index==request_size )\n                        goto activate_threads;\n                }\n            }\n        }\n    }\n\nactivate_threads:\n\n    for( unsigned i=0; i<index; ++i ) {\n        omp_server_thread* thr = enlisted[i];\n        if( to_activate[i]==wk_from_asleep )\n            thr->get_virtual_processor()->Activate( thr );\n        job* j = thr->wait_for_job();\n        array[i] = j;\n        thr->omp_data.produce( client(), *j, cookie, i PRODUCE_ARG(*this) );\n    }\n\n    if( index==request_size )\n        return;\n\n    // If we come to this point, it must be because dynamic==false\n    // Create Oversubscribers..\n\n    // Note that our policy is such that MinConcurrency==MaxConcurrency.\n    // RM will deliver MaxConcurrency of VirtualProcessors and no more.\n    __TBB_ASSERT( request_size>index, NULL );\n    unsigned n = request_size - index;\n    std::vector<server_thread*> thr_vec(n);\n    typedef std::vector<server_thread*>::iterator iterator_thr;\n    my_thread_map.create_oversubscribers( n, thr_vec, *this, map_mtx );\n    for( iterator_thr ti=thr_vec.begin(); ti!=thr_vec.end(); ++ti ) {\n        omp_server_thread* thr = (omp_server_thread*) *ti;\n        __TBB_ASSERT( thr, \"thread not created?\" );\n        // Thread is already grabbed; since it is nrewly created, we need to activate it.\n        thr->get_virtual_processor()->Activate( thr );\n        job* j = thr->wait_for_job();\n        array[index] = j;\n        thr->omp_data.produce( client(), *j, cookie, index PRODUCE_ARG(*this) );\n        ++index;\n    }\n}\n\n#if _WIN32||_WIN64\nvoid omp_connection_v2::deactivate( rml::job* j )\n{\n    my_thread_map.adjust_balance(1);\n#if TBB_USE_ASSERT\n    net_delta -= 1;\n#endif\n    omp_server_thread* thr = (omp_server_thread*) scratch_ptr( *j );\n    (thr->get_virtual_processor())->Deactivate( thr );\n}\n\nvoid omp_connection_v2::reactivate( rml::job* j )\n{\n    // Should not adjust the_balance because OMP client is supposed to\n    // do try_increase_load() to reserve the threads to use.\n    omp_server_thread* thr = (omp_server_thread*) scratch_ptr( *j );\n    (thr->get_virtual_processor())->Activate( thr );\n}\n#endif /* !_WIN32||_WIN64 */\n\n#endif  /* RML_USE_WCRM */\n\n//! Wake up some available tbb threads\nvoid wakeup_some_tbb_threads()\n{\n    /* First, atomically grab the connection, then increase the server ref count to keep\n       it from being released prematurely.  Second, check if the balance is available for TBB\n       and the tbb conneciton has slack to exploit.  If the answer is true, go ahead and\n       try to wake some up. */\n    if( generic_connection<tbb_server,tbb_client >::get_addr(active_tbb_connections)==0 )\n        // the next connection will see the change; return.\n        return;\n\nstart_it_over:\n    int n_curr_readers = ++current_tbb_conn_readers;\n    if( n_curr_readers>1 ) // I lost\n        return;\n    // if n_curr_readers==1, i am the first one, so I will take responsibility for waking tbb threads up.\n\n    // update the current epoch\n    current_tbb_conn_reader_epoch = close_tbb_connection_event_count;\n\n    // read and clear\n    // Newly added connection will not invalidate the pointer, and it will\n    // compete with the current one to claim coins.\n    // One that is about to close the connection increments the event count\n    // after it removes the connection from the list.  But it will keep around\n    // the connection until all readers including this one catch up. So, reading\n    // the head and clearing the lock bit should be o.k.\n    generic_connection<tbb_server,tbb_client>* next_conn_wake_up = generic_connection<tbb_server,tbb_client>::get_addr( active_tbb_connections );\n\n    for( ; next_conn_wake_up; ) {\n        /* some threads are creating tbb server threads; they may not see my changes made to the_balance */\n        /* When a thread is in adjust_job_count_estimate() to increase the slack\n           RML tries to activate worker threads on behalf of the requesting thread\n           by repeatedly drawing a coin from the bank optimistically and grabbing a\n           thread.  If it finds the bank overdrafted, it returns the coin back to\n           the bank and returns the control to the thread (return from the method).\n           There lies a tiny timing hole.\n\n           When the overdraft occurs (note that multiple masters may be in\n           adjust_job_count_estimate() so the_balance can be any negative value) and\n           a worker returns from the TBB work at that moment, its returning the coin\n           does not bump up the_balance over 0, so it happily returns from\n           wakeup_some_tbb_threads() without attempting to give coins to worker threads\n           that are ready.\n        */\n        while( ((tbb_connection_v2*)next_conn_wake_up)->n_adjust_job_count_requests>0 )\n            __TBB_Yield();\n\n        int bal = the_balance;\n        n_curr_readers = current_tbb_conn_readers; // get the snapshot\n        if( bal<=0 ) break;\n        // if the connection is deleted, the following will immediately return because its slack would be 0 or less.\n\n        tbb_connection_v2* tbb_conn = (tbb_connection_v2*)next_conn_wake_up;\n        int my_slack = tbb_conn->my_slack;\n        if( my_slack>0 ) tbb_conn->wakeup_tbb_threads( my_slack );\n        next_conn_wake_up = next_conn_wake_up->next_conn;\n    }\n\n    int delta = current_tbb_conn_readers -= n_curr_readers;\n    //if delta>0, more threads entered the routine since this one took the snapshot\n    if( delta>0 ) {\n        current_tbb_conn_readers = 0;\n        if( the_balance>0 && generic_connection<tbb_server,tbb_client >::get_addr(active_tbb_connections)!=0 )\n            goto start_it_over;\n    }\n\n    // Signal any connection that is waiting for me to complete my access that I am done.\n    current_tbb_conn_reader_epoch = close_tbb_connection_event_count;\n}\n\n#if !RML_USE_WCRM\nint omp_connection_v2::try_increase_load( size_type n, bool strict ) {\n    __TBB_ASSERT(int(n)>=0,NULL);\n    if( strict ) {\n        the_balance -= int(n);\n    } else {\n        int avail, old;\n        do {\n            avail = the_balance;\n            if( avail<=0 ) {\n                // No atomic read-write-modify operation necessary.\n                return avail;\n            }\n            // don't read the_balance; if it changes, compare_and_swap will fail anyway.\n            old = the_balance.compare_and_swap( int(n)<avail ? avail-n : 0, avail );\n        } while( old!=avail );\n        if( int(n)>avail )\n            n=avail;\n    }\n#if TBB_USE_ASSERT\n    net_delta += n;\n#endif /* TBB_USE_ASSERT */\n    return n;\n}\n\nvoid omp_connection_v2::decrease_load( size_type n ) {\n    __TBB_ASSERT(int(n)>=0,NULL);\n    my_thread_map.adjust_balance(int(n));\n#if TBB_USE_ASSERT\n    net_delta -= n;\n#endif /* TBB_USE_ASSERT */\n}\n\nvoid omp_connection_v2::get_threads( size_type request_size, void* cookie, job* array[] ) {\n\n    if( !request_size )\n        return;\n\n    unsigned index = 0;\n    for(;;) { // don't return until all request_size threads are grabbed.\n        // Need to grab some threads\n        thread_map::iterator k_end=my_thread_map.end();\n        for( thread_map::iterator k=my_thread_map.begin(); k!=k_end; ++k ) {\n            // If another thread added *k, there is a tiny timing window where thread() is invalid.\n            server_thread& t = k->wait_for_thread();\n            if( t.try_grab_for( ts_omp_busy ) ) {\n                // The preincrement instead of post-increment of index is deliberate.\n                job& j = k->wait_for_job();\n                array[index] = &j;\n                t.omp_dispatch.produce( client(), j, cookie, index PRODUCE_ARG(*this) );\n                if( ++index==request_size )\n                    return;\n            }\n        }\n        // Need to allocate more threads\n        for( unsigned i=index; i<request_size; ++i ) {\n            __TBB_ASSERT( index<request_size, NULL );\n            thread_map::value_type* k = my_thread_map.add_one_thread( true );\n#if TBB_USE_ASSERT\n            if( !k ) {\n                // Client erred\n                __TBB_ASSERT(false, \"server::get_threads: exceeded job_count\\n\");\n            }\n#endif\n            my_thread_map.bind_one_thread( *this, *k );\n            server_thread& t = k->thread();\n            if( t.try_grab_for( ts_omp_busy ) ) {\n                job& j = k->wait_for_job();\n                array[index] = &j;\n                // The preincrement instead of post-increment of index is deliberate.\n                t.omp_dispatch.produce( client(), j, cookie, index PRODUCE_ARG(*this) );\n                if( ++index==request_size )\n                    return;\n            } // else someone else snatched it.\n        }\n    }\n}\n#endif /* !RML_USE_WCRM */\n\n//------------------------------------------------------------------------\n// Methods of omp_dispatch_type\n//------------------------------------------------------------------------\nvoid omp_dispatch_type::consume() {\n    // Wait for short window between when master sets state of this thread to ts_omp_busy\n    // and master thread calls produce.\n    job_type* j;\n    tbb::internal::atomic_backoff backoff;\n    while( (j = job)==NULL ) backoff.pause();\n    job = static_cast<job_type*>(NULL);\n    client->process(*j,cookie,index);\n#if TBB_USE_ASSERT\n    // Return of method process implies \"decrease_load\" from client's viewpoint, even though\n    // the actual adjustment of the_balance only happens when this thread really goes to sleep.\n    --server->net_delta;\n#endif /* TBB_USE_ASSERT */\n}\n\n#if !RML_USE_WCRM\n#if _WIN32||_WIN64\nvoid omp_connection_v2::deactivate( rml::job* j )\n{\n#if TBB_USE_ASSERT\n    net_delta -= 1;\n#endif\n    __TBB_ASSERT( j, NULL );\n    server_thread* thr = (server_thread*) scratch_ptr( *j );\n    thr->deactivate();\n}\n\nvoid omp_connection_v2::reactivate( rml::job* j )\n{\n    // Should not adjust the_balance because OMP client is supposed to\n    // do try_increase_load() to reserve the threads to use.\n    __TBB_ASSERT( j, NULL );\n    server_thread* thr = (server_thread*) scratch_ptr( *j );\n    thr->reactivate();\n}\n#endif /* _WIN32||_WIN64 */\n\n//------------------------------------------------------------------------\n// Methods of server_thread\n//------------------------------------------------------------------------\n\nserver_thread::server_thread() :\n    ref_count(0),\n    link(NULL),\n    my_map_pos(),\n    my_conn(NULL), my_job(NULL), my_ja(NULL)\n{\n    state = ts_idle;\n    terminate = false;\n#if TBB_USE_ASSERT\n    has_active_thread = false;\n#endif /* TBB_USE_ASSERT */\n}\n\nserver_thread::~server_thread() {\n    __TBB_ASSERT( !has_active_thread, NULL );\n}\n\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    // Suppress overzealous compiler warnings about an initialized variable 'sink_for_alloca' not referenced\n    #pragma warning(push)\n    #pragma warning(disable:4189)\n#endif\n__RML_DECL_THREAD_ROUTINE server_thread::thread_routine( void* arg ) {\n    server_thread* self = static_cast<server_thread*>(arg);\n    AVOID_64K_ALIASING( self->my_index );\n#if TBB_USE_ASSERT\n    __TBB_ASSERT( !self->has_active_thread, NULL );\n    self->has_active_thread = true;\n#endif /* TBB_USE_ASSERT */\n    self->loop();\n    return 0;\n}\n#if _MSC_VER && !defined(__INTEL_COMPILER)\n    #pragma warning(pop)\n#endif\n\nvoid server_thread::launch( size_t stack_size ) {\n#if USE_WINTHREAD\n    thread_monitor::launch( thread_routine, this, stack_size, &this->my_index );\n#else\n    thread_monitor::launch( thread_routine, this, stack_size );\n#endif /* USE_PTHREAD */\n}\n\nvoid server_thread::sleep_perhaps( thread_state_t asleep ) {\n    if( terminate ) return;\n    __TBB_ASSERT( asleep==ts_asleep, NULL );\n    thread_monitor::cookie c;\n    monitor.prepare_wait(c);\n    if( state.compare_and_swap( asleep, ts_idle )==ts_idle ) {\n        if( !terminate ) {\n            monitor.commit_wait(c);\n            // Someone else woke me up.  The compare_and_swap further below deals with spurious wakeups.\n        } else {\n            monitor.cancel_wait();\n        }\n        thread_state_t s = read_state();\n        if( s==ts_asleep ) {\n            state.compare_and_swap( ts_idle, ts_asleep );\n            // I woke myself up, either because I cancelled the wait or suffered a spurious wakeup.\n        } else {\n            // Someone else woke me up; there the_balance is decremented by 1. -- tbb only\n            if( !is_omp_thread ) {\n                __TBB_ASSERT( s==ts_tbb_busy||s==ts_idle, NULL );\n            }\n        }\n    } else {\n        // someone else made it busy ; see try_grab_for when state==ts_idle.\n        __TBB_ASSERT( state==ts_omp_busy||state==ts_tbb_busy, NULL );\n        monitor.cancel_wait();\n    }\n    __TBB_ASSERT( read_state()!=asleep, \"a thread can only put itself to sleep\" );\n}\n\nbool server_thread::wakeup( thread_state_t to, thread_state_t from ) {\n    bool success = false;\n    __TBB_ASSERT( from==ts_asleep && (to==ts_idle||to==ts_omp_busy||to==ts_tbb_busy), NULL );\n    if( state.compare_and_swap( to, from )==from ) {\n        if( !is_omp_thread ) __TBB_ASSERT( to==ts_idle||to==ts_tbb_busy, NULL );\n        // There is a small timing window that permits balance to become negative,\n        // but such occurrences are probably rare enough to not worry about, since\n        // at worst the result is slight temporary oversubscription.\n        monitor.notify();\n        success = true;\n    }\n    return success;\n}\n\n//! Attempt to change a thread's state to ts_omp_busy, and waking it up if necessary.\nbool server_thread::try_grab_for( thread_state_t target_state ) {\n    bool success = false;\n    switch( read_state() ) {\n        case ts_asleep:\n            success = wakeup( target_state, ts_asleep );\n            break;\n        case ts_idle:\n            success = state.compare_and_swap( target_state, ts_idle )==ts_idle;\n            break;\n        default:\n            // Thread is not available to be part of an OpenMP thread team.\n            break;\n    }\n    return success;\n}\n\n#if _WIN32||_WIN64\nvoid server_thread::deactivate() {\n    thread_state_t es = (thread_state_t) my_extra_state.fetch_and_store( ts_deactivated );\n    __TBB_ASSERT( my_extra_state==ts_deactivated, \"someone else tampered with my_extra_state?\" );\n    if( es==ts_none )\n        state = ts_idle;\n    else\n        __TBB_ASSERT( es==ts_reactivated, \"Cannot call deactivate() while in ts_deactivated\" );\n        // only the thread can transition itself from ts_deactivted to ts_none\n    __TBB_ASSERT( my_extra_state==ts_deactivated, \"someone else tampered with my_extra_state?\" );\n    my_extra_state = ts_none; // release the critical section\n    int bal = ++the_balance;\n    if( bal>0 )\n        wakeup_some_tbb_threads();\n    if( es==ts_none )\n        sleep_perhaps( ts_asleep );\n}\n\nvoid server_thread::reactivate() {\n    thread_state_t es;\n    do {\n        while( (es=read_extra_state())==ts_deactivated )\n            __TBB_Yield();\n        if( es==ts_reactivated ) {\n            __TBB_ASSERT( false, \"two Reactivate() calls in a row.  Should not happen\" );\n            return;\n        }\n        __TBB_ASSERT( es==ts_none, NULL );\n    } while( (thread_state_t)my_extra_state.compare_and_swap( ts_reactivated, ts_none )!=ts_none );\n    if( state!=ts_omp_busy ) {\n        my_extra_state = ts_none;\n        while( !try_grab_for( ts_omp_busy ) )\n            __TBB_Yield();\n    }\n}\n#endif /* _WIN32||_WIN64 */\n\n\ntemplate<typename Connection>\nbool server_thread::destroy_job( Connection& c ) {\n    __TBB_ASSERT( !is_omp_thread||(state==ts_idle||state==ts_omp_busy), NULL );\n    __TBB_ASSERT(  is_omp_thread||(state==ts_idle||state==ts_tbb_busy), NULL );\n    if( !is_omp_thread ) {\n        __TBB_ASSERT( state==ts_idle||state==ts_tbb_busy, NULL );\n        if( state==ts_idle )\n            state.compare_and_swap( ts_done, ts_idle );\n        // 'state' may be set to ts_tbb_busy by another thread.\n\n        if( state==ts_tbb_busy ) { // return the coin to the deposit\n            // need to deposit first to let the next connection see the change\n            ++the_balance;\n            state = ts_done; // no other thread changes the state when it is ts_*_busy\n        }\n    }\n    if( job_automaton* ja = my_ja ) {\n        rml::job* j;\n        if( ja->try_plug(j) ) {\n            __TBB_ASSERT( j, NULL );\n            c.client().cleanup(*j);\n            c.remove_client_ref();\n        } else {\n            // Some other thread took responsibility for cleaning up the job.\n        }\n    }\n    // Must do remove client reference first, because execution of\n    // c.remove_ref() can cause *this to be destroyed.\n    int k = remove_ref();\n    __TBB_ASSERT_EX( k==0, \"more than one references?\" );\n#if TBB_USE_ASSERT\n    has_active_thread = false;\n#endif /* TBB_USE_ASSERT */\n    c.remove_server_ref();\n    return true;\n}\n\nbool server_thread::do_termination() {\n    if( is_omp_thread )\n        return destroy_job( *static_cast<omp_connection_v2*>(my_conn) );\n    else\n        return destroy_job( *static_cast<tbb_connection_v2*>(my_conn) );\n}\n\n//! Loop that each thread executes\nvoid server_thread::loop() {\n    if( is_omp_thread )\n        static_cast<omp_connection_v2*>(my_conn)->make_job( *this, *my_ja );\n    else\n        static_cast<tbb_connection_v2*>(my_conn)->make_job( *this, *my_ja );\n    for(;;) {\n        __TBB_Yield();\n        if( state==ts_idle )\n            sleep_perhaps( ts_asleep );\n\n        // Check whether I should quit.\n        if( terminate )\n            if( do_termination() )\n                return;\n\n        // read the state\n        thread_state_t s = read_state();\n        __TBB_ASSERT( s==ts_idle||s==ts_omp_busy||s==ts_tbb_busy, NULL );\n\n        if( s==ts_omp_busy ) {\n            // Enslaved by OpenMP team.\n            omp_dispatch.consume();\n            /* here wake tbb threads up if feasible */\n            if( ++the_balance>0 )\n                wakeup_some_tbb_threads();\n            state = ts_idle;\n        } else if( s==ts_tbb_busy ) {\n            // do some TBB work.\n            __TBB_ASSERT( my_conn && my_job, NULL );\n            tbb_connection_v2& conn = *static_cast<tbb_connection_v2*>(my_conn);\n            // give openmp higher priority\n            bool has_coin = true;\n            if( conn.has_slack() ) {\n                // it has the coin, it should trip to the scheduler at least once as long as its slack is positive\n                do {\n                    if( conn.try_process( *this, *my_job ) )\n                        if( conn.has_slack() && the_balance>=0 )\n                            has_coin = !conn.wakeup_next_thread( my_map_pos );\n                } while( has_coin && conn.has_slack() && the_balance>=0 );\n            }\n            state = ts_idle;\n            if( has_coin ) {\n                ++the_balance; // return the coin back to the deposit\n                if( conn.has_slack() ) { // a new adjust_job_request_estimate() is in progress\n                                         // it may have missed my changes to state and/or the_balance\n                    if( --the_balance>=0 ) { // try to grab the coin back\n                        // I got the coin\n                        if( state.compare_and_swap( ts_tbb_busy, ts_idle )!=ts_idle )\n                            ++the_balance; // someone else enlisted me.\n                    } else {\n                        // overdraft. return the coin\n                        ++the_balance;\n                    }\n                } // else the new request will see my changes to state & the_balance.\n            }\n            /* here wake tbb threads up if feasible */\n            if( the_balance>0 )\n                wakeup_some_tbb_threads();\n        }\n    }\n}\n#endif /* !RML_USE_WCRM */\n\n#if RML_USE_WCRM\n\nclass tbb_connection_v2;\nclass omp_connection_v2;\n\n#define CREATE_SCHEDULER_POLICY(policy,min_thrs,max_thrs,stack_size) \\\n    try {                                                                 \\\n        policy = new SchedulerPolicy (7,                                  \\\n                          SchedulerKind, RML_THREAD_KIND, /*defined in _rml_serer_msrt.h*/ \\\n                          MinConcurrency, min_thrs,                       \\\n                          MaxConcurrency, max_thrs,                       \\\n                          TargetOversubscriptionFactor, 1,                \\\n                          ContextStackSize, stack_size/1000, /*ConcRT:kB, iRML:bytes*/ \\\n                          ContextPriority, THREAD_PRIORITY_NORMAL,        \\\n                          DynamicProgressFeedback, ProgressFeedbackDisabled ); \\\n    } catch ( invalid_scheduler_policy_key & ) {                               \\\n        __TBB_ASSERT( false, \"invalid scheduler policy key exception caught\" );\\\n    } catch ( invalid_scheduler_policy_value & ) {                        \\\n        __TBB_ASSERT( false, \"invalid scheduler policy value exception caught\" );\\\n    }\n\nstatic unsigned int core_count;\nstatic tbb::atomic<int> core_count_inited;\n\n\nstatic unsigned int get_processor_count()\n{\n    if( core_count_inited!=2 ) {\n        if( core_count_inited.compare_and_swap( 1, 0 )==0 ) {\n            core_count = GetProcessorCount();\n            core_count_inited = 2;\n        } else {\n            tbb::internal::spin_wait_until_eq( core_count_inited, 2 );\n        }\n    }\n    return core_count;\n}\n\ntemplate<typename Connection>\nscheduler<Connection>::scheduler( Connection& conn ) : uid(GetSchedulerId()), my_conn(conn) {}\n\ntemplate<>\nscheduler<tbb_connection_v2>::scheduler( tbb_connection_v2& conn ) : uid(GetSchedulerId()), my_conn(conn)\n{\n    rml::client& cl = my_conn.client();\n    unsigned max_job_count = cl.max_job_count();\n    unsigned count = get_processor_count();\n    __TBB_ASSERT( max_job_count>0, \"max job count must be positive\" );\n    __TBB_ASSERT( count>1, \"The processor count must be greater than 1\" );\n    if( max_job_count>count-1) max_job_count = count-1;\n    CREATE_SCHEDULER_POLICY( my_policy, 0, max_job_count, cl.min_stack_size() );\n}\n\n#if __RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\ntemplate<>\nvoid scheduler<tbb_connection_v2>::RemoveVirtualProcessors( IVirtualProcessorRoot**, unsigned int)\n{\n}\n#else\ntemplate<>\nvoid scheduler<tbb_connection_v2>::RemoveVirtualProcessors( IVirtualProcessorRoot** vproots, unsigned int count )\n{\n    if( !my_conn.is_closing() )\n        my_conn.remove_virtual_processors( vproots, count );\n}\n#endif\n\ntemplate<>\nvoid scheduler<tbb_connection_v2>::NotifyResourcesExternallyIdle( IVirtualProcessorRoot** /*vproots*/, unsigned int /*count*/)\n{\n    __TBB_ASSERT( false, \"NotifyResourcesExternallyIdle() is not allowed for TBB\" );\n}\n\ntemplate<>\nvoid scheduler<tbb_connection_v2>::NotifyResourcesExternallyBusy( IVirtualProcessorRoot** /*vproots*/, unsigned int /*count*/ )\n{\n    __TBB_ASSERT( false, \"NotifyResourcesExternallyBusy() is not allowed for TBB\" );\n}\n\ntemplate<>\nscheduler<omp_connection_v2>::scheduler( omp_connection_v2& conn ) : uid(GetSchedulerId()), my_conn(conn)\n{\n    unsigned count = get_processor_count();\n    rml::client& cl = my_conn.client();\n    __TBB_ASSERT( count>1, \"The processor count must be greater than 1\" );\n    CREATE_SCHEDULER_POLICY( my_policy, count-1, count-1, cl.min_stack_size() );\n}\n\ntemplate<>\nvoid scheduler<omp_connection_v2>::RemoveVirtualProcessors( IVirtualProcessorRoot** /*vproots*/, unsigned int /*count*/ ) {\n    __TBB_ASSERT( false, \"RemoveVirtualProcessors() is not allowed for OMP\" );\n}\n\ntemplate<>\nvoid scheduler<omp_connection_v2>::NotifyResourcesExternallyIdle( IVirtualProcessorRoot** vproots, unsigned int count ){\n    if( !my_conn.is_closing() )\n        my_conn.notify_resources_externally_idle( vproots, count );\n}\n\ntemplate<>\nvoid scheduler<omp_connection_v2>::NotifyResourcesExternallyBusy( IVirtualProcessorRoot** vproots, unsigned int count ){\n    if( !my_conn.is_closing() )\n        my_conn.notify_resources_externally_busy( vproots, count );\n}\n\n/* ts_idle, ts_asleep, ts_busy */\nvoid tbb_server_thread::Dispatch( DispatchState* ) {\n    // Activate() will resume a thread right after Deactivate() as if it returns from the call\n    tbb_connection_v2* tbb_conn = static_cast<tbb_connection_v2*>(my_conn);\n    make_job( *tbb_conn, *this );\n\n    for( ;; ) {\n        // Try to wake some tbb threads if the balance is positive.\n        // When a thread is added by ConcRT and enter here for the first time,\n        // the thread may wake itself up (i.e., atomically change its state to ts_busy.\n        if( the_balance>0 )\n             wakeup_some_tbb_threads();\n        if( read_state()!=ts_busy )\n            if( sleep_perhaps() )\n                return;\n        if( terminate )\n            if( initiate_termination() )\n                return;\n        if( read_state()==ts_busy ) {\n            // this thread has a coin (i.e., state=ts_busy; it should trip to the scheduler at least once\n            if ( tbb_conn->has_slack() ) {\n                do {\n                    tbb_conn->try_process( *wait_for_job() );\n                } while( tbb_conn->has_slack() && the_balance>=0 && !is_removed() );\n            }\n            __TBB_ASSERT( read_state()==ts_busy, \"thread is not in busy state after returning from process()\" );\n            // see remove_virtual_processors()\n            if( my_state.compare_and_swap( ts_idle, ts_busy )==ts_busy ) {\n                int bal = ++the_balance;\n                if( tbb_conn->has_slack() ) {\n                    // slack is positive, volunteer to help\n                    bal = --the_balance;  // try to grab the coin back\n                    if( bal>=0 ) { // got the coin back\n                        if( my_state.compare_and_swap( ts_busy, ts_idle )!=ts_idle )\n                            ++the_balance; // someone else enlisted me.\n                        // else my_state is ts_busy, I will come back to tbb_conn->try_process().\n                    } else {\n                        // overdraft. return the coin\n                        ++the_balance;\n                    }\n                } // else the new request will see my changes to state & the_balance.\n            } else {\n                __TBB_ASSERT( false, \"someone tampered with my state\" );\n            }\n        } // someone else might set the state to somthing other than ts_idle\n    }\n}\n\nvoid omp_server_thread::Dispatch( DispatchState* ) {\n    // Activate() will resume a thread right after Deactivate() as if it returns from the call\n    make_job( *static_cast<omp_connection_v2*>(my_conn), *this );\n\n    for( ;; ) {\n        if( read_state()!=ts_busy )\n            sleep_perhaps();\n        if( terminate ) {\n            if( initiate_termination() )\n                return;\n        }\n        if( read_state()==ts_busy ) {\n            omp_data.consume();\n            __TBB_ASSERT( read_state()==ts_busy, \"thread is not in busy state after returning from process()\" );\n            my_thread_map.adjust_balance( 1 );\n            set_state( ts_idle );\n        }\n        // someone else might set the state to somthing other than ts_idle\n    }\n}\n\n//! Attempt to change a thread's state to ts_omp_busy, and waking it up if necessary.\nthread_grab_t server_thread_rep::try_grab_for() {\n    thread_grab_t res = wk_failed;\n    thread_state_t s = read_state();\n    switch( s ) {\n    case ts_asleep:\n        if( wakeup( ts_busy, ts_asleep ) )\n            res = wk_from_asleep;\n        __TBB_ASSERT( res==wk_failed||read_state()==ts_busy, NULL );\n        break;\n    case ts_idle:\n        if( my_state.compare_and_swap( ts_busy, ts_idle )==ts_idle )\n            res = wk_from_idle;\n        // At this point a thread is grabbed (i.e., its state has  changed to ts_busy.\n        // It is possible that the thread 1) processes the job, returns from process() and\n        // sets its state ts_idle again.  In some cases, it even sets its state to ts_asleep.\n        break;\n    default:\n        break;\n    }\n    return res;\n}\n\nbool tbb_server_thread::switch_out() {\n    thread_state_t s = read_state();\n    __TBB_ASSERT( s==ts_asleep||s==ts_busy, NULL );\n    // This thread comes back from the TBB scheduler, and changed its state to ts_asleep successfully.\n    // The master enlisted it and woke it up by Activate()'ing it; now it is emerging from Deactivated().\n    // ConcRT requested for removal of the vp associated with the thread, and RML marks it removed.\n    // Now, it has ts_busy, and removed. -- we should remove it.\n    IExecutionResource* old_vp = my_execution_resource;\n    if( s==ts_busy ) {\n        ++the_balance;\n        my_state = ts_asleep;\n    }\n    IThreadProxy* proxy = my_proxy;\n    __TBB_ASSERT( proxy, NULL );\n    my_execution_resource = (IExecutionResource*) c_remove_prepare;\n    old_vp->Remove( my_scheduler );\n    my_execution_resource = (IExecutionResource*) c_remove_returned;\n    int cnt = --activation_count;\n    __TBB_ASSERT_EX( cnt==0||cnt==1, \"too many activations?\" );\n    proxy->SwitchOut();\n    if( terminate ) {\n        bool activated = activation_count==1;\n#if TBB_USE_ASSERT\n        /* In a rare sequence of events, a thread comes out of SwitchOut with activation_count==1.\n         * 1) The thread is SwitchOut'ed.\n         * 2) AddVirtualProcessors() arrived and the thread is Activated.\n         * 3) The thread is coming out of SwitchOut().\n         * 4) request_close_connection arrives and inform the thread that it is time to terminate.\n         * 5) The thread hits the check and falls into the path with 'activated==true'.\n         * In that case, do the clean-up but do not switch to the thread scavenger; rather simply return to RM.\n         */\n        if( activated ) {\n            // thread is 'revived' in add_virtual_processors after being Activated().\n            // so, if the thread extra state is still marked 'removed', it will shortly change to 'none'\n            // i.e., !is_remove().  The thread state is changed to ts_idle before the extra state, so\n            // the thread's state should be either ts_idle or ts_done.\n            while( is_removed() )\n                __TBB_Yield();\n            thread_state_t s = read_state();\n            __TBB_ASSERT( s==ts_idle || s==ts_done, NULL );\n        }\n#endif\n        __TBB_ASSERT( my_state==ts_asleep||my_state==ts_idle, NULL );\n        // it is possible that in make_job() the thread may not have a chance to create a job.\n        // my_job may not be set if the thread did not get a chance to process client's job (i.e., call try_process())\n        rml::job* j;\n        if( my_job_automaton.try_plug(j) ) {\n            __TBB_ASSERT( j, NULL );\n            my_client.cleanup(*j);\n            my_conn->remove_client_ref();\n        }\n        // Must do remove client reference first, because execution of\n        // c.remove_ref() can cause *this to be destroyed.\n        if( !activated )\n            proxy->SwitchTo( my_thread_map.get_thread_scavenger(), Idle );\n        my_conn->remove_server_ref();\n        return true;\n    }\n    // We revive a thread in add_virtual_processors() after we Activate the thread on a new virtual processor.\n    // So briefly wait until the thread's my_execution_resource gets set.\n    while( get_virtual_processor()==c_remove_returned )\n        __TBB_Yield();\n    return false;\n}\n\nbool tbb_server_thread::sleep_perhaps () {\n    if( terminate ) return false;\n    thread_state_t s = read_state();\n    if( s==ts_idle ) {\n        if( my_state.compare_and_swap( ts_asleep, ts_idle )==ts_idle ) {\n            // If a thread is between read_state() and compare_and_swap(), and the master tries to terminate,\n            // the master's compare_and_swap() will fail because the thread's state is ts_idle.\n            // We need to check if terminate is true or not before letting the thread go to sleep oetherwise\n            // we will miss the terminate signal.\n            if( !terminate ) {\n                if( !is_removed() ) {\n                    --activation_count;\n                    get_virtual_processor()->Deactivate( this );\n                }\n                if( is_removed() ) {\n                    if( switch_out() )\n                        return true;\n                    __TBB_ASSERT( my_execution_resource>c_remove_returned, NULL );\n                }\n                // in add_virtual_processors(), when we revive a thread, we change its state after Activate the thread\n                // in that case the state may be ts_asleep for a short period\n                while( read_state()==ts_asleep )\n                    __TBB_Yield();\n            } else {\n                if( my_state.compare_and_swap( ts_done, ts_asleep )!=ts_asleep ) {\n                    --activation_count;\n                    // unbind() changed my state. It will call Activate(). So issue a matching Deactivate()\n                    get_virtual_processor()->Deactivate( this );\n                }\n            }\n        }\n    } else {\n        __TBB_ASSERT( s==ts_busy, NULL );\n    }\n    return false;\n}\n\nvoid omp_server_thread::sleep_perhaps () {\n    if( terminate ) return;\n    thread_state_t s = read_state();\n    if( s==ts_idle ) {\n        if( my_state.compare_and_swap( ts_asleep, ts_idle )==ts_idle ) {\n            // If a thread is between read_state() and compare_and_swap(), and the master tries to terminate,\n            // the master's compare_and_swap() will fail because the thread's state is ts_idle.\n            // We need to check if terminate is true or not before letting the thread go to sleep oetherwise\n            // we will miss the terminate signal.\n            if( !terminate ) {\n                get_virtual_processor()->Deactivate( this );\n                __TBB_ASSERT( !is_removed(), \"OMP threads should not be deprived of a virtual processor\" );\n                __TBB_ASSERT( read_state()!=ts_asleep, NULL );\n            } else {\n                if( my_state.compare_and_swap( ts_done, ts_asleep )!=ts_asleep )\n                    // unbind() changed my state. It will call Activate(). So issue a matching Deactivate()\n                    get_virtual_processor()->Deactivate( this );\n            }\n        }\n    } else {\n        __TBB_ASSERT( s==ts_busy, NULL );\n    }\n}\n\nbool tbb_server_thread::initiate_termination() {\n    if( read_state()==ts_busy ) {\n        int bal = ++the_balance;\n        if( bal>0 ) wakeup_some_tbb_threads();\n    }\n    return destroy_job( (tbb_connection_v2*) my_conn );\n}\n\ntemplate<typename Connection>\nbool server_thread_rep::destroy_job( Connection* c ) {\n    __TBB_ASSERT( my_state!=ts_asleep, NULL );\n    rml::job* j;\n    if( my_job_automaton.try_plug(j) ) {\n        __TBB_ASSERT( j, NULL );\n        my_client.cleanup(*j);\n        c->remove_client_ref();\n    }\n    // Must do remove client reference first, because execution of\n    // c.remove_ref() can cause *this to be destroyed.\n    c->remove_server_ref();\n    return true;\n}\n\nvoid thread_map::assist_cleanup( bool assist_null_only ) {\n    // To avoid deadlock, the current thread *must* help out with cleanups that have not started,\n    // becausd the thread that created the job may be busy for a long time.\n    for( iterator i = begin(); i!=end(); ++i ) {\n        rml::job* j=0;\n        server_thread* thr = (*i).second;\n        job_automaton& ja = thr->my_job_automaton;\n        if( assist_null_only ? ja.try_plug_null() : ja.try_plug(j) ) {\n            if( j ) {\n                my_client.cleanup(*j);\n            } else {\n                // server thread did not get a chance to create a job.\n            }\n            remove_client_ref();\n        }\n    }\n}\n\nvoid thread_map::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count, tbb_connection_v2& conn, ::tbb::spin_mutex& mtx )\n{\n#if TBB_USE_ASSERT\n    int req_cnt = ++n_add_vp_requests;\n    __TBB_ASSERT( req_cnt==1, NULL );\n#endif\n    std::vector<thread_map::iterator> vec(count);\n    std::vector<tbb_server_thread*> tvec(count);\n    iterator end;\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        __TBB_ASSERT( my_map.size()==0||count==1, NULL );\n        end = my_map.end(); //remember 'end' at the time of 'find'\n        // find entries in the map for those VPs that were previosly added and then removed.\n        for( size_t i=0; i<count; ++i ) {\n            vec[i] = my_map.find( (key_type) vproots[i] );\n#if TBB_USE_DEBUG\n            if( vec[i]!=end ) {\n                tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n                IVirtualProcessorRoot* v = t->get_virtual_processor();\n                __TBB_ASSERT( v==c_remove_prepare||v==c_remove_returned, NULL );\n            }\n#endif\n        }\n\n        iterator nxt = my_map.begin();\n        for( size_t i=0; i<count; ++i ) {\n            if( vec[i]!=end ) {\n#if TBB_USE_ASSERT\n                tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n                __TBB_ASSERT( t->read_state()==ts_asleep, NULL );\n                IVirtualProcessorRoot* r = t->get_virtual_processor();\n                __TBB_ASSERT( r==c_remove_prepare||r==c_remove_returned, NULL );\n#endif\n                continue;\n            }\n\n            if( my_unrealized_threads>0 ) {\n                --my_unrealized_threads;\n            } else {\n                __TBB_ASSERT( nxt!=end, \"nxt should not be thread_map::iterator::end\" );\n                // find a removed thread context for i\n                for( ; nxt!=end; ++nxt ) {\n                    tbb_server_thread* t = (tbb_server_thread*) (*nxt).second;\n                    if( t->is_removed() && t->read_state()==ts_asleep && t->get_virtual_processor()==c_remove_returned ) {\n                        vec[i] = nxt++;\n                        break;\n                    }\n                }\n                // break target\n                if( vec[i]==end ) // ignore excessive VP.\n                    vproots[i] = NULL;\n            }\n        }\n    }\n\n    for( size_t i=0; i<count; ++i ) {\n        __TBB_ASSERT( !tvec[i], NULL );\n        if( vec[i]==end ) {\n            if( vproots[i] ) {\n                tvec[i] = my_tbb_allocator.allocate(1);\n                new ( tvec[i] ) tbb_server_thread( false, my_scheduler, (IExecutionResource*)vproots[i], &conn, *this, my_client );\n            }\n#if TBB_USE_ASSERT\n        } else {\n            tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n            __TBB_ASSERT( t->GetProxy(), \"Proxy is cleared?\" );\n#endif\n        }\n    }\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        bool closing = is_closing();\n\n        for( size_t i=0; i<count; ++i ) {\n            if( vec[i]==end ) {\n                if( vproots[i] ) {\n                    thread_map::key_type key = (thread_map::key_type) vproots[i];\n                    vec[i] = insert( key, (server_thread*) tvec[i] );\n                    my_client_ref_count.add_ref();\n                    my_server_ref_count.add_ref();\n                }\n            } else if( !closing ) {\n                tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n\n                if( (*vec[i]).first!=(thread_map::key_type)vproots[i] ) {\n                    my_map.erase( vec[i] );\n                    thread_map::key_type key = (thread_map::key_type) vproots[i];\n                    __TBB_ASSERT( key, NULL );\n                    vec[i] = insert( key, t );\n                }\n                __TBB_ASSERT( t->read_state()==ts_asleep, NULL );\n                // We did not decrement server/client ref count when a thread is removed.\n                // So, don't increment server/client ref count here.\n            }\n        }\n\n        // we could check is_closing() earlier.  That requires marking the newly allocated server_thread objects\n        // that are not inserted into the thread_map, and deallocate them.  Doing so seems more cumbersome\n        // than simply adding these to the thread_map and let thread_map's destructor take care of reclamation.\n        __TBB_ASSERT( closing==is_closing(), NULL );\n        if( closing ) return;\n    }\n\n    for( size_t i=0; i<count; ++i ) {\n        if( vproots[i] ) {\n            tbb_server_thread* t = (tbb_server_thread*) (*vec[i]).second;\n            __TBB_ASSERT( tvec[i]!=NULL||t->GetProxy(), \"Proxy is cleared?\" );\n            if( t->is_removed() )\n                __TBB_ASSERT( t->get_virtual_processor()==c_remove_returned, NULL );\n            int cnt = ++t->activation_count;\n            __TBB_ASSERT_EX( cnt==0||cnt==1, NULL );\n            vproots[i]->Activate( t );\n            if( t->is_removed() )\n                t->revive( my_scheduler, vproots[i], my_client );\n        }\n    }\n#if TBB_USE_ASSERT\n    req_cnt = --n_add_vp_requests;\n    __TBB_ASSERT( req_cnt==0, NULL );\n#endif\n}\n\nvoid thread_map::remove_virtual_processors( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx ) {\n    if( my_map.size()==0 )\n        return;\n    tbb::spin_mutex::scoped_lock lck( mtx );\n\n    if( is_closing() ) return;\n\n    for( unsigned int c=0; c<count; ++c ) {\n        iterator i = my_map.find( (key_type) vproots[c] );\n        if( i==my_map.end() ) {\n            thread_scavenger_thread* tst = my_thread_scavenger_thread;\n            if( !tst ) {\n                // Remove unknown vp from my scheduler;\n                vproots[c]->Remove( my_scheduler );\n            } else {\n                while( (tst=my_thread_scavenger_thread)==c_claimed )\n                    __TBB_Yield();\n                if( vproots[c]!=tst->get_virtual_processor() )\n                    vproots[c]->Remove( my_scheduler );\n            }\n            continue;\n        }\n        tbb_server_thread* thr = (tbb_server_thread*) (*i).second;\n        __TBB_ASSERT( thr->tbb_thread, \"incorrect type of server_thread\" );\n        thr->set_removed();\n        if( thr->read_state()==ts_asleep ) {\n            while( thr->activation_count>0 ) {\n                if( thr->get_virtual_processor()<=c_remove_returned )\n                    break;\n                __TBB_Yield();\n            }\n            if( thr->get_virtual_processor()>c_remove_returned ) {\n                // the thread is in Deactivated state\n                ++thr->activation_count;\n                // wake the thread up so that it Switches Out itself.\n                thr->get_virtual_processor()->Activate( thr );\n            } // else, it is Switched Out\n        } // else the thread will see that it is removed and proceed to switch itself out without Deactivation\n    }\n}\n\nvoid thread_map::add_virtual_processors( IVirtualProcessorRoot** vproots, unsigned int count, omp_connection_v2& conn, ::tbb::spin_mutex& mtx )\n{\n    std::vector<thread_map::iterator> vec(count);\n    std::vector<server_thread*> tvec(count);\n    iterator end;\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        // read the map\n        end = my_map.end(); //remember 'end' at the time of 'find'\n        for( size_t i=0; i<count; ++i )\n            vec[i] = my_map.find( (key_type) vproots[i] );\n    }\n\n    for( size_t i=0; i<count; ++i ) {\n        __TBB_ASSERT( !tvec[i], NULL );\n        if( vec[i]==end ) {\n            tvec[i] = my_omp_allocator.allocate(1);\n            new ( tvec[i] ) omp_server_thread( false, my_scheduler, (IExecutionResource*)vproots[i], &conn, *this, my_client );\n        }\n    }\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        for( size_t i=0; i<count; ++i ) {\n            if( vec[i]==my_map.end() ) {\n                thread_map::key_type key = (thread_map::key_type) vproots[i];\n                vec[i] = insert( key, tvec[i] );\n                my_client_ref_count.add_ref();\n                my_server_ref_count.add_ref();\n            }\n        }\n\n        // we could check is_closing() earlier.  That requires marking the newly allocated server_thread objects\n        // that are not inserted into the thread_map, and deallocate them.  Doing so seems more cumbersome\n        // than simply adding these to the thread_map and let thread_map's destructor take care of reclamation.\n        if( is_closing() ) return;\n    }\n\n    for( size_t i=0; i<count; ++i )\n        vproots[i]->Activate( (*vec[i]).second );\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        for( size_t i=0; i<count; ++i )\n            original_exec_resources.push_back( vproots[i] );\n    }\n}\n\nvoid thread_map::mark_virtual_processors_as_lent( IVirtualProcessorRoot** vproots, unsigned count, ::tbb::spin_mutex& mtx ) {\n    tbb::spin_mutex::scoped_lock lck( mtx );\n\n    if( is_closing() ) return;\n\n    iterator end = my_map.end();\n    for( unsigned int c=0; c<count; ++c ) {\n        iterator i = my_map.find( (key_type) vproots[c] );\n        if( i==end ) {\n            // The vproc has not been added to the map in create_oversubscribers()\n            my_map.insert( hash_map_type::value_type( (key_type) vproots[c], (server_thread*)1 ) );\n        } else {\n            server_thread* thr = (*i).second;\n            if( ((uintptr_t)thr)&~(uintptr_t)1 ) {\n                __TBB_ASSERT( !thr->is_removed(), \"incorrectly removed\" );\n                ((omp_server_thread*)thr)->set_lent();\n            }\n        }\n    }\n}\n\nvoid thread_map::create_oversubscribers( unsigned n, std::vector<server_thread*>& thr_vec, omp_connection_v2& conn, ::tbb::spin_mutex& mtx ) {\n    std::vector<IExecutionResource*> curr_exec_rsc;\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        curr_exec_rsc = original_exec_resources; // copy construct\n    }\n    typedef std::vector<IExecutionResource*>::iterator iterator_er;\n    typedef ::std::vector<std::pair<hash_map_type::key_type, hash_map_type::mapped_type> > hash_val_vector_t;\n    hash_val_vector_t v_vec(n);\n    iterator_er begin = curr_exec_rsc.begin();\n    iterator_er end   = curr_exec_rsc.end();\n    iterator_er i = begin;\n    for( unsigned c=0; c<n; ++c ) {\n        IVirtualProcessorRoot* vpr = my_scheduler_proxy->CreateOversubscriber( *i );\n        omp_server_thread* t = new ( my_omp_allocator.allocate(1) ) omp_server_thread( true, my_scheduler, (IExecutionResource*)vpr, &conn, *this, my_client );\n        thr_vec[c] = t;\n        v_vec[c] = hash_map_type::value_type( (key_type) vpr, t );\n        if( ++i==end ) i = begin;\n    }\n\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        if( is_closing() ) return;\n\n        iterator end = my_map.end();\n        unsigned c = 0;\n        for( hash_val_vector_t::iterator vi=v_vec.begin(); vi!=v_vec.end(); ++vi, ++c ) {\n            iterator i = my_map.find( (key_type) (*vi).first );\n            if( i==end ) {\n                my_map.insert( *vi );\n            } else {\n                // the vproc has not been added to the map in mark_virtual_processors_as_returned();\n                uintptr_t lent = (uintptr_t) (*i).second;\n                __TBB_ASSERT( lent<=1, \"vproc map entry added incorrectly?\");\n                (*i).second = thr_vec[c];\n                if( lent )\n                    ((omp_server_thread*)thr_vec[c])->set_lent();\n                else\n                    ((omp_server_thread*)thr_vec[c])->set_returned();\n            }\n            my_client_ref_count.add_ref();\n            my_server_ref_count.add_ref();\n        }\n    }\n}\n\nvoid thread_map::wakeup_tbb_threads( int c, ::tbb::spin_mutex& mtx ) {\n    std::vector<tbb_server_thread*> vec(c);\n\n    size_t idx = 0;\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        if( is_closing() ) return;\n        // only one RML thread is in here to wake worker threads up.\n\n        int bal = the_balance;\n        int cnt = c<bal ? c : bal;\n\n        if( cnt<=0 ) { return; }\n\n        for( iterator i=begin(); i!=end(); ++i ) {\n            tbb_server_thread* thr = (tbb_server_thread*) (*i).second;\n            // ConcRT RM should take threads away from TBB scheduler instead of lending them to another scheduler\n            if( thr->is_removed() )\n                continue;\n\n            if( --the_balance>=0 ) {\n                thread_grab_t res;\n                while( (res=thr->try_grab_for())!=wk_from_idle ) {\n                    if( res==wk_from_asleep ) {\n                        vec[idx++] = thr;\n                        break;\n                    } else {\n                        thread_state_t s = thr->read_state();\n                        if( s==ts_busy ) {// failed because already assigned. move on.\n                            ++the_balance;\n                            goto skip;\n                        }\n                    }\n                }\n                thread_state_t s = thr->read_state();\n                __TBB_ASSERT_EX( s==ts_busy, \"should have set the state to ts_busy\" );\n                if( --cnt==0 )\n                    break;\n            } else {\n                // overdraft\n                ++the_balance;\n                break;\n            }\nskip:\n            ;\n        }\n    }\n\n    for( size_t i=0; i<idx; ++i ) {\n        tbb_server_thread* thr = vec[i];\n        __TBB_ASSERT( thr, NULL );\n        thread_state_t s = thr->read_state();\n        __TBB_ASSERT_EX( s==ts_busy, \"should have set the state to ts_busy\" );\n        ++thr->activation_count;\n        thr->get_virtual_processor()->Activate( thr );\n    }\n\n}\n\nvoid thread_map::mark_virtual_processors_as_returned( IVirtualProcessorRoot** vprocs, unsigned int count, tbb::spin_mutex& mtx ) {\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n\n        if( is_closing() ) return;\n\n        iterator end = my_map.end();\n        for(unsigned c=0; c<count; ++c ) {\n            iterator i = my_map.find( (key_type) vprocs[c] );\n            if( i==end ) {\n                // the vproc has not been added to the map in create_oversubscribers()\n                my_map.insert( hash_map_type::value_type( (key_type) vprocs[c], static_cast<server_thread*>(0) ) );\n            } else {\n                omp_server_thread* thr = (omp_server_thread*) (*i).second;\n                if( ((uintptr_t)thr)&~(uintptr_t)1 ) {\n                    __TBB_ASSERT( !thr->is_removed(), \"incorrectly removed\" );\n                    // we shoud not make any assumption on the initial state of an added vproc.\n                    thr->set_returned();\n                }\n            }\n        }\n    }\n}\n\n\nvoid thread_map::unbind( rml::server& /*server*/, tbb::spin_mutex& mtx ) {\n    {\n        tbb::spin_mutex::scoped_lock lck( mtx );\n        shutdown_in_progress = true;  // ignore any callbacks from ConcRT RM\n\n        // Ask each server_thread to cleanup its job for this server.\n        for( iterator i = begin(); i!=end(); ++i ) {\n            server_thread* t = (*i).second;\n            t->terminate = true;\n            if( t->is_removed() ) {\n                // This is for TBB only as ConcRT RM does not request OMP schedulers to remove virtual processors\n                if( t->read_state()==ts_asleep ) {\n                    __TBB_ASSERT( my_thread_scavenger_thread, \"this is TBB connection; thread_scavenger_thread must be allocated\" );\n                    // thread is on its way to switch_out; see remove_virtual_processors() where\n                    // the thread is Activated() to bring it back from 'Deactivated' in sleep_perhaps()\n                    // now assume that the thread will go to SwitchOut()\n#if TBB_USE_ASSERT\n                    while( t->get_virtual_processor()>c_remove_returned )\n                        __TBB_Yield();\n#endif\n                    // A removed thread is supposed to proceed to SwithcOut.\n                    // There, we remove client&server references.\n                }\n            } else {\n                if( t->wakeup( ts_done, ts_asleep ) ) {\n                    if( t->tbb_thread )\n                        ++((tbb_server_thread*)t)->activation_count;\n                    t->get_virtual_processor()->Activate( t );\n                    // We mark in the thread_map such that when termination sequence started, we ignore\n                    // all notification from ConcRT RM.\n                }\n            }\n        }\n    }\n    // Remove extra ref to client.\n    remove_client_ref();\n\n    if( my_thread_scavenger_thread ) {\n        thread_scavenger_thread* tst;\n        while( (tst=my_thread_scavenger_thread)==c_claimed )\n            __TBB_Yield();\n#if TBB_USE_ASSERT\n        ++my_thread_scavenger_thread->activation_count;\n#endif\n        tst->get_virtual_processor()->Activate( tst );\n    }\n}\n\n#if !__RML_REMOVE_VIRTUAL_PROCESSORS_DISABLED\nvoid thread_map::allocate_thread_scavenger( IExecutionResource* v )\n{\n    if( my_thread_scavenger_thread>c_claimed ) return;\n    thread_scavenger_thread* c = my_thread_scavenger_thread.fetch_and_store((thread_scavenger_thread*)c_claimed);\n    if( c==NULL ) { // successfully claimed\n        add_server_ref();\n#if TBB_USE_ASSERT\n        ++n_thread_scavengers_created;\n#endif\n        __TBB_ASSERT( v, NULL );\n        IVirtualProcessorRoot* vpr = my_scheduler_proxy->CreateOversubscriber( v );\n        my_thread_scavenger_thread = c = new ( my_scavenger_allocator.allocate(1) ) thread_scavenger_thread( my_scheduler, vpr, *this );\n#if TBB_USE_ASSERT\n        ++c->activation_count;\n#endif\n        vpr->Activate( c );\n    } else if( c>c_claimed ) {\n        my_thread_scavenger_thread = c;\n    }\n}\n#endif\n\nvoid thread_scavenger_thread::Dispatch( DispatchState* )\n{\n    __TBB_ASSERT( my_proxy, NULL );\n#if TBB_USE_ASSERT\n    --activation_count;\n#endif\n    get_virtual_processor()->Deactivate( this );\n    for( thread_map::iterator i=my_thread_map.begin(); i!=my_thread_map.end(); ++i ) {\n        tbb_server_thread* t = (tbb_server_thread*) (*i).second;\n        if( t->read_state()==ts_asleep && t->is_removed() ) {\n            while( t->get_execution_resource()!=c_remove_returned )\n                __TBB_Yield();\n            my_proxy->SwitchTo( t, Blocking );\n        }\n    }\n    get_virtual_processor()->Remove( my_scheduler );\n    my_thread_map.remove_server_ref();\n    // signal to the connection scavenger that i am done with the map.\n    __TBB_ASSERT( activation_count==1, NULL );\n    set_state( ts_done );\n}\n\n//! Windows \"DllMain\" that handles startup and shutdown of dynamic library.\nextern \"C\" bool WINAPI DllMain( HINSTANCE /*hinstDLL*/, DWORD fwdReason, LPVOID lpvReserved ) {\n    void assist_cleanup_connections();\n    if( fwdReason==DLL_PROCESS_DETACH ) {\n        // dll is being unloaded\n        if( !lpvReserved ) // if FreeLibrary has been called\n            assist_cleanup_connections();\n    }\n    return true;\n}\n\nvoid free_all_connections( uintptr_t conn_ex ) {\n    while( conn_ex ) {\n        bool is_tbb = (conn_ex&2)>0;\n        //clear extra bits\n        uintptr_t curr_conn = conn_ex & ~(uintptr_t)3;\n        __TBB_ASSERT( curr_conn, NULL );\n\n        // Wait for worker threads to return\n        if( is_tbb ) {\n            tbb_connection_v2* tbb_conn = reinterpret_cast<tbb_connection_v2*>(curr_conn);\n            conn_ex = reinterpret_cast<uintptr_t>(tbb_conn->next_conn);\n            while( tbb_conn->my_thread_map.remove_server_ref()>0 )\n                __TBB_Yield();\n            delete tbb_conn;\n        } else {\n            omp_connection_v2* omp_conn = reinterpret_cast<omp_connection_v2*>(curr_conn);\n            conn_ex = reinterpret_cast<uintptr_t>(omp_conn->next_conn);\n            while( omp_conn->my_thread_map.remove_server_ref()>0 )\n                __TBB_Yield();\n            delete omp_conn;\n        }\n    }\n}\n\nvoid assist_cleanup_connections()\n{\n    //signal to connection_scavenger_thread to terminate\n    uintptr_t tail = connections_to_reclaim.tail;\n    while( connections_to_reclaim.tail.compare_and_swap( garbage_connection_queue::plugged, tail )!=tail ) {\n        __TBB_Yield();\n        tail = connections_to_reclaim.tail;\n    }\n\n    __TBB_ASSERT( connection_scavenger.state==ts_busy || connection_scavenger.state==ts_asleep, NULL );\n    // Scavenger thread may be busy freeing connections\n    DWORD thr_exit_code = STILL_ACTIVE;\n    while( connection_scavenger.state==ts_busy ) {\n        if( GetExitCodeThread( connection_scavenger.thr_handle, &thr_exit_code )>0 )\n            if( thr_exit_code!=STILL_ACTIVE )\n                break;\n        __TBB_Yield();\n        thr_exit_code = STILL_ACTIVE;\n    }\n    if( connection_scavenger.state==ts_asleep && thr_exit_code==STILL_ACTIVE )\n        connection_scavenger.wakeup(); // wake the connection scavenger thread up\n\n    // it is possible that the connection scavenger thread already exited.  Take over its responsibility.\n    if( tail && connections_to_reclaim.tail!=garbage_connection_queue::plugged_acked ) {\n        // atomically claim the head of the list.\n        uintptr_t head = connections_to_reclaim.head.fetch_and_store( garbage_connection_queue::empty );\n        if( head==garbage_connection_queue::empty )\n            head = tail;\n        connection_scavenger.process_requests( head );\n    }\n    __TBB_ASSERT( connections_to_reclaim.tail==garbage_connection_queue::plugged||connections_to_reclaim.tail==garbage_connection_queue::plugged_acked, \"someone else added a request after termination has initiated\" );\n    __TBB_ASSERT( (unsigned)the_balance==the_default_concurrency, NULL );\n}\n\nvoid connection_scavenger_thread::sleep_perhaps() {\n    uintptr_t tail = connections_to_reclaim.tail;\n    // connections_to_reclaim.tail==garbage_connection_queue::plugged --> terminate,\n    // connections_to_reclaim.tail>garbage_connection_queue::plugged : we got work to do\n    if( tail>=garbage_connection_queue::plugged ) return;\n    __TBB_ASSERT( !tail, NULL );\n    thread_monitor::cookie c;\n    monitor.prepare_wait(c);\n    if( state.compare_and_swap( ts_asleep, ts_busy )==ts_busy ) {\n        if( connections_to_reclaim.tail!=garbage_connection_queue::plugged ) {\n            monitor.commit_wait(c);\n            // Someone else woke me up.  The compare_and_swap further below deals with spurious wakeups.\n        } else {\n            monitor.cancel_wait();\n        }\n        thread_state_t s = state;\n        if( s==ts_asleep ) // if spurious wakeup.\n            state.compare_and_swap( ts_busy, ts_asleep );\n            // I woke myself up, either because I cancelled the wait or suffered a spurious wakeup.\n    } else {\n        __TBB_ASSERT( false, \"someone else tampered with my state\" );\n    }\n    __TBB_ASSERT( state==ts_busy, \"a thread can only put itself to sleep\" );\n}\n\nvoid connection_scavenger_thread::process_requests( uintptr_t conn_ex )\n{\n    __TBB_ASSERT( conn_ex>1, NULL );\n    __TBB_ASSERT( n_scavenger_threads==1||connections_to_reclaim.tail==garbage_connection_queue::plugged, \"more than one connection_scavenger_thread being active?\" );\n\n    bool done = false;\n    while( !done ) {\n        bool is_tbb = (conn_ex&2)>0;\n        //clear extra bits\n        uintptr_t curr_conn = conn_ex & ~(uintptr_t)3;\n\n        // no contention. there is only one connection_scavenger_thread!!\n        uintptr_t next_conn;\n        tbb_connection_v2* tbb_conn = NULL;\n        omp_connection_v2* omp_conn = NULL;\n        // Wait for worker threads to return\n        if( is_tbb ) {\n            tbb_conn = reinterpret_cast<tbb_connection_v2*>(curr_conn);\n            next_conn = reinterpret_cast<uintptr_t>(tbb_conn->next_conn);\n            while( tbb_conn->my_thread_map.get_server_ref_count()>1 )\n                __TBB_Yield();\n        } else {\n            omp_conn = reinterpret_cast<omp_connection_v2*>(curr_conn);\n            next_conn = reinterpret_cast<uintptr_t>(omp_conn->next_conn);\n            while( omp_conn->my_thread_map.get_server_ref_count()>1 )\n                __TBB_Yield();\n        }\n\n        //someone else may try to write into this connection object.\n        //So access next_conn field first before remove the extra server ref count.\n\n        if( next_conn==0 ) {\n            uintptr_t tail = connections_to_reclaim.tail;\n            if( tail==garbage_connection_queue::plugged ) {\n                tail = garbage_connection_queue::plugged_acked; // connection scavenger saw the flag, and it freed all connections.\n                done = true;\n            } else if( tail==conn_ex ) {\n                if( connections_to_reclaim.tail.compare_and_swap( garbage_connection_queue::empty, tail )==tail ) {\n                    __TBB_ASSERT( !connections_to_reclaim.head, NULL );\n                    done = true;\n                }\n            }\n\n            if( !done ) {\n                // A new connection to close is added to connections_to_reclaim.tail;\n                // Wait for curr_conn->next_conn to be set.\n                if( is_tbb ) {\n                    while( !tbb_conn->next_conn )\n                        __TBB_Yield();\n                    conn_ex = reinterpret_cast<uintptr_t>(tbb_conn->next_conn);\n                } else {\n                    while( !omp_conn->next_conn )\n                        __TBB_Yield();\n                    conn_ex = reinterpret_cast<uintptr_t>(omp_conn->next_conn);\n                }\n            }\n        } else {\n            conn_ex = next_conn;\n        }\n        __TBB_ASSERT( conn_ex, NULL );\n        if( is_tbb )\n            // remove extra srever ref count; this will trigger Shutdown/Release of ConcRT RM\n            tbb_conn->remove_server_ref();\n        else\n            // remove extra srever ref count; this will trigger Shutdown/Release of ConcRT RM\n            omp_conn->remove_server_ref();\n    }\n}\n\n__RML_DECL_THREAD_ROUTINE connection_scavenger_thread::thread_routine( void* arg ) {\n    connection_scavenger_thread* thr = (connection_scavenger_thread*) arg;\n    thr->state = ts_busy;\n    thr->thr_handle = GetCurrentThread();\n#if TBB_USE_ASSERT\n    ++thr->n_scavenger_threads;\n#endif\n    for(;;) {\n        __TBB_Yield();\n        thr->sleep_perhaps();\n        if( connections_to_reclaim.tail==garbage_connection_queue::plugged || connections_to_reclaim.tail==garbage_connection_queue::plugged_acked ) {\n            thr->state = ts_asleep;\n            return 0;\n        }\n\n        __TBB_ASSERT( connections_to_reclaim.tail!=garbage_connection_queue::plugged_acked, NULL );\n        __TBB_ASSERT( connections_to_reclaim.tail>garbage_connection_queue::plugged && (connections_to_reclaim.tail&garbage_connection_queue::plugged)==0 , NULL );\n        while( connections_to_reclaim.head==garbage_connection_queue::empty )\n            __TBB_Yield();\n        uintptr_t head = connections_to_reclaim.head.fetch_and_store( garbage_connection_queue::empty );\n        thr->process_requests( head );\n        wakeup_some_tbb_threads();\n    }\n}\n\ntemplate<typename Server, typename Client>\nvoid connection_scavenger_thread::add_request( generic_connection<Server,Client>* conn_to_close )\n{\n    uintptr_t conn_ex = (uintptr_t)conn_to_close | (connection_traits<Server,Client>::is_tbb<<1);\n    __TBB_ASSERT( !conn_to_close->next_conn, NULL );\n    const uintptr_t old_tail_ex = connections_to_reclaim.tail.fetch_and_store(conn_ex);\n    __TBB_ASSERT( old_tail_ex==0||old_tail_ex>garbage_connection_queue::plugged_acked, \"Unloading DLL called while this connection is being closed?\" );\n\n    if( old_tail_ex==garbage_connection_queue::empty )\n        connections_to_reclaim.head = conn_ex;\n    else {\n        bool is_tbb = (old_tail_ex&2)>0;\n        uintptr_t old_tail = old_tail_ex & ~(uintptr_t)3;\n        if( is_tbb )\n            reinterpret_cast<tbb_connection_v2*>(old_tail)->next_conn = reinterpret_cast<tbb_connection_v2*>(conn_ex);\n        else\n            reinterpret_cast<omp_connection_v2*>(old_tail)->next_conn = reinterpret_cast<omp_connection_v2*>(conn_ex);\n    }\n\n    if( state==ts_asleep )\n        wakeup();\n}\n\ntemplate<>\nuintptr_t connection_scavenger_thread::grab_and_prepend( generic_connection<tbb_server,tbb_client>* /*last_conn_to_close*/ ) { return 0;}\n\ntemplate<>\nuintptr_t connection_scavenger_thread::grab_and_prepend( generic_connection<omp_server,omp_client>* last_conn_to_close )\n{\n    uintptr_t conn_ex = (uintptr_t)last_conn_to_close;\n    uintptr_t head = connections_to_reclaim.head.fetch_and_store( garbage_connection_queue::empty );\n    reinterpret_cast<omp_connection_v2*>(last_conn_to_close)->next_conn = reinterpret_cast<omp_connection_v2*>(head);\n    return conn_ex;\n}\n\nextern \"C\" ULONGLONG NTAPI VerSetConditionMask( ULONGLONG, DWORD, BYTE);\n\nbool is_windows7_or_later ()\n{\n    try {\n        return GetOSVersion()>=IResourceManager::Win7OrLater;\n    } catch( ... ) {\n        return false;\n    }\n}\n\n#endif /* RML_USE_WCRM */\n\ntemplate<typename Connection, typename Server, typename Client>\nstatic factory::status_type connect( factory& f, Server*& server, Client& client ) {\n    server = new Connection(*static_cast<wait_counter*>(f.scratch_ptr),client);\n    return factory::st_success;\n}\n\nvoid init_rml_module () {\n    the_balance = the_default_concurrency = tbb::internal::AvailableHwConcurrency() - 1;\n#if RML_USE_WCRM\n    connection_scavenger.launch();\n#endif\n}\n\nextern \"C\" factory::status_type __RML_open_factory( factory& f, version_type& server_version, version_type client_version ) {\n    // Hack to keep this library from being closed by causing the first client's dlopen to not have a corresponding dlclose.\n    // This code will be removed once we figure out how to do shutdown of the RML perfectly.\n    static tbb::atomic<bool> one_time_flag;\n    if( one_time_flag.compare_and_swap(true,false)==false) {\n        __TBB_ASSERT( (size_t)f.library_handle!=factory::c_dont_unload, NULL );\n#if _WIN32||_WIN64\n        f.library_handle = reinterpret_cast<HMODULE>(factory::c_dont_unload);\n#else\n        f.library_handle = reinterpret_cast<void*>(factory::c_dont_unload);\n#endif\n    }\n    // End of hack\n\n    // Initialize the_balance only once\n    tbb::internal::atomic_do_once ( &init_rml_module, rml_module_state );\n\n    server_version = SERVER_VERSION;\n    f.scratch_ptr = 0;\n    if( client_version==0 ) {\n        return factory::st_incompatible;\n#if RML_USE_WCRM\n    } else if ( !is_windows7_or_later() ) {\n#if TBB_USE_DEBUG\n        fprintf(stderr, \"This version of the RML library requires Windows 7 to run on.\\nConnection request denied.\\n\");\n#endif\n        return factory::st_incompatible;\n#endif\n    } else {\n#if TBB_USE_DEBUG\n        if( client_version<EARLIEST_COMPATIBLE_CLIENT_VERSION )\n            fprintf(stderr, \"This client library is too old for the current RML server.\\nThe connection request is granted but oversubscription/undersubscription may occur.\\n\");\n#endif\n        f.scratch_ptr = new wait_counter;\n        return factory::st_success;\n    }\n}\n\nextern \"C\" void __RML_close_factory( factory& f ) {\n    if( wait_counter* fc = static_cast<wait_counter*>(f.scratch_ptr) ) {\n        f.scratch_ptr = 0;\n        fc->wait();\n        size_t bal = the_balance;\n        f.scratch_ptr = (void*)bal;\n        delete fc;\n    }\n}\n\nvoid call_with_build_date_str( ::rml::server_info_callback_t cb, void* arg );\n\n}} // rml::internal\n\nnamespace tbb {\nnamespace internal {\nnamespace rml {\n\nextern \"C\" tbb_factory::status_type __TBB_make_rml_server( tbb_factory& f, tbb_server*& server, tbb_client& client ) {\n    return ::rml::internal::connect< ::rml::internal::tbb_connection_v2>(f,server,client);\n}\n\nextern \"C\" void __TBB_call_with_my_server_info( ::rml::server_info_callback_t cb, void* arg ) {\n    return ::rml::internal::call_with_build_date_str( cb, arg );\n}\n\n}}}\n\nnamespace __kmp {\nnamespace rml {\n\nextern \"C\" omp_factory::status_type __KMP_make_rml_server( omp_factory& f, omp_server*& server, omp_client& client ) {\n    return ::rml::internal::connect< ::rml::internal::omp_connection_v2>(f,server,client);\n}\n\nextern \"C\" void __KMP_call_with_my_server_info( ::rml::server_info_callback_t cb, void* arg ) {\n    return ::rml::internal::call_with_build_date_str( cb, arg );\n}\n\n}}\n\n/*\n * RML server info\n */\n#include \"version_string.ver\"\n\n#ifndef __TBB_VERSION_STRINGS\n#pragma message(\"Warning: version_string.ver isn't generated properly by version_info.sh script!\")\n#endif\n\n// We use the build time as the RML server info. TBB is required to build RML, so we make it the same as the TBB build time.\n#ifndef __TBB_DATETIME\n#define __TBB_DATETIME __DATE__ \" \" __TIME__\n#endif\n\n#if !RML_USE_WCRM\n#define RML_SERVER_BUILD_TIME \"Intel(R) RML library built: \" __TBB_DATETIME\n#define RML_SERVER_VERSION_ST \"Intel(R) RML library version: v\" TOSTRING(SERVER_VERSION)\n#else\n#define RML_SERVER_BUILD_TIME \"Intel(R) RML library built: \" __TBB_DATETIME\n#define RML_SERVER_VERSION_ST \"Intel(R) RML library version: v\" TOSTRING(SERVER_VERSION) \" on ConcRT RM with \" RML_THREAD_KIND_STRING\n#endif\n\nnamespace rml {\nnamespace internal {\n\nvoid call_with_build_date_str( ::rml::server_info_callback_t cb, void* arg )\n{\n    (*cb)( arg, RML_SERVER_BUILD_TIME );\n    (*cb)( arg, RML_SERVER_VERSION_ST );\n}\n}} // rml::internal\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/build/android.inc": "# Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n#\n# This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n# you can redistribute it and/or modify it under the terms of the GNU General Public License\n# version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n# distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See  the GNU General Public License for more details.   You should have received a copy of\n# the  GNU General Public License along with Threading Building Blocks; if not, write to the\n# Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n#\n# As a special exception,  you may use this file  as part of a free software library without\n# restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n# functions from this file, or you compile this file and link it with other files to produce\n# an executable,  this file does not by itself cause the resulting executable to be covered\n# by the GNU General Public License. This exception does not however invalidate any other\n# reasons why the executable file might be covered by the GNU General Public License.\n\n#\n# Extra gmake command-line parameters for use with Android:\n#\n#    dlopen_workaround:  Some OS versions need workaround for dlopen to avoid recursive calls.\n#\n\n####### Detections and Commands ###############################################\n\nifeq (android,$(findstring android,$(tbb_os)))\n    $(error TBB only supports cross-compilation for Android. Specify \"target=android\" instead.)\nendif\n\nifneq (\"command line\",\"$(origin arch)\")\n    ifeq (icc,$(compiler))\n        export COMPILER_VERSION := ICC: $(shell icc -V </dev/null 2>&1 | grep 'Version')\n        ifneq (,$(findstring IA-32, $(COMPILER_VERSION)))\n            export arch:=ia32\n        else ifneq (,$(findstring Intel(R) 64, $(COMPILER_VERSION)))\n            export arch:=intel64\n        else\n            $(error \"No support for Android in $(COMPILER_VERSION)\")\n        endif\n\n    else\n        ifdef ANDROID_SERIAL\n            uname_m:=$(shell adb shell uname -m)\n            ifeq (i686,$(uname_m))\n                export arch:=ia32\n            else\n                export arch:=$(uname_m)\n            endif\n        else\n            ifndef arch\n                $(error \"No target architecture specified and \\'ANDROID_SERIAL\\' environment variable specifying target device not set\")\n            endif\n        endif\n    endif\nendif\n\n# Many OS versions (Android 4.0.[0-3] for example) need workaround for dlopen to avoid non-recursive loader lock hang\nexport dlopen_workaround = 1\n\n# Android platform only supported from TBB 4.1 forward\nNO_LEGACY_TESTS = 1\n\n\n",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/build/android.gcc.inc": "# Copyright 2005-2015 Intel Corporation.  All Rights Reserved.\n#\n# This file is part of Threading Building Blocks. Threading Building Blocks is free software;\n# you can redistribute it and/or modify it under the terms of the GNU General Public License\n# version 2  as  published  by  the  Free Software Foundation.  Threading Building Blocks is\n# distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the\n# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See  the GNU General Public License for more details.   You should have received a copy of\n# the  GNU General Public License along with Threading Building Blocks; if not, write to the\n# Free Software Foundation, Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA 02110-1301 USA\n#\n# As a special exception,  you may use this file  as part of a free software library without\n# restriction.  Specifically,  if other files instantiate templates  or use macros or inline\n# functions from this file, or you compile this file and link it with other files to produce\n# an executable,  this file does not by itself cause the resulting executable to be covered\n# by the GNU General Public License. This exception does not however invalidate any other\n# reasons why the executable file might be covered by the GNU General Public License.\n\n\nCOMPILE_ONLY = -c -MMD\nPREPROC_ONLY = -E -x c++\nINCLUDE_KEY = -I\nDEFINE_KEY = -D\nOUTPUT_KEY = -o #\nOUTPUTOBJ_KEY = -o #\nPIC_KEY = -fPIC\nWARNING_AS_ERROR_KEY = -Werror\nWARNING_KEY = -Wall\nTEST_WARNING_KEY = -Wshadow -Wcast-qual -Woverloaded-virtual -Wnon-virtual-dtor -Wextra\n\nWARNING_SUPPRESS = -Wno-parentheses -Wno-non-virtual-dtor\nDYLIB_KEY = -shared\nEXPORT_KEY = -Wl,--version-script,\nLIBDL = -ldl\n\nCPLUS = $(tbb_tool_prefix)g++\nCONLY = $(tbb_tool_prefix)gcc\n\n# -soname is necessary for proper linkage to TBB prebuilt libraries when building application with Android SDK\nLIB_LINK_FLAGS = $(DYLIB_KEY) -Wl,-soname=$(BUILDING_LIBRARY)\n\n# pie is necessary for test executables to work and might be removed if newer NDK will add it implicitly\nPIE_FLAG = -pie\nifeq ($(APP_PIE), false)\n    PIE_FLAG=\nendif\n\nLINK_FLAGS = -Wl,-rpath-link=. -rdynamic $(PIE_FLAG)\nC_FLAGS = $(CPLUS_FLAGS)\n\n# gcc 4.4 and higher support C++11\nifneq (,$(shell $(CPLUS) -dumpversion | egrep  \"^(4\\.[4-9]|[5-9])\"))\n    # On Android/gcc 4.4.3, -std=c++0x causes ::int64_t and ::uint64_t to be undefined.\n    CPP11_FLAGS = -std=gnu++0x $(DEFINE_KEY)_TBB_CPP0X\nendif\n\nifeq ($(cfg), release)\n    CPLUS_FLAGS = -O2\nendif\nifeq ($(cfg), debug)\n    CPLUS_FLAGS = -g -O0 $(DEFINE_KEY)TBB_USE_DEBUG\nendif\n\nCPLUS_FLAGS += $(DEFINE_KEY)USE_PTHREAD $(DEFINE_KEY)_GLIBCXX_HAVE_FENV_H\n\nifneq (,$(findstring $(arch),ia32 intel64))\n    CPLUS_FLAGS += $(DEFINE_KEY)DO_ITT_NOTIFY\nendif\n\nifneq (0, $(dlopen_workaround))\n    CPLUS_FLAGS += $(DEFINE_KEY)__TBB_USE_DLOPEN_REENTRANCY_WORKAROUND=1\n    CPLUS_FLAGS += $(DEFINE_KEY)__TBB_USE_DLOPEN_MAIN_PROGRAM_WORKAROUND=1\nelse\n    CPLUS_FLAGS += $(DEFINE_KEY)__TBB_USE_DLOPEN_REENTRANCY_WORKAROUND=0\n    CPLUS_FLAGS += $(DEFINE_KEY)__TBB_USE_DLOPEN_MAIN_PROGRAM_WORKAROUND=0\nendif\n\nifeq (0, $(dynamic_load))\n     CPLUS_FLAGS += $(DEFINE_KEY)__TBB_DYNAMIC_LOAD_ENABLED=0\nendif\n\n\n# Paths to the NDK prebuilt tools and libraries\nCPLUS_FLAGS    += --sysroot=$(SYSROOT)\nLIB_LINK_FLAGS += --sysroot=$(SYSROOT)\nLIBS           = -L$(CPLUS_LIB_PATH) -lgnustl_shared\n\n# This causes CPP11_FLAGS to be issued twice for test_lambda.cpp\n# TODO: Fix this in general for all platforms once the correct strategy is determined.\nifneq (00,$(lambdas)$(cpp0x))\n    CXX_ONLY_FLAGS += $(CPP11_FLAGS)\nendif\n\nifeq (ia32,$(arch))\n    # TODO: Determine best setting of -march and add to CPLUS_FLAGS\n    CPLUS_FLAGS += -m32 \n    LIB_LINK_FLAGS += -m32\nendif\n\nifeq (intel64,$(arch))\n    CPLUS_FLAGS += -m64\n    LIB_LINK_FLAGS += -m64\nendif\n\nifeq (arm,$(findstring arm,$(arch)))\n    CPLUS_FLAGS += -march=armv7-a $(DEFINE_KEY)TBB_USE_GCC_BUILTINS=1 $(DEFINE_KEY)__TBB_64BIT_ATOMICS=0\nendif\n\n#------------------------------------------------------------------------------\n# Setting assembler data.\n#------------------------------------------------------------------------------\nTBB_ASM.OBJ=\nMALLOC_ASM.OBJ=\n\nASM = $(tbb_tool_prefix)as\nifeq (intel64,$(arch))\n    ASM_FLAGS += --64\nendif\nifeq (ia32,$(arch))\n    ASM_FLAGS += --32\nendif\nifeq ($(cfg),debug)\n    ASM_FLAGS += -g\nendif\n\nASSEMBLY_SOURCE=$(arch)-gas\n#------------------------------------------------------------------------------\n# End of setting assembler data.\n#------------------------------------------------------------------------------\n\n#------------------------------------------------------------------------------\n# Setting tbbmalloc data.\n#------------------------------------------------------------------------------\n\nM_CPLUS_FLAGS = $(CPLUS_FLAGS) -fno-rtti -fno-exceptions\n\n#------------------------------------------------------------------------------\n# End of setting tbbmalloc data.\n#------------------------------------------------------------------------------\n"
    },
    "skipped": [
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/test/test_opencl_precompiled_kernel_gpu.clbin",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/test/test_opencl_kernel_64.spir",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/src/test/test_opencl_kernel_32.spir",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/task_arena/fractal/msvs/small.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/task_arena/fractal/msvs/gui.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/tachyon/msvs/small.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/tachyon/msvs/gui.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/StoreLogo.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/SplashScreen.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/SmallLogo.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/tachyon/msvs/win8ui/Assets/Logo.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/seismic/msvs/small.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/seismic/msvs/SeismicSimulation.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/polygon_overlay/speedup.gif",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/parallel_for/game_of_life/msvs/app.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/examples/common/gui/xcode/tbbExample/en.lproj/MainMenu.nib",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/tab_s.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/tab_h.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/tab_b.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/tab_a.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/sync_on.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/sync_off.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/open.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/nav_h.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/nav_g.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/nav_f.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2vertline.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2splitbar.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2pnode.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2plastnode.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2ns.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2node.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2mo.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2mnode.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2mlastnode.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2link.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2lastnode.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2folderopen.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2folderclosed.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2doc.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2cl.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/ftv2blank.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/doxygen.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/closed.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/bdwn.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/bc_s.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00211.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00210.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00209.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00207.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00206.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00201.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00200.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00199.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00198.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00197.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00194.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00191.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00190.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00189.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00188.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00187.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00185.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00184.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00183.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00181.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00180.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00179.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00178.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00177.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00176.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00175.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00174.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00173.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00172.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00171.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00170.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00169.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00168.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00167.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00166.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00165.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00164.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00163.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00160.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00157.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00156.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00155.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00144.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00143.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00142.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00141.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00140.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00139.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00138.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00134.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00133.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00132.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00131.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00130.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00129.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00128.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00126.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00125.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00124.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00119.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00113.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00112.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00111.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00110.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00109.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00108.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00107.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00103.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00102.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00098.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00097.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00096.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00094.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00092.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00091.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00090.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00089.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00088.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00087.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00086.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00085.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00084.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00083.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00082.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00080.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00078.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00076.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00073.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00070.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00069.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00068.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00064.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00063.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00062.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00061.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00059.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00058.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00056.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00055.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00054.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00052.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00051.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00049.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00048.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00047.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00045.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00044.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00043.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00040.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00039.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00038.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00034.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00033.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00032.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00031.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00026.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00025.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00023.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00022.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00021.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00020.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00019.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00014.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00013.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00012.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-intel-tbb-4.4.2-oq2yct3t5w73qqakq4jy5zlnehafnsg6/spack-src/doc/html/a00010.png"
    ],
    "total_files": 1524
}