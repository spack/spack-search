{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/common/dlsym.h": "/*\n * Copyright 2016-2017, Intel Corporation\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in\n *       the documentation and/or other materials provided with the\n *       distribution.\n *\n *     * Neither the name of the copyright holder nor the names of its\n *       contributors may be used to endorse or promote products derived\n *       from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/*\n * dlsym.h -- dynamic linking utilities with library-specific implementation\n */\n\n#ifndef PMDK_DLSYM_H\n#define PMDK_DLSYM_H 1\n\n#include \"out.h\"\n\n#if defined(USE_LIBDL) && !defined(_WIN32)\n\n#include <dlfcn.h>\n\n/*\n * util_dlopen -- calls real dlopen()\n */\nstatic inline void *\nutil_dlopen(const char *filename)\n{\n\tLOG(3, \"filename %s\", filename);\n\n\treturn dlopen(filename, RTLD_NOW);\n}\n\n/*\n * util_dlerror -- calls real dlerror()\n */\nstatic inline char *\nutil_dlerror(void)\n{\n\treturn dlerror();\n}\n\n/*\n * util_dlsym -- calls real dlsym()\n */\nstatic inline void *\nutil_dlsym(void *handle, const char *symbol)\n{\n\tLOG(3, \"handle %p symbol %s\", handle, symbol);\n\n\treturn dlsym(handle, symbol);\n}\n\n/*\n * util_dlclose -- calls real dlclose()\n */\nstatic inline int\nutil_dlclose(void *handle)\n{\n\tLOG(3, \"handle %p\", handle);\n\n\treturn dlclose(handle);\n}\n\n#else /* empty functions */\n\n/*\n * util_dlopen -- empty function\n */\nstatic inline void *\nutil_dlopen(const char *filename)\n{\n\terrno = ENOSYS;\n\treturn NULL;\n}\n\n/*\n * util_dlerror -- empty function\n */\nstatic inline char *\nutil_dlerror(void)\n{\n\terrno = ENOSYS;\n\treturn NULL;\n}\n\n/*\n * util_dlsym -- empty function\n */\nstatic inline void *\nutil_dlsym(void *handle, const char *symbol)\n{\n\terrno = ENOSYS;\n\treturn NULL;\n}\n\n/*\n * util_dlclose -- empty function\n */\nstatic inline int\nutil_dlclose(void *handle)\n{\n\terrno = ENOSYS;\n\treturn 0;\n}\n\n#endif\n\n#endif\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/common/set.c": "/*\n * Copyright 2015-2018, Intel Corporation\n * Copyright (c) 2016, Microsoft Corporation. All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in\n *       the documentation and/or other materials provided with the\n *       distribution.\n *\n *     * Neither the name of the copyright holder nor the names of its\n *       contributors may be used to endorse or promote products derived\n *       from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/*\n * set.c -- pool set utilities\n */\n\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/stat.h>\n#include <sys/mman.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <stdint.h>\n#include <endian.h>\n#include <errno.h>\n#include <stddef.h>\n#include <time.h>\n#include <ctype.h>\n#include <linux/limits.h>\n#include <sys/mman.h>\n\n#include \"libpmem.h\"\n#include \"librpmem.h\"\n#include \"set.h\"\n#include \"file.h\"\n#include \"os.h\"\n#include \"mmap.h\"\n#include \"util.h\"\n#include \"out.h\"\n#include \"dlsym.h\"\n#include \"valgrind_internal.h\"\n#include \"sys_util.h\"\n#include \"util_pmem.h\"\n#include \"fs.h\"\n#include \"os_deep.h\"\n#include \"badblock.h\"\n\n#define LIBRARY_REMOTE \"librpmem.so.1\"\n#define SIZE_AUTODETECT_STR \"AUTO\"\n\n#define PMEM_EXT \".pmem\"\n#define PMEM_EXT_LEN sizeof(PMEM_EXT)\n#define PMEM_FILE_PADDING 6\n#define PMEM_FILE_NAME_MAX_LEN 20\n#define PMEM_FILE_MAX_LEN (PMEM_FILE_NAME_MAX_LEN + PMEM_FILE_PADDING)\n\nstatic RPMEMpool *(*Rpmem_create)(const char *target, const char *pool_set_name,\n\t\t\tvoid *pool_addr, size_t pool_size, unsigned *nlanes,\n\t\t\tconst struct rpmem_pool_attr *rpmem_attr);\nstatic RPMEMpool *(*Rpmem_open)(const char *target, const char *pool_set_name,\n\t\t\tvoid *pool_addr, size_t pool_size, unsigned *nlanes,\n\t\t\tstruct rpmem_pool_attr *rpmem_attr);\nint (*Rpmem_close)(RPMEMpool *rpp);\nint (*Rpmem_persist)(RPMEMpool *rpp, size_t offset, size_t length,\n\t\t\tunsigned lane, unsigned flags);\nint (*Rpmem_deep_persist)(RPMEMpool *rpp, size_t offset, size_t length,\n\t\t\tunsigned lane);\nint (*Rpmem_read)(RPMEMpool *rpp, void *buff, size_t offset,\n\t\tsize_t length, unsigned lane);\nint (*Rpmem_remove)(const char *target, const char *pool_set_name, int flags);\nint (*Rpmem_set_attr)(RPMEMpool *rpp, const struct rpmem_pool_attr *rattr);\n\nstatic int Remote_replication_available;\nstatic os_mutex_t Remote_lock;\nstatic void *Rpmem_handle_remote;\n\nint Prefault_at_open = 0;\nint Prefault_at_create = 0;\nint SDS_at_create = POOL_FEAT_INCOMPAT_DEFAULT & POOL_E_FEAT_SDS ? 1 : 0;\n\n\n/* list of pool set option names and flags */\nstatic struct pool_set_option Options[] = {\n\t{ \"SINGLEHDR\", OPTION_SINGLEHDR },\n#ifndef _WIN32\n\t{ \"NOHDRS\", OPTION_NOHDRS },\n#endif\n\t{ NULL, OPTION_UNKNOWN }\n};\n\n/*\n * util_remote_init -- initialize remote replication\n */\nvoid\nutil_remote_init(void)\n{\n\tLOG(3, NULL);\n\n\t/* XXX Is duplicate initialization really okay? */\n\tif (!Remote_replication_available) {\n\t\tutil_mutex_init(&Remote_lock);\n\t\tRemote_replication_available = 1;\n\t}\n}\n\n/*\n * util_remote_fini -- finalize remote replication\n */\nvoid\nutil_remote_fini(void)\n{\n\tLOG(3, NULL);\n\n\tutil_remote_unload();\n\n\t/* XXX Okay to be here if not initialized? */\n\tif (Remote_replication_available) {\n\t\tRemote_replication_available = 0;\n\t\tutil_mutex_destroy(&Remote_lock);\n\t}\n}\n\n/*\n * util_dl_check_error -- check libdl error\n */\nstatic int\nutil_dl_check_error(void *handle, const char *func)\n{\n\tLOG(15, \"handle %p func %s\", handle, func);\n\n\tif (handle == NULL) {\n\t\tchar *errstr = util_dlerror();\n\t\tif (errstr)\n\t\t\tERR(\"%s(): %s\", func, errstr);\n\t\terrno = ELIBACC;\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n/*\n * util_remote_unload_core -- (internal) unload remote library (core function)\n */\nstatic void\nutil_remote_unload_core(void)\n{\n\tif (Rpmem_handle_remote != NULL) {\n\t\tutil_dlclose(Rpmem_handle_remote);\n\t\tRpmem_handle_remote = NULL;\n\t}\n\tRpmem_create = NULL;\n\tRpmem_open = NULL;\n\tRpmem_close = NULL;\n\tRpmem_persist = NULL;\n\tRpmem_deep_persist = NULL;\n\tRpmem_read = NULL;\n\tRpmem_remove = NULL;\n\tRpmem_set_attr = NULL;\n}\n\n/*\n * util_remote_unload -- unload remote library\n */\nvoid\nutil_remote_unload(void)\n{\n\tLOG(3, NULL);\n\n\tif (!Remote_replication_available)\n\t\treturn;\n\n\tutil_mutex_lock(&Remote_lock);\n\n\tutil_remote_unload_core();\n\n\tutil_mutex_unlock(&Remote_lock);\n}\n\n/*\n * util_remote_load -- load remote library\n */\nint\nutil_remote_load(void)\n{\n\tLOG(3, NULL);\n\n\tif (!Remote_replication_available) {\n\t\tERR(\"remote replication is not available\");\n\t\treturn -1;\n\t}\n\n\tCHECK_FUNC_COMPATIBLE(rpmem_create, *Rpmem_create);\n\tCHECK_FUNC_COMPATIBLE(rpmem_open, *Rpmem_open);\n\tCHECK_FUNC_COMPATIBLE(rpmem_close, *Rpmem_close);\n\tCHECK_FUNC_COMPATIBLE(rpmem_persist, *Rpmem_persist);\n\tCHECK_FUNC_COMPATIBLE(rpmem_deep_persist, *Rpmem_deep_persist);\n\tCHECK_FUNC_COMPATIBLE(rpmem_read, *Rpmem_read);\n\tCHECK_FUNC_COMPATIBLE(rpmem_remove, *Rpmem_remove);\n\n\tutil_mutex_lock(&Remote_lock);\n\n\tif (Rpmem_handle_remote)\n\t\tgoto end;\n\n\tRpmem_handle_remote = util_dlopen(LIBRARY_REMOTE);\n\tif (util_dl_check_error(Rpmem_handle_remote, \"dlopen\")) {\n\t\tERR(\"the pool set requires a remote replica, \"\n\t\t    \"but the '%s' library cannot be loaded\",\n\t\t    LIBRARY_REMOTE);\n\t\tgoto err;\n\t}\n\n\tRpmem_create = util_dlsym(Rpmem_handle_remote, \"rpmem_create\");\n\tif (util_dl_check_error(Rpmem_create, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_create' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_open = util_dlsym(Rpmem_handle_remote, \"rpmem_open\");\n\tif (util_dl_check_error(Rpmem_open, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_open' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_close = util_dlsym(Rpmem_handle_remote, \"rpmem_close\");\n\tif (util_dl_check_error(Rpmem_close, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_close' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_persist = util_dlsym(Rpmem_handle_remote, \"rpmem_persist\");\n\tif (util_dl_check_error(Rpmem_persist, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_persist' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_deep_persist = util_dlsym(Rpmem_handle_remote,\n\t\t\t\"rpmem_deep_persist\");\n\tif (util_dl_check_error(Rpmem_deep_persist, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_deep_persist' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_read = util_dlsym(Rpmem_handle_remote, \"rpmem_read\");\n\tif (util_dl_check_error(Rpmem_read, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_read' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_remove = util_dlsym(Rpmem_handle_remote, \"rpmem_remove\");\n\tif (util_dl_check_error(Rpmem_remove, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_remove' not found\");\n\t\tgoto err;\n\t}\n\n\tRpmem_set_attr = util_dlsym(Rpmem_handle_remote, \"rpmem_set_attr\");\n\tif (util_dl_check_error(Rpmem_set_attr, \"dlsym\")) {\n\t\tERR(\"symbol 'rpmem_set_attr' not found\");\n\t\tgoto err;\n\t}\n\nend:\n\tutil_mutex_unlock(&Remote_lock);\n\treturn 0;\n\nerr:\n\tLOG(4, \"error clean up\");\n\tutil_remote_unload_core();\n\tutil_mutex_unlock(&Remote_lock);\n\treturn -1;\n}\n\n/* reserve space for size, path and some whitespace and/or comment */\n#define PARSER_MAX_LINE (PATH_MAX + 1024)\n\nenum parser_codes {\n\tPARSER_CONTINUE = 0,\n\tPARSER_PMEMPOOLSET,\n\tPARSER_REPLICA,\n\tPARSER_INVALID_TOKEN,\n\tPARSER_REMOTE_REPLICA_EXPECTED,\n\tPARSER_WRONG_SIZE,\n\tPARSER_CANNOT_READ_SIZE,\n\tPARSER_ABSOLUTE_PATH_EXPECTED,\n\tPARSER_RELATIVE_PATH_EXPECTED,\n\tPARSER_SET_NO_PARTS,\n\tPARSER_REP_NO_PARTS,\n\tPARSER_REMOTE_REP_UNEXPECTED_PARTS,\n\tPARSER_SIZE_MISMATCH,\n\tPARSER_OUT_OF_MEMORY,\n\tPARSER_OPTION_UNKNOWN,\n\tPARSER_OPTION_EXPECTED,\n\tPARSER_FORMAT_OK,\n\tPARSER_MAX_CODE\n};\n\nstatic const char *parser_errstr[PARSER_MAX_CODE] = {\n\t\"\", /* parsing */\n\t\"the first line must be exactly 'PMEMPOOLSET'\",\n\t\"exactly 'REPLICA' expected\",\n\t\"invalid token found in the current line\",\n\t\"address of remote node and descriptor of remote pool set expected\",\n\t\"incorrect format of size\",\n\t\"cannot determine size of a part\",\n\t\"incorrect path (must be an absolute one)\",\n\t\"incorrect descriptor (must be a relative path)\",\n\t\"no pool set parts\",\n\t\"no replica parts\",\n\t\"unexpected parts for remote replica\",\n\t\"sizes of pool set and replica mismatch\",\n\t\"allocating memory failed\",\n\t\"unknown option\",\n\t\"missing option name\",\n\t\"\" /* format correct */\n};\n\n/*\n * util_replica_force_page_allocation - (internal) forces page allocation for\n * replica\n */\nstatic void\nutil_replica_force_page_allocation(struct pool_replica *rep)\n{\n\tvolatile char *cur_addr = rep->part[0].addr;\n\tchar *addr_end = (char *)cur_addr + rep->resvsize;\n\tfor (; cur_addr < addr_end; cur_addr += Pagesize) {\n\t\t*cur_addr = *cur_addr;\n\t\tVALGRIND_SET_CLEAN(cur_addr, 1);\n\t}\n}\n\n/*\n * util_map_hdr -- map a header of a pool set\n */\nint\nutil_map_hdr(struct pool_set_part *part, int flags, int rdonly)\n{\n\tLOG(3, \"part %p flags %d\", part, flags);\n\n\tCOMPILE_ERROR_ON(POOL_HDR_SIZE == 0);\n\tASSERTeq(POOL_HDR_SIZE % Pagesize, 0);\n\n\t/*\n\t * Workaround for Device DAX not allowing to map a portion\n\t * of the device if offset/length are not aligned to the internal\n\t * device alignment (page size).  I.e. if the device alignment\n\t * is 2M, we cannot map the 4K header, but need to align the mapping\n\t * length to 2M.\n\t *\n\t * According to mmap(2), system should automatically align mapping\n\t * length to be a multiple of the underlying page size, but it's\n\t * not true for Device DAX.\n\t */\n\tsize_t hdrsize = part->alignment > POOL_HDR_SIZE\n\t\t\t? part->alignment : POOL_HDR_SIZE;\n\n\tvoid *addr = NULL;\n\n#if VG_MEMCHECK_ENABLED\n\tif (On_valgrind) {\n\t\t/* this is required only for Device DAX & memcheck */\n\t\taddr = util_map_hint(hdrsize, hdrsize);\n\t\tif (addr == MAP_FAILED) {\n\t\t\tERR(\"cannot find a contiguous region of given size\");\n\t\t\t/* there's nothing we can do */\n\t\t\treturn -1;\n\t\t}\n\t}\n#endif\n\n\tint prot = rdonly ? PROT_READ : PROT_READ|PROT_WRITE;\n\tvoid *hdrp = util_map_sync(addr, hdrsize, prot, flags,\n\t\t\tpart->fd, 0, &part->hdr_map_sync);\n\tif (hdrp == MAP_FAILED) {\n\t\tERR(\"!mmap: %s\", part->path);\n\t\treturn -1;\n\t}\n\n\tpart->hdrsize = hdrsize;\n\tpart->hdr = hdrp;\n\n\tVALGRIND_REGISTER_PMEM_MAPPING(part->hdr, part->hdrsize);\n\tVALGRIND_REGISTER_PMEM_FILE(part->fd, part->hdr, part->hdrsize, 0);\n\n\treturn 0;\n}\n\n/*\n * util_unmap_hdr -- unmap pool set part header\n */\nvoid\nutil_unmap_hdr(struct pool_set_part *part)\n{\n\tif (part->hdr == NULL || part->hdrsize == 0)\n\t\treturn;\n\n\tLOG(4, \"munmap: addr %p size %zu\", part->hdr, part->hdrsize);\n\tVALGRIND_REMOVE_PMEM_MAPPING(part->hdr, part->hdrsize);\n\tif (munmap(part->hdr, part->hdrsize) != 0)\n\t\t/* this means there's a bug on the caller side */\n\t\tFATAL(\"!munmap: %s\", part->path);\n\tpart->hdr = NULL;\n\tpart->hdrsize = 0;\n}\n\n/*\n * util_map_part -- map a part of a pool set\n */\nint\nutil_map_part(struct pool_set_part *part, void *addr, size_t size,\n\tsize_t offset, int flags, int rdonly)\n{\n\tLOG(3, \"part %p addr %p size %zu offset %zu flags %d\",\n\t\tpart, addr, size, offset, flags);\n\n\tASSERTeq((uintptr_t)addr % Mmap_align, 0);\n\tASSERTeq(offset % Mmap_align, 0);\n\tASSERTeq(size % Mmap_align, 0);\n\tASSERT(((os_off_t)offset) >= 0);\n\tASSERTeq(offset % part->alignment, 0);\n\tASSERT(offset < part->filesize);\n\n\tif (!size)\n\t\tsize = (part->filesize - offset) & ~(part->alignment - 1);\n\telse\n\t\tsize = roundup(size, part->alignment);\n\n\tint prot = rdonly ? PROT_READ : PROT_READ | PROT_WRITE;\n\tvoid *addrp = util_map_sync(addr, size, prot, flags, part->fd,\n\t\t\t(os_off_t)offset, &part->map_sync);\n\tif (addrp == MAP_FAILED) {\n\t\tERR(\"!mmap: %s\", part->path);\n\t\treturn -1;\n\t}\n\n\tif (addr != NULL && (flags & MAP_FIXED) && addrp != addr) {\n\t\tERR(\"unable to map at requested address %p\", addr);\n\t\tmunmap(addrp, size);\n\t\treturn -1;\n\t}\n\n\tpart->addr = addrp;\n\tpart->size = size;\n\n\tVALGRIND_REGISTER_PMEM_MAPPING(part->addr, part->size);\n\tVALGRIND_REGISTER_PMEM_FILE(part->fd, part->addr, part->size, offset);\n\n\treturn 0;\n}\n\n/*\n * util_unmap_part -- unmap a part of a pool set\n */\nint\nutil_unmap_part(struct pool_set_part *part)\n{\n\tLOG(3, \"part %p\", part);\n\n\tif (part->addr != NULL && part->size != 0) {\n\t\tLOG(4, \"munmap: addr %p size %zu\", part->addr, part->size);\n\t\tVALGRIND_REMOVE_PMEM_MAPPING(part->addr, part->size);\n\t\tif (munmap(part->addr, part->size) != 0) {\n\t\t\tERR(\"!munmap: %s\", part->path);\n\t\t}\n\n\t\tpart->addr = NULL;\n\t\tpart->size = 0;\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_unmap_parts -- unmap parts from start_index to the end_index\n */\nint\nutil_unmap_parts(struct pool_replica *rep, unsigned start_index,\n\tunsigned end_index)\n{\n\tLOG(3, \"rep: %p, start_index: %u, end_index: %u\", rep, start_index,\n\t\tend_index);\n\n\tfor (unsigned p = start_index; p <= end_index; p++)\n\t\tutil_unmap_part(&rep->part[p]);\n\n\treturn 0;\n}\n\n/*\n * util_poolset_free -- free pool set info\n */\nvoid\nutil_poolset_free(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tif (rep->remote == NULL) {\n\t\t\t/* only local replicas have paths */\n\t\t\tfor (unsigned p = 0; p < rep->nallocated; p++) {\n\t\t\t\tFree((void *)(rep->part[p].path));\n\t\t\t}\n\t\t} else {\n\t\t\t/* remote replica */\n\t\t\tASSERTeq(rep->nparts, 1);\n\t\t\tFree(rep->remote->node_addr);\n\t\t\tFree(rep->remote->pool_desc);\n\t\t\tFree(rep->remote);\n\t\t}\n\t\tstruct pool_set_directory *d;\n\t\tVEC_FOREACH_BY_PTR(d, &rep->directory) {\n\t\t\tFree((void *)d->path);\n\t\t}\n\t\tVEC_DELETE(&rep->directory);\n\t\tFree(set->replica[r]);\n\t}\n\tFree(set->path);\n\tFree(set);\n}\n\n/*\n * util_poolset_open -- open all replicas from a poolset\n */\nint\nutil_poolset_open(struct pool_set *set)\n{\n\tfor (unsigned r = 0; r < set->nreplicas; ++r) {\n\t\tif (util_replica_open(set, r, MAP_SHARED)) {\n\t\t\tLOG(2, \"replica open failed: replica %u\", r);\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_replica_close_local -- close local replica, optionally delete the\n *                             replica's parts\n */\nint\nutil_replica_close_local(struct pool_replica *rep, unsigned repn,\n\t\tenum del_parts_mode del)\n{\n\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\tif (rep->part[p].fd != -1)\n\t\t\t(void) os_close(rep->part[p].fd);\n\n\t\tif ((del == DELETE_CREATED_PARTS && rep->part[p].created) ||\n\t\t\t\tdel == DELETE_ALL_PARTS) {\n\t\t\tLOG(4, \"unlink %s\", rep->part[p].path);\n\t\t\tint olderrno = errno;\n\t\t\tif (util_unlink(rep->part[p].path) && errno != ENOENT) {\n\t\t\t\tERR(\"!unlink %s failed (part %u, replica %u)\",\n\t\t\t\t\t\trep->part[p].path, p, repn);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\terrno = olderrno;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * util_replica_close_remote -- close remote replica, optionally delete the\n *                              replica\n */\nint\nutil_replica_close_remote(struct pool_replica *rep, unsigned repn,\n\t\tenum del_parts_mode del)\n{\n\tif (!rep->remote)\n\t\treturn 0;\n\n\tif (rep->remote->rpp) {\n\t\tLOG(4, \"closing remote replica #%u\", repn);\n\t\tRpmem_close(rep->remote->rpp);\n\t\trep->remote->rpp = NULL;\n\t}\n\n\tif ((del == DELETE_CREATED_PARTS && rep->part[0].created) ||\n\t\t\tdel == DELETE_ALL_PARTS) {\n\t\tLOG(4, \"removing remote replica #%u\", repn);\n\t\tint ret = Rpmem_remove(rep->remote->node_addr,\n\t\t\trep->remote->pool_desc, 0);\n\t\tif (ret) {\n\t\t\tLOG(1, \"!removing remote replica #%u failed\", repn);\n\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * util_poolset_close -- unmap and close all the parts of the pool set,\n *                       optionally delete parts\n */\nvoid\nutil_poolset_close(struct pool_set *set, enum del_parts_mode del)\n{\n\tLOG(3, \"set %p del %d\", set, del);\n\n\tint oerrno = errno;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tutil_replica_close(set, r);\n\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tif (!rep->remote)\n\t\t\t(void) util_replica_close_local(rep, r, del);\n\t\telse\n\t\t\t(void) util_replica_close_remote(rep, r, del);\n\t}\n\n\t/*\n\t * XXX On FreeBSD, mmap()ing a file does not increment the flock()\n\t *     reference count, so we had to keep the files open until now.\n\t */\n#ifdef __FreeBSD__\n\tutil_poolset_fdclose_always(set);\n#endif\n\tutil_poolset_free(set);\n\n\terrno = oerrno;\n}\n\n/*\n * util_poolset_chmod -- change mode for all created files related to pool set\n */\nint\nutil_poolset_chmod(struct pool_set *set, mode_t mode)\n{\n\tLOG(3, \"set %p mode %o\", set, mode);\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\n\t\t/* skip remote replicas */\n\t\tif (rep->remote != NULL)\n\t\t\tcontinue;\n\n\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\tstruct pool_set_part *part = &rep->part[p];\n\n\t\t\t/* skip not created or closed parts */\n\t\t\tif (!part->created || part->fd == -1)\n\t\t\t\tcontinue;\n\n\t\t\tos_stat_t stbuf;\n\t\t\tif (os_fstat(part->fd, &stbuf) != 0) {\n\t\t\t\tERR(\"!fstat %d %s\", part->fd, part->path);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (stbuf.st_mode & ~(unsigned)S_IFMT) {\n\t\t\t\tLOG(1, \"file permissions changed during pool \"\n\t\t\t\t\t\"initialization, file: %s (%o)\",\n\t\t\t\t\tpart->path,\n\t\t\t\t\tstbuf.st_mode & ~(unsigned)S_IFMT);\n\t\t\t}\n\n\t\t\tif (os_chmod(part->path, mode)) {\n\t\t\t\tERR(\"!chmod %u/%u/%s\", r, p, part->path);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_fdclose_always -- close file descriptors related to pool set\n */\nvoid\nutil_poolset_fdclose_always(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++)\n\t\tutil_replica_fdclose(set->replica[r]);\n}\n\n/*\n * util_poolset_fdclose -- close pool set file descriptors if not FreeBSD\n *\n * XXX On FreeBSD, mmap()ing a file does not increment the flock()\n *\treference count, so we need to keep the files open.\n */\nvoid\nutil_poolset_fdclose(struct pool_set *set)\n{\n#ifdef __FreeBSD__\n\tLOG(3, \"set %p: holding open\", set);\n#else\n\tutil_poolset_fdclose_always(set);\n#endif\n}\n\n/*\n * util_autodetect_size -- (internal) retrieves size of an existing file\n */\nstatic ssize_t\nutil_autodetect_size(const char *path)\n{\n\tenum file_type type = util_file_get_type(path);\n\tif (type < 0)\n\t\treturn -1;\n\n\tif (type == TYPE_NORMAL) {\n\t\tERR(\"size autodetection is supported only for device dax\");\n\t\treturn -1;\n\t}\n\n\treturn util_file_get_size(path);\n}\n\n/*\n * parser_read_line -- (internal) read line and validate size and path\n *                      from a pool set file\n */\nstatic enum parser_codes\nparser_read_line(char *line, size_t *size, char **path)\n{\n\tint ret;\n\tchar *size_str;\n\tchar *path_str;\n\tchar *rest_str;\n\tchar *saveptr = NULL; /* must be NULL initialized on Windows */\n\n\tsize_str = strtok_r(line, \" \\t\", &saveptr);\n\tpath_str = strtok_r(NULL, \" \\t\", &saveptr);\n\trest_str = strtok_r(NULL, \" \\t\", &saveptr);\n\n\tif (!size_str || !path_str || rest_str)\n\t\treturn PARSER_INVALID_TOKEN;\n\n\tLOG(10, \"size '%s' path '%s'\", size_str, path_str);\n\n\t/*\n\t * A format of the size is checked in detail. As regards the path,\n\t * it is checked only if the read path is an absolute path.\n\t * The rest should be checked during creating/opening the file.\n\t */\n\n\t/* check if the read path is an absolute path */\n\tif (!util_is_absolute_path(path_str))\n\t\treturn PARSER_ABSOLUTE_PATH_EXPECTED;\n\n\t*path = Strdup(path_str);\n\tif (!(*path)) {\n\t\tERR(\"!Strdup\");\n\t\treturn PARSER_OUT_OF_MEMORY;\n\t}\n\n\tif (strcmp(SIZE_AUTODETECT_STR, size_str) == 0) {\n\t\t/*\n\t\t * XXX: this should be done after the parsing completes, but\n\t\t * currently this operation is performed in simply too many\n\t\t * places in the code to move this someplace else.\n\t\t */\n\t\tssize_t s = util_autodetect_size(path_str);\n\t\tif (s < 0) {\n\t\t\tFree(*path);\n\t\t\t*path = NULL;\n\t\t\treturn PARSER_CANNOT_READ_SIZE;\n\t\t}\n\n\t\t*size = (size_t)s;\n\n\t\treturn PARSER_CONTINUE;\n\t}\n\n\tret = util_parse_size(size_str, size);\n\tif (ret != 0 || *size == 0) {\n\t\tFree(*path);\n\t\t*path = NULL;\n\t\treturn PARSER_WRONG_SIZE;\n\t}\n\n\treturn PARSER_CONTINUE;\n}\n\n/*\n * parser_read_replica -- (internal) read line and validate remote replica\n *                        from a pool set file\n */\nstatic enum parser_codes\nparser_read_replica(char *line, char **node_addr, char **pool_desc)\n{\n\tchar *addr_str;\n\tchar *desc_str;\n\tchar *rest_str;\n\tchar *saveptr = NULL; /* must be NULL initialized on Windows */\n\n\taddr_str = strtok_r(line, \" \\t\", &saveptr);\n\tdesc_str = strtok_r(NULL, \" \\t\", &saveptr);\n\trest_str = strtok_r(NULL, \" \\t\", &saveptr);\n\n\tif (!addr_str || !desc_str)\n\t\treturn PARSER_REMOTE_REPLICA_EXPECTED;\n\n\tif (rest_str)\n\t\treturn PARSER_INVALID_TOKEN;\n\n\tLOG(10, \"node address '%s' pool set descriptor '%s'\",\n\t\taddr_str, desc_str);\n\n\t/* check if the descriptor is a relative path */\n\tif (util_is_absolute_path(desc_str))\n\t\treturn PARSER_RELATIVE_PATH_EXPECTED;\n\n\t*node_addr = Strdup(addr_str);\n\t*pool_desc = Strdup(desc_str);\n\n\tif (!(*node_addr) || !(*pool_desc)) {\n\t\tERR(\"!Strdup\");\n\t\tif (*node_addr)\n\t\t\tFree(*node_addr);\n\t\tif (*pool_desc)\n\t\t\tFree(*pool_desc);\n\t\treturn PARSER_OUT_OF_MEMORY;\n\t}\n\n\treturn PARSER_CONTINUE;\n}\n\n/*\n * parser_read_options -- (internal) read line and validate options\n */\nstatic enum parser_codes\nparser_read_options(char *line, unsigned *options)\n{\n\tLOG(3, \"line '%s'\", line);\n\n\tint opt_cnt = 0;\n\tchar *saveptr = NULL; /* must be NULL initialized on Windows */\n\n\tchar *opt_str = strtok_r(line, \" \\t\", &saveptr);\n\twhile (opt_str != NULL) {\n\t\tLOG(4, \"option '%s'\", opt_str);\n\n\t\tint i = 0;\n\t\twhile (Options[i].name && strcmp(opt_str, Options[i].name) != 0)\n\t\t\ti++;\n\n\t\tif (Options[i].name == NULL) {\n\t\t\tLOG(4, \"unknown option '%s'\", opt_str);\n\t\t\treturn PARSER_OPTION_UNKNOWN;\n\t\t}\n\n\t\tif (*options & Options[i].flag)\n\t\t\tLOG(4, \"duplicated option '%s'\", opt_str);\n\n\t\t*options |= Options[i].flag;\n\n\t\topt_cnt++;\n\t\topt_str = strtok_r(NULL, \" \\t\", &saveptr);\n\t}\n\n\tif (opt_cnt == 0)\n\t\treturn PARSER_OPTION_EXPECTED;\n\n\treturn PARSER_CONTINUE;\n}\n\n/*\n * util_replica_reserve -- reserves part slots capacity in a replica\n */\nstatic int\nutil_replica_reserve(struct pool_replica **repp, unsigned n)\n{\n\tLOG(3, \"replica %p n %u\", *repp, n);\n\n\tstruct pool_replica *rep = *repp;\n\tif (rep->nallocated >= n)\n\t\treturn 0;\n\n\trep = Realloc(rep, sizeof(struct pool_replica) +\n\t\t(n) * sizeof(struct pool_set_part));\n\tif (rep == NULL) {\n\t\tERR(\"!Realloc\");\n\t\treturn -1;\n\t}\n\n\tsize_t nsize = sizeof(struct pool_set_part) * (n - rep->nallocated);\n\tmemset(rep->part + rep->nallocated, 0, nsize);\n\n\trep->nallocated = n;\n\t*repp = rep;\n\n\treturn 0;\n}\n\n/*\n * util_replica_add_part_by_idx -- (internal) allocates, initializes and adds a\n *\tpart structure at the provided location in the replica info\n */\nstatic int\nutil_replica_add_part_by_idx(struct pool_replica **repp,\n\tconst char *path, size_t filesize, unsigned p)\n{\n\tLOG(3, \"replica %p path %s filesize %zu\", *repp, path, filesize);\n\n\tif (util_replica_reserve(repp, p + 1) != 0)\n\t\treturn -1;\n\n\tstruct pool_replica *rep = *repp;\n\tASSERTne(rep, NULL);\n\n\tint is_dev_dax = 0;\n\n\tif (path != NULL) {\n\t\tenum file_type type = util_file_get_type(path);\n\t\tif (type == OTHER_ERROR)\n\t\t\treturn -1;\n\n\t\tis_dev_dax = type == TYPE_DEVDAX;\n\t}\n\n\trep->part[p].path = path;\n\trep->part[p].filesize = filesize;\n\trep->part[p].fd = -1;\n\trep->part[p].is_dev_dax = is_dev_dax;\n\trep->part[p].created = 0;\n\trep->part[p].hdr = NULL;\n\trep->part[p].addr = NULL;\n\trep->part[p].remote_hdr = NULL;\n\trep->part[p].has_bad_blocks = 0;\n\n\tif (is_dev_dax)\n\t\trep->part[p].alignment = util_file_device_dax_alignment(path);\n\telse\n\t\trep->part[p].alignment = Mmap_align;\n\n\tASSERTne(rep->part[p].alignment, 0);\n\n\trep->nparts += 1;\n\n\treturn 0;\n}\n\n/*\n * util_replica_add_part -- adds a next part in replica info\n */\nstatic int\nutil_replica_add_part(struct pool_replica **repp,\n\tconst char *path, size_t filesize)\n{\n\tLOG(3, \"replica %p path \\\"%s\\\" filesize %zu\", *repp, path, filesize);\n\n\treturn util_replica_add_part_by_idx(repp, path,\n\t\tfilesize, (*repp)->nparts);\n}\n\n/*\n * util_parse_add_part -- (internal) add a new part file to the replica info\n */\nstatic int\nutil_parse_add_part(struct pool_set *set, const char *path, size_t filesize)\n{\n\tLOG(3, \"set %p path %s filesize %zu\", set, path, filesize);\n\n\tASSERTne(set, NULL);\n\n\tif (set->directory_based) {\n\t\tERR(\"cannot mix directories and files in a set\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\treturn util_replica_add_part(&set->replica[set->nreplicas - 1],\n\t\tpath, filesize);\n}\n\n/*\n * util_parse_add_directory --\n *\t(internal) add a new directory to the replica info\n */\nstatic int\nutil_parse_add_directory(struct pool_set *set, const char *path,\n\tsize_t filesize)\n{\n\tLOG(3, \"set %p path %s filesize %zu\", set, path, filesize);\n\n\tASSERTne(set, NULL);\n\n\tstruct pool_replica *rep = set->replica[set->nreplicas - 1];\n\tASSERTne(rep, NULL);\n\n\tif (set->directory_based == 0) {\n\t\tif (rep->nparts > 0 || set->nreplicas > 1) {\n\t\t\tERR(\"cannot mix directories and files in a set\");\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\t\tset->directory_based = 1;\n\t}\n\n\tchar *rpath = util_part_realpath(path);\n\tif (rpath == NULL) {\n\t\tERR(\"cannot resolve realpath of new directory\");\n\t\treturn -1;\n\t}\n\n\tfor (unsigned i = 0; i < set->nreplicas; ++i) {\n\t\tstruct pool_replica *r = set->replica[i];\n\t\tstruct pool_set_directory *dir;\n\t\tchar *dpath = NULL;\n\t\tVEC_FOREACH_BY_PTR(dir, &r->directory) {\n\t\t\tdpath = util_part_realpath(dir->path);\n\t\t\tASSERTne(dpath, NULL); /* must have been resolved */\n\t\t\tif (strcmp(rpath, dpath) == 0) {\n\t\t\t\tERR(\"cannot use the same directory twice\");\n\t\t\t\terrno = EEXIST;\n\t\t\t\tfree(dpath);\n\t\t\t\tfree(rpath);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tfree(dpath);\n\t\t}\n\t}\n\tfree(rpath);\n\n\tstruct pool_set_directory d;\n\td.path = path;\n\td.resvsize = filesize;\n\n\tif (VEC_PUSH_BACK(&rep->directory, d) != 0)\n\t\treturn -1;\n\n\trep->resvsize += filesize;\n\n\treturn 0;\n}\n\n/*\n * util_parse_add_element --\n *\t(internal) add a new element to the replica info\n */\nstatic int\nutil_parse_add_element(struct pool_set *set, const char *path, size_t filesize)\n{\n\tLOG(3, \"set %p path %s filesize %zu\", set, path, filesize);\n\n\tos_stat_t stat;\n\n\tint olderrno = errno;\n\n\tif (os_stat(path, &stat) == 0 && S_ISDIR(stat.st_mode))\n\t\treturn util_parse_add_directory(set, path, filesize);\n\n\terrno = olderrno;\n\n\treturn util_parse_add_part(set, path, filesize);\n}\n\n/*\n * util_parse_add_replica -- (internal) add a new replica to the pool set info\n */\nstatic int\nutil_parse_add_replica(struct pool_set **setp)\n{\n\tLOG(3, \"setp %p\", setp);\n\n\tASSERTne(setp, NULL);\n\n\tstruct pool_set *set = *setp;\n\tASSERTne(set, NULL);\n\n\tset = Realloc(set, sizeof(struct pool_set) +\n\t\t\t(set->nreplicas + 1) * sizeof(struct pool_replica *));\n\tif (set == NULL) {\n\t\tERR(\"!Realloc\");\n\t\treturn -1;\n\t}\n\t*setp = set;\n\n\tstruct pool_replica *rep;\n\trep = Zalloc(sizeof(struct pool_replica));\n\tif (rep == NULL) {\n\t\tERR(\"!Zalloc\");\n\t\treturn -1;\n\t}\n\n\tVEC_INIT(&rep->directory);\n\n\tunsigned r = set->nreplicas++;\n\n\tset->replica[r] = rep;\n\n\treturn 0;\n}\n\n/*\n * util_replica_check_map_sync -- (internal) check MAP_SYNC restrictions\n */\nstatic int\nutil_replica_check_map_sync(struct pool_set *set, unsigned repidx,\n\tint check_hdr)\n{\n\tLOG(3, \"set %p repidx %u\", set, repidx);\n\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\tint map_sync = rep->part[0].map_sync;\n\n\n\tfor (unsigned p = 1; p < rep->nparts; p++) {\n\t\tif (map_sync != rep->part[p].map_sync) {\n\t\t\tERR(\"replica #%u part %u %smapped with MAP_SYNC\",\n\t\t\t\trepidx, p, rep->part[p].map_sync ? \"\" : \"not\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (check_hdr) {\n\t\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\t\tif (map_sync != rep->part[p].hdr_map_sync) {\n\t\t\t\tERR(\"replica #%u part %u header %smapped \"\n\t\t\t\t\t\"with MAP_SYNC\", repidx, p,\n\t\t\t\t\trep->part[p].hdr_map_sync ?\n\t\t\t\t\t\"\" : \"not\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_check_devdax -- (internal) check Device DAX restrictions\n */\nstatic int\nutil_poolset_check_devdax(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\n\tif (set->directory_based)\n\t\treturn 0;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tint is_dev_dax = rep->part[0].is_dev_dax;\n\n\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\tif (rep->part[p].is_dev_dax != is_dev_dax) {\n\t\t\t\tERR(\n\t\t\t\t\t\"either all the parts must be Device DAX or none\");\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (is_dev_dax && rep->nparts > 1 &&\n\t\t\t\t\t(set->options & (OPTION_SINGLEHDR |\n\t\t\t\t\tOPTION_NOHDRS)) == 0 &&\n\t\t\t    util_file_device_dax_alignment(rep->part[p].path)\n\t\t\t\t\t!= Pagesize) {\n\t\t\t\tERR(\n\t\t\t\t\t\"Multiple DAX devices with alignment other than 4KB. Use the SINGLEHDR poolset option.\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * util_poolset_check_options -- (internal) check if poolset options are\n *                               admissible\n */\nstatic int\nutil_poolset_check_options(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\tif ((set->options & OPTION_SINGLEHDR) &&\n\t\t\t(set->options & OPTION_NOHDRS)) {\n\t\tERR(\n\t\t\"both SINGLEHDR and NOHDR poolset options used at the same time\");\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n/*\n * util_poolset_set_size -- (internal) calculate pool size\n */\nstatic void\nutil_poolset_set_size(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\n\tset->poolsize = SIZE_MAX;\n\tset->resvsize = SIZE_MAX;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\n\t\tif (set->options & OPTION_SINGLEHDR)\n\t\t\trep->nhdrs = 1;\n\t\telse if (set->options & OPTION_NOHDRS)\n\t\t\trep->nhdrs = 0;\n\t\telse\n\t\t\trep->nhdrs = rep->nparts;\n\n\t\trep->repsize = 0;\n\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\trep->repsize +=\n\t\t\t\t(rep->part[p].filesize & ~(Mmap_align - 1));\n\t\t}\n\t\tif (rep->nhdrs > 0)\n\t\t\trep->repsize -= (rep->nhdrs - 1) * Mmap_align;\n\n\t\tif (rep->resvsize == 0)\n\t\t\trep->resvsize = rep->repsize;\n\n\t\t/*\n\t\t * Calculate pool size - choose the smallest replica size.\n\t\t * Ignore remote replicas.\n\t\t */\n\t\tif (rep->remote == NULL && rep->repsize < set->poolsize)\n\t\t\tset->poolsize = rep->repsize;\n\t\tif (rep->remote == NULL && rep->resvsize < set->resvsize)\n\t\t\tset->resvsize = rep->resvsize;\n\t}\n\n\tLOG(3, \"pool size set to %zu\", set->poolsize);\n}\n\n/*\n * util_parse_add_remote_replica -- (internal) add a new remote replica\n *                                  to the pool set info\n */\nstatic int\nutil_parse_add_remote_replica(struct pool_set **setp, char *node_addr,\n\t\t\t\tchar *pool_desc)\n{\n\tLOG(3, \"setp %p node_addr %s pool_desc %s\", setp, node_addr, pool_desc);\n\n\tASSERTne(setp, NULL);\n\tASSERTne(node_addr, NULL);\n\tASSERTne(pool_desc, NULL);\n\n\tint ret = util_parse_add_replica(setp);\n\tif (ret != 0)\n\t\treturn ret;\n\n\t/*\n\t * A remote replica has one fake part of size equal twice pool header\n\t * size for storing pool header and pool descriptor.\n\t */\n\tret = util_parse_add_part(*setp, NULL, 2 * POOL_HDR_SIZE);\n\tif (ret != 0)\n\t\treturn ret;\n\n\tstruct pool_set *set = *setp;\n\tstruct pool_replica *rep = set->replica[set->nreplicas - 1];\n\tASSERTne(rep, NULL);\n\n\trep->remote = Zalloc(sizeof(struct remote_replica));\n\tif (rep->remote == NULL) {\n\t\tERR(\"!Malloc\");\n\t\treturn -1;\n\t}\n\trep->remote->node_addr = node_addr;\n\trep->remote->pool_desc = pool_desc;\n\tset->remote = 1;\n\n\treturn 0;\n}\n\n/*\n * util_readline -- read line from stream\n */\nstatic char *\nutil_readline(FILE *fh)\n{\n\tLOG(10, \"fh %p\", fh);\n\n\tsize_t bufsize = PARSER_MAX_LINE;\n\tsize_t position = 0;\n\tchar *buffer = NULL;\n\n\tdo {\n\t\tchar *tmp = buffer;\n\t\tbuffer = Realloc(buffer, bufsize);\n\t\tif (buffer == NULL) {\n\t\t\tFree(tmp);\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/* ensure if we can cast bufsize to int */\n\t\tASSERT(bufsize / 2 <= INT_MAX);\n\t\tASSERT((bufsize - position) >= (bufsize / 2));\n\t\tchar *s = util_fgets(buffer + position, (int)bufsize / 2, fh);\n\t\tif (s == NULL) {\n\t\t\tFree(buffer);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tposition = strlen(buffer);\n\t\tbufsize *= 2;\n\t} while (!feof(fh) && buffer[position - 1] != '\\n');\n\n\treturn buffer;\n}\n\n\n/*\n * util_part_idx_by_file_name -- (internal) retrieves the part index from a\n *\tname of the file that is an element of a directory poolset\n */\nstatic long\nutil_part_idx_by_file_name(const char *filename)\n{\n\tLOG(3, \"filename \\\"%s\\\"\", filename);\n\n\tint olderrno = errno;\n\terrno = 0;\n\tlong part_idx = strtol(filename, NULL, 10);\n\tif (errno != 0)\n\t\treturn -1;\n\n\terrno = olderrno;\n\n\treturn part_idx;\n}\n\n/*\n * util_poolset_directory_load -- (internal) loads and initializes all\n *\texisting parts in a single directory\n */\nstatic int\nutil_poolset_directory_load(struct pool_replica **repp, const char *directory)\n{\n\tLOG(3, \"rep %p dir \\\"%s\\\"\", *repp, directory);\n\n\tstruct fs *f = fs_new(directory);\n\tif (f == NULL) {\n\t\tERR(\"!fs_new: \\\"%s\\\"\", directory);\n\t\treturn -1;\n\t}\n\n\tint nparts = 0;\n\tchar *path = NULL;\n\n\tstruct fs_entry *entry;\n\twhile ((entry = fs_read(f)) != NULL) {\n\t\tif (entry->level != 1)\n\t\t\tcontinue;\n\t\tif (entry->type != FS_ENTRY_FILE)\n\t\t\tcontinue;\n\t\tif (entry->namelen < PMEM_EXT_LEN)\n\t\t\tcontinue;\n\t\tconst char *ext = entry->path + entry->pathlen -\n\t\t\tPMEM_EXT_LEN + 1;\n\t\tif (strcmp(PMEM_EXT, ext) != 0)\n\t\t\tcontinue;\n\n\t\tlong part_idx = util_part_idx_by_file_name(entry->name);\n\t\tif (part_idx < 0)\n\t\t\tcontinue;\n\n\t\tssize_t size = util_file_get_size(entry->path);\n\t\tif (size < 0) {\n\t\t\tLOG(2,\n\t\t\t\"cannot read size of file (%s) in a poolset directory\",\n\t\t\tentry->path);\n\t\t\tgoto err;\n\t\t}\n\n\t\tif ((path = Strdup(entry->path)) == NULL) {\n\t\t\tERR(\"!Strdup\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (util_replica_add_part_by_idx(repp, path,\n\t\t\t\t(size_t)size, (unsigned)part_idx) != 0) {\n\t\t\tERR(\"unable to load part %s\", entry->path);\n\t\t\tgoto err;\n\t\t}\n\t\tnparts++;\n\t}\n\n\tfs_delete(f);\n\treturn nparts;\n\nerr:\n\tfs_delete(f);\n\treturn -1;\n}\n\n/*\n * util_poolset_directories_load -- (internal) loads and initializes all\n *\texisting parts in the poolset directories\n */\nstatic int\nutil_poolset_directories_load(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\n\tif (!set->directory_based)\n\t\treturn 0;\n\n\tunsigned next_part_id = 0;\n\tunsigned max_parts_rep = 0;\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tnext_part_id = 0;\n\n\t\tstruct pool_set_directory *d;\n\t\tint nparts = 0;\n\t\tint prev_nparts = 0;\n\t\tVEC_FOREACH_BY_PTR(d, &set->replica[r]->directory) {\n\t\t\tprev_nparts = nparts;\n\t\t\tnparts = util_poolset_directory_load(&set->replica[r],\n\t\t\t\td->path);\n\t\t\tif (nparts < 0) {\n\t\t\t\tERR(\"failed to load parts from directory %s\",\n\t\t\t\t\td->path);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tnext_part_id += (unsigned)nparts;\n\n\t\t\t/* always try to evenly spread files across dirs */\n\t\t\tif (r == 0 && prev_nparts > nparts)\n\t\t\t\tset->next_directory_id++;\n\t\t}\n\n\t\tif (next_part_id > set->replica[max_parts_rep]->nparts)\n\t\t\tmax_parts_rep = r;\n\n\t\tif (r == 0)\n\t\t\tset->next_id = next_part_id;\n\t}\n\n\t/*\n\t * In order to maintain the same semantics of poolset parsing for\n\t * regular poolsets and directory poolsets, we need to speculatively\n\t * recreate the information regarding any missing parts in replicas.\n\t */\n\tstruct pool_replica *rep;\n\tstruct pool_replica *mrep = set->replica[max_parts_rep];\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tif (set->replica[r]->nparts == mrep->nparts)\n\t\t\tcontinue;\n\n\t\tif (VEC_SIZE(&set->replica[r]->directory) == 0) {\n\t\t\tERR(\"no directories in replica\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (util_replica_reserve(&set->replica[r], mrep->nparts) != 0)\n\t\t\treturn -1;\n\n\t\trep = set->replica[r];\n\n\t\tstruct pool_set_directory *d = VEC_GET(&rep->directory, 0);\n\n\t\tfor (unsigned pidx = 0; pidx < rep->nallocated; ++pidx) {\n\t\t\tstruct pool_set_part *p = &rep->part[pidx];\n\t\t\t*p = mrep->part[pidx];\n\n\t\t\tsize_t path_len = strlen(d->path) + PMEM_FILE_MAX_LEN;\n\t\t\tif ((p->path = Malloc(path_len)) == NULL) {\n\t\t\t\tERR(\"!Malloc\");\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tsnprintf((char *)p->path, path_len,\n\t\t\t\t\t\"%s\" OS_DIR_SEP_STR \"%0*u%s\",\n\t\t\t\t\td->path, PMEM_FILE_PADDING,\n\t\t\t\t\tpidx, PMEM_EXT);\n\t\t}\n\t\trep->nparts = mrep->nparts;\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_parse -- parse pool set config file\n *\n * Returns 0 if the file is a valid poolset config file,\n * and -1 in case of any error.\n *\n * XXX: use memory mapped file\n */\nint\nutil_poolset_parse(struct pool_set **setp, const char *path, int fd)\n{\n\tLOG(3, \"setp %p path %s fd %d\", setp, path, fd);\n\n\tstruct pool_set *set = NULL;\n\tenum parser_codes result;\n\tchar *line;\n\tchar *ppath;\n\tchar *pool_desc;\n\tchar *node_addr;\n\tchar *cp;\n\tsize_t psize;\n\tFILE *fs;\n\tint oerrno;\n\n\tif (os_lseek(fd, 0, SEEK_SET) != 0) {\n\t\tERR(\"!lseek %d\", fd);\n\t\treturn -1;\n\t}\n\n\tfd = dup(fd);\n\tif (fd < 0) {\n\t\tERR(\"!dup\");\n\t\treturn -1;\n\t}\n\n\t/* associate a stream with the file descriptor */\n\tif ((fs = os_fdopen(fd, \"r\")) == NULL) {\n\t\tERR(\"!fdopen %d\", fd);\n\t\tos_close(fd);\n\t\treturn -1;\n\t}\n\n\tunsigned nlines = 0;\n\tunsigned nparts = 0; /* number of parts in current replica */\n\n\t/* read the first line */\n\tline = util_readline(fs);\n\tif (line == NULL) {\n\t\tERR(\"!Reading poolset file\");\n\t\tgoto err;\n\t}\n\tnlines++;\n\n\tset = Zalloc(sizeof(struct pool_set));\n\tif (set == NULL) {\n\t\tERR(\"!Malloc for pool set\");\n\t\tgoto err;\n\t}\n\n\tset->path = Strdup(path);\n\tif (set->path == NULL)  {\n\t\tERR(\"!Strdup\");\n\t\tgoto err;\n\t}\n\n\t/* check also if the last character is '\\n' */\n\tif (strncmp(line, POOLSET_HDR_SIG, POOLSET_HDR_SIG_LEN) == 0 &&\n\t    line[POOLSET_HDR_SIG_LEN] == '\\n') {\n\t\t/* 'PMEMPOOLSET' signature detected */\n\t\tLOG(10, \"PMEMPOOLSET\");\n\n\t\tint ret = util_parse_add_replica(&set);\n\t\tif (ret != 0)\n\t\t\tgoto err;\n\n\t\tnparts = 0;\n\t\tresult = PARSER_CONTINUE;\n\t} else {\n\t\tresult = PARSER_PMEMPOOLSET;\n\t}\n\n\twhile (result == PARSER_CONTINUE) {\n\t\tFree(line);\n\t\t/* read next line */\n\t\tline = util_readline(fs);\n\t\tnlines++;\n\n\t\tif (line) {\n\t\t\t/* chop off newline and comments */\n\t\t\tif ((cp = strchr(line, '\\n')) != NULL)\n\t\t\t\t*cp = '\\0';\n\t\t\tif (cp != line && (cp = strchr(line, '#')) != NULL)\n\t\t\t\t*cp = '\\0';\n\n\t\t\t/* skip comments and blank lines */\n\t\t\tif (cp == line)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (!line) {\n\t\t\tif (nparts >= 1) {\n\t\t\t\tresult = PARSER_FORMAT_OK;\n\t\t\t} else {\n\t\t\t\tif (set->nreplicas == 1)\n\t\t\t\t\tresult = PARSER_SET_NO_PARTS;\n\t\t\t\telse\n\t\t\t\t\tresult = PARSER_REP_NO_PARTS;\n\t\t\t}\n\t\t} else if (strncmp(line, POOLSET_OPTION_SIG,\n\t\t\t\t\tPOOLSET_OPTION_SIG_LEN) == 0) {\n\t\t\tresult = parser_read_options(\n\t\t\t\t\tline + POOLSET_OPTION_SIG_LEN,\n\t\t\t\t\t&set->options);\n\t\t\tif (result == PARSER_CONTINUE) {\n\t\t\t\tLOG(10, \"OPTIONS: %x\", set->options);\n\t\t\t}\n\t\t} else if (strncmp(line, POOLSET_REPLICA_SIG,\n\t\t\t\t\tPOOLSET_REPLICA_SIG_LEN) == 0) {\n\t\t\tif (line[POOLSET_REPLICA_SIG_LEN] != '\\0') {\n\t\t\t\t/* something more than 'REPLICA' */\n\t\t\t\tchar c = line[POOLSET_REPLICA_SIG_LEN];\n\t\t\t\tif (!isblank((unsigned char)c)) {\n\t\t\t\t\tresult = PARSER_REPLICA;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\t/* check if it is a remote replica */\n\t\t\t\tresult = parser_read_replica(\n\t\t\t\t\t\tline + POOLSET_REPLICA_SIG_LEN,\n\t\t\t\t\t\t&node_addr, &pool_desc);\n\t\t\t\tif (result == PARSER_CONTINUE) {\n\t\t\t\t\t/* remote REPLICA */\n\t\t\t\t\tLOG(10, \"REMOTE REPLICA \"\n\t\t\t\t\t\t\"node address '%s' \"\n\t\t\t\t\t\t\"pool set descriptor '%s'\",\n\t\t\t\t\t\tnode_addr, pool_desc);\n\t\t\t\t\tif (util_parse_add_remote_replica(&set,\n\t\t\t\t\t\t\tnode_addr, pool_desc))\n\t\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t} else if (nparts >= 1) {\n\t\t\t\t/* 'REPLICA' signature detected */\n\t\t\t\tLOG(10, \"REPLICA\");\n\n\t\t\t\tint ret = util_parse_add_replica(&set);\n\t\t\t\tif (ret != 0)\n\t\t\t\t\tgoto err;\n\n\t\t\t\tnparts = 0;\n\t\t\t\tresult = PARSER_CONTINUE;\n\t\t\t} else {\n\t\t\t\tif (set->nreplicas == 1)\n\t\t\t\t\tresult = PARSER_SET_NO_PARTS;\n\t\t\t\telse\n\t\t\t\t\tresult = PARSER_REP_NO_PARTS;\n\t\t\t}\n\t\t} else {\n\t\t\t/* there could be no parts for remote replicas */\n\t\t\tif (set->replica[set->nreplicas - 1]->remote) {\n\t\t\t\tresult = PARSER_REMOTE_REP_UNEXPECTED_PARTS;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* read size and path */\n\t\t\tresult = parser_read_line(line, &psize, &ppath);\n\t\t\tif (result == PARSER_CONTINUE) {\n\t\t\t\t/* add a new pool's part to the list */\n\t\t\t\tint ret = util_parse_add_element(set,\n\t\t\t\t\tppath, psize);\n\t\t\t\tif (ret != 0) {\n\t\t\t\t\tFree(ppath);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t\tnparts++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (result != PARSER_FORMAT_OK) {\n\t\tERR(\"%s [%s:%d]\", path, parser_errstr[result], nlines);\n\t\tswitch (result) {\n\t\tcase PARSER_CANNOT_READ_SIZE:\n\t\tcase PARSER_OUT_OF_MEMORY:\n\t\t\t/* do not overwrite errno */\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terrno = EINVAL;\n\t\t}\n\t\tgoto err;\n\t}\n\n\tif (util_poolset_check_devdax(set) != 0) {\n\t\terrno = EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (util_poolset_directories_load(set) != 0) {\n\t\tERR(\"cannot load part files from directories\");\n\t\tgoto err;\n\t}\n\n\tLOG(4, \"set file format correct (%s)\", path);\n\t(void) os_fclose(fs);\n\tFree(line);\n\tutil_poolset_check_options(set);\n\tutil_poolset_set_size(set);\n\t*setp = set;\n\treturn 0;\n\nerr:\n\toerrno = errno;\n\tFree(line);\n\t(void) os_fclose(fs);\n\tif (set)\n\t\tutil_poolset_free(set);\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_poolset_single -- (internal) create a one-part pool set\n *\n * On success returns a pointer to a newly allocated and initialized\n * pool set structure.  Otherwise, NULL is returned.\n */\nstatic struct pool_set *\nutil_poolset_single(const char *path, size_t filesize, int create,\n\tint ignore_sds)\n{\n\tLOG(3, \"path %s filesize %zu create %d\",\n\t\t\tpath, filesize, create);\n\n\tenum file_type type = util_file_get_type(path);\n\tif (type == OTHER_ERROR)\n\t\treturn NULL;\n\n\tstruct pool_set *set;\n\tset = Zalloc(sizeof(struct pool_set) +\n\t\t\tsizeof(struct pool_replica *));\n\tif (set == NULL) {\n\t\tERR(\"!Malloc for pool set\");\n\t\treturn NULL;\n\t}\n\n\tset->path = Strdup(path);\n\tif (set->path == NULL)  {\n\t\tERR(\"!Strdup\");\n\t\tFree(set);\n\t\treturn NULL;\n\t}\n\n\tstruct pool_replica *rep;\n\trep = Zalloc(sizeof(struct pool_replica) +\n\t\t\tsizeof(struct pool_set_part));\n\tif (rep == NULL) {\n\t\tERR(\"!Malloc for pool set replica\");\n\t\tFree(set->path);\n\t\tFree(set);\n\t\treturn NULL;\n\t}\n\n\tVEC_INIT(&rep->directory);\n\n\tset->replica[0] = rep;\n\n\trep->part[0].filesize = filesize;\n\trep->part[0].path = Strdup(path);\n\trep->part[0].fd = -1;\t/* will be filled out by util_poolset_file() */\n\trep->part[0].is_dev_dax = type == TYPE_DEVDAX;\n\trep->part[0].created = create;\n\trep->part[0].hdr = NULL;\n\trep->part[0].addr = NULL;\n\trep->part[0].has_bad_blocks = 0;\n\n\tif (rep->part[0].is_dev_dax)\n\t\trep->part[0].alignment = util_file_device_dax_alignment(path);\n\telse\n\t\trep->part[0].alignment = Mmap_align;\n\n\tASSERTne(rep->part[0].alignment, 0);\n\n\trep->nallocated = 1;\n\trep->nparts = 1;\n\trep->nhdrs = 1;\n\n\t/* it does not have a remote replica */\n\trep->remote = NULL;\n\tset->remote = 0;\n\n\t/* round down to the nearest mapping alignment boundary */\n\trep->repsize = rep->part[0].filesize & ~(rep->part[0].alignment - 1);\n\trep->resvsize = rep->repsize;\n\n\tset->poolsize = rep->repsize;\n\tset->resvsize = rep->resvsize;\n\n\tset->nreplicas = 1;\n\tset->ignore_sds = ignore_sds || (set->options & OPTION_NOHDRS);\n\n\treturn set;\n}\n\n/*\n * util_part_open -- open or create a single part file\n */\nint\nutil_part_open(struct pool_set_part *part, size_t minsize, int create)\n{\n\tLOG(3, \"part %p minsize %zu create %d\", part, minsize, create);\n\n\tint exists = util_file_exists(part->path);\n\tif (exists < 0)\n\t\treturn -1;\n\n\tif (exists)\n\t\tcreate = 0;\n\n\tpart->created = 0;\n\tif (create) {\n\t\tpart->fd = util_file_create(part->path, part->filesize,\n\t\t\t\tminsize);\n\t\tif (part->fd == -1) {\n\t\t\tLOG(2, \"failed to create file: %s\", part->path);\n\t\t\treturn -1;\n\t\t}\n\t\tpart->created = 1;\n\t} else {\n\t\tsize_t size = 0;\n\t\tint flags = O_RDWR;\n\t\tpart->fd = util_file_open(part->path, &size, minsize, flags);\n\t\tif (part->fd == -1) {\n\t\t\tLOG(2, \"failed to open file: %s\", part->path);\n\t\t\treturn -1;\n\t\t}\n\n\t\t/* check if filesize matches */\n\t\tif (part->filesize != size) {\n\t\t\tERR(\"file size does not match config: %s, %zu != %zu\",\n\t\t\t\tpart->path, size, part->filesize);\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_part_fdclose -- close part file\n */\nvoid\nutil_part_fdclose(struct pool_set_part *part)\n{\n\tLOG(3, \"part %p\", part);\n\n\tif (part->fd != -1) {\n\t\t(void) os_close(part->fd);\n\t\tpart->fd = -1;\n\t}\n}\n\n/*\n * util_set_rpmem_attr -- (internal) overwrite existing pool attributes\n *\n * does not set uuid, next_part_uuid, prev_part_uuid\n */\nstatic void\nutil_set_rpmem_attr(struct pool_hdr *hdrp, const struct rpmem_pool_attr *rattr)\n{\n\tLOG(5, \"hdrp %p rattr %p\", hdrp, rattr);\n\tmemcpy(hdrp->signature, rattr->signature, POOL_HDR_SIG_LEN);\n\thdrp->major = rattr->major;\n\thdrp->features.compat = rattr->compat_features;\n\thdrp->features.incompat = rattr->incompat_features;\n\thdrp->features.ro_compat = rattr->ro_compat_features;\n\tmemcpy(hdrp->poolset_uuid, rattr->poolset_uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(hdrp->next_repl_uuid, rattr->next_uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(hdrp->prev_repl_uuid, rattr->prev_uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(&hdrp->arch_flags, rattr->user_flags, sizeof(struct arch_flags));\n}\n\n/*\n * util_get_rpmem_attr -- (internal) get attributes for remote replica header\n */\nstatic void\nutil_get_rpmem_attr(struct rpmem_pool_attr *rattr, const struct pool_hdr *hdrp)\n{\n\tLOG(5, \"rpmem_attr %p hdrp %p\", rattr, hdrp);\n\tASSERTne(rattr, NULL);\n\tmemcpy(rattr->signature, hdrp->signature, POOL_HDR_SIG_LEN);\n\trattr->major = hdrp->major;\n\trattr->compat_features = hdrp->features.compat;\n\trattr->incompat_features = hdrp->features.incompat;\n\trattr->ro_compat_features = hdrp->features.ro_compat;\n\tmemcpy(rattr->poolset_uuid, hdrp->poolset_uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(rattr->uuid, hdrp->uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(rattr->next_uuid, hdrp->next_repl_uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(rattr->prev_uuid, hdrp->prev_repl_uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(rattr->user_flags, &hdrp->arch_flags, sizeof(struct arch_flags));\n}\n\n/*\n * util_remote_store_attr -- (internal) store attributes read from remote\n *                           replica in the local volatile pool header\n */\nstatic void\nutil_remote_store_attr(struct pool_hdr *hdrp,\n\t\tconst struct rpmem_pool_attr *rattr)\n{\n\tLOG(4, \"hdrp %p rpmem_attr %p\", hdrp, rattr);\n\n\tutil_set_rpmem_attr(hdrp, rattr);\n\tmemcpy(hdrp->uuid, rattr->uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(hdrp->next_part_uuid, rattr->uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(hdrp->prev_part_uuid, rattr->uuid, POOL_HDR_UUID_LEN);\n}\n\n/*\n * util_update_remote_header --  update attributes of a remote replica;\n *                               the remote replica must be open\n */\nint\nutil_update_remote_header(struct pool_set *set, unsigned repn)\n{\n\tLOG(3, \"set %p, repn %u\", set, repn);\n\n\tASSERTne(REP(set, repn)->remote, NULL);\n\tASSERTne(REP(set, repn)->remote->rpp, NULL);\n\n\tstruct pool_replica *rep = REP(set, repn);\n\tstruct pool_hdr *hdr = HDR(rep, 0);\n\n\t/* get attributes from the local pool header */\n\tstruct rpmem_pool_attr attributes;\n\tutil_get_rpmem_attr(&attributes, hdr);\n\n\t/* push the attributes to the remote replica */\n\tRPMEMpool *rpp = rep->remote->rpp;\n\tint ret = Rpmem_set_attr(rpp, &attributes);\n\tif (ret) {\n\t\tERR(\"!Rpmem_set_attr\");\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n/*\n * util_pool_close_remote -- close a remote replica\n */\nint\nutil_pool_close_remote(RPMEMpool *rpp)\n{\n\tLOG(3, \"rpp %p\", rpp);\n\n\treturn Rpmem_close(rpp);\n}\n\n/*\n * util_poolset_remote_open -- open or create a remote replica\n */\nint\nutil_poolset_remote_open(struct pool_replica *rep, unsigned repidx,\n\t\t\tsize_t minsize, int create, void *pool_addr,\n\t\t\tsize_t pool_size, unsigned *nlanes)\n{\n\tLOG(3, \"rep %p repidx %u minsize %zu create %d \"\n\t\t\"pool_addr %p pool_size %zu nlanes %p\",\n\t\trep, repidx, minsize, create,\n\t\tpool_addr, pool_size, nlanes);\n\n\tASSERTne(nlanes, NULL);\n\n\tif (!Rpmem_handle_remote) {\n\t\treturn -1;\n\t}\n\n\tunsigned remote_nlanes = *nlanes;\n\n\tif (create) {\n\t\tstruct rpmem_pool_attr rpmem_attr_create;\n\t\tutil_get_rpmem_attr(&rpmem_attr_create, rep->part[0].hdr);\n\n\t\trep->remote->rpp = Rpmem_create(rep->remote->node_addr,\n\t\t\t\t\t\trep->remote->pool_desc,\n\t\t\t\t\t\tpool_addr,\n\t\t\t\t\t\tpool_size,\n\t\t\t\t\t\t&remote_nlanes,\n\t\t\t\t\t\t&rpmem_attr_create);\n\t\tif (rep->remote->rpp == NULL) {\n\t\t\tERR(\"creating remote replica #%u failed\", repidx);\n\t\t\treturn -1;\n\t\t}\n\t\trep->part[0].created = 1;\n\t} else { /* open */\n\t\tstruct rpmem_pool_attr rpmem_attr_open;\n\n\t\trep->remote->rpp = Rpmem_open(rep->remote->node_addr,\n\t\t\t\t\t\trep->remote->pool_desc,\n\t\t\t\t\t\tpool_addr,\n\t\t\t\t\t\tpool_size,\n\t\t\t\t\t\t&remote_nlanes,\n\t\t\t\t\t\t&rpmem_attr_open);\n\t\tif (rep->remote->rpp == NULL) {\n\t\t\tERR(\"opening remote replica #%u failed\", repidx);\n\t\t\treturn -1;\n\t\t}\n\n\t\tutil_remote_store_attr(rep->part[0].hdr, &rpmem_attr_open);\n\t}\n\n\tif (remote_nlanes < *nlanes)\n\t\t*nlanes = remote_nlanes;\n\n\treturn 0;\n}\n\n/*\n * util_poolset_files_local -- (internal) open or create all the local\n *                              part files of a pool set and replica sets\n */\nstatic int\nutil_poolset_files_local(struct pool_set *set, size_t minpartsize, int create)\n{\n\tLOG(3, \"set %p minpartsize %zu create %d\", set, minpartsize, create);\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tif (!rep->remote) {\n\t\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\t\tif (util_part_open(&rep->part[p], minpartsize,\n\t\t\t\t\t\tcreate))\n\t\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_remote_replica_open -- open remote replica\n */\nint\nutil_poolset_remote_replica_open(struct pool_set *set, unsigned repidx,\n\tsize_t minsize, int create, unsigned *nlanes)\n{\n#ifndef _WIN32\n\t/*\n\t * This is a workaround for an issue with using device dax with\n\t * libibverbs. To handle fork() function calls correctly libfabric use\n\t * ibv_fork_init(3) which makes all registered memory being madvised\n\t * with MADV_DONTFORK flag. In libpmemobj the remote replication is\n\t * performed without pool header (first 4k). In such case the address\n\t * passed to madvise(2) is aligned to 4k, but device dax can require\n\t * different alignment (default is 2MB). This workaround madvises the\n\t * entire memory region before registering it by fi_mr_reg(3).\n\t *\n\t * The librpmem client requires fork() support to work correctly.\n\t */\n\tif (set->replica[0]->part[0].is_dev_dax) {\n\t\tint ret = os_madvise(set->replica[0]->part[0].addr,\n\t\t\t\tset->replica[0]->part[0].filesize,\n\t\t\t\tMADV_DONTFORK);\n\t\tif (ret) {\n\t\t\tERR(\"!madvise\");\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\n\tvoid *pool_addr = (void *)((uintptr_t)set->replica[0]->part[0].addr);\n\n\treturn util_poolset_remote_open(set->replica[repidx], repidx, minsize,\n\t\t\tcreate, pool_addr, set->poolsize, nlanes);\n}\n\n/*\n * util_poolset_files_remote -- (internal) open or create all the remote\n *                              part files of a pool set and replica sets\n */\nstatic int\nutil_poolset_files_remote(struct pool_set *set, size_t minsize,\n\t\t\t\tunsigned *nlanes, int create)\n{\n\tLOG(3, \"set %p minsize %zu nlanes %p create %d\",\n\t\tset, minsize, nlanes, create);\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tif (rep->remote) {\n\t\t\tif (util_poolset_remote_replica_open(set, r,\n\t\t\t\tminsize, create, nlanes))\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_read -- read memory pool set file\n *\n * On success returns 0 and a pointer to a newly allocated structure\n * containing the info of all the parts of the pool set and replicas.\n */\nint\nutil_poolset_read(struct pool_set **setp, const char *path)\n{\n\tLOG(3, \"setp %p path %s\", setp, path);\n\n\tint oerrno;\n\tint ret = 0;\n\tint fd;\n\n\tif ((fd = os_open(path, O_RDONLY)) < 0) {\n\t\tERR(\"!open: path \\\"%s\\\"\", path);\n\t\treturn -1;\n\t}\n\n\tret = util_poolset_parse(setp, path, fd);\n\n\toerrno = errno;\n\t(void) os_close(fd);\n\terrno = oerrno;\n\treturn ret;\n}\n\n/*\n * util_poolset_create_set -- create a new pool set structure\n *\n * On success returns 0 and a pointer to a newly allocated structure\n * containing the info of all the parts of the pool set and replicas.\n */\nint\nutil_poolset_create_set(struct pool_set **setp, const char *path,\n\tsize_t poolsize, size_t minsize, int ignore_sds)\n{\n\tLOG(3, \"setp %p path %s poolsize %zu minsize %zu\",\n\t\tsetp, path, poolsize, minsize);\n\n\tint oerrno;\n\tint ret = 0;\n\tint fd;\n\tsize_t size = 0;\n\n\tenum file_type type = util_file_get_type(path);\n\tif (type == OTHER_ERROR)\n\t\treturn -1;\n\n\tif (poolsize != 0) {\n\t\tif (type == TYPE_DEVDAX) {\n\t\t\tERR(\"size must be zero for device dax\");\n\t\t\treturn -1;\n\t\t}\n\t\t*setp = util_poolset_single(path, poolsize, 1, ignore_sds);\n\t\tif (*setp == NULL)\n\t\t\treturn -1;\n\n\t\treturn 0;\n\t}\n\n\t/* do not check minsize */\n\tif ((fd = util_file_open(path, &size, 0, O_RDONLY)) == -1)\n\t\treturn -1;\n\n\tchar signature[POOLSET_HDR_SIG_LEN];\n\tif (type == TYPE_NORMAL) {\n\t\t/*\n\t\t * read returns ssize_t, but we know it will return value\n\t\t * between -1 and POOLSET_HDR_SIG_LEN (11), so we can safely\n\t\t * cast it to int\n\t\t */\n\t\tret = (int)read(fd, signature, POOLSET_HDR_SIG_LEN);\n\t\tif (ret < 0) {\n\t\t\tERR(\"!read %d\", fd);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tif (type == TYPE_DEVDAX || ret < POOLSET_HDR_SIG_LEN ||\n\t    strncmp(signature, POOLSET_HDR_SIG, POOLSET_HDR_SIG_LEN)) {\n\t\tLOG(4, \"not a pool set header\");\n\t\t(void) os_close(fd);\n\n\t\tif (size < minsize) {\n\t\t\tERR(\"file is not a poolset file and its size (%zu)\"\n\t\t\t\t\" is smaller than %zu\", size, minsize);\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\t\t*setp = util_poolset_single(path, size, 0, ignore_sds);\n\t\tif (*setp == NULL)\n\t\t\treturn -1;\n\n\t\treturn 0;\n\t}\n\n\tret = util_poolset_parse(setp, path, fd);\n\tif (ret)\n\t\tgoto err;\n\n\t(*setp)->ignore_sds = ignore_sds || ((*setp)->options & OPTION_NOHDRS);\n#ifdef _WIN32\n\t/* remote replication is not supported on Windows */\n\tif ((*setp)->remote) {\n\t\tutil_poolset_free(*setp);\n\t\tERR(\"remote replication is not supported on Windows\");\n\t\terrno = ENOTSUP;\n\t\tret = -1;\n\t\tgoto err;\n\t}\n#endif /* _WIN32 */\n\nerr:\n\toerrno = errno;\n\t(void) os_close(fd);\n\terrno = oerrno;\n\treturn ret;\n}\n\n/*\n * util_poolset_check_header_options -- (internal) check if poolset options\n *                                      match given flags\n */\nstatic int\nutil_poolset_check_header_options(struct pool_set *set, uint32_t incompat)\n{\n\tLOG(3, \"set %p, incompat %#x\", set, incompat);\n\n\tif (((set->options & OPTION_SINGLEHDR) == 0) !=\n\t\t\t((incompat & POOL_FEAT_SINGLEHDR) == 0)) {\n\t\tERR(\n\t\t\t\"poolset file options (%u) do not match incompat feature flags (%#x)\",\n\t\t\tset->options, incompat);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n/*\n * util_header_create -- create header of a single pool set file\n */\nint\nutil_header_create(struct pool_set *set, unsigned repidx, unsigned partidx,\n\tconst struct pool_attr *attr, int overwrite)\n{\n\tLOG(3, \"set %p repidx %u partidx %u attr %p overwrite %d\", set, repidx,\n\t\tpartidx, attr, overwrite);\n\n\tASSERTne(attr, NULL);\n\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\t/* opaque info lives at the beginning of mapped memory pool */\n\tstruct pool_hdr *hdrp = rep->part[partidx].hdr;\n\n\t/* check if the pool header is all zeros */\n\tif (!util_is_zeroed(hdrp, sizeof(*hdrp)) && !overwrite) {\n\t\tERR(\"Non-empty file detected\");\n\t\terrno = EEXIST;\n\t\treturn -1;\n\t}\n\n\t/* create pool's header */\n\tutil_pool_attr2hdr(hdrp, attr);\n\n\tif (set->options & OPTION_SINGLEHDR)\n\t\thdrp->features.incompat |= POOL_FEAT_SINGLEHDR;\n\n\tmemcpy(hdrp->poolset_uuid, set->uuid, POOL_HDR_UUID_LEN);\n\tmemcpy(hdrp->uuid, PART(rep, partidx)->uuid, POOL_HDR_UUID_LEN);\n\n\t/* link parts */\n\tif (set->options & OPTION_SINGLEHDR) {\n\t\t/* next/prev part point to part #0 */\n\t\tASSERTeq(partidx, 0);\n\t\tmemcpy(hdrp->prev_part_uuid, PART(rep, 0)->uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\t\tmemcpy(hdrp->next_part_uuid, PART(rep, 0)->uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\t} else {\n\t\tmemcpy(hdrp->prev_part_uuid, PARTP(rep, partidx)->uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\t\tmemcpy(hdrp->next_part_uuid, PARTN(rep, partidx)->uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\t}\n\n\t/* link replicas */\n\tif (!util_is_zeroed(attr->prev_repl_uuid, POOL_HDR_UUID_LEN)) {\n\t\tmemcpy(hdrp->prev_repl_uuid, attr->prev_repl_uuid,\n\t\t\t\tPOOL_HDR_UUID_LEN);\n\t} else {\n\t\tmemcpy(hdrp->prev_repl_uuid, PART(REPP(set, repidx), 0)->uuid,\n\t\t\tPOOL_HDR_UUID_LEN);\n\t}\n\tif (!util_is_zeroed(attr->next_repl_uuid, POOL_HDR_UUID_LEN)) {\n\t\tmemcpy(hdrp->next_repl_uuid, attr->next_repl_uuid,\n\t\t\t\tPOOL_HDR_UUID_LEN);\n\t} else {\n\t\tmemcpy(hdrp->next_repl_uuid, PART(REPN(set, repidx), 0)->uuid,\n\t\t\tPOOL_HDR_UUID_LEN);\n\t}\n\n\tif (!rep->remote) {\n\t\tos_stat_t stbuf;\n\n\t\tif (os_fstat(rep->part[partidx].fd, &stbuf) != 0) {\n\t\t\tERR(\"!fstat\");\n\t\t\treturn -1;\n\t\t}\n\t\tASSERT(stbuf.st_ctime);\n\t\thdrp->crtime = (uint64_t)stbuf.st_ctime;\n\t}\n\n\tint arch_is_zeroed = util_is_zeroed(attr->arch_flags,\n\t\t\tPOOL_HDR_ARCH_LEN);\n\tif (arch_is_zeroed)\n\t\tutil_get_arch_flags(&hdrp->arch_flags);\n\n\tutil_convert2le_hdr(hdrp);\n\n\tif (!arch_is_zeroed) {\n\t\tmemcpy(&hdrp->arch_flags, attr->arch_flags, POOL_HDR_ARCH_LEN);\n\t}\n\n\tif (!set->ignore_sds && partidx == 0 && !rep->remote) {\n\t\tshutdown_state_init(&hdrp->sds, rep);\n\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\tif (shutdown_state_add_part(&hdrp->sds,\n\t\t\t\t\tPART(rep, p)->path, rep))\n\t\t\t\treturn -1;\n\t\t}\n\t\tshutdown_state_set_dirty(&hdrp->sds, rep);\n\t}\n\n\tutil_checksum(hdrp, sizeof(*hdrp), &hdrp->checksum,\n\t\t1, POOL_HDR_CSUM_END_OFF(hdrp));\n\n\t/* store pool's header */\n\tutil_persist_auto(rep->is_pmem, hdrp, sizeof(*hdrp));\n\n\treturn 0;\n}\n\n/*\n * util_header_check -- (internal) validate header of a single pool set file\n */\nstatic int\nutil_header_check(struct pool_set *set, unsigned repidx, unsigned partidx,\n\tconst struct pool_attr *attr)\n{\n\tLOG(3, \"set %p repidx %u partidx %u attr %p\", set, repidx, partidx,\n\t\t\tattr);\n\n\tASSERTne(attr, NULL);\n\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\t/* opaque info lives at the beginning of mapped memory pool */\n\tstruct pool_hdr *hdrp = rep->part[partidx].hdr;\n\tstruct pool_hdr hdr;\n\n\tmemcpy(&hdr, hdrp, sizeof(hdr));\n\n\t/* local copy of a remote header does not need to be converted */\n\tif (rep->remote == NULL)\n\t\tutil_convert2h_hdr_nocheck(&hdr);\n\n\t/* to be valid, a header must have a major version of at least 1 */\n\tif (hdr.major == 0) {\n\t\tERR(\"invalid major version (0)\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check signature */\n\tif (memcmp(hdr.signature, attr->signature, POOL_HDR_SIG_LEN)) {\n\t\tERR(\"wrong pool type: \\\"%.8s\\\"\", hdr.signature);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check format version number */\n\tif (hdr.major != attr->major) {\n\t\tERR(\"pool version %d (library expects %d)\", hdr.major,\n\t\t\t\tattr->major);\n\t\tif (hdr.major < attr->major)\n\t\t\tERR(\n\t\t\t\t\"Please run the pmdk-convert utility to upgrade the pool.\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\trep->part[partidx].rdonly = 0;\n\n\tint retval = util_feature_check(&hdr, attr->features);\n\tif (retval < 0)\n\t\treturn -1;\n\n\tif (retval == 0)\n\t\trep->part[partidx].rdonly = 1;\n\n\tif (rep->remote == NULL) {\n\t\t/*\n\t\t * and to be valid, the fields must checksum correctly\n\t\t *\n\t\t * NOTE: checksum validation is performed after format version\n\t\t * and feature check, because if POOL_FEAT_CKSUM_2K flag is set,\n\t\t * we want to report it as incompatible feature, rather than\n\t\t * invalid checksum.\n\t\t */\n\t\tif (!util_checksum(&hdr, sizeof(hdr), &hdr.checksum,\n\t\t\t\t0, POOL_HDR_CSUM_END_OFF(&hdr))) {\n\t\t\tERR(\"invalid checksum of pool header\");\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\n\t\tLOG(3, \"valid header, signature \\\"%.8s\\\"\", hdr.signature);\n\t}\n\n\tif (util_check_arch_flags(&hdr.arch_flags)) {\n\t\tERR(\"wrong architecture flags\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check pool set UUID */\n\tif (memcmp(HDR(REP(set, 0), 0)->poolset_uuid, hdr.poolset_uuid,\n\t\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\tERR(\"wrong pool set UUID\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check pool set linkage */\n\tif (memcmp(HDRP(rep, partidx)->uuid, hdr.prev_part_uuid,\n\t\t\t\t\t\tPOOL_HDR_UUID_LEN) ||\n\t    memcmp(HDRN(rep, partidx)->uuid, hdr.next_part_uuid,\n\t\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\tERR(\"wrong part UUID\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check format version */\n\tif (HDR(rep, 0)->major != hdrp->major) {\n\t\tERR(\"incompatible pool format\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check compatibility features */\n\tif (HDR(rep, 0)->features.compat != hdrp->features.compat ||\n\t    HDR(rep, 0)->features.incompat != hdrp->features.incompat ||\n\t    HDR(rep, 0)->features.ro_compat != hdrp->features.ro_compat) {\n\t\tERR(\"incompatible feature flags\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check poolset options */\n\tif (util_poolset_check_header_options(set,\n\t\t\tHDR(rep, 0)->features.incompat))\n\t\treturn -1;\n\n\treturn 0;\n}\n\n/*\n * util_header_check_remote -- (internal) validate header of a remote\n *                             pool set file\n */\nstatic int\nutil_header_check_remote(struct pool_set *set, unsigned partidx)\n{\n\tLOG(3, \"set %p partidx %u \", set, partidx);\n\n\t/* there is only one replica in remote poolset */\n\tstruct pool_replica *rep = set->replica[0];\n\t/* opaque info lives at the beginning of mapped memory pool */\n\tstruct pool_hdr *hdrp = rep->part[partidx].hdr;\n\tstruct pool_hdr hdr;\n\n\tif (util_is_zeroed(hdrp, sizeof(*hdrp))) {\n\t\tERR(\"pool header zeroed\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\tmemcpy(&hdr, hdrp, sizeof(hdr));\n\n\tutil_convert2h_hdr_nocheck(&hdr);\n\n\t/* valid header found */\n\tif (memcmp(HDR(rep, 0)->signature, hdrp->signature, POOL_HDR_SIG_LEN)) {\n\t\tERR(\"pool signature mismatch in part %d\", partidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check format version */\n\tif (HDR(rep, 0)->major != hdrp->major) {\n\t\tERR(\"pool version mismatch in part %d\", partidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check compatibility features */\n\tif (HDR(rep, 0)->features.compat != hdrp->features.compat) {\n\t\tERR(\"'may have' compatibility flags mismatch in part %d\",\n\t\t\t\t\t\t\t\tpartidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\tif (HDR(rep, 0)->features.incompat != hdrp->features.incompat) {\n\t\tERR(\"'must support' compatibility flags mismatch in part %d\",\n\t\t\t\t\t\t\t\tpartidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\tif (HDR(rep, 0)->features.ro_compat != hdrp->features.ro_compat) {\n\t\tERR(\"'force read-only' compatibility flags mismatch in part %d\",\n\t\t\t\t\t\t\t\tpartidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/*\n\t * and to be valid, the fields must checksum correctly\n\t *\n\t * NOTE: checksum validation is performed after format version and\n\t * feature check, because if POOL_FEAT_CKSUM_2K flag is set,\n\t * we want to report it as incompatible feature, rather than invalid\n\t * checksum.\n\t */\n\tif (!util_checksum(&hdr, sizeof(hdr), &hdr.checksum,\n\t\t\t0, POOL_HDR_CSUM_END_OFF(&hdr))) {\n\t\tERR(\"invalid checksum of pool header\");\n\t\treturn -1;\n\t}\n\n\tLOG(3, \"valid header, signature \\\"%.8s\\\"\", hdr.signature);\n\n\t/* check pool set UUID */\n\tif (memcmp(HDR(rep, 0)->poolset_uuid, hdrp->poolset_uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\tERR(\"wrong pool set UUID in part %d\", partidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check previous replica UUID */\n\tif (memcmp(HDR(rep, 0)->prev_repl_uuid, hdrp->prev_repl_uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\tERR(\"wrong previous replica UUID in part %d\", partidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check next replica UUID */\n\tif (memcmp(HDR(rep, 0)->next_repl_uuid, hdrp->next_repl_uuid,\n\t\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\tERR(\"wrong next replica UUID in part %d\", partidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\tif (memcmp(&HDR(rep, 0)->arch_flags, &hdrp->arch_flags,\n\t\t\t\t\t\tsizeof(hdrp->arch_flags))) {\n\t\tERR(\"wrong architecture flags\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* check pool set linkage */\n\tif (memcmp(HDRP(rep, partidx)->uuid, hdrp->prev_part_uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN) ||\n\t    memcmp(HDRN(rep, partidx)->uuid, hdrp->next_part_uuid,\n\t\t\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\tERR(\"wrong part UUID in part %d\", partidx);\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\n\t/* read shutdown state toggle from header */\n\tset->ignore_sds |= IGNORE_SDS(HDR(rep, 0));\n\n\tif (!set->ignore_sds && partidx == 0) {\n\t\tstruct shutdown_state sds;\n\t\tshutdown_state_init(&sds, NULL);\n\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\tif (shutdown_state_add_part(&sds,\n\t\t\t\t\tPART(rep, p)->path, NULL))\n\t\t\t\treturn -1;\n\t\t}\n\n\t\tif (shutdown_state_check(&sds, &hdrp->sds, rep)) {\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\n\t\tshutdown_state_set_dirty(&hdrp->sds, rep);\n\t}\n\n\n\trep->part[partidx].rdonly = 0;\n\n\treturn 0;\n}\n\n/*\n * util_replica_set_is_pmem -- sets per-replica is_pmem flag\n *\n * The replica is PMEM if:\n * - all parts are on device dax, or\n * - all parts are mapped with MAP_SYNC.\n *\n * It's enough to check only first part because it's already verified\n * that either all or none parts are device dax or mapped with MAP_SYNC.\n */\nstatic inline void\nutil_replica_set_is_pmem(struct pool_replica *rep)\n{\n\trep->is_pmem = rep->part[0].is_dev_dax || rep->part[0].map_sync ||\n\t\tpmem_is_pmem(rep->part[0].addr, rep->resvsize);\n}\n\n/*\n * util_replica_map_local -- (internal) map memory pool for local replica\n */\nstatic int\nutil_replica_map_local(struct pool_set *set, unsigned repidx, int flags)\n{\n\tLOG(3, \"set %p repidx %u flags %d\", set, repidx, flags);\n\n\t/*\n\t * XXX: Like we reserve space for all parts in this replica when we map\n\t * the first part, we need to reserve the space for all replicas\n\t * upfront.  It is not necessary that the replicas are contiguous but\n\t * that way we would not fragment the memory much.  I think we should\n\t * leave this to MM, but let's have a note as per our collective minds.\n\t */\n\n#ifndef _WIN32\n\tint remaining_retries = 0;\n#else\n\tint remaining_retries = 10;\n#endif\n\tint retry_for_contiguous_addr;\n\tsize_t mapsize;\n\t/* header size for all headers but the first one */\n\tsize_t hdrsize = (set->options & (OPTION_SINGLEHDR | OPTION_NOHDRS)) ?\n\t\t\t0 : Mmap_align;\n\tvoid *addr;\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\tASSERTeq(rep->remote, NULL);\n\tASSERTne(rep->part, NULL);\n\n\tdo {\n\t\tretry_for_contiguous_addr = 0;\n\t\tmapsize = rep->part[0].filesize & ~(Mmap_align - 1);\n\n\t\t/* determine a hint address for mmap() */\n\t\taddr = util_map_hint(rep->resvsize, 0);\n\t\tif (addr == MAP_FAILED) {\n\t\t\tERR(\"cannot find a contiguous region of given size\");\n\t\t\treturn -1;\n\t\t}\n\n\t\t/* map the first part and reserve space for remaining parts */\n\t\tif (util_map_part(&rep->part[0], addr, rep->resvsize, 0,\n\t\t\t\tflags, 0) != 0) {\n\t\t\tLOG(2, \"pool mapping failed - replica #%u part #0\",\n\t\t\t\trepidx);\n\t\t\treturn -1;\n\t\t}\n\n\t\tVALGRIND_REGISTER_PMEM_MAPPING(rep->part[0].addr,\n\t\t\t\trep->part[0].size);\n\t\tVALGRIND_REGISTER_PMEM_FILE(rep->part[0].fd,\n\t\t\t\trep->part[0].addr, rep->part[0].size, 0);\n\n\t\tset->zeroed &= rep->part[0].created;\n\n\t\taddr = (char *)rep->part[0].addr + mapsize;\n\n\t\t/*\n\t\t * map the remaining parts of the usable pool space\n\t\t * (aligned to memory mapping granularity)\n\t\t */\n\t\tfor (unsigned p = 1; p < rep->nparts; p++) {\n\t\t\t/* map data part */\n\t\t\tif (util_map_part(&rep->part[p], addr, 0, hdrsize,\n\t\t\t\t\tflags | MAP_FIXED, 0) != 0) {\n\t\t\t\t/*\n\t\t\t\t * if we can't map the part at the address we\n\t\t\t\t * asked for, unmap all the parts that are\n\t\t\t\t * mapped and remap at a different address.\n\t\t\t\t */\n\t\t\t\tif ((errno == EINVAL) &&\n\t\t\t\t    (remaining_retries > 0)) {\n\t\t\t\t\tLOG(2, \"usable space mapping failed - \"\n\t\t\t\t\t\t\"part #%d - retrying\", p);\n\t\t\t\t\tretry_for_contiguous_addr = 1;\n\t\t\t\t\tremaining_retries--;\n\n\t\t\t\t\tutil_unmap_parts(rep, 0, p - 1);\n\n\t\t\t\t\t/* release rest of the VA reserved */\n\t\t\t\t\tASSERTne(addr, NULL);\n\t\t\t\t\tASSERTne(addr, MAP_FAILED);\n\t\t\t\t\tmunmap(addr, rep->resvsize - mapsize);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tLOG(2, \"usable space mapping failed - part #%d\",\n\t\t\t\t\tp);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tVALGRIND_REGISTER_PMEM_FILE(rep->part[p].fd,\n\t\t\t\trep->part[p].addr, rep->part[p].size,\n\t\t\t\thdrsize);\n\n\t\t\tmapsize += rep->part[p].size;\n\t\t\tset->zeroed &= rep->part[p].created;\n\t\t\taddr = (char *)addr + rep->part[p].size;\n\t\t}\n\t} while (retry_for_contiguous_addr);\n\n\t/*\n\t * Initially part[0].size is the size of address space\n\t * reservation for all parts from given replica. After\n\t * mapping that space we need to overwrite part[0].size\n\t * with its actual size to be consistent - size for each\n\t * part should be the actual mapping size of this part\n\t * only - it simplifies future calculations.\n\t */\n\trep->part[0].size = rep->part[0].filesize & ~(Mmap_align - 1);\n\n\tif (util_replica_check_map_sync(set, repidx, 0))\n\t\tgoto err;\n\n\tutil_replica_set_is_pmem(rep);\n\n\tif (Prefault_at_create)\n\t\tutil_replica_force_page_allocation(rep);\n\n\tASSERTeq(mapsize, rep->repsize);\n\n\tLOG(3, \"replica #%u addr %p\", repidx, rep->part[0].addr);\n\n\treturn 0;\n\nerr:\n\tLOG(4, \"error clean up\");\n\tint oerrno = errno;\n\tif (mapsize < rep->repsize) {\n\t\tASSERTne(rep->part[0].addr, NULL);\n\t\tASSERTne(rep->part[0].addr, MAP_FAILED);\n\t\tmunmap(rep->part[0].addr, rep->resvsize - mapsize);\n\t}\n\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\tutil_unmap_part(&rep->part[p]);\n\t}\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_replica_init_headers_local -- (internal) initialize pool headers\n */\nstatic int\nutil_replica_init_headers_local(struct pool_set *set, unsigned repidx,\n\tint flags, const struct pool_attr *attr)\n{\n\tLOG(3, \"set %p repidx %u flags %d attr %p\", set, repidx, flags, attr);\n\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\t/* map all headers - don't care about the address */\n\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\tif (util_map_hdr(&rep->part[p], flags, 0) != 0) {\n\t\t\tLOG(2, \"header mapping failed - part #%d\", p);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* create headers, set UUID's */\n\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\tif (util_header_create(set, repidx, p, attr, 0) != 0) {\n\t\t\tLOG(2, \"header creation failed - part #%d\", p);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* unmap all headers */\n\tfor (unsigned p = 0; p < rep->nhdrs; p++)\n\t\tutil_unmap_hdr(&rep->part[p]);\n\n\treturn 0;\n\nerr:\n\tLOG(4, \"error clean up\");\n\tint oerrno = errno;\n\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\tutil_unmap_hdr(&rep->part[p]);\n\t}\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_replica_create_local -- (internal) create a new memory pool for local\n * replica\n */\nstatic int\nutil_replica_create_local(struct pool_set *set, unsigned repidx, int flags,\n\tconst struct pool_attr *attr)\n{\n\tLOG(3, \"set %p repidx %u flags %d attr %p\", set, repidx, flags, attr);\n\n\t/*\n\t * the first replica has to be mapped prior to remote ones so if\n\t * a replica is already mapped skip mapping creation\n\t */\n\tif (PART(REP(set, repidx), 0)->addr == NULL) {\n\t\tif (util_replica_map_local(set, repidx, flags) != 0) {\n\t\t\tLOG(2, \"replica #%u map failed\", repidx);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (attr == NULL)\n\t\treturn 0;\n\n\tif (util_replica_init_headers_local(set, repidx, flags, attr) != 0) {\n\t\tLOG(2, \"replica #%u headers initialization failed\", repidx);\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n/*\n * util_replica_create_remote -- (internal) create a new memory pool\n *                               for remote replica\n */\nstatic int\nutil_replica_create_remote(struct pool_set *set, unsigned repidx, int flags,\n\tconst struct pool_attr *attr)\n{\n\tLOG(3, \"set %p repidx %u flags %d attr %p\", set, repidx, flags, attr);\n\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\tASSERTne(rep->remote, NULL);\n\tASSERTne(rep->part, NULL);\n\tASSERTeq(rep->nparts, 1);\n\tASSERTeq(rep->nhdrs, 1);\n\tASSERTne(attr, NULL);\n\n\tstruct pool_set_part *part = rep->part;\n\n\t/*\n\t * A remote replica has one fake part of size equal twice pool header\n\t * size for storing pool header and pool descriptor.\n\t */\n\tpart->size = rep->repsize;\n\tASSERT(IS_PAGE_ALIGNED(part->size));\n\tpart->remote_hdr = Zalloc(part->size + Pagesize);\n\tif (!part->remote_hdr) {\n\t\tERR(\"!Zalloc\");\n\t\treturn -1;\n\t}\n\n\tpart->hdr = PAGE_ALIGN_UP(part->remote_hdr);\n\tpart->addr = PAGE_ALIGN_UP(part->remote_hdr);\n\tpart->hdrsize = POOL_HDR_SIZE;\n\n\t/* create header, set UUID's */\n\tif (util_header_create(set, repidx, 0, attr, 0) != 0) {\n\t\tLOG(2, \"header creation failed - part #0\");\n\t\tFree(part->remote_hdr);\n\t\treturn -1;\n\t}\n\n\tLOG(3, \"replica #%u addr %p\", repidx, rep->part[0].addr);\n\n\treturn 0;\n}\n\n/*\n * util_replica_close -- close a memory pool replica\n *\n * This function unmaps all mapped memory regions.\n */\nint\nutil_replica_close(struct pool_set *set, unsigned repidx)\n{\n\tLOG(3, \"set %p repidx %u\", set, repidx);\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\tif (rep->remote == NULL) {\n\t\tstruct pool_set_part *part = PART(rep, 0);\n\t\tif (!set->ignore_sds && part->addr != NULL &&\n\t\t\t\tpart->size != 0) {\n\t\t\tstruct pool_hdr *hdr = part->addr;\n\t\t\tRANGE_RW(hdr, sizeof(*hdr), part->is_dev_dax);\n\t\t\t/*\n\t\t\t * deep drain will call msync on one page in each\n\t\t\t * part in replica to trigger WPQ flush.\n\t\t\t * This pages may have been marked as\n\t\t\t * undefined/inaccessible, but msyncing such memory\n\t\t\t * is not a bug, so as a workaround temporarily\n\t\t\t * disable error reporting.\n\t\t\t */\n\t\t\tVALGRIND_DO_DISABLE_ERROR_REPORTING;\n\t\t\tutil_replica_deep_drain(part->addr, rep->repsize,\n\t\t\t\tset, repidx);\n\t\t\tVALGRIND_DO_ENABLE_ERROR_REPORTING;\n\t\t\tshutdown_state_clear_dirty(&hdr->sds, rep);\n\t\t}\n\t\tfor (unsigned p = 0; p < rep->nhdrs; p++)\n\t\t\tutil_unmap_hdr(&rep->part[p]);\n\n\t\trep->part[0].size = rep->resvsize;\n\t\tutil_unmap_part(&rep->part[0]);\n\t} else {\n\t\tLOG(4, \"freeing volatile header of remote replica #%u\", repidx);\n\t\tFree(rep->part[0].remote_hdr);\n\t\trep->part[0].remote_hdr = NULL;\n\t\trep->part[0].hdr = NULL;\n\t\trep->part[0].hdrsize = 0;\n\t\trep->part[0].addr = NULL;\n\t\trep->part[0].size = 0;\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_append_new_part -- (internal) creates a new part in each replica\n *\tof the poolset\n */\nstatic int\nutil_poolset_append_new_part(struct pool_set *set, size_t size)\n{\n\tLOG(3, \"set %p size %zu\", set, size);\n\n\tif (!set->directory_based)\n\t\treturn -1;\n\n\tstruct pool_set_directory *d;\n\tsize_t directory_id;\n\tchar *path;\n\tsize_t path_len;\n\n\tunsigned r;\n\tfor (r = 0; r < set->nreplicas; ++r) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\n\t\tdirectory_id = set->next_directory_id %\n\t\t\tVEC_SIZE(&rep->directory);\n\t\td = VEC_GET(&rep->directory, directory_id);\n\n\t\tpath_len = strlen(d->path) + PMEM_FILE_MAX_LEN;\n\t\tif ((path = Malloc(path_len)) == NULL) {\n\t\t\tERR(\"!Malloc\");\n\t\t\tgoto err_part_init;\n\t\t}\n\n\t\tsnprintf(path, path_len, \"%s\" OS_DIR_SEP_STR \"%0*u%s\",\n\t\t\td->path, PMEM_FILE_PADDING, set->next_id, PMEM_EXT);\n\n\t\tif (util_replica_add_part(&set->replica[r], path, size) != 0)\n\t\t\tFATAL(\"cannot add a new part to the replica info\");\n\t}\n\n\tset->next_directory_id += 1;\n\tset->next_id += 1;\n\n\tutil_poolset_set_size(set);\n\n\treturn 0;\n\nerr_part_init:\n\t/* for each replica 0..r-1 remove the last part */\n\tfor (unsigned rn = 0; rn < r; ++rn) {\n\t\tstruct pool_replica *rep = set->replica[rn];\n\t\tunsigned pidx = rep->nparts - 1;\n\t\tFree((void *)(rep->part[pidx].path));\n\t\trep->part[pidx].path = NULL;\n\t\trep->nparts--;\n\t}\n\n\treturn -1;\n}\n\n/*\n * util_pool_extend -- extends the poolset by the provided size\n */\nvoid *\nutil_pool_extend(struct pool_set *set, size_t *size, size_t minpartsize)\n{\n\tLOG(3, \"set %p size %zu minpartsize %zu\", set, *size, minpartsize);\n\n\tif (*size == 0) {\n\t\tERR(\"cannot extend pool by 0 bytes\");\n\t\treturn NULL;\n\t}\n\n\tif ((set->options & OPTION_SINGLEHDR) == 0) {\n\t\tERR(\n\t\t\"extending the pool by appending parts with headers is not supported!\");\n\t\treturn NULL;\n\t}\n\n\tif (set->poolsize + *size > set->resvsize) {\n\t\t*size = set->resvsize - set->poolsize;\n\t\tif (*size < minpartsize) {\n\t\t\tERR(\"exceeded reservation size\");\n\t\t\treturn NULL;\n\t\t}\n\t\tLOG(4, \"extend size adjusted to not exceed reservation size\");\n\t}\n\n\tsize_t old_poolsize = set->poolsize;\n\n\tif (util_poolset_append_new_part(set, *size) != 0) {\n\t\tERR(\"unable to append a new part to the pool\");\n\t\treturn NULL;\n\t}\n\n\tsize_t hdrsize = (set->options & OPTION_SINGLEHDR) ? 0 : Mmap_align;\n\tvoid *addr = NULL;\n\tvoid *addr_base = NULL;\n\n\tunsigned r;\n\tfor (r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tunsigned pidx = rep->nparts - 1;\n\t\tstruct pool_set_part *p = &rep->part[pidx];\n\n\t\tif (util_part_open(p, 0, 1 /* create */) != 0) {\n\t\t\tERR(\"cannot open the new part\");\n\t\t\tgoto err;\n\t\t}\n\n\t\taddr = (char *)rep->part[0].addr + old_poolsize;\n\t\tif (addr_base == NULL)\n\t\t\taddr_base = addr;\n\n\t\tif (util_map_part(p, addr, 0, hdrsize,\n\t\t\t\tMAP_SHARED | MAP_FIXED, 0) != 0) {\n\t\t\tERR(\"cannot map the new part\");\n\t\t\tgoto err;\n\t\t}\n\n\t\t/*\n\t\t * new part must be mapped the same way as all the rest\n\t\t * within a replica\n\t\t */\n\t\tif (p->map_sync != rep->part[0].map_sync) {\n\t\t\tif (p->map_sync)\n\t\t\t\tERR(\"new part cannot be mapped with MAP_SYNC\");\n\t\t\telse\n\t\t\t\tERR(\"new part mapped with MAP_SYNC\");\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* XXX: mode should be the same as for pmemxxx_create() */\n\tif (util_poolset_chmod(set, S_IWUSR | S_IRUSR))\n\t\tgoto err;\n\n\tutil_poolset_fdclose(set);\n\n\treturn addr_base;\n\nerr:\n\tfor (unsigned rn = 0; rn <= r; ++rn) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tunsigned pidx = rep->nparts - 1;\n\t\tstruct pool_set_part *p = &rep->part[pidx];\n\t\trep->nparts--;\n\n\t\tif (p->fd != 0)\n\t\t\t(void) os_close(p->fd);\n\t\tif (p->created)\n\t\t\tos_unlink(p->path);\n\t\tFree((void *)p->path);\n\t\tp->path = NULL;\n\t}\n\tutil_poolset_set_size(set);\n\n\treturn NULL;\n}\n\n/*\n * util_print_bad_files_cb -- (internal) callback printing names of pool files\n *                            containing bad blocks\n */\nstatic int\nutil_print_bad_files_cb(struct part_file *pf, void *arg)\n{\n\tif (!pf->is_remote && pf->part && pf->part->has_bad_blocks)\n\t\tERR(\"file contains bad blocks -- '%s'\", pf->part->path);\n\n\treturn 0;\n}\n\n/*\n * util_pool_create_uuids -- create a new memory pool (set or a single file)\n *                           with given uuids\n *\n * On success returns 0 and a pointer to a newly allocated structure\n * containing the info of all the parts of the pool set and replicas.\n */\nint\nutil_pool_create_uuids(struct pool_set **setp, const char *path,\n\tsize_t poolsize, size_t minsize, size_t minpartsize,\n\tconst struct pool_attr *attr, unsigned *nlanes, int can_have_rep,\n\tint remote)\n{\n\tLOG(3, \"setp %p path %s poolsize %zu minsize %zu minpartsize %zu \"\n\t\t\"pattr %p nlanes %p can_have_rep %i remote %i\", setp, path,\n\t\tpoolsize, minsize, minpartsize, attr, nlanes, can_have_rep,\n\t\tremote);\n\n\t/* attributes cannot be NULL for local replicas */\n\tASSERT(remote || attr != NULL);\n\n\tint flags = MAP_SHARED;\n\tint oerrno;\n\n\tint exists = util_file_exists(path);\n\tif (exists < 0)\n\t\treturn -1;\n\n\t/* check if file exists */\n\tif (poolsize > 0 && exists) {\n\t\tERR(\"file %s already exists\", path);\n\t\terrno = EEXIST;\n\t\treturn -1;\n\t}\n\n\tint ret = util_poolset_create_set(setp, path, poolsize, minsize,\n\t\t\tIGNORE_SDS(attr));\n\tif (ret < 0) {\n\t\tLOG(2, \"cannot create pool set -- '%s'\", path);\n\t\treturn -1;\n\t}\n\n\tstruct pool_set *set = *setp;\n\n\tASSERT(set->nreplicas > 0);\n\n\tif (!remote && (set->options & OPTION_NOHDRS)) {\n\t\tERR(\n\t\t\t\"the NOHDRS poolset option is not supported for local poolsets\");\n\t\terrno = EINVAL;\n\t\tgoto err_poolset_free;\n\t}\n\n\tif ((attr == NULL) != ((set->options & OPTION_NOHDRS) != 0)) {\n\t\tERR(\n\t\t\t\"pool attributes are not supported for poolsets without headers (with the NOHDRS option)\");\n\t\terrno = EINVAL;\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (set->directory_based && ((set->options & OPTION_SINGLEHDR) == 0)) {\n\t\tERR(\n\t\t\t\"directory based pools are not supported for poolsets with headers (without SINGLEHDR option)\");\n\t\terrno = EINVAL;\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (set->resvsize < minsize) {\n\t\tERR(\"reservation pool size %zu smaller than %zu\",\n\t\t\tset->resvsize, minsize);\n\t\terrno = EINVAL;\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (set->directory_based && set->poolsize == 0 &&\n\t\t\tutil_poolset_append_new_part(set, minsize) != 0) {\n\t\tERR(\"cannot create a new part in provided directories\");\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (attr != NULL &&\n\t    (attr->features.compat & POOL_FEAT_CHECK_BAD_BLOCKS)) {\n\t\tint bbs = badblocks_check_poolset(set, 1 /* create */);\n\t\tif (bbs < 0) {\n\t\t\tLOG(1,\n\t\t\t\t\"failed to check pool set for bad blocks -- '%s'\",\n\t\t\t\tpath);\n\t\t\tgoto err_poolset_free;\n\t\t}\n\n\t\tif (bbs > 0) {\n\t\t\tutil_poolset_foreach_part_struct(set,\n\t\t\t\t\t\t\tutil_print_bad_files_cb,\n\t\t\t\t\t\t\tNULL);\n\t\t\tERR(\n\t\t\t\t\"pool set contains bad blocks and cannot be created, run 'pmempool create --bad-blocks' utility to clear bad blocks and create a pool\");\n\t\t\terrno = EIO;\n\t\t\tgoto err_poolset_free;\n\t\t}\n\t}\n\n\tif (set->poolsize < minsize) {\n\t\tERR(\"net pool size %zu smaller than %zu\",\n\t\t\tset->poolsize, minsize);\n\t\terrno = EINVAL;\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (remote) {\n\t\t/* it is a remote replica - it cannot have replicas */\n\t\tif (set->nreplicas > 1) {\n\t\t\tLOG(2, \"remote pool set cannot have replicas\");\n\t\t\terrno = EINVAL;\n\t\t\tgoto err_poolset_free;\n\t\t}\n\n\t\t/* check if poolset options match remote pool attributes */\n\t\tif (attr != NULL &&\n\t\t\t\t((set->options & OPTION_SINGLEHDR) == 0) !=\n\t\t\t\t((attr->features.incompat &\n\t\t\t\t\t\tPOOL_FEAT_SINGLEHDR) == 0)) {\n\t\t\tERR(\n\t\t\t\t\"pool incompat feature flags and remote poolset options do not match\");\n\t\t\terrno = EINVAL;\n\t\t\tgoto err_poolset_free;\n\t\t}\n\t}\n\n\tif (!can_have_rep && set->nreplicas > 1) {\n\t\tERR(\"replication not supported\");\n\t\terrno = ENOTSUP;\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (set->remote && util_remote_load()) {\n\t\tERR(\n\t\t\t\"the pool set requires a remote replica, but the '%s' library cannot be loaded\",\n\t\t\tLIBRARY_REMOTE);\n\t\tgoto err_poolset_free;\n\t}\n\n\tset->zeroed = 1;\n\n\tif (attr != NULL) {\n\t\tif (!util_is_zeroed(attr->poolset_uuid, POOL_HDR_UUID_LEN)) {\n\t\t\tmemcpy(set->uuid, attr->poolset_uuid,\n\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\t\t} else {\n\t\t\t/* generate pool set UUID */\n\t\t\tret = util_uuid_generate(set->uuid);\n\t\t\tif (ret < 0) {\n\t\t\t\tLOG(2, \"cannot generate pool set UUID\");\n\t\t\t\tgoto err_poolset;\n\t\t\t}\n\t\t}\n\n\t\t/* generate UUID's for all the parts */\n\t\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\t\tstruct pool_replica *rep = set->replica[r];\n\t\t\tfor (unsigned i = 0; i < rep->nhdrs; i++) {\n\t\t\t\tret = util_uuid_generate(rep->part[i].uuid);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tLOG(2,\n\t\t\t\t\t\"cannot generate pool set part UUID\");\n\t\t\t\t\tgoto err_poolset;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* overwrite UUID of the first part if given */\n\t\tif (!util_is_zeroed(attr->first_part_uuid, POOL_HDR_UUID_LEN)) {\n\t\t\tmemcpy(set->replica[0]->part[0].uuid,\n\t\t\t\tattr->first_part_uuid, POOL_HDR_UUID_LEN);\n\t\t}\n\t}\n\n\tret = util_poolset_files_local(set, minpartsize, 1);\n\tif (ret != 0)\n\t\tgoto err_poolset;\n\n\t/* map first local replica - it has to exist prior to remote ones */\n\tret = util_replica_map_local(set, 0, flags);\n\tif (ret != 0)\n\t\tgoto err_poolset;\n\n\t/* prepare remote replicas first */\n\tif (set->remote) {\n\t\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\t\tif (REP(set, r)->remote == NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (util_replica_create_remote(set, r, flags, attr) !=\n\t\t\t\t\t0) {\n\t\t\t\tLOG(2, \"replica #%u creation failed\", r);\n\t\t\t\tgoto err_create;\n\t\t\t}\n\t\t}\n\n\t\tret = util_poolset_files_remote(set, minsize, nlanes,\n\t\t\t\t1 /* create */);\n\t\tif (ret != 0)\n\t\t\tgoto err_create;\n\t}\n\n\t/* prepare local replicas */\n\tif (remote) {\n\t\tif (util_replica_create_local(set, 0, flags, attr) != 0) {\n\t\t\tLOG(2, \"replica #0 creation failed\");\n\t\t\tgoto err_create;\n\t\t}\n\t} else {\n\t\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\t\tif (REP(set, r)->remote != NULL) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (util_replica_create_local(set, r, flags, attr) !=\n\t\t\t\t\t0) {\n\t\t\t\tLOG(2, \"replica #%u creation failed\", r);\n\t\t\t\tgoto err_create;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_create:\n\toerrno = errno;\n\tfor (unsigned r = 0; r < set->nreplicas; r++)\n\t\tutil_replica_close(set, r);\n\terrno = oerrno;\nerr_poolset:\n\toerrno = errno;\n\tutil_poolset_close(set, DELETE_CREATED_PARTS);\n\terrno = oerrno;\n\treturn -1;\n\nerr_poolset_free:\n\toerrno = errno;\n\tutil_poolset_free(set);\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_pool_create -- create a new memory pool (set or a single file)\n *\n * On success returns 0 and a pointer to a newly allocated structure\n * containing the info of all the parts of the pool set and replicas.\n */\nint\nutil_pool_create(struct pool_set **setp, const char *path, size_t poolsize,\n\tsize_t minsize, size_t minpartsize, const struct pool_attr *attr,\n\tunsigned *nlanes, int can_have_rep)\n{\n\tLOG(3, \"setp %p path %s poolsize %zu minsize %zu minpartsize %zu \"\n\t\t\"attr %p nlanes %p can_have_rep %i\", setp, path, poolsize,\n\t\tminsize, minpartsize, attr, nlanes, can_have_rep);\n\n\treturn util_pool_create_uuids(setp, path, poolsize, minsize,\n\t\t\tminpartsize, attr, nlanes, can_have_rep, POOL_LOCAL);\n}\n\n/*\n * util_replica_open_local -- (internal) open a memory pool local replica\n */\nstatic int\nutil_replica_open_local(struct pool_set *set, unsigned repidx, int flags)\n{\n\tLOG(3, \"set %p repidx %u flags %d\", set, repidx, flags);\n\n\tint remaining_retries = 10;\n\tint retry_for_contiguous_addr;\n\tsize_t mapsize;\n\tsize_t hdrsize = (set->options & (OPTION_SINGLEHDR | OPTION_NOHDRS)) ?\n\t\t\t0 : Mmap_align;\n\tstruct pool_replica *rep = set->replica[repidx];\n\tvoid *addr = NULL;\n\n\tdo {\n\t\tretry_for_contiguous_addr = 0;\n\n\t\t/* determine a hint address for mmap() if not specified */\n\t\tif (addr == NULL)\n\t\t\taddr = util_map_hint(rep->resvsize, 0);\n\t\tif (addr == MAP_FAILED) {\n\t\t\tERR(\"cannot find a contiguous region of given size\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tmapsize = rep->part[0].filesize & ~(Mmap_align - 1);\n\n\t\t/* map the first part and reserve space for remaining parts */\n\t\tif (util_map_part(&rep->part[0], addr, rep->resvsize, 0,\n\t\t\t\tflags, 0) != 0) {\n\t\t\tLOG(2, \"pool mapping failed - replica #%u part #0\",\n\t\t\t\trepidx);\n\t\t\treturn -1;\n\t\t}\n\n\t\tVALGRIND_REGISTER_PMEM_MAPPING(rep->part[0].addr,\n\t\t\trep->resvsize);\n\t\tVALGRIND_REGISTER_PMEM_FILE(rep->part[0].fd,\n\t\t\trep->part[0].addr, rep->resvsize, 0);\n\n\t\t/* map all headers - don't care about the address */\n\t\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\t\tif (util_map_hdr(&rep->part[p], flags, 0) != 0) {\n\t\t\t\tLOG(2, \"header mapping failed - part #%d\", p);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\taddr = (char *)rep->part[0].addr + mapsize;\n\n\t\t/*\n\t\t * map the remaining parts of the usable pool space\n\t\t * (aligned to memory mapping granularity)\n\t\t */\n\t\tfor (unsigned p = 1; p < rep->nparts; p++) {\n\t\t\tstruct pool_set_part *part = &rep->part[p];\n\t\t\tsize_t targetsize = mapsize +\n\t\t\t\tALIGN_DOWN(part->filesize - hdrsize,\n\t\t\t\tpart->alignment);\n\t\t\tif (targetsize > rep->resvsize) {\n\t\t\t\tERR(\n\t\t\t\t\t\"pool mapping failed - address space reservation too small\");\n\t\t\t\terrno = EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\t/* map data part */\n\t\t\tif (util_map_part(part, addr, 0, hdrsize,\n\t\t\t\t\tflags | MAP_FIXED, 0) != 0) {\n\t\t\t\t/*\n\t\t\t\t * if we can't map the part at the address we\n\t\t\t\t * asked for, unmap all the parts that are\n\t\t\t\t * mapped and remap at a different address.\n\t\t\t\t */\n\t\t\t\tif ((errno == EINVAL) &&\n\t\t\t\t    (remaining_retries > 0)) {\n\t\t\t\t\tLOG(2, \"usable space mapping failed - \"\n\t\t\t\t\t\t\"part #%d - retrying\", p);\n\t\t\t\t\tretry_for_contiguous_addr = 1;\n\t\t\t\t\tremaining_retries--;\n\n\t\t\t\t\tutil_unmap_parts(rep, 0, p - 1);\n\n\t\t\t\t\t/* release rest of the VA reserved */\n\t\t\t\t\tmunmap(rep->part[0].addr,\n\t\t\t\t\t\trep->resvsize);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tLOG(2, \"usable space mapping failed - part #%d\",\n\t\t\t\t\tp);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tVALGRIND_REGISTER_PMEM_FILE(part->fd,\n\t\t\t\tpart->addr, part->size, hdrsize);\n\n\t\t\tmapsize += part->size;\n\t\t\taddr = (char *)addr + part->size;\n\t\t}\n\t} while (retry_for_contiguous_addr);\n\n\t/*\n\t * Initially part[0].size is the size of address space\n\t * reservation for all parts from given replica. After\n\t * mapping that space we need to overwrite part[0].size\n\t * with its actual size to be consistent - size for each\n\t * part should be the actual mapping size of this part\n\t * only - it simplifies future calculations.\n\t */\n\trep->part[0].size = rep->part[0].filesize & ~(Mmap_align - 1);\n\n\tif (util_replica_check_map_sync(set, repidx, 1))\n\t\tgoto err;\n\n\tutil_replica_set_is_pmem(rep);\n\n\tif (Prefault_at_open)\n\t\tutil_replica_force_page_allocation(rep);\n\n\tASSERTeq(mapsize, rep->repsize);\n\n\t/* calculate pool size - choose the smallest replica size */\n\tif (rep->repsize < set->poolsize)\n\t\tset->poolsize = rep->repsize;\n\n\tLOG(3, \"replica addr %p\", rep->part[0].addr);\n\n\treturn 0;\nerr:\n\tLOG(4, \"error clean up\");\n\tint oerrno = errno;\n\tif (mapsize < rep->repsize) {\n\t\tASSERTne(rep->part[0].addr, NULL);\n\t\tASSERTne(rep->part[0].addr, MAP_FAILED);\n\t\tmunmap(rep->part[0].addr, rep->resvsize - mapsize);\n\t}\n\tfor (unsigned p = 0; p < rep->nhdrs; p++)\n\t\tutil_unmap_hdr(&rep->part[p]);\n\tfor (unsigned p = 0; p < rep->nparts; p++)\n\t\tutil_unmap_part(&rep->part[p]);\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_replica_open_remote -- open a memory pool for remote replica\n */\nint\nutil_replica_open_remote(struct pool_set *set, unsigned repidx, int flags)\n{\n\tLOG(3, \"set %p repidx %u flags %d\", set, repidx, flags);\n\n\tstruct pool_replica *rep = set->replica[repidx];\n\n\tASSERTne(rep->remote, NULL);\n\tASSERTne(rep->part, NULL);\n\tASSERTeq(rep->nparts, 1);\n\tASSERTeq(rep->nhdrs, 1);\n\n\tstruct pool_set_part *part = rep->part;\n\n\tpart->size = rep->repsize;\n\tASSERT(IS_PAGE_ALIGNED(part->size));\n\tpart->remote_hdr = Zalloc(part->size + Pagesize);\n\tif (!part->remote_hdr) {\n\t\tERR(\"!Zalloc\");\n\t\treturn -1;\n\t}\n\n\tpart->hdr = PAGE_ALIGN_UP(part->remote_hdr);\n\tpart->addr = PAGE_ALIGN_UP(part->remote_hdr);\n\tpart->hdrsize = POOL_HDR_SIZE;\n\n\tLOG(3, \"replica #%u addr %p\", repidx, rep->part[0].addr);\n\n\treturn 0;\n}\n\n/*\n * util_replica_open -- open a memory pool replica\n */\nint\nutil_replica_open(struct pool_set *set, unsigned repidx, int flags)\n{\n\tLOG(3, \"set %p repidx %u flags %d\", set, repidx, flags);\n\n\tif (set->replica[repidx]->remote)\n\t\treturn util_replica_open_remote(set, repidx, flags);\n\n\treturn util_replica_open_local(set, repidx, flags);\n}\n\n/*\n * util_replica_set_attr -- overwrite existing replica attributes\n */\nint\nutil_replica_set_attr(struct pool_replica *rep,\n\t\tconst struct rpmem_pool_attr *rattr)\n{\n\tLOG(3, \"rep %p, rattr %p\", rep, rattr);\n\tASSERT(rattr != NULL || rep->nhdrs == 0);\n\n\tif (rattr != NULL && rep->nhdrs == 0) {\n\t\tERR(\n\t\t\"cannot set pool attributes for a replica without headers (with the NOHDRS option)\");\n\t\terrno = EINVAL;\n\t\treturn -1;\n\t}\n\tint flags = MAP_SHARED;\n\n\t/* map all headers - don't care about the address */\n\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\tif (util_map_hdr(&rep->part[p], flags, 0) != 0) {\n\t\t\tLOG(2, \"header mapping failed - part #%d\", p);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\tASSERTne(rattr, NULL);\n\n\t\tstruct pool_hdr *hdrp = HDR(rep, p);\n\t\tASSERTne(hdrp, NULL);\n\t\tutil_convert2h_hdr_nocheck(hdrp);\n\n\t\tutil_set_rpmem_attr(hdrp, rattr);\n\n\t\tif (hdrp == HDR(rep, 0))\n\t\t\tmemcpy(hdrp->uuid, rattr->uuid, POOL_HDR_UUID_LEN);\n\t\tif (hdrp == HDRP(rep, 0))\n\t\t\tmemcpy(hdrp->next_part_uuid, rattr->uuid,\n\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\t\tif (hdrp == HDRN(rep, 0))\n\t\t\tmemcpy(hdrp->prev_part_uuid, rattr->uuid,\n\t\t\t\t\tPOOL_HDR_UUID_LEN);\n\n\t\tutil_convert2le_hdr(hdrp);\n\n\t\tutil_checksum(hdrp, sizeof(*hdrp), &hdrp->checksum,\n\t\t\t1, POOL_HDR_CSUM_END_OFF(hdrp));\n\n\t\t/* store pool's header */\n\t\tutil_persist_auto(rep->is_pmem, hdrp, sizeof(*hdrp));\n\t}\n\n\t/* unmap all headers */\n\tfor (unsigned p = 0; p < rep->nhdrs; p++)\n\t\tutil_unmap_hdr(&rep->part[p]);\n\n\treturn 0;\nerr:\n\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\tutil_unmap_hdr(&rep->part[p]);\n\t}\n\treturn -1;\n}\n\n/*\n * util_get_attr_from_header -- get pool attributes from a pool header\n */\nvoid\nutil_pool_hdr2attr(struct pool_attr *attr, struct pool_hdr *hdr)\n{\n\tLOG(3, \"attr %p, hdr %p\", attr, hdr);\n\tASSERTne(attr, NULL);\n\tASSERTne(hdr, NULL);\n\tmemset(attr, 0, sizeof(*attr));\n\tmemcpy(attr->signature, hdr->signature, POOL_HDR_SIG_LEN);\n\tattr->major = hdr->major;\n\tattr->features.compat = hdr->features.compat;\n\tattr->features.incompat = hdr->features.incompat;\n\tattr->features.ro_compat = hdr->features.ro_compat;\n\tmemcpy(attr->poolset_uuid, hdr->poolset_uuid, POOL_HDR_UUID_LEN);\n}\n\n/*\n * util_copy_attr_to_header -- copy pool attributes into pool header\n */\nvoid\nutil_pool_attr2hdr(struct pool_hdr *hdr, const struct pool_attr *attr)\n{\n\tLOG(3, \"hdr %p, attr %p\", hdr, attr);\n\tASSERTne(hdr, NULL);\n\tASSERTne(attr, NULL);\n\tmemcpy(hdr->signature, attr->signature, POOL_HDR_SIG_LEN);\n\thdr->major = attr->major;\n\thdr->features.compat = attr->features.compat;\n\thdr->features.incompat = attr->features.incompat;\n\thdr->features.ro_compat = attr->features.ro_compat;\n}\n\n/*\n * util_unmap_all_hdrs -- unmap all pool set headers\n */\nstatic void\nutil_unmap_all_hdrs(struct pool_set *set)\n{\n\tLOG(3, \"set %p\", set);\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tif (rep->remote == NULL) {\n\t\t\tfor (unsigned p = 0; p < rep->nhdrs; p++)\n\t\t\t\tutil_unmap_hdr(&rep->part[p]);\n\t\t} else {\n\t\t\t/*\n\t\t\t * hdr & hdrsize were set only for util_header_check(),\n\t\t\t * they will not be used any more. The memory will be\n\t\t\t * freed by util_replica_close()\n\t\t\t */\n\t\t\trep->part[0].hdr = NULL;\n\t\t\trep->part[0].hdrsize = 0;\n\t\t}\n\t}\n}\n\n/*\n * util_replica_check -- check headers, check UUID's, check replicas linkage\n */\nstatic int\nutil_replica_check(struct pool_set *set, const struct pool_attr *attr)\n{\n\tLOG(3, \"set %p attr %p\", set, attr);\n\n\t/* read shutdown state toggle from header */\n\tset->ignore_sds |= IGNORE_SDS(HDR(REP(set, 0), 0));\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\t\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\t\tif (util_header_check(set, r, p, attr) != 0) {\n\t\t\t\tLOG(2, \"header check failed - part #%d\", p);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tset->rdonly |= rep->part[p].rdonly;\n\t\t}\n\n\t\tif (memcmp(HDR(REPP(set, r), 0)->uuid,\n\t\t\t\t\tHDR(REP(set, r), 0)->prev_repl_uuid,\n\t\t\t\t\tPOOL_HDR_UUID_LEN) ||\n\t\t    memcmp(HDR(REPN(set, r), 0)->uuid,\n\t\t\t\t\tHDR(REP(set, r), 0)->next_repl_uuid,\n\t\t\t\t\tPOOL_HDR_UUID_LEN)) {\n\t\t\tERR(\"wrong replica UUID\");\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\t\tif (!set->ignore_sds && !rep->remote && rep->nhdrs) {\n\t\t\tstruct shutdown_state sds;\n\t\t\tshutdown_state_init(&sds, NULL);\n\t\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\t\tif (shutdown_state_add_part(&sds,\n\t\t\t\t\t\tPART(rep, p)->path, NULL))\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tASSERTne(rep->nhdrs, 0);\n\t\t\tASSERTne(rep->nparts, 0);\n\t\t\tif (shutdown_state_check(&sds, &HDR(rep, 0)->sds,\n\t\t\t\t\trep)) {\n\t\t\t\tLOG(2, \"ADR failure detected\");\n\t\t\t\terrno = EINVAL;\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tshutdown_state_set_dirty(&HDR(rep, 0)->sds,\n\t\t\t\trep);\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * util_pool_has_device_dax -- (internal) check if poolset has any device dax\n */\nint\nutil_pool_has_device_dax(struct pool_set *set)\n{\n\tfor (unsigned r = 0; r < set->nreplicas; ++r) {\n\t\tstruct pool_replica *rep = REP(set, r);\n\t\t/* either all the parts must be Device DAX or none */\n\t\tif (PART(rep, 0)->is_dev_dax)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * util_pool_open_nocheck -- open a memory pool (set or a single file)\n *\n * This function opens a pool set without checking the header values.\n */\nint\nutil_pool_open_nocheck(struct pool_set *set, unsigned flags)\n{\n\tLOG(3, \"set %p flags 0x%x\", set, flags);\n\n\tint cow = flags & POOL_OPEN_COW;\n\n\tif (cow && util_pool_has_device_dax(set)) {\n\t\tERR(\"device dax cannot be mapped privately\");\n\t\terrno = ENOTSUP;\n\t\treturn -1;\n\t}\n\n\tint mmap_flags = cow ? MAP_PRIVATE|MAP_NORESERVE : MAP_SHARED;\n\tint oerrno;\n\n\tASSERTne(set, NULL);\n\tASSERT(set->nreplicas > 0);\n\n\tif (flags & POOL_OPEN_CHECK_BAD_BLOCKS) {\n\t\t/* check if any bad block recovery file exists */\n\t\tif (badblocks_recovery_file_exists(set)) {\n\t\t\tERR(\n\t\t\t\t\"error: a bad block recovery file exists, run 'pmempool sync --bad-blocks' utility to try to recover the pool\");\n\t\t\terrno = EINVAL;\n\t\t\treturn -1;\n\t\t}\n\n\t\tint bbs = badblocks_check_poolset(set, 0 /* not create */);\n\t\tif (bbs < 0) {\n\t\t\tLOG(1, \"failed to check pool set for bad blocks\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (bbs > 0) {\n\t\t\tif (flags & POOL_OPEN_IGNORE_BAD_BLOCKS) {\n\t\t\t\tLOG(1,\n\t\t\t\t\t\"WARNING: pool set contains bad blocks, ignoring\");\n\t\t\t} else {\n\t\t\t\tERR(\n\t\t\t\t\t\"pool set contains bad blocks and cannot be opened, run 'pmempool sync --bad-blocks' utility to try to recover the pool\");\n\t\t\t\terrno = EIO;\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (set->remote && util_remote_load()) {\n\t\tERR(\"the pool set requires a remote replica, \"\n\t\t\t\"but the '%s' library cannot be loaded\",\n\t\t\tLIBRARY_REMOTE);\n\t\treturn -1;\n\t}\n\n\tint ret = util_poolset_files_local(set, 0 /* minpartsize */, 0);\n\tif (ret != 0)\n\t\tgoto err_poolset;\n\n\tset->rdonly = 0;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tif (util_replica_open(set, r, mmap_flags) != 0) {\n\t\t\tLOG(2, \"replica #%u open failed\", r);\n\t\t\tgoto err_replica;\n\t\t}\n\t}\n\n\tif (set->remote) {\n\t\tret = util_poolset_files_remote(set, 0, NULL, 0);\n\t\tif (ret != 0)\n\t\t\tgoto err_replica;\n\t}\n\n\tutil_unmap_all_hdrs(set);\n\n\treturn 0;\n\nerr_replica:\n\tLOG(4, \"error clean up\");\n\toerrno = errno;\n\tfor (unsigned r = 0; r < set->nreplicas; r++)\n\t\tutil_replica_close(set, r);\n\terrno = oerrno;\nerr_poolset:\n\toerrno = errno;\n\tutil_poolset_close(set, DO_NOT_DELETE_PARTS);\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_read_compat_features -- (internal) read compat features from the header\n */\nstatic int\nutil_read_compat_features(struct pool_set *set, uint32_t *compat_features)\n{\n\tLOG(3, \"set %p pcompat_features %p\", set, compat_features);\n\n\t*compat_features = 0;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct pool_replica *rep = set->replica[r];\n\n\t\tif (rep->remote)\n\t\t\tcontinue;\n\n\t\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\t\tstruct pool_set_part *part = &rep->part[p];\n\n\t\t\tif (util_part_open(part, 0, 0 /* create */)) {\n\t\t\t\tLOG(1, \"!cannot open the part -- \\\"%s\\\"\",\n\t\t\t\t\tpart->path);\n\t\t\t\t/* try to open the next part */\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (util_map_hdr(part, MAP_SHARED, 0) != 0) {\n\t\t\t\tLOG(1, \"header mapping failed -- \\\"%s\\\"\",\n\t\t\t\t\tpart->path);\n\t\t\t\tutil_part_fdclose(part);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tstruct pool_hdr *hdrp = part->hdr;\n\t\t\t*compat_features = hdrp->features.compat;\n\n\t\t\tutil_unmap_hdr(part);\n\t\t\tutil_part_fdclose(part);\n\n\t\t\t/* exit on the first successfully opened part */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_pool_open -- open a memory pool (set or a single file)\n *\n * This routine does all the work, but takes a rdonly flag so internal\n * calls can map a read-only pool if required.\n */\nint\nutil_pool_open(struct pool_set **setp, const char *path, size_t minpartsize,\n\tconst struct pool_attr *attr, unsigned *nlanes, void *addr,\n\tunsigned flags)\n{\n\tLOG(3, \"setp %p path %s minpartsize %zu attr %p nlanes %p \"\n\t\t\"addr %p flags 0x%x \", setp, path, minpartsize, attr, nlanes,\n\t\taddr, flags);\n\n\tint cow = flags & POOL_OPEN_COW;\n\tint mmap_flags = cow ? MAP_PRIVATE|MAP_NORESERVE : MAP_SHARED;\n\tint oerrno;\n\n\t/* do not check minsize */\n\tint ret = util_poolset_create_set(setp, path, 0, 0,\n\t\t\t\t\t\tflags & POOL_OPEN_IGNORE_SDS);\n\tif (ret < 0) {\n\t\tLOG(2, \"cannot open pool set -- '%s'\", path);\n\t\treturn -1;\n\t}\n\n\tif (cow && (*setp)->replica[0]->part[0].is_dev_dax) {\n\t\tERR(\"device dax cannot be mapped privately\");\n\t\terrno = ENOTSUP;\n\t\tgoto err_poolset_free;\n\t}\n\n\tstruct pool_set *set = *setp;\n\n\tASSERT(set->nreplicas > 0);\n\n\tuint32_t compat_features;\n\n\tif (util_read_compat_features(set, &compat_features)) {\n\t\tLOG(1, \"reading compat features failed\");\n\t\tgoto err_poolset_free;\n\t}\n\n\tif (compat_features & POOL_FEAT_CHECK_BAD_BLOCKS) {\n\t\t/* check if any bad block recovery file exists */\n\t\tif (badblocks_recovery_file_exists(set)) {\n\t\t\tERR(\n\t\t\t\t\"error: a bad block recovery file exists, run 'pmempool sync --bad-blocks' utility to try to recover the pool\");\n\t\t\terrno = EINVAL;\n\t\t\tgoto err_poolset_free;\n\t\t}\n\n\t\tint bbs = badblocks_check_poolset(set, 0 /* not create */);\n\t\tif (bbs < 0) {\n\t\t\tLOG(1,\n\t\t\t\t\"failed to check pool set for bad blocks -- '%s'\",\n\t\t\t\tpath);\n\t\t\tgoto err_poolset_free;\n\t\t}\n\n\t\tif (bbs > 0) {\n\t\t\tif (flags & POOL_OPEN_IGNORE_BAD_BLOCKS) {\n\t\t\t\tLOG(1,\n\t\t\t\t\t\"WARNING: pool set contains bad blocks, ignoring -- '%s'\",\n\t\t\t\t\tpath);\n\t\t\t} else {\n\t\t\t\tERR(\n\t\t\t\t\t\"pool set contains bad blocks and cannot be opened, run 'pmempool sync --bad-blocks' utility to try to recover the pool -- '%s'\",\n\t\t\t\t\tpath);\n\t\t\t\terrno = EIO;\n\t\t\t\tgoto err_poolset_free;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (set->remote && util_remote_load()) {\n\t\tERR(\n\t\t\t\"the pool set requires a remote replica, but the '%s' library cannot be loaded\",\n\t\t\tLIBRARY_REMOTE);\n\t\tgoto err_poolset_free;\n\t}\n\n\tret = util_poolset_files_local(set, minpartsize, 0);\n\tif (ret != 0)\n\t\tgoto err_poolset;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tif (util_replica_open(set, r, mmap_flags) != 0) {\n\t\t\tLOG(2, \"replica #%u open failed\", r);\n\t\t\tgoto err_replica;\n\t\t}\n\t}\n\n\tif (set->remote) {\n\t\t/* do not check minsize */\n\t\tret = util_poolset_files_remote(set, 0, nlanes, 0);\n\t\tif (ret != 0)\n\t\t\tgoto err_replica;\n\t}\n\n\t/* check headers, check UUID's, check replicas linkage */\n\tif (attr != NULL && util_replica_check(set, attr))\n\t\tgoto err_replica;\n\n\t/* unmap all headers */\n\tutil_unmap_all_hdrs(set);\n\n\treturn 0;\n\nerr_replica:\n\tLOG(4, \"error clean up\");\n\toerrno = errno;\n\tfor (unsigned r = 0; r < set->nreplicas; r++)\n\t\tutil_replica_close(set, r);\n\terrno = oerrno;\nerr_poolset:\n\toerrno = errno;\n\tutil_poolset_close(set, DO_NOT_DELETE_PARTS);\n\terrno = oerrno;\n\treturn -1;\n\nerr_poolset_free:\n\toerrno = errno;\n\tutil_poolset_free(*setp);\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_pool_open_remote -- open a remote pool set file\n *\n * This routine does all the work, but takes a rdonly flag so internal\n * calls can map a read-only pool if required.\n */\nint\nutil_pool_open_remote(struct pool_set **setp, const char *path, int cow,\n\tsize_t minpartsize, struct rpmem_pool_attr *rattr)\n{\n\tLOG(3, \"setp %p path %s cow %d minpartsize %zu rattr %p\",\n\t\tsetp, path, cow, minpartsize, rattr);\n\n\tint flags = cow ? MAP_PRIVATE|MAP_NORESERVE : MAP_SHARED;\n\tint oerrno;\n\n\t/* do not check minsize */\n\tint ret = util_poolset_create_set(setp, path, 0, 0, 0);\n\tif (ret < 0) {\n\t\tLOG(2, \"cannot open pool set -- '%s'\", path);\n\t\treturn -1;\n\t}\n\n\tif (cow && (*setp)->replica[0]->part[0].is_dev_dax) {\n\t\tERR(\"device dax cannot be mapped privately\");\n\t\terrno = ENOTSUP;\n\t\treturn -1;\n\t}\n\n\tstruct pool_set *set = *setp;\n\n\tif (set->nreplicas > 1) {\n\t\tLOG(2, \"remote pool set cannot have replicas\");\n\t\tgoto err_poolset;\n\t}\n\n\tuint32_t compat_features;\n\n\tif (util_read_compat_features(set, &compat_features)) {\n\t\tLOG(1, \"reading compat features failed\");\n\t\tgoto err_poolset;\n\t}\n\n\tif (compat_features & POOL_FEAT_CHECK_BAD_BLOCKS) {\n\t\t/* check if there are any bad blocks */\n\t\tint bbs = badblocks_check_poolset(set, 0 /* not create */);\n\t\tif (bbs < 0) {\n\t\t\tLOG(1,\n\t\t\t\t\"failed to check the remote replica for bad blocks -- '%s'\",\n\t\t\t\tpath);\n\t\t\tgoto err_poolset;\n\t\t}\n\n\t\tif (bbs > 0) {\n\t\t\tERR(\n\t\t\t\t\"remote replica contains bad blocks and cannot be opened, run 'pmempool sync --bad-blocks' utility to recreate it -- '%s'\",\n\t\t\t\tpath);\n\t\t\terrno = EIO;\n\t\t\tgoto err_poolset;\n\t\t}\n\t}\n\n\tret = util_poolset_files_local(set, minpartsize, 0);\n\tif (ret != 0)\n\t\tgoto err_poolset;\n\n\tif (util_replica_open(set, 0, flags) != 0) {\n\t\tLOG(2, \"replica open failed\");\n\t\tgoto err_replica;\n\t}\n\n\tstruct pool_replica *rep = set->replica[0];\n\n\tset->rdonly |= rep->part[0].rdonly;\n\n\t/* check headers, check UUID's, check replicas linkage */\n\tfor (unsigned p = 0; p < rep->nhdrs; p++) {\n\t\tif (util_header_check_remote(set, p) != 0) {\n\t\t\tLOG(2, \"header check failed - part #%d\", p);\n\t\t\tgoto err_replica;\n\t\t}\n\t\tset->rdonly |= rep->part[p].rdonly;\n\t}\n\n\tif (rep->nhdrs > 0) {\n\t\t/* header exists, copy pool attributes */\n\t\tstruct pool_hdr *hdr = rep->part[0].hdr;\n\t\tutil_get_rpmem_attr(rattr, hdr);\n\t} else {\n\t\t/* header does not exist, zero pool attributes */\n\t\tmemset(rattr, 0, sizeof(*rattr));\n\t}\n\n\t/* unmap all headers */\n\tfor (unsigned p = 0; p < rep->nhdrs; p++)\n\t\tutil_unmap_hdr(&rep->part[p]);\n\n\treturn 0;\n\nerr_replica:\n\tLOG(4, \"error clean up\");\n\toerrno = errno;\n\tutil_replica_close(set, 0);\n\terrno = oerrno;\nerr_poolset:\n\toerrno = errno;\n\tutil_poolset_close(set, DO_NOT_DELETE_PARTS);\n\terrno = oerrno;\n\treturn -1;\n}\n\n/*\n * util_is_poolset_file -- check if specified file is a poolset file\n *\n * Return value:\n * -1 - error\n *  0 - not a poolset\n *  1 - is a poolset\n */\nint\nutil_is_poolset_file(const char *path)\n{\n\tenum file_type type = util_file_get_type(path);\n\tif (type < 0)\n\t\treturn -1;\n\n\tif (type == TYPE_DEVDAX)\n\t\treturn 0;\n\n\tint fd = util_file_open(path, NULL, 0, O_RDONLY);\n\tif (fd < 0)\n\t\treturn -1;\n\n\tint ret = 0;\n\tssize_t sret;\n\tchar signature[POOLSET_HDR_SIG_LEN];\n\tsize_t rd = 0;\n\tdo {\n\t\tsret = util_read(fd, &signature[rd], sizeof(signature) - rd);\n\t\tif (sret > 0)\n\t\t\trd += (size_t)sret;\n\t} while (sret > 0);\n\tif (sret < 0) {\n\t\tERR(\"!read\");\n\t\tret = -1;\n\t\tgoto out;\n\t} else if (rd != sizeof(signature)) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (memcmp(signature, POOLSET_HDR_SIG, POOLSET_HDR_SIG_LEN) == 0)\n\t\tret = 1;\nout:\n\tos_close(fd);\n\treturn ret;\n}\n/*\n * util_poolset_foreach_part_struct -- walk through all poolset file parts\n *                                  of the given set\n *\n * Stops processing if callback returns non-zero value.\n * The value returned by callback is returned to the caller.\n */\nint\nutil_poolset_foreach_part_struct(struct pool_set *set,\n\tint (*callback)(struct part_file *pf, void *arg), void *arg)\n{\n\tLOG(3, \"set %p callback %p arg %p\", set, callback, arg);\n\n\tASSERTne(callback, NULL);\n\n\tint ret;\n\n\tfor (unsigned r = 0; r < set->nreplicas; r++) {\n\t\tstruct part_file cbdata;\n\t\tif (set->replica[r]->remote) {\n\t\t\tcbdata.is_remote = 1;\n\t\t\tcbdata.remote = set->replica[r]->remote;\n\t\t\tcbdata.part = NULL;\n\t\t\tret = (*callback)(&cbdata, arg);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else {\n\t\t\tcbdata.is_remote = 0;\n\t\t\tcbdata.remote = NULL;\n\t\t\tfor (unsigned p = 0; p < set->replica[r]->nparts; p++) {\n\t\t\t\tcbdata.part = &set->replica[r]->part[p];\n\t\t\t\tret = (*callback)(&cbdata, arg);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * util_poolset_foreach_part -- walk through all poolset file parts\n *\n * Stops processing if callback returns non-zero value.\n * The value returned by callback is returned to the caller.\n *\n * Return value:\n *  0 - all part files have been processed\n * -1 - parsing poolset file error\n */\nint\nutil_poolset_foreach_part(const char *path,\n\tint (*callback)(struct part_file *pf, void *arg), void *arg)\n{\n\tLOG(3, \"path %s callback %p arg %p\", path, callback, arg);\n\n\tASSERTne(callback, NULL);\n\n\tint fd = os_open(path, O_RDONLY);\n\tif (fd < 0) {\n\t\tERR(\"!open: path \\\"%s\\\"\", path);\n\t\treturn -1;\n\t}\n\n\tstruct pool_set *set;\n\tint ret = util_poolset_parse(&set, path, fd);\n\tif (ret) {\n\t\tERR(\"util_poolset_parse failed -- '%s'\", path);\n\t\tret = -1;\n\t\tgoto err_close;\n\t}\n\n\tret = util_poolset_foreach_part_struct(set, callback, arg);\n\n\t/*\n\t * Make sure callback does not return -1,\n\t * because this value is reserved for parsing\n\t * error.\n\t */\n\tASSERTne(ret, -1);\n\tutil_poolset_free(set);\n\nerr_close:\n\tos_close(fd);\n\treturn ret;\n}\n\n/*\n * util_poolset_size -- get size of poolset, returns 0 on error\n */\nsize_t\nutil_poolset_size(const char *path)\n{\n\tint fd = os_open(path, O_RDONLY);\n\tif (fd < 0)\n\t\treturn 0;\n\n\tsize_t size = 0;\n\tstruct pool_set *set;\n\tif (util_poolset_parse(&set, path, fd))\n\t\tgoto err_close;\n\n\tsize = set->poolsize;\n\n\tutil_poolset_free(set);\nerr_close:\n\tos_close(fd);\n\treturn size;\n}\n\n/*\n * util_replica_fdclose -- close all parts of given replica\n */\nvoid\nutil_replica_fdclose(struct pool_replica *rep)\n{\n\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\tstruct pool_set_part *part = &rep->part[p];\n\t\tutil_part_fdclose(part);\n\t}\n}\n\n/*\n * util_replica_deep_common -- performs common calculations\n * on all parts from replica to define intersection ranges\n * for final flushing operations that take place in\n * os_part_deep_common function.\n */\nint\nutil_replica_deep_common(const void *addr, size_t len, struct pool_set *set,\n\t\t\t\tunsigned replica_id, int flush)\n{\n\tLOG(3, \"addr %p len %zu set %p replica_id %u flush %d\",\n\t\taddr, len, set, replica_id, flush);\n\n\tstruct pool_replica *rep = set->replica[replica_id];\n\tuintptr_t rep_start = (uintptr_t)rep->part[0].addr;\n\tuintptr_t rep_end = rep_start + rep->repsize;\n\tuintptr_t start = (uintptr_t)addr;\n\tuintptr_t end = start + len;\n\n\tASSERT(start >= rep_start);\n\tASSERT(end <= rep_end);\n\n\tfor (unsigned p = 0; p < rep->nparts; p++) {\n\t\tstruct pool_set_part *part = &rep->part[p];\n\t\tuintptr_t part_start = (uintptr_t)part->addr;\n\t\tuintptr_t part_end = part_start + part->size;\n\t\t/* init intersection start and end addresses */\n\t\tuintptr_t range_start = start;\n\t\tuintptr_t range_end = end;\n\n\t\tif (part_start > end || part_end < start)\n\t\t\tcontinue;\n\t\t/* recalculate intersection addresses */\n\t\tif (part_start > start)\n\t\t\trange_start = part_start;\n\t\tif (part_end < end)\n\t\t\trange_end = part_end;\n\t\tsize_t range_len = range_end - range_start;\n\n\t\tLOG(15, \"perform deep flushing for replica %u \"\n\t\t\t\"part %p, addr %p, len %lu\",\n\t\t\treplica_id, part, (void *)range_start, range_len);\n\t\tif (os_part_deep_common(rep, p, (void *)range_start,\n\t\t\t\trange_len, flush)) {\n\t\t\tLOG(1, \"os_part_deep_common(%p, %p, %lu)\",\n\t\t\t\tpart, (void *)range_start, range_len);\n\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * util_replica_deep_persist -- wrapper for util_replica_deep_common\n * Calling the target precedes initialization of function that\n * partly defines way of deep replica flushing.\n */\nint\nutil_replica_deep_persist(const void *addr, size_t len, struct pool_set *set,\n\t\t\t\tunsigned replica_id)\n{\n\tLOG(3, \"addr %p len %zu set %p replica_id %u\",\n\t\taddr, len, set, replica_id);\n\n\tint flush = 1;\n\treturn util_replica_deep_common(addr, len, set, replica_id, flush);\n}\n\n/*\n * util_replica_deep_drain -- wrapper for util_replica_deep_common\n * Calling the target precedes initialization of function that\n * partly defines way of deep replica flushing.\n */\nint\nutil_replica_deep_drain(const void *addr, size_t len, struct pool_set *set,\n\t\t\t\tunsigned replica_id)\n{\n\tLOG(3, \"addr %p len %zu set %p replica_id %u\",\n\t\taddr, len, set, replica_id);\n\n\tint flush = 0;\n\treturn util_replica_deep_common(addr, len, set, replica_id, flush);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/common/os.h": "/*\n * Copyright 2017-2018, Intel Corporation\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in\n *       the documentation and/or other materials provided with the\n *       distribution.\n *\n *     * Neither the name of the copyright holder nor the names of its\n *       contributors may be used to endorse or promote products derived\n *       from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/*\n * os.h -- os abstaction layer\n */\n\n#ifndef PMDK_OS_H\n#define PMDK_OS_H 1\n\n#include <sys/stat.h>\n#include <stdio.h>\n#include <unistd.h>\n\n#include \"errno_freebsd.h\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#ifndef _WIN32\n#define OS_DIR_SEPARATOR '/'\n#define OS_DIR_SEP_STR \"/\"\n#else\n#define OS_DIR_SEPARATOR '\\\\'\n#define OS_DIR_SEP_STR \"\\\\\"\n#endif\n\n#ifndef _WIN32\n\n/* madvise() */\n#ifdef __FreeBSD__\n#define os_madvise minherit\n#define MADV_DONTFORK INHERIT_NONE\n#else\n#define os_madvise madvise\n#endif\n\n/* dlopen() */\n#ifdef __FreeBSD__\n#define RTLD_DEEPBIND 0\t/* XXX */\n#endif\n\n/* major(), minor() */\n#ifdef __FreeBSD__\n#define os_major (unsigned)major\n#define os_minor (unsigned)minor\n#else\n#define os_major major\n#define os_minor minor\n#endif\n\n#endif /* #ifndef _WIN32 */\n\nstruct iovec;\n\n/* os_flock */\n#define OS_LOCK_SH 1\n#define OS_LOCK_EX 2\n#define OS_LOCK_NB 4\n#define OS_LOCK_UN 8\n\n#ifndef _WIN32\ntypedef struct stat os_stat_t;\n#define os_fstat\tfstat\n#define os_lseek\tlseek\n#else\ntypedef struct _stat64 os_stat_t;\n#define os_fstat\t_fstat64\n#define os_lseek\t_lseeki64\n#endif\n\n#define os_close close\n#define os_fclose fclose\n\n#ifndef _WIN32\ntypedef off_t os_off_t;\n#else\n/* XXX: os_off_t defined in platform.h */\n#endif\nint os_open(const char *pathname, int flags, ...);\nint os_fsync(int fd);\nint os_fsync_dir(const char *dir_name);\nint os_stat(const char *pathname, os_stat_t *buf);\nint os_unlink(const char *pathname);\nint os_access(const char *pathname, int mode);\nFILE *os_fopen(const char *pathname, const char *mode);\nFILE *os_fdopen(int fd, const char *mode);\nint os_chmod(const char *pathname, mode_t mode);\nint os_mkstemp(char *temp);\nint os_posix_fallocate(int fd, os_off_t offset, os_off_t len);\nint os_ftruncate(int fd, os_off_t length);\nint os_flock(int fd, int operation);\nssize_t os_writev(int fd, const struct iovec *iov, int iovcnt);\nint os_clock_gettime(int id, struct timespec *ts);\nunsigned os_rand_r(unsigned *seedp);\nint os_unsetenv(const char *name);\nint os_setenv(const char *name, const char *value, int overwrite);\nchar *os_getenv(const char *name);\nconst char *os_strsignal(int sig);\nint os_execv(const char *path, char *const argv[]);\n\n/*\n * XXX: missing APis (used in ut_file.c)\n *\n * rename\n * read\n * write\n */\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* os.h */\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/libvmmalloc/libvmmalloc.c": "/*\n * Copyright 2014-2018, Intel Corporation\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in\n *       the documentation and/or other materials provided with the\n *       distribution.\n *\n *     * Neither the name of the copyright holder nor the names of its\n *       contributors may be used to endorse or promote products derived\n *       from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/*\n * libvmmalloc.c -- entry points for libvmmalloc\n *\n * NOTES:\n * 1) Since some standard library functions (fopen, sprintf) use malloc\n *    internally, then at initialization phase, malloc(3) calls are redirected\n *    to the standard jemalloc interfaces that operate on a system heap.\n *    There is no need to track these allocations.  For small allocations,\n *    jemalloc is able to detect the corresponding pool the memory was\n *    allocated from, and Vmp argument is actually ignored.  So, it is safe\n *    to reclaim this memory using je_vmem_pool_free().\n *    The problem may occur for huge allocations only (>2MB), but it seems\n *    like such allocations do not happen at initialization phase.\n *\n * 2) Debug traces in malloc(3) functions are not available until library\n *    initialization (vmem pool creation) is completed.  This is to avoid\n *    recursive calls to malloc, leading to stack overflow.\n *\n * 3) Malloc hooks in glibc are overridden to prevent any references to glibc's\n *    malloc(3) functions in case the application uses dlopen with\n *    RTLD_DEEPBIND flag. (Not relevant for FreeBSD since FreeBSD supports\n *    neither malloc hooks nor RTLD_DEEPBIND.)\n *\n * 4) If the process forks, there is no separate log file open for a new\n *    process, even if the configured log file name is terminated with \"-\".\n *\n * 5) Fork options 2 and 3 are currently not supported on FreeBSD because\n *    locks are dynamically allocated on FreeBSD and hence they would be cloned\n *    as part of the pool. This may be solvable.\n */\n\n#define _GNU_SOURCE\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/mman.h>\n#include <sys/param.h>\n#include <errno.h>\n#include <stdint.h>\n#include <signal.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <pthread.h>\n#ifndef __FreeBSD__\n#include <malloc.h>\n#endif\n\n#include \"libvmem.h\"\n#include \"libvmmalloc.h\"\n\n#include \"jemalloc.h\"\n#include \"pmemcommon.h\"\n#include \"file.h\"\n#include \"os.h\"\n#include \"os_thread.h\"\n#include \"vmem.h\"\n#include \"vmmalloc.h\"\n#include \"valgrind_internal.h\"\n\n#define HUGE (2 * 1024 * 1024)\n\n/*\n * private to this file...\n */\nstatic size_t Header_size;\nstatic VMEM *Vmp;\nstatic char *Dir;\nstatic int Fd;\nstatic int Fd_clone;\nstatic int Private;\nstatic int Forkopt = 1; /* default behavior - remap as private */\nstatic bool Destructed; /* when set - ignore all calls (do not call jemalloc) */\n\n/*\n * malloc -- allocate a block of size bytes\n */\n__ATTR_MALLOC__\n__ATTR_ALLOC_SIZE__(1)\nvoid *\nmalloc(size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_malloc(size);\n\t}\n\tLOG(4, \"size %zu\", size);\n\treturn je_vmem_pool_malloc(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size), size);\n}\n\n/*\n * calloc -- allocate a block of nmemb * size bytes and set its contents to zero\n */\n__ATTR_MALLOC__\n__ATTR_ALLOC_SIZE__(1, 2)\nvoid *\ncalloc(size_t nmemb, size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\tif (Vmp == NULL) {\n\t\tASSERT((nmemb * size) <= HUGE);\n\t\treturn je_vmem_calloc(nmemb, size);\n\t}\n\tLOG(4, \"nmemb %zu, size %zu\", nmemb, size);\n\treturn je_vmem_pool_calloc((pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\tnmemb, size);\n}\n\n/*\n * realloc -- resize a block previously allocated by malloc\n */\n__ATTR_ALLOC_SIZE__(2)\nvoid *\nrealloc(void *ptr, size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_realloc(ptr, size);\n\t}\n\tLOG(4, \"ptr %p, size %zu\", ptr, size);\n\treturn je_vmem_pool_ralloc((pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\tptr, size);\n}\n\n/*\n * free -- free a block previously allocated by malloc\n */\nvoid\nfree(void *ptr)\n{\n\tif (unlikely(Destructed))\n\t\treturn;\n\n\tif (Vmp == NULL) {\n\t\tje_vmem_free(ptr);\n\t\treturn;\n\t}\n\tLOG(4, \"ptr %p\", ptr);\n\tje_vmem_pool_free((pool_t *)((uintptr_t)Vmp + Header_size), ptr);\n}\n\n/*\n * cfree -- free a block previously allocated by calloc\n *\n * the implementation is identical to free()\n *\n * XXX Not supported on FreeBSD, but we define it anyway\n */\nvoid\ncfree(void *ptr)\n{\n\tif (unlikely(Destructed))\n\t\treturn;\n\n\tif (Vmp == NULL) {\n\t\tje_vmem_free(ptr);\n\t\treturn;\n\t}\n\tLOG(4, \"ptr %p\", ptr);\n\tje_vmem_pool_free((pool_t *)((uintptr_t)Vmp + Header_size), ptr);\n}\n\n/*\n * memalign -- allocate a block of size bytes, starting on an address\n * that is a multiple of boundary\n *\n * XXX Not supported on FreeBSD, but we define it anyway\n */\n__ATTR_MALLOC__\n__ATTR_ALLOC_ALIGN__(1)\n__ATTR_ALLOC_SIZE__(2)\nvoid *\nmemalign(size_t boundary, size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_aligned_alloc(boundary, size);\n\t}\n\tLOG(4, \"boundary %zu  size %zu\", boundary, size);\n\treturn je_vmem_pool_aligned_alloc(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\tboundary, size);\n}\n\n/*\n * aligned_alloc -- allocate a block of size bytes, starting on an address\n * that is a multiple of alignment\n *\n * size must be a multiple of alignment\n */\n__ATTR_MALLOC__\n__ATTR_ALLOC_ALIGN__(1)\n__ATTR_ALLOC_SIZE__(2)\nvoid *\naligned_alloc(size_t alignment, size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\t/* XXX - check if size is a multiple of alignment */\n\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_aligned_alloc(alignment, size);\n\t}\n\tLOG(4, \"alignment %zu  size %zu\", alignment, size);\n\treturn je_vmem_pool_aligned_alloc(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\talignment, size);\n}\n\n/*\n * posix_memalign -- allocate a block of size bytes, starting on an address\n * that is a multiple of alignment\n */\n__ATTR_NONNULL__(1)\nint\nposix_memalign(void **memptr, size_t alignment, size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn ENOMEM;\n\n\tint ret = 0;\n\tint oerrno = errno;\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_posix_memalign(memptr, alignment, size);\n\t}\n\tLOG(4, \"alignment %zu  size %zu\", alignment, size);\n\t*memptr = je_vmem_pool_aligned_alloc(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\talignment, size);\n\tif (*memptr == NULL)\n\t\tret = errno;\n\terrno = oerrno;\n\treturn ret;\n}\n\n/*\n * valloc -- allocate a block of size bytes, starting on a page boundary\n */\n__ATTR_MALLOC__\n__ATTR_ALLOC_SIZE__(1)\nvoid *\nvalloc(size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\tASSERTne(Pagesize, 0);\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_aligned_alloc(Pagesize, size);\n\t}\n\tLOG(4, \"size %zu\", size);\n\treturn je_vmem_pool_aligned_alloc(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\tPagesize, size);\n}\n\n/*\n * pvalloc -- allocate a block of size bytes, starting on a page boundary\n *\n * Requested size is also aligned to page boundary.\n *\n * XXX Not supported on FreeBSD, but we define it anyway.\n */\n__ATTR_MALLOC__\n__ATTR_ALLOC_SIZE__(1)\nvoid *\npvalloc(size_t size)\n{\n\tif (unlikely(Destructed))\n\t\treturn NULL;\n\n\tASSERTne(Pagesize, 0);\n\tif (Vmp == NULL) {\n\t\tASSERT(size <= HUGE);\n\t\treturn je_vmem_aligned_alloc(Pagesize, roundup(size, Pagesize));\n\t}\n\tLOG(4, \"size %zu\", size);\n\treturn je_vmem_pool_aligned_alloc(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\tPagesize, roundup(size, Pagesize));\n}\n\n/*\n * malloc_usable_size -- get usable size of allocation\n */\nsize_t\nmalloc_usable_size(void *ptr)\n{\n\tif (unlikely(Destructed))\n\t\treturn 0;\n\n\tif (Vmp == NULL) {\n\t\treturn je_vmem_malloc_usable_size(ptr);\n\t}\n\tLOG(4, \"ptr %p\", ptr);\n\treturn je_vmem_pool_malloc_usable_size(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size), ptr);\n}\n\n#if (defined(__GLIBC__) && !defined(__UCLIBC__))\n\n#ifndef __MALLOC_HOOK_VOLATILE\n#define __MALLOC_HOOK_VOLATILE\n#endif\n\n/*\n * Interpose malloc hooks in glibc.  Even if the application uses dlopen\n * with RTLD_DEEPBIND flag, all the references to libc's malloc(3) functions\n * will be redirected to libvmmalloc.\n */\nvoid *(*__MALLOC_HOOK_VOLATILE __malloc_hook) (size_t size,\n\tconst void *caller) = (void *)malloc;\nvoid *(*__MALLOC_HOOK_VOLATILE __realloc_hook) (void *ptr, size_t size,\n\tconst void *caller) = (void *)realloc;\nvoid (*__MALLOC_HOOK_VOLATILE __free_hook) (void *ptr, const void *caller) =\n\t(void *)free;\nvoid *(*__MALLOC_HOOK_VOLATILE __memalign_hook) (size_t size, size_t alignment,\n\tconst void *caller) = (void *)memalign;\n#endif\n\n/*\n * print_jemalloc_messages -- (internal) custom print function, for jemalloc\n *\n * Prints traces from jemalloc. All traces from jemalloc\n * are considered as error messages.\n */\nstatic void\nprint_jemalloc_messages(void *ignore, const char *s)\n{\n\tLOG_NONL(1, \"%s\", s);\n}\n\n/*\n * print_jemalloc_stats -- (internal) print function for jemalloc statistics\n */\nstatic void\nprint_jemalloc_stats(void *ignore, const char *s)\n{\n\tLOG_NONL(0, \"%s\", s);\n}\n\n/*\n * libvmmalloc_create -- (internal) create a memory pool in a temp file\n */\nstatic VMEM *\nlibvmmalloc_create(const char *dir, size_t size)\n{\n\tLOG(3, \"dir \\\"%s\\\" size %zu\", dir, size);\n\n\tif (size < VMMALLOC_MIN_POOL) {\n\t\tLOG(1, \"size %zu smaller than %zu\", size, VMMALLOC_MIN_POOL);\n\t\terrno = EINVAL;\n\t\treturn NULL;\n\t}\n\n\t/* silently enforce multiple of page size */\n\tsize = roundup(size, Pagesize);\n\n\tFd = util_tmpfile(dir, \"/vmem.XXXXXX\", O_EXCL);\n\tif (Fd == -1)\n\t\treturn NULL;\n\n\tif ((errno = os_posix_fallocate(Fd, 0, (os_off_t)size)) != 0) {\n\t\tERR(\"!posix_fallocate\");\n\t\t(void) os_close(Fd);\n\t\treturn NULL;\n\t}\n\n\tvoid *addr;\n\tif ((addr = util_map(Fd, size, MAP_SHARED, 0, 4 << 20, NULL)) == NULL) {\n\t\t(void) os_close(Fd);\n\t\treturn NULL;\n\t}\n\n\t/* store opaque info at beginning of mapped area */\n\tstruct vmem *vmp = addr;\n\tmemset(&vmp->hdr, '\\0', sizeof(vmp->hdr));\n\tmemcpy(vmp->hdr.signature, VMEM_HDR_SIG, POOL_HDR_SIG_LEN);\n\tvmp->addr = addr;\n\tvmp->size = size;\n\tvmp->caller_mapped = 0;\n\n\t/* Prepare pool for jemalloc */\n\tif (je_vmem_pool_create((void *)((uintptr_t)addr + Header_size),\n\t\t\tsize - Header_size, 1 /* zeroed */,\n\t\t\t1 /* empty */) == NULL) {\n\t\tLOG(1, \"vmem pool creation failed\");\n\t\tutil_unmap(vmp->addr, vmp->size);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * If possible, turn off all permissions on the pool header page.\n\t *\n\t * The prototype PMFS doesn't allow this when large pages are in\n\t * use. It is not considered an error if this fails.\n\t */\n\tutil_range_none(addr, sizeof(struct pool_hdr));\n\n\tLOG(3, \"vmp %p\", vmp);\n\treturn vmp;\n}\n\n/*\n * libvmmalloc_clone - (internal) clone the entire pool\n */\nstatic int\nlibvmmalloc_clone(void)\n{\n\tLOG(3, NULL);\n\tint err;\n\tFd_clone = util_tmpfile(Dir, \"/vmem.XXXXXX\", O_EXCL);\n\tif (Fd_clone == -1)\n\t\treturn -1;\n\n\terr = os_posix_fallocate(Fd_clone, 0, (os_off_t)Vmp->size);\n\tif (err != 0) {\n\t\terrno = err;\n\t\tERR(\"!posix_fallocate\");\n\t\tgoto err_close;\n\t}\n\n\tvoid *addr = mmap(NULL, Vmp->size, PROT_READ|PROT_WRITE,\n\t\t\tMAP_SHARED, Fd_clone, 0);\n\tif (addr == MAP_FAILED) {\n\t\tLOG(1, \"!mmap\");\n\t\tgoto err_close;\n\t}\n\n\tLOG(3, \"copy the entire pool file: dst %p src %p size %zu\",\n\t\t\taddr, Vmp->addr, Vmp->size);\n\n\tutil_range_rw(Vmp->addr, sizeof(struct pool_hdr));\n\n\t/*\n\t * Part of vmem pool was probably freed at some point, so Valgrind\n\t * marked it as undefined/inaccessible. We need to duplicate the whole\n\t * pool, so as a workaround temporarily disable error reporting.\n\t */\n\tVALGRIND_DO_DISABLE_ERROR_REPORTING;\n\tmemcpy(addr, Vmp->addr, Vmp->size);\n\tVALGRIND_DO_ENABLE_ERROR_REPORTING;\n\n\tif (munmap(addr, Vmp->size)) {\n\t\tERR(\"!munmap\");\n\t\tgoto err_close;\n\t}\n\tutil_range_none(Vmp->addr, sizeof(struct pool_hdr));\n\treturn 0;\n\nerr_close:\n\t(void) os_close(Fd_clone);\n\treturn -1;\n}\n\n/*\n * remap_as_private -- (internal) remap the pool as private\n */\nstatic void\nremap_as_private(void)\n{\n\tLOG(3, \"remap the pool file as private\");\n\n\tvoid *r = mmap(Vmp->addr, Vmp->size, PROT_READ|PROT_WRITE,\n\t\t\tMAP_PRIVATE|MAP_FIXED, Fd, 0);\n\n\tif (r == MAP_FAILED) {\n\t\tout_log(NULL, 0, NULL, 0,\n\t\t\t\"Error (libvmmalloc): remapping failed\\n\");\n\t\tabort();\n\t}\n\n\tif (r != Vmp->addr) {\n\t\tout_log(NULL, 0, NULL, 0,\n\t\t\t\"Error (libvmmalloc): wrong address\\n\");\n\t\tabort();\n\t}\n\n\tPrivate = 1;\n}\n\n/*\n * libvmmalloc_prefork -- (internal) prepare for fork()\n *\n * Clones the entire pool or remaps it with MAP_PRIVATE flag.\n */\nstatic void\nlibvmmalloc_prefork(void)\n{\n\tLOG(3, NULL);\n\n\t/*\n\t * There's no need to grab any locks here, as jemalloc pre-fork handler\n\t * is executed first, and it does all the synchronization.\n\t */\n\n\tASSERTne(Vmp, NULL);\n\tASSERTne(Dir, NULL);\n\n\tif (Private) {\n\t\tLOG(3, \"already mapped as private - do nothing\");\n\t\treturn;\n\t}\n\n\tswitch (Forkopt) {\n\tcase 3:\n\t\t/* clone the entire pool; if it fails - remap it as private */\n\t\tLOG(3, \"clone or remap\");\n\n\tcase 2:\n\t\tLOG(3, \"clone the entire pool file\");\n\n\t\tif (libvmmalloc_clone() == 0)\n\t\t\tbreak;\n\n\t\tif (Forkopt == 2) {\n\t\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\t\"pool cloning failed\\n\");\n\t\t\tabort();\n\t\t}\n\t\t/* cloning failed; fall-thru to remapping */\n\n\tcase 1:\n\t\tremap_as_private();\n\t\tbreak;\n\n\tcase 0:\n\t\tLOG(3, \"do nothing\");\n\t\tbreak;\n\n\tdefault:\n\t\tFATAL(\"invalid fork action %d\", Forkopt);\n\t}\n}\n\n/*\n * libvmmalloc_postfork_parent -- (internal) parent post-fork handler\n */\nstatic void\nlibvmmalloc_postfork_parent(void)\n{\n\tLOG(3, NULL);\n\n\tif (Forkopt == 0) {\n\t\t/* do nothing */\n\t\treturn;\n\t}\n\n\tif (Private) {\n\t\tLOG(3, \"pool mapped as private - do nothing\");\n\t} else {\n\t\tLOG(3, \"close the cloned pool file\");\n\t\t(void) os_close(Fd_clone);\n\t}\n}\n\n/*\n * libvmmalloc_postfork_child -- (internal) child post-fork handler\n */\nstatic void\nlibvmmalloc_postfork_child(void)\n{\n\tLOG(3, NULL);\n\n\tif (Forkopt == 0) {\n\t\t/* do nothing */\n\t\treturn;\n\t}\n\n\tif (Private) {\n\t\tLOG(3, \"pool mapped as private - do nothing\");\n\t} else {\n\t\tLOG(3, \"close the original pool file\");\n\t\t(void) os_close(Fd);\n\t\tFd = Fd_clone;\n\n\t\tvoid *addr = Vmp->addr;\n\t\tsize_t size = Vmp->size;\n\n\t\tLOG(3, \"mapping cloned pool file at %p\", addr);\n\t\tVmp = mmap(addr, size, PROT_READ|PROT_WRITE,\n\t\t\t\tMAP_SHARED|MAP_FIXED, Fd, 0);\n\t\tif (Vmp == MAP_FAILED) {\n\t\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\t\"mapping failed\\n\");\n\t\t\tabort();\n\t\t}\n\n\t\tif (Vmp != addr) {\n\t\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\t\"wrong address\\n\");\n\t\t\tabort();\n\t\t}\n\t}\n\n\t/* XXX - open a new log file, with the new PID in the name */\n}\n\n/*\n * libvmmalloc_init -- load-time initialization for libvmmalloc\n *\n * Called automatically by the run-time loader.\n * The constructor priority guarantees this is executed before\n * libjemalloc constructor.\n */\n__attribute__((constructor(101)))\nstatic void\nlibvmmalloc_init(void)\n{\n\tchar *env_str;\n\tsize_t size;\n\n\t/*\n\t * Register fork handlers before jemalloc initialization.\n\t * This provides the correct order of fork handlers execution.\n\t * Note that the first malloc() will trigger jemalloc init, so we\n\t * have to register fork handlers before the call to out_init(),\n\t * as it may indirectly call malloc() when opening the log file.\n\t */\n\tif (os_thread_atfork(libvmmalloc_prefork,\n\t\t\tlibvmmalloc_postfork_parent,\n\t\t\tlibvmmalloc_postfork_child) != 0) {\n\t\tperror(\"Error (libvmmalloc): os_thread_atfork\");\n\t\tabort();\n\t}\n\n\tcommon_init(VMMALLOC_LOG_PREFIX, VMMALLOC_LOG_LEVEL_VAR,\n\t\t\tVMMALLOC_LOG_FILE_VAR, VMMALLOC_MAJOR_VERSION,\n\t\t\tVMMALLOC_MINOR_VERSION);\n\tout_set_vsnprintf_func(je_vmem_navsnprintf);\n\tLOG(3, NULL);\n\n\t/* set up jemalloc messages to a custom print function */\n\tje_vmem_malloc_message = print_jemalloc_messages;\n\n\tHeader_size = roundup(sizeof(VMEM), Pagesize);\n\n\tif ((Dir = os_getenv(VMMALLOC_POOL_DIR_VAR)) == NULL) {\n\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\"environment variable %s not specified\",\n\t\t\t\tVMMALLOC_POOL_DIR_VAR);\n\t\tabort();\n\t}\n\n\tif ((env_str = os_getenv(VMMALLOC_POOL_SIZE_VAR)) == NULL) {\n\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\"environment variable %s not specified\",\n\t\t\t\tVMMALLOC_POOL_SIZE_VAR);\n\t\tabort();\n\t} else {\n\t\tlong long v = atoll(env_str);\n\t\tif (v < 0) {\n\t\t\tout_log(NULL, 0, NULL, 0,\n\t\t\t\t\"Error (libvmmalloc): negative %s\",\n\t\t\t\tVMMALLOC_POOL_SIZE_VAR);\n\t\t\tabort();\n\t\t}\n\n\t\tsize = (size_t)v;\n\t}\n\n\tif (size < VMMALLOC_MIN_POOL) {\n\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\"%s value is less than minimum (%zu < %zu)\",\n\t\t\t\tVMMALLOC_POOL_SIZE_VAR, size,\n\t\t\t\tVMMALLOC_MIN_POOL);\n\t\tabort();\n\t}\n\n\tif ((env_str = os_getenv(VMMALLOC_FORK_VAR)) != NULL) {\n\t\tForkopt = atoi(env_str);\n\t\tif (Forkopt < 0 || Forkopt > 3) {\n\t\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\t\"incorrect %s value (%d)\",\n\t\t\t\t\tVMMALLOC_FORK_VAR, Forkopt);\n\t\t\t\tabort();\n\t\t}\n#ifdef __FreeBSD__\n\t\tif (Forkopt > 1) {\n\t\t\tout_log(NULL, 0, NULL, 0, \"Error (libvmmalloc): \"\n\t\t\t\t\t\"%s value %d not supported on FreeBSD\",\n\t\t\t\t\tVMMALLOC_FORK_VAR, Forkopt);\n\t\t\t\tabort();\n\t\t}\n#endif\n\t\tLOG(4, \"Fork action %d\", Forkopt);\n\t}\n\n\t/*\n\t * XXX - vmem_create() could be used here, but then we need to\n\t * link vmem.o, including all the vmem API.\n\t */\n\tVmp = libvmmalloc_create(Dir, size);\n\tif (Vmp == NULL) {\n\t\tout_log(NULL, 0, NULL, 0, \"!Error (libvmmalloc): \"\n\t\t\t\t\"vmem pool creation failed\");\n\t\tabort();\n\t}\n\n\tLOG(2, \"initialization completed\");\n}\n\n/*\n * libvmmalloc_fini -- libvmmalloc cleanup routine\n *\n * Called automatically when the process terminates and prints\n * some basic allocator statistics.\n */\n__attribute__((destructor(102)))\nstatic void\nlibvmmalloc_fini(void)\n{\n\tLOG(3, NULL);\n\n\tchar *env_str = os_getenv(VMMALLOC_LOG_STATS_VAR);\n\tif ((env_str != NULL) && strcmp(env_str, \"1\") == 0) {\n\t\tLOG_NONL(0, \"\\n=========   system heap  ========\\n\");\n\t\tje_vmem_malloc_stats_print(\n\t\t\tprint_jemalloc_stats, NULL, \"gba\");\n\n\t\tLOG_NONL(0, \"\\n=========    vmem pool   ========\\n\");\n\t\tje_vmem_pool_malloc_stats_print(\n\t\t\t(pool_t *)((uintptr_t)Vmp + Header_size),\n\t\t\tprint_jemalloc_stats, NULL, \"gba\");\n\t}\n\n\tcommon_fini();\n\tDestructed = true;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/ld.supp": "{\n   <insert_a_suppression_name_here>\n   Memcheck:Cond\n   fun:index\n   fun:expand_dynamic_string_token\n   fun:_dl_map_object\n   fun:map_doit\n   fun:_dl_catch_error\n   fun:do_preload\n   fun:dl_main\n   fun:_dl_sysdep_start\n   fun:_dl_start\n   obj:/lib/x86_64-linux-gnu/ld-2.*.so\n   obj:*\n   obj:*\n}\n{\n   <insert_a_suppression_name_here>\n   Memcheck:Cond\n   fun:index\n   fun:expand_dynamic_string_token\n   fun:_dl_map_object\n   fun:map_doit\n   fun:_dl_catch_error\n   fun:do_preload\n   fun:handle_ld_preload\n   fun:dl_main\n   fun:_dl_sysdep_start\n   fun:_dl_start\n   obj:/lib/x86_64-linux-gnu/ld-2.*.so\n   obj:*\n}\n{\n   <Leak in dlopen, pmem/issues#858>\n   Memcheck:Leak\n   ...\n   fun:_dl_init\n   fun:dl_open_worker\n   fun:_dl_catch_error\n   ...\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/memcheck-dlopen.supp": "{\n   dlopen suppression\n   Memcheck:Leak\n   fun:malloc\n   fun:strdup\n   ...\n   fun:call_init\n   fun:_dl_init\n   fun:dl_open_worker\n   fun:_dl_catch_exception\n   fun:_dl_open\n   ...\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/vmmalloc_init/vmmalloc_init.c": "/*\n * Copyright 2014-2017, Intel Corporation\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in\n *       the documentation and/or other materials provided with the\n *       distribution.\n *\n *     * Neither the name of the copyright holder nor the names of its\n *       contributors may be used to endorse or promote products derived\n *       from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/*\n * vmmalloc_init.c -- unit test for libvmmalloc initialization\n *\n * usage: vmmalloc_init [d|l]\n */\n\n#include <stdlib.h>\n#include <dlfcn.h>\n#include \"unittest.h\"\n\nstatic void *(*Falloc)(size_t size, int val);\n\nint\nmain(int argc, char *argv[])\n{\n\tvoid *handle = NULL;\n\tvoid *ptr;\n\n\tSTART(argc, argv, \"vmmalloc_init\");\n\n\tif (argc > 2)\n\t\tUT_FATAL(\"usage: %s [d|l]\", argv[0]);\n\n\tif (argc == 2) {\n\t\tswitch (argv[1][0]) {\n\t\tcase 'd':\n\t\t\tUT_OUT(\"deep binding\");\n\t\t\thandle = dlopen(\"./libtest.so\",\n\t\t\t\tRTLD_NOW | RTLD_LOCAL | RTLD_DEEPBIND);\n\t\t\tbreak;\n\t\tcase 'l':\n\t\t\tUT_OUT(\"lazy binding\");\n\t\t\thandle = dlopen(\"./libtest.so\", RTLD_LAZY);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tUT_FATAL(\"usage: %s [d|l]\", argv[0]);\n\t\t}\n\n\t\tif (handle == NULL)\n\t\t\tUT_OUT(\"dlopen: %s\", dlerror());\n\t\tUT_ASSERTne(handle, NULL);\n\n\t\tFalloc = dlsym(handle, \"falloc\");\n\t\tUT_ASSERTne(Falloc, NULL);\n\t}\n\n\tptr = malloc(4321);\n\tfree(ptr);\n\n\tif (argc == 2) {\n\t\t/*\n\t\t * NOTE: falloc calls malloc internally.\n\t\t * If libtest is loaded with RTLD_DEEPBIND flag, then it will\n\t\t * use its own lookup scope in preference to global symbols\n\t\t * from already loaded (LD_PRELOAD) libvmmalloc.  So, falloc\n\t\t * will call the stock libc's malloc.\n\t\t * However, since we override the malloc hooks, a call to libc's\n\t\t * malloc will be redirected to libvmmalloc anyway, and the\n\t\t * memory can be safely reclaimed using libvmmalloc's free.\n\t\t */\n\t\tptr = Falloc(4321, 0xaa);\n\t\tfree(ptr);\n\t}\n\n\tif (handle != NULL)\n\t\tdlclose(handle);\n\n\tDONE(NULL);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/unittest/unittest.sh": "#\n# Copyright 2014-2018, Intel Corporation\n# Copyright (c) 2016, Microsoft Corporation. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in\n#       the documentation and/or other materials provided with the\n#       distribution.\n#\n#     * Neither the name of the copyright holder nor the names of its\n#       contributors may be used to endorse or promote products derived\n#       from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\nset -e\n\n# make sure we have a well defined locale for string operations here\nexport LC_ALL=\"C\"\n#export LC_ALL=\"en_US.UTF-8\"\n\n. ../testconfig.sh\n\nfunction verbose_msg() {\n\tif [ \"$UNITTEST_LOG_LEVEL\" -ge 2 ]; then\n\t\techo \"$*\"\n\tfi\n}\n\nfunction msg() {\n\tif [ \"$UNITTEST_LOG_LEVEL\" -ge 1 ]; then\n\t\techo \"$*\"\n\tfi\n}\n\nfunction fatal() {\n\techo \"$*\" >&2\n\texit 1\n}\n\nif [ -z \"${UNITTEST_NAME}\" ]; then\n\tCURDIR=$(basename $(pwd))\n\tSCRIPTNAME=$(basename $0)\n\n\texport UNITTEST_NAME=$CURDIR/$SCRIPTNAME\n\texport UNITTEST_NUM=$(echo $SCRIPTNAME | sed \"s/TEST//\")\nfi\n\n# defaults\n[ \"$UNITTEST_LOG_LEVEL\" ] || UNITTEST_LOG_LEVEL=2\n[ \"$GREP\" ] || GREP=\"grep -a\"\n[ \"$TEST\" ] || TEST=check\n[ \"$FS\" ] || FS=any\n[ \"$BUILD\" ] || BUILD=debug\n[ \"$CHECK_TYPE\" ] || CHECK_TYPE=auto\n[ \"$CHECK_POOL\" ] || CHECK_POOL=0\n[ \"$VERBOSE\" ] || VERBOSE=0\n[ -n \"${SUFFIX+x}\" ] || SUFFIX=\"\ud83d\ude18\u281d\u2827\u280d\u2807\u0257PMDK\u04dc\u297a\ud83d\ude4b\"\n\nexport UNITTEST_LOG_LEVEL GREP TEST FS BUILD CHECK_TYPE CHECK_POOL VERBOSE SUFFIX\n\nVMMALLOC=libvmmalloc.so.1\nTOOLS=../tools\nLIB_TOOLS=\"../../tools\"\n# Paths to some useful tools\n[ \"$PMEMPOOL\" ] || PMEMPOOL=$LIB_TOOLS/pmempool/pmempool\n[ \"$DAXIO\" ] || DAXIO=$LIB_TOOLS/daxio/daxio\n[ \"$PMEMSPOIL\" ] || PMEMSPOIL=$TOOLS/pmemspoil/pmemspoil.static-nondebug\n[ \"$BTTCREATE\" ] || BTTCREATE=$TOOLS/bttcreate/bttcreate.static-nondebug\n[ \"$PMEMWRITE\" ] || PMEMWRITE=$TOOLS/pmemwrite/pmemwrite\n[ \"$PMEMALLOC\" ] || PMEMALLOC=$TOOLS/pmemalloc/pmemalloc\n[ \"$PMEMOBJCLI\" ] || PMEMOBJCLI=$TOOLS/pmemobjcli/pmemobjcli\n[ \"$PMEMDETECT\" ] || PMEMDETECT=$TOOLS/pmemdetect/pmemdetect.static-nondebug\n[ \"$PMREORDER\" ] || PMREORDER=$LIB_TOOLS/pmreorder/pmreorder.py\n[ \"$FIP\" ] || FIP=$TOOLS/fip/fip\n[ \"$DDMAP\" ] || DDMAP=$TOOLS/ddmap/ddmap\n[ \"$CMPMAP\" ] || CMPMAP=$TOOLS/cmpmap/cmpmap\n[ \"$EXTENTS\" ] || EXTENTS=$TOOLS/extents/extents\n[ \"$FALLOCATE_DETECT\" ] || FALLOCATE_DETECT=$TOOLS/fallocate_detect/fallocate_detect.static-nondebug\n[ \"$OBJ_VERIFY\" ] || OBJ_VERIFY=$TOOLS/obj_verify/obj_verify\n[ \"$USC_PERMISSION\" ] || USC_PERMISSION=$TOOLS/usc_permission_check/usc_permission_check.static-nondebug\n\n# force globs to fail if they don't match\nshopt -s failglob\n\n# number of remote nodes required in the current unit test\nNODES_MAX=-1\n\n# sizes of aligments\nSIZE_4KB=4096\nSIZE_2MB=2097152\n\n# PMEMOBJ limitations\nPMEMOBJ_MAX_ALLOC_SIZE=17177771968\n\n# SSH and SCP options\nSSH_OPTS=\"-o BatchMode=yes\"\nSCP_OPTS=\"-o BatchMode=yes -r -p\"\n\n# list of common files to be copied to all remote nodes\nDIR_SRC=\"../..\"\nFILES_COMMON_DIR=\"\\\n$DIR_SRC/test/*.supp \\\n$DIR_SRC/tools/rpmemd/rpmemd \\\n$DIR_SRC/tools/pmempool/pmempool \\\n$DIR_SRC/test/tools/extents/extents \\\n$DIR_SRC/test/tools/obj_verify/obj_verify \\\n$DIR_SRC/test/tools/ctrld/ctrld \\\n$DIR_SRC/test/tools/fip/fip\"\n\n# Portability\nVALGRIND_SUPP=\"--suppressions=../ld.supp --suppressions=../memcheck-libunwind.supp\"\nif [ \"$(uname -s)\" = \"FreeBSD\" ]; then\n\tDATE=\"gdate\"\n\tDD=\"gdd\"\n\tFALLOCATE=\"mkfile\"\n\tVM_OVERCOMMIT=\"[ $(sysctl vm.overcommit | awk '{print $2}') == 0 ]\"\n\tRM_ONEFS=\"-x\"\n\tSTAT_MODE=\"-f%Lp\"\n\tSTAT_PERM=\"-f%Sp\"\n\tSTAT_SIZE=\"-f%z\"\n\tSTRACE=\"truss\"\n\tVALGRIND_SUPP=\"$VALGRIND_SUPP --suppressions=../freebsd.supp\"\nelse\n\tDATE=\"date\"\n\tDD=\"dd\"\n\tFALLOCATE=\"fallocate -l\"\n\tVM_OVERCOMMIT=\"[ $(cat /proc/sys/vm/overcommit_memory) != 2 ]\"\n\tRM_ONEFS=\"--one-file-system\"\n\tSTAT_MODE=\"-c%a\"\n\tSTAT_PERM=\"-c%A\"\n\tSTAT_SIZE=\"-c%s\"\n\tSTRACE=\"strace\"\nfi\n\n# array of lists of PID files to be cleaned in case of an error\nNODE_PID_FILES[0]=\"\"\n\n#\n# The variable TEST_LD_LIBRARY_PATH is constructed so the test pulls in\n# the appropriate library from this source tree.  To override this behavior\n# (i.e. to force the test to use the libraries installed elsewhere on\n# the system), set TEST_LD_LIBRARY_PATH and this script will not override it.\n#\n# For example, in a test directory, run:\n#\tTEST_LD_LIBRARY_PATH=/usr/lib ./TEST0\n#\n[ \"$TEST_LD_LIBRARY_PATH\" ] || {\n\tcase \"$BUILD\"\n\tin\n\tdebug|static-debug)\n\t\tif [ -z \"$PMDK_LIB_PATH_DEBUG\" ]; then\n\t\t\tTEST_LD_LIBRARY_PATH=../../debug\n\t\t\tREMOTE_LD_LIBRARY_PATH=../debug\n\t\telse\n\t\t\tTEST_LD_LIBRARY_PATH=$PMDK_LIB_PATH_DEBUG\n\t\t\tREMOTE_LD_LIBRARY_PATH=$PMDK_LIB_PATH_DEBUG\n\t\tfi\n\t\t;;\n\tnondebug|static-nondebug)\n\t\tif [ -z \"$PMDK_LIB_PATH_NONDEBUG\" ]; then\n\t\t\tTEST_LD_LIBRARY_PATH=../../nondebug\n\t\t\tREMOTE_LD_LIBRARY_PATH=../nondebug\n\t\telse\n\t\t\tTEST_LD_LIBRARY_PATH=$PMDK_LIB_PATH_NONDEBUG\n\t\t\tREMOTE_LD_LIBRARY_PATH=$PMDK_LIB_PATH_NONDEBUG\n\t\tfi\n\t\t;;\n\tesac\n}\n\n#\n# When running static binary tests, append the build type to the binary\n#\ncase \"$BUILD\"\nin\n\tstatic-*)\n\t\tEXESUFFIX=.$BUILD\n\t\t;;\nesac\n\n#\n# The variable DIR is constructed so the test uses that directory when\n# constructing test files.  DIR is chosen based on the fs-type for\n# this test, and if the appropriate fs-type doesn't have a directory\n# defined in testconfig.sh, the test is skipped.\n#\n# This behavior can be overridden by setting DIR.  For example:\n#\tDIR=/force/test/dir ./TEST0\n#\ncurtestdir=`basename $PWD`\n\n# just in case\nif [ ! \"$curtestdir\" ]; then\n\tfatal \"curtestdir does not have a value\"\nfi\n\ncurtestdir=test_$curtestdir\n\nif [ ! \"$UNITTEST_NUM\" ]; then\n\tfatal \"UNITTEST_NUM does not have a value\"\nfi\n\nif [ ! \"$UNITTEST_NAME\" ]; then\n\tfatal \"UNITTEST_NAME does not have a value\"\nfi\n\nREAL_FS=$FS\nif [ \"$DIR\" ]; then\n\tDIR=$DIR/$curtestdir$UNITTEST_NUM\nelse\n\tcase \"$FS\"\n\tin\n\tpmem)\n\t\t# if a variable is set - it must point to a valid directory\n\t\tif [ \"$PMEM_FS_DIR\" == \"\" ]; then\n\t\t\tfatal \"$UNITTEST_NAME: PMEM_FS_DIR is not set\"\n\t\tfi\n\t\tDIR=$PMEM_FS_DIR/$DIRSUFFIX/$curtestdir$UNITTEST_NUM\n\t\tif [ \"$PMEM_FS_DIR_FORCE_PMEM\" = \"1\" ] || [ \"$PMEM_FS_DIR_FORCE_PMEM\" = \"2\" ]; then\n\t\t\texport PMEM_IS_PMEM_FORCE=1\n\t\tfi\n\t\t;;\n\tnon-pmem)\n\t\t# if a variable is set - it must point to a valid directory\n\t\tif [ \"$NON_PMEM_FS_DIR\" == \"\" ]; then\n\t\t\tfatal \"$UNITTEST_NAME: NON_PMEM_FS_DIR is not set\"\n\t\tfi\n\t\tDIR=$NON_PMEM_FS_DIR/$DIRSUFFIX/$curtestdir$UNITTEST_NUM\n\t\t;;\n\tany)\n\t\tif [ \"$PMEM_FS_DIR\" != \"\" ]; then\n\t\t\tDIR=$PMEM_FS_DIR/$DIRSUFFIX/$curtestdir$UNITTEST_NUM\n\t\t\tREAL_FS=pmem\n\t\t\tif [ \"$PMEM_FS_DIR_FORCE_PMEM\" = \"1\" ] || [ \"$PMEM_FS_DIR_FORCE_PMEM\" = \"2\" ]; then\n\t\t\t\texport PMEM_IS_PMEM_FORCE=1\n\t\t\tfi\n\t\telif [ \"$NON_PMEM_FS_DIR\" != \"\" ]; then\n\t\t\tDIR=$NON_PMEM_FS_DIR/$DIRSUFFIX/$curtestdir$UNITTEST_NUM\n\t\t\tREAL_FS=non-pmem\n\t\telse\n\t\t\tfatal \"$UNITTEST_NAME: fs-type=any and both env vars are empty\"\n\t\tfi\n\t\t;;\n\tnone)\n\t\tDIR=/dev/null/not_existing_dir/$DIRSUFFIX/$curtestdir$UNITTEST_NUM\n\t\t;;\n\t*)\n\t\tverbose_msg \"$UNITTEST_NAME: SKIP fs-type $FS (not configured)\"\n\t\texit 0\n\t\t;;\n\tesac\nfi\n\n#\n# The default is to turn on library logging to level 3 and save it to local files.\n# Tests that don't want it on, should override these environment variables.\n#\nexport VMEM_LOG_LEVEL=3\nexport VMEM_LOG_FILE=vmem$UNITTEST_NUM.log\nexport PMEM_LOG_LEVEL=3\nexport PMEM_LOG_FILE=pmem$UNITTEST_NUM.log\nexport PMEMBLK_LOG_LEVEL=3\nexport PMEMBLK_LOG_FILE=pmemblk$UNITTEST_NUM.log\nexport PMEMLOG_LOG_LEVEL=3\nexport PMEMLOG_LOG_FILE=pmemlog$UNITTEST_NUM.log\nexport PMEMOBJ_LOG_LEVEL=3\nexport PMEMOBJ_LOG_FILE=pmemobj$UNITTEST_NUM.log\nexport PMEMPOOL_LOG_LEVEL=3\nexport PMEMPOOL_LOG_FILE=pmempool$UNITTEST_NUM.log\nexport PMREORDER_LOG_FILE=pmreorder$UNITTEST_NUM.log\n\nexport VMMALLOC_POOL_SIZE=$((16 * 1024 * 1024))\nexport VMMALLOC_LOG_LEVEL=3\nexport VMMALLOC_LOG_FILE=vmmalloc$UNITTEST_NUM.log\n\nexport OUT_LOG_FILE=out$UNITTEST_NUM.log\nexport ERR_LOG_FILE=err$UNITTEST_NUM.log\nexport TRACE_LOG_FILE=trace$UNITTEST_NUM.log\nexport PREP_LOG_FILE=prep$UNITTEST_NUM.log\n\nexport VALGRIND_LOG_FILE=${CHECK_TYPE}${UNITTEST_NUM}.log\nexport VALIDATE_VALGRIND_LOG=1\n\nexport RPMEM_LOG_LEVEL=3\nexport RPMEM_LOG_FILE=rpmem$UNITTEST_NUM.log\nexport RPMEMD_LOG_LEVEL=info\nexport RPMEMD_LOG_FILE=rpmemd$UNITTEST_NUM.log\n\nexport REMOTE_VARS=\"\nRPMEMD_LOG_FILE\nRPMEMD_LOG_LEVEL\nRPMEM_LOG_FILE\nRPMEM_LOG_LEVEL\nPMEM_LOG_FILE\nPMEM_LOG_LEVEL\nPMEMOBJ_LOG_FILE\nPMEMOBJ_LOG_LEVEL\nPMEMPOOL_LOG_FILE\nPMEMPOOL_LOG_LEVEL\"\n\n[ \"$UT_DUMP_LINES\" ] || UT_DUMP_LINES=30\n\nexport CHECK_POOL_LOG_FILE=check_pool_${BUILD}_${UNITTEST_NUM}.log\n\n# In case a lock is required for Device DAXes\nDEVDAX_LOCK=../devdax.lock\n\n#\n# store_exit_on_error -- store on a stack a sign that reflects the current state\n#                        of the 'errexit' shell option\n#\nfunction store_exit_on_error() {\n\tif [ \"${-#*e}\" != \"$-\" ]; then\n\t\testack+=-\n\telse\n\t\testack+=+\n\tfi\n}\n\n#\n# restore_exit_on_error -- restore the state of the 'errexit' shell option\n#\nfunction restore_exit_on_error() {\n\tif [ -z $estack ]; then\n\t\tfatal \"error: store_exit_on_error function has to be called first\"\n\tfi\n\n\teval \"set ${estack:${#estack}-1:1}e\"\n\testack=${estack%?}\n}\n\n#\n# disable_exit_on_error -- store the state of the 'errexit' shell option and\n#                          disable it\n#\nfunction disable_exit_on_error() {\n\tstore_exit_on_error\n\tset +e\n}\n\n\n#\n# get_files -- print list of files in the current directory matching the given regex to stdout\n#\n# This function has been implemented to workaround a race condition in\n# `find`, which fails if any file disappears in the middle of the operation.\n#\n# example, to list all *.log files in the current directory\n#\tget_files \".*\\.log\"\nfunction get_files() {\n\tdisable_exit_on_error\n\tls -1 | grep -E \"^$*$\"\n\trestore_exit_on_error\n}\n\n#\n# get_executables -- print list of executable files in the current directory to stdout\n#\n# This function has been implemented to workaround a race condition in\n# `find`, which fails if any file disappears in the middle of the operation.\n#\nfunction get_executables() {\n\tdisable_exit_on_error\n\tfor c in *\n\tdo\n\t\tif [ -f $c -a -x $c ]\n\t\tthen\n\t\t\techo \"$c\"\n\t\tfi\n\tdone\n\trestore_exit_on_error\n}\n\n#\n# convert_to_bytes -- converts the string with K, M, G or T suffixes\n# to bytes\n#\n# example:\n#   \"1G\" --> \"1073741824\"\n#   \"2T\" --> \"2199023255552\"\n#   \"3k\" --> \"3072\"\n#   \"1K\" --> \"1024\"\n#   \"10\" --> \"10\"\n#\nfunction convert_to_bytes() {\n\tsize=\"$(echo $1 | tr '[:upper:]' '[:lower:]')\"\n\tif [[ $size == *kib ]]\n\tthen size=$(($(echo $size | tr -d 'kib') * 1024))\n\telif [[ $size == *mib ]]\n\tthen size=$(($(echo $size | tr -d 'mib') * 1024 * 1024))\n\telif [[ $size == *gib ]]\n\tthen size=$(($(echo $size | tr -d 'gib') * 1024 * 1024 * 1024))\n\telif [[ $size == *tib ]]\n\tthen size=$(($(echo $size | tr -d 'tib') * 1024 * 1024 * 1024 * 1024))\n\telif [[ $size == *pib ]]\n\tthen size=$(($(echo $size | tr -d 'pib') * 1024 * 1024 * 1024 * 1024 * 1024))\n\telif [[ $size == *kb ]]\n\tthen size=$(($(echo $size | tr -d 'kb') * 1000))\n\telif [[ $size == *mb ]]\n\tthen size=$(($(echo $size | tr -d 'mb') * 1000 * 1000))\n\telif [[ $size == *gb ]]\n\tthen size=$(($(echo $size | tr -d 'gb') * 1000 * 1000 * 1000))\n\telif [[ $size == *tb ]]\n\tthen size=$(($(echo $size | tr -d 'tb') * 1000 * 1000 * 1000 * 1000))\n\telif [[ $size == *pb ]]\n\tthen size=$(($(echo $size | tr -d 'pb') * 1000 * 1000 * 1000 * 1000 * 1000))\n\telif [[ $size == *b ]]\n\tthen size=$(($(echo $size | tr -d 'b')))\n\telif [[ $size == *k ]]\n\tthen size=$(($(echo $size | tr -d 'k') * 1024))\n\telif [[ $size == *m ]]\n\tthen size=$(($(echo $size | tr -d 'm') * 1024 * 1024))\n\telif [[ $size == *g ]]\n\tthen size=$(($(echo $size | tr -d 'g') * 1024 * 1024 * 1024))\n\telif [[ $size == *t ]]\n\tthen size=$(($(echo $size | tr -d 't') * 1024 * 1024 * 1024 * 1024))\n\telif [[ $size == *p ]]\n\tthen size=$(($(echo $size | tr -d 'p') * 1024 * 1024 * 1024 * 1024 * 1024))\n\tfi\n\n\techo \"$size\"\n}\n\n#\n# create_file -- create zeroed out files of a given length\n#\n# example, to create two files, each 1GB in size:\n#\tcreate_file 1G testfile1 testfile2\n#\nfunction create_file() {\n\tsize=$(convert_to_bytes $1)\n\tshift\n\tfor file in $*\n\tdo\n\t\t$DD if=/dev/zero of=$file bs=1M count=$size iflag=count_bytes >> $PREP_LOG_FILE\n\tdone\n}\n\n#\n# create_nonzeroed_file -- create non-zeroed files of a given length\n#\n# A given first kilobytes of the file is zeroed out.\n#\n# example, to create two files, each 1GB in size, with first 4K zeroed\n#\tcreate_nonzeroed_file 1G 4K testfile1 testfile2\n#\nfunction create_nonzeroed_file() {\n\toffset=$(convert_to_bytes $2)\n\tsize=$(($(convert_to_bytes $1) - $offset))\n\tshift 2\n\tfor file in $*\n\tdo\n\t\ttruncate -s ${offset} $file >> $PREP_LOG_FILE\n\t\t$DD if=/dev/zero bs=1K count=${size} iflag=count_bytes 2>>$PREP_LOG_FILE | tr '\\0' '\\132' >> $file\n\tdone\n}\n\n#\n# create_holey_file -- create holey files of a given length\n#\n# examples:\n#\tcreate_holey_file 1024k testfile1 testfile2\n#\tcreate_holey_file 2048M testfile1 testfile2\n#\tcreate_holey_file 234 testfile1\n#\tcreate_holey_file 2340b testfile1\n#\n# Input unit size is in bytes with optional suffixes like k, KB, M, etc.\n#\n\nfunction create_holey_file() {\n\tsize=$(convert_to_bytes $1)\n\tshift\n\tfor file in $*\n\tdo\n\t\ttruncate -s ${size} $file >> $PREP_LOG_FILE\n\tdone\n}\n\n#\n# create_poolset -- create a dummy pool set\n#\n# Creates a pool set file using the provided list of part sizes and paths.\n# Optionally, it also creates the selected part files (zeroed, partially zeroed\n# or non-zeroed) with requested size and mode.  The actual file size may be\n# different than the part size in the pool set file.\n# 'r' or 'R' on the list of arguments indicate the beginning of the next\n# replica set and 'm' or 'M' the beginning of the next remote replica set.\n# 'o' or 'O' indicates the next argument is a pool set option.\n# A remote replica requires two parameters: a target node and a pool set\n# descriptor.\n#\n# Each part argument has the following format:\n#   psize:ppath[:cmd[:fsize[:mode]]]\n#\n# where:\n#   psize - part size or AUTO (only for DAX device)\n#   ppath - path\n#   cmd   - (optional) can be:\n#            x - do nothing (may be skipped if there's no 'fsize', 'mode')\n#            z - create zeroed (holey) file\n#            n - create non-zeroed file\n#            h - create non-zeroed file, but with zeroed header (first 4KB)\n#            d - create directory\n#   fsize - (optional) the actual size of the part file (if 'cmd' is not 'x')\n#   mode  - (optional) same format as for 'chmod' command\n#\n# Each remote replica argument has the following format:\n#   node:desc\n#\n# where:\n#   node - target node\n#   desc - pool set descriptor\n#\n# example:\n#   The following command define a pool set consisting of two parts: 16MB\n#   and 32MB, a local replica with only one part of 48MB and a remote replica.\n#   The first part file is not created, the second is zeroed.  The only replica\n#   part is non-zeroed. Also, the last file is read-only and its size\n#   does not match the information from pool set file. The last but one line\n#   describes a remote replica. The SINGLEHDR poolset option is set, so only\n#   the first part in each replica contains a pool header. The remote poolset\n#   also has to have the SINGLEHDR option.\n#\n#\tcreate_poolset ./pool.set 16M:testfile1 32M:testfile2:z \\\n#\t\t\t\tR 48M:testfile3:n:11M:0400 \\\n#\t\t\t\tM remote_node:remote_pool.set \\\n#                               O SINGLEHDR\n#\nfunction create_poolset() {\n\tpsfile=$1\n\tshift 1\n\techo \"PMEMPOOLSET\" > $psfile\n\twhile [ \"$1\" ]\n\tdo\n\t\tif [ \"$1\" = \"M\" ] || [ \"$1\" = \"m\" ] # remote replica\n\t\tthen\n\t\t\tshift 1\n\n\t\t\tcmd=$1\n\t\t\tshift 1\n\n\t\t\t# extract last \":\" separated segment as descriptor\n\t\t\t# extract everything before last \":\" as node address\n\t\t\t# this extraction method is compatible with IPv6 and IPv4\n\t\t\tnode=${cmd%:*}\n\t\t\tdesc=${cmd##*:}\n\n\t\t\techo \"REPLICA $node $desc\" >> $psfile\n\t\t\tcontinue\n\t\tfi\n\n\t\tif [ \"$1\" = \"R\" ] || [ \"$1\" = \"r\" ]\n\t\tthen\n\t\t\techo \"REPLICA\" >> $psfile\n\t\t\tshift 1\n\t\t\tcontinue\n\t\tfi\n\n\t\tif [ \"$1\" = \"O\" ] || [ \"$1\" = \"o\" ]\n\t\tthen\n\t\t\techo \"OPTION $2\" >> $psfile\n\t\t\tshift 2\n\t\t\tcontinue\n\t\tfi\n\n\t\tcmd=$1\n\t\tfparms=(${cmd//:/ })\n\t\tshift 1\n\n\t\tfsize=${fparms[0]}\n\t\tfpath=${fparms[1]}\n\t\tcmd=${fparms[2]}\n\t\tasize=${fparms[3]}\n\t\tmode=${fparms[4]}\n\n\t\tif [ ! $asize ]; then\n\t\t\tasize=$fsize\n\t\tfi\n\n\t\tif [ \"$asize\" != \"AUTO\" ]; then\n\t\t\tasize=$(convert_to_bytes $asize)\n\t\tfi\n\n\t\tcase \"$cmd\"\n\t\tin\n\t\tx)\n\t\t\t# do nothing\n\t\t\t;;\n\t\tz)\n\t\t\t# zeroed (holey) file\n\t\t\ttruncate -s $asize $fpath >> $PREP_LOG_FILE\n\t\t\t;;\n\t\tn)\n\t\t\t# non-zeroed file\n\t\t\t$DD if=/dev/zero bs=$asize count=1 2>>$PREP_LOG_FILE | tr '\\0' '\\132' >> $fpath\n\t\t\t;;\n\t\th)\n\t\t\t# non-zeroed file, except 4K header\n\t\t\ttruncate -s 4K $fpath >> prep$UNITTEST_NUM.log\n\t\t\t$DD if=/dev/zero bs=$asize count=1 2>>$PREP_LOG_FILE | tr '\\0' '\\132' >> $fpath\n\t\t\ttruncate -s $asize $fpath >> $PREP_LOG_FILE\n\t\t\t;;\n\t\td)\n\t\t\tmkdir -p $fpath\n\t\t\t;;\n\t\tesac\n\n\t\tif [ $mode ]; then\n\t\t\tchmod $mode $fpath\n\t\tfi\n\n\t\techo \"$fsize $fpath\" >> $psfile\n\tdone\n}\n\nfunction dump_last_n_lines() {\n\tif [ \"$1\" != \"\" -a -f \"$1\" ]; then\n\t\tln=`wc -l < $1`\n\t\tif [ $ln -gt $UT_DUMP_LINES ]; then\n\t\t\techo -e \"Last $UT_DUMP_LINES lines of $1 below (whole file has $ln lines).\" >&2\n\t\t\tln=$UT_DUMP_LINES\n\t\telse\n\t\t\techo -e \"$1 below.\" >&2\n\t\tfi\n\t\tpaste -d \" \" <(yes $UNITTEST_NAME $1 | head -n $ln) <(tail -n $ln $1) >&2\n\t\techo >&2\n\tfi\n}\n\n# https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=810295\n# https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=780173\n# https://bugs.kde.org/show_bug.cgi?id=303877\n#\n# valgrind issues an unsuppressable warning when exceeding\n# the brk segment, causing matching failures. We can safely\n# ignore it because malloc() will fallback to mmap() anyway.\nfunction valgrind_ignore_warnings() {\n\tcat $1 | grep -v \\\n\t\t-e \"WARNING: Serious error when reading debug info\" \\\n\t\t-e \"When reading debug info from \" \\\n\t\t-e \"Ignoring non-Dwarf2/3/4 block in .debug_info\" \\\n\t\t-e \"Last block truncated in .debug_info; ignoring\" \\\n\t\t-e \"parse_CU_Header: is neither DWARF2 nor DWARF3 nor DWARF4\" \\\n\t\t-e \"brk segment overflow\" \\\n\t\t-e \"see section Limitations in user manual\" \\\n\t\t-e \"Warning: set address range perms: large range\"\\\n\t\t-e \"further instances of this message will not be shown\"\\\n\t\t-e \"get_Form_contents: DW_FORM_GNU_strp_alt used, but no alternate .debug_str\"\\\n\t\t>  $1.tmp\n\tmv $1.tmp $1\n}\n\n#\n# get_trace -- return tracing tool command line if applicable\n#\tusage: get_trace <check type> <log file> [<node>]\n#\nfunction get_trace() {\n\tif [ \"$1\" == \"none\" ]; then\n\t\techo \"$TRACE\"\n\t\treturn\n\tfi\n\tlocal exe=$VALGRINDEXE\n\tlocal check_type=$1\n\tlocal log_file=$2\n\tlocal opts=\"$VALGRIND_OPTS\"\n\tlocal node=-1\n\t[ \"$#\" -eq 3 ] && node=$3\n\n\tif [ \"$check_type\" = \"memcheck\" -a \"$MEMCHECK_DONT_CHECK_LEAKS\" != \"1\" ]; then\n\t\topts=\"$opts --leak-check=full\"\n\tfi\n\tif [ \"$check_type\" = \"pmemcheck\" ]; then\n\t\t# Before Skylake, Intel CPUs did not have clflushopt instruction, so\n\t\t# pmem_flush and pmem_persist both translated to clflush.\n\t\t# This means that missing pmem_drain after pmem_flush could only be\n\t\t# detected on Skylake+ CPUs.\n\t\t# This option tells pmemcheck to expect fence (sfence or\n\t\t# VALGRIND_PMC_DO_FENCE client request, used by pmem_drain) after\n\t\t# clflush and makes pmemcheck output the same on pre-Skylake and\n\t\t# post-Skylake CPUs.\n\t\topts=\"$opts --expect-fence-after-clflush=yes\"\n\tfi\n\n\topts=\"$opts $VALGRIND_SUPP\"\n\tif [ \"$node\" -ne -1 ]; then\n\t\texe=${NODE_VALGRINDEXE[$node]}\n\t\topts=\"$opts\"\n\n\t\tcase \"$check_type\" in\n\t\tmemcheck)\n\t\t\topts=\"$opts --suppressions=../memcheck-libibverbs.supp\"\n\t\t\t;;\n\t\thelgrind)\n\t\t\topts=\"$opts --suppressions=../helgrind-cxgb4.supp\"\n\t\t\topts=\"$opts --suppressions=../helgrind-libfabric.supp\"\n\t\t\t;;\n\t\tdrd)\n\t\t\topts=\"$opts --suppressions=../drd-libfabric.supp\"\n\t\t\t;;\n\t\tesac\n\tfi\n\n\techo \"$exe --tool=$check_type --log-file=$log_file $opts $TRACE\"\n\treturn\n}\n\n#\n# validate_valgrind_log -- validate valgrind log\n#\tusage: validate_valgrind_log <log-file>\n#\nfunction validate_valgrind_log() {\n\t[ \"$VALIDATE_VALGRIND_LOG\" != \"1\" ] && return\n\t# fail if there are valgrind errors found or\n\t# if it detects overlapping chunks\n\tif [ ! -e \"$1.match\" ] && grep \\\n\t\t-e \"ERROR SUMMARY: [^0]\" \\\n\t\t-e \"Bad mempool\" \\\n\t\t$1 >/dev/null ;\n\tthen\n\t\tmsg=\"failed\"\n\t\t[ -t 2 ] && command -v tput >/dev/null && msg=\"$(tput setaf 1)$msg$(tput sgr0)\"\n\t\techo -e \"$UNITTEST_NAME $msg with Valgrind. See $1. Last 20 lines below.\" >&2\n\t\tpaste -d \" \" <(yes $UNITTEST_NAME $1 | head -n 20) <(tail -n 20 $1) >&2\n\t\tfalse\n\tfi\n}\n\n#\n# expect_normal_exit -- run a given command, expect it to exit 0\n#\n# if VALGRIND_DISABLED is not empty valgrind tool will be omitted\n#\nfunction expect_normal_exit() {\n\tlocal VALGRIND_LOG_FILE=${CHECK_TYPE}${UNITTEST_NUM}.log\n\tlocal N=$2\n\n\t# in case of a remote execution disable valgrind check if valgrind is not\n\t# enabled on node\n\tlocal _CHECK_TYPE=$CHECK_TYPE\n\tif [ \"x$VALGRIND_DISABLED\" != \"x\" ]; then\n\t\t_CHECK_TYPE=none\n\tfi\n\tif [ \"$1\" == \"run_on_node\" -o \"$1\" == \"run_on_node_background\" ]; then\n\t\tif [ -z $(is_valgrind_enabled_on_node $N) ]; then\n\t\t\t_CHECK_TYPE=\"none\"\n\t\tfi\n\telse\n\t\tN=-1\n\tfi\n\n\tif [ -n \"$TRACE\" ]; then\n\t\tcase \"$1\"\n\t\tin\n\t\t*_on_node*)\n\t\t\tmsg \"$UNITTEST_NAME: SKIP: TRACE is not supported if test is executed on remote nodes\"\n\t\t\texit 0\n\t\tesac\n\tfi\n\n\tlocal trace=$(get_trace $_CHECK_TYPE $VALGRIND_LOG_FILE $N)\n\n\tif [ \"$MEMCHECK_DONT_CHECK_LEAKS\" = \"1\" -a \"$CHECK_TYPE\" = \"memcheck\" ]; then\n\t\texport OLD_ASAN_OPTIONS=\"${ASAN_OPTIONS}\"\n\t\texport ASAN_OPTIONS=\"detect_leaks=0 ${ASAN_OPTIONS}\"\n\tfi\n\n\tif [ \"$CHECK_TYPE\" = \"helgrind\" ]; then\n\t\texport VALGRIND_OPTS=\"--suppressions=../helgrind-log.supp\"\n\tfi\n\n\tif [ \"$CHECK_TYPE\" = \"memcheck\" ]; then\n\t\texport VALGRIND_OPTS=\"$VALGRIND_OPTS --suppressions=../memcheck-dlopen.supp\"\n\tfi\n\n\t# in case of preloading libvmmalloc.so.1 force valgrind to not override malloc\n\tif [ -n \"$VALGRINDEXE\" -a -n \"$TEST_LD_PRELOAD\" ]; then\n\t\tif [ $(valgrind_version) -ge 312 ]; then\n\t\t\tpreload=`basename $TEST_LD_PRELOAD`\n\t\tfi\n\t\tif [ \"$preload\" == \"$VMMALLOC\" ]; then\n\t\t\texport VALGRIND_OPTS=\"$VALGRIND_OPTS --soname-synonyms=somalloc=nouserintercepts\"\n\t\tfi\n\tfi\n\n\tlocal REMOTE_VALGRIND_LOG=0\n\tif [ \"$CHECK_TYPE\" != \"none\" ]; then\n\t        case \"$1\"\n\t        in\n\t        run_on_node)\n\t\t\tREMOTE_VALGRIND_LOG=1\n\t\t\ttrace=\"$1 $2 $trace\"\n\t\t\t[ $# -ge 2  ] && shift 2 || shift $#\n\t                ;;\n\t        run_on_node_background)\n\t\t\ttrace=\"$1 $2 $3 $trace\"\n\t\t\t[ $# -ge 3  ] && shift 3 || shift $#\n\t                ;;\n\t        wait_on_node|wait_on_node_port|kill_on_node)\n\t\t\t[ \"$1\" = \"wait_on_node\" ] && REMOTE_VALGRIND_LOG=1\n\t\t\ttrace=\"$1 $2 $3 $4\"\n\t\t\t[ $# -ge 4  ] && shift 4 || shift $#\n\t                ;;\n\t        esac\n\tfi\n\n\tif [ \"$CHECK_TYPE\" = \"drd\" ]; then\n\t\texport VALGRIND_OPTS=\"$VALGRIND_OPTS --suppressions=../drd-log.supp\"\n\tfi\n\n\tdisable_exit_on_error\n\n\teval $ECHO LD_LIBRARY_PATH=$TEST_LD_LIBRARY_PATH LD_PRELOAD=$TEST_LD_PRELOAD \\\n\t\t$trace $*\n\tret=$?\n\n\tif [ $REMOTE_VALGRIND_LOG -eq 1 ]; then\n\t\tfor node in $CHECK_NODES\n\t\tdo\n\t\t\tlocal new_log_file=node\\_$node\\_$VALGRIND_LOG_FILE\n\t\t\tcopy_files_from_node $node \".\" ${NODE_TEST_DIR[$node]}/$VALGRIND_LOG_FILE\n\t\t\tmv $VALGRIND_LOG_FILE $new_log_file\n\t\tdone\n\tfi\n\trestore_exit_on_error\n\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tif [ \"$ret\" -gt \"128\" ]; then\n\t\t\tmsg=\"crashed (signal $(($ret - 128)))\"\n\t\telse\n\t\t\tmsg=\"failed with exit code $ret\"\n\t\tfi\n\t\t[ -t 2 ] && command -v tput >/dev/null && msg=\"$(tput setaf 1)$msg$(tput sgr0)\"\n\n\t\tif [ -f $ERR_LOG_FILE ]; then\n\t\t\tif [ \"$UNITTEST_LOG_LEVEL\" -ge \"1\" ]; then\n\t\t\t\techo -e \"$UNITTEST_NAME $msg. $ERR_LOG_FILE below.\" >&2\n\t\t\t\tcat $ERR_LOG_FILE >&2\n\t\t\telse\n\t\t\t\techo -e \"$UNITTEST_NAME $msg. $ERR_LOG_FILE above.\" >&2\n\t\t\tfi\n\t\telse\n\t\t\techo -e \"$UNITTEST_NAME $msg.\" >&2\n\t\tfi\n\t\tif [ \"$CHECK_TYPE\" != \"none\" -a -f $VALGRIND_LOG_FILE ]; then\n\t\t\tdump_last_n_lines $VALGRIND_LOG_FILE\n\t\tfi\n\n\t\t# ignore Ctrl-C\n\t\tif [ $ret != 130 ]; then\n\t\t\tfor f in $(get_files \"node_.*${UNITTEST_NUM}\\.log\"); do\n\t\t\t\tdump_last_n_lines $f\n\t\t\tdone\n\n\t\t\tdump_last_n_lines $TRACE_LOG_FILE\n\t\t\tdump_last_n_lines $PMEM_LOG_FILE\n\t\t\tdump_last_n_lines $PMEMOBJ_LOG_FILE\n\t\t\tdump_last_n_lines $PMEMLOG_LOG_FILE\n\t\t\tdump_last_n_lines $PMEMBLK_LOG_FILE\n\t\t\tdump_last_n_lines $PMEMPOOL_LOG_FILE\n\t\t\tdump_last_n_lines $VMEM_LOG_FILE\n\t\t\tdump_last_n_lines $VMMALLOC_LOG_FILE\n\t\t\tdump_last_n_lines $RPMEM_LOG_FILE\n\t\t\tdump_last_n_lines $RPMEMD_LOG_FILE\n\t\tfi\n\n\t\t[ $NODES_MAX -ge 0 ] && clean_all_remote_nodes\n\n\t\tfalse\n\tfi\n\tif [ \"$CHECK_TYPE\" != \"none\" ]; then\n\t\tif [ $REMOTE_VALGRIND_LOG -eq 1 ]; then\n\t\t\tfor node in $CHECK_NODES\n\t\t\tdo\n\t\t\t\tlocal log_file=node\\_$node\\_$VALGRIND_LOG_FILE\n\t\t\t\tvalgrind_ignore_warnings $new_log_file\n\t\t\t\tvalidate_valgrind_log $new_log_file\n\t\t\tdone\n\t\telse\n\t\t\tif [ -f $VALGRIND_LOG_FILE ]; then\n\t\t\t\tvalgrind_ignore_warnings $VALGRIND_LOG_FILE\n\t\t\t\tvalidate_valgrind_log $VALGRIND_LOG_FILE\n\t\t\tfi\n\t\tfi\n\tfi\n\n\tif [ \"$MEMCHECK_DONT_CHECK_LEAKS\" = \"1\" -a \"$CHECK_TYPE\" = \"memcheck\" ]; then\n\t\texport ASAN_OPTIONS=\"${OLD_ASAN_OPTIONS}\"\n\tfi\n}\n\n#\n# expect_abnormal_exit -- run a given command, expect it to exit non-zero\n#\nfunction expect_abnormal_exit() {\n\tif [ -n \"$TRACE\" ]; then\n\t\tcase \"$1\"\n\t\tin\n\t\t*_on_node*)\n\t\t\tmsg \"$UNITTEST_NAME: SKIP: TRACE is not supported if test is executed on remote nodes\"\n\t\t\texit 0\n\t\tesac\n\tfi\n\n\t# in case of preloading libvmmalloc.so.1 force valgrind to not override malloc\n\tif [ -n \"$VALGRINDEXE\" -a -n \"$TEST_LD_PRELOAD\" ]; then\n\t\tif [ $(valgrind_version) -ge 312 ]; then\n\t\t\tpreload=`basename $TEST_LD_PRELOAD`\n\t\tfi\n\t\tif [ \"$preload\" == \"$VMMALLOC\" ]; then\n\t\t\texport VALGRIND_OPTS=\"$VALGRIND_OPTS --soname-synonyms=somalloc=nouserintercepts\"\n\t\tfi\n\tfi\n\n\tif [ \"$CHECK_TYPE\" = \"drd\" ]; then\n\t\texport VALGRIND_OPTS=\"$VALGRIND_OPTS --suppressions=../drd-log.supp\"\n\tfi\n\n\tdisable_exit_on_error\n\teval $ECHO ASAN_OPTIONS=\"detect_leaks=0 ${ASAN_OPTIONS}\" \\\n\t\tLD_LIBRARY_PATH=$TEST_LD_LIBRARY_PATH LD_PRELOAD=$TEST_LD_PRELOAD $TRACE $*\n\tret=$?\n\trestore_exit_on_error\n\n\tif [ \"$ret\" -eq \"0\" ]; then\n\t\tmsg=\"succeeded\"\n\t\t[ -t 2 ] && command -v tput >/dev/null && msg=\"$(tput setaf 1)$msg$(tput sgr0)\"\n\n\t\techo -e \"$UNITTEST_NAME command $msg unexpectedly.\" >&2\n\n\t\t[ $NODES_MAX -ge 0 ] && clean_all_remote_nodes\n\n\t\tfalse\n\tfi\n}\n\n#\n# check_pool -- run pmempool check on specified pool file\n#\nfunction check_pool() {\n\tif [ \"$CHECK_POOL\" == \"1\" ]\n\tthen\n\t\tif [ \"$VERBOSE\" != \"0\" ]\n\t\tthen\n\t\t\techo \"$UNITTEST_NAME: checking consistency of pool ${1}\"\n\t\tfi\n\t\t${PMEMPOOL}.static-nondebug check $1 2>&1 1>>$CHECK_POOL_LOG_FILE\n\tfi\n}\n\n#\n# check_pools -- run pmempool check on specified pool files\n#\nfunction check_pools() {\n\tif [ \"$CHECK_POOL\" == \"1\" ]\n\tthen\n\t\tfor f in $*\n\t\tdo\n\t\t\tcheck_pool $f\n\t\tdone\n\tfi\n}\n\n#\n# require_unlimited_vm -- require unlimited virtual memory\n#\n# This implies requirements for:\n# - overcommit_memory enabled (/proc/sys/vm/overcommit_memory is 0 or 1)\n# - unlimited virtual memory (ulimit -v is unlimited)\n#\nfunction require_unlimited_vm() {\n\t$VM_OVERCOMMIT && [ $(ulimit -v) = \"unlimited\" ] && return\n\tmsg \"$UNITTEST_NAME: SKIP required: overcommit_memory enabled and unlimited virtual memory\"\n\texit 0\n}\n\n#\n# require_linked_with_ndctl -- require an executable linked with libndctl\n#\n# usage: require_linked_with_ndctl <executable-file>\n#\nfunction require_linked_with_ndctl() {\n\t[ \"$1\" == \"\" -o ! -x \"$1\" ] && \\\n\t\tfatal \"$UNITTEST_NAME: ERROR: require_linked_with_ndctl() requires one argument - an executable file\"\n\tlocal lddndctl=$(LD_LIBRARY_PATH=$TEST_LD_LIBRARY_PATH ldd $1 | $GREP -ce \"libndctl\")\n\t[ \"$lddndctl\" == \"1\" ] && return\n\tmsg \"$UNITTEST_NAME: SKIP required: executable $1 linked with libndctl\"\n\texit 0\n}\n\n#\n# require_sudo_allowed -- require sudo command is allowed\n#\nfunction require_sudo_allowed() {\n\tif [ \"$ENABLE_SUDO_TESTS\" != \"y\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: tests using 'sudo' are not enabled in testconfig.sh (ENABLE_SUDO_TESTS)\"\n\t\texit 0\n\tfi\n\n\tif ! timeout --signal=SIGKILL --kill-after=3s 3s sudo date >/dev/null 2>&1\n\tthen\n\t\tmsg \"$UNITTEST_NAME: SKIP required: sudo allowed\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_sudo_allowed_node -- require sudo command on a remote node\n#\n# usage: require_sudo_allowed_node <node-number>\n#\nfunction require_sudo_allowed_node() {\n\tif [ \"$ENABLE_SUDO_TESTS\" != \"y\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: tests using 'sudo' are not enabled in testconfig.sh (ENABLE_SUDO_TESTS)\"\n\t\texit 0\n\tfi\n\n\tif ! run_on_node $1 \"timeout --signal=SIGKILL --kill-after=3s 3s sudo date\" >/dev/null 2>&1\n\tthen\n\t\tmsg \"$UNITTEST_NAME: SKIP required: sudo allowed on node $1\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_no_superuser -- require user without superuser rights\n#\nfunction require_no_superuser() {\n\tlocal user_id=$(id -u)\n\t[ \"$user_id\" != \"0\" ] && return\n\tmsg \"$UNITTEST_NAME: SKIP required: run without superuser rights\"\n\texit 0\n}\n\n#\n# require_no_freebsd -- Skip test on FreeBSD\n#\nfunction require_no_freebsd() {\n\t[ \"$(uname -s)\" != \"FreeBSD\" ] && return\n\tmsg \"$UNITTEST_NAME: SKIP: Not supported on FreeBSD\"\n\texit 0\n}\n\n#\n# require_procfs -- Skip test if /proc is not mounted\n#\nfunction require_procfs() {\n\tmount | grep -q \"/proc\" && return\n\tmsg \"$UNITTEST_NAME: SKIP: /proc not mounted\"\n\texit 0\n}\n\nfunction get_arch() {\n\tgcc -dumpmachine | awk -F'[/-]' '{print $1}'\n}\n\nfunction require_x86_64() {\n\t[ $(get_arch) = \"x86_64\" ] && return\n\tmsg \"$UNITTEST_NAME: SKIP: Not supported on arch != x86_64\"\n\texit 0\n}\n\n#\n# require_test_type -- only allow script to continue for a certain test type\n#\nfunction require_test_type() {\n\treq_test_type=1\n\tfor type in $*\n\tdo\n\t\tcase \"$TEST\"\n\t\tin\n\t\tall)\n\t\t\t# \"all\" is a synonym of \"short + medium + long\"\n\t\t\treturn\n\t\t\t;;\n\t\tcheck)\n\t\t\t# \"check\" is a synonym of \"short + medium\"\n\t\t\t[ \"$type\" = \"short\" -o \"$type\" = \"medium\" ] && return\n\t\t\t;;\n\t\t*)\n\t\t\t[ \"$type\" = \"$TEST\" ] && return\n\t\t\t;;\n\t\tesac\n\tdone\n\tverbose_msg \"$UNITTEST_NAME: SKIP test-type $TEST ($* required)\"\n\texit 0\n}\n\n#\n# require_dev_dax_region -- check if region id file exist for dev dax\n#\nfunction require_dev_dax_region() {\n\tlocal prefix=\"$UNITTEST_NAME: SKIP\"\n\tlocal cmd=\"$PMEMDETECT -r\"\n\n\tfor path in ${DEVICE_DAX_PATH[@]}\n\tdo\n\t\tdisable_exit_on_error\n\t\tout=$($cmd $path 2>&1)\n\t\tret=$?\n\t\trestore_exit_on_error\n\n\t\tif [ \"$ret\" == \"0\" ]; then\n\t\t\tcontinue\n\t\telif [ \"$ret\" == \"1\" ]; then\n\t\t\tmsg \"$prefix $out\"\n\t\t\texit 0\n\t\telse\n\t\t\tfatal \"$UNITTEST_NAME: pmemdetect: $out\"\n\t\tfi\n\tdone\n\tDEVDAX_TO_LOCK=1\n}\n\n#\n# lock_devdax -- acquire a lock on Device DAXes\n#\nlock_devdax() {\n\texec {DEVDAX_LOCK_FD}> $DEVDAX_LOCK\n\tflock $DEVDAX_LOCK_FD\n}\n\n#\n# unlock_devdax -- release a lock on Device DAXes\n#\nunlock_devdax() {\n\tflock -u $DEVDAX_LOCK_FD\n\teval \"exec ${DEVDAX_LOCK_FD}>&-\"\n}\n\n#\n# require_dev_dax_node -- common function for require_dax_devices and\n# node_require_dax_device\n#\n# usage: require_dev_dax_node <N devices> [<node>]\n#\nfunction require_dev_dax_node() {\n\treq_dax_dev=1\n\tif  [ \"$req_dax_dev_align\" == \"1\" ]; then\n\t\tfatal \"$UNITTEST_NAME: Do not use 'require_(node_)dax_devices' and \"\n\t\t\t\"'require_(node_)dax_device_alignments' together. Use the latter instead.\"\n\tfi\n\n\tlocal min=$1\n\tlocal node=$2\n\tif [ -n \"$node\" ]; then\n\t\tlocal DIR=${NODE_WORKING_DIR[$node]}/$curtestdir\n\t\tlocal prefix=\"$UNITTEST_NAME: SKIP NODE $node:\"\n\t\tlocal device_dax_path=(${NODE_DEVICE_DAX_PATH[$node]})\n\t\tif [  ${#device_dax_path[@]} -lt $min ]; then\n\t\t\tmsg \"$prefix NODE_${node}_DEVICE_DAX_PATH does not specify enough dax devices (min: $min)\"\n\t\t\texit 0\n\t\tfi\n\t\tlocal cmd=\"ssh $SSH_OPTS ${NODE[$node]} cd $DIR && LD_LIBRARY_PATH=$REMOTE_LD_LIBRARY_PATH ../pmemdetect -d\"\n\telse\n\t\tlocal prefix=\"$UNITTEST_NAME: SKIP\"\n\t\tif [ ${#DEVICE_DAX_PATH[@]} -lt $min ]; then\n\t\t\tmsg \"$prefix DEVICE_DAX_PATH does not specify enough dax devices (min: $min)\"\n\t\t\texit 0\n\t\tfi\n\t\tlocal device_dax_path=${DEVICE_DAX_PATH[@]}\n\t\tlocal cmd=\"$PMEMDETECT -d\"\n\tfi\n\n\tfor path in ${device_dax_path[@]}\n\tdo\n\t\tdisable_exit_on_error\n\t\tout=$($cmd $path 2>&1)\n\t\tret=$?\n\t\trestore_exit_on_error\n\n\t\tif [ \"$ret\" == \"0\" ]; then\n\t\t\tcontinue\n\t\telif [ \"$ret\" == \"1\" ]; then\n\t\t\tmsg \"$prefix $out\"\n\t\t\texit 0\n\t\telse\n\t\t\tfatal \"$UNITTEST_NAME: pmemdetect: $out\"\n\t\tfi\n\tdone\n\tDEVDAX_TO_LOCK=1\n}\n\n#\n# require_dax_devices -- only allow script to continue if there is a required\n# number of Device DAX devices\n#\nfunction require_dax_devices() {\n\trequire_dev_dax_node $1\n}\n\n#\n# require_node_dax_device -- only allow script to continue if specified node\n# has enough Device DAX devices defined in testconfig.sh\n#\nfunction require_node_dax_device() {\n\tvalidate_node_number $1\n\trequire_dev_dax_node $2 $1\n}\n\n#\n# require_no_unicode -- overwrite unicode suffix to empty string\n#\nfunction require_no_unicode() {\n\texport SUFFIX=\"\"\n}\n\n#\n# get_node_devdax_path -- get path of a Device DAX device on a node\n#\n# usage: get_node_devdax_path <node> <device>\n#\nget_node_devdax_path() {\n\tlocal node=$1\n\tlocal device=$2\n\tlocal device_dax_path=(${NODE_DEVICE_DAX_PATH[$node]})\n\techo ${device_dax_path[$device]}\n}\n\n#\n# dax_device_zero -- zero all local dax devices\n#\ndax_device_zero() {\n\tfor path in ${DEVICE_DAX_PATH[@]}\n\tdo\n\t\t${PMEMPOOL}.static-debug rm -f $path\n\tdone\n}\n\n#\n# node_dax_device_zero -- zero all dax devices on a node\n#\nnode_dax_device_zero() {\n\tlocal node=$1\n\tlocal DIR=${NODE_WORKING_DIR[$node]}/$curtestdir\n\tlocal prefix=\"$UNITTEST_NAME: SKIP NODE $node:\"\n\tlocal device_dax_path=(${NODE_DEVICE_DAX_PATH[$node]})\n\tlocal cmd=\"ssh $SSH_OPTS ${NODE[$node]} cd $DIR && LD_LIBRARY_PATH=$REMOTE_LD_LIBRARY_PATH ../pmempool rm -f\"\n\n\tfor path in ${device_dax_path[@]}\n\tdo\n\t\tdisable_exit_on_error\n\t\tout=$($cmd $path 2>&1)\n\t\tret=$?\n\t\trestore_exit_on_error\n\n\t\tif [ \"$ret\" == \"0\" ]; then\n\t\t\tcontinue\n\t\telif [ \"$ret\" == \"1\" ]; then\n\t\t\tmsg \"$prefix $out\"\n\t\t\texit 0\n\t\telse\n\t\t\tfatal \"$UNITTEST_NAME: pmempool rm: $out\"\n\t\tfi\n\tdone\n\n}\n\n#\n# get_devdax_size -- get the size of a device dax\n#\nfunction get_devdax_size() {\n\tlocal device=$1\n\tlocal path=${DEVICE_DAX_PATH[$device]}\n\tlocal major_hex=$(stat -c \"%t\" $path)\n\tlocal minor_hex=$(stat -c \"%T\" $path)\n\tlocal major_dec=$((16#$major_hex))\n\tlocal minor_dec=$((16#$minor_hex))\n\tcat /sys/dev/char/$major_dec:$minor_dec/size\n}\n\n#\n# get_node_devdax_size -- get the size of a device dax on a node\n#\nfunction get_node_devdax_size() {\n\tlocal node=$1\n\tlocal device=$2\n\tlocal device_dax_path=(${NODE_DEVICE_DAX_PATH[$node]})\n\tlocal path=${device_dax_path[$device]}\n\tlocal cmd_prefix=\"ssh $SSH_OPTS ${NODE[$node]} \"\n\n\tdisable_exit_on_error\n\tout=$($cmd_prefix stat -c %t $path 2>&1)\n\tret=$?\n\trestore_exit_on_error\n\tif [ \"$ret\" != \"0\" ]; then\n\t\tfatal \"$UNITTEST_NAME: stat on node $node: $out\"\n\tfi\n\tlocal major=$((16#$out))\n\n\tdisable_exit_on_error\n\tout=$($cmd_prefix stat -c %T $path 2>&1)\n\tret=$?\n\trestore_exit_on_error\n\tif [ \"$ret\" != \"0\" ]; then\n\t\tfatal \"$UNITTEST_NAME: stat on node $node: $out\"\n\tfi\n\tlocal minor=$((16#$out))\n\n\tdisable_exit_on_error\n\tout=$($cmd_prefix \"cat /sys/dev/char/$major:$minor/size\" 2>&1)\n\tret=$?\n\trestore_exit_on_error\n\tif [ \"$ret\" != \"0\" ]; then\n\t\tfatal \"$UNITTEST_NAME: stat on node $node: $out\"\n\tfi\n\techo $out\n}\n\n#\n# dax_get_alignment -- get the alignment of a device dax\n#\nfunction dax_get_alignment() {\n\tmajor_hex=$(stat -c \"%t\" $1)\n\tminor_hex=$(stat -c \"%T\" $1)\n\tmajor_dec=$((16#$major_hex))\n\tminor_dec=$((16#$minor_hex))\n\tcat /sys/dev/char/$major_dec:$minor_dec/device/align\n}\n\n\n#\n# require_dax_device_node_alignments -- only allow script to continue if\n#    the internal Device DAX alignments on a remote nodes are as specified.\n# If necessary, it sorts DEVICE_DAX_PATH entries to match\n# the requested alignment order.\n#\n# usage: require_node_dax_device_alignments <node> <alignment1> [ alignment2 ... ]\n#\nfunction require_node_dax_device_alignments() {\n\treq_dax_dev_align=1\n\tif  [ \"$req_dax_dev\" == \"$1\" ]; then\n\t\tfatal \"$UNITTEST_NAME: Do not use 'require_(node_)dax_devices' and \"\n\t\t\t\"'require_(node_)dax_device_alignments' together. Use the latter instead.\"\n\tfi\n\n\tlocal node=$1\n\tshift\n\n\tif [ \"$node\" == \"-1\" ]; then\n\t\tlocal device_dax_path=(${DEVICE_DAX_PATH[@]})\n\t\tlocal cmd=\"$PMEMDETECT -a\"\n\telse\n\t\tlocal device_dax_path=(${NODE_DEVICE_DAX_PATH[$node]})\n\t\tlocal DIR=${NODE_WORKING_DIR[$node]}/$curtestdir\n\t\tlocal cmd=\"ssh $SSH_OPTS ${NODE[$node]} cd $DIR && LD_LIBRARY_PATH=$REMOTE_LD_LIBRARY_PATH ../pmemdetect -a\"\n\tfi\n\n\tlocal cnt=${#device_dax_path[@]}\n\tlocal j=0\n\n\tfor alignment in $*\n\tdo\n\t\tfor (( i=j; i<cnt; i++ ))\n\t\tdo\n\t\t\tpath=${device_dax_path[$i]}\n\n\t\t\tdisable_exit_on_error\n\t\t\tout=$($cmd $alignment $path 2>&1)\n\t\t\tret=$?\n\t\t\trestore_exit_on_error\n\n\t\t\tif [ \"$ret\" == \"0\" ]; then\n\t\t\t\tif [ $i -ne $j ]; then\n\t\t\t\t\t# swap device paths\n\t\t\t\t\ttmp=${device_dax_path[$j]}\n\t\t\t\t\tdevice_dax_path[$j]=$path\n\t\t\t\t\tdevice_dax_path[$i]=$tmp\n\t\t\t\t\tif [ \"$node\" == \"-1\" ]; then\n\t\t\t\t\t\tDEVICE_DAX_PATH=(${device_dax_path[@]})\n\t\t\t\t\telse\n\t\t\t\t\t\tNODE_DEVICE_DAX_PATH[$node]=${device_dax_path[@]}\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\t\tbreak\n\t\t\tfi\n\t\tdone\n\n\t\tif [ $i -eq $cnt ]; then\n\t\t\tif [ \"$node\" == \"-1\" ]; then\n\t\t\t\tmsg \"$UNITTEST_NAME: SKIP DEVICE_DAX_PATH\"\\\n\t\t\t\t\t\"does not specify enough dax devices or they don't have required alignments (min: $#, alignments: $*)\"\n\t\t\telse\n\t\t\t\tmsg \"$UNITTEST_NAME: SKIP NODE $node: NODE_${node}_DEVICE_DAX_PATH\"\\\n\t\t\t\t\t\"does not specify enough dax devices or they don't have required alignments (min: $#, alignments: $*)\"\n\t\t\tfi\n\t\t\texit 0\n\t\tfi\n\n\t\tj=$(( j + 1 ))\n\tdone\n}\n\n#\n# require_dax_device_alignments -- only allow script to continue if\n#    the internal Device DAX alignments are as specified.\n# If necessary, it sorts DEVICE_DAX_PATH entries to match\n# the requested alignment order.\n#\n# usage: require_dax_device_alignments alignment1 [ alignment2 ... ]\n#\nrequire_dax_device_alignments() {\n\trequire_node_dax_device_alignments -1 $*\n}\n\n\n#\n# require_fs_type -- only allow script to continue for a certain fs type\n#\nfunction require_fs_type() {\n\treq_fs_type=1\n\tfor type in $*\n\tdo\n\t\t# treat any as either pmem or non-pmem\n\t\t[ \"$type\" = \"$FS\" ] ||\n\t\t\t([ -n \"${FORCE_FS:+x}\" ] && [ \"$type\" = \"any\" ] &&\n\t\t\t[ \"$FS\" != \"none\" ]) && return\n\tdone\n\tverbose_msg \"$UNITTEST_NAME: SKIP fs-type $FS ($* required)\"\n\texit 0\n}\n\n\n#\n# require_native_fallocate -- verify if filesystem supports fallocate\n#\nfunction require_native_fallocate() {\n\trequire_fs_type pmem non-pmem\n\n\tset +e\n\t$FALLOCATE_DETECT $1\n\tstatus=$?\n\tset -e\n\n\tif [ $status -eq 1 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: filesystem does not support fallocate\"\n\t\texit 0\n\telif [ $status -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: fallocate_detect failed\"\n\t\texit 1\n\tfi\n}\n\n#\n# require_usc_persmission -- verify if usc can be read with current persmission\n#\nfunction require_usc_permission() {\n\tset +e\n\t$USC_PERMISSION $1 2> $DIR/usc_permission.txt\n\tstatus=$?\n\tset -e\n\n\t# check if there were any messages printed to stderr, skip test if there were\n\tusc_stderr=$(cat $DIR/usc_permission.txt | wc -c)\n\n\trm -f $DIR/usc_permission.txt\n\n\tif [ $status -eq 1 ] || [ $usc_stderr -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: missing permissions to read usc\"\n\t\texit 0\n\telif [ $status -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: usc_permission_check failed\"\n\t\texit 1\n\tfi\n}\n\n#\n# require_fs_name -- verify if the $DIR is on the required file system\n#\n# Must be AFTER setup() because $DIR must exist\n#\nfunction require_fs_name() {\n\tfsname=`df $DIR -PT | awk '{if (NR == 2) print $2}'`\n\n\tfor name in $*\n\tdo\n\t\tif [ \"$name\" == \"$fsname\" ]; then\n\t\t\treturn\n\t\tfi\n\tdone\n\n\techo \"$UNITTEST_NAME: SKIP file system $fsname ($* required)\"\n\texit 0\n}\n\n#\n# require_build_type -- only allow script to continue for a certain build type\n#\nfunction require_build_type() {\n\tfor type in $*\n\tdo\n\t\t[ \"$type\" = \"$BUILD\" ] && return\n\tdone\n\tverbose_msg \"$UNITTEST_NAME: SKIP build-type $BUILD ($* required)\"\n\texit 0\n}\n\n#\n# require_command -- only allow script to continue if specified command exists\n#\nfunction require_command() {\n\tif ! which $1 &>/dev/null; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: '$1' command required\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_command_node -- only allow script to continue if specified command exists on a remote node\n#\n# usage: require_command_node <node-number>\n#\nfunction require_command_node() {\n\tif ! run_on_node $1 \"which $2 &>/dev/null\"; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: node $1: '$2' command required\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_kernel_module -- only allow script to continue if specified kernel module exists\n#\n# usage: require_kernel_module <module_name> [path_to_modinfo]\n#\nfunction require_kernel_module() {\n\tMODULE=$1\n\tMODINFO=$2\n\n\tif [ \"$MODINFO\" == \"\" ]; then\n\t\tset +e\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\tMODINFO=$(which modinfo 2>/dev/null)\n\t\tset -e\n\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\t[ -x /usr/sbin/modinfo ] && MODINFO=/usr/sbin/modinfo\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\t[ -x /sbin/modinfo ] && MODINFO=/sbin/modinfo\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\tmsg \"$UNITTEST_NAME: SKIP: modinfo command required\" && \\\n\t\t\texit 0\n\telse\n\t\t[ ! -x $MODINFO ] && \\\n\t\t\tmsg \"$UNITTEST_NAME: SKIP: modinfo command required\" && \\\n\t\t\texit 0\n\tfi\n\n\t$MODINFO -F name $MODULE &>/dev/null && true\n\tif [ $? -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: '$MODULE' kernel module required\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_kernel_module_node -- only allow script to continue if specified kernel module exists on a remote node\n#\n# usage: require_kernel_module_node <node> <module_name> [path_to_modinfo]\n#\nfunction require_kernel_module_node() {\n\tNODE_N=$1\n\tMODULE=$2\n\tMODINFO=$3\n\n\tif [ \"$MODINFO\" == \"\" ]; then\n\t\tset +e\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\tMODINFO=$(run_on_node $NODE_N which modinfo 2>/dev/null)\n\t\tset -e\n\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\trun_on_node $NODE_N \"test -x /usr/sbin/modinfo\" && MODINFO=/usr/sbin/modinfo\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\trun_on_node $NODE_N \"test -x /sbin/modinfo\" && MODINFO=/sbin/modinfo\n\t\t[ \"$MODINFO\" == \"\" ] && \\\n\t\t\tmsg \"$UNITTEST_NAME: SKIP: node $NODE_N: modinfo command required\" && \\\n\t\t\texit 0\n\telse\n\t\trun_on_node $NODE_N \"test ! -x $MODINFO\" && \\\n\t\t\tmsg \"$UNITTEST_NAME: SKIP: node $NODE_N: modinfo command required\" && \\\n\t\t\texit 0\n\tfi\n\n\trun_on_node $NODE_N \"$MODINFO -F name $MODULE &>/dev/null\" && true\n\tif [ $? -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: node $NODE_N: '$MODULE' kernel module required\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_pkg -- only allow script to continue if specified package exists\n#    usage: require_pkg <package name> [<package minimal version>]\n#\nfunction require_pkg() {\n\tif ! command -v pkg-config 1>/dev/null\n\tthen\n\t\tmsg \"$UNITTEST_NAME: SKIP pkg-config required\"\n\t\texit 0\n\tfi\n\n\tlocal COMMAND=\"pkg-config $1\"\n\tlocal MSG=\"$UNITTEST_NAME: SKIP '$1' package\"\n\tif [ \"$#\" -eq \"2\" ]; then\n\t\tCOMMAND=\"$COMMAND --atleast-version $2\"\n\t\tMSG=\"$MSG (version >= $2)\"\n\tfi\n\tMSG=\"$MSG required\"\n\tif ! $COMMAND\n\tthen\n\t\tmsg \"$MSG\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_node_pkg -- only allow script to continue if specified package exists\n# on specified node\n#    usage: require_node_pkg <node> <package name> [<package minimal version>]\n#\nfunction require_node_pkg() {\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tshift\n\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\tlocal COMMAND=\"${NODE_ENV[$N]}\"\n\tif [ -n \"${NODE_LD_LIBRARY_PATH[$N]}\" ]; then\n\t\tlocal PKG_CONFIG_PATH=${NODE_LD_LIBRARY_PATH[$N]//:/\\/pkgconfig:}/pkgconfig\n\t\tCOMMAND=\"$COMMAND PKG_CONFIG_PATH=\\$PKG_CONFIG_PATH:$PKG_CONFIG_PATH\"\n\tfi\n\n\tCOMMAND=\"$COMMAND pkg-config $1\"\n\tMSG=\"$UNITTEST_NAME: SKIP NODE $N: '$1' package\"\n\tif [ \"$#\" -eq \"2\" ]; then\n\t\tCOMMAND=\"$COMMAND --atleast-version $2\"\n\t\tMSG=\"$MSG (version >= $2)\"\n\tfi\n\tMSG=\"$MSG required\"\n\n\tdisable_exit_on_error\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"$COMMAND\" 2>&1\n\tret=$?\n\trestore_exit_on_error\n\n\tif [ \"$ret\" == 1 ]; then\n\t\tmsg \"$MSG\"\n\t\texit 0\n\tfi\n}\n\n#\n# configure_valgrind -- only allow script to continue when settings match\n#\nfunction configure_valgrind() {\n\tcase \"$1\"\n\tin\n\tmemcheck|pmemcheck|helgrind|drd|force-disable)\n\t\t;;\n\t*)\n\t\tusage \"bad test-type: $1\"\n\t\t;;\n\tesac\n\n\tif [ \"$CHECK_TYPE\" == \"none\" ]; then\n\t\tif [ \"$1\" == \"force-disable\" ]; then\n\t\t\tmsg \"$UNITTEST_NAME: all valgrind tests disabled\"\n\t\telif [ \"$2\" = \"force-enable\" ]; then\n\t\t\tCHECK_TYPE=\"$1\"\n\t\t\trequire_valgrind_tool $1 $3\n\t\telif [ \"$2\" = \"force-disable\" ]; then\n\t\t\tCHECK_TYPE=none\n\t\telse\n\t\t\tfatal \"invalid parameter\"\n\t\tfi\n\telse\n\t\tif [ \"$1\" == \"force-disable\" ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP RUNTESTS script parameter $CHECK_TYPE tries to enable valgrind test when all valgrind tests are disabled in TEST\"\n\t\t\texit 0\n\t\telif [ \"$CHECK_TYPE\" != \"$1\" -a \"$2\" == \"force-enable\" ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP RUNTESTS script parameter $CHECK_TYPE tries to enable different valgrind test than one defined in TEST\"\n\t\t\texit 0\n\t\telif [ \"$CHECK_TYPE\" == \"$1\" -a \"$2\" == \"force-disable\" ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP RUNTESTS script parameter $CHECK_TYPE tries to enable test defined in TEST as force-disable\"\n\t\t\texit 0\n\t\tfi\n\t\trequire_valgrind_tool $CHECK_TYPE $3\n\tfi\n\n\tif [ \"$UT_VALGRIND_SKIP_PRINT_MISMATCHED\" == 1 ]; then\n\t\texport UT_SKIP_PRINT_MISMATCHED=1\n\tfi\n}\n\n#\n# valgrind_version_no_check -- returns Valgrind version without checking\n#   for valgrind first\n#\nfunction valgrind_version_no_check() {\n\trequire_command bc\n\t$VALGRINDEXE --version | sed \"s/valgrind-\\([0-9]*\\)\\.\\([0-9]*\\).*/\\1*100+\\2/\" | bc\n}\n\n#\n# require_valgrind -- continue script execution only if\n#\tvalgrind package is installed\n#\nfunction require_valgrind() {\n\trequire_no_asan\n\tdisable_exit_on_error\n\tVALGRINDEXE=`which valgrind 2>/dev/null`\n\tlocal ret=$?\n\trestore_exit_on_error\n\tif [ $ret -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP valgrind required\"\n\t\texit 0\n\tfi\n\t[ $NODES_MAX -lt 0 ] && return;\n\n\tif [ ! -z \"$1\" ]; then\n\t\tavailable=$(valgrind_version_no_check)\n\t\trequired=`echo $1 | sed \"s/\\([0-9]*\\)\\.\\([0-9]*\\).*/\\1*100+\\2/\" | bc`\n\n\t\tif [ $available -lt $required ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP valgrind required (ver $1 or later)\"\n\t\t\texit 0\n\t\tfi\n\tfi\n\n\tfor N in $NODES_SEQ; do\n\t\tif [ \"${NODE_VALGRINDEXE[$N]}\" = \"\" ]; then\n\t\t\tdisable_exit_on_error\n\t\t\tNODE_VALGRINDEXE[$N]=$(ssh $SSH_OPTS ${NODE[$N]} \"which valgrind 2>/dev/null\")\n\t\t\tret=$?\n\t\t\trestore_exit_on_error\n\t\t\tif [ $ret -ne 0 ]; then\n\t\t\t\tmsg \"$UNITTEST_NAME: SKIP valgrind required on remote node #$N\"\n\t\t\t\texit 0\n\t\t\tfi\n\t\tfi\n\tdone\n}\n\n#\n# valgrind_version -- returns Valgrind version\n#\nfunction valgrind_version() {\n\trequire_valgrind\n\tvalgrind_version_no_check\n}\n\n#\n# require_valgrind_tool -- continue script execution only if valgrind with\n#\tspecified tool is installed\n#\n#\tusage: require_valgrind_tool <tool> [<binary>]\n#\nfunction require_valgrind_tool() {\n\trequire_valgrind\n\tlocal tool=$1\n\tlocal binary=$2\n\tlocal dir=.\n\t[ -d \"$2\" ] && dir=\"$2\" && binary=\n\tpushd \"$dir\" > /dev/null\n\t[ -n \"$binary\" ] || binary=$(get_executables)\n\tif [ -z \"$binary\" ]; then\n\t\tfatal \"require_valgrind_tool: error: no binary found\"\n\tfi\n\tstrings ${binary} 2>&1 | \\\n\tgrep -q \"compiled with support for Valgrind $tool\" && true\n\tif [ $? -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP not compiled with support for Valgrind $tool\"\n\t\texit 0\n\tfi\n\n\tif [ \"$tool\" == \"helgrind\" ]; then\n\t\tvalgrind --tool=$tool --help 2>&1 | \\\n\t\tgrep -qi \"$tool is Copyright (c)\" && true\n\t\tif [ $? -ne 0 ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP Valgrind with $tool required\"\n\t\t\texit 0;\n\t\tfi\n\tfi\n\tif [ \"$tool\" == \"pmemcheck\" ]; then\n\t\tout=`valgrind --tool=$tool --help 2>&1` && true\n\t\techo \"$out\" | grep -qi \"$tool is Copyright (c)\" && true\n\t\tif [ $? -ne 0 ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP Valgrind with $tool required\"\n\t\t\texit 0;\n\t\tfi\n\t\techo \"$out\" | grep -qi \"expect-fence-after-clflush\" && true\n\t\tif [ $? -ne 0 ]; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP pmemcheck does not support --expect-fence-after-clflush option. Please update it to the latest version.\"\n\t\t\texit 0;\n\t\tfi\n\tfi\n\tpopd > /dev/null\n\treturn 0\n}\n\n#\n# set_valgrind_exe_name -- set the actual Valgrind executable name\n#\n# On some systems (Ubuntu), \"valgrind\" is a shell script that calls\n# the actual executable \"valgrind.bin\".\n# The wrapper script doesn't work well with LD_PRELOAD, so we want\n# to call Valgrind directly.\n#\nfunction set_valgrind_exe_name() {\n\tif [ \"$VALGRINDEXE\" = \"\" ]; then\n\t\tfatal \"set_valgrind_exe_name: error: valgrind is not set up\"\n\tfi\n\n\tlocal VALGRINDDIR=`dirname $VALGRINDEXE`\n\tif [ -x $VALGRINDDIR/valgrind.bin ]; then\n\t\tVALGRINDEXE=$VALGRINDDIR/valgrind.bin\n\tfi\n\n\t[ $NODES_MAX -lt 0 ] && return;\n\tfor N in $NODES_SEQ; do\n\t\tlocal COMMAND=\"\\\n\t\t\t[ -x $(dirname ${NODE_VALGRINDEXE[$N]})/valgrind.bin ] && \\\n\t\t\techo $(dirname ${NODE_VALGRINDEXE[$N]})/valgrind.bin || \\\n\t\t\techo ${NODE_VALGRINDEXE[$N]}\"\n\t\tNODE_VALGRINDEXE[$N]=$(ssh $SSH_OPTS ${NODE[$N]} $COMMAND)\n\t\tif [ $? -ne 0 ]; then\n\t\t\tfatal ${NODE_VALGRINDEXE[$N]}\n\t\tfi\n\tdone\n}\n\n#\n# require_no_asan_for - continue script execution only if passed binary does\n#\tNOT require libasan\n#\nfunction require_no_asan_for() {\n\tdisable_exit_on_error\n\tnm $1 | grep -q __asan_\n\tASAN_ENABLED=$?\n\trestore_exit_on_error\n\tif [ \"$ASAN_ENABLED\" == \"0\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: ASAN enabled\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_cxx11 -- continue script execution only if C++11 supporting compiler\n#\tis installed\n#\nfunction require_cxx11() {\n\t[ \"$CXX\" ] || CXX=c++\n\n\tCXX11_AVAILABLE=`echo \"int main(){return 0;}\" |\\\n\t\t$CXX -std=c++11 -x c++ -o /dev/null - 2>/dev/null &&\\\n\t\techo y || echo n`\n\n\tif [ \"$CXX11_AVAILABLE\" == \"n\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: C++11 required\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_no_asan - continue script execution only if libpmem does NOT require\n#\tlibasan\n#\nfunction require_no_asan() {\n\tcase \"$BUILD\"\n\tin\n\tdebug)\n\t\trequire_no_asan_for ../../debug/libpmem.so\n\t\t;;\n\tnondebug)\n\t\trequire_no_asan_for ../../nondebug/libpmem.so\n\t\t;;\n\tstatic-debug)\n\t\trequire_no_asan_for ../../debug/libpmem.a\n\t\t;;\n\tstatic-nondebug)\n\t\trequire_no_asan_for ../../nondebug/libpmem.a\n\t\t;;\n\tesac\n}\n\n#\n# require_tty - continue script execution only if standard output is a terminal\n#\nfunction require_tty() {\n\tif ! tty >/dev/null; then\n\t\tmsg \"$UNITTEST_NAME: SKIP no terminal\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_binary -- continue script execution only if the binary has been compiled\n#\n# In case of conditional compilation, skip this test.\n#\nfunction require_binary() {\n\tif [ -z \"$1\" ]; then\n\t\tfatal \"require_binary: error: binary not provided\"\n\tfi\n\tif [ ! -x \"$1\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP no binary found\"\n\t\texit 0\n\tfi\n\n\treturn\n}\n\n#\n# require_preload - continue script execution only if supplied\n#\texecutable does not generate SIGABRT\n#\n#\tUsed to check that LD_PRELOAD of, e.g., libvmmalloc is possible\n#\n#\tusage: require_preload <errorstr> <executable> [<exec_args>]\n#\nfunction require_preload() {\n\tmsg=$1\n\tshift\n\ttrap SIGABRT\n\tdisable_exit_on_error\n\tret=$(LD_LIBRARY_PATH=$TEST_LD_LIBRARY_PATH LD_PRELOAD=$TEST_LD_PRELOAD $* 2>&1 /dev/null)\n\tret=$?\n\trestore_exit_on_error\n\tif [ $ret == 134 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: $msg not supported\"\n\t\trm -f $1.core\n\t\texit 0\n\tfi\n}\n\n#\n# require_sds -- continue script execution only if binary is compiled with\n#\tshutdown state support\n#\n#\tusage: require_sds <binary>\n#\nfunction require_sds() {\n\tlocal binary=$1\n\tlocal dir=.\n\tif [ -z \"$binary\" ]; then\n\t\tfatal \"require_sds: error: no binary found\"\n\tfi\n\tstrings ${binary} 2>&1 | \\\n\t\tgrep -q \"compiled with support for shutdown state\" && true\n\tif [ $? -ne 0 ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP not compiled with support for shutdown state\"\n\t\texit 0\n\tfi\n\treturn 0\n}\n\n#\n# require_no_sds -- continue script execution only if binary is NOT compiled with\n#\tshutdown state support\n#\n#\tusage: require_no_sds <binary>\n#\nfunction require_no_sds() {\n\tlocal binary=$1\n\tlocal dir=.\n\tif [ -z \"$binary\" ]; then\n\t\tfatal \"require_sds: error: no binary found\"\n\tfi\n\tset +e\n\tfound=$(strings ${binary} 2>&1 | \\\n\t\tgrep -c \"compiled with support for shutdown state\")\n\tset -e\n\tif [ \"$found\" -ne \"0\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP compiled with support for shutdown state\"\n\t\texit 0\n\tfi\n\treturn 0\n}\n\n#\n# check_absolute_path -- continue script execution only if $DIR path is\n#                        an absolute path; do not resolve symlinks\n#\nfunction check_absolute_path() {\n\tif [ \"${DIR:0:1}\" != \"/\" ]; then\n\t\tfatal \"Directory \\$DIR has to be an absolute path. $DIR was given.\"\n\tfi\n}\n\n#\n# run_command -- run a command in a verbose or quiet way\n#\nfunction run_command()\n{\n\tlocal COMMAND=\"$*\"\n\tif [ \"$VERBOSE\" != \"0\" ]; then\n\t\techo \"$ $COMMAND\"\n\t\t$COMMAND\n\telse\n\t\t$COMMAND\n\tfi\n}\n\n\n#\n# validate_node_number -- validate a node number\n#\nfunction validate_node_number() {\n\n\t[ $1 -gt $NODES_MAX ] \\\n\t\t&& fatal \"error: node number ($1) greater than maximum allowed node number ($NODES_MAX)\"\n\treturn 0\n}\n\n#\n# clean_remote_node -- usage: clean_remote_node <node> <list-of-pid-files>\n#\nfunction clean_remote_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tshift\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\n\t# register the list of PID files to be cleaned in case of an error\n\tNODE_PID_FILES[$N]=\"${NODE_PID_FILES[$N]} $*\"\n\n\t# clean the remote node\n\tdisable_exit_on_error\n\tfor pidfile in ${NODE_PID_FILES[$N]}; do\n\t\trequire_ctrld_err $N $pidfile\n\t\trun_command ssh $SSH_OPTS ${NODE[$N]} \"\\\n\t\t\tcd $DIR && [ -f $pidfile ] && \\\n\t\t\t../ctrld $pidfile kill SIGINT && \\\n\t\t\t../ctrld $pidfile wait 1 ; \\\n\t\t\trm -f $pidfile\"\n\tdone;\n\trestore_exit_on_error\n\n\treturn 0\n}\n\n#\n# clean_all_remote_nodes -- clean all remote nodes in case of an error\n#\nfunction clean_all_remote_nodes() {\n\n\tmsg \"$UNITTEST_NAME: CLEAN (cleaning processes on remote nodes)\"\n\n\tlocal N=0\n\tdisable_exit_on_error\n\tfor N in $NODES_SEQ; do\n\t\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\t\tfor pidfile in ${NODE_PID_FILES[$N]}; do\n\t\t\trun_command ssh $SSH_OPTS ${NODE[$N]} \"\\\n\t\t\t\tcd $DIR && [ -f $pidfile ] && \\\n\t\t\t\t../ctrld $pidfile kill SIGINT && \\\n\t\t\t\t../ctrld $pidfile wait 1 ; \\\n\t\t\t\trm -f $pidfile\"\n\t\tdone\n\tdone\n\trestore_exit_on_error\n\n\treturn 0\n}\n\n#\n# export_vars_node -- export specified variables on specified node\n#\nfunction export_vars_node() {\n\tlocal N=$1\n\tshift\n\tvalidate_node_number $N\n\tfor var in \"$@\"; do\n\t\tNODE_ENV[$N]=\"${NODE_ENV[$N]} $var=${!var}\"\n\tdone\n}\n\n#\n# require_nodes_libfabric -- only allow script to continue if libfabric with\n#                            optionally specified provider is available on\n#                            specified node\n#    usage: require_nodes_libfabric <node> <provider> [<libfabric-version>]\n#\nfunction require_node_libfabric() {\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal provider=$2\n\t# Minimal required version of libfabric.\n\t# Keep in sync with requirements in src/common.inc.\n\tlocal version=${3:-1.4.2}\n\n\trequire_pkg libfabric \"$version\"\n\trequire_node_pkg $N libfabric \"$version\"\n\tif [ \"$RPMEM_DISABLE_LIBIBVERBS\" != \"y\" ]; then\n\t\tif ! fi_info --list | grep -q verbs; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP libfabric not compiled with verbs provider\"\n\t\t\texit 0\n\t\tfi\n\n\t\tif ! run_on_node $N \"fi_info --list | grep -q verbs\"; then\n\t\t\tmsg \"$UNITTEST_NAME: SKIP libfabric on node $N not compiled with verbs provider\"\n\t\t\texit 0\n\n\t\tfi\n\tfi\n\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\tlocal COMMAND=\"$COMMAND ${NODE_ENV[$N]}\"\n\tCOMMAND=\"$COMMAND LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:$REMOTE_LD_LIBRARY_PATH:${NODE_LD_LIBRARY_PATH[$N]}\"\n\tCOMMAND=\"$COMMAND ../fip ${NODE_ADDR[$N]} $provider\"\n\n\tdisable_exit_on_error\n\tfip_out=$(ssh $SSH_OPTS ${NODE[$N]} \"cd $DIR && $COMMAND\" 2>&1)\n\tret=$?\n\trestore_exit_on_error\n\n\tif [ \"$ret\" == \"0\" ]; then\n\t\treturn\n\telif [ \"$ret\" == \"1\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP NODE $N: $fip_out\"\n\t\texit 0\n\telse\n\t\tfatal \"NODE $N: require_libfabric $provider: $fip_out\"\n\tfi\n}\n\n#\n# check_if_node_is_reachable -- check if the $1 node is reachable\n#\nfunction check_if_node_is_reachable() {\n\tdisable_exit_on_error\n\trun_command ssh $SSH_OPTS ${NODE[$1]} exit\n\tlocal ret=$?\n\trestore_exit_on_error\n\treturn $ret\n}\n\n#\n# require_nodes -- only allow script to continue for a certain number\n#                  of defined and reachable nodes\n#\n# Input arguments:\n#   NODE[]               - (required) array of nodes' addresses\n#   NODE_WORKING_DIR[]   - (required) array of nodes' working directories\n#\nfunction require_nodes() {\n\n\tlocal N_NODES=${#NODE[@]}\n\tlocal N=$1\n\n\t[ -z \"$N\" ] \\\n\t\t&& fatal \"require_nodes: missing reguired parameter: number of nodes\"\n\n\t# if it has already been called, check if number of required nodes is bigger than previously\n\t[ -n \"$NODES_MAX\" ] \\\n\t\t&& [ $(($N - 1)) -le $NODES_MAX ] && return\n\n\t[ $N -gt $N_NODES ] \\\n\t\t&& msg \"$UNITTEST_NAME: SKIP: requires $N node(s), but $N_NODES node(s) provided\" \\\n\t\t&& exit 0\n\n\tNODES_MAX=$(($N - 1))\n\tNODES_SEQ=$(seq -s' ' 0 $NODES_MAX)\n\n\t# check if all required nodes are reachable\n\tfor N in $NODES_SEQ; do\n\t\t# validate node's address\n\t\t[ \"${NODE[$N]}\" = \"\" ] \\\n\t\t\t&& msg \"$UNITTEST_NAME: SKIP: address of node #$N is not provided\" \\\n\t\t\t&& exit 0\n\n\t\t# validate the working directory\n\t\t[ \"${NODE_WORKING_DIR[$N]}\" = \"\" ] \\\n\t\t\t&& fatal \"error: working directory for node #$N (${NODE[$N]}) is not provided\"\n\n\t\t# check if the node is reachable\n\t\tcheck_if_node_is_reachable $N\n\t\t[ $? -ne 0 ] \\\n\t\t\t&& fatal \"error: node #$N (${NODE[$N]}) is unreachable\"\n\n\t\t# clear the list of PID files for each node\n\t\tNODE_PID_FILES[$N]=\"\"\n\t\tNODE_TEST_DIR[$N]=${NODE_WORKING_DIR[$N]}/$curtestdir\n\t\tNODE_DIR[$N]=${NODE_WORKING_DIR[$N]}/$curtestdir/data/\n\n\t\trequire_node_log_files $N $ERR_LOG_FILE $OUT_LOG_FILE $TRACE_LOG_FILE\n\n\t\tif [ \"$CHECK_TYPE\" != \"none\" -a \"${NODE_VALGRINDEXE[$N]}\" = \"\" ]; then\n\t\t\tdisable_exit_on_error\n\t\t\tNODE_VALGRINDEXE[$N]=$(ssh $SSH_OPTS ${NODE[$N]} \"which valgrind 2>/dev/null\")\n\t\t\tlocal ret=$?\n\t\t\trestore_exit_on_error\n\t\t\tif [ $ret -ne 0 ]; then\n\t\t\t\tmsg \"$UNITTEST_NAME: SKIP valgrind required on remote node #$N\"\n\t\t\t\texit 0\n\t\t\tfi\n\t\tfi\n\tdone\n\n\t# remove all log files of the current unit test from the required nodes\n\t# and export the 'log' variables to these nodes\n\tfor N in $NODES_SEQ; do\n\t\tfor f in $(get_files \"node_${N}.*${UNITTEST_NUM}\\.log\"); do\n\t\t\trm -f $f\n\t\tdone\n\t\texport_vars_node $N $REMOTE_VARS\n\tdone\n\n\t# register function to clean all remote nodes in case of an error or SIGINT\n\ttrap clean_all_remote_nodes ERR SIGINT\n\n\treturn 0\n}\n\n#\n# check_files_on_node -- check if specified files exist on given node\n#\nfunction check_files_on_node() {\n\tvalidate_node_number $1\n\tlocal N=$1\n\tshift\n\tlocal REMOTE_DIR=${NODE_DIR[$N]}\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"for f in $*; do if [ ! -f $REMOTE_DIR/\\$f ]; then echo \\\"Missing file \\$f on node #$N\\\" 1>&2; exit 1; fi; done\"\n}\n\n#\n# check_no_files_on_node -- check if specified files does not exist on given node\n#\nfunction check_no_files_on_node() {\n\tvalidate_node_number $1\n\tlocal N=$1\n\tshift\n\tlocal REMOTE_DIR=${NODE_DIR[$N]}\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"for f in $*; do if [ -f $REMOTE_DIR/\\$f ]; then echo \\\"Not deleted file \\$f on node #$N\\\" 1>&2; exit 1; fi; done\"\n}\n\n#\n# copy_files_to_node -- copy all required files to the given remote node\n#    usage: copy_files_to_node <node> <destination dir> <file_1> [<file_2>] ...\n#\nfunction copy_files_to_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal DEST_DIR=$2\n\tshift 2\n\t[ $# -eq 0 ] &&\\\n\t\tfatal \"error: copy_files_to_node(): no files provided\"\n\n\t# copy all required files\n\trun_command scp $SCP_OPTS $@ ${NODE[$N]}:$DEST_DIR > /dev/null\n\n\treturn 0\n}\n\n#\n# copy_files_from_node -- copy all required files from the given remote node\n#    usage: copy_files_from_node <node> <destination_dir> <file_1> [<file_2>] ...\n#\nfunction copy_files_from_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal DEST_DIR=$2\n\t[ ! -d $DEST_DIR ] &&\\\n\t\tfatal \"error: destination directory $DEST_DIR does not exist\"\n\tshift 2\n\t[ $# -eq 0 ] &&\\\n\t\tfatal \"error: copy_files_from_node(): no files provided\"\n\n\t# compress required files, copy and extract\n\tlocal temp_file=node_${N}_temp_file.tar\n\tfiles=\"\"\n\tdir_name=\"\"\n\n\tfiles=$(basename -a $@)\n\tdir_name=$(dirname $1)\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd $dir_name && tar -czf $temp_file $files\"\n\trun_command scp $SCP_OPTS ${NODE[$N]}:$dir_name/$temp_file $DEST_DIR > /dev/null\n\n\tcd $DEST_DIR \\\n\t\t&& tar -xzf $temp_file \\\n\t\t&& rm $temp_file \\\n\t\t&& cd - > /dev/null\n\n\treturn 0\n}\n\n#\n# copy_log_files -- copy log files from remote node\n#\nfunction copy_log_files() {\n\tlocal NODE_SCP_LOG_FILES[0]=\"\"\n\tfor N in $NODES_SEQ; do\n\t\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\t\tfor file in ${NODE_LOG_FILES[$N]}; do\n\t\t\tNODE_SCP_LOG_FILES[$N]=\"${NODE_SCP_LOG_FILES[$N]} ${NODE[$N]}:$DIR/${file}\"\n\t\tdone\n\t\t[ \"${NODE_SCP_LOG_FILES[$N]}\" ] && run_command scp $SCP_OPTS ${NODE_SCP_LOG_FILES[$N]} . &>> $PREP_LOG_FILE\n\t\tfor file in ${NODE_LOG_FILES[$N]}; do\n\t\t\t[ -f $file ] && mv $file node_${N}_${file}\n\t\tdone\n\tdone\n}\n\n#\n# rm_files_from_node -- removes all listed files from the given remote node\n#    usage: rm_files_from_node <node> <file_1> [<file_2>] ...\n#\nfunction rm_files_from_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tshift\n\t[ $# -eq 0 ] &&\\\n\t\tfatal \"error: rm_files_from_node(): no files provided\"\n\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"rm -f $@\"\n\n\treturn 0\n}\n\n#\n#\n# require_node_log_files -- store log files which must be copied from\n#                           specified node on failure\n#\nfunction require_node_log_files() {\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tshift\n\n\tNODE_LOG_FILES[$N]=\"${NODE_LOG_FILES[$N]} $*\"\n}\n\n#\n# require_ctrld_err -- store ctrld's log files to copy from specified\n#                      node on failure\n#\nfunction require_ctrld_err() {\n\tlocal N=$1\n\tlocal PID_FILE=$2\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\tfor cmd in run wait kill wait_port; do\n\t\tNODE_LOG_FILES[$N]=\"${NODE_LOG_FILES[$N]} $PID_FILE.$cmd.ctrld.log\"\n\tdone\n}\n\n#\n# run_on_node -- usage: run_on_node <node> <command>\n#\n#                Run the <command> in background on the remote <node>.\n#                LD_LIBRARY_PATH for the n-th remote node can be provided\n#                in the array NODE_LD_LIBRARY_PATH[n]\n#\nfunction run_on_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tshift\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\tlocal COMMAND=\"UNITTEST_NUM=$UNITTEST_NUM UNITTEST_NAME=$UNITTEST_NAME\"\n\tCOMMAND=\"$COMMAND UNITTEST_LOG_LEVEL=1\"\n\tCOMMAND=\"$COMMAND ${NODE_ENV[$N]}\"\n\tCOMMAND=\"$COMMAND LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:$REMOTE_LD_LIBRARY_PATH:${NODE_LD_LIBRARY_PATH[$N]} $*\"\n\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd $DIR && $COMMAND\"\n\tret=$?\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tcopy_log_files\n\tfi\n\n\treturn $ret\n}\n\n#\n# run_on_node_background -- usage:\n#                           run_on_node_background <node> <pid-file> <command>\n#\n#                           Run the <command> in background on the remote <node>\n#                           and create a <pid-file> for this process.\n#                           LD_LIBRARY_PATH for the n-th remote node\n#                           can be provided in the array NODE_LD_LIBRARY_PATH[n]\n#\nfunction run_on_node_background() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal PID_FILE=$2\n\tshift\n\tshift\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\tlocal COMMAND=\"UNITTEST_NUM=$UNITTEST_NUM UNITTEST_NAME=$UNITTEST_NAME\"\n\tCOMMAND=\"$COMMAND UNITTEST_LOG_LEVEL=1\"\n\tCOMMAND=\"$COMMAND ${NODE_ENV[$N]}\"\n\tCOMMAND=\"$COMMAND LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:$REMOTE_LD_LIBRARY_PATH:${NODE_LD_LIBRARY_PATH[$N]}\"\n\tCOMMAND=\"$COMMAND ../ctrld $PID_FILE run $RUNTEST_TIMEOUT $*\"\n\n\t# register the PID file to be cleaned in case of an error\n\tNODE_PID_FILES[$N]=\"${NODE_PID_FILES[$N]} $PID_FILE\"\n\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd $DIR && $COMMAND\"\n\tret=$?\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tcopy_log_files\n\tfi\n\n\treturn $ret\n}\n\n#\n# wait_on_node -- usage: wait_on_node <node> <pid-file> [<timeout>]\n#\n#                 Wait until the process with the <pid-file> on the <node>\n#                 exits or <timeout> expires.\n#\nfunction wait_on_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal PID_FILE=$2\n\tlocal TIMEOUT=$3\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd $DIR && ../ctrld $PID_FILE wait $TIMEOUT\"\n\tret=$?\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tcopy_log_files\n\tfi\n\n\treturn $ret\n}\n\n#\n# wait_on_node_port -- usage: wait_on_node_port <node> <pid-file> <portno>\n#\n#                      Wait until the process with the <pid-file> on the <node>\n#                      opens the port <portno>.\n#\nfunction wait_on_node_port() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal PID_FILE=$2\n\tlocal PORTNO=$3\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd $DIR && ../ctrld $PID_FILE wait_port $PORTNO\"\n\tret=$?\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tcopy_log_files\n\tfi\n\n\treturn $ret\n}\n\n#\n# kill_on_node -- usage: kill_on_node <node> <pid-file> <signo>\n#\n#                 Send the <signo> signal to the process with the <pid-file>\n#                 on the <node>.\n#\nfunction kill_on_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tlocal PID_FILE=$2\n\tlocal SIGNO=$3\n\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\n\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd $DIR && ../ctrld $PID_FILE kill $SIGNO\"\n\tret=$?\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tcopy_log_files\n\tfi\n\n\treturn $ret\n}\n\n#\n# create_holey_file_on_node -- create holey files of a given length\n#   usage: create_holey_file_on_node <node> <size>\n#\n# example, to create two files, each 1GB in size on node 0:\n#\tcreate_holey_file_on_node 0 1G testfile1 testfile2\n#\n# Input unit size is in bytes with optional suffixes like k, KB, M, etc.\n#\nfunction create_holey_file_on_node() {\n\n\tvalidate_node_number $1\n\n\tlocal N=$1\n\tsize=$(convert_to_bytes $2)\n\tshift 2\n\tfor file in $*\n\tdo\n\t\trun_on_node $N truncate -s ${size} $file >> $PREP_LOG_FILE\n\tdone\n}\n\n#\n# setup -- print message that test setup is commencing\n#\nfunction setup() {\n\n\tDIR=$DIR$SUFFIX\n\texport VMMALLOC_POOL_DIR=\"$DIR\"\n\n\t# writes test working directory to temporary file\n\t# that allows read location of data after test failure\n\tif [ -f \"$TEMP_LOC\" ]; then\n\t\techo \"$DIR\" > $TEMP_LOC\n\tfi\n\n\t# test type must be explicitly specified\n\tif [ \"$req_test_type\" != \"1\" ]; then\n\t\tfatal \"error: required test type is not specified\"\n\tfi\n\n\t# fs type \"none\" must be explicitly enabled\n\tif [ \"$FS\" = \"none\" -a \"$req_fs_type\" != \"1\" ]; then\n\t\texit 0\n\tfi\n\n\t# fs type \"any\" must be explicitly enabled\n\tif [ \"$FS\" = \"any\" -a \"$req_fs_type\" != \"1\" ]; then\n\t\texit 0\n\tfi\n\n\tif [ \"$CHECK_TYPE\" != \"none\" ]; then\n\t\trequire_valgrind\n\t\texport VALGRIND_LOG_FILE=$CHECK_TYPE${UNITTEST_NUM}.log\n\t\tMCSTR=\"/$CHECK_TYPE\"\n\telse\n\t\tMCSTR=\"\"\n\tfi\n\n\t[ -n \"$RPMEM_PROVIDER\" ] && PROV=\"/$RPMEM_PROVIDER\"\n\t[ -n \"$RPMEM_PM\" ] && PM=\"/$RPMEM_PM\"\n\n\tmsg \"$UNITTEST_NAME: SETUP ($TEST/$REAL_FS/$BUILD$MCSTR$PROV$PM)\"\n\n\tfor f in $(get_files \".*[a-zA-Z_]${UNITTEST_NUM}\\.log\"); do\n\t\trm -f $f\n\tdone\n\n\t# $DIR has to be an absolute path\n\tcheck_absolute_path\n\n\tif [ \"$FS\" != \"none\" ]; then\n\t\tif [ -d \"$DIR\" ]; then\n\t\t\trm $RM_ONEFS -rf -- $DIR\n\t\tfi\n\n\t\tmkdir -p $DIR\n\tfi\n\tif [ \"$TM\" = \"1\" ]; then\n\t\tstart_time=$($DATE +%s.%N)\n\tfi\n\n\tif [ \"$DEVDAX_TO_LOCK\" == 1 ]; then\n\t\tlock_devdax\n\tfi\n}\n\n#\n# check_log_empty -- if match file does not exist, assume log should be empty\n#\nfunction check_log_empty() {\n\tif [ ! -f ${1}.match ] && [ $(get_size $1) -ne 0 ]; then\n\t\techo \"unexpected output in $1\"\n\t\tdump_last_n_lines $1\n\t\texit 1\n\tfi\n}\n\n#\n# check_local -- check local test results (using .match files)\n#\nfunction check_local() {\n\tif [ \"$UT_SKIP_PRINT_MISMATCHED\" == 1 ]; then\n\t\toption=-q\n\tfi\n\n\tcheck_log_empty $ERR_LOG_FILE\n\n\tFILES=$(get_files \"[^0-9w]*${UNITTEST_NUM}\\.log\\.match\")\n\tif [ -n \"$FILES\" ]; then\n\t\t../match $option $FILES\n\tfi\n}\n\n#\n# match -- execute match\n#\nfunction match() {\n\t../match $@\n}\n\n#\n# check -- check local or remote test results (using .match files)\n#\nfunction check() {\n\tif [ $NODES_MAX -lt 0 ]; then\n\t\tcheck_local\n\telse\n\t\tFILES=$(get_files \"node_[0-9]+_[^0-9w]*${UNITTEST_NUM}\\.log\\.match\")\n\n\t\tlocal NODE_MATCH_FILES[0]=\"\"\n\t\tlocal NODE_SCP_MATCH_FILES[0]=\"\"\n\t\tfor file in $FILES; do\n\t\t\tlocal N=`echo $file | cut -d\"_\" -f2`\n\t\t\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\t\t\tlocal FILE=`echo $file | cut -d\"_\" -f3 | sed \"s/\\.match$//g\"`\n\t\t\tvalidate_node_number $N\n\t\t\tNODE_MATCH_FILES[$N]=\"${NODE_MATCH_FILES[$N]} $FILE\"\n\t\t\tNODE_SCP_MATCH_FILES[$N]=\"${NODE_SCP_MATCH_FILES[$N]} ${NODE[$N]}:$DIR/$FILE\"\n\t\tdone\n\n\t\tfor N in $NODES_SEQ; do\n\t\t\t[ \"${NODE_SCP_MATCH_FILES[$N]}\" ] && run_command scp $SCP_OPTS ${NODE_SCP_MATCH_FILES[$N]} . > /dev/null\n\t\t\tfor file in ${NODE_MATCH_FILES[$N]}; do\n\t\t\t\tmv $file node_${N}_${file}\n\t\t\tdone\n\t\tdone\n\n\t\tif [ \"$UT_SKIP_PRINT_MISMATCHED\" == 1 ]; then\n\t\t\toption=-q\n\t\tfi\n\n\t\tfor N in $NODES_SEQ; do\n\t\t\tcheck_log_empty node_${N}_${ERR_LOG_FILE}\n\t\tdone\n\n\t\tif [ -n \"$FILES\" ]; then\n\t\t\tmatch $option $FILES\n\t\tfi\n\tfi\n}\n\n#\n# pass -- print message that the test has passed\n#\nfunction pass() {\n\tif [ \"$DEVDAX_TO_LOCK\" == 1 ]; then\n\t\tunlock_devdax\n\tfi\n\n\tif [ \"$TM\" = \"1\" ]; then\n\t\tend_time=$($DATE +%s.%N)\n\n\t\tstart_time_sec=$($DATE -d \"0 $start_time sec\" +%s)\n\t\tend_time_sec=$($DATE -d \"0 $end_time sec\" +%s)\n\n\t\tdays=$(((end_time_sec - start_time_sec) / (24*3600)))\n\t\tdays=$(printf \"%03d\" $days)\n\n\t\ttm=$($DATE -d \"0 $end_time sec - $start_time sec\" +%H:%M:%S.%N)\n\t\ttm=$(echo \"$days:$tm\" | sed -e \"s/^000://g\" -e \"s/^00://g\" -e \"s/^00://g\" -e \"s/\\([0-9]*\\)\\.\\([0-9][0-9][0-9]\\).*/\\1.\\2/\")\n\t\ttm=\"\\t\\t\\t[$tm s]\"\n\telse\n\t\ttm=\"\"\n\tfi\n\tmsg=\"PASS\"\n\t[ -t 1 ] && command -v tput >/dev/null && msg=\"$(tput setaf 2)$msg$(tput sgr0)\"\n\tif [ \"$UNITTEST_LOG_LEVEL\" -ge 1 ]; then\n\t\techo -e \"$UNITTEST_NAME: $msg$tm\"\n\tfi\n\tif [ \"$FS\" != \"none\" ]; then\n\t\trm $RM_ONEFS -rf -- $DIR\n\tfi\n}\n\n# Length of pool file's signature\nSIG_LEN=8\n\n# Offset and length of pmemobj layout\nLAYOUT_OFFSET=4096\nLAYOUT_LEN=1024\n\n# Length of arena's signature\nARENA_SIG_LEN=16\n\n# Signature of BTT Arena\nARENA_SIG=\"BTT_ARENA_INFO\"\n\n# Offset to first arena\nARENA_OFF=8192\n\n#\n# check_file -- check if file exists and print error message if not\n#\ncheck_file()\n{\n\tif [ ! -f $1 ]\n\tthen\n\t\tfatal \"Missing file: ${1}\"\n\tfi\n}\n\n#\n# check_files -- check if files exist and print error message if not\n#\ncheck_files()\n{\n\tfor file in $*\n\tdo\n\t\tcheck_file $file\n\tdone\n}\n\n#\n# check_no_file -- check if file has been deleted and print error message if not\n#\ncheck_no_file()\n{\n\tif [ -f $1 ]\n\tthen\n\t\tfatal \"Not deleted file: ${1}\"\n\tfi\n}\n\n#\n# check_no_files -- check if files has been deleted and print error message if not\n#\ncheck_no_files()\n{\n\tfor file in $*\n\tdo\n\t\tcheck_no_file $file\n\tdone\n}\n\n#\n# get_size -- return size of file (0 if file does not exist)\n#\nget_size()\n{\n\tif [ ! -f $1 ]; then\n\t\techo \"0\"\n\telse\n\t\tstat $STAT_SIZE $1\n\tfi\n}\n\n#\n# get_mode -- return mode of file\n#\nget_mode()\n{\n\tstat $STAT_MODE $1\n}\n\n#\n# check_size -- validate file size\n#\ncheck_size()\n{\n\tlocal size=$1\n\tlocal file=$2\n\tlocal file_size=$(get_size $file)\n\n\tif [[ $size != $file_size ]]\n\tthen\n\t\tfatal \"error: wrong size ${file_size} != ${size}\"\n\tfi\n}\n\n#\n# check_mode -- validate file mode\n#\ncheck_mode()\n{\n\tlocal mode=$1\n\tlocal file=$2\n\tlocal file_mode=$(get_mode $file)\n\n\tif [[ $mode != $file_mode ]]\n\tthen\n\t\tfatal \"error: wrong mode ${file_mode} != ${mode}\"\n\tfi\n}\n\n#\n# check_signature -- check if file contains specified signature\n#\ncheck_signature()\n{\n\tlocal sig=$1\n\tlocal file=$2\n\tlocal file_sig=$($DD if=$file bs=1 count=$SIG_LEN 2>/dev/null | tr -d \\\\0)\n\n\tif [[ $sig != $file_sig ]]\n\tthen\n\t\tfatal \"error: $file: signature doesn't match ${file_sig} != ${sig}\"\n\tfi\n}\n\n#\n# check_signatures -- check if multiple files contain specified signature\n#\ncheck_signatures()\n{\n\tlocal sig=$1\n\tshift 1\n\tfor file in $*\n\tdo\n\t\tcheck_signature $sig $file\n\tdone\n}\n\n#\n# check_layout -- check if pmemobj pool contains specified layout\n#\ncheck_layout()\n{\n\tlocal layout=$1\n\tlocal file=$2\n\tlocal file_layout=$($DD if=$file bs=1\\\n\t\tskip=$LAYOUT_OFFSET count=$LAYOUT_LEN 2>/dev/null | tr -d \\\\0)\n\n\tif [[ $layout != $file_layout ]]\n\tthen\n\t\tfatal \"error: layout doesn't match ${file_layout} != ${layout}\"\n\tfi\n}\n\n#\n# check_arena -- check if file contains specified arena signature\n#\ncheck_arena()\n{\n\tlocal file=$1\n\tlocal sig=$($DD if=$file bs=1 skip=$ARENA_OFF count=$ARENA_SIG_LEN 2>/dev/null | tr -d \\\\0)\n\n\tif [[ $sig != $ARENA_SIG ]]\n\tthen\n\t\tfatal \"error: can't find arena signature\"\n\tfi\n}\n\n#\n# dump_pool_info -- dump selected pool metadata and/or user data\n#\nfunction dump_pool_info() {\n\t# ignore selected header fields that differ by definition\n\t${PMEMPOOL}.static-nondebug info $* | sed -e \"/^UUID/,/^Checksum/d\"\n}\n\n#\n# compare_replicas -- check replicas consistency by comparing `pmempool info` output\n#\nfunction compare_replicas() {\n\tdisable_exit_on_error\n\tdiff <(dump_pool_info $1 $2) <(dump_pool_info $1 $3) -I \"^path\" -I \"^size\"\n\trestore_exit_on_error\n}\n\n#\n# get_node_dir -- returns node dir for current test\n#    usage: get_node_dir <node>\n#\nfunction get_node_dir() {\n\tvalidate_node_number $1\n\techo ${NODE_WORKING_DIR[$1]}/$curtestdir\n}\n\n#\n# init_rpmem_on_node -- prepare rpmem environment variables on node\n#    usage: init_rpmem_on_node <master-node> <slave-node-1> [<slave-node-2> ...]\n#\n# example:\n#    The following command initialize rpmem environment variables on the node 1\n#    to perform replication to node 0, node 2 and node 3.\n#    Additionally:\n#    - on node 2 rpmemd pid will be stored in file.pid\n#    - on node 3 no pid file will be created (SKIP) and rpmemd will use\n#      file.conf config file\n#\n#       init_rpmem_on_node 1 0 2:file.pid 3:SKIP:file.conf\n#\nfunction init_rpmem_on_node() {\n\tlocal master=$1\n\tshift\n\n\tvalidate_node_number $master\n\n\tcase \"$RPMEM_PM\" in\n\tAPM|GPSPM)\n\t\t;;\n\t*)\n\t\tmsg \"$UNITTEST_NAME: SKIP required: RPMEM_PM is invalid or empty\"\n\t\texit 0\n\t\t;;\n\tesac\n\n\t# Workaround for SIGSEGV in the infinipath-psm during abort\n\t# The infinipath-psm is registering a signal handler and do not unregister\n\t# it when rpmem handle is dlclosed. SIGABRT (potentially any other signal)\n\t# would try to call the signal handler which does not exist after dlclose.\n\t# Issue require a fix in the infinipath-psm or the libfabric.\n\tIPATH_NO_BACKTRACE=1\n\texport_vars_node $master IPATH_NO_BACKTRACE\n\n\tRPMEM_CMD=\"\"\n\tlocal SEPARATOR=\"|\"\n\tfor slave in \"$@\"\n\tdo\n\t\tslave=(${slave//:/ })\n\t\tconf=${slave[2]}\n\t\tpid=${slave[1]}\n\t\tslave=${slave[0]}\n\n\t\tvalidate_node_number $slave\n\t\tlocal poolset_dir=${NODE_DIR[$slave]}\n\t\tif [ -n \"${RPMEM_POOLSET_DIR[$slave]}\" ]; then\n\t\t\tpoolset_dir=${RPMEM_POOLSET_DIR[$slave]}\n\t\tfi\n\t\tlocal trace=\n\t\tif [ -n \"$(is_valgrind_enabled_on_node $slave)\" ]; then\n\t\t\tlog_file=${CHECK_TYPE}${UNITTEST_NUM}.log\n\t\t\ttrace=$(get_trace $CHECK_TYPE $log_file $slave)\n\t\tfi\n\t\tif [ -n \"$pid\" -a \"$pid\" != \"SKIP\" ]; then\n\t\t\ttrace=\"$trace ../ctrld $pid exe\"\n\t\tfi\n\t\tif [ -n ${UNITTEST_DO_NOT_CHECK_OPEN_FILES+x} ]; then\n\t\t\texport_vars_node $slave UNITTEST_DO_NOT_CHECK_OPEN_FILES\n\t\tfi\n\t\tif [ -n ${IPATH_NO_BACKTRACE+x} ]; then\n\t\t\texport_vars_node $slave IPATH_NO_BACKTRACE\n\t\tfi\n\t\tCMD=\"cd ${NODE_TEST_DIR[$slave]} && \"\n\n\t\t# Force pmem for APM. Otherwise in case of lack of a pmem rpmemd will\n\t\t# silently fallback to GPSPM.\n\t\t[ \"$RPMEM_PM\" == \"APM\" ] && CMD=\"$CMD PMEM_IS_PMEM_FORCE=1\"\n\n\t\tCMD=\"$CMD ${NODE_ENV[$slave]}\"\n\t\tCMD=\"$CMD LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:$REMOTE_LD_LIBRARY_PATH:${NODE_LD_LIBRARY_PATH[$slave]}\"\n\t\tCMD=\"$CMD $trace ../rpmemd\"\n\t\tCMD=\"$CMD --log-file=$RPMEMD_LOG_FILE\"\n\t\tCMD=\"$CMD --log-level=$RPMEMD_LOG_LEVEL\"\n\t\tCMD=\"$CMD --poolset-dir=$poolset_dir\"\n\n\t\tif [ -n \"$conf\" ]; then\n\t\t\tCMD=\"$CMD --config=$conf\"\n\t\tfi\n\n\t\tif [ \"$RPMEM_PM\" == \"APM\" ]; then\n\t\t\tCMD=\"$CMD --persist-apm\"\n\t\tfi\n\n\t\tif [ \"$RPMEM_CMD\" ]; then\n\t\t\tRPMEM_CMD=\"$RPMEM_CMD$SEPARATOR$CMD\"\n\t\telse\n\t\t\tRPMEM_CMD=$CMD\n\t\tfi\n\n\t\trequire_node_log_files $slave rpmemd$UNITTEST_NUM.log\n\tdone\n\tRPMEM_CMD=\"\\\"$RPMEM_CMD\\\"\"\n\n\tRPMEM_ENABLE_SOCKETS=0\n\tRPMEM_ENABLE_VERBS=0\n\n\tcase \"$RPMEM_PROVIDER\" in\n\tsockets)\n\t\tRPMEM_ENABLE_SOCKETS=1\n\t\t;;\n\tverbs)\n\t\tRPMEM_ENABLE_VERBS=1\n\t\t;;\n\t*)\n\t\tmsg \"$UNITTEST_NAME: SKIP required: RPMEM_PROVIDER is invalid or empty\"\n\t\texit 0\n\t\t;;\n\tesac\n\n\texport_vars_node $master RPMEM_CMD\n\texport_vars_node $master RPMEM_ENABLE_SOCKETS\n\texport_vars_node $master RPMEM_ENABLE_VERBS\n\n\tif [ -n ${UNITTEST_DO_NOT_CHECK_OPEN_FILES+x} ]; then\n\t\texport_vars_node $master UNITTEST_DO_NOT_CHECK_OPEN_FILES\n\tfi\n\tif [ -n ${PMEMOBJ_NLANES+x} ]; then\n\t\texport_vars_node $master PMEMOBJ_NLANES\n\tfi\n\tif [ -n ${RPMEM_MAX_NLANES+x} ]; then\n\t\texport_vars_node $master RPMEM_MAX_NLANES\n\tfi\n\n\trequire_node_log_files $master rpmem$UNITTEST_NUM.log\n\trequire_node_log_files $master $PMEMOBJ_LOG_FILE\n}\n\n#\n# init_valgrind_on_node -- prepare valgrind on nodes\n#    usage: init_valgrind_on_node <node list>\n#\nfunction init_valgrind_on_node() {\n\t# When librpmem is preloaded libfabric does not close all opened files\n\t# before list of opened files is checked.\n\tlocal UNITTEST_DO_NOT_CHECK_OPEN_FILES=1\n\tlocal LD_PRELOAD=../$BUILD/librpmem.so\n\tCHECK_NODES=\"\"\n\n\tfor node in \"$@\"\n\tdo\n\t\tvalidate_node_number $node\n\t\texport_vars_node $node LD_PRELOAD\n\t\texport_vars_node $node UNITTEST_DO_NOT_CHECK_OPEN_FILES\n\t\tCHECK_NODES=\"$CHECK_NODES $node\"\n\tdone\n}\n\n#\n# is_valgrind_enabled_on_node -- echo the node number if the node has\n#                                initialized valgrind environment by calling\n#                                init_valgrind_on_node\n#    usage: is_valgrind_enabled_on_node <node>\n#\nfunction is_valgrind_enabled_on_node() {\n\tfor node in $CHECK_NODES\n\tdo\n\t\tif [ \"$node\" -eq \"$1\" ]; then\n\t\t\techo $1\n\t\t\treturn\n\t\tfi\n\tdone\n\treturn\n}\n\n#\n# pack_all_libs -- put all libraries and their links to one tarball\n#\nfunction pack_all_libs() {\n\tlocal LIBS_TAR_DIR=$(pwd)/$1\n\tcd $DIR_SRC\n\ttar -cf $LIBS_TAR_DIR ./debug/*.so* ./nondebug/*.so*\n\tcd - > /dev/null\n}\n\n#\n# copy_common_to_remote_nodes -- copy common files to all remote nodes\n#\nfunction copy_common_to_remote_nodes() {\n\n\tlocal NODES_ALL_MAX=$((${#NODE[@]} - 1))\n\tlocal NODES_ALL_SEQ=$(seq -s' ' 0 $NODES_ALL_MAX)\n\n\tDIR_SYNC=$1\n\tif [ \"$DIR_SYNC\" != \"\" ]; then\n\t\t[ ! -d $DIR_SYNC ] \\\n\t\t&& fatal \"error: $DIR_SYNC does not exist or is not a directory\"\n\tfi\n\n\t# add all libraries to the 'to-copy' list\n\tlocal LIBS_TAR=libs.tar\n\tpack_all_libs $LIBS_TAR\n\n\tif [ \"$DIR_SYNC\" != \"\" -a \"$(ls $DIR_SYNC)\" != \"\" ]; then\n\t\tFILES_COMMON_DIR=\"$DIR_SYNC/* $LIBS_TAR\"\n\telse\n\t\tFILES_COMMON_DIR=\"$FILES_COMMON_DIR $LIBS_TAR\"\n\tfi\n\n\tfor N in $NODES_ALL_SEQ; do\n\t\t# validate node's address\n\t\t[ \"${NODE[$N]}\" = \"\" ] \\\n\t\t\t&& fatal \"error: address of node #$N is not provided\"\n\n\t\tcheck_if_node_is_reachable $N\n\t\t[ $? -ne 0 ] \\\n\t\t\t&& msg \"warning: node #$N (${NODE[$N]}) is unreachable, skipping...\" \\\n\t\t\t&& continue\n\n\t\t# validate the working directory\n\t\t[ \"${NODE_WORKING_DIR[$N]}\" = \"\" ] \\\n\t\t\t&& msg \": warning: working directory for node #$N (${NODE[$N]}) is not provided, skipping...\" \\\n\t\t\t&& continue\n\n\t\t# create the working dir if it does not exist\n\t\trun_command ssh $SSH_OPTS ${NODE[$N]} \"mkdir -p ${NODE_WORKING_DIR[$N]}\"\n\n\t\t# copy all common files\n\t\trun_command scp $SCP_OPTS $FILES_COMMON_DIR ${NODE[$N]}:${NODE_WORKING_DIR[$N]} > /dev/null\n\t\t# unpack libraries\n\t\trun_command ssh $SSH_OPTS ${NODE[$N]} \"cd ${NODE_WORKING_DIR[$N]} \\\n\t\t\t&& tar -xf $LIBS_TAR && rm -f $LIBS_TAR\"\n\tdone\n\n\trm -f $LIBS_TAR\n}\n\n#\n# copy_test_to_remote_nodes -- copy all unit test binaries to all remote nodes\n#\nfunction copy_test_to_remote_nodes() {\n\n\tlocal NODES_ALL_MAX=$((${#NODE[@]} - 1))\n\tlocal NODES_ALL_SEQ=$(seq -s' ' 0 $NODES_ALL_MAX)\n\n\tfor N in $NODES_ALL_SEQ; do\n\t\t# validate node's address\n\t\t[ \"${NODE[$N]}\" = \"\" ] \\\n\t\t\t&& fatal \"error: address of node #$N is not provided\"\n\n\t\tcheck_if_node_is_reachable $N\n\t\t[ $? -ne 0 ] \\\n\t\t\t&& msg \"warning: node #$N (${NODE[$N]}) is unreachable, skipping...\" \\\n\t\t\t&& continue\n\n\t\t# validate the working directory\n\t\t[ \"${NODE_WORKING_DIR[$N]}\" = \"\" ] \\\n\t\t\t&& msg \": warning: working directory for node #$N (${NODE[$N]}) is not provided, skipping...\" \\\n\t\t\t&& continue\n\n\t\tlocal DIR=${NODE_WORKING_DIR[$N]}/$curtestdir\n\n\t\t# create a new test dir\n\t\trun_command ssh $SSH_OPTS ${NODE[$N]} \"rm -rf $DIR && mkdir -p $DIR\"\n\n\t\t# create the working data dir\n\t\trun_command ssh $SSH_OPTS ${NODE[$N]} \"mkdir -p \\\n\t\t\t${DIR}/data\"\n\n\t\t# copy all required files\n\t\t[ $# -gt 0 ] && run_command scp $SCP_OPTS $* ${NODE[$N]}:$DIR > /dev/null\n\tdone\n\n\treturn 0\n}\n\n#\n# enable_log_append -- turn on appending to the log files rather than truncating them\n# It also removes all log files created by tests: out*.log, err*.log and trace*.log\n#\nfunction enable_log_append() {\n\trm -f $OUT_LOG_FILE\n\trm -f $ERR_LOG_FILE\n\trm -f $TRACE_LOG_FILE\n\texport UNITTEST_LOG_APPEND=1\n}\n\n# clean data directory on all remote\n# nodes if remote test failed\nif [ \"$CLEAN_FAILED_REMOTE\" == \"y\" ]; then\n\tNODES_ALL=$((${#NODE[@]} - 1))\n\tMYPID=$$\n\n\tfor ((i=0;i<=$NODES_ALL;i++));\n\tdo\n\n\t\tif [[ -z \"${NODE_WORKING_DIR[$i]}\" || -z \"$curtestdir\" ]]; then\n\t\t\techo \"Invalid path to tests data: ${NODE_WORKING_DIR[$i]}/$curtestdir/data/\"\n\t\t\texit 1\n\t\tfi\n\n\t\tN[$i]=${NODE_WORKING_DIR[$i]}/$curtestdir/data/\n\t\trun_command ssh $SSH_OPTS ${NODE[$i]} \"rm -rf ${N[$i]}; mkdir ${N[$i]}\"\n\n\t\tif [ $? -eq 0 ]; then\n\t\t\tverbose_msg \"Removed data from: ${NODE[$i]}:${N[$i]}\"\n\t\tfi\n\tdone\n\texit 0\nfi\n\n# calculate the minimum of two or more numbers\nminimum() {\n\tlocal min=$1\n\tshift\n\tfor val in $*; do\n\t\tif [[ \"$val\" < \"$min\" ]]; then\n\t\t\tmin=$val\n\t\tfi\n\tdone\n\techo $min\n}\n\n#\n# count_lines - count number of lines that match pattern $1 in file $2\n#\nfunction count_lines() {\n\t# grep returns 1 on no match\n\tdisable_exit_on_error\n\t$GREP -ce \"$1\" $2\n\trestore_exit_on_error\n}\n\n#\n# get_pmemcheck_version() - return pmemcheck API major or minor version\n#\tusage: get_pmemcheck_version <0|1>\n#\nfunction get_pmemcheck_version()\n{\n\tPMEMCHECK_VERSION=$($VALGRINDEXE --tool=pmemcheck true 2>&1 \\\n\t\t\t| head -n 1 | sed \"s/.*-\\([0-9.]*\\),.*/\\1/\")\n\n\tOIFS=$IFS\n\tIFS=\".\"\n\tPMEMCHECK_MAJ_MIN=($PMEMCHECK_VERSION)\n\tIFS=$OIFS\n\tPMEMCHECK_VERSION_PART=${PMEMCHECK_MAJ_MIN[$1]}\n\n\techo \"$PMEMCHECK_VERSION_PART\"\n}\n\n#\n# require_pmemcheck_version_ge - check if pmemcheck API\n# version is greater or equal to required value\n#\tusage: require_pmemcheck_version_ge <major> <minor>\n#\nfunction require_pmemcheck_version_ge()\n{\n\trequire_valgrind_tool pmemcheck\n\n\tREQUIRE_MAJOR=$1\n\tREQUIRE_MINOR=$2\n\tPMEMCHECK_MAJOR=$(get_pmemcheck_version 0)\n\tPMEMCHECK_MINOR=$(get_pmemcheck_version 1)\n\n\t# compare MAJOR\n\tif [ $PMEMCHECK_MAJOR -gt $REQUIRE_MAJOR ]; then\n\t\treturn 0\n\tfi\n\n\t# compare MINOR\n\tif [ $PMEMCHECK_MAJOR -eq $REQUIRE_MAJOR ]; then\n\t\tif [ $PMEMCHECK_MINOR -ge $REQUIRE_MINOR ]; then\n\t\t\treturn 0\n\t\tfi\n\tfi\n\n\tmsg \"$UNITTEST_NAME: SKIP pmemcheck API version:\" \\\n\t\t\"$PMEMCHECK_MAJOR.$PMEMCHECK_MINOR\" \\\n\t\t\"is less than required\" \\\n\t\t\"$REQUIRE_MAJOR.$REQUIRE_MINOR\"\n\n\texit 0\n}\n\n#\n# require_pmemcheck_version_lt - check if pmemcheck API\n# version is less than required value\n#\tusage: require_pmemcheck_version_lt <major> <minor>\n#\nfunction require_pmemcheck_version_lt()\n{\n\trequire_valgrind_tool pmemcheck\n\n\tREQUIRE_MAJOR=$1\n\tREQUIRE_MINOR=$2\n\tPMEMCHECK_MAJOR=$(get_pmemcheck_version 0)\n\tPMEMCHECK_MINOR=$(get_pmemcheck_version 1)\n\n\t# compare MAJOR\n\tif [ $PMEMCHECK_MAJOR -lt $REQUIRE_MAJOR ]; then\n\t\treturn 0\n\tfi\n\n\t# compare MINOR\n\tif [ $PMEMCHECK_MAJOR -eq $REQUIRE_MAJOR ]; then\n\t\tif [ $PMEMCHECK_MINOR -lt $REQUIRE_MINOR ]; then\n\t\t\treturn 0\n\t\tfi\n\tfi\n\n\tmsg \"$UNITTEST_NAME: SKIP pmemcheck API version:\" \\\n\t\t\"$PMEMCHECK_MAJOR.$PMEMCHECK_MINOR\" \\\n\t\t\"is greater or equal than\" \\\n\t\t\"$REQUIRE_MAJOR.$REQUIRE_MINOR\"\n\n\texit 0\n}\n\n#\n# require_python_3 -- check if python3 is available\n#\nfunction require_python3()\n{\n\tif hash python3 &>/dev/null;\n\tthen\n\t\tPYTHON_EXE=python3\n\telse\n\t\tPYTHON_EXE=python\n\tfi\n\n\tcase \"$($PYTHON_EXE --version 2>&1)\" in\n\t    *\" 3.\"*)\n\t\treturn\n\t\t;;\n\t    *)\n\t\tmsg \"$UNITTEST_NAME: SKIP: required python version 3\"\n\t\texit 0\n\t\t;;\n\tesac\n}\n\n#\n# require_pmreorder -- check all necessarily conditions to run pmreorder\n#\nfunction require_pmreorder()\n{\n\t# python3 and valgrind are necessary\n\trequire_python3\n\t# pmemcheck is required to generate store_log\n\tconfigure_valgrind pmemcheck force-enable\n\t# pmreorder tool does not support unicode yet\n\trequire_no_unicode\n}\n\n#\n# pmreorder_run_tool -- run pmreorder with parameters and return exit status\n#\n# 1 - reorder engine type [nochecker|full|noreorder|partial|accumulative]\n# 2 - marker-engine pairs in format: MARKER=ENGINE,MARKER1=ENGINE1 or\n#     config file in json format: { \"MARKER\":\"ENGINE\",\"MARKER1\":\"ENGINE1\" }\n# 3 - the path to the checker binary/library and  remaining parameters which\n#     will be passed to the consistency checker binary.\n#     If you are using a library checker, prepend '-n funcname'\n#\nfunction pmreorder_run_tool()\n{\n\trm -f pmreorder$UNITTEST_NUM.log\n\tdisable_exit_on_error\n\tLD_LIBRARY_PATH=$TEST_LD_LIBRARY_PATH $PYTHON_EXE $PMREORDER \\\n\t\t-l store_log$UNITTEST_NUM.log \\\n\t\t-o pmreorder$UNITTEST_NUM.log \\\n\t\t-r $1 \\\n\t\t-x $2 \\\n\t\t-p \"$3\"\n\tret=$?\n\trestore_exit_on_error\n\techo $ret\n}\n\n#\n# pmreorder_expect_success -- run pmreoreder with forwarded parameters,\n#\t\t\t\texpect it to exit zero\n#\nfunction pmreorder_expect_success()\n{\n\tret=$(pmreorder_run_tool \"$@\")\n\n\tif [ \"$ret\" -ne \"0\" ]; then\n\t\tmsg=\"failed with exit code $ret\"\n\t\t[ -t 2 ] && command -v tput >/dev/null && msg=\"$(tput setaf 1)$msg$(tput sgr0)\"\n\n\t\t# exit code 130 - script terminated by user (Control-C)\n\t\tif [ \"$ret\" -ne \"130\" ]; then\n\n\t\t\techo -e \"$UNITTEST_NAME $msg.\" >&2\n\t\t\tdump_last_n_lines $PMREORDER_LOG_FILE\n\t\tfi\n\n\t\tfalse\n\tfi\n}\n\n#\n# pmreorder_expect_failure -- run pmreoreder with forwarded parameters,\n#\t\t\t\texpect it to exit non zero\n#\nfunction pmreorder_expect_failure()\n{\n\tret=$(pmreorder_run_tool \"$@\")\n\n\tif [ \"$ret\" -eq \"0\" ]; then\n\t\tmsg=\"succeeded\"\n\t\t[ -t 2 ] && command -v tput >/dev/null && msg=\"$(tput setaf 1)$msg$(tput sgr0)\"\n\n\t\techo -e \"$UNITTEST_NAME command $msg unexpectedly.\" >&2\n\n\t\tfalse\n\tfi\n}\n\n#\n# pmreorder_create_store_log -- perform a reordering test\n#\n# This function expects 5 additional parameters. They are in order:\n# 1 - the pool file to be tested\n# 2 - the application and necessary parameters to run pmemcheck logging\n#\nfunction pmreorder_create_store_log()\n{\n\t#copy original file and perform store logging\n\tcp $1 \"$1.pmr\"\n\trm -f store_log$UNITTEST_NUM.log\n\n\tLD_LIBRARY_PATH=$TEST_LD_LIBRARY_PATH $VALGRINDEXE \\\n\t\t\t--tool=pmemcheck -q \\\n\t\t\t--log-stores=yes \\\n\t\t\t--print-summary=no \\\n\t\t\t--log-file=store_log$UNITTEST_NUM.log \\\n\t\t\t--log-stores-stacktraces=yes \\\n\t\t\t--log-stores-stacktraces-depth=2 \\\n\t\t\t--expect-fence-after-clflush=yes \\\n\t\t\t$2\n\n\t# uncomment this line for debug purposes\n\t# mv $1 \"$1.bak\"\n\tmv \"$1.pmr\" $1\n}\n\n#\n# require_free_space -- check if there is enough free space to run the test\n# Example, checking if there is 1 GB of free space on disk:\n# require_free_space 1G\n#\nfunction require_free_space() {\n\treq_free_space=$(convert_to_bytes $1)\n\toutput=$(df -k $DIR)\n\tfound=false\n\ti=1\n\tfor elem in $(echo \"$output\" | head -1); do\n\t\tif [ ${elem:0:5} == \"Avail\" ]; then\n\t\t\tfound=true\n\t\t\tbreak\n\t\telse\n\t\t\tlet \"i+=1\"\n\t\tfi\n\tdone\n\tif [ $found = true ]; then\n\t\trow=$(echo \"$output\" | tail -1)\n\t\tfree_space=$(( $(echo $row | awk \"{print \\$$i}\")*1024 ))\n\telse\n\t\tmsg \"$UNITTEST_NAME: SKIP: unable to check free space\"\n\t\texit 0\n\tfi\n\tif [ $free_space -lt $req_free_space ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: not enough free space ($1 required)\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_max_devdax_size -- checks that dev dax is smaller than requested\n#\n# usage: require_max_devdax_size <dev-dax-num> <max-size>\n#\nfunction require_max_devdax_size() {\n\tcur_sz=$(get_devdax_size 0)\n\tmax_size=$2\n\tif [ $cur_sz -ge $max_size ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: DevDAX $1 is too big for this test (max $2 required)\"\n\t\texit 0\n\tfi\n}\n\n#\n# require_nfit_tests_enabled - check if tests using the nfit_test kernel module are not enabled\n#\nfunction require_nfit_tests_enabled() {\n\tif [ \"$ENABLE_NFIT_TESTS\" != \"y\" ]; then\n\t\tmsg \"$UNITTEST_NAME: SKIP: tests using the nfit_test kernel module are not enabled in testconfig.sh (ENABLE_NFIT_TESTS)\"\n\t\texit 0\n\tfi\n}\n\n#\n# create_recovery_file - create bad block recovery file\n#\n# Usage: create_recovery_file <file> [<offset_1> <length_1> ...]\n#\nfunction create_recovery_file() {\n\t[ $# -lt 1 ] && fatal \"create_recovery_file(): not enough parameters: $*\"\n\n\tFILE=$1\n\tshift\n\trm -f $FILE\n\n\twhile [ $# -ge 2 ]; do\n\t\tOFFSET=$1\n\t\tLENGTH=$2\n\t\tshift 2\n\t\techo \"$(($OFFSET * 512)) $(($LENGTH * 512))\" >> $FILE\n\tdone\n\n\t# write the finish flag\n\techo \"0 0\" >> $FILE\n}\n\n#\n# zero_blocks - zero blocks in a file\n#\n# Usage: zero_blocks <file> <offset> <length>\n#\nfunction zero_blocks() {\n\t[ $# -lt 3 ] && fatal \"zero_blocks(): not enough parameters: $*\"\n\n\tFILE=$1\n\tshift\n\n\twhile [ $# -ge 2 ]; do\n\t\tOFFSET=$1\n\t\tLENGTH=$2\n\t\tshift 2\n\t\tdd if=/dev/zero of=$FILE bs=512 seek=$OFFSET count=$LENGTH conv=notrunc status=none\n\tdone\n}\n\n#\n# turn_on_checking_bad_blocks -- set the compat_feature POOL_FEAT_CHECK_BAD_BLOCKS on\n#\nfunction turn_on_checking_bad_blocks()\n{\n\tFILE=$1\n\n\texpect_normal_exit \"$PMEMPOOL feature -e CHECK_BAD_BLOCKS $FILE &>> $PREP_LOG_FILE\"\n}\n\n#\n# turn_on_checking_bad_blocks_node -- set the compat_feature POOL_FEAT_CHECK_BAD_BLOCKS on\n#\nfunction turn_on_checking_bad_blocks_node()\n{\n\tFILE=$2\n\n\texpect_normal_exit run_on_node $1 \"../pmempool feature -e CHECK_BAD_BLOCKS $FILE &>> $PREP_LOG_FILE\"\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/jemalloc/src/jemalloc.c": "#define\tJEMALLOC_C_\n#include \"jemalloc/internal/jemalloc_internal.h\"\n\n/******************************************************************************/\n/* Data. */\nmalloc_tsd_data(, arenas, tsd_pool_t, TSD_POOL_INITIALIZER)\nmalloc_tsd_data(, thread_allocated, thread_allocated_t,\n    THREAD_ALLOCATED_INITIALIZER)\n\n/* Runtime configuration options. */\nconst char\t*je_malloc_conf;\nbool\topt_abort =\n#ifdef JEMALLOC_DEBUG\n    true\n#else\n    false\n#endif\n    ;\nbool\topt_junk =\n#if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))\n    true\n#else\n    false\n#endif\n    ;\nsize_t\topt_quarantine = ZU(0);\nbool\topt_redzone = false;\nbool\topt_utrace = false;\nbool\topt_xmalloc = false;\nbool\topt_zero = false;\nsize_t\topt_narenas = 0;\n\n/* Initialized to true if the process is running inside Valgrind. */\nbool\tin_valgrind;\n\n\nunsigned\tnpools_cnt;\t/* actual number of pools */\nunsigned\tnpools; \t/* size of the pools[] array */\nunsigned\tncpus;\n\npool_t\t\t**pools;\npool_t\t\tbase_pool;\nunsigned\tpool_seqno = 0;\nbool\t\tpools_shared_data_initialized;\n\n/*\n * Custom malloc() and free() for shared data and for data needed to\n * initialize pool. If not defined functions then base_pool will be\n * created for allocations from RAM.\n */\nvoid\t*(*base_malloc_fn)(size_t);\nvoid\t(*base_free_fn)(void *);\n\n/* Set to true once the allocator has been initialized. */\nstatic bool\t\tmalloc_initialized = false;\nstatic bool\t\tbase_pool_initialized = false;\n\n#ifdef JEMALLOC_THREADED_INIT\n/* Used to let the initializing thread recursively allocate. */\n#  define NO_INITIALIZER\t((unsigned long)0)\n#  define INITIALIZER\t\tpthread_self()\n#  define IS_INITIALIZER\t(malloc_initializer == pthread_self())\nstatic pthread_t\t\tmalloc_initializer = NO_INITIALIZER;\n#else\n#  define NO_INITIALIZER\tfalse\n#  define INITIALIZER\t\ttrue\n#  define IS_INITIALIZER\tmalloc_initializer\nstatic bool\t\t\tmalloc_initializer = NO_INITIALIZER;\n#endif\n\n/* Used to avoid initialization races. */\n#ifdef _WIN32\nstatic malloc_mutex_t\tinit_lock;\n\nJEMALLOC_ATTR(constructor)\nstatic void WINAPI\n_init_init_lock(void)\n{\n\n\tmalloc_mutex_init(&init_lock);\n\tmalloc_mutex_init(&pools_lock);\n\tmalloc_mutex_init(&pool_base_lock);\n}\n\n#ifdef _MSC_VER\n# pragma comment(linker, \"/include:__init_init_lock\")\n# pragma section(\".CRT$XCU\", read)\nJEMALLOC_SECTION(\".CRT$XCU\") JEMALLOC_ATTR(used)\nconst void (WINAPI *__init_init_lock)(void) = _init_init_lock;\n#endif\n\n#else\nstatic malloc_mutex_t\tinit_lock = MALLOC_MUTEX_INITIALIZER;\n#endif\n\ntypedef struct {\n\tvoid\t*p;\t/* Input pointer (as in realloc(p, s)). */\n\tsize_t\ts;\t/* Request size. */\n\tvoid\t*r;\t/* Result pointer. */\n} malloc_utrace_t;\n\n#ifdef JEMALLOC_UTRACE\n#  define UTRACE(a, b, c) do {\t\t\t\t\t\t\\\n\tif (opt_utrace) {\t\t\t\t\t\t\\\n\t\tint utrace_serrno = errno;\t\t\t\t\\\n\t\tmalloc_utrace_t ut;\t\t\t\t\t\\\n\t\tut.p = (a);\t\t\t\t\t\t\\\n\t\tut.s = (b);\t\t\t\t\t\t\\\n\t\tut.r = (c);\t\t\t\t\t\t\\\n\t\tutrace(&ut, sizeof(ut));\t\t\t\t\\\n\t\terrno = utrace_serrno;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n#else\n#  define UTRACE(a, b, c) do { (void)(a); (void)(b); (void)(c); } while (0)\n#endif\n\n/* data structures for callbacks used in je_pool_check() to browse trees */\ntypedef struct {\n\tpool_memory_range_node_t *list;\n\tsize_t size;\n\tint error;\n} check_data_cb_t;\n\n/******************************************************************************/\n/*\n * Function prototypes for static functions that are referenced prior to\n * definition.\n */\n\nstatic bool\tmalloc_init_hard(void);\nstatic bool\tmalloc_init_base_pool(void);\nstatic void\t*base_malloc_default(size_t size);\nstatic void\tbase_free_default(void *ptr);\n\n/******************************************************************************/\n/*\n * Begin miscellaneous support functions.\n */\n\n/* Create a new arena and insert it into the arenas array at index ind. */\narena_t *\narenas_extend(pool_t *pool, unsigned ind)\n{\n\tarena_t *ret;\n\n\tret = (arena_t *)base_alloc(pool, sizeof(arena_t));\n\tif (ret != NULL && arena_new(pool, ret, ind) == false) {\n\t\tpool->arenas[ind] = ret;\n\t\treturn (ret);\n\t}\n\t/* Only reached if there is an OOM error. */\n\n\t/*\n\t * OOM here is quite inconvenient to propagate, since dealing with it\n\t * would require a check for failure in the fast path.  Instead, punt\n\t * by using arenas[0].  In practice, this is an extremely unlikely\n\t * failure.\n\t */\n\tmalloc_write(\"<jemalloc>: Error initializing arena\\n\");\n\tif (opt_abort)\n\t\tabort();\n\n\treturn (pool->arenas[0]);\n}\n\n/* Slow path, called only by choose_arena(). */\narena_t *\nchoose_arena_hard(pool_t *pool)\n{\n\tarena_t *ret;\n\ttsd_pool_t *tsd;\n\n\tif (pool->narenas_auto > 1) {\n\t\tunsigned i, choose, first_null;\n\n\t\tchoose = 0;\n\t\tfirst_null = pool->narenas_auto;\n\t\tmalloc_rwlock_wrlock(&pool->arenas_lock);\n\t\tassert(pool->arenas[0] != NULL);\n\t\tfor (i = 1; i < pool->narenas_auto; i++) {\n\t\t\tif (pool->arenas[i] != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Choose the first arena that has the lowest\n\t\t\t\t * number of threads assigned to it.\n\t\t\t\t */\n\t\t\t\tif (pool->arenas[i]->nthreads <\n\t\t\t\t    pool->arenas[choose]->nthreads)\n\t\t\t\t\tchoose = i;\n\t\t\t} else if (first_null == pool->narenas_auto) {\n\t\t\t\t/*\n\t\t\t\t * Record the index of the first uninitialized\n\t\t\t\t * arena, in case all extant arenas are in use.\n\t\t\t\t *\n\t\t\t\t * NB: It is possible for there to be\n\t\t\t\t * discontinuities in terms of initialized\n\t\t\t\t * versus uninitialized arenas, due to the\n\t\t\t\t * \"thread.arena\" mallctl.\n\t\t\t\t */\n\t\t\t\tfirst_null = i;\n\t\t\t}\n\t\t}\n\n\t\tif (pool->arenas[choose]->nthreads == 0\n\t\t    || first_null == pool->narenas_auto) {\n\t\t\t/*\n\t\t\t * Use an unloaded arena, or the least loaded arena if\n\t\t\t * all arenas are already initialized.\n\t\t\t */\n\t\t\tret = pool->arenas[choose];\n\t\t} else {\n\t\t\t/* Initialize a new arena. */\n\t\t\tret = arenas_extend(pool, first_null);\n\t\t}\n\t\tret->nthreads++;\n\t\tmalloc_rwlock_unlock(&pool->arenas_lock);\n\t} else {\n\t\tret = pool->arenas[0];\n\t\tmalloc_rwlock_wrlock(&pool->arenas_lock);\n\t\tret->nthreads++;\n\t\tmalloc_rwlock_unlock(&pool->arenas_lock);\n\t}\n\n\ttsd = arenas_tsd_get();\n\ttsd->seqno[pool->pool_id] = pool->seqno;\n\ttsd->arenas[pool->pool_id] = ret;\n\n\n\treturn (ret);\n}\n\nstatic void\nstats_print_atexit(void)\n{\n\n\tif (config_tcache && config_stats) {\n\t\tunsigned narenas, i, j;\n\t\tpool_t *pool;\n\n\t\t/*\n\t\t * Merge stats from extant threads.  This is racy, since\n\t\t * individual threads do not lock when recording tcache stats\n\t\t * events.  As a consequence, the final stats may be slightly\n\t\t * out of date by the time they are reported, if other threads\n\t\t * continue to allocate.\n\t\t */\n\t\tmalloc_mutex_lock(&pools_lock);\n\t\tfor (i = 0; i < npools; i++) {\n\t\t\tpool = pools[i];\n\t\t\tif (pool != NULL) {\n\t\t\t\tfor (j = 0, narenas = narenas_total_get(pool); j < narenas; j++) {\n\t\t\t\t\tarena_t *arena = pool->arenas[j];\n\t\t\t\t\tif (arena != NULL) {\n\t\t\t\t\t\ttcache_t *tcache;\n\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * tcache_stats_merge() locks bins, so if any\n\t\t\t\t\t\t * code is introduced that acquires both arena\n\t\t\t\t\t\t * and bin locks in the opposite order,\n\t\t\t\t\t\t * deadlocks may result.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tmalloc_mutex_lock(&arena->lock);\n\t\t\t\t\t\tql_foreach(tcache, &arena->tcache_ql, link) {\n\t\t\t\t\t\t\ttcache_stats_merge(tcache, arena);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmalloc_mutex_unlock(&arena->lock);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tmalloc_mutex_unlock(&pools_lock);\n\t}\n\tje_malloc_stats_print(NULL, NULL, NULL);\n}\n\n/*\n * End miscellaneous support functions.\n */\n/******************************************************************************/\n/*\n * Begin initialization functions.\n */\n\nstatic unsigned\nmalloc_ncpus(void)\n{\n\tlong result;\n\n#ifdef _WIN32\n\tSYSTEM_INFO si;\n\tGetSystemInfo(&si);\n\tresult = si.dwNumberOfProcessors;\n#else\n\tresult = sysconf(_SC_NPROCESSORS_ONLN);\n#endif\n\treturn ((result == -1) ? 1 : (unsigned)result);\n}\n\nbool\narenas_tsd_extend(tsd_pool_t *tsd, unsigned len)\n{\n\tassert(len < POOLS_MAX);\n\n\t/* round up the new length to the nearest power of 2... */\n\tsize_t npools = 1ULL << (32 - __builtin_clz(len + 1));\n\n\t/* ... but not less than */\n\tif (npools < POOLS_MIN)\n\t\tnpools = POOLS_MIN;\n\n\tunsigned *tseqno = base_malloc_fn(npools * sizeof (unsigned));\n\tif (tseqno == NULL)\n\t\treturn (true);\n\n\tif (tsd->seqno != NULL)\n\t\tmemcpy(tseqno, tsd->seqno, tsd->npools * sizeof (unsigned));\n\tmemset(&tseqno[tsd->npools], 0, (npools - tsd->npools) * sizeof (unsigned));\n\n\tarena_t **tarenas = base_malloc_fn(npools * sizeof (arena_t *));\n\tif (tarenas == NULL) {\n\t\tbase_free_fn(tseqno);\n\t\treturn (true);\n\t}\n\n\tif (tsd->arenas != NULL)\n\t\tmemcpy(tarenas, tsd->arenas, tsd->npools * sizeof (arena_t *));\n\tmemset(&tarenas[tsd->npools], 0, (npools - tsd->npools) * sizeof (arena_t *));\n\n\tbase_free_fn(tsd->seqno);\n\ttsd->seqno = tseqno;\n\tbase_free_fn(tsd->arenas);\n\ttsd->arenas = tarenas;\n\n\ttsd->npools = npools;\n\n\treturn (false);\n}\n\nvoid\narenas_cleanup(void *arg)\n{\n\tunsigned i;\n\tpool_t *pool;\n\ttsd_pool_t *tsd = arg;\n\n\tmalloc_mutex_lock(&pools_lock);\n\tfor (i = 0; i < tsd->npools; i++) {\n\t\tpool = pools[i];\n\t\tif (pool != NULL) {\n\t\t\tif (pool->seqno == tsd->seqno[i] && tsd->arenas[i] != NULL) {\n\t\t\t\tmalloc_rwlock_wrlock(&pool->arenas_lock);\n\t\t\t\ttsd->arenas[i]->nthreads--;\n\t\t\t\tmalloc_rwlock_unlock(&pool->arenas_lock);\n\t\t\t}\n\t\t}\n\t}\n\n\tbase_free_fn(tsd->seqno);\n\tbase_free_fn(tsd->arenas);\n\ttsd->npools = 0;\n\n\tmalloc_mutex_unlock(&pools_lock);\n\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_thread_init(void)\n{\n\tif (config_fill && opt_quarantine && base_malloc_fn == base_malloc_default) {\n\t\t/* create pool base and call quarantine_alloc_hook() inside */\n\t\treturn (malloc_init_base_pool());\n\t}\n\treturn (false);\n}\n\nJEMALLOC_ALWAYS_INLINE_C bool\nmalloc_init(void)\n{\n\n\tif (malloc_initialized == false && malloc_init_hard())\n\t\treturn (true);\n\n\treturn (false);\n}\n\nstatic bool\nmalloc_init_base_pool(void)\n{\n\tmalloc_mutex_lock(&pool_base_lock);\n\n\tif (base_pool_initialized) {\n\t\t/*\n\t\t * Another thread initialized the base pool before this one\n\t\t * acquired pools_lock.\n\t\t */\n\t\tmalloc_mutex_unlock(&pool_base_lock);\n\t\treturn (false);\n\t}\n\n\tif (malloc_init()) {\n\t\tmalloc_mutex_unlock(&pool_base_lock);\n\t\treturn (true);\n\t}\n\n\tif (pool_new(&base_pool, 0)) {\n\t\tmalloc_mutex_unlock(&pool_base_lock);\n\t\treturn (true);\n\t}\n\n\tpools = base_calloc(&base_pool, sizeof(pool_t *), POOLS_MIN);\n\tif (pools == NULL) {\n\t\tmalloc_mutex_unlock(&pool_base_lock);\n\t\treturn (true);\n\t}\n\n\tpools[0] = &base_pool;\n\tpools[0]->seqno = ++pool_seqno;\n\tnpools_cnt++;\n\tnpools = POOLS_MIN;\n\n\tbase_pool_initialized = true;\n\tmalloc_mutex_unlock(&pool_base_lock);\n\n\t/*\n\t * TSD initialization can't be safely done as a side effect of\n\t * deallocation, because it is possible for a thread to do nothing but\n\t * deallocate its TLS data via free(), in which case writing to TLS\n\t * would cause write-after-free memory corruption.  The quarantine\n\t * facility *only* gets used as a side effect of deallocation, so make\n\t * a best effort attempt at initializing its TSD by hooking all\n\t * allocation events.\n\t */\n\tif (config_fill && opt_quarantine)\n\t\tquarantine_alloc_hook();\n\n\t/*\n\t * In the JEMALLOC_LAZY_LOCK case we had to defer initializing the\n\t * arenas_lock until base pool initialization was complete. Deferral\n\t * is safe because there are no other threads yet. We will actually\n\t * recurse here, but since base_pool_initialized is set we will\n\t * drop out of the recursion in the check at the top of this function.\n\t */\n\tif (!isthreaded) {\n\t\tif (malloc_rwlock_init(&base_pool.arenas_lock))\n\t\t\treturn (true);\n\t}\n\n\treturn (false);\n}\n\nstatic bool\nmalloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,\n    char const **v_p, size_t *vlen_p)\n{\n\tbool accept;\n\tconst char *opts = *opts_p;\n\n\t*k_p = opts;\n\n\tfor (accept = false; accept == false;) {\n\t\tswitch (*opts) {\n\t\tcase 'A': case 'B': case 'C': case 'D': case 'E': case 'F':\n\t\tcase 'G': case 'H': case 'I': case 'J': case 'K': case 'L':\n\t\tcase 'M': case 'N': case 'O': case 'P': case 'Q': case 'R':\n\t\tcase 'S': case 'T': case 'U': case 'V': case 'W': case 'X':\n\t\tcase 'Y': case 'Z':\n\t\tcase 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n\t\tcase 'g': case 'h': case 'i': case 'j': case 'k': case 'l':\n\t\tcase 'm': case 'n': case 'o': case 'p': case 'q': case 'r':\n\t\tcase 's': case 't': case 'u': case 'v': case 'w': case 'x':\n\t\tcase 'y': case 'z':\n\t\tcase '0': case '1': case '2': case '3': case '4': case '5':\n\t\tcase '6': case '7': case '8': case '9':\n\t\tcase '_':\n\t\t\topts++;\n\t\t\tbreak;\n\t\tcase ':':\n\t\t\topts++;\n\t\t\t*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;\n\t\t\t*v_p = opts;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\tif (opts != *opts_p) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with key\\n\");\n\t\t\t}\n\t\t\treturn (true);\n\t\tdefault:\n\t\t\tmalloc_write(\"<jemalloc>: Malformed conf string\\n\");\n\t\t\treturn (true);\n\t\t}\n\t}\n\n\tfor (accept = false; accept == false;) {\n\t\tswitch (*opts) {\n\t\tcase ',':\n\t\t\topts++;\n\t\t\t/*\n\t\t\t * Look ahead one character here, because the next time\n\t\t\t * this function is called, it will assume that end of\n\t\t\t * input has been cleanly reached if no input remains,\n\t\t\t * but we have optimistically already consumed the\n\t\t\t * comma if one exists.\n\t\t\t */\n\t\t\tif (*opts == '\\0') {\n\t\t\t\tmalloc_write(\"<jemalloc>: Conf string ends \"\n\t\t\t\t    \"with comma\\n\");\n\t\t\t}\n\t\t\t*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tcase '\\0':\n\t\t\t*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\topts++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t*opts_p = opts;\n\treturn (false);\n}\n\nstatic void\nmalloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,\n    size_t vlen)\n{\n\n\tmalloc_printf(\"<jemalloc>: %s: %.*s:%.*s\\n\", msg, (int)klen, k,\n\t    (int)vlen, v);\n}\n\nstatic void\nmalloc_conf_init(void)\n{\n\tunsigned i;\n\tchar buf[JE_PATH_MAX + 1];\n\tconst char *opts, *k, *v;\n\tsize_t klen, vlen;\n\n\t/*\n\t * Automatically configure valgrind before processing options.  The\n\t * valgrind option remains in jemalloc 3.x for compatibility reasons.\n\t */\n\tif (config_valgrind) {\n\t\tin_valgrind = (RUNNING_ON_VALGRIND != 0) ? true : false;\n\t\tif (config_fill && in_valgrind) {\n\t\t\topt_junk = false;\n\t\t\tassert(opt_zero == false);\n\t\t\topt_quarantine = JEMALLOC_VALGRIND_QUARANTINE_DEFAULT;\n\t\t\topt_redzone = true;\n\t\t}\n\t\tif (config_tcache && in_valgrind)\n\t\t\topt_tcache = false;\n\t}\n\n\tfor (i = 0; i < 3; i++) {\n\t\t/* Get runtime configuration. */\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tif (je_malloc_conf != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Use options that were compiled into the\n\t\t\t\t * program.\n\t\t\t\t */\n\t\t\t\topts = je_malloc_conf;\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 1: {\n\t\t\tint linklen = 0;\n#ifndef _WIN32\n\t\t\tint saved_errno = errno;\n\t\t\tconst char *linkname =\n#  ifdef JEMALLOC_PREFIX\n\t\t\t    \"/etc/\"JEMALLOC_PREFIX\"malloc.conf\"\n#  else\n\t\t\t    \"/etc/malloc.conf\"\n#  endif\n\t\t\t    ;\n\n\t\t\t/*\n\t\t\t * Try to use the contents of the \"/etc/malloc.conf\"\n\t\t\t * symbolic link's name.\n\t\t\t */\n\t\t\tlinklen = readlink(linkname, buf, sizeof(buf) - 1);\n\t\t\tif (linklen == -1) {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tlinklen = 0;\n\t\t\t\t/* restore errno */\n\t\t\t\tset_errno(saved_errno);\n\t\t\t}\n#endif\n\t\t\tbuf[linklen] = '\\0';\n\t\t\topts = buf;\n\t\t\tbreak;\n\t\t} case 2: {\n\t\t\tconst char *envname =\n#ifdef JEMALLOC_PREFIX\n\t\t\t    JEMALLOC_CPREFIX\"MALLOC_CONF\"\n#else\n\t\t\t    \"MALLOC_CONF\"\n#endif\n\t\t\t    ;\n\n\t\t\tif ((opts = getenv(envname)) != NULL) {\n\t\t\t\t/*\n\t\t\t\t * Do nothing; opts is already initialized to\n\t\t\t\t * the value of the MALLOC_CONF environment\n\t\t\t\t * variable.\n\t\t\t\t */\n\t\t\t} else {\n\t\t\t\t/* No configuration specified. */\n\t\t\t\tbuf[0] = '\\0';\n\t\t\t\topts = buf;\n\t\t\t}\n\t\t\tbreak;\n\t\t} default:\n\t\t\tnot_reached();\n\t\t\tbuf[0] = '\\0';\n\t\t\topts = buf;\n\t\t}\n\n\t\twhile (*opts != '\\0' && malloc_conf_next(&opts, &k, &klen, &v,\n\t\t    &vlen) == false) {\n#define\tCONF_MATCH(n)\t\t\t\t\t\t\t\\\n\t(sizeof(n)-1 == klen && strncmp(n, k, klen) == 0)\n#define\tCONF_HANDLE_BOOL(o, n, cont)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tif (strncmp(\"true\", v, vlen) == 0 &&\t\\\n\t\t\t\t    vlen == sizeof(\"true\")-1)\t\t\\\n\t\t\t\t\to = true;\t\t\t\\\n\t\t\t\telse if (strncmp(\"false\", v, vlen) ==\t\\\n\t\t\t\t    0 && vlen == sizeof(\"false\")-1)\t\\\n\t\t\t\t\to = false;\t\t\t\\\n\t\t\t\telse {\t\t\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tif (cont)\t\t\t\t\\\n\t\t\t\t\tcontinue;\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_SIZE_T(o, n, min, max, clip)\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tuintmax_t um;\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tum = malloc_strtoumax(v, &end, 0);\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (clip) {\t\t\t\\\n\t\t\t\t\tif ((min) != 0 && um < (min))\t\\\n\t\t\t\t\t\to = min;\t\t\\\n\t\t\t\t\telse if (um > (max))\t\t\\\n\t\t\t\t\t\to = max;\t\t\\\n\t\t\t\t\telse\t\t\t\t\\\n\t\t\t\t\t\to = um;\t\t\t\\\n\t\t\t\t} else {\t\t\t\t\\\n\t\t\t\t\tif (((min) != 0 && um < (min)) ||\t\\\n\t\t\t\t\t    um > (max)) {\t\t\t\\\n\t\t\t\t\t\tmalloc_conf_error(\t\\\n\t\t\t\t\t\t    \"Out-of-range \"\t\\\n\t\t\t\t\t\t    \"conf value\",\t\\\n\t\t\t\t\t\t    k, klen, v, vlen);\t\\\n\t\t\t\t\t} else\t\t\t\t\\\n\t\t\t\t\t\to = um;\t\t\t\\\n\t\t\t\t}\t\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_SSIZE_T(o, n, min, max)\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tlong l;\t\t\t\t\t\\\n\t\t\t\tchar *end;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t\tset_errno(0);\t\t\t\t\\\n\t\t\t\tl = strtol(v, &end, 0);\t\t\t\\\n\t\t\t\tif (get_errno() != 0 || (uintptr_t)end -\\\n\t\t\t\t    (uintptr_t)v != vlen) {\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Invalid conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else if (l < (ssize_t)(min) || l >\t\\\n\t\t\t\t    (ssize_t)(max)) {\t\t\t\\\n\t\t\t\t\tmalloc_conf_error(\t\t\\\n\t\t\t\t\t    \"Out-of-range conf value\",\t\\\n\t\t\t\t\t    k, klen, v, vlen);\t\t\\\n\t\t\t\t} else\t\t\t\t\t\\\n\t\t\t\t\to = l;\t\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n#define\tCONF_HANDLE_CHAR_P(o, n, d)\t\t\t\t\t\\\n\t\t\tif (CONF_MATCH(n)) {\t\t\t\t\\\n\t\t\t\tsize_t cpylen = (vlen <=\t\t\\\n\t\t\t\t    sizeof(o)-1) ? vlen :\t\t\\\n\t\t\t\t    sizeof(o)-1;\t\t\t\\\n\t\t\t\tstrncpy(o, v, cpylen);\t\t\t\\\n\t\t\t\to[cpylen] = '\\0';\t\t\t\\\n\t\t\t\tcontinue;\t\t\t\t\\\n\t\t\t}\n\n\t\t\tCONF_HANDLE_BOOL(opt_abort, \"abort\", true)\n\t\t\t/*\n\t\t\t * Chunks always require at least one header page, plus\n\t\t\t * one data page in the absence of redzones, or three\n\t\t\t * pages in the presence of redzones.  In order to\n\t\t\t * simplify options processing, fix the limit based on\n\t\t\t * config_fill.\n\t\t\t */\n\t\t\tCONF_HANDLE_SIZE_T(opt_lg_chunk, \"lg_chunk\", LG_PAGE +\n\t\t\t    (config_fill ? 2 : 1), (sizeof(size_t) << 3) - 1,\n\t\t\t    true)\n\t\t\tif (strncmp(\"dss\", k, klen) == 0) {\n\t\t\t\tint i;\n\t\t\t\tbool match = false;\n\t\t\t\tfor (i = 0; i < dss_prec_limit; i++) {\n\t\t\t\t\tif (strncmp(dss_prec_names[i], v, vlen)\n\t\t\t\t\t    == 0) {\n\t\t\t\t\t\tif (chunk_dss_prec_set(i)) {\n\t\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\t    \"Error setting dss\",\n\t\t\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\topt_dss =\n\t\t\t\t\t\t\t    dss_prec_names[i];\n\t\t\t\t\t\t\tmatch = true;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (match == false) {\n\t\t\t\t\tmalloc_conf_error(\"Invalid conf value\",\n\t\t\t\t\t    k, klen, v, vlen);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tCONF_HANDLE_SIZE_T(opt_narenas, \"narenas\", 1,\n\t\t\t    SIZE_T_MAX, false)\n\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_dirty_mult, \"lg_dirty_mult\",\n\t\t\t    -1, (sizeof(size_t) << 3) - 1)\n\t\t\tCONF_HANDLE_BOOL(opt_stats_print, \"stats_print\", true)\n\t\t\tif (config_fill) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_junk, \"junk\", true)\n\t\t\t\tCONF_HANDLE_SIZE_T(opt_quarantine, \"quarantine\",\n\t\t\t\t    0, SIZE_T_MAX, false)\n\t\t\t\tCONF_HANDLE_BOOL(opt_redzone, \"redzone\", true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_zero, \"zero\", true)\n\t\t\t}\n\t\t\tif (config_utrace) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_utrace, \"utrace\", true)\n\t\t\t}\n\t\t\tif (config_xmalloc) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_xmalloc, \"xmalloc\", true)\n\t\t\t}\n\t\t\tif (config_tcache) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_tcache, \"tcache\",\n\t\t\t\t    !config_valgrind || !in_valgrind)\n\t\t\t\tif (CONF_MATCH(\"tcache\")) {\n\t\t\t\t\tassert(config_valgrind && in_valgrind);\n\t\t\t\t\tif (opt_tcache) {\n\t\t\t\t\t\topt_tcache = false;\n\t\t\t\t\t\tmalloc_conf_error(\n\t\t\t\t\t\t\"tcache cannot be enabled \"\n\t\t\t\t\t\t\"while running inside Valgrind\",\n\t\t\t\t\t\tk, klen, v, vlen);\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_tcache_max,\n\t\t\t\t    \"lg_tcache_max\", -1,\n\t\t\t\t    (sizeof(size_t) << 3) - 1)\n\t\t\t}\n\t\t\tif (config_prof) {\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof, \"prof\", true)\n\t\t\t\tCONF_HANDLE_CHAR_P(opt_prof_prefix,\n\t\t\t\t    \"prof_prefix\", \"jeprof\")\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_active, \"prof_active\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_sample,\n\t\t\t\t    \"lg_prof_sample\", 0,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_accum, \"prof_accum\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_SSIZE_T(opt_lg_prof_interval,\n\t\t\t\t    \"lg_prof_interval\", -1,\n\t\t\t\t    (sizeof(uint64_t) << 3) - 1)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_gdump, \"prof_gdump\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_final, \"prof_final\",\n\t\t\t\t    true)\n\t\t\t\tCONF_HANDLE_BOOL(opt_prof_leak, \"prof_leak\",\n\t\t\t\t    true)\n\t\t\t}\n\t\t\tmalloc_conf_error(\"Invalid conf pair\", k, klen, v,\n\t\t\t    vlen);\n#undef CONF_MATCH\n#undef CONF_HANDLE_BOOL\n#undef CONF_HANDLE_SIZE_T\n#undef CONF_HANDLE_SSIZE_T\n#undef CONF_HANDLE_CHAR_P\n\t\t}\n\t}\n}\n\nstatic bool\nmalloc_init_hard(void)\n{\n\tmalloc_mutex_lock(&init_lock);\n\tif (malloc_initialized || IS_INITIALIZER) {\n\t\t/*\n\t\t * Another thread initialized the allocator before this one\n\t\t * acquired init_lock, or this thread is the initializing\n\t\t * thread, and it is recursively allocating.\n\t\t */\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (false);\n\t}\n#ifdef JEMALLOC_THREADED_INIT\n\tif (malloc_initializer != NO_INITIALIZER && IS_INITIALIZER == false) {\n\t\t/* Busy-wait until the initializing thread completes. */\n\t\tdo {\n\t\t\tmalloc_mutex_unlock(&init_lock);\n\t\t\tCPU_SPINWAIT;\n\t\t\tmalloc_mutex_lock(&init_lock);\n\t\t} while (malloc_initialized == false);\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (false);\n\t}\n#endif\n\tmalloc_initializer = INITIALIZER;\n\n\tmalloc_tsd_boot();\n\tif (config_prof)\n\t\tprof_boot0();\n\n\tmalloc_conf_init();\n\n\tif (opt_stats_print) {\n\t\t/* Print statistics at exit. */\n\t\tif (atexit(stats_print_atexit) != 0) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in atexit()\\n\");\n\t\t\tif (opt_abort)\n\t\t\t\tabort();\n\t\t}\n\t}\n\n\tpools_shared_data_initialized = false;\n\tif (base_malloc_fn == NULL && base_free_fn == NULL) {\n\t\tbase_malloc_fn = base_malloc_default;\n\t\tbase_free_fn = base_free_default;\n\t}\n\n\tif (chunk_global_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (ctl_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_prof)\n\t\tprof_boot1();\n\n\tarena_params_boot();\n\n\t/* Initialize allocation counters before any allocations can occur. */\n\tif (config_stats && thread_allocated_tsd_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (arenas_tsd_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_tcache && tcache_boot1()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_fill && quarantine_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (config_prof && prof_boot2()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tmalloc_mutex_unlock(&init_lock);\n\t/**********************************************************************/\n\t/* Recursive allocation may follow. */\n\n\tncpus = malloc_ncpus();\n\n#if (!defined(JEMALLOC_MUTEX_INIT_CB) && !defined(JEMALLOC_ZONE) \\\n    && !defined(_WIN32) && !defined(__native_client__))\n\t/* LinuxThreads's pthread_atfork() allocates. */\n\tif (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,\n\t    jemalloc_postfork_child) != 0) {\n\t\tmalloc_write(\"<jemalloc>: Error in pthread_atfork()\\n\");\n\t\tif (opt_abort)\n\t\t\tabort();\n\t}\n#endif\n\n\t/* Done recursively allocating. */\n\t/**********************************************************************/\n\tmalloc_mutex_lock(&init_lock);\n\n\tif (mutex_boot()) {\n\t\tmalloc_mutex_unlock(&init_lock);\n\t\treturn (true);\n\t}\n\n\tif (opt_narenas == 0) {\n\t\t/*\n\t\t * For SMP systems, create more than one arena per CPU by\n\t\t * default.\n\t\t */\n\t\tif (ncpus > 1)\n\t\t\topt_narenas = ncpus << 2;\n\t\telse\n\t\t\topt_narenas = 1;\n\t}\n\n\tmalloc_initialized = true;\n\tmalloc_mutex_unlock(&init_lock);\n\n\treturn (false);\n}\n\n/*\n * End initialization functions.\n */\n/******************************************************************************/\n/*\n * Begin malloc(3)-compatible functions.\n */\n\nstatic void *\nimalloc_prof_sample(size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = imalloc(SMALL_MAXCLASS+1);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = imalloc(usize);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimalloc_prof(size_t usize)\n{\n\tvoid *p;\n\tprof_thr_cnt_t *cnt;\n\n\tPROF_ALLOC_PREP(usize, cnt);\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = imalloc_prof_sample(usize, cnt);\n\telse\n\t\tp = imalloc(usize);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_malloc(p, usize, cnt);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimalloc_body(size_t size, size_t *usize)\n{\n\n\tif (malloc_init_base_pool())\n\t\treturn (NULL);\n\n\tif (config_prof && opt_prof) {\n\t\t*usize = s2u(size);\n\t\treturn (imalloc_prof(*usize));\n\t}\n\n\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t*usize = s2u(size);\n\treturn (imalloc(size));\n}\n\nvoid *\nje_malloc(size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tret = imalloc_body(size, &usize);\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in malloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && ret != NULL) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, false);\n\treturn (ret);\n}\n\nstatic void *\nimemalign_prof_sample(size_t alignment, size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tassert(sa2u(SMALL_MAXCLASS+1, alignment) != 0);\n\t\tp = ipalloc(sa2u(SMALL_MAXCLASS+1, alignment), alignment,\n\t\t    false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = ipalloc(usize, alignment, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimemalign_prof(size_t alignment, size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = imemalign_prof_sample(alignment, usize, cnt);\n\telse\n\t\tp = ipalloc(usize, alignment, false);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_malloc(p, usize, cnt);\n\n\treturn (p);\n}\n\nJEMALLOC_ATTR(nonnull(1))\nstatic int\nimemalign(void **memptr, size_t alignment, size_t size, size_t min_alignment)\n{\n\tint ret;\n\tsize_t usize;\n\tvoid *result;\n\n\tassert(min_alignment != 0);\n\n\tif (malloc_init_base_pool()) {\n\t\tresult = NULL;\n\t\tgoto label_oom;\n\t} else {\n\t\tif (size == 0)\n\t\t\tsize = 1;\n\n\t\t/* Make sure that alignment is a large enough power of 2. */\n\t\tif (((alignment - 1) & alignment) != 0\n\t\t    || (alignment < min_alignment)) {\n\t\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Error allocating \"\n\t\t\t\t    \"aligned memory: invalid alignment\\n\");\n\t\t\t\tabort();\n\t\t\t}\n\t\t\tresult = NULL;\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tusize = sa2u(size, alignment);\n\t\tif (usize == 0) {\n\t\t\tresult = NULL;\n\t\t\tgoto label_oom;\n\t\t}\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tprof_thr_cnt_t *cnt;\n\n\t\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\t\tresult = imemalign_prof(alignment, usize, cnt);\n\t\t} else\n\t\t\tresult = ipalloc(usize, alignment, false);\n\t\tif (result == NULL)\n\t\t\tgoto label_oom;\n\t}\n\n\t*memptr = result;\n\tret = 0;\nlabel_return:\n\tif (config_stats && result != NULL) {\n\t\tassert(usize == isalloc(result, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, result);\n\treturn (ret);\nlabel_oom:\n\tassert(result == NULL);\n\tif (config_xmalloc && opt_xmalloc) {\n\t\tmalloc_write(\"<jemalloc>: Error allocating aligned memory: \"\n\t\t    \"out of memory\\n\");\n\t\tabort();\n\t}\n\tret = ENOMEM;\n\tgoto label_return;\n}\n\nint\nje_posix_memalign(void **memptr, size_t alignment, size_t size)\n{\n\tint ret = imemalign(memptr, alignment, size, sizeof(void *));\n\tJEMALLOC_VALGRIND_MALLOC(ret == 0, *memptr, isalloc(*memptr,\n\t    config_prof), false);\n\treturn (ret);\n}\n\nvoid *\nje_aligned_alloc(size_t alignment, size_t size)\n{\n\tvoid *ret;\n\tint err;\n\n\tif ((err = imemalign(&ret, alignment, size, 1)) != 0) {\n\t\tret = NULL;\n\t\tset_errno(err);\n\t}\n\tJEMALLOC_VALGRIND_MALLOC(err == 0, ret, isalloc(ret, config_prof),\n\t    false);\n\treturn (ret);\n}\n\nstatic void *\nicalloc_prof_sample(size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = icalloc(SMALL_MAXCLASS+1);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = icalloc(usize);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nicalloc_prof(size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = icalloc_prof_sample(usize, cnt);\n\telse\n\t\tp = icalloc(usize);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_malloc(p, usize, cnt);\n\n\treturn (p);\n}\n\nvoid *\nje_calloc(size_t num, size_t size)\n{\n\tvoid *ret;\n\tsize_t num_size;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (malloc_init_base_pool()) {\n\t\tnum_size = 0;\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tnum_size = num * size;\n\tif (num_size == 0) {\n\t\tif (num == 0 || size == 0)\n\t\t\tnum_size = 1;\n\t\telse {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\t/*\n\t * Try to avoid division here.  We know that it isn't possible to\n\t * overflow during multiplication if neither operand uses any of the\n\t * most significant half of the bits in a size_t.\n\t */\n\t} else if (((num | size) & (SIZE_T_MAX << (sizeof(size_t) << 2)))\n\t    && (num_size / size != num)) {\n\t\t/* size_t overflow. */\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tif (config_prof && opt_prof) {\n\t\tprof_thr_cnt_t *cnt;\n\n\t\tusize = s2u(num_size);\n\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\tret = icalloc_prof(usize, cnt);\n\t} else {\n\t\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t\tusize = s2u(num_size);\n\t\tret = icalloc(num_size);\n\t}\n\nlabel_return:\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in calloc(): out of \"\n\t\t\t    \"memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && ret != NULL) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, num_size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, true);\n\treturn (ret);\n}\n\nstatic void *\nirealloc_prof_sample(void *oldptr, size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = iralloc(oldptr, SMALL_MAXCLASS+1, 0, 0, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = iralloc(oldptr, usize, 0, 0, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nirealloc_prof(void *oldptr, size_t old_usize, size_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\tprof_ctx_t *old_ctx;\n\n\told_ctx = prof_ctx_get(oldptr);\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = irealloc_prof_sample(oldptr, usize, cnt);\n\telse\n\t\tp = iralloc(oldptr, usize, 0, 0, false);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_realloc(p, usize, cnt, old_usize, old_ctx);\n\n\treturn (p);\n}\n\nJEMALLOC_INLINE_C void\nifree(void *ptr)\n{\n\tsize_t usize;\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = isalloc(ptr, config_prof);\n\t\tprof_free(ptr, usize);\n\t} else if (config_stats || config_valgrind)\n\t\tusize = isalloc(ptr, config_prof);\n\tif (config_stats)\n\t\tthread_allocated_tsd_get()->deallocated += usize;\n\tif (config_valgrind && in_valgrind)\n\t\trzsize = p2rz(ptr);\n\tiqalloc(ptr);\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n}\n\nvoid *\nje_realloc(void *ptr, size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t old_usize = 0;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (size == 0) {\n\t\tif (ptr != NULL) {\n\t\t\t/* realloc(ptr, 0) is equivalent to free(ptr). */\n\t\t\tUTRACE(ptr, 0, 0);\n\t\t\tifree(ptr);\n\t\t\treturn (NULL);\n\t\t}\n\t\tsize = 1;\n\t}\n\n\tif (ptr != NULL) {\n\t\tassert(malloc_initialized || IS_INITIALIZER);\n\t\tif (malloc_thread_init())\n\t\t\treturn (NULL);\n\n\t\tif ((config_prof && opt_prof) || config_stats ||\n\t\t    (config_valgrind && in_valgrind))\n\t\t\told_usize = isalloc(ptr, config_prof);\n\t\tif (config_valgrind && in_valgrind)\n\t\t\told_rzsize = config_prof ? p2rz(ptr) : u2rz(old_usize);\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tprof_thr_cnt_t *cnt;\n\n\t\t\tusize = s2u(size);\n\t\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\t\tret = irealloc_prof(ptr, old_usize, usize, cnt);\n\t\t} else {\n\t\t\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t\t\tusize = s2u(size);\n\t\t\tret = iralloc(ptr, size, 0, 0, false);\n\t\t}\n\t} else {\n\t\t/* realloc(NULL, size) is equivalent to malloc(size). */\n\t\tret = imalloc_body(size, &usize);\n\t}\n\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in realloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && ret != NULL) {\n\t\tthread_allocated_t *ta;\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tta = thread_allocated_tsd_get();\n\t\tta->allocated += usize;\n\t\tta->deallocated += old_usize;\n\t}\n\tUTRACE(ptr, size, ret);\n\n\tJEMALLOC_VALGRIND_REALLOC(true, ret, usize, true, ptr, old_usize,\n\t\t    old_rzsize, true, false);\n\treturn (ret);\n}\n\nvoid\nje_free(void *ptr)\n{\n\n\tUTRACE(ptr, 0, 0);\n\tif (ptr != NULL)\n\t\tifree(ptr);\n}\n\n/*\n * End malloc(3)-compatible functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard override functions.\n */\n\n#ifdef JEMALLOC_OVERRIDE_MEMALIGN\nvoid *\nje_memalign(size_t alignment, size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\timemalign(&ret, alignment, size, 1);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, size, false);\n\treturn (ret);\n}\n#endif\n\n#ifdef JEMALLOC_OVERRIDE_VALLOC\nvoid *\nje_valloc(size_t size)\n{\n\tvoid *ret JEMALLOC_CC_SILENCE_INIT(NULL);\n\timemalign(&ret, PAGE, size, 1);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, size, false);\n\treturn (ret);\n}\n#endif\n\n/*\n * is_malloc(je_malloc) is some macro magic to detect if jemalloc_defs.h has\n * #define je_malloc malloc\n */\n#define\tmalloc_is_malloc 1\n#define\tis_malloc_(a) malloc_is_ ## a\n#define\tis_malloc(a) is_malloc_(a)\n\n#if ((is_malloc(je_malloc) == 1) && defined(__GLIBC__) && !defined(__UCLIBC__))\n/*\n * glibc provides the RTLD_DEEPBIND flag for dlopen which can make it possible\n * to inconsistently reference libc's malloc(3)-compatible functions\n * (https://bugzilla.mozilla.org/show_bug.cgi?id=493541).\n *\n * These definitions interpose hooks in glibc.  The functions are actually\n * passed an extra argument for the caller return address, which will be\n * ignored.\n */\nJEMALLOC_EXPORT void (*__free_hook)(void *ptr) = je_free;\nJEMALLOC_EXPORT void *(*__malloc_hook)(size_t size) = je_malloc;\nJEMALLOC_EXPORT void *(*__realloc_hook)(void *ptr, size_t size) = je_realloc;\nJEMALLOC_EXPORT void *(*__memalign_hook)(size_t alignment, size_t size) =\n    je_memalign;\n#endif\n\n/*\n * End non-standard override functions.\n */\n/******************************************************************************/\n/*\n * Begin non-standard functions.\n */\n\nstatic void *\nbase_malloc_default(size_t size)\n{\n\n\treturn base_alloc(&base_pool, size);\n}\n\nstatic void\nbase_free_default(void *ptr)\n{\n\n}\n\nstatic void\nje_base_pool_destroy(void)\n{\n\tif (base_pool_initialized == false)\n\t\treturn;\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\n\tpool_destroy(&base_pool);\n\tmalloc_mutex_destroy(&pool_base_lock);\n\tmalloc_mutex_destroy(&pools_lock);\n#endif\n}\n\nbool\npools_shared_data_create(void)\n{\n\tif (malloc_init())\n\t\treturn (true);\n\n\tif (pools_shared_data_initialized)\n\t\treturn (false);\n\n\tif (config_tcache && tcache_boot0())\n\t\treturn (true);\n\n\tpools_shared_data_initialized = true;\n\n\treturn (false);\n}\n\nvoid\npools_shared_data_destroy(void)\n{\n\t/* Only destroy when no pools exist */\n\tif (npools == 0) {\n\t\tpools_shared_data_initialized = false;\n\n\t\tbase_free_fn(tcache_bin_info);\n\t\ttcache_bin_info = NULL;\n\t}\n}\n\n#ifdef JEMALLOC_VALGRIND\n/*\n * Iterates through all the chunks/allocations on the heap and marks them\n * as defined/undefined.\n */\nstatic extent_node_t *\nvg_tree_binary_iter_cb(extent_tree_t *tree, extent_node_t *node, void *arg)\n{\n\tassert(node->size != 0);\n\tint noaccess = *(int *)arg;\n\n\tif (noaccess) {\n\t\tJEMALLOC_VALGRIND_MAKE_MEM_NOACCESS(node->addr, node->size);\n\t} else {\n\t\t/* assume memory is defined */\n\t\tJEMALLOC_VALGRIND_MALLOC(1, node->addr, node->size, 1);\n\t\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(node->addr, node->size);\n\t}\n\n\treturn (NULL);\n}\n\n/*\n * Iterates through all the chunks/allocations on the heap and marks them\n * as defined/undefined.\n */\nstatic arena_chunk_map_t *\nvg_tree_chunks_avail_iter_cb(arena_avail_tree_t *tree,\n\tarena_chunk_map_t *map, void *arg)\n{\n\tint noaccess = *(int *)arg;\n\n\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(map, sizeof(*map));\n\n\tassert((map->bits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) == 0);\n\tassert((map->bits & ~PAGE_MASK) != 0);\n\n\tsize_t chunk_size = (map->bits & ~PAGE_MASK);\n\tarena_chunk_t *run_chunk = CHUNK_ADDR2BASE(map);\n\n\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(run_chunk, sizeof(*run_chunk));\n\n\tsize_t pageind = arena_mapelm_to_pageind(map);\n\tvoid *chunk_addr = (void *)((uintptr_t)run_chunk + (pageind << LG_PAGE));\n\n\tif (noaccess) {\n\t\tJEMALLOC_VALGRIND_MAKE_MEM_NOACCESS(chunk_addr, chunk_size);\n\t} else {\n\t\tJEMALLOC_VALGRIND_MALLOC(1, chunk_addr, chunk_size, 1);\n\t\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(chunk_addr, chunk_size);\n\t}\n\n\treturn (NULL);\n}\n\n/*\n * Reinitializes memcheck state if run under Valgrind.\n * Iterates through all the chunks/allocations on the heap and marks them\n * as defined/undefined.\n */\nstatic int\nvg_pool_init(pool_t *pool, size_t size)\n{\n\t/*\n\t * There is no need to grab any locks here, as the pool is not\n\t * being used yet.\n\t */\n\n\t/* mark base_alloc used space as defined */\n\tchar *base_start = (char *)CACHELINE_CEILING((uintptr_t)pool +\n\t\t\tsizeof(pool_t));\n\tchar *base_end = pool->base_next_addr;\n\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(base_start, base_end - base_start);\n\tJEMALLOC_VALGRIND_MAKE_MEM_NOACCESS(base_end,\n\t\t\t(char *)pool->base_past_addr - base_end);\n\n\t/* pointer to the address of chunks, align the address to chunksize */\n\tvoid *usable_addr =\n\t\t(void *)CHUNK_CEILING((uintptr_t)pool->base_next_addr);\n\n\t/* usable chunks space, must be multiple of chunksize */\n\tsize_t usable_size =\n\t\t(size - (uintptr_t)((char *)usable_addr - (char *)pool))\n\t\t& ~chunksize_mask;\n\n\t/* initially mark the entire heap as defined */\n\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(\n\t\t\tusable_addr,\n\t\t\tusable_size);\n\n\t/* iterate through unused (available) chunks - mark as NOACCESS */\n\tint noaccess = 1;\n\textent_tree_szad_iter(&pool->chunks_szad_mmap, NULL,\n\t\t\tvg_tree_binary_iter_cb, &noaccess);\n\n\t/* iterate through huge allocations - mark as MALLOCLIKE */\n\tnoaccess = 0;\n\textent_tree_ad_iter(&pool->huge, NULL,\n\t\t\tvg_tree_binary_iter_cb, &noaccess);\n\n\t/* iterate through arenas/runs */\n\tfor (unsigned i = 0; i < pool->narenas_total; ++i) {\n\t\tarena_t *arena = pool->arenas[i];\n\t\tif (arena != NULL) {\n\t\t\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(arena,\n\t\t\t\t\tsizeof(*arena));\n\n\t\t\t/* bins */\n\t\t\tfor (unsigned b = 0; b < NBINS; b++) {\n\t\t\t\tarena_bin_t *bin = &arena->bins[b];\n\n\t\t\t\tif (bin->runcur != NULL)\n\t\t\t\t\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(\n\t\t\t\t\t\tbin->runcur,\n\t\t\t\t\t\tsizeof(*(bin->runcur)));\n\t\t\t}\n\n\t\t\tnoaccess = 1; /* XXX */\n\t\t\tarena_runs_avail_tree_iter(arena,\n\t\t\t\tvg_tree_chunks_avail_iter_cb, &noaccess);\n\n\t\t\tarena_chunk_t *spare = arena->spare;\n\t\t\tif (spare != NULL) {\n\t\t\t\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(\n\t\t\t\t\t\tspare, sizeof(*spare));\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 1;\n}\n#endif /* JEMALLOC_VALGRIND */\n\n/*\n * Creates a new pool.\n * Initializes the heap and all the allocator metadata.\n */\nstatic pool_t *\npool_create_empty(pool_t *pool, size_t size, int zeroed, unsigned pool_id)\n{\n\tsize_t result;\n\n\tif (!zeroed)\n\t\tmemset(pool, 0, sizeof (pool_t));\n\n\t/*\n\t * preinit base allocator in unused space, align the address\n\t * to the cache line\n\t */\n\tpool->base_next_addr = (void *)CACHELINE_CEILING((uintptr_t)pool +\n\t\tsizeof (pool_t));\n\tpool->base_past_addr = (void *)((uintptr_t)pool + size);\n\n\t/* prepare pool and internal structures */\n\tif (pool_new(pool, pool_id)) {\n\t\tassert(pools[pool_id] == NULL);\n\t\tpools_shared_data_destroy();\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * preallocate the chunk tree nodes for the maximum possible\n\t * number of chunks\n\t */\n\tresult = base_node_prealloc(pool, size/chunksize);\n\tassert(result == 0);\n\n\tassert(pools[pool_id] == NULL);\n\tpool->seqno = pool_seqno++;\n\tpools[pool_id] = pool;\n\tnpools_cnt++;\n\n\tpool->memory_range_list =\n\t\t\tbase_alloc(pool, sizeof(*pool->memory_range_list));\n\n\t/* pointer to the address of chunks, align the address to chunksize */\n\tvoid *usable_addr =\n\t\t(void *)CHUNK_CEILING((uintptr_t)pool->base_next_addr);\n\n\t/* reduce end of base allocator up to chunks start */\n\tpool->base_past_addr = usable_addr;\n\n\t/* usable chunks space, must be multiple of chunksize */\n\tsize_t usable_size =\n\t\t(size - (uintptr_t)((char *)usable_addr - (char *)pool))\n\t\t& ~chunksize_mask;\n\n\tassert(usable_size > 0);\n\n\tmalloc_mutex_lock(&pool->memory_range_mtx);\n\tpool->memory_range_list->next = NULL;\n\tpool->memory_range_list->addr = (uintptr_t)pool;\n\tpool->memory_range_list->addr_end = (uintptr_t)pool + size;\n\tpool->memory_range_list->usable_addr = (uintptr_t)usable_addr;\n\tpool->memory_range_list->usable_addr_end =\n\t\t\t(uintptr_t)usable_addr + usable_size;\n\tmalloc_mutex_unlock(&pool->memory_range_mtx);\n\n\t/* register the usable pool space as a single big chunk */\n\tchunk_record(pool,\n\t\t&pool->chunks_szad_mmap, &pool->chunks_ad_mmap,\n\t\tusable_addr, usable_size, zeroed);\n\n\tpool->ctl_initialized = false;\n\n\treturn pool;\n}\n\n/*\n * Opens an existing pool (i.e. pmemcto pool).\n * Only the run-time state needs to be re-initialized.\n */\nstatic pool_t *\npool_open(pool_t *pool, size_t size, unsigned pool_id)\n{\n\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED(pool, sizeof(pool_t));\n\n\t/* prepare pool's runtime state */\n\tif (pool_runtime_init(pool, pool_id)) {\n\t\tmalloc_mutex_unlock(&pools_lock);\n\t\treturn NULL;\n\t}\n\n\tassert(pools[pool_id] == NULL);\n\tpool->seqno = pool_seqno++;\n\tpools[pool_id] = pool;\n\tnpools_cnt++;\n\n\treturn pool;\n}\n\npool_t *\nje_pool_create(void *addr, size_t size, int zeroed, int empty)\n{\n\tif (malloc_init())\n\t\treturn (NULL);\n\n\tif (addr == NULL || size < POOL_MINIMAL_SIZE)\n\t\treturn (NULL);\n\n\tpool_t *pool = (pool_t *)addr;\n\tunsigned pool_id;\n\n\t/* Preinit base pool if not exist, before lock pool_lock */\n\tif (malloc_init_base_pool())\n\t\treturn (NULL);\n\n\tmalloc_mutex_lock(&pools_lock);\n\n\tassert(pools != NULL);\n\tassert(npools > 0);\n\n\t/*\n\t * Find unused pool ID.\n\t * Pool 0 is a special pool with reserved ID. Pool is created during\n\t * malloc_init_pool_base() and allocates memory from RAM.\n\t */\n\tfor (pool_id = 1; pool_id < npools; ++pool_id) {\n\t\tif (pools[pool_id] == NULL)\n\t\t\tbreak;\n\t}\n\n\tif (pool_id == npools && npools < POOLS_MAX) {\n\t\tsize_t npools_new = npools * 2;\n\t\tpool_t **pools_new = base_alloc(&base_pool,\n\t\t\t\t\tnpools_new * sizeof (pool_t *));\n\t\tif (pools_new == NULL)\n\t\t\tgoto err;\n\n\t\tmemcpy(pools_new, pools, npools * sizeof (pool_t *));\n\t\tmemset(&pools_new[npools], 0,\n\t\t\t\t(npools_new - npools) * sizeof (pool_t *));\n\n\t\tpools = pools_new;\n\t\tnpools = npools_new;\n\t}\n\n\tif (pool_id == POOLS_MAX) {\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_create(): \"\n\t\t\t\"exceeded max number of pools (%u)\\n\", POOLS_MAX);\n\t\tgoto err;\n\t}\n\n\tpool_t *ret;\n\tif (empty) {\n\t\tret = pool_create_empty(pool, size, zeroed, pool_id);\n\t} else {\n\t\tret = pool_open(pool, size, pool_id);\n\t}\n\n\tmalloc_mutex_unlock(&pools_lock);\n\n#ifdef JEMALLOC_VALGRIND\n\t/* must be done with unlocked 'pools_lock' */\n\tif (config_valgrind && !empty)\n\t\tvg_pool_init(pool, size);\n#endif\n\n\treturn ret;\n\nerr:\n\tmalloc_mutex_unlock(&pools_lock);\n\treturn (NULL);\n}\n\nint\nje_pool_delete(pool_t *pool)\n{\n\tunsigned pool_id = pool->pool_id;\n\n\t/* Remove pool from global array */\n\tmalloc_mutex_lock(&pools_lock);\n\n\tif ((pool_id == 0) || (pool_id >= npools) || (pools[pool_id] != pool)) {\n\t\tmalloc_mutex_unlock(&pools_lock);\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_delete(): \"\n\t\t\t\"invalid pool_id (%u)\\n\", pool_id);\n\t\treturn -1;\n\t}\n\n\tpool_destroy(pool);\n\tpools[pool_id] = NULL;\n\tnpools_cnt--;\n\tpools_shared_data_destroy();\n\n\tmalloc_mutex_unlock(&pools_lock);\n\treturn 0;\n}\n\nstatic int\ncheck_is_unzeroed(void *ptr, size_t size)\n{\n\tsize_t i;\n\tsize_t *p = (size_t *)ptr;\n\tsize /= sizeof(size_t);\n\tfor (i = 0; i < size; i++) {\n\t\tif (p[i])\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic extent_node_t *\ncheck_tree_binary_iter_cb(extent_tree_t *tree, extent_node_t *node, void *arg)\n{\n\tcheck_data_cb_t *arg_cb = arg;\n\n\tif (node->size == 0) {\n\t\targ_cb->error += 1;\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"chunk 0x%p size is zero\\n\", node);\n\t\t/* returns value other than NULL to break iteration */\n\t\treturn (void*)(UINTPTR_MAX);\n\t}\n\n\targ_cb->size += node->size;\n\n\tif (node->zeroed && check_is_unzeroed(node->addr, node->size)) {\n\t\targ_cb->error += 1;\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"chunk 0x%p, is marked as zeroed, but is dirty\\n\",\n\t\t\tnode->addr);\n\t\t/* returns value other than NULL to break iteration */\n\t\treturn (void*)(UINTPTR_MAX);\n\t}\n\n\t/* check chunks address is inside pool memory */\n\tpool_memory_range_node_t *list = arg_cb->list;\n\tuintptr_t addr = (uintptr_t)node->addr;\n\tuintptr_t addr_end = (uintptr_t)node->addr + node->size;\n\twhile (list != NULL) {\n\t\tif ((list->usable_addr <= addr) &&\n\t\t\t(addr < list->usable_addr_end) &&\n\t\t\t(list->usable_addr < addr_end) &&\n\t\t\t(addr_end <= list->usable_addr_end)) {\n\t\t\t\t/* return NULL to continue iterations of tree */\n\t\t\t\treturn (NULL);\n\t\t}\n\t\tlist = list->next;\n\t}\n\n\targ_cb->error += 1;\n\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"incorrect address chunk 0x%p, out of memory pool\\n\",\n\t\t\tnode->addr);\n\n\t/* returns value other than NULL to break iteration */\n\treturn (void*)(UINTPTR_MAX);\n}\n\nstatic arena_chunk_map_t *\ncheck_tree_chunks_avail_iter_cb(arena_avail_tree_t *tree,\n\tarena_chunk_map_t *map, void *arg)\n{\n\tcheck_data_cb_t *arg_cb = arg;\n\n\tif ((map->bits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) != 0) {\n\t\targ_cb->error += 1;\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"flags in map->bits %zu are incorrect\\n\", map->bits);\n\t\t/* returns value other than NULL to break iteration */\n\t\treturn (void*)(UINTPTR_MAX);\n\t}\n\n\tif ((map->bits & ~PAGE_MASK) == 0) {\n\t\targ_cb->error += 1;\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"chunk_map 0x%p size is zero\\n\", map);\n\t\t/* returns value other than NULL to break iteration */\n\t\treturn (void*)(UINTPTR_MAX);\n\t}\n\n\tsize_t chunk_size = (map->bits & ~PAGE_MASK);\n\targ_cb->size += chunk_size;\n\n\tarena_chunk_t *run_chunk = CHUNK_ADDR2BASE(map);\n\tsize_t pageind = arena_mapelm_to_pageind(map);\n\tvoid *chunk_addr = (void *)((uintptr_t)run_chunk + (pageind << LG_PAGE));\n\n\tif (((map->bits & (CHUNK_MAP_UNZEROED | CHUNK_MAP_DIRTY)) == 0) &&\n\t\t\tcheck_is_unzeroed(chunk_addr, chunk_size)) {\n\t\targ_cb->error += 1;\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"chunk_map 0x%p, is marked as zeroed, but is dirty\\n\",\n\t\t\tmap);\n\t\t/* returns value other than NULL to break iteration */\n\t\treturn (void*)(UINTPTR_MAX);\n\t}\n\n\t/* check chunks address is inside pool memory */\n\tpool_memory_range_node_t *list = arg_cb->list;\n\tuintptr_t addr = (uintptr_t)chunk_addr;\n\tuintptr_t addr_end = (uintptr_t)chunk_addr +  chunk_size;\n\twhile (list != NULL) {\n\t\tif ((list->usable_addr <= addr) &&\n\t\t\t(addr < list->usable_addr_end) &&\n\t\t\t(list->usable_addr < addr_end) &&\n\t\t\t(addr_end <= list->usable_addr_end)) {\n\t\t\t\t/* return NULL to continue iterations of tree */\n\t\t\t\treturn (NULL);\n\t\t}\n\t\tlist = list->next;\n\t}\n\n\targ_cb->error += 1;\n\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\"incorrect address chunk_map 0x%p, out of memory pool\\n\",\n\t\t\tchunk_addr);\n\n\t/* returns value other than NULL to break iteration */\n\treturn (void*)(UINTPTR_MAX);\n}\n\nint\nje_pool_check(pool_t *pool)\n{\n\tsize_t total_size = 0;\n\tunsigned i;\n\tpool_memory_range_node_t *node;\n\n\tmalloc_mutex_lock(&pools_lock);\n\n\tif ((pool->pool_id == 0) || (pool->pool_id >= npools)) {\n\t\tmalloc_write(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\t\"invalid pool id\\n\");\n\t\tmalloc_mutex_unlock(&pools_lock);\n\t\treturn -1;\n\t}\n\n\tif (pools[pool->pool_id] != pool) {\n\t\tmalloc_write(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\t\"invalid pool handle, probably pool was deleted\\n\");\n\t\tmalloc_mutex_unlock(&pools_lock);\n\t\treturn -1;\n\t}\n\n\tmalloc_mutex_lock(&pool->memory_range_mtx);\n\n\t/* check memory regions defined correctly */\n\tnode = pool->memory_range_list;\n\twhile (node != NULL) {\n\t\tsize_t node_size = node->usable_addr_end - node->usable_addr;\n\t\ttotal_size += node_size;\n\t\tif ((node->addr > node->usable_addr) ||\n\t\t\t(node->addr_end < node->usable_addr_end) ||\n\t\t\t(node->usable_addr >= node->usable_addr_end)) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\t\t\"corrupted pool memory\\n\");\n\t\t\tmalloc_mutex_unlock(&pool->memory_range_mtx);\n\t\t\tmalloc_mutex_unlock(&pools_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* for the purpose of further checks we need to mark it as defined */\n\t\tJEMALLOC_VALGRIND_MAKE_MEM_DEFINED((void *)node->usable_addr,\n\t\t\t\tnode_size);\n\n\t\tnode = node->next;\n\t}\n\n\t/* check memory collision with other pools */\n\tfor (i = 1; i < npools; i++) {\n\t\tpool_t *pool_cmp = pools[i];\n\t\tif (pool_cmp != NULL && i != pool->pool_id) {\n\t\t\tnode = pool->memory_range_list;\n\t\t\twhile (node != NULL) {\n\t\t\t\tpool_memory_range_node_t *node2 = pool_cmp->memory_range_list;\n\t\t\t\twhile (node2 != NULL) {\n\t\t\t\t\tif ((node->addr <= node2->addr &&\n\t\t\t\t\t\t\tnode2->addr < node->addr_end) ||\n\t\t\t\t\t\t\t(node2->addr <= node->addr &&\n\t\t\t\t\t\t\tnode->addr < node2->addr_end)) {\n\n\t\t\t\t\t\tmalloc_write(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\t\t\t\t\"pool uses the same as another pool\\n\");\n\t\t\t\t\t\tmalloc_mutex_unlock(&pool->memory_range_mtx);\n\t\t\t\t\t\tmalloc_mutex_unlock(&pools_lock);\n\t\t\t\t\t\treturn 0;\n\t\t\t\t\t}\n\t\t\t\t\tnode2 = node2->next;\n\t\t\t\t}\n\t\t\t\tnode = node->next;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* check the addresses of the chunks are inside memory region */\n\tcheck_data_cb_t arg_cb;\n\targ_cb.list = pool->memory_range_list;\n\targ_cb.size = 0;\n\targ_cb.error = 0;\n\n\tmalloc_mutex_lock(&pool->chunks_mtx);\n\tmalloc_rwlock_wrlock(&pool->arenas_lock);\n\textent_tree_szad_iter(&pool->chunks_szad_mmap, NULL,\n\t\tcheck_tree_binary_iter_cb, &arg_cb);\n\n\tfor (i = 0; i < pool->narenas_total && arg_cb.error == 0; ++i) {\n\t\tarena_t *arena = pool->arenas[i];\n\t\tif (arena != NULL) {\n\t\t\tmalloc_mutex_lock(&arena->lock);\n\n\t\t\tarena_runs_avail_tree_iter(arena, check_tree_chunks_avail_iter_cb,\n\t\t\t\t&arg_cb);\n\n\t\t\tarena_chunk_t *spare = arena->spare;\n\t\t\tif (spare != NULL) {\n\t\t\t\tsize_t spare_size = arena_mapbits_unallocated_size_get(spare,\n\t\t\t\t\tmap_bias);\n\n\t\t\t\targ_cb.size += spare_size;\n\n\t\t\t\t/* check that spare is zeroed */\n\t\t\t\tif ((arena_mapbits_unzeroed_get(spare, map_bias) == 0) &&\n\t\t\t\t\t\tcheck_is_unzeroed(\n\t\t\t\t\t\t\t(void *)((uintptr_t)spare + (map_bias << LG_PAGE)),\n\t\t\t\t\t\t\tspare_size)) {\n\n\t\t\t\t\targ_cb.error += 1;\n\t\t\t\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): \"\n\t\t\t\t\t\t\"spare 0x%p, is marked as zeroed, but is dirty\\n\",\n\t\t\t\t\t\tspare);\n\t\t\t\t}\n\t\t\t}\n\t\t\tmalloc_mutex_unlock(&arena->lock);\n\t\t}\n\t}\n\n\tmalloc_rwlock_unlock(&pool->arenas_lock);\n\tmalloc_mutex_unlock(&pool->chunks_mtx);\n\n\tmalloc_mutex_unlock(&pool->memory_range_mtx);\n\tmalloc_mutex_unlock(&pools_lock);\n\n\tif (arg_cb.error != 0) {\n\t\treturn 0;\n\t}\n\n\tif (total_size < arg_cb.size) {\n\t\tmalloc_printf(\"<jemalloc>: Error in pool_check(): total size of all \"\n\t\t\t\"chunks: %zu is greater than associated memory range size: %zu\\n\",\n\t\t\targ_cb.size, total_size);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/*\n * add more memory to a pool\n */\nsize_t\nje_pool_extend(pool_t *pool, void *addr, size_t size, int zeroed)\n{\n\tchar *usable_addr = addr;\n\tsize_t nodes_number = size/chunksize;\n\tif (size < POOL_MINIMAL_SIZE)\n\t\treturn 0;\n\n\t/* preallocate the chunk tree nodes for the max possible number of chunks */\n\tnodes_number = base_node_prealloc(pool, nodes_number);\n\tpool_memory_range_node_t *node = base_alloc(pool,\n\t\tsizeof (*pool->memory_range_list));\n\n\tif (nodes_number > 0 || node == NULL) {\n\t\t/*\n\t\t * If base allocation using existing chunks fails, then use the new\n\t\t * chunk as a source for further base allocations.\n\t\t */\n\t\tmalloc_mutex_lock(&pool->base_mtx);\n\t\t/* preinit base allocator in unused space */\n\t\tpool->base_next_addr = (void *)CACHELINE_CEILING((uintptr_t)addr);\n\t\tpool->base_past_addr = (void *)((uintptr_t)addr + size);\n\t\tmalloc_mutex_unlock(&pool->base_mtx);\n\n\t\tif (nodes_number > 0)\n\t\t\tnodes_number = base_node_prealloc(pool, nodes_number);\n\t\tassert(nodes_number == 0);\n\n\t\tif (node == NULL)\n\t\t\tnode = base_alloc(pool, sizeof (*pool->memory_range_list));\n\t\tassert(node != NULL);\n\n\t\t/* pointer to the address of chunks, align the address to chunksize */\n\t\tusable_addr = (void *)CHUNK_CEILING((uintptr_t)pool->base_next_addr);\n\t\t/* reduce end of base allocator up to chunks */\n\t\tpool->base_past_addr = usable_addr;\n\t}\n\n\tusable_addr = (void *)CHUNK_CEILING((uintptr_t)usable_addr);\n\n\tsize_t usable_size = (size - (uintptr_t)(usable_addr - (char *)addr))\n\t\t& ~chunksize_mask;\n\n\tassert(usable_size > 0);\n\n\tnode->addr = (uintptr_t)addr;\n\tnode->addr_end = (uintptr_t)addr + size;\n\tnode->usable_addr = (uintptr_t)usable_addr;\n\tnode->usable_addr_end = (uintptr_t)usable_addr + usable_size;\n\n\tmalloc_mutex_lock(&pool->memory_range_mtx);\n\tnode->next = pool->memory_range_list;\n\tpool->memory_range_list = node;\n\n\tchunk_record(pool,\n\t\t&pool->chunks_szad_mmap, &pool->chunks_ad_mmap,\n\t\tusable_addr, usable_size, zeroed);\n\n\tmalloc_mutex_unlock(&pool->memory_range_mtx);\n\n\treturn usable_size;\n}\n\nstatic void *\npool_ialloc_prof_sample(pool_t *pool, size_t usize, prof_thr_cnt_t *cnt,\n\tvoid *(*ialloc)(pool_t *, size_t))\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = ialloc(pool, SMALL_MAXCLASS+1);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = ialloc(pool, usize);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\npool_ialloc_prof(pool_t *pool, size_t usize,\n\tvoid *(*ialloc)(pool_t *, size_t))\n{\n\tvoid *p;\n\tprof_thr_cnt_t *cnt;\n\n\tPROF_ALLOC_PREP(usize, cnt);\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = pool_ialloc_prof_sample(pool, usize, cnt, ialloc);\n\telse\n\t\tp = ialloc(pool, usize);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_malloc(p, usize, cnt);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\npool_imalloc_body(pool_t *pool, size_t size, size_t *usize)\n{\n\n\tif (malloc_init())\n\t\treturn (NULL);\n\n\tif (config_prof && opt_prof) {\n\t\t*usize = s2u(size);\n\t\treturn (pool_ialloc_prof(pool, *usize, pool_imalloc));\n\t}\n\n\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t*usize = s2u(size);\n\treturn (pool_imalloc(pool, size));\n}\n\nvoid *\nje_pool_malloc(pool_t *pool, size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tret = pool_imalloc_body(pool, size, &usize);\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in pool_malloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && ret != NULL) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, false);\n\treturn (ret);\n}\n\nvoid *\nje_pool_calloc(pool_t *pool, size_t num, size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t num_size;\n\n\tnum_size = num * size;\n\tif (num_size == 0) {\n\t\tif (num == 0 || size == 0)\n\t\t\tnum_size = 1;\n\t\telse {\n\t\t\tret = NULL;\n\t\t\tgoto label_return;\n\t\t}\n\n\t} else if (((num | size) & (SIZE_T_MAX << (sizeof(size_t) << 2)))\n\t    && (num_size / size != num)) {\n\t\tret = NULL;\n\t\tgoto label_return;\n\t}\n\n\tif (config_prof && opt_prof) {\n\t\tusize = s2u(num_size);\n\t\tret = pool_ialloc_prof(pool, usize, pool_icalloc);\n\t} else {\n\t\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t\tusize = s2u(num_size);\n\t\tret = pool_icalloc(pool, num_size);\n\t}\n\nlabel_return:\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in pool_calloc(): \"\n\t\t\t\t\"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && ret != NULL) {\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, num_size, ret);\n\tJEMALLOC_VALGRIND_MALLOC(ret != NULL, ret, usize, true);\n\treturn (ret);\n}\n\nstatic void *\npool_irealloc_prof_sample(pool_t *pool, void *oldptr, size_t usize,\n\tprof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = pool_iralloc(pool, oldptr, SMALL_MAXCLASS+1, 0, 0, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = pool_iralloc(pool, oldptr, usize, 0, 0, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\npool_irealloc_prof(pool_t *pool, void *oldptr, size_t old_usize,\n\tsize_t usize, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\tprof_ctx_t *old_ctx;\n\n\told_ctx = prof_ctx_get(oldptr);\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = pool_irealloc_prof_sample(pool, oldptr, usize, cnt);\n\telse\n\t\tp = pool_iralloc(pool, oldptr, usize, 0, 0, false);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_realloc(p, usize, cnt, old_usize, old_ctx);\n\n\treturn (p);\n}\n\nJEMALLOC_INLINE_C void\npool_ifree(pool_t *pool, void *ptr)\n{\n\tsize_t usize;\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tarena_chunk_t *chunk;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (config_prof && opt_prof) {\n\t\tusize = isalloc(ptr, config_prof);\n\t\tprof_free(ptr, usize);\n\t} else if (config_stats || config_valgrind)\n\t\tusize = isalloc(ptr, config_prof);\n\tif (config_stats)\n\t\tthread_allocated_tsd_get()->deallocated += usize;\n\tif (config_valgrind && in_valgrind)\n\t\trzsize = p2rz(ptr);\n\n\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n\tif (chunk != ptr)\n\t\tarena_dalloc(chunk, ptr, true);\n\telse\n\t\thuge_dalloc(pool, ptr);\n\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n}\n\nvoid *\nje_pool_ralloc(pool_t *pool, void *ptr, size_t size)\n{\n\tvoid *ret;\n\tsize_t usize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t old_usize = 0;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\n\tif (size == 0) {\n\t\tif (ptr != NULL) {\n\t\t\t/* realloc(ptr, 0) is equivalent to free(ptr). */\n\t\t\tUTRACE(ptr, 0, 0);\n\t\t\tpool_ifree(pool, ptr);\n\t\t\treturn (NULL);\n\t\t}\n\t\tsize = 1;\n\t}\n\n\tif (ptr != NULL) {\n\t\tassert(malloc_initialized || IS_INITIALIZER);\n\t\tmalloc_init();\n\n\t\tif ((config_prof && opt_prof) || config_stats ||\n\t\t    (config_valgrind && in_valgrind))\n\t\t\told_usize = isalloc(ptr, config_prof);\n\t\tif (config_valgrind && in_valgrind)\n\t\t\told_rzsize = config_prof ? p2rz(ptr) : u2rz(old_usize);\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tprof_thr_cnt_t *cnt;\n\n\t\t\tusize = s2u(size);\n\t\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\t\tret = pool_irealloc_prof(pool, ptr, old_usize,\n\t\t\t\tusize, cnt);\n\t\t} else {\n\t\t\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t\t\tusize = s2u(size);\n\t\t\tret = pool_iralloc(pool, ptr, size, 0, 0, false);\n\t\t}\n\t} else {\n\t\t/* realloc(NULL, size) is equivalent to malloc(size). */\n\t\tret = pool_imalloc_body(pool, size, &usize);\n\t}\n\n\tif (ret == NULL) {\n\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\tmalloc_write(\"<jemalloc>: Error in pool_ralloc(): \"\n\t\t\t    \"out of memory\\n\");\n\t\t\tabort();\n\t\t}\n\t\tset_errno(ENOMEM);\n\t}\n\tif (config_stats && ret != NULL) {\n\t\tthread_allocated_t *ta;\n\t\tassert(usize == isalloc(ret, config_prof));\n\t\tta = thread_allocated_tsd_get();\n\t\tta->allocated += usize;\n\t\tta->deallocated += old_usize;\n\t}\n\tUTRACE(ptr, size, ret);\n\n\tJEMALLOC_VALGRIND_REALLOC(true, ret, usize, true, ptr, old_usize,\n\t    old_rzsize, true, false);\n\treturn (ret);\n}\n\nstatic void *\npool_imemalign_prof_sample(pool_t *pool, size_t alignment, size_t usize,\n\tprof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tassert(sa2u(SMALL_MAXCLASS+1, alignment) != 0);\n\t\tp = pool_ipalloc(pool, sa2u(SMALL_MAXCLASS+1, alignment),\n\t\t\talignment, false);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = pool_ipalloc(pool, usize, alignment, false);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\npool_imemalign_prof(pool_t *pool, size_t alignment, size_t usize,\n\tprof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = pool_imemalign_prof_sample(pool, alignment, usize, cnt);\n\telse\n\t\tp = pool_ipalloc(pool, usize, alignment, false);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_malloc(p, usize, cnt);\n\n\treturn (p);\n}\n\nJEMALLOC_ATTR(nonnull(1))\nstatic int\npool_imemalign(pool_t *pool, void **memptr, size_t alignment, size_t size,\n\tsize_t min_alignment)\n{\n\tint ret;\n\tsize_t usize;\n\tvoid *result;\n\n\tassert(min_alignment != 0);\n\n\tif (malloc_init()) {\n\t\tresult = NULL;\n\t\tgoto label_oom;\n\t} else {\n\t\tif (size == 0)\n\t\t\tsize = 1;\n\n\t\t/* Make sure that alignment is a large enough power of 2. */\n\t\tif (((alignment - 1) & alignment) != 0\n\t\t    || (alignment < min_alignment)) {\n\t\t\tif (config_xmalloc && opt_xmalloc) {\n\t\t\t\tmalloc_write(\"<jemalloc>: Error allocating pool\"\n\t\t\t\t    \" aligned memory: invalid alignment\\n\");\n\t\t\t\tabort();\n\t\t\t}\n\t\t\tresult = NULL;\n\t\t\tret = EINVAL;\n\t\t\tgoto label_return;\n\t\t}\n\n\t\tusize = sa2u(size, alignment);\n\t\tif (usize == 0) {\n\t\t\tresult = NULL;\n\t\t\tgoto label_oom;\n\t\t}\n\n\t\tif (config_prof && opt_prof) {\n\t\t\tprof_thr_cnt_t *cnt;\n\n\t\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\t\tresult = pool_imemalign_prof(pool, alignment,\n\t\t\t\tusize, cnt);\n\t\t} else\n\t\t\tresult = pool_ipalloc(pool, usize, alignment, false);\n\t\tif (result == NULL)\n\t\t\tgoto label_oom;\n\t}\n\n\t*memptr = result;\n\tret = 0;\nlabel_return:\n\tif (config_stats && result != NULL) {\n\t\tassert(usize == isalloc(result, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, result);\n\treturn (ret);\nlabel_oom:\n\tassert(result == NULL);\n\tif (config_xmalloc && opt_xmalloc) {\n\t\tmalloc_write(\"<jemalloc>: Error allocating pool \"\n\t\t\t\"aligned memory: out of memory\\n\");\n\t\tabort();\n\t}\n\tret = ENOMEM;\n\tgoto label_return;\n}\n\nvoid *\nje_pool_aligned_alloc(pool_t *pool, size_t alignment, size_t size)\n{\n\tvoid *ret;\n\tint err;\n\n\tif ((err = pool_imemalign(pool, &ret, alignment, size, 1)) != 0) {\n\t\tret = NULL;\n\t\tset_errno(err);\n\t}\n\tJEMALLOC_VALGRIND_MALLOC(err == 0, ret, isalloc(ret, config_prof),\n\t    false);\n\treturn (ret);\n}\n\nvoid\nje_pool_free(pool_t *pool, void *ptr)\n{\n\tUTRACE(ptr, 0, 0);\n\tif (ptr != NULL)\n\t\tpool_ifree(pool, ptr);\n}\n\nvoid\nje_pool_malloc_stats_print(pool_t *pool,\n\t\t\t\tvoid (*write_cb)(void *, const char *),\n\t\t\t\tvoid *cbopaque, const char *opts)\n{\n\n\tstats_print(pool, write_cb, cbopaque, opts);\n}\n\nvoid\nje_pool_set_alloc_funcs(void *(*malloc_func)(size_t),\n\t\t\t\tvoid (*free_func)(void *))\n{\n\tif (malloc_func != NULL && free_func != NULL) {\n\t\tmalloc_mutex_lock(&pool_base_lock);\n\t\tif (pools == NULL) {\n\t\t\tbase_malloc_fn = malloc_func;\n\t\t\tbase_free_fn = free_func;\n\t\t}\n\t\tmalloc_mutex_unlock(&pool_base_lock);\n\t}\n}\n\nsize_t\nje_pool_malloc_usable_size(pool_t *pool, void *ptr)\n{\n\tassert(malloc_initialized || IS_INITIALIZER);\n\tif (malloc_thread_init())\n\t\treturn 0;\n\n\tif (config_ivsalloc) {\n\t\t/* Return 0 if ptr is not within a chunk managed by jemalloc. */\n\t\tif (rtree_get(pool->chunks_rtree, (uintptr_t)CHUNK_ADDR2BASE(ptr)) == 0)\n\t\t\treturn 0;\n\t}\n\n\treturn (ptr != NULL) ? pool_isalloc(pool, ptr, config_prof) : 0;\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx(size_t usize, size_t alignment, bool zero, bool try_tcache,\n    arena_t *arena)\n{\n\n\tassert(usize == ((alignment == 0) ? s2u(usize) : sa2u(usize,\n\t    alignment)));\n\n\tif (alignment != 0)\n\t\treturn (ipalloct(usize, alignment, zero, try_tcache, arena));\n\telse if (zero)\n\t\treturn (icalloct(usize, try_tcache, arena));\n\telse\n\t\treturn (imalloct(usize, try_tcache, arena));\n}\n\nstatic void *\nimallocx_prof_sample(size_t usize, size_t alignment, bool zero, bool try_tcache,\n    arena_t *arena, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tsize_t usize_promoted = (alignment == 0) ?\n\t\t    s2u(SMALL_MAXCLASS+1) : sa2u(SMALL_MAXCLASS+1, alignment);\n\t\tassert(usize_promoted != 0);\n\t\tp = imallocx(usize_promoted, alignment, zero, try_tcache,\n\t\t    arena);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else\n\t\tp = imallocx(usize, alignment, zero, try_tcache, arena);\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nimallocx_prof(size_t usize, size_t alignment, bool zero, bool try_tcache,\n    arena_t *arena, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif ((uintptr_t)cnt != (uintptr_t)1U) {\n\t\tp = imallocx_prof_sample(usize, alignment, zero, try_tcache,\n\t\t    arena, cnt);\n\t} else\n\t\tp = imallocx(usize, alignment, zero, try_tcache, arena);\n\tif (p == NULL)\n\t\treturn (NULL);\n\tprof_malloc(p, usize, cnt);\n\n\treturn (p);\n}\n\nvoid *\nje_mallocx(size_t size, int flags)\n{\n\tvoid *p;\n\tsize_t usize;\n\tsize_t alignment = (ZU(1) << (flags & MALLOCX_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\tbool zero = flags & MALLOCX_ZERO;\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tpool_t *pool = &base_pool;\n\tarena_t dummy_arena;\n\tDUMMY_ARENA_INITIALIZE(dummy_arena, pool);\n\tarena_t *arena;\n\tbool try_tcache;\n\n\tassert(size != 0);\n\n\tif (malloc_init_base_pool())\n\t\tgoto label_oom;\n\n\tif (arena_ind != UINT_MAX) {\n\t\tmalloc_rwlock_rdlock(&pool->arenas_lock);\n\t\tarena = pool->arenas[arena_ind];\n\t\tmalloc_rwlock_unlock(&pool->arenas_lock);\n\t\ttry_tcache = false;\n\t} else {\n\t\tarena = &dummy_arena;\n\t\ttry_tcache = true;\n\t}\n\n\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\tassert(usize != 0);\n\n\tif (config_prof && opt_prof) {\n\t\tprof_thr_cnt_t *cnt;\n\n\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\tp = imallocx_prof(usize, alignment, zero, try_tcache, arena,\n\t\t    cnt);\n\t} else\n\t\tp = imallocx(usize, alignment, zero, try_tcache, arena);\n\tif (p == NULL)\n\t\tgoto label_oom;\n\n\tif (config_stats) {\n\t\tassert(usize == isalloc(p, config_prof));\n\t\tthread_allocated_tsd_get()->allocated += usize;\n\t}\n\tUTRACE(0, size, p);\n\tJEMALLOC_VALGRIND_MALLOC(true, p, usize, zero);\n\treturn (p);\nlabel_oom:\n\tif (config_xmalloc && opt_xmalloc) {\n\t\tmalloc_write(\"<jemalloc>: Error in mallocx(): out of memory\\n\");\n\t\tabort();\n\t}\n\tUTRACE(0, size, 0);\n\treturn (NULL);\n}\n\nstatic void *\nirallocx_prof_sample(void *oldptr, size_t size, size_t alignment, size_t usize,\n    bool zero, bool try_tcache_alloc, bool try_tcache_dalloc, arena_t *arena,\n    prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\n\tif (cnt == NULL)\n\t\treturn (NULL);\n\tif (usize <= SMALL_MAXCLASS) {\n\t\tp = iralloct(oldptr, SMALL_MAXCLASS+1, (SMALL_MAXCLASS+1 >=\n\t\t    size) ? 0 : size - (SMALL_MAXCLASS+1), alignment, zero,\n\t\t    try_tcache_alloc, try_tcache_dalloc, arena);\n\t\tif (p == NULL)\n\t\t\treturn (NULL);\n\t\tarena_prof_promoted(p, usize);\n\t} else {\n\t\tp = iralloct(oldptr, size, 0, alignment, zero,\n\t\t    try_tcache_alloc, try_tcache_dalloc, arena);\n\t}\n\n\treturn (p);\n}\n\nJEMALLOC_ALWAYS_INLINE_C void *\nirallocx_prof(void *oldptr, size_t old_usize, size_t size, size_t alignment,\n    size_t *usize, bool zero, bool try_tcache_alloc, bool try_tcache_dalloc,\n    arena_t *arena, prof_thr_cnt_t *cnt)\n{\n\tvoid *p;\n\tprof_ctx_t *old_ctx;\n\n\told_ctx = prof_ctx_get(oldptr);\n\tif ((uintptr_t)cnt != (uintptr_t)1U)\n\t\tp = irallocx_prof_sample(oldptr, size, alignment, *usize, zero,\n\t\t    try_tcache_alloc, try_tcache_dalloc, arena, cnt);\n\telse {\n\t\tp = iralloct(oldptr, size, 0, alignment, zero,\n\t\t    try_tcache_alloc, try_tcache_dalloc, arena);\n\t}\n\tif (p == NULL)\n\t\treturn (NULL);\n\n\tif (p == oldptr && alignment != 0) {\n\t\t/*\n\t\t * The allocation did not move, so it is possible that the size\n\t\t * class is smaller than would guarantee the requested\n\t\t * alignment, and that the alignment constraint was\n\t\t * serendipitously satisfied.  Additionally, old_usize may not\n\t\t * be the same as the current usize because of in-place large\n\t\t * reallocation.  Therefore, query the actual value of usize.\n\t\t */\n\t\t*usize = isalloc(p, config_prof);\n\t}\n\tprof_realloc(p, *usize, cnt, old_usize, old_ctx);\n\n\treturn (p);\n}\n\nvoid *\nje_rallocx(void *ptr, size_t size, int flags)\n{\n\tvoid *p;\n\tsize_t usize, old_usize;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = (ZU(1) << (flags & MALLOCX_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\tbool zero = flags & MALLOCX_ZERO;\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tpool_t *pool = &base_pool;\n\tarena_t dummy_arena;\n\tDUMMY_ARENA_INITIALIZE(dummy_arena, pool);\n\tbool try_tcache_alloc, try_tcache_dalloc;\n\tarena_t *arena;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\tif (malloc_thread_init())\n\t\treturn (NULL);\n\n\tif (arena_ind != UINT_MAX) {\n\t\tarena_chunk_t *chunk;\n\t\ttry_tcache_alloc = false;\n\t\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n\t\ttry_tcache_dalloc = (chunk == ptr || chunk->arena !=\n\t\t    pool->arenas[arena_ind]);\n\t\tarena = pool->arenas[arena_ind];\n\t} else {\n\t\ttry_tcache_alloc = true;\n\t\ttry_tcache_dalloc = true;\n\t\tarena = &dummy_arena;\n\t}\n\n\tif ((config_prof && opt_prof) || config_stats ||\n\t    (config_valgrind && in_valgrind))\n\t\told_usize = isalloc(ptr, config_prof);\n\tif (config_valgrind && in_valgrind)\n\t\told_rzsize = u2rz(old_usize);\n\n\tif (config_prof && opt_prof) {\n\t\tprof_thr_cnt_t *cnt;\n\n\t\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\t\tassert(usize != 0);\n\t\tPROF_ALLOC_PREP(usize, cnt);\n\t\tp = irallocx_prof(ptr, old_usize, size, alignment, &usize, zero,\n\t\t    try_tcache_alloc, try_tcache_dalloc, arena, cnt);\n\t\tif (p == NULL)\n\t\t\tgoto label_oom;\n\t} else {\n\t\tp = iralloct(ptr, size, 0, alignment, zero, try_tcache_alloc,\n\t\t    try_tcache_dalloc, arena);\n\t\tif (p == NULL)\n\t\t\tgoto label_oom;\n\t\tif (config_stats || (config_valgrind && in_valgrind))\n\t\t\tusize = isalloc(p, config_prof);\n\t}\n\n\tif (config_stats) {\n\t\tthread_allocated_t *ta;\n\t\tta = thread_allocated_tsd_get();\n\t\tta->allocated += usize;\n\t\tta->deallocated += old_usize;\n\t}\n\tUTRACE(ptr, size, p);\n\tJEMALLOC_VALGRIND_REALLOC(true, p, usize, false, ptr, old_usize,\n\t    old_rzsize, false, zero);\n\treturn (p);\nlabel_oom:\n\tif (config_xmalloc && opt_xmalloc) {\n\t\tmalloc_write(\"<jemalloc>: Error in rallocx(): out of memory\\n\");\n\t\tabort();\n\t}\n\tUTRACE(ptr, size, 0);\n\treturn (NULL);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\nixallocx_helper(void *ptr, size_t old_usize, size_t size, size_t extra,\n    size_t alignment, bool zero, arena_t *arena)\n{\n\tsize_t usize;\n\n\tif (ixalloc(ptr, size, extra, alignment, zero))\n\t\treturn (old_usize);\n\tusize = isalloc(ptr, config_prof);\n\n\treturn (usize);\n}\n\nstatic size_t\nixallocx_prof_sample(void *ptr, size_t old_usize, size_t size, size_t extra,\n    size_t alignment, size_t max_usize, bool zero, arena_t *arena,\n    prof_thr_cnt_t *cnt)\n{\n\tsize_t usize;\n\n\tif (cnt == NULL)\n\t\treturn (old_usize);\n\t/* Use minimum usize to determine whether promotion may happen. */\n\tif (((alignment == 0) ? s2u(size) : sa2u(size, alignment)) <=\n\t    SMALL_MAXCLASS) {\n\t\tif (ixalloc(ptr, SMALL_MAXCLASS+1, (SMALL_MAXCLASS+1 >=\n\t\t    size+extra) ? 0 : size+extra - (SMALL_MAXCLASS+1),\n\t\t    alignment, zero))\n\t\t\treturn (old_usize);\n\t\tusize = isalloc(ptr, config_prof);\n\t\tif (max_usize < PAGE)\n\t\t\tarena_prof_promoted(ptr, usize);\n\t} else {\n\t\tusize = ixallocx_helper(ptr, old_usize, size, extra, alignment,\n\t\t    zero, arena);\n\t}\n\n\treturn (usize);\n}\n\nJEMALLOC_ALWAYS_INLINE_C size_t\nixallocx_prof(void *ptr, size_t old_usize, size_t size, size_t extra,\n    size_t alignment, size_t max_usize, bool zero, arena_t *arena,\n    prof_thr_cnt_t *cnt)\n{\n\tsize_t usize;\n\tprof_ctx_t *old_ctx;\n\n\told_ctx = prof_ctx_get(ptr);\n\tif ((uintptr_t)cnt != (uintptr_t)1U) {\n\t\tusize = ixallocx_prof_sample(ptr, old_usize, size, extra,\n\t\t    alignment, zero, max_usize, arena, cnt);\n\t} else {\n\t\tusize = ixallocx_helper(ptr, old_usize, size, extra, alignment,\n\t\t    zero, arena);\n\t}\n\tif (usize == old_usize)\n\t\treturn (usize);\n\tprof_realloc(ptr, usize, cnt, old_usize, old_ctx);\n\n\treturn (usize);\n}\n\nsize_t\nje_xallocx(void *ptr, size_t size, size_t extra, int flags)\n{\n\tsize_t usize, old_usize;\n\tUNUSED size_t old_rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tsize_t alignment = (ZU(1) << (flags & MALLOCX_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\tbool zero = flags & MALLOCX_ZERO;\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tpool_t *pool = &base_pool;\n\tarena_t dummy_arena;\n\tDUMMY_ARENA_INITIALIZE(dummy_arena, pool);\n\tarena_t *arena;\n\n\tassert(ptr != NULL);\n\tassert(size != 0);\n\tassert(SIZE_T_MAX - size >= extra);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\tif (malloc_thread_init())\n\t\t return (0);\n\n\tif (arena_ind != UINT_MAX)\n\t\tarena = pool->arenas[arena_ind];\n\telse\n\t\tarena = &dummy_arena;\n\n\told_usize = isalloc(ptr, config_prof);\n\tif (config_valgrind && in_valgrind)\n\t\told_rzsize = u2rz(old_usize);\n\n\tif (config_prof && opt_prof) {\n\t\tprof_thr_cnt_t *cnt;\n\t\t/*\n\t\t * usize isn't knowable before ixalloc() returns when extra is\n\t\t * non-zero.  Therefore, compute its maximum possible value and\n\t\t * use that in PROF_ALLOC_PREP() to decide whether to capture a\n\t\t * backtrace.  prof_realloc() will use the actual usize to\n\t\t * decide whether to sample.\n\t\t */\n\t\tsize_t max_usize = (alignment == 0) ? s2u(size+extra) :\n\t\t    sa2u(size+extra, alignment);\n\t\tPROF_ALLOC_PREP(max_usize, cnt);\n\t\tusize = ixallocx_prof(ptr, old_usize, size, extra, alignment,\n\t\t    max_usize, zero, arena, cnt);\n\t} else {\n\t\tusize = ixallocx_helper(ptr, old_usize, size, extra, alignment,\n\t\t    zero, arena);\n\t}\n\tif (usize == old_usize)\n\t\tgoto label_not_resized;\n\n\tif (config_stats) {\n\t\tthread_allocated_t *ta;\n\t\tta = thread_allocated_tsd_get();\n\t\tta->allocated += usize;\n\t\tta->deallocated += old_usize;\n\t}\n\tJEMALLOC_VALGRIND_REALLOC(false, ptr, usize, false, ptr, old_usize,\n\t    old_rzsize, false, zero);\nlabel_not_resized:\n\tUTRACE(ptr, size, ptr);\n\treturn (usize);\n}\n\nsize_t\nje_sallocx(const void *ptr, int flags)\n{\n\tsize_t usize;\n\n\tassert(malloc_initialized || IS_INITIALIZER);\n\tif (malloc_thread_init())\n\t\treturn (0);\n\n\tif (config_ivsalloc)\n\t\tusize = ivsalloc(ptr, config_prof);\n\telse {\n\t\tassert(ptr != NULL);\n\t\tusize = isalloc(ptr, config_prof);\n\t}\n\n\treturn (usize);\n}\n\nvoid\nje_dallocx(void *ptr, int flags)\n{\n\tsize_t usize;\n\tUNUSED size_t rzsize JEMALLOC_CC_SILENCE_INIT(0);\n\tunsigned arena_ind = ((unsigned)(flags >> 8)) - 1;\n\tpool_t *pool = &base_pool;\n\tbool try_tcache;\n\n\tassert(ptr != NULL);\n\tassert(malloc_initialized || IS_INITIALIZER);\n\n\tif (arena_ind != UINT_MAX) {\n\t\tarena_chunk_t *chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n\t\ttry_tcache = (chunk == ptr || chunk->arena !=\n\t\t    pool->arenas[arena_ind]);\n\t} else\n\t\ttry_tcache = true;\n\n\tUTRACE(ptr, 0, 0);\n\tif (config_stats || config_valgrind)\n\t\tusize = isalloc(ptr, config_prof);\n\tif (config_prof && opt_prof) {\n\t\tif (config_stats == false && config_valgrind == false)\n\t\t\tusize = isalloc(ptr, config_prof);\n\t\tprof_free(ptr, usize);\n\t}\n\tif (config_stats)\n\t\tthread_allocated_tsd_get()->deallocated += usize;\n\tif (config_valgrind && in_valgrind)\n\t\trzsize = p2rz(ptr);\n\tiqalloct(ptr, try_tcache);\n\tJEMALLOC_VALGRIND_FREE(ptr, rzsize);\n}\n\nsize_t\nje_nallocx(size_t size, int flags)\n{\n\tsize_t usize;\n\tsize_t alignment = (ZU(1) << (flags & MALLOCX_LG_ALIGN_MASK)\n\t    & (SIZE_T_MAX-1));\n\n\tassert(size != 0);\n\n\tif (malloc_init_base_pool())\n\t\treturn (0);\n\n\tusize = (alignment == 0) ? s2u(size) : sa2u(size, alignment);\n\tassert(usize != 0);\n\treturn (usize);\n}\n\nint\nje_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,\n    size_t newlen)\n{\n\treturn (ctl_byname(name, oldp, oldlenp, newp, newlen));\n}\n\nint\nje_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp)\n{\n\treturn (ctl_nametomib(name, mibp, miblenp));\n}\n\nint\nje_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,\n  void *newp, size_t newlen)\n{\n\treturn (ctl_bymib(mib, miblen, oldp, oldlenp, newp, newlen));\n}\n\nint\nje_navsnprintf(char *str, size_t size, const char *format, va_list ap)\n{\n\treturn malloc_vsnprintf(str, size, format, ap);\n}\n\nvoid\nje_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,\n    const char *opts)\n{\n\tstats_print(&base_pool, write_cb, cbopaque, opts);\n}\n\nsize_t\nje_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr)\n{\n\tsize_t ret;\n\n\tassert(malloc_initialized || IS_INITIALIZER);\n\tif (malloc_thread_init())\n\t\treturn (0);\n\n\tif (config_ivsalloc)\n\t\tret = ivsalloc(ptr, config_prof);\n\telse\n\t\tret = (ptr != NULL) ? isalloc(ptr, config_prof) : 0;\n\n\treturn (ret);\n}\n\n/*\n * End non-standard functions.\n */\n/******************************************************************************/\n/*\n * The following functions are used by threading libraries for protection of\n * malloc during fork().\n */\n\n/*\n * If an application creates a thread before doing any allocation in the main\n * thread, then calls fork(2) in the main thread followed by memory allocation\n * in the child process, a race can occur that results in deadlock within the\n * child: the main thread may have forked while the created thread had\n * partially initialized the allocator.  Ordinarily jemalloc prevents\n * fork/malloc races via the following functions it registers during\n * initialization using pthread_atfork(), but of course that does no good if\n * the allocator isn't fully initialized at fork time.  The following library\n * constructor is a partial solution to this problem.  It may still possible to\n * trigger the deadlock described above, but doing so would involve forking via\n * a library constructor that runs before jemalloc's runs.\n */\nJEMALLOC_ATTR(constructor(102))\nvoid\njemalloc_constructor(void)\n{\n\n\tmalloc_init();\n}\n\nJEMALLOC_ATTR(destructor(101))\nvoid\njemalloc_destructor(void)\n{\n\tif (base_pool_initialized == false)\n\t\treturn;\n\n\ttcache_thread_cleanup(tcache_tsd_get());\n\tarenas_cleanup(arenas_tsd_get());\n\tje_base_pool_destroy();\n}\n\n#define FOREACH_POOL(func)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tunsigned i;\t\t\t\t\t\t\\\n\tfor (i = 0; i < npools; i++) {\t\t\t\\\n\t\tif (pools[i] != NULL)\t\t\t\t\\\n\t\t\t(func)(pools[i]);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n} while(0)\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_prefork(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_prefork(void)\n#endif\n{\n\tunsigned i, j;\n\tpool_t *pool;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (malloc_initialized == false)\n\t\treturn;\n#endif\n\tassert(malloc_initialized);\n\n\t/* Acquire all mutexes in a safe order. */\n\tctl_prefork();\n\tprof_prefork();\n\tpool_prefork();\n\n\tfor (i = 0; i < npools; i++) {\n\t\tpool = pools[i];\n\t\tif (pool != NULL) {\n\t\t\tmalloc_rwlock_prefork(&pool->arenas_lock);\n\t\t\tfor (j = 0; j < pool->narenas_total; j++) {\n\t\t\t\tif (pool->arenas[j] != NULL)\n\t\t\t\t\tarena_prefork(pool->arenas[j]);\n\t\t\t}\n\t\t}\n\t}\n\n\tFOREACH_POOL(chunk_prefork0);\n\tFOREACH_POOL(base_prefork);\n\tFOREACH_POOL(chunk_prefork1);\n\tchunk_dss_prefork();\n\n\n\tFOREACH_POOL(huge_prefork);\n}\n\n#ifndef JEMALLOC_MUTEX_INIT_CB\nvoid\njemalloc_postfork_parent(void)\n#else\nJEMALLOC_EXPORT void\n_malloc_postfork(void)\n#endif\n{\n\tunsigned i, j;\n\tpool_t *pool;\n\n#ifdef JEMALLOC_MUTEX_INIT_CB\n\tif (malloc_initialized == false)\n\t\treturn;\n#endif\n\tassert(malloc_initialized);\n\n\t/* Release all mutexes, now that fork() has completed. */\n\tFOREACH_POOL(huge_postfork_parent);\n\n\n\tchunk_dss_postfork_parent();\n\tFOREACH_POOL(chunk_postfork_parent1);\n\tFOREACH_POOL(base_postfork_parent);\n\tFOREACH_POOL(chunk_postfork_parent0);\n\n\tfor (i = 0; i < npools; i++) {\n\t\tpool = pools[i];\n\t\tif (pool != NULL) {\n\t\t\tfor (j = 0; j < pool->narenas_total; j++) {\n\t\t\t\tif (pool->arenas[j] != NULL)\n\t\t\t\t\tarena_postfork_parent(pool->arenas[j]);\n\t\t\t}\n\t\t\tmalloc_rwlock_postfork_parent(&pool->arenas_lock);\n\t\t}\n\t}\n\n\tpool_postfork_parent();\n\tprof_postfork_parent();\n\tctl_postfork_parent();\n}\n\nvoid\njemalloc_postfork_child(void)\n{\n\tunsigned i, j;\n\tpool_t *pool;\n\n\tassert(malloc_initialized);\n\n\t/* Release all mutexes, now that fork() has completed. */\n\tFOREACH_POOL(huge_postfork_child);\n\n\n\tchunk_dss_postfork_child();\n\tFOREACH_POOL(chunk_postfork_child1);\n\tFOREACH_POOL(base_postfork_child);\n\tFOREACH_POOL(chunk_postfork_child0);\n\n\tfor (i = 0; i < npools; i++) {\n\t\tpool = pools[i];\n\t\tif (pool != NULL) {\n\t\t\tfor (j = 0; j < pool->narenas_total; j++) {\n\t\t\t\tif (pool->arenas[j] != NULL)\n\t\t\t\t\tarena_postfork_child(pool->arenas[j]);\n\t\t\t}\n\t\t\tmalloc_rwlock_postfork_child(&pool->arenas_lock);\n\t\t}\n\t}\n\n\tpool_postfork_child();\n\tprof_postfork_child();\n\tctl_postfork_child();\n}\n\n/******************************************************************************/\n/*\n * The following functions are used for TLS allocation/deallocation in static\n * binaries on FreeBSD.  The primary difference between these and i[mcd]alloc()\n * is that these avoid accessing TLS variables.\n */\n\nstatic void *\na0alloc(size_t size, bool zero)\n{\n\n\tif (malloc_init_base_pool())\n\t\treturn (NULL);\n\n\tif (size == 0)\n\t\tsize = 1;\n\n\tif (size <= arena_maxclass)\n\t\treturn (arena_malloc(base_pool.arenas[0], size, zero, false));\n\telse\n\t\treturn (huge_malloc(NULL, size, zero));\n}\n\nvoid *\na0malloc(size_t size)\n{\n\n\treturn (a0alloc(size, false));\n}\n\nvoid *\na0calloc(size_t num, size_t size)\n{\n\n\treturn (a0alloc(num * size, true));\n}\n\nvoid\na0free(void *ptr)\n{\n\tarena_chunk_t *chunk;\n\n\tif (ptr == NULL)\n\t\treturn;\n\n\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n\tif (chunk != ptr)\n\t\tarena_dalloc(chunk, ptr, false);\n\telse\n\t\thuge_dalloc(&base_pool, ptr);\n}\n\n/******************************************************************************/\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/doc/librpmem/librpmem.7.md": "---\nlayout: manual\nContent-Style: 'text/css'\ntitle: _MP(LIBRPMEM, 7)\ncollection: librpmem\nheader: PMDK\ndate: rpmem API version 1.2\n...\n\n[comment]: <> (Copyright 2016-2017, Intel Corporation)\n\n[comment]: <> (Redistribution and use in source and binary forms, with or without)\n[comment]: <> (modification, are permitted provided that the following conditions)\n[comment]: <> (are met:)\n[comment]: <> (    * Redistributions of source code must retain the above copyright)\n[comment]: <> (      notice, this list of conditions and the following disclaimer.)\n[comment]: <> (    * Redistributions in binary form must reproduce the above copyright)\n[comment]: <> (      notice, this list of conditions and the following disclaimer in)\n[comment]: <> (      the documentation and/or other materials provided with the)\n[comment]: <> (      distribution.)\n[comment]: <> (    * Neither the name of the copyright holder nor the names of its)\n[comment]: <> (      contributors may be used to endorse or promote products derived)\n[comment]: <> (      from this software without specific prior written permission.)\n\n[comment]: <> (THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS)\n[comment]: <> (\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT)\n[comment]: <> (LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR)\n[comment]: <> (A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT)\n[comment]: <> (OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,)\n[comment]: <> (SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT)\n[comment]: <> (LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,)\n[comment]: <> (DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY)\n[comment]: <> (THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT)\n[comment]: <> ((INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE)\n[comment]: <> (OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.)\n\n[comment]: <> (librpmem.7 -- man page for librpmem)\n\n\n[NAME](#name)<br />\n[SYNOPSIS](#synopsis)<br />\n[DESCRIPTION](#description)<br />\n[TARGET NODE ADDRESS FORMAT](#target-node-address-format)<br />\n[REMOTE POOL ATTRIBUTES](#remote-pool-attributes)<br />\n[SSH](#ssh)<br />\n[FORK](#fork)<br />\n[CAVEATS](#caveats)<br />\n[LIBRARY API VERSIONING](#library-api-versioning-1)<br />\n[ENVIRONMENT](#environment)<br />\n[DEBUGGING AND ERROR HANDLING](#debugging-and-error-handling)<br />\n[EXAMPLE](#example)<br />\n[ACKNOWLEDGEMENTS](#acknowledgements)<br />\n[SEE ALSO](#see-also)\n\n\n# NAME #\n\n**librpmem** - remote persistent memory support library (EXPERIMENTAL)\n\n\n# SYNOPSIS #\n\n```c\n#include <librpmem.h>\ncc ... -lrpmem\n```\n\n##### Library API versioning: #####\n\n```c\nconst char *rpmem_check_version(\n\tunsigned major_required,\n\tunsigned minor_required);\n```\n\n##### Error handling: #####\n\n```c\nconst char *rpmem_errormsg(void);\n```\n\n##### Other library functions: #####\n\nA description of other **librpmem** functions can be found on the following\nmanual pages:\n\n+ **rpmem_create**(3), **rpmem_persist**(3)\n\n\n# DESCRIPTION #\n\n**librpmem** provides low-level support for remote access to\n*persistent memory* (pmem) utilizing RDMA-capable RNICs. The library can be\nused to remotely replicate a memory region over the RDMA protocol. It utilizes\nan appropriate persistency mechanism based on the remote node's platform\ncapabilities. **librpmem** utilizes the **ssh**(1) client to authenticate\na user on the remote node, and for encryption of the connection's out-of-band\nconfiguration data. See **SSH**, below, for details.\n\nThe maximum replicated memory region size can not be bigger than the maximum\nlocked-in-memory address space limit. See **memlock** in **limits.conf**(5)\nfor more details.\n\nThis library is for applications that use remote persistent memory directly,\nwithout the help of any library-supplied transactions or memory\nallocation. Higher-level libraries that build on **libpmem**(7) are\navailable and are recommended for most applications, see:\n\n+ **libpmemobj**(7), a general use persistent memory API, providing memory\nallocation and transactional operations on variable-sized objects.\n\n\n# TARGET NODE ADDRESS FORMAT #\n\n```\n[<user>@]<hostname>[:<port>]\n```\n\nThe target node address is described by the *hostname* which the client\nconnects to, with an optional *user* name. The user must be authorized\nto authenticate to the remote machine without querying for password/passphrase.\nThe optional *port* number is used to establish the SSH connection. The default\nport number is 22.\n\n\n# REMOTE POOL ATTRIBUTES #\n\nThe *rpmem_pool_attr* structure describes a remote pool and is stored in remote\npool's metadata. This structure must be passed to the **rpmem_create**(3)\nfunction by caller when creating a pool on remote node. When opening the pool\nusing **rpmem_open**(3) function the appropriate fields are read from pool's\nmetadata and returned back to the caller.\n\n```c\n#define RPMEM_POOL_HDR_SIG_LEN    8\n#define RPMEM_POOL_HDR_UUID_LEN   16\n#define RPMEM_POOL_USER_FLAGS_LEN 16\n\nstruct rpmem_pool_attr {\n\tchar signature[RPMEM_POOL_HDR_SIG_LEN];\n\tuint32_t major;\n\tuint32_t compat_features;\n\tuint32_t incompat_features;\n\tuint32_t ro_compat_features;\n\tunsigned char poolset_uuid[RPMEM_POOL_HDR_UUID_LEN];\n\tunsigned char uuid[RPMEM_POOL_HDR_UUID_LEN];\n\tunsigned char next_uuid[RPMEM_POOL_HDR_UUID_LEN];\n\tunsigned char prev_uuid[RPMEM_POOL_HDR_UUID_LEN];\n\tunsigned char user_flags[RPMEM_POOL_USER_FLAGS_LEN];\n};\n```\n\nThe *signature* field is an 8-byte field which describes the pool's on-media\nformat.\n\nThe *major* field is a major version number of the pool's on-media format.\n\nThe *compat_features* field is a mask describing compatibility of pool's\non-media format optional features.\n\nThe *incompat_features* field is a mask describing compatibility of pool's\non-media format required features.\n\nThe *ro_compat_features* field is a mask describing compatibility of pool's\non-media format features. If these features are not available,\nthe pool shall be opened in read-only mode.\n\nThe *poolset_uuid* field is an UUID of the pool which the remote pool is\nassociated with.\n\nThe *uuid* field is an UUID of a first part of the remote pool. This field can\nbe used to connect the remote pool with other pools in a list.\n\nThe *next_uuid* and *prev_uuid* fields are UUIDs of next and previous replicas\nrespectively. These fields can be used to connect the remote pool with other\npools in a list.\n\nThe *user_flags* field is a 16-byte user-defined flags.\n\n\n# SSH #\n\n**librpmem** utilizes the **ssh**(1) client to login and execute the\n**rpmemd**(1) process on the remote node. By default, **ssh**(1)\nis executed with the **-4** option, which forces using **IPv4** addressing.\n\nFor debugging purposes, both the ssh client and the commands executed\non the remote node may be overridden by setting the **RPMEM_SSH** and\n**RPMEM_CMD** environment variables, respectively. See **ENVIRONMENT**\nfor details.\n\n\n# FORK #\nThe **ssh**(1) client is executed\nby **rpmem_open**(3) and **rpmem_create**(3) after forking a child process\nusing **fork**(2).  The application must take this into account when\nusing **wait**(2) and **waitpid**(2), which may return the *PID* of\nthe **ssh**(1) process executed by **librpmem**.\n\nIf **fork**(2) support is not enabled in **libibverbs**,\n**rpmem_open**(3) and **rpmem_create**(3) will fail.\nBy default, **fabric**(7) initializes **libibverbs** with **fork**(2) support\nby calling the **ibv_fork_init**(3) function. See **fi_verbs**(7) for more\ndetails.\n\n\n# CAVEATS #\n\n**librpmem** relies on the library destructor being called from the main thread.\nFor this reason, all functions that might trigger destruction (e.g.\n**dlclose**(3)) should be called in the main thread. Otherwise some of the\nresources associated with that thread might not be cleaned up properly.\n\n**librpmem** registers a pool as a single memory region. A Chelsio T4 and T5\nhardware can not handle a memory region greater than or equal to 8GB due to\na hardware bug. So *pool_size* value for **rpmem_create**(3) and **rpmem_open**(3)\nusing this hardware can not be greater than or equal to 8GB.\n\n# LIBRARY API VERSIONING #\n\nThis section describes how the library API is versioned,\nallowing applications to work with an evolving API.\n\nThe **rpmem_check_version**() function is used to see if the installed\n**librpmem** supports the version of the library API required by an\napplication. The easiest way to do this is for the application to supply\nthe compile-time version information, supplied by defines in\n**\\<librpmem.h\\>**, like this:\n\n```c\nreason = rpmem_check_version(RPMEM_MAJOR_VERSION,\n                             RPMEM_MINOR_VERSION);\nif (reason != NULL) {\n\t/* version check failed, reason string tells you why */\n}\n```\n\nAny mismatch in the major version number is considered a failure, but a\nlibrary with a newer minor version number will pass this check since\nincreasing minor versions imply backwards compatibility.\n\nAn application can also check specifically for the existence of an\ninterface by checking for the version where that interface was\nintroduced. These versions are documented in this man page as follows:\nunless otherwise specified, all interfaces described here are available\nin version 1.0 of the library. Interfaces added after version 1.0 will\ncontain the text *introduced in version x.y* in the section of this\nmanual describing the feature.\n\nWhen the version check performed by **rpmem_check_version**() is\nsuccessful, the return value is NULL. Otherwise the return value is a\nstatic string describing the reason for failing the version check. The\nstring returned by **rpmem_check_version**() must not be modified or\nfreed.\n\n\n# ENVIRONMENT #\n\n**librpmem** can change its default behavior based on the following\nenvironment variables. These are largely intended for testing and are\nnot normally required.\n\n\n+ **RPMEM_SSH**=*ssh_client*\n\nSetting this environment variable overrides the default **ssh**(1) client\ncommand name.\n\n+ **RPMEM_CMD**=*cmd*\n\nSetting this environment variable overrides the default command executed on\nthe remote node using either **ssh**(1) or the alternative remote shell command\nspecified by **RPMEM_SSH**.\n\n**RPMEM_CMD** can contain multiple commands separated by a vertical bar (`|`).\nEach consecutive command is executed on the remote node in order read from a\npool set file. This environment variable is read when the library is\ninitialized, so **RPMEM_CMD** must be set prior to application launch (or\nprior to **dlopen**(3) if **librpmem** is being dynamically loaded).\n\n+ **RPMEM_ENABLE_SOCKETS**=0\\|1\n\nSetting this variable to 1 enables using **fi_sockets**(7) provider for\nin-band RDMA connection. The *sockets* provider does not support IPv6.\nIt is required to disable IPv6 system wide if **RPMEM_ENABLE_SOCKETS** == 1 and\n*target* == localhost (or any other loopback interface address) and\n**SSH_CONNECTION** variable (see **ssh**(1) for more details) contains IPv6\naddress after ssh to loopback interface. By default the *sockets* provider is\ndisabled.\n\n* **RPMEM_ENABLE_VERBS**=0\\|1\n\nSetting this variable to 0 disables using **fi_verbs**(7) provider for\nin-band RDMA connection. The *verbs* provider is enabled by default.\n\n* **RPMEM_MAX_NLANES**=*num*\n\nLimit the maximum number of lanes to *num*. See **LANES**, in **rpmem_create**(3), for details.\n\n\n# DEBUGGING AND ERROR HANDLING #\n\nIf an error is detected during the call to a **librpmem** function, the\napplication may retrieve an error message describing the reason for the failure\nfrom **rpmem_errormsg**(). This function returns a pointer to a static buffer\ncontaining the last error message logged for the current thread. If *errno*\nwas set, the error message may include a description of the corresponding\nerror code as returned by **strerror**(3). The error message buffer is\nthread-local; errors encountered in one thread do not affect its value in\nother threads. The buffer is never cleared by any library function; its\ncontent is significant only when the return value of the immediately preceding\ncall to a **librpmem** function indicated an error, or if *errno* was set.\nThe application must not modify or free the error message string, but it may\nbe modified by subsequent calls to other library functions.\n\nTwo versions of **librpmem** are typically available on a development\nsystem. The normal version, accessed when a program is linked using the\n**-lrpmem** option, is optimized for performance. That version skips checks\nthat impact performance and never logs any trace information or performs any\nrun-time assertions.\n\nA second version of **librpmem**, accessed when a program uses the libraries\nunder _DEBUGLIBPATH(), contains run-time assertions and trace points. The\ntypical way to access the debug version is to set the environment variable\n**LD_LIBRARY_PATH** to _LDLIBPATH(). Debugging output is\ncontrolled using the following environment variables. These variables have\nno effect on the non-debug version of the library.\n\n+ **RPMEM_LOG_LEVEL**\n\nThe value of **RPMEM_LOG_LEVEL** enables trace points in the debug version\nof the library, as follows:\n\n+ **0** - This is the default level when **RPMEM_LOG_LEVEL** is not set.\nNo log messages are emitted at this level.\n\n+ **1** - Additional details on any errors detected are logged\n(in addition to returning the *errno*-based errors as usual).\nThe same information may be retrieved using **rpmem_errormsg**().\n\n+ **2** - A trace of basic operations is logged.\n\n+ **3** - Enables a very verbose amount of function call\ntracing in the library.\n\n+ **4** - Enables voluminous and fairly obscure tracing information\nthat is likely only useful to the **librpmem** developers.\n\nUnless **RPMEM_LOG_FILE** is set, debugging output is written to *stderr*.\n\n+ **RPMEM_LOG_FILE**\n\nSpecifies the name of a file where all logging information should be written.\nIf the last character in the name is \"-\", the *PID* of the current process will\nbe appended to the file name when the log file is created. If\n**RPMEM_LOG_FILE** is not set, logging output is written to *stderr*.\n\n\n# EXAMPLE #\n\nThe following example uses **librpmem** to create a remote pool on given\ntarget node identified by given pool set name. The associated local memory\npool is zeroed and the data is made persistent on remote node. Upon success\nthe remote pool is closed.\n\n```c\n#include <stdio.h>\n#include <string.h>\n\n#include <librpmem.h>\n\n#define POOL_SIZE    (32 * 1024 * 1024)\n#define NLANES        4\nunsigned char pool[POOL_SIZE];\n\nint\nmain(int argc, char *argv[])\n{\n\tint ret;\n\tunsigned nlanes = NLANES;\n\n\t/* fill pool_attributes */\n\tstruct rpmem_pool_attr pool_attr;\n\tmemset(&pool_attr, 0, sizeof(pool_attr));\n\n\t/* create a remote pool */\n\tRPMEMpool *rpp = rpmem_create(\"localhost\", \"pool.set\",\n\t\t\tpool, POOL_SIZE, &nlanes, &pool_attr);\n\tif (!rpp) {\n\t\tfprintf(stderr, \"rpmem_create: %s\\n\", rpmem_errormsg());\n\t\treturn 1;\n\t}\n\n\t/* store data on local pool */\n\tmemset(pool, 0, POOL_SIZE);\n\n\t/* make local data persistent on remote node */\n\tret = rpmem_persist(rpp, 0, POOL_SIZE, 0, 0);\n\tif (ret) {\n\t\tfprintf(stderr, \"rpmem_persist: %s\\n\", rpmem_errormsg());\n\t\treturn 1;\n\t}\n\n\t/* close the remote pool */\n\tret = rpmem_close(rpp);\n\tif (ret) {\n\t\tfprintf(stderr, \"rpmem_close: %s\\n\", rpmem_errormsg());\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n```\n\n# NOTE #\n\nThe **librpmem** API is experimental and may be subject to change in the future.\nHowever, using the remote replication in **libpmemobj**(7) is safe and backward\ncompatibility will be preserved.\n\n# ACKNOWLEDGEMENTS #\n\n**librpmem** builds on the persistent memory programming model\nrecommended by the SNIA NVM Programming Technical Work Group:\n<http://snia.org/nvmp>\n\n\n# SEE ALSO #\n\n**rpmemd**(1), **ssh**(1), **fork**(2), **dlclose**(3), **dlopen**(3),\n**ibv_fork_init**(3), **rpmem_create**(3), **rpmem_open**(3),\n**rpmem_persist**(3), **strerror**(3), **limits.conf**(5), **fabric**(7),\n**fi_sockets**(7), **fi_verbs**(7), **libpmem**(7), **libpmemblk**(7),\n**libpmemlog**(7), **libpmemobj**(7)\nand **<http://pmem.io>**\n",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/doc/generated/librpmem.7": ".\\\" Automatically generated by Pandoc 2.1.3\n.\\\"\n.TH \"LIBRPMEM\" \"7\" \"2018-10-17\" \"PMDK - rpmem API version 1.2\" \"PMDK Programmer's Manual\"\n.hy\n.\\\" Copyright 2014-2018, Intel Corporation\n.\\\"\n.\\\" Redistribution and use in source and binary forms, with or without\n.\\\" modification, are permitted provided that the following conditions\n.\\\" are met:\n.\\\"\n.\\\"     * Redistributions of source code must retain the above copyright\n.\\\"       notice, this list of conditions and the following disclaimer.\n.\\\"\n.\\\"     * Redistributions in binary form must reproduce the above copyright\n.\\\"       notice, this list of conditions and the following disclaimer in\n.\\\"       the documentation and/or other materials provided with the\n.\\\"       distribution.\n.\\\"\n.\\\"     * Neither the name of the copyright holder nor the names of its\n.\\\"       contributors may be used to endorse or promote products derived\n.\\\"       from this software without specific prior written permission.\n.\\\"\n.\\\" THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n.\\\" \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n.\\\" LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n.\\\" A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n.\\\" OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n.\\\" SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n.\\\" LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n.\\\" DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n.\\\" THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n.\\\" (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n.\\\" OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n.SH NAME\n.PP\n\\f[B]librpmem\\f[] \\- remote persistent memory support library\n(EXPERIMENTAL)\n.SH SYNOPSIS\n.IP\n.nf\n\\f[C]\n#include\\ <librpmem.h>\ncc\\ ...\\ \\-lrpmem\n\\f[]\n.fi\n.SS Library API versioning:\n.IP\n.nf\n\\f[C]\nconst\\ char\\ *rpmem_check_version(\n\\ \\ \\ \\ unsigned\\ major_required,\n\\ \\ \\ \\ unsigned\\ minor_required);\n\\f[]\n.fi\n.SS Error handling:\n.IP\n.nf\n\\f[C]\nconst\\ char\\ *rpmem_errormsg(void);\n\\f[]\n.fi\n.SS Other library functions:\n.PP\nA description of other \\f[B]librpmem\\f[] functions can be found on the\nfollowing manual pages:\n.IP \\[bu] 2\n\\f[B]rpmem_create\\f[](3), \\f[B]rpmem_persist\\f[](3)\n.SH DESCRIPTION\n.PP\n\\f[B]librpmem\\f[] provides low\\-level support for remote access to\n\\f[I]persistent memory\\f[] (pmem) utilizing RDMA\\-capable RNICs.\nThe library can be used to remotely replicate a memory region over the\nRDMA protocol.\nIt utilizes an appropriate persistency mechanism based on the remote\nnode's platform capabilities.\n\\f[B]librpmem\\f[] utilizes the \\f[B]ssh\\f[](1) client to authenticate a\nuser on the remote node, and for encryption of the connection's\nout\\-of\\-band configuration data.\nSee \\f[B]SSH\\f[], below, for details.\n.PP\nThe maximum replicated memory region size can not be bigger than the\nmaximum locked\\-in\\-memory address space limit.\nSee \\f[B]memlock\\f[] in \\f[B]limits.conf\\f[](5) for more details.\n.PP\nThis library is for applications that use remote persistent memory\ndirectly, without the help of any library\\-supplied transactions or\nmemory allocation.\nHigher\\-level libraries that build on \\f[B]libpmem\\f[](7) are available\nand are recommended for most applications, see:\n.IP \\[bu] 2\n\\f[B]libpmemobj\\f[](7), a general use persistent memory API, providing\nmemory allocation and transactional operations on variable\\-sized\nobjects.\n.SH TARGET NODE ADDRESS FORMAT\n.IP\n.nf\n\\f[C]\n[<user>\\@]<hostname>[:<port>]\n\\f[]\n.fi\n.PP\nThe target node address is described by the \\f[I]hostname\\f[] which the\nclient connects to, with an optional \\f[I]user\\f[] name.\nThe user must be authorized to authenticate to the remote machine\nwithout querying for password/passphrase.\nThe optional \\f[I]port\\f[] number is used to establish the SSH\nconnection.\nThe default port number is 22.\n.SH REMOTE POOL ATTRIBUTES\n.PP\nThe \\f[I]rpmem_pool_attr\\f[] structure describes a remote pool and is\nstored in remote pool's metadata.\nThis structure must be passed to the \\f[B]rpmem_create\\f[](3) function\nby caller when creating a pool on remote node.\nWhen opening the pool using \\f[B]rpmem_open\\f[](3) function the\nappropriate fields are read from pool's metadata and returned back to\nthe caller.\n.IP\n.nf\n\\f[C]\n#define\\ RPMEM_POOL_HDR_SIG_LEN\\ \\ \\ \\ 8\n#define\\ RPMEM_POOL_HDR_UUID_LEN\\ \\ \\ 16\n#define\\ RPMEM_POOL_USER_FLAGS_LEN\\ 16\n\nstruct\\ rpmem_pool_attr\\ {\n\\ \\ \\ \\ char\\ signature[RPMEM_POOL_HDR_SIG_LEN];\n\\ \\ \\ \\ uint32_t\\ major;\n\\ \\ \\ \\ uint32_t\\ compat_features;\n\\ \\ \\ \\ uint32_t\\ incompat_features;\n\\ \\ \\ \\ uint32_t\\ ro_compat_features;\n\\ \\ \\ \\ unsigned\\ char\\ poolset_uuid[RPMEM_POOL_HDR_UUID_LEN];\n\\ \\ \\ \\ unsigned\\ char\\ uuid[RPMEM_POOL_HDR_UUID_LEN];\n\\ \\ \\ \\ unsigned\\ char\\ next_uuid[RPMEM_POOL_HDR_UUID_LEN];\n\\ \\ \\ \\ unsigned\\ char\\ prev_uuid[RPMEM_POOL_HDR_UUID_LEN];\n\\ \\ \\ \\ unsigned\\ char\\ user_flags[RPMEM_POOL_USER_FLAGS_LEN];\n};\n\\f[]\n.fi\n.PP\nThe \\f[I]signature\\f[] field is an 8\\-byte field which describes the\npool's on\\-media format.\n.PP\nThe \\f[I]major\\f[] field is a major version number of the pool's\non\\-media format.\n.PP\nThe \\f[I]compat_features\\f[] field is a mask describing compatibility of\npool's on\\-media format optional features.\n.PP\nThe \\f[I]incompat_features\\f[] field is a mask describing compatibility\nof pool's on\\-media format required features.\n.PP\nThe \\f[I]ro_compat_features\\f[] field is a mask describing compatibility\nof pool's on\\-media format features.\nIf these features are not available, the pool shall be opened in\nread\\-only mode.\n.PP\nThe \\f[I]poolset_uuid\\f[] field is an UUID of the pool which the remote\npool is associated with.\n.PP\nThe \\f[I]uuid\\f[] field is an UUID of a first part of the remote pool.\nThis field can be used to connect the remote pool with other pools in a\nlist.\n.PP\nThe \\f[I]next_uuid\\f[] and \\f[I]prev_uuid\\f[] fields are UUIDs of next\nand previous replicas respectively.\nThese fields can be used to connect the remote pool with other pools in\na list.\n.PP\nThe \\f[I]user_flags\\f[] field is a 16\\-byte user\\-defined flags.\n.SH SSH\n.PP\n\\f[B]librpmem\\f[] utilizes the \\f[B]ssh\\f[](1) client to login and\nexecute the \\f[B]rpmemd\\f[](1) process on the remote node.\nBy default, \\f[B]ssh\\f[](1) is executed with the \\f[B]\\-4\\f[] option,\nwhich forces using \\f[B]IPv4\\f[] addressing.\n.PP\nFor debugging purposes, both the ssh client and the commands executed on\nthe remote node may be overridden by setting the \\f[B]RPMEM_SSH\\f[] and\n\\f[B]RPMEM_CMD\\f[] environment variables, respectively.\nSee \\f[B]ENVIRONMENT\\f[] for details.\n.SH FORK\n.PP\nThe \\f[B]ssh\\f[](1) client is executed by \\f[B]rpmem_open\\f[](3) and\n\\f[B]rpmem_create\\f[](3) after forking a child process using\n\\f[B]fork\\f[](2).\nThe application must take this into account when using \\f[B]wait\\f[](2)\nand \\f[B]waitpid\\f[](2), which may return the \\f[I]PID\\f[] of the\n\\f[B]ssh\\f[](1) process executed by \\f[B]librpmem\\f[].\n.PP\nIf \\f[B]fork\\f[](2) support is not enabled in \\f[B]libibverbs\\f[],\n\\f[B]rpmem_open\\f[](3) and \\f[B]rpmem_create\\f[](3) will fail.\nBy default, \\f[B]fabric\\f[](7) initializes \\f[B]libibverbs\\f[] with\n\\f[B]fork\\f[](2) support by calling the \\f[B]ibv_fork_init\\f[](3)\nfunction.\nSee \\f[B]fi_verbs\\f[](7) for more details.\n.SH CAVEATS\n.PP\n\\f[B]librpmem\\f[] relies on the library destructor being called from the\nmain thread.\nFor this reason, all functions that might trigger destruction (e.g.\n\\f[B]dlclose\\f[](3)) should be called in the main thread.\nOtherwise some of the resources associated with that thread might not be\ncleaned up properly.\n.PP\n\\f[B]librpmem\\f[] registers a pool as a single memory region.\nA Chelsio T4 and T5 hardware can not handle a memory region greater than\nor equal to 8GB due to a hardware bug.\nSo \\f[I]pool_size\\f[] value for \\f[B]rpmem_create\\f[](3) and\n\\f[B]rpmem_open\\f[](3) using this hardware can not be greater than or\nequal to 8GB.\n.SH LIBRARY API VERSIONING\n.PP\nThis section describes how the library API is versioned, allowing\napplications to work with an evolving API.\n.PP\nThe \\f[B]rpmem_check_version\\f[]() function is used to see if the\ninstalled \\f[B]librpmem\\f[] supports the version of the library API\nrequired by an application.\nThe easiest way to do this is for the application to supply the\ncompile\\-time version information, supplied by defines in\n\\f[B]<librpmem.h>\\f[], like this:\n.IP\n.nf\n\\f[C]\nreason\\ =\\ rpmem_check_version(RPMEM_MAJOR_VERSION,\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ RPMEM_MINOR_VERSION);\nif\\ (reason\\ !=\\ NULL)\\ {\n\\ \\ \\ \\ /*\\ version\\ check\\ failed,\\ reason\\ string\\ tells\\ you\\ why\\ */\n}\n\\f[]\n.fi\n.PP\nAny mismatch in the major version number is considered a failure, but a\nlibrary with a newer minor version number will pass this check since\nincreasing minor versions imply backwards compatibility.\n.PP\nAn application can also check specifically for the existence of an\ninterface by checking for the version where that interface was\nintroduced.\nThese versions are documented in this man page as follows: unless\notherwise specified, all interfaces described here are available in\nversion 1.0 of the library.\nInterfaces added after version 1.0 will contain the text \\f[I]introduced\nin version x.y\\f[] in the section of this manual describing the feature.\n.PP\nWhen the version check performed by \\f[B]rpmem_check_version\\f[]() is\nsuccessful, the return value is NULL.\nOtherwise the return value is a static string describing the reason for\nfailing the version check.\nThe string returned by \\f[B]rpmem_check_version\\f[]() must not be\nmodified or freed.\n.SH ENVIRONMENT\n.PP\n\\f[B]librpmem\\f[] can change its default behavior based on the following\nenvironment variables.\nThese are largely intended for testing and are not normally required.\n.IP \\[bu] 2\n\\f[B]RPMEM_SSH\\f[]=\\f[I]ssh_client\\f[]\n.PP\nSetting this environment variable overrides the default \\f[B]ssh\\f[](1)\nclient command name.\n.IP \\[bu] 2\n\\f[B]RPMEM_CMD\\f[]=\\f[I]cmd\\f[]\n.PP\nSetting this environment variable overrides the default command executed\non the remote node using either \\f[B]ssh\\f[](1) or the alternative\nremote shell command specified by \\f[B]RPMEM_SSH\\f[].\n.PP\n\\f[B]RPMEM_CMD\\f[] can contain multiple commands separated by a vertical\nbar (\\f[C]|\\f[]).\nEach consecutive command is executed on the remote node in order read\nfrom a pool set file.\nThis environment variable is read when the library is initialized, so\n\\f[B]RPMEM_CMD\\f[] must be set prior to application launch (or prior to\n\\f[B]dlopen\\f[](3) if \\f[B]librpmem\\f[] is being dynamically loaded).\n.IP \\[bu] 2\n\\f[B]RPMEM_ENABLE_SOCKETS\\f[]=0|1\n.PP\nSetting this variable to 1 enables using \\f[B]fi_sockets\\f[](7) provider\nfor in\\-band RDMA connection.\nThe \\f[I]sockets\\f[] provider does not support IPv6.\nIt is required to disable IPv6 system wide if\n\\f[B]RPMEM_ENABLE_SOCKETS\\f[] == 1 and \\f[I]target\\f[] == localhost (or\nany other loopback interface address) and \\f[B]SSH_CONNECTION\\f[]\nvariable (see \\f[B]ssh\\f[](1) for more details) contains IPv6 address\nafter ssh to loopback interface.\nBy default the \\f[I]sockets\\f[] provider is disabled.\n.IP \\[bu] 2\n\\f[B]RPMEM_ENABLE_VERBS\\f[]=0|1\n.PP\nSetting this variable to 0 disables using \\f[B]fi_verbs\\f[](7) provider\nfor in\\-band RDMA connection.\nThe \\f[I]verbs\\f[] provider is enabled by default.\n.IP \\[bu] 2\n\\f[B]RPMEM_MAX_NLANES\\f[]=\\f[I]num\\f[]\n.PP\nLimit the maximum number of lanes to \\f[I]num\\f[].\nSee \\f[B]LANES\\f[], in \\f[B]rpmem_create\\f[](3), for details.\n.SH DEBUGGING AND ERROR HANDLING\n.PP\nIf an error is detected during the call to a \\f[B]librpmem\\f[] function,\nthe application may retrieve an error message describing the reason for\nthe failure from \\f[B]rpmem_errormsg\\f[]().\nThis function returns a pointer to a static buffer containing the last\nerror message logged for the current thread.\nIf \\f[I]errno\\f[] was set, the error message may include a description\nof the corresponding error code as returned by \\f[B]strerror\\f[](3).\nThe error message buffer is thread\\-local; errors encountered in one\nthread do not affect its value in other threads.\nThe buffer is never cleared by any library function; its content is\nsignificant only when the return value of the immediately preceding call\nto a \\f[B]librpmem\\f[] function indicated an error, or if \\f[I]errno\\f[]\nwas set.\nThe application must not modify or free the error message string, but it\nmay be modified by subsequent calls to other library functions.\n.PP\nTwo versions of \\f[B]librpmem\\f[] are typically available on a\ndevelopment system.\nThe normal version, accessed when a program is linked using the\n\\f[B]\\-lrpmem\\f[] option, is optimized for performance.\nThat version skips checks that impact performance and never logs any\ntrace information or performs any run\\-time assertions.\n.PP\nA second version of \\f[B]librpmem\\f[], accessed when a program uses the\nlibraries under \\f[B]/usr/lib/pmdk_debug\\f[], contains run\\-time\nassertions and trace points.\nThe typical way to access the debug version is to set the environment\nvariable \\f[B]LD_LIBRARY_PATH\\f[] to \\f[B]/usr/lib/pmdk_debug\\f[] or\n\\f[B]/usr/lib64/pmdk_debug\\f[], as appropriate.\nDebugging output is controlled using the following environment\nvariables.\nThese variables have no effect on the non\\-debug version of the library.\n.IP \\[bu] 2\n\\f[B]RPMEM_LOG_LEVEL\\f[]\n.PP\nThe value of \\f[B]RPMEM_LOG_LEVEL\\f[] enables trace points in the debug\nversion of the library, as follows:\n.IP \\[bu] 2\n\\f[B]0\\f[] \\- This is the default level when \\f[B]RPMEM_LOG_LEVEL\\f[] is\nnot set.\nNo log messages are emitted at this level.\n.IP \\[bu] 2\n\\f[B]1\\f[] \\- Additional details on any errors detected are logged (in\naddition to returning the \\f[I]errno\\f[]\\-based errors as usual).\nThe same information may be retrieved using \\f[B]rpmem_errormsg\\f[]().\n.IP \\[bu] 2\n\\f[B]2\\f[] \\- A trace of basic operations is logged.\n.IP \\[bu] 2\n\\f[B]3\\f[] \\- Enables a very verbose amount of function call tracing in\nthe library.\n.IP \\[bu] 2\n\\f[B]4\\f[] \\- Enables voluminous and fairly obscure tracing information\nthat is likely only useful to the \\f[B]librpmem\\f[] developers.\n.PP\nUnless \\f[B]RPMEM_LOG_FILE\\f[] is set, debugging output is written to\n\\f[I]stderr\\f[].\n.IP \\[bu] 2\n\\f[B]RPMEM_LOG_FILE\\f[]\n.PP\nSpecifies the name of a file where all logging information should be\nwritten.\nIf the last character in the name is \\[lq]\\-\\[rq], the \\f[I]PID\\f[] of\nthe current process will be appended to the file name when the log file\nis created.\nIf \\f[B]RPMEM_LOG_FILE\\f[] is not set, logging output is written to\n\\f[I]stderr\\f[].\n.SH EXAMPLE\n.PP\nThe following example uses \\f[B]librpmem\\f[] to create a remote pool on\ngiven target node identified by given pool set name.\nThe associated local memory pool is zeroed and the data is made\npersistent on remote node.\nUpon success the remote pool is closed.\n.IP\n.nf\n\\f[C]\n#include\\ <stdio.h>\n#include\\ <string.h>\n\n#include\\ <librpmem.h>\n\n#define\\ POOL_SIZE\\ \\ \\ \\ (32\\ *\\ 1024\\ *\\ 1024)\n#define\\ NLANES\\ \\ \\ \\ \\ \\ \\ \\ 4\nunsigned\\ char\\ pool[POOL_SIZE];\n\nint\nmain(int\\ argc,\\ char\\ *argv[])\n{\n\\ \\ \\ \\ int\\ ret;\n\\ \\ \\ \\ unsigned\\ nlanes\\ =\\ NLANES;\n\n\\ \\ \\ \\ /*\\ fill\\ pool_attributes\\ */\n\\ \\ \\ \\ struct\\ rpmem_pool_attr\\ pool_attr;\n\\ \\ \\ \\ memset(&pool_attr,\\ 0,\\ sizeof(pool_attr));\n\n\\ \\ \\ \\ /*\\ create\\ a\\ remote\\ pool\\ */\n\\ \\ \\ \\ RPMEMpool\\ *rpp\\ =\\ rpmem_create(\"localhost\",\\ \"pool.set\",\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ pool,\\ POOL_SIZE,\\ &nlanes,\\ &pool_attr);\n\\ \\ \\ \\ if\\ (!rpp)\\ {\n\\ \\ \\ \\ \\ \\ \\ \\ fprintf(stderr,\\ \"rpmem_create:\\ %s\\\\n\",\\ rpmem_errormsg());\n\\ \\ \\ \\ \\ \\ \\ \\ return\\ 1;\n\\ \\ \\ \\ }\n\n\\ \\ \\ \\ /*\\ store\\ data\\ on\\ local\\ pool\\ */\n\\ \\ \\ \\ memset(pool,\\ 0,\\ POOL_SIZE);\n\n\\ \\ \\ \\ /*\\ make\\ local\\ data\\ persistent\\ on\\ remote\\ node\\ */\n\\ \\ \\ \\ ret\\ =\\ rpmem_persist(rpp,\\ 0,\\ POOL_SIZE,\\ 0,\\ 0);\n\\ \\ \\ \\ if\\ (ret)\\ {\n\\ \\ \\ \\ \\ \\ \\ \\ fprintf(stderr,\\ \"rpmem_persist:\\ %s\\\\n\",\\ rpmem_errormsg());\n\\ \\ \\ \\ \\ \\ \\ \\ return\\ 1;\n\\ \\ \\ \\ }\n\n\\ \\ \\ \\ /*\\ close\\ the\\ remote\\ pool\\ */\n\\ \\ \\ \\ ret\\ =\\ rpmem_close(rpp);\n\\ \\ \\ \\ if\\ (ret)\\ {\n\\ \\ \\ \\ \\ \\ \\ \\ fprintf(stderr,\\ \"rpmem_close:\\ %s\\\\n\",\\ rpmem_errormsg());\n\\ \\ \\ \\ \\ \\ \\ \\ return\\ 1;\n\\ \\ \\ \\ }\n\n\\ \\ \\ \\ return\\ 0;\n}\n\\f[]\n.fi\n.SH NOTE\n.PP\nThe \\f[B]librpmem\\f[] API is experimental and may be subject to change\nin the future.\nHowever, using the remote replication in \\f[B]libpmemobj\\f[](7) is safe\nand backward compatibility will be preserved.\n.SH ACKNOWLEDGEMENTS\n.PP\n\\f[B]librpmem\\f[] builds on the persistent memory programming model\nrecommended by the SNIA NVM Programming Technical Work Group:\n<http://snia.org/nvmp>\n.SH SEE ALSO\n.PP\n\\f[B]rpmemd\\f[](1), \\f[B]ssh\\f[](1), \\f[B]fork\\f[](2),\n\\f[B]dlclose\\f[](3), \\f[B]dlopen\\f[](3), \\f[B]ibv_fork_init\\f[](3),\n\\f[B]rpmem_create\\f[](3), \\f[B]rpmem_open\\f[](3),\n\\f[B]rpmem_persist\\f[](3), \\f[B]strerror\\f[](3),\n\\f[B]limits.conf\\f[](5), \\f[B]fabric\\f[](7), \\f[B]fi_sockets\\f[](7),\n\\f[B]fi_verbs\\f[](7), \\f[B]libpmem\\f[](7), \\f[B]libpmemblk\\f[](7),\n\\f[B]libpmemlog\\f[](7), \\f[B]libpmemobj\\f[](7) and\n\\f[B]<http://pmem.io>\\f[]\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/util_poolset/grep5w.log.match",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/util_poolset/grep4w.log.match",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/test/util_poolset/grep6w.log.match",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/src/tools/pmempool/pmempool.rc",
        "/tmp/vanessa/spack-stage/spack-stage-pmdk-1.5-dat5o2zfoi3bkzqpz7pcvgdnc4uqzlym/spack-src/res/PMDK.ico"
    ],
    "total_files": 3974
}