{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/meson_options.txt": "# Please keep these options sorted alphabetically.\n\noption('allow_invalid_socket_id', type: 'boolean', value: false,\n\tdescription: 'allow out-of-range NUMA socket id\\'s for platforms that don\\'t report the value correctly')\noption('drivers_install_subdir', type: 'string', value: 'dpdk/pmds-<VERSION>',\n\tdescription: 'Subdirectory of libdir where to install PMDs. Defaults to using a versioned subdirectory.')\noption('enable_docs', type: 'boolean', value: false,\n\tdescription: 'build documentation')\noption('enable_kmods', type: 'boolean', value: true,\n\tdescription: 'build kernel modules')\noption('examples', type: 'string', value: '',\n\tdescription: 'Comma-separated list of examples to build by default')\noption('ibverbs_link', type: 'combo', choices : ['shared', 'dlopen'], value: 'shared',\n\tdescription: 'Linkage method (shared/dlopen) for Mellanox PMDs with ibverbs dependencies.')\noption('include_subdir_arch', type: 'string', value: '',\n\tdescription: 'subdirectory where to install arch-dependent headers')\noption('kernel_dir', type: 'string', value: '',\n\tdescription: 'path to the kernel for building kernel modules, they will be installed in $DEST_DIR/$kernel_dir/../extra/dpdk')\noption('lib_musdk_dir', type: 'string', value: '',\n\tdescription: 'path to the MUSDK library installation directory')\noption('machine', type: 'string', value: 'native',\n\tdescription: 'set the target machine type')\noption('max_ethports', type: 'integer', value: 32,\n\tdescription: 'maximum number of Ethernet devices')\noption('max_lcores', type: 'integer', value: 128,\n\tdescription: 'maximum number of cores/threads supported by EAL')\noption('max_numa_nodes', type: 'integer', value: 4,\n\tdescription: 'maximum number of NUMA nodes supported by EAL')\noption('per_library_versions', type: 'boolean', value: true,\n\tdescription: 'true: each lib gets its own version number, false: DPDK version used for each lib')\noption('tests', type: 'boolean', value: true,\n\tdescription: 'build unit tests')\noption('use_hpet', type: 'boolean', value: false,\n\tdescription: 'use HPET timer in EAL')\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/lib/librte_eal/common/eal_common_options.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright(c) 2010-2014 Intel Corporation.\n * Copyright(c) 2014 6WIND S.A.\n */\n\n#include <stdlib.h>\n#include <unistd.h>\n#include <string.h>\n#include <syslog.h>\n#include <ctype.h>\n#include <limits.h>\n#include <errno.h>\n#include <getopt.h>\n#include <dlfcn.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <dirent.h>\n\n#include <rte_string_fns.h>\n#include <rte_eal.h>\n#include <rte_log.h>\n#include <rte_lcore.h>\n#include <rte_tailq.h>\n#include <rte_version.h>\n#include <rte_devargs.h>\n#include <rte_memcpy.h>\n\n#include \"eal_internal_cfg.h\"\n#include \"eal_options.h\"\n#include \"eal_filesystem.h\"\n#include \"eal_private.h\"\n\n#define BITS_PER_HEX 4\n#define LCORE_OPT_LST 1\n#define LCORE_OPT_MSK 2\n#define LCORE_OPT_MAP 3\n\nconst char\neal_short_options[] =\n\t\"b:\" /* pci-blacklist */\n\t\"c:\" /* coremask */\n\t\"s:\" /* service coremask */\n\t\"d:\" /* driver */\n\t\"h\"  /* help */\n\t\"l:\" /* corelist */\n\t\"S:\" /* service corelist */\n\t\"m:\" /* memory size */\n\t\"n:\" /* memory channels */\n\t\"r:\" /* memory ranks */\n\t\"v\"  /* version */\n\t\"w:\" /* pci-whitelist */\n\t;\n\nconst struct option\neal_long_options[] = {\n\t{OPT_BASE_VIRTADDR,     1, NULL, OPT_BASE_VIRTADDR_NUM    },\n\t{OPT_CREATE_UIO_DEV,    0, NULL, OPT_CREATE_UIO_DEV_NUM   },\n\t{OPT_FILE_PREFIX,       1, NULL, OPT_FILE_PREFIX_NUM      },\n\t{OPT_HELP,              0, NULL, OPT_HELP_NUM             },\n\t{OPT_HUGE_DIR,          1, NULL, OPT_HUGE_DIR_NUM         },\n\t{OPT_HUGE_UNLINK,       0, NULL, OPT_HUGE_UNLINK_NUM      },\n\t{OPT_IOVA_MODE,\t        1, NULL, OPT_IOVA_MODE_NUM        },\n\t{OPT_LCORES,            1, NULL, OPT_LCORES_NUM           },\n\t{OPT_LOG_LEVEL,         1, NULL, OPT_LOG_LEVEL_NUM        },\n\t{OPT_MASTER_LCORE,      1, NULL, OPT_MASTER_LCORE_NUM     },\n\t{OPT_MBUF_POOL_OPS_NAME, 1, NULL, OPT_MBUF_POOL_OPS_NAME_NUM},\n\t{OPT_NO_HPET,           0, NULL, OPT_NO_HPET_NUM          },\n\t{OPT_NO_HUGE,           0, NULL, OPT_NO_HUGE_NUM          },\n\t{OPT_NO_PCI,            0, NULL, OPT_NO_PCI_NUM           },\n\t{OPT_NO_SHCONF,         0, NULL, OPT_NO_SHCONF_NUM        },\n\t{OPT_IN_MEMORY,         0, NULL, OPT_IN_MEMORY_NUM        },\n\t{OPT_PCI_BLACKLIST,     1, NULL, OPT_PCI_BLACKLIST_NUM    },\n\t{OPT_PCI_WHITELIST,     1, NULL, OPT_PCI_WHITELIST_NUM    },\n\t{OPT_PROC_TYPE,         1, NULL, OPT_PROC_TYPE_NUM        },\n\t{OPT_SOCKET_MEM,        1, NULL, OPT_SOCKET_MEM_NUM       },\n\t{OPT_SOCKET_LIMIT,      1, NULL, OPT_SOCKET_LIMIT_NUM     },\n\t{OPT_SYSLOG,            1, NULL, OPT_SYSLOG_NUM           },\n\t{OPT_VDEV,              1, NULL, OPT_VDEV_NUM             },\n\t{OPT_VFIO_INTR,         1, NULL, OPT_VFIO_INTR_NUM        },\n\t{OPT_VMWARE_TSC_MAP,    0, NULL, OPT_VMWARE_TSC_MAP_NUM   },\n\t{OPT_LEGACY_MEM,        0, NULL, OPT_LEGACY_MEM_NUM       },\n\t{OPT_SINGLE_FILE_SEGMENTS, 0, NULL, OPT_SINGLE_FILE_SEGMENTS_NUM},\n\t{OPT_MATCH_ALLOCATIONS, 0, NULL, OPT_MATCH_ALLOCATIONS_NUM},\n\t{0,                     0, NULL, 0                        }\n};\n\nTAILQ_HEAD(shared_driver_list, shared_driver);\n\n/* Definition for shared object drivers. */\nstruct shared_driver {\n\tTAILQ_ENTRY(shared_driver) next;\n\n\tchar    name[PATH_MAX];\n\tvoid*   lib_handle;\n};\n\n/* List of external loadable drivers */\nstatic struct shared_driver_list solib_list =\nTAILQ_HEAD_INITIALIZER(solib_list);\n\n/* Default path of external loadable drivers */\nstatic const char *default_solib_dir = RTE_EAL_PMD_PATH;\n\n/*\n * Stringified version of solib path used by dpdk-pmdinfo.py\n * Note: PLEASE DO NOT ALTER THIS without making a corresponding\n * change to usertools/dpdk-pmdinfo.py\n */\nstatic const char dpdk_solib_path[] __attribute__((used)) =\n\"DPDK_PLUGIN_PATH=\" RTE_EAL_PMD_PATH;\n\nTAILQ_HEAD(device_option_list, device_option);\n\nstruct device_option {\n\tTAILQ_ENTRY(device_option) next;\n\n\tenum rte_devtype type;\n\tchar arg[];\n};\n\nstatic struct device_option_list devopt_list =\nTAILQ_HEAD_INITIALIZER(devopt_list);\n\nstatic int master_lcore_parsed;\nstatic int mem_parsed;\nstatic int core_parsed;\n\nstatic int\neal_option_device_add(enum rte_devtype type, const char *optarg)\n{\n\tstruct device_option *devopt;\n\tsize_t optlen;\n\tint ret;\n\n\toptlen = strlen(optarg) + 1;\n\tdevopt = calloc(1, sizeof(*devopt) + optlen);\n\tif (devopt == NULL) {\n\t\tRTE_LOG(ERR, EAL, \"Unable to allocate device option\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tdevopt->type = type;\n\tret = strlcpy(devopt->arg, optarg, optlen);\n\tif (ret < 0) {\n\t\tRTE_LOG(ERR, EAL, \"Unable to copy device option\\n\");\n\t\tfree(devopt);\n\t\treturn -EINVAL;\n\t}\n\tTAILQ_INSERT_TAIL(&devopt_list, devopt, next);\n\treturn 0;\n}\n\nint\neal_option_device_parse(void)\n{\n\tstruct device_option *devopt;\n\tvoid *tmp;\n\tint ret = 0;\n\n\tTAILQ_FOREACH_SAFE(devopt, &devopt_list, next, tmp) {\n\t\tif (ret == 0) {\n\t\t\tret = rte_devargs_add(devopt->type, devopt->arg);\n\t\t\tif (ret)\n\t\t\t\tRTE_LOG(ERR, EAL, \"Unable to parse device '%s'\\n\",\n\t\t\t\t\tdevopt->arg);\n\t\t}\n\t\tTAILQ_REMOVE(&devopt_list, devopt, next);\n\t\tfree(devopt);\n\t}\n\treturn ret;\n}\n\nconst char *\neal_get_hugefile_prefix(void)\n{\n\tif (internal_config.hugefile_prefix != NULL)\n\t\treturn internal_config.hugefile_prefix;\n\treturn HUGEFILE_PREFIX_DEFAULT;\n}\n\nvoid\neal_reset_internal_config(struct internal_config *internal_cfg)\n{\n\tint i;\n\n\tinternal_cfg->memory = 0;\n\tinternal_cfg->force_nrank = 0;\n\tinternal_cfg->force_nchannel = 0;\n\tinternal_cfg->hugefile_prefix = NULL;\n\tinternal_cfg->hugepage_dir = NULL;\n\tinternal_cfg->force_sockets = 0;\n\t/* zero out the NUMA config */\n\tfor (i = 0; i < RTE_MAX_NUMA_NODES; i++)\n\t\tinternal_cfg->socket_mem[i] = 0;\n\tinternal_cfg->force_socket_limits = 0;\n\t/* zero out the NUMA limits config */\n\tfor (i = 0; i < RTE_MAX_NUMA_NODES; i++)\n\t\tinternal_cfg->socket_limit[i] = 0;\n\t/* zero out hugedir descriptors */\n\tfor (i = 0; i < MAX_HUGEPAGE_SIZES; i++) {\n\t\tmemset(&internal_cfg->hugepage_info[i], 0,\n\t\t\t\tsizeof(internal_cfg->hugepage_info[0]));\n\t\tinternal_cfg->hugepage_info[i].lock_descriptor = -1;\n\t}\n\tinternal_cfg->base_virtaddr = 0;\n\n\tinternal_cfg->syslog_facility = LOG_DAEMON;\n\n\t/* if set to NONE, interrupt mode is determined automatically */\n\tinternal_cfg->vfio_intr_mode = RTE_INTR_MODE_NONE;\n\n#ifdef RTE_LIBEAL_USE_HPET\n\tinternal_cfg->no_hpet = 0;\n#else\n\tinternal_cfg->no_hpet = 1;\n#endif\n\tinternal_cfg->vmware_tsc_map = 0;\n\tinternal_cfg->create_uio_dev = 0;\n\tinternal_cfg->iova_mode = RTE_IOVA_DC;\n\tinternal_cfg->user_mbuf_pool_ops_name = NULL;\n\tCPU_ZERO(&internal_cfg->ctrl_cpuset);\n\tinternal_cfg->init_complete = 0;\n}\n\nstatic int\neal_plugin_add(const char *path)\n{\n\tstruct shared_driver *solib;\n\n\tsolib = malloc(sizeof(*solib));\n\tif (solib == NULL) {\n\t\tRTE_LOG(ERR, EAL, \"malloc(solib) failed\\n\");\n\t\treturn -1;\n\t}\n\tmemset(solib, 0, sizeof(*solib));\n\tstrlcpy(solib->name, path, PATH_MAX-1);\n\tsolib->name[PATH_MAX-1] = 0;\n\tTAILQ_INSERT_TAIL(&solib_list, solib, next);\n\n\treturn 0;\n}\n\nstatic int\neal_plugindir_init(const char *path)\n{\n\tDIR *d = NULL;\n\tstruct dirent *dent = NULL;\n\tchar sopath[PATH_MAX];\n\n\tif (path == NULL || *path == '\\0')\n\t\treturn 0;\n\n\td = opendir(path);\n\tif (d == NULL) {\n\t\tRTE_LOG(ERR, EAL, \"failed to open directory %s: %s\\n\",\n\t\t\tpath, strerror(errno));\n\t\treturn -1;\n\t}\n\n\twhile ((dent = readdir(d)) != NULL) {\n\t\tstruct stat sb;\n\n\t\tsnprintf(sopath, PATH_MAX-1, \"%s/%s\", path, dent->d_name);\n\t\tsopath[PATH_MAX-1] = 0;\n\n\t\tif (!(stat(sopath, &sb) == 0 && S_ISREG(sb.st_mode)))\n\t\t\tcontinue;\n\n\t\tif (eal_plugin_add(sopath) == -1)\n\t\t\tbreak;\n\t}\n\n\tclosedir(d);\n\t/* XXX this ignores failures from readdir() itself */\n\treturn (dent == NULL) ? 0 : -1;\n}\n\nint\neal_plugins_init(void)\n{\n\tstruct shared_driver *solib = NULL;\n\tstruct stat sb;\n\n\tif (*default_solib_dir != '\\0' && stat(default_solib_dir, &sb) == 0 &&\n\t\t\t\tS_ISDIR(sb.st_mode))\n\t\teal_plugin_add(default_solib_dir);\n\n\tTAILQ_FOREACH(solib, &solib_list, next) {\n\n\t\tif (stat(solib->name, &sb) == 0 && S_ISDIR(sb.st_mode)) {\n\t\t\tif (eal_plugindir_init(solib->name) == -1) {\n\t\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\t\"Cannot init plugin directory %s\\n\",\n\t\t\t\t\tsolib->name);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t} else {\n\t\t\tRTE_LOG(DEBUG, EAL, \"open shared lib %s\\n\",\n\t\t\t\tsolib->name);\n\t\t\tsolib->lib_handle = dlopen(solib->name, RTLD_NOW);\n\t\t\tif (solib->lib_handle == NULL) {\n\t\t\t\tRTE_LOG(ERR, EAL, \"%s\\n\", dlerror());\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\n\t}\n\treturn 0;\n}\n\n/*\n * Parse the coremask given as argument (hexadecimal string) and fill\n * the global configuration (core role and core count) with the parsed\n * value.\n */\nstatic int xdigit2val(unsigned char c)\n{\n\tint val;\n\n\tif (isdigit(c))\n\t\tval = c - '0';\n\telse if (isupper(c))\n\t\tval = c - 'A' + 10;\n\telse\n\t\tval = c - 'a' + 10;\n\treturn val;\n}\n\nstatic int\neal_parse_service_coremask(const char *coremask)\n{\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\tint i, j, idx = 0;\n\tunsigned int count = 0;\n\tchar c;\n\tint val;\n\tuint32_t taken_lcore_count = 0;\n\n\tif (coremask == NULL)\n\t\treturn -1;\n\t/* Remove all blank characters ahead and after .\n\t * Remove 0x/0X if exists.\n\t */\n\twhile (isblank(*coremask))\n\t\tcoremask++;\n\tif (coremask[0] == '0' && ((coremask[1] == 'x')\n\t\t|| (coremask[1] == 'X')))\n\t\tcoremask += 2;\n\ti = strlen(coremask);\n\twhile ((i > 0) && isblank(coremask[i - 1]))\n\t\ti--;\n\n\tif (i == 0)\n\t\treturn -1;\n\n\tfor (i = i - 1; i >= 0 && idx < RTE_MAX_LCORE; i--) {\n\t\tc = coremask[i];\n\t\tif (isxdigit(c) == 0) {\n\t\t\t/* invalid characters */\n\t\t\treturn -1;\n\t\t}\n\t\tval = xdigit2val(c);\n\t\tfor (j = 0; j < BITS_PER_HEX && idx < RTE_MAX_LCORE;\n\t\t\t\tj++, idx++) {\n\t\t\tif ((1 << j) & val) {\n\t\t\t\t/* handle master lcore already parsed */\n\t\t\t\tuint32_t lcore = idx;\n\t\t\t\tif (master_lcore_parsed &&\n\t\t\t\t\t\tcfg->master_lcore == lcore) {\n\t\t\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\t\t\"lcore %u is master lcore, cannot use as service core\\n\",\n\t\t\t\t\t\tidx);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\n\t\t\t\tif (!lcore_config[idx].detected) {\n\t\t\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\t\t\"lcore %u unavailable\\n\", idx);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\n\t\t\t\tif (cfg->lcore_role[idx] == ROLE_RTE)\n\t\t\t\t\ttaken_lcore_count++;\n\n\t\t\t\tlcore_config[idx].core_role = ROLE_SERVICE;\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (; i >= 0; i--)\n\t\tif (coremask[i] != '0')\n\t\t\treturn -1;\n\n\tfor (; idx < RTE_MAX_LCORE; idx++)\n\t\tlcore_config[idx].core_index = -1;\n\n\tif (count == 0)\n\t\treturn -1;\n\n\tif (core_parsed && taken_lcore_count != count) {\n\t\tRTE_LOG(WARNING, EAL,\n\t\t\t\"Not all service cores are in the coremask. \"\n\t\t\t\"Please ensure -c or -l includes service cores\\n\");\n\t}\n\n\tcfg->service_lcore_count = count;\n\treturn 0;\n}\n\nstatic int\neal_service_cores_parsed(void)\n{\n\tint idx;\n\tfor (idx = 0; idx < RTE_MAX_LCORE; idx++) {\n\t\tif (lcore_config[idx].core_role == ROLE_SERVICE)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int\nupdate_lcore_config(int *cores)\n{\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\tunsigned int count = 0;\n\tunsigned int i;\n\tint ret = 0;\n\n\tfor (i = 0; i < RTE_MAX_LCORE; i++) {\n\t\tif (cores[i] != -1) {\n\t\t\tif (!lcore_config[i].detected) {\n\t\t\t\tRTE_LOG(ERR, EAL, \"lcore %u unavailable\\n\", i);\n\t\t\t\tret = -1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tcfg->lcore_role[i] = ROLE_RTE;\n\t\t\tcount++;\n\t\t} else {\n\t\t\tcfg->lcore_role[i] = ROLE_OFF;\n\t\t}\n\t\tlcore_config[i].core_index = cores[i];\n\t}\n\tif (!ret)\n\t\tcfg->lcore_count = count;\n\treturn ret;\n}\n\nstatic int\neal_parse_coremask(const char *coremask, int *cores)\n{\n\tunsigned count = 0;\n\tint i, j, idx;\n\tint val;\n\tchar c;\n\n\tfor (idx = 0; idx < RTE_MAX_LCORE; idx++)\n\t\tcores[idx] = -1;\n\tidx = 0;\n\n\t/* Remove all blank characters ahead and after .\n\t * Remove 0x/0X if exists.\n\t */\n\twhile (isblank(*coremask))\n\t\tcoremask++;\n\tif (coremask[0] == '0' && ((coremask[1] == 'x')\n\t\t|| (coremask[1] == 'X')))\n\t\tcoremask += 2;\n\ti = strlen(coremask);\n\twhile ((i > 0) && isblank(coremask[i - 1]))\n\t\ti--;\n\tif (i == 0)\n\t\treturn -1;\n\n\tfor (i = i - 1; i >= 0 && idx < RTE_MAX_LCORE; i--) {\n\t\tc = coremask[i];\n\t\tif (isxdigit(c) == 0) {\n\t\t\t/* invalid characters */\n\t\t\treturn -1;\n\t\t}\n\t\tval = xdigit2val(c);\n\t\tfor (j = 0; j < BITS_PER_HEX && idx < RTE_MAX_LCORE; j++, idx++)\n\t\t{\n\t\t\tif ((1 << j) & val) {\n\t\t\t\tcores[idx] = count;\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\tfor (; i >= 0; i--)\n\t\tif (coremask[i] != '0')\n\t\t\treturn -1;\n\tif (count == 0)\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic int\neal_parse_service_corelist(const char *corelist)\n{\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\tint i, idx = 0;\n\tunsigned count = 0;\n\tchar *end = NULL;\n\tint min, max;\n\tuint32_t taken_lcore_count = 0;\n\n\tif (corelist == NULL)\n\t\treturn -1;\n\n\t/* Remove all blank characters ahead and after */\n\twhile (isblank(*corelist))\n\t\tcorelist++;\n\ti = strlen(corelist);\n\twhile ((i > 0) && isblank(corelist[i - 1]))\n\t\ti--;\n\n\t/* Get list of cores */\n\tmin = RTE_MAX_LCORE;\n\tdo {\n\t\twhile (isblank(*corelist))\n\t\t\tcorelist++;\n\t\tif (*corelist == '\\0')\n\t\t\treturn -1;\n\t\terrno = 0;\n\t\tidx = strtoul(corelist, &end, 10);\n\t\tif (errno || end == NULL)\n\t\t\treturn -1;\n\t\twhile (isblank(*end))\n\t\t\tend++;\n\t\tif (*end == '-') {\n\t\t\tmin = idx;\n\t\t} else if ((*end == ',') || (*end == '\\0')) {\n\t\t\tmax = idx;\n\t\t\tif (min == RTE_MAX_LCORE)\n\t\t\t\tmin = idx;\n\t\t\tfor (idx = min; idx <= max; idx++) {\n\t\t\t\tif (cfg->lcore_role[idx] != ROLE_SERVICE) {\n\t\t\t\t\t/* handle master lcore already parsed */\n\t\t\t\t\tuint32_t lcore = idx;\n\t\t\t\t\tif (cfg->master_lcore == lcore &&\n\t\t\t\t\t\t\tmaster_lcore_parsed) {\n\t\t\t\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\t\t\t\"Error: lcore %u is master lcore, cannot use as service core\\n\",\n\t\t\t\t\t\t\tidx);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tif (cfg->lcore_role[idx] == ROLE_RTE)\n\t\t\t\t\t\ttaken_lcore_count++;\n\n\t\t\t\t\tlcore_config[idx].core_role =\n\t\t\t\t\t\t\tROLE_SERVICE;\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tmin = RTE_MAX_LCORE;\n\t\t} else\n\t\t\treturn -1;\n\t\tcorelist = end + 1;\n\t} while (*end != '\\0');\n\n\tif (count == 0)\n\t\treturn -1;\n\n\tif (core_parsed && taken_lcore_count != count) {\n\t\tRTE_LOG(WARNING, EAL,\n\t\t\t\"Not all service cores were in the coremask. \"\n\t\t\t\"Please ensure -c or -l includes service cores\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic int\neal_parse_corelist(const char *corelist, int *cores)\n{\n\tunsigned count = 0;\n\tchar *end = NULL;\n\tint min, max;\n\tint idx;\n\n\tfor (idx = 0; idx < RTE_MAX_LCORE; idx++)\n\t\tcores[idx] = -1;\n\n\t/* Remove all blank characters ahead */\n\twhile (isblank(*corelist))\n\t\tcorelist++;\n\n\t/* Get list of cores */\n\tmin = RTE_MAX_LCORE;\n\tdo {\n\t\twhile (isblank(*corelist))\n\t\t\tcorelist++;\n\t\tif (*corelist == '\\0')\n\t\t\treturn -1;\n\t\terrno = 0;\n\t\tidx = strtol(corelist, &end, 10);\n\t\tif (errno || end == NULL)\n\t\t\treturn -1;\n\t\tif (idx < 0 || idx >= RTE_MAX_LCORE)\n\t\t\treturn -1;\n\t\twhile (isblank(*end))\n\t\t\tend++;\n\t\tif (*end == '-') {\n\t\t\tmin = idx;\n\t\t} else if ((*end == ',') || (*end == '\\0')) {\n\t\t\tmax = idx;\n\t\t\tif (min == RTE_MAX_LCORE)\n\t\t\t\tmin = idx;\n\t\t\tfor (idx = min; idx <= max; idx++) {\n\t\t\t\tif (cores[idx] == -1) {\n\t\t\t\t\tcores[idx] = count;\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tmin = RTE_MAX_LCORE;\n\t\t} else\n\t\t\treturn -1;\n\t\tcorelist = end + 1;\n\t} while (*end != '\\0');\n\n\tif (count == 0)\n\t\treturn -1;\n\treturn 0;\n}\n\n/* Changes the lcore id of the master thread */\nstatic int\neal_parse_master_lcore(const char *arg)\n{\n\tchar *parsing_end;\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\n\terrno = 0;\n\tcfg->master_lcore = (uint32_t) strtol(arg, &parsing_end, 0);\n\tif (errno || parsing_end[0] != 0)\n\t\treturn -1;\n\tif (cfg->master_lcore >= RTE_MAX_LCORE)\n\t\treturn -1;\n\tmaster_lcore_parsed = 1;\n\n\t/* ensure master core is not used as service core */\n\tif (lcore_config[cfg->master_lcore].core_role == ROLE_SERVICE) {\n\t\tRTE_LOG(ERR, EAL,\n\t\t\t\"Error: Master lcore is used as a service core\\n\");\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Parse elem, the elem could be single number/range or '(' ')' group\n * 1) A single number elem, it's just a simple digit. e.g. 9\n * 2) A single range elem, two digits with a '-' between. e.g. 2-6\n * 3) A group elem, combines multiple 1) or 2) with '( )'. e.g (0,2-4,6)\n *    Within group elem, '-' used for a range separator;\n *                       ',' used for a single number.\n */\nstatic int\neal_parse_set(const char *input, uint16_t set[], unsigned num)\n{\n\tunsigned idx;\n\tconst char *str = input;\n\tchar *end = NULL;\n\tunsigned min, max;\n\n\tmemset(set, 0, num * sizeof(uint16_t));\n\n\twhile (isblank(*str))\n\t\tstr++;\n\n\t/* only digit or left bracket is qualify for start point */\n\tif ((!isdigit(*str) && *str != '(') || *str == '\\0')\n\t\treturn -1;\n\n\t/* process single number or single range of number */\n\tif (*str != '(') {\n\t\terrno = 0;\n\t\tidx = strtoul(str, &end, 10);\n\t\tif (errno || end == NULL || idx >= num)\n\t\t\treturn -1;\n\t\telse {\n\t\t\twhile (isblank(*end))\n\t\t\t\tend++;\n\n\t\t\tmin = idx;\n\t\t\tmax = idx;\n\t\t\tif (*end == '-') {\n\t\t\t\t/* process single <number>-<number> */\n\t\t\t\tend++;\n\t\t\t\twhile (isblank(*end))\n\t\t\t\t\tend++;\n\t\t\t\tif (!isdigit(*end))\n\t\t\t\t\treturn -1;\n\n\t\t\t\terrno = 0;\n\t\t\t\tidx = strtoul(end, &end, 10);\n\t\t\t\tif (errno || end == NULL || idx >= num)\n\t\t\t\t\treturn -1;\n\t\t\t\tmax = idx;\n\t\t\t\twhile (isblank(*end))\n\t\t\t\t\tend++;\n\t\t\t\tif (*end != ',' && *end != '\\0')\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tif (*end != ',' && *end != '\\0' &&\n\t\t\t    *end != '@')\n\t\t\t\treturn -1;\n\n\t\t\tfor (idx = RTE_MIN(min, max);\n\t\t\t     idx <= RTE_MAX(min, max); idx++)\n\t\t\t\tset[idx] = 1;\n\n\t\t\treturn end - input;\n\t\t}\n\t}\n\n\t/* process set within bracket */\n\tstr++;\n\twhile (isblank(*str))\n\t\tstr++;\n\tif (*str == '\\0')\n\t\treturn -1;\n\n\tmin = RTE_MAX_LCORE;\n\tdo {\n\n\t\t/* go ahead to the first digit */\n\t\twhile (isblank(*str))\n\t\t\tstr++;\n\t\tif (!isdigit(*str))\n\t\t\treturn -1;\n\n\t\t/* get the digit value */\n\t\terrno = 0;\n\t\tidx = strtoul(str, &end, 10);\n\t\tif (errno || end == NULL || idx >= num)\n\t\t\treturn -1;\n\n\t\t/* go ahead to separator '-',',' and ')' */\n\t\twhile (isblank(*end))\n\t\t\tend++;\n\t\tif (*end == '-') {\n\t\t\tif (min == RTE_MAX_LCORE)\n\t\t\t\tmin = idx;\n\t\t\telse /* avoid continuous '-' */\n\t\t\t\treturn -1;\n\t\t} else if ((*end == ',') || (*end == ')')) {\n\t\t\tmax = idx;\n\t\t\tif (min == RTE_MAX_LCORE)\n\t\t\t\tmin = idx;\n\t\t\tfor (idx = RTE_MIN(min, max);\n\t\t\t     idx <= RTE_MAX(min, max); idx++)\n\t\t\t\tset[idx] = 1;\n\n\t\t\tmin = RTE_MAX_LCORE;\n\t\t} else\n\t\t\treturn -1;\n\n\t\tstr = end + 1;\n\t} while (*end != '\\0' && *end != ')');\n\n\t/*\n\t * to avoid failure that tail blank makes end character check fail\n\t * in eal_parse_lcores( )\n\t */\n\twhile (isblank(*str))\n\t\tstr++;\n\n\treturn str - input;\n}\n\n/* convert from set array to cpuset bitmap */\nstatic int\nconvert_to_cpuset(rte_cpuset_t *cpusetp,\n\t      uint16_t *set, unsigned num)\n{\n\tunsigned idx;\n\n\tCPU_ZERO(cpusetp);\n\n\tfor (idx = 0; idx < num; idx++) {\n\t\tif (!set[idx])\n\t\t\tcontinue;\n\n\t\tif (!lcore_config[idx].detected) {\n\t\t\tRTE_LOG(ERR, EAL, \"core %u \"\n\t\t\t\t\"unavailable\\n\", idx);\n\t\t\treturn -1;\n\t\t}\n\n\t\tCPU_SET(idx, cpusetp);\n\t}\n\n\treturn 0;\n}\n\n/*\n * The format pattern: --lcores='<lcores[@cpus]>[<,lcores[@cpus]>...]'\n * lcores, cpus could be a single digit/range or a group.\n * '(' and ')' are necessary if it's a group.\n * If not supply '@cpus', the value of cpus uses the same as lcores.\n * e.g. '1,2@(5-7),(3-5)@(0,2),(0,6),7-8' means start 9 EAL thread as below\n *   lcore 0 runs on cpuset 0x41 (cpu 0,6)\n *   lcore 1 runs on cpuset 0x2 (cpu 1)\n *   lcore 2 runs on cpuset 0xe0 (cpu 5,6,7)\n *   lcore 3,4,5 runs on cpuset 0x5 (cpu 0,2)\n *   lcore 6 runs on cpuset 0x41 (cpu 0,6)\n *   lcore 7 runs on cpuset 0x80 (cpu 7)\n *   lcore 8 runs on cpuset 0x100 (cpu 8)\n */\nstatic int\neal_parse_lcores(const char *lcores)\n{\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\tstatic uint16_t set[RTE_MAX_LCORE];\n\tunsigned idx = 0;\n\tunsigned count = 0;\n\tconst char *lcore_start = NULL;\n\tconst char *end = NULL;\n\tint offset;\n\trte_cpuset_t cpuset;\n\tint lflags;\n\tint ret = -1;\n\n\tif (lcores == NULL)\n\t\treturn -1;\n\n\t/* Remove all blank characters ahead and after */\n\twhile (isblank(*lcores))\n\t\tlcores++;\n\n\tCPU_ZERO(&cpuset);\n\n\t/* Reset lcore config */\n\tfor (idx = 0; idx < RTE_MAX_LCORE; idx++) {\n\t\tcfg->lcore_role[idx] = ROLE_OFF;\n\t\tlcore_config[idx].core_index = -1;\n\t\tCPU_ZERO(&lcore_config[idx].cpuset);\n\t}\n\n\t/* Get list of cores */\n\tdo {\n\t\twhile (isblank(*lcores))\n\t\t\tlcores++;\n\t\tif (*lcores == '\\0')\n\t\t\tgoto err;\n\n\t\tlflags = 0;\n\n\t\t/* record lcore_set start point */\n\t\tlcore_start = lcores;\n\n\t\t/* go across a complete bracket */\n\t\tif (*lcore_start == '(') {\n\t\t\tlcores += strcspn(lcores, \")\");\n\t\t\tif (*lcores++ == '\\0')\n\t\t\t\tgoto err;\n\t\t}\n\n\t\t/* scan the separator '@', ','(next) or '\\0'(finish) */\n\t\tlcores += strcspn(lcores, \"@,\");\n\n\t\tif (*lcores == '@') {\n\t\t\t/* explicit assign cpu_set */\n\t\t\toffset = eal_parse_set(lcores + 1, set, RTE_DIM(set));\n\t\t\tif (offset < 0)\n\t\t\t\tgoto err;\n\n\t\t\t/* prepare cpu_set and update the end cursor */\n\t\t\tif (0 > convert_to_cpuset(&cpuset,\n\t\t\t\t\t\t  set, RTE_DIM(set)))\n\t\t\t\tgoto err;\n\t\t\tend = lcores + 1 + offset;\n\t\t} else { /* ',' or '\\0' */\n\t\t\t/* haven't given cpu_set, current loop done */\n\t\t\tend = lcores;\n\n\t\t\t/* go back to check <number>-<number> */\n\t\t\toffset = strcspn(lcore_start, \"(-\");\n\t\t\tif (offset < (end - lcore_start) &&\n\t\t\t    *(lcore_start + offset) != '(')\n\t\t\t\tlflags = 1;\n\t\t}\n\n\t\tif (*end != ',' && *end != '\\0')\n\t\t\tgoto err;\n\n\t\t/* parse lcore_set from start point */\n\t\tif (0 > eal_parse_set(lcore_start, set, RTE_DIM(set)))\n\t\t\tgoto err;\n\n\t\t/* without '@', by default using lcore_set as cpu_set */\n\t\tif (*lcores != '@' &&\n\t\t    0 > convert_to_cpuset(&cpuset, set, RTE_DIM(set)))\n\t\t\tgoto err;\n\n\t\t/* start to update lcore_set */\n\t\tfor (idx = 0; idx < RTE_MAX_LCORE; idx++) {\n\t\t\tif (!set[idx])\n\t\t\t\tcontinue;\n\n\t\t\tif (cfg->lcore_role[idx] != ROLE_RTE) {\n\t\t\t\tlcore_config[idx].core_index = count;\n\t\t\t\tcfg->lcore_role[idx] = ROLE_RTE;\n\t\t\t\tcount++;\n\t\t\t}\n\n\t\t\tif (lflags) {\n\t\t\t\tCPU_ZERO(&cpuset);\n\t\t\t\tCPU_SET(idx, &cpuset);\n\t\t\t}\n\t\t\trte_memcpy(&lcore_config[idx].cpuset, &cpuset,\n\t\t\t\t   sizeof(rte_cpuset_t));\n\t\t}\n\n\t\tlcores = end + 1;\n\t} while (*end != '\\0');\n\n\tif (count == 0)\n\t\tgoto err;\n\n\tcfg->lcore_count = count;\n\tret = 0;\n\nerr:\n\n\treturn ret;\n}\n\nstatic int\neal_parse_syslog(const char *facility, struct internal_config *conf)\n{\n\tint i;\n\tstatic const struct {\n\t\tconst char *name;\n\t\tint value;\n\t} map[] = {\n\t\t{ \"auth\", LOG_AUTH },\n\t\t{ \"cron\", LOG_CRON },\n\t\t{ \"daemon\", LOG_DAEMON },\n\t\t{ \"ftp\", LOG_FTP },\n\t\t{ \"kern\", LOG_KERN },\n\t\t{ \"lpr\", LOG_LPR },\n\t\t{ \"mail\", LOG_MAIL },\n\t\t{ \"news\", LOG_NEWS },\n\t\t{ \"syslog\", LOG_SYSLOG },\n\t\t{ \"user\", LOG_USER },\n\t\t{ \"uucp\", LOG_UUCP },\n\t\t{ \"local0\", LOG_LOCAL0 },\n\t\t{ \"local1\", LOG_LOCAL1 },\n\t\t{ \"local2\", LOG_LOCAL2 },\n\t\t{ \"local3\", LOG_LOCAL3 },\n\t\t{ \"local4\", LOG_LOCAL4 },\n\t\t{ \"local5\", LOG_LOCAL5 },\n\t\t{ \"local6\", LOG_LOCAL6 },\n\t\t{ \"local7\", LOG_LOCAL7 },\n\t\t{ NULL, 0 }\n\t};\n\n\tfor (i = 0; map[i].name; i++) {\n\t\tif (!strcmp(facility, map[i].name)) {\n\t\t\tconf->syslog_facility = map[i].value;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -1;\n}\n\nstatic int\neal_parse_log_priority(const char *level)\n{\n\tstatic const char * const levels[] = {\n\t\t[RTE_LOG_EMERG]   = \"emergency\",\n\t\t[RTE_LOG_ALERT]   = \"alert\",\n\t\t[RTE_LOG_CRIT]    = \"critical\",\n\t\t[RTE_LOG_ERR]     = \"error\",\n\t\t[RTE_LOG_WARNING] = \"warning\",\n\t\t[RTE_LOG_NOTICE]  = \"notice\",\n\t\t[RTE_LOG_INFO]    = \"info\",\n\t\t[RTE_LOG_DEBUG]   = \"debug\",\n\t};\n\tsize_t len = strlen(level);\n\tunsigned long tmp;\n\tchar *end;\n\tunsigned int i;\n\n\tif (len == 0)\n\t\treturn -1;\n\n\t/* look for named values, skip 0 which is not a valid level */\n\tfor (i = 1; i < RTE_DIM(levels); i++) {\n\t\tif (strncmp(levels[i], level, len) == 0)\n\t\t\treturn i;\n\t}\n\n\t/* not a string, maybe it is numeric */\n\terrno = 0;\n\ttmp = strtoul(level, &end, 0);\n\n\t/* check for errors */\n\tif (errno != 0 || end == NULL || *end != '\\0' ||\n\t    tmp >= UINT32_MAX)\n\t\treturn -1;\n\n\treturn tmp;\n}\n\nstatic int\neal_parse_log_level(const char *arg)\n{\n\tconst char *pattern = NULL;\n\tconst char *regex = NULL;\n\tchar *str, *level;\n\tint priority;\n\n\tstr = strdup(arg);\n\tif (str == NULL)\n\t\treturn -1;\n\n\tif ((level = strchr(str, ','))) {\n\t\tregex = str;\n\t\t*level++ = '\\0';\n\t} else if ((level = strchr(str, ':'))) {\n\t\tpattern = str;\n\t\t*level++ = '\\0';\n\t} else {\n\t\tlevel = str;\n\t}\n\n\tpriority = eal_parse_log_priority(level);\n\tif (priority < 0) {\n\t\tfprintf(stderr, \"invalid log priority: %s\\n\", level);\n\t\tgoto fail;\n\t}\n\n\tif (regex) {\n\t\tif (rte_log_set_level_regexp(regex, priority) < 0) {\n\t\t\tfprintf(stderr, \"cannot set log level %s,%d\\n\",\n\t\t\t\tpattern, priority);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (rte_log_save_regexp(regex, priority) < 0)\n\t\t\tgoto fail;\n\t} else if (pattern) {\n\t\tif (rte_log_set_level_pattern(pattern, priority) < 0) {\n\t\t\tfprintf(stderr, \"cannot set log level %s:%d\\n\",\n\t\t\t\tpattern, priority);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (rte_log_save_pattern(pattern, priority) < 0)\n\t\t\tgoto fail;\n\t} else {\n\t\trte_log_set_global_level(priority);\n\t}\n\n\tfree(str);\n\treturn 0;\n\nfail:\n\tfree(str);\n\treturn -1;\n}\n\nstatic enum rte_proc_type_t\neal_parse_proc_type(const char *arg)\n{\n\tif (strncasecmp(arg, \"primary\", sizeof(\"primary\")) == 0)\n\t\treturn RTE_PROC_PRIMARY;\n\tif (strncasecmp(arg, \"secondary\", sizeof(\"secondary\")) == 0)\n\t\treturn RTE_PROC_SECONDARY;\n\tif (strncasecmp(arg, \"auto\", sizeof(\"auto\")) == 0)\n\t\treturn RTE_PROC_AUTO;\n\n\treturn RTE_PROC_INVALID;\n}\n\nstatic int\neal_parse_iova_mode(const char *name)\n{\n\tint mode;\n\n\tif (name == NULL)\n\t\treturn -1;\n\n\tif (!strcmp(\"pa\", name))\n\t\tmode = RTE_IOVA_PA;\n\telse if (!strcmp(\"va\", name))\n\t\tmode = RTE_IOVA_VA;\n\telse\n\t\treturn -1;\n\n\tinternal_config.iova_mode = mode;\n\treturn 0;\n}\n\n/* caller is responsible for freeing the returned string */\nstatic char *\navailable_cores(void)\n{\n\tchar *str = NULL;\n\tint previous;\n\tint sequence;\n\tchar *tmp;\n\tint idx;\n\n\t/* find the first available cpu */\n\tfor (idx = 0; idx < RTE_MAX_LCORE; idx++) {\n\t\tif (!lcore_config[idx].detected)\n\t\t\tcontinue;\n\t\tbreak;\n\t}\n\tif (idx >= RTE_MAX_LCORE)\n\t\treturn NULL;\n\n\t/* first sequence */\n\tif (asprintf(&str, \"%d\", idx) < 0)\n\t\treturn NULL;\n\tprevious = idx;\n\tsequence = 0;\n\n\tfor (idx++ ; idx < RTE_MAX_LCORE; idx++) {\n\t\tif (!lcore_config[idx].detected)\n\t\t\tcontinue;\n\n\t\tif (idx == previous + 1) {\n\t\t\tprevious = idx;\n\t\t\tsequence = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* finish current sequence */\n\t\tif (sequence) {\n\t\t\tif (asprintf(&tmp, \"%s-%d\", str, previous) < 0) {\n\t\t\t\tfree(str);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tfree(str);\n\t\t\tstr = tmp;\n\t\t}\n\n\t\t/* new sequence */\n\t\tif (asprintf(&tmp, \"%s,%d\", str, idx) < 0) {\n\t\t\tfree(str);\n\t\t\treturn NULL;\n\t\t}\n\t\tfree(str);\n\t\tstr = tmp;\n\t\tprevious = idx;\n\t\tsequence = 0;\n\t}\n\n\t/* finish last sequence */\n\tif (sequence) {\n\t\tif (asprintf(&tmp, \"%s-%d\", str, previous) < 0) {\n\t\t\tfree(str);\n\t\t\treturn NULL;\n\t\t}\n\t\tfree(str);\n\t\tstr = tmp;\n\t}\n\n\treturn str;\n}\n\nint\neal_parse_common_option(int opt, const char *optarg,\n\t\t\tstruct internal_config *conf)\n{\n\tstatic int b_used;\n\tstatic int w_used;\n\n\tswitch (opt) {\n\t/* blacklist */\n\tcase 'b':\n\t\tif (w_used)\n\t\t\tgoto bw_used;\n\t\tif (eal_option_device_add(RTE_DEVTYPE_BLACKLISTED_PCI,\n\t\t\t\toptarg) < 0) {\n\t\t\treturn -1;\n\t\t}\n\t\tb_used = 1;\n\t\tbreak;\n\t/* whitelist */\n\tcase 'w':\n\t\tif (b_used)\n\t\t\tgoto bw_used;\n\t\tif (eal_option_device_add(RTE_DEVTYPE_WHITELISTED_PCI,\n\t\t\t\toptarg) < 0) {\n\t\t\treturn -1;\n\t\t}\n\t\tw_used = 1;\n\t\tbreak;\n\t/* coremask */\n\tcase 'c': {\n\t\tint lcore_indexes[RTE_MAX_LCORE];\n\n\t\tif (eal_service_cores_parsed())\n\t\t\tRTE_LOG(WARNING, EAL,\n\t\t\t\t\"Service cores parsed before dataplane cores. Please ensure -c is before -s or -S\\n\");\n\t\tif (eal_parse_coremask(optarg, lcore_indexes) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid coremask syntax\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (update_lcore_config(lcore_indexes) < 0) {\n\t\t\tchar *available = available_cores();\n\n\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\"invalid coremask, please check specified cores are part of %s\\n\",\n\t\t\t\tavailable);\n\t\t\tfree(available);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (core_parsed) {\n\t\t\tRTE_LOG(ERR, EAL, \"Option -c is ignored, because (%s) is set!\\n\",\n\t\t\t\t(core_parsed == LCORE_OPT_LST) ? \"-l\" :\n\t\t\t\t(core_parsed == LCORE_OPT_MAP) ? \"--lcore\" :\n\t\t\t\t\"-c\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tcore_parsed = LCORE_OPT_MSK;\n\t\tbreak;\n\t}\n\t/* corelist */\n\tcase 'l': {\n\t\tint lcore_indexes[RTE_MAX_LCORE];\n\n\t\tif (eal_service_cores_parsed())\n\t\t\tRTE_LOG(WARNING, EAL,\n\t\t\t\t\"Service cores parsed before dataplane cores. Please ensure -l is before -s or -S\\n\");\n\n\t\tif (eal_parse_corelist(optarg, lcore_indexes) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid core list syntax\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (update_lcore_config(lcore_indexes) < 0) {\n\t\t\tchar *available = available_cores();\n\n\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\"invalid core list, please check specified cores are part of %s\\n\",\n\t\t\t\tavailable);\n\t\t\tfree(available);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (core_parsed) {\n\t\t\tRTE_LOG(ERR, EAL, \"Option -l is ignored, because (%s) is set!\\n\",\n\t\t\t\t(core_parsed == LCORE_OPT_MSK) ? \"-c\" :\n\t\t\t\t(core_parsed == LCORE_OPT_MAP) ? \"--lcore\" :\n\t\t\t\t\"-l\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tcore_parsed = LCORE_OPT_LST;\n\t\tbreak;\n\t}\n\t/* service coremask */\n\tcase 's':\n\t\tif (eal_parse_service_coremask(optarg) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid service coremask\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\t/* service corelist */\n\tcase 'S':\n\t\tif (eal_parse_service_corelist(optarg) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid service core list\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\t/* size of memory */\n\tcase 'm':\n\t\tconf->memory = atoi(optarg);\n\t\tconf->memory *= 1024ULL;\n\t\tconf->memory *= 1024ULL;\n\t\tmem_parsed = 1;\n\t\tbreak;\n\t/* force number of channels */\n\tcase 'n':\n\t\tconf->force_nchannel = atoi(optarg);\n\t\tif (conf->force_nchannel == 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid channel number\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\t/* force number of ranks */\n\tcase 'r':\n\t\tconf->force_nrank = atoi(optarg);\n\t\tif (conf->force_nrank == 0 ||\n\t\t    conf->force_nrank > 16) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid rank number\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\t/* force loading of external driver */\n\tcase 'd':\n\t\tif (eal_plugin_add(optarg) == -1)\n\t\t\treturn -1;\n\t\tbreak;\n\tcase 'v':\n\t\t/* since message is explicitly requested by user, we\n\t\t * write message at highest log level so it can always\n\t\t * be seen\n\t\t * even if info or warning messages are disabled */\n\t\tRTE_LOG(CRIT, EAL, \"RTE Version: '%s'\\n\", rte_version());\n\t\tbreak;\n\n\t/* long options */\n\tcase OPT_HUGE_UNLINK_NUM:\n\t\tconf->hugepage_unlink = 1;\n\t\tbreak;\n\n\tcase OPT_NO_HUGE_NUM:\n\t\tconf->no_hugetlbfs = 1;\n\t\t/* no-huge is legacy mem */\n\t\tconf->legacy_mem = 1;\n\t\tbreak;\n\n\tcase OPT_NO_PCI_NUM:\n\t\tconf->no_pci = 1;\n\t\tbreak;\n\n\tcase OPT_NO_HPET_NUM:\n\t\tconf->no_hpet = 1;\n\t\tbreak;\n\n\tcase OPT_VMWARE_TSC_MAP_NUM:\n\t\tconf->vmware_tsc_map = 1;\n\t\tbreak;\n\n\tcase OPT_NO_SHCONF_NUM:\n\t\tconf->no_shconf = 1;\n\t\tbreak;\n\n\tcase OPT_IN_MEMORY_NUM:\n\t\tconf->in_memory = 1;\n\t\t/* in-memory is a superset of noshconf and huge-unlink */\n\t\tconf->no_shconf = 1;\n\t\tconf->hugepage_unlink = 1;\n\t\tbreak;\n\n\tcase OPT_PROC_TYPE_NUM:\n\t\tconf->process_type = eal_parse_proc_type(optarg);\n\t\tbreak;\n\n\tcase OPT_MASTER_LCORE_NUM:\n\t\tif (eal_parse_master_lcore(optarg) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid parameter for --\"\n\t\t\t\t\tOPT_MASTER_LCORE \"\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\n\tcase OPT_VDEV_NUM:\n\t\tif (eal_option_device_add(RTE_DEVTYPE_VIRTUAL,\n\t\t\t\toptarg) < 0) {\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\n\tcase OPT_SYSLOG_NUM:\n\t\tif (eal_parse_syslog(optarg, conf) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid parameters for --\"\n\t\t\t\t\tOPT_SYSLOG \"\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\n\tcase OPT_LOG_LEVEL_NUM: {\n\t\tif (eal_parse_log_level(optarg) < 0) {\n\t\t\tRTE_LOG(ERR, EAL,\n\t\t\t\t\"invalid parameters for --\"\n\t\t\t\tOPT_LOG_LEVEL \"\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\t}\n\tcase OPT_LCORES_NUM:\n\t\tif (eal_parse_lcores(optarg) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid parameter for --\"\n\t\t\t\tOPT_LCORES \"\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (core_parsed) {\n\t\t\tRTE_LOG(ERR, EAL, \"Option --lcore is ignored, because (%s) is set!\\n\",\n\t\t\t\t(core_parsed == LCORE_OPT_LST) ? \"-l\" :\n\t\t\t\t(core_parsed == LCORE_OPT_MSK) ? \"-c\" :\n\t\t\t\t\"--lcore\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tcore_parsed = LCORE_OPT_MAP;\n\t\tbreak;\n\tcase OPT_LEGACY_MEM_NUM:\n\t\tconf->legacy_mem = 1;\n\t\tbreak;\n\tcase OPT_SINGLE_FILE_SEGMENTS_NUM:\n\t\tconf->single_file_segments = 1;\n\t\tbreak;\n\tcase OPT_IOVA_MODE_NUM:\n\t\tif (eal_parse_iova_mode(optarg) < 0) {\n\t\t\tRTE_LOG(ERR, EAL, \"invalid parameters for --\"\n\t\t\t\tOPT_IOVA_MODE \"\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n\n\t/* don't know what to do, leave this to caller */\n\tdefault:\n\t\treturn 1;\n\n\t}\n\n\treturn 0;\nbw_used:\n\tRTE_LOG(ERR, EAL, \"Options blacklist (-b) and whitelist (-w) \"\n\t\t\"cannot be used at the same time\\n\");\n\treturn -1;\n}\n\nstatic void\neal_auto_detect_cores(struct rte_config *cfg)\n{\n\tunsigned int lcore_id;\n\tunsigned int removed = 0;\n\trte_cpuset_t affinity_set;\n\n\tif (pthread_getaffinity_np(pthread_self(), sizeof(rte_cpuset_t),\n\t\t\t\t&affinity_set))\n\t\tCPU_ZERO(&affinity_set);\n\n\tfor (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++) {\n\t\tif (cfg->lcore_role[lcore_id] == ROLE_RTE &&\n\t\t    !CPU_ISSET(lcore_id, &affinity_set)) {\n\t\t\tcfg->lcore_role[lcore_id] = ROLE_OFF;\n\t\t\tremoved++;\n\t\t}\n\t}\n\n\tcfg->lcore_count -= removed;\n}\n\nstatic void\ncompute_ctrl_threads_cpuset(struct internal_config *internal_cfg)\n{\n\trte_cpuset_t *cpuset = &internal_cfg->ctrl_cpuset;\n\trte_cpuset_t default_set;\n\tunsigned int lcore_id;\n\n\tfor (lcore_id = 0; lcore_id < RTE_MAX_LCORE; lcore_id++) {\n\t\tif (eal_cpu_detected(lcore_id) &&\n\t\t\t\trte_lcore_has_role(lcore_id, ROLE_OFF)) {\n\t\t\tCPU_SET(lcore_id, cpuset);\n\t\t}\n\t}\n\n\tif (pthread_getaffinity_np(pthread_self(), sizeof(rte_cpuset_t),\n\t\t\t\t&default_set))\n\t\tCPU_ZERO(&default_set);\n\n\tRTE_CPU_AND(cpuset, cpuset, &default_set);\n\n\t/* if no detected CPU is off, use master core */\n\tif (!CPU_COUNT(cpuset))\n\t\tCPU_SET(rte_get_master_lcore(), cpuset);\n}\n\nint\neal_cleanup_config(struct internal_config *internal_cfg)\n{\n\tif (internal_cfg->hugefile_prefix != NULL)\n\t\tfree(internal_cfg->hugefile_prefix);\n\tif (internal_cfg->hugepage_dir != NULL)\n\t\tfree(internal_cfg->hugepage_dir);\n\tif (internal_cfg->user_mbuf_pool_ops_name != NULL)\n\t\tfree(internal_cfg->user_mbuf_pool_ops_name);\n\n\treturn 0;\n}\n\nint\neal_adjust_config(struct internal_config *internal_cfg)\n{\n\tint i;\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\n\tif (!core_parsed)\n\t\teal_auto_detect_cores(cfg);\n\n\tif (internal_config.process_type == RTE_PROC_AUTO)\n\t\tinternal_config.process_type = eal_proc_type_detect();\n\n\t/* default master lcore is the first one */\n\tif (!master_lcore_parsed) {\n\t\tcfg->master_lcore = rte_get_next_lcore(-1, 0, 0);\n\t\tif (cfg->master_lcore >= RTE_MAX_LCORE)\n\t\t\treturn -1;\n\t\tlcore_config[cfg->master_lcore].core_role = ROLE_RTE;\n\t}\n\n\tcompute_ctrl_threads_cpuset(internal_cfg);\n\n\t/* if no memory amounts were requested, this will result in 0 and\n\t * will be overridden later, right after eal_hugepage_info_init() */\n\tfor (i = 0; i < RTE_MAX_NUMA_NODES; i++)\n\t\tinternal_cfg->memory += internal_cfg->socket_mem[i];\n\n\treturn 0;\n}\n\nint\neal_check_common_options(struct internal_config *internal_cfg)\n{\n\tstruct rte_config *cfg = rte_eal_get_configuration();\n\n\tif (cfg->lcore_role[cfg->master_lcore] != ROLE_RTE) {\n\t\tRTE_LOG(ERR, EAL, \"Master lcore is not enabled for DPDK\\n\");\n\t\treturn -1;\n\t}\n\n\tif (internal_cfg->process_type == RTE_PROC_INVALID) {\n\t\tRTE_LOG(ERR, EAL, \"Invalid process type specified\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->hugefile_prefix != NULL &&\n\t\t\tstrlen(internal_cfg->hugefile_prefix) < 1) {\n\t\tRTE_LOG(ERR, EAL, \"Invalid length of --\" OPT_FILE_PREFIX \" option\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->hugepage_dir != NULL &&\n\t\t\tstrlen(internal_cfg->hugepage_dir) < 1) {\n\t\tRTE_LOG(ERR, EAL, \"Invalid length of --\" OPT_HUGE_DIR\" option\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->user_mbuf_pool_ops_name != NULL &&\n\t\t\tstrlen(internal_cfg->user_mbuf_pool_ops_name) < 1) {\n\t\tRTE_LOG(ERR, EAL, \"Invalid length of --\" OPT_MBUF_POOL_OPS_NAME\" option\\n\");\n\t\treturn -1;\n\t}\n\tif (index(eal_get_hugefile_prefix(), '%') != NULL) {\n\t\tRTE_LOG(ERR, EAL, \"Invalid char, '%%', in --\"OPT_FILE_PREFIX\" \"\n\t\t\t\"option\\n\");\n\t\treturn -1;\n\t}\n\tif (mem_parsed && internal_cfg->force_sockets == 1) {\n\t\tRTE_LOG(ERR, EAL, \"Options -m and --\"OPT_SOCKET_MEM\" cannot \"\n\t\t\t\"be specified at the same time\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->no_hugetlbfs && internal_cfg->force_sockets == 1) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_SOCKET_MEM\" cannot \"\n\t\t\t\"be specified together with --\"OPT_NO_HUGE\"\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->no_hugetlbfs && internal_cfg->hugepage_unlink &&\n\t\t\t!internal_cfg->in_memory) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_HUGE_UNLINK\" cannot \"\n\t\t\t\"be specified together with --\"OPT_NO_HUGE\"\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_config.force_socket_limits && internal_config.legacy_mem) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_SOCKET_LIMIT\n\t\t\t\" is only supported in non-legacy memory mode\\n\");\n\t}\n\tif (internal_cfg->single_file_segments &&\n\t\t\tinternal_cfg->hugepage_unlink &&\n\t\t\t!internal_cfg->in_memory) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_SINGLE_FILE_SEGMENTS\" is \"\n\t\t\t\"not compatible with --\"OPT_HUGE_UNLINK\"\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->legacy_mem &&\n\t\t\tinternal_cfg->in_memory) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_LEGACY_MEM\" is not compatible \"\n\t\t\t\t\"with --\"OPT_IN_MEMORY\"\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->legacy_mem && internal_cfg->match_allocations) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_LEGACY_MEM\" is not compatible \"\n\t\t\t\t\"with --\"OPT_MATCH_ALLOCATIONS\"\\n\");\n\t\treturn -1;\n\t}\n\tif (internal_cfg->no_hugetlbfs && internal_cfg->match_allocations) {\n\t\tRTE_LOG(ERR, EAL, \"Option --\"OPT_NO_HUGE\" is not compatible \"\n\t\t\t\t\"with --\"OPT_MATCH_ALLOCATIONS\"\\n\");\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nvoid\neal_common_usage(void)\n{\n\tprintf(\"[options]\\n\\n\"\n\t       \"EAL common options:\\n\"\n\t       \"  -c COREMASK         Hexadecimal bitmask of cores to run on\\n\"\n\t       \"  -l CORELIST         List of cores to run on\\n\"\n\t       \"                      The argument format is <c1>[-c2][,c3[-c4],...]\\n\"\n\t       \"                      where c1, c2, etc are core indexes between 0 and %d\\n\"\n\t       \"  --\"OPT_LCORES\" COREMAP    Map lcore set to physical cpu set\\n\"\n\t       \"                      The argument format is\\n\"\n\t       \"                            '<lcores[@cpus]>[<,lcores[@cpus]>...]'\\n\"\n\t       \"                      lcores and cpus list are grouped by '(' and ')'\\n\"\n\t       \"                      Within the group, '-' is used for range separator,\\n\"\n\t       \"                      ',' is used for single number separator.\\n\"\n\t       \"                      '( )' can be omitted for single element group,\\n\"\n\t       \"                      '@' can be omitted if cpus and lcores have the same value\\n\"\n\t       \"  -s SERVICE COREMASK Hexadecimal bitmask of cores to be used as service cores\\n\"\n\t       \"  --\"OPT_MASTER_LCORE\" ID   Core ID that is used as master\\n\"\n\t       \"  --\"OPT_MBUF_POOL_OPS_NAME\" Pool ops name for mbuf to use\\n\"\n\t       \"  -n CHANNELS         Number of memory channels\\n\"\n\t       \"  -m MB               Memory to allocate (see also --\"OPT_SOCKET_MEM\")\\n\"\n\t       \"  -r RANKS            Force number of memory ranks (don't detect)\\n\"\n\t       \"  -b, --\"OPT_PCI_BLACKLIST\" Add a PCI device in black list.\\n\"\n\t       \"                      Prevent EAL from using this PCI device. The argument\\n\"\n\t       \"                      format is <domain:bus:devid.func>.\\n\"\n\t       \"  -w, --\"OPT_PCI_WHITELIST\" Add a PCI device in white list.\\n\"\n\t       \"                      Only use the specified PCI devices. The argument format\\n\"\n\t       \"                      is <[domain:]bus:devid.func>. This option can be present\\n\"\n\t       \"                      several times (once per device).\\n\"\n\t       \"                      [NOTE: PCI whitelist cannot be used with -b option]\\n\"\n\t       \"  --\"OPT_VDEV\"              Add a virtual device.\\n\"\n\t       \"                      The argument format is <driver><id>[,key=val,...]\\n\"\n\t       \"                      (ex: --vdev=net_pcap0,iface=eth2).\\n\"\n\t       \"  --\"OPT_IOVA_MODE\"   Set IOVA mode. 'pa' for IOVA_PA\\n\"\n\t       \"                      'va' for IOVA_VA\\n\"\n\t       \"  -d LIB.so|DIR       Add a driver or driver directory\\n\"\n\t       \"                      (can be used multiple times)\\n\"\n\t       \"  --\"OPT_VMWARE_TSC_MAP\"    Use VMware TSC map instead of native RDTSC\\n\"\n\t       \"  --\"OPT_PROC_TYPE\"         Type of this process (primary|secondary|auto)\\n\"\n\t       \"  --\"OPT_SYSLOG\"            Set syslog facility\\n\"\n\t       \"  --\"OPT_LOG_LEVEL\"=<int>   Set global log level\\n\"\n\t       \"  --\"OPT_LOG_LEVEL\"=<type-match>:<int>\\n\"\n\t       \"                      Set specific log level\\n\"\n\t       \"  -v                  Display version information on startup\\n\"\n\t       \"  -h, --help          This help\\n\"\n\t       \"  --\"OPT_IN_MEMORY\"   Operate entirely in memory. This will\\n\"\n\t       \"                      disable secondary process support\\n\"\n\t       \"\\nEAL options for DEBUG use only:\\n\"\n\t       \"  --\"OPT_HUGE_UNLINK\"       Unlink hugepage files after init\\n\"\n\t       \"  --\"OPT_NO_HUGE\"           Use malloc instead of hugetlbfs\\n\"\n\t       \"  --\"OPT_NO_PCI\"            Disable PCI\\n\"\n\t       \"  --\"OPT_NO_HPET\"           Disable HPET\\n\"\n\t       \"  --\"OPT_NO_SHCONF\"         No shared config (mmap'd files)\\n\"\n\t       \"\\n\", RTE_MAX_LCORE);\n\trte_option_usage();\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/mlx4/meson.build": "# SPDX-License-Identifier: BSD-3-Clause\n# Copyright 2018 6WIND S.A.\n# Copyright 2018 Mellanox Technologies, Ltd\n\npmd_dlopen = (get_option('ibverbs_link') == 'dlopen')\nLIB_GLUE_BASE = 'librte_pmd_mlx4_glue.so'\nLIB_GLUE_VERSION = '18.02.0'\nLIB_GLUE = LIB_GLUE_BASE + '.' + LIB_GLUE_VERSION\nif pmd_dlopen\n\tdpdk_conf.set('RTE_IBVERBS_LINK_DLOPEN', 1)\n\tcflags += [\n\t\t'-DMLX4_GLUE=\"@0@\"'.format(LIB_GLUE),\n\t\t'-DMLX4_GLUE_VERSION=\"@0@\"'.format(LIB_GLUE_VERSION),\n\t]\nendif\nlibnames = [ 'mnl', 'mlx4', 'ibverbs' ]\nlibs = []\nbuild = true\nforeach libname:libnames\n\tlib = dependency('lib' + libname, required:false)\n\tif not lib.found()\n\t\tlib = cc.find_library(libname, required:false)\n\tendif\n\tif lib.found()\n\t\tlibs += [ lib ]\n\telse\n\t\tbuild = false\n\tendif\nendforeach\n# Compile PMD\nif build\n\tallow_experimental_apis = true\n\text_deps += libs\n\tsources = files(\n\t\t'mlx4.c',\n\t\t'mlx4_ethdev.c',\n\t\t'mlx4_flow.c',\n\t\t'mlx4_intr.c',\n\t\t'mlx4_mp.c',\n\t\t'mlx4_mr.c',\n\t\t'mlx4_rxq.c',\n\t\t'mlx4_rxtx.c',\n\t\t'mlx4_txq.c',\n\t\t'mlx4_utils.c',\n\t)\n\tif not pmd_dlopen\n\t\tsources += files('mlx4_glue.c')\n\tendif\n\tcflags_options = [\n\t\t'-Wextra',\n\t\t'-std=c11',\n\t\t'-Wno-strict-prototypes',\n\t\t'-D_BSD_SOURCE',\n\t\t'-D_DEFAULT_SOURCE',\n\t\t'-D_XOPEN_SOURCE=600'\n\t]\n\tforeach option:cflags_options\n\t\tif cc.has_argument(option)\n\t\t\tcflags += option\n\t\tendif\n\tendforeach\n\tif get_option('buildtype').contains('debug')\n\t\tcflags += [ '-pedantic', '-UNDEBUG', '-DPEDANTIC' ]\n\telse\n\t\tcflags += [ '-DNDEBUG', '-UPEDANTIC' ]\n\tendif\n\t# To maintain the compatibility with the make build system\n\t# mlx4_autoconf.h file is still generated.\n\t# input array for meson member search:\n\t# [ \"MACRO to define if found\", \"header for the search\",\n\t#   \"symbol to search\",\"struct member to search\" ]\n\t#\n\thas_member_args = [\n\t\t[ 'HAVE_IBV_MLX4_WQE_LSO_SEG', 'infiniband/mlx4dv.h',\n\t\t'struct mlx4_wqe_lso_seg', 'mss_hdr_size' ],\n\t]\n\t# input array for meson symbol search:\n\t# [ \"MACRO to define if found\", \"header for the search\",\n\t#   \"symbol to search\" ]\n\thas_sym_args = [\n\t\t[ 'HAVE_IBV_MLX4_BUF_ALLOCATORS', 'infiniband/mlx4dv.h',\n\t\t'MLX4DV_SET_CTX_ATTR_BUF_ALLOCATORS' ],\n\t\t[ 'HAVE_IBV_MLX4_UAR_MMAP_OFFSET', 'infiniband/mlx4dv.h',\n\t\t'MLX4DV_QP_MASK_UAR_MMAP_OFFSET' ],\n\t]\n\tconfig = configuration_data()\n\tforeach arg:has_sym_args\n\t\tconfig.set(arg[0], cc.has_header_symbol(arg[1], arg[2]))\n\tendforeach\n\tforeach arg:has_member_args\n\t\tfile_prefix = '#include<' + arg[1] + '>'\n\t\tconfig.set(arg[0], cc.has_member(arg[2], arg[3],\n\t\t\tprefix : file_prefix))\n\tendforeach\n\tconfigure_file(output : 'mlx4_autoconf.h', configuration : config)\nendif\n# Build Glue Library\nif pmd_dlopen and build\n\tdlopen_name = 'mlx4_glue'\n\tdlopen_lib_name = driver_name_fmt.format(dlopen_name)\n\tdlopen_so_version = LIB_GLUE_VERSION\n\tdlopen_sources = files('mlx4_glue.c')\n\tdlopen_install_dir = [ eal_pmd_path + '-glue' ]\n\tshared_lib = shared_library(\n\t\tdlopen_lib_name,\n\t\tdlopen_sources,\n\t\tinclude_directories: global_inc,\n\t\tc_args: cflags,\n\t\tdependencies: libs,\n\t\tlink_args: [\n\t\t'-Wl,-export-dynamic',\n\t\t'-Wl,-h,@0@'.format(LIB_GLUE),\n\t\t],\n\t\tsoversion: dlopen_so_version,\n\t\tinstall: true,\n\t\tinstall_dir: dlopen_install_dir,\n\t)\nendif\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/mlx4/mlx4.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright 2012 6WIND S.A.\n * Copyright 2012 Mellanox Technologies, Ltd\n */\n\n/**\n * @file\n * mlx4 driver initialization.\n */\n\n#include <assert.h>\n#include <dlfcn.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <stddef.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/mman.h>\n#include <unistd.h>\n\n/* Verbs headers do not support -pedantic. */\n#ifdef PEDANTIC\n#pragma GCC diagnostic ignored \"-Wpedantic\"\n#endif\n#include <infiniband/verbs.h>\n#ifdef PEDANTIC\n#pragma GCC diagnostic error \"-Wpedantic\"\n#endif\n\n#include <rte_common.h>\n#include <rte_config.h>\n#include <rte_dev.h>\n#include <rte_errno.h>\n#include <rte_ethdev_driver.h>\n#include <rte_ethdev_pci.h>\n#include <rte_ether.h>\n#include <rte_flow.h>\n#include <rte_interrupts.h>\n#include <rte_kvargs.h>\n#include <rte_malloc.h>\n#include <rte_mbuf.h>\n\n#include \"mlx4.h\"\n#include \"mlx4_glue.h\"\n#include \"mlx4_flow.h\"\n#include \"mlx4_mr.h\"\n#include \"mlx4_rxtx.h\"\n#include \"mlx4_utils.h\"\n\nstatic const char *MZ_MLX4_PMD_SHARED_DATA = \"mlx4_pmd_shared_data\";\n\n/* Shared memory between primary and secondary processes. */\nstruct mlx4_shared_data *mlx4_shared_data;\n\n/* Spinlock for mlx4_shared_data allocation. */\nstatic rte_spinlock_t mlx4_shared_data_lock = RTE_SPINLOCK_INITIALIZER;\n\n/* Process local data for secondary processes. */\nstatic struct mlx4_local_data mlx4_local_data;\n\n/** Configuration structure for device arguments. */\nstruct mlx4_conf {\n\tstruct {\n\t\tuint32_t present; /**< Bit-field for existing ports. */\n\t\tuint32_t enabled; /**< Bit-field for user-enabled ports. */\n\t} ports;\n\tint mr_ext_memseg_en;\n\t/** Whether memseg should be extended for MR creation. */\n};\n\n/* Available parameters list. */\nconst char *pmd_mlx4_init_params[] = {\n\tMLX4_PMD_PORT_KVARG,\n\tMLX4_MR_EXT_MEMSEG_EN_KVARG,\n\tNULL,\n};\n\nstatic void mlx4_dev_stop(struct rte_eth_dev *dev);\n\n/**\n * Initialize shared data between primary and secondary process.\n *\n * A memzone is reserved by primary process and secondary processes attach to\n * the memzone.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_init_shared_data(void)\n{\n\tconst struct rte_memzone *mz;\n\tint ret = 0;\n\n\trte_spinlock_lock(&mlx4_shared_data_lock);\n\tif (mlx4_shared_data == NULL) {\n\t\tif (rte_eal_process_type() == RTE_PROC_PRIMARY) {\n\t\t\t/* Allocate shared memory. */\n\t\t\tmz = rte_memzone_reserve(MZ_MLX4_PMD_SHARED_DATA,\n\t\t\t\t\t\t sizeof(*mlx4_shared_data),\n\t\t\t\t\t\t SOCKET_ID_ANY, 0);\n\t\t\tif (mz == NULL) {\n\t\t\t\tERROR(\"Cannot allocate mlx4 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx4_shared_data = mz->addr;\n\t\t\tmemset(mlx4_shared_data, 0, sizeof(*mlx4_shared_data));\n\t\t\trte_spinlock_init(&mlx4_shared_data->lock);\n\t\t} else {\n\t\t\t/* Lookup allocated shared memory. */\n\t\t\tmz = rte_memzone_lookup(MZ_MLX4_PMD_SHARED_DATA);\n\t\t\tif (mz == NULL) {\n\t\t\t\tERROR(\"Cannot attach mlx4 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx4_shared_data = mz->addr;\n\t\t\tmemset(&mlx4_local_data, 0, sizeof(mlx4_local_data));\n\t\t}\n\t}\nerror:\n\trte_spinlock_unlock(&mlx4_shared_data_lock);\n\treturn ret;\n}\n\n#ifdef HAVE_IBV_MLX4_BUF_ALLOCATORS\n/**\n * Verbs callback to allocate a memory. This function should allocate the space\n * according to the size provided residing inside a huge page.\n * Please note that all allocation must respect the alignment from libmlx4\n * (i.e. currently sysconf(_SC_PAGESIZE)).\n *\n * @param[in] size\n *   The size in bytes of the memory to allocate.\n * @param[in] data\n *   A pointer to the callback data.\n *\n * @return\n *   Allocated buffer, NULL otherwise and rte_errno is set.\n */\nstatic void *\nmlx4_alloc_verbs_buf(size_t size, void *data)\n{\n\tstruct mlx4_priv *priv = data;\n\tvoid *ret;\n\tsize_t alignment = sysconf(_SC_PAGESIZE);\n\tunsigned int socket = SOCKET_ID_ANY;\n\n\tif (priv->verbs_alloc_ctx.type == MLX4_VERBS_ALLOC_TYPE_TX_QUEUE) {\n\t\tconst struct txq *txq = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = txq->socket;\n\t} else if (priv->verbs_alloc_ctx.type ==\n\t\t   MLX4_VERBS_ALLOC_TYPE_RX_QUEUE) {\n\t\tconst struct rxq *rxq = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = rxq->socket;\n\t}\n\tassert(data != NULL);\n\tret = rte_malloc_socket(__func__, size, alignment, socket);\n\tif (!ret && size)\n\t\trte_errno = ENOMEM;\n\treturn ret;\n}\n\n/**\n * Verbs callback to free a memory.\n *\n * @param[in] ptr\n *   A pointer to the memory to free.\n * @param[in] data\n *   A pointer to the callback data.\n */\nstatic void\nmlx4_free_verbs_buf(void *ptr, void *data __rte_unused)\n{\n\tassert(data != NULL);\n\trte_free(ptr);\n}\n#endif\n\n/**\n * Initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_proc_priv_init(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_proc_priv *ppriv;\n\tsize_t ppriv_size;\n\n\t/*\n\t * UAR register table follows the process private structure. BlueFlame\n\t * registers for Tx queues are stored in the table.\n\t */\n\tppriv_size = sizeof(struct mlx4_proc_priv) +\n\t\t     dev->data->nb_tx_queues * sizeof(void *);\n\tppriv = rte_malloc_socket(\"mlx4_proc_priv\", ppriv_size,\n\t\t\t\t  RTE_CACHE_LINE_SIZE, dev->device->numa_node);\n\tif (!ppriv) {\n\t\trte_errno = ENOMEM;\n\t\treturn -rte_errno;\n\t}\n\tppriv->uar_table_sz = ppriv_size;\n\tdev->process_private = ppriv;\n\treturn 0;\n}\n\n/**\n * Un-initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_proc_priv_uninit(struct rte_eth_dev *dev)\n{\n\tif (!dev->process_private)\n\t\treturn;\n\trte_free(dev->process_private);\n\tdev->process_private = NULL;\n}\n\n/**\n * DPDK callback for Ethernet device configuration.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_dev_configure(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tstruct rte_flow_error error;\n\tint ret;\n\n\t/* Prepare internal flow rules. */\n\tret = mlx4_flow_sync(priv, &error);\n\tif (ret) {\n\t\tERROR(\"cannot set up internal flow rules (code %d, \\\"%s\\\"),\"\n\t\t      \" flow error type %d, cause %p, message: %s\",\n\t\t      -ret, strerror(-ret), error.type, error.cause,\n\t\t      error.message ? error.message : \"(unspecified)\");\n\t\tgoto exit;\n\t}\n\tret = mlx4_intr_install(priv);\n\tif (ret) {\n\t\tERROR(\"%p: interrupt handler installation failed\",\n\t\t      (void *)dev);\n\t\tgoto exit;\n\t}\n\tret = mlx4_proc_priv_init(dev);\n\tif (ret) {\n\t\tERROR(\"%p: process private data allocation failed\",\n\t\t      (void *)dev);\n\t\tgoto exit;\n\t}\nexit:\n\treturn ret;\n}\n\n/**\n * DPDK callback to start the device.\n *\n * Simulate device start by initializing common RSS resources and attaching\n * all configured flows.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_dev_start(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tstruct rte_flow_error error;\n\tint ret;\n\n\tif (priv->started)\n\t\treturn 0;\n\tDEBUG(\"%p: attaching configured flows to all RX queues\", (void *)dev);\n\tpriv->started = 1;\n\tret = mlx4_rss_init(priv);\n\tif (ret) {\n\t\tERROR(\"%p: cannot initialize RSS resources: %s\",\n\t\t      (void *)dev, strerror(-ret));\n\t\tgoto err;\n\t}\n#ifndef NDEBUG\n\tmlx4_mr_dump_dev(dev);\n#endif\n\tret = mlx4_rxq_intr_enable(priv);\n\tif (ret) {\n\t\tERROR(\"%p: interrupt handler installation failed\",\n\t\t     (void *)dev);\n\t\tgoto err;\n\t}\n\tret = mlx4_flow_sync(priv, &error);\n\tif (ret) {\n\t\tERROR(\"%p: cannot attach flow rules (code %d, \\\"%s\\\"),\"\n\t\t      \" flow error type %d, cause %p, message: %s\",\n\t\t      (void *)dev,\n\t\t      -ret, strerror(-ret), error.type, error.cause,\n\t\t      error.message ? error.message : \"(unspecified)\");\n\t\tgoto err;\n\t}\n\trte_wmb();\n\tdev->tx_pkt_burst = mlx4_tx_burst;\n\tdev->rx_pkt_burst = mlx4_rx_burst;\n\t/* Enable datapath on secondary process. */\n\tmlx4_mp_req_start_rxtx(dev);\n\treturn 0;\nerr:\n\tmlx4_dev_stop(dev);\n\treturn ret;\n}\n\n/**\n * DPDK callback to stop the device.\n *\n * Simulate device stop by detaching all configured flows.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_dev_stop(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\n\tif (!priv->started)\n\t\treturn;\n\tDEBUG(\"%p: detaching flows from all RX queues\", (void *)dev);\n\tpriv->started = 0;\n\tdev->tx_pkt_burst = mlx4_tx_burst_removed;\n\tdev->rx_pkt_burst = mlx4_rx_burst_removed;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx4_mp_req_stop_rxtx(dev);\n\tmlx4_flow_sync(priv, NULL);\n\tmlx4_rxq_intr_disable(priv);\n\tmlx4_rss_deinit(priv);\n}\n\n/**\n * DPDK callback to close the device.\n *\n * Destroy all queues and objects, free memory.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx4_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct mlx4_priv *priv = dev->data->dev_private;\n\tunsigned int i;\n\n\tDEBUG(\"%p: closing device \\\"%s\\\"\",\n\t      (void *)dev,\n\t      ((priv->ctx != NULL) ? priv->ctx->device->name : \"\"));\n\tdev->rx_pkt_burst = mlx4_rx_burst_removed;\n\tdev->tx_pkt_burst = mlx4_tx_burst_removed;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx4_mp_req_stop_rxtx(dev);\n\tmlx4_flow_clean(priv);\n\tmlx4_rss_deinit(priv);\n\tfor (i = 0; i != dev->data->nb_rx_queues; ++i)\n\t\tmlx4_rx_queue_release(dev->data->rx_queues[i]);\n\tfor (i = 0; i != dev->data->nb_tx_queues; ++i)\n\t\tmlx4_tx_queue_release(dev->data->tx_queues[i]);\n\tmlx4_proc_priv_uninit(dev);\n\tmlx4_mr_release(dev);\n\tif (priv->pd != NULL) {\n\t\tassert(priv->ctx != NULL);\n\t\tclaim_zero(mlx4_glue->dealloc_pd(priv->pd));\n\t\tclaim_zero(mlx4_glue->close_device(priv->ctx));\n\t} else\n\t\tassert(priv->ctx == NULL);\n\tmlx4_intr_uninstall(priv);\n\tmemset(priv, 0, sizeof(*priv));\n}\n\nstatic const struct eth_dev_ops mlx4_dev_ops = {\n\t.dev_configure = mlx4_dev_configure,\n\t.dev_start = mlx4_dev_start,\n\t.dev_stop = mlx4_dev_stop,\n\t.dev_set_link_down = mlx4_dev_set_link_down,\n\t.dev_set_link_up = mlx4_dev_set_link_up,\n\t.dev_close = mlx4_dev_close,\n\t.link_update = mlx4_link_update,\n\t.promiscuous_enable = mlx4_promiscuous_enable,\n\t.promiscuous_disable = mlx4_promiscuous_disable,\n\t.allmulticast_enable = mlx4_allmulticast_enable,\n\t.allmulticast_disable = mlx4_allmulticast_disable,\n\t.mac_addr_remove = mlx4_mac_addr_remove,\n\t.mac_addr_add = mlx4_mac_addr_add,\n\t.mac_addr_set = mlx4_mac_addr_set,\n\t.set_mc_addr_list = mlx4_set_mc_addr_list,\n\t.stats_get = mlx4_stats_get,\n\t.stats_reset = mlx4_stats_reset,\n\t.fw_version_get = mlx4_fw_version_get,\n\t.dev_infos_get = mlx4_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx4_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx4_vlan_filter_set,\n\t.rx_queue_setup = mlx4_rx_queue_setup,\n\t.tx_queue_setup = mlx4_tx_queue_setup,\n\t.rx_queue_release = mlx4_rx_queue_release,\n\t.tx_queue_release = mlx4_tx_queue_release,\n\t.flow_ctrl_get = mlx4_flow_ctrl_get,\n\t.flow_ctrl_set = mlx4_flow_ctrl_set,\n\t.mtu_set = mlx4_mtu_set,\n\t.filter_ctrl = mlx4_filter_ctrl,\n\t.rx_queue_intr_enable = mlx4_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx4_rx_intr_disable,\n\t.is_removed = mlx4_is_removed,\n};\n\n/* Available operations from secondary process. */\nstatic const struct eth_dev_ops mlx4_dev_sec_ops = {\n\t.stats_get = mlx4_stats_get,\n\t.stats_reset = mlx4_stats_reset,\n\t.fw_version_get = mlx4_fw_version_get,\n\t.dev_infos_get = mlx4_dev_infos_get,\n};\n\n/**\n * Get PCI information from struct ibv_device.\n *\n * @param device\n *   Pointer to Ethernet device structure.\n * @param[out] pci_addr\n *   PCI bus address output buffer.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_ibv_device_to_pci_addr(const struct ibv_device *device,\n\t\t\t    struct rte_pci_addr *pci_addr)\n{\n\tFILE *file;\n\tchar line[32];\n\tMKSTR(path, \"%s/device/uevent\", device->ibdev_path);\n\n\tfile = fopen(path, \"rb\");\n\tif (file == NULL) {\n\t\trte_errno = errno;\n\t\treturn -rte_errno;\n\t}\n\twhile (fgets(line, sizeof(line), file) == line) {\n\t\tsize_t len = strlen(line);\n\t\tint ret;\n\n\t\t/* Truncate long lines. */\n\t\tif (len == (sizeof(line) - 1))\n\t\t\twhile (line[(len - 1)] != '\\n') {\n\t\t\t\tret = fgetc(file);\n\t\t\t\tif (ret == EOF)\n\t\t\t\t\tbreak;\n\t\t\t\tline[(len - 1)] = ret;\n\t\t\t}\n\t\t/* Extract information. */\n\t\tif (sscanf(line,\n\t\t\t   \"PCI_SLOT_NAME=\"\n\t\t\t   \"%\" SCNx32 \":%\" SCNx8 \":%\" SCNx8 \".%\" SCNx8 \"\\n\",\n\t\t\t   &pci_addr->domain,\n\t\t\t   &pci_addr->bus,\n\t\t\t   &pci_addr->devid,\n\t\t\t   &pci_addr->function) == 4) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfclose(file);\n\treturn 0;\n}\n\n/**\n * Verify and store value for device argument.\n *\n * @param[in] key\n *   Key argument to verify.\n * @param[in] val\n *   Value associated with key.\n * @param[in, out] conf\n *   Shared configuration data.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_arg_parse(const char *key, const char *val, struct mlx4_conf *conf)\n{\n\tunsigned long tmp;\n\n\terrno = 0;\n\ttmp = strtoul(val, NULL, 0);\n\tif (errno) {\n\t\trte_errno = errno;\n\t\tWARN(\"%s: \\\"%s\\\" is not a valid integer\", key, val);\n\t\treturn -rte_errno;\n\t}\n\tif (strcmp(MLX4_PMD_PORT_KVARG, key) == 0) {\n\t\tuint32_t ports = rte_log2_u32(conf->ports.present + 1);\n\n\t\tif (tmp >= ports) {\n\t\t\tERROR(\"port index %lu outside range [0,%\" PRIu32 \")\",\n\t\t\t      tmp, ports);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(conf->ports.present & (1 << tmp))) {\n\t\t\trte_errno = EINVAL;\n\t\t\tERROR(\"invalid port index %lu\", tmp);\n\t\t\treturn -rte_errno;\n\t\t}\n\t\tconf->ports.enabled |= 1 << tmp;\n\t} else if (strcmp(MLX4_MR_EXT_MEMSEG_EN_KVARG, key) == 0) {\n\t\tconf->mr_ext_memseg_en = !!tmp;\n\t} else {\n\t\trte_errno = EINVAL;\n\t\tWARN(\"%s: unknown parameter\", key);\n\t\treturn -rte_errno;\n\t}\n\treturn 0;\n}\n\n/**\n * Parse device parameters.\n *\n * @param devargs\n *   Device arguments structure.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_args(struct rte_devargs *devargs, struct mlx4_conf *conf)\n{\n\tstruct rte_kvargs *kvlist;\n\tunsigned int arg_count;\n\tint ret = 0;\n\tint i;\n\n\tif (devargs == NULL)\n\t\treturn 0;\n\tkvlist = rte_kvargs_parse(devargs->args, pmd_mlx4_init_params);\n\tif (kvlist == NULL) {\n\t\trte_errno = EINVAL;\n\t\tERROR(\"failed to parse kvargs\");\n\t\treturn -rte_errno;\n\t}\n\t/* Process parameters. */\n\tfor (i = 0; pmd_mlx4_init_params[i]; ++i) {\n\t\targ_count = rte_kvargs_count(kvlist, pmd_mlx4_init_params[i]);\n\t\twhile (arg_count-- > 0) {\n\t\t\tret = rte_kvargs_process(kvlist,\n\t\t\t\t\t\t pmd_mlx4_init_params[i],\n\t\t\t\t\t\t (int (*)(const char *,\n\t\t\t\t\t\t\t  const char *,\n\t\t\t\t\t\t\t  void *))\n\t\t\t\t\t\t mlx4_arg_parse,\n\t\t\t\t\t\t conf);\n\t\t\tif (ret != 0)\n\t\t\t\tgoto free_kvlist;\n\t\t}\n\t}\nfree_kvlist:\n\trte_kvargs_free(kvlist);\n\treturn ret;\n}\n\n/**\n * Interpret RSS capabilities reported by device.\n *\n * This function returns the set of usable Verbs RSS hash fields, kernel\n * quirks taken into account.\n *\n * @param ctx\n *   Verbs context.\n * @param pd\n *   Verbs protection domain.\n * @param device_attr_ex\n *   Extended device attributes to interpret.\n *\n * @return\n *   Usable RSS hash fields mask in Verbs format.\n */\nstatic uint64_t\nmlx4_hw_rss_sup(struct ibv_context *ctx, struct ibv_pd *pd,\n\t\tstruct ibv_device_attr_ex *device_attr_ex)\n{\n\tuint64_t hw_rss_sup = device_attr_ex->rss_caps.rx_hash_fields_mask;\n\tstruct ibv_cq *cq = NULL;\n\tstruct ibv_wq *wq = NULL;\n\tstruct ibv_rwq_ind_table *ind = NULL;\n\tstruct ibv_qp *qp = NULL;\n\n\tif (!hw_rss_sup) {\n\t\tWARN(\"no RSS capabilities reported; disabling support for UDP\"\n\t\t     \" RSS and inner VXLAN RSS\");\n\t\treturn IBV_RX_HASH_SRC_IPV4 | IBV_RX_HASH_DST_IPV4 |\n\t\t\tIBV_RX_HASH_SRC_IPV6 | IBV_RX_HASH_DST_IPV6 |\n\t\t\tIBV_RX_HASH_SRC_PORT_TCP | IBV_RX_HASH_DST_PORT_TCP;\n\t}\n\tif (!(hw_rss_sup & IBV_RX_HASH_INNER))\n\t\treturn hw_rss_sup;\n\t/*\n\t * Although reported as supported, missing code in some Linux\n\t * versions (v4.15, v4.16) prevents the creation of hash QPs with\n\t * inner capability.\n\t *\n\t * There is no choice but to attempt to instantiate a temporary RSS\n\t * context in order to confirm its support.\n\t */\n\tcq = mlx4_glue->create_cq(ctx, 1, NULL, NULL, 0);\n\twq = cq ? mlx4_glue->create_wq\n\t\t(ctx,\n\t\t &(struct ibv_wq_init_attr){\n\t\t\t.wq_type = IBV_WQT_RQ,\n\t\t\t.max_wr = 1,\n\t\t\t.max_sge = 1,\n\t\t\t.pd = pd,\n\t\t\t.cq = cq,\n\t\t }) : NULL;\n\tind = wq ? mlx4_glue->create_rwq_ind_table\n\t\t(ctx,\n\t\t &(struct ibv_rwq_ind_table_init_attr){\n\t\t\t.log_ind_tbl_size = 0,\n\t\t\t.ind_tbl = &wq,\n\t\t\t.comp_mask = 0,\n\t\t }) : NULL;\n\tqp = ind ? mlx4_glue->create_qp_ex\n\t\t(ctx,\n\t\t &(struct ibv_qp_init_attr_ex){\n\t\t\t.comp_mask =\n\t\t\t\t(IBV_QP_INIT_ATTR_PD |\n\t\t\t\t IBV_QP_INIT_ATTR_RX_HASH |\n\t\t\t\t IBV_QP_INIT_ATTR_IND_TABLE),\n\t\t\t.qp_type = IBV_QPT_RAW_PACKET,\n\t\t\t.pd = pd,\n\t\t\t.rwq_ind_tbl = ind,\n\t\t\t.rx_hash_conf = {\n\t\t\t\t.rx_hash_function = IBV_RX_HASH_FUNC_TOEPLITZ,\n\t\t\t\t.rx_hash_key_len = MLX4_RSS_HASH_KEY_SIZE,\n\t\t\t\t.rx_hash_key = mlx4_rss_hash_key_default,\n\t\t\t\t.rx_hash_fields_mask = hw_rss_sup,\n\t\t\t},\n\t\t }) : NULL;\n\tif (!qp) {\n\t\tWARN(\"disabling unusable inner RSS capability due to kernel\"\n\t\t     \" quirk\");\n\t\thw_rss_sup &= ~IBV_RX_HASH_INNER;\n\t} else {\n\t\tclaim_zero(mlx4_glue->destroy_qp(qp));\n\t}\n\tif (ind)\n\t\tclaim_zero(mlx4_glue->destroy_rwq_ind_table(ind));\n\tif (wq)\n\t\tclaim_zero(mlx4_glue->destroy_wq(wq));\n\tif (cq)\n\t\tclaim_zero(mlx4_glue->destroy_cq(cq));\n\treturn hw_rss_sup;\n}\n\nstatic struct rte_pci_driver mlx4_driver;\n\n/**\n * PMD global initialization.\n *\n * Independent from individual device, this function initializes global\n * per-PMD data structures distinguishing primary and secondary processes.\n * Hence, each initialization is called once per a process.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_init_once(void)\n{\n\tstruct mlx4_shared_data *sd;\n\tstruct mlx4_local_data *ld = &mlx4_local_data;\n\n\tif (mlx4_init_shared_data())\n\t\treturn -rte_errno;\n\tsd = mlx4_shared_data;\n\tassert(sd);\n\trte_spinlock_lock(&sd->lock);\n\tswitch (rte_eal_process_type()) {\n\tcase RTE_PROC_PRIMARY:\n\t\tif (sd->init_done)\n\t\t\tbreak;\n\t\tLIST_INIT(&sd->mem_event_cb_list);\n\t\trte_rwlock_init(&sd->mem_event_rwlock);\n\t\trte_mem_event_callback_register(\"MLX4_MEM_EVENT_CB\",\n\t\t\t\t\t\tmlx4_mr_mem_event_cb, NULL);\n\t\tmlx4_mp_init_primary();\n\t\tsd->init_done = true;\n\t\tbreak;\n\tcase RTE_PROC_SECONDARY:\n\t\tif (ld->init_done)\n\t\t\tbreak;\n\t\tmlx4_mp_init_secondary();\n\t\t++sd->secondary_cnt;\n\t\tld->init_done = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\trte_spinlock_unlock(&sd->lock);\n\treturn 0;\n}\n\n/**\n * DPDK callback to register a PCI device.\n *\n * This function creates an Ethernet device for each port of a given\n * PCI device.\n *\n * @param[in] pci_drv\n *   PCI driver structure (mlx4_driver).\n * @param[in] pci_dev\n *   PCI device information.\n *\n * @return\n *   0 on success, negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx4_pci_probe(struct rte_pci_driver *pci_drv, struct rte_pci_device *pci_dev)\n{\n\tstruct ibv_device **list;\n\tstruct ibv_device *ibv_dev;\n\tint err = 0;\n\tstruct ibv_context *attr_ctx = NULL;\n\tstruct ibv_device_attr device_attr;\n\tstruct ibv_device_attr_ex device_attr_ex;\n\tstruct mlx4_conf conf = {\n\t\t.ports.present = 0,\n\t\t.mr_ext_memseg_en = 1,\n\t};\n\tunsigned int vf;\n\tint i;\n\n\t(void)pci_drv;\n\terr = mlx4_init_once();\n\tif (err) {\n\t\tERROR(\"unable to init PMD global data: %s\",\n\t\t      strerror(rte_errno));\n\t\treturn -rte_errno;\n\t}\n\tassert(pci_drv == &mlx4_driver);\n\tlist = mlx4_glue->get_device_list(&i);\n\tif (list == NULL) {\n\t\trte_errno = errno;\n\t\tassert(rte_errno);\n\t\tif (rte_errno == ENOSYS)\n\t\t\tERROR(\"cannot list devices, is ib_uverbs loaded?\");\n\t\treturn -rte_errno;\n\t}\n\tassert(i >= 0);\n\t/*\n\t * For each listed device, check related sysfs entry against\n\t * the provided PCI ID.\n\t */\n\twhile (i != 0) {\n\t\tstruct rte_pci_addr pci_addr;\n\n\t\t--i;\n\t\tDEBUG(\"checking device \\\"%s\\\"\", list[i]->name);\n\t\tif (mlx4_ibv_device_to_pci_addr(list[i], &pci_addr))\n\t\t\tcontinue;\n\t\tif ((pci_dev->addr.domain != pci_addr.domain) ||\n\t\t    (pci_dev->addr.bus != pci_addr.bus) ||\n\t\t    (pci_dev->addr.devid != pci_addr.devid) ||\n\t\t    (pci_dev->addr.function != pci_addr.function))\n\t\t\tcontinue;\n\t\tvf = (pci_dev->id.device_id ==\n\t\t      PCI_DEVICE_ID_MELLANOX_CONNECTX3VF);\n\t\tINFO(\"PCI information matches, using device \\\"%s\\\" (VF: %s)\",\n\t\t     list[i]->name, (vf ? \"true\" : \"false\"));\n\t\tattr_ctx = mlx4_glue->open_device(list[i]);\n\t\terr = errno;\n\t\tbreak;\n\t}\n\tif (attr_ctx == NULL) {\n\t\tmlx4_glue->free_device_list(list);\n\t\tswitch (err) {\n\t\tcase 0:\n\t\t\trte_errno = ENODEV;\n\t\t\tERROR(\"cannot access device, is mlx4_ib loaded?\");\n\t\t\treturn -rte_errno;\n\t\tcase EINVAL:\n\t\t\trte_errno = EINVAL;\n\t\t\tERROR(\"cannot use device, are drivers up to date?\");\n\t\t\treturn -rte_errno;\n\t\t}\n\t\tassert(err > 0);\n\t\trte_errno = err;\n\t\treturn -rte_errno;\n\t}\n\tibv_dev = list[i];\n\tDEBUG(\"device opened\");\n\tif (mlx4_glue->query_device(attr_ctx, &device_attr)) {\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tINFO(\"%u port(s) detected\", device_attr.phys_port_cnt);\n\tconf.ports.present |= (UINT64_C(1) << device_attr.phys_port_cnt) - 1;\n\tif (mlx4_args(pci_dev->device.devargs, &conf)) {\n\t\tERROR(\"failed to process device arguments\");\n\t\terr = EINVAL;\n\t\tgoto error;\n\t}\n\t/* Use all ports when none are defined */\n\tif (!conf.ports.enabled)\n\t\tconf.ports.enabled = conf.ports.present;\n\t/* Retrieve extended device attributes. */\n\tif (mlx4_glue->query_device_ex(attr_ctx, NULL, &device_attr_ex)) {\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tassert(device_attr.max_sge >= MLX4_MAX_SGE);\n\tfor (i = 0; i < device_attr.phys_port_cnt; i++) {\n\t\tuint32_t port = i + 1; /* ports are indexed from one */\n\t\tstruct ibv_context *ctx = NULL;\n\t\tstruct ibv_port_attr port_attr;\n\t\tstruct ibv_pd *pd = NULL;\n\t\tstruct mlx4_priv *priv = NULL;\n\t\tstruct rte_eth_dev *eth_dev = NULL;\n\t\tstruct ether_addr mac;\n\t\tchar name[RTE_ETH_NAME_MAX_LEN];\n\n\t\t/* If port is not enabled, skip. */\n\t\tif (!(conf.ports.enabled & (1 << i)))\n\t\t\tcontinue;\n\t\tDEBUG(\"using port %u\", port);\n\t\tctx = mlx4_glue->open_device(ibv_dev);\n\t\tif (ctx == NULL) {\n\t\t\terr = ENODEV;\n\t\t\tgoto port_error;\n\t\t}\n\t\tsnprintf(name, sizeof(name), \"%s port %u\",\n\t\t\t mlx4_glue->get_device_name(ibv_dev), port);\n\t\tif (rte_eal_process_type() == RTE_PROC_SECONDARY) {\n\t\t\teth_dev = rte_eth_dev_attach_secondary(name);\n\t\t\tif (eth_dev == NULL) {\n\t\t\t\tERROR(\"can not attach rte ethdev\");\n\t\t\t\trte_errno = ENOMEM;\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tpriv = eth_dev->data->dev_private;\n\t\t\tif (!priv->verbs_alloc_ctx.enabled) {\n\t\t\t\tERROR(\"secondary process is not supported\"\n\t\t\t\t      \" due to lack of external allocator\"\n\t\t\t\t      \" from Verbs\");\n\t\t\t\trte_errno = ENOTSUP;\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\teth_dev->device = &pci_dev->device;\n\t\t\teth_dev->dev_ops = &mlx4_dev_sec_ops;\n\t\t\terr = mlx4_proc_priv_init(eth_dev);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t\t/* Receive command fd from primary process. */\n\t\t\terr = mlx4_mp_req_verbs_cmd_fd(eth_dev);\n\t\t\tif (err < 0) {\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/* Remap UAR for Tx queues. */\n\t\t\terr = mlx4_tx_uar_init_secondary(eth_dev, err);\n\t\t\tif (err) {\n\t\t\t\terr = rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Ethdev pointer is still required as input since\n\t\t\t * the primary device is not accessible from the\n\t\t\t * secondary process.\n\t\t\t */\n\t\t\teth_dev->tx_pkt_burst = mlx4_tx_burst;\n\t\t\teth_dev->rx_pkt_burst = mlx4_rx_burst;\n\t\t\tclaim_zero(mlx4_glue->close_device(ctx));\n\t\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\t\t\trte_eth_dev_probing_finish(eth_dev);\n\t\t\tcontinue;\n\t\t}\n\t\t/* Check port status. */\n\t\terr = mlx4_glue->query_port(ctx, port, &port_attr);\n\t\tif (err) {\n\t\t\terr = ENODEV;\n\t\t\tERROR(\"port query failed: %s\", strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\tif (port_attr.link_layer != IBV_LINK_LAYER_ETHERNET) {\n\t\t\terr = ENOTSUP;\n\t\t\tERROR(\"port %d is not configured in Ethernet mode\",\n\t\t\t      port);\n\t\t\tgoto port_error;\n\t\t}\n\t\tif (port_attr.state != IBV_PORT_ACTIVE)\n\t\t\tDEBUG(\"port %d is not active: \\\"%s\\\" (%d)\",\n\t\t\t      port, mlx4_glue->port_state_str(port_attr.state),\n\t\t\t      port_attr.state);\n\t\t/* Make asynchronous FD non-blocking to handle interrupts. */\n\t\terr = mlx4_fd_set_non_blocking(ctx->async_fd);\n\t\tif (err) {\n\t\t\tERROR(\"cannot make asynchronous FD non-blocking: %s\",\n\t\t\t      strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* Allocate protection domain. */\n\t\tpd = mlx4_glue->alloc_pd(ctx);\n\t\tif (pd == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"PD allocation failure\");\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* from rte_ethdev.c */\n\t\tpriv = rte_zmalloc(\"ethdev private structure\",\n\t\t\t\t   sizeof(*priv),\n\t\t\t\t   RTE_CACHE_LINE_SIZE);\n\t\tif (priv == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"priv allocation failure\");\n\t\t\tgoto port_error;\n\t\t}\n\t\tpriv->ctx = ctx;\n\t\tpriv->device_attr = device_attr;\n\t\tpriv->port = port;\n\t\tpriv->pd = pd;\n\t\tpriv->mtu = ETHER_MTU;\n\t\tpriv->vf = vf;\n\t\tpriv->hw_csum =\t!!(device_attr.device_cap_flags &\n\t\t\t\t   IBV_DEVICE_RAW_IP_CSUM);\n\t\tDEBUG(\"checksum offloading is %ssupported\",\n\t\t      (priv->hw_csum ? \"\" : \"not \"));\n\t\t/* Only ConnectX-3 Pro supports tunneling. */\n\t\tpriv->hw_csum_l2tun =\n\t\t\tpriv->hw_csum &&\n\t\t\t(device_attr.vendor_part_id ==\n\t\t\t PCI_DEVICE_ID_MELLANOX_CONNECTX3PRO);\n\t\tDEBUG(\"L2 tunnel checksum offloads are %ssupported\",\n\t\t      priv->hw_csum_l2tun ? \"\" : \"not \");\n\t\tpriv->hw_rss_sup = mlx4_hw_rss_sup(priv->ctx, priv->pd,\n\t\t\t\t\t\t   &device_attr_ex);\n\t\tDEBUG(\"supported RSS hash fields mask: %016\" PRIx64,\n\t\t      priv->hw_rss_sup);\n\t\tpriv->hw_rss_max_qps =\n\t\t\tdevice_attr_ex.rss_caps.max_rwq_indirection_table_size;\n\t\tDEBUG(\"MAX RSS queues %d\", priv->hw_rss_max_qps);\n\t\tpriv->hw_fcs_strip = !!(device_attr_ex.raw_packet_caps &\n\t\t\t\t\tIBV_RAW_PACKET_CAP_SCATTER_FCS);\n\t\tDEBUG(\"FCS stripping toggling is %ssupported\",\n\t\t      priv->hw_fcs_strip ? \"\" : \"not \");\n\t\tpriv->tso =\n\t\t\t((device_attr_ex.tso_caps.max_tso > 0) &&\n\t\t\t (device_attr_ex.tso_caps.supported_qpts &\n\t\t\t  (1 << IBV_QPT_RAW_PACKET)));\n\t\tif (priv->tso)\n\t\t\tpriv->tso_max_payload_sz =\n\t\t\t\t\tdevice_attr_ex.tso_caps.max_tso;\n\t\tDEBUG(\"TSO is %ssupported\",\n\t\t      priv->tso ? \"\" : \"not \");\n\t\tpriv->mr_ext_memseg_en = conf.mr_ext_memseg_en;\n\t\t/* Configure the first MAC address by default. */\n\t\terr = mlx4_get_mac(priv, &mac.addr_bytes);\n\t\tif (err) {\n\t\t\tERROR(\"cannot get MAC address, is mlx4_en loaded?\"\n\t\t\t      \" (error: %s)\", strerror(err));\n\t\t\tgoto port_error;\n\t\t}\n\t\tINFO(\"port %u MAC address is %02x:%02x:%02x:%02x:%02x:%02x\",\n\t\t     priv->port,\n\t\t     mac.addr_bytes[0], mac.addr_bytes[1],\n\t\t     mac.addr_bytes[2], mac.addr_bytes[3],\n\t\t     mac.addr_bytes[4], mac.addr_bytes[5]);\n\t\t/* Register MAC address. */\n\t\tpriv->mac[0] = mac;\n#ifndef NDEBUG\n\t\t{\n\t\t\tchar ifname[IF_NAMESIZE];\n\n\t\t\tif (mlx4_get_ifname(priv, &ifname) == 0)\n\t\t\t\tDEBUG(\"port %u ifname is \\\"%s\\\"\",\n\t\t\t\t      priv->port, ifname);\n\t\t\telse\n\t\t\t\tDEBUG(\"port %u ifname is unknown\", priv->port);\n\t\t}\n#endif\n\t\t/* Get actual MTU if possible. */\n\t\tmlx4_mtu_get(priv, &priv->mtu);\n\t\tDEBUG(\"port %u MTU is %u\", priv->port, priv->mtu);\n\t\teth_dev = rte_eth_dev_allocate(name);\n\t\tif (eth_dev == NULL) {\n\t\t\terr = ENOMEM;\n\t\t\tERROR(\"can not allocate rte ethdev\");\n\t\t\tgoto port_error;\n\t\t}\n\t\teth_dev->data->dev_private = priv;\n\t\teth_dev->data->mac_addrs = priv->mac;\n\t\teth_dev->device = &pci_dev->device;\n\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\t\t/* Initialize local interrupt handle for current port. */\n\t\tpriv->intr_handle = (struct rte_intr_handle){\n\t\t\t.fd = -1,\n\t\t\t.type = RTE_INTR_HANDLE_EXT,\n\t\t};\n\t\t/*\n\t\t * Override ethdev interrupt handle pointer with private\n\t\t * handle instead of that of the parent PCI device used by\n\t\t * default. This prevents it from being shared between all\n\t\t * ports of the same PCI device since each of them is\n\t\t * associated its own Verbs context.\n\t\t *\n\t\t * Rx interrupts in particular require this as the PMD has\n\t\t * no control over the registration of queue interrupts\n\t\t * besides setting up eth_dev->intr_handle, the rest is\n\t\t * handled by rte_intr_rx_ctl().\n\t\t */\n\t\teth_dev->intr_handle = &priv->intr_handle;\n\t\tpriv->dev_data = eth_dev->data;\n\t\teth_dev->dev_ops = &mlx4_dev_ops;\n#ifdef HAVE_IBV_MLX4_BUF_ALLOCATORS\n\t\t/* Hint libmlx4 to use PMD allocator for data plane resources */\n\t\tstruct mlx4dv_ctx_allocators alctr = {\n\t\t\t.alloc = &mlx4_alloc_verbs_buf,\n\t\t\t.free = &mlx4_free_verbs_buf,\n\t\t\t.data = priv,\n\t\t};\n\t\terr = mlx4_glue->dv_set_context_attr\n\t\t\t(ctx, MLX4DV_SET_CTX_ATTR_BUF_ALLOCATORS,\n\t\t\t (void *)((uintptr_t)&alctr));\n\t\tif (err)\n\t\t\tWARN(\"Verbs external allocator is not supported\");\n\t\telse\n\t\t\tpriv->verbs_alloc_ctx.enabled = 1;\n#endif\n\t\t/* Bring Ethernet device up. */\n\t\tDEBUG(\"forcing Ethernet interface up\");\n\t\tmlx4_dev_set_link_up(eth_dev);\n\t\t/* Update link status once if waiting for LSC. */\n\t\tif (eth_dev->data->dev_flags & RTE_ETH_DEV_INTR_LSC)\n\t\t\tmlx4_link_update(eth_dev, 0);\n\t\t/*\n\t\t * Once the device is added to the list of memory event\n\t\t * callback, its global MR cache table cannot be expanded\n\t\t * on the fly because of deadlock. If it overflows, lookup\n\t\t * should be done by searching MR list linearly, which is slow.\n\t\t */\n\t\terr = mlx4_mr_btree_init(&priv->mr.cache,\n\t\t\t\t\t MLX4_MR_BTREE_CACHE_N * 2,\n\t\t\t\t\t eth_dev->device->numa_node);\n\t\tif (err) {\n\t\t\t/* rte_errno is already set. */\n\t\t\tgoto port_error;\n\t\t}\n\t\t/* Add device to memory callback list. */\n\t\trte_rwlock_write_lock(&mlx4_shared_data->mem_event_rwlock);\n\t\tLIST_INSERT_HEAD(&mlx4_shared_data->mem_event_cb_list,\n\t\t\t\t priv, mem_event_cb);\n\t\trte_rwlock_write_unlock(&mlx4_shared_data->mem_event_rwlock);\n\t\trte_eth_dev_probing_finish(eth_dev);\n\t\tcontinue;\nport_error:\n\t\trte_free(priv);\n\t\tif (eth_dev != NULL)\n\t\t\teth_dev->data->dev_private = NULL;\n\t\tif (pd)\n\t\t\tclaim_zero(mlx4_glue->dealloc_pd(pd));\n\t\tif (ctx)\n\t\t\tclaim_zero(mlx4_glue->close_device(ctx));\n\t\tif (eth_dev != NULL) {\n\t\t\t/* mac_addrs must not be freed because part of dev_private */\n\t\t\teth_dev->data->mac_addrs = NULL;\n\t\t\trte_eth_dev_release_port(eth_dev);\n\t\t}\n\t\tbreak;\n\t}\n\t/*\n\t * XXX if something went wrong in the loop above, there is a resource\n\t * leak (ctx, pd, priv, dpdk ethdev) but we can do nothing about it as\n\t * long as the dpdk does not provide a way to deallocate a ethdev and a\n\t * way to enumerate the registered ethdevs to free the previous ones.\n\t */\nerror:\n\tif (attr_ctx)\n\t\tclaim_zero(mlx4_glue->close_device(attr_ctx));\n\tif (list)\n\t\tmlx4_glue->free_device_list(list);\n\tif (err)\n\t\trte_errno = err;\n\treturn -err;\n}\n\nstatic const struct rte_pci_id mlx4_pci_id_map[] = {\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3PRO)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX3VF)\n\t},\n\t{\n\t\t.vendor_id = 0\n\t}\n};\n\nstatic struct rte_pci_driver mlx4_driver = {\n\t.driver = {\n\t\t.name = MLX4_DRIVER_NAME\n\t},\n\t.id_table = mlx4_pci_id_map,\n\t.probe = mlx4_pci_probe,\n\t.drv_flags = RTE_PCI_DRV_INTR_LSC |\n\t\t     RTE_PCI_DRV_INTR_RMV,\n};\n\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\n/**\n * Suffix RTE_EAL_PMD_PATH with \"-glue\".\n *\n * This function performs a sanity check on RTE_EAL_PMD_PATH before\n * suffixing its last component.\n *\n * @param buf[out]\n *   Output buffer, should be large enough otherwise NULL is returned.\n * @param size\n *   Size of @p out.\n *\n * @return\n *   Pointer to @p buf or @p NULL in case suffix cannot be appended.\n */\nstatic char *\nmlx4_glue_path(char *buf, size_t size)\n{\n\tstatic const char *const bad[] = { \"/\", \".\", \"..\", NULL };\n\tconst char *path = RTE_EAL_PMD_PATH;\n\tsize_t len = strlen(path);\n\tsize_t off;\n\tint i;\n\n\twhile (len && path[len - 1] == '/')\n\t\t--len;\n\tfor (off = len; off && path[off - 1] != '/'; --off)\n\t\t;\n\tfor (i = 0; bad[i]; ++i)\n\t\tif (!strncmp(path + off, bad[i], (int)(len - off)))\n\t\t\tgoto error;\n\ti = snprintf(buf, size, \"%.*s-glue\", (int)len, path);\n\tif (i == -1 || (size_t)i >= size)\n\t\tgoto error;\n\treturn buf;\nerror:\n\tERROR(\"unable to append \\\"-glue\\\" to last component of\"\n\t      \" RTE_EAL_PMD_PATH (\\\"\" RTE_EAL_PMD_PATH \"\\\"),\"\n\t      \" please re-configure DPDK\");\n\treturn NULL;\n}\n\n/**\n * Initialization routine for run-time dependency on rdma-core.\n */\nstatic int\nmlx4_glue_init(void)\n{\n\tchar glue_path[sizeof(RTE_EAL_PMD_PATH) - 1 + sizeof(\"-glue\")];\n\tconst char *path[] = {\n\t\t/*\n\t\t * A basic security check is necessary before trusting\n\t\t * MLX4_GLUE_PATH, which may override RTE_EAL_PMD_PATH.\n\t\t */\n\t\t(geteuid() == getuid() && getegid() == getgid() ?\n\t\t getenv(\"MLX4_GLUE_PATH\") : NULL),\n\t\t/*\n\t\t * When RTE_EAL_PMD_PATH is set, use its glue-suffixed\n\t\t * variant, otherwise let dlopen() look up libraries on its\n\t\t * own.\n\t\t */\n\t\t(*RTE_EAL_PMD_PATH ?\n\t\t mlx4_glue_path(glue_path, sizeof(glue_path)) : \"\"),\n\t};\n\tunsigned int i = 0;\n\tvoid *handle = NULL;\n\tvoid **sym;\n\tconst char *dlmsg;\n\n\twhile (!handle && i != RTE_DIM(path)) {\n\t\tconst char *end;\n\t\tsize_t len;\n\t\tint ret;\n\n\t\tif (!path[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\tend = strpbrk(path[i], \":;\");\n\t\tif (!end)\n\t\t\tend = path[i] + strlen(path[i]);\n\t\tlen = end - path[i];\n\t\tret = 0;\n\t\tdo {\n\t\t\tchar name[ret + 1];\n\n\t\t\tret = snprintf(name, sizeof(name), \"%.*s%s\" MLX4_GLUE,\n\t\t\t\t       (int)len, path[i],\n\t\t\t\t       (!len || *(end - 1) == '/') ? \"\" : \"/\");\n\t\t\tif (ret == -1)\n\t\t\t\tbreak;\n\t\t\tif (sizeof(name) != (size_t)ret + 1)\n\t\t\t\tcontinue;\n\t\t\tDEBUG(\"looking for rdma-core glue as \\\"%s\\\"\", name);\n\t\t\thandle = dlopen(name, RTLD_LAZY);\n\t\t\tbreak;\n\t\t} while (1);\n\t\tpath[i] = end + 1;\n\t\tif (!*end)\n\t\t\t++i;\n\t}\n\tif (!handle) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tWARN(\"cannot load glue library: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tsym = dlsym(handle, \"mlx4_glue\");\n\tif (!sym || !*sym) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tERROR(\"cannot resolve glue symbol: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tmlx4_glue = *sym;\n\treturn 0;\nglue_error:\n\tif (handle)\n\t\tdlclose(handle);\n\tWARN(\"cannot initialize PMD due to missing run-time\"\n\t     \" dependency on rdma-core libraries (libibverbs,\"\n\t     \" libmlx4)\");\n\treturn -rte_errno;\n}\n\n#endif\n\n/**\n * Driver initialization routine.\n */\nRTE_INIT(rte_mlx4_pmd_init)\n{\n\t/*\n\t * MLX4_DEVICE_FATAL_CLEANUP tells ibv_destroy functions we\n\t * want to get success errno value in case of calling them\n\t * when the device was removed.\n\t */\n\tsetenv(\"MLX4_DEVICE_FATAL_CLEANUP\", \"1\", 1);\n\t/*\n\t * RDMAV_HUGEPAGES_SAFE tells ibv_fork_init() we intend to use\n\t * huge pages. Calling ibv_fork_init() during init allows\n\t * applications to use fork() safely for purposes other than\n\t * using this PMD, which is not supported in forked processes.\n\t */\n\tsetenv(\"RDMAV_HUGEPAGES_SAFE\", \"1\", 1);\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\tif (mlx4_glue_init())\n\t\treturn;\n\tassert(mlx4_glue);\n#endif\n#ifndef NDEBUG\n\t/* Glue structure must not contain any NULL pointers. */\n\t{\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i != sizeof(*mlx4_glue) / sizeof(void *); ++i)\n\t\t\tassert(((const void *const *)mlx4_glue)[i]);\n\t}\n#endif\n\tif (strcmp(mlx4_glue->version, MLX4_GLUE_VERSION)) {\n\t\tERROR(\"rdma-core glue \\\"%s\\\" mismatch: \\\"%s\\\" is required\",\n\t\t      mlx4_glue->version, MLX4_GLUE_VERSION);\n\t\treturn;\n\t}\n\tmlx4_glue->fork_init();\n\trte_pci_register(&mlx4_driver);\n}\n\nRTE_PMD_EXPORT_NAME(net_mlx4, __COUNTER__);\nRTE_PMD_REGISTER_PCI_TABLE(net_mlx4, mlx4_pci_id_map);\nRTE_PMD_REGISTER_KMOD_DEP(net_mlx4,\n\t\"* ib_uverbs & mlx4_en & mlx4_core & mlx4_ib\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/ark/ark_ethdev.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright (c) 2015-2018 Atomic Rules LLC\n */\n\n#include <unistd.h>\n#include <sys/stat.h>\n#include <dlfcn.h>\n\n#include <rte_bus_pci.h>\n#include <rte_ethdev_pci.h>\n#include <rte_kvargs.h>\n\n#include \"ark_global.h\"\n#include \"ark_logs.h\"\n#include \"ark_ethdev_tx.h\"\n#include \"ark_ethdev_rx.h\"\n#include \"ark_mpu.h\"\n#include \"ark_ddm.h\"\n#include \"ark_udm.h\"\n#include \"ark_rqp.h\"\n#include \"ark_pktdir.h\"\n#include \"ark_pktgen.h\"\n#include \"ark_pktchkr.h\"\n\n/*  Internal prototypes */\nstatic int eth_ark_check_args(struct ark_adapter *ark, const char *params);\nstatic int eth_ark_dev_init(struct rte_eth_dev *dev);\nstatic int ark_config_device(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_uninit(struct rte_eth_dev *eth_dev);\nstatic int eth_ark_dev_configure(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_start(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_stop(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_close(struct rte_eth_dev *dev);\nstatic void eth_ark_dev_info_get(struct rte_eth_dev *dev,\n\t\t\t\t struct rte_eth_dev_info *dev_info);\nstatic int eth_ark_dev_link_update(struct rte_eth_dev *dev,\n\t\t\t\t   int wait_to_complete);\nstatic int eth_ark_dev_set_link_up(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_set_link_down(struct rte_eth_dev *dev);\nstatic int eth_ark_dev_stats_get(struct rte_eth_dev *dev,\n\t\t\t\t  struct rte_eth_stats *stats);\nstatic void eth_ark_dev_stats_reset(struct rte_eth_dev *dev);\nstatic int eth_ark_set_default_mac_addr(struct rte_eth_dev *dev,\n\t\t\t\t\t struct ether_addr *mac_addr);\nstatic int eth_ark_macaddr_add(struct rte_eth_dev *dev,\n\t\t\t       struct ether_addr *mac_addr,\n\t\t\t       uint32_t index,\n\t\t\t       uint32_t pool);\nstatic void eth_ark_macaddr_remove(struct rte_eth_dev *dev,\n\t\t\t\t   uint32_t index);\nstatic int  eth_ark_set_mtu(struct rte_eth_dev *dev, uint16_t size);\n\n/*\n * The packet generator is a functional block used to generate packet\n * patterns for testing.  It is not intended for nominal use.\n */\n#define ARK_PKTGEN_ARG \"Pkt_gen\"\n\n/*\n * The packet checker is a functional block used to verify packet\n * patterns for testing.  It is not intended for nominal use.\n */\n#define ARK_PKTCHKR_ARG \"Pkt_chkr\"\n\n/*\n * The packet director is used to select the internal ingress and\n * egress packets paths during testing.  It is not intended for\n * nominal use.\n */\n#define ARK_PKTDIR_ARG \"Pkt_dir\"\n\n/* Devinfo configurations */\n#define ARK_RX_MAX_QUEUE (4096 * 4)\n#define ARK_RX_MIN_QUEUE (512)\n#define ARK_RX_MAX_PKT_LEN ((16 * 1024) - 128)\n#define ARK_RX_MIN_BUFSIZE (1024)\n\n#define ARK_TX_MAX_QUEUE (4096 * 4)\n#define ARK_TX_MIN_QUEUE (256)\n\nstatic const char * const valid_arguments[] = {\n\tARK_PKTGEN_ARG,\n\tARK_PKTCHKR_ARG,\n\tARK_PKTDIR_ARG,\n\tNULL\n};\n\nstatic const struct rte_pci_id pci_id_ark_map[] = {\n\t{RTE_PCI_DEVICE(0x1d6c, 0x100d)},\n\t{RTE_PCI_DEVICE(0x1d6c, 0x100e)},\n\t{.vendor_id = 0, /* sentinel */ },\n};\n\nstatic int\neth_ark_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,\n\t\tstruct rte_pci_device *pci_dev)\n{\n\tstruct rte_eth_dev *eth_dev;\n\tint ret;\n\n\teth_dev = rte_eth_dev_pci_allocate(pci_dev, sizeof(struct ark_adapter));\n\n\tif (eth_dev == NULL)\n\t\treturn -ENOMEM;\n\n\tret = eth_ark_dev_init(eth_dev);\n\tif (ret)\n\t\trte_eth_dev_pci_release(eth_dev);\n\n\treturn ret;\n}\n\nstatic int\neth_ark_pci_remove(struct rte_pci_device *pci_dev)\n{\n\treturn rte_eth_dev_pci_generic_remove(pci_dev, eth_ark_dev_uninit);\n}\n\nstatic struct rte_pci_driver rte_ark_pmd = {\n\t.id_table = pci_id_ark_map,\n\t.drv_flags = RTE_PCI_DRV_NEED_MAPPING | RTE_PCI_DRV_INTR_LSC,\n\t.probe = eth_ark_pci_probe,\n\t.remove = eth_ark_pci_remove,\n};\n\nstatic const struct eth_dev_ops ark_eth_dev_ops = {\n\t.dev_configure = eth_ark_dev_configure,\n\t.dev_start = eth_ark_dev_start,\n\t.dev_stop = eth_ark_dev_stop,\n\t.dev_close = eth_ark_dev_close,\n\n\t.dev_infos_get = eth_ark_dev_info_get,\n\n\t.rx_queue_setup = eth_ark_dev_rx_queue_setup,\n\t.rx_queue_count = eth_ark_dev_rx_queue_count,\n\t.tx_queue_setup = eth_ark_tx_queue_setup,\n\n\t.link_update = eth_ark_dev_link_update,\n\t.dev_set_link_up = eth_ark_dev_set_link_up,\n\t.dev_set_link_down = eth_ark_dev_set_link_down,\n\n\t.rx_queue_start = eth_ark_rx_start_queue,\n\t.rx_queue_stop = eth_ark_rx_stop_queue,\n\n\t.tx_queue_start = eth_ark_tx_queue_start,\n\t.tx_queue_stop = eth_ark_tx_queue_stop,\n\n\t.stats_get = eth_ark_dev_stats_get,\n\t.stats_reset = eth_ark_dev_stats_reset,\n\n\t.mac_addr_add = eth_ark_macaddr_add,\n\t.mac_addr_remove = eth_ark_macaddr_remove,\n\t.mac_addr_set = eth_ark_set_default_mac_addr,\n\n\t.mtu_set = eth_ark_set_mtu,\n};\n\nstatic int\ncheck_for_ext(struct ark_adapter *ark)\n{\n\tint found = 0;\n\n\t/* Get the env */\n\tconst char *dllpath = getenv(\"ARK_EXT_PATH\");\n\n\tif (dllpath == NULL) {\n\t\tPMD_DEBUG_LOG(DEBUG, \"ARK EXT NO dll path specified\\n\");\n\t\treturn 0;\n\t}\n\tPMD_DRV_LOG(INFO, \"ARK EXT found dll path at %s\\n\", dllpath);\n\n\t/* Open and load the .so */\n\tark->d_handle = dlopen(dllpath, RTLD_LOCAL | RTLD_LAZY);\n\tif (ark->d_handle == NULL) {\n\t\tPMD_DRV_LOG(ERR, \"Could not load user extension %s\\n\",\n\t\t\t    dllpath);\n\t\treturn -1;\n\t}\n\tPMD_DRV_LOG(INFO, \"SUCCESS: loaded user extension %s\\n\",\n\t\t\t    dllpath);\n\n\t/* Get the entry points */\n\tark->user_ext.dev_init =\n\t\t(void *(*)(struct rte_eth_dev *, void *, int))\n\t\tdlsym(ark->d_handle, \"dev_init\");\n\tPMD_DEBUG_LOG(DEBUG, \"device ext init pointer = %p\\n\",\n\t\t      ark->user_ext.dev_init);\n\tark->user_ext.dev_get_port_count =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_get_port_count\");\n\tark->user_ext.dev_uninit =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_uninit\");\n\tark->user_ext.dev_configure =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_configure\");\n\tark->user_ext.dev_start =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_start\");\n\tark->user_ext.dev_stop =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_stop\");\n\tark->user_ext.dev_close =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_close\");\n\tark->user_ext.link_update =\n\t\t(int (*)(struct rte_eth_dev *, int, void *))\n\t\tdlsym(ark->d_handle, \"link_update\");\n\tark->user_ext.dev_set_link_up =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_set_link_up\");\n\tark->user_ext.dev_set_link_down =\n\t\t(int (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"dev_set_link_down\");\n\tark->user_ext.stats_get =\n\t\t(int (*)(struct rte_eth_dev *, struct rte_eth_stats *,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"stats_get\");\n\tark->user_ext.stats_reset =\n\t\t(void (*)(struct rte_eth_dev *, void *))\n\t\tdlsym(ark->d_handle, \"stats_reset\");\n\tark->user_ext.mac_addr_add =\n\t\t(void (*)(struct rte_eth_dev *, struct ether_addr *, uint32_t,\n\t\t\t  uint32_t, void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_add\");\n\tark->user_ext.mac_addr_remove =\n\t\t(void (*)(struct rte_eth_dev *, uint32_t, void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_remove\");\n\tark->user_ext.mac_addr_set =\n\t\t(void (*)(struct rte_eth_dev *, struct ether_addr *,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"mac_addr_set\");\n\tark->user_ext.set_mtu =\n\t\t(int (*)(struct rte_eth_dev *, uint16_t,\n\t\t\t  void *))\n\t\tdlsym(ark->d_handle, \"set_mtu\");\n\n\treturn found;\n}\n\nstatic int\neth_ark_dev_init(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tstruct rte_pci_device *pci_dev;\n\tint ret;\n\tint port_count = 1;\n\tint p;\n\n\tark->eth_dev = dev;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\t/* Check to see if there is an extension that we need to load */\n\tret = check_for_ext(ark);\n\tif (ret)\n\t\treturn ret;\n\tpci_dev = RTE_ETH_DEV_TO_PCI(dev);\n\trte_eth_copy_pci_info(dev, pci_dev);\n\n\t/* Use dummy function until setup */\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts_noop;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts_noop;\n\n\tark->bar0 = (uint8_t *)pci_dev->mem_resource[0].addr;\n\tark->a_bar = (uint8_t *)pci_dev->mem_resource[2].addr;\n\n\tark->sysctrl.v  = (void *)&ark->bar0[ARK_SYSCTRL_BASE];\n\tark->mpurx.v  = (void *)&ark->bar0[ARK_MPU_RX_BASE];\n\tark->udm.v  = (void *)&ark->bar0[ARK_UDM_BASE];\n\tark->mputx.v  = (void *)&ark->bar0[ARK_MPU_TX_BASE];\n\tark->ddm.v  = (void *)&ark->bar0[ARK_DDM_BASE];\n\tark->cmac.v  = (void *)&ark->bar0[ARK_CMAC_BASE];\n\tark->external.v  = (void *)&ark->bar0[ARK_EXTERNAL_BASE];\n\tark->pktdir.v  = (void *)&ark->bar0[ARK_PKTDIR_BASE];\n\tark->pktgen.v  = (void *)&ark->bar0[ARK_PKTGEN_BASE];\n\tark->pktchkr.v  = (void *)&ark->bar0[ARK_PKTCHKR_BASE];\n\n\tark->rqpacing =\n\t\t(struct ark_rqpace_t *)(ark->bar0 + ARK_RCPACING_BASE);\n\tark->started = 0;\n\n\tPMD_DEBUG_LOG(INFO, \"Sys Ctrl Const = 0x%x  HW Commit_ID: %08x\\n\",\n\t\t      ark->sysctrl.t32[4],\n\t\t      rte_be_to_cpu_32(ark->sysctrl.t32[0x20 / 4]));\n\tPMD_DRV_LOG(INFO, \"Arkville HW Commit_ID: %08x\\n\",\n\t\t    rte_be_to_cpu_32(ark->sysctrl.t32[0x20 / 4]));\n\n\t/* If HW sanity test fails, return an error */\n\tif (ark->sysctrl.t32[4] != 0xcafef00d) {\n\t\tPMD_DRV_LOG(ERR,\n\t\t\t    \"HW Sanity test has failed, expected constant\"\n\t\t\t    \" 0x%x, read 0x%x (%s)\\n\",\n\t\t\t    0xcafef00d,\n\t\t\t    ark->sysctrl.t32[4], __func__);\n\t\treturn -1;\n\t}\n\tif (ark->sysctrl.t32[3] != 0) {\n\t\tif (ark_rqp_lasped(ark->rqpacing)) {\n\t\t\tPMD_DRV_LOG(ERR, \"Arkville Evaluation System - \"\n\t\t\t\t    \"Timer has Expired\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tPMD_DRV_LOG(WARNING, \"Arkville Evaluation System - \"\n\t\t\t    \"Timer is Running\\n\");\n\t}\n\n\tPMD_DRV_LOG(INFO,\n\t\t    \"HW Sanity test has PASSED, expected constant\"\n\t\t    \" 0x%x, read 0x%x (%s)\\n\",\n\t\t    0xcafef00d, ark->sysctrl.t32[4], __func__);\n\n\t/* We are a single function multi-port device. */\n\tret = ark_config_device(dev);\n\tif (ret)\n\t\treturn -1;\n\n\tdev->dev_ops = &ark_eth_dev_ops;\n\n\tdev->data->mac_addrs = rte_zmalloc(\"ark\", ETHER_ADDR_LEN, 0);\n\tif (!dev->data->mac_addrs) {\n\t\tPMD_DRV_LOG(ERR,\n\t\t\t    \"Failed to allocated memory for storing mac address\"\n\t\t\t    );\n\t}\n\n\tif (ark->user_ext.dev_init) {\n\t\tark->user_data[dev->data->port_id] =\n\t\t\tark->user_ext.dev_init(dev, ark->a_bar, 0);\n\t\tif (!ark->user_data[dev->data->port_id]) {\n\t\t\tPMD_DRV_LOG(INFO,\n\t\t\t\t    \"Failed to initialize PMD extension!\"\n\t\t\t\t    \" continuing without it\\n\");\n\t\t\tmemset(&ark->user_ext, 0, sizeof(struct ark_user_ext));\n\t\t\tdlclose(ark->d_handle);\n\t\t}\n\t}\n\n\tif (pci_dev->device.devargs)\n\t\tret = eth_ark_check_args(ark, pci_dev->device.devargs->args);\n\telse\n\t\tPMD_DRV_LOG(INFO, \"No Device args found\\n\");\n\n\tif (ret)\n\t\tgoto error;\n\t/*\n\t * We will create additional devices based on the number of requested\n\t * ports\n\t */\n\tif (ark->user_ext.dev_get_port_count)\n\t\tport_count =\n\t\t\tark->user_ext.dev_get_port_count(dev,\n\t\t\t\t ark->user_data[dev->data->port_id]);\n\tark->num_ports = port_count;\n\n\tfor (p = 0; p < port_count; p++) {\n\t\tstruct rte_eth_dev *eth_dev;\n\t\tchar name[RTE_ETH_NAME_MAX_LEN];\n\n\t\tsnprintf(name, sizeof(name), \"arketh%d\",\n\t\t\t dev->data->port_id + p);\n\n\t\tif (p == 0) {\n\t\t\t/* First port is already allocated by DPDK */\n\t\t\teth_dev = ark->eth_dev;\n\t\t\trte_eth_dev_probing_finish(eth_dev);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* reserve an ethdev entry */\n\t\teth_dev = rte_eth_dev_allocate(name);\n\t\tif (!eth_dev) {\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"Could not allocate eth_dev for port %d\\n\",\n\t\t\t\t    p);\n\t\t\tgoto error;\n\t\t}\n\n\t\teth_dev->device = &pci_dev->device;\n\t\teth_dev->data->dev_private = ark;\n\t\teth_dev->dev_ops = ark->eth_dev->dev_ops;\n\t\teth_dev->tx_pkt_burst = ark->eth_dev->tx_pkt_burst;\n\t\teth_dev->rx_pkt_burst = ark->eth_dev->rx_pkt_burst;\n\n\t\trte_eth_copy_pci_info(eth_dev, pci_dev);\n\n\t\teth_dev->data->mac_addrs = rte_zmalloc(name, ETHER_ADDR_LEN, 0);\n\t\tif (!eth_dev->data->mac_addrs) {\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"Memory allocation for MAC failed!\"\n\t\t\t\t    \" Exiting.\\n\");\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (ark->user_ext.dev_init) {\n\t\t\tark->user_data[eth_dev->data->port_id] =\n\t\t\t\tark->user_ext.dev_init(dev, ark->a_bar, p);\n\t\t}\n\n\t\trte_eth_dev_probing_finish(eth_dev);\n\t}\n\n\treturn ret;\n\n error:\n\tif (dev->data->mac_addrs)\n\t\trte_free(dev->data->mac_addrs);\n\treturn -1;\n}\n\n/*\n *Initial device configuration when device is opened\n * setup the DDM, and UDM\n * Called once per PCIE device\n */\nstatic int\nark_config_device(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tuint16_t num_q, i;\n\tstruct ark_mpu_t *mpu;\n\n\t/*\n\t * Make sure that the packet director, generator and checker are in a\n\t * known state\n\t */\n\tark->start_pg = 0;\n\tark->pg = ark_pktgen_init(ark->pktgen.v, 0, 1);\n\tif (ark->pg == NULL)\n\t\treturn -1;\n\tark_pktgen_reset(ark->pg);\n\tark->pc = ark_pktchkr_init(ark->pktchkr.v, 0, 1);\n\tif (ark->pc == NULL)\n\t\treturn -1;\n\tark_pktchkr_stop(ark->pc);\n\tark->pd = ark_pktdir_init(ark->pktdir.v);\n\tif (ark->pd == NULL)\n\t\treturn -1;\n\n\t/* Verify HW */\n\tif (ark_udm_verify(ark->udm.v))\n\t\treturn -1;\n\tif (ark_ddm_verify(ark->ddm.v))\n\t\treturn -1;\n\n\t/* UDM */\n\tif (ark_udm_reset(ark->udm.v)) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to stop and reset UDM\\n\");\n\t\treturn -1;\n\t}\n\t/* Keep in reset until the MPU are cleared */\n\n\t/* MPU reset */\n\tmpu = ark->mpurx.v;\n\tnum_q = ark_api_num_queues(mpu);\n\tark->rx_queues = num_q;\n\tfor (i = 0; i < num_q; i++) {\n\t\tark_mpu_reset(mpu);\n\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t}\n\n\tark_udm_stop(ark->udm.v, 0);\n\tark_udm_configure(ark->udm.v,\n\t\t\t  RTE_PKTMBUF_HEADROOM,\n\t\t\t  RTE_MBUF_DEFAULT_DATAROOM,\n\t\t\t  ARK_RX_WRITE_TIME_NS);\n\tark_udm_stats_reset(ark->udm.v);\n\tark_udm_stop(ark->udm.v, 0);\n\n\t/* TX -- DDM */\n\tif (ark_ddm_stop(ark->ddm.v, 1))\n\t\tPMD_DRV_LOG(ERR, \"Unable to stop DDM\\n\");\n\n\tmpu = ark->mputx.v;\n\tnum_q = ark_api_num_queues(mpu);\n\tark->tx_queues = num_q;\n\tfor (i = 0; i < num_q; i++) {\n\t\tark_mpu_reset(mpu);\n\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t}\n\n\tark_ddm_reset(ark->ddm.v);\n\tark_ddm_stats_reset(ark->ddm.v);\n\n\tark_ddm_stop(ark->ddm.v, 0);\n\tark_rqp_stats_reset(ark->rqpacing);\n\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_uninit(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (rte_eal_process_type() != RTE_PROC_PRIMARY)\n\t\treturn 0;\n\n\tif (ark->user_ext.dev_uninit)\n\t\tark->user_ext.dev_uninit(dev,\n\t\t\t ark->user_data[dev->data->port_id]);\n\n\tark_pktgen_uninit(ark->pg);\n\tark_pktchkr_uninit(ark->pc);\n\n\tdev->dev_ops = NULL;\n\tdev->rx_pkt_burst = NULL;\n\tdev->tx_pkt_burst = NULL;\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_configure(struct rte_eth_dev *dev)\n{\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\teth_ark_dev_set_link_up(dev);\n\tif (ark->user_ext.dev_configure)\n\t\treturn ark->user_ext.dev_configure(dev,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic void *\ndelay_pg_start(void *arg)\n{\n\tstruct ark_adapter *ark = (struct ark_adapter *)arg;\n\n\t/* This function is used exclusively for regression testing, We\n\t * perform a blind sleep here to ensure that the external test\n\t * application has time to setup the test before we generate packets\n\t */\n\tusleep(100000);\n\tark_pktgen_run(ark->pg);\n\treturn NULL;\n}\n\nstatic int\neth_ark_dev_start(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tint i;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\t/* RX Side */\n\t/* start UDM */\n\tark_udm_start(ark->udm.v);\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_ark_rx_start_queue(dev, i);\n\n\t/* TX Side */\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_ark_tx_queue_start(dev, i);\n\n\t/* start DDM */\n\tark_ddm_start(ark->ddm.v);\n\n\tark->started = 1;\n\t/* set xmit and receive function */\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts;\n\n\tif (ark->start_pg)\n\t\tark_pktchkr_run(ark->pc);\n\n\tif (ark->start_pg && (dev->data->port_id == 0)) {\n\t\tpthread_t thread;\n\n\t\t/* Delay packet generatpr start allow the hardware to be ready\n\t\t * This is only used for sanity checking with internal generator\n\t\t */\n\t\tif (pthread_create(&thread, NULL, delay_pg_start, ark)) {\n\t\t\tPMD_DRV_LOG(ERR, \"Could not create pktgen \"\n\t\t\t\t    \"starter thread\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (ark->user_ext.dev_start)\n\t\tark->user_ext.dev_start(dev,\n\t\t\tark->user_data[dev->data->port_id]);\n\n\treturn 0;\n}\n\nstatic void\neth_ark_dev_stop(struct rte_eth_dev *dev)\n{\n\tuint16_t i;\n\tint status;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tstruct ark_mpu_t *mpu;\n\n\tPMD_FUNC_LOG(DEBUG, \"\\n\");\n\n\tif (ark->started == 0)\n\t\treturn;\n\tark->started = 0;\n\n\t/* Stop the extension first */\n\tif (ark->user_ext.dev_stop)\n\t\tark->user_ext.dev_stop(dev,\n\t\t       ark->user_data[dev->data->port_id]);\n\n\t/* Stop the packet generator */\n\tif (ark->start_pg)\n\t\tark_pktgen_pause(ark->pg);\n\n\tdev->rx_pkt_burst = &eth_ark_recv_pkts_noop;\n\tdev->tx_pkt_burst = &eth_ark_xmit_pkts_noop;\n\n\t/* STOP TX Side */\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++) {\n\t\tstatus = eth_ark_tx_queue_stop(dev, i);\n\t\tif (status != 0) {\n\t\t\tuint16_t port = dev->data->port_id;\n\t\t\tPMD_DRV_LOG(ERR,\n\t\t\t\t    \"tx_queue stop anomaly\"\n\t\t\t\t    \" port %u, queue %u\\n\",\n\t\t\t\t    port, i);\n\t\t}\n\t}\n\n\t/* Stop DDM */\n\t/* Wait up to 0.1 second.  each stop is up to 1000 * 10 useconds */\n\tfor (i = 0; i < 10; i++) {\n\t\tstatus = ark_ddm_stop(ark->ddm.v, 1);\n\t\tif (status == 0)\n\t\t\tbreak;\n\t}\n\tif (status || i != 0) {\n\t\tPMD_DRV_LOG(ERR, \"DDM stop anomaly. status:\"\n\t\t\t    \" %d iter: %u. (%s)\\n\",\n\t\t\t    status,\n\t\t\t    i,\n\t\t\t    __func__);\n\t\tark_ddm_dump(ark->ddm.v, \"Stop anomaly\");\n\n\t\tmpu = ark->mputx.v;\n\t\tfor (i = 0; i < ark->tx_queues; i++) {\n\t\t\tark_mpu_dump(mpu, \"DDM failure dump\", i);\n\t\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t\t}\n\t}\n\n\t/* STOP RX Side */\n\t/* Stop UDM  multiple tries attempted */\n\tfor (i = 0; i < 10; i++) {\n\t\tstatus = ark_udm_stop(ark->udm.v, 1);\n\t\tif (status == 0)\n\t\t\tbreak;\n\t}\n\tif (status || i != 0) {\n\t\tPMD_DRV_LOG(ERR, \"UDM stop anomaly. status %d iter: %u. (%s)\\n\",\n\t\t\t    status, i, __func__);\n\t\tark_udm_dump(ark->udm.v, \"Stop anomaly\");\n\n\t\tmpu = ark->mpurx.v;\n\t\tfor (i = 0; i < ark->rx_queues; i++) {\n\t\t\tark_mpu_dump(mpu, \"UDM Stop anomaly\", i);\n\t\t\tmpu = RTE_PTR_ADD(mpu, ARK_MPU_QOFFSET);\n\t\t}\n\t}\n\n\tark_udm_dump_stats(ark->udm.v, \"Post stop\");\n\tark_udm_dump_perf(ark->udm.v, \"Post stop\");\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_ark_rx_dump_queue(dev, i, __func__);\n\n\t/* Stop the packet checker if it is running */\n\tif (ark->start_pg) {\n\t\tark_pktchkr_dump_stats(ark->pc);\n\t\tark_pktchkr_stop(ark->pc);\n\t}\n}\n\nstatic void\neth_ark_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tuint16_t i;\n\n\tif (ark->user_ext.dev_close)\n\t\tark->user_ext.dev_close(dev,\n\t\t ark->user_data[dev->data->port_id]);\n\n\teth_ark_dev_stop(dev);\n\teth_ark_udm_force_close(dev);\n\n\t/*\n\t * TODO This should only be called once for the device during shutdown\n\t */\n\tark_rqp_dump(ark->rqpacing);\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++) {\n\t\teth_ark_tx_queue_release(dev->data->tx_queues[i]);\n\t\tdev->data->tx_queues[i] = 0;\n\t}\n\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++) {\n\t\teth_ark_dev_rx_queue_release(dev->data->rx_queues[i]);\n\t\tdev->data->rx_queues[i] = 0;\n\t}\n}\n\nstatic void\neth_ark_dev_info_get(struct rte_eth_dev *dev,\n\t\t     struct rte_eth_dev_info *dev_info)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\tstruct ark_mpu_t *tx_mpu = RTE_PTR_ADD(ark->bar0, ARK_MPU_TX_BASE);\n\tstruct ark_mpu_t *rx_mpu = RTE_PTR_ADD(ark->bar0, ARK_MPU_RX_BASE);\n\tuint16_t ports = ark->num_ports;\n\n\tdev_info->max_rx_pktlen = ARK_RX_MAX_PKT_LEN;\n\tdev_info->min_rx_bufsize = ARK_RX_MIN_BUFSIZE;\n\n\tdev_info->max_rx_queues = ark_api_num_queues_per_port(rx_mpu, ports);\n\tdev_info->max_tx_queues = ark_api_num_queues_per_port(tx_mpu, ports);\n\n\tdev_info->rx_desc_lim = (struct rte_eth_desc_lim) {\n\t\t.nb_max = ARK_RX_MAX_QUEUE,\n\t\t.nb_min = ARK_RX_MIN_QUEUE,\n\t\t.nb_align = ARK_RX_MIN_QUEUE}; /* power of 2 */\n\n\tdev_info->tx_desc_lim = (struct rte_eth_desc_lim) {\n\t\t.nb_max = ARK_TX_MAX_QUEUE,\n\t\t.nb_min = ARK_TX_MIN_QUEUE,\n\t\t.nb_align = ARK_TX_MIN_QUEUE}; /* power of 2 */\n\n\t/* ARK PMD supports all line rates, how do we indicate that here ?? */\n\tdev_info->speed_capa = (ETH_LINK_SPEED_1G |\n\t\t\t\tETH_LINK_SPEED_10G |\n\t\t\t\tETH_LINK_SPEED_25G |\n\t\t\t\tETH_LINK_SPEED_40G |\n\t\t\t\tETH_LINK_SPEED_50G |\n\t\t\t\tETH_LINK_SPEED_100G);\n}\n\nstatic int\neth_ark_dev_link_update(struct rte_eth_dev *dev, int wait_to_complete)\n{\n\tPMD_DEBUG_LOG(DEBUG, \"link status = %d\\n\",\n\t\t\tdev->data->dev_link.link_status);\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.link_update) {\n\t\treturn ark->user_ext.link_update\n\t\t\t(dev, wait_to_complete,\n\t\t\t ark->user_data[dev->data->port_id]);\n\t}\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_set_link_up(struct rte_eth_dev *dev)\n{\n\tdev->data->dev_link.link_status = 1;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.dev_set_link_up)\n\t\treturn ark->user_ext.dev_set_link_up(dev,\n\t\t\t     ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_set_link_down(struct rte_eth_dev *dev)\n{\n\tdev->data->dev_link.link_status = 0;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.dev_set_link_down)\n\t\treturn ark->user_ext.dev_set_link_down(dev,\n\t\t       ark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic int\neth_ark_dev_stats_get(struct rte_eth_dev *dev, struct rte_eth_stats *stats)\n{\n\tuint16_t i;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tstats->ipackets = 0;\n\tstats->ibytes = 0;\n\tstats->opackets = 0;\n\tstats->obytes = 0;\n\tstats->imissed = 0;\n\tstats->oerrors = 0;\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_tx_queue_stats_get(dev->data->tx_queues[i], stats);\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_rx_queue_stats_get(dev->data->rx_queues[i], stats);\n\tif (ark->user_ext.stats_get)\n\t\treturn ark->user_ext.stats_get(dev, stats,\n\t\t\tark->user_data[dev->data->port_id]);\n\treturn 0;\n}\n\nstatic void\neth_ark_dev_stats_reset(struct rte_eth_dev *dev)\n{\n\tuint16_t i;\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tfor (i = 0; i < dev->data->nb_tx_queues; i++)\n\t\teth_tx_queue_stats_reset(dev->data->tx_queues[i]);\n\tfor (i = 0; i < dev->data->nb_rx_queues; i++)\n\t\teth_rx_queue_stats_reset(dev->data->rx_queues[i]);\n\tif (ark->user_ext.stats_reset)\n\t\tark->user_ext.stats_reset(dev,\n\t\t\t  ark->user_data[dev->data->port_id]);\n}\n\nstatic int\neth_ark_macaddr_add(struct rte_eth_dev *dev,\n\t\t    struct ether_addr *mac_addr,\n\t\t    uint32_t index,\n\t\t    uint32_t pool)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_add) {\n\t\tark->user_ext.mac_addr_add(dev,\n\t\t\t\t\t   mac_addr,\n\t\t\t\t\t   index,\n\t\t\t\t\t   pool,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\t\treturn 0;\n\t}\n\treturn -ENOTSUP;\n}\n\nstatic void\neth_ark_macaddr_remove(struct rte_eth_dev *dev, uint32_t index)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_remove)\n\t\tark->user_ext.mac_addr_remove(dev, index,\n\t\t\t      ark->user_data[dev->data->port_id]);\n}\n\nstatic int\neth_ark_set_default_mac_addr(struct rte_eth_dev *dev,\n\t\t\t     struct ether_addr *mac_addr)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.mac_addr_set) {\n\t\tark->user_ext.mac_addr_set(dev, mac_addr,\n\t\t\t   ark->user_data[dev->data->port_id]);\n\t\treturn 0;\n\t}\n\treturn -ENOTSUP;\n}\n\nstatic int\neth_ark_set_mtu(struct rte_eth_dev *dev, uint16_t  size)\n{\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)dev->data->dev_private;\n\n\tif (ark->user_ext.set_mtu)\n\t\treturn ark->user_ext.set_mtu(dev, size,\n\t\t\t     ark->user_data[dev->data->port_id]);\n\n\treturn -ENOTSUP;\n}\n\nstatic inline int\nprocess_pktdir_arg(const char *key, const char *value,\n\t\t   void *extra_args)\n{\n\tPMD_FUNC_LOG(DEBUG, \"key = %s, value = %s\\n\",\n\t\t    key, value);\n\tstruct ark_adapter *ark =\n\t\t(struct ark_adapter *)extra_args;\n\n\tark->pkt_dir_v = strtol(value, NULL, 16);\n\tPMD_FUNC_LOG(DEBUG, \"pkt_dir_v = 0x%x\\n\", ark->pkt_dir_v);\n\treturn 0;\n}\n\nstatic inline int\nprocess_file_args(const char *key, const char *value, void *extra_args)\n{\n\tPMD_FUNC_LOG(DEBUG, \"key = %s, value = %s\\n\",\n\t\t    key, value);\n\tchar *args = (char *)extra_args;\n\n\t/* Open the configuration file */\n\tFILE *file = fopen(value, \"r\");\n\tchar line[ARK_MAX_ARG_LEN];\n\tint  size = 0;\n\tint first = 1;\n\n\tif (file == NULL) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to open \"\n\t\t\t    \"config file %s\\n\", value);\n\t\treturn -1;\n\t}\n\n\twhile (fgets(line, sizeof(line), file)) {\n\t\tsize += strlen(line);\n\t\tif (size >= ARK_MAX_ARG_LEN) {\n\t\t\tPMD_DRV_LOG(ERR, \"Unable to parse file %s args, \"\n\t\t\t\t    \"parameter list is too long\\n\", value);\n\t\t\tfclose(file);\n\t\t\treturn -1;\n\t\t}\n\t\tif (first) {\n\t\t\tstrncpy(args, line, ARK_MAX_ARG_LEN);\n\t\t\tfirst = 0;\n\t\t} else {\n\t\t\tstrncat(args, line, ARK_MAX_ARG_LEN);\n\t\t}\n\t}\n\tPMD_FUNC_LOG(DEBUG, \"file = %s\\n\", args);\n\tfclose(file);\n\treturn 0;\n}\n\nstatic int\neth_ark_check_args(struct ark_adapter *ark, const char *params)\n{\n\tstruct rte_kvargs *kvlist;\n\tunsigned int k_idx;\n\tstruct rte_kvargs_pair *pair = NULL;\n\tint ret = -1;\n\n\tkvlist = rte_kvargs_parse(params, valid_arguments);\n\tif (kvlist == NULL)\n\t\treturn 0;\n\n\tark->pkt_gen_args[0] = 0;\n\tark->pkt_chkr_args[0] = 0;\n\n\tfor (k_idx = 0; k_idx < kvlist->count; k_idx++) {\n\t\tpair = &kvlist->pairs[k_idx];\n\t\tPMD_FUNC_LOG(DEBUG, \"**** Arg passed to PMD = %s:%s\\n\",\n\t\t\t     pair->key,\n\t\t\t     pair->value);\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTDIR_ARG,\n\t\t\t       &process_pktdir_arg,\n\t\t\t       ark) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTDIR_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTGEN_ARG,\n\t\t\t       &process_file_args,\n\t\t\t       ark->pkt_gen_args) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTGEN_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tif (rte_kvargs_process(kvlist,\n\t\t\t       ARK_PKTCHKR_ARG,\n\t\t\t       &process_file_args,\n\t\t\t       ark->pkt_chkr_args) != 0) {\n\t\tPMD_DRV_LOG(ERR, \"Unable to parse arg %s\\n\", ARK_PKTCHKR_ARG);\n\t\tgoto free_kvlist;\n\t}\n\n\tPMD_DRV_LOG(INFO, \"packet director set to 0x%x\\n\", ark->pkt_dir_v);\n\t/* Setup the packet director */\n\tark_pktdir_setup(ark->pd, ark->pkt_dir_v);\n\n\t/* Setup the packet generator */\n\tif (ark->pkt_gen_args[0]) {\n\t\tPMD_DRV_LOG(INFO, \"Setting up the packet generator\\n\");\n\t\tark_pktgen_parse(ark->pkt_gen_args);\n\t\tark_pktgen_reset(ark->pg);\n\t\tark_pktgen_setup(ark->pg);\n\t\tark->start_pg = 1;\n\t}\n\n\t/* Setup the packet checker */\n\tif (ark->pkt_chkr_args[0]) {\n\t\tark_pktchkr_parse(ark->pkt_chkr_args);\n\t\tark_pktchkr_setup(ark->pc);\n\t}\n\n\tret = 0;\n\nfree_kvlist:\n\trte_kvargs_free(kvlist);\n\n\treturn ret;\n}\n\nRTE_PMD_REGISTER_PCI(net_ark, rte_ark_pmd);\nRTE_PMD_REGISTER_KMOD_DEP(net_ark, \"* igb_uio | uio_pci_generic \");\nRTE_PMD_REGISTER_PCI_TABLE(net_ark, pci_id_ark_map);\nRTE_PMD_REGISTER_PARAM_STRING(net_ark,\n\t\t\t      ARK_PKTGEN_ARG \"=<filename> \"\n\t\t\t      ARK_PKTCHKR_ARG \"=<filename> \"\n\t\t\t      ARK_PKTDIR_ARG \"=<bitmap>\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/mlx5/mlx5.c": "/* SPDX-License-Identifier: BSD-3-Clause\n * Copyright 2015 6WIND S.A.\n * Copyright 2015 Mellanox Technologies, Ltd\n */\n\n#include <stddef.h>\n#include <unistd.h>\n#include <string.h>\n#include <assert.h>\n#include <dlfcn.h>\n#include <stdint.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <net/if.h>\n#include <sys/mman.h>\n#include <linux/rtnetlink.h>\n\n/* Verbs header. */\n/* ISO C doesn't support unnamed structs/unions, disabling -pedantic. */\n#ifdef PEDANTIC\n#pragma GCC diagnostic ignored \"-Wpedantic\"\n#endif\n#include <infiniband/verbs.h>\n#ifdef PEDANTIC\n#pragma GCC diagnostic error \"-Wpedantic\"\n#endif\n\n#include <rte_malloc.h>\n#include <rte_ethdev_driver.h>\n#include <rte_ethdev_pci.h>\n#include <rte_pci.h>\n#include <rte_bus_pci.h>\n#include <rte_common.h>\n#include <rte_config.h>\n#include <rte_eal_memconfig.h>\n#include <rte_kvargs.h>\n#include <rte_rwlock.h>\n#include <rte_spinlock.h>\n#include <rte_string_fns.h>\n\n#include \"mlx5.h\"\n#include \"mlx5_utils.h\"\n#include \"mlx5_rxtx.h\"\n#include \"mlx5_autoconf.h\"\n#include \"mlx5_defs.h\"\n#include \"mlx5_glue.h\"\n#include \"mlx5_mr.h\"\n#include \"mlx5_flow.h\"\n\n/* Device parameter to enable RX completion queue compression. */\n#define MLX5_RXQ_CQE_COMP_EN \"rxq_cqe_comp_en\"\n\n/* Device parameter to enable RX completion entry padding to 128B. */\n#define MLX5_RXQ_CQE_PAD_EN \"rxq_cqe_pad_en\"\n\n/* Device parameter to enable padding Rx packet to cacheline size. */\n#define MLX5_RXQ_PKT_PAD_EN \"rxq_pkt_pad_en\"\n\n/* Device parameter to enable Multi-Packet Rx queue. */\n#define MLX5_RX_MPRQ_EN \"mprq_en\"\n\n/* Device parameter to configure log 2 of the number of strides for MPRQ. */\n#define MLX5_RX_MPRQ_LOG_STRIDE_NUM \"mprq_log_stride_num\"\n\n/* Device parameter to limit the size of memcpy'd packet for MPRQ. */\n#define MLX5_RX_MPRQ_MAX_MEMCPY_LEN \"mprq_max_memcpy_len\"\n\n/* Device parameter to set the minimum number of Rx queues to enable MPRQ. */\n#define MLX5_RXQS_MIN_MPRQ \"rxqs_min_mprq\"\n\n/* Device parameter to configure inline send. */\n#define MLX5_TXQ_INLINE \"txq_inline\"\n\n/*\n * Device parameter to configure the number of TX queues threshold for\n * enabling inline send.\n */\n#define MLX5_TXQS_MIN_INLINE \"txqs_min_inline\"\n\n/*\n * Device parameter to configure the number of TX queues threshold for\n * enabling vectorized Tx.\n */\n#define MLX5_TXQS_MAX_VEC \"txqs_max_vec\"\n\n/* Device parameter to enable multi-packet send WQEs. */\n#define MLX5_TXQ_MPW_EN \"txq_mpw_en\"\n\n/* Device parameter to include 2 dsegs in the title WQEBB. */\n#define MLX5_TXQ_MPW_HDR_DSEG_EN \"txq_mpw_hdr_dseg_en\"\n\n/* Device parameter to limit the size of inlining packet. */\n#define MLX5_TXQ_MAX_INLINE_LEN \"txq_max_inline_len\"\n\n/* Device parameter to enable hardware Tx vector. */\n#define MLX5_TX_VEC_EN \"tx_vec_en\"\n\n/* Device parameter to enable hardware Rx vector. */\n#define MLX5_RX_VEC_EN \"rx_vec_en\"\n\n/* Allow L3 VXLAN flow creation. */\n#define MLX5_L3_VXLAN_EN \"l3_vxlan_en\"\n\n/* Activate DV E-Switch flow steering. */\n#define MLX5_DV_ESW_EN \"dv_esw_en\"\n\n/* Activate DV flow steering. */\n#define MLX5_DV_FLOW_EN \"dv_flow_en\"\n\n/* Activate Netlink support in VF mode. */\n#define MLX5_VF_NL_EN \"vf_nl_en\"\n\n/* Enable extending memsegs when creating a MR. */\n#define MLX5_MR_EXT_MEMSEG_EN \"mr_ext_memseg_en\"\n\n/* Select port representors to instantiate. */\n#define MLX5_REPRESENTOR \"representor\"\n\n#ifndef HAVE_IBV_MLX5_MOD_MPW\n#define MLX5DV_CONTEXT_FLAGS_MPW_ALLOWED (1 << 2)\n#define MLX5DV_CONTEXT_FLAGS_ENHANCED_MPW (1 << 3)\n#endif\n\n#ifndef HAVE_IBV_MLX5_MOD_CQE_128B_COMP\n#define MLX5DV_CONTEXT_FLAGS_CQE_128B_COMP (1 << 4)\n#endif\n\nstatic const char *MZ_MLX5_PMD_SHARED_DATA = \"mlx5_pmd_shared_data\";\n\n/* Shared memory between primary and secondary processes. */\nstruct mlx5_shared_data *mlx5_shared_data;\n\n/* Spinlock for mlx5_shared_data allocation. */\nstatic rte_spinlock_t mlx5_shared_data_lock = RTE_SPINLOCK_INITIALIZER;\n\n/* Process local data for secondary processes. */\nstatic struct mlx5_local_data mlx5_local_data;\n\n/** Driver-specific log messages type. */\nint mlx5_logtype;\n\n/** Data associated with devices to spawn. */\nstruct mlx5_dev_spawn_data {\n\tuint32_t ifindex; /**< Network interface index. */\n\tuint32_t max_port; /**< IB device maximal port index. */\n\tuint32_t ibv_port; /**< IB device physical port index. */\n\tstruct mlx5_switch_info info; /**< Switch information. */\n\tstruct ibv_device *ibv_dev; /**< Associated IB device. */\n\tstruct rte_eth_dev *eth_dev; /**< Associated Ethernet device. */\n\tstruct rte_pci_device *pci_dev; /**< Backend PCI device. */\n};\n\nstatic LIST_HEAD(, mlx5_ibv_shared) mlx5_ibv_list = LIST_HEAD_INITIALIZER();\nstatic pthread_mutex_t mlx5_ibv_list_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n/**\n * Allocate shared IB device context. If there is multiport device the\n * master and representors will share this context, if there is single\n * port dedicated IB device, the context will be used by only given\n * port due to unification.\n *\n * Routine first searches the context for the specified IB device name,\n * if found the shared context assumed and reference counter is incremented.\n * If no context found the new one is created and initialized with specified\n * IB device context and parameters.\n *\n * @param[in] spawn\n *   Pointer to the IB device attributes (name, port, etc).\n *\n * @return\n *   Pointer to mlx5_ibv_shared object on success,\n *   otherwise NULL and rte_errno is set.\n */\nstatic struct mlx5_ibv_shared *\nmlx5_alloc_shared_ibctx(const struct mlx5_dev_spawn_data *spawn)\n{\n\tstruct mlx5_ibv_shared *sh;\n\tint err = 0;\n\tuint32_t i;\n\n\tassert(spawn);\n\t/* Secondary process should not create the shared context. */\n\tassert(rte_eal_process_type() == RTE_PROC_PRIMARY);\n\tpthread_mutex_lock(&mlx5_ibv_list_mutex);\n\t/* Search for IB context by device name. */\n\tLIST_FOREACH(sh, &mlx5_ibv_list, next) {\n\t\tif (!strcmp(sh->ibdev_name, spawn->ibv_dev->name)) {\n\t\t\tsh->refcnt++;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\t/* No device found, we have to create new shared context. */\n\tassert(spawn->max_port);\n\tsh = rte_zmalloc(\"ethdev shared ib context\",\n\t\t\t sizeof(struct mlx5_ibv_shared) +\n\t\t\t spawn->max_port *\n\t\t\t sizeof(struct mlx5_ibv_shared_port),\n\t\t\t RTE_CACHE_LINE_SIZE);\n\tif (!sh) {\n\t\tDRV_LOG(ERR, \"shared context allocation failure\");\n\t\trte_errno  = ENOMEM;\n\t\tgoto exit;\n\t}\n\t/* Try to open IB device with DV first, then usual Verbs. */\n\terrno = 0;\n\tsh->ctx = mlx5_glue->dv_open_device(spawn->ibv_dev);\n\tif (sh->ctx) {\n\t\tsh->devx = 1;\n\t\tDRV_LOG(DEBUG, \"DevX is supported\");\n\t} else {\n\t\tsh->ctx = mlx5_glue->open_device(spawn->ibv_dev);\n\t\tif (!sh->ctx) {\n\t\t\terr = errno ? errno : ENODEV;\n\t\t\tgoto error;\n\t\t}\n\t\tDRV_LOG(DEBUG, \"DevX is NOT supported\");\n\t}\n\terr = mlx5_glue->query_device_ex(sh->ctx, NULL, &sh->device_attr);\n\tif (err) {\n\t\tDRV_LOG(DEBUG, \"ibv_query_device_ex() failed\");\n\t\tgoto error;\n\t}\n\tsh->refcnt = 1;\n\tsh->max_port = spawn->max_port;\n\tstrncpy(sh->ibdev_name, sh->ctx->device->name,\n\t\tsizeof(sh->ibdev_name));\n\tstrncpy(sh->ibdev_path, sh->ctx->device->ibdev_path,\n\t\tsizeof(sh->ibdev_path));\n\tsh->pci_dev = spawn->pci_dev;\n\tpthread_mutex_init(&sh->intr_mutex, NULL);\n\t/*\n\t * Setting port_id to max unallowed value means\n\t * there is no interrupt subhandler installed for\n\t * the given port index i.\n\t */\n\tfor (i = 0; i < sh->max_port; i++)\n\t\tsh->port[i].ih_port_id = RTE_MAX_ETHPORTS;\n\tsh->pd = mlx5_glue->alloc_pd(sh->ctx);\n\tif (sh->pd == NULL) {\n\t\tDRV_LOG(ERR, \"PD allocation failure\");\n\t\terr = ENOMEM;\n\t\tgoto error;\n\t}\n\t/*\n\t * Once the device is added to the list of memory event\n\t * callback, its global MR cache table cannot be expanded\n\t * on the fly because of deadlock. If it overflows, lookup\n\t * should be done by searching MR list linearly, which is slow.\n\t *\n\t * At this point the device is not added to the memory\n\t * event list yet, context is just being created.\n\t */\n\terr = mlx5_mr_btree_init(&sh->mr.cache,\n\t\t\t\t MLX5_MR_BTREE_CACHE_N * 2,\n\t\t\t\t sh->pci_dev->device.numa_node);\n\tif (err) {\n\t\terr = rte_errno;\n\t\tgoto error;\n\t}\n\tLIST_INSERT_HEAD(&mlx5_ibv_list, sh, next);\nexit:\n\tpthread_mutex_unlock(&mlx5_ibv_list_mutex);\n\treturn sh;\nerror:\n\tpthread_mutex_unlock(&mlx5_ibv_list_mutex);\n\tassert(sh);\n\tif (sh->pd)\n\t\tclaim_zero(mlx5_glue->dealloc_pd(sh->pd));\n\tif (sh->ctx)\n\t\tclaim_zero(mlx5_glue->close_device(sh->ctx));\n\trte_free(sh);\n\tassert(err > 0);\n\trte_errno = err;\n\treturn NULL;\n}\n\n/**\n * Free shared IB device context. Decrement counter and if zero free\n * all allocated resources and close handles.\n *\n * @param[in] sh\n *   Pointer to mlx5_ibv_shared object to free\n */\nstatic void\nmlx5_free_shared_ibctx(struct mlx5_ibv_shared *sh)\n{\n\tpthread_mutex_lock(&mlx5_ibv_list_mutex);\n#ifndef NDEBUG\n\t/* Check the object presence in the list. */\n\tstruct mlx5_ibv_shared *lctx;\n\n\tLIST_FOREACH(lctx, &mlx5_ibv_list, next)\n\t\tif (lctx == sh)\n\t\t\tbreak;\n\tassert(lctx);\n\tif (lctx != sh) {\n\t\tDRV_LOG(ERR, \"Freeing non-existing shared IB context\");\n\t\tgoto exit;\n\t}\n#endif\n\tassert(sh);\n\tassert(sh->refcnt);\n\t/* Secondary process should not free the shared context. */\n\tassert(rte_eal_process_type() == RTE_PROC_PRIMARY);\n\tif (--sh->refcnt)\n\t\tgoto exit;\n\t/* Release created Memory Regions. */\n\tmlx5_mr_release(sh);\n\tLIST_REMOVE(sh, next);\n\t/*\n\t *  Ensure there is no async event handler installed.\n\t *  Only primary process handles async device events.\n\t **/\n\tassert(!sh->intr_cnt);\n\tif (sh->intr_cnt)\n\t\trte_intr_callback_unregister\n\t\t\t(&sh->intr_handle, mlx5_dev_interrupt_handler, sh);\n\tpthread_mutex_destroy(&sh->intr_mutex);\n\tif (sh->pd)\n\t\tclaim_zero(mlx5_glue->dealloc_pd(sh->pd));\n\tif (sh->ctx)\n\t\tclaim_zero(mlx5_glue->close_device(sh->ctx));\n\trte_free(sh);\nexit:\n\tpthread_mutex_unlock(&mlx5_ibv_list_mutex);\n}\n\n/**\n * Initialize DR related data within private structure.\n * Routine checks the reference counter and does actual\n * resources creation/initialization only if counter is zero.\n *\n * @param[in] priv\n *   Pointer to the private device data structure.\n *\n * @return\n *   Zero on success, positive error code otherwise.\n */\nstatic int\nmlx5_alloc_shared_dr(struct mlx5_priv *priv)\n{\n#ifdef HAVE_MLX5DV_DR\n\tstruct mlx5_ibv_shared *sh = priv->sh;\n\tint err = 0;\n\tvoid *domain;\n\n\tassert(sh);\n\tif (sh->dv_refcnt) {\n\t\t/* Shared DV/DR structures is already initialized. */\n\t\tsh->dv_refcnt++;\n\t\tpriv->dr_shared = 1;\n\t\treturn 0;\n\t}\n\t/* Reference counter is zero, we should initialize structures. */\n\tdomain = mlx5_glue->dr_create_domain(sh->ctx,\n\t\t\t\t\t     MLX5DV_DR_DOMAIN_TYPE_NIC_RX);\n\tif (!domain) {\n\t\tDRV_LOG(ERR, \"ingress mlx5dv_dr_create_domain failed\");\n\t\terr = errno;\n\t\tgoto error;\n\t}\n\tsh->rx_domain = domain;\n\tdomain = mlx5_glue->dr_create_domain(sh->ctx,\n\t\t\t\t\t     MLX5DV_DR_DOMAIN_TYPE_NIC_TX);\n\tif (!domain) {\n\t\tDRV_LOG(ERR, \"egress mlx5dv_dr_create_domain failed\");\n\t\terr = errno;\n\t\tgoto error;\n\t}\n\tpthread_mutex_init(&sh->dv_mutex, NULL);\n\tsh->tx_domain = domain;\n#ifdef HAVE_MLX5DV_DR_ESWITCH\n\tif (priv->config.dv_esw_en) {\n\t\tdomain  = mlx5_glue->dr_create_domain\n\t\t\t(sh->ctx, MLX5DV_DR_DOMAIN_TYPE_FDB);\n\t\tif (!domain) {\n\t\t\tDRV_LOG(ERR, \"FDB mlx5dv_dr_create_domain failed\");\n\t\t\terr = errno;\n\t\t\tgoto error;\n\t\t}\n\t\tsh->fdb_domain = domain;\n\t\tsh->esw_drop_action = mlx5_glue->dr_create_flow_action_drop();\n\t}\n#endif\n\tsh->dv_refcnt++;\n\tpriv->dr_shared = 1;\n\treturn 0;\n\nerror:\n       /* Rollback the created objects. */\n\tif (sh->rx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->rx_domain);\n\t\tsh->rx_domain = NULL;\n\t}\n\tif (sh->tx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->tx_domain);\n\t\tsh->tx_domain = NULL;\n\t}\n\tif (sh->fdb_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->fdb_domain);\n\t\tsh->fdb_domain = NULL;\n\t}\n\tif (sh->esw_drop_action) {\n\t\tmlx5_glue->destroy_flow_action(sh->esw_drop_action);\n\t\tsh->esw_drop_action = NULL;\n\t}\n\treturn err;\n#else\n\t(void)priv;\n\treturn 0;\n#endif\n}\n\n/**\n * Destroy DR related data within private structure.\n *\n * @param[in] priv\n *   Pointer to the private device data structure.\n */\nstatic void\nmlx5_free_shared_dr(struct mlx5_priv *priv)\n{\n#ifdef HAVE_MLX5DV_DR\n\tstruct mlx5_ibv_shared *sh;\n\n\tif (!priv->dr_shared)\n\t\treturn;\n\tpriv->dr_shared = 0;\n\tsh = priv->sh;\n\tassert(sh);\n\tassert(sh->dv_refcnt);\n\tif (sh->dv_refcnt && --sh->dv_refcnt)\n\t\treturn;\n\tif (sh->rx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->rx_domain);\n\t\tsh->rx_domain = NULL;\n\t}\n\tif (sh->tx_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->tx_domain);\n\t\tsh->tx_domain = NULL;\n\t}\n#ifdef HAVE_MLX5DV_DR_ESWITCH\n\tif (sh->fdb_domain) {\n\t\tmlx5_glue->dr_destroy_domain(sh->fdb_domain);\n\t\tsh->fdb_domain = NULL;\n\t}\n\tif (sh->esw_drop_action) {\n\t\tmlx5_glue->destroy_flow_action(sh->esw_drop_action);\n\t\tsh->esw_drop_action = NULL;\n\t}\n#endif\n\tpthread_mutex_destroy(&sh->dv_mutex);\n#else\n\t(void)priv;\n#endif\n}\n\n/**\n * Initialize shared data between primary and secondary process.\n *\n * A memzone is reserved by primary process and secondary processes attach to\n * the memzone.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_init_shared_data(void)\n{\n\tconst struct rte_memzone *mz;\n\tint ret = 0;\n\n\trte_spinlock_lock(&mlx5_shared_data_lock);\n\tif (mlx5_shared_data == NULL) {\n\t\tif (rte_eal_process_type() == RTE_PROC_PRIMARY) {\n\t\t\t/* Allocate shared memory. */\n\t\t\tmz = rte_memzone_reserve(MZ_MLX5_PMD_SHARED_DATA,\n\t\t\t\t\t\t sizeof(*mlx5_shared_data),\n\t\t\t\t\t\t SOCKET_ID_ANY, 0);\n\t\t\tif (mz == NULL) {\n\t\t\t\tDRV_LOG(ERR,\n\t\t\t\t\t\"Cannot allocate mlx5 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx5_shared_data = mz->addr;\n\t\t\tmemset(mlx5_shared_data, 0, sizeof(*mlx5_shared_data));\n\t\t\trte_spinlock_init(&mlx5_shared_data->lock);\n\t\t} else {\n\t\t\t/* Lookup allocated shared memory. */\n\t\t\tmz = rte_memzone_lookup(MZ_MLX5_PMD_SHARED_DATA);\n\t\t\tif (mz == NULL) {\n\t\t\t\tDRV_LOG(ERR,\n\t\t\t\t\t\"Cannot attach mlx5 shared data\\n\");\n\t\t\t\tret = -rte_errno;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tmlx5_shared_data = mz->addr;\n\t\t\tmemset(&mlx5_local_data, 0, sizeof(mlx5_local_data));\n\t\t}\n\t}\nerror:\n\trte_spinlock_unlock(&mlx5_shared_data_lock);\n\treturn ret;\n}\n\n/**\n * Retrieve integer value from environment variable.\n *\n * @param[in] name\n *   Environment variable name.\n *\n * @return\n *   Integer value, 0 if the variable is not set.\n */\nint\nmlx5_getenv_int(const char *name)\n{\n\tconst char *val = getenv(name);\n\n\tif (val == NULL)\n\t\treturn 0;\n\treturn atoi(val);\n}\n\n/**\n * Verbs callback to allocate a memory. This function should allocate the space\n * according to the size provided residing inside a huge page.\n * Please note that all allocation must respect the alignment from libmlx5\n * (i.e. currently sysconf(_SC_PAGESIZE)).\n *\n * @param[in] size\n *   The size in bytes of the memory to allocate.\n * @param[in] data\n *   A pointer to the callback data.\n *\n * @return\n *   Allocated buffer, NULL otherwise and rte_errno is set.\n */\nstatic void *\nmlx5_alloc_verbs_buf(size_t size, void *data)\n{\n\tstruct mlx5_priv *priv = data;\n\tvoid *ret;\n\tsize_t alignment = sysconf(_SC_PAGESIZE);\n\tunsigned int socket = SOCKET_ID_ANY;\n\n\tif (priv->verbs_alloc_ctx.type == MLX5_VERBS_ALLOC_TYPE_TX_QUEUE) {\n\t\tconst struct mlx5_txq_ctrl *ctrl = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = ctrl->socket;\n\t} else if (priv->verbs_alloc_ctx.type ==\n\t\t   MLX5_VERBS_ALLOC_TYPE_RX_QUEUE) {\n\t\tconst struct mlx5_rxq_ctrl *ctrl = priv->verbs_alloc_ctx.obj;\n\n\t\tsocket = ctrl->socket;\n\t}\n\tassert(data != NULL);\n\tret = rte_malloc_socket(__func__, size, alignment, socket);\n\tif (!ret && size)\n\t\trte_errno = ENOMEM;\n\treturn ret;\n}\n\n/**\n * Verbs callback to free a memory.\n *\n * @param[in] ptr\n *   A pointer to the memory to free.\n * @param[in] data\n *   A pointer to the callback data.\n */\nstatic void\nmlx5_free_verbs_buf(void *ptr, void *data __rte_unused)\n{\n\tassert(data != NULL);\n\trte_free(ptr);\n}\n\n/**\n * Initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nint\nmlx5_proc_priv_init(struct rte_eth_dev *dev)\n{\n\tstruct mlx5_priv *priv = dev->data->dev_private;\n\tstruct mlx5_proc_priv *ppriv;\n\tsize_t ppriv_size;\n\n\t/*\n\t * UAR register table follows the process private structure. BlueFlame\n\t * registers for Tx queues are stored in the table.\n\t */\n\tppriv_size =\n\t\tsizeof(struct mlx5_proc_priv) + priv->txqs_n * sizeof(void *);\n\tppriv = rte_malloc_socket(\"mlx5_proc_priv\", ppriv_size,\n\t\t\t\t  RTE_CACHE_LINE_SIZE, dev->device->numa_node);\n\tif (!ppriv) {\n\t\trte_errno = ENOMEM;\n\t\treturn -rte_errno;\n\t}\n\tppriv->uar_table_sz = ppriv_size;\n\tdev->process_private = ppriv;\n\treturn 0;\n}\n\n/**\n * Un-initialize process private data structure.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx5_proc_priv_uninit(struct rte_eth_dev *dev)\n{\n\tif (!dev->process_private)\n\t\treturn;\n\trte_free(dev->process_private);\n\tdev->process_private = NULL;\n}\n\n/**\n * DPDK callback to close the device.\n *\n * Destroy all queues and objects, free memory.\n *\n * @param dev\n *   Pointer to Ethernet device structure.\n */\nstatic void\nmlx5_dev_close(struct rte_eth_dev *dev)\n{\n\tstruct mlx5_priv *priv = dev->data->dev_private;\n\tunsigned int i;\n\tint ret;\n\n\tDRV_LOG(DEBUG, \"port %u closing device \\\"%s\\\"\",\n\t\tdev->data->port_id,\n\t\t((priv->sh->ctx != NULL) ? priv->sh->ctx->device->name : \"\"));\n\t/* In case mlx5_dev_stop() has not been called. */\n\tmlx5_dev_interrupt_handler_uninstall(dev);\n\tmlx5_traffic_disable(dev);\n\tmlx5_flow_flush(dev, NULL);\n\t/* Prevent crashes when queues are still in use. */\n\tdev->rx_pkt_burst = removed_rx_burst;\n\tdev->tx_pkt_burst = removed_tx_burst;\n\trte_wmb();\n\t/* Disable datapath on secondary process. */\n\tmlx5_mp_req_stop_rxtx(dev);\n\tif (priv->rxqs != NULL) {\n\t\t/* XXX race condition if mlx5_rx_burst() is still running. */\n\t\tusleep(1000);\n\t\tfor (i = 0; (i != priv->rxqs_n); ++i)\n\t\t\tmlx5_rxq_release(dev, i);\n\t\tpriv->rxqs_n = 0;\n\t\tpriv->rxqs = NULL;\n\t}\n\tif (priv->txqs != NULL) {\n\t\t/* XXX race condition if mlx5_tx_burst() is still running. */\n\t\tusleep(1000);\n\t\tfor (i = 0; (i != priv->txqs_n); ++i)\n\t\t\tmlx5_txq_release(dev, i);\n\t\tpriv->txqs_n = 0;\n\t\tpriv->txqs = NULL;\n\t}\n\tmlx5_proc_priv_uninit(dev);\n\tmlx5_mprq_free_mp(dev);\n\t/* Remove from memory callback device list. */\n\trte_rwlock_write_lock(&mlx5_shared_data->mem_event_rwlock);\n\tassert(priv->sh);\n\tLIST_REMOVE(priv->sh, mem_event_cb);\n\trte_rwlock_write_unlock(&mlx5_shared_data->mem_event_rwlock);\n\tmlx5_free_shared_dr(priv);\n\tif (priv->rss_conf.rss_key != NULL)\n\t\trte_free(priv->rss_conf.rss_key);\n\tif (priv->reta_idx != NULL)\n\t\trte_free(priv->reta_idx);\n\tif (priv->config.vf)\n\t\tmlx5_nl_mac_addr_flush(dev);\n\tif (priv->nl_socket_route >= 0)\n\t\tclose(priv->nl_socket_route);\n\tif (priv->nl_socket_rdma >= 0)\n\t\tclose(priv->nl_socket_rdma);\n\tif (priv->tcf_context)\n\t\tmlx5_flow_tcf_context_destroy(priv->tcf_context);\n\tif (priv->sh) {\n\t\t/*\n\t\t * Free the shared context in last turn, because the cleanup\n\t\t * routines above may use some shared fields, like\n\t\t * mlx5_nl_mac_addr_flush() uses ibdev_path for retrieveing\n\t\t * ifindex if Netlink fails.\n\t\t */\n\t\tmlx5_free_shared_ibctx(priv->sh);\n\t\tpriv->sh = NULL;\n\t}\n\tret = mlx5_hrxq_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some hash Rx queue still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_ind_table_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some indirection table still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_rxq_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Verbs Rx queue still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_rxq_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Rx queues still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_txq_ibv_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Verbs Tx queue still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_txq_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some Tx queues still remain\",\n\t\t\tdev->data->port_id);\n\tret = mlx5_flow_verify(dev);\n\tif (ret)\n\t\tDRV_LOG(WARNING, \"port %u some flows still remain\",\n\t\t\tdev->data->port_id);\n\tif (priv->domain_id != RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID) {\n\t\tunsigned int c = 0;\n\t\tuint16_t port_id;\n\n\t\tRTE_ETH_FOREACH_DEV_OF(port_id, dev->device) {\n\t\t\tstruct mlx5_priv *opriv =\n\t\t\t\trte_eth_devices[port_id].data->dev_private;\n\n\t\t\tif (!opriv ||\n\t\t\t    opriv->domain_id != priv->domain_id ||\n\t\t\t    &rte_eth_devices[port_id] == dev)\n\t\t\t\tcontinue;\n\t\t\t++c;\n\t\t}\n\t\tif (!c)\n\t\t\tclaim_zero(rte_eth_switch_domain_free(priv->domain_id));\n\t}\n\tmemset(priv, 0, sizeof(*priv));\n\tpriv->domain_id = RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID;\n\t/*\n\t * Reset mac_addrs to NULL such that it is not freed as part of\n\t * rte_eth_dev_release_port(). mac_addrs is part of dev_private so\n\t * it is freed when dev_private is freed.\n\t */\n\tdev->data->mac_addrs = NULL;\n}\n\nconst struct eth_dev_ops mlx5_dev_ops = {\n\t.dev_configure = mlx5_dev_configure,\n\t.dev_start = mlx5_dev_start,\n\t.dev_stop = mlx5_dev_stop,\n\t.dev_set_link_down = mlx5_set_link_down,\n\t.dev_set_link_up = mlx5_set_link_up,\n\t.dev_close = mlx5_dev_close,\n\t.promiscuous_enable = mlx5_promiscuous_enable,\n\t.promiscuous_disable = mlx5_promiscuous_disable,\n\t.allmulticast_enable = mlx5_allmulticast_enable,\n\t.allmulticast_disable = mlx5_allmulticast_disable,\n\t.link_update = mlx5_link_update,\n\t.stats_get = mlx5_stats_get,\n\t.stats_reset = mlx5_stats_reset,\n\t.xstats_get = mlx5_xstats_get,\n\t.xstats_reset = mlx5_xstats_reset,\n\t.xstats_get_names = mlx5_xstats_get_names,\n\t.fw_version_get = mlx5_fw_version_get,\n\t.dev_infos_get = mlx5_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx5_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx5_vlan_filter_set,\n\t.rx_queue_setup = mlx5_rx_queue_setup,\n\t.tx_queue_setup = mlx5_tx_queue_setup,\n\t.rx_queue_release = mlx5_rx_queue_release,\n\t.tx_queue_release = mlx5_tx_queue_release,\n\t.flow_ctrl_get = mlx5_dev_get_flow_ctrl,\n\t.flow_ctrl_set = mlx5_dev_set_flow_ctrl,\n\t.mac_addr_remove = mlx5_mac_addr_remove,\n\t.mac_addr_add = mlx5_mac_addr_add,\n\t.mac_addr_set = mlx5_mac_addr_set,\n\t.set_mc_addr_list = mlx5_set_mc_addr_list,\n\t.mtu_set = mlx5_dev_set_mtu,\n\t.vlan_strip_queue_set = mlx5_vlan_strip_queue_set,\n\t.vlan_offload_set = mlx5_vlan_offload_set,\n\t.reta_update = mlx5_dev_rss_reta_update,\n\t.reta_query = mlx5_dev_rss_reta_query,\n\t.rss_hash_update = mlx5_rss_hash_update,\n\t.rss_hash_conf_get = mlx5_rss_hash_conf_get,\n\t.filter_ctrl = mlx5_dev_filter_ctrl,\n\t.rx_descriptor_status = mlx5_rx_descriptor_status,\n\t.tx_descriptor_status = mlx5_tx_descriptor_status,\n\t.rx_queue_count = mlx5_rx_queue_count,\n\t.rx_queue_intr_enable = mlx5_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx5_rx_intr_disable,\n\t.is_removed = mlx5_is_removed,\n};\n\n/* Available operations from secondary process. */\nstatic const struct eth_dev_ops mlx5_dev_sec_ops = {\n\t.stats_get = mlx5_stats_get,\n\t.stats_reset = mlx5_stats_reset,\n\t.xstats_get = mlx5_xstats_get,\n\t.xstats_reset = mlx5_xstats_reset,\n\t.xstats_get_names = mlx5_xstats_get_names,\n\t.fw_version_get = mlx5_fw_version_get,\n\t.dev_infos_get = mlx5_dev_infos_get,\n\t.rx_descriptor_status = mlx5_rx_descriptor_status,\n\t.tx_descriptor_status = mlx5_tx_descriptor_status,\n};\n\n/* Available operations in flow isolated mode. */\nconst struct eth_dev_ops mlx5_dev_ops_isolate = {\n\t.dev_configure = mlx5_dev_configure,\n\t.dev_start = mlx5_dev_start,\n\t.dev_stop = mlx5_dev_stop,\n\t.dev_set_link_down = mlx5_set_link_down,\n\t.dev_set_link_up = mlx5_set_link_up,\n\t.dev_close = mlx5_dev_close,\n\t.promiscuous_enable = mlx5_promiscuous_enable,\n\t.promiscuous_disable = mlx5_promiscuous_disable,\n\t.allmulticast_enable = mlx5_allmulticast_enable,\n\t.allmulticast_disable = mlx5_allmulticast_disable,\n\t.link_update = mlx5_link_update,\n\t.stats_get = mlx5_stats_get,\n\t.stats_reset = mlx5_stats_reset,\n\t.xstats_get = mlx5_xstats_get,\n\t.xstats_reset = mlx5_xstats_reset,\n\t.xstats_get_names = mlx5_xstats_get_names,\n\t.fw_version_get = mlx5_fw_version_get,\n\t.dev_infos_get = mlx5_dev_infos_get,\n\t.dev_supported_ptypes_get = mlx5_dev_supported_ptypes_get,\n\t.vlan_filter_set = mlx5_vlan_filter_set,\n\t.rx_queue_setup = mlx5_rx_queue_setup,\n\t.tx_queue_setup = mlx5_tx_queue_setup,\n\t.rx_queue_release = mlx5_rx_queue_release,\n\t.tx_queue_release = mlx5_tx_queue_release,\n\t.flow_ctrl_get = mlx5_dev_get_flow_ctrl,\n\t.flow_ctrl_set = mlx5_dev_set_flow_ctrl,\n\t.mac_addr_remove = mlx5_mac_addr_remove,\n\t.mac_addr_add = mlx5_mac_addr_add,\n\t.mac_addr_set = mlx5_mac_addr_set,\n\t.set_mc_addr_list = mlx5_set_mc_addr_list,\n\t.mtu_set = mlx5_dev_set_mtu,\n\t.vlan_strip_queue_set = mlx5_vlan_strip_queue_set,\n\t.vlan_offload_set = mlx5_vlan_offload_set,\n\t.filter_ctrl = mlx5_dev_filter_ctrl,\n\t.rx_descriptor_status = mlx5_rx_descriptor_status,\n\t.tx_descriptor_status = mlx5_tx_descriptor_status,\n\t.rx_queue_intr_enable = mlx5_rx_intr_enable,\n\t.rx_queue_intr_disable = mlx5_rx_intr_disable,\n\t.is_removed = mlx5_is_removed,\n};\n\n/**\n * Verify and store value for device argument.\n *\n * @param[in] key\n *   Key argument to verify.\n * @param[in] val\n *   Value associated with key.\n * @param opaque\n *   User data.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_args_check(const char *key, const char *val, void *opaque)\n{\n\tstruct mlx5_dev_config *config = opaque;\n\tunsigned long tmp;\n\n\t/* No-op, port representors are processed in mlx5_dev_spawn(). */\n\tif (!strcmp(MLX5_REPRESENTOR, key))\n\t\treturn 0;\n\terrno = 0;\n\ttmp = strtoul(val, NULL, 0);\n\tif (errno) {\n\t\trte_errno = errno;\n\t\tDRV_LOG(WARNING, \"%s: \\\"%s\\\" is not a valid integer\", key, val);\n\t\treturn -rte_errno;\n\t}\n\tif (strcmp(MLX5_RXQ_CQE_COMP_EN, key) == 0) {\n\t\tconfig->cqe_comp = !!tmp;\n\t} else if (strcmp(MLX5_RXQ_CQE_PAD_EN, key) == 0) {\n\t\tconfig->cqe_pad = !!tmp;\n\t} else if (strcmp(MLX5_RXQ_PKT_PAD_EN, key) == 0) {\n\t\tconfig->hw_padding = !!tmp;\n\t} else if (strcmp(MLX5_RX_MPRQ_EN, key) == 0) {\n\t\tconfig->mprq.enabled = !!tmp;\n\t} else if (strcmp(MLX5_RX_MPRQ_LOG_STRIDE_NUM, key) == 0) {\n\t\tconfig->mprq.stride_num_n = tmp;\n\t} else if (strcmp(MLX5_RX_MPRQ_MAX_MEMCPY_LEN, key) == 0) {\n\t\tconfig->mprq.max_memcpy_len = tmp;\n\t} else if (strcmp(MLX5_RXQS_MIN_MPRQ, key) == 0) {\n\t\tconfig->mprq.min_rxqs_num = tmp;\n\t} else if (strcmp(MLX5_TXQ_INLINE, key) == 0) {\n\t\tconfig->txq_inline = tmp;\n\t} else if (strcmp(MLX5_TXQS_MIN_INLINE, key) == 0) {\n\t\tconfig->txqs_inline = tmp;\n\t} else if (strcmp(MLX5_TXQS_MAX_VEC, key) == 0) {\n\t\tconfig->txqs_vec = tmp;\n\t} else if (strcmp(MLX5_TXQ_MPW_EN, key) == 0) {\n\t\tconfig->mps = !!tmp;\n\t} else if (strcmp(MLX5_TXQ_MPW_HDR_DSEG_EN, key) == 0) {\n\t\tconfig->mpw_hdr_dseg = !!tmp;\n\t} else if (strcmp(MLX5_TXQ_MAX_INLINE_LEN, key) == 0) {\n\t\tconfig->inline_max_packet_sz = tmp;\n\t} else if (strcmp(MLX5_TX_VEC_EN, key) == 0) {\n\t\tconfig->tx_vec_en = !!tmp;\n\t} else if (strcmp(MLX5_RX_VEC_EN, key) == 0) {\n\t\tconfig->rx_vec_en = !!tmp;\n\t} else if (strcmp(MLX5_L3_VXLAN_EN, key) == 0) {\n\t\tconfig->l3_vxlan_en = !!tmp;\n\t} else if (strcmp(MLX5_VF_NL_EN, key) == 0) {\n\t\tconfig->vf_nl_en = !!tmp;\n\t} else if (strcmp(MLX5_DV_ESW_EN, key) == 0) {\n\t\tconfig->dv_esw_en = !!tmp;\n\t} else if (strcmp(MLX5_DV_FLOW_EN, key) == 0) {\n\t\tconfig->dv_flow_en = !!tmp;\n\t} else if (strcmp(MLX5_MR_EXT_MEMSEG_EN, key) == 0) {\n\t\tconfig->mr_ext_memseg_en = !!tmp;\n\t} else {\n\t\tDRV_LOG(WARNING, \"%s: unknown parameter\", key);\n\t\trte_errno = EINVAL;\n\t\treturn -rte_errno;\n\t}\n\treturn 0;\n}\n\n/**\n * Parse device parameters.\n *\n * @param config\n *   Pointer to device configuration structure.\n * @param devargs\n *   Device arguments structure.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_args(struct mlx5_dev_config *config, struct rte_devargs *devargs)\n{\n\tconst char **params = (const char *[]){\n\t\tMLX5_RXQ_CQE_COMP_EN,\n\t\tMLX5_RXQ_CQE_PAD_EN,\n\t\tMLX5_RXQ_PKT_PAD_EN,\n\t\tMLX5_RX_MPRQ_EN,\n\t\tMLX5_RX_MPRQ_LOG_STRIDE_NUM,\n\t\tMLX5_RX_MPRQ_MAX_MEMCPY_LEN,\n\t\tMLX5_RXQS_MIN_MPRQ,\n\t\tMLX5_TXQ_INLINE,\n\t\tMLX5_TXQS_MIN_INLINE,\n\t\tMLX5_TXQS_MAX_VEC,\n\t\tMLX5_TXQ_MPW_EN,\n\t\tMLX5_TXQ_MPW_HDR_DSEG_EN,\n\t\tMLX5_TXQ_MAX_INLINE_LEN,\n\t\tMLX5_TX_VEC_EN,\n\t\tMLX5_RX_VEC_EN,\n\t\tMLX5_L3_VXLAN_EN,\n\t\tMLX5_VF_NL_EN,\n\t\tMLX5_DV_ESW_EN,\n\t\tMLX5_DV_FLOW_EN,\n\t\tMLX5_MR_EXT_MEMSEG_EN,\n\t\tMLX5_REPRESENTOR,\n\t\tNULL,\n\t};\n\tstruct rte_kvargs *kvlist;\n\tint ret = 0;\n\tint i;\n\n\tif (devargs == NULL)\n\t\treturn 0;\n\t/* Following UGLY cast is done to pass checkpatch. */\n\tkvlist = rte_kvargs_parse(devargs->args, params);\n\tif (kvlist == NULL)\n\t\treturn 0;\n\t/* Process parameters. */\n\tfor (i = 0; (params[i] != NULL); ++i) {\n\t\tif (rte_kvargs_count(kvlist, params[i])) {\n\t\t\tret = rte_kvargs_process(kvlist, params[i],\n\t\t\t\t\t\t mlx5_args_check, config);\n\t\t\tif (ret) {\n\t\t\t\trte_errno = EINVAL;\n\t\t\t\trte_kvargs_free(kvlist);\n\t\t\t\treturn -rte_errno;\n\t\t\t}\n\t\t}\n\t}\n\trte_kvargs_free(kvlist);\n\treturn 0;\n}\n\nstatic struct rte_pci_driver mlx5_driver;\n\n/**\n * PMD global initialization.\n *\n * Independent from individual device, this function initializes global\n * per-PMD data structures distinguishing primary and secondary processes.\n * Hence, each initialization is called once per a process.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_init_once(void)\n{\n\tstruct mlx5_shared_data *sd;\n\tstruct mlx5_local_data *ld = &mlx5_local_data;\n\n\tif (mlx5_init_shared_data())\n\t\treturn -rte_errno;\n\tsd = mlx5_shared_data;\n\tassert(sd);\n\trte_spinlock_lock(&sd->lock);\n\tswitch (rte_eal_process_type()) {\n\tcase RTE_PROC_PRIMARY:\n\t\tif (sd->init_done)\n\t\t\tbreak;\n\t\tLIST_INIT(&sd->mem_event_cb_list);\n\t\trte_rwlock_init(&sd->mem_event_rwlock);\n\t\trte_mem_event_callback_register(\"MLX5_MEM_EVENT_CB\",\n\t\t\t\t\t\tmlx5_mr_mem_event_cb, NULL);\n\t\tmlx5_mp_init_primary();\n\t\tsd->init_done = true;\n\t\tbreak;\n\tcase RTE_PROC_SECONDARY:\n\t\tif (ld->init_done)\n\t\t\tbreak;\n\t\tmlx5_mp_init_secondary();\n\t\t++sd->secondary_cnt;\n\t\tld->init_done = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\trte_spinlock_unlock(&sd->lock);\n\treturn 0;\n}\n\n/**\n * Spawn an Ethernet device from Verbs information.\n *\n * @param dpdk_dev\n *   Backing DPDK device.\n * @param spawn\n *   Verbs device parameters (name, port, switch_info) to spawn.\n * @param config\n *   Device configuration parameters.\n *\n * @return\n *   A valid Ethernet device object on success, NULL otherwise and rte_errno\n *   is set. The following errors are defined:\n *\n *   EBUSY: device is not supposed to be spawned.\n *   EEXIST: device is already spawned\n */\nstatic struct rte_eth_dev *\nmlx5_dev_spawn(struct rte_device *dpdk_dev,\n\t       struct mlx5_dev_spawn_data *spawn,\n\t       struct mlx5_dev_config config)\n{\n\tconst struct mlx5_switch_info *switch_info = &spawn->info;\n\tstruct mlx5_ibv_shared *sh = NULL;\n\tstruct ibv_port_attr port_attr;\n\tstruct mlx5dv_context dv_attr = { .comp_mask = 0 };\n\tstruct rte_eth_dev *eth_dev = NULL;\n\tstruct mlx5_priv *priv = NULL;\n\tint err = 0;\n\tunsigned int hw_padding = 0;\n\tunsigned int mps;\n\tunsigned int cqe_comp;\n\tunsigned int cqe_pad = 0;\n\tunsigned int tunnel_en = 0;\n\tunsigned int mpls_en = 0;\n\tunsigned int swp = 0;\n\tunsigned int mprq = 0;\n\tunsigned int mprq_min_stride_size_n = 0;\n\tunsigned int mprq_max_stride_size_n = 0;\n\tunsigned int mprq_min_stride_num_n = 0;\n\tunsigned int mprq_max_stride_num_n = 0;\n\tstruct ether_addr mac;\n\tchar name[RTE_ETH_NAME_MAX_LEN];\n\tint own_domain_id = 0;\n\tuint16_t port_id;\n\tunsigned int i;\n\n\t/* Determine if this port representor is supposed to be spawned. */\n\tif (switch_info->representor && dpdk_dev->devargs) {\n\t\tstruct rte_eth_devargs eth_da;\n\n\t\terr = rte_eth_devargs_parse(dpdk_dev->devargs->args, &eth_da);\n\t\tif (err) {\n\t\t\trte_errno = -err;\n\t\t\tDRV_LOG(ERR, \"failed to process device arguments: %s\",\n\t\t\t\tstrerror(rte_errno));\n\t\t\treturn NULL;\n\t\t}\n\t\tfor (i = 0; i < eth_da.nb_representor_ports; ++i)\n\t\t\tif (eth_da.representor_ports[i] ==\n\t\t\t    (uint16_t)switch_info->port_name)\n\t\t\t\tbreak;\n\t\tif (i == eth_da.nb_representor_ports) {\n\t\t\trte_errno = EBUSY;\n\t\t\treturn NULL;\n\t\t}\n\t}\n\t/* Build device name. */\n\tif (!switch_info->representor)\n\t\tstrlcpy(name, dpdk_dev->name, sizeof(name));\n\telse\n\t\tsnprintf(name, sizeof(name), \"%s_representor_%u\",\n\t\t\t dpdk_dev->name, switch_info->port_name);\n\t/* check if the device is already spawned */\n\tif (rte_eth_dev_get_port_by_name(name, &port_id) == 0) {\n\t\trte_errno = EEXIST;\n\t\treturn NULL;\n\t}\n\tDRV_LOG(DEBUG, \"naming Ethernet device \\\"%s\\\"\", name);\n\tif (rte_eal_process_type() == RTE_PROC_SECONDARY) {\n\t\teth_dev = rte_eth_dev_attach_secondary(name);\n\t\tif (eth_dev == NULL) {\n\t\t\tDRV_LOG(ERR, \"can not attach rte ethdev\");\n\t\t\trte_errno = ENOMEM;\n\t\t\treturn NULL;\n\t\t}\n\t\teth_dev->device = dpdk_dev;\n\t\teth_dev->dev_ops = &mlx5_dev_sec_ops;\n\t\terr = mlx5_proc_priv_init(eth_dev);\n\t\tif (err)\n\t\t\treturn NULL;\n\t\t/* Receive command fd from primary process */\n\t\terr = mlx5_mp_req_verbs_cmd_fd(eth_dev);\n\t\tif (err < 0)\n\t\t\treturn NULL;\n\t\t/* Remap UAR for Tx queues. */\n\t\terr = mlx5_tx_uar_init_secondary(eth_dev, err);\n\t\tif (err)\n\t\t\treturn NULL;\n\t\t/*\n\t\t * Ethdev pointer is still required as input since\n\t\t * the primary device is not accessible from the\n\t\t * secondary process.\n\t\t */\n\t\teth_dev->rx_pkt_burst = mlx5_select_rx_function(eth_dev);\n\t\teth_dev->tx_pkt_burst = mlx5_select_tx_function(eth_dev);\n\t\treturn eth_dev;\n\t}\n\tsh = mlx5_alloc_shared_ibctx(spawn);\n\tif (!sh)\n\t\treturn NULL;\n\tconfig.devx = sh->devx;\n#ifdef HAVE_IBV_MLX5_MOD_SWP\n\tdv_attr.comp_mask |= MLX5DV_CONTEXT_MASK_SWP;\n#endif\n\t/*\n\t * Multi-packet send is supported by ConnectX-4 Lx PF as well\n\t * as all ConnectX-5 devices.\n\t */\n#ifdef HAVE_IBV_DEVICE_TUNNEL_SUPPORT\n\tdv_attr.comp_mask |= MLX5DV_CONTEXT_MASK_TUNNEL_OFFLOADS;\n#endif\n#ifdef HAVE_IBV_DEVICE_STRIDING_RQ_SUPPORT\n\tdv_attr.comp_mask |= MLX5DV_CONTEXT_MASK_STRIDING_RQ;\n#endif\n\tmlx5_glue->dv_query_device(sh->ctx, &dv_attr);\n\tif (dv_attr.flags & MLX5DV_CONTEXT_FLAGS_MPW_ALLOWED) {\n\t\tif (dv_attr.flags & MLX5DV_CONTEXT_FLAGS_ENHANCED_MPW) {\n\t\t\tDRV_LOG(DEBUG, \"enhanced MPW is supported\");\n\t\t\tmps = MLX5_MPW_ENHANCED;\n\t\t} else {\n\t\t\tDRV_LOG(DEBUG, \"MPW is supported\");\n\t\t\tmps = MLX5_MPW;\n\t\t}\n\t} else {\n\t\tDRV_LOG(DEBUG, \"MPW isn't supported\");\n\t\tmps = MLX5_MPW_DISABLED;\n\t}\n#ifdef HAVE_IBV_MLX5_MOD_SWP\n\tif (dv_attr.comp_mask & MLX5DV_CONTEXT_MASK_SWP)\n\t\tswp = dv_attr.sw_parsing_caps.sw_parsing_offloads;\n\tDRV_LOG(DEBUG, \"SWP support: %u\", swp);\n#endif\n\tconfig.swp = !!swp;\n#ifdef HAVE_IBV_DEVICE_STRIDING_RQ_SUPPORT\n\tif (dv_attr.comp_mask & MLX5DV_CONTEXT_MASK_STRIDING_RQ) {\n\t\tstruct mlx5dv_striding_rq_caps mprq_caps =\n\t\t\tdv_attr.striding_rq_caps;\n\n\t\tDRV_LOG(DEBUG, \"\\tmin_single_stride_log_num_of_bytes: %d\",\n\t\t\tmprq_caps.min_single_stride_log_num_of_bytes);\n\t\tDRV_LOG(DEBUG, \"\\tmax_single_stride_log_num_of_bytes: %d\",\n\t\t\tmprq_caps.max_single_stride_log_num_of_bytes);\n\t\tDRV_LOG(DEBUG, \"\\tmin_single_wqe_log_num_of_strides: %d\",\n\t\t\tmprq_caps.min_single_wqe_log_num_of_strides);\n\t\tDRV_LOG(DEBUG, \"\\tmax_single_wqe_log_num_of_strides: %d\",\n\t\t\tmprq_caps.max_single_wqe_log_num_of_strides);\n\t\tDRV_LOG(DEBUG, \"\\tsupported_qpts: %d\",\n\t\t\tmprq_caps.supported_qpts);\n\t\tDRV_LOG(DEBUG, \"device supports Multi-Packet RQ\");\n\t\tmprq = 1;\n\t\tmprq_min_stride_size_n =\n\t\t\tmprq_caps.min_single_stride_log_num_of_bytes;\n\t\tmprq_max_stride_size_n =\n\t\t\tmprq_caps.max_single_stride_log_num_of_bytes;\n\t\tmprq_min_stride_num_n =\n\t\t\tmprq_caps.min_single_wqe_log_num_of_strides;\n\t\tmprq_max_stride_num_n =\n\t\t\tmprq_caps.max_single_wqe_log_num_of_strides;\n\t\tconfig.mprq.stride_num_n = RTE_MAX(MLX5_MPRQ_STRIDE_NUM_N,\n\t\t\t\t\t\t   mprq_min_stride_num_n);\n\t}\n#endif\n\tif (RTE_CACHE_LINE_SIZE == 128 &&\n\t    !(dv_attr.flags & MLX5DV_CONTEXT_FLAGS_CQE_128B_COMP))\n\t\tcqe_comp = 0;\n\telse\n\t\tcqe_comp = 1;\n\tconfig.cqe_comp = cqe_comp;\n#ifdef HAVE_IBV_MLX5_MOD_CQE_128B_PAD\n\t/* Whether device supports 128B Rx CQE padding. */\n\tcqe_pad = RTE_CACHE_LINE_SIZE == 128 &&\n\t\t  (dv_attr.flags & MLX5DV_CONTEXT_FLAGS_CQE_128B_PAD);\n#endif\n#ifdef HAVE_IBV_DEVICE_TUNNEL_SUPPORT\n\tif (dv_attr.comp_mask & MLX5DV_CONTEXT_MASK_TUNNEL_OFFLOADS) {\n\t\ttunnel_en = ((dv_attr.tunnel_offloads_caps &\n\t\t\t      MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_VXLAN) &&\n\t\t\t     (dv_attr.tunnel_offloads_caps &\n\t\t\t      MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_GRE));\n\t}\n\tDRV_LOG(DEBUG, \"tunnel offloading is %ssupported\",\n\t\ttunnel_en ? \"\" : \"not \");\n#else\n\tDRV_LOG(WARNING,\n\t\t\"tunnel offloading disabled due to old OFED/rdma-core version\");\n#endif\n\tconfig.tunnel_en = tunnel_en;\n#ifdef HAVE_IBV_DEVICE_MPLS_SUPPORT\n\tmpls_en = ((dv_attr.tunnel_offloads_caps &\n\t\t    MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_CW_MPLS_OVER_GRE) &&\n\t\t   (dv_attr.tunnel_offloads_caps &\n\t\t    MLX5DV_RAW_PACKET_CAP_TUNNELED_OFFLOAD_CW_MPLS_OVER_UDP));\n\tDRV_LOG(DEBUG, \"MPLS over GRE/UDP tunnel offloading is %ssupported\",\n\t\tmpls_en ? \"\" : \"not \");\n#else\n\tDRV_LOG(WARNING, \"MPLS over GRE/UDP tunnel offloading disabled due to\"\n\t\t\" old OFED/rdma-core version or firmware configuration\");\n#endif\n\tconfig.mpls_en = mpls_en;\n\t/* Check port status. */\n\terr = mlx5_glue->query_port(sh->ctx, spawn->ibv_port, &port_attr);\n\tif (err) {\n\t\tDRV_LOG(ERR, \"port query failed: %s\", strerror(err));\n\t\tgoto error;\n\t}\n\tif (port_attr.link_layer != IBV_LINK_LAYER_ETHERNET) {\n\t\tDRV_LOG(ERR, \"port is not configured in Ethernet mode\");\n\t\terr = EINVAL;\n\t\tgoto error;\n\t}\n\tif (port_attr.state != IBV_PORT_ACTIVE)\n\t\tDRV_LOG(DEBUG, \"port is not active: \\\"%s\\\" (%d)\",\n\t\t\tmlx5_glue->port_state_str(port_attr.state),\n\t\t\tport_attr.state);\n\t/* Allocate private eth device data. */\n\tpriv = rte_zmalloc(\"ethdev private structure\",\n\t\t\t   sizeof(*priv),\n\t\t\t   RTE_CACHE_LINE_SIZE);\n\tif (priv == NULL) {\n\t\tDRV_LOG(ERR, \"priv allocation failure\");\n\t\terr = ENOMEM;\n\t\tgoto error;\n\t}\n\tpriv->sh = sh;\n\tpriv->ibv_port = spawn->ibv_port;\n\tpriv->mtu = ETHER_MTU;\n#ifndef RTE_ARCH_64\n\t/* Initialize UAR access locks for 32bit implementations. */\n\trte_spinlock_init(&priv->uar_lock_cq);\n\tfor (i = 0; i < MLX5_UAR_PAGE_NUM_MAX; i++)\n\t\trte_spinlock_init(&priv->uar_lock[i]);\n#endif\n\t/* Some internal functions rely on Netlink sockets, open them now. */\n\tpriv->nl_socket_rdma = mlx5_nl_init(NETLINK_RDMA);\n\tpriv->nl_socket_route =\tmlx5_nl_init(NETLINK_ROUTE);\n\tpriv->nl_sn = 0;\n\tpriv->representor = !!switch_info->representor;\n\tpriv->master = !!switch_info->master;\n\tpriv->domain_id = RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID;\n\t/*\n\t * Currently we support single E-Switch per PF configurations\n\t * only and vport_id field contains the vport index for\n\t * associated VF, which is deduced from representor port name.\n\t * For example, let's have the IB device port 10, it has\n\t * attached network device eth0, which has port name attribute\n\t * pf0vf2, we can deduce the VF number as 2, and set vport index\n\t * as 3 (2+1). This assigning schema should be changed if the\n\t * multiple E-Switch instances per PF configurations or/and PCI\n\t * subfunctions are added.\n\t */\n\tpriv->vport_id = switch_info->representor ?\n\t\t\t switch_info->port_name + 1 : -1;\n\t/* representor_id field keeps the unmodified port/VF index. */\n\tpriv->representor_id = switch_info->representor ?\n\t\t\t       switch_info->port_name : -1;\n\t/*\n\t * Look for sibling devices in order to reuse their switch domain\n\t * if any, otherwise allocate one.\n\t */\n\tRTE_ETH_FOREACH_DEV_OF(port_id, dpdk_dev) {\n\t\tconst struct mlx5_priv *opriv =\n\t\t\trte_eth_devices[port_id].data->dev_private;\n\n\t\tif (!opriv ||\n\t\t\topriv->domain_id ==\n\t\t\tRTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID)\n\t\t\tcontinue;\n\t\tpriv->domain_id = opriv->domain_id;\n\t\tbreak;\n\t}\n\tif (priv->domain_id == RTE_ETH_DEV_SWITCH_DOMAIN_ID_INVALID) {\n\t\terr = rte_eth_switch_domain_alloc(&priv->domain_id);\n\t\tif (err) {\n\t\t\terr = rte_errno;\n\t\t\tDRV_LOG(ERR, \"unable to allocate switch domain: %s\",\n\t\t\t\tstrerror(rte_errno));\n\t\t\tgoto error;\n\t\t}\n\t\town_domain_id = 1;\n\t}\n\terr = mlx5_args(&config, dpdk_dev->devargs);\n\tif (err) {\n\t\terr = rte_errno;\n\t\tDRV_LOG(ERR, \"failed to process device arguments: %s\",\n\t\t\tstrerror(rte_errno));\n\t\tgoto error;\n\t}\n\tconfig.hw_csum = !!(sh->device_attr.device_cap_flags_ex &\n\t\t\t    IBV_DEVICE_RAW_IP_CSUM);\n\tDRV_LOG(DEBUG, \"checksum offloading is %ssupported\",\n\t\t(config.hw_csum ? \"\" : \"not \"));\n#if !defined(HAVE_IBV_DEVICE_COUNTERS_SET_V42) && \\\n\t!defined(HAVE_IBV_DEVICE_COUNTERS_SET_V45)\n\tDRV_LOG(DEBUG, \"counters are not supported\");\n#endif\n#ifndef HAVE_IBV_FLOW_DV_SUPPORT\n\tif (config.dv_flow_en) {\n\t\tDRV_LOG(WARNING, \"DV flow is not supported\");\n\t\tconfig.dv_flow_en = 0;\n\t}\n#endif\n\tconfig.ind_table_max_size =\n\t\tsh->device_attr.rss_caps.max_rwq_indirection_table_size;\n\t/*\n\t * Remove this check once DPDK supports larger/variable\n\t * indirection tables.\n\t */\n\tif (config.ind_table_max_size > (unsigned int)ETH_RSS_RETA_SIZE_512)\n\t\tconfig.ind_table_max_size = ETH_RSS_RETA_SIZE_512;\n\tDRV_LOG(DEBUG, \"maximum Rx indirection table size is %u\",\n\t\tconfig.ind_table_max_size);\n\tconfig.hw_vlan_strip = !!(sh->device_attr.raw_packet_caps &\n\t\t\t\t  IBV_RAW_PACKET_CAP_CVLAN_STRIPPING);\n\tDRV_LOG(DEBUG, \"VLAN stripping is %ssupported\",\n\t\t(config.hw_vlan_strip ? \"\" : \"not \"));\n\tconfig.hw_fcs_strip = !!(sh->device_attr.raw_packet_caps &\n\t\t\t\t IBV_RAW_PACKET_CAP_SCATTER_FCS);\n\tDRV_LOG(DEBUG, \"FCS stripping configuration is %ssupported\",\n\t\t(config.hw_fcs_strip ? \"\" : \"not \"));\n#if defined(HAVE_IBV_WQ_FLAG_RX_END_PADDING)\n\thw_padding = !!sh->device_attr.rx_pad_end_addr_align;\n#elif defined(HAVE_IBV_WQ_FLAGS_PCI_WRITE_END_PADDING)\n\thw_padding = !!(sh->device_attr.device_cap_flags_ex &\n\t\t\tIBV_DEVICE_PCI_WRITE_END_PADDING);\n#endif\n\tif (config.hw_padding && !hw_padding) {\n\t\tDRV_LOG(DEBUG, \"Rx end alignment padding isn't supported\");\n\t\tconfig.hw_padding = 0;\n\t} else if (config.hw_padding) {\n\t\tDRV_LOG(DEBUG, \"Rx end alignment padding is enabled\");\n\t}\n\tconfig.tso = (sh->device_attr.tso_caps.max_tso > 0 &&\n\t\t      (sh->device_attr.tso_caps.supported_qpts &\n\t\t       (1 << IBV_QPT_RAW_PACKET)));\n\tif (config.tso)\n\t\tconfig.tso_max_payload_sz = sh->device_attr.tso_caps.max_tso;\n\t/*\n\t * MPW is disabled by default, while the Enhanced MPW is enabled\n\t * by default.\n\t */\n\tif (config.mps == MLX5_ARG_UNSET)\n\t\tconfig.mps = (mps == MLX5_MPW_ENHANCED) ? MLX5_MPW_ENHANCED :\n\t\t\t\t\t\t\t  MLX5_MPW_DISABLED;\n\telse\n\t\tconfig.mps = config.mps ? mps : MLX5_MPW_DISABLED;\n\tDRV_LOG(INFO, \"%sMPS is %s\",\n\t\tconfig.mps == MLX5_MPW_ENHANCED ? \"enhanced \" : \"\",\n\t\tconfig.mps != MLX5_MPW_DISABLED ? \"enabled\" : \"disabled\");\n\tif (config.cqe_comp && !cqe_comp) {\n\t\tDRV_LOG(WARNING, \"Rx CQE compression isn't supported\");\n\t\tconfig.cqe_comp = 0;\n\t}\n\tif (config.cqe_pad && !cqe_pad) {\n\t\tDRV_LOG(WARNING, \"Rx CQE padding isn't supported\");\n\t\tconfig.cqe_pad = 0;\n\t} else if (config.cqe_pad) {\n\t\tDRV_LOG(INFO, \"Rx CQE padding is enabled\");\n\t}\n\tif (config.mprq.enabled && mprq) {\n\t\tif (config.mprq.stride_num_n > mprq_max_stride_num_n ||\n\t\t    config.mprq.stride_num_n < mprq_min_stride_num_n) {\n\t\t\tconfig.mprq.stride_num_n =\n\t\t\t\tRTE_MAX(MLX5_MPRQ_STRIDE_NUM_N,\n\t\t\t\t\tmprq_min_stride_num_n);\n\t\t\tDRV_LOG(WARNING,\n\t\t\t\t\"the number of strides\"\n\t\t\t\t\" for Multi-Packet RQ is out of range,\"\n\t\t\t\t\" setting default value (%u)\",\n\t\t\t\t1 << config.mprq.stride_num_n);\n\t\t}\n\t\tconfig.mprq.min_stride_size_n = mprq_min_stride_size_n;\n\t\tconfig.mprq.max_stride_size_n = mprq_max_stride_size_n;\n\t} else if (config.mprq.enabled && !mprq) {\n\t\tDRV_LOG(WARNING, \"Multi-Packet RQ isn't supported\");\n\t\tconfig.mprq.enabled = 0;\n\t}\n\teth_dev = rte_eth_dev_allocate(name);\n\tif (eth_dev == NULL) {\n\t\tDRV_LOG(ERR, \"can not allocate rte ethdev\");\n\t\terr = ENOMEM;\n\t\tgoto error;\n\t}\n\t/* Flag to call rte_eth_dev_release_port() in rte_eth_dev_close(). */\n\teth_dev->data->dev_flags |= RTE_ETH_DEV_CLOSE_REMOVE;\n\tif (priv->representor) {\n\t\teth_dev->data->dev_flags |= RTE_ETH_DEV_REPRESENTOR;\n\t\teth_dev->data->representor_id = priv->representor_id;\n\t}\n\teth_dev->data->dev_private = priv;\n\tpriv->dev_data = eth_dev->data;\n\teth_dev->data->mac_addrs = priv->mac;\n\teth_dev->device = dpdk_dev;\n\t/* Configure the first MAC address by default. */\n\tif (mlx5_get_mac(eth_dev, &mac.addr_bytes)) {\n\t\tDRV_LOG(ERR,\n\t\t\t\"port %u cannot get MAC address, is mlx5_en\"\n\t\t\t\" loaded? (errno: %s)\",\n\t\t\teth_dev->data->port_id, strerror(rte_errno));\n\t\terr = ENODEV;\n\t\tgoto error;\n\t}\n\tDRV_LOG(INFO,\n\t\t\"port %u MAC address is %02x:%02x:%02x:%02x:%02x:%02x\",\n\t\teth_dev->data->port_id,\n\t\tmac.addr_bytes[0], mac.addr_bytes[1],\n\t\tmac.addr_bytes[2], mac.addr_bytes[3],\n\t\tmac.addr_bytes[4], mac.addr_bytes[5]);\n#ifndef NDEBUG\n\t{\n\t\tchar ifname[IF_NAMESIZE];\n\n\t\tif (mlx5_get_ifname(eth_dev, &ifname) == 0)\n\t\t\tDRV_LOG(DEBUG, \"port %u ifname is \\\"%s\\\"\",\n\t\t\t\teth_dev->data->port_id, ifname);\n\t\telse\n\t\t\tDRV_LOG(DEBUG, \"port %u ifname is unknown\",\n\t\t\t\teth_dev->data->port_id);\n\t}\n#endif\n\t/* Get actual MTU if possible. */\n\terr = mlx5_get_mtu(eth_dev, &priv->mtu);\n\tif (err) {\n\t\terr = rte_errno;\n\t\tgoto error;\n\t}\n\tDRV_LOG(DEBUG, \"port %u MTU is %u\", eth_dev->data->port_id,\n\t\tpriv->mtu);\n\t/* Initialize burst functions to prevent crashes before link-up. */\n\teth_dev->rx_pkt_burst = removed_rx_burst;\n\teth_dev->tx_pkt_burst = removed_tx_burst;\n\teth_dev->dev_ops = &mlx5_dev_ops;\n\t/* Register MAC address. */\n\tclaim_zero(mlx5_mac_addr_add(eth_dev, &mac, 0, 0));\n\tif (config.vf && config.vf_nl_en)\n\t\tmlx5_nl_mac_addr_sync(eth_dev);\n\tpriv->tcf_context = mlx5_flow_tcf_context_create();\n\tif (!priv->tcf_context) {\n\t\terr = -rte_errno;\n\t\tDRV_LOG(WARNING,\n\t\t\t\"flow rules relying on switch offloads will not be\"\n\t\t\t\" supported: cannot open libmnl socket: %s\",\n\t\t\tstrerror(rte_errno));\n\t} else {\n\t\tstruct rte_flow_error error;\n\t\tunsigned int ifindex = mlx5_ifindex(eth_dev);\n\n\t\tif (!ifindex) {\n\t\t\terr = -rte_errno;\n\t\t\terror.message =\n\t\t\t\t\"cannot retrieve network interface index\";\n\t\t} else {\n\t\t\terr = mlx5_flow_tcf_init(priv->tcf_context,\n\t\t\t\t\t\t ifindex, &error);\n\t\t}\n\t\tif (err) {\n\t\t\tDRV_LOG(WARNING,\n\t\t\t\t\"flow rules relying on switch offloads will\"\n\t\t\t\t\" not be supported: %s: %s\",\n\t\t\t\terror.message, strerror(rte_errno));\n\t\t\tmlx5_flow_tcf_context_destroy(priv->tcf_context);\n\t\t\tpriv->tcf_context = NULL;\n\t\t}\n\t}\n\tTAILQ_INIT(&priv->flows);\n\tTAILQ_INIT(&priv->ctrl_flows);\n\t/* Hint libmlx5 to use PMD allocator for data plane resources */\n\tstruct mlx5dv_ctx_allocators alctr = {\n\t\t.alloc = &mlx5_alloc_verbs_buf,\n\t\t.free = &mlx5_free_verbs_buf,\n\t\t.data = priv,\n\t};\n\tmlx5_glue->dv_set_context_attr(sh->ctx,\n\t\t\t\t       MLX5DV_CTX_ATTR_BUF_ALLOCATORS,\n\t\t\t\t       (void *)((uintptr_t)&alctr));\n\t/* Bring Ethernet device up. */\n\tDRV_LOG(DEBUG, \"port %u forcing Ethernet interface up\",\n\t\teth_dev->data->port_id);\n\tmlx5_set_link_up(eth_dev);\n\t/*\n\t * Even though the interrupt handler is not installed yet,\n\t * interrupts will still trigger on the async_fd from\n\t * Verbs context returned by ibv_open_device().\n\t */\n\tmlx5_link_update(eth_dev, 0);\n#ifdef HAVE_IBV_DEVX_OBJ\n\tif (config.devx) {\n\t\terr = mlx5_devx_cmd_query_hca_attr(sh->ctx, &config.hca_attr);\n\t\tif (err) {\n\t\t\terr = -err;\n\t\t\tgoto error;\n\t\t}\n\t}\n#endif\n#ifdef HAVE_MLX5DV_DR_ESWITCH\n\tif (!(config.hca_attr.eswitch_manager && config.dv_flow_en &&\n\t      (switch_info->representor || switch_info->master)))\n\t\tconfig.dv_esw_en = 0;\n#else\n\tconfig.dv_esw_en = 0;\n#endif\n\t/* Store device configuration on private structure. */\n\tpriv->config = config;\n\tif (config.dv_flow_en) {\n\t\terr = mlx5_alloc_shared_dr(priv);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\t/* Supported Verbs flow priority number detection. */\n\terr = mlx5_flow_discover_priorities(eth_dev);\n\tif (err < 0) {\n\t\terr = -err;\n\t\tgoto error;\n\t}\n\tpriv->config.flow_prio = err;\n\t/* Add device to memory callback list. */\n\trte_rwlock_write_lock(&mlx5_shared_data->mem_event_rwlock);\n\tLIST_INSERT_HEAD(&mlx5_shared_data->mem_event_cb_list,\n\t\t\t sh, mem_event_cb);\n\trte_rwlock_write_unlock(&mlx5_shared_data->mem_event_rwlock);\n\treturn eth_dev;\nerror:\n\tif (priv) {\n\t\tif (priv->sh)\n\t\t\tmlx5_free_shared_dr(priv);\n\t\tif (priv->nl_socket_route >= 0)\n\t\t\tclose(priv->nl_socket_route);\n\t\tif (priv->nl_socket_rdma >= 0)\n\t\t\tclose(priv->nl_socket_rdma);\n\t\tif (priv->tcf_context)\n\t\t\tmlx5_flow_tcf_context_destroy(priv->tcf_context);\n\t\tif (own_domain_id)\n\t\t\tclaim_zero(rte_eth_switch_domain_free(priv->domain_id));\n\t\trte_free(priv);\n\t\tif (eth_dev != NULL)\n\t\t\teth_dev->data->dev_private = NULL;\n\t}\n\tif (eth_dev != NULL) {\n\t\t/* mac_addrs must not be freed alone because part of dev_private */\n\t\teth_dev->data->mac_addrs = NULL;\n\t\trte_eth_dev_release_port(eth_dev);\n\t}\n\tif (sh)\n\t\tmlx5_free_shared_ibctx(sh);\n\tassert(err > 0);\n\trte_errno = err;\n\treturn NULL;\n}\n\n/**\n * Comparison callback to sort device data.\n *\n * This is meant to be used with qsort().\n *\n * @param a[in]\n *   Pointer to pointer to first data object.\n * @param b[in]\n *   Pointer to pointer to second data object.\n *\n * @return\n *   0 if both objects are equal, less than 0 if the first argument is less\n *   than the second, greater than 0 otherwise.\n */\nstatic int\nmlx5_dev_spawn_data_cmp(const void *a, const void *b)\n{\n\tconst struct mlx5_switch_info *si_a =\n\t\t&((const struct mlx5_dev_spawn_data *)a)->info;\n\tconst struct mlx5_switch_info *si_b =\n\t\t&((const struct mlx5_dev_spawn_data *)b)->info;\n\tint ret;\n\n\t/* Master device first. */\n\tret = si_b->master - si_a->master;\n\tif (ret)\n\t\treturn ret;\n\t/* Then representor devices. */\n\tret = si_b->representor - si_a->representor;\n\tif (ret)\n\t\treturn ret;\n\t/* Unidentified devices come last in no specific order. */\n\tif (!si_a->representor)\n\t\treturn 0;\n\t/* Order representors by name. */\n\treturn si_a->port_name - si_b->port_name;\n}\n\n/**\n * DPDK callback to register a PCI device.\n *\n * This function spawns Ethernet devices out of a given PCI device.\n *\n * @param[in] pci_drv\n *   PCI driver structure (mlx5_driver).\n * @param[in] pci_dev\n *   PCI device information.\n *\n * @return\n *   0 on success, a negative errno value otherwise and rte_errno is set.\n */\nstatic int\nmlx5_pci_probe(struct rte_pci_driver *pci_drv __rte_unused,\n\t       struct rte_pci_device *pci_dev)\n{\n\tstruct ibv_device **ibv_list;\n\t/*\n\t * Number of found IB Devices matching with requested PCI BDF.\n\t * nd != 1 means there are multiple IB devices over the same\n\t * PCI device and we have representors and master.\n\t */\n\tunsigned int nd = 0;\n\t/*\n\t * Number of found IB device Ports. nd = 1 and np = 1..n means\n\t * we have the single multiport IB device, and there may be\n\t * representors attached to some of found ports.\n\t */\n\tunsigned int np = 0;\n\t/*\n\t * Number of DPDK ethernet devices to Spawn - either over\n\t * multiple IB devices or multiple ports of single IB device.\n\t * Actually this is the number of iterations to spawn.\n\t */\n\tunsigned int ns = 0;\n\tstruct mlx5_dev_config dev_config;\n\tint ret;\n\n\tret = mlx5_init_once();\n\tif (ret) {\n\t\tDRV_LOG(ERR, \"unable to init PMD global data: %s\",\n\t\t\tstrerror(rte_errno));\n\t\treturn -rte_errno;\n\t}\n\tassert(pci_drv == &mlx5_driver);\n\terrno = 0;\n\tibv_list = mlx5_glue->get_device_list(&ret);\n\tif (!ibv_list) {\n\t\trte_errno = errno ? errno : ENOSYS;\n\t\tDRV_LOG(ERR, \"cannot list devices, is ib_uverbs loaded?\");\n\t\treturn -rte_errno;\n\t}\n\t/*\n\t * First scan the list of all Infiniband devices to find\n\t * matching ones, gathering into the list.\n\t */\n\tstruct ibv_device *ibv_match[ret + 1];\n\tint nl_route = -1;\n\tint nl_rdma = -1;\n\tunsigned int i;\n\n\twhile (ret-- > 0) {\n\t\tstruct rte_pci_addr pci_addr;\n\n\t\tDRV_LOG(DEBUG, \"checking device \\\"%s\\\"\", ibv_list[ret]->name);\n\t\tif (mlx5_ibv_device_to_pci_addr(ibv_list[ret], &pci_addr))\n\t\t\tcontinue;\n\t\tif (pci_dev->addr.domain != pci_addr.domain ||\n\t\t    pci_dev->addr.bus != pci_addr.bus ||\n\t\t    pci_dev->addr.devid != pci_addr.devid ||\n\t\t    pci_dev->addr.function != pci_addr.function)\n\t\t\tcontinue;\n\t\tDRV_LOG(INFO, \"PCI information matches for device \\\"%s\\\"\",\n\t\t\tibv_list[ret]->name);\n\t\tibv_match[nd++] = ibv_list[ret];\n\t}\n\tibv_match[nd] = NULL;\n\tif (!nd) {\n\t\t/* No device matches, just complain and bail out. */\n\t\tmlx5_glue->free_device_list(ibv_list);\n\t\tDRV_LOG(WARNING,\n\t\t\t\"no Verbs device matches PCI device \" PCI_PRI_FMT \",\"\n\t\t\t\" are kernel drivers loaded?\",\n\t\t\tpci_dev->addr.domain, pci_dev->addr.bus,\n\t\t\tpci_dev->addr.devid, pci_dev->addr.function);\n\t\trte_errno = ENOENT;\n\t\tret = -rte_errno;\n\t\treturn ret;\n\t}\n\tnl_route = mlx5_nl_init(NETLINK_ROUTE);\n\tnl_rdma = mlx5_nl_init(NETLINK_RDMA);\n\tif (nd == 1) {\n\t\t/*\n\t\t * Found single matching device may have multiple ports.\n\t\t * Each port may be representor, we have to check the port\n\t\t * number and check the representors existence.\n\t\t */\n\t\tif (nl_rdma >= 0)\n\t\t\tnp = mlx5_nl_portnum(nl_rdma, ibv_match[0]->name);\n\t\tif (!np)\n\t\t\tDRV_LOG(WARNING, \"can not get IB device \\\"%s\\\"\"\n\t\t\t\t\t \" ports number\", ibv_match[0]->name);\n\t}\n\t/*\n\t * Now we can determine the maximal\n\t * amount of devices to be spawned.\n\t */\n\tstruct mlx5_dev_spawn_data list[np ? np : nd];\n\n\tif (np > 1) {\n\t\t/*\n\t\t * Single IB device with multiple ports found,\n\t\t * it may be E-Switch master device and representors.\n\t\t * We have to perform identification trough the ports.\n\t\t */\n\t\tassert(nl_rdma >= 0);\n\t\tassert(ns == 0);\n\t\tassert(nd == 1);\n\t\tfor (i = 1; i <= np; ++i) {\n\t\t\tlist[ns].max_port = np;\n\t\t\tlist[ns].ibv_port = i;\n\t\t\tlist[ns].ibv_dev = ibv_match[0];\n\t\t\tlist[ns].eth_dev = NULL;\n\t\t\tlist[ns].pci_dev = pci_dev;\n\t\t\tlist[ns].ifindex = mlx5_nl_ifindex\n\t\t\t\t\t(nl_rdma, list[ns].ibv_dev->name, i);\n\t\t\tif (!list[ns].ifindex) {\n\t\t\t\t/*\n\t\t\t\t * No network interface index found for the\n\t\t\t\t * specified port, it means there is no\n\t\t\t\t * representor on this port. It's OK,\n\t\t\t\t * there can be disabled ports, for example\n\t\t\t\t * if sriov_numvfs < sriov_totalvfs.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tret = -1;\n\t\t\tif (nl_route >= 0)\n\t\t\t\tret = mlx5_nl_switch_info\n\t\t\t\t\t       (nl_route,\n\t\t\t\t\t\tlist[ns].ifindex,\n\t\t\t\t\t\t&list[ns].info);\n\t\t\tif (ret || (!list[ns].info.representor &&\n\t\t\t\t    !list[ns].info.master)) {\n\t\t\t\t/*\n\t\t\t\t * We failed to recognize representors with\n\t\t\t\t * Netlink, let's try to perform the task\n\t\t\t\t * with sysfs.\n\t\t\t\t */\n\t\t\t\tret =  mlx5_sysfs_switch_info\n\t\t\t\t\t\t(list[ns].ifindex,\n\t\t\t\t\t\t &list[ns].info);\n\t\t\t}\n\t\t\tif (!ret && (list[ns].info.representor ^\n\t\t\t\t     list[ns].info.master))\n\t\t\t\tns++;\n\t\t}\n\t\tif (!ns) {\n\t\t\tDRV_LOG(ERR,\n\t\t\t\t\"unable to recognize master/representors\"\n\t\t\t\t\" on the IB device with multiple ports\");\n\t\t\trte_errno = ENOENT;\n\t\t\tret = -rte_errno;\n\t\t\tgoto exit;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * The existence of several matching entries (nd > 1) means\n\t\t * port representors have been instantiated. No existing Verbs\n\t\t * call nor sysfs entries can tell them apart, this can only\n\t\t * be done through Netlink calls assuming kernel drivers are\n\t\t * recent enough to support them.\n\t\t *\n\t\t * In the event of identification failure through Netlink,\n\t\t * try again through sysfs, then:\n\t\t *\n\t\t * 1. A single IB device matches (nd == 1) with single\n\t\t *    port (np=0/1) and is not a representor, assume\n\t\t *    no switch support.\n\t\t *\n\t\t * 2. Otherwise no safe assumptions can be made;\n\t\t *    complain louder and bail out.\n\t\t */\n\t\tnp = 1;\n\t\tfor (i = 0; i != nd; ++i) {\n\t\t\tmemset(&list[ns].info, 0, sizeof(list[ns].info));\n\t\t\tlist[ns].max_port = 1;\n\t\t\tlist[ns].ibv_port = 1;\n\t\t\tlist[ns].ibv_dev = ibv_match[i];\n\t\t\tlist[ns].eth_dev = NULL;\n\t\t\tlist[ns].pci_dev = pci_dev;\n\t\t\tlist[ns].ifindex = 0;\n\t\t\tif (nl_rdma >= 0)\n\t\t\t\tlist[ns].ifindex = mlx5_nl_ifindex\n\t\t\t\t\t(nl_rdma, list[ns].ibv_dev->name, 1);\n\t\t\tif (!list[ns].ifindex) {\n\t\t\t\tchar ifname[IF_NAMESIZE];\n\n\t\t\t\t/*\n\t\t\t\t * Netlink failed, it may happen with old\n\t\t\t\t * ib_core kernel driver (before 4.16).\n\t\t\t\t * We can assume there is old driver because\n\t\t\t\t * here we are processing single ports IB\n\t\t\t\t * devices. Let's try sysfs to retrieve\n\t\t\t\t * the ifindex. The method works for\n\t\t\t\t * master device only.\n\t\t\t\t */\n\t\t\t\tif (nd > 1) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Multiple devices found, assume\n\t\t\t\t\t * representors, can not distinguish\n\t\t\t\t\t * master/representor and retrieve\n\t\t\t\t\t * ifindex via sysfs.\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tret = mlx5_get_master_ifname\n\t\t\t\t\t(ibv_match[i]->ibdev_path, &ifname);\n\t\t\t\tif (!ret)\n\t\t\t\t\tlist[ns].ifindex =\n\t\t\t\t\t\tif_nametoindex(ifname);\n\t\t\t\tif (!list[ns].ifindex) {\n\t\t\t\t\t/*\n\t\t\t\t\t * No network interface index found\n\t\t\t\t\t * for the specified device, it means\n\t\t\t\t\t * there it is neither representor\n\t\t\t\t\t * nor master.\n\t\t\t\t\t */\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tret = -1;\n\t\t\tif (nl_route >= 0)\n\t\t\t\tret = mlx5_nl_switch_info\n\t\t\t\t\t       (nl_route,\n\t\t\t\t\t\tlist[ns].ifindex,\n\t\t\t\t\t\t&list[ns].info);\n\t\t\tif (ret || (!list[ns].info.representor &&\n\t\t\t\t    !list[ns].info.master)) {\n\t\t\t\t/*\n\t\t\t\t * We failed to recognize representors with\n\t\t\t\t * Netlink, let's try to perform the task\n\t\t\t\t * with sysfs.\n\t\t\t\t */\n\t\t\t\tret =  mlx5_sysfs_switch_info\n\t\t\t\t\t\t(list[ns].ifindex,\n\t\t\t\t\t\t &list[ns].info);\n\t\t\t}\n\t\t\tif (!ret && (list[ns].info.representor ^\n\t\t\t\t     list[ns].info.master)) {\n\t\t\t\tns++;\n\t\t\t} else if ((nd == 1) &&\n\t\t\t\t   !list[ns].info.representor &&\n\t\t\t\t   !list[ns].info.master) {\n\t\t\t\t/*\n\t\t\t\t * Single IB device with\n\t\t\t\t * one physical port and\n\t\t\t\t * attached network device.\n\t\t\t\t * May be SRIOV is not enabled\n\t\t\t\t * or there is no representors.\n\t\t\t\t */\n\t\t\t\tDRV_LOG(INFO, \"no E-Switch support detected\");\n\t\t\t\tns++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!ns) {\n\t\t\tDRV_LOG(ERR,\n\t\t\t\t\"unable to recognize master/representors\"\n\t\t\t\t\" on the multiple IB devices\");\n\t\t\trte_errno = ENOENT;\n\t\t\tret = -rte_errno;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\tassert(ns);\n\t/*\n\t * Sort list to probe devices in natural order for users convenience\n\t * (i.e. master first, then representors from lowest to highest ID).\n\t */\n\tqsort(list, ns, sizeof(*list), mlx5_dev_spawn_data_cmp);\n\t/* Default configuration. */\n\tdev_config = (struct mlx5_dev_config){\n\t\t.hw_padding = 0,\n\t\t.mps = MLX5_ARG_UNSET,\n\t\t.tx_vec_en = 1,\n\t\t.rx_vec_en = 1,\n\t\t.txq_inline = MLX5_ARG_UNSET,\n\t\t.txqs_inline = MLX5_ARG_UNSET,\n\t\t.txqs_vec = MLX5_ARG_UNSET,\n\t\t.inline_max_packet_sz = MLX5_ARG_UNSET,\n\t\t.vf_nl_en = 1,\n\t\t.mr_ext_memseg_en = 1,\n\t\t.mprq = {\n\t\t\t.enabled = 0, /* Disabled by default. */\n\t\t\t.stride_num_n = MLX5_MPRQ_STRIDE_NUM_N,\n\t\t\t.max_memcpy_len = MLX5_MPRQ_MEMCPY_DEFAULT_LEN,\n\t\t\t.min_rxqs_num = MLX5_MPRQ_MIN_RXQS,\n\t\t},\n\t\t.dv_esw_en = 1,\n\t};\n\t/* Device specific configuration. */\n\tswitch (pci_dev->id.device_id) {\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX5BF:\n\t\tdev_config.txqs_vec = MLX5_VPMD_MAX_TXQS_BLUEFIELD;\n\t\tbreak;\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX4VF:\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX4LXVF:\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX5VF:\n\tcase PCI_DEVICE_ID_MELLANOX_CONNECTX5EXVF:\n\t\tdev_config.vf = 1;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\t/* Set architecture-dependent default value if unset. */\n\tif (dev_config.txqs_vec == MLX5_ARG_UNSET)\n\t\tdev_config.txqs_vec = MLX5_VPMD_MAX_TXQS;\n\tfor (i = 0; i != ns; ++i) {\n\t\tuint32_t restore;\n\n\t\tlist[i].eth_dev = mlx5_dev_spawn(&pci_dev->device,\n\t\t\t\t\t\t &list[i],\n\t\t\t\t\t\t dev_config);\n\t\tif (!list[i].eth_dev) {\n\t\t\tif (rte_errno != EBUSY && rte_errno != EEXIST)\n\t\t\t\tbreak;\n\t\t\t/* Device is disabled or already spawned. Ignore it. */\n\t\t\tcontinue;\n\t\t}\n\t\trestore = list[i].eth_dev->data->dev_flags;\n\t\trte_eth_copy_pci_info(list[i].eth_dev, pci_dev);\n\t\t/* Restore non-PCI flags cleared by the above call. */\n\t\tlist[i].eth_dev->data->dev_flags |= restore;\n\t\trte_eth_dev_probing_finish(list[i].eth_dev);\n\t}\n\tif (i != ns) {\n\t\tDRV_LOG(ERR,\n\t\t\t\"probe of PCI device \" PCI_PRI_FMT \" aborted after\"\n\t\t\t\" encountering an error: %s\",\n\t\t\tpci_dev->addr.domain, pci_dev->addr.bus,\n\t\t\tpci_dev->addr.devid, pci_dev->addr.function,\n\t\t\tstrerror(rte_errno));\n\t\tret = -rte_errno;\n\t\t/* Roll back. */\n\t\twhile (i--) {\n\t\t\tif (!list[i].eth_dev)\n\t\t\t\tcontinue;\n\t\t\tmlx5_dev_close(list[i].eth_dev);\n\t\t\t/* mac_addrs must not be freed because in dev_private */\n\t\t\tlist[i].eth_dev->data->mac_addrs = NULL;\n\t\t\tclaim_zero(rte_eth_dev_release_port(list[i].eth_dev));\n\t\t}\n\t\t/* Restore original error. */\n\t\trte_errno = -ret;\n\t} else {\n\t\tret = 0;\n\t}\nexit:\n\t/*\n\t * Do the routine cleanup:\n\t * - close opened Netlink sockets\n\t * - free the Infiniband device list\n\t */\n\tif (nl_rdma >= 0)\n\t\tclose(nl_rdma);\n\tif (nl_route >= 0)\n\t\tclose(nl_route);\n\tassert(ibv_list);\n\tmlx5_glue->free_device_list(ibv_list);\n\treturn ret;\n}\n\n/**\n * DPDK callback to remove a PCI device.\n *\n * This function removes all Ethernet devices belong to a given PCI device.\n *\n * @param[in] pci_dev\n *   Pointer to the PCI device.\n *\n * @return\n *   0 on success, the function cannot fail.\n */\nstatic int\nmlx5_pci_remove(struct rte_pci_device *pci_dev)\n{\n\tuint16_t port_id;\n\n\tRTE_ETH_FOREACH_DEV_OF(port_id, &pci_dev->device)\n\t\trte_eth_dev_close(port_id);\n\treturn 0;\n}\n\nstatic const struct rte_pci_id mlx5_pci_id_map[] = {\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4VF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4LX)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX4LXVF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5VF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5EX)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5EXVF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5BF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t       PCI_DEVICE_ID_MELLANOX_CONNECTX5BFVF)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t\tPCI_DEVICE_ID_MELLANOX_CONNECTX6)\n\t},\n\t{\n\t\tRTE_PCI_DEVICE(PCI_VENDOR_ID_MELLANOX,\n\t\t\t\tPCI_DEVICE_ID_MELLANOX_CONNECTX6VF)\n\t},\n\t{\n\t\t.vendor_id = 0\n\t}\n};\n\nstatic struct rte_pci_driver mlx5_driver = {\n\t.driver = {\n\t\t.name = MLX5_DRIVER_NAME\n\t},\n\t.id_table = mlx5_pci_id_map,\n\t.probe = mlx5_pci_probe,\n\t.remove = mlx5_pci_remove,\n\t.dma_map = mlx5_dma_map,\n\t.dma_unmap = mlx5_dma_unmap,\n\t.drv_flags = (RTE_PCI_DRV_INTR_LSC | RTE_PCI_DRV_INTR_RMV |\n\t\t      RTE_PCI_DRV_PROBE_AGAIN),\n};\n\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\n/**\n * Suffix RTE_EAL_PMD_PATH with \"-glue\".\n *\n * This function performs a sanity check on RTE_EAL_PMD_PATH before\n * suffixing its last component.\n *\n * @param buf[out]\n *   Output buffer, should be large enough otherwise NULL is returned.\n * @param size\n *   Size of @p out.\n *\n * @return\n *   Pointer to @p buf or @p NULL in case suffix cannot be appended.\n */\nstatic char *\nmlx5_glue_path(char *buf, size_t size)\n{\n\tstatic const char *const bad[] = { \"/\", \".\", \"..\", NULL };\n\tconst char *path = RTE_EAL_PMD_PATH;\n\tsize_t len = strlen(path);\n\tsize_t off;\n\tint i;\n\n\twhile (len && path[len - 1] == '/')\n\t\t--len;\n\tfor (off = len; off && path[off - 1] != '/'; --off)\n\t\t;\n\tfor (i = 0; bad[i]; ++i)\n\t\tif (!strncmp(path + off, bad[i], (int)(len - off)))\n\t\t\tgoto error;\n\ti = snprintf(buf, size, \"%.*s-glue\", (int)len, path);\n\tif (i == -1 || (size_t)i >= size)\n\t\tgoto error;\n\treturn buf;\nerror:\n\tDRV_LOG(ERR,\n\t\t\"unable to append \\\"-glue\\\" to last component of\"\n\t\t\" RTE_EAL_PMD_PATH (\\\"\" RTE_EAL_PMD_PATH \"\\\"),\"\n\t\t\" please re-configure DPDK\");\n\treturn NULL;\n}\n\n/**\n * Initialization routine for run-time dependency on rdma-core.\n */\nstatic int\nmlx5_glue_init(void)\n{\n\tchar glue_path[sizeof(RTE_EAL_PMD_PATH) - 1 + sizeof(\"-glue\")];\n\tconst char *path[] = {\n\t\t/*\n\t\t * A basic security check is necessary before trusting\n\t\t * MLX5_GLUE_PATH, which may override RTE_EAL_PMD_PATH.\n\t\t */\n\t\t(geteuid() == getuid() && getegid() == getgid() ?\n\t\t getenv(\"MLX5_GLUE_PATH\") : NULL),\n\t\t/*\n\t\t * When RTE_EAL_PMD_PATH is set, use its glue-suffixed\n\t\t * variant, otherwise let dlopen() look up libraries on its\n\t\t * own.\n\t\t */\n\t\t(*RTE_EAL_PMD_PATH ?\n\t\t mlx5_glue_path(glue_path, sizeof(glue_path)) : \"\"),\n\t};\n\tunsigned int i = 0;\n\tvoid *handle = NULL;\n\tvoid **sym;\n\tconst char *dlmsg;\n\n\twhile (!handle && i != RTE_DIM(path)) {\n\t\tconst char *end;\n\t\tsize_t len;\n\t\tint ret;\n\n\t\tif (!path[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\tend = strpbrk(path[i], \":;\");\n\t\tif (!end)\n\t\t\tend = path[i] + strlen(path[i]);\n\t\tlen = end - path[i];\n\t\tret = 0;\n\t\tdo {\n\t\t\tchar name[ret + 1];\n\n\t\t\tret = snprintf(name, sizeof(name), \"%.*s%s\" MLX5_GLUE,\n\t\t\t\t       (int)len, path[i],\n\t\t\t\t       (!len || *(end - 1) == '/') ? \"\" : \"/\");\n\t\t\tif (ret == -1)\n\t\t\t\tbreak;\n\t\t\tif (sizeof(name) != (size_t)ret + 1)\n\t\t\t\tcontinue;\n\t\t\tDRV_LOG(DEBUG, \"looking for rdma-core glue as \\\"%s\\\"\",\n\t\t\t\tname);\n\t\t\thandle = dlopen(name, RTLD_LAZY);\n\t\t\tbreak;\n\t\t} while (1);\n\t\tpath[i] = end + 1;\n\t\tif (!*end)\n\t\t\t++i;\n\t}\n\tif (!handle) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tDRV_LOG(WARNING, \"cannot load glue library: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tsym = dlsym(handle, \"mlx5_glue\");\n\tif (!sym || !*sym) {\n\t\trte_errno = EINVAL;\n\t\tdlmsg = dlerror();\n\t\tif (dlmsg)\n\t\t\tDRV_LOG(ERR, \"cannot resolve glue symbol: %s\", dlmsg);\n\t\tgoto glue_error;\n\t}\n\tmlx5_glue = *sym;\n\treturn 0;\nglue_error:\n\tif (handle)\n\t\tdlclose(handle);\n\tDRV_LOG(WARNING,\n\t\t\"cannot initialize PMD due to missing run-time dependency on\"\n\t\t\" rdma-core libraries (libibverbs, libmlx5)\");\n\treturn -rte_errno;\n}\n\n#endif\n\n/**\n * Driver initialization routine.\n */\nRTE_INIT(rte_mlx5_pmd_init)\n{\n\t/* Initialize driver log type. */\n\tmlx5_logtype = rte_log_register(\"pmd.net.mlx5\");\n\tif (mlx5_logtype >= 0)\n\t\trte_log_set_level(mlx5_logtype, RTE_LOG_NOTICE);\n\n\t/* Build the static tables for Verbs conversion. */\n\tmlx5_set_ptype_table();\n\tmlx5_set_cksum_table();\n\tmlx5_set_swp_types_table();\n\t/*\n\t * RDMAV_HUGEPAGES_SAFE tells ibv_fork_init() we intend to use\n\t * huge pages. Calling ibv_fork_init() during init allows\n\t * applications to use fork() safely for purposes other than\n\t * using this PMD, which is not supported in forked processes.\n\t */\n\tsetenv(\"RDMAV_HUGEPAGES_SAFE\", \"1\", 1);\n\t/* Match the size of Rx completion entry to the size of a cacheline. */\n\tif (RTE_CACHE_LINE_SIZE == 128)\n\t\tsetenv(\"MLX5_CQE_SIZE\", \"128\", 0);\n\t/*\n\t * MLX5_DEVICE_FATAL_CLEANUP tells ibv_destroy functions to\n\t * cleanup all the Verbs resources even when the device was removed.\n\t */\n\tsetenv(\"MLX5_DEVICE_FATAL_CLEANUP\", \"1\", 1);\n#ifdef RTE_IBVERBS_LINK_DLOPEN\n\tif (mlx5_glue_init())\n\t\treturn;\n\tassert(mlx5_glue);\n#endif\n#ifndef NDEBUG\n\t/* Glue structure must not contain any NULL pointers. */\n\t{\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i != sizeof(*mlx5_glue) / sizeof(void *); ++i)\n\t\t\tassert(((const void *const *)mlx5_glue)[i]);\n\t}\n#endif\n\tif (strcmp(mlx5_glue->version, MLX5_GLUE_VERSION)) {\n\t\tDRV_LOG(ERR,\n\t\t\t\"rdma-core glue \\\"%s\\\" mismatch: \\\"%s\\\" is required\",\n\t\t\tmlx5_glue->version, MLX5_GLUE_VERSION);\n\t\treturn;\n\t}\n\tmlx5_glue->fork_init();\n\trte_pci_register(&mlx5_driver);\n}\n\nRTE_PMD_EXPORT_NAME(net_mlx5, __COUNTER__);\nRTE_PMD_REGISTER_PCI_TABLE(net_mlx5, mlx5_pci_id_map);\nRTE_PMD_REGISTER_KMOD_DEP(net_mlx5, \"* ib_uverbs & mlx5_core & mlx5_ib\");\n",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/drivers/net/mlx5/meson.build": "# SPDX-License-Identifier: BSD-3-Clause\n# Copyright 2018 6WIND S.A.\n# Copyright 2018 Mellanox Technologies, Ltd\n\npmd_dlopen = (get_option('ibverbs_link') == 'dlopen')\nLIB_GLUE_BASE = 'librte_pmd_mlx5_glue.so'\nLIB_GLUE_VERSION = '19.05.0'\nLIB_GLUE = LIB_GLUE_BASE + '.' + LIB_GLUE_VERSION\nif pmd_dlopen\n\tdpdk_conf.set('RTE_IBVERBS_LINK_DLOPEN', 1)\n\tcflags += [\n\t\t'-DMLX5_GLUE=\"@0@\"'.format(LIB_GLUE),\n\t\t'-DMLX5_GLUE_VERSION=\"@0@\"'.format(LIB_GLUE_VERSION),\n\t]\nendif\nlibnames = [ 'mnl', 'mlx5', 'ibverbs' ]\nlibs = []\nbuild = true\nforeach libname:libnames\n\tlib = dependency('lib' + libname, required:false)\n\tif not lib.found()\n\t\tlib = cc.find_library(libname, required:false)\n\tendif\n\tif lib.found()\n\t\tlibs += [ lib ]\n\telse\n\t\tbuild = false\n\tendif\nendforeach\nif build\n\tallow_experimental_apis = true\n\text_deps += libs\n\tsources = files(\n\t\t'mlx5.c',\n\t\t'mlx5_ethdev.c',\n\t\t'mlx5_flow.c',\n\t\t'mlx5_flow_dv.c',\n\t\t'mlx5_flow_tcf.c',\n\t\t'mlx5_flow_verbs.c',\n\t\t'mlx5_mac.c',\n\t\t'mlx5_mr.c',\n\t\t'mlx5_nl.c',\n\t\t'mlx5_rss.c',\n\t\t'mlx5_rxmode.c',\n\t\t'mlx5_rxq.c',\n\t\t'mlx5_rxtx.c',\n\t\t'mlx5_mp.c',\n\t\t'mlx5_stats.c',\n\t\t'mlx5_trigger.c',\n\t\t'mlx5_txq.c',\n\t\t'mlx5_vlan.c',\n\t\t'mlx5_devx_cmds.c',\n\t)\n\tif dpdk_conf.has('RTE_ARCH_X86_64') or dpdk_conf.has('RTE_ARCH_ARM64')\n\t\tsources += files('mlx5_rxtx_vec.c')\n\tendif\n\tif not pmd_dlopen\n\t\tsources += files('mlx5_glue.c')\n\tendif\n\tcflags_options = [\n\t\t'-Wextra',\n\t\t'-std=c11',\n\t\t'-Wno-strict-prototypes',\n\t\t'-D_BSD_SOURCE',\n\t\t'-D_DEFAULT_SOURCE',\n\t\t'-D_XOPEN_SOURCE=600'\n\t]\n\tforeach option:cflags_options\n\t\tif cc.has_argument(option)\n\t\t\tcflags += option\n\t\tendif\n\tendforeach\n\tif get_option('buildtype').contains('debug')\n\t\tcflags += [ '-pedantic', '-UNDEBUG', '-DPEDANTIC' ]\n\telse\n\t\tcflags += [ '-DNDEBUG', '-UPEDANTIC' ]\n\tendif\n\t# To maintain the compatibility with the make build system\n\t# mlx5_autoconf.h file is still generated.\n\t# input array for meson member search:\n\t# [ \"MACRO to define if found\", \"header for the search\",\n\t#   \"symbol to search\", \"struct member to search\" ]\n\thas_member_args = [\n\t\t[ 'HAVE_IBV_MLX5_MOD_SWP', 'infiniband/mlx5dv.h',\n\t\t'struct mlx5dv_sw_parsing_caps', 'sw_parsing_offloads' ],\n\t\t[ 'HAVE_IBV_DEVICE_COUNTERS_SET_V42', 'infiniband/verbs.h',\n\t\t'struct ibv_counter_set_init_attr', 'counter_set_id' ],\n\t\t[ 'HAVE_IBV_DEVICE_COUNTERS_SET_V45', 'infiniband/verbs.h',\n\t\t'struct ibv_counters_init_attr', 'comp_mask' ],\n\t]\n\t# input array for meson symbol search:\n\t# [ \"MACRO to define if found\", \"header for the search\",\n\t#   \"symbol to search\" ]\n\thas_sym_args = [\n\t\t[ 'HAVE_IBV_DEVICE_STRIDING_RQ_SUPPORT', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_CQE_RES_FORMAT_CSUM_STRIDX' ],\n\t\t[ 'HAVE_IBV_DEVICE_TUNNEL_SUPPORT', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_CONTEXT_MASK_TUNNEL_OFFLOADS' ],\n\t\t[ 'HAVE_IBV_MLX5_MOD_MPW', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_CONTEXT_FLAGS_MPW_ALLOWED' ],\n\t\t[ 'HAVE_IBV_MLX5_MOD_CQE_128B_COMP', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_CONTEXT_FLAGS_CQE_128B_COMP' ],\n\t\t[ 'HAVE_IBV_MLX5_MOD_CQE_128B_PAD', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_CQ_INIT_ATTR_FLAGS_CQE_PAD' ],\n\t\t[ 'HAVE_IBV_FLOW_DV_SUPPORT', 'infiniband/mlx5dv.h',\n\t\t'mlx5dv_create_flow_action_packet_reformat' ],\n\t\t[ 'HAVE_IBV_DEVICE_MPLS_SUPPORT', 'infiniband/verbs.h',\n\t\t'IBV_FLOW_SPEC_MPLS' ],\n\t\t[ 'HAVE_IBV_WQ_FLAGS_PCI_WRITE_END_PADDING', 'infiniband/verbs.h',\n\t\t'IBV_WQ_FLAGS_PCI_WRITE_END_PADDING' ],\n\t\t[ 'HAVE_IBV_WQ_FLAG_RX_END_PADDING', 'infiniband/verbs.h',\n\t\t'IBV_WQ_FLAG_RX_END_PADDING' ],\n\t\t[ 'HAVE_IBV_DEVX_OBJ', 'infiniband/mlx5dv.h',\n\t\t'mlx5dv_devx_obj_create' ],\n\t\t[ 'HAVE_IBV_FLOW_DEVX_COUNTERS', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_FLOW_ACTION_COUNTERS_DEVX' ],\n\t\t[ 'HAVE_MLX5DV_DR', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_DR_DOMAIN_TYPE_NIC_RX' ],\n\t\t[ 'HAVE_MLX5DV_DR_ESWITCH', 'infiniband/mlx5dv.h',\n\t\t'MLX5DV_DR_DOMAIN_TYPE_FDB' ],\n\t\t[ 'HAVE_SUPPORTED_40000baseKR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_40000baseKR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_40000baseCR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_40000baseCR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_40000baseSR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_40000baseSR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_40000baseLR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_40000baseLR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_56000baseKR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_56000baseKR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_56000baseCR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_56000baseCR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_56000baseSR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_56000baseSR4_Full' ],\n\t\t[ 'HAVE_SUPPORTED_56000baseLR4_Full', 'linux/ethtool.h',\n\t\t'SUPPORTED_56000baseLR4_Full' ],\n\t\t[ 'HAVE_ETHTOOL_LINK_MODE_25G', 'linux/ethtool.h',\n\t\t'ETHTOOL_LINK_MODE_25000baseCR_Full_BIT' ],\n\t\t[ 'HAVE_ETHTOOL_LINK_MODE_50G', 'linux/ethtool.h',\n\t\t'ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT' ],\n\t\t[ 'HAVE_ETHTOOL_LINK_MODE_100G', 'linux/ethtool.h',\n\t\t'ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT' ],\n\t\t[ 'HAVE_IFLA_NUM_VF', 'linux/if_link.h',\n\t\t'IFLA_NUM_VF' ],\n\t\t[ 'HAVE_IFLA_EXT_MASK', 'linux/if_link.h',\n\t\t'IFLA_EXT_MASK' ],\n\t\t[ 'HAVE_IFLA_PHYS_SWITCH_ID', 'linux/if_link.h',\n\t\t'IFLA_PHYS_SWITCH_ID' ],\n\t\t[ 'HAVE_IFLA_PHYS_PORT_NAME', 'linux/if_link.h',\n\t\t'IFLA_PHYS_PORT_NAME' ],\n\t\t[ 'HAVE_IFLA_VXLAN_COLLECT_METADATA', 'linux/if_link.h',\n\t\t'IFLA_VXLAN_COLLECT_METADATA' ],\n\t\t[ 'HAVE_TCA_CHAIN', 'linux/rtnetlink.h',\n\t\t'TCA_CHAIN' ],\n\t\t[ 'HAVE_TCA_FLOWER_ACT', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_ACT' ],\n\t\t[ 'HAVE_TCA_FLOWER_FLAGS', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_FLAGS' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ETH_TYPE', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ETH_TYPE' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ETH_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ETH_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ETH_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ETH_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ETH_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ETH_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ETH_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ETH_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IP_PROTO', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IP_PROTO' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV4_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV4_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV4_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV4_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV4_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV4_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV4_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV4_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV6_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV6_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV6_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV6_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV6_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV6_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IPV6_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IPV6_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_TCP_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_TCP_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_TCP_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_TCP_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_TCP_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_TCP_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_TCP_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_TCP_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_UDP_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_UDP_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_UDP_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_UDP_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_UDP_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_UDP_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_UDP_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_UDP_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_VLAN_ID', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_VLAN_ID' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_VLAN_PRIO', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_VLAN_PRIO' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_VLAN_ETH_TYPE', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_VLAN_ETH_TYPE' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_TCP_FLAGS', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_TCP_FLAGS' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_TCP_FLAGS_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_TCP_FLAGS_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IP_TOS', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IP_TOS' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IP_TOS_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IP_TOS_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IP_TTL', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IP_TTL' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_IP_TTL_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_IP_TTL_MASK' ],\n\t\t[ 'HAVE_TC_ACT_GOTO_CHAIN', 'linux/pkt_cls.h',\n\t\t'TC_ACT_GOTO_CHAIN' ],\n\t\t[ 'HAVE_TC_ACT_VLAN', 'linux/tc_act/tc_vlan.h',\n\t\t'TCA_VLAN_PUSH_VLAN_PRIORITY' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_KEY_ID', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_KEY_ID' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV4_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV4_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV4_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV4_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV4_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV4_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV4_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV4_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV6_SRC', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV6_SRC' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV6_SRC_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV6_SRC_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV6_DST', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV6_DST' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IPV6_DST_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IPV6_DST_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_UDP_SRC_PORT', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_UDP_SRC_PORT' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_UDP_SRC_PORT_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_UDP_SRC_PORT_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_UDP_DST_PORT', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_UDP_DST_PORT' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_UDP_DST_PORT_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_UDP_DST_PORT_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IP_TOS', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IP_TOS' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IP_TOS_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IP_TOS_MASK' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IP_TTL', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IP_TTL' ],\n\t\t[ 'HAVE_TCA_FLOWER_KEY_ENC_IP_TTL_MASK', 'linux/pkt_cls.h',\n\t\t'TCA_FLOWER_KEY_ENC_IP_TTL_MASK' ],\n\t\t[ 'HAVE_TC_ACT_TUNNEL_KEY', 'linux/tc_act/tc_tunnel_key.h',\n\t\t'TCA_ACT_TUNNEL_KEY' ],\n\t\t[ 'HAVE_TCA_TUNNEL_KEY_ENC_DST_PORT', 'linux/tc_act/tc_tunnel_key.h',\n\t\t'TCA_TUNNEL_KEY_ENC_DST_PORT' ],\n\t\t[ 'HAVE_TCA_TUNNEL_KEY_ENC_TOS', 'linux/tc_act/tc_tunnel_key.h',\n\t\t'TCA_TUNNEL_KEY_ENC_TOS' ],\n\t\t[ 'HAVE_TCA_TUNNEL_KEY_ENC_TTL', 'linux/tc_act/tc_tunnel_key.h',\n\t\t'TCA_TUNNEL_KEY_ENC_TTL' ],\n\t\t[ 'HAVE_TCA_TUNNEL_KEY_NO_CSUM', 'linux/tc_act/tc_tunnel_key.h',\n\t\t'TCA_TUNNEL_KEY_NO_CSUM' ],\n\t\t[ 'HAVE_TC_ACT_PEDIT', 'linux/tc_act/tc_pedit.h',\n\t\t'TCA_PEDIT_KEY_EX_HDR_TYPE_UDP' ],\n\t\t[ 'HAVE_RDMA_NL_NLDEV', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NL_NLDEV' ],\n\t\t[ 'HAVE_RDMA_NLDEV_CMD_GET', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NLDEV_CMD_GET' ],\n\t\t[ 'HAVE_RDMA_NLDEV_CMD_PORT_GET', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NLDEV_CMD_PORT_GET' ],\n\t\t[ 'HAVE_RDMA_NLDEV_ATTR_DEV_INDEX', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NLDEV_ATTR_DEV_INDEX' ],\n\t\t[ 'HAVE_RDMA_NLDEV_ATTR_DEV_NAME', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NLDEV_ATTR_DEV_NAME' ],\n\t\t[ 'HAVE_RDMA_NLDEV_ATTR_PORT_INDEX', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NLDEV_ATTR_PORT_INDEX' ],\n\t\t[ 'HAVE_RDMA_NLDEV_ATTR_NDEV_INDEX', 'rdma/rdma_netlink.h',\n\t\t'RDMA_NLDEV_ATTR_NDEV_INDEX' ],\n\t]\n\tconfig = configuration_data()\n\tforeach arg:has_sym_args\n\t\tconfig.set(arg[0], cc.has_header_symbol(arg[1], arg[2]))\n\tendforeach\n\tforeach arg:has_member_args\n\t\tfile_prefix = '#include<' + arg[1] + '>'\n\t\tconfig.set(arg[0], cc.has_member(arg[2], arg[3],\n\t\t\tprefix : file_prefix))\n\tendforeach\n\tconfigure_file(output : 'mlx5_autoconf.h', configuration : config)\nendif\n# Build Glue Library\nif pmd_dlopen and build\n\tdlopen_name = 'mlx5_glue'\n\tdlopen_lib_name = driver_name_fmt.format(dlopen_name)\n\tdlopen_so_version = LIB_GLUE_VERSION\n\tdlopen_sources = files('mlx5_glue.c')\n\tdlopen_install_dir = [ eal_pmd_path + '-glue' ]\n\tshared_lib = shared_library(\n\t\tdlopen_lib_name,\n\t\tdlopen_sources,\n\t\tinclude_directories: global_inc,\n\t\tc_args: cflags,\n\t\tdependencies: libs,\n\t\tlink_args: [\n\t\t'-Wl,-export-dynamic',\n\t\t'-Wl,-h,@0@'.format(LIB_GLUE),\n\t\t],\n\t\tsoversion: dlopen_so_version,\n\t\tinstall: true,\n\t\tinstall_dir: dlopen_install_dir,\n\t)\nendif\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/forward_stats.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/console.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/host_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/fast_pkt_proc.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/perf_benchmark.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/vm_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/vswitch_vm.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/host_vm_comms_qemu.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/inter_vm_comms.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/vmxnet3_int.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/nics/img/single_port_nic.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/load_bal_app_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/quickassist_block_diagram.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/l2_fwd_virtenv_benchmark_setup.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/pipeline_overview.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/kernel_nic.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/example_rules.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/client_svr_sym_multi_proc_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/sym_multi_proc_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/test_pipeline_app.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/ring_pipeline_perf_setup.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/qos_sched_app_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/ipv4_acl_rule.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/sample_app_ug/img/threads_pipelines.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/ex_data_flow_tru_dropper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure35.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure33.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/m_definition.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/ewma_filter_eq_1.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/kernel_nic_intf.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/prefetch_pipeline.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/drop_probability_eq3.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/packet_distributor2.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pkt_flow_kni.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/tbl24_tbl8_tbl8.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/packet_distributor1.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/hier_sched_blk.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/ewma_filter_eq_2.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/eq2_factor.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure34.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/kni_traffic_flow.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pkt_drop_probability.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pipe_prefetch_sm.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/sched_hier_per_port.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/data_struct_per_port.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/drop_probability_eq4.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/vhost_net_arch.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/flow_tru_droppper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure39.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/pkt_proc_pipeline_qos.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure32.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/tbl24_tbl8.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure38.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/blk_diag_dropper.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/eq2_expression.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/figure37.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/guides/prog_guide/img/drop_probability_graph.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/logo/DPDK_logo_vertical_rev_small.png",
        "/tmp/vanessa/spack-stage/spack-stage-dpdk-19.05-66hybj5puztce3whbppvnzg4fqhx4akn/spack-src/doc/logo/DPDK_logo_horizontal_tag.png"
    ],
    "total_files": 3537
}