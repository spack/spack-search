{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-parsec-1.1.0-yujvhtviwotvwcwavhg6cnb7vgtusrkw/spack-src/dplasma/cores/cuda_zgemm.c": "/*\n * Copyright (c) 2010-2012 The University of Tennessee and The University\n *                         of Tennessee Research Foundation.  All rights\n *                         reserved.\n *\n * @precisions normal z -> z c d s\n *\n */\n#include \"dague_config.h\"\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <core_blas.h>\n#include <core_blas.h>\n#if defined(PRECISION_z) || defined(PRECISION_c)\n#include <cuComplex.h>\n#endif\n#include \"dague.h\"\n#include \"gpu_data.h\"\n#include \"execution_unit.h\"\n#include \"scheduling.h\"\n#include \"fifo.h\"\n#include \"datarepo.h\"\n#include \"data_dist/matrix/matrix.h\"\n\n#include \"cuda_zgemm.h\"\n\n#define flow_A  1\n#define flow_B  2\n#define flow_C  0\n\n#define KERNEL_NAME zgemm\n\ntypedef void (*cuda_zgemm_t) ( char TRANSA, char TRANSB, int m, int n, int k,\n                               dague_complex64_t alpha, dague_complex64_t *d_A, int lda,\n                                                        dague_complex64_t *d_B, int ldb,\n                               dague_complex64_t beta,  dague_complex64_t *d_C, int ldc,\n                               CUstream stream );\n\ncuda_zgemm_t* zgemm_functions;\n\n#define FORCE_UNDEFINED_SYMBOL(x) void* __ ## x ## _fp =(void*)&x;\nextern cuda_zgemm_t magmablas_zgemm_SM11;\nFORCE_UNDEFINED_SYMBOL(magmablas_zgemm_SM11)\nextern cuda_zgemm_t magmablas_zgemm_SM13;\nFORCE_UNDEFINED_SYMBOL(magmablas_zgemm_SM13)\nextern cuda_zgemm_t magmablas_zgemm_SM20;\nFORCE_UNDEFINED_SYMBOL(magmablas_zgemm_SM20)\n\nstatic inline\nint gpu_kernel_push_zgemm( gpu_device_t* gpu_device,\n                           dague_gpu_context_t* this_task,\n                           dague_gpu_exec_stream_t* gpu_stream);\n\nstatic inline\nint gpu_kernel_submit_zgemm( gpu_device_t* gpu_device,\n                           dague_gpu_context_t* this_task,\n                           dague_gpu_exec_stream_t* gpu_stream);\n\nstatic inline\nint gpu_kernel_pop_zgemm( gpu_device_t* gpu_device,\n                           dague_gpu_context_t* this_task,\n                           dague_gpu_exec_stream_t* gpu_stream);\n\nstatic inline\nint  gpu_kernel_epilog_zgemm( gpu_device_t* gpu_device,\n                              dague_gpu_context_t* this_task );\n\ntypedef struct dague_zgemm_args_s {\n    dague_gpu_context_t super;\n    int pushout;\n    dague_complex64_t alpha, beta;\n    PLASMA_enum transA, transB;\n    int M, N, K;\n    int Am, An, lda, Bm, Bn, ldb, Cm, Cn, ldc;\n    size_t sizeA, sizeB, sizeC;\n    dague_ddesc_t *ddescA, *ddescB, *ddescC;\n} dague_zgemm_args_t;\n\n#include \"gpu_scheduling.h\"\n\nstatic int ndevices = 0;\n\nint gpu_kernel_init_zgemm( dague_context_t* dague_context )\n{\n    char *env;\n    int i, dindex, nbgpus;\n    (void)dague_context;\n\n    nbgpus = dague_active_gpu();\n    zgemm_functions = calloc(nbgpus, sizeof(cuda_zgemm_t));\n\n    for( i = dindex = 0; i < nbgpus; i++ ) {\n        gpu_device_t* gpu_device;\n        CUresult status;\n        void* fn;\n        void* dlh;\n        char library_name[FILENAME_MAX];\n        char function_name[FILENAME_MAX];\n\n        gpu_device = gpu_enabled_devices[i];\n        fn = NULL;\n\n        status = cuCtxPushCurrent( gpu_device->ctx );\n        DAGUE_CUDA_CHECK_ERROR( \"(INIT) cuCtxPushCurrent \", status, {continue;} );\n        int major = gpu_device->major, minor = gpu_device->minor;\n\n    retry_lesser_sm_version:\n        snprintf(function_name, FILENAME_MAX, \"magmablas_zgemm_SM%d%d\", major, minor);\n        env = getenv(\"DAGUE_CUCORES_LIB\");\n        if(NULL == env) {\n            snprintf(library_name,  FILENAME_MAX, \"libdplasma_cucores_sm%d%d.so\",  major, minor);\n        }\n        else {\n            snprintf(library_name,  FILENAME_MAX, \"%s\", env);\n        }\n\n        dlh = dlopen(library_name, RTLD_NOW | RTLD_NODELETE );\n        if(NULL == dlh) {\n            if(env) ERROR((\"Could not find %s library: %s\\n\"\n                           \"  It is derived from environment DAGUE_CUCORES_LIB=%s\\n\"\n                           \"  To resolve this issue, set this variable to the correct path\\n\"\n                           \"    ex: /path/libdplasma_cucores_sm20.so\\n\"\n                           \"  Or unset it to use the default GPU kernels\\n\"\n                           , library_name, dlerror(), env));\n            DEBUG3((\"Could not find %s dynamic library (%s)\\n\", library_name, dlerror()));\n        }\n        else {\n            fn = dlsym(dlh, function_name);\n            dlclose(dlh);\n        }\n\n        /* Couldn't load from dynamic libs, try static */\n        if(NULL == fn) {\n            DEBUG3((\"No dynamic function %s found, loading from statically linked\\n\", function_name));\n            dlh = dlopen(NULL, RTLD_NOW | RTLD_NODELETE);\n            if(NULL == dlh) ERROR((\"Error parsing static libs: %s\\n\", dlerror()));\n            fn = dlsym(dlh, function_name);\n            if(env && fn) WARNING((\"Internal static function %s used (because library %s didn't loaded correctly)\\n\", function_name, library_name));\n            dlclose(dlh);\n        }\n\n        /* Still not found?? skip this GPU */\n        if(NULL == fn) {\n            STATUS((\"No function %s found for GPU %d\\n\", function_name, i));\n            if(minor > 0) {\n                minor--;\n                goto retry_lesser_sm_version;\n            } else\n            {\n                major--; minor = 9;\n                if(major > 0) goto retry_lesser_sm_version;\n            }\n            status = cuCtxPopCurrent(NULL);\n            continue;\n        }\n\n        status = cuCtxPopCurrent(NULL);\n        DAGUE_CUDA_CHECK_ERROR( \"(INIT) cuCtxPopCurrent \", status,\n                                {continue;} );\n\n        gpu_device->index = (uint8_t)dindex;\n        zgemm_functions[dindex] = (cuda_zgemm_t)fn;\n        gpu_enabled_devices[dindex++] = gpu_device;\n    }\n\n    /* Update the number of GPUs available */\n    dague_data_enable_gpu( dindex );\n    ndevices = dindex;\n    assert( nbgpus == ndevices ); /* the code for when some devices can load some functions but not others is not yet correct, blanket protection against this */\n\n    return 0;\n}\n\n/**\n *  This function schedule the move of all the data required for a\n *  specific task from the main memory into the GPU memory.\n *\n *  Returns:\n *     a positive number: the number of data to be moved.\n *     -1: data cannot be moved into the GPU.\n *     -2: No more room on the GPU to move this data.\n */\nstatic inline int\ngpu_kernel_push_zgemm( gpu_device_t        *gpu_device,\n                       dague_gpu_context_t *gpu_task,\n                       dague_gpu_exec_stream_t* gpu_stream)\n{\n    int ret, move_data_count = 0;\n    int sizeloc[MAX_PARAM_COUNT];\n    dague_execution_context_t *this_task = gpu_task->ec;\n    dague_zgemm_args_t        *args = (dague_zgemm_args_t*)gpu_task;\n\n    /* WARNING: A has to be the first data,\n     *          B the second one and\n     *          C the third one.\n     * if the kernel swap A and B it won't work\n     */\n    moesi_get_master(args->ddescA->moesi_map, TILED_MATRIX_KEY(args->ddescA, args->Am, args->An ),\n                     &(this_task->data[flow_A].moesi_master));\n    if( NULL == (this_task->data[flow_A].moesi_master)->device_copies[gpu_device->index])\n        move_data_count++;\n\n    moesi_get_master(args->ddescB->moesi_map, TILED_MATRIX_KEY(args->ddescB, args->Bm, args->Bn ),\n                     &(this_task->data[flow_B].moesi_master));\n    if( NULL == (this_task->data[flow_B].moesi_master)->device_copies[gpu_device->index])\n        move_data_count++;\n\n    moesi_get_master(args->ddescC->moesi_map, TILED_MATRIX_KEY(args->ddescC, args->Cm, args->Cn ),\n                     &(this_task->data[flow_C].moesi_master));\n    if( NULL == (this_task->data[flow_C].moesi_master)->device_copies[gpu_device->index])\n        move_data_count++;\n\n    this_task->data[3].moesi_master =  NULL;  /* last element */\n\n    if( 0 != move_data_count ) { /* Try to reserve enough room for all data */\n        sizeloc[flow_A] = args->sizeA;\n        sizeloc[flow_B] = args->sizeB;\n        sizeloc[flow_C] = args->sizeC;\n\n        ret = dague_gpu_data_reserve_device_space( gpu_device,\n                                                   this_task,\n                                                   sizeloc,\n                                                   move_data_count );\n        if( ret < 0 ) {\n            goto release_and_return_error;\n        }\n    }\n\n    assert( NULL != gpu_elem_obtain_from_master(this_task->data[flow_A].moesi_master, gpu_device->index) );\n    assert( NULL != gpu_elem_obtain_from_master(this_task->data[flow_B].moesi_master, gpu_device->index) );\n    assert( NULL != gpu_elem_obtain_from_master(this_task->data[flow_C].moesi_master, gpu_device->index) );\n\n    DAGUE_TASK_PROF_TRACE_IF(gpu_stream->prof_event_track_enable,\n                             gpu_stream->profiling,\n                             (-1 == gpu_stream->prof_event_key_start ?\n                              DAGUE_PROF_FUNC_KEY_START(this_task->dague_object,\n                                                        this_task->function->function_id) :\n                              gpu_stream->prof_event_key_start),\n                             this_task);\n\n    DEBUG3((\"GPU[%1d]:\\tIN  Data of %s(%d, %d) on GPU\\n\", gpu_device->device_index,\n            this_task->function->in[flow_A]->name, args->Am, args->An));\n    ret = dague_gpu_data_stage_in( gpu_device, this_task->function->in[flow_A]->flow_flags,\n                                   &(this_task->data[flow_A]), args->sizeA, gpu_stream->cuda_stream );\n    if( ret < 0 ) {\n        goto release_and_return_error;\n    }\n\n    DEBUG3((\"GPU[%1d]:\\tIN  Data of %s(%d, %d) on GPU\\n\", gpu_device->device_index,\n            this_task->function->in[flow_B]->name, args->Bm, args->Bn));\n    ret = dague_gpu_data_stage_in( gpu_device, this_task->function->in[flow_B]->flow_flags,\n                                   &(this_task->data[flow_B]), args->sizeB, gpu_stream->cuda_stream );\n    if( ret < 0 ) {\n        goto release_and_return_error;\n    }\n\n    DEBUG3((\"GPU[%1d]:\\tIN  Data of %s(%d, %d) on GPU\\n\", gpu_device->device_index,\n            this_task->function->in[flow_C]->name, args->Cm, args->Cn));\n    ret = dague_gpu_data_stage_in( gpu_device, this_task->function->in[flow_C]->flow_flags,\n                                   &(this_task->data[flow_C]), args->sizeC, gpu_stream->cuda_stream );\n    if( ret < 0 ) {\n        goto release_and_return_error;\n    }\n  release_and_return_error:\n    return ret;\n}\n\n\nstatic inline int\ngpu_kernel_submit_zgemm( gpu_device_t        *gpu_device,\n                         dague_gpu_context_t *gpu_task,\n                         dague_gpu_exec_stream_t* gpu_stream )\n{\n    dague_execution_context_t *this_task = gpu_task->ec;\n    dague_zgemm_args_t        *args = (dague_zgemm_args_t*)gpu_task;\n    gpu_elem_t *gpu_elem_A = NULL, *gpu_elem_B = NULL, *gpu_elem_C = NULL;\n    CUdeviceptr d_A, d_B, d_C;\n    cudaError_t status;\n#if DAGUE_DEBUG_VERBOSE >= 2\n    char tmp[MAX_TASK_STRLEN];\n#endif\n\n    cuda_zgemm_t cuda_zgemm = zgemm_functions[gpu_device->index];\n\n    gpu_elem_A = gpu_elem_obtain_from_master(this_task->data[flow_A].moesi_master, gpu_device->index);\n    gpu_elem_B = gpu_elem_obtain_from_master(this_task->data[flow_B].moesi_master, gpu_device->index);\n    gpu_elem_C = gpu_elem_obtain_from_master(this_task->data[flow_C].moesi_master, gpu_device->index);\n    d_A = gpu_elem_A->gpu_mem_ptr;\n    d_B = gpu_elem_B->gpu_mem_ptr;\n    d_C = gpu_elem_C->gpu_mem_ptr;\n\n    DEBUG2(( \"GPU[%1d]:\\tEnqueue on device %s priority %d\\n\", gpu_device->device_index,\n             dague_snprintf_execution_context(tmp, MAX_TASK_STRLEN, this_task),\n             this_task->priority ));\n\n    DAGUE_TASK_PROF_TRACE_IF(gpu_stream->prof_event_track_enable,\n                             gpu_stream->profiling,\n                             (-1 == gpu_stream->prof_event_key_start ?\n                              DAGUE_PROF_FUNC_KEY_START(this_task->dague_object,\n                                                        this_task->function->function_id) :\n                              gpu_stream->prof_event_key_start),\n                             this_task);\n\n    status = cudaSuccess;\n    cuda_zgemm( lapack_const(args->transA), lapack_const(args->transB), args->M, args->N, args->K,\n                args->alpha, (dague_complex64_t*)d_A, args->lda,\n                             (dague_complex64_t*)d_B, args->ldb,\n                args->beta,  (dague_complex64_t*)d_C, args->ldc,\n                gpu_stream->cuda_stream );\n\n    DAGUE_CUDA_CHECK_ERROR( \"cuLaunchGridAsync \", status,\n                              {return -1;} );\n\n    return 0;\n}\n\n/**\n *  This function schedule the move of all the modified data for a\n *  specific task from the GPU memory into the main memory.\n *\n *  Returns: negative number if any error occured.\n *           positive: the number of data to be moved.\n */\nstatic inline int\ngpu_kernel_pop_zgemm( gpu_device_t        *gpu_device,\n                      dague_gpu_context_t *gpu_task,\n                      dague_gpu_exec_stream_t* gpu_stream)\n{\n    dague_execution_context_t *this_task = gpu_task->ec;\n    dague_zgemm_args_t        *args = (dague_zgemm_args_t*)gpu_task;\n    gpu_elem_t *gpu_elem = NULL;\n    int return_code = 0, how_many = 0, i;\n    cudaError_t status;\n\n    for( i = 0; NULL != this_task->function->in[i]; i++ ) {\n        gpu_elem = gpu_elem_obtain_from_master(this_task->data[i].moesi_master, gpu_device->index);\n        if( this_task->function->in[i]->flow_flags & FLOW_ACCESS_READ ) {\n            gpu_elem->moesi.readers--; assert(gpu_elem->moesi.readers >= 0);\n            if( (0 == gpu_elem->moesi.readers) &&\n                !(this_task->function->in[i]->flow_flags & FLOW_ACCESS_WRITE) ) {\n                dague_list_item_ring_chop((dague_list_item_t*)gpu_elem);\n                DAGUE_LIST_ITEM_CONSTRUCT(gpu_elem); /* TODO: singleton instead? */\n                dague_ulist_fifo_push(gpu_device->gpu_mem_lru, (dague_list_item_t*)gpu_elem);\n            }\n        }\n        if( this_task->function->in[i]->flow_flags & FLOW_ACCESS_WRITE ) {\n            gpu_elem = gpu_elem_obtain_from_master(this_task->data[i].moesi_master, gpu_device->index);\n\n            /* Stage the transfer of the data back to main memory */\n            gpu_device->required_data_out += args->sizeC;\n            assert( ((dague_list_item_t*)gpu_elem)->list_next == (dague_list_item_t*)gpu_elem );\n            assert( ((dague_list_item_t*)gpu_elem)->list_prev == (dague_list_item_t*)gpu_elem );\n\n            if( args->pushout ) {  /* n == (k + 1) */\n                DEBUG3((\"GPU[%1d]:\\tOUT Data of %s key %d\\n\", gpu_device->device_index,\n                        this_task->function->in[i]->name, this_task->data[i].moesi_master->key));\n                DAGUE_TASK_PROF_TRACE_IF(gpu_stream->prof_event_track_enable,\n                                         gpu_stream->profiling,\n                                         (-1 == gpu_stream->prof_event_key_start ?\n                                          DAGUE_PROF_FUNC_KEY_START(this_task->dague_object,\n                                                                    this_task->function->function_id) :\n                                          gpu_stream->prof_event_key_start),\n                                         this_task);\n                /* Move the data back into main memory */\n                DEBUG3((\"GPU:\\tMove D2H data %x (D %p:H %p) %d bytes to GPU %d\\n\",\n                        this_task->data[i].moesi_master->key, (void*)gpu_elem->gpu_mem_ptr,\n                        ADATA(this_task->data[i].data), args->sizeC, gpu_device->device_index));\n                status = (cudaError_t)cuMemcpyDtoHAsync( ADATA(this_task->data[i].data),\n                                                         gpu_elem->gpu_mem_ptr, args->sizeC, gpu_stream->cuda_stream );\n                DAGUE_CUDA_CHECK_ERROR( \"cuMemcpyDtoHAsync from device \", status,\n                                        { WARNING((\"data %s <<%p>> -> <<%p>>\\n\", this_task->function->in[i]->name,\n                                                  (void*)gpu_elem->gpu_mem_ptr, (void*)ADATA(this_task->data[i].data)));\n                                          return_code = -2;\n                                          goto release_and_return_error;} );\n                gpu_device->transferred_data_out += args->sizeC; /* TODO: not hardcoded, use datatype size */\n                how_many++;\n            }\n        }\n    }\n\n release_and_return_error:\n    return (return_code < 0 ? return_code : how_many);\n}\n\n/**\n * Make sure all data on the device is correctly put back into the queues.\n */\nstatic inline int\ngpu_kernel_epilog_zgemm( gpu_device_t        *gpu_device,\n                         dague_gpu_context_t *gpu_task )\n{\n    dague_execution_context_t *this_task = gpu_task->ec;\n    dague_zgemm_args_t        *args = (dague_zgemm_args_t*)gpu_task;\n    gpu_elem_t* gpu_elem;\n    moesi_master_t* master;\n    int i;\n\n    for( i = 0; NULL != (master = this_task->data[i].moesi_master); i++ ) {\n        if( !(this_task->function->in[i]->flow_flags & FLOW_ACCESS_WRITE) ) continue;\n\n        gpu_elem = gpu_elem_obtain_from_master(master, gpu_device->index);\n        assert( MOESI_OWNED == gpu_elem->moesi.coherency_state );\n        gpu_elem->moesi.coherency_state = MOESI_SHARED;\n        master->version = gpu_elem->moesi.version;\n        master->owner_device = -1;\n\n        if( args->pushout ) {  /* n == (k  + 1) */\n            dague_ulist_fifo_push(gpu_device->gpu_mem_lru, (dague_list_item_t*)gpu_elem);\n        } else {\n            dague_ulist_fifo_push(gpu_device->gpu_mem_owned_lru, (dague_list_item_t*)gpu_elem);\n        }\n    }\n    return 0;\n}\n\n\n/**\n * Try to execute a GEMM on a GPU.\n *\n * Returns:\n *  0 - if the GEMM should be executed by some other meaning (in this case the\n *         execution context is not released).\n * -1 - if the GEMM is scheduled to be executed on a GPU.\n */\n\n/**\n * This version is based on 4 streams: one for transfers from the memory to\n * the GPU, 2 for kernel executions and one for tranfers from the GPU into\n * the main memory. The synchronization on each stream is based on CUDA events,\n * such an event indicate that a specific epoch of the lifetime of a task has\n * been completed. Each type of stream (in, exec and out) has a pending FIFO,\n * where tasks ready to jump to the respective step are waiting.\n */\nint gpu_zgemm( dague_execution_unit_t* eu_context,\n               dague_execution_context_t* this_task,\n               int pushout,\n               PLASMA_enum transA, PLASMA_enum transB,\n               int M, int N, int K,\n               dague_complex64_t alpha, int Am, int An, const tiled_matrix_desc_t *descA, int lda,\n                                        int Bm, int Bn, const tiled_matrix_desc_t *descB, int ldb,\n               dague_complex64_t beta,  int Cm, int Cn, const tiled_matrix_desc_t *descC, int ldc )\n{\n    int which_gpu;\n    dague_zgemm_args_t *gpu_task = (dague_zgemm_args_t*)malloc(sizeof(dague_zgemm_args_t));\n\n    DAGUE_LIST_ITEM_CONSTRUCT(gpu_task);\n    gpu_task->super.ec = this_task;\n    gpu_task->pushout  = pushout;\n    gpu_task->alpha    = alpha;\n    gpu_task->beta     = beta;\n    gpu_task->transA   = transA;\n    gpu_task->transB   = transB;\n    gpu_task->M        = M;\n    gpu_task->N        = N;\n    gpu_task->K        = K;\n    gpu_task->Am       = Am;\n    gpu_task->An       = An;\n    gpu_task->lda      = lda;\n    gpu_task->Bm       = Bm;\n    gpu_task->Bn       = Bn;\n    gpu_task->ldb      = ldb;\n    gpu_task->Cm       = Cm;\n    gpu_task->Cn       = Cn;\n    gpu_task->ldc      = ldc;\n    gpu_task->sizeA    = sizeof(dague_complex64_t) * (size_t)lda * (( transA == PlasmaNoTrans ) ? K : M );\n    gpu_task->sizeB    = sizeof(dague_complex64_t) * (size_t)ldb * (( transB == PlasmaNoTrans ) ? N : K );\n    gpu_task->sizeC    = sizeof(dague_complex64_t) * (size_t)ldc * N;\n    gpu_task->ddescA   = (dague_ddesc_t*)descA;\n    gpu_task->ddescB   = (dague_ddesc_t*)descB;\n    gpu_task->ddescC   = (dague_ddesc_t*)descC;\n\n    /* We always schedule the task on the GPU owning the C tile. */\n    which_gpu = moesi_locate_device_with_valid_copy( descC->super.moesi_map, TILED_MATRIX_KEY( descC, Cm, Cn) );\n    if( which_gpu < 0 ) {  /* this is the first time we see this tile.\n                            * Let's decide which GPU will work on it. */\n        int best_index = -1;  /* cores */\n        /* There are 3 types of GEMMs kernels: the ones waiting on the\n         * execution contextes queues to be investigated, the current one\n         * which is investigated for execution on the context of the current\n         * execution context, and the ones already queued on the GPUs. The\n         * decision regarding the status of the current GEMM should be therefore\n         * based only on the number of pending tasks on the GPUs.\n         */\n        float weight, best_weight = device_load[0] + device_weight[0];\n        for( which_gpu = 0; which_gpu < ndevices; which_gpu++ ) {\n            weight = device_load[which_gpu+1] + device_weight[which_gpu+1];\n            if( best_weight > weight ) {\n                best_index = which_gpu;\n                best_weight = weight;\n            }\n        }\n        if( best_index == -1 ) {\n            dague_atomic_inc_32b( &dague_cpu_counter );\n            return -99;\n        }\n        which_gpu = best_index;\n    }\n    /* Update the load of the selected GPU */\n    device_load[which_gpu+1] += device_weight[which_gpu+1];\n\n    return gpu_kernel_scheduler_zgemm( eu_context, (dague_gpu_context_t*)gpu_task, which_gpu );\n}\n"
    },
    "skipped": [],
    "total_files": 618
}