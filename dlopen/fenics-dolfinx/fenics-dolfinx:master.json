{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-fenics-dolfinx-master-bi2pqtkbnpdp2ztbtnp3iosgxb7e3ib4/spack-src/python/dolfinx/__init__.py": "# Copyright (C) 2017 Chris N. Richardson and Garth N. Wells\n#\n# This file is part of DOLFINX (https://www.fenicsproject.org)\n#\n# SPDX-License-Identifier:    LGPL-3.0-or-later\n\"\"\"Main module for DOLFINX\"\"\"\n\n# flake8: noqa\n\n# Store dl open flags to restore them after import\nimport sys\nstored_dlopen_flags = sys.getdlopenflags()\n\n# Developer note: below is related to OpenMPI\n# Fix dlopen flags (may need reorganising)\nif \"linux\" in sys.platform:\n    # FIXME: What with other platforms?\n    try:\n        from ctypes import RTLD_NOW, RTLD_GLOBAL\n    except ImportError:\n        RTLD_NOW = 2\n        RTLD_GLOBAL = 256\n    sys.setdlopenflags(RTLD_NOW | RTLD_GLOBAL)\ndel sys\n\n# Reset dl open flags\n# sys.setdlopenflags(stored_dlopen_flags)\n# del sys\n\n# Import cpp modules\nfrom .cpp import __version__\n\n\nfrom dolfinx.common import (has_debug, has_petsc_complex, has_kahip,\n                           has_parmetis, git_commit_hash, TimingType, timing,\n                           list_timings)\n\nimport dolfinx.log\n\nfrom dolfinx.generation import (IntervalMesh, BoxMesh, RectangleMesh,\n                               UnitIntervalMesh, UnitSquareMesh, UnitCubeMesh)\n\nfrom .cpp.mesh import Topology, Geometry\n\nfrom .cpp.nls import NewtonSolver\n\nfrom .fem.form import Form\nfrom .fem.dirichletbc import DirichletBC\nfrom .fem import (FunctionSpace, VectorFunctionSpace,\n                  TensorFunctionSpace, Constant, Expression, Function)\n\nfrom .mesh import MeshTags\n\n# Initialise logging\nfrom dolfinx import cpp\nimport sys\ncpp.common.init_logging(sys.argv)\ndel sys\n\ndef get_include(user=False):\n    import os\n    d = os.path.dirname(__file__)\n    if os.path.exists(os.path.join(d, \"include\")):\n        # Package is installed\n        return os.path.join(d, \"include\")\n    else:\n        # Package is from a source directory\n        return os.path.join(os.path.dirname(d), \"src\")\n",
        "/tmp/vanessa/spack-stage/spack-stage-fenics-dolfinx-master-bi2pqtkbnpdp2ztbtnp3iosgxb7e3ib4/spack-src/python/test/unit/fem/test_custom_assembler.py": "# Copyright (C) 2019-2020 Garth N. Wells\n#\n# This file is part of DOLFINX (https://www.fenicsproject.org)\n#\n# SPDX-License-Identifier:    LGPL-3.0-or-later\n\"\"\"Tests for custom Python assemblers\"\"\"\n\nimport ctypes\nimport ctypes.util\nimport importlib\nimport math\nimport os\nimport pathlib\nimport time\n\nimport cffi\nimport dolfinx\nimport numba\nimport numba.core.typing.cffi_utils as cffi_support\nimport numpy as np\nimport petsc4py.lib\nimport pytest\nimport ufl\nfrom dolfinx.jit import dolfinx_pc\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\nfrom petsc4py import get_config as PETSc_get_config\nfrom ufl import dx, inner\n\n# Get details of PETSc install\npetsc_dir = PETSc_get_config()['PETSC_DIR']\npetsc_arch = petsc4py.lib.getPathArchPETSc()[1]\n\n\n# Get PETSc int and scalar types\nif np.dtype(PETSc.ScalarType).kind == 'c':\n    complex = True\nelse:\n    complex = False\n\nscalar_size = np.dtype(PETSc.ScalarType).itemsize\nindex_size = np.dtype(PETSc.IntType).itemsize\n\nif index_size == 8:\n    c_int_t = \"int64_t\"\n    ctypes_index = ctypes.c_int64\nelif index_size == 4:\n    c_int_t = \"int32_t\"\n    ctypes_index = ctypes.c_int32\nelse:\n    raise RuntimeError(\"Cannot translate PETSc index size into a C type, index_size: {}.\".format(index_size))\n\nif complex and scalar_size == 16:\n    c_scalar_t = \"double _Complex\"\n    numba_scalar_t = numba.types.complex128\nelif complex and scalar_size == 8:\n    c_scalar_t = \"float _Complex\"\n    numba_scalar_t = numba.types.complex64\nelif not complex and scalar_size == 8:\n    c_scalar_t = \"double\"\n    numba_scalar_t = numba.types.float64\nelif not complex and scalar_size == 4:\n    c_scalar_t = \"float\"\n    numba_scalar_t = numba.types.float32\nelse:\n    raise RuntimeError(\n        \"Cannot translate PETSc scalar type to a C type, complex: {} size: {}.\".format(complex, scalar_size))\n\n\n# Load PETSc library via ctypes\npetsc_lib_name = ctypes.util.find_library(\"petsc\")\nif petsc_lib_name is not None:\n    petsc_lib_ctypes = ctypes.CDLL(petsc_lib_name)\nelse:\n    try:\n        petsc_lib_ctypes = ctypes.CDLL(os.path.join(petsc_dir, petsc_arch, \"lib\", \"libpetsc.so\"))\n    except OSError:\n        petsc_lib_ctypes = ctypes.CDLL(os.path.join(petsc_dir, petsc_arch, \"lib\", \"libpetsc.dylib\"))\n    except OSError:\n        print(\"Could not load PETSc library for CFFI (ABI mode).\")\n        raise\n\n# Get the PETSc MatSetValuesLocal function via ctypes\nMatSetValues_ctypes = petsc_lib_ctypes.MatSetValuesLocal\nMatSetValues_ctypes.argtypes = (ctypes.c_void_p, ctypes_index, ctypes.POINTER(\n    ctypes_index), ctypes_index, ctypes.POINTER(ctypes_index), ctypes.c_void_p, ctypes.c_int)\ndel petsc_lib_ctypes\n\n\n# CFFI - register complex types\nffi = cffi.FFI()\ncffi_support.register_type(ffi.typeof('double _Complex'), numba.types.complex128)\ncffi_support.register_type(ffi.typeof('float _Complex'), numba.types.complex64)\n\n# Get MatSetValuesLocal from PETSc available via cffi in ABI mode\nffi.cdef(\"\"\"int MatSetValuesLocal(void* mat, {0} nrow, const {0}* irow,\n                                  {0} ncol, const {0}* icol, const {1}* y, int addv);\n\"\"\".format(c_int_t, c_scalar_t))\n\n\nif petsc_lib_name is not None:\n    petsc_lib_cffi = ffi.dlopen(petsc_lib_name)\nelse:\n    try:\n        petsc_lib_cffi = ffi.dlopen(os.path.join(petsc_dir, petsc_arch, \"lib\", \"libpetsc.so\"))\n    except OSError:\n        petsc_lib_cffi = ffi.dlopen(os.path.join(petsc_dir, petsc_arch, \"lib\", \"libpetsc.dylib\"))\n    except OSError:\n        print(\"Could not load PETSc library for CFFI (ABI mode).\")\n        raise\nMatSetValues_abi = petsc_lib_cffi.MatSetValuesLocal\n\n\n# @pytest.fixture\ndef get_matsetvalues_api():\n    \"\"\"Make MatSetValuesLocal from PETSc available via cffi in API mode\"\"\"\n    worker = os.getenv('PYTEST_XDIST_WORKER', None)\n    module_name = \"_petsc_cffi_{}\".format(worker)\n    if MPI.COMM_WORLD.Get_rank() == 0:\n        ffibuilder = cffi.FFI()\n        ffibuilder.cdef(\"\"\"\n            typedef int... PetscInt;\n            typedef ... PetscScalar;\n            typedef int... InsertMode;\n            int MatSetValuesLocal(void* mat, PetscInt nrow, const PetscInt* irow,\n                                PetscInt ncol, const PetscInt* icol,\n                                const PetscScalar* y, InsertMode addv);\n        \"\"\")\n        ffibuilder.set_source(module_name, \"\"\"\n            #include \"petscmat.h\"\n        \"\"\",\n                              libraries=['petsc'],\n                              include_dirs=[os.path.join(petsc_dir, petsc_arch, 'include'),\n                                            os.path.join(petsc_dir, 'include')] + dolfinx_pc[\"include_dirs\"],\n                              library_dirs=[os.path.join(petsc_dir, petsc_arch, 'lib')],\n                              extra_compile_args=[])\n\n        # Build module in same directory as test file\n        path = pathlib.Path(__file__).parent.absolute()\n        ffibuilder.compile(tmpdir=path, verbose=True)\n\n    MPI.COMM_WORLD.Barrier()\n\n    spec = importlib.util.find_spec(module_name)\n    if spec is None:\n        raise ImportError(\"Failed to find CFFI generated module\")\n    module = importlib.util.module_from_spec(spec)\n\n    cffi_support.register_module(module)\n    cffi_support.register_type(module.ffi.typeof(\"PetscScalar\"), numba_scalar_t)\n    return module.lib.MatSetValuesLocal\n\n\n# See https://github.com/numba/numba/issues/4036 for why we need 'sink'\n@numba.njit\ndef sink(*args):\n    pass\n\n\n@numba.njit(fastmath=True)\ndef area(x0, x1, x2) -> float:\n    \"\"\"Compute the area of a triangle embedded in 2D from the three vertices\"\"\"\n    a = (x1[0] - x2[0])**2 + (x1[1] - x2[1])**2\n    b = (x0[0] - x2[0])**2 + (x0[1] - x2[1])**2\n    c = (x0[0] - x1[0])**2 + (x0[1] - x1[1])**2\n    return math.sqrt(2 * (a * b + a * c + b * c) - (a**2 + b**2 + c**2)) / 4.0\n\n\n@numba.njit(fastmath=True)\ndef assemble_vector(b, mesh, dofmap, num_cells):\n    \"\"\"Assemble simple linear form over a mesh into the array b\"\"\"\n    v, x = mesh\n    q0, q1 = 1 / 3.0, 1 / 3.0\n    for cell in range(num_cells):\n        # FIXME: This assumes a particular geometry dof layout\n        A = area(x[v[cell, 0]], x[v[cell, 1]], x[v[cell, 2]])\n        b[dofmap[cell, 0]] += A * (1.0 - q0 - q1)\n        b[dofmap[cell, 1]] += A * q0\n        b[dofmap[cell, 2]] += A * q1\n\n\n@numba.njit(parallel=True, fastmath=True)\ndef assemble_vector_parallel(b, v, x, dofmap_t_data, dofmap_t_offsets, num_cells):\n    \"\"\"Assemble simple linear form over a mesh into the array b using a parallel loop\"\"\"\n    q0 = 1 / 3.0\n    q1 = 1 / 3.0\n    b_unassembled = np.empty((num_cells, 3), dtype=b.dtype)\n    for cell in numba.prange(num_cells):\n        # FIXME: This assumes a particular geometry dof layout\n        A = area(x[v[cell, 0]], x[v[cell, 1]], x[v[cell, 2]])\n        b_unassembled[cell, 0] = A * (1.0 - q0 - q1)\n        b_unassembled[cell, 1] = A * q0\n        b_unassembled[cell, 2] = A * q1\n\n    # Accumulate values in RHS\n    _b_unassembled = b_unassembled.reshape(num_cells * 3)\n    for index in numba.prange(dofmap_t_offsets.shape[0] - 1):\n        for p in range(dofmap_t_offsets[index], dofmap_t_offsets[index + 1]):\n            b[index] += _b_unassembled[dofmap_t_data[p]]\n\n\n@numba.njit(fastmath=True)\ndef assemble_vector_ufc(b, kernel, mesh, dofmap, num_cells):\n    \"\"\"Assemble provided FFCX/UFC kernel over a mesh into the array b\"\"\"\n    v, x = mesh\n    entity_local_index = np.array([0], dtype=np.intc)\n    perm = np.array([0], dtype=np.uint8)\n    geometry = np.zeros((3, 2))\n    coeffs = np.zeros(1, dtype=PETSc.ScalarType)\n    constants = np.zeros(1, dtype=PETSc.ScalarType)\n\n    b_local = np.zeros(3, dtype=PETSc.ScalarType)\n    for cell in range(num_cells):\n        # FIXME: This assumes a particular geometry dof layout\n        for j in range(3):\n            geometry[j] = x[v[cell, j], 0:2]\n        b_local.fill(0.0)\n        kernel(ffi.from_buffer(b_local), ffi.from_buffer(coeffs),\n               ffi.from_buffer(constants),\n               ffi.from_buffer(geometry), ffi.from_buffer(entity_local_index),\n               ffi.from_buffer(perm), 0)\n        for j in range(3):\n            b[dofmap[cell, j]] += b_local[j]\n\n\n@numba.njit(fastmath=True)\ndef assemble_matrix_cffi(A, mesh, dofmap, num_cells, set_vals, mode):\n    \"\"\"Assemble P1 mass matrix over a mesh into the PETSc matrix A\"\"\"\n\n    # Mesh data\n    v, x = mesh\n\n    # Quadrature points and weights\n    q = np.array([[0.5, 0.0], [0.5, 0.5], [0.0, 0.5]], dtype=np.double)\n    weights = np.full(3, 1.0 / 3.0, dtype=np.double)\n\n    # Loop over cells\n    N = np.empty(3, dtype=np.double)\n    A_local = np.empty((3, 3), dtype=PETSc.ScalarType)\n    for cell in range(num_cells):\n        cell_area = area(x[v[cell, 0]], x[v[cell, 1]], x[v[cell, 2]])\n\n        # Loop over quadrature points\n        A_local[:] = 0.0\n        for j in range(q.shape[0]):\n            N[0], N[1], N[2] = 1.0 - q[j, 0] - q[j, 1], q[j, 0], q[j, 1]\n            for row in range(3):\n                for col in range(3):\n                    A_local[row, col] += weights[j] * cell_area * N[row] * N[col]\n\n        # Add to global tensor\n        pos = dofmap[cell, :]\n        set_vals(A, 3, ffi.from_buffer(pos), 3, ffi.from_buffer(pos), ffi.from_buffer(A_local), mode)\n    sink(A_local, dofmap)\n\n\n@numba.njit\ndef assemble_matrix_ctypes(A, mesh, dofmap, num_cells, set_vals, mode):\n    \"\"\"Assemble P1 mass matrix over a mesh into the PETSc matrix A\"\"\"\n    v, x = mesh\n    q = np.array([[0.5, 0.0], [0.5, 0.5], [0.0, 0.5]], dtype=np.double)\n    weights = np.full(3, 1.0 / 3.0, dtype=np.double)\n\n    # Loop over cells\n    N = np.empty(3, dtype=np.double)\n    A_local = np.empty((3, 3), dtype=PETSc.ScalarType)\n    for cell in range(num_cells):\n        # FIXME: This assumes a particular geometry dof layout\n        cell_area = area(x[v[cell, 0]], x[v[cell, 1]], x[v[cell, 2]])\n\n        # Loop over quadrature points\n        A_local[:] = 0.0\n        for j in range(q.shape[0]):\n            N[0], N[1], N[2] = 1.0 - q[j, 0] - q[j, 1], q[j, 0], q[j, 1]\n            for row in range(3):\n                for col in range(3):\n                    A_local[row, col] += weights[j] * cell_area * N[row] * N[col]\n\n        rows = cols = dofmap[cell, :]\n        set_vals(A, 3, rows.ctypes, 3, cols.ctypes, A_local.ctypes, mode)\n\n\ndef test_custom_mesh_loop_rank1():\n\n    # Create mesh and function space\n    mesh = dolfinx.generation.UnitSquareMesh(MPI.COMM_WORLD, 64, 64)\n    V = dolfinx.FunctionSpace(mesh, (\"Lagrange\", 1))\n\n    # Unpack mesh and dofmap data\n    num_owned_cells = mesh.topology.index_map(mesh.topology.dim).size_local\n    num_cells = num_owned_cells + mesh.topology.index_map(mesh.topology.dim).num_ghosts\n    x_dofs = mesh.geometry.dofmap.array.reshape(num_cells, 3)\n    x = mesh.geometry.x\n    dofmap = V.dofmap.list.array.reshape(num_cells, 3)\n    dofmap_t = dolfinx.cpp.fem.transpose_dofmap(V.dofmap.list, num_owned_cells)\n\n    # Assemble with pure Numba function (two passes, first will include\n    # JIT overhead)\n    b0 = dolfinx.Function(V)\n    for i in range(2):\n        with b0.vector.localForm() as b:\n            b.set(0.0)\n            start = time.time()\n            assemble_vector(np.asarray(b), (x_dofs, x), dofmap, num_owned_cells)\n            end = time.time()\n            print(\"Time (numba, pass {}): {}\".format(i, end - start))\n    b0.vector.ghostUpdate(addv=PETSc.InsertMode.ADD, mode=PETSc.ScatterMode.REVERSE)\n    assert b0.vector.sum() == pytest.approx(1.0)\n\n    # Assemble with pure Numba function using parallel loop (two passes,\n    # first will include JIT overhead)\n    btmp = dolfinx.Function(V)\n    for i in range(2):\n        with btmp.vector.localForm() as b:\n            b.set(0.0)\n            start = time.time()\n            assemble_vector_parallel(np.asarray(b), x_dofs, x,\n                                     dofmap_t.array, dofmap_t.offsets,\n                                     num_owned_cells)\n            end = time.time()\n            print(\"Time (numba parallel, pass {}): {}\".format(i, end - start))\n    btmp.vector.ghostUpdate(addv=PETSc.InsertMode.ADD, mode=PETSc.ScatterMode.REVERSE)\n    assert (btmp.vector - b0.vector).norm() == pytest.approx(0.0)\n\n    # Test against generated code and general assembler\n    v = ufl.TestFunction(V)\n    L = inner(1.0, v) * dx\n    start = time.time()\n    b1 = dolfinx.fem.assemble_vector(L)\n    end = time.time()\n    print(\"Time (C++, pass 0):\", end - start)\n\n    with b1.localForm() as b_local:\n        b_local.set(0.0)\n    start = time.time()\n    dolfinx.fem.assemble_vector(b1, L)\n    end = time.time()\n    print(\"Time (C++, pass 1):\", end - start)\n    b1.ghostUpdate(addv=PETSc.InsertMode.ADD, mode=PETSc.ScatterMode.REVERSE)\n    assert (b1 - b0.vector).norm() == pytest.approx(0.0)\n\n    # Assemble using generated tabulate_tensor kernel and Numba assembler\n    b3 = dolfinx.Function(V)\n    ufc_form = dolfinx.jit.ffcx_jit(mesh.mpi_comm(), L)\n    kernel = ufc_form.create_cell_integral(-1).tabulate_tensor\n    for i in range(2):\n        with b3.vector.localForm() as b:\n            b.set(0.0)\n            start = time.time()\n            assemble_vector_ufc(np.asarray(b), kernel, (x_dofs, x), dofmap, num_owned_cells)\n            end = time.time()\n            print(\"Time (numba/cffi, pass {}): {}\".format(i, end - start))\n    b3.vector.ghostUpdate(addv=PETSc.InsertMode.ADD, mode=PETSc.ScatterMode.REVERSE)\n    assert (b3.vector - b0.vector).norm() == pytest.approx(0.0)\n\n\ndef test_custom_mesh_loop_ctypes_rank2():\n    \"\"\"Test numba assembler for bilinear form\"\"\"\n\n    # Create mesh and function space\n    mesh = dolfinx.generation.UnitSquareMesh(MPI.COMM_WORLD, 64, 64)\n    V = dolfinx.FunctionSpace(mesh, (\"Lagrange\", 1))\n\n    # Extract mesh and dofmap data\n    num_owned_cells = mesh.topology.index_map(mesh.topology.dim).size_local\n    num_cells = num_owned_cells + mesh.topology.index_map(mesh.topology.dim).num_ghosts\n    x_dofs = mesh.geometry.dofmap.array.reshape(num_cells, 3)\n    x = mesh.geometry.x\n    dofmap = V.dofmap.list.array.reshape(num_cells, 3).astype(np.dtype(PETSc.IntType))\n\n    # Generated case with general assembler\n    u, v = ufl.TrialFunction(V), ufl.TestFunction(V)\n    a = inner(u, v) * dx\n    A0 = dolfinx.fem.assemble_matrix(a)\n    A0.assemble()\n    A0.zeroEntries()\n\n    start = time.time()\n    dolfinx.fem.assemble_matrix(A0, a)\n    end = time.time()\n    print(\"Time (C++, pass 2):\", end - start)\n    A0.assemble()\n\n    # Custom case\n    A1 = A0.copy()\n    for i in range(2):\n        A1.zeroEntries()\n        mat = A1.handle\n        start = time.time()\n        assemble_matrix_ctypes(mat, (x_dofs, x), dofmap, num_owned_cells,\n                               MatSetValues_ctypes, PETSc.InsertMode.ADD_VALUES)\n        end = time.time()\n        print(\"Time (numba, pass {}): {}\".format(i, end - start))\n        A1.assemble()\n\n    assert (A0 - A1).norm() == pytest.approx(0.0, abs=1.0e-9)\n\n\n@pytest.mark.parametrize(\"set_vals\", [MatSetValues_abi, get_matsetvalues_api()])\ndef test_custom_mesh_loop_cffi_rank2(set_vals):\n    \"\"\"Test numba assembler for bilinear form\"\"\"\n\n    mesh = dolfinx.generation.UnitSquareMesh(MPI.COMM_WORLD, 64, 64)\n    V = dolfinx.FunctionSpace(mesh, (\"Lagrange\", 1))\n\n    # Test against generated code and general assembler\n    u, v = ufl.TrialFunction(V), ufl.TestFunction(V)\n    a = inner(u, v) * dx\n    A0 = dolfinx.fem.assemble_matrix(a)\n    A0.assemble()\n\n    A0.zeroEntries()\n    start = time.time()\n    dolfinx.fem.assemble_matrix(A0, a)\n    end = time.time()\n    print(\"Time (C++, pass 2):\", end - start)\n    A0.assemble()\n\n    # Unpack mesh and dofmap data\n    num_owned_cells = mesh.topology.index_map(mesh.topology.dim).size_local\n    num_cells = num_owned_cells + mesh.topology.index_map(mesh.topology.dim).num_ghosts\n    x_dofs = mesh.geometry.dofmap.array.reshape(num_cells, 3)\n    x = mesh.geometry.x\n    dofmap = V.dofmap.list.array.reshape(num_cells, 3).astype(np.dtype(PETSc.IntType))\n\n    A1 = A0.copy()\n    for i in range(2):\n        A1.zeroEntries()\n        start = time.time()\n        assemble_matrix_cffi(A1.handle, (x_dofs, x), dofmap, num_owned_cells, set_vals, PETSc.InsertMode.ADD_VALUES)\n        end = time.time()\n        print(\"Time (Numba, pass {}): {}\".format(i, end - start))\n        A1.assemble()\n\n    assert (A1 - A0).norm() == pytest.approx(0.0)\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-fenics-dolfinx-master-bi2pqtkbnpdp2ztbtnp3iosgxb7e3ib4/spack-src/.git/objects/pack/pack-154f20467f3c6e3a87966cfe1c4730bf5dc586aa.pack",
        "/tmp/vanessa/spack-stage/spack-stage-fenics-dolfinx-master-bi2pqtkbnpdp2ztbtnp3iosgxb7e3ib4/spack-src/.git/objects/pack/pack-154f20467f3c6e3a87966cfe1c4730bf5dc586aa.idx",
        "/tmp/vanessa/spack-stage/spack-stage-fenics-dolfinx-master-bi2pqtkbnpdp2ztbtnp3iosgxb7e3ib4/spack-src/python/demo/dolfin_fine.h5",
        "/tmp/vanessa/spack-stage/spack-stage-fenics-dolfinx-master-bi2pqtkbnpdp2ztbtnp3iosgxb7e3ib4/spack-src/python/demo/poisson/poisson_u.png"
    ],
    "total_files": 372
}