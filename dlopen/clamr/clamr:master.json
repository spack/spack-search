{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/l7/l7_init.c": "/*\n *  Copyright (c) 2011-2019, Triad National Security, LLC.\n *  All rights Reserved.\n *\n *  CLAMR -- LA-CC-11-094\n *\n *  Copyright 2011-2019. Triad National Security, LLC. This software was produced \n *  under U.S. Government contract 89233218CNA000001 for Los Alamos National \n *  Laboratory (LANL), which is operated by Triad National Security, LLC \n *  for the U.S. Department of Energy. The U.S. Government has rights to use, \n *  reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR\n *  TRIAD NATIONAL SECURITY, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR \n *  ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is modified\n *  to produce derivative works, such modified software should be clearly marked,\n *  so as not to confuse it with the version available from LANL.\n *\n *  Additionally, redistribution and use in source and binary forms, with or without\n *  modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the Triad National Security, LLC, Los Alamos \n *       National Laboratory, LANL, the U.S. Government, nor the names of its \n *       contributors may be used to endorse or promote products derived from \n *       this software without specific prior written permission.\n *  \n *  THIS SOFTWARE IS PROVIDED BY THE TRIAD NATIONAL SECURITY, LLC AND \n *  CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT \n *  NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n *  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL TRIAD NATIONAL\n *  SECURITY, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n *  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n *  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n *  OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n *  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n *  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n *  POSSIBILITY OF SUCH DAMAGE.\n */  \n\n/* This define causes this routine to have the storage for\n * for global variables here and declared extern everywhere\n * else\n */\n#define L7_EXTERN\n#include \"l7.h\"\n#include \"l7p.h\"\n#include <stdlib.h>\n#include <dlfcn.h>\n\n#define L7_LOCATION \"L7_INIT\"\n\n#ifdef HAVE_OPENCL\n#include \"l7_kernel.inc\"\n#endif\n\n#ifdef HAVE_QUO\nstatic int getSubCommProcs(QUO_context c, int numpes, int *vLen, int **v);\nstatic int subCommInit(QUO_SubComm *subcomm, int np1s, int *p1who);\n#endif\n#define WARNING_SUPPRESSION 0\n\nint L7_Init (\n    int *mype,\n    int *numpes,\n    int *argc,\n    char **argv,\n    int do_quo_setup,\n    int lttrace_on\n    )\n{\n    /*\n     * Purpose\n     * =======\n     * L7_INIT initializes the communication environment (if it has\n     * not yet been initialized) and initializes some internal variables.\n     *\n     * Arguments\n     * =========\n     * mype         (output) int*\n     * numpes       (output) int*\n     * argc         (input/output) int* from main\n     * argv         (input/output) char** from main\n     * \n     * Return Value\n     * ============\n     * Returns zero if successful and non-zero for error\n     * \n     * Notes\n     * =====\n     * 1) If MPI has not been initialized when this subroutine is called,\n     *    L7 will do so. In this case, L7 will also take responsibility for\n     *    terminating MPI when L7_TERMINATE is called.\n     * \n     */\n    \n    /*\n     * Local variables\n     */\n    \n   int ierr;\n\n#if defined(HAVE_MPI)\n   \n    int flag;    /* MPI_Initialized input. */\n      \n    /*\n     * Executable Statements\n     */\n\n#ifndef HAVE_QUO\n   // To get rid of compiler warning\n   if (WARNING_SUPPRESSION && do_quo_setup == 99) printf(\"DEBUG do_quo_setup = %d\\n\",do_quo_setup);\n#endif\n#ifndef HAVE_LTTRACE\n   // To get rid of compiler warning\n   if (WARNING_SUPPRESSION && lttrace_on == 99) printf(\"DEBUG lttrace_on = %d\\n\",lttrace_on);\n#endif\n\n#ifdef HAVE_LTTRACE\n   if (lttrace_on){\n      void *handle = dlopen(\"lttrace.so\",RTLD_LAZY);\n      if (! handle) {\n         printf(\"DEBUG -- open failed\\n\");\n      }\n      int (*init_lttrace)(int *argc, char ***argv);\n      init_lttrace = (int (*)(int *, char ***))dlsym(handle, \"initialize_lttrace\");\n   }  \n#endif\n      \n    if ( l7.initialized != 0 ) {\n       ierr = -1;\n       L7_ASSERT( l7.initialized == 0, \"L7 already initialized\", -1 );\n    }\n      \n    ierr = MPI_Initialized ( &flag );\n    L7_ASSERT( ierr == MPI_SUCCESS, \"MPI_Initialized\", ierr );\n      \n    if ( !flag && *numpes != -1){\n        ierr = MPI_Init(argc, &argv);\n        L7_ASSERT( ierr == MPI_SUCCESS, \"MPI_Init\", ierr);\n          \n          l7.initialized_mpi = 1;\n          l7.mpi_initialized = 1;\n    }\n    else {\n       l7.initialized_mpi = 0;\n       l7.mpi_initialized = 0;\n    }\n      \n    if (*numpes != -1) {\n        ierr = MPI_Comm_rank (MPI_COMM_WORLD, &l7.penum );\n        L7_ASSERT( ierr == MPI_SUCCESS, \"MPI_Comm_rank\", ierr );\n\n        ierr = MPI_Comm_size (MPI_COMM_WORLD, &l7.numpes );\n        L7_ASSERT( ierr == MPI_SUCCESS, \"MPI_Comm_size\", ierr );\n        *mype = l7.penum;\n        *numpes = l7.numpes;\n    }\n    else {\n        l7.penum = 0;\n        l7.numpes = 1;\n        *mype = 0;\n        *numpes = 1;\n    }\n      \n   l7.sizeof_workspace = 0;\n    \n   l7.sizeof_send_buffer = 0;\n    \n   l7.initialized = 1;\n\n#ifdef HAVE_QUO\n   if (do_quo_setup) {\n      // init QUO -- all MPI processes MUST do this at the same time.\n      // create shorthand for context and Subcomm\n      QUO_context context = l7.subComm.context;\n      QUO_SubComm subcomm = l7.subComm;\n\n      subcomm.rank = -1;\n      subcomm.size = -1;\n\n      int rc = QUO_ERR;\n      if (QUO_SUCCESS != (rc = QUO_create(&context))){\n         printf(\"QUO_create failure: rc = %d\\n\",rc);\n      } \n\n      if (l7.penum == 0) {\n         int nnodes, nnumanodes, nsockets, ncores, npus, nqids;\n         if (QUO_SUCCESS != QUO_nnodes(context, &nnodes)){\n            printf(\"QUO_nnodes failure\\n\");\n         }\n         if (QUO_SUCCESS != QUO_nnumanodes(context, &nnumanodes)){\n            printf(\"QUO_nnumanodes failure\\n\");\n         }\n         if (QUO_SUCCESS != QUO_nsockets(context, &nsockets)){\n            printf(\"QUO_nsockets failure\\n\");\n         }\n         if (QUO_SUCCESS != QUO_ncores(context, &ncores)){\n            printf(\"QUO_ncores failure\\n\");\n         }\n         if (QUO_SUCCESS != QUO_npus(context, &npus)){\n            printf(\"QUO_npus failure\\n\");\n         }\n         if (QUO_SUCCESS != QUO_nqids(context, &nqids)){\n            printf(\"QUO_nqids failure\\n\");\n         }\n\n         printf(\"### System Info ###\\n\");\n         printf(\"# Nodes: %d\\n\",nnodes);\n         printf(\"# NUMA Nodes: %d\\n\",nnumanodes);\n         printf(\"# Sockets: %d\\n\",nsockets);\n         printf(\"# Cores: %d\\n\",ncores);\n         printf(\"# PUs: %d\\n\",npus);\n         printf(\"# MPI Procs on Node: %d\\n\",nqids);\n      }\n\n      int numPEsInSubComm = 0;\n      int *cwPEs = NULL;\n      rc = QUO_ERR;\n\n      getSubCommProcs(context, l7.numpes, &numPEsInSubComm, &cwPEs);\n\n      int n;\n      if (QUO_SUCCESS != (rc = QUO_id(context, &n))) {\n         printf(\"QUO_id: rc = %d\\n\",rc);\n         exit(1);\n      }\n      if (0 == n) {\n         // add hostname -- could be diff on diff systems\n         printf(\"MPI_COMM_WORLD ranks in subComm: \");\n         for (int i = 0; i < numPEsInSubComm; ++i) {\n             printf(\" %d \",cwPEs[i]);\n         }\n         printf(\"\\n\");\n      }\n\n      // now actually create the darn thing...\n      rc = subCommInit(&subcomm, numPEsInSubComm, cwPEs);\n      if (cwPEs) free(cwPEs);\n\n      // at this point subComm is ready to use for those inSubComm\n\n      int member_global[l7.numpes];\n      int subcomm_rank_global[l7.numpes];\n      MPI_Gather(&subcomm.member, 1, MPI_INT,\n                 member_global, 1, MPI_INT,\n                 0, MPI_COMM_WORLD);\n      MPI_Gather(&subcomm.rank, 1, MPI_INT,\n                 subcomm_rank_global, 1, MPI_INT,\n                 0, MPI_COMM_WORLD);\n\n      if (l7.penum == 0) {\n         for (int ip = 0; ip < l7.numpes; ip++){\n            if (member_global[ip]) {\n               printf(\"rank: %d is subComm rank: %d\\n\",ip, subcomm_rank_global[ip]);\n            }\n         }\n\n        int world_size, subcomm_size;\n        MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n        MPI_Comm_size(subcomm.comm, &subcomm_size);\n        printf(\"Size comm world %d subComm %d\\n\",world_size, subcomm_size);\n      }\n\n   }\n\n#endif\n\n\n#else\n\n   *mype = 0;\n   *numpes = 1;\n\n#endif /* HAVE_MPI */\n\n   return(ierr);     \n}\nvoid L7_Dev_Init(void)\n{\n#ifdef HAVE_OPENCL\n   if (l7.numpes > 1) {\n        cl_context context = ezcl_get_context();\n\n        l7.kernel_pack_short_have_data   = ezcl_create_kernel_wsource(context, l7_kern_source, \"pack_short_have_data_cl\");\n        l7.kernel_pack_int_have_data     = ezcl_create_kernel_wsource(context, l7_kern_source, \"pack_int_have_data_cl\");\n        l7.kernel_pack_float_have_data   = ezcl_create_kernel_wsource(context, l7_kern_source, \"pack_float_have_data_cl\");\n        l7.kernel_pack_double_have_data  = ezcl_create_kernel_wsource(context, l7_kern_source, \"pack_double_have_data_cl\");\n        l7.kernel_copy_ghost_short_data  = ezcl_create_kernel_wsource(context, l7_kern_source, \"copy_ghost_short_data_cl\");\n        l7.kernel_copy_ghost_int_data    = ezcl_create_kernel_wsource(context, l7_kern_source, \"copy_ghost_int_data_cl\");\n        l7.kernel_copy_ghost_float_data  = ezcl_create_kernel_wsource(context, l7_kern_source, \"copy_ghost_float_data_cl\");\n        l7.kernel_copy_ghost_double_data = ezcl_create_kernel_wsource(context, l7_kern_source, \"copy_ghost_double_data_cl\");\n   }\n#endif\n}\n\n#ifdef HAVE_QUO\nstatic int getSubCommProcs(QUO_context c, int numpes, int *vLen, int **v)\n{\n    QUO_obj_type_t resPrio[] = {QUO_OBJ_NODE,\n                                QUO_OBJ_SOCKET};\n    // default target resource is a NUMA node\n    QUO_obj_type_t targetRes = QUO_OBJ_NODE;\n    // number of target resources and max number of procs per resource\n    int nRes = 0, maxProcPerRes = 1;\n    int res_assigned = 0;\n    int totalWorkers = 0;\n    int rc = MPI_SUCCESS;\n    // array that hold whether or not a particular rank is going to do work\n    int *workContribs = NULL;\n    // MPI_COMM_WORLD ranks of the selected workers\n    int *workerRanks = NULL;\n\n    // figure out what we are going to distribute work over\n    for (uint i = 0; i < sizeof(resPrio) / sizeof(resPrio[0]); ++i) {\n        if (QUO_SUCCESS != (rc = QUO_nobjs_by_type(c, resPrio[i], &nRes))){\n           printf(\"QUO_nobjs_by_type: rc = %d\\n\",rc);\n           exit(1);\n        }\n        if (nRes > 0){\n            targetRes = resPrio[i];\n            break;\n        }\n    }\n    // failure -- fix this path at some point\n    if (0 == nRes) return 1;\n    /* let quo distribute workers over the sockets. if res_assigned is 1\n     * after this call, then i have been chosen. */\n    int isel = 0;\n    if (QUO_SUCCESS != (rc = QUO_auto_distrib(c, targetRes,\n                                              maxProcPerRes, &isel))) {\n        printf(\"QUO_auto_distrib: rc = %d\\n\",rc);\n        exit(1);\n    }\n    if (isel == 1) {\n    //if (*c.quo->autoDistrib(targetRes, maxProcPerRes)) {\n        res_assigned = 1;\n    }\n\n   /* array that hold whether or not a particular rank is going to do work */\n   workContribs = (int *)calloc(numpes, sizeof(*workContribs));\n   if (!workContribs) return 1;\n\n   if (MPI_SUCCESS != (rc = MPI_Allgather(&res_assigned, 1, MPI_INT,\n                                          workContribs, 1, MPI_INT,\n                                          MPI_COMM_WORLD))) {\n       return 1;\n   }\n   /* now iterate over the array and count the total number of workers */\n   for (int i = 0; i < numpes; ++i) {\n       if (1 == workContribs[i]) ++totalWorkers;\n   }\n   workerRanks = (int *)calloc(totalWorkers, sizeof(*workerRanks));\n   if (!workerRanks) return 1;\n   /* populate the array with the worker comm world ranks */\n   for (int i = 0, j = 0; i < numpes; ++i) {\n       if (1 == workContribs[i]) {\n           workerRanks[j++] = i;\n       }\n   }\n   *vLen = totalWorkers;\n   *v = workerRanks;\n   if (workContribs) free(workContribs);\n   return 0;\n}\n\nstatic int\nsubCommInit(QUO_SubComm *subcomm,\n            int np1s /* number of participants |p1who| */,\n            int *p1who /* the participating ranks (MPI_COMM_WORLD) */)\n{\n    int rc = QUO_SUCCESS;\n    MPI_Group worldGroup;\n    MPI_Group p1_group;\n    /* ////////////////////////////////////////////////////////////////////// */\n    /* now create our own communicator based on the rank ids passed here */\n    /* ////////////////////////////////////////////////////////////////////// */\n    if (MPI_SUCCESS != MPI_Comm_group(MPI_COMM_WORLD, &worldGroup)) {\n        rc = QUO_ERR_MPI;\n        goto out;\n    }\n    if (MPI_SUCCESS != MPI_Group_incl(worldGroup, np1s,\n                                      p1who, &p1_group)) {\n        rc = QUO_ERR_MPI;\n        goto out;\n    }\n    if (MPI_SUCCESS != MPI_Comm_create(MPI_COMM_WORLD,\n                                       p1_group,\n                                       &((*subcomm).comm))) {\n        rc = QUO_ERR_MPI;\n        goto out;\n    }\n    /* am i in the new communicator? */\n    (*subcomm).member = (MPI_COMM_NULL == (*subcomm).comm) ? 0 : 1;\n    if ((*subcomm).member) {\n        if (MPI_SUCCESS != MPI_Comm_size((*subcomm).comm, &(*subcomm).size)) {\n            rc = QUO_ERR_MPI;\n            goto out;\n        }\n        if (MPI_SUCCESS != MPI_Comm_rank((*subcomm).comm, &(*subcomm).rank)) {\n            rc = QUO_ERR_MPI;\n            goto out;\n        }\n    }\nout:\n    if (MPI_SUCCESS != MPI_Group_free(&worldGroup)) return 1;\n    return (QUO_SUCCESS == rc) ? 0 : 1;\n}\n\n#endif\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/FreeSansBold.ttf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/serialefficiencies.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/intelflops.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/archefficiencies.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/intelintelflopband.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/runmethodefficiencies.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/intelgccflopband.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/intelvolume.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PerfPortPlots/parallelefficiencies.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/.git/objects/pack/pack-8f8b3c9b65c0e5d774322058911536a19e7a8ba0.pack",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/.git/objects/pack/pack-8f8b3c9b65c0e5d774322058911536a19e7a8ba0.idx",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/ezcl/docs/EZCL_logo.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/lib/Devel/SmallProf.pm",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/lib/Parallel/ForkManager.pm",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/html/cts_logo.jpg",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/html/cts_logo2.jpg",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/html/cts_banner.jpg",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/share/doc/cts/CTS_Users_Guide.sxw",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/share/doc/cts/CTS_Users_Guide.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/cts/share/doc/cts/CTS_Users_Guide.doc",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/tests/hlopenmp_paper/clamr_graph_paper.png",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/l7/docs/L7_logo.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/MallocPlus/docs/MallocPlus_logo.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-clamr-master-ve6qzp2g72j6hqmdhu6kfoqf74yfqh6g/spack-src/PowerParser/docs/PowerParser_logo.pdf"
    ],
    "total_files": 1046
}