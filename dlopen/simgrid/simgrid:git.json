{
    "matches": {
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/src/mc/Session.cpp": "/* Copyright (c) 2015-2020. The SimGrid Team. All rights reserved.          */\n\n/* This program is free software; you can redistribute it and/or modify it\n * under the terms of the license (GNU LGPL) which comes with this package. */\n\n#include \"src/mc/Session.hpp\"\n#include \"src/mc/checker/Checker.hpp\"\n#include \"src/mc/mc_config.hpp\"\n#include \"src/internal_config.h\" // HAVE_SMPI\n#if HAVE_SMPI\n#include \"smpi/smpi.h\"\n#endif\n#include \"src/mc/mc_private.hpp\"\n#include \"src/mc/mc_state.hpp\"\n#include \"xbt/log.h\"\n#include \"xbt/system_error.hpp\"\n\n#include <array>\n#include <memory>\n#include <string>\n\n#include <fcntl.h>\n#ifdef __linux__\n#include <sys/prctl.h>\n#endif\n\nXBT_LOG_NEW_DEFAULT_SUBCATEGORY(mc_Session, mc, \"Model-checker session\");\n\nnamespace simgrid {\nnamespace mc {\n\ntemplate <class Code> void run_child_process(int socket, Code code)\n{\n  /* On startup, simix_global_init() calls simgrid::mc::Client::initialize(), which checks whether the MC_ENV_SOCKET_FD\n   * env variable is set. If so, MC mode is assumed, and the client is setup from its side\n   */\n\n#ifdef __linux__\n  // Make sure we do not outlive our parent\n  sigset_t mask;\n  sigemptyset (&mask);\n  xbt_assert(sigprocmask(SIG_SETMASK, &mask, nullptr) >= 0, \"Could not unblock signals\");\n  xbt_assert(prctl(PR_SET_PDEATHSIG, SIGHUP) == 0, \"Could not PR_SET_PDEATHSIG\");\n#endif\n\n  // Remove CLOEXEC to pass the socket to the application\n  int fdflags = fcntl(socket, F_GETFD, 0);\n  xbt_assert(fdflags != -1 && fcntl(socket, F_SETFD, fdflags & ~FD_CLOEXEC) != -1,\n             \"Could not remove CLOEXEC for socket\");\n\n  // Disable lazy relocation in the model-checked process to prevent the application from\n  // modifying its .got.plt during snapshot.\n  setenv(\"LC_BIND_NOW\", \"1\", 1);\n\n  setenv(MC_ENV_SOCKET_FD, std::to_string(socket).c_str(), 1);\n\n  code();\n}\n\nSession::Session(const std::function<void()>& code)\n{\n#if HAVE_SMPI\n  smpi_init_options();//only performed once\n  xbt_assert(smpi_cfg_privatization() != SmpiPrivStrategies::MMAP,\n             \"Please use the dlopen privatization schema when model-checking SMPI code\");\n#endif\n\n  // Create an AF_LOCAL socketpair used for exchanging messages\n  // between the model-checker process (ourselves) and the model-checked\n  // process:\n  int sockets[2];\n  int res = socketpair(AF_LOCAL, SOCK_SEQPACKET | SOCK_CLOEXEC, 0, sockets);\n  xbt_assert(res != -1, \"Could not create socketpair\");\n\n  pid_t pid = fork();\n  xbt_assert(pid >= 0, \"Could not fork model-checked process\");\n\n  if (pid == 0) { // Child\n    ::close(sockets[1]);\n    run_child_process(sockets[0], code);\n    DIE_IMPOSSIBLE;\n  }\n\n  // Parent (model-checker):\n  ::close(sockets[0]);\n\n  xbt_assert(mc_model_checker == nullptr, \"Did you manage to start the MC twice in this process?\");\n\n  auto process = std::make_unique<simgrid::mc::RemoteSimulation>(pid);\n  model_checker_ = std::make_unique<simgrid::mc::ModelChecker>(std::move(process), sockets[1]);\n\n  mc_model_checker = model_checker_.get();\n  model_checker_->start();\n}\n\nSession::~Session()\n{\n  this->close();\n}\n\n/** Take the initial snapshot of the application, that must be stopped. */\nvoid Session::initialize()\n{\n  xbt_assert(initial_snapshot_ == nullptr);\n  model_checker_->wait_for_requests();\n  initial_snapshot_ = std::make_shared<simgrid::mc::Snapshot>(0);\n}\n\nvoid Session::execute(Transition const& transition) const\n{\n  model_checker_->handle_simcall(transition);\n  model_checker_->wait_for_requests();\n}\n\nvoid Session::restore_initial_state() const\n{\n  this->initial_snapshot_->restore(&model_checker_->get_remote_simulation());\n}\n\nvoid Session::log_state() const\n{\n  model_checker_->getChecker()->log_state();\n\n  if (not _sg_mc_dot_output_file.get().empty()) {\n    fprintf(dot_output, \"}\\n\");\n    fclose(dot_output);\n  }\n  if (getenv(\"SIMGRID_MC_SYSTEM_STATISTICS\")){\n    int ret=system(\"free\");\n    if (ret != 0)\n      XBT_WARN(\"Call to system(free) did not return 0, but %d\", ret);\n  }\n}\n\nvoid Session::close()\n{\n  initial_snapshot_ = nullptr;\n  if (model_checker_) {\n    model_checker_->shutdown();\n    model_checker_   = nullptr;\n    mc_model_checker = nullptr;\n  }\n}\n\nbool Session::actor_is_enabled(aid_t pid) const\n{\n  s_mc_message_actor_enabled_t msg{simgrid::mc::MessageType::ACTOR_ENABLED, pid};\n  model_checker_->channel().send(msg);\n  std::array<char, MC_MESSAGE_LENGTH> buff;\n  ssize_t received = model_checker_->channel().receive(buff.data(), buff.size(), true);\n  xbt_assert(received == sizeof(s_mc_message_int_t), \"Unexpected size in answer to ACTOR_ENABLED\");\n  return ((s_mc_message_int_t*)buff.data())->value;\n}\n\nsimgrid::mc::Session* session;\n\n}\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/src/smpi/smpi_main.c": "/* Copyright (c) 2007-2020. The SimGrid Team. All rights reserved.          */\n\n/* This program is free software; you can redistribute it and/or modify it\n * under the terms of the license (GNU LGPL) which comes with this package. */\n\n#include <stdio.h>\n#include <stdlib.h>\n\n#include <smpi/smpi.h>\n\nint main(int argc, char **argv)\n{\n  if (getenv(\"SMPI_PRETEND_CC\") != NULL) {\n    /* Hack to ensure that smpicc can pretend to be a simple compiler. Particularly handy to pass it to the\n     * configuration tools. This one is used with dlopen privatization. */\n    return 0;\n  }\n\n  if (argc < 2) {\n    fprintf(stderr, \"Usage: smpimain <program to launch>\\n\");\n    exit(1);\n  }\n  return smpi_main(argv[1], argc - 1, argv + 1);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/src/smpi/internals/smpi_config.cpp": "/* Copyright (c) 2008-2020. The SimGrid Team. All rights reserved.          */\n\n/* This program is free software; you can redistribute it and/or modify it\n * under the terms of the license (GNU LGPL) which comes with this package. */\n#include \"smpi_config.hpp\"\n#include \"include/xbt/config.hpp\"\n#include \"mc/mc.h\"\n#include \"private.hpp\"\n#include \"smpi_coll.hpp\"\n#include \"src/simix/smx_private.hpp\"\n#include \"xbt/parse_units.hpp\"\n\n#include <cfloat> /* DBL_MAX */\n#include <boost/algorithm/string.hpp> /* trim */\n#include <boost/tokenizer.hpp>\n\n#if SIMGRID_HAVE_MC\n#include \"src/mc/mc_config.hpp\"\n#endif\n\n#if defined(__APPLE__)\n# include <AvailabilityMacros.h>\n# ifndef MAC_OS_X_VERSION_10_12\n#   define MAC_OS_X_VERSION_10_12 101200\n# endif\nconstexpr bool HAVE_WORKING_MMAP = (MAC_OS_X_VERSION_MIN_REQUIRED >= MAC_OS_X_VERSION_10_12);\n#elif defined(__FreeBSD__) || defined(__FreeBSD_kernel__) || defined(__sun) || defined(__HAIKU__)\nconstexpr bool HAVE_WORKING_MMAP = false;\n#else\nconstexpr bool HAVE_WORKING_MMAP = true;\n#endif\n\nbool _smpi_options_initialized=false;\nSharedMallocType _smpi_cfg_shared_malloc = SharedMallocType::GLOBAL;\nSmpiPrivStrategies _smpi_cfg_privatization = SmpiPrivStrategies::NONE;\ndouble _smpi_cfg_host_speed;\n\nXBT_LOG_NEW_DEFAULT_SUBCATEGORY(smpi_config, smpi, \"Logging specific to SMPI (config)\");\n\nsimgrid::config::Flag<std::string> _smpi_cfg_host_speed_string{\n    \"smpi/host-speed\",\n    \"Speed of the host running the simulation (in flop/s). \"\n    \"Used to bench the operations.\",\n    \"20000f\", [](const std::string& str) {\n      _smpi_cfg_host_speed = xbt_parse_get_speed(\"smpi/host-speed\", 1, str.c_str(), \"option\", \"smpi/host-speed\");\n      xbt_assert(_smpi_cfg_host_speed > 0.0, \"Invalid value (%s) for 'smpi/host-speed': it must be positive.\",\n                 _smpi_cfg_host_speed_string.get().c_str());\n    }};\n\nsimgrid::config::Flag<bool> _smpi_cfg_simulate_computation{\n  \"smpi/simulate-computation\", \"Whether the computational part of the simulated application should be simulated.\",\n   true};\nsimgrid::config::Flag<std::string> _smpi_cfg_shared_malloc_string{\n  \"smpi/shared-malloc\", \"Whether SMPI_SHARED_MALLOC is enabled. Disable it for debugging purposes.\", \"global\", \n  [](const std::string& val) {   \n    if ((val == \"yes\") || (val == \"1\") || (val == \"on\") || (val == \"global\")) {\n      _smpi_cfg_shared_malloc = SharedMallocType::GLOBAL;\n    } else if (val == \"local\") {\n      _smpi_cfg_shared_malloc = SharedMallocType::LOCAL;\n    } else if ((val == \"no\") || (val == \"0\") || (val == \"off\")) {\n      _smpi_cfg_shared_malloc = SharedMallocType::NONE;\n    } else {\n      xbt_die(\"Invalid value '%s' for option smpi/shared-malloc. Possible values: 'on' or 'global', 'local', 'off'\",\n      val.c_str());\n    } \n  } };\n\nsimgrid::config::Flag<double> _smpi_cfg_cpu_threshold{\n  \"smpi/cpu-threshold\", \"Minimal computation time (in seconds) not discarded, or -1 for infinity.\", 1e-6,\n  [](const double& val){\n    if (val < 0)\n      _smpi_cfg_cpu_threshold = DBL_MAX;\n  }};\n\nsimgrid::config::Flag<int> _smpi_cfg_async_small_thresh{\"smpi/async-small-thresh\",\n                                                        \"Maximal size of messages that are to be sent asynchronously, without waiting for the receiver\",\n                                                        0};\nsimgrid::config::Flag<int> _smpi_cfg_detached_send_thresh{\"smpi/send-is-detached-thresh\",\n                                                          \"Threshold of message size where MPI_Send stops behaving like MPI_Isend and becomes MPI_Ssend\", \n                                                          65536};\nsimgrid::config::Flag<bool> _smpi_cfg_grow_injected_times{\"smpi/grow-injected-times\",\n                                                          \"Whether we want to make the injected time in MPI_Iprobe and MPI_Test grow, to \"\n                                                          \"allow faster simulation. This can make simulation less precise, though.\",\n                                                          true};\nsimgrid::config::Flag<double> _smpi_cfg_iprobe_cpu_usage{\"smpi/iprobe-cpu-usage\",\n                                                        \"Maximum usage of CPUs by MPI_Iprobe() calls. We've observed that MPI_Iprobes \"\n                                                        \"consume significantly less power than the maximum of a specific application. \"\n                                                        \"This value is then (Iprobe_Usage/Max_Application_Usage).\",\n                                                        1.0};\n\nsimgrid::config::Flag<bool>  _smpi_cfg_trace_call_location{\"smpi/trace-call-location\",\n                                                           \"Should filename and linenumber of MPI calls be traced?\", false};\nsimgrid::config::Flag<bool> _smpi_cfg_trace_call_use_absolute_path{\"smpi/trace-call-use-absolute-path\",\n                                                                   \"Should filenames for trace-call tracing be absolute or not?\", false};\nsimgrid::config::Flag<std::string> _smpi_cfg_comp_adjustment_file{\"smpi/comp-adjustment-file\",\n    \"A file containing speedups or slowdowns for some parts of the code.\", \n    \"\", [](const std::string& filename){\n      if (not filename.empty()) {\n        std::ifstream fstream(filename);\n        xbt_assert(fstream.is_open(), \"Could not open file %s. Does it exist?\", filename.c_str());\n        std::string line;\n        using Tokenizer = boost::tokenizer<boost::escaped_list_separator<char>>;\n        std::getline(fstream, line); // Skip the header line\n        while (std::getline(fstream, line)) {\n          Tokenizer tok(line);\n          Tokenizer::iterator it  = tok.begin();\n          Tokenizer::iterator end = std::next(tok.begin());\n          std::string location = *it;\n          boost::trim(location);\n          location2speedup.insert(std::pair<std::string, double>(location, std::stod(*end)));\n        }\n      }\n    }};\n    \n#if HAVE_PAPI\n  simgrid::config::Flag<std::string> _smpi_cfg_papi_events_file{\"smpi/papi-events\",\n                                                                \"This switch enables tracking the specified counters with PAPI\", \"\"};\n#endif\n\nsimgrid::config::Flag<double> _smpi_cfg_auto_shared_malloc_thresh(\"smpi/auto-shared-malloc-thresh\",\n                                                                  \"Threshold size for the automatic sharing of memory\", \n                                                                  0);\n\ndouble smpi_cfg_host_speed(){\n  return _smpi_cfg_host_speed;\n}\n\nbool smpi_cfg_simulate_computation(){\n  return _smpi_cfg_simulate_computation;\n}\n\nSharedMallocType smpi_cfg_shared_malloc(){\n  return _smpi_cfg_shared_malloc;\n}\n\ndouble smpi_cfg_cpu_thresh(){\n  return _smpi_cfg_cpu_threshold;\n}\n\nSmpiPrivStrategies smpi_cfg_privatization(){\n  return _smpi_cfg_privatization;\n}\n\nint smpi_cfg_async_small_thresh(){\n  return _smpi_cfg_async_small_thresh;\n}\n\nint smpi_cfg_detached_send_thresh(){\n  return _smpi_cfg_detached_send_thresh;\n}\n\nbool smpi_cfg_grow_injected_times(){\n  return _smpi_cfg_grow_injected_times;\n}\n\ndouble smpi_cfg_iprobe_cpu_usage(){\n  return _smpi_cfg_iprobe_cpu_usage;\n}\n\nbool smpi_cfg_trace_call_location(){\n  return _smpi_cfg_trace_call_location;\n}\n\nbool smpi_cfg_trace_call_use_absolute_path(){\n  return _smpi_cfg_trace_call_use_absolute_path;\n}\n\nstd::string smpi_cfg_comp_adjustment_file(){\n  return _smpi_cfg_comp_adjustment_file;\n}\n#if HAVE_PAPI\nstd::string smpi_cfg_papi_events_file(){\n  return _smpi_cfg_papi_events_file;\n}\n#endif\ndouble smpi_cfg_auto_shared_malloc_thresh(){\n  return _smpi_cfg_auto_shared_malloc_thresh;\n}\n\nvoid smpi_init_options(){\n  // return if already called\n  if(_smpi_options_initialized)\n    return;\n  simgrid::config::declare_flag<bool>(\"smpi/display-timing\", \"Whether we should display the timing after simulation.\", false);\n  simgrid::config::declare_flag<bool>(\"smpi/keep-temps\", \"Whether we should keep the generated temporary files.\", false);\n  simgrid::config::declare_flag<std::string>(\"smpi/tmpdir\", \"tmp dir for dlopen files\", \"/tmp\");\n\n  simgrid::config::declare_flag<std::string>(\"smpi/coll-selector\", \"Which collective selector to use\", \"default\");\n  simgrid::config::declare_flag<std::string>(\"smpi/gather\", \"Which collective to use for gather\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/allgather\", \"Which collective to use for allgather\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/barrier\", \"Which collective to use for barrier\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/reduce_scatter\", \"Which collective to use for reduce_scatter\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/scatter\", \"Which collective to use for scatter\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/allgatherv\", \"Which collective to use for allgatherv\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/allreduce\", \"Which collective to use for allreduce\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/alltoall\", \"Which collective to use for alltoall\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/alltoallv\", \"Which collective to use for alltoallv\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/bcast\", \"Which collective to use for bcast\", \"\");\n  simgrid::config::declare_flag<std::string>(\"smpi/reduce\", \"Which collective to use for reduce\", \"\");\n\n  const char* default_privatization = std::getenv(\"SMPI_PRIVATIZATION\");\n  if (default_privatization == nullptr)\n    default_privatization = \"no\";\n\n\n  simgrid::config::declare_flag<std::string>( \"smpi/privatization\", \n    \"How we should privatize global variable at runtime (no, yes, mmap, dlopen).\",\n    default_privatization, [](const std::string& smpi_privatize_option){\n      if (smpi_privatize_option == \"no\" || smpi_privatize_option == \"0\")\n        _smpi_cfg_privatization = SmpiPrivStrategies::NONE;\n      else if (smpi_privatize_option == \"yes\" || smpi_privatize_option == \"1\")\n        _smpi_cfg_privatization = SmpiPrivStrategies::DEFAULT;\n      else if (smpi_privatize_option == \"mmap\")\n        _smpi_cfg_privatization = SmpiPrivStrategies::MMAP;\n      else if (smpi_privatize_option == \"dlopen\")\n        _smpi_cfg_privatization = SmpiPrivStrategies::DLOPEN;\n      else\n        xbt_die(\"Invalid value for smpi/privatization: '%s'\", smpi_privatize_option.c_str());\n        \n      if (not SMPI_switch_data_segment) {\n        XBT_DEBUG(\"Running without smpi_main(); disable smpi/privatization.\");\n        _smpi_cfg_privatization = SmpiPrivStrategies::NONE;\n      }\n      if (not HAVE_WORKING_MMAP && _smpi_cfg_privatization == SmpiPrivStrategies::MMAP) {\n        XBT_INFO(\"mmap privatization is broken on this platform, switching to dlopen privatization instead.\");\n        _smpi_cfg_privatization = SmpiPrivStrategies::DLOPEN;\n      }\n    });\n\n  simgrid::config::declare_flag<std::string>(\"smpi/privatize-libs\", \n                                            \"Add libraries (; separated) to privatize (libgfortran for example).\"\n                                            \"You need to provide the full names of the files (libgfortran.so.4), or its full path\", \n                                            \"\");\n  simgrid::config::declare_flag<double>(\"smpi/shared-malloc-blocksize\",\n                                        \"Size of the bogus file which will be created for global shared allocations\", \n                                        1UL << 20);\n  simgrid::config::declare_flag<std::string>(\"smpi/shared-malloc-hugepage\",\n                                             \"Path to a mounted hugetlbfs, to use huge pages with shared malloc.\", \n                                             \"\");\n\n  simgrid::config::declare_flag<std::string>(\n      \"smpi/os\", \"Small messages timings (MPI_Send minimum time for small messages)\", \"0:0:0:0:0\");\n  simgrid::config::declare_flag<std::string>(\n      \"smpi/ois\", \"Small messages timings (MPI_Isend minimum time for small messages)\", \"0:0:0:0:0\");\n  simgrid::config::declare_flag<std::string>(\n      \"smpi/or\", \"Small messages timings (MPI_Recv minimum time for small messages)\", \"0:0:0:0:0\");\n  _smpi_options_initialized=true;\n}\n\nvoid smpi_check_options()\n{\n#if SIMGRID_HAVE_MC\n  if (MC_is_active()) {\n    if (_sg_mc_buffering == \"zero\")\n      simgrid::config::set_value<int>(\"smpi/send-is-detached-thresh\", 0);\n    else if (_sg_mc_buffering == \"infty\")\n      simgrid::config::set_value<int>(\"smpi/send-is-detached-thresh\", INT_MAX);\n    else\n      THROW_IMPOSSIBLE;\n  }\n#endif\n\n  xbt_assert(smpi_cfg_async_small_thresh() <= smpi_cfg_detached_send_thresh(),\n             \"smpi/async-small-thresh (=%d) should be smaller or equal to smpi/send-is-detached-thresh (=%d)\",\n             smpi_cfg_async_small_thresh(),\n             smpi_cfg_detached_send_thresh());\n\n  if (simgrid::config::is_default(\"smpi/host-speed\") && not MC_is_active()) {\n    XBT_INFO(\"You did not set the power of the host running the simulation.  \"\n             \"The timings will certainly not be accurate.  \"\n             \"Use the option \\\"--cfg=smpi/host-speed:<flops>\\\" to set its value.  \"\n             \"Check \"\n             \"https://simgrid.org/doc/latest/Configuring_SimGrid.html#automatic-benchmarking-of-smpi-code for more \"\n             \"information.\");\n  }\n\n  simgrid::smpi::colls::set_collectives();\n  simgrid::smpi::colls::smpi_coll_cleanup_callback = nullptr;\n}\n\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/src/smpi/internals/smpi_global.cpp": "/* Copyright (c) 2007-2020. The SimGrid Team. All rights reserved.          */\n\n/* This program is free software; you can redistribute it and/or modify it\n * under the terms of the license (GNU LGPL) which comes with this package. */\n\n#include \"mc/mc.h\"\n#include \"simgrid/s4u/Engine.hpp\"\n#include \"simgrid/plugins/file_system.h\"\n#include \"smpi_coll.hpp\"\n#include \"smpi_f2c.hpp\"\n#include \"smpi_host.hpp\"\n#include \"smpi_config.hpp\"\n#include \"src/kernel/activity/CommImpl.hpp\"\n#include \"src/simix/smx_private.hpp\"\n#include \"src/smpi/include/smpi_actor.hpp\"\n#include \"xbt/config.hpp\"\n#include \"xbt/file.hpp\"\n\n#include <algorithm>\n#include <array>\n#include <boost/algorithm/string.hpp> /* split */\n#include <boost/tokenizer.hpp>\n#include <cinttypes>\n#include <cstdint> /* intmax_t */\n#include <dlfcn.h>\n#include <fcntl.h>\n#include <fstream>\n#include <sys/stat.h>\n\n#if SG_HAVE_SENDFILE\n#include <sys/sendfile.h>\n#endif\n\n#if HAVE_PAPI\n#include \"papi.h\"\n#endif\n\n#if not defined(__APPLE__) && not defined(__HAIKU__)\n#include <link.h>\n#endif\n\nXBT_LOG_NEW_DEFAULT_SUBCATEGORY(smpi_kernel, smpi, \"Logging specific to SMPI (kernel)\");\n\n#if SMPI_IFORT\n  extern \"C\" void for_rtl_init_ (int *, char **);\n  extern \"C\" void for_rtl_finish_ ();\n#elif SMPI_FLANG\n  extern \"C\" void __io_set_argc(int);\n  extern \"C\" void __io_set_argv(char **);\n#elif SMPI_GFORTRAN\n  extern \"C\" void _gfortran_set_args(int, char **);\n#endif\n\n/* RTLD_DEEPBIND is a bad idea of GNU ld that obviously does not exist on other platforms\n * See https://www.akkadia.org/drepper/dsohowto.pdf\n * and https://lists.freebsd.org/pipermail/freebsd-current/2016-March/060284.html\n*/\n#if !RTLD_DEEPBIND || HAVE_SANITIZER_ADDRESS || HAVE_SANITIZER_THREAD\n#define WANT_RTLD_DEEPBIND 0\n#else\n#define WANT_RTLD_DEEPBIND RTLD_DEEPBIND\n#endif\n\n#if HAVE_PAPI\nstd::string papi_default_config_name = \"default\";\nstd::map</* computation unit name */ std::string, papi_process_data> units2papi_setup;\n#endif\n\nstd::unordered_map<std::string, double> location2speedup;\n\nstatic int smpi_exit_status = 0;\nextern double smpi_total_benched_time;\nxbt_os_timer_t global_timer;\nstatic std::vector<std::string> privatize_libs_paths;\n/**\n * Setting MPI_COMM_WORLD to MPI_COMM_UNINITIALIZED (it's a variable)\n * is important because the implementation of MPI_Comm checks\n * \"this == MPI_COMM_UNINITIALIZED\"? If yes, it uses smpi_process()->comm_world()\n * instead of \"this\".\n * This is basically how we only have one global variable but all processes have\n * different communicators (the one their SMPI instance uses).\n *\n * See smpi_comm.cpp and the functions therein for details.\n */\nMPI_Comm MPI_COMM_WORLD = MPI_COMM_UNINITIALIZED;\n// No instance gets manually created; check also the smpirun.in script as\n// this default name is used there as well (when the <actor> tag is generated).\nstatic const std::string smpi_default_instance_name(\"smpirun\");\nstatic simgrid::config::Flag<double> smpi_init_sleep(\n  \"smpi/init\", \"Time to inject inside a call to MPI_Init\", 0.0);\n\nvoid (*smpi_comm_copy_data_callback)(simgrid::kernel::activity::CommImpl*, void*,\n                                     size_t) = &smpi_comm_copy_buffer_callback;\n\nsimgrid::smpi::ActorExt* smpi_process()\n{\n  simgrid::s4u::ActorPtr me = simgrid::s4u::Actor::self();\n\n  if (me == nullptr) // This happens sometimes (eg, when linking against NS3 because it pulls openMPI...)\n    return nullptr;\n\n  return me->extension<simgrid::smpi::ActorExt>();\n}\n\nsimgrid::smpi::ActorExt* smpi_process_remote(simgrid::s4u::ActorPtr actor)\n{\n  if (actor.get() == nullptr)\n    return nullptr;\n  return actor->extension<simgrid::smpi::ActorExt>();\n}\n\nMPI_Comm smpi_process_comm_self(){\n  return smpi_process()->comm_self();\n}\n\nMPI_Info smpi_process_info_env(){\n  return smpi_process()->info_env();\n}\n\nvoid * smpi_process_get_user_data(){\n  return simgrid::s4u::Actor::self()->get_data();\n}\n\nvoid smpi_process_set_user_data(void *data){\n  simgrid::s4u::Actor::self()->set_data(data);\n}\n\nvoid smpi_comm_set_copy_data_callback(void (*callback) (smx_activity_t, void*, size_t))\n{\n  static void (*saved_callback)(smx_activity_t, void*, size_t);\n  saved_callback               = callback;\n  smpi_comm_copy_data_callback = [](simgrid::kernel::activity::CommImpl* comm, void* buff, size_t size) {\n    saved_callback(comm, buff, size);\n  };\n}\n\nstatic void memcpy_private(void* dest, const void* src, const std::vector<std::pair<size_t, size_t>>& private_blocks)\n{\n  for (auto const& block : private_blocks)\n    memcpy((uint8_t*)dest+block.first, (uint8_t*)src+block.first, block.second-block.first);\n}\n\nstatic void check_blocks(const std::vector<std::pair<size_t, size_t>>& private_blocks, size_t buff_size)\n{\n  for (auto const& block : private_blocks)\n    xbt_assert(block.first <= block.second && block.second <= buff_size, \"Oops, bug in shared malloc.\");\n}\n\nstatic void smpi_cleanup_comm_after_copy(simgrid::kernel::activity::CommImpl* comm, void* buff){\n  if (comm->detached()) {\n    // if this is a detached send, the source buffer was duplicated by SMPI\n    // sender to make the original buffer available to the application ASAP\n    xbt_free(buff);\n    //It seems that the request is used after the call there this should be free somewhere else but where???\n    //xbt_free(comm->comm.src_data);// inside SMPI the request is kept inside the user data and should be free\n    comm->src_buff_ = nullptr;\n  }\n}\n\nvoid smpi_comm_copy_buffer_callback(simgrid::kernel::activity::CommImpl* comm, void* buff, size_t buff_size)\n{\n  size_t src_offset                     = 0;\n  size_t dst_offset                     = 0;\n  std::vector<std::pair<size_t, size_t>> src_private_blocks;\n  std::vector<std::pair<size_t, size_t>> dst_private_blocks;\n  XBT_DEBUG(\"Copy the data over\");\n  if(smpi_is_shared(buff, src_private_blocks, &src_offset)) {\n    src_private_blocks = shift_and_frame_private_blocks(src_private_blocks, src_offset, buff_size);\n    if (src_private_blocks.empty()) { // simple shared malloc ... return.\n      XBT_VERB(\"Sender is shared. Let's ignore it.\");\n      smpi_cleanup_comm_after_copy(comm, buff);\n      return;\n    }\n  }\n  else {\n    src_private_blocks.clear();\n    src_private_blocks.emplace_back(0, buff_size);\n  }\n  if (smpi_is_shared((char*)comm->dst_buff_, dst_private_blocks, &dst_offset)) {\n    dst_private_blocks = shift_and_frame_private_blocks(dst_private_blocks, dst_offset, buff_size);\n    if (dst_private_blocks.empty()) { // simple shared malloc ... return.\n      XBT_VERB(\"Receiver is shared. Let's ignore it.\");\n      smpi_cleanup_comm_after_copy(comm, buff);\n      return;\n    }\n  }\n  else {\n    dst_private_blocks.clear();\n    dst_private_blocks.emplace_back(0, buff_size);\n  }\n  check_blocks(src_private_blocks, buff_size);\n  check_blocks(dst_private_blocks, buff_size);\n  auto private_blocks = merge_private_blocks(src_private_blocks, dst_private_blocks);\n  check_blocks(private_blocks, buff_size);\n  void* tmpbuff=buff;\n  if ((smpi_cfg_privatization() == SmpiPrivStrategies::MMAP) &&\n      (static_cast<char*>(buff) >= smpi_data_exe_start) &&\n      (static_cast<char*>(buff) < smpi_data_exe_start + smpi_data_exe_size)) {\n    XBT_DEBUG(\"Privatization : We are copying from a zone inside global memory... Saving data to temp buffer !\");\n    smpi_switch_data_segment(comm->src_actor_->get_iface());\n    tmpbuff = xbt_malloc(buff_size);\n    memcpy_private(tmpbuff, buff, private_blocks);\n  }\n\n  if ((smpi_cfg_privatization() == SmpiPrivStrategies::MMAP) &&\n      ((char*)comm->dst_buff_ >= smpi_data_exe_start) &&\n      ((char*)comm->dst_buff_ < smpi_data_exe_start + smpi_data_exe_size)) {\n    XBT_DEBUG(\"Privatization : We are copying to a zone inside global memory - Switch data segment\");\n    smpi_switch_data_segment(comm->dst_actor_->get_iface());\n  }\n  XBT_DEBUG(\"Copying %zu bytes from %p to %p\", buff_size, tmpbuff, comm->dst_buff_);\n  memcpy_private(comm->dst_buff_, tmpbuff, private_blocks);\n\n  smpi_cleanup_comm_after_copy(comm,buff);\n  if (tmpbuff != buff)\n    xbt_free(tmpbuff);\n}\n\nvoid smpi_comm_null_copy_buffer_callback(simgrid::kernel::activity::CommImpl*, void*, size_t)\n{\n  /* nothing done in this version */\n}\n\nint smpi_enabled() {\n  return MPI_COMM_WORLD != MPI_COMM_UNINITIALIZED;\n}\n\nstatic void smpi_init_papi()\n{\n#if HAVE_PAPI\n  // This map holds for each computation unit (such as \"default\" or \"process1\" etc.)\n  // the configuration as given by the user (counter data as a pair of (counter_name, counter_counter))\n  // and the (computed) event_set.\n\n  if (not smpi_cfg_papi_events_file().empty()) {\n    if (PAPI_library_init(PAPI_VER_CURRENT) != PAPI_VER_CURRENT)\n      XBT_ERROR(\"Could not initialize PAPI library; is it correctly installed and linked?\"\n                \" Expected version is %u\", PAPI_VER_CURRENT);\n\n    using Tokenizer = boost::tokenizer<boost::char_separator<char>>;\n    boost::char_separator<char> separator_units(\";\");\n    std::string str = smpi_cfg_papi_events_file();\n    Tokenizer tokens(str, separator_units);\n\n    // Iterate over all the computational units. This could be processes, hosts, threads, ranks... You name it.\n    // I'm not exactly sure what we will support eventually, so I'll leave it at the general term \"units\".\n    for (auto const& unit_it : tokens) {\n      boost::char_separator<char> separator_events(\":\");\n      Tokenizer event_tokens(unit_it, separator_events);\n\n      int event_set = PAPI_NULL;\n      if (PAPI_create_eventset(&event_set) != PAPI_OK) {\n        // TODO: Should this let the whole simulation die?\n        XBT_CRITICAL(\"Could not create PAPI event set during init.\");\n      }\n\n      // NOTE: We cannot use a map here, as we must obey the order of the counters\n      // This is important for PAPI: We need to map the values of counters back to the event_names (so, when PAPI_read()\n      // has finished)!\n      papi_counter_t counters2values;\n\n      // Iterate over all counters that were specified for this specific unit.\n      // Note that we need to remove the name of the unit (that could also be the \"default\" value), which always comes\n      // first. Hence, we start at ++(events.begin())!\n      for (Tokenizer::iterator events_it = ++(event_tokens.begin()); events_it != event_tokens.end(); ++events_it) {\n        int event_code   = PAPI_NULL;\n        auto* event_name = const_cast<char*>((*events_it).c_str());\n        if (PAPI_event_name_to_code(event_name, &event_code) != PAPI_OK) {\n          XBT_CRITICAL(\"Could not find PAPI event '%s'. Skipping.\", event_name);\n          continue;\n        }\n        if (PAPI_add_event(event_set, event_code) != PAPI_OK) {\n          XBT_ERROR(\"Could not add PAPI event '%s'. Skipping.\", event_name);\n          continue;\n        }\n        XBT_DEBUG(\"Successfully added PAPI event '%s' to the event set.\", event_name);\n\n        counters2values.push_back(\n            // We cannot just pass *events_it, as this is of type const basic_string\n            std::make_pair(std::string(*events_it), 0LL));\n      }\n\n      std::string unit_name    = *(event_tokens.begin());\n      papi_process_data config = {.counter_data = std::move(counters2values), .event_set = event_set};\n\n      units2papi_setup.insert(std::make_pair(unit_name, std::move(config)));\n    }\n  }\n#endif\n}\n\nusing smpi_entry_point_type         = std::function<int(int argc, char* argv[])>;\nusing smpi_c_entry_point_type       = int (*)(int argc, char** argv);\nusing smpi_fortran_entry_point_type = void (*)();\n\ntemplate <typename F>\nstatic int smpi_run_entry_point(const F& entry_point, const std::string& executable_path, std::vector<std::string> args)\n{\n  // copy C strings, we need them writable\n  auto* args4argv = new std::vector<char*>(args.size());\n  std::transform(begin(args), end(args), begin(*args4argv), [](const std::string& s) { return xbt_strdup(s.c_str()); });\n\n  // set argv[0] to executable_path\n  xbt_free((*args4argv)[0]);\n  (*args4argv)[0] = xbt_strdup(executable_path.c_str());\n\n#if !SMPI_IFORT\n  // take a copy of args4argv to keep reference of the allocated strings\n  const std::vector<char*> args2str(*args4argv);\n#endif\n  int argc = args4argv->size();\n  args4argv->push_back(nullptr);\n  char** argv = args4argv->data();\n\n#if SMPI_IFORT\n  for_rtl_init_ (&argc, argv);\n#elif SMPI_FLANG\n  __io_set_argc(argc);\n  __io_set_argv(argv);\n#elif SMPI_GFORTRAN\n  _gfortran_set_args(argc, argv);\n#endif \n  int res = entry_point(argc, argv);\n\n#if SMPI_IFORT\n  for_rtl_finish_ ();\n#else\n  for (char* s : args2str)\n    xbt_free(s);\n  delete args4argv;\n#endif\n\n  if (res != 0){\n    XBT_WARN(\"SMPI process did not return 0. Return value : %d\", res);\n    if (smpi_exit_status == 0)\n      smpi_exit_status = res;\n  }\n  return 0;\n}\n\n\n// TODO, remove the number of functions involved here\nstatic smpi_entry_point_type smpi_resolve_function(void* handle)\n{\n  auto* entry_point_fortran = reinterpret_cast<smpi_fortran_entry_point_type>(dlsym(handle, \"user_main_\"));\n  if (entry_point_fortran != nullptr) {\n    return [entry_point_fortran](int, char**) {\n      entry_point_fortran();\n      return 0;\n    };\n  }\n\n  auto* entry_point = reinterpret_cast<smpi_c_entry_point_type>(dlsym(handle, \"main\"));\n  if (entry_point != nullptr) {\n    return entry_point;\n  }\n\n  return smpi_entry_point_type();\n}\n\nstatic void smpi_copy_file(const std::string& src, const std::string& target, off_t fdin_size)\n{\n  int fdin = open(src.c_str(), O_RDONLY);\n  xbt_assert(fdin >= 0, \"Cannot read from %s. Please make sure that the file exists and is executable.\", src.c_str());\n  int fdout = open(target.c_str(), O_CREAT | O_RDWR, S_IRWXU);\n  xbt_assert(fdout >= 0, \"Cannot write into %s\", target.c_str());\n\n  XBT_DEBUG(\"Copy %\" PRIdMAX \" bytes into %s\", static_cast<intmax_t>(fdin_size), target.c_str());\n#if SG_HAVE_SENDFILE\n  ssize_t sent_size = sendfile(fdout, fdin, nullptr, fdin_size);\n  if (sent_size == fdin_size) {\n    close(fdin);\n    close(fdout);\n    return;\n  } else if (sent_size != -1 || errno != ENOSYS) {\n    xbt_die(\"Error while copying %s: only %zd bytes copied instead of %\" PRIdMAX \" (errno: %d -- %s)\", target.c_str(),\n            sent_size, static_cast<intmax_t>(fdin_size), errno, strerror(errno));\n  }\n#endif\n  // If this point is reached, sendfile() actually is not available.  Copy file by hand.\n  std::vector<unsigned char> buf(1024 * 1024 * 4);\n  while (ssize_t got = read(fdin, buf.data(), buf.size())) {\n    if (got == -1) {\n      xbt_assert(errno == EINTR, \"Cannot read from %s\", src.c_str());\n    } else {\n      const unsigned char* p = buf.data();\n      ssize_t todo           = got;\n      while (ssize_t done = write(fdout, p, todo)) {\n        if (done == -1) {\n          xbt_assert(errno == EINTR, \"Cannot write into %s\", target.c_str());\n        } else {\n          p += done;\n          todo -= done;\n        }\n      }\n    }\n  }\n  close(fdin);\n  close(fdout);\n}\n\n#if not defined(__APPLE__) && not defined(__HAIKU__)\nstatic int visit_libs(struct dl_phdr_info* info, size_t, void* data)\n{\n  auto* libname    = static_cast<std::string*>(data);\n  std::string path = info->dlpi_name;\n  if (path.find(*libname) != std::string::npos) {\n    *libname = std::move(path);\n    return 1;\n  }\n  return 0;\n}\n#endif\n\nstatic void smpi_init_privatization_dlopen(const std::string& executable)\n{\n  // Prepare the copy of the binary (get its size)\n  struct stat fdin_stat;\n  stat(executable.c_str(), &fdin_stat);\n  off_t fdin_size         = fdin_stat.st_size;\n\n  std::string libnames = simgrid::config::get_value<std::string>(\"smpi/privatize-libs\");\n  if (not libnames.empty()) {\n    // split option\n    std::vector<std::string> privatize_libs;\n    boost::split(privatize_libs, libnames, boost::is_any_of(\";\"));\n\n    for (auto const& libname : privatize_libs) {\n      // load the library once to add it to the local libs, to get the absolute path\n      void* libhandle = dlopen(libname.c_str(), RTLD_LAZY);\n      xbt_assert(libhandle != nullptr, \n\t\t      \"Cannot dlopen %s - check your settings in smpi/privatize-libs\", libname.c_str());\n      // get library name from path\n      std::string fullpath = libname;\n#if not defined(__APPLE__) && not defined(__HAIKU__)\n      XBT_ATTRIB_UNUSED int dl_iterate_res = dl_iterate_phdr(visit_libs, &fullpath);\n      xbt_assert(dl_iterate_res != 0, \"Can't find a linked %s - check your settings in smpi/privatize-libs\",\n                 fullpath.c_str());\n      XBT_DEBUG(\"Extra lib to privatize '%s' found\", fullpath.c_str());\n#else\n      xbt_die(\"smpi/privatize-libs is not (yet) compatible with OSX nor with Haiku\");\n#endif\n      privatize_libs_paths.emplace_back(std::move(fullpath));\n      dlclose(libhandle);\n    }\n  }\n\n  simgrid::s4u::Engine::get_instance()->register_default([executable, fdin_size](std::vector<std::string> args) {\n    return std::function<void()>([executable, fdin_size, args] {\n      static std::size_t rank = 0;\n      // Copy the dynamic library:\n      simgrid::xbt::Path path(executable);\n      std::string target_executable = simgrid::config::get_value<std::string>(\"smpi/tmpdir\") + \"/\" +\n          path.get_base_name() + \"_\" + std::to_string(getpid()) + \"_\" + std::to_string(rank) + \".so\";\n\n      smpi_copy_file(executable, target_executable, fdin_size);\n      // if smpi/privatize-libs is set, duplicate pointed lib and link each executable copy to a different one.\n      std::vector<std::string> target_libs;\n      for (auto const& libpath : privatize_libs_paths) {\n        // if we were given a full path, strip it\n        size_t index = libpath.find_last_of(\"/\\\\\");\n        std::string libname;\n        if (index != std::string::npos)\n          libname = libpath.substr(index + 1);\n\n        if (not libname.empty()) {\n          // load the library to add it to the local libs, to get the absolute path\n          struct stat fdin_stat2;\n          stat(libpath.c_str(), &fdin_stat2);\n          off_t fdin_size2 = fdin_stat2.st_size;\n\n          // Copy the dynamic library, the new name must be the same length as the old one\n          // just replace the name with 7 digits for the rank and the rest of the name.\n          unsigned int pad = 7;\n          if (libname.length() < pad)\n            pad = libname.length();\n          std::string target_libname = std::string(pad - std::to_string(rank).length(), '0') + std::to_string(rank) + libname.substr(pad);\n          std::string target_lib = simgrid::config::get_value<std::string>(\"smpi/tmpdir\") + \"/\" + target_libname;\n          target_libs.push_back(target_lib);\n          XBT_DEBUG(\"copy lib %s to %s, with size %lld\", libpath.c_str(), target_lib.c_str(), (long long)fdin_size2);\n          smpi_copy_file(libpath, target_lib, fdin_size2);\n\n          std::string sedcommand = \"sed -i -e 's/\" + libname + \"/\" + target_libname + \"/g' \" + target_executable;\n          int status             = system(sedcommand.c_str());\n          xbt_assert(status == 0, \"error while applying sed command %s \\n\", sedcommand.c_str());\n        }\n      }\n\n      rank++;\n      // Load the copy and resolve the entry point:\n      void* handle    = dlopen(target_executable.c_str(), RTLD_LAZY | RTLD_LOCAL | WANT_RTLD_DEEPBIND);\n      int saved_errno = errno;\n      if (not simgrid::config::get_value<bool>(\"smpi/keep-temps\")) {\n        unlink(target_executable.c_str());\n        for (const std::string& target_lib : target_libs)\n          unlink(target_lib.c_str());\n      }\n      xbt_assert(handle != nullptr, \"dlopen failed: %s (errno: %d -- %s)\", dlerror(), saved_errno,\n                 strerror(saved_errno));\n\n      smpi_entry_point_type entry_point = smpi_resolve_function(handle);\n      xbt_assert(entry_point, \"Could not resolve entry point\");\n      smpi_run_entry_point(entry_point, executable, args);\n    });\n  });\n}\n\nstatic void smpi_init_privatization_no_dlopen(const std::string& executable)\n{\n  if (smpi_cfg_privatization() == SmpiPrivStrategies::MMAP)\n    smpi_prepare_global_memory_segment();\n\n  // Load the dynamic library and resolve the entry point:\n  void* handle = dlopen(executable.c_str(), RTLD_LAZY | RTLD_LOCAL);\n  xbt_assert(handle != nullptr, \"dlopen failed for %s: %s (errno: %d -- %s)\", executable.c_str(), dlerror(), errno,\n             strerror(errno));\n  smpi_entry_point_type entry_point = smpi_resolve_function(handle);\n  xbt_assert(entry_point, \"main not found in %s\", executable.c_str());\n\n  if (smpi_cfg_privatization() == SmpiPrivStrategies::MMAP)\n    smpi_backup_global_memory_segment();\n\n  // Execute the same entry point for each simulated process:\n  simgrid::s4u::Engine::get_instance()->register_default([entry_point, executable](std::vector<std::string> args) {\n    return std::function<void()>(\n        [entry_point, executable, args] { smpi_run_entry_point(entry_point, executable, args); });\n  });\n}\n\nint smpi_main(const char* executable, int argc, char* argv[])\n{\n  if (getenv(\"SMPI_PRETEND_CC\") != nullptr) {\n    /* Hack to ensure that smpicc can pretend to be a simple compiler. Particularly handy to pass it to the\n     * configuration tools */\n    return 0;\n  }\n  \n  SMPI_switch_data_segment = &smpi_switch_data_segment;\n  smpi_init_options();\n  simgrid::instr::init();\n  SIMIX_global_init(&argc, argv);\n\n  auto engine              = simgrid::s4u::Engine::get_instance();\n\n  sg_storage_file_system_init();\n  // parse the platform file: get the host list\n  engine->load_platform(argv[1]);\n  SIMIX_comm_set_copy_data_callback(smpi_comm_copy_buffer_callback);\n\n  if (smpi_cfg_privatization() == SmpiPrivStrategies::DLOPEN)\n    smpi_init_privatization_dlopen(executable);\n  else\n    smpi_init_privatization_no_dlopen(executable);\n\n  simgrid::smpi::colls::set_collectives();\n  simgrid::smpi::colls::smpi_coll_cleanup_callback = nullptr;\n  \n  SMPI_init();\n\n  /* This is a ... heavy way to count the MPI ranks */\n  int rank_counts = 0;\n  simgrid::s4u::Actor::on_creation.connect([&rank_counts](const simgrid::s4u::Actor& actor) {\n    if (not actor.is_daemon())\n      rank_counts++;\n  });\n  engine->load_deployment(argv[2]);\n\n  SMPI_app_instance_register(smpi_default_instance_name.c_str(), nullptr, rank_counts);\n  MPI_COMM_WORLD = *smpi_deployment_comm_world(smpi_default_instance_name);\n\n  /* Clean IO before the run */\n  fflush(stdout);\n  fflush(stderr);\n\n  if (MC_is_active()) {\n    MC_run();\n  } else {\n    SIMIX_run();\n\n    xbt_os_walltimer_stop(global_timer);\n    if (simgrid::config::get_value<bool>(\"smpi/display-timing\")) {\n      double global_time = xbt_os_timer_elapsed(global_timer);\n      XBT_INFO(\"Simulated time: %g seconds. \\n\\n\"\n          \"The simulation took %g seconds (after parsing and platform setup)\\n\"\n          \"%g seconds were actual computation of the application\",\n          SIMIX_get_clock(), global_time , smpi_total_benched_time);\n\n      if (smpi_total_benched_time/global_time>=0.75)\n      XBT_INFO(\"More than 75%% of the time was spent inside the application code.\\n\"\n      \"You may want to use sampling functions or trace replay to reduce this.\");\n    }\n  }\n  SMPI_finalize();\n\n  return smpi_exit_status;\n}\n\n// Called either directly from the user code, or from the code called by smpirun\nvoid SMPI_init(){\n  smpi_init_options();\n  simgrid::s4u::Actor::on_creation.connect([](simgrid::s4u::Actor& actor) {\n    if (not actor.is_daemon())\n      actor.extension_set<simgrid::smpi::ActorExt>(new simgrid::smpi::ActorExt(&actor));\n  });\n  simgrid::s4u::Host::on_creation.connect(\n      [](simgrid::s4u::Host& host) { host.extension_set(new simgrid::smpi::Host(&host)); });\n  for (auto const& host : simgrid::s4u::Engine::get_instance()->get_all_hosts())\n    host->extension_set(new simgrid::smpi::Host(host));\n\n  if (not MC_is_active()) {\n    global_timer = xbt_os_timer_new();\n    xbt_os_walltimer_start(global_timer);\n  }\n  smpi_init_papi();\n  smpi_check_options();\n}\n\nvoid SMPI_finalize()\n{\n  smpi_bench_destroy();\n  smpi_shared_destroy();\n  smpi_deployment_cleanup_instances();\n\n  if (simgrid::smpi::colls::smpi_coll_cleanup_callback != nullptr)\n    simgrid::smpi::colls::smpi_coll_cleanup_callback();\n\n  MPI_COMM_WORLD = MPI_COMM_NULL;\n\n  if (not MC_is_active()) {\n    xbt_os_timer_free(global_timer);\n  }\n\n  if (smpi_cfg_privatization() == SmpiPrivStrategies::MMAP)\n    smpi_destroy_global_memory_segments();\n  if (simgrid::smpi::F2C::lookup() != nullptr)\n    simgrid::smpi::F2C::delete_lookup();\n}\n\nvoid smpi_mpi_init() {\n  smpi_init_fortran_types();\n  if(smpi_init_sleep > 0)\n    simgrid::s4u::this_actor::sleep_for(smpi_init_sleep);\n}\n\nvoid SMPI_thread_create() {\n  TRACE_smpi_init(simgrid::s4u::this_actor::get_pid(), __func__);\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/src/simix/smx_context.cpp": "/* a fast and simple context switching library                              */\n\n/* Copyright (c) 2009-2020. The SimGrid Team. All rights reserved.          */\n\n/* This program is free software; you can redistribute it and/or modify it\n * under the terms of the license (GNU LGPL) which comes with this package. */\n\n#include \"src/internal_config.h\"\n#include \"src/simix/smx_private.hpp\"\n#include \"smpi/smpi.h\"\n#include \"xbt/config.hpp\"\n\n#include <initializer_list>\n#include <thread>\n\nXBT_LOG_NEW_DEFAULT_SUBCATEGORY(simix_context, simix, \"Context switching mechanism\");\n\nconstexpr std::initializer_list<std::pair<const char*, simgrid::kernel::context::ContextFactoryInitializer>>\n    context_factories = {\n#if HAVE_RAW_CONTEXTS\n        {\"raw\", &simgrid::kernel::context::raw_factory},\n#endif\n#if HAVE_UCONTEXT_CONTEXTS\n        {\"ucontext\", &simgrid::kernel::context::sysv_factory},\n#endif\n#if HAVE_BOOST_CONTEXTS\n        {\"boost\", &simgrid::kernel::context::boost_factory},\n#endif\n        {\"thread\", &simgrid::kernel::context::thread_factory},\n};\n\nstatic_assert(context_factories.size() > 0, \"No context factories are enabled for this build\");\n\n// Create the list of possible contexts:\nstatic inline\nstd::string contexts_list()\n{\n  std::string res;\n  std::string sep = \"\";\n  for (auto const& factory : context_factories) {\n    res += sep + factory.first;\n    sep = \", \";\n  }\n  return res;\n}\n\nstatic simgrid::config::Flag<std::string>\n    context_factory_name(\"contexts/factory\", (std::string(\"Possible values: \") + contexts_list()).c_str(),\n                         context_factories.begin()->first);\n\nunsigned smx_context_stack_size;\nunsigned smx_context_guard_size;\nstatic int smx_parallel_contexts = 1;\nstatic e_xbt_parmap_mode_t smx_parallel_synchronization_mode = XBT_PARMAP_DEFAULT;\n\n/**\n * This function is called by SIMIX_global_init() to initialize the context module.\n */\nvoid SIMIX_context_mod_init()\n{\n  xbt_assert(simix_global->context_factory == nullptr);\n\n#if HAVE_SMPI && (defined(__APPLE__) || defined(__NetBSD__))\n  smpi_init_options();\n  std::string priv = simgrid::config::get_value<std::string>(\"smpi/privatization\");\n  if (context_factory_name == \"thread\" && (priv == \"dlopen\" || priv == \"yes\" || priv == \"default\" || priv == \"1\")) {\n    XBT_WARN(\"dlopen+thread broken on Apple and BSD. Switching to raw contexts.\");\n    context_factory_name = \"raw\";\n  }\n#endif\n\n#if HAVE_SMPI && defined(__FreeBSD__)\n  smpi_init_options();\n  if (context_factory_name == \"thread\" && simgrid::config::get_value<std::string>(\"smpi/privatization\") != \"no\") {\n    XBT_WARN(\"mmap broken on FreeBSD, but dlopen+thread broken too. Switching to dlopen+raw contexts.\");\n    context_factory_name = \"raw\";\n  }\n#endif\n\n  /* select the context factory to use to create the contexts */\n  if (simgrid::kernel::context::factory_initializer != nullptr) { // Give Java a chance to hijack the factory mechanism\n    simix_global->context_factory = simgrid::kernel::context::factory_initializer();\n    return;\n  }\n  /* use the factory specified by --cfg=contexts/factory:value */\n  for (auto const& factory : context_factories)\n    if (context_factory_name == factory.first) {\n      simix_global->context_factory = factory.second();\n      break;\n    }\n\n  if (simix_global->context_factory == nullptr) {\n    XBT_ERROR(\"Invalid context factory specified. Valid factories on this machine:\");\n#if HAVE_RAW_CONTEXTS\n    XBT_ERROR(\"  raw: high performance context factory implemented specifically for SimGrid\");\n#else\n    XBT_ERROR(\"  (raw contexts were disabled at compilation time on this machine -- check configure logs for details)\");\n#endif\n#if HAVE_UCONTEXT_CONTEXTS\n    XBT_ERROR(\"  ucontext: classical system V contexts (implemented with makecontext, swapcontext and friends)\");\n#else\n    XBT_ERROR(\"  (ucontext was disabled at compilation time on this machine -- check configure logs for details)\");\n#endif\n#if HAVE_BOOST_CONTEXTS\n    XBT_ERROR(\"  boost: this uses the boost libraries context implementation\");\n#else\n    XBT_ERROR(\"  (boost was disabled at compilation time on this machine -- check configure logs for details. Did you install the libboost-context-dev package?)\");\n#endif\n    XBT_ERROR(\"  thread: slow portability layer using pthreads as provided by gcc\");\n    xbt_die(\"Please use a valid factory.\");\n  }\n}\n\n/**\n * This function is called by SIMIX_clean() to finalize the context module.\n */\nvoid SIMIX_context_mod_exit()\n{\n  delete simix_global->context_factory;\n  simix_global->context_factory = nullptr;\n}\n\n/** @brief Returns whether some parallel threads are used for the user contexts. */\nint SIMIX_context_is_parallel() {\n  return smx_parallel_contexts > 1;\n}\n\n/**\n * @brief Returns the number of parallel threads used for the user contexts.\n * @return the number of threads (1 means no parallelism)\n */\nint SIMIX_context_get_nthreads() {\n  return smx_parallel_contexts;\n}\n\n/**\n * @brief Sets the number of parallel threads to use\n * for the user contexts.\n *\n * This function should be called before initializing SIMIX.\n * A value of 1 means no parallelism (1 thread only).\n * If the value is greater than 1, the thread support must be enabled.\n *\n * @param nb_threads the number of threads to use\n */\nvoid SIMIX_context_set_nthreads(int nb_threads) {\n  if (nb_threads<=0) {\n    nb_threads = std::thread::hardware_concurrency();\n    XBT_INFO(\"Auto-setting contexts/nthreads to %d\", nb_threads);\n  }\n  smx_parallel_contexts = nb_threads;\n}\n\n/**\n * @brief Returns the synchronization mode used when processes are run in\n * parallel.\n * @return how threads are synchronized if processes are run in parallel\n */\ne_xbt_parmap_mode_t SIMIX_context_get_parallel_mode() {\n  return smx_parallel_synchronization_mode;\n}\n\n/**\n * @brief Sets the synchronization mode to use when processes are run in\n * parallel.\n * @param mode how to synchronize threads if processes are run in parallel\n */\nvoid SIMIX_context_set_parallel_mode(e_xbt_parmap_mode_t mode) {\n  smx_parallel_synchronization_mode = mode;\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/teshsuite/smpi/CMakeLists.txt": "if(enable_smpi)\n  if(WIN32)\n    set(CMAKE_C_FLAGS \"-include ${CMAKE_HOME_DIRECTORY}/include/smpi/smpi_main.h\")\n  else()\n    set(CMAKE_C_COMPILER \"${CMAKE_BINARY_DIR}/smpi_script/bin/smpicc\")\n  endif()\n\n  include_directories(BEFORE \"${CMAKE_HOME_DIRECTORY}/include/smpi\")\n  foreach(x coll-allgather coll-allgatherv coll-allreduce coll-alltoall coll-alltoallv coll-barrier coll-bcast\n            coll-gather coll-reduce coll-reduce-scatter coll-scatter macro-sample pt2pt-dsend pt2pt-pingpong\n            type-hvector type-indexed type-struct type-vector bug-17132 gh-139 timers privatization \n            io-simple io-simple-at io-all io-all-at io-shared io-ordered topo-cart-sub)\n    add_executable       (${x}  EXCLUDE_FROM_ALL ${x}/${x}.c)\n    target_link_libraries(${x}  simgrid)\n    set_target_properties(${x}  PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/${x})\n    add_dependencies(tests ${x})\n  endforeach()\n\n  if(NOT WIN32)\n    foreach(x macro-shared auto-shared macro-partial-shared macro-partial-shared-communication )\n      add_executable       (${x}  EXCLUDE_FROM_ALL ${x}/${x}.c)\n      target_link_libraries(${x}  simgrid)\n      set_target_properties(${x}  PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/${x})\n      add_dependencies(tests ${x})\n    endforeach()\n  endif()\n\n  if(enable_smpi AND SMPI_FORTRAN)\n    set(CMAKE_Fortran_COMPILER \"${CMAKE_BINARY_DIR}/smpi_script/bin/smpif90\")\n    add_executable       (fort_args EXCLUDE_FROM_ALL fort_args/fort_args.f90)\n    target_link_libraries(fort_args simgrid)\n    target_link_libraries(fort_args mpi)\n    set_target_properties(fort_args PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/fort_args)\n    add_dependencies(tests fort_args)\n  endif()\nendif()\n\nforeach(x coll-allgather coll-allgatherv coll-allreduce coll-alltoall coll-alltoallv coll-barrier coll-bcast\n    coll-gather coll-reduce coll-reduce-scatter coll-scatter macro-sample pt2pt-dsend pt2pt-pingpong\n    type-hvector type-indexed type-struct type-vector bug-17132 gh-139 timers privatization\n    macro-shared auto-shared macro-partial-shared macro-partial-shared-communication\n    io-simple io-simple-at io-all io-all-at io-shared io-ordered topo-cart-sub)\n  set(tesh_files    ${tesh_files}    ${CMAKE_CURRENT_SOURCE_DIR}/${x}/${x}.tesh)\n  set(teshsuite_src ${teshsuite_src} ${CMAKE_CURRENT_SOURCE_DIR}/${x}/${x}.c)\nendforeach()\n\nset(teshsuite_src ${teshsuite_src} ${CMAKE_CURRENT_SOURCE_DIR}/fort_args/fort_args.f90 PARENT_SCOPE)\nset(tesh_files    ${tesh_files}     ${CMAKE_CURRENT_SOURCE_DIR}/coll-allreduce/coll-allreduce-large.tesh\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/coll-allreduce/coll-allreduce-automatic.tesh\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/coll-alltoall/clusters.tesh\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/pt2pt-pingpong/broken_hostfiles.tesh\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/pt2pt-pingpong/TI_output.tesh\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/fort_args/fort_args.tesh  PARENT_SCOPE)\nset(bin_files       ${bin_files}    ${CMAKE_CURRENT_SOURCE_DIR}/hostfile\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/hostfile_cluster\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/hostfile_coll\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/hostfile_mpich\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/hostfile_io\n                                    ${CMAKE_CURRENT_SOURCE_DIR}/hostfile_empty  PARENT_SCOPE)\n\n\nif(enable_smpi)\n  if(NOT WIN32)\n    ADD_TESH_FACTORIES(tesh-smpi-macro-shared \"thread;ucontext;raw;boost\" --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/macro-shared --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/macro-shared macro-shared.tesh)\n    ADD_TESH_FACTORIES(tesh-smpi-auto-shared \"thread;ucontext;raw;boost\" --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/auto-shared --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/auto-shared auto-shared.tesh)\n    ADD_TESH_FACTORIES(tesh-smpi-macro-partial-shared \"thread;ucontext;raw;boost\" --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/macro-partial-shared --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/macro-partial-shared macro-partial-shared.tesh)\n    ADD_TESH_FACTORIES(tesh-smpi-macro-partial-shared-communication \"thread;ucontext;raw;boost\" --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/macro-partial-shared-communication --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/macro-partial-shared-communication macro-partial-shared-communication.tesh)\n  endif()\n\n  foreach(x coll-allgather coll-allgatherv coll-allreduce coll-alltoall coll-alltoallv coll-barrier coll-bcast\n            coll-gather coll-reduce coll-reduce-scatter coll-scatter macro-sample pt2pt-dsend pt2pt-pingpong\n\t    type-hvector type-indexed type-struct type-vector bug-17132 timers io-simple io-simple-at io-all io-all-at io-shared io-ordered topo-cart-sub)\n    ADD_TESH_FACTORIES(tesh-smpi-${x} \"thread;ucontext;raw;boost\" --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv srcdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/${x} --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/${x} ${x}.tesh)\n  endforeach()\n\n  if(SMPI_FORTRAN)\n    ADD_TESH_FACTORIES(tesh-smpi-fort_args \"thread;ucontext;raw;boost\" --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv srcdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/fort_args --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/fort_args fort_args.tesh)\n  endif()\n\n  foreach (ALLGATHER 2dmesh 3dmesh bruck GB loosely_lr NTSLR NTSLR_NB pair rdb  rhv ring SMP_NTS smp_simple spreading_simple\n                     ompi mpich ompi_neighborexchange mvapich2 mvapich2_smp impi)\n    ADD_TESH(tesh-smpi-coll-allgather-${ALLGATHER} --cfg smpi/allgather:${ALLGATHER} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-allgather --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-allgather coll-allgather.tesh)\n  endforeach()\n\n  foreach (ALLGATHERV GB pair ring ompi mpich ompi_neighborexchange ompi_bruck mpich_rdb mpich_ring mvapich2 impi)\n    ADD_TESH(tesh-smpi-coll-allgatherv-${ALLGATHERV} --cfg smpi/allgatherv:${ALLGATHERV} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-allgatherv --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-allgatherv coll-allgatherv.tesh)\n  endforeach()\n\n  foreach (ALLREDUCE lr rab1 rab2 rab_rdb rdb smp_binomial smp_binomial_pipeline smp_rdb smp_rsag smp_rsag_lr impi\n                     smp_rsag_rab redbcast ompi mpich ompi_ring_segmented mvapich2 mvapich2_rs mvapich2_two_level)\n    ADD_TESH(tesh-smpi-coll-allreduce-${ALLREDUCE} --cfg smpi/allreduce:${ALLREDUCE} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-allreduce --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-allreduce coll-allreduce.tesh)\n  endforeach()\n\n  foreach (ALLTOALL 2dmesh 3dmesh pair pair_rma pair_one_barrier pair_light_barrier pair_mpi_barrier rdb ring\n                    ring_light_barrier ring_mpi_barrier ring_one_barrier bruck basic_linear ompi mpich mvapich2\n                    mvapich2_scatter_dest impi)\n    ADD_TESH(tesh-smpi-coll-alltoall-${ALLTOALL} --cfg smpi/alltoall:${ALLTOALL} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-alltoall --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-alltoall coll-alltoall.tesh)\n  endforeach()\n\n  foreach (ALLTOALLV pair pair_light_barrier pair_mpi_barrier pair_one_barrier  ring ring_light_barrier ring_mpi_barrier\n                     ring_one_barrier bruck ompi mpich mvapich2 ompi_basic_linear impi)\n    ADD_TESH(tesh-smpi-coll-alltoallv-${ALLTOALLV} --cfg smpi/alltoallv:${ALLTOALLV} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-alltoallv --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-alltoallv coll-alltoallv.tesh)\n  endforeach()\n\n  foreach (BARRIER ompi mpich mpich_smp ompi_basic_linear ompi_tree ompi_bruck ompi_recursivedoubling ompi_doublering mvapich2_pair mvapich2 impi)\n      ADD_TESH(tesh-smpi-coll-barrier-${BARRIER} --cfg smpi/barrier:${BARRIER} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-barrier --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-barrier coll-barrier.tesh)\n  endforeach()\n\n  foreach (BCAST arrival_pattern_aware arrival_pattern_aware_wait arrival_scatter binomial_tree flattree\n                 flattree_pipeline NTSB NTSL NTSL_Isend scatter_LR_allgather scatter_rdb_allgather SMP_binary\n                 SMP_binomial SMP_linear ompi mpich ompi_split_bintree ompi_pipeline mvapich2 mvapich2_intra_node\n                 mvapich2_knomial_intra_node impi)\n    ADD_TESH(tesh-smpi-coll-bcast-${BCAST} --cfg smpi/bcast:${BCAST} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-bcast --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-bcast coll-bcast.tesh)\n  endforeach()\n\n  foreach (GATHER ompi mpich ompi_basic_linear ompi_linear_sync ompi_binomial mvapich2 mvapich2_two_level impi)\n    ADD_TESH(tesh-smpi-coll-gather-${GATHER} --cfg smpi/gather:${GATHER} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-gather --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-gather coll-gather.tesh)\n  endforeach()\n\n  foreach (REDUCE arrival_pattern_aware binomial flat_tree NTSL scatter_gather ompi mpich ompi_chain ompi_binary impi\n                  ompi_basic_linear ompi_binomial ompi_in_order_binary mvapich2 mvapich2_knomial mvapich2_two_level rab)\n    ADD_TESH(tesh-smpi-coll-reduce-${REDUCE} --cfg smpi/reduce:${REDUCE} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-reduce --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-reduce coll-reduce.tesh)\n  endforeach()\n\n  foreach (REDUCE_SCATTER ompi mpich ompi_basic_recursivehalving ompi_ring mpich_noncomm mpich_pair mvapich2 mpich_rdb impi)\n    ADD_TESH(tesh-smpi-coll-reduce-scatter-${REDUCE_SCATTER} --cfg smpi/reduce_scatter:${REDUCE_SCATTER} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-reduce-scatter --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-reduce-scatter coll-reduce-scatter.tesh)\n  endforeach()\n\n  foreach (SCATTER ompi mpich ompi_basic_linear ompi_binomial mvapich2 mvapich2_two_level_binomial mvapich2_two_level_direct impi)\n    ADD_TESH(tesh-smpi-coll-scatter-${SCATTER} --cfg smpi/scatter:${SCATTER} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-scatter --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-scatter coll-scatter.tesh)\n  endforeach()\n\n  # Extra allreduce test: large automatic\n  ADD_TESH(tesh-smpi-coll-allreduce-large --cfg smpi/allreduce:ompi_ring_segmented --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-allreduce --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-allreduce coll-allreduce-large.tesh)\n  ADD_TESH(tesh-smpi-coll-allreduce-automatic --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-allreduce --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-allreduce coll-allreduce-automatic.tesh)\n\n  # Extra allreduce test: cluster-types\n  ADD_TESH(tesh-smpi-cluster-types --cfg smpi/alltoall:mvapich2 --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/coll-alltoall --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/coll-alltoall clusters.tesh)\n\n  # Extra pt2pt pingpong test: broken usage ti-tracing\n  ADD_TESH_FACTORIES(tesh-smpi-broken  \"thread\"   --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/pt2pt-pingpong --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/pt2pt-pingpong broken_hostfiles.tesh)\n  ADD_TESH(tesh-smpi-replay-ti-tracing            --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv srcdir=${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/pt2pt-pingpong --cd ${CMAKE_BINARY_DIR}/teshsuite/smpi/pt2pt-pingpong ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/pt2pt-pingpong/TI_output.tesh)\n  ADD_TESH_FACTORIES(tesh-smpi-gh-139  \"thread\"   --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/gh-139 --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/gh-139 gh-139.tesh)\n  \n  # Simple privatization tests\n  if(HAVE_PRIVATIZATION)\n    foreach(PRIVATIZATION dlopen mmap)\n      ADD_TESH_FACTORIES(tesh-smpi-privatization-${PRIVATIZATION}  \"thread;ucontext;raw;boost\" --setenv privatization=${PRIVATIZATION} --setenv platfdir=${CMAKE_HOME_DIRECTORY}/examples/platforms --setenv bindir=${CMAKE_BINARY_DIR}/teshsuite/smpi/privatization --cd ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/privatization privatization.tesh)\n    endforeach()\n  endif()\nendif()\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/teshsuite/smpi/mpich3-test/coll/CMakeLists.txt": "if(enable_smpi AND enable_smpi_MPICH3_testsuite)\n  if(WIN32)\n    set(CMAKE_C_FLAGS \"-include ${CMAKE_HOME_DIRECTORY}/include/smpi/smpi_main.h\")\n  else()\n    set(CMAKE_C_COMPILER \"${CMAKE_BINARY_DIR}/smpi_script/bin/smpicc\")\n    set(CMAKE_Fortran_COMPILER \"${CMAKE_BINARY_DIR}/smpi_script/bin/smpiff\")\n  endif()\n\n  include_directories(BEFORE \"${CMAKE_HOME_DIRECTORY}/include/smpi\")\n  include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}/../include/\")\n \n  foreach(test allgather2 allgather3 allgather_struct allgatherv2 allgatherv3\n          allred2 allred3 allred4 allred5 allred6 allredmany alltoall1 \n          alltoallv0 alltoallv alltoallw1 alltoallw2 alltoallw_zeros\n          bcasttest bcastzerotype coll2 coll3 coll4 coll5 coll6 coll7 coll8\n          coll9 coll10 coll11 coll12 coll13 exscan exscan2 gather gather2 \n          gather_big ibarrier longuser nonblocking nonblocking2 iallred\n        # icallgather icallgatherv icallreduce\n        # icalltoall icalltoallv icalltoallw icbarrier icbcast\n        # icgather icgatherv icreduce icscatter icscatterv\n         opband opbor opbxor opland oplor oplxor opmax opmaxloc\n         opmin opminloc opprod opsum \n         #nonblocking3\n          op_commutative red3 red4 redscat2 redscat3 redscatbkinter redscatblk3\n          redscat red_scat_block red_scat_block2 allgatherv4 allred\n        # redscatinter\n          reduce_local scantst scatter2 scatter3 scattern scatterv\n         uoplong\n         )\n    add_executable(${test} EXCLUDE_FROM_ALL ${test}.c)\n    add_dependencies(tests ${test})\n    target_link_libraries(${test} simgrid mtest_c)\n  endforeach()\n\n  set_target_properties(allred PROPERTIES COMPILE_FLAGS \"-O0\" LINK_FLAGS \"-O0\")\n\n  foreach(test bcast_full bcast_min_datatypes bcast_comm_world)\n    add_executable(${test} EXCLUDE_FROM_ALL bcast.c)\n    add_dependencies(tests ${test})\n    target_link_libraries(${test} simgrid mtest_c)\n  endforeach()      \n  set_target_properties(bcast_min_datatypes PROPERTIES COMPILE_FLAGS \"-DBCAST_MIN_DATATYPES_ONLY\" LINK_FLAGS \"-DBCAST_MIN_DATATYPES_ONLY\")\n  set_target_properties(bcast_comm_world PROPERTIES COMPILE_FLAGS \"-DBCAST_COMM_WORLD_ONLY\" LINK_FLAGS \"-DBCAST_COMM_WORLD_ONLY\")\n  \n  add_executable(reduce_mpich EXCLUDE_FROM_ALL reduce.c)\n  add_dependencies(tests reduce_mpich)\n  target_link_libraries(reduce_mpich  simgrid mtest_c)\n  \n  # These tests take 5 to 15 seconds to run, so we don't want to run them several times.\n  # But at the same time, we'd like to check if they work for all factories and all privatization algorithm\n  # Thus the current matrix\n\n  MACRO(ADD_MPICH3_COLL SELECTOR FACTORY PRIVATIZATION)\n    set(NAME \"test-smpi-mpich3-coll-${SELECTOR}\")\n    set(ARGS \"-execarg=--cfg=smpi/coll-selector:${SELECTOR}\" ${ARGN})\n    if(NOT \"${PRIVATIZATION}\" STREQUAL \"\" AND HAVE_PRIVATIZATION)\n      set(NAME \"${NAME}-${PRIVATIZATION}\")\n      set(ARGS ${ARGS} \"-execarg=--cfg=smpi/privatization:${PRIVATIZATION}\")\n    endif()\n    string(TOUPPER \"HAVE_${FACTORY}_CONTEXTS\" HAVE_FACTORY)\n    if(NOT \"${FACTORY}\" STREQUAL \"\" AND ${HAVE_FACTORY})\n      set(NAME \"${NAME}-${FACTORY}\")\n      set(ARGS ${ARGS} \"-execarg=--cfg=contexts/factory:${FACTORY}\")\n    endif()\n    ADD_TEST(${NAME} ${CMAKE_COMMAND} -E chdir ${CMAKE_BINARY_DIR}/teshsuite/smpi/mpich3-test/coll ${PERL_EXECUTABLE} ${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/mpich3-test/runtests \"-wrapper=${TESH_WRAPPER}\" -mpiexec=${CMAKE_BINARY_DIR}/smpi_script/bin/smpirun -srcdir=${CMAKE_HOME_DIRECTORY}/teshsuite/smpi/mpich3-test/coll -tests=testlist ${ARGS})\n    SET_TESTS_PROPERTIES(${NAME} PROPERTIES PASS_REGULAR_EXPRESSION \"tests passed!\")\n  ENDMACRO()\n\n  # Test default selector; default factory; default privatization\n  ADD_MPICH3_COLL(default \"\" \"\")\n\n  # Test OMPI selector: thread factory, dlopen privatization\n  ADD_MPICH3_COLL(ompi \"thread\" \"dlopen\" -execarg=--cfg=smpi/bcast:binomial_tree)\n\n  # Test MPICH selector: boost factory, dlopen privatization\n  ADD_MPICH3_COLL(mpich \"boost\" \"dlopen\")\n\n  # Test MVAPICH2 selector: ucontext factory, mmap privatization\n  ADD_MPICH3_COLL(mvapich2 \"ucontext\" \"mmap\")\n\n  # Test IMPI selector: raw factory, mmap privatization\n  ADD_MPICH3_COLL(impi \"raw\" \"mmap\")\n\nendif()\n\nset(examples_src  ${examples_src}\n ${CMAKE_CURRENT_SOURCE_DIR}/allgather2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allgather_struct.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allgather3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allgatherv2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allgatherv3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allgatherv4.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allred2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allred3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allred4.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allred5.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allred6.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allred.c \n ${CMAKE_CURRENT_SOURCE_DIR}/allredmany.c \n ${CMAKE_CURRENT_SOURCE_DIR}/alltoall1.c \n ${CMAKE_CURRENT_SOURCE_DIR}/alltoallv0.c \n ${CMAKE_CURRENT_SOURCE_DIR}/alltoallv.c \n ${CMAKE_CURRENT_SOURCE_DIR}/alltoallw1.c \n ${CMAKE_CURRENT_SOURCE_DIR}/alltoallw2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/alltoallw_zeros.c \n ${CMAKE_CURRENT_SOURCE_DIR}/bcast.c \n ${CMAKE_CURRENT_SOURCE_DIR}/bcasttest.c \n ${CMAKE_CURRENT_SOURCE_DIR}/bcastzerotype.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll10.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll11.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll12.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll13.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll4.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll5.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll6.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll7.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll8.c \n ${CMAKE_CURRENT_SOURCE_DIR}/coll9.c \n ${CMAKE_CURRENT_SOURCE_DIR}/exscan2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/exscan.c \n ${CMAKE_CURRENT_SOURCE_DIR}/gather2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/gather_big.c \n ${CMAKE_CURRENT_SOURCE_DIR}/gather.c \n ${CMAKE_CURRENT_SOURCE_DIR}/iallred.c \n ${CMAKE_CURRENT_SOURCE_DIR}/ibarrier.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icallgather.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icallgatherv.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icallreduce.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icalltoall.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icalltoallv.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icalltoallw.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icbarrier.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icbcast.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icgather.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icgatherv.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icreduce.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icscatter.c \n ${CMAKE_CURRENT_SOURCE_DIR}/icscatterv.c \n ${CMAKE_CURRENT_SOURCE_DIR}/longuser.c \n ${CMAKE_CURRENT_SOURCE_DIR}/nonblocking2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/nonblocking3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/nonblocking.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opband.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opbor.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opbxor.c \n ${CMAKE_CURRENT_SOURCE_DIR}/op_commutative.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opland.c \n ${CMAKE_CURRENT_SOURCE_DIR}/oplor.c \n ${CMAKE_CURRENT_SOURCE_DIR}/oplxor.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opmax.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opmaxloc.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opmin.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opminloc.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opprod.c \n ${CMAKE_CURRENT_SOURCE_DIR}/opsum.c \n ${CMAKE_CURRENT_SOURCE_DIR}/red3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/red4.c \n ${CMAKE_CURRENT_SOURCE_DIR}/redscat2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/redscat3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/redscatbkinter.c \n ${CMAKE_CURRENT_SOURCE_DIR}/redscatblk3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/red_scat_block2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/red_scat_block.c \n ${CMAKE_CURRENT_SOURCE_DIR}/redscat.c \n ${CMAKE_CURRENT_SOURCE_DIR}/redscatinter.c \n ${CMAKE_CURRENT_SOURCE_DIR}/reduce.c \n ${CMAKE_CURRENT_SOURCE_DIR}/reduce_local.c \n ${CMAKE_CURRENT_SOURCE_DIR}/scantst.c \n ${CMAKE_CURRENT_SOURCE_DIR}/scatter2.c \n ${CMAKE_CURRENT_SOURCE_DIR}/scatter3.c \n ${CMAKE_CURRENT_SOURCE_DIR}/scattern.c \n ${CMAKE_CURRENT_SOURCE_DIR}/scatterv.c \n ${CMAKE_CURRENT_SOURCE_DIR}/uoplong.c \n  PARENT_SCOPE)\nset(txt_files  ${txt_files}  ${CMAKE_CURRENT_SOURCE_DIR}/testlist  PARENT_SCOPE)\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/tools/simgrid.supp": "# Valgrind suppressions for stuff that we cannot control\n\n# Memory leaks in standard tools (e.g. dash, tail, or sort)\n{\n   Memory leak in /bin tools\n   Memcheck:Leak\n   ...\n   obj:/bin/*\n}\n\n{\n   Memory leak in /usr/bin tools\n   Memcheck:Leak\n   ...\n   obj:/usr/bin/*\n}\n\n{\n   Memory leak in cmake\n   Memcheck:Leak\n   match-leak-kinds:reachable\n   ...\n   fun:_ZN4Json5Value13nullSingletonEv\n   obj:*/libjsoncpp.so*\n   ...\n   fun:_dl_init\n}\n\n# There's problem in glibc, where makecontext does not reset the EBP register,\n# and backtrace goes too far when walking up the stack frames\n{\n   Invalid read in backtrace, called after makecontext\n   Memcheck:Addr4\n   fun:backtrace\n   ...\n   fun:makecontext\n}\n\n#There seems to be an issue with libc using an uninitialized value somewhere in dlopen\n{\n   Invalid read in dl_start\n   Memcheck:Cond\n   fun:index\n   fun:expand_dynamic_string_token\n   ...\n   fun:_dl_start\n}\n\n# There are memory leaks when using dlopen\n{\n   Memory leak in libc/dlopen\n   Memcheck:Leak\n   ...\n   fun:_dlerror_run\n}\n\n# 72704 bytes leak from GCC >5.1 https://gcc.gnu.org/bugzilla/show_bug.cgi?id=64535\n{\n   Memory leak in dl_init\n   Memcheck:Leak\n   match-leak-kinds:reachable\n   fun:malloc\n   obj:/usr/lib/*/libstdc++.so.*\n   fun:call_init.part.0\n   ...\n   fun:_dl_init\n}\n\n#Ignore leaks in SMPI sample codes\n{\n   Leaks in SMPI sample codes\n   Memcheck:Leak\n   match-leak-kinds: all\n   fun:malloc\n   fun:smpi_simulated_main_\n}\n\n# Memory leaks appearing to be in libcgraph.  They can be seen with the\n# following simple program:\n# ,----\n# | #include <stdio.h>\n# | #include <graphviz/cgraph.h>\n# | int main(int argc, char *argv[])\n# | {\n# |     if (argc == 1) {\n# |         printf(\"Usage: %s <dotfile>\\n\", argv[0]);\n# |         return 1;\n# |     }\n# |     Agraph_t *g;\n# |     FILE *inf = fopen(argv[1], \"r\");\n# |     g = agread(inf, 0);\n# |     fclose(inf);\n# |     agclose(g);\n# |     return 0;\n# | }\n# `----\n{\n   Memory leak in libcgraph (1/3)\n   Memcheck:Leak\n   fun:malloc\n   ...\n   fun:aaglex\n   fun:aagparse\n   fun:agconcat\n}\n{\n   Memory leak in libcgraph (2/3)\n   Memcheck:Leak\n   fun:calloc\n   ...\n   obj:*/libcgraph.so*\n   fun:aagparse\n   fun:agconcat\n}\n{\n   Memory leak in libcgraph (3/3)\n   Memcheck:Leak\n   fun:malloc\n   ...\n   fun:agnode\n   obj:*/libcgraph.so*\n   fun:aagparse\n   fun:agconcat\n}\n\n# We're not interested by memory leaks in the Lua interpreter\n{\n   Memory leak in lua\n   Memcheck:Leak\n   ...\n   fun:luaD_precall\n}\n\n# libunwind seems to be using msync poorly, thus triggering these\n# https://github.com/JuliaLang/julia/issues/4533\n{\n   msync unwind\n   Memcheck:Param\n   msync(start)\n   ...\n   obj:*/libpthread*.so\n   ...\n}\n\n{\n   ignore unwind cruft\n   Memcheck:Param\n   rt_sigprocmask(set)\n   ...\n   obj:/usr/lib/x86_64-linux-gnu/libunwind.so.*\n   ...\n}\n{\n   ignore unwind cruft\n   Memcheck:Param\n   msync(start)\n   ...\n   obj:/usr/lib/x86_64-linux-gnu/libunwind.so.*\n   ...\n}\n{\n   ignore unwind cruft\n   Memcheck:Param\n   write(buf)\n   ...\n   fun:_ULx86_64_step\n   obj:/usr/lib/x86_64-linux-gnu/libunwind.so.*\n}\n\n{\n   ignore unwind invalid reads\n   Memcheck:Addr8\n   fun:_Ux86_64_setcontext\n}\n\n# Java cruft\n{\n  JavaCruft 1\n  Memcheck:Addr4\n  ...\n  fun:_ZN9JavaCalls11call_helperEP9JavaValueP12methodHandleP17JavaCallArgumentsP6Thread\n  fun:JVM_DoPrivileged\n  ...\n}\n{\n   JavaCruft 2\n   Memcheck:Cond\n   ...\n   fun:_ZN13CompileBroker25invoke_compiler_on_methodEP11CompileTask\n   ...\n}\n\n{\n   Somewhere within the Java conditions and monitors\n   Memcheck:Cond\n   fun:MarsagliaXORV\n   ...\n}\n\n#ignore python cruft\n{\n   ignore python cruft 1\n   Memcheck:Cond\n   ...\n   obj:/usr/bin/python*\n}\n\n{\n   ignore python cruft 2\n   Memcheck:Addr4\n   ...\n   obj:/usr/bin/python*\n}\n\n{\n   ignore python cruft 3\n   Memcheck:Value8\n   ...\n   obj:/usr/bin/python*\n}\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/tools/tesh/tesh.py": "#! @PYTHON_EXECUTABLE@\n# -*- coding: utf-8 -*-\n\"\"\"\n\ntesh -- testing shell\n========================\n\nCopyright (c) 2012-2020. The SimGrid Team. All rights reserved.\n\nThis program is free software; you can redistribute it and/or modify it\nunder the terms of the license (GNU LGPL) which comes with this package.\n\n#TODO: child of child of child that printfs. Does it work?\n#TODO: a child dies after its parent. What happen?\n\n#TODO: regular expression in output\n#ex: >> Time taken: [0-9]+s\n#TODO: linked regular expression in output\n#ex:\n# >> Bytes sent: ([0-9]+)\n# >> Bytes recv: \\1\n# then, even better:\n# ! expect (\\1 > 500)\n\n\"\"\"\n\nimport sys\nimport os\nimport shlex\nimport re\nimport difflib\nimport signal\nimport argparse\nimport time\n\nif sys.version_info[0] == 3:\n    import subprocess\n    import _thread\nelse:\n    raise \"This program is expected to run with Python3 only\"\n\n##############\n#\n# Utilities\n#\n#\n\ndef isWindows():\n    return sys.platform.startswith('win')\n\n# Singleton metaclass that works in Python 2 & 3\n# http://stackoverflow.com/questions/6760685/creating-a-singleton-in-python\n\nclass _Singleton(type):\n    \"\"\" A metaclass that creates a Singleton base class when called. \"\"\"\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super(_Singleton, cls).__call__(*args, **kwargs)\n        return cls._instances[cls]\n\nclass Singleton(_Singleton('SingletonMeta', (object,), {})):\n    pass\n\nSIGNALS_TO_NAMES_DICT = dict((getattr(signal, n), n)\n                             for n in dir(signal) if n.startswith('SIG') and '_' not in n)\n\nreturn_code = 0\n\n# exit correctly\ndef tesh_exit(errcode):\n    # If you do not flush some prints are skipped\n    sys.stdout.flush()\n    # os._exit exit even when executed within a thread\n    os._exit(errcode)\n\n\ndef fatal_error(msg):\n    print(\"[Tesh/CRITICAL] \" + str(msg))\n    tesh_exit(1)\n\n\n# Set an environment variable.\n# arg must be a string with the format \"variable=value\"\ndef setenv(arg):\n    print(\"[Tesh/INFO] setenv \" + arg)\n    t = arg.split(\"=\", 1)\n    os.environ[t[0]] = t[1]\n    # os.putenv(t[0], t[1]) does not work\n    # see http://stackoverflow.com/questions/17705419/python-os-environ-os-putenv-usr-bin-env\n\n\n# http://stackoverflow.com/questions/30734967/how-to-expand-environment-variables-in-python-as-bash-does\ndef expandvars2(path):\n    return re.sub(r'(?<!\\\\)\\$[A-Za-z_][A-Za-z0-9_]*', '', os.path.expandvars(path))\n\n\n# https://github.com/Cadair/jupyter_environment_kernels/issues/10\ntry:\n    FileNotFoundError\nexcept NameError:\n    # py2\n    FileNotFoundError = OSError\n\n##############\n#\n# Cleanup on signal\n#\n#\n\n# Global variable. Stores which process group should be killed (or None otherwise)\nrunning_pids = list()\n\n# Tests whether the process is dead already\ndef process_is_dead(pid):\n    try:\n        os.kill(pid, 0)\n    except ProcessLookupError:\n        return True\n    except OSError as err:\n        if err.errno == errno.ESRCH: # ESRCH == No such process. The process is now dead\n            return True\n    return False\n\n# This function send TERM signal + KILL signal after 0.2s to the group of the specified process\ndef kill_process_group(pid):\n    if pid is None:  # Nobody to kill. We don't know who to kill on windows, or we don't have anyone to kill on signal handler\n        return\n\n    try:\n        pgid = os.getpgid(pid)\n    except:\n        # os.getpgid failed. Ok, don't cleanup.\n        return\n    \n    try:\n        os.killpg(pgid, signal.SIGTERM)\n        if process_is_dead(pid):\n            return\n        time.sleep(0.2)\n        os.killpg(pgid, signal.SIGKILL)\n    except OSError:\n        # os.killpg failed. OK. Some subprocesses may still be running.\n        pass\n\ndef signal_handler(signal, frame):\n    print(\"Caught signal {}\".format(SIGNALS_TO_NAMES_DICT[signal]))\n    global running_pids\n    running_pids_copy = running_pids # Just in case of interthread conflicts.\n    for pid in running_pids_copy:\n        kill_process_group(pid)\n    running_pids.clear()\n    tesh_exit(5)\n\n\n##############\n#\n# Classes\n#\n#\n\n\n# read file line per line (and concat line that ends with \"\\\")\nclass FileReader(Singleton):\n    def __init__(self, filename=None):\n        if filename is None:\n            self.filename = \"(stdin)\"\n            self.f = sys.stdin\n        else:\n            self.filename_raw = filename\n            self.filename = os.path.basename(filename)\n            self.abspath = os.path.abspath(filename)\n            self.f = open(self.filename_raw)\n\n        self.linenumber = 0\n\n    def __repr__(self):\n        return self.filename + \":\" + str(self.linenumber)\n\n    def readfullline(self):\n        try:\n            line = next(self.f)\n            self.linenumber += 1\n        except StopIteration:\n            return None\n        if line[-1] == \"\\n\":\n            txt = line[0:-1]\n        else:\n            txt = line\n        while len(line) > 1 and line[-2] == \"\\\\\":\n            txt = txt[0:-1]\n            line = next(self.f)\n            self.linenumber += 1\n            txt += line[0:-1]\n        return txt\n\n\n# keep the state of tesh (mostly configuration values)\nclass TeshState(Singleton):\n    def __init__(self):\n        self.threads = []\n        self.args_suffix = \"\"\n        self.ignore_regexps_common = []\n        self.jenkins = False  # not a Jenkins run by default\n        self.timeout = 10  # default value: 10 sec\n        self.wrapper = None\n        self.keep = False\n\n    def add_thread(self, thread):\n        self.threads.append(thread)\n\n    def join_all_threads(self):\n        for t in self.threads:\n            t.acquire()\n            t.release()\n\n# Command line object\n\n\nclass Cmd(object):\n    def __init__(self):\n        self.input_pipe = []\n        self.output_pipe_stdout = []\n        self.output_pipe_stderr = []\n        self.timeout = TeshState().timeout\n        self.args = None\n        self.linenumber = -1\n\n        self.background = False\n        # Python threads loose the cwd\n        self.cwd = os.getcwd()\n\n        self.ignore_output = False\n        self.expect_return = [0]\n\n        self.output_display = False\n\n        self.sort = -1\n\n        self.ignore_regexps = TeshState().ignore_regexps_common\n\n    def add_input_pipe(self, l):\n        self.input_pipe.append(l)\n\n    def add_output_pipe_stdout(self, l):\n        self.output_pipe_stdout.append(l)\n\n    def add_output_pipe_stderr(self, l):\n        self.output_pipe_stderr.append(l)\n\n    def set_cmd(self, args, linenumber):\n        self.args = args\n        self.linenumber = linenumber\n\n    def add_ignore(self, txt):\n        self.ignore_regexps.append(re.compile(txt))\n\n    def remove_ignored_lines(self, lines):\n        for ign in self.ignore_regexps:\n            lines = [l for l in lines if not ign.match(l)]\n        return lines\n\n    def _cmd_mkfile(self, argline):\n        filename = argline[len(\"mkfile \"):]\n        file = open(filename, \"w\")\n        if file is None:\n            fatal_error(\"Unable to create file \" + filename)\n        file.write(\"\\n\".join(self.input_pipe))\n        file.write(\"\\n\")\n        file.close()\n\n    def _cmd_cd(self, argline):\n        args = shlex.split(argline)\n        if len(args) != 2:\n            fatal_error(\"Too many arguments to cd\")\n        try:\n            os.chdir(args[1])\n            print(\"[Tesh/INFO] change directory to \" + args[1])\n        except FileNotFoundError:\n            print(\"Chdir to \" + args[1] + \" failed: No such file or directory\")\n            print(\"Test suite `\" + FileReader().filename + \"': NOK (system error)\")\n            tesh_exit(4)\n\n    # Run the Cmd if possible.\n    # Return False if nothing has been ran.\n\n    def run_if_possible(self):\n        if self.can_run():\n            if self.background:\n                lock = _thread.allocate_lock()\n                lock.acquire()\n                TeshState().add_thread(lock)\n                _thread.start_new_thread(Cmd._run, (self, lock))\n            else:\n                self._run()\n            return True\n        else:\n            return False\n\n    def _run(self, lock=None):\n        # Python threads loose the cwd\n        os.chdir(self.cwd)\n\n        # retrocompatibility: support ${aaa:=.} variable format\n        def replace_perl_variables(m):\n            vname = m.group(1)\n            vdefault = m.group(2)\n            if vname in os.environ:\n                return \"$\" + vname\n            return vdefault\n\n        self.args = re.sub(r\"\\${(\\w+):=([^}]*)}\", replace_perl_variables, self.args)\n\n        # replace bash environment variables ($THINGS) to their values\n        self.args = expandvars2(self.args)\n\n        if re.match(\"^mkfile \", self.args) is not None:\n            self._cmd_mkfile(self.args)\n            if lock is not None:\n                lock.release()\n            return\n\n        if re.match(\"^cd \", self.args) is not None:\n            self._cmd_cd(self.args)\n            if lock is not None:\n                lock.release()\n            return\n\n        if TeshState().wrapper is not None:\n            self.timeout *= 20\n            self.args = TeshState().wrapper + self.args\n        elif re.match(\".*smpirun.*\", self.args) is not None:\n            self.args = \"sh \" + self.args\n        if TeshState().jenkins and self.timeout is not None:\n            self.timeout *= 10\n\n        self.args += TeshState().args_suffix\n\n        logs = list()\n        logs.append(\"[{file}:{number}] {args}\".format(file=FileReader().filename,\n            number=self.linenumber, args=self.args))\n\n        args = shlex.split(self.args)\n\n        global running_pids\n        local_pid = None\n        global return_code\n\n        try:\n            preexec_function = None\n            if not isWindows():\n                preexec_function = lambda: os.setpgid(0, 0)\n            proc = subprocess.Popen(\n                args,\n                bufsize=1,\n                stdin=subprocess.PIPE,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                preexec_fn=preexec_function)\n            if not isWindows():\n                local_pid = proc.pid\n                running_pids.append(local_pid)\n        except PermissionError:\n            logs.append(\"[{file}:{number}] Cannot start '{cmd}': The binary is not executable.\".format(\n                file=FileReader().filename, number=self.linenumber, cmd=args[0]))\n            logs.append(\"[{file}:{number}] Current dir: {dir}\".format(file=FileReader().filename,\n                number=self.linenumber, dir=os.getcwd()))\n            return_code = max(3, return_code)\n            print('\\n'.join(logs))\n            return\n        except NotADirectoryError:\n            logs.append(\"[{file}:{number}] Cannot start '{cmd}': The path to binary does not exist.\".format(\n                file=FileReader().filename, number=self.linenumber, cmd=args[0]))\n            logs.append(\"[{file}:{number}] Current dir: {dir}\".format(file=FileReader().filename,\n                number=self.linenumber, dir=os.getcwd()))\n            return_code = max(3, return_code)\n            print('\\n'.join(logs))\n            return\n        except FileNotFoundError:\n            logs.append(\"[{file}:{number}] Cannot start '{cmd}': File not found.\".format(\n                file=FileReader().filename, number=self.linenumber, cmd=args[0]))\n            return_code = max(3, return_code)\n            print('\\n'.join(logs))\n            return\n        except OSError as osE:\n            if osE.errno == 8:\n                osE.strerror += \"\\nOSError: [Errno 8] Executed scripts should start with shebang line (like #!/usr/bin/env sh)\"\n            raise osE\n\n        cmdName = FileReader().filename + \":\" + str(self.linenumber)\n        try:\n            (stdout_data, stderr_data) = proc.communicate(\"\\n\".join(self.input_pipe), self.timeout)\n            local_pid = None\n            timeout_reached = False\n        except subprocess.TimeoutExpired:\n            timeout_reached = True\n            logs.append(\"Test suite `{file}': NOK (<{cmd}> timeout after {timeout} sec)\".format(\n                file=FileReader().filename, cmd=cmdName, timeout=self.timeout))\n            running_pids.remove(local_pid)\n            kill_process_group(local_pid)\n            # Try to get the output of the timeout process, to help in debugging.\n            try:\n                (stdout_data, stderr_data) = proc.communicate(timeout=1)\n            except subprocess.TimeoutExpired:\n                logs.append(\"[{file}:{number}] Could not retrieve output. Killing the process group failed?\".format(\n                    file=FileReader().filename, number=self.linenumber))\n                return_code = max(3, return_code)\n                print('\\n'.join(logs))\n                return\n\n        if self.output_display:\n            logs.append(str(stdout_data))\n\n        # remove text colors\n        ansi_escape = re.compile(r'\\x1b[^m]*m')\n        stdout_data = ansi_escape.sub('', stdout_data)\n\n        if self.ignore_output:\n            logs.append(\"(ignoring the output of <{cmd}> as requested)\".format(cmd=cmdName))\n        else:\n            stdouta = stdout_data.split(\"\\n\")\n            while stdouta and stdouta[-1] == \"\":\n                del stdouta[-1]\n            stdouta = self.remove_ignored_lines(stdouta)\n            stdcpy = stdouta[:]\n\n            # Mimic the \"sort\" bash command, which is case unsensitive.\n            if self.sort == 0:\n                stdouta.sort(key=lambda x: x.lower())\n                self.output_pipe_stdout.sort(key=lambda x: x.lower())\n            elif self.sort > 0:\n                stdouta.sort(key=lambda x: x[:self.sort].lower())\n                self.output_pipe_stdout.sort(key=lambda x: x[:self.sort].lower())\n\n            diff = list(\n                difflib.unified_diff(\n                    self.output_pipe_stdout,\n                    stdouta,\n                    lineterm=\"\",\n                    fromfile='expected',\n                    tofile='obtained'))\n            if diff:\n                logs.append(\"Output of <{cmd}> mismatch:\".format(cmd=cmdName))\n                if self.sort >= 0:  # If sorted, truncate the diff output and show the unsorted version\n                    difflen = 0\n                    for line in diff:\n                        if difflen < 50:\n                            print(line)\n                        difflen += 1\n                    if difflen > 50:\n                        logs.append(\"(diff truncated after 50 lines)\")\n                    logs.append(\"Unsorted observed output:\\n\")\n                    for line in stdcpy:\n                        logs.append(line)\n                else:  # If not sorted, just display the diff\n                    for line in diff:\n                        logs.append(line)\n\n                logs.append(\"Test suite `{file}': NOK (<{cmd}> output mismatch)\".format(\n                    file=FileReader().filename, cmd=cmdName))\n                if lock is not None:\n                    lock.release()\n                if TeshState().keep:\n                    f = open('obtained', 'w')\n                    obtained = stdout_data.split(\"\\n\")\n                    while obtained and obtained[-1] == \"\":\n                        del obtained[-1]\n                    obtained = self.remove_ignored_lines(obtained)\n                    for line in obtained:\n                        f.write(\"> \" + line + \"\\n\")\n                    f.close()\n                    logs.append(\"Obtained output kept as requested: {path}\".format(path=os.path.abspath(\"obtained\")))\n                return_code = max(2, return_code)\n                print('\\n'.join(logs))\n                return\n\n        if timeout_reached:\n            return_code = max(3, return_code)\n            print('\\n'.join(logs))\n            return\n\n        if not proc.returncode in self.expect_return:\n            if proc.returncode >= 0:\n                logs.append(\"Test suite `{file}': NOK (<{cmd}> returned code {code})\".format(\n                    file=FileReader().filename, cmd=cmdName, code=proc.returncode))\n                if lock is not None:\n                    lock.release()\n                return_code = max(2, return_code)\n                print('\\n'.join(logs))\n                return\n            else:\n                logs.append(\"Test suite `{file}': NOK (<{cmd}> got signal {sig})\".format(\n                    file=FileReader().filename, cmd=cmdName,\n                    sig=SIGNALS_TO_NAMES_DICT[-proc.returncode]))\n                if lock is not None:\n                    lock.release()\n                return_code = max(max(-proc.returncode, 1), return_code)\n                print('\\n'.join(logs))\n                return\n\n        if lock is not None:\n            lock.release()\n\n        print('\\n'.join(logs))\n\n    def can_run(self):\n        return self.args is not None\n\n##############\n#\n# Main\n#\n#\n\nif __name__ == '__main__':\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    parser = argparse.ArgumentParser(description='tesh -- testing shell')\n    group1 = parser.add_argument_group('Options')\n    group1.add_argument('teshfile', nargs='?', help='Name of teshfile, stdin if omitted')\n    group1.add_argument(\n        '--cd',\n        metavar='some/directory',\n        help='ask tesh to switch the working directory before launching the tests')\n    group1.add_argument('--setenv', metavar='var=value', action='append', help='set a specific environment variable')\n    group1.add_argument('--cfg', metavar='arg', action='append', help='add parameter --cfg=arg to each command line')\n    group1.add_argument('--log', metavar='arg', action='append', help='add parameter --log=arg to each command line')\n    group1.add_argument(\n        '--ignore-jenkins',\n        action='store_true',\n        help='ignore all cruft generated on SimGrid continuous integration servers')\n    group1.add_argument('--wrapper', metavar='arg', help='Run each command in the provided wrapper (eg valgrind)')\n    group1.add_argument(\n        '--keep',\n        action='store_true',\n        help='Keep the obtained output when it does not match the expected one')\n\n    options = parser.parse_args()\n\n    if options.cd is not None:\n        print(\"[Tesh/INFO] change directory to \" + options.cd)\n        os.chdir(options.cd)\n\n    if options.ignore_jenkins:\n        print(\"Ignore all cruft seen on SimGrid's continuous integration servers\")\n        # Note: regexps should match at the beginning of lines\n        TeshState().ignore_regexps_common = [\n            re.compile(r\"profiling:\"),\n            re.compile(r\"Unable to clean temporary file C:\"),\n            re.compile(r\".*Configuration change: Set 'contexts/\"),\n            re.compile(r\"Picked up JAVA_TOOL_OPTIONS: \"),\n            re.compile(r\"Picked up _JAVA_OPTIONS: \"),\n            re.compile(r\"==[0-9]+== ?WARNING: ASan doesn't fully support\"),\n            re.compile(r\"==[0-9]+== ?WARNING: ASan is ignoring requested __asan_handle_no_return: stack top:\"),\n            re.compile(r\"False positive error reports may follow\"),\n            re.compile(r\"For details see http://code\\.google\\.com/p/address-sanitizer/issues/detail\\?id=189\"),\n            re.compile(r\"For details see https://github\\.com/google/sanitizers/issues/189\"),\n            re.compile(r\"Python runtime initialized with LC_CTYPE=C .*\"),\n            # Seen on CircleCI\n            re.compile(r\"cmake: /usr/local/lib/libcurl\\.so\\.4: no version information available \\(required by cmake\\)\"),\n            re.compile(r\".*mmap broken on FreeBSD, but dlopen\\+thread broken too\\. Switching to dlopen\\+raw contexts\\.\"),\n            re.compile(r\".*dlopen\\+thread broken on Apple and BSD\\. Switching to raw contexts\\.\"),\n        ]\n        TeshState().jenkins = True  # This is a Jenkins build\n\n    if options.teshfile is None:\n        f = FileReader(None)\n        print(\"Test suite from stdin\")\n    else:\n        if not os.path.isfile(options.teshfile):\n            print(\"Cannot open teshfile '\" + options.teshfile + \"': File not found\")\n            tesh_exit(3)\n        f = FileReader(options.teshfile)\n        print(\"Test suite '\" + f.abspath + \"'\")\n\n    if options.setenv is not None:\n        for e in options.setenv:\n            setenv(e)\n\n    if options.cfg is not None:\n        for c in options.cfg:\n            TeshState().args_suffix += \" --cfg=\" + c\n    if options.log is not None:\n        for l in options.log:\n            TeshState().args_suffix += \" --log=\" + l\n\n    if options.wrapper is not None:\n        TeshState().wrapper = options.wrapper\n\n    if options.keep:\n        TeshState().keep = True\n\n    # cmd holds the current command line\n    # tech commands will add some parameters to it\n    # when ready, we execute it.\n    cmd = Cmd()\n\n    line = f.readfullline()\n    while line is not None:\n        # print(\">>=============\"+line+\"==<<\")\n        if not line:\n            #print (\"END CMD block\")\n            if cmd.run_if_possible():\n                cmd = Cmd()\n\n        elif line[0] == \"#\":\n            pass\n\n        elif line[0:2] == \"p \":\n            print(\"[\" + str(FileReader()) + \"] \" + line[2:])\n\n        elif line[0:2] == \"< \":\n            cmd.add_input_pipe(line[2:])\n        elif line[0:1] == \"<\":\n            cmd.add_input_pipe(line[1:])\n\n        elif line[0:2] == \"> \":\n            cmd.add_output_pipe_stdout(line[2:])\n        elif line[0:1] == \">\":\n            cmd.add_output_pipe_stdout(line[1:])\n\n        elif line[0:2] == \"$ \":\n            if cmd.run_if_possible():\n                cmd = Cmd()\n            cmd.set_cmd(line[2:], f.linenumber)\n\n        elif line[0:2] == \"& \":\n            if cmd.run_if_possible():\n                cmd = Cmd()\n            cmd.set_cmd(line[2:], f.linenumber)\n            cmd.background = True\n\n        elif line[0:15] == \"! output ignore\":\n            cmd.ignore_output = True\n            #print(\"cmd.ignore_output = True\")\n        elif line[0:16] == \"! output display\":\n            cmd.output_display = True\n            cmd.ignore_output = True\n        elif line[0:15] == \"! expect return\":\n            cmd.expect_return = [int(line[16:])]\n            #print(\"expect return \"+str(int(line[16:])))\n        elif line[0:15] == \"! expect signal\":\n            cmd.expect_return = []\n            for sig in (line[16:]).split(\"|\"):\n                # get the signal integer value from the signal module\n                if sig not in signal.__dict__:\n                    fatal_error(\"unrecognized signal '\" + sig + \"'\")\n                sig = int(signal.__dict__[sig])\n                # popen return -signal when a process ends with a signal\n                cmd.expect_return.append(-sig)\n        elif line[0:len(\"! timeout \")] == \"! timeout \":\n            if \"no\" in line[len(\"! timeout \"):]:\n                cmd.timeout = None\n            else:\n                cmd.timeout = int(line[len(\"! timeout \"):])\n\n        elif line[0:len(\"! output sort\")] == \"! output sort\":\n            if len(line) >= len(\"! output sort \"):\n                sort = int(line[len(\"! output sort \"):])\n            else:\n                sort = 0\n            cmd.sort = sort\n        elif line[0:len(\"! setenv \")] == \"! setenv \":\n            setenv(line[len(\"! setenv \"):])\n\n        elif line[0:len(\"! ignore \")] == \"! ignore \":\n            cmd.add_ignore(line[len(\"! ignore \"):])\n\n        else:\n            fatal_error(\"UNRECOGNIZED OPTION\")\n\n        line = f.readfullline()\n\n    cmd.run_if_possible()\n\n    TeshState().join_all_threads()\n\n    if return_code == 0:\n        if f.filename == \"(stdin)\":\n            print(\"Test suite from stdin OK\")\n        else:\n            print(\"Test suite `\" + f.filename + \"' OK\")\n    else:\n        tesh_exit(return_code)\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/tools/cmake/MakeLib.cmake": "### Make Libs\n\n# On macOS, specify that rpath is useful to look for the dependencies\n# See https://gitlab.kitware.com/cmake/community/wikis/doc/cmake/RPATH-handling and Java.cmake\nset(CMAKE_MACOSX_RPATH TRUE)\nif(APPLE)\n  SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE) # When installed, use system path\n  set(CMAKE_SKIP_BUILD_RPATH FALSE)         # When executing from build tree, take the lib from the build path if exists\n  set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE) # When executing from build tree, take the lib from the system path if exists\n\n  # add the current location of libsimgrid-java.dynlib as a location for libsimgrid.dynlib\n  # (useful when unpacking the native libraries from the jarfile)\n  set(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_FULL_LIBDIR}\")\nendif()\n\n###############################\n# Declare the library content #\n###############################\n\n# Actually declare our libraries\nadd_library(simgrid SHARED ${simgrid_sources})\nset_target_properties(simgrid PROPERTIES VERSION ${libsimgrid_version})\n# The library can obviously use the internal headers\nset_property(TARGET simgrid\n             APPEND PROPERTY INCLUDE_DIRECTORIES \"${INTERNAL_INCLUDES}\")\n\nadd_dependencies(simgrid maintainer_files)\n\nif(enable_model-checking)\n  add_executable(simgrid-mc ${MC_SIMGRID_MC_SRC})\n  target_link_libraries(simgrid-mc simgrid)\n  set_target_properties(simgrid-mc\n                        PROPERTIES RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/bin\")\n  set_property(TARGET simgrid-mc\n               APPEND PROPERTY INCLUDE_DIRECTORIES \"${INTERNAL_INCLUDES}\")\n  install(TARGETS simgrid-mc # install that binary without breaking the rpath on Mac\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}/)\n  add_dependencies(tests simgrid-mc)\nendif()\n\n\n# Compute the dependencies of SimGrid\n#####################################\n# search for dlopen\nif(\"${CMAKE_SYSTEM_NAME}\" MATCHES \"kFreeBSD|Linux|SunOS\")\n  find_library(DL_LIBRARY dl)\nendif()\nmark_as_advanced(DL_LIBRARY)\n\nif (HAVE_BOOST_CONTEXTS)\n  target_link_libraries(simgrid ${Boost_CONTEXT_LIBRARY})\nendif()\n\nif (HAVE_BOOST_STACKTRACE_BACKTRACE)\n  target_link_libraries(simgrid ${Boost_STACKTRACE_BACKTRACE_LIBRARY})\nendif()\n\nif (HAVE_BOOST_ADDR2LINE_BACKTRACE)\n  target_link_libraries(simgrid ${Boost_STACKTRACE_ADDR2LINE_LIBRARY})\nendif()\n\nif(CMAKE_USE_PTHREADS_INIT)\n  target_link_libraries(simgrid ${CMAKE_THREAD_LIBS_INIT})\nendif()\n\nif(SIMGRID_HAVE_LUA)\n  ADD_CUSTOM_TARGET(link_simgrid_lua ALL\n    DEPENDS \tsimgrid\n    )\n  SET(SIMGRID_DEP \"${SIMGRID_DEP} ${LUA_LIBRARY} ${DL_LIBRARY}\")\nendif()\n\nif(HAVE_PAPI)\n  SET(SIMGRID_DEP \"${SIMGRID_DEP} -lpapi\")\nendif()\n\nif(HAVE_GRAPHVIZ)\n  if(HAVE_CGRAPH_LIB)\n    SET(SIMGRID_DEP \"${SIMGRID_DEP} -lcgraph\")\n  else()\n    if(HAVE_AGRAPH_LIB)\n      SET(SIMGRID_DEP \"${SIMGRID_DEP} -lagraph -lcdt\")\n    endif()\n  endif()\nendif()\n\nif(SIMGRID_HAVE_MC AND NOT ${DL_LIBRARY} STREQUAL \"\")\n  SET(SIMGRID_DEP \"${SIMGRID_DEP} ${DL_LIBRARY}\")\nendif()\n\nif(HAVE_POSIX_GETTIME)\n  SET(SIMGRID_DEP \"${SIMGRID_DEP} -lrt\")\nendif()\n\nif(\"${CMAKE_SYSTEM_NAME}\" STREQUAL \"FreeBSD\")\n  set(SIMGRID_DEP \"${SIMGRID_DEP} -lprocstat\")\nendif()\n\n# Compute the dependencies of SMPI\n##################################\n\nif(enable_smpi)\n  if(NOT ${DL_LIBRARY} STREQUAL \"\")\n    set(SIMGRID_DEP \"${SIMGRID_DEP} ${DL_LIBRARY}\") # for privatization\n  endif()\n\n  add_executable(smpimain src/smpi/smpi_main.c)\n  target_link_libraries(smpimain simgrid)\n  set_target_properties(smpimain\n    PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib/simgrid)\n  install(TARGETS smpimain # install that binary without breaking the rpath on Mac\n    RUNTIME DESTINATION ${CMAKE_INSTALL_LIBDIR}/simgrid)\n  add_dependencies(tests smpimain)\n\n  add_executable(smpireplaymain src/smpi/smpi_replay_main.cpp)\n  target_compile_options(smpireplaymain PRIVATE -fpic)\n  target_link_libraries(smpireplaymain simgrid -fpic -shared)\n  set_target_properties(smpireplaymain\n    PROPERTIES RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib/simgrid)\n  install(TARGETS smpireplaymain # install that binary without breaking the rpath on Mac\n    RUNTIME DESTINATION ${CMAKE_INSTALL_LIBDIR}/simgrid)\n  add_dependencies(tests smpireplaymain)\n\n  if(SMPI_FORTRAN)\n    if(CMAKE_Fortran_COMPILER_ID MATCHES \"GNU\")\n      SET(SIMGRID_DEP \"${SIMGRID_DEP} -lgfortran\")\n    elseif(CMAKE_Fortran_COMPILER_ID MATCHES \"Intel\")\n      SET(SIMGRID_DEP \"${SIMGRID_DEP} -lifcore\")\n    elseif(CMAKE_Fortran_COMPILER_ID MATCHES \"PGI|Flang\")\n      SET(SIMGRID_DEP \"${SIMGRID_DEP} -lflang\")\n      if(\"${CMAKE_SYSTEM}\" MATCHES \"FreeBSD\")\n        set(SIMGRID_DEP \"${SIMGRID_DEP} -lexecinfo\")\n        if (\"${CMAKE_SYSTEM_VERSION}\" MATCHES \"12\")\n            set(SIMGRID_DEP \"${SIMGRID_DEP} -lpgmath\")\n        endif()\n        if (\"${CMAKE_SYSTEM_VERSION}\" MATCHES \"12\\.1\")\n            set(SIMGRID_DEP \"${SIMGRID_DEP} -lomp\")\n        endif()\n      endif()\n    endif()\n  endif()\n\nendif()\n\nif(enable_smpi AND APPLE)\n  set(SIMGRID_DEP \"${SIMGRID_DEP} -Wl,-U -Wl,_smpi_simulated_main\")\nendif()\n\n# See https://github.com/HewlettPackard/foedus_code/blob/master/foedus-core/cmake/FindGccAtomic.cmake\nFIND_LIBRARY(GCCLIBATOMIC_LIBRARY NAMES atomic atomic.so.1 libatomic.so.1\n  HINTS\n    $ENV{HOME}/local/lib64\n    $ENV{HOME}/local/lib\n    /usr/local/lib64\n    /usr/local/lib\n    /opt/local/lib64\n    /opt/local/lib\n    /usr/lib64\n    /usr/lib\n    /lib64\n    /lib\n)\n\n# Fix a FTBFS on armel, mips, mipsel and friends (Debian's #872881)\nif(CMAKE_COMPILER_IS_GNUCC AND GCCLIBATOMIC_LIBRARY)\n    set(SIMGRID_DEP   \"${SIMGRID_DEP}   -Wl,--as-needed -latomic -Wl,--no-as-needed\")\nendif()\nmark_as_advanced(GCCLIBATOMIC_LIBRARY)\n\nif(enable_model-checking AND (NOT LINKER_VERSION VERSION_LESS \"2.30\"))\n    set(SIMGRID_DEP   \"${SIMGRID_DEP}   -Wl,-znorelro -Wl,-znoseparate-code\")\nendif()\n\ntarget_link_libraries(simgrid \t${SIMGRID_DEP})\n\n# Dependencies from maintainer mode\n###################################\nif(enable_maintainer_mode)\n  add_dependencies(simgrid smpi_generated_headers_call_location_tracing)\nendif()\nif(enable_maintainer_mode AND PYTHON_EXE)\n  add_dependencies(simgrid simcalls_generated_src)\nendif()\nif(enable_maintainer_mode AND BISON_EXE AND LEX_EXE)\n  add_dependencies(simgrid automaton_generated_src)\nendif()\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/app_smpi.rst": ".. _SMPI_doc:\n\n===============================\nSMPI: Simulate MPI Applications\n===============================\n\n.. raw:: html\n\n   <object id=\"TOC\" data=\"graphical-toc.svg\" type=\"image/svg+xml\"></object>\n   <script>\n   window.onload=function() { // Wait for the SVG to be loaded before changing it\n     var elem=document.querySelector(\"#TOC\").contentDocument.getElementById(\"SMPIBox\")\n     elem.style=\"opacity:0.93999999;fill:#ff0000;fill-opacity:0.1\";\n     elem.style=\"opacity:0.93999999;fill:#ff0000;fill-opacity:0.1;stroke:#000000;stroke-width:0.35277778;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1\";\n   }\n   </script>\n   <br/>\n   <br/>\n\nSMPI enables the study of MPI application by emulating them on top of\nthe SimGrid simulator. This is particularly interesting to study\nexisting MPI applications within the comfort of the simulator.\n\nTo get started with SMPI, you should head to :ref:`the SMPI tutorial\n<usecase_smpi>`. You may also want to read the `SMPI reference\narticle <https://hal.inria.fr/hal-01415484>`_ or these `introductory\nslides <http://simgrid.org/tutorials/simgrid-smpi-101.pdf>`_.  If you\nare new to MPI, you should first take our online `SMPI CourseWare\n<https://simgrid.github.io/SMPI_CourseWare/>`_. It consists in several\nprojects that progressively introduce the MPI concepts. It proposes to\nuse SimGrid and SMPI to run the experiments, but the learning\nobjectives are centered on MPI itself.\n\nOur goal is to enable the study of **unmodified MPI applications**.\nSome constructs and features are still missing, but we can probably\nadd them on demand.  If you already used MPI before, SMPI should sound\nvery familiar to you: Use smpicc instead of mpicc, and smpirun instead\nof mpirun. The main difference is that smpirun takes a :ref:`simulated\nplatform <platform>` as an extra parameter.\n\nFor **further scalability**, you may modify your code to speed up your\nstudies or save memory space.  Maximal **simulation accuracy**\nrequires some specific care from you.\n\n.. _SMPI_online:\n\n-----------------\nUsing SMPI online\n-----------------\n\nIn this mode, your application is actually executed. Every computation\noccurs for real while every communication is simulated. In addition,\nthe executions are automatically benchmarked so that their timings can\nbe applied within the simulator.\n\nSMPI can also go offline by replaying a trace. :ref:`Trace replay\n<SMPI_offline>` is usually ways faster than online simulation (because\nthe computation are skipped), but it can only applied to applications\nwith constant execution and communication patterns (for the exact same\nreason).\n\n...................\nCompiling your Code\n...................\n\nIf your application is in C, then simply use ``smpicc`` as a\ncompiler just like you use mpicc with other MPI implementations. This\nscript still calls your default compiler (gcc, clang, ...) and adds\nthe right compilation flags along the way. If your application is in\nC++, Fortran 77 or Fortran 90, use respectively ``smpicxx``,\n``smpiff`` or ``smpif90``.\n\nIf you use cmake, set the variables ``MPI_C_COMPILER``, ``MPI_CXX_COMPILER`` and\n``MPI_Fortran_COMPILER`` to the full path of smpicc, smpicxx and smpiff (or\nsmpif90), respectively. Example:\n\n.. code-block:: shell\n\n   cmake -DMPI_C_COMPILER=/opt/simgrid/bin/smpicc -DMPI_CXX_COMPILER=/opt/simgrid/bin/smpicxx -DMPI_Fortran_COMPILER=/opt/simgrid/bin/smpiff .\n\n....................\nSimulating your Code\n....................\n\nUse the ``smpirun`` script as follows:\n\n.. code-block:: shell\n\n   smpirun -hostfile my_hostfile.txt -platform my_platform.xml ./program -blah\n\n- ``my_hostfile.txt`` is a classical MPI hostfile (that is, this file\n  lists the machines on which the processes must be dispatched, one\n  per line)\n- ``my_platform.xml`` is a classical SimGrid platform file. Of course,\n  the hosts of the hostfile must exist in the provided platform.\n- ``./program`` is the MPI program to simulate, that you compiled with ``smpicc``\n- ``-blah`` is a command-line parameter passed to this program.\n\n``smpirun`` accepts other parameters, such as ``-np`` if you don't\nwant to use all the hosts defined in the hostfile, ``-map`` to display\non which host each rank gets mapped of ``-trace`` to activate the\ntracing during the simulation. You can get the full list by running\n``smpirun -help``\n\nFinally, you can pass :ref:`any valid SimGrid parameter <options>` to your\nprogram. In particular, you can pass ``--cfg=network/model:ns-3`` to\nswitch to use :ref:`model_ns3`. These parameters should be placed after\nthe name of your binary on the command line.\n\n...............................\nDebugging your Code within SMPI\n...............................\n\nIf you want to explore the automatic platform and deployment files\nthat are generated by ``smpirun``, add ``-keep-temps`` to the command\nline.\n\nYou can also run your simulation within valgrind or gdb using the\nfollowing commands. Once in GDB, each MPI ranks will be represented as\na regular thread, and you can explore the state of each of them as\nusual.\n\n.. code-block:: shell\n\n   smpirun -wrapper valgrind ...other args...\n   smpirun -wrapper \"gdb --args\" --cfg=contexts/factory:thread ...other args...\n\n.. _SMPI_use_colls:\n\n................................\nSimulating Collective Operations\n................................\n\nMPI collective operations are crucial to the performance of MPI\napplications and must be carefully optimized according to many\nparameters. Every existing implementation provides several algorithms\nfor each collective operation, and selects by default the best suited\none, depending on the sizes sent, the number of nodes, the\ncommunicator, or the communication library being used.  These\ndecisions are based on empirical results and theoretical complexity\nestimation, and are very different between MPI implementations. In\nmost cases, the users can also manually tune the algorithm used for\neach collective operation.\n\nSMPI can simulate the behavior of several MPI implementations:\nOpenMPI, MPICH, `STAR-MPI <http://star-mpi.sourceforge.net/>`_, and\nMVAPICH2. For that, it provides 115 collective algorithms and several\nselector algorithms, that were collected directly in the source code\nof the targeted MPI implementations.\n\nYou can switch the automatic selector through the\n``smpi/coll-selector`` configuration item. Possible values:\n\n - **ompi:** default selection logic of OpenMPI (version 3.1.2)\n - **mpich**: default selection logic of MPICH (version 3.3b)\n - **mvapich2**: selection logic of MVAPICH2 (version 1.9) tuned\n   on the Stampede cluster\n - **impi**: preliminary version of an Intel MPI selector (version\n   4.1.3, also tuned for the Stampede cluster). Due the closed source\n   nature of Intel MPI, some of the algorithms described in the\n   documentation are not available, and are replaced by mvapich ones.\n - **default**: legacy algorithms used in the earlier days of\n   SimGrid. Do not use for serious perform performance studies.\n\n.. todo:: default should not even exist.\n\n....................\nAvailable Algorithms\n....................\n\nYou can also pick the algorithm used for each collective with the\ncorresponding configuration item. For example, to use the pairwise\nalltoall algorithm, one should add ``--cfg=smpi/alltoall:pair`` to the\nline. This will override the selector (if any) for this algorithm.  It\nmeans that the selected algorithm will be used\n\n.. Warning:: Some collective may require specific conditions to be\n   executed correctly (for instance having a communicator with a power\n   of two number of nodes only), which are currently not enforced by\n   Simgrid. Some crashes can be expected while trying these algorithms\n   with unusual sizes/parameters\n\nMPI_Alltoall\n^^^^^^^^^^^^\n\nMost of these are best described in `STAR-MPI's white paper <https://doi.org/10.1145/1183401.1183431>`_.\n\n - default: naive one, by default\n - ompi: use openmpi selector for the alltoall operations\n - mpich: use mpich selector for the alltoall operations\n - mvapich2: use mvapich2 selector for the alltoall operations\n - impi: use intel mpi selector for the alltoall operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - bruck: Described by Bruck et.al. in `this paper <http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=642949>`_\n - 2dmesh: organizes the nodes as a two dimensional mesh, and perform allgather\n   along the dimensions\n - 3dmesh: adds a third dimension to the previous algorithm\n - rdb: recursive doubling: extends the mesh to a nth dimension, each one\n   containing two nodes\n - pair: pairwise exchange, only works for power of 2 procs, size-1 steps,\n   each process sends and receives from the same process at each step\n - pair_light_barrier: same, with small barriers between steps to avoid\n   contention\n - pair_mpi_barrier: same, with MPI_Barrier used\n - pair_one_barrier: only one barrier at the beginning\n - ring: size-1 steps, at each step a process send to process (n+i)%size, and receives from (n-i)%size\n - ring_light_barrier: same, with small barriers between some phases to avoid contention\n - ring_mpi_barrier: same, with MPI_Barrier used\n - ring_one_barrier: only one barrier at the beginning\n - basic_linear: posts all receives and all sends,\n   starts the communications, and waits for all communication to finish\n - mvapich2_scatter_dest: isend/irecv with scattered destinations, posting only a few messages at the same time\n\nMPI_Alltoallv\n^^^^^^^^^^^^^\n - default: naive one, by default\n - ompi: use openmpi selector for the alltoallv operations\n - mpich: use mpich selector for the alltoallv operations\n - mvapich2: use mvapich2 selector for the alltoallv operations\n - impi: use intel mpi selector for the alltoallv operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - bruck: same as alltoall\n - pair: same as alltoall\n - pair_light_barrier: same as alltoall\n - pair_mpi_barrier: same as alltoall\n - pair_one_barrier: same as alltoall\n - ring: same as alltoall\n - ring_light_barrier: same as alltoall\n - ring_mpi_barrier: same as alltoall\n - ring_one_barrier: same as alltoall\n - ompi_basic_linear: same as alltoall\n\nMPI_Gather\n^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the gather operations\n - mpich: use mpich selector for the gather operations\n - mvapich2: use mvapich2 selector for the gather operations\n - impi: use intel mpi selector for the gather operations\n - automatic (experimental): use an automatic self-benchmarking algorithm which will iterate over all implemented versions and output the best\n - ompi_basic_linear: basic linear algorithm from openmpi, each process sends to the root\n - ompi_binomial: binomial tree algorithm\n - ompi_linear_sync: same as basic linear, but with a synchronization at the\n   beginning and message cut into two segments.\n - mvapich2_two_level: SMP-aware version from MVAPICH. Gather first intra-node (defaults to mpich's gather), and then exchange with only one process/node. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster.\n\nMPI_Barrier\n^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the barrier operations\n - mpich: use mpich selector for the barrier operations\n - mvapich2: use mvapich2 selector for the barrier operations\n - impi: use intel mpi selector for the barrier operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - ompi_basic_linear: all processes send to root\n - ompi_two_procs: special case for two processes\n - ompi_bruck: nsteps = sqrt(size), at each step, exchange data with rank-2^k and rank+2^k\n - ompi_recursivedoubling: recursive doubling algorithm\n - ompi_tree: recursive doubling type algorithm, with tree structure\n - ompi_doublering: double ring algorithm\n - mvapich2_pair: pairwise algorithm\n - mpich_smp: barrier intra-node, then inter-node\n\nMPI_Scatter\n^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the scatter operations\n - mpich: use mpich selector for the scatter operations\n - mvapich2: use mvapich2 selector for the scatter operations\n - impi: use intel mpi selector for the scatter operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - ompi_basic_linear: basic linear scatter\n - ompi_binomial: binomial tree scatter\n - mvapich2_two_level_direct: SMP aware algorithm, with an intra-node stage (default set to mpich selector), and then a basic linear inter node stage. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster. \n - mvapich2_two_level_binomial: SMP aware algorithm, with an intra-node stage (default set to mpich selector), and then a binomial phase. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster.\n\nMPI_Reduce\n^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the reduce operations\n - mpich: use mpich selector for the reduce operations\n - mvapich2: use mvapich2 selector for the reduce operations\n - impi: use intel mpi selector for the reduce operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - arrival_pattern_aware: root exchanges with the first process to arrive\n - binomial: uses a binomial tree\n - flat_tree: uses a flat tree\n - NTSL: Non-topology-specific pipelined linear-bcast function\n   0->1, 1->2 ,2->3, ....., ->last node: in a pipeline fashion, with segments\n   of 8192 bytes\n - scatter_gather: scatter then gather\n - ompi_chain: openmpi reduce algorithms are built on the same basis, but the\n   topology is generated differently for each flavor\n   chain = chain with spacing of size/2, and segment size of 64KB\n - ompi_pipeline: same with pipeline (chain with spacing of 1), segment size\n   depends on the communicator size and the message size\n - ompi_binary: same with binary tree, segment size of 32KB\n - ompi_in_order_binary: same with binary tree, enforcing order on the\n   operations\n - ompi_binomial: same with binomial algo (redundant with default binomial\n   one in most cases)\n - ompi_basic_linear: basic algorithm, each process sends to root\n - mvapich2_knomial: k-nomial algorithm. Default factor is 4 (mvapich2 selector adapts it through tuning)\n - mvapich2_two_level: SMP-aware reduce, with default set to mpich both for intra and inter communicators. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster.\n - rab: `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_'s reduce algorithm\n\nMPI_Allreduce\n^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the allreduce operations\n - mpich: use mpich selector for the allreduce operations\n - mvapich2: use mvapich2 selector for the allreduce operations\n - impi: use intel mpi selector for the allreduce operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - lr: logical ring reduce-scatter then logical ring allgather\n - rab1: variations of the  `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_ algorithm: reduce_scatter then allgather\n - rab2: variations of the  `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_ algorithm: alltoall then allgather\n - rab_rsag: variation of the  `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_ algorithm: recursive doubling\n   reduce_scatter then recursive doubling allgather\n - rdb: recursive doubling\n - smp_binomial: binomial tree with smp: binomial intra\n   SMP reduce, inter reduce, inter broadcast then intra broadcast\n - smp_binomial_pipeline: same with segment size = 4096 bytes\n - smp_rdb: intra: binomial allreduce, inter: Recursive\n   doubling allreduce, intra: binomial broadcast\n - smp_rsag: intra: binomial allreduce, inter: reduce-scatter,\n   inter:allgather, intra: binomial broadcast\n - smp_rsag_lr: intra: binomial allreduce, inter: logical ring\n   reduce-scatter, logical ring inter:allgather, intra: binomial broadcast\n - smp_rsag_rab: intra: binomial allreduce, inter: rab\n   reduce-scatter, rab inter:allgather, intra: binomial broadcast\n - redbcast: reduce then broadcast, using default or tuned algorithms if specified\n - ompi_ring_segmented: ring algorithm used by OpenMPI\n - mvapich2_rs: rdb for small messages, reduce-scatter then allgather else\n - mvapich2_two_level: SMP-aware algorithm, with mpich as intra algorithm, and rdb as inter (Change this behavior by using mvapich2 selector to use tuned values)\n - rab: default `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_ implementation\n\nMPI_Reduce_scatter\n^^^^^^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the reduce_scatter operations\n - mpich: use mpich selector for the reduce_scatter operations\n - mvapich2: use mvapich2 selector for the reduce_scatter operations\n - impi: use intel mpi selector for the reduce_scatter operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - ompi_basic_recursivehalving: recursive halving version from OpenMPI\n - ompi_ring: ring version from OpenMPI\n - mpich_pair: pairwise exchange version from MPICH\n - mpich_rdb: recursive doubling version from MPICH\n - mpich_noncomm: only works for power of 2 procs, recursive doubling for noncommutative ops\n\n\nMPI_Allgather\n^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the allgather operations\n - mpich: use mpich selector for the allgather operations\n - mvapich2: use mvapich2 selector for the allgather operations\n - impi: use intel mpi selector for the allgather operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - 2dmesh: see alltoall\n - 3dmesh: see alltoall\n - bruck: Described by Bruck et.al. in <a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=642949\">\n   Efficient algorithms for all-to-all communications in multiport message-passing systems</a>\n - GB: Gather - Broadcast (uses tuned version if specified)\n - loosely_lr: Logical Ring with grouping by core (hardcoded, default\n   processes/node: 4)\n - NTSLR: Non Topology Specific Logical Ring\n - NTSLR_NB: Non Topology Specific Logical Ring, Non Blocking operations\n - pair: see alltoall\n - rdb: see alltoall\n - rhv: only power of 2 number of processes\n - ring: see alltoall\n - SMP_NTS: gather to root of each SMP, then every root of each SMP node\n   post INTER-SMP Sendrecv, then do INTRA-SMP Bcast for each receiving message,\n   using logical ring algorithm (hardcoded, default processes/SMP: 8)\n - smp_simple: gather to root of each SMP, then every root of each SMP node\n   post INTER-SMP Sendrecv, then do INTRA-SMP Bcast for each receiving message,\n   using simple algorithm (hardcoded, default processes/SMP: 8)\n - spreading_simple: from node i, order of communications is i -> i + 1, i ->\n   i + 2, ..., i -> (i + p -1) % P\n - ompi_neighborexchange: Neighbor Exchange algorithm for allgather.\n   Described by Chen et.al. in  `Performance Evaluation of Allgather\n   Algorithms on Terascale Linux Cluster with Fast Ethernet <http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1592302>`_\n - mvapich2_smp: SMP aware algorithm, performing intra-node gather, inter-node allgather with one process/node, and bcast intra-node\n\nMPI_Allgatherv\n^^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the allgatherv operations\n - mpich: use mpich selector for the allgatherv operations\n - mvapich2: use mvapich2 selector for the allgatherv operations\n - impi: use intel mpi selector for the allgatherv operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - GB: Gatherv - Broadcast (uses tuned version if specified, but only for Bcast, gatherv is not tuned)\n - pair: see alltoall\n - ring: see alltoall\n - ompi_neighborexchange: see allgather\n - ompi_bruck: see allgather\n - mpich_rdb: recursive doubling algorithm from MPICH\n - mpich_ring: ring algorithm from MPICh - performs differently from the  one from STAR-MPI\n\nMPI_Bcast\n^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the bcast operations\n - mpich: use mpich selector for the bcast operations\n - mvapich2: use mvapich2 selector for the bcast operations\n - impi: use intel mpi selector for the bcast operations\n - automatic (experimental): use an automatic self-benchmarking algorithm\n - arrival_pattern_aware: root exchanges with the first process to arrive\n - arrival_pattern_aware_wait: same with slight variation\n - binomial_tree: binomial tree exchange\n - flattree: flat tree exchange\n - flattree_pipeline: flat tree exchange, message split into 8192 bytes pieces\n - NTSB: Non-topology-specific pipelined binary tree with 8192 bytes pieces\n - NTSL: Non-topology-specific pipelined linear with 8192 bytes pieces\n - NTSL_Isend: Non-topology-specific pipelined linear with 8192 bytes pieces, asynchronous communications\n - scatter_LR_allgather: scatter followed by logical ring allgather\n - scatter_rdb_allgather: scatter followed by recursive doubling allgather\n - arrival_scatter: arrival pattern aware scatter-allgather\n - SMP_binary: binary tree algorithm with 8 cores/SMP\n - SMP_binomial: binomial tree algorithm with 8 cores/SMP\n - SMP_linear: linear algorithm with 8 cores/SMP\n - ompi_split_bintree: binary tree algorithm from OpenMPI, with message split in 8192 bytes pieces\n - ompi_pipeline: pipeline algorithm from OpenMPI, with message split in 128KB pieces\n - mvapich2_inter_node: Inter node default mvapich worker\n - mvapich2_intra_node: Intra node default mvapich worker\n - mvapich2_knomial_intra_node:  k-nomial intra node default mvapich worker. default factor is 4.\n\nAutomatic Evaluation\n^^^^^^^^^^^^^^^^^^^^\n\n.. warning:: This is still very experimental.\n\nAn automatic version is available for each collective (or even as a selector). This specific\nversion will loop over all other implemented algorithm for this particular collective, and apply\nthem while benchmarking the time taken for each process. It will then output the quickest for\neach process, and the global quickest. This is still unstable, and a few algorithms which need\nspecific number of nodes may crash.\n\nAdding an algorithm\n^^^^^^^^^^^^^^^^^^^\n\nTo add a new algorithm, one should check in the src/smpi/colls folder\nhow other algorithms are coded. Using plain MPI code inside Simgrid\ncan't be done, so algorithms have to be changed to use smpi version of\nthe calls instead (MPI_Send will become smpi_mpi_send). Some functions\nmay have different signatures than their MPI counterpart, please check\nthe other algorithms or contact us using the `>SimGrid\ndevelopers mailing list <http://lists.gforge.inria.fr/mailman/listinfo/simgrid-devel>`_.\n\nExample: adding a \"pair\" version of the Alltoall collective.\n\n - Implement it in a file called alltoall-pair.c in the src/smpi/colls folder. This file should include colls_private.hpp.\n\n - The name of the new algorithm function should be smpi_coll_tuned_alltoall_pair, with the same signature as MPI_Alltoall.\n\n - Once the adaptation to SMPI code is done, add a reference to the file (\"src/smpi/colls/alltoall-pair.c\") in the SMPI_SRC part of the DefinePackages.cmake file inside buildtools/cmake, to allow the file to be built and distributed.\n\n - To register the new version of the algorithm, simply add a line to the corresponding macro in src/smpi/colls/cools.h ( add a \"COLL_APPLY(action, COLL_ALLTOALL_SIG, pair)\" to the COLL_ALLTOALLS macro ). The algorithm should now be compiled and be selected when using --cfg=smpi/alltoall:pair at runtime.\n\n - To add a test for the algorithm inside Simgrid's test suite, juste add the new algorithm name in the ALLTOALL_COLL list found inside teshsuite/smpi/CMakeLists.txt . When running ctest, a test for the new algorithm should be generated and executed. If it does not pass, please check your code or contact us.\n\n - Please submit your patch for inclusion in SMPI, for example through a pull request on GitHub or directly per email.\n\n\nTracing of Internal Communications\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, the collective operations are traced as a unique operation\nbecause tracing all point-to-point communications composing them could\nresult in overloaded, hard to interpret traces. If you want to debug\nand compare collective algorithms, you should set the\n``tracing/smpi/internals`` configuration item to 1 instead of 0.\n\nHere are examples of two alltoall collective algorithms runs on 16 nodes,\nthe first one with a ring algorithm, the second with a pairwise one.\n\n.. image:: /img/smpi_simgrid_alltoall_ring_16.png\n   :align: center\n\nAlltoall on 16 Nodes with the Ring Algorithm.\n\n.. image:: /img/smpi_simgrid_alltoall_pair_16.png\n   :align: center\n\nAlltoall on 16 Nodes with the Pairwise Algorithm.\n\n-------------------------\nWhat can run within SMPI?\n-------------------------\n\nYou can run unmodified MPI applications (both C/C++ and Fortran) within\nSMPI, provided that you only use MPI calls that we implemented. Global\nvariables should be handled correctly on Linux systems.\n\n....................\nMPI coverage of SMPI\n....................\n\nOur coverage of the interface is very decent, but still incomplete;\nGiven the size of the MPI standard, we may well never manage to\nimplement absolutely all existing primitives. Currently, we have\nalmost no support for I/O primitives, but we still pass a very large\namount of the MPICH coverage tests.\n\nThe full list of not yet implemented functions is documented in the\nfile `include/smpi/smpi.h\n<https://framagit.org/simgrid/simgrid/tree/master/include/smpi/smpi.h>`_\nin your version of SimGrid, between two lines containing the ``FIXME``\nmarker. If you really miss a feature, please get in touch with us: we\ncan guide you though the SimGrid code to help you implementing it, and\nwe'd be glad to integrate your contribution to the main project.\n\n.. _SMPI_what_globals:\n\n.................................\nPrivatization of global variables\n.................................\n\nConcerning the globals, the problem comes from the fact that usually,\nMPI processes run as real UNIX processes while they are all folded\ninto threads of a unique system process in SMPI. Global variables are\nusually private to each MPI process while they become shared between\nthe processes in SMPI.  The problem and some potential solutions are\ndiscussed in this article: `Automatic Handling of Global Variables for\nMulti-threaded MPI Programs\n<http://charm.cs.illinois.edu/newPapers/11-23/paper.pdf>` (note that\nthis article does not deal with SMPI but with a competing solution\ncalled AMPI that suffers of the same issue).  This point used to be\nproblematic in SimGrid, but the problem should now be handled\nautomatically on Linux.\n\nOlder versions of SimGrid came with a script that automatically\nprivatized the globals through static analysis of the source code. But\nour implementation was not robust enough to be used in production, so\nit was removed at some point. Currently, SMPI comes with two\nprivatization mechanisms that you can :ref:`select at runtime\n<cfg=smpi/privatization>`.  The dlopen approach is used by\ndefault as it is much faster and still very robust.  The mmap approach\nis an older approach that proves to be slower.\n\nWith the **mmap approach**, SMPI duplicates and dynamically switch the\n``.data`` and ``.bss`` segments of the ELF process when switching the\nMPI ranks. This allows each ranks to have its own copy of the global\nvariables.  No copy actually occurs as this mechanism uses ``mmap()``\nfor efficiency. This mechanism is considered to be very robust on all\nsystems supporting ``mmap()`` (Linux and most BSDs). Its performance\nis questionable since each context switch between MPI ranks induces\nseveral syscalls to change the ``mmap`` that redirects the ``.data``\nand ``.bss`` segments to the copies of the new rank. The code will\nalso be copied several times in memory, inducing a slight increase of\nmemory occupation.\n\nAnother limitation is that SMPI only accounts for global variables\ndefined in the executable. If the processes use external global\nvariables from dynamic libraries, they won't be switched\ncorrectly. The easiest way to solve this is to statically link against\nthe library with these globals. This way, each MPI rank will get its\nown copy of these libraries. Of course you should never statically\nlink against the SimGrid library itself.\n\nWith the **dlopen approach**, SMPI loads several copies of the same\nexecutable in memory as if it were a library, so that the global\nvariables get naturally duplicated. It first requires the executable\nto be compiled as a relocatable binary, which is less common for\nprograms than for libraries. But most distributions are now compiled\nthis way for security reason as it allows one to randomize the address\nspace layout. It should thus be safe to compile most (any?) program\nthis way.  The second trick is that the dynamic linker refuses to link\nthe exact same file several times, be it a library or a relocatable\nexecutable. It makes perfectly sense in the general case, but we need\nto circumvent this rule of thumb in our case. To that extend, the\nbinary is copied in a temporary file before being re-linked against.\n``dlmopen()`` cannot be used as it only allows 256 contextes, and as it\nwould also duplicate simgrid itself.\n\nThis approach greatly speeds up the context switching, down to about\n40 CPU cycles with our raw contextes, instead of requesting several\nsyscalls with the ``mmap()`` approach. Another advantage is that it\npermits one to run the SMPI contexts in parallel, which is obviously not\npossible with the ``mmap()`` approach. It was tricky to implement, but\nwe are not aware of any flaws, so smpirun activates it by default.\n\nIn the future, it may be possible to further reduce the memory and\ndisk consumption. It seems that we could `punch holes\n<https://lwn.net/Articles/415889/>`_ in the files before dl-loading\nthem to remove the code and constants, and mmap these area onto a\nunique copy. If done correctly, this would reduce the disk- and\nmemory- usage to the bare minimum, and would also reduce the pressure\non the CPU instruction cache. See the `relevant bug\n<https://github.com/simgrid/simgrid/issues/137>`_ on github for\nimplementation leads.\\n\n\nAlso, currently, only the binary is copied and dlopen-ed for each MPI\nrank. We could probably extend this to external dependencies, but for\nnow, any external dependencies must be statically linked into your\napplication. As usual, simgrid itself shall never be statically linked\nin your app. You don't want to give a copy of SimGrid to each MPI rank:\nthat's ways too much for them to deal with.\n\n.. todo: speak of smpi/privatize-libs here\n\n----------------------------------------------\nAdapting your MPI code for further scalability\n----------------------------------------------\n\nAs detailed in the `reference article\n<http://hal.inria.fr/hal-01415484>`_, you may want to adapt your code\nto improve the simulation performance. But these tricks may seriously\nhinder the result quality (or even prevent the app to run) if used\nwrongly. We assume that if you want to simulate an HPC application,\nyou know what you are doing. Don't prove us wrong!\n\n..............................\nReducing your memory footprint\n..............................\n\nIf you get short on memory (the whole app is executed on a single node when\nsimulated), you should have a look at the SMPI_SHARED_MALLOC and\nSMPI_SHARED_FREE macros. It allows one to share memory areas between processes: The\npurpose of these macro is that the same line malloc on each process will point\nto the exact same memory area. So if you have a malloc of 2M and you have 16\nprocesses, this macro will change your memory consumption from 2M*16 to 2M\nonly. Only one block for all processes.\n\nIf your program is ok with a block containing garbage value because all\nprocesses write and read to the same place without any kind of coordination,\nthen this macro can dramatically shrink your memory consumption. For example,\nthat will be very beneficial to a matrix multiplication code, as all blocks will\nbe stored on the same area. Of course, the resulting computations will useless,\nbut you can still study the application behavior this way.\n\nNaturally, this won't work if your code is data-dependent. For example, a Jacobi\niterative computation depends on the result computed by the code to detect\nconvergence conditions, so turning them into garbage by sharing the same memory\narea between processes does not seem very wise. You cannot use the\nSMPI_SHARED_MALLOC macro in this case, sorry.\n\nThis feature is demoed by the example file\n`examples/smpi/NAS/dt.c <https://framagit.org/simgrid/simgrid/tree/master/examples/smpi/NAS/dt.c>`_\n\n.. _SMPI_use_faster:\n\n.........................\nToward Faster Simulations\n.........................\n\nIf your application is too slow, try using SMPI_SAMPLE_LOCAL,\nSMPI_SAMPLE_GLOBAL and friends to indicate which computation loops can\nbe sampled. Some of the loop iterations will be executed to measure\ntheir duration, and this duration will be used for the subsequent\niterations. These samples are done per processor with\nSMPI_SAMPLE_LOCAL, and shared between all processors with\nSMPI_SAMPLE_GLOBAL. Of course, none of this will work if the execution\ntime of your loop iteration are not stable.\n\nThis feature is demoed by the example file\n`examples/smpi/NAS/ep.c <https://framagit.org/simgrid/simgrid/tree/master/examples/smpi/NAS/ep.c>`_\n\n.............................\nEnsuring Accurate Simulations\n.............................\n\nOut of the box, SimGrid may give you fairly accurate results, but\nthere is a plenty of factors that could go wrong and make your results\ninaccurate or even plainly wrong. Actually, you can only get accurate\nresults of a nicely built model, including both the system hardware\nand your application. Such models are hard to pass over and reuse in\nother settings, because elements that are not relevant to an\napplication (say, the latency of point-to-point communications,\ncollective operation implementation details or CPU-network\ninteraction) may be irrelevant to another application. The dream of\nthe perfect model, encompassing every aspects is only a chimera, as\nthe only perfect model of the reality is the reality. If you go for\nsimulation, then you have to ignore some irrelevant aspects of the\nreality, but which aspects are irrelevant is actually\napplication-dependent...\n\nThe only way to assess whether your settings provide accurate results\nis to double-check these results. If possible, you should first run\nthe same experiment in simulation and in real life, gathering as much\ninformation as you can. Try to understand the discrepancies in the\nresults that you observe between both settings (visualization can be\nprecious for that). Then, try to modify your model (of the platform,\nof the collective operations) to reduce the most preeminent differences.\n\nIf the discrepancies come from the computing time, try adapting the\n``smpi/host-speed``: reduce it if your simulation runs faster than in\nreality. If the error come from the communication, then you need to\nfiddle with your platform file.\n\nBe inventive in your modeling. Don't be afraid if the names given by\nSimGrid does not match the real names: we got very good results by\nmodeling multicore/GPU machines with a set of separate hosts\ninterconnected with very fast networks (but don't trust your model\nbecause it has the right names in the right place either).\n\nFinally, you may want to check `this article\n<https://hal.inria.fr/hal-00907887>`_ on the classical pitfalls in\nmodeling distributed systems.\n\n-------------------------\nTroubleshooting with SMPI\n-------------------------\n\n.................................\n./configure refuses to use smpicc\n.................................\n\nIf your ``./configure`` reports that the compiler is not\nfunctional or that you are cross-compiling, try to define the\n``SMPI_PRETEND_CC`` environment variable before running the\nconfiguration.\n\n.. code-block:: shell\n\n   SMPI_PRETEND_CC=1 ./configure # here come the configure parameters\n   make\n\nIndeed, the programs compiled with ``smpicc`` cannot be executed\nwithout ``smpirun`` (they are shared libraries and do weird things on\nstartup), while configure wants to test them directly.  With\n``SMPI_PRETEND_CC`` smpicc does not compile as shared, and the SMPI\ninitialization stops and returns 0 before doing anything that would\nfail without ``smpirun``.\n\n.. warning::\n\n  Make sure that SMPI_PRETEND_CC is only set when calling ./configure,\n  not during the actual execution, or any program compiled with smpicc\n  will stop before starting.\n\n..............................................\n./configure does not pick smpicc as a compiler\n..............................................\n\nIn addition to the previous answers, some projects also need to be\nexplicitly told what compiler to use, as follows:\n\n.. code-block:: shell\n\n   SMPI_PRETEND_CC=1 ./configure CC=smpicc # here come the other configure parameters\n   make\n\nMaybe your configure is using another variable, such as ``cc`` (in\nlower case) or similar. Just check the logs.\n\n.....................................\nerror: unknown type name 'useconds_t'\n.....................................\n\nTry to add ``-D_GNU_SOURCE`` to your compilation line to get ride\nof that error.\n\nThe reason is that SMPI provides its own version of ``usleep(3)``\nto override it and to block in the simulation world, not in the real\none. It needs the ``useconds_t`` type for that, which is declared\nonly if you declare ``_GNU_SOURCE`` before including\n``unistd.h``. If your project includes that header file before\nSMPI, then you need to ensure that you pass the right configuration\ndefines as advised above.\n\n\n\n.. _SMPI_offline:\n\n-----------------------------\nTrace Replay and Offline SMPI\n-----------------------------\n\nAlthough SMPI is often used for :ref:`online simulation\n<SMPI_online>`, where the application is executed for real, you can\nalso go for offline simulation through trace replay.\n\nSimGrid uses time-independent traces, in which each actor is given a\nscript of the actions to do sequentially. These trace files can\nactually be captured with the online version of SMPI, as follows:\n\n.. code-block:: shell\n\n   $ smpirun -trace-ti --cfg=tracing/filename:LU.A.32 -np 32 -platform ../cluster_backbone.xml bin/lu.A.32\n\nThe produced trace is composed of a file ``LU.A.32`` and a folder\n``LU.A.32_files``. The file names don't match with the MPI ranks, but\nthat's expected.\n\nTo replay this with SMPI, you need to first compile the provided\n``smpi_replay.cpp`` file, that comes from\n`simgrid/examples/smpi/replay\n<https://framagit.org/simgrid/simgrid/tree/master/examples/smpi/replay>`_.\n\n.. code-block:: shell\n\n   $ smpicxx ../replay.cpp -O3 -o ../smpi_replay\n\nAfterward, you can replay your trace in SMPI as follows:\n\n   $ smpirun -np 32 -platform ../cluster_torus.xml -ext smpi_replay ../smpi_replay LU.A.32\n\nAll the outputs are gone, as the application is not really simulated\nhere. Its trace is simply replayed. But if you visualize the live\nsimulation and the replay, you will see that the behavior is\nunchanged. The simulation does not run much faster on this very\nexample, but this becomes very interesting when your application\nis computationally hungry.\n",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/Configuring_SimGrid.rst": ".. _options:\n\nConfiguring SimGrid\n===================\n\n.. raw:: html\n\n   <object id=\"TOC\" data=\"graphical-toc.svg\" type=\"image/svg+xml\"></object>\n   <script>\n   window.onload=function() { // Wait for the SVG to be loaded before changing it\n     var elem=document.querySelector(\"#TOC\").contentDocument.getElementById(\"ConfigBox\")\n     elem.style=\"opacity:0.93999999;fill:#ff0000;fill-opacity:0.1;stroke:#000000;stroke-width:0.35277778;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1\";\n   }\n   </script>\n   <br/>\n   <br/>\n\nA number of options can be given at runtime to change the default\nSimGrid behavior. For a complete list of all configuration options\naccepted by the SimGrid version used in your simulator, simply pass\nthe --help configuration flag to your program. If some of the options\nare not documented on this page, this is a bug that you should please\nreport so that we can fix it. Note that some of the options presented\nhere may not be available in your simulators, depending on the\n:ref:`compile-time options <install_src_config>` that you used.\n\nSetting Configuration Items\n---------------------------\n\nThere is several way to pass configuration options to the simulators.\nThe most common way is to use the ``--cfg`` command line argument. For\nexample, to set the item ``Item`` to the value ``Value``, simply\ntype the following on the command-line:\n\n.. code-block:: shell\n\n   my_simulator --cfg=Item:Value (other arguments)\n\nSeveral ``--cfg`` command line arguments can naturally be used. If you\nneed to include spaces in the argument, don't forget to quote the\nargument. You can even escape the included quotes (write ``@'`` for ``'`` if\nyou have your argument between simple quotes).\n\nAnother solution is to use the ``<config>`` tag in the platform file. The\nonly restriction is that this tag must occur before the first\nplatform element (be it ``<zone>``, ``<cluster>``, ``<peer>`` or whatever).\nThe ``<config>`` tag takes an ``id`` attribute, but it is currently\nignored so you don't really need to pass it. The important part is that\nwithin that tag, you can pass one or several ``<prop>`` tags to specify\nthe configuration to use. For example, setting ``Item`` to ``Value``\ncan be done by adding the following to the beginning of your platform\nfile:\n\n.. code-block:: xml\n\n  <config>\n    <prop id=\"Item\" value=\"Value\"/>\n  </config>\n\nA last solution is to pass your configuration directly in your program\nwith :cpp:func:`simgrid::s4u::Engine::set_config` or :cpp:func:`MSG_config`.\n\n.. code-block:: cpp\n\n   #include <simgrid/s4u.hpp>\n\n   int main(int argc, char *argv[]) {\n     simgrid::s4u::Engine e(&argc, argv);\n\n     simgrid::s4u::Engine::set_config(\"Item:Value\");\n\n     // Rest of your code\n   }\n\n.. _options_list:\n\nExisting Configuration Items\n----------------------------\n\n.. note::\n  The full list can be retrieved by passing ``--help`` and\n  ``--help-cfg`` to an executable that uses SimGrid. Try passing\n  ``help`` as a value to get the list of values accepted by a given\n  option. For example, ``--cfg=plugin:help`` will give you the list\n  of plugins available in your installation of SimGrid.\n\n- **contexts/factory:** :ref:`cfg=contexts/factory`\n- **contexts/guard-size:** :ref:`cfg=contexts/guard-size`\n- **contexts/nthreads:** :ref:`cfg=contexts/nthreads`\n- **contexts/stack-size:** :ref:`cfg=contexts/stack-size`\n- **contexts/synchro:** :ref:`cfg=contexts/synchro`\n\n- **cpu/maxmin-selective-update:** :ref:`Cpu Optimization Level <options_model_optim>`\n- **cpu/model:** :ref:`options_model_select`\n- **cpu/optim:** :ref:`Cpu Optimization Level <options_model_optim>`\n\n- **debug/breakpoint:** :ref:`cfg=debug/breakpoint`\n- **debug/clean-atexit:** :ref:`cfg=debug/clean-atexit`\n- **debug/verbose-exit:** :ref:`cfg=debug/verbose-exit`\n\n- **exception/cutpath:** :ref:`cfg=exception/cutpath`\n\n- **host/model:** :ref:`options_model_select`\n\n- **maxmin/precision:** :ref:`cfg=maxmin/precision`\n- **maxmin/concurrency-limit:** :ref:`cfg=maxmin/concurrency-limit`\n\n- **msg/debug-multiple-use:** :ref:`cfg=msg/debug-multiple-use`\n\n- **model-check:** :ref:`options_modelchecking`\n- **model-check/checkpoint:** :ref:`cfg=model-check/checkpoint`\n- **model-check/communications-determinism:** :ref:`cfg=model-check/communications-determinism`\n- **model-check/dot-output:** :ref:`cfg=model-check/dot-output`\n- **model-check/max-depth:** :ref:`cfg=model-check/max-depth`\n- **model-check/property:** :ref:`cfg=model-check/property`\n- **model-check/reduction:** :ref:`cfg=model-check/reduction`\n- **model-check/replay:** :ref:`cfg=model-check/replay`\n- **model-check/send-determinism:** :ref:`cfg=model-check/send-determinism`\n- **model-check/termination:** :ref:`cfg=model-check/termination`\n- **model-check/timeout:** :ref:`cfg=model-check/timeout`\n- **model-check/visited:** :ref:`cfg=model-check/visited`\n\n- **network/bandwidth-factor:** :ref:`cfg=network/bandwidth-factor`\n- **network/crosstraffic:** :ref:`cfg=network/crosstraffic`\n- **network/latency-factor:** :ref:`cfg=network/latency-factor`\n- **network/loopback-lat:** :ref:`cfg=network/loopback`\n- **network/loopback-bw:** :ref:`cfg=network/loopback`\n- **network/maxmin-selective-update:** :ref:`Network Optimization Level <options_model_optim>`\n- **network/model:** :ref:`options_model_select`\n- **network/optim:** :ref:`Network Optimization Level <options_model_optim>`\n- **network/TCP-gamma:** :ref:`cfg=network/TCP-gamma`\n- **network/weight-S:** :ref:`cfg=network/weight-S`\n\n- **ns3/TcpModel:** :ref:`options_pls`\n- **ns3/seed:** :ref:`options_pls`\n- **path:** :ref:`cfg=path`\n- **plugin:** :ref:`cfg=plugin`\n\n- **storage/max_file_descriptors:** :ref:`cfg=storage/max_file_descriptors`\n\n- **surf/precision:** :ref:`cfg=surf/precision`\n\n- **For collective operations of SMPI,** please refer to Section :ref:`cfg=smpi/coll-selector`\n- **smpi/async-small-thresh:** :ref:`cfg=smpi/async-small-thresh`\n- **smpi/buffering:** :ref:`cfg=smpi/buffering`\n- **smpi/bw-factor:** :ref:`cfg=smpi/bw-factor`\n- **smpi/coll-selector:** :ref:`cfg=smpi/coll-selector`\n- **smpi/comp-adjustment-file:** :ref:`cfg=smpi/comp-adjustment-file`\n- **smpi/cpu-threshold:** :ref:`cfg=smpi/cpu-threshold`\n- **smpi/display-timing:** :ref:`cfg=smpi/display-timing`\n- **smpi/grow-injected-times:** :ref:`cfg=smpi/grow-injected-times`\n- **smpi/host-speed:** :ref:`cfg=smpi/host-speed`\n- **smpi/IB-penalty-factors:** :ref:`cfg=smpi/IB-penalty-factors`\n- **smpi/iprobe:** :ref:`cfg=smpi/iprobe`\n- **smpi/iprobe-cpu-usage:** :ref:`cfg=smpi/iprobe-cpu-usage`\n- **smpi/init:** :ref:`cfg=smpi/init`\n- **smpi/keep-temps:** :ref:`cfg=smpi/keep-temps`\n- **smpi/lat-factor:** :ref:`cfg=smpi/lat-factor`\n- **smpi/ois:** :ref:`cfg=smpi/ois`\n- **smpi/or:** :ref:`cfg=smpi/or`\n- **smpi/os:** :ref:`cfg=smpi/os`\n- **smpi/papi-events:** :ref:`cfg=smpi/papi-events`\n- **smpi/privatization:** :ref:`cfg=smpi/privatization`\n- **smpi/privatize-libs:** :ref:`cfg=smpi/privatize-libs`\n- **smpi/send-is-detached-thresh:** :ref:`cfg=smpi/send-is-detached-thresh`\n- **smpi/shared-malloc:** :ref:`cfg=smpi/shared-malloc`\n- **smpi/shared-malloc-hugepage:** :ref:`cfg=smpi/shared-malloc-hugepage`\n- **smpi/simulate-computation:** :ref:`cfg=smpi/simulate-computation`\n- **smpi/test:** :ref:`cfg=smpi/test`\n- **smpi/wtime:** :ref:`cfg=smpi/wtime`\n\n- **Tracing configuration options** can be found in Section :ref:`tracing_tracing_options`\n\n- **storage/model:** :ref:`options_model_select`\n\n- **vm/model:** :ref:`options_model_select`\n\n.. _options_model:\n\nConfiguring the Platform Models\n-------------------------------\n\n.. _options_model_select:\n\nChoosing the Platform Models\n............................\n\nSimGrid comes with several network, CPU and disk models built in,\nand you can change the used model at runtime by changing the passed\nconfiguration. The three main configuration items are given below.\nFor each of these items, passing the special ``help`` value gives you\na short description of all possible values (for example,\n``--cfg=network/model:help`` will present all provided network\nmodels). Also, ``--help-models`` should provide information about all\nmodels for all existing resources.\n\n- ``network/model``: specify the used network model. Possible values:\n\n  - **LV08 (default one):** Realistic network analytic model\n    (slow-start modeled by multiplying latency by 13.01, bandwidth by\n    .97; bottleneck sharing uses a payload of S=20537 for evaluating\n    RTT). Described in `Accuracy Study and Improvement of Network\n    Simulation in the SimGrid Framework\n    <http://mescal.imag.fr/membres/arnaud.legrand/articles/simutools09.pdf>`_.\n  - **Constant:** Simplistic network model where all communication\n    take a constant time (one second). This model provides the lowest\n    realism, but is (marginally) faster.\n  - **SMPI:** Realistic network model specifically tailored for HPC\n    settings (accurate modeling of slow start with correction factors on\n    three intervals: < 1KiB, < 64 KiB, >= 64 KiB). This model can be\n    :ref:`further configured <options_model_network>`.\n  - **IB:** Realistic network model specifically tailored for HPC\n    settings with InfiniBand networks (accurate modeling contention\n    behavior, based on the model explained in `this PhD work\n    <http://mescal.imag.fr/membres/jean-marc.vincent/index.html/PhD/Vienne.pdf>`_.\n    This model can be :ref:`further configured <options_model_network>`.\n  - **CM02:** Legacy network analytic model. Very similar to LV08, but\n    without corrective factors. The timings of small messages are thus\n    poorly modeled. This model is described in `A Network Model for\n    Simulation of Grid Application\n    <https://hal.inria.fr/inria-00071989/document>`_.\n  - **ns-3** (only available if you compiled SimGrid accordingly):\n    Use the packet-level network\n    simulators as network models (see :ref:`model_ns3`).\n    This model can be :ref:`further configured <options_pls>`.\n\n- ``cpu/model``: specify the used CPU model.  We have only one model\n  for now:\n\n  - **Cas01:** Simplistic CPU model (time=size/speed)\n\n- ``host/model``: The host concept is the aggregation of a CPU with a\n  network card. Three models exists, but actually, only 2 of them are\n  interesting. The \"compound\" one is simply due to the way our\n  internal code is organized, and can easily be ignored. So at the\n  end, you have two host models: The default one allows aggregation of\n  an existing CPU model with an existing network model, but does not\n  allow parallel tasks because these beasts need some collaboration\n  between the network and CPU model. That is why, ptask_07 is used by\n  default when using SimDag.\n\n  - **default:** Default host model. Currently, CPU:Cas01 and\n    network:LV08 (with cross traffic enabled)\n  - **compound:** Host model that is automatically chosen if\n    you change the network and CPU models\n  - **ptask_L07:** Host model somehow similar to Cas01+CM02 but\n    allowing \"parallel tasks\", that are intended to model the moldable\n    tasks of the grid scheduling literature.\n\n- ``storage/model``: specify the used storage model. Only one model is\n  provided so far.\n- ``vm/model``: specify the model for virtual machines. Only one model\n  is provided so far.\n\n.. todo: make 'compound' the default host model.\n\n.. _options_model_optim:\n\nOptimization Level\n..................\n\nThe network and CPU models that are based on lmm_solve (that\nis, all our analytical models) accept specific optimization\nconfigurations.\n\n  - items ``network/optim`` and ``cpu/optim`` (both default to 'Lazy'):\n\n    - **Lazy:** Lazy action management (partial invalidation in lmm +\n      heap in action remaining).\n    - **TI:** Trace integration. Highly optimized mode when using\n      availability traces (only available for the Cas01 CPU model for\n      now).\n    - **Full:** Full update of remaining and variables. Slow but may be\n      useful when debugging.\n\n  - items ``network/maxmin-selective-update`` and\n    ``cpu/maxmin-selective-update``: configure whether the underlying\n    should be lazily updated or not. It should have no impact on the\n    computed timings, but should speed up the computation. |br| It is\n    still possible to disable this feature because it can reveal\n    counter-productive in very specific scenarios where the\n    interaction level is high. In particular, if all your\n    communication share a given backbone link, you should disable it:\n    without it, a simple regular loop is used to update each\n    communication. With it, each of them is still updated (because of\n    the dependency induced by the backbone), but through a complicated\n    and slow pattern that follows the actual dependencies.\n\n.. _cfg=maxmin/precision:\n.. _cfg=surf/precision:\n\nNumerical Precision\n...................\n\n**Option** ``maxmin/precision`` **Default:** 0.00001 (in flops or bytes) |br|\n**Option** ``surf/precision`` **Default:** 0.00001 (in seconds)\n\nThe analytical models handle a lot of floating point values. It is\npossible to change the epsilon used to update and compare them through\nthis configuration item. Changing it may speedup the simulation by\ndiscarding very small actions, at the price of a reduced numerical\nprecision. You can modify separately the precision used to manipulate\ntimings (in seconds) and the one used to manipulate amounts of work\n(in flops or bytes).\n\n.. _cfg=maxmin/concurrency-limit:\n\nConcurrency Limit\n.................\n\n**Option** ``maxmin/concurrency-limit`` **Default:** -1 (no limit)\n\nThe maximum number of variables per resource can be tuned through this\noption. You can have as many simultaneous actions per resources as you\nwant. If your simulation presents a very high level of concurrency, it\nmay help to use e.g. 100 as a value here. It means that at most 100\nactions can consume a resource at a given time. The extraneous actions\nare queued and wait until the amount of concurrency of the considered\nresource lowers under the given boundary.\n\nSuch limitations help both to the simulation speed and simulation accuracy\non highly constrained scenarios, but the simulation speed suffers of this\nsetting on regular (less constrained) scenarios so it is off by default.\n\n.. _options_model_network:\n\nConfiguring the Network Model\n.............................\n\n.. _cfg=network/TCP-gamma:\n\nMaximal TCP Window Size\n^^^^^^^^^^^^^^^^^^^^^^^\n\n**Option** ``network/TCP-gamma`` **Default:** 4194304\n\nThe analytical models need to know the maximal TCP window size to take\nthe TCP congestion mechanism into account.  On Linux, this value can\nbe retrieved using the following commands. Both give a set of values,\nand you should use the last one, which is the maximal size.\n\n.. code-block:: shell\n\n   cat /proc/sys/net/ipv4/tcp_rmem # gives the sender window\n   cat /proc/sys/net/ipv4/tcp_wmem # gives the receiver window\n\n.. _cfg=smpi/IB-penalty-factors:\n.. _cfg=network/bandwidth-factor:\n.. _cfg=network/latency-factor:\n.. _cfg=network/weight-S:\n\nCorrecting Important Network Parameters\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSimGrid can take network irregularities such as a slow startup or\nchanging behavior depending on the message size into account.  You\nshould not change these values unless you really know what you're\ndoing.  The corresponding values were computed through data fitting\none the timings of packet-level simulators, as described in `Accuracy\nStudy and Improvement of Network Simulation in the SimGrid Framework\n<http://mescal.imag.fr/membres/arnaud.legrand/articles/simutools09.pdf>`_.\n\n\nIf you are using the SMPI model, these correction coefficients are\nthemselves corrected by constant values depending on the size of the\nexchange.  By default SMPI uses factors computed on the Stampede\nSupercomputer at TACC, with optimal deployment of processes on\nnodes. Again, only hardcore experts should bother about this fact.\n\nInfiniBand network behavior can be modeled through 3 parameters\n``smpi/IB-penalty-factors:\"\u03b2e;\u03b2s;\u03b3s\"``, as explained in `this PhD\nthesis\n<http://mescal.imag.fr/membres/jean-marc.vincent/index.html/PhD/Vienne.pdf>`_.\n\n.. todo:: This section should be rewritten, and actually explain the\n\t  options network/bandwidth-factor, network/latency-factor,\n\t  network/weight-S.\n\n.. _cfg=network/crosstraffic:\n\nSimulating Cross-Traffic\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nSince SimGrid v3.7, cross-traffic effects can be taken into account in\nanalytical simulations. It means that ongoing and incoming\ncommunication flows are treated independently. In addition, the LV08\nmodel adds 0.05 of usage on the opposite direction for each new\ncreated flow. This can be useful to simulate some important TCP\nphenomena such as ack compression.\n\nFor that to work, your platform must have two links for each\npair of interconnected hosts. An example of usable platform is\navailable in ``examples/platforms/crosstraffic.xml``.\n\nThis is activated through the ``network/crosstraffic`` item, that\ncan be set to 0 (disable this feature) or 1 (enable it).\n\nNote that with the default host model this option is activated by default.\n\n.. _cfg=network/loopback:\n\nConfiguring loopback link\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSeveral network model provide an implicit loopback link to account for local \ncommunication on a host. By default it has a 10GBps bandwidth and a null latency.\nThis can be changed with ``network/loopback-lat`` and ``network/loopback-bw`` \nitems.\n\n.. _cfg=smpi/async-small-thresh:\n\nSimulating Asynchronous Send\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(this configuration item is experimental and may change or disappear)\n\nIt is possible to specify that messages below a certain size (in bytes) will be\nsent as soon as the call to MPI_Send is issued, without waiting for\nthe correspondent receive. This threshold can be configured through\nthe ``smpi/async-small-thresh`` item. The default value is 0. This\nbehavior can also be manually set for mailboxes, by setting the\nreceiving mode of the mailbox with a call to\n:cpp:func:`MSG_mailbox_set_async`. After this, all messages sent to\nthis mailbox will have this behavior regardless of the message size.\n\nThis value needs to be smaller than or equals to the threshold set at\n:ref:`cfg=smpi/send-is-detached-thresh`, because asynchronous messages\nare meant to be detached as well.\n\n.. _options_pls:\n\nConfiguring ns-3\n^^^^^^^^^^^^^^^^\n\n**Option** ``ns3/TcpModel`` **Default:** \"default\" (ns-3 default)\n\nWhen using ns-3, there is an extra item ``ns3/TcpModel``, corresponding\nto the ``ns3::TcpL4Protocol::SocketType`` configuration item in\nns-3. The only valid values (enforced on the SimGrid side) are\n'default' (no change to the ns-3 configuration), 'NewReno' or 'Reno' or\n'Tahoe'.\n\n**Option** ``ns3/seed`` **Default:** \"\" (don't set the seed in ns-3)\n\nThis option is the random seed to provide to ns-3 with\n``ns3::RngSeedManager::SetSeed`` and ``ns3::RngSeedManager::SetRun``.\n\nIf left blank, no seed is set in ns-3. If the value 'time' is\nprovided, the current amount of seconds since epoch is used as a seed.\nOtherwise, the provided value must be a number to use as a seed.\n\nConfiguring the Storage model\n.............................\n\n.. _cfg=storage/max_file_descriptors:\n\nFile Descriptor Count per Host\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n**Option** ``storage/max_file_descriptors`` **Default:** 1024\n\nEach host maintains a fixed-size array of its file descriptors. You\ncan change its size through this item to either enlarge it if your\napplication requires it or to reduce it to save memory space.\n\n.. _cfg=plugin:\n\nActivating Plugins\n------------------\n\nSimGrid plugins allow one to extend the framework without changing its\nsource code directly. Read the source code of the existing plugins to\nlearn how to do so (in ``src/plugins``), and ask your questions to the\nusual channels (Stack Overflow, Mailing list, IRC). The basic idea is\nthat plugins usually register callbacks to some signals of interest.\nIf they need to store some information about a given object (Link, CPU\nor Actor), they do so through the use of a dedicated object extension.\n\nSome of the existing plugins can be activated from the command line,\nmeaning that you can activate them from the command line without any\nmodification to your simulation code. For example, you can activate\nthe host energy plugin by adding ``--cfg=plugin:host_energy`` to your\ncommand line.\n\nHere is a partial list of plugins that can be activated this way. You can get\nthe full list by passing ``--cfg=plugin:help`` to your simulator.\n\n  - :ref:`Host Energy <plugin_host_energy>`: models the energy dissipation of the compute units.\n  - :ref:`Link Energy <plugin_link_energy>`: models the energy dissipation of the network.\n  - :ref:`Host Load <plugin_host_load>`: monitors the load of the compute units.\n\n.. _options_modelchecking:\n\nConfiguring the Model-Checking\n------------------------------\n\nTo enable SimGrid's model-checking support, the program should\nbe executed using the simgrid-mc wrapper:\n\n.. code-block:: shell\n\n   simgrid-mc ./my_program\n\nSafety properties are expressed as assertions using the function\n:cpp:func:`void MC_assert(int prop)`.\n\n.. _cfg=smpi/buffering:\n\nSpecifying the MPI buffering behavior\n.....................................\n\n**Option** ``smpi/buffering`` **Default:** infty\n\nBuffering in MPI has a huge impact on the communication semantic. For example,\nstandard blocking sends are synchronous calls when the system buffers are full\nwhile these calls can complete immediately without even requiring a matching\nreceive call for small messages sent when the system buffers are empty.\n\nIn SMPI, this depends on the message size, that is compared against two thresholds:\n\n- if (size < :ref:`smpi/async-small-thresh <cfg=smpi/async-small-thresh>`) then\n  MPI_Send returns immediately, even if the corresponding receive has not be issued yet.\n- if (:ref:`smpi/async-small-thresh <cfg=smpi/async-small-thresh>` < size < :ref:`smpi/send-is-detached-thresh <cfg=smpi/send-is-detached-thresh>`) then\n  MPI_Send returns as soon as the corresponding receive has been issued. This is known as the eager mode.\n- if (:ref:`smpi/send-is-detached-thresh <cfg=smpi/send-is-detached-thresh>` < size) then\n  MPI_Send returns only when the message has actually been sent over the network. This is known as the rendez-vous mode.\n\nThe ``smpi/buffering`` (only valid with MC) option gives an easier interface to choose between these semantics. It can take two values:\n\n- **zero:** means that buffering should be disabled. All communications are actually blocking.\n- **infty:** means that buffering should be made infinite. All communications are non-blocking.\n\n.. _cfg=model-check/property:\n\nSpecifying a liveness property\n..............................\n\n**Option** ``model-check/property`` **Default:** unset\n\nIf you want to specify liveness properties, you have to pass them on\nthe command line, specifying the name of the file containing the\nproperty, as formatted by the `ltl2ba <https://github.com/utwente-fmt/ltl2ba>`_ program.\nNote that ltl2ba is not part of SimGrid and must be installed separately.\n\n.. code-block:: shell\n\n   simgrid-mc ./my_program --cfg=model-check/property:<filename>\n\n.. _cfg=model-check/checkpoint:\n\nGoing for Stateful Verification\n...............................\n\nBy default, the system is backtracked to its initial state to explore\nanother path, instead of backtracking to the exact step before the fork\nthat we want to explore (this is called stateless verification). This\nis done this way because saving intermediate states can rapidly\nexhaust the available memory. If you want, you can change the value of\nthe ``model-check/checkpoint`` item. For example,\n``--cfg=model-check/checkpoint:1`` asks to take a checkpoint every\nstep.  Beware, this will certainly explode your memory. Larger values\nare probably better, make sure to experiment a bit to find the right\nsetting for your specific system.\n\n.. _cfg=model-check/reduction:\n\nSpecifying the kind of reduction\n................................\n\nThe main issue when using the model-checking is the state space\nexplosion. You can activate some reduction technique with\n``--cfg=model-check/reduction:<technique>``. For now, this\nconfiguration variable can take 2 values:\n\n - **none:** Do not apply any kind of reduction (mandatory for\n   liveness properties, as our current DPOR algorithm breaks cycles)\n - **dpor:** Apply Dynamic Partial Ordering Reduction. Only valid if\n   you verify local safety properties (default value for safety\n   checks).\n\nAnother way to mitigate the state space explosion is to search for\ncycles in the exploration with the :ref:`cfg=model-check/visited`\nconfiguration. Note that DPOR and state-equality reduction may not\nplay well together. You should choose between them.\n\nOur current DPOR implementation could be improved in may ways. We are\ncurrently improving its efficiency (both in term of reduction ability\nand computational speed), and future work could make it compatible\nwith liveness properties.\n\n.. _cfg=model-check/visited:\n\nSize of Cycle Detection Set (state equality reduction)\n......................................................\n\nMc SimGrid can be asked to search for cycles during the exploration,\ni.e. situations where a new explored state is in fact the same state\nthan a previous one.. This can prove useful to mitigate the state\nspace explosion with safety properties, and this is the crux when\nsearching for counter-examples to the liveness properties.\n\nNote that this feature may break the current implementation of the\nDPOR reduction technique.\n\nThe ``model-check/visited`` item is the maximum number of states, which\nare stored in memory. If the maximum number of snapshotted state is\nreached, some states will be removed from the memory and some cycles\nmight be missed. Small values can lead to incorrect verifications, but\nlarge values can exhaust your memory and be CPU intensive as each new\nstate must be compared to that amount of older saved states.\n\nThe default settings depend on the kind of exploration. With safety\nchecking, no state is snapshotted and cycles cannot be detected. With\nliveness checking, all states are snapshotted because missing a cycle\ncould hinder the exploration soundness.\n\n.. _cfg=model-check/termination:\n\nNon-Termination Detection\n.........................\n\nThe ``model-check/termination`` configuration item can be used to\nreport if a non-termination execution path has been found. This is a\npath with a cycle, which means that the program might never terminate.\n\nThis only works in safety mode, not in liveness mode.\n\nThis options is disabled by default.\n\n.. _cfg=model-check/dot-output:\n\nDot Output\n..........\n\nIf set, the ``model-check/dot-output`` configuration item is the name\nof a file in which to write a dot file of the path leading to the\nproperty violation discovered (safety or liveness violation), as well\nas the cycle for liveness properties. This dot file can then be fed to the\ngraphviz dot tool to generate a corresponding graphical representation.\n\n.. _cfg=model-check/max-depth:\n\nExploration Depth Limit\n.......................\n\nThe ``model-checker/max-depth`` can set the maximum depth of the\nexploration graph of the model checker. If this limit is reached, a\nlogging message is sent and the results might not be exact.\n\nBy default, there is no depth limit.\n\n.. _cfg=model-check/timeout:\n\nHandling of Timeouts\n....................\n\nBy default, the model checker does not handle timeout conditions: the `wait`\noperations never time out. With the ``model-check/timeout`` configuration item\nset to **yes**, the model checker will explore timeouts of `wait` operations.\n\n.. _cfg=model-check/communications-determinism:\n.. _cfg=model-check/send-determinism:\n\nCommunication Determinism\n.........................\n\nThe ``model-check/communications-determinism`` and\n``model-check/send-determinism`` items can be used to select the\ncommunication determinism mode of the model checker, which checks\ndeterminism properties of the communications of an application.\n\n.. _options_mc_perf:\n\nVerification Performance Considerations\n.......................................\n\nThe size of the stacks can have a huge impact on the memory\nconsumption when using model-checking. By default, each snapshot will\nsave a copy of the whole stacks and not only of the part that is\nreally meaningful: you should expect the contribution of the memory\nconsumption of the snapshots to be:\n:math:`\\text{number of processes} \\times \\text{stack size} \\times \\text{number of states}`.\n\nWhen compiled against the model checker, the stacks are not\nprotected with guards: if the stack size is too small for your\napplication, the stack will silently overflow into other parts of the\nmemory (see :ref:`contexts/guard-size <cfg=contexts/guard-size>`).\n\n.. _cfg=model-check/replay:\n\nReplaying buggy execution paths from the model checker\n......................................................\n\nDebugging the problems reported by the model checker is challenging:\nFirst, the application under verification cannot be debugged with gdb\nbecause the model checker already traces it. Then, the model checker may\nexplore several execution paths before encountering the issue, making it\nvery difficult to understand the output. Fortunately, SimGrid provides\nthe execution path leading to any reported issue so that you can replay\nthis path reported by the model checker, enabling the usage of classical\ndebugging tools.\n\nWhen the model checker finds an interesting path in the application\nexecution graph (where a safety or liveness property is violated), it\ngenerates an identifier for this path. Here is an example of the output:\n\n.. code-block:: shell\n\n   [  0.000000] (0:@) Check a safety property\n   [  0.000000] (0:@) **************************\n   [  0.000000] (0:@) *** PROPERTY NOT VALID ***\n   [  0.000000] (0:@) **************************\n   [  0.000000] (0:@) Counter-example execution trace:\n   [  0.000000] (0:@)   [(1)Tremblay (app)] MC_RANDOM(3)\n   [  0.000000] (0:@)   [(1)Tremblay (app)] MC_RANDOM(4)\n   [  0.000000] (0:@) Path = 1/3;1/4\n   [  0.000000] (0:@) Expanded states = 27\n   [  0.000000] (0:@) Visited states = 68\n   [  0.000000] (0:@) Executed transitions = 46\n\nThe interesting line is ``Path = 1/3;1/4``, which means that you should use\n``--cfg=model-check/replay:1/3;1/4`` to replay your application on the buggy\nexecution path. All options (but the model checker related ones) must\nremain the same. In particular, if you ran your application with\n``smpirun -wrapper simgrid-mc``, then do it again. Remove all\nMC-related options, keep non-MC-related ones and add\n``--cfg=model-check/replay:???``.\n\nCurrently, if the path is of the form ``X;Y;Z``, each number denotes\nthe actor's pid that is selected at each indecision point. If it's of\nthe form ``X/a;Y/b``, the X and Y are the selected pids while the a\nand b are the return values of their simcalls. In the previous\nexample, ``1/3;1/4``, you can see from the full output that the actor\n1 is doing MC_RANDOM simcalls, so the 3 and 4 simply denote the values\nthat these simcall return.\n\nConfiguring the User Code Virtualization\n----------------------------------------\n\n.. _cfg=contexts/factory:\n\nSelecting the Virtualization Factory\n....................................\n\n**Option** contexts/factory **Default:** \"raw\"\n\nIn SimGrid, the user code is virtualized in a specific mechanism that\nallows the simulation kernel to control its execution: when a user\nprocess requires a blocking action (such as sending a message), it is\ninterrupted, and only gets released when the simulated clock reaches\nthe point where the blocking operation is done. This is explained\ngraphically in the `relevant tutorial, available online\n<https://simgrid.org/tutorials/simgrid-simix-101.pdf>`_.\n\nIn SimGrid, the containers in which user processes are virtualized are\ncalled contexts. Several context factory are provided, and you can\nselect the one you want to use with the ``contexts/factory``\nconfiguration item. Some of the following may not exist on your\nmachine because of portability issues. In any case, the default one\nshould be the most effcient one (please report bugs if the\nauto-detection fails for you). They are approximately sorted here from\nthe slowest to the most efficient:\n\n - **thread:** very slow factory using full featured threads (either\n   pthreads or windows native threads). They are slow but very\n   standard. Some debuggers or profilers only work with this factory.\n - **java:** Java applications are virtualized onto java threads (that\n   are regular pthreads registered to the JVM)\n - **ucontext:** fast factory using System V contexts (Linux and FreeBSD only)\n - **boost:** This uses the `context\n   implementation <http://www.boost.org/doc/libs/1_59_0/libs/context/doc/html/index.html>`_\n   of the boost library for a performance that is comparable to our\n   raw implementation.\n   |br| Install the relevant library (e.g. with the\n   libboost-contexts-dev package on Debian/Ubuntu) and recompile\n   SimGrid.\n - **raw:** amazingly fast factory using a context switching mechanism\n   of our own, directly implemented in assembly (only available for x86\n   and amd64 platforms for now) and without any unneeded system call.\n\nThe main reason to change this setting is when the debugging tools become\nfooled by the optimized context factories. Threads are the most\ndebugging-friendly contexts, as they allow one to set breakpoints\nanywhere with gdb and visualize backtraces for all processes, in order\nto debug concurrency issues. Valgrind is also more comfortable with\nthreads, but it should be usable with all factories (Exception: the\ncallgrind tool really dislikes raw and ucontext factories).\n\n.. _cfg=contexts/stack-size:\n\nAdapting the Stack Size\n.......................\n\n**Option** ``contexts/stack-size`` **Default:** 8192 KiB\n\nEach virtualized used process is executed using a specific system\nstack. The size of this stack has a huge impact on the simulation\nscalability, but its default value is rather large. This is because\nthe error messages that you get when the stack size is too small are\nrather disturbing: this leads to stack overflow (overwriting other\nstacks), leading to segfaults with corrupted stack traces.\n\nIf you want to push the scalability limits of your code, you might\nwant to reduce the ``contexts/stack-size`` item. Its default value is\n8192 (in KiB), while our Chord simulation works with stacks as small\nas 16 KiB, for example. You can ensure that some actors have a specific\nsize by simply changing the value of this configuration item before\ncreating these actors. The :cpp:func:`simgrid::s4u::Engine::set_config` \nfunctions are handy for that.\n\nThis *setting is ignored* when using the thread factory (because there\nis no way to modify the stack size with C++ system threads). Instead,\nyou should compile SimGrid and your application with\n``-fsplit-stack``. Note that this compilation flag is not compatible\nwith the model checker right now.\n\nThe operating system should only allocate memory for the pages of the\nstack which are actually used and you might not need to use this in\nmost cases. However, this setting is very important when using the\nmodel checker (see :ref:`options_mc_perf`).\n\n.. _cfg=contexts/guard-size:\n\nDisabling Stack Guard Pages\n...........................\n\n**Option** ``contexts/guard-size`` **Default** 1 page in most case (0 pages on Windows or with MC)\n\nUnless you use the threads context factory (see\n:ref:`cfg=contexts/factory`), a stack guard page is usually used\nwhich prevents the stack of a given actor from overflowing on another\nstack. But the performance impact may become prohibitive when the\namount of actors increases.  The option ``contexts/guard-size`` is the\nnumber of stack guard pages used.  By setting it to 0, no guard pages\nwill be used: in this case, you should avoid using small stacks (with\n:ref:`contexts/stack-size <cfg=contexts/stack-size>`) as the stack\nwill silently overflow on other parts of the memory.\n\nWhen no stack guard page is created, stacks may then silently overflow\non other parts of the memory if their size is too small for the\napplication.\n\n.. _cfg=contexts/nthreads:\n.. _cfg=contexts/synchro:\n\nRunning User Code in Parallel\n.............................\n\nParallel execution of the user code is only considered stable in\nSimGrid v3.7 and higher, and mostly for MSG simulations. SMPI\nsimulations may well fail in parallel mode. It is described in\n`INRIA RR-7653 <http://hal.inria.fr/inria-00602216/>`_.\n\nIf you are using the **ucontext** or **raw** context factories, you can\nrequest to execute the user code in parallel. Several threads are\nlaunched, each of them handling the same number of user contexts at each\nrun. To activate this, set the ``contexts/nthreads`` item to the amount\nof cores that you have in your computer (or lower than 1 to have the\namount of cores auto-detected).\n\nWhen parallel execution is activated, you can choose the\nsynchronization schema used with the ``contexts/synchro`` item,\nwhich value is either:\n\n - **futex:** ultra optimized synchronisation schema, based on futexes\n   (fast user-mode mutexes), and thus only available on Linux systems.\n   This is the default mode when available.\n - **posix:** slow but portable synchronisation using only POSIX\n   primitives.\n - **busy_wait:** not really a synchronisation: the worker threads\n   constantly request new contexts to execute. It should be the most\n   efficient synchronisation schema, but it loads all the cores of\n   your machine for no good reason. You probably prefer the other less\n   eager schemas.\n\nConfiguring the Tracing\n-----------------------\n\nThe :ref:`tracing subsystem <outcomes_vizu>` can be configured in\nseveral different ways depending on the nature of the simulator (MSG,\nSimDag, SMPI) and the kind of traces that need to be obtained. See the\n:ref:`Tracing Configuration Options subsection\n<tracing_tracing_options>` to get a detailed description of each\nconfiguration option.\n\nWe detail here a simple way to get the traces working for you, even if\nyou never used the tracing API.\n\n\n- Any SimGrid-based simulator (MSG, SimDag, SMPI, ...) and raw traces:\n\n  .. code-block:: shell\n\n     --cfg=tracing:yes --cfg=tracing/uncategorized:yes\n\n  The first parameter activates the tracing subsystem, and the second\n  tells it to trace host and link utilization (without any\n  categorization).\n\n- MSG or SimDag-based simulator and categorized traces (you need to\n  declare categories and classify your tasks according to them) \n\n  .. code-block:: shell\n\n     --cfg=tracing:yes --cfg=tracing/categorized:yes\n\n  The first parameter activates the tracing subsystem, and the second\n  tells it to trace host and link categorized utilization.\n\n- SMPI simulator and traces for a space/time view:\n\n  .. code-block:: shell\n\n     smpirun -trace ...\n\n  The `-trace` parameter for the smpirun script runs the simulation\n  with ``--cfg=tracing:yes --cfg=tracing/smpi:yes``. Check the\n  smpirun's `-help` parameter for additional tracing options.\n\nSometimes you might want to put additional information on the trace to\ncorrectly identify them later, or to provide data that can be used to\nreproduce an experiment. You have two ways to do that:\n\n- Add a string on top of the trace file as comment:\n\n  .. code-block:: shell\n\n     --cfg=tracing/comment:my_simulation_identifier\n\n- Add the contents of a textual file on top of the trace file as comment:\n\n  .. code-block:: shell\n\n     --cfg=tracing/comment-file:my_file_with_additional_information.txt\n\nPlease, use these two parameters (for comments) to make reproducible\nsimulations. For additional details about this and all tracing\noptions, check See the :ref:`tracing_tracing_options`.\n\nConfiguring MSG\n---------------\n\n.. _cfg=msg/debug-multiple-use:\n\nDebugging MSG Code\n..................\n\n**Option** ``msg/debug-multiple-use`` **Default:** off\n\nSometimes your application may try to send a task that is still being\nexecuted somewhere else, making it impossible to send this task. However,\nfor debugging purposes, one may want to know what the other host is/was\ndoing. This option shows a backtrace of the other process.\n\nConfiguring SMPI\n----------------\n\nThe SMPI interface provides several specific configuration items.\nThese are not easy to see, since the code is usually launched through the\n``smiprun`` script directly.\n\n.. _cfg=smpi/host-speed:\n.. _cfg=smpi/cpu-threshold:\n.. _cfg=smpi/simulate-computation:\n\nAutomatic Benchmarking of SMPI Code\n...................................\n\nIn SMPI, the sequential code is automatically benchmarked, and these\ncomputations are automatically reported to the simulator. That is to\nsay that if you have a large computation between a ``MPI_Recv()`` and\na ``MPI_Send()``, SMPI will automatically benchmark the duration of\nthis code, and create an execution task within the simulator to take\nthis into account. For that, the actual duration is measured on the\nhost machine and then scaled to the power of the corresponding\nsimulated machine. The variable ``smpi/host-speed`` allows one to\nspecify the computational speed of the host machine (in flop/s by\ndefault) to use when scaling the execution times.\n\nThe default value is ``smpi/host-speed=20kf`` (= 20,000 flop/s). This\nis probably underestimated for most machines, leading SimGrid to\noverestimate the amount of flops in the execution blocks that are\nautomatically injected in the simulator. As a result, the execution\ntime of the whole application will probably be overestimated until you\nuse a realistic value.\n\nWhen the code consists of numerous consecutive MPI calls, the\nprevious mechanism feeds the simulation kernel with numerous tiny\ncomputations. The ``smpi/cpu-threshold`` item becomes handy when this\nimpacts badly on the simulation performance. It specifies a threshold (in\nseconds) below which the execution chunks are not reported to the\nsimulation kernel (default value: 1e-6).\n\n.. note:: The option ``smpi/cpu-threshold`` ignores any computation\n   time spent below this threshold. SMPI does not consider the\n   `amount of time` of these computations; there is no offset for\n   this. Hence, a value that is too small, may lead to unreliable\n   simulation results.\n\nIn some cases, however, one may wish to disable simulation of\nthe computation of an application. This is the case when SMPI is used not to\nsimulate an MPI application, but instead an MPI code that performs\n\"live replay\" of another MPI app (e.g., ScalaTrace's replay tool, or\nvarious on-line simulators that run an app at scale). In this case the\ncomputation of the replay/simulation logic should not be simulated by\nSMPI. Instead, the replay tool or on-line simulator will issue\n\"computation events\", which correspond to the actual MPI simulation\nbeing replayed/simulated. At the moment, these computation events can\nbe simulated using SMPI by calling internal smpi_execute*() functions.\n\nTo disable the benchmarking/simulation of a computation in the simulated\napplication, the variable ``smpi/simulate-computation`` should be set\nto **no**.  This option just ignores the timings in your simulation; it\nstill executes the computations itself. If you want to stop SMPI from\ndoing that, you should check the SMPI_SAMPLE macros, documented in\nSection :ref:`SMPI_use_faster`.\n\n+------------------------------------+-------------------------+-----------------------------+\n|  Solution                          | Computations executed?  | Computations simulated?     |\n+====================================+=========================+=============================+\n| --cfg=smpi/simulate-computation:no | Yes                     | Never                       |\n+------------------------------------+-------------------------+-----------------------------+\n| --cfg=smpi/cpu-threshold:42        | Yes, in all cases       | If it lasts over 42 seconds |\n+------------------------------------+-------------------------+-----------------------------+\n| SMPI_SAMPLE() macro                | Only once per loop nest | Always                      |\n+------------------------------------+-------------------------+-----------------------------+\n\n.. _cfg=smpi/comp-adjustment-file:\n\nSlow-down or speed-up parts of your code\n........................................\n\n**Option** ``smpi/comp-adjustment-file:`` **Default:** unset\n\nThis option allows you to pass a file that contains two columns: The\nfirst column defines the section that will be subject to a speedup;\nthe second column is the speedup. For instance:\n\n.. code-block:: shell\n\n  \"start:stop\",\"ratio\"\n  \"exchange_1.f:30:exchange_1.f:130\",1.18244559422142\n\nThe first line is the header - you must include it.  The following\nline means that the code between two consecutive MPI calls on line 30\nin exchange_1.f and line 130 in exchange_1.f should receive a speedup\nof 1.18244559422142. The value for the second column is therefore a\nspeedup, if it is larger than 1 and a slowdown if it is smaller\nthan 1. Nothing will be changed if it is equal to 1.\n\nOf course, you can set any arbitrary filenames you want (so the start\nand end don't have to be in the same file), but be aware that this\nmechanism only supports `consecutive calls!`\n\nPlease note that you must pass the ``-trace-call-location`` flag to\nsmpicc or smpiff, respectively. This flag activates some internal\nmacro definitions that help with obtaining the call location.\n\n.. _cfg=smpi/bw-factor:\n\nBandwidth Factors\n.................\n\n**Option** ``smpi/bw-factor``\n|br| **Default:** 65472:0.940694;15424:0.697866;9376:0.58729;5776:1.08739;3484:0.77493;1426:0.608902;732:0.341987;257:0.338112;0:0.812084\n\nThe possible throughput of network links is often dependent on the\nmessage sizes, as protocols may adapt to different message sizes. With\nthis option, a series of message sizes and factors are given, helping\nthe simulation to be more realistic. For instance, the current default\nvalue means that messages with size 65472 bytes and more will get a total of\nMAX_BANDWIDTH*0.940694, messages of size 15424 to 65471 will get\nMAX_BANDWIDTH*0.697866, and so on (where MAX_BANDWIDTH denotes the\nbandwidth of the link).\n\nAn experimental script to compute these factors is available online. See\nhttps://framagit.org/simgrid/platform-calibration/\nhttps://simgrid.org/contrib/smpi-saturation-doc.html\n\n.. _cfg=smpi/display-timing:\n\nReporting Simulation Time\n.........................\n\n**Option** ``smpi/display-timing`` **Default:** 0 (false)\n\nMost of the time, you run MPI code with SMPI to compute the time it\nwould take to run it on a platform. But since the code is run through\nthe ``smpirun`` script, you don't have any control on the launcher\ncode, making it difficult to report the simulated time when the\nsimulation ends. If you enable the ``smpi/display-timing`` item,\n``smpirun`` will display this information when the simulation\nends.\n\n.. _cfg=smpi/keep-temps:\n\nKeeping temporary files after simulation\n........................................\n\n**Option** ``smpi/keep-temps`` **default:** 0 (false)\n\nSMPI usually generates a lot of temporary files that are cleaned after\nuse. This option requests to preserve them, for example to debug or\nprofile your code. Indeed, the binary files are removed very early\nunder the dlopen privatization schema, which tends to fool the\ndebuggers.\n\n.. _cfg=smpi/lat-factor:\n\nLatency factors\n...............\n\n**Option** ``smpi/lat-factor`` |br|\n**default:** 65472:11.6436;15424:3.48845;9376:2.59299;5776:2.18796;3484:1.88101;1426:1.61075;732:1.9503;257:1.95341;0:2.01467\n\nThe motivation and syntax for this option is identical to the motivation/syntax\nof :ref:`cfg=smpi/bw-factor`.\n\nThere is an important difference, though: While smpi/bw-factor `reduces` the\nactual bandwidth (i.e., values between 0 and 1 are valid), latency factors\nincrease the latency, i.e., values larger than or equal to 1 are valid here.\n\n.. _cfg=smpi/papi-events:\n\nTrace hardware counters with PAPI\n.................................\n\n**Option** ``smpi/papi-events`` **default:** unset\n\nWhen the PAPI support is compiled into SimGrid, this option takes the\nnames of PAPI counters and adds their respective values to the trace\nfiles (See Section :ref:`tracing_tracing_options`).\n\n.. warning::\n\n   This feature currently requires superuser privileges, as registers\n   are queried.  Only use this feature with code you trust! Call\n   smpirun for instance via ``smpirun -wrapper \"sudo \"\n   <your-parameters>`` or run ``sudo sh -c \"echo 0 >\n   /proc/sys/kernel/perf_event_paranoid\"`` In the later case, sudo\n   will not be required.\n\nIt is planned to make this feature available on a per-process (or per-thread?) basis.\nThe first draft, however, just implements a \"global\" (i.e., for all processes) set\nof counters, the \"default\" set.\n\n.. code-block:: shell\n\n   --cfg=smpi/papi-events:\"default:PAPI_L3_LDM:PAPI_L2_LDM\"\n\n.. _cfg=smpi/privatization:\n\nAutomatic Privatization of Global Variables\n...........................................\n\n**Option** ``smpi/privatization`` **default:** \"dlopen\" (when using smpirun)\n\nMPI executables are usually meant to be executed in separate\nprocesses, but SMPI is executed in only one process. Global variables\nfrom executables will be placed in the same memory region and shared\nbetween processes, causing intricate bugs.  Several options are\npossible to avoid this, as described in the main `SMPI publication\n<https://hal.inria.fr/hal-01415484>`_ and in the :ref:`SMPI\ndocumentation <SMPI_what_globals>`. SimGrid provides two ways of\nautomatically privatizing the globals, and this option allows one to\nchoose between them.\n\n  - **no** (default when not using smpirun): Do not automatically\n    privatize variables.  Pass ``-no-privatize`` to smpirun to disable\n    this feature.\n  - **dlopen** or **yes** (default when using smpirun): Link multiple\n    times against the binary.\n  - **mmap** (slower, but maybe somewhat more stable):\n    Runtime automatic switching of the data segments.\n\n.. warning::\n   This configuration option cannot be set in your platform file. You can only\n   pass it as an argument to smpirun.\n\n.. _cfg=smpi/privatize-libs:\n\nAutomatic privatization of global variables inside external libraries\n.....................................................................\n\n**Option** ``smpi/privatize-libs`` **default:** unset\n\n**Linux/BSD only:** When using dlopen (default) privatization,\nprivatize specific shared libraries with internal global variables, if\nthey can't be linked statically.  For example libgfortran is usually\nused for Fortran I/O and indexes in files can be mixed up.\n\nMultiple libraries can be given, semicolon separated.\n\nThis configuration option can only use either full paths to libraries,\nor full names.  Check with ldd the name of the library you want to\nuse.  For example:\n\n.. code-block:: shell\n\n   ldd allpairf90\n      ...\n      libgfortran.so.3 => /usr/lib/x86_64-linux-gnu/libgfortran.so.3 (0x00007fbb4d91b000)\n      ...\n\nThen you can use ``--cfg=smpi/privatize-libs:libgfortran.so.3``\nor ``--cfg=smpi/privatize-libs:/usr/lib/x86_64-linux-gnu/libgfortran.so.3``,\nbut not ``libgfortran`` nor ``libgfortran.so``.\n\n.. _cfg=smpi/send-is-detached-thresh:\n\nSimulating MPI detached send\n............................\n\n**Option** ``smpi/send-is-detached-thresh`` **default:** 65536\n\nThis threshold specifies the size in bytes under which the send will\nreturn immediately. This is different from the threshold detailed in\n:ref:`cfg=smpi/async-small-thresh` because the message is not\nreally sent when the send is posted. SMPI still waits for the\ncorresponding receive to be posted, in order to perform the communication\noperation.\n\n.. _cfg=smpi/coll-selector:\n\nSimulating MPI collective algorithms\n....................................\n\n**Option** ``smpi/coll-selector`` **Possible values:** naive (default), ompi, mpich\n\nSMPI implements more than 100 different algorithms for MPI collective\ncommunication, to accurately simulate the behavior of most of the\nexisting MPI libraries. The ``smpi/coll-selector`` item can be used to\nselect the decision logic either of the OpenMPI or the MPICH libraries. (By\ndefault SMPI uses naive version of collective operations.)\n\nEach collective operation can be manually selected with a\n``smpi/collective_name:algo_name``. Available algorithms are listed in\n:ref:`SMPI_use_colls`.\n\n.. TODO:: All available collective algorithms will be made available\n          via the ``smpirun --help-coll`` command.\n\n.. _cfg=smpi/iprobe:\n\nInject constant times for MPI_Iprobe\n....................................\n\n**Option** ``smpi/iprobe`` **default:** 0.0001\n\nThe behavior and motivation for this configuration option is identical\nwith :ref:`smpi/test <cfg=smpi/test>`, but for the function\n``MPI_Iprobe()``\n\n.. _cfg=smpi/iprobe-cpu-usage:\n\nReduce speed for iprobe calls\n.............................\n\n**Option** ``smpi/iprobe-cpu-usage`` **default:** 1 (no change)\n\nMPI_Iprobe calls can be heavily used in applications. To account\ncorrectly for the energy that cores spend probing, it is necessary to\nreduce the load that these calls cause inside SimGrid.\n\nFor instance, we measured a maximum power consumption of 220 W for a\nparticular application but only 180 W while this application was\nprobing. Hence, the correct factor that should be passed to this\noption would be 180/220 = 0.81.\n\n.. _cfg=smpi/init:\n\nInject constant times for MPI_Init\n..................................\n\n**Option** ``smpi/init`` **default:** 0\n\nThe behavior and motivation for this configuration option is identical\nwith :ref:`smpi/test <cfg=smpi/test>`, but for the function ``MPI_Init()``.\n\n.. _cfg=smpi/ois:\n\nInject constant times for MPI_Isend()\n.....................................\n\n**Option** ``smpi/ois``\n\nThe behavior and motivation for this configuration option is identical\nwith :ref:`smpi/os <cfg=smpi/os>`, but for the function ``MPI_Isend()``.\n\n.. _cfg=smpi/os:\n\nInject constant times for MPI_send()\n....................................\n\n**Option** ``smpi/os``\n\nIn several network models such as LogP, send (MPI_Send, MPI_Isend) and\nreceive (MPI_Recv) operations incur costs (i.e., they consume CPU\ntime). SMPI can factor these costs in as well, but the user has to\nconfigure SMPI accordingly as these values may vary by machine.  This\ncan be done by using ``smpi/os`` for MPI_Send operations; for MPI_Isend\nand MPI_Recv, use ``smpi/ois`` and ``smpi/or``, respectively. These work\nexactly as ``smpi/ois``.\n\nThis item can consist of multiple sections; each section takes three\nvalues, for example ``1:3:2;10:5:1``.  The sections are divided by \";\"\nso this example contains two sections.  Furthermore, each section\nconsists of three values.\n\n1. The first value denotes the minimum size in bytes for this section to take effect;\n   read it as \"if message size is greater than this value (and other section has a larger\n   first value that is also smaller than the message size), use this\".\n   In the first section above, this value is \"1\".\n\n2. The second value is the startup time; this is a constant value that will always\n   be charged, no matter what the size of the message. In the first section above,\n   this value is \"3\".\n\n3. The third value is the `per-byte` cost. That is, it is charged for every\n   byte of the message (incurring cost messageSize*cost_per_byte)\n   and hence accounts also for larger messages. In the first\n   section of the example above, this value is \"2\".\n\nNow, SMPI always checks which section it should use for a given\nmessage; that is, if a message of size 11 is sent with the\nconfiguration of the example above, only the second section will be\nused, not the first, as the first value of the second section is\ncloser to the message size. Hence, when ``smpi/os=1:3:2;10:5:1``, a\nmessage of size 11 incurs the following cost inside MPI_Send:\n``5+11*1`` because 5 is the startup cost and 1 is the cost per byte.\n\nNote that the order of sections can be arbitrary; they will be ordered internally.\n\n.. _cfg=smpi/or:\n\nInject constant times for MPI_Recv()\n....................................\n\n**Option** ``smpi/or``\n\nThe behavior and motivation for this configuration option is identical\nwith :ref:`smpi/os <cfg=smpi/os>`, but for the function ``MPI_Recv()``.\n\n.. _cfg=smpi/test:\n.. _cfg=smpi/grow-injected-times:\n\nInject constant times for MPI_Test\n..................................\n\n**Option** ``smpi/test`` **default:** 0.0001\n\nBy setting this option, you can control the amount of time a process\nsleeps when MPI_Test() is called; this is important, because SimGrid\nnormally only advances the time while communication is happening and\nthus, MPI_Test will not add to the time, resulting in deadlock if it is\nused as a break-condition as in the following example:\n\n.. code-block:: cpp\n\n   while(!flag) {\n       MPI_Test(request, flag, status);\n       ...\n   }\n\nTo speed up execution, we use a counter to keep track of how often we\nchecked if the handle is now valid or not. Hence, we actually\nuse counter*SLEEP_TIME, that is, the time MPI_Test() causes the\nprocess to sleep increases linearly with the number of previously\nfailed tests. This behavior can be disabled by setting\n``smpi/grow-injected-times`` to **no**. This will also disable this\nbehavior for MPI_Iprobe.\n\n.. _cfg=smpi/shared-malloc:\n.. _cfg=smpi/shared-malloc-hugepage:\n\nFactorize malloc()s\n...................\n\n**Option** ``smpi/shared-malloc`` **Possible values:** global (default), local\n\nIf your simulation consumes too much memory, you may want to modify\nyour code so that the working areas are shared by all MPI ranks. For\nexample, in a block-cyclic matrix multiplication, you will only\nallocate one set of blocks, and all processes will share them.\nNaturally, this will lead to very wrong results, but this will save a\nlot of memory. So this is still desirable for some studies. For more on\nthe motivation for that feature, please refer to the `relevant section\n<https://simgrid.github.io/SMPI_CourseWare/topic_understanding_performance/matrixmultiplication>`_\nof the SMPI CourseWare (see Activity #2.2 of the pointed\nassignment). In practice, change the calls for malloc() and free() into\nSMPI_SHARED_MALLOC() and SMPI_SHARED_FREE().\n\nSMPI provides two algorithms for this feature. The first one, called \n``local``, allocates one block per call to SMPI_SHARED_MALLOC()\n(each call site gets its own block) ,and this block is shared\namong all MPI ranks.  This is implemented with the shm_* functions\nto create a new POSIX shared memory object (kept in RAM, in /dev/shm)\nfor each shared block.\n\nWith the ``global`` algorithm, each call to SMPI_SHARED_MALLOC()\nreturns a new address, but it only points to a shadow block: its memory\narea is mapped on a 1 MiB file on disk. If the returned block is of size\nN MiB, then the same file is mapped N times to cover the whole block.\nAt the end, no matter how many times you call SMPI_SHARED_MALLOC, this will\nonly consume 1 MiB in memory.\n\nYou can disable this behavior and come back to regular mallocs (for\nexample for debugging purposes) using ``no`` as a value.\n\nIf you want to keep private some parts of the buffer, for instance if these\nparts are used by the application logic and should not be corrupted, you\ncan use SMPI_PARTIAL_SHARED_MALLOC(size, offsets, offsets_count). For example:\n\n.. code-block:: cpp\n\n   mem = SMPI_PARTIAL_SHARED_MALLOC(500, {27,42 , 100,200}, 2);\n\nThis will allocate 500 bytes to mem, such that mem[27..41] and\nmem[100..199] are shared while other area remain private.\n\nThen, it can be deallocated by calling SMPI_SHARED_FREE(mem).\n\nWhen smpi/shared-malloc:global is used, the memory consumption problem\nis solved, but it may induce too much load on the kernel's pages table.\nIn this case, you should use huge pages so that the kernel creates only one\nentry per MB of malloced data instead of one entry per 4 kB.\nTo activate this, you must mount a hugetlbfs on your system and allocate\nat least one huge page:\n\n.. code-block:: shell\n\n    mkdir /home/huge\n    sudo mount none /home/huge -t hugetlbfs -o rw,mode=0777\n    sudo sh -c 'echo 1 > /proc/sys/vm/nr_hugepages' # echo more if you need more\n\nThen, you can pass the option\n``--cfg=smpi/shared-malloc-hugepage:/home/huge`` to smpirun to\nactually activate the huge page support in shared mallocs.\n\n.. _cfg=smpi/wtime:\n\nInject constant times for MPI_Wtime, gettimeofday and clock_gettime\n...................................................................\n\n**Option** ``smpi/wtime`` **default:** 10 ns\n\nThis option controls the amount of (simulated) time spent in calls to\nMPI_Wtime(), gettimeofday() and clock_gettime(). If you set this value\nto 0, the simulated clock is not advanced in these calls, which leads\nto issues if your application contains such a loop:\n\n.. code-block:: cpp\n\n   while(MPI_Wtime() < some_time_bound) {\n        /* some tests, with no communication nor computation */\n   }\n\nWhen the option smpi/wtime is set to 0, the time advances only on\ncommunications and computations. So the previous code results in an\ninfinite loop: the current [simulated] time will never reach\n``some_time_bound``.  This infinite loop is avoided when that option\nis set to a small value, as it is by default since SimGrid v3.21.\n\nNote that if your application does not contain any loop depending on\nthe current time only, then setting this option to a non-zero value\nwill slow down your simulations by a tiny bit: the simulation loop has\nto be broken out of and reset each time your code asks for the current time.\nIf the simulation speed really matters to you, you can avoid this\nextra delay by setting smpi/wtime to 0.\n\nOther Configurations\n--------------------\n\n.. _cfg=debug/clean-atexit:\n\nCleanup at Termination\n......................\n\n**Option** ``debug/clean-atexit`` **default:** on\n\nIf your code is segfaulting during its finalization, it may help to\ndisable this option to request that SimGrid not attempt any cleanups at\nthe end of the simulation. Since the Unix process is ending anyway,\nthe operating system will wipe it all.\n\n.. _cfg=path:\n\nSearch Path\n...........\n\n**Option** ``path`` **default:** . (current dir)\n\nIt is possible to specify a list of directories to search in for the\ntrace files (see :ref:`pf_trace`) by using this configuration\nitem. To add several directory to the path, set the configuration\nitem several times, as in ``--cfg=path:toto --cfg=path:tutu``\n\n.. _cfg=debug/breakpoint:\n\nSet a Breakpoint\n................\n\n**Option** ``debug/breakpoint`` **default:** unset\n\nThis configuration option sets a breakpoint: when the simulated clock\nreaches the given time, a SIGTRAP is raised.  This can be used to stop\nthe execution and get a backtrace with a debugger.\n\nIt is also possible to set the breakpoint from inside the debugger, by\nwriting in global variable simgrid::simix::breakpoint. For example,\nwith gdb:\n\n.. code-block:: shell\n\n   set variable simgrid::simix::breakpoint = 3.1416\n\n.. _cfg=debug/verbose-exit:\n\nBehavior on Ctrl-C\n..................\n\n**Option** ``debug/verbose-exit`` **default:** on\n\nBy default, when Ctrl-C is pressed, the status of all existing actors\nis displayed before exiting the simulation. This is very useful to\ndebug your code, but it can become troublesome if you have many\nactors. Set this configuration item to **off** to disable this\nfeature.\n\n.. _cfg=exception/cutpath:\n\nTruncate local path from exception backtrace\n............................................\n\n**Option** ``exception/cutpath`` **default:** off\n\nThis configuration option is used to remove the path from the\nbacktrace shown when an exception is thrown. This is mainly useful for\nthe tests: the full file path would makes the tests non-reproducible because\nthe paths of source files depend of the build settings. That would\nbreak most of the tests since their output is continually compared.\n\nLogging Configuration\n---------------------\n\nThis can be done by using XBT. Go to :ref:`XBT_log` for more details.\n\n.. |br| raw:: html\n\n   <br />\n"
    },
    "skipped": [
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/.git/objects/pack/pack-7425f7583187787cc0d723a82f605cceea73c235.idx",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/.git/objects/pack/pack-7425f7583187787cc0d723a82f605cceea73c235.pack",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/tuto_s4u/img/Rscript-screenshot.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/tuto_s4u/img/vite-screenshot.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/tuto_s4u/img/result.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/img/smpi_simgrid_alltoall_pair_16.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/img/smpi_simgrid_alltoall_ring_16.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/img/zone_hierarchy.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/img/eclipseScreenShot.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/img/extlink.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/tuto_smpi/3hosts.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/docs/source/tuto_smpi/img/lu.S.4.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/Paje_MSG_screenshot_thn.jpg",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/storage_sample_scenario.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/output.goal.pdf",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/Paje_MSG_screenshot.jpg",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/SGicon.gif",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/eclipseScreenShot.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/SGicon.icns",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/SGicon.ico",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/awstats_logo3.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/simgrid_logo_2011.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/simgrid_logo_2011.gif",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/poster_thumbnail.png",
        "/tmp/vanessa/spack-stage/spack-stage-simgrid-git-hvh7hm26fuqxfx4ottnhkgshkz2gm6yf/spack-src/doc/webcruft/simgrid_logo_2011_small.png"
    ],
    "total_files": 2772
}