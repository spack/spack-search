{
    "matches": {
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/doxygen/community.doc": "/**\n@page community The SimGrid Community\n\n@tableofcontents\n\nSimGrid is a free software, written by a community of people. It\nstarted as a little software to help ourselves in our own research,\nand as more people put their input into the pot, it turned into\nsomething that we hope to be valuable to many people. So yes. We hope\nthat SimGrid is helping you doing what you want, and that you will\njoin our community of happy simgriders.\n\n@section community_contact Contacting the community\n\nThere are several locations where you can connect and discuss about\nSimGrid. If you have a question, please have a look at the\ndocumentation and examples first, but if some remain don't hesitate to\nask the community for help. If you do not have a question, just come to us\nand say hello! We love earing about how people use SimGrid.\n\n - For questions or remarks, drop us an email on the\n   <a href=\"mailto:simgrid-user@lists.gforge.inria.fr\">User Mailing list</a> \n   (to subscribe, visit the [web interface](http://lists.gforge.inria.fr/mailman/listinfo/simgrid-user));\n   you can also check out [our archives](http://lists.gforge.inria.fr/pipermail/simgrid-user/).\n   We prefer you to <b>not use private emails</b>. SimGrid is an open\n   framework, and you never know who have the time and knowledge to\n   answer your question, so please keep messages on the public mailing list.\n - Join us on IRC and ask your question directly on the channel \\#simgrid at\n   \\b irc.debian.org (or use the ugly [web interface](https://webchat.oftc.net/?channels=%23simgrid)\n   if you don't have a [real client](https://en.wikipedia.org/wiki/Comparison_of_Internet_Relay_Chat_clients)\n   installed).<br>\n   Be warned that even if many people are connected to\n   the chanel, they may not be staring at their IRC windows.\n   So don't be surprised if you don't get an answer in the \n   second, and turn to the mailing lists if nobody seems to be there.\n - Asking your question on [StackOverflow](http://stackoverflow.com/questions/tagged/simgrid) is also a good idea, as this\n   site is very well indexed. We answer questions there too (don't\n   forget to use the SimGrid tag in your question so that we can see\n   it), and they remain usable for the next users. \n\n@section community_giveback Giving back to SimGrid\n\nWe are sometimes asked by users how to give back to the project. Here\nare some ideas, but if you have new ones, feel free to share them with us.\n\n@subsection contributing_spread Spread the word\n\nThere are many ways to help the SimGrid project. The first and most\nnatural one is to <b>use it for your research, and say so</b>. Cite\nthe SimGrid framework in your papers and discuss of its advantages with\nyour colleagues to spread the word. When we ask for new fundings to\nsustain the project, the amount of publications enabled by SimGrid is\nalways the first question we get. The more you use the framework,\nthe better for us. \n\nMake sure that your scientific publications using SimGrid actually\ncite the [right paper](http://simgrid.gforge.inria.fr/Publications.html).\nAlso make sure that these citations are correctly listed on \n<a href=\"http://simgrid.gforge.inria.fr/Usages.html\">our list</a>.\n\nYou can also <b>help us constituting an active and welcoming user\ncommunity</b>. Subscribe to the mailing lists, and answer the\nquestions that newscomers have if you can. Point them (gently ;) to\nthe relevant part of the documentation on need, and help them becoming\npart of our community too. \n\nAnother easy way to help the project is to add a link to the <a\nhref=\"http://simgrid.gforge.inria.fr\">SimGrid homepage</a> on your\nsite to <b>improve SimGrid's ranking in search engines</b>.\n\nFinally, if you organize a scientific event where you expect many\npotential users, <b>you can invite us to give a tutorial on SimGrid</b>. We\nfound that 45 minutes to one hour is very sharp, but doable. It\nallows us to explain the main motivations and outcomes of the project in\norder to motivate the attendees get more information on SimGrid, and\neventually improve their scientific habits by using a sound simulation\nframework.  \n<a href=\"http://people.irisa.fr/Martin.Quinson/blog/2012/1120/Simgrid_at_Louvain/\">Here</a>\nis an example of such a presentation.\n\n@subsection contributing_bugs Reporting (and fixing) any issue you find\n\nBecause of its size and complexity, SimGrid is not perfect and\ncontains a large amount of glitches and issues. When you find one,\ndon't assume that it's here because we don't care. It survived only\nbecause nobody told us. We unfortunately cannot endlessly review our\nlarge code and documentation base. So please, <b>report any issue you\nfind</b>, be it a typo in the documentation, a paragraph that needs to\nbe reworded, a bug in the code, or any other problem. The best way to\ndo so is to open an issue on our GitHub's \n<a href=\"https://github.com/simgrid/simgrid/issues\">Bug Tracker</a> so\nthat we don't forget about it (if you want to put some attachment, you\ncan use \n[this other bugtracker](https://gforge.inria.fr/tracker/?atid=165&group_id=12&func=browse)\ninstead). \n\nThe worst way to report such issues is to go through private emails.\nThese are unreliable, and we are trying to develop SimGrid openly, so\nprivate discussions are to be avoided if possible. \n\nIf you can provide a patch fixing the issue you report, that's even\nbetter. If you cannot, then you need to give us a minimal working\nexample (MWE), that is a ready to use solution that reproduces the\nproblem you face. Your bug will take much more time\nfor us to reproduce and fix if you don't give us the MWE, so you want\nto help us helping you to get things efficient.\n\nOf course, a very good way to give back to the SimGrid community is to\n<b>triage and fix the bugs in the Bug Tracking Systems</b>. If you\ncan come up with a patch fixing them, we will be more than happy to\napply your changes so that the whole community enjoys them.\n\n@section community_extend Extending SimGrid and its Ecosystem\n\n@subsection contributing_contrib Contributing features and associated tools\n\nIf you deeply miss a feature in the framework, you should consider\nimplementing it yourself. SimGrid is free software, meaning that you are\nfree to help yourself. Of course, we'll do our best to assist you in\nthis task, so don't hesitate to contact us with your idea.\n\nYou could write a new plugin extending SimGrid in some way, or a\nrouting model for another kind of network. But even if you write your own\nplatform file, this is probably interesting to other users too, and\ncould be included to SimGrid. Modeling accurately a given platform is\na difficult work, which outcome is very precious to us.\n\nOr maybe you developed an independent tool on top of SimGrid. We'd\nlove helping you gaining visibility by listing it in our \n<a href=\"http://simgrid.gforge.inria.fr/contrib.html\">Contrib\nsection</a>. \n\n@subsection contributing_todo Possible Enhancements\n\nIf you want to start working on the SimGrid codebase, here are a few\nideas of things that could be done to improve the current code (not all of them\nare difficult, do trust yourself ;)\n\n@subsubsection contributing_todo_cxxification Migration to C++\n\nThe code is being migrated to C++ but a large part is still C (or C++ with\nC idioms). It would be valuable to replace C idioms with C++ ones:\n\n - replace XBT structures and C dynamic arrays with C++ containers;\n\n - replace `char*` strings with `std::string`;\n\n - use exception-safe RAII (`std::unique_ptr`, etc.) instead of explicit\n   `malloc/free` or `new/delete`;\n\n - use `std::function` (or template functionoid arguments) instead of function\n   pointers;\n\n#### Exceptions\n\nSimGrid used to implement exceptions in C. This has been replaced with C++\nexceptions but some bits of the C exceptions are still remaining:\n\n - `xbt_ex` was the type of C exceptions. It is now a standard C++ exception.\n    We might want to remove this exception and use a more idiomatic C++\n    solution with dedicated exception classes for different errors.\n    `std::system_error` might be used as well by replacing some `xbt_errcat_t`\n    with custom subclasses of `std::error_category`.\n\n - The C API currently throws exceptions. Throwing exceptions out of a C API is\n   not very friendly. C code does not expect them, cannot catch them and cannot\n   handle resource management properly in face of exceptions. We should clearly\n   separate the C++ API and the C API and catch all exceptions before they get\n   ouf of C APIs.\n\n#### Time and duration\n\nSome support for C++11-style time/duration is implemented (see `chrono.hpp`)\nbut only available in some (S4U) APIs. It would be nice to add support for\nthem in the rest of the C++ code.\n\nA related change would be to avoid using \"-1\" to mean \"forever\" at least in S4U\nand in the internal code. For compatibility, MSG should probably keep this\nsemantic. We should probably always use separate functions\n(`wait` vs `wait_for`).\n\n#### Futures and Promises\n\n - Some features are missing in the Maestro future implementation\n  (`simgrid::kernel::Future`, `simgrid::kernel::Promise`)\n  could be extended to support additional features:\n  `when_any`, `shared_future`, etc.\n\n - The corresponding feature might then be implemented in the user process\n   futures (`simgrid::simix::Future`).\n\n - Currently `.then()` is not available for user futures. We would need to add\n   a basic user event loop in order to queue the pending continuations.\n\n - We might need to provide an option to cancel a pending operation. This\n   might be achieved by defining some `Action` or `Operation` class with an\n   API compatible with `Future` (and convertible to it) but with an\n   additional `.cancel()` method.\n\n@subsubsection contributing_todo_smpi SMPI\n\n#### Process-based privatization\n\nCurrently, all the simulated processes live in the same process as the SimGrid\nsimulator. The benefit is that we don't have to do context switches and IPC\nbetween the simulator and the processes.\n\nThe fact that they share the same address space means that one memory corruption\nin one simulated process can propagate to the other ones and to the SimGrid\nsimulator itself.\n\nMoreover, the current design for SMPI applications is to compile the MPI code\nnormally and execute it once per simulated process in the same system process:\nThis means that all the existing simulated MPI processes share the same virtual\naddress space and share by default the same global variables. This is not\ncorrect as each MPI process is expected to use its own address space and have\nits own global variables. In order to fix, this problem we have an optional\nSMPI privatization feature which creates an instanciation of the executable\ndata segment per MPI process and map the correct one (using `mmap`) at each\ncontext switch.\n\nThis approach has many problems:\n\n 1. It is not completely safe. We only handle SMPI privatization for the global\n    variables in the execute data segment. Shared objects are ignored but some\n    may contain global variables which may need to be privatized:\n\n    - libsimgrid for example must not be privatized because it contains\n      shared state for the simulator;\n\n    - libc must not be privatized for the same reason (but some global variables\n      in the libc may not be privatized);\n\n    - if we use global variables of some shared object in the executable, this\n      global variable will be instanciated in the executable (because of copy\n      relocation) and will be privatized even if it shoud not.\n\n 2. We cannot execute the MPI processes in parallel. Only one can execute at\n    the same time because only one privatization segment can be mapped at a\n    given time.\n\nIn order to fix this, the standard solution is to move each MPI process in its\nown system process and use IPC to communicate with the simulator. One concern would\nbe the impact on performance and memory consumption:\n\n - It would introduce a lot of context switches and IPC communications between\n   the MPI processes and the SimGrid simulator. However, currently every context\n   switch needs a `mmap` for SMPI privatization which is costly as well\n   (TLB flush).\n\n - Instanciating a lot of processes might consume more memory which might be a\n   problem if we want to simulate a lot of MPI processes. Compiling MPI programs\n   as static executables with a lightweight libc might help and we might want to\n   support that. The SMPI processes should probably not embed all the SimGrid\n   simulator and its dependencies, the C++ runtime, etc.\n\nWe would need to modify the model-checker as well which currently can only\nmanage on model-checked process. For the model-checker we can expect some\nbenefits from this approach: if a process did not execute, we know its state\ndid not change and we don't need to take its snapshot and compare its state.\n\nOther solutions for this might include:\n\n - Mapping each MPI process in the process of the simulator but in a different\n   symbol namespace (see `dlmopen`). Each process would have its own separate\n   instanciation and would not share libraries.\n\n - Instanciate each MPI process in a separate lightweight VM (for example based\n   on WebAssembly) in the simualtor process.\n\n@subsubsection contributing_todo_mc Model-checker\n\n#### Overhaul the state comparison code\n\nThe state comparison code is quite complicated. It has very long functions and\nis programmed mostly using C idioms and is difficult to understand and debug.\nIt is in need of an overhaul:\n\n  - cleanup, refactoring, usage of C++ features.\n\n  - The state comparison code works by infering types of blocks allocated on the\n    heap by following pointers from known roots (global variables, local\n    variables). Usually the first type found for a given block is used even if\n    a better one could be found later. By using a first pass of type inference,\n    on each snapshot before comparing the states, we might use a better type\n    information on the different blocks.\n\n  - We might benefit from adding logic for handling some known types. For\n    example, both `std::string` and `std::vector` have a capacity which might\n    be larger than the current size of the container. We should ignore\n    the corresponding elements when comparing the states and infering the types.\n\n  - Another difficulty in the state comparison code is the detection of\n    dangling pointers. We cannot easily know if a pointer is dangling and\n    dangling pointers might lead us to choose the wrong type when infering\n    heap blocks. We might mitigate this problem by delaying the reallocation of\n    a freed block until there is no blocks pointing to it anymore using some\n    sort of basic garbage-collector.\n\n#### Hashing the states\n\nIn order to speed up the state comparison an idea was to create a hash of the\nstate. Only states with the same hash would need to be compared using the\nstate comparison algorithm. Some information should not be inclueded in the\nhash in order to avoid considering different states which would otherwise\nwould have been considered equal.\n\nThe states could be indexed by their hash. Currently they are indexed\nby the number of processes and the amount of heap currently allocated\n(see `DerefAndCompareByNbProcessesAndUsedHeap`).\n\nGood candidate informations for the state hashing:\n\n - number of processes;\n\n - their backtraces (instruction addresses);\n\n - their current simcall numbers;\n\n - some simcall arguments (eg. number of elements in a waitany);\n\n - number of pending communications;\n\n - etc.\n\nSome basic infrastructure for this is already in the code (see `mc_hash.cpp`)\nbut it is currently disabled.\n\n#### Separate the model-checker code from libsimgrid\n\n#### Interface with the model-checked processes\n\nThe model-checker reads many information about the model-checked process by\n`process_vm_readv()`-ing brutally the data structure of the model-checked\nprocess leading to some inefficient code such as maintaining copies of complex\nC++ structures in XBT dynars. We need a sane way to expose the relevant\ninformation to the model-checker.\n\n#### Generic simcalls\n\nWe have introduced some generic simcalls which can be used to execute a\ncallback in SimGrid Maestro context. It makes it a lot easier to interface\nthe simulated process with the maestro. However, the callbacks for the\nmodel-checker which cannot decide how it should handle them. We would need a\nsolution for this if we want to be able to replace the simcalls the\nmodel-checker cares about by generic simcalls.\n\n#### Defining an API for writing Model-Checking algorithms\n\nCurrently, writing a new model-checking algorithms in SimGridMC is quite\ndifficult: the logic of the model-checking algorithm is mixed with a lot of\nlow-level concerns about the way the model-checker is implemented. This makes it\ndifficult to write new algorithms and difficult to understand, debug, and modify\nthe existing ones. We need a clean API to express the model-checking algorithms\nin a form which is closer to the text-book/paper description. This API must\nbe exposed in a a language which is more adequate to this task.\n\nTasks:\n\n  1. Design and implement a clean API to express model-checking algorithms.\n     A `Session` class currently exists for this but is not feature complete\n     and should probably be rewritten. It should be easy to create bindings\n     for different languages on top of this API.\n\n  2. Create a binding to some better suited, dynamic, scripting language\n     (e.g., Lua).\n\n  3. Rewrite the existing model-checking algorithms in this language using the\n     new API.\n\n*/",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/app_smpi.rst": ".. _SMPI_doc:\n\n===============================\nSMPI: Simulate MPI Applications\n===============================\n\n.. raw:: html\n\n   <object id=\"TOC\" data=\"graphical-toc.svg\" width=\"100%\" type=\"image/svg+xml\"></object>\n   <script>\n   window.onload=function() { // Wait for the SVG to be loaded before changing it\n     var elem=document.querySelector(\"#TOC\").contentDocument.getElementById(\"SMPIBox\")\n     elem.style=\"opacity:0.93999999;fill:#ff0000;fill-opacity:0.1\";\n   }\n   </script>\n   <br/>\n   <br/>\n\nSMPI enables the study of MPI application by emulating them on top of\nthe SimGrid simulator. This is particularly interesting to study\nexisting MPI applications within the comfort of the simulator.\n\nTo get started with SMPI, you should head to `the SMPI tutorial\n<usecase_smpi>`_. You may also want to read the `SMPI reference\narticle <https://hal.inria.fr/hal-01415484>`_ or these `introductory\nslides <http://simgrid.org/tutorials/simgrid-smpi-101.pdf>`_.  If you\nare new to MPI, you should first take our online `SMPI CourseWare\n<https://simgrid.github.io/SMPI_CourseWare/>`_. It consists in several\nprojects that progressively introduce the MPI concepts. It proposes to\nuse SimGrid and SMPI to run the experiments, but the learning\nobjectives are centered on MPI itself.\n\nOur goal is to enable the study of **unmodified MPI applications**.\nSome constructs and features are still missing, but we can probably\nadd them on demand.  If you already used MPI before, SMPI should sound\nvery familiar to you: Use smpicc instead of mpicc, and smpirun instead\nof mpirun. The main difference is that smpirun takes a :ref:`simulated\nplatform <platform>` as an extra parameter.\n\nFor **further scalability**, you may modify your code to speed up your\nstudies or save memory space.  Maximal **simulation accuracy**\nrequires some specific care from you.\n\n----------\nUsing SMPI\n----------\n\n...................\nCompiling your Code\n...................\n\nIf your application is in C, then simply use ``smpicc`` as a\ncompiler just like you use mpicc with other MPI implementations. This\nscript still calls your default compiler (gcc, clang, ...) and adds\nthe right compilation flags along the way. If your application is in\nC++, Fortran 77 or Fortran 90, use respectively ``smpicxx``,\n``smpiff`` or ``smpif90``.\n\n....................\nSimulating your Code\n....................\n\nUse the ``smpirun`` script as follows:\n\n.. code-block:: shell\n\n   smpirun -hostfile my_hostfile.txt -platform my_platform.xml ./program -blah\n\n- ``my_hostfile.txt`` is a classical MPI hostfile (that is, this file\n  lists the machines on which the processes must be dispatched, one\n  per line)\n- ``my_platform.xml`` is a classical SimGrid platform file. Of course,\n  the hosts of the hostfile must exist in the provided platform.\n- ``./program`` is the MPI program to simulate, that you compiled with ``smpicc``\n- ``-blah`` is a command-line parameter passed to this program.\n\n``smpirun`` accepts other parameters, such as ``-np`` if you don't\nwant to use all the hosts defined in the hostfile, ``-map`` to display\non which host each rank gets mapped of ``-trace`` to activate the\ntracing during the simulation. You can get the full list by running\n``smpirun -help``\n\n...............................\nDebugging your Code within SMPI\n...............................\n\nIf you want to explore the automatic platform and deployment files\nthat are generated by ``smpirun``, add ``-keep-temps`` to the command\nline.\n\nYou can also run your simulation within valgrind or gdb using the\nfollowing commands. Once in GDB, each MPI ranks will be represented as\na regular thread, and you can explore the state of each of them as\nusual.\n\n.. code-block:: shell\n\n   smpirun -wrapper valgrind ...other args...\n   smpirun -wrapper \"gdb --args\" --cfg=contexts/factory:thread ...other args...\n\n.. _SMPI_use_colls:\n\n................................   \nSimulating Collective Operations\n................................\n\nMPI collective operations are crucial to the performance of MPI\napplications and must be carefully optimized according to many\nparameters. Every existing implementation provides several algorithms\nfor each collective operation, and selects by default the best suited\none, depending on the sizes sent, the number of nodes, the\ncommunicator, or the communication library being used.  These\ndecisions are based on empirical results and theoretical complexity\nestimation, and are very different between MPI implementations. In\nmost cases, the users can also manually tune the algorithm used for\neach collective operation.\n\nSMPI can simulate the behavior of several MPI implementations:\nOpenMPI, MPICH, `STAR-MPI <http://star-mpi.sourceforge.net/>`_, and\nMVAPICH2. For that, it provides 115 collective algorithms and several\nselector algorithms, that were collected directly in the source code\nof the targeted MPI implementations.\n\nYou can switch the automatic selector through the\n``smpi/coll-selector`` configuration item. Possible values:\n\n - **ompi:** default selection logic of OpenMPI (version 3.1.2)\n - **mpich**: default selection logic of MPICH (version 3.3b)\n - **mvapich2**: selection logic of MVAPICH2 (version 1.9) tuned\n   on the Stampede cluster   \n - **impi**: preliminary version of an Intel MPI selector (version\n   4.1.3, also tuned for the Stampede cluster). Due the closed source\n   nature of Intel MPI, some of the algorithms described in the\n   documentation are not available, and are replaced by mvapich ones.   \n - **default**: legacy algorithms used in the earlier days of\n   SimGrid. Do not use for serious perform performance studies.\n\n.. todo:: default should not even exist.   \n\n....................\nAvailable Algorithms\n....................\n\nYou can also pick the algorithm used for each collective with the\ncorresponding configuration item. For example, to use the pairwise\nalltoall algorithm, one should add ``--cfg=smpi/alltoall:pair`` to the\nline. This will override the selector (if any) for this algorithm.  It\nmeans that the selected algorithm will be used\n\n.. Warning:: Some collective may require specific conditions to be\n   executed correctly (for instance having a communicator with a power\n   of two number of nodes only), which are currently not enforced by\n   Simgrid. Some crashes can be expected while trying these algorithms\n   with unusual sizes/parameters\n\nMPI_Alltoall\n^^^^^^^^^^^^\n\nMost of these are best described in `STAR-MPI <http://www.cs.arizona.edu/~dkl/research/papers/ics06.pdf>`_.\n\n - default: naive one, by default\n - ompi: use openmpi selector for the alltoall operations\n - mpich: use mpich selector for the alltoall operations\n - mvapich2: use mvapich2 selector for the alltoall operations\n - impi: use intel mpi selector for the alltoall operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - bruck: Described by Bruck et.al. in <a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=642949\">this paper</a>\n - 2dmesh: organizes the nodes as a two dimensional mesh, and perform allgather \n   along the dimensions\n - 3dmesh: adds a third dimension to the previous algorithm\n - rdb: recursive doubling: extends the mesh to a nth dimension, each one \n   containing two nodes\n - pair: pairwise exchange, only works for power of 2 procs, size-1 steps,\n   each process sends and receives from the same process at each step\n - pair_light_barrier: same, with small barriers between steps to avoid\n   contention\n - pair_mpi_barrier: same, with MPI_Barrier used\n - pair_one_barrier: only one barrier at the beginning\n - ring: size-1 steps, at each step a process send to process (n+i)%size, and receives from (n-i)%size\n - ring_light_barrier: same, with small barriers between some phases to avoid contention\n - ring_mpi_barrier: same, with MPI_Barrier used\n - ring_one_barrier: only one barrier at the beginning\n - basic_linear: posts all receives and all sends,\n   starts the communications, and waits for all communication to finish\n - mvapich2_scatter_dest: isend/irecv with scattered destinations, posting only a few messages at the same time\n\nMPI_Alltoallv\n^^^^^^^^^^^^^\n - default: naive one, by default\n - ompi: use openmpi selector for the alltoallv operations\n - mpich: use mpich selector for the alltoallv operations\n - mvapich2: use mvapich2 selector for the alltoallv operations\n - impi: use intel mpi selector for the alltoallv operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - bruck: same as alltoall\n - pair: same as alltoall\n - pair_light_barrier: same as alltoall\n - pair_mpi_barrier: same as alltoall\n - pair_one_barrier: same as alltoall\n - ring: same as alltoall\n - ring_light_barrier: same as alltoall\n - ring_mpi_barrier: same as alltoall\n - ring_one_barrier: same as alltoall\n - ompi_basic_linear: same as alltoall\n\nMPI_Gather\n^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the gather operations\n - mpich: use mpich selector for the gather operations\n - mvapich2: use mvapich2 selector for the gather operations\n - impi: use intel mpi selector for the gather operations\n - automatic (experimental): use an automatic self-benchmarking algorithm which will iterate over all implemented versions and output the best\n - ompi_basic_linear: basic linear algorithm from openmpi, each process sends to the root\n - ompi_binomial: binomial tree algorithm\n - ompi_linear_sync: same as basic linear, but with a synchronization at the\n   beginning and message cut into two segments.\n - mvapich2_two_level: SMP-aware version from MVAPICH. Gather first intra-node (defaults to mpich's gather), and then exchange with only one process/node. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster.\n\nMPI_Barrier\n^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the barrier operations\n - mpich: use mpich selector for the barrier operations\n - mvapich2: use mvapich2 selector for the barrier operations\n - impi: use intel mpi selector for the barrier operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - ompi_basic_linear: all processes send to root\n - ompi_two_procs: special case for two processes\n - ompi_bruck: nsteps = sqrt(size), at each step, exchange data with rank-2^k and rank+2^k\n - ompi_recursivedoubling: recursive doubling algorithm\n - ompi_tree: recursive doubling type algorithm, with tree structure\n - ompi_doublering: double ring algorithm\n - mvapich2_pair: pairwise algorithm\n - mpich_smp: barrier intra-node, then inter-node\n\nMPI_Scatter\n^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the scatter operations\n - mpich: use mpich selector for the scatter operations\n - mvapich2: use mvapich2 selector for the scatter operations\n - impi: use intel mpi selector for the scatter operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - ompi_basic_linear: basic linear scatter \n - ompi_binomial: binomial tree scatter\n - mvapich2_two_level_direct: SMP aware algorithm, with an intra-node stage (default set to mpich selector), and then a basic linear inter node stage. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster. \n - mvapich2_two_level_binomial: SMP aware algorithm, with an intra-node stage (default set to mpich selector), and then a binomial phase. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster.\n\nMPI_Reduce\n^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the reduce operations\n - mpich: use mpich selector for the reduce operations\n - mvapich2: use mvapich2 selector for the reduce operations\n - impi: use intel mpi selector for the reduce operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - arrival_pattern_aware: root exchanges with the first process to arrive\n - binomial: uses a binomial tree\n - flat_tree: uses a flat tree\n - NTSL: Non-topology-specific pipelined linear-bcast function \n   0->1, 1->2 ,2->3, ....., ->last node: in a pipeline fashion, with segments\n   of 8192 bytes\n - scatter_gather: scatter then gather\n - ompi_chain: openmpi reduce algorithms are built on the same basis, but the\n   topology is generated differently for each flavor\n   chain = chain with spacing of size/2, and segment size of 64KB \n - ompi_pipeline: same with pipeline (chain with spacing of 1), segment size \n   depends on the communicator size and the message size\n - ompi_binary: same with binary tree, segment size of 32KB\n - ompi_in_order_binary: same with binary tree, enforcing order on the \n   operations\n - ompi_binomial: same with binomial algo (redundant with default binomial \n   one in most cases)\n - ompi_basic_linear: basic algorithm, each process sends to root\n - mvapich2_knomial: k-nomial algorithm. Default factor is 4 (mvapich2 selector adapts it through tuning)\n - mvapich2_two_level: SMP-aware reduce, with default set to mpich both for intra and inter communicators. Use mvapich2 selector to change these to tuned algorithms for Stampede cluster.\n - rab: `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_'s reduce algorithm \n\nMPI_Allreduce\n^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the allreduce operations\n - mpich: use mpich selector for the allreduce operations\n - mvapich2: use mvapich2 selector for the allreduce operations\n - impi: use intel mpi selector for the allreduce operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - lr: logical ring reduce-scatter then logical ring allgather\n - rab1: variations of the  <a href=\"https://fs.hlrs.de/projects/par/mpi//myreduce.html\">Rabenseifner</a> algorithm: reduce_scatter then allgather\n - rab2: variations of the  <a href=\"https://fs.hlrs.de/projects/par/mpi//myreduce.html\">Rabenseifner</a> algorithm: alltoall then allgather\n - rab_rsag: variation of the  <a href=\"https://fs.hlrs.de/projects/par/mpi//myreduce.html\">Rabenseifner</a> algorithm: recursive doubling \n   reduce_scatter then recursive doubling allgather \n - rdb: recursive doubling\n - smp_binomial: binomial tree with smp: binomial intra \n   SMP reduce, inter reduce, inter broadcast then intra broadcast\n - smp_binomial_pipeline: same with segment size = 4096 bytes\n - smp_rdb: intra: binomial allreduce, inter: Recursive \n   doubling allreduce, intra: binomial broadcast\n - smp_rsag: intra: binomial allreduce, inter: reduce-scatter, \n   inter:allgather, intra: binomial broadcast\n - smp_rsag_lr: intra: binomial allreduce, inter: logical ring \n   reduce-scatter, logical ring inter:allgather, intra: binomial broadcast\n - smp_rsag_rab: intra: binomial allreduce, inter: rab\n   reduce-scatter, rab inter:allgather, intra: binomial broadcast\n - redbcast: reduce then broadcast, using default or tuned algorithms if specified\n - ompi_ring_segmented: ring algorithm used by OpenMPI\n - mvapich2_rs: rdb for small messages, reduce-scatter then allgather else\n - mvapich2_two_level: SMP-aware algorithm, with mpich as intra algoritm, and rdb as inter (Change this behavior by using mvapich2 selector to use tuned values)\n - rab: default `Rabenseifner <https://fs.hlrs.de/projects/par/mpi//myreduce.html>`_ implementation\n\nMPI_Reduce_scatter\n^^^^^^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the reduce_scatter operations\n - mpich: use mpich selector for the reduce_scatter operations\n - mvapich2: use mvapich2 selector for the reduce_scatter operations\n - impi: use intel mpi selector for the reduce_scatter operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - ompi_basic_recursivehalving: recursive halving version from OpenMPI\n - ompi_ring: ring version from OpenMPI\n - mpich_pair: pairwise exchange version from MPICH\n - mpich_rdb: recursive doubling version from MPICH\n - mpich_noncomm: only works for power of 2 procs, recursive doubling for noncommutative ops\n\n\nMPI_Allgather\n^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the allgather operations\n - mpich: use mpich selector for the allgather operations\n - mvapich2: use mvapich2 selector for the allgather operations\n - impi: use intel mpi selector for the allgather operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - 2dmesh: see alltoall\n - 3dmesh: see alltoall\n - bruck: Described by Bruck et.al. in <a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=642949\">\n   Efficient algorithms for all-to-all communications in multiport message-passing systems</a> \n - GB: Gather - Broadcast (uses tuned version if specified)\n - loosely_lr: Logical Ring with grouping by core (hardcoded, default \n   processes/node: 4)\n - NTSLR: Non Topology Specific Logical Ring\n - NTSLR_NB: Non Topology Specific Logical Ring, Non Blocking operations\n - pair: see alltoall\n - rdb: see alltoall\n - rhv: only power of 2 number of processes\n - ring: see alltoall\n - SMP_NTS: gather to root of each SMP, then every root of each SMP node \n   post INTER-SMP Sendrecv, then do INTRA-SMP Bcast for each receiving message, \n   using logical ring algorithm (hardcoded, default processes/SMP: 8)\n - smp_simple: gather to root of each SMP, then every root of each SMP node \n   post INTER-SMP Sendrecv, then do INTRA-SMP Bcast for each receiving message, \n   using simple algorithm (hardcoded, default processes/SMP: 8)\n - spreading_simple: from node i, order of communications is i -> i + 1, i ->\n   i + 2, ..., i -> (i + p -1) % P\n - ompi_neighborexchange: Neighbor Exchange algorithm for allgather. \n   Described by Chen et.al. in  `Performance Evaluation of Allgather\n   Algorithms on Terascale Linux Cluster with Fast Ethernet <http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1592302>`_\n - mvapich2_smp: SMP aware algorithm, performing intra-node gather, inter-node allgather with one process/node, and bcast intra-node\n\nMPI_Allgatherv\n^^^^^^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the allgatherv operations\n - mpich: use mpich selector for the allgatherv operations\n - mvapich2: use mvapich2 selector for the allgatherv operations\n - impi: use intel mpi selector for the allgatherv operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - GB: Gatherv - Broadcast (uses tuned version if specified, but only for Bcast, gatherv is not tuned)\n - pair: see alltoall\n - ring: see alltoall\n - ompi_neighborexchange: see allgather\n - ompi_bruck: see allgather\n - mpich_rdb: recursive doubling algorithm from MPICH\n - mpich_ring: ring algorithm from MPICh - performs differently from the  one from STAR-MPI\n\nMPI_Bcast\n^^^^^^^^^\n\n - default: naive one, by default\n - ompi: use openmpi selector for the bcast operations\n - mpich: use mpich selector for the bcast operations\n - mvapich2: use mvapich2 selector for the bcast operations\n - impi: use intel mpi selector for the bcast operations\n - automatic (experimental): use an automatic self-benchmarking algorithm \n - arrival_pattern_aware: root exchanges with the first process to arrive\n - arrival_pattern_aware_wait: same with slight variation\n - binomial_tree: binomial tree exchange\n - flattree: flat tree exchange\n - flattree_pipeline: flat tree exchange, message split into 8192 bytes pieces\n - NTSB: Non-topology-specific pipelined binary tree with 8192 bytes pieces\n - NTSL: Non-topology-specific pipelined linear with 8192 bytes pieces\n - NTSL_Isend: Non-topology-specific pipelined linear with 8192 bytes pieces, asynchronous communications\n - scatter_LR_allgather: scatter followed by logical ring allgather\n - scatter_rdb_allgather: scatter followed by recursive doubling allgather\n - arrival_scatter: arrival pattern aware scatter-allgather\n - SMP_binary: binary tree algorithm with 8 cores/SMP\n - SMP_binomial: binomial tree algorithm with 8 cores/SMP\n - SMP_linear: linear algorithm with 8 cores/SMP\n - ompi_split_bintree: binary tree algorithm from OpenMPI, with message split in 8192 bytes pieces\n - ompi_pipeline: pipeline algorithm from OpenMPI, with message split in 128KB pieces\n - mvapich2_inter_node: Inter node default mvapich worker \n - mvapich2_intra_node: Intra node default mvapich worker\n - mvapich2_knomial_intra_node:  k-nomial intra node default mvapich worker. default factor is 4.\n\nAutomatic Evaluation\n^^^^^^^^^^^^^^^^^^^^\n\n.. warning:: This is still very experimental.\n\nAn automatic version is available for each collective (or even as a selector). This specific \nversion will loop over all other implemented algorithm for this particular collective, and apply \nthem while benchmarking the time taken for each process. It will then output the quickest for \neach process, and the global quickest. This is still unstable, and a few algorithms which need \nspecific number of nodes may crash.\n\nAdding an algorithm\n^^^^^^^^^^^^^^^^^^^\n\nTo add a new algorithm, one should check in the src/smpi/colls folder\nhow other algorithms are coded. Using plain MPI code inside Simgrid\ncan't be done, so algorithms have to be changed to use smpi version of\nthe calls instead (MPI_Send will become smpi_mpi_send). Some functions\nmay have different signatures than their MPI counterpart, please check\nthe other algorithms or contact us using the `>SimGrid\ndevelopers mailing list <http://lists.gforge.inria.fr/mailman/listinfo/simgrid-devel>`_.\n\nExample: adding a \"pair\" version of the Alltoall collective.\n\n - Implement it in a file called alltoall-pair.c in the src/smpi/colls folder. This file should include colls_private.hpp.\n\n - The name of the new algorithm function should be smpi_coll_tuned_alltoall_pair, with the same signature as MPI_Alltoall.\n\n - Once the adaptation to SMPI code is done, add a reference to the file (\"src/smpi/colls/alltoall-pair.c\") in the SMPI_SRC part of the DefinePackages.cmake file inside buildtools/cmake, to allow the file to be built and distributed.\n\n - To register the new version of the algorithm, simply add a line to the corresponding macro in src/smpi/colls/cools.h ( add a \"COLL_APPLY(action, COLL_ALLTOALL_SIG, pair)\" to the COLL_ALLTOALLS macro ). The algorithm should now be compiled and be selected when using --cfg=smpi/alltoall:pair at runtime.\n\n - To add a test for the algorithm inside Simgrid's test suite, juste add the new algorithm name in the ALLTOALL_COLL list found inside teshsuite/smpi/CMakeLists.txt . When running ctest, a test for the new algorithm should be generated and executed. If it does not pass, please check your code or contact us.\n\n - Please submit your patch for inclusion in SMPI, for example through a pull request on GitHub or directly per email.\n\n\nTracing of Internal Communications\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, the collective operations are traced as a unique operation\nbecause tracing all point-to-point communications composing them could\nresult in overloaded, hard to interpret traces. If you want to debug\nand compare collective algorithms, you should set the\n``tracing/smpi/internals`` configuration item to 1 instead of 0.\n\nHere are examples of two alltoall collective algorithms runs on 16 nodes, \nthe first one with a ring algorithm, the second with a pairwise one.\n\n.. image:: /img/smpi_simgrid_alltoall_ring_16.png\n   :align: center\n\t   \nAlltoall on 16 Nodes with the Ring Algorithm.\n\n.. image:: /img/smpi_simgrid_alltoall_pair_16.png\n   :align: center\n\t   \nAlltoall on 16 Nodes with the Pairwise Algorithm.\n\n-------------------------\nWhat can run within SMPI?\n-------------------------\n\nYou can run unmodified MPI applications (both C/C++ and Fortran) within\nSMPI, provided that you only use MPI calls that we implemented. Global\nvariables should be handled correctly on Linux systems.\n\n....................\nMPI coverage of SMPI\n....................\n\nOur coverage of the interface is very decent, but still incomplete;\nGiven the size of the MPI standard, we may well never manage to \nimplement absolutely all existing primitives. Currently, we have\nalmost no support for I/O primitives, but we still pass a very large\namount of the MPICH coverage tests.\n\nThe full list of not yet implemented functions is documented in the\nfile `include/smpi/smpi.h\n<https://framagit.org/simgrid/simgrid/tree/master/include/smpi/smpi.h>`_\nin your version of SimGrid, between two lines containing the ``FIXME``\nmarker. If you really miss a feature, please get in touch with us: we\ncan guide you though the SimGrid code to help you implementing it, and\nwe'd be glad to integrate your contribution to the main project.\n\n.. _SMPI_what_globals:\n\n.................................\nPrivatization of global variables\n.................................\n\nConcerning the globals, the problem comes from the fact that usually,\nMPI processes run as real UNIX processes while they are all folded\ninto threads of a unique system process in SMPI. Global variables are\nusually private to each MPI process while they become shared between\nthe processes in SMPI.  The problem and some potential solutions are\ndiscussed in this article: `Automatic Handling of Global Variables for\nMulti-threaded MPI Programs\n<http://charm.cs.illinois.edu/newPapers/11-23/paper.pdf>` (note that\nthis article does not deal with SMPI but with a competing solution\ncalled AMPI that suffers of the same issue).  This point used to be\nproblematic in SimGrid, but the problem should now be handled\nautomatically on Linux.\n\nOlder versions of SimGrid came with a script that automatically\nprivatized the globals through static analysis of the source code. But\nour implementation was not robust enough to be used in production, so\nit was removed at some point. Currently, SMPI comes with two\nprivatization mechanisms that you can :ref:`select at runtime\n<options_smpi_privatization>`_.  The dlopen approach is used by\ndefault as it is much faster and still very robust.  The mmap approach\nis an older approach that proves to be slower.\n\nWith the **mmap approach**, SMPI duplicates and dynamically switch the\n``.data`` and ``.bss`` segments of the ELF process when switching the\nMPI ranks. This allows each ranks to have its own copy of the global\nvariables.  No copy actually occures as this mechanism uses ``mmap()``\nfor efficiency. This mechanism is considered to be very robust on all\nsystems supporting ``mmap()`` (Linux and most BSDs). Its performance\nis questionable since each context switch between MPI ranks induces\nseveral syscalls to change the ``mmap`` that redirects the ``.data``\nand ``.bss`` segments to the copies of the new rank. The code will\nalso be copied several times in memory, inducing a slight increase of\nmemory occupation.\n\nAnother limitation is that SMPI only accounts for global variables\ndefined in the executable. If the processes use external global\nvariables from dynamic libraries, they won't be switched\ncorrectly. The easiest way to solve this is to statically link against\nthe library with these globals. This way, each MPI rank will get its\nown copy of these libraries. Of course you should never statically\nlink against the SimGrid library itself.\n\nWith the **dlopen approach**, SMPI loads several copies of the same\nexecutable in memory as if it were a library, so that the global\nvariables get naturally dupplicated. It first requires the executable\nto be compiled as a relocatable binary, which is less common for\nprograms than for libraries. But most distributions are now compiled\nthis way for security reason as it allows to randomize the address\nspace layout. It should thus be safe to compile most (any?) program\nthis way.  The second trick is that the dynamic linker refuses to link\nthe exact same file several times, be it a library or a relocatable\nexecutable. It makes perfectly sense in the general case, but we need\nto circumvent this rule of thumb in our case. To that extend, the\nbinary is copied in a temporary file before being re-linked against.\n``dlmopen()`` cannot be used as it only allows 256 contextes, and as it\nwould also dupplicate simgrid itself.\n\nThis approach greatly speeds up the context switching, down to about\n40 CPU cycles with our raw contextes, instead of requesting several\nsyscalls with the ``mmap()`` approach. Another advantage is that it\npermits to run the SMPI contexts in parallel, which is obviously not\npossible with the ``mmap()`` approach. It was tricky to implement, but\nwe are not aware of any flaws, so smpirun activates it by default.\n\nIn the future, it may be possible to further reduce the memory and\ndisk consumption. It seems that we could `punch holes\n<https://lwn.net/Articles/415889/>`_ in the files before dl-loading\nthem to remove the code and constants, and mmap these area onto a\nunique copy. If done correctly, this would reduce the disk- and\nmemory- usage to the bare minimum, and would also reduce the pressure\non the CPU instruction cache. See the `relevant bug\n<https://github.com/simgrid/simgrid/issues/137>`_ on github for\nimplementation leads.\\n\n\nAlso, currently, only the binary is copied and dlopen-ed for each MPI\nrank. We could probably extend this to external dependencies, but for\nnow, any external dependencies must be statically linked into your\napplication. As usual, simgrid itself shall never be statically linked\nin your app. You don't want to give a copy of SimGrid to each MPI rank:\nthat's ways too much for them to deal with.\n\n.. todo: speak of smpi/privatize-libs here\n\n----------------------------------------------\nAdapting your MPI code for further scalability\n----------------------------------------------\n\nAs detailed in the `reference article\n<http://hal.inria.fr/hal-01415484>`_, you may want to adapt your code\nto improve the simulation performance. But these tricks may seriously\nhinder the result quality (or even prevent the app to run) if used\nwrongly. We assume that if you want to simulate an HPC application,\nyou know what you are doing. Don't prove us wrong!\n\n..............................\nReducing your memory footprint\n..............................\n\nIf you get short on memory (the whole app is executed on a single node when\nsimulated), you should have a look at the SMPI_SHARED_MALLOC and\nSMPI_SHARED_FREE macros. It allows to share memory areas between processes: The\npurpose of these macro is that the same line malloc on each process will point\nto the exact same memory area. So if you have a malloc of 2M and you have 16\nprocesses, this macro will change your memory consumption from 2M*16 to 2M\nonly. Only one block for all processes.\n\nIf your program is ok with a block containing garbage value because all\nprocesses write and read to the same place without any kind of coordination,\nthen this macro can dramatically shrink your memory consumption. For example,\nthat will be very beneficial to a matrix multiplication code, as all blocks will\nbe stored on the same area. Of course, the resulting computations will useless,\nbut you can still study the application behavior this way. \n\nNaturally, this won't work if your code is data-dependent. For example, a Jacobi\niterative computation depends on the result computed by the code to detect\nconvergence conditions, so turning them into garbage by sharing the same memory\narea between processes does not seem very wise. You cannot use the\nSMPI_SHARED_MALLOC macro in this case, sorry.\n\nThis feature is demoed by the example file\n`examples/smpi/NAS/dt.c <https://framagit.org/simgrid/simgrid/tree/master/examples/smpi/NAS/dt.c>`_\n\n.........................\nToward Faster Simulations\n.........................\n\nIf your application is too slow, try using SMPI_SAMPLE_LOCAL,\nSMPI_SAMPLE_GLOBAL and friends to indicate which computation loops can\nbe sampled. Some of the loop iterations will be executed to measure\ntheir duration, and this duration will be used for the subsequent\niterations. These samples are done per processor with\nSMPI_SAMPLE_LOCAL, and shared between all processors with\nSMPI_SAMPLE_GLOBAL. Of course, none of this will work if the execution\ntime of your loop iteration are not stable.\n\nThis feature is demoed by the example file \n`examples/smpi/NAS/ep.c <https://framagit.org/simgrid/simgrid/tree/master/examples/smpi/NAS/ep.c>`_\n\n.............................\nEnsuring Accurate Simulations\n.............................\n\nOut of the box, SimGrid may give you fairly accurate results, but\nthere is a plenty of factors that could go wrong and make your results\ninaccurate or even plainly wrong. Actually, you can only get accurate\nresults of a nicely built model, including both the system hardware\nand your application. Such models are hard to pass over and reuse in\nother settings, because elements that are not relevant to an\napplication (say, the latency of point-to-point communications,\ncollective operation implementation details or CPU-network\ninteraction) may be irrelevant to another application. The dream of\nthe perfect model, encompassing every aspects is only a chimera, as\nthe only perfect model of the reality is the reality. If you go for\nsimulation, then you have to ignore some irrelevant aspects of the\nreality, but which aspects are irrelevant is actually\napplication-dependent...\n\nThe only way to assess whether your settings provide accurate results\nis to double-check these results. If possible, you should first run\nthe same experiment in simulation and in real life, gathering as much\ninformation as you can. Try to understand the discrepancies in the\nresults that you observe between both settings (visualization can be\nprecious for that). Then, try to modify your model (of the platform,\nof the collective operations) to reduce the most preeminent differences.\n\nIf the discrepancies come from the computing time, try adapting the \n``smpi/host-speed``: reduce it if your simulation runs faster than in\nreality. If the error come from the communication, then you need to\nfiddle with your platform file.\n\nBe inventive in your modeling. Don't be afraid if the names given by\nSimGrid does not match the real names: we got very good results by\nmodeling multicore/GPU machines with a set of separate hosts\ninterconnected with very fast networks (but don't trust your model\nbecause it has the right names in the right place either).\n\nFinally, you may want to check `this article\n<https://hal.inria.fr/hal-00907887>`_ on the classical pitfalls in\nmodeling distributed systems.\n\n-------------------------\nTroubleshooting with SMPI\n-------------------------\n\n.................................\n./configure refuses to use smpicc\n.................................\n\nIf your ``./configure`` reports that the compiler is not\nfunctional or that you are cross-compiling, try to define the\n``SMPI_PRETEND_CC`` environment variable before running the\nconfiguration.\n\n.. code-block:: shell\n\n   SMPI_PRETEND_CC=1 ./configure # here come the configure parameters\n   make\n\nIndeed, the programs compiled with ``smpicc`` cannot be executed\nwithout ``smpirun`` (they are shared libraries and do weird things on\nstartup), while configure wants to test them directly.  With\n``SMPI_PRETEND_CC`` smpicc does not compile as shared, and the SMPI\ninitialization stops and returns 0 before doing anything that would\nfail without ``smpirun``.\n\n.. warning::\n\n  Make sure that SMPI_PRETEND_CC is only set when calling ./configure,\n  not during the actual execution, or any program compiled with smpicc\n  will stop before starting.\n\n..............................................\n./configure does not pick smpicc as a compiler\n..............................................\n\nIn addition to the previous answers, some projects also need to be\nexplicitely told what compiler to use, as follows:\n\n.. code-block:: shell\n\t\t\n   SMPI_PRETEND_CC=1 ./configure CC=smpicc # here come the other configure parameters\n   make\n\nMaybe your configure is using another variable, such as ``cc`` (in\nlower case) or similar. Just check the logs.\n\n.....................................\nerror: unknown type name 'useconds_t'\n.....................................\n\nTry to add ``-D_GNU_SOURCE`` to your compilation line to get ride\nof that error.\n\nThe reason is that SMPI provides its own version of ``usleep(3)``\nto override it and to block in the simulation world, not in the real\none. It needs the ``useconds_t`` type for that, which is declared\nonly if you declare ``_GNU_SOURCE`` before including\n``unistd.h``. If your project includes that header file before\nSMPI, then you need to ensure that you pass the right configuration\ndefines as advised above.\n"
    },
    "skipped": [
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/Paje_MSG_screenshot_thn.jpg",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/poster_thumbnail.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/SGicon.ico",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/Paje_MSG_screenshot.jpg",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/simgrid_logo_2011.gif",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/eclipseScreenShot.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/simgrid_logo_2011.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/awstats_logo3.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/AS_hierarchy.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/storage_sample_scenario.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/simgrid_logo_2011_small.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/SGicon.gif",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/output.goal.pdf",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/doc/webcruft/SGicon.icns",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/img/extlink.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/img/smpi_simgrid_alltoall_pair_16.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/img/eclipseScreenShot.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/img/smpi_simgrid_alltoall_ring_16.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/tuto_smpi/3hosts.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/tuto_smpi/img/lu.S.4.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/tuto_s4u/img/Rscript-screenshot.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/tuto_s4u/img/vite-screenshot.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-simgrid-3.21-6b3dtkuodmcsnmvm5ehdup6kmyv3p3xh/spack-src/docs/source/tuto_s4u/img/result.png"
    ],
    "total_files": 2531
}