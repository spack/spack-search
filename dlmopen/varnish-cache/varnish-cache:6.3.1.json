{
    "matches": {
        "/var/tmp/sochat1/spack-stage/spack-stage-varnish-cache-6.3.1-zej3im3nvgsv7vct2elpcepvddrldyg7/spack-src/bin/varnishd/storage/storage_umem.c": "/*-\n * Copyright (c) 2006 Verdens Gang AS\n * Copyright (c) 2006-2011 Varnish Software AS\n * Copyright 2017 UPLEX - Nils Goroll Systemoptimierung\n * All rights reserved.\n *\n * Authors: Poul-Henning Kamp <phk@phk.freebsd.dk>\n *\t    Nils Goroll <nils.goroll@uplex.de>\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED.  IN NO EVENT SHALL AUTHOR OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n *\n * Storage method based on libumem\n */\n\n#include \"config.h\"\n\n#if defined(HAVE_UMEM_H)\n\n#include \"cache/cache_varnishd.h\"\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <umem.h>\n#include <dlfcn.h>\n#include <link.h>\n#include <string.h>\n\n#include \"storage/storage.h\"\n#include \"storage/storage_simple.h\"\n\n#include \"vnum.h\"\n#include \"common/heritage.h\"\n\n#include \"VSC_smu.h\"\n\nstruct smu_sc {\n\tunsigned\t\tmagic;\n#define SMU_SC_MAGIC\t\t0x7695f68e\n\tstruct lock\t\tsmu_mtx;\n\tsize_t\t\t\tsmu_max;\n\tsize_t\t\t\tsmu_alloc;\n\tstruct VSC_smu\t\t*stats;\n\tumem_cache_t\t\t*smu_cache;\n};\n\nstruct smu {\n\tunsigned\t\tmagic;\n#define SMU_MAGIC\t\t0x3773300c\n\tstruct storage\t\ts;\n\tsize_t\t\t\tsz;\n\tstruct smu_sc\t\t*sc;\n};\n\n/*\n * We only want the umem slab allocator for cache storage, not also as a\n * substitute for malloc and friends. So we don't link with libumem, but\n * use dlopen/dlsym to get the slab allocator interface into function\n * pointers.\n */\ntypedef void * (*umem_alloc_f)(size_t size, int flags);\ntypedef void (*umem_free_f)(void *buf, size_t size);\ntypedef umem_cache_t * (*umem_cache_create_f)(char *debug_name, size_t bufsize,\n    size_t align, umem_constructor_t *constructor,\n    umem_destructor_t *destructor, umem_reclaim_t *reclaim,\n    void *callback_data, vmem_t *source, int cflags);\ntypedef void (*umem_cache_destroy_f)(umem_cache_t *cache);\ntypedef void * (*umem_cache_alloc_f)(umem_cache_t *cache, int flags);\ntypedef void (*umem_cache_free_f)(umem_cache_t *cache, void *buffer);\n\nstatic void *libumem_hndl = NULL;\nstatic umem_alloc_f umem_allocf = NULL;\nstatic umem_free_f umem_freef = NULL;\nstatic umem_cache_create_f umem_cache_createf = NULL;\nstatic umem_cache_destroy_f umem_cache_destroyf = NULL;\nstatic umem_cache_alloc_f umem_cache_allocf = NULL;\nstatic umem_cache_free_f umem_cache_freef = NULL;\n\nstatic const char * const def_umem_options = \"perthread_cache=0,backend=mmap\";\nstatic const char * const env_umem_options = \"UMEM_OPTIONS\";\n\n/* init required per cache get:\n   smu->sz = size\n   smu->s.ptr;\n   smu->s.space = size\n*/\n\nstatic inline void\nsmu_smu_init(struct smu *smu, struct smu_sc *sc)\n{\n\tINIT_OBJ(smu, SMU_MAGIC);\n\tsmu->s.magic = STORAGE_MAGIC;\n\tsmu->s.priv = smu;\n\tsmu->sc = sc;\n}\n\nstatic int v_matchproto_(umem_constructor_t)\nsmu_smu_constructor(void *buffer, void *callback_data, int flags)\n{\n\tstruct smu *smu = buffer;\n\tstruct smu_sc *sc;\n\n\t(void) flags;\n\tCAST_OBJ_NOTNULL(sc, callback_data, SMU_SC_MAGIC);\n\tsmu_smu_init(smu, sc);\n\treturn (0);\n}\n\nstatic void v_matchproto_(umem_destructor_t)\n\tsmu_smu_destructor(void *buffer, void *callback_data)\n{\n\tstruct smu *smu;\n\tstruct smu_sc *sc;\n\n\tCAST_OBJ_NOTNULL(smu, buffer, SMU_MAGIC);\n\tCAST_OBJ_NOTNULL(sc, callback_data, SMU_SC_MAGIC);\n\tCHECK_OBJ_NOTNULL(&(smu->s), STORAGE_MAGIC);\n\tassert(smu->s.priv == smu);\n\tassert(smu->sc == sc);\n}\n\nstatic struct VSC_lck *lck_smu;\n\nstatic struct storage * v_matchproto_(sml_alloc_f)\nsmu_alloc(const struct stevedore *st, size_t size)\n{\n\tstruct smu_sc *smu_sc;\n\tstruct smu *smu = NULL;\n\tvoid *p;\n\n\tCAST_OBJ_NOTNULL(smu_sc, st->priv, SMU_SC_MAGIC);\n\tLck_Lock(&smu_sc->smu_mtx);\n\tsmu_sc->stats->c_req++;\n\tif (smu_sc->smu_alloc + size > smu_sc->smu_max) {\n\t\tsmu_sc->stats->c_fail++;\n\t\tsize = 0;\n\t} else {\n\t\tsmu_sc->smu_alloc += size;\n\t\tsmu_sc->stats->c_bytes += size;\n\t\tsmu_sc->stats->g_alloc++;\n\t\tsmu_sc->stats->g_bytes += size;\n\t\tif (smu_sc->smu_max != SIZE_MAX)\n\t\t\tsmu_sc->stats->g_space -= size;\n\t}\n\tLck_Unlock(&smu_sc->smu_mtx);\n\n\tif (size == 0)\n\t\treturn (NULL);\n\n\t/*\n\t * Do not collaps the smu allocation with smu->s.ptr: it is not\n\t * a good idea.  Not only would it make ->trim impossible,\n\t * performance-wise it would be a catastropy with chunksized\n\t * allocations growing another full page, just to accommodate the smu.\n\t */\n\n\tp = umem_allocf(size, UMEM_DEFAULT);\n\tif (p != NULL) {\n\t\tAN(smu_sc->smu_cache);\n\t\tsmu = umem_cache_allocf(smu_sc->smu_cache, UMEM_DEFAULT);\n\t\tif (smu != NULL)\n\t\t\tsmu->s.ptr = p;\n\t\telse\n\t\t\tumem_freef(p, size);\n\t}\n\tif (smu == NULL) {\n\t\tLck_Lock(&smu_sc->smu_mtx);\n\t\t/*\n\t\t * XXX: Not nice to have counters go backwards, but we do\n\t\t * XXX: Not want to pick up the lock twice just for stats.\n\t\t */\n\t\tsmu_sc->stats->c_fail++;\n\t\tsmu_sc->smu_alloc -= size;\n\t\tsmu_sc->stats->c_bytes -= size;\n\t\tsmu_sc->stats->g_alloc--;\n\t\tsmu_sc->stats->g_bytes -= size;\n\t\tif (smu_sc->smu_max != SIZE_MAX)\n\t\t\tsmu_sc->stats->g_space += size;\n\t\tLck_Unlock(&smu_sc->smu_mtx);\n\t\treturn (NULL);\n\t}\n\tsmu->sz = size;\n\tsmu->s.space = size;\n\treturn (&smu->s);\n}\n\nstatic void v_matchproto_(sml_free_f)\nsmu_free(struct storage *s)\n{\n\tstruct smu *smu;\n\tstruct smu_sc *sc;\n\n\tCHECK_OBJ_NOTNULL(s, STORAGE_MAGIC);\n\tCAST_OBJ_NOTNULL(smu, s->priv, SMU_MAGIC);\n\tCAST_OBJ_NOTNULL(sc, smu->sc, SMU_SC_MAGIC);\n\n\tLck_Lock(&sc->smu_mtx);\n\tsc->smu_alloc -= smu->sz;\n\tsc->stats->g_alloc--;\n\tsc->stats->g_bytes -= smu->sz;\n\tsc->stats->c_freed += smu->sz;\n\tif (sc->smu_max != SIZE_MAX)\n\t\tsc->stats->g_space += smu->sz;\n\tLck_Unlock(&sc->smu_mtx);\n\n\tumem_freef(smu->s.ptr, smu->sz);\n\tsmu_smu_init(smu, sc);\n\tumem_cache_freef(sc->smu_cache, smu);\n}\n\nstatic VCL_BYTES v_matchproto_(stv_var_used_space)\nsmu_used_space(const struct stevedore *st)\n{\n\tstruct smu_sc *smu_sc;\n\n\tCAST_OBJ_NOTNULL(smu_sc, st->priv, SMU_SC_MAGIC);\n\treturn (smu_sc->smu_alloc);\n}\n\nstatic VCL_BYTES v_matchproto_(stv_var_free_space)\nsmu_free_space(const struct stevedore *st)\n{\n\tstruct smu_sc *smu_sc;\n\n\tCAST_OBJ_NOTNULL(smu_sc, st->priv, SMU_SC_MAGIC);\n\treturn (smu_sc->smu_max - smu_sc->smu_alloc);\n}\n\nstatic void\nsmu_umem_loaded_warn(void)\n{\n\tconst char *e;\n\tstatic int warned = 0;\n\n\tif (warned++)\n\t\treturn;\n\n\tfprintf(stderr, \"notice:\\tlibumem was already found to be loaded\\n\"\n\t\t\"\\tand will likely be used for all allocations\\n\");\n\n\te = getenv(env_umem_options);\n\tif (e == NULL || ! strstr(e, def_umem_options))\n\t\tfprintf(stderr, \"\\tit is recommended to set %s=%s \"\n\t\t\t\"before starting varnish\\n\",\n\t\t\tenv_umem_options, def_umem_options);\n}\n\nstatic int\nsmu_umem_loaded(void)\n{\n\tvoid *h = NULL;\n\n\th = dlopen(\"libumem.so\", RTLD_NOLOAD);\n\tif (h) {\n\t\tAZ(dlclose(h));\n\t\treturn (1);\n\t}\n\n\th = dlsym(RTLD_DEFAULT, \"umem_alloc\");\n\tif (h)\n\t\treturn (1);\n\n\treturn (0);\n}\n\nstatic void\nsmu_init(struct stevedore *parent, int ac, char * const *av)\n{\n\tstatic int inited = 0;\n\tconst char *e;\n\tuintmax_t u;\n\tstruct smu_sc *sc;\n\n\tASSERT_MGT();\n\tALLOC_OBJ(sc, SMU_SC_MAGIC);\n\tAN(sc);\n\tsc->smu_max = SIZE_MAX;\n\tassert(sc->smu_max == SIZE_MAX);\n\tparent->priv = sc;\n\n\tAZ(av[ac]);\n\tif (ac > 1)\n\t\tARGV_ERR(\"(-sumem) too many arguments\\n\");\n\n\tif (ac == 1 && *av[0] != '\\0') {\n\t\te = VNUM_2bytes(av[0], &u, 0);\n\t\tif (e != NULL)\n\t\t\tARGV_ERR(\"(-sumem) size \\\"%s\\\": %s\\n\", av[0], e);\n\t\tif ((u != (uintmax_t)(size_t)u))\n\t\t\tARGV_ERR(\"(-sumem) size \\\"%s\\\": too big\\n\", av[0]);\n\t\tif (u < 1024*1024)\n\t\t\tARGV_ERR(\"(-sumem) size \\\"%s\\\": too small, \"\n\t\t\t\t \"did you forget to specify M or G?\\n\", av[0]);\n\t\tsc->smu_max = u;\n\t}\n\n\tif (inited++)\n\t\treturn;\n\n\tif (smu_umem_loaded())\n\t\tsmu_umem_loaded_warn();\n\telse\n\t\tAZ(setenv(env_umem_options, def_umem_options, 0));\n\n\t/* Check if these load in the management process. */\n\t(void) dlerror();\n\tlibumem_hndl = dlmopen(LM_ID_NEWLM, \"libumem.so\", RTLD_LAZY);\n\tif (libumem_hndl == NULL)\n\t\tARGV_ERR(\"(-sumem) cannot open libumem.so: %s\", dlerror());\n\n#define DLSYM_UMEM(fptr,sym)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\t(void) dlerror();\t\t\t\t\t\\\n\t\tif (dlsym(libumem_hndl, #sym) == NULL)\t\t\t\\\n\t\t\tARGV_ERR(\"(-sumem) cannot find symbol \"\t\t\\\n\t\t\t\t #sym \": %s\",\t\t\t\t\\\n\t\t\t\t dlerror());\t\t\t\t\\\n\t\tfptr = NULL;\t\t\t\t\t\t\\\n\t} while(0)\n\n\tDLSYM_UMEM(umem_allocf, umem_alloc);\n\tDLSYM_UMEM(umem_freef, umem_free);\n\tDLSYM_UMEM(umem_cache_createf, umem_cache_create);\n\tDLSYM_UMEM(umem_cache_destroyf, umem_cache_destroy);\n\tDLSYM_UMEM(umem_cache_allocf, umem_cache_alloc);\n\tDLSYM_UMEM(umem_cache_freef, umem_cache_free);\n\n#undef DLSYM_UMEM\n\n\tAZ(dlclose(libumem_hndl));\n\tlibumem_hndl = NULL;\n}\n\n/*\n * Load the symbols for use in the child process, assert if they fail to load.\n */\nstatic void\nsmu_open_init(void)\n{\n\tstatic int inited = 0;\n\n\tif (inited++) {\n\t\tAN(libumem_hndl);\n\t\tAN(umem_allocf);\n\t\treturn;\n\t}\n\n\tif (smu_umem_loaded())\n\t\tsmu_umem_loaded_warn();\n\telse\n\t\tAN(getenv(env_umem_options));\n\n\tAZ(libumem_hndl);\n\tlibumem_hndl = dlopen(\"libumem.so\", RTLD_LAZY);\n\tAN(libumem_hndl);\n\n#define DLSYM_UMEM(fptr,sym)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tfptr = (sym ## _f) dlsym(libumem_hndl, #sym);\t\\\n\t\tAN(fptr);\t\t\t\t\t\\\n\t} while(0)\n\n\tDLSYM_UMEM(umem_allocf, umem_alloc);\n\tDLSYM_UMEM(umem_freef, umem_free);\n\tDLSYM_UMEM(umem_cache_createf, umem_cache_create);\n\tDLSYM_UMEM(umem_cache_destroyf, umem_cache_destroy);\n\tDLSYM_UMEM(umem_cache_allocf, umem_cache_alloc);\n\tDLSYM_UMEM(umem_cache_freef, umem_cache_free);\n\n#undef DLSYM_UMEM\n}\n\nstatic void v_matchproto_(storage_open_f)\nsmu_open(struct stevedore *st)\n{\n\tstruct smu_sc *smu_sc;\n\tchar ident[strlen(st->ident) + 1];\n\n\tASSERT_CLI();\n\tst->lru = LRU_Alloc();\n\tif (lck_smu == NULL)\n\t\tlck_smu = Lck_CreateClass(NULL, \"smu\");\n\tCAST_OBJ_NOTNULL(smu_sc, st->priv, SMU_SC_MAGIC);\n\tLck_New(&smu_sc->smu_mtx, lck_smu);\n\tsmu_sc->stats = VSC_smu_New(NULL, NULL, st->ident);\n\tif (smu_sc->smu_max != SIZE_MAX)\n\t\tsmu_sc->stats->g_space = smu_sc->smu_max;\n\n\tsmu_open_init();\n\n\tAN(strcpy(ident, st->ident));\n\tsmu_sc->smu_cache = umem_cache_createf(ident,\n\t\t\t\t\t  sizeof(struct smu),\n\t\t\t\t\t  0,\t\t// align\n\t\t\t\t\t  smu_smu_constructor,\n\t\t\t\t\t  smu_smu_destructor,\n\t\t\t\t\t  NULL,\t\t// reclaim\n\t\t\t\t\t  smu_sc,\t// callback_data\n\t\t\t\t\t  NULL,\t\t// source\n\t\t\t\t\t  0\t\t// cflags\n\t\t);\n\tAN(smu_sc->smu_cache);\n}\n\nstatic void v_matchproto_(storage_close_f)\nsmu_close(const struct stevedore *st, int warn)\n{\n\tstruct smu_sc *smu_sc;\n\n\tASSERT_CLI();\n\n\tCAST_OBJ_NOTNULL(smu_sc, st->priv, SMU_SC_MAGIC);\n\tif (warn)\n\t\treturn;\n\tumem_cache_destroyf(smu_sc->smu_cache);\n\tsmu_sc->smu_cache = NULL;\n\n\t/*\n\t   XXX TODO?\n\t   - LRU_Free\n\t   - Lck Destroy\n\t*/\n}\n\nconst struct stevedore smu_stevedore = {\n\t.magic\t\t=\tSTEVEDORE_MAGIC,\n\t.name\t\t=\t\"umem\",\n\t.init\t\t=\tsmu_init,\n\t.open\t\t=\tsmu_open,\n\t.close\t\t=\tsmu_close,\n\t.sml_alloc\t=\tsmu_alloc,\n\t.sml_free\t=\tsmu_free,\n\t.allocobj\t=\tSML_allocobj,\n\t.panic\t\t=\tSML_panic,\n\t.methods\t=\t&SML_methods,\n\t.var_free_space =\tsmu_free_space,\n\t.var_used_space =\tsmu_used_space,\n};\n\n#endif /* HAVE_UMEM_H */\n"
    },
    "skipped": [
        "/var/tmp/sochat1/spack-stage/spack-stage-varnish-cache-6.3.1-zej3im3nvgsv7vct2elpcepvddrldyg7/spack-src/doc/sphinx/phk/bjarne.jpeg"
    ],
    "total_files": 1480
}