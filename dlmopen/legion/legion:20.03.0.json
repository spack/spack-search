{
    "matches": {
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/runtime/realm/python/python_module.cc": "/* Copyright 2020 Stanford University, NVIDIA Corporation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include \"realm/python/python_module.h\"\n#include \"realm/python/python_internal.h\"\n\n#include \"realm/numa/numasysif.h\"\n#include \"realm/logging.h\"\n#include \"realm/cmdline.h\"\n#include \"realm/proc_impl.h\"\n#include \"realm/threads.h\"\n#include \"realm/runtime_impl.h\"\n#include \"realm/utils.h\"\n\n#include <dlfcn.h>\n#ifdef REALM_USE_DLMOPEN\n#include <link.h>\n#endif // REALM_USE_DLMOPEN\n\n#include <list>\n\nnamespace Realm {\n\n  Logger log_py(\"python\");\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonAPI\n\n  PythonAPI::PythonAPI(void *_handle)\n    : handle(_handle)\n  {\n    get_symbol(this->Py_DecRef, \"Py_DecRef\");\n    get_symbol(this->Py_Finalize, \"Py_Finalize\");\n    get_symbol(this->Py_InitializeEx, \"Py_InitializeEx\");\n\n    get_symbol(this->PyByteArray_FromStringAndSize, \"PyByteArray_FromStringAndSize\");\n\n    get_symbol(this->PyEval_InitThreads, \"PyEval_InitThreads\");\n\n#ifdef USE_PYGILSTATE_CALLS\n    get_symbol(this->PyGILState_Ensure, \"PyGILState_Ensure\");\n    get_symbol(this->PyGILState_Release, \"PyGILState_Release\");\n#else\n    get_symbol(this->PyThreadState_New, \"PyThreadState_New\");\n    get_symbol(this->PyThreadState_Clear, \"PyThreadState_Clear\");\n    get_symbol(this->PyThreadState_Delete, \"PyThreadState_Delete\");\n    get_symbol(this->PyThreadState_Get, \"PyThreadState_Get\");\n#endif\n    get_symbol(this->PyEval_RestoreThread, \"PyEval_RestoreThread\");\n    get_symbol(this->PyEval_SaveThread, \"PyEval_SaveThread\");\n\n    get_symbol(this->PyThreadState_Swap, \"PyThreadState_Swap\");\n\n    get_symbol(this->PyErr_PrintEx, \"PyErr_PrintEx\");\n\n    get_symbol(this->PyImport_ImportModule, \"PyImport_ImportModule\");\n    get_symbol(this->PyModule_GetDict, \"PyModule_GetDict\");\n\n    get_symbol(this->PyLong_FromUnsignedLong, \"PyLong_FromUnsignedLong\");\n\n    get_symbol(this->PyObject_CallFunction, \"PyObject_CallFunction\");\n    get_symbol(this->PyObject_CallObject, \"PyObject_CallObject\");\n    get_symbol(this->PyObject_GetAttrString, \"PyObject_GetAttrString\");\n    get_symbol(this->PyObject_Print, \"PyObject_Print\");\n\n    get_symbol(this->PyRun_SimpleString, \"PyRun_SimpleString\");\n    get_symbol(this->PyRun_String, \"PyRun_String\");\n\n    get_symbol(this->PyTuple_New, \"PyTuple_New\");\n    get_symbol(this->PyTuple_SetItem, \"PyTuple_SetItem\");\n  }\n\n  template<typename T>\n  void PythonAPI::get_symbol(T &fn, const char *symbol,\n                             bool missing_ok /*= false*/)\n  {\n    fn = reinterpret_cast<T>(dlsym(handle, symbol));\n    if(!fn && !missing_ok) {\n      const char *error = dlerror();\n      log_py.fatal() << \"failed to find symbol '\" << symbol << \"': \" << error;\n      assert(false);\n    }\n  }\n\n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonInterpreter\n\n#ifdef REALM_USE_DLMOPEN\n  // dlmproxy symbol lookups have to happen in a function we define so that\n  //  dl[v]sym searches in the right place\n  static void *dlmproxy_lookup(const char *symname, const char *symver)\n  {\n    \n    void *handle = 0;\n    void *sym = (symver ?\n\t\t   dlvsym(handle, symname, symver) :\n\t\t   dlsym(handle, symname));\n    if(sym)\n      log_py.debug() << \"found symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\") << \" ptr=\" << sym;\n    else\n      log_py.warning() << \"missing symbol: name=\" << symname << \" ver=\" << (symver ? symver : \"(none)\");\n    return sym;\n  }\n#endif\n\n  PythonInterpreter::PythonInterpreter() \n  {\n#ifdef REALM_PYTHON_LIB\n    const char *python_lib = REALM_PYTHON_LIB;\n#else\n    const char *python_lib = \"libpython2.7.so\";\n#endif\n\n#ifdef REALM_USE_DLMOPEN\n    // loading libpython into its own namespace will cause it to try to bring\n    //   in a second copy of libpthread.so.0, which is fairly disastrous\n    // we deal with it by loading a \"dlmproxy\" of pthreads that tunnels all \n    //   pthreads calls back to the (only) version in the main executable\n    const char *dlmproxy_filename = getenv(\"DLMPROXY_LIBPTHREAD\");\n    if(!dlmproxy_filename)\n      dlmproxy_filename = \"dlmproxy_libpthread.so.0\";\n    dlmproxy_handle = dlmopen(LM_ID_NEWLM,\n\t\t\t      dlmproxy_filename,\n\t\t\t      RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_LAZY);\n    if(!dlmproxy_handle) {\n      const char *error = dlerror();\n      log_py.fatal() << \"HELP!  Use of dlmopen for python requires dlmproxy for pthreads!  Failed to\\n\"\n\t\t     << \"  load: \" << dlmproxy_filename << \"\\n\"\n\t\t     << \"  error: \" << error;\n      assert(false);\n    }\n\n    // now that the proxy is loaded, we need to tell it where the real\n    //  libpthreads functions are\n    {\n      void *sym = dlsym(dlmproxy_handle, \"dlmproxy_load_symbols\");\n      assert(sym != 0);\n      ((void (*)(void *(*)(const char *, const char *)))sym)(dlmproxy_lookup);\n    }\n\n    // now we can load libpython, but make sure we do it in the new namespace\n    Lmid_t lmid;\n    int ret = dlinfo(dlmproxy_handle, RTLD_DI_LMID, &lmid);\n    assert(ret == 0);\n\n    handle = dlmopen(lmid, python_lib, RTLD_DEEPBIND | RTLD_GLOBAL | RTLD_NOW);\n#else\n    // life is so much easier if we use dlopen (but we only get one copy then)\n    handle = dlopen(python_lib, RTLD_GLOBAL | RTLD_LAZY);\n#endif\n    if (!handle) {\n      const char *error = dlerror();\n      log_py.fatal() << error;\n      assert(false);\n    }\n\n    api = new PythonAPI(handle);\n\n    (api->Py_InitializeEx)(0 /*!initsigs*/);\n    (api->PyEval_InitThreads)();\n    //(api->Py_Finalize)();\n\n    //PyThreadState *state;\n    //state = (api->PyEval_SaveThread)();\n    //(api->PyEval_RestoreThread)(state);\n\n    //(api->PyRun_SimpleString)(\"print 'hello Python world!'\");\n\n    //PythonSourceImplementation psi(\"taskreg_helper\", \"task1\");\n    //find_or_import_function(&psi);\n  }\n\n  PythonInterpreter::~PythonInterpreter()\n  {\n    (api->Py_Finalize)();\n\n    delete api;\n\n    if (dlclose(handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"libpython dlclose error: \" << error;\n      assert(false);\n    }\n\n#ifdef REALM_USE_DLMOPEN\n    if (dlclose(dlmproxy_handle)) {\n      const char *error = dlerror();\n      log_py.fatal() << \"dlmproxy dlclose error: \" << error;\n      assert(false);\n    }\n#endif\n  }\n\n  PyObject *PythonInterpreter::find_or_import_function(const PythonSourceImplementation *psi)\n  {\n    //log_py.print() << \"attempting to acquire python lock\";\n    //(api->PyEval_AcquireLock)();\n    //log_py.print() << \"lock acquired\";\n\n    // not calling PythonInterpreter::import_module here because we want the\n    //  PyObject result\n    log_py.debug() << \"attempting to import module: \" << psi->module_name;\n    PyObject *module = (api->PyImport_ImportModule)(psi->module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << psi->module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(module, stdout, 0); printf(\"\\n\");\n\n    PyObject *function = module;\n    for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n           ie = psi->function_name.end(); function && it != ie; ++it) {\n      function = (api->PyObject_GetAttrString)(function, it->c_str());\n    }\n    if (!function) {\n      {\n        LoggerMessage m = log_py.fatal();\n        m << \"unable to import Python function \";\n        for (std::vector<std::string>::const_iterator it = psi->function_name.begin(),\n               ie = psi->function_name.begin(); it != ie; ++it) {\n          m << *it;\n          if (it + 1 != ie) {\n            m << \".\";\n          }\n        }\n        m << \" from module \" << psi->module_name;\n      }\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    //(api->PyObject_Print)(function, stdout, 0); printf(\"\\n\");\n\n    //(api->PyObject_CallFunction)(function, \"iii\", 1, 2, 3);\n\n    (api->Py_DecRef)(module);\n\n    return function;\n  }\n\n  void PythonInterpreter::import_module(const std::string& module_name)\n  {\n    log_py.debug() << \"attempting to import module: \" << module_name;\n    PyObject *module = (api->PyImport_ImportModule)(module_name.c_str());\n    if (!module) {\n      log_py.fatal() << \"unable to import Python module \" << module_name;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(module);\n  }\n\n  void PythonInterpreter::run_string(const std::string& script_text)\n  {\n    // from Python.h\n    const int Py_file_input = 257;\n\n    log_py.debug() << \"running python string: \" << script_text;\n    PyObject *mainmod = (api->PyImport_ImportModule)(\"__main__\");\n    assert(mainmod != 0);\n    PyObject *globals = (api->PyModule_GetDict)(mainmod);\n    assert(globals != 0);\n    PyObject *res = (api->PyRun_String)(script_text.c_str(),\n\t\t\t\t\tPy_file_input,\n\t\t\t\t\tglobals,\n\t\t\t\t\tglobals);\n    if(!res) {\n      log_py.fatal() << \"unable to run python string:\" << script_text;\n      (api->PyErr_PrintEx)(0);\n      (api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n    (api->Py_DecRef)(res);\n    (api->Py_DecRef)(globals);\n    (api->Py_DecRef)(mainmod);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class PythonThreadTaskScheduler\n\n  PythonThreadTaskScheduler::PythonThreadTaskScheduler(LocalPythonProcessor *_pyproc,\n\t\t\t\t\t\t       CoreReservation& _core_rsrv)\n    : KernelThreadTaskScheduler(_pyproc->me, _core_rsrv)\n    , pyproc(_pyproc)\n    , interpreter_ready(false)\n  {}\n\n  // both real and internal tasks need to be wrapped with acquires of the GIL\n  bool PythonThreadTaskScheduler::execute_task(Task *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    bool ok = KernelThreadTaskScheduler::execute_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n\n    return ok;\n  }\n  \n  void PythonThreadTaskScheduler::execute_internal_task(InternalTask *task)\n  {\n    // make our python thread state active, acquiring the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (pyproc->interpreter->api->PyGILState_Ensure)();\n#else\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << pythread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pythread);\n#endif\n\n    KernelThreadTaskScheduler::execute_internal_task(task);\n\n    // release the GIL\n#ifdef USE_PYGILSTATE_CALLS\n    (pyproc->interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pythread);\n#endif\n  }\n    \n  void PythonThreadTaskScheduler::python_scheduler_loop(void)\n  {\n    // global startup of python interpreter if needed\n    if(!interpreter_ready) {\n      log_py.info() << \"creating interpreter\";\n      pyproc->create_interpreter();\n      interpreter_ready = true;\n    }\n\n#ifdef USE_PYGILSTATE_CALLS\n    // our PyThreadState is implicit when using the PyGILState calls\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = 0;\n#else\n    // always create and remember our own python thread - does NOT require GIL\n    PyThreadState *pythread = (pyproc->interpreter->api->PyThreadState_New)(pyproc->master_thread->interp);\n    log_py.debug() << \"created python thread: \" << pythread;\n    \n    assert(pythread != 0);\n    assert(pythreads.count(Thread::self()) == 0);\n    pythreads[Thread::self()] = pythread;\n#endif\n\n    // take lock and go into normal task scheduler loop\n    {\n      AutoLock<> al(lock);\n      KernelThreadTaskScheduler::scheduler_loop();\n    }\n#if 0\n    // now go into main scheduler loop, holding scheduler lock for whole thing\n    AutoLock<> al(lock);\n    while(true) {\n      // remember the work counter value before we start so that we don't iterate\n      //   unnecessarily\n      long long old_work_counter = work_counter.read_counter();\n\n      // first priority - task registration\n      while(!taskreg_queue.empty()) {\n\tLocalPythonProcessor::TaskRegistration *treg = taskreg_queue.front();\n\ttaskreg_queue.pop_front();\n\t\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\t\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = TaskQueue::PRI_POS_INF;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  pyproc->perform_task_registration(treg);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n      }\n\n      // if we have both resumable and new ready tasks, we want the one that\n      //  is the highest priority, with ties going to resumable tasks - we\n      //  can do this cleanly by taking advantage of the fact that the\n      //  resumable_workers queue uses the scheduler lock, so can't change\n      //  during this call\n      // peek at the top thing (if any) in that queue, and then try to find\n      //  a ready task with higher priority\n      int resumable_priority = ResumableQueue::PRI_NEG_INF;\n      resumable_workers.peek(&resumable_priority);\n\n      // try to get a new task then\n      int task_priority = resumable_priority;\n      Task *task = TaskQueue::get_best_task(task_queues, task_priority);\n\n      // did we find work to do?\n      if(task) {\n\t// one fewer unassigned worker\n\tupdate_worker_count(0, -1);\n\n\t// we'll run the task after letting go of the lock, but update this thread's\n\t//  priority here\n\tworker_priorities[Thread::self()] = task_priority;\n\n\t// release the lock while we run the task\n\tlock.unlock();\n\n#ifndef NDEBUG\n\tbool ok =\n#endif\n\t  execute_task(task);\n\tassert(ok);  // no fault recovery yet\n\n\tlock.lock();\n\n\tworker_priorities.erase(Thread::self());\n\n\t// and we're back to being unassigned\n\tupdate_worker_count(0, +1);\n\tcontinue;\n      }\n\n      // having checked for higher-priority ready tasks, we can always\n      //  take the highest-priority resumable task, if any, and run it\n      if(!resumable_workers.empty()) {\n\tThread *yield_to = resumable_workers.get(0); // priority is irrelevant\n\tassert(yield_to != Thread::self());\n\n\t// this should only happen if we're at the max active worker count (otherwise\n\t//  somebody should have just woken this guy up earlier), and reduces the \n\t// unassigned worker count by one\n\tupdate_worker_count(0, -1);\n\n\tidle_workers.push_back(Thread::self());\n\tworker_sleep(yield_to);\n\n\t// loop around and check both queues again\n\tcontinue;\n      }\n\n      {\n\t// no ready or resumable tasks?  thumb twiddling time\n\n\t// are we shutting down?\n\tif(shutdown_flag) {\n\t  // yes, we can terminate - wake up an idler (if any) first though\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t  } else {\n\t    // nobody to wake, so -1 active/unassigned worker\n\t    update_worker_count(-1, -1, false); // ok to drop below mins\n\t    worker_terminate(0);\n\t  }\n\t  return;\n\t}\n\n\t// do we have more unassigned and idle tasks than we need?\n\tint total_idle_count = (unassigned_worker_count +\n\t\t\t\t(int)(idle_workers.size()));\n\tif(total_idle_count > cfg_max_idle_workers) {\n\t  // if there are sleeping idlers, terminate in favor of one of those - keeps\n\t  //  worker counts constant\n\t  if(!idle_workers.empty()) {\n\t    Thread *to_wake = idle_workers.back();\n\t    assert(to_wake != Thread::self());\n\t    idle_workers.pop_back();\n\t    // no net change in worker counts\n\t    worker_terminate(to_wake);\n\t    return;\n\t  }\n\t}\n\n\t// no, stay awake but suspend until there's a chance that the next iteration\n\t//  of this loop would turn out different\n\twait_for_work(old_work_counter);\n      }\n    }\n    // should never get here\n    assert(0);\n#endif\n  }\n\n  Thread *PythonThreadTaskScheduler::worker_create(bool make_active)\n  {\n    // lock is held by caller\n    ThreadLaunchParameters tlp;\n    Thread *t = Thread::create_kernel_thread<PythonThreadTaskScheduler,\n\t\t\t\t\t     &PythonThreadTaskScheduler::python_scheduler_loop>(this,\n\t\t\t\t\t\t\t\t\t\t\t\ttlp,\n\t\t\t\t\t\t\t\t\t\t\t\tcore_rsrv,\n\t\t\t\t\t\t\t\t\t\t\t\tthis);\n    all_workers.insert(t);\n    if(make_active)\n      active_workers.insert(t);\n    return t;\n  }\n \n  // called by a worker thread when it needs to wait for something (and we\n  //   should release the GIL)\n  void PythonThreadTaskScheduler::thread_blocking(Thread *thread)\n  {\n    // if this gets called before we're done initializing the interpreter,\n    //  we need a simple blocking wait\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n\n      log_py.debug() << \"waiting during initialization\";\n      bool really_blocked = try_update_thread_state(thread,\n\t\t\t\t\t\t    Thread::STATE_BLOCKING,\n\t\t\t\t\t\t    Thread::STATE_BLOCKED);\n      if(!really_blocked) return;\n\n      while(true) {\n\tlong long old_work_counter = work_counter.read_counter();\n\n\tif(!resumable_workers.empty()) {\n\t  Thread *t = resumable_workers.get(0);\n\t  assert(t == thread);\n\t  log_py.debug() << \"awake again\";\n\t  return;\n\t}\n\n\twait_for_work(old_work_counter);\n      }\n    }\n\n    // if we got here through a cffi call, the GIL has already been released,\n    //  so try to handle that case here - a call PyEval_SaveThread\n    //  if the GIL is not held will assert-fail, and while a call to\n    //  PyThreadState_Swap is technically illegal (and unsafe if python-created\n    //  threads exist), it does what we want for now\n    // NOTE: we use PyEval_{Save,Restore}Thread here even if USE_PYGILSTATE_CALLS\n    //  is defined, as a call to PyGILState_Release will destroy a thread\n    //  context - the Save/Restore take care of the actual lock, and since we\n    //  restore each python thread on the OS thread that owned it intially, the\n    //  PyGILState TLS stuff should remain consistent\n    PyThreadState *saved = (pyproc->interpreter->api->PyThreadState_Swap)(0);\n    if(saved != 0) {\n      log_py.info() << \"python worker sleeping - releasing GIL\";\n      // put it back so we can save it properly\n      (pyproc->interpreter->api->PyThreadState_Swap)(saved);\n      // would like to sanity-check that this returns the expected thread state,\n      //  but that would require taking the PythonThreadTaskScheduler's lock\n      (pyproc->interpreter->api->PyEval_SaveThread)();\n      log_py.debug() << \"SaveThread -> \" << saved;\n    } else\n      log_py.info() << \"python worker sleeping - GIL already released\";\n    \n    KernelThreadTaskScheduler::thread_blocking(thread);\n\n    if(saved) {\n      log_py.info() << \"python worker awake - acquiring GIL\";\n      log_py.debug() << \"RestoreThread <- \" << saved;\n      (pyproc->interpreter->api->PyEval_RestoreThread)(saved);\n    } else\n      log_py.info() << \"python worker awake - not acquiring GIL\";\n  }\n\n  void PythonThreadTaskScheduler::thread_ready(Thread *thread)\n  {\n    // handle the wakening of the initialization thread specially\n    if(!interpreter_ready) {\n      AutoLock<> al(lock);\n      resumable_workers.put(thread, 0);\n    } else {\n      KernelThreadTaskScheduler::thread_ready(thread);\n    }\n  }\n\n  void PythonThreadTaskScheduler::worker_terminate(Thread *switch_to)\n  {\n#ifdef USE_PYGILSTATE_CALLS\n    // nothing to do?  pythreads entry was a placeholder\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    pythreads.erase(it);\n\n#else\n    // before we can kill the kernel thread, we need to tear down the python thread\n    std::map<Thread *, PyThreadState *>::iterator it = pythreads.find(Thread::self());\n    assert(it != pythreads.end());\n    PyThreadState *pythread = it->second;\n    pythreads.erase(it);\n\n    log_py.debug() << \"destroying python thread: \" << pythread;\n    \n    // our thread should not be active\n    assert((pyproc->interpreter->api->PyThreadState_Swap)(0) == 0);\n\n    // switch to the master thread, retaining the GIL\n    log_py.debug() << \"RestoreThread <- \" << pyproc->master_thread;\n    (pyproc->interpreter->api->PyEval_RestoreThread)(pyproc->master_thread);\n\n    // clear and delete the worker thread\n    (pyproc->interpreter->api->PyThreadState_Clear)(pythread);\n    (pyproc->interpreter->api->PyThreadState_Delete)(pythread);\n\n    // release the GIL\n    PyThreadState *saved = (pyproc->interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == pyproc->master_thread);\n#endif\n\n    // TODO: tear down interpreter if last thread\n    if(shutdown_flag && pythreads.empty())\n      pyproc->destroy_interpreter();\n\n    KernelThreadTaskScheduler::worker_terminate(switch_to);\n  }\n\n  \n  ////////////////////////////////////////////////////////////////////////\n  //\n  // class LocalPythonProcessor\n\n  LocalPythonProcessor::LocalPythonProcessor(Processor _me, int _numa_node,\n                                             CoreReservationSet& crs,\n                                             size_t _stack_size,\n\t\t\t\t\t     const std::vector<std::string>& _import_modules,\n\t\t\t\t\t     const std::vector<std::string>& _init_scripts)\n    : ProcessorImpl(_me, Processor::PY_PROC)\n    , numa_node(_numa_node)\n    , import_modules(_import_modules)\n    , init_scripts(_init_scripts)\n    , interpreter(0)\n    , ready_task_count(stringbuilder() << \"realm/proc \" << me << \"/ready tasks\")\n  {\n    task_queue.set_gauge(&ready_task_count);\n    deferred_spawn_cache.clear();\n\n    CoreReservationParameters params;\n    params.set_num_cores(1);\n    params.set_numa_domain(numa_node);\n    params.set_alu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_fpu_usage(params.CORE_USAGE_EXCLUSIVE);\n    params.set_ldst_usage(params.CORE_USAGE_SHARED);\n    params.set_max_stack_size(_stack_size);\n\n    std::string name = stringbuilder() << \"Python\" << numa_node << \" proc \" << _me;\n\n    core_rsrv = new CoreReservation(name, crs, params);\n\n    sched = new PythonThreadTaskScheduler(this, *core_rsrv);\n    sched->add_task_queue(&task_queue);\n  }\n\n  LocalPythonProcessor::~LocalPythonProcessor(void)\n  {\n    delete core_rsrv;\n    delete sched;\n  }\n\n  // starts worker threads and performs any per-processor initialization\n  void LocalPythonProcessor::start_threads(void)\n  {\n    // finally, fire up the scheduler\n    sched->start();\n  }\n\n  void LocalPythonProcessor::shutdown(void)\n  {\n    log_py.info() << \"shutting down\";\n\n    sched->shutdown();\n    deferred_spawn_cache.flush();\n  }\n\n  void LocalPythonProcessor::create_interpreter(void)\n  {\n    assert(interpreter == 0);\n  \n    // create a python interpreter that stays entirely within this thread\n    interpreter = new PythonInterpreter;\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (interpreter->api->PyGILState_Ensure)();\n    assert(gilstate = PyGILState_UNLOCKED);\n#else\n    master_thread = (interpreter->api->PyThreadState_Get)();\n#endif\n\n    // always need the python threading module\n    interpreter->import_module(\"threading\");\n    \n    // perform requested initialization\n    for(std::vector<std::string>::const_iterator it = import_modules.begin();\n\tit != import_modules.end();\n\t++it)\n      interpreter->import_module(*it);\n\n    for(std::vector<std::string>::const_iterator it = init_scripts.begin();\n\tit != init_scripts.end();\n\t++it)\n      interpreter->run_string(*it);\n\n    // default state is GIL _released_\n#ifdef USE_PYGILSTATE_CALLS\n    (interpreter->api->PyGILState_Release)(gilstate);\n#else\n    PyThreadState *saved = (interpreter->api->PyEval_SaveThread)();\n    log_py.debug() << \"SaveThread -> \" << saved;\n    assert(saved == master_thread);\n#endif\n  }\n\n  void LocalPythonProcessor::destroy_interpreter(void)\n  {\n    assert(interpreter != 0);\n\n    log_py.info() << \"destroying interpreter\";\n\n    // take GIL with master thread\n#ifdef USE_PYGILSTATE_CALLS\n    PyGILState_STATE gilstate = (interpreter->api->PyGILState_Ensure)();\n    assert(gilstate == PyGILState_UNLOCKED);\n#else\n    assert((interpreter->api->PyThreadState_Swap)(0) == 0);\n    log_py.debug() << \"RestoreThread <- \" << master_thread;\n    (interpreter->api->PyEval_RestoreThread)(master_thread);\n#endif\n\n    // during shutdown, the threading module tries to remove the Thread object\n    //  associated with this kernel thread - if that doesn't exist (because we're\n    //  shutting down from a different thread that we initialized the interpreter\n    //  _and_ nobody called threading.current_thread() from this kernel thread),\n    //  we'll get a KeyError in threading.py\n    // resolve this by calling threading.current_thread() here, using __import__\n    //  to deal with the case where 'import threading' never got called\n    (interpreter->api->PyRun_SimpleString)(\"__import__('threading').current_thread()\");\n\n    delete interpreter;\n    interpreter = 0;\n    master_thread = 0;\n  }\n  \n  bool LocalPythonProcessor::perform_task_registration(LocalPythonProcessor::TaskRegistration *treg)\n  {\n    // first, make sure we haven't seen this task id before\n    if(task_table.count(treg->func_id) > 0) {\n      log_py.fatal() << \"duplicate task registration: proc=\" << me << \" func=\" << treg->func_id;\n      assert(0);\n    }\n\n    // next, see if we have a Python function to register\n    const PythonSourceImplementation *psi = treg->codedesc->find_impl<PythonSourceImplementation>();\n    if(!psi) {\n      log_py.fatal() << \"invalid code descriptor for python proc: \" << *(treg->codedesc);\n      assert(0);\n    }\n\n    PyObject *fnptr = interpreter->find_or_import_function(psi);\n    assert(fnptr != 0);\n\n    log_py.info() << \"task \" << treg->func_id << \" registered on \" << me << \": \" << *(treg->codedesc);\n\n    TaskTableEntry &tte = task_table[treg->func_id];\n    tte.fnptr = fnptr;\n    tte.user_data.swap(treg->user_data);\n\n    delete treg->codedesc;\n    delete treg;\n\n    return true;\n  }\n\n  void LocalPythonProcessor::enqueue_task(Task *task)\n  {\n    task_queue.enqueue_task(task);\n  }\n\n  void LocalPythonProcessor::enqueue_tasks(Task::TaskList& tasks)\n  {\n    task_queue.enqueue_tasks(tasks);\n  }\n\n  void LocalPythonProcessor::spawn_task(Processor::TaskFuncID func_id,\n\t\t\t\t\tconst void *args, size_t arglen,\n\t\t\t\t\tconst ProfilingRequestSet &reqs,\n\t\t\t\t\tEvent start_event,\n\t\t\t\t\tGenEventImpl *finish_event,\n\t\t\t\t\tEventImpl::gen_t finish_gen,\n\t\t\t\t\tint priority)\n  {\n    // create a task object for this\n    Task *task = new Task(me, func_id, args, arglen, reqs,\n\t\t\t  start_event, finish_event, finish_gen, priority);\n    get_runtime()->optable.add_local_operation(finish_event->make_event(finish_gen), task);\n\n    enqueue_or_defer_task(task, start_event, &deferred_spawn_cache);\n  }\n\n  void LocalPythonProcessor::add_to_group(ProcessorGroup *group)\n  {\n    // add the group's task queue to our scheduler too\n    sched->add_task_queue(&group->task_queue);\n  }\n\n  void LocalPythonProcessor::register_task(Processor::TaskFuncID func_id,\n                                           CodeDescriptor& codedesc,\n                                           const ByteArrayRef& user_data)\n  {\n    TaskRegistration *treg = new TaskRegistration;\n    treg->proc = this;\n    treg->func_id = func_id;\n    treg->codedesc = new CodeDescriptor(codedesc);\n    treg->user_data = user_data;\n    sched->add_internal_task(treg);\n  }\n\n  void LocalPythonProcessor::execute_task(Processor::TaskFuncID func_id,\n\t\t\t\t\t  const ByteArrayRef& task_args)\n  {\n    std::map<Processor::TaskFuncID, TaskTableEntry>::const_iterator it = task_table.find(func_id);\n    if(it == task_table.end()) {\n      // TODO: remove this hack once the tools are available to the HLR to call these directly\n      if(func_id < Processor::TASK_ID_FIRST_AVAILABLE) {\n\tlog_py.info() << \"task \" << func_id << \" not registered on \" << me << \": ignoring missing legacy setup/shutdown task\";\n\treturn;\n      }\n      log_py.fatal() << \"task \" << func_id << \" not registered on \" << me;\n      assert(0);\n    }\n\n    const TaskTableEntry& tte = it->second;\n\n    log_py.debug() << \"task \" << func_id << \" executing on \" << me << \": \" << ((void *)(tte.fnptr));\n\n    PyObject *arg1 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)task_args.base(),\n\t\t\t\t\t\t   task_args.size());\n    assert(arg1 != 0);\n    PyObject *arg2 = (interpreter->api->PyByteArray_FromStringAndSize)(\n                                                   (const char *)tte.user_data.base(),\n\t\t\t\t\t\t   tte.user_data.size());\n    assert(arg2 != 0);\n    // TODO: make into a Python realm.Processor object\n    PyObject *arg3 = (interpreter->api->PyLong_FromUnsignedLong)(me.id);\n    assert(arg3 != 0);\n\n    PyObject *args = (interpreter->api->PyTuple_New)(3);\n    assert(args != 0);\n    (interpreter->api->PyTuple_SetItem)(args, 0, arg1);\n    (interpreter->api->PyTuple_SetItem)(args, 1, arg2);\n    (interpreter->api->PyTuple_SetItem)(args, 2, arg3);\n\n    //printf(\"args = \"); (interpreter->api->PyObject_Print)(args, stdout, 0); printf(\"\\n\");\n\n    PyObject *res = (interpreter->api->PyObject_CallObject)(tte.fnptr, args);\n\n    (interpreter->api->Py_DecRef)(args);\n\n    //printf(\"res = \"); PyObject_Print(res, stdout, 0); printf(\"\\n\");\n    if(res != 0) {\n      (interpreter->api->Py_DecRef)(res);\n    } else {\n      log_py.fatal() << \"python exception occurred within task:\";\n      (interpreter->api->PyErr_PrintEx)(0);\n      (interpreter->api->Py_Finalize)(); // otherwise Python doesn't flush its buffers\n      assert(0);\n    }\n  }\n\n  namespace Python {\n\n    ////////////////////////////////////////////////////////////////////////\n    //\n    // class PythonModule\n\n    /*static*/ std::vector<std::string> PythonModule::extra_import_modules;\n\n    PythonModule::PythonModule(void)\n      : Module(\"python\")\n      , cfg_num_python_cpus(0)\n      , cfg_use_numa(false)\n      , cfg_stack_size(2 << 20)\n    {\n    }\n\n    PythonModule::~PythonModule(void)\n    {}\n\n    /*static*/ void PythonModule::import_python_module(const char *module_name)\n    {\n      extra_import_modules.push_back(module_name);\n    }\n\n    /*static*/ Module *PythonModule::create_module(RuntimeImpl *runtime,\n                                                 std::vector<std::string>& cmdline)\n    {\n      // create a module to fill in with stuff - we'll delete it if numa is\n      //  disabled\n      PythonModule *m = new PythonModule;\n\n      // first order of business - read command line parameters\n      {\n        CommandLineParser cp;\n\n        cp.add_option_int(\"-ll:py\", m->cfg_num_python_cpus)\n\t  .add_option_int(\"-ll:pynuma\", m->cfg_use_numa)\n\t  .add_option_int_units(\"-ll:pystack\", m->cfg_stack_size, 'm')\n\t  .add_option_stringlist(\"-ll:pyimport\", m->cfg_import_modules)\n\t  .add_option_stringlist(\"-ll:pyinit\", m->cfg_init_scripts);\n\n        bool ok = cp.parse_command_line(cmdline);\n        if(!ok) {\n          log_py.fatal() << \"error reading Python command line parameters\";\n          assert(false);\n        }\n      }\n\n      // add extra module imports requested by the application\n      m->cfg_import_modules.insert(m->cfg_import_modules.end(),\n                                   extra_import_modules.begin(),\n                                   extra_import_modules.end());\n\n      // if no cpus were requested, there's no point\n      if(m->cfg_num_python_cpus == 0) {\n        log_py.debug() << \"no Python cpus requested\";\n        delete m;\n        return 0;\n      }\n\n#ifndef REALM_USE_DLMOPEN\n      // Multiple CPUs are only allowed if we're using dlmopen.\n      if(m->cfg_num_python_cpus > 1) {\n        log_py.fatal() << \"support for multiple Python CPUs is not available: recompile with USE_DLMOPEN\";\n        assert(false);\n      }\n#endif\n\n      // get number/sizes of NUMA nodes -\n      //   disable (with a warning) numa binding if support not found\n      if(m->cfg_use_numa) {\n        std::map<int, NumaNodeCpuInfo> cpuinfo;\n        if(numasysif_numa_available() &&\n           numasysif_get_cpu_info(cpuinfo) &&\n           !cpuinfo.empty()) {\n          // filter out any numa domains with insufficient core counts\n          int cores_needed = m->cfg_num_python_cpus;\n          for(std::map<int, NumaNodeCpuInfo>::const_iterator it = cpuinfo.begin();\n              it != cpuinfo.end();\n              ++it) {\n            const NumaNodeCpuInfo& ci = it->second;\n            if(ci.cores_available >= cores_needed) {\n              m->active_numa_domains.insert(ci.node_id);\n            } else {\n              log_py.warning() << \"not enough cores in NUMA domain \" << ci.node_id << \" (\" << ci.cores_available << \" < \" << cores_needed << \")\";\n            }\n          }\n        } else {\n          log_py.warning() << \"numa support not found (or not working)\";\n          m->cfg_use_numa = false;\n        }\n      }\n\n      // if we don't end up with any active numa domains,\n      //  use NUMA_DOMAIN_DONTCARE\n      // actually, use the value (-1) since it seems to cause link errors!?\n      if(m->active_numa_domains.empty())\n        m->active_numa_domains.insert(-1 /*CoreReservationParameters::NUMA_DOMAIN_DONTCARE*/);\n\n      return m;\n    }\n\n    // do any general initialization - this is called after all configuration is\n    //  complete\n    void PythonModule::initialize(RuntimeImpl *runtime)\n    {\n      Module::initialize(runtime);\n    }\n\n    // create any processors provided by the module (default == do nothing)\n    //  (each new ProcessorImpl should use a Processor from\n    //   RuntimeImpl::next_local_processor_id)\n    void PythonModule::create_processors(RuntimeImpl *runtime)\n    {\n      Module::create_processors(runtime);\n\n      for(std::set<int>::const_iterator it = active_numa_domains.begin();\n          it != active_numa_domains.end();\n          ++it) {\n        int cpu_node = *it;\n        for(int i = 0; i < cfg_num_python_cpus; i++) {\n          Processor p = runtime->next_local_processor_id();\n          ProcessorImpl *pi = new LocalPythonProcessor(p, cpu_node,\n                                                       runtime->core_reservation_set(),\n                                                       cfg_stack_size,\n\t\t\t\t\t\t       cfg_import_modules,\n\t\t\t\t\t\t       cfg_init_scripts);\n          runtime->add_processor(pi);\n\n          // create affinities between this processor and system/reg memories\n          // if the memory is one we created, use the kernel-reported distance\n          // to adjust the answer\n          std::vector<MemoryImpl *>& local_mems = runtime->nodes[Network::my_node_id].memories;\n          for(std::vector<MemoryImpl *>::iterator it2 = local_mems.begin();\n              it2 != local_mems.end();\n              ++it2) {\n            Memory::Kind kind = (*it2)->get_kind();\n            if((kind != Memory::SYSTEM_MEM) && (kind != Memory::REGDMA_MEM))\n              continue;\n\n            Machine::ProcessorMemoryAffinity pma;\n            pma.p = p;\n            pma.m = (*it2)->me;\n\n            // use the same made-up numbers as in\n            //  runtime_impl.cc\n            if(kind == Memory::SYSTEM_MEM) {\n              pma.bandwidth = 100;  // \"large\"\n              pma.latency = 5;      // \"small\"\n            } else {\n              pma.bandwidth = 80;   // \"large\"\n              pma.latency = 10;     // \"small\"\n            }\n\n            runtime->add_proc_mem_affinity(pma);\n          }\n        }\n      }\n    }\n\n    // clean up any common resources created by the module - this will be called\n    //  after all memories/processors/etc. have been shut down and destroyed\n    void PythonModule::cleanup(void)\n    {\n      Module::cleanup();\n    }\n\n  }; // namespace Python\n\n}; // namespace Realm\n"
    },
    "skipped": [
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/language/examples/pagerank/sample.dat",
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/language/examples/mssp/small/result_3.dat",
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/language/examples/mssp/small/edges.dat",
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/doc/arch/persistent/hdf5/figs/high-level-design.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout.pptx",
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.png",
        "/var/tmp/sochat1/spack-stage/spack-stage-legion-20.03.0-toqhnyoqpla63iqrbjorgbmxse5csguu/spack-src/doc/arch/persistent/hdf5/figs/hdf5-layout-climate.pdf"
    ],
    "total_files": 2039
}